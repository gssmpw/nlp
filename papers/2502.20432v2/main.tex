\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\PassOptionsToPackage{numbers}{natbib}
\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3.5cm,right=3.5cm,marginparwidth=2.5cm]{geometry}
  
% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{natbib} 
\usepackage{subcaption}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{caption} % Enables \ContinuedFloat for multi-page tables
\usepackage{float} % Helps manage table placement
\usepackage{placeins} % Prevents floating elements from being split


\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage[title]{appendix}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multicol}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{enumitem}
\usepackage{titlesec}


\usepackage{blindtext}
\usepackage{lmodern}
\usepackage{titlesec} 
\usepackage{xcolor}
\definecolor{gaoliang}{RGB}{192,44,56} 
\definecolor{heye}{RGB}{26, 104, 64} 
\newcommand{\sig}[1]{\textcolor{gaoliang}{\textbf{#1}}}

\font\myfont=cmr12 at 13pt

\title{
    \vspace*{-1cm} 
    \rule{\linewidth}{1.5pt} \\[0.3cm]
    {\bfseries \fontsize{13pt}{18pt}\selectfont Large Language Model Strategic Reasoning Evaluation\\ through Behavioral Game Theory} \\
    \rule{\linewidth}{1.5pt}
}
\author{
    \myfont Jingru Jia*, Zehua Yuan*, Junhao Pan, Paul E. McNamara, Deming Chen \\[0.8cm] 
    \myfont University of Illinois Urbana-Champaign
    \\[0.2cm]
    \texttt{\{jingruj3, zehuay2, jpan22, mcnamar1, dchen\} @ illinois.edu} % Email addresses in a typewriter font
}

\date{}



\begin{document}

\maketitle

\vspace{0.5cm}

\begin{abstract}
Strategic decision-making involves interactive reasoning where agents adapt their choices in response to others, yet existing evaluations of large language models (LLMs) often emphasize Nash Equilibrium (NE) approximation, overlooking the mechanisms driving their strategic choices. To bridge this gap, we introduce an evaluation framework grounded in behavioral game theory, disentangling reasoning capability from contextual effects. Testing 22 state-of-the-art LLMs, we find that GPT-o3-mini, GPT-o1, and DeepSeek-R1 dominate most games yet also demonstrate that the model scale alone does not determine performance. In terms of prompting enhancement, Chain-of-Thought (CoT) prompting is not universally effective, as it increases strategic reasoning only for models at certain levels while providing limited gains elsewhere. Additionally, we investigate the impact of encoded demographic features on the models, observing that certain assignments impact the decision-making pattern. For instance, GPT-4o shows stronger strategic reasoning with female traits than males, while Gemma assigns higher reasoning levels to heterosexual identities compared to other sexual orientations, indicating inherent biases. These findings underscore the need for ethical standards and contextual alignment to balance improved reasoning with fairness. 
\end{abstract}


\textbf{Keyword:} Multi-agent, LLM, Reasoning, Fairness, Game Theory

\vspace{1cm}

\section{Introduction}
The rapid development of large language models (LLMs) and generative AI has significantly broadened their applications, transitioning from basic text generation \cite{kolasani2023optimizing} and completion tasks to serving as sophisticated agents \cite{article,wu2024autogen,gan2024application,dong2023towards}. 
% Unlike traditional reinforcement learning-based AI, which excels in tasks like defeating human chess players through extensive training to learn optimal moves \cite{silver2018general}, deploying AI agents in interactive environments with other agents or humans presents a broader and more complex challenge due to the lack of standardization \cite{guo2024large,ijcai2024p890,han2024llmmultiagentsystemschallenges,jin2024llms}.
While benchmarks for language and math tasks assess specific abilities of LLMs \cite{hoffmann2022training,gema2024we,wei2024measuring}, practical deployment alongside other agents or humans presents a broader and more complex challenge. This necessitates evaluating reasoning capabilities that integrate and extend these individual skills on language, vision, or math \cite{guo2024large,ijcai2024p890,han2024llmmultiagentsystemschallenges,jin2024llms,riemer2025positiontheorymindbenchmarks}.

When AI agents are deployed in real-world applications, they often encounter decision-making scenarios requiring cooperation or competition with other entities—a process known as strategic reasoning \cite{zhang2024llm,hao2024llm,gandhi2023strategic}. One-shot strategic reasoning specifically refers to an agent's ability to make a single, one-time action based on its reasoning in an interactive multi-agent environment.
Consider an AI agent participating in a high-stakes negotiation, such as allocating resources in a disaster relief effort among multiple parties. The agent must evaluate the needs and strategies of other agents, analyze available resources, and understand the broader context to make a single, impactful decision. This one-shot reasoning is critical, as the decision cannot be altered once made, and it directly affects the outcomes for all participants. Such scenarios highlight the importance of robust frameworks for evaluating and improving one-shot strategic reasoning in AI systems \cite{10.1145/3670691,dalrymple2024towards}.
% This raises critical questions about evaluating AI agents: How can an agent reason strategically in the absence of idealized conditions assumed by traditional game theory? Moreover, how can we develop evaluation metrics that account for the stochastic and context-dependent nature of real-world interactions, moving beyond the limitations of NE as a singular indicator?
% For instance, consider deploying an AI agent as a stock trader. In a dynamic and competitive stock market with abundant information and numerous competing agents, the core questions become: how can the agent make optimal decisions, and how can those decisions be effectively evaluated?


% Strategic reasoning, in this context, refers to dynamic and iterative decision-making that unfolds in response to other agents' actions.
Previous research on LLMs' strategic reasoning has predominantly focused on their ability to achieve Nash Equilibrium (NE) in game-theoretic settings \cite{zhang2024llm,wang2024strategic,gandhi2023strategic,hua2024game}. While NE is a cornerstone concept in evaluating rational behavior, relying solely on whether LLMs can reach NE provides an incomplete assessment of their reasoning capabilities \cite{huang2024far,herr2024large}. 
% This approach is analogous to testing their ability to solve theoretical exam problems in game theory, rather than probing their deeper strategic reasoning processes in interactive contexts.
The primary limitation of using NE as a measurement lies in its strict assumptions and focus on outcomes rather than the decision-making processes \cite{baba2012toward}. NE assumes perfect rationality, which fails to account for the variability and bounded reasoning capability inherent in real-world problems. For LLMs—probabilistic models trained on human-generated data—this assumption is particularly problematic \cite{zellinger2025rational,liu2024exploring}. Their stochastic nature makes NE impractical as a comprehensive evaluation metric, and its assumption of perfect rationality falls into the circular reasoning fallacy when assessing LLMs' rationality.


In this work, we move beyond traditional game theory and instead explore the factors shaping their strategic behavior. Drawing on the Truncated Quantal Response Equilibrium (TQRE) from behavioral game theory \cite{rogers2009heterogeneous}, we identify two key causes of deviations from optimal responses: (1) reasoning capability, reflecting an agent's ability and depth to think rationally and adaptively, and (2) contextual structure, which influences reasoning through environmental complexity and acts as a confounder. Based on these factors, we develop a mechanism that controls for contextual effects and evaluates LLMs' strategic reasoning across multi-level contexts and games. In summary, our key contributions are as follows:

\begin{enumerate}
    \item We propose an evaluation framework to systematically assess reasoning capability while accounting for contextual complexity. Testing 22 state-of-the-art (SOTA) LLMs on 13 abstracted real-world games using one-shot prompting. We find that while massive models like GPT-o1, GPT-o3-mini, and DeepSeek-R1 lead in most tasks, smaller models can outperform or match their reasoning in others. This highlights the potential of task-specific usability over sheer model size and architecture.
    \item We evaluate the impact of Chain-of-Thought (CoT) prompting on reasoning across all models and find that it is not a universal enhancer. CoT promotes reasoning when a model's capability is limited. However, it can introduce distractions for LLMs with higher reasoning levels, leading to suboptimal choices. This finding aligns with recent studies showing that while CoT improves mathematical and sequential reasoning, it can also amplify distractions, varying its overall effectiveness.
    \item Lastly, embedding demographic features before reasoning revealed biases even in advanced models like DeepSeek-R1 or GPT-4o when processing minority features. In addition, distilled knowledge could also cause bias injected into previous non-biased models such as DeepSeek-R1-Distilled-LLaMA. This suggests that superior reasoning ability does not necessarily yield desirable or ethical outcomes, highlighting the need for a balanced approach and calibration in future LLM development.

\end{enumerate}

\section{Background and Related Literature}
\subsection{LLM Behavior Study}

\textbf{Individual Decision-making} \space
Recent research \cite{costarelli2024gamebench,zhang2024llm,wang2024strategic} has increasingly focused on investigating the behavior of LLMs as both individual decision-makers and strategic decision-makers, exploring their alignment with human behavior \cite{brand2023using,chen2023emergence,horton2023large,dognin2024contextual}. From an economic behavior perspective, LLMs demonstrate patterns resembling human behaviors, such as preferences under uncertainty and risk aversion \cite{jia2024decision}. From a social science perspective, LLMs demonstrate human-like behavior in moral judgment experiments, exhibiting reasoning patterns similar to those of humans \cite{dillion2023can, jia2024experimentalstudycompetitivemarket,scherrer2024evaluating}.

In the context of individual decision-making, several studies \cite{jia2024decision, meng2024ai, horton2023large} have revealed notable parallels between LLMs and humans, including differences in risk preferences, probability weighting, and loss aversion. Others \cite{gupta2023bias, jia2024decision} highlight decision-making inconsistencies in LLMs when profile information with minority attributes is embedded, where LLMs may demonstrate contradictory interpretations of group-specific behaviors. 
% These studies provide valuable insights into LLMs’ individual decision-making patterns, laying a strong foundation for exploring more complex, interactive reasoning.

\textbf{Strategic Decision-making}\space
Recent research also evaluates LLMs in strategic contexts, where reasoning involves dynamic, adaptive responses in cooperative or competitive environments. 
% Existing evaluations of LLM strategic reasoning are predominantly grounded in game theory. 
For instance, several studies explore LLMs’ abilities in achieving rationality within game-theoretic frameworks, such as mixed-strategy NE games \cite{silva2024large} and classic strategic games like the prisoner’s dilemma \cite{brookins2023playing, guo2023gpt}. Some works investigate the adaptability of LLMs to various prompts that influence cooperative and competitive behavior \cite{phelps2023machine}, and others evaluate their capacity to replicate human-like reasoning patterns in multi-agent settings \cite{aher2023using, xu2024magic}. While these studies provide an important starting point, many are limited to binary assessments of whether LLMs meet NE \cite{hua2024game, duan2024gtbench} without exploring the underlying mechanisms driving their strategic reasoning capabilities. This creates a challenge in explaining deviations from optimal strategies and introduces circular reasoning in LLM evaluation, where the assumption of perfect rationality undermines the very purpose of evaluating reasoning processes. Notably, \cite{fan2024can} and \cite{zhang2024k} help relieve these issues by exploring reasoning through stepwise evaluation and Cognitive Hierarchy (CH) models, respectively, but remain constrained by deterministic context assumptions.
% Despite these efforts, the approaches heavily reliant on NE face challenges, including unrealistic assumptions of perfect rationality, an inability to explain the reasons behind deviations from optimal strategies, and a circular logic in LLM evaluation, where assuming perfect rationality undermines the purpose of testing reasoning processes. Notably, \citet{fan2024can} and \citet{zhang2024k} make promising initial attempts in addressing these issues by exploring reasoning processes through stepwise evaluation and Cognitive Hierarchy (CH) models, respectively.


% Another study explores reasoning capabilities across different personality profiles but is constrained by limited game scenarios and contextual diversity \cite{gupta2023bias}.

% Despite these efforts, existing approaches face three significant challenges:
% \begin{enumerate}
%     \item \textbf{Rationality Boundaries}: NE assumes that all agents possess perfect rationality and infinite reasoning capacity, which contradicts observed human behavior and the bounded rationality inherent in LLMs.
%     \item \textbf{Poor Predictive Power}: NE often fails to predict outcomes in experimental and real-world games, as players frequently deviate from optimal strategies due to bounded rationality and heuristic-based decision-making.
%     \item \textbf{Circular Reasoning in LLM Evaluation}: Applying NE to LLMs introduces a circular logic problem—assuming LLMs are perfectly rational by design undermines the purpose of testing their reasoning processes through NE.
% \end{enumerate}

% While some works based on Cognitive Hierarchy (CH) models have offered deeper insights \cite{zhang2024k}, they still struggle with limited predictive power due to their deterministic design. These limitations underscore the need for new evaluation frameworks capable of addressing the complexities of LLM reasoning and strategic decision-making.
% 1. Individual decision making: our previous paper, and other papers which are worse than ours.

% 2. Strategic decision making: LLM game theory papers. Elicit the research gap that existed papers only rely on NE. 
% (1)Rationality Boundaries: NE assumes that all players are perfectly rational with infinite reasoning capacity, which is inconsistent with observed human behavior.
% (2)Poor Predictive Power: NE often fails to predict outcomes in experimental and real-world games, where players deviate from optimal strategies due to bounded rationality and heuristic-based decisions.
% (3)Circular Reasoning in LLMs: Applying NE to LLMs creates a circular logic problem—if LLMs are assumed to be perfectly rational by design, testing for their ability to reach NE adds little insight into their actual reasoning process.

\subsection{Behavioral Game Theory}

Behavioral game theory models strategic interactions by incorporating bounded rationality, decision noise, and social preferences, addressing deviations from Nash Equilibrium observed in real-world and experimental games \citep{camerer1997progress, camerer2015behavioral}.

One key finding in behavioral game theory is the prevalence of stochastic decision-making, where players respond probabilistically to payoffs rather than always choosing optimally \citep{haile2008empirical, chen2012bounded}. For example, in private-value auctions, where bidders have independent valuations, overbidding is common due to bounded rationality and noise \citep{goeree2001ten}. In public goods games, where individuals decide how much to contribute to a shared resource, contributions initially exceed Nash Equilibrium (NE) predictions due to fairness before declining with payoff adaptation \citep{fehr1999theory}. These behaviors are effectively modeled by Quantal Response Equilibrium (QRE), which incorporates decision noise to explain such deviations \citep{mckelvey1995quantal}. 

Another key insight is the heterogeneity in reasoning levels, where players vary from relying on simple heuristics to engaging in iterative strategic thinking \citep{chong2016generalized, camerer2004cognitive, bardsley2010explaining}. Cognitive hierarchy models capture this variation, showing that limited reasoning often dominates in complex environments \citep{camerer2004cognitive, stahl1994experimental}.

Structured environments allow limited reasoning to yield near-optimal payoffs, while complex settings amplify stochastic deviations \citep{mckelvey1998quantal}. For instance, in real-world experiments on bargaining games, where players take turns making and responding to offers, players deviate further from Nash Equilibrium (NE) as complexity increases \citep{goeree2002quantal}. Truncated Quantal Response Equilibrium (TQRE) \citep{rogers2009heterogeneous} extends prior probabilistic models by integrating bounded rationality and reasoning depth, capturing the interplay between environmental structure, decision noise, and strategic behavior.

Behavioral game theory offers superior predictive power and richer insights into strategic interactions by incorporating bounded rationality, decision noise, and heterogeneous reasoning, addressing complexities that traditional game theory cannot fully capture \citep{costa2006cognition, camerer2004cognitive, camerer2004behavioral, camerer2011behavioral}. Building on TQRE, our work is the first to focus on eliciting and evaluating reasoning capabilities in LLMs while carefully accounting for contextual confounders in multi-agent environments.

\section{Preliminary}

% Strategic decision-making is fundamentally grounded in game theory, which provides a framework for analyzing interactions between rational agents. A more realistic version incorporates ideas from behavioral economics, relaxing strict assumptions of perfect rationality and infinite reasoning capacity, and becomes behavioral game theory. The Truncated Quantal Response Equilibrium (TQRE) model, which integrates stochastic decision-making and bounded reasoning, is particularly suited for evaluating LLMs as it aligns with their probabilistic nature and limited cognitive depth.

% In a strategic-form game $\Gamma = (N, \{A_i\}_{i=1}^N, \{u_i\}_{i=1}^N)$, players choose strategies from their respective action sets to maximize their utility functions. A strategy profile $(\sigma_1, \ldots, \sigma_N)$ is a Nash Equilibrium if no player can unilaterally improve their expected utility. Formally, for all players $i \in N$:
% \[
% U_i(\sigma_i^*, \sigma_{-i}^*) \geq U_i(\sigma_i, \sigma_{-i}^*), \quad \forall \sigma_i \in \Delta(A_i),
% \]
% where $\sigma_i$ represents a mixed strategy over the action set $A_i$, and $\sigma_{-i}$ denotes the strategies of all other players.

% The expected utility for player $i$ under $(\sigma_i, \sigma_{-i})$ is defined as:
% \[
% U_i(\sigma_i, \sigma_{-i}) = \sum_{a \in A} \sigma_i(a_i) \sigma_{-i}(a_{-i}) u_i(a).
% \]

% To address the limitations of NE, QRE introduces stochastic decision-making, allowing players to select actions with probabilities proportional to their expected payoffs. The logit specification for QRE defines the probability of player $i$ choosing action $a_{ij}$ as:
% \[
% p_{ij}(\lambda) = \frac{\exp(\lambda U_{ij})}{\sum_{k \in A_i} \exp(\lambda U_{ik})},
% \]
% where $\lambda > 0$ is the rationality parameter, and $U_{ij}$ represents the expected utility of action $a_{ij}$ given opponents’ strategies:
% \[
% U_{ij} = \sum_{a_{-i} \in A_{-i}} \sigma_{-i}(a_{-i}) u_i(a_{ij}, a_{-i}).
% \]

% TQRE extends QRE by incorporating hierarchical reasoning inspired by Cognitive Hierarchy (CH). Players' reasoning capabilities are captured by their levels of strategic reasoning:
% Level-0 players act randomly, choosing uniformly across their available actions.Higher-level players iteratively best respond to the aggregated behavior of lower-level players. The probability of being a Level-$k$ player is model by a Poisson distribution with mean $\tau$:
% \[
% g(k) = \frac{e^{-\tau} \tau^k}{k!}.
% \]

% A Level-$k$ player in TQRE evaluates the expected utility of action $a_{ij}$ based on the probabilistic strategies of all reasoning levels up to $k-1$. This utility is given by:
% \[
% U_{ij}^{(k)} = \sum_{a_{-i} \in A_{-i}} \prod_{h=0}^{k-1} \sigma_{-i}^{(h)}(a_{-i}) u_i(a_{ij}, a_{-i}),
% \]
% where $\sigma_{-i}^{(h)}$ represents the strategy profile of Level-$h$ players.

% Given this utility, the probability of a Level-$k$ player choosing $a_{ij}$ follows the logit form:
% \[
% p_{ij}^{(k)}(\lambda) = \frac{\exp(\lambda U_{ij}^{(k)})}{\sum_{a \in A_i} \exp(\lambda U_{ia}^{(k)})}.
% \]

% To account for heterogeneous rationality across players, the mixed strategy $\sigma_{ij}^{(k)}$ integrates over a distribution of rationality parameters:
% \[
% \sigma_{ij}^{(k)} = \int_0^\infty p_{ij}^{(k)}(\lambda) f_i(\lambda) d\lambda,
% \]
% where $f_i(\lambda)$ is the density function of $\lambda$ for player $i$.

% \textbf{Parameters to be estimated}

% The parameter $\lambda > 0$ allows the model to account for varying degrees of bounded rationality among players. Larger values of $\lambda$ correspond to near-deterministic behavior, where players consistently select the action with the highest expected utility, while smaller values indicate noisier, more stochastic decision-making. Parameter $\tau > 0$ enables the model to integrate heterogeneous levels of reasoning into the probabilistic decision-making process. It reflects the average cognitive depth of players, with higher values indicating that players are, on average, reasoning through more levels of strategic thought. 


Strategic decision-making is fundamentally grounded in game theory, which provides a framework for analyzing interactions between rational agents. It evolves into behavioral game theory by incorporating bounded rationality and cognitive limits. The TQRE model, which integrates stochastic decision-making and bounded reasoning, is particularly suited for evaluating LLMs as it aligns with their probabilistic nature and limited cognitive depth.

In a strategic-form game $\Gamma = (N, \{A_i\}_{i=1}^N, \{u_i\}_{i=1}^N)$, players choose strategies from their respective action sets to maximize their utility functions. A strategy profile $(\sigma_1, \ldots, \sigma_N)$ achieves a NE if no player can unilaterally improve their expected utility. Formally, for all players $i \in N$:
$U_i(\sigma_i^*, \sigma_{-i}^*) \geq U_i(\sigma_i, \sigma_{-i}^*), \quad \forall \sigma_i \in \Delta(A_i)$,
where $\sigma_i$ represents a mixed strategy over the action set $A_i$, and $\sigma_{-i}$ denotes the strategies of all other players.

The expected utility (payoff values) for player $i$ under $(\sigma_i, \sigma_{-i})$ depends on both their own strategy and the strategies of others:
\[
U_i(\sigma_i, \sigma_{-i}) = \sum_{a \in A} \sigma_i(a_i) \sigma_{-i}(a_{-i}) u_i(a).
\]
Although NE provides a sound theoretical benchmark and is useful in computational tasks, its assumptions of infinite reasoning capacity and deterministic behavior often fail to align with empirical observations \citep{camerer2011behavioral,plott2008handbook,nagel1995unraveling,fehr1999theory,cooper1990selection}.

Quantal Response Equilibrium (QRE) introduces stochastic decision-making into the framework to address these limitations. In QRE, players choose actions with probabilities proportional to the expected utilities of the actions. The probability of player \(i\) selecting action \(a_{ij}\) is defined as:
\[
p_{ij}(\lambda, U) = \frac{\exp(\lambda U_{ij})}{\sum_{a \in A_i} \exp(\lambda U_{ia})},
\]
where \(U_{ij}\) is the expected utility of action \(a_{ij}\), and \(\lambda > 0\) controls decision precision. As \(\lambda \to \infty\), QRE converges to NE, selecting the highest-utility action deterministically, while a lower \(\lambda\) introduces more randomness.

TQRE extends QRE by introducing two key refinements: heterogeneity in reasoning levels and contextual scaling of payoff sensitivity. TQRE uses the Cognitive Hierarchy framework to model heterogeneity in reasoning, where players differ in strategic depth. Reasoning levels \( k \) follow a truncated Poisson distribution:  
$f_k = \frac{\tau^k e^{-\tau}}{k!}$ where \( \tau \) is the mean reasoning level. A level-\( k \) player computes expected utility by aggregating strategies of lower-level players (\( h < k \)). For action \( a_{ij} \), their expected utility is:
\[
U_{ij}^{(k)} = \sum_{a_{-i} \in A_{-i}} \prod_{h=0}^{k-1} \sigma_{-i}^{(h)}(a_{-i}) u_i(a_{ij}, a_{-i}),
\]
where \(\sigma_{-i}^{(h)}\) represents the mixed strategies of level-\( h \) players.
TQRE also includes a contextual scaling parameter \(\gamma > 0\) to adjust for environmental influences on reasoning. By accounting for \(\gamma\), the model ensures that the reasoning level \(k\) isolates strategic depth from external factors. The rationality parameter at level \(k\) is: $\lambda_k = \gamma \cdot k$, linking reasoning depth to payoff sensitivity in a context-adjusted framework.

The probabilistic choice for a level-\(k\) player is given by:
\[
p_{ij}^{(k)} = \frac{\exp(\lambda_k U_{ij}^{(k)})}{\sum_{a \in A_i} \exp(\lambda_k U_{ia}^{(k)})}.
\]
To compute the overall probability of choosing action \(a_{ij}\) in the population, TQRE aggregates over all reasoning levels:
\[
p_{ij} = \sum_{k=0}^\infty f_k \cdot p_{ij}^{(k)},
\]
\(f_k\) is the probability of a player with reasoning level \(k\).

To summarize, TQRE framework introduces two key parameters: \(\tau\), the reasoning level of the sample, and \(\gamma\), the contextual scaling. These parameters jointly determine the rationality parameter \(\lambda\) for the reasoning levels across the sample,
balancing cognitive capability, represented by \(\tau\), and game complexity, represented by \(\gamma\). As \(\tau \to \infty\) and \(\gamma \to \infty\), the model converges to NE, where all players are perfectly rational and deterministic.
% \[
% p_{ij} = \delta_{ij, \arg \max_{a_i \in A_i} U_{ij}}.
% \]



\section{Framework and Design}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{framework_new.png}
    \vspace{-0.5em}
    \caption{Three main steps in the framework include (1) converting decision-making challenges to abstracted games, (2) prompting agents to make decisions, and (3) applying the TQRE model to estimate parameters of interest, assess demographic bias, and evaluate prompt effects.}
    \label{fig:frame}
    \vspace{-1em}
\end{figure*}

The framework for evaluating LLMs' strategic reasoning is outlined in Figure~\ref{fig:frame}. The framework accounts for the end-to-end steps from game abstraction and designing for behavioral evaluation of the LLMs. 

% The framework for evaluating LLMs' strategic reasoning, outlined in Figure \ref{fig:frame}, begins with a game design module. These games are meticulously developed and selected to capture a wide range of strategic interactions, enabling the evaluation of LLMs' abilities in multiple contexts.

% The games are then applied to the Responder Model, where LLMs select a row or column in a payoff matrix, simulating strategic decision-making. The resulting response data are analyzed using the TQRE Model to extract key reasoning parameters. These parameters, derived from original LLMs, CoT-augmented LLMs, and models embedded with demographic features, are further evaluated through regression and behavioral analyses. 

\subsection{Game Library Design}
The game library consists of diverse game types and payoff matrices.
% leveraging the payoff and probability lists for each selection made by LLMs to model reasoning levels and contextual factors. 
% Unlike previous works, the incorporation of context levels enables precise estimation of reasoning level by guiding the design of games. 
Each game incorporates different context levels by design to enable precise estimation of reasoning levels and dynamically assess the models' reasoning boundaries.
Drawing on established frameworks in behavioral game theory, we construct games under two primary categories: \textbf{complete information games}, where payoffs and strategies are common knowledge, and \textbf{incomplete information games}, where players form probabilistic beliefs about unknown elements of the game.

The complete information games include four distinct subtypes designed to capture a range of interactions:
\begin{enumerate}
    \item \textbf{Competitive (zero-sum) games}: Players' interests are strictly opposed, with one’s gain equaling the other’s loss, as the total payoff remains constant. These games require adversarial reasoning to determine optimal strategy.
    \item \textbf{Cooperation games} (e.g., Stag Hunt): These games emphasize the trade-off between mutual coordination and individual incentives. Players achieve the highest payoffs when they cooperate, but the risk of unilateral defection can lead to suboptimal outcomes. Success depends on trust and assurance in the other player’s actions.
    \item \textbf{Mixed-motive games} (e.g., Prisoner’s Dilemma): Characterized by a tension between one's incentives to defect and potential gains from mutual cooperation. These games are essential for studying trust, reciprocity, and strategic foresight.
    \item \textbf{Sequential games}: One player (leader) moves first, while the other (follower) observes and responds optimally. These games involve backward induction, where the follower’s optimal strategy influences the leader’s initial decision. 
\end{enumerate}

The incomplete information games simulate environments in which players must infer unknown elements, such as their opponent’s type or payoff structure. These games incorporate probabilistic reasoning, with opponent types \(t\) distributed according to \(\mathbf{P}(t)\). Two key games are:

\begin{enumerate}
    \item \textbf{Bayesian coordination games}: Players make decisions based on posterior beliefs about their opponent’s type, adjusting their strategies accordingly. The uncertainty about the opponent’s characteristics influences equilibrium selection and coordination success.
    \item \textbf{Signaling games}: One player (the sender) conveys a signal \(s\) to influence the other player’s (receiver’s) beliefs and subsequent strategy. 
\end{enumerate}
% This study develops a systematic game library comprising 13 games that cover the aforementioned types and subtypes. To enhance the versatility of these games, we introduce variations in payoff magnitude (e.g., high-stakes vs. low-stakes) and asymmetry (\(u_1 \neq u_2\))to enable a deeper exploration of risk-reward dynamics and distributional considerations. Additionally, we incorporate a game previously designed and tested with human subjects in behavioral game theory research \cite{stahl1994experimental, camerer2004behavioral} to facilitate a comparison of reasoning levels between humans and LLMs. For the data collection, we implement each game across 30 repeated rounds and record the choices made by the models. These repeated choices are then aggregated and converted into probabilities. This approach provides a significant methodological advantage: rather than relying on one-shot data collection or merely analyzing the frequency of reaching Nash equilibrium, it aggregates repeated responses to capture a broader and more robust representation of decision-making tendencies. By treating these repeated responses as a sample, the method allows for a more comprehensive understanding of the models' probabilistic strategies. 
In addition, we selected the SW10 game from previous research \citep{stahl1994experimental} for its ability to distinguish LLM reasoning levels when compared to human samples.
More specific details of the games and corresponding prompts are presented in the Appendix. 
% This study develops a systematic game library. Drawing on established frameworks in game theory and behavioral game theory, we construct games under two primary categories: \textbf{complete information games}, where payoffs and strategies are common knowledge, and \textbf{incomplete information games}, where players form probabilistic beliefs about unknown elements of the game. 

% The complete information games include four distinct subtypes designed to capture a range of interactions: 

% a. Competitive (zero-sum) games. The sum of payoffs is zero (\(\sum u_1 + u_2 = 0\)), exemplifying adversarial dynamics.

% b. Cooperation games (e.g., Stag Hunt). Highlight the interplay between mutual coordination and individual incentives, often resulting in Pareto-optimal outcomes. 

% c. Mixed-motive games (e.g., Prisoner’s Dilemma).  Characterized by a tension between individual incentives to defect and potential gains from mutual cooperation. 

% d. Sequential games. In this type of games, Player 1 (leader) moves first, and Player 2 (follower) observes and responds optimally. 

% To enhance the versatility of these games, we introduce variations in payoff magnitude (e.g., high-stake vs. low-stake) and asymmetry (\(u_1 \neq u_2\)), to enable a deeper exploration of risk-reward dynamics and distributional considerations. More specific details of the games are presented in the Appendix. 

% The incomplete information games simulate environments in which players must infer unknown elements such as their opponent’s type or payoff structure. These games incorporate probabilistic reasoning, with opponent types \(t \sim \mathbf{T}\) distributed according to \(\mathbf{P}(t)\). Three key models are included:

% a. Bayesian coordination games, where players make decisions based on posterior beliefs about their opponent’s type, influencing the resulting payoffs (\(\mathbf{P}_t\)).

% b.Signaling games. One player conveys a signal \(s\) to influence the other player’s strategy.

% c. Screening games. Players use strategy selection to reveal or exploit private information.




\subsection{Parameter Estimation}

Building on the probabilistic data collected, we estimate the key reasoning parameters using the TQRE model.

\textbf{Step 1: Setting up the Estimation Equation.} 

We begin by inputting the observed probabilities and utilities derived from the payoff matrices. For example, in a two-player game where each player \(i \in \{1, 2\}\) has an action set \(A_i = \{a_{i1}, a_{i2}\}\), let the utilities for player \(1\) be defined as \(u_1(a_{11}, a_{21}) = U_{11}, u_1(a_{11}, a_{22}) = U_{12}, \dots\).

The expected utility for player \(1\) at reasoning level \(k\) when choosing action \(a_{11}\), is :
{
\begin{align*}
U_{11}^{(k)} &= \sum_{a_{-1} \in A_{-1}} \prod_{h=0}^{k-1} \sigma_{-1}^{(h)}(a_{-1}) \cdot u_1(a_{11}, a_{-1}),
\end{align*}}
where:
{
\begin{align*}
\sigma_{-1}^{(h)}(a_{-1}) &= 
\frac{\exp\left(\lambda_h \cdot U_{-1}^{(h)}(a_{-1})\right)}{\sum_{a \in A_{-1}} \exp\left(\lambda_h \cdot U_{-1}^{(h)}(a)\right)}.
\end{align*}}

% The probability of a level-\(k\) player choosing \(a_{11}\) is:
% \begin{align*}
% p_{11}^{(k)} &= 
% \frac{\exp\left(\lambda_k \cdot U_{11}^{(k)}\right)}{\sum_{a \in A_1} \exp\left(\lambda_k \cdot U_{1a}^{(k)}\right)}.
% \end{align*}

% Substituting \(\lambda_k = \gamma \cdot k\) and expanding \(U_{11}^{(k)}\), we have:
then, 
{
\begin{align*}
p_{11}^{(k)} &= 
\frac{\exp\left(\gamma \cdot k \cdot \sum_{a_{-1} \in A_{-1}} \prod_{h=0}^{k-1} \sigma_{-1}^{(h)}(a_{-1}) \cdot u_1(a_{11}, a_{-1})\right)}{\sum_{a_{1j} \in A_1} \exp\left(\gamma \cdot k \cdot \sum_{a_{-1} \in A_{-1}} \prod_{h=0}^{k-1} \sigma_{-1}^{(h)}(a_{-1}) \cdot u_1(a_{1j}, a_{-1})\right)}.
\end{align*}}

\textbf{Step 2: Aggregating Probabilities Across All Levels.}

The overall probability of selecting \(a_{11}\) in the population is computed as:
\[
p_{11} = \sum_{k=0}^{K_{\text{max}}} f_k \cdot p_{11}^{(k)}, \text{where } f_k = \frac{\tau^k e^{-\tau}}{k!}\
\]

% {\tiny

% \[\implies p_{11} = \sum_{k=0}^{K_{\text{max}}} \frac{\tau^k e^{-\tau}}{k!} \cdot 
% \frac{\exp\left(\gamma \cdot k \cdot \sum_{a_{-1} \in A_{-1}} \prod_{h=0}^{k-1} \sigma_{-1}^{(h)}(a_{-1}) \cdot u_1(a_{11}, a_{-1})\right)}
% {\sum_{a_{1j} \in A_1} \exp\left(\gamma \cdot k \cdot \sum_{a_{-1} \in A_{-1}} \prod_{h=0}^{k-1} \sigma_{-1}^{(h)}(a_{-1}) \cdot u_1(a_{1j}, a_{-1})\right)}.
% \]}

\textbf{Step 3: Maximum Likelihood Estimation (MLE).} 

Building on the aggregated probabilities derived in \textbf{Step 2}, we now estimate the parameters \(\tau\) (mean reasoning level) and \(\gamma\) (contextual scaling) using MLE. For each game \(g\), the likelihood function is constructed based on the observed strategy frequencies and the predicted probabilities derived from the TQRE model:
\[
\ln L_g(\tau, \gamma) = \sum_{i \in N_g} \sum_{j=1}^{J_g} c_{ijg} \ln(p_{ijg}(\tau, \gamma)),
\]
where \(c_{ijg}\) is the observed frequency of player \(i\) choosing strategy \(j\), and \(p_{ijg}(\tau, \gamma)\) is the predicted probability from a logit response function based on expected utilities (\textbf{Step 1}). Parameters are estimated separately for each game to capture context-specific reasoning.



\subsection{Behavior Evaluation}
As the final step of this framework, we draw a comparison between the vanilla model and models enhanced with external mechanisms, such as incorporating demographic features as system context or using CoT prompting to boost reasoning levels.
When introducing a boosting mechanism, we aim to evaluate how it influences the models' ability to make contextually appropriate decisions in strategic scenarios, particularly in games with practical implications.
% , as opposed to purely abstract or logic-driven tasks.

With the inclusion of demographic features, we apply regression analysis to uncover the relationships between reasoning levels and individual features. These relationships provide insights into whether the model exhibits biases stemming from the training data. Such analyses are crucial for understanding the broader implications of model design and its alignment with fairness and representational objectives.

\section{Evaluation and Results}
Using the proposed framework, we evaluated 22 SOTA LLMs under three settings: baseline, CoT-embedded, and demographic-feature-embedded, utilizing the game library. The results reveal that while some advanced models, such as DeepSeek-R1, excel in certain tasks, they do not consistently outperform all games with respect to reasoning capability. In some cases, models with modest size and lower capabilities demonstrate comparable or even reasoning levels. Furthermore, our analysis uncovered significant biases in minority demographic features, including religion, marital status, and sexual orientation. The complete results are provided in the Appendix. Our findings also show that the knowledge distillation could lead to further bias as shown in DeepSeek-R1 distilled models.   
% These findings highlight the need for careful consideration of fairness and representational equity when designing and deploying LLMs in diverse applications.

\subsection{Experimental Design}
\textbf{Models:} We evaluate reasoning capabilities of general-purpose language models across different scales. The objective of our study is to examine whether larger models or those excelling in individual benchmarks demonstrate superior strategic reasoning.
The selected models include GPT-4o-2024-08-06 \cite{hurst2024gpt}, GPT-o1-preview \cite{jaech2024openai}, GPT-o3-mini \cite{o3-mini-system-card-feb10}, DeepSeek-R1 \cite{guo2025deepseek}, DeepSeek-V2.5 \cite{deepseekv2}, DeepSeek-V3 \cite{deepseekai2025deepseekv3technicalreport}, gemma-2-27b-it \cite{gemma_2024}, Gemini-2.0-Flash-Thinking \cite{geminiteam2024geminifamilyhighlycapable}, Granite-3.1-8B-Dense \cite{mishra2024granitecodemodelsfamily}, Granite-3.1-3B-MoE, Claude-3-Opus \cite{TheC3}, internlm2\_5-20b-chat \cite{cai2024internlm2}, Meta-LLaMA-3.1-405B-Instruct \cite{grattafiori2024llama3herdmodels}, Meta-LLaMA-3.1-8B-Instruct, QwQ-32B-Preview \cite{qwq-32b-preview,qwen2}, and glm-4-9b-chat \cite{glm2024chatglm}. In addition, we selected the DeepSeek-R1 Distilled model on LLaMA-70B, LLaMA-8B, Qwen-32B, Qwen-14B, Qwen-7B, and Qwen-1.5B. These models are selected to examine whether distillation introduces additional biases in the inherited model.  
Selected models specialize in tasks involving reasoning, mathematics, and logic understanding. All models are set with a temperature of 1, top-p of 1, and no repetition penalty, enabling unrestricted generation while ensuring result consistency.

\textbf{Comparisons:} 
The baseline for all models is performing the games in their vanilla form. Subsequently, all models without internal CoT are augmented with a zero-shot CoT in the prompt. Additionally, demographic features are introduced through prompts as system knowledge across all models, and the same games are performed under these conditions. 
Finally, we incorporate CoT prompting with embedded demographic features as system knowledge to evaluate their combined effect.
Each individual game was tested 30 times to ensure meaningful and non-random results.  

\textbf{Embedded Demographic Features:}  
The demographic features include 10 distinct socio-demographic groups as shown in Table \ref{tab:demo_feature}. These personas represent a diverse demographic landscape. We generate the distribution by randomly assigning foundational demographic features to LLMs and recording their responses.

\begin{table}[h!]\centering 
% \resizebox{0.8\textwidth}{!} 
{%
% \renewcommand{\arraystretch}{1.15}
\caption{The Demographic Features across 10 socio-demographic groups. }
\vspace{5pt}
\label{tab:demo_feature}
\begin{tabular}{ll}
\hline
\multicolumn{1}{c}{\textbf{Group}} & \multicolumn{1}{c}{\textbf{Demographic Features}}\\ \hline\hline
Sex & male, female \\ \hline
Education Level & below secondary, graduate or above, and in between \\ \hline
Marital Status & never married, married, widowed, divorced \\ \hline
Living Area & rural, urban \\ \hline
Age & \textless 25, 25-54, \textgreater 55 \\ \hline
Sex Orientation & heterosexual, homosexual, bisexual, asexual \\ \hline
Disability & physically-disabled, able-bodied \\ \hline
Race & African, Hispanic, Asian, Caucasian \\ \hline
Religion & Jewish, Christian, Atheist, other religious \\ \hline
Political Affiliation & \begin{tabular}[c]{@{}l@{}}lifelong Democrat, lifelong Republican, Barack Obama supporter, \\ Donald Trump supporter\end{tabular} \\ \hline \hline
\end{tabular}}
\setlength{\abovecaptionskip}{5pt plus 2pt minus 1pt}
\setlength{\belowcaptionskip}{2pt plus 2pt minus 1pt}
\vspace{-0.5em}
\end{table}



\subsection{Baseline Evaluation}



\begin{table*}[h!]
\caption{Strategic Reasoning Performance ($\tau$) Across Different Game Settings.}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccclccclccclcccc}
\hline
\multirow{3}{*}{} & \multicolumn{11}{c}{\textbf{Complete Information Game}} &  & \multicolumn{4}{c}{\textbf{Incomplete Information Game}} \\ \cline{2-12} \cline{14-17} 
 & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Competitive \\ Games\end{tabular}} &  & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Cooperation \\ Games\end{tabular}} &  & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Mixed-motive\\ Games\end{tabular}} &  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Seq.\\ Games\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Bayesian\\ Games\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Signaling\\ Games\end{tabular}} \\ \cline{2-4} \cline{6-8} \cline{10-12} \cline{15-16}
 & BL & HS & LS &  & BL & HP & AP &  & BL & H-Pun & L-Pun &  &  & p=0.5 & p=0.9 &  \\ \hline
GPT-4o & 1.543 & 1.936 & 0.729 &  & 0.602 & 0.143 & 1.505 &  & 1.665 & 0.537 & 1.499 &  & 0.222 & 1.973 & 1.953 & 3.590 \\
GPT-o1 & \sig{4.741} & \sig{4.311} & 3.126 &  & 2.800 & 0.628 & 1.322 &  & 0.143 & 0.069 & 1.481 &  & \sig{2.846} & 4.225 & \sig{4.225} & 3.980 \\
GPT-o3-mini & 1.309 & 2.087 & 2.430 &  & \sig{3.550} & \sig{4.226} & \sig{3.944} &  & 3.352 & \sig{3.944} & \sig{2.931} &  & 0.635 & \sig{4.225} & \sig{4.225} & 3.079 \\
Gemma-V2 & 0.320 & 1.485 & 1.593 &  & 0.183 & 0.003 & 1.118 &  & 0.981 & 0.290 & 0.851 &  & 0.735 & 1.831 & 0.187 & 3.069 \\
Gemini-2.0 & 1.202 & 1.202 & 1.958 &  & 2.487 & 1.609 & 0.070 &  & 2.461 & 0.223 & 1.230 &  & 1.683 & \sig{4.225} & \sig{4.225} & 3.920 \\
Claude-3-Opus & 1.131 & 0.982 & 0.036 &  & 3.390 & 1.322 & 1.505 &  & 1.333 & 1.000 & 1.220 &  & 0.717 & 2.014 & \sig{4.225} & 3.623 \\
Granite-3.1-8B & 1.428 & 0.003 & 3.429 &  & 1.127 & 0.128 & 1.476 &  & 1.360 & 0.198 & 1.376 &  & 1.480 & 1.191 & 1.261 & 1.527 \\
Granite-3.1-MoE & 1.098 & 1.257 & 1.336 &  & 3.396 & 2.405 & 2.515 &  & \sig{3.895} & 3.524 & 2.508 &  & 1.560 & 2.119 & 2.407 & 2.832 \\
LLaMA-3.1-405B & 0.988 & 0.910 & 0.177 &  & 0.379 & 0.001 & 1.118 &  & 1.368 & 0.290 & 1.180 &  & 0.688 & 1.658 & 0.226 & 1.543 \\
LLaMA-3.1-8B & 1.298 & 0.964 & 1.548 &  & 1.066 & 0.717 & 0.000 &  & 1.431 & 0.173 & 1.475 &  & 0.357 & 0.340 & 0.069 & 2.336 \\
InternLM-V2 & 0.124 & 0.882 & 1.511 &  & 0.869 & 0.143 & 1.311 &  & 1.449 & 0.537 & 1.396 &  & 0.579 & 0.943 & 0.384 & 0.339 \\
QwQ-32B & 3.599 & 1.435 & \sig{3.892} &  & 2.398 & 3.434 & 0.916 &  & 2.564 & 0.406 & 1.427 &  & 0.144 & 0.405 & 1.197 & 2.904 \\
GLM-4 & 1.177 & 0.962 & 1.488 &  & 1.250 & 0.436 & 1.247 &  & 1.392 & 0.173 & 1.481 &  & 0.748 & 0.810 & 0.639 & 1.252 \\
DeepSeek-V2.5 & 1.108 & 0.913 & 1.413 &  & 1.215 & 0.480 & 1.247 &  & 1.269 & 0.290 & 1.190 &  & 0.174 & 0.736 & 1.120 & 1.455 \\
DeepSeek-V3 & 0.142 & 0.390 & 0.042 &  & 0.183 & 0.003 & 1.118 &  & 1.399 & 0.173 & 1.346 &  & 0.883 & 1.295 & 1.347 & 3.410 \\
DeepSeek-R1 & 0.075 & 1.198 & 0.233 &  & \sig{3.550} & \sig{4.226} & \sig{3.944} & \textit{} & 2.718 & 1.322 & 2.929 &  & 0.516 & \sig{4.225} & \sig{4.225} & 3.079 \\ \hline
\multicolumn{4}{l}{\textbf{DeepSeek-R1-Distill Models:}} &  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} &  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} &  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
LLaMA-70B & 0.488 & 0.028 & 0.072 &  & 0.730 & 0.619 & 1.512 &  & 2.376 & 0.745 & 1.228 &  & 0.884 & \sig{4.225} & 3.066 & 3.460 \\
LLaMA-8B & 0.341 & 0.261 & 0.150 &  & 0.628 & - & 0.000 &  & 1.394 & 0.633 & 1.266 &  & 0.068 & 1.796 & 1.966 & 3.245 \\
Qwen-1.5B & 0.421 & 0.434 & 0.013 &  & 2.180 & 0.990 & 0.916 &  & 1.420 & 0.397 & 1.224 &  & 0.119 & 0.883 & 0.884 & 1.430 \\
Qwen-14B & 0.518 & 0.633 & 0.047 &  & 2.487 & 0.606 & 1.945 &  & 1.445 & 1.179 & 2.033 &  & 0.097 & 1.980 & 1.407 & \sig{3.791} \\
Qwen-32B & 0.790 & 1.045 & 0.335 &  & 1.093 & 0.916 & 0.406 &  & 2.472 & 0.773 & 1.399 &  & 0.075 & \sig{4.225} & \sig{4.225} & \sig{3.731} \\
Qwen-7B & 0.120 & 1.045 & 0.084 &  & 0.907 & 0.163 & 1.470 &  & 1.457 & 0.677 & 1.147 &  & 1.528 & 0.645 & 0.994 & 3.112 \\ \hline
\end{tabular}
}
\label{tab: full_result_tau}
\begin{flushleft}
\footnotesize \textit{Note:} \textbf{\sig{Bold values}} indicate the highest strategic reasoning level ($\tau$) observed for that game. A dash (-) indicates cases where the model struggles to establish a stable level of reasoning due to potential convergence challenges. Abbreviations: **HS** = High Stake, **LS** = Low Stake, **BL** = Baseline, **HP** = High Payoff, **AP** = Asymmetric Payoff, **H-Pun** = High Punishment, **L-Pun** = Low Punishment.
\end{flushleft}
\end{table*}


\begin{figure}[ht]
    \centering
\includegraphics[width=0.85\linewidth]{tau_by_model.png}
\vspace{5pt}\caption{Performance of all models across game types}
    \label{fig:tau_by_model}
    
\end{figure}

\begin{table*}[ht]
\caption{Strategic Reasoning Performance ($\tau$) Across Different Game Settings in CoT.}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccccccc}
\hline
\multirow{3}{*}{} & \multicolumn{11}{c}{\textbf{Complete Information Game}} &  & \multicolumn{4}{c}{\textbf{Incomplete Information Game}} \\ \cline{2-12} \cline{14-17} 
 & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Competitive \\ Games\end{tabular}} &  & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Cooperation \\ Games\end{tabular}} &  & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Mixed-motive\\ Games\end{tabular}} &  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Seq.\\ Games\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Bayesian\\ Games\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Signaling\\ Games\end{tabular}} \\ \cline{2-4} \cline{6-8} \cline{10-12} \cline{15-16}
 & BL & HS & LS &  & BL & HP & AP &  & BL & H-Pun & L-Pun &  &  & p=0.5 & p=0.9 &  \\ \hline
Claude-3-Opus-CoT & \sig{4.264} & 3.327 & - &  & 0.270 & 1.053 & 1.311 &  & 1.181 & 0.127 & 0.140 &  & 1.692 & 0.502 & 0.564 & 1.489 \\
DeepSeek-V2.5-CoT & 1.427 & 1.867 & 1.103 &  & 3.046 & 2.510 & 1.376 &  & 2.608 & 1.322 & 3.319 &  & 1.646 & 1.058 & 0.854 & 0.405 \\
DeepSeek-V3-CoT & 1.179 & 0.480 & - &  & 3.269 & 1.322 & 0.628 &  & 2.815 & 1.253 & 2.831 &  & 0.369 & 0.913 & 1.039 & 0.079 \\
Gemma-V2-CoT & 0.566 & 0.848 & 1.469 &  & 0.629 & 0.986 & 0.070 &  & 0.511 & 0.069 & 0.916 &  & 0.717 & 1.202 & 0.782 & 1.419 \\
Gemini-2.0-CoT & 1.005 & 1.410 & - &  & 1.080 & 0.777 & 1.515 &  & 1.350 & 0.896 & 1.314 &  & 0.209 & 0.879 & 1.057 & 1.529 \\
GPT-4o-CoT & 0.511 & 0.288 & 1.063 &  & 0.871 & 0.762 & 1.515 &  & 1.411 & 0.629 & 1.311 &  & 1.596 & \sig{4.225} & \sig{3.066} & 3.976 \\
Granite-3.1-8B-CoT & 3.646 & 1.048 & \sig{4.247} &  & 1.990 & 1.022 & 0.333 &  & 1.426 & 0.020 & 1.480 &  & 1.440 & 2.141 & 1.138 & 0.908 \\
Granite-3.1-3B MoE-CoT & 3.193 & 3.301 & 0.159 &  & 0.400 & 0.177 & \sig{2.256} &  & 1.518 & 1.160 & 1.139 &  & \sig{2.843} & 1.956 & 0.223 & 2.009 \\
LLaMA-3.1-405B-CoT & 0.314 & 1.416 & 0.049 &  & 0.357 & 0.835 & 0.560 &  & 0.154 & 0.223 & 0.693 &  & 0.182 & 0.738 & 0.857 & 1.521 \\
LLaMA-3.1-8B-CoT & 0.942 & 1.007 & 0.263 &  & 0.118 & 0.829 & 0.342 &  & 1.383 & 0.960 & 0.158 &  & 0.152 & 0.736 & 0.981 & 1.511 \\
InternLM-V2-CoT & 0.512 & 1.039 & 1.552 &  & 1.067 & 1.021 & 1.247 &  & 1.371 & 0.927 & 1.364 &  & 0.077 & 1.068 & 1.220 & 2.338 \\
QwQ-32B-CoT & 3.207 & 1.491 & \sig{3.956} &  & \sig{3.550} & \sig{4.226} & 0.628 &  & \sig{3.352} & \sig{3.944} & 3.080 &  & 0.068 & 0.438 & 1.024 & \sig{4.226} \\
GLM-4-CoT & 0.442 & 0.645 & 0.221 &  & 0.069 & 1.119 & 0.406 &  & 0.069 & 0.795 & 0.223 &  & 0.360 & 1.058 & 0.871 & 1.486 \\ \hline
\end{tabular}%
}
\label{tab: full_result_tau_CoT}
\begin{flushleft}
\footnotesize \textit{Note:} \textbf{\sig{}Bold values} indicate the highest strategic reasoning level ($\tau$) observed for that game. A dash (-) indicates cases where the model struggles to establish a stable level of reasoning due to potential convergence challenges. Abbreviations: **HS** = High Stake, **LS** = Low Stake, **BL** = Baseline, **HP** = High Payoff, **AP** = Asymmetric Payoff, **H-Pun** = High Punishment, **L-Pun** = Low Punishment.
\end{flushleft}
\end{table*}


% In the baseline evaluation, we analyze the strategic reasoning capabilities of LLMs without incorporating additional features. 
The baseline setup provides a foundational understanding of how LLMs perform in strategic games when relying solely on their inherent reasoning mechanisms. The results are presented in Table \ref{tab: full_result_tau}. The distribution of $\tau$ is shown in Figure \ref{fig:tau_by_model} for all models.

Among all evaluated models, \textbf{GPT-o1}, \textbf{GPT-o3-mini}, and \textbf{DeepSeek-R1} outperform others, with GPT-o1 ranking first in 4 games, GPT-o3-mini leading in 7 games, and DeepSeek-R1 securing the top position in 5 games. Some games have co-leaders, where multiple models achieve the highest strategic reasoning level. In these cases, certain LLMs consistently reach the optimal strategy—often aligning with Nash Equilibrium (NE)—demonstrating their ability to fully reason through the game structure. However, not all models achieve this level of performance, highlighting variations in reasoning depth. This underscores both the strengths of high-performing models in structured settings and the effectiveness of our framework in distinguishing optimal decision-making from suboptimal or inconsistent reasoning. Additionally, our findings reveal significant variations in reasoning capabilities across different game types.
% LLMs generally elicit higher reasoning in competitive games, reflecting models' ability to adapt to structured environments with clear incentives. 
GPT-o1 consistently ranks among the top models in competitive and incomplete-information games, suggesting an optimization for rational, goal-oriented decision-making and adversarial reasoning. DeepSeek-R1 excels in cooperative and mixed-motive games, likely due to its enhanced ability to model social interactions and optimize mutual benefits under reinforcement learning rewards. GPT-o3-mini, meanwhile, demonstrates strong performance across a broader range of games, particularly excelling in cooperative, mixed-motive, and incomplete-information games. Its success in both social and strategic reasoning tasks suggests a balanced capability in handling trade-offs, adapting to uncertainty, and leveraging probabilistic inference in decision-making.
% This suggests that GPT-o1 excels in direct strategic computation, while DeepSeek-R1 adapts better to uncertain interactions.


The performance gap between stronger models, such as \textbf{DeepSeek-R1}, \textbf{GPT-o1} and \textbf{GPT-o3-mini}, and weaker ones, like \textbf{DeepSeek V2.5}, \textbf{DeepSeek V3} and \textbf{GPT-4o}, is notable across most games. Nevertheless, advanced models do not always dominate all game types. Smaller models like Gemma-2-27B and LLaMA-3.1-8B sometimes match or outperform larger counterparts, and DeepSeek-R1 is a top model despite not being the largest. Additionally, R1-distilled models can outperform significantly larger models, despite being derived from a much smaller base model. This suggests that model size is not the sole factor in reasoning quality. Instead, context, model structure, and training data likely play a crucial role, highlighting the strong influence of contextual adaptation on LLMs' reasoning abilities.



% These findings reveal that LLMs’ reasoning capabilities are heavily influenced by context. This variability highlights the importance of adopting evaluation frameworks like ours, which move beyond deterministic assumptions to capture the interplay of reasoning capabilities and contextual factors, addressing the limitations of traditional circular approaches that rely solely on predefined benchmarks like NE.

\textbf{Comparing with vs. without Chain-of-Thought}

The performances of the strategic reasoning level after implementing the CoT prompt are presented in Table \ref{tab: full_result_tau_CoT}. Intuitively, many believe CoT can likely enhance a model's reasoning ability. However, our results show that CoT's impact on strategic reasoning is nuanced, with no consistent improvement across all models and game types.
 
\begin{figure}[ht]
    \centering
    \includegraphics[width= 0.8\linewidth]{base_vs_cot.png}

    \caption{Comparison of Tau between baseline and CoT in the Competitive-Base game}

    \label{fig:base_vs_cot}
\end{figure}

Figure~\ref{fig:base_vs_cot} shows an example comparing $\tau$ of each model in the Competitive-Base game with and without CoT prompting. 
While some models improve, others remain unchanged or even decline. Notably, a model that gains from CoT in one game may not in another. This variation may reflect how each model processes and benefits from step-by-step explicit reasoning—some are better at translating CoT text into genuinely more iterative strategies, whereas others might get sidetracked or produce inconsistent lines of thought. 
This is consistent with prior findings suggesting that the "CoT does not always help" \citep{duan2024gtbench, carlander2024controlled}, particularly in strategic settings. 
Nonetheless, a close examination of Figure~\ref{fig:dist_tau_base} reveals that the bars are aggregated around $\tau < 1.8$, meaning more models have improved $\tau$ when original reasoning capability is low, indicating the potential of the CoT prompting technique to enhance reasoning levels. That is, when a model's reasoning capability is very low, CoT can provide a slight improvement, elevating it to a moderate level. 






\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{dist_tau_base.png}
        \vspace{-1.5em}
    \caption{Comparison of the distribution of Tau between baseline and CoT prompting for all models across all game types}
    \label{fig:dist_tau_base}
       
\end{figure}
% A particularly interesting pattern roughly emerges among weaker models (i.e., those with comparatively low $\tau$ without CoT). 
For those low-reasoning capability systems, CoT is more likely to provide enough structured guidance to improve their reasoning process, leading to modest but meaningful increases in $\tau$. This suggests that CoT can partially compensate for weaker internal representations of strategic play, giving the model a chance to articulate an “opponent’s perspective” or weigh payoffs more carefully. In contrast, as shown in Figure \ref{fig:base_vs_cot}, stronger models, such as GPT-4o, which already demonstrate moderate $\tau$ in baseline evaluation, do not always see significant additional gains from CoT. One possibility is that these models internally perform iterative reasoning even without explicit prompts, so the CoT text adds less incremental benefit to their final outputs.

\textbf{Comparison with Human}

We compare all model settings, incorporating both internal and external CoT where applicable, against human subjects in the same game setting \citep{stahl1994experimental, rogers2009heterogeneous}. Several key observations emerge, as shown in Figure~\ref{fig:human_comp}.
A total of 17 models outperform humans in this benchmark, underscoring how certain LLMs can excel beyond typical human reasoning levels in specific strategic scenarios. Models such as GPT-4 CoT, DSk-R1\_Qwen-14B, and other large-parameter systems often occupy top ranks, suggesting that a combination of scale, refined architectures, and careful prompting (e.g., CoT) can yield substantial gains in strategic reasoning. 

Despite being outperformed by the strongest models, humans rank in the middle tier, surpassing multiple LLMs. This suggests that while SOTA models exhibit advanced strategic reasoning, human decision-making remains robust and sophistication. It is important to note that this evaluation represents just one specific game setting, providing a snapshot rather than a definitive ranking across all strategic tasks. As performance varies across different types of games, a broader analysis is necessary to fully understand the comparative strengths and weaknesses of each model in diverse strategic environments.




\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{human_comp.png}
    \vspace{-1em}
    \caption{Comparison among human participants and LLMs on the SW10 game \citep{stahl1994experimental}}
    \vspace{-0.5em}
    \label{fig:human_comp}
\end{figure}



While the reasoning levels demonstrated by the models provide valuable insights, it is important to question whether "higher is always better." In fact, alignment with human expectations and contextual requirements is paramount. Some models outperform humans in structured scenarios, but such dominance does not necessarily translate into broader applicability or ethical alignment with human decision-making norms.  For example, in scenarios requiring fairness, trust, or cooperation—such as resource allocation or collaborative decision-making—models that align closely with human reasoning may be preferable. Conversely, outperforming human reasoning may be beneficial in structured, high-stakes tasks requiring precise optimization. Ultimately, reasoning capabilities must be calibrated to respect the objectives and constraints of each scenario. While surpassing humans demonstrates technical sophistication, achieving alignment with human values and societal goals remains the more meaningful and complex challenge.

\subsection{With Encoded Demographic Feature Evaluation}


 After establishing each model’s strategic reasoning level, we explore and evaluate the correlation between demographic variables and the model's reasoning capability through Ordinary Least Square (OLS) regression. Specifically, we estimate the following regression model:

\begin{equation*}
\tau_m = \alpha + \beta D + \epsilon
\end{equation*}

where $\tau_m$ represents the estimated reasoning level of model $m$, $D$ is a vector of demographic variables (e.g., gender, age, education, and political affiliation) and $\epsilon$ is the error term. The coefficient $\beta$ and its associated significant level captures the effect of demographic attributes on the model's reasoning performance.



Several models exhibited distinct levels of “sensitivity” to these variables, as shown in Appendix Table \ref{tab:demo_base}. 




\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{gen_gpt.png}
        \vspace{-1.5em}
        \caption{}
        \label{fig:gen_gpt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{sex_gem.png}
        \vspace{-1.5em}
        \caption{}
        \label{fig:sex_gem}
    \end{subfigure}
        \vspace{-1em}
    \caption{Selected average parameter values where regression coefficients (indicators of the relationship between variables in a statistical model) are significant in this category with non-external-CoT models.}
    \vspace{-0.5em}
    \label{fig:demo_plot_1}
\end{figure}

GPT-4o and Gemma-2 illustrate these effects as notable cases. For instance, in Figure \ref{fig:gen_gpt}, GPT-4o's reasoning levels varied by gender, with female features increasing and male features decreasing reasoning levels. Similarly, Figure \ref{fig:sex_gem} illustrates how Gemma-2 exhibited pronounced differences based on sexual orientation, with reasoning levels spiking for heterosexuals compared to other minority categories. These findings suggest that the model's internal processing may partially rely on linguistic cues tied to users' personal backgrounds.


By contrast, LLaMA-3.1-8B and QwQ-32B showed fewer demographic correlations, indicating less sensitivity to user attributes in strategic depth allocation. Some models, such as LLaMA-3.1-405B, fell into an intermediate range, with only a handful of demographic variables—and often ones related to sexual orientation or political orientation—reaching standard or near-standard significance. These findings may indicate which types of demographic features are likely to trigger bias within LLMs. Although these results do not necessarily point to large-scale biases, they do indicate that certain demographic signals can shape how these models reason about or respond to strategic prompts, potentially raising fairness concerns.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{reli_gem.png}
        \vspace{-1.5em}
        \caption{}
        \label{fig:reli_gem}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{reli_intern.png}
        \vspace{-1.5em}
        \caption{}
        \label{fig:reli_gem_cot}
    \end{subfigure}
    \vspace{-1em}
    \caption{Impact of CoT on the same models with the same demographic features}

    \label{fig:demo_plot_2}
\end{figure}


We then introduced CoT prompting to see whether explicit step-by-step reasoning heightened or diminished the observed correlations and presented the results in Appendix Table \ref{tab:demo_cot}. Overall, more demographic variables became significant under CoT—even for models that were previously neutral—reflecting a tendency for CoT to alter how the model weighs user cues. For example, Figure \ref{fig:demo_plot_2} shows that Gemma-2 displayed marked increases for "Jewish" features, while InternLM, initially neutral, developed religious disparities. These findings highlight fairness concerns, as CoT can both expose latent biases and introduce new correlations, making fairness dynamic across prompts. CoT’s ability to introduce new correlations underscores that fairness is not static across prompt variations. If a model prioritizes certain demographic signals, CoT may amplify rather than mitigate bias. This underscores the need for ongoing, prompt-specific bias audits as its use expands.

Biases in distilled models present a notable challenge. While distillation improves the reasoning ability of smaller models, this enhancement is most evident in cooperation and mixed-motive game settings. However, it also introduces additional biases, which must be carefully considered.
In the vanilla form of LLaMA, even after embedding CoT, bias remains limited, with only one or two features affecting reasoning levels. However, knowledge distillation introduces additional biases in both LLaMA-8B and LLaMA-70B. Compared to CoT alone, distillation amplifies sensitivity to demographic factors, as DSk-R1\_LLaMA70B exhibits a significant bias toward age and race. These findings highlight the importance of carefully monitoring knowledge distillation. If malicious data are used to train the teacher model, the resulting biases may propagate to the student model, potentially leading to harmful behaviors. More critically, even with benign training data, hidden biases can transfer during distillation, underscoring the need for caution when applying this technique.



% \begin{table*}[]
% \centering
% \caption{LLMs Sensitivity to Demographic Features with Chain-of-Thought through Regression Analysis}
% \hspace*{-0.05\textwidth} % Shift table slightly left
% \resizebox{1.1\textwidth}{!}{%
% \begin{tabular}{lccccccccccccc}
% \hline
% \multicolumn{1}{c}{\textbf{Feature}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V3\end{tabular}} & \textbf{QwQ} & \textbf{\begin{tabular}[c]{@{}c@{}}GLM\\ 4\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Claude\\ 3\\ Opus\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V2.5\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemini\\ 2.0\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemma\\ V2\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPT\\ 4o\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}InternLM\\ V2\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 405B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ MoE\end{tabular}} \\ \hline
% \multirow{2}{*}{\textbf{\textless{}25 years old}} & 0.046 & -0.002 & -0.114 & 0.022 & 0.108 & -0.007 & -0.010 & -0.064 & \textbf{0.388} & -0.022 & -0.086 & 0.525 & -0.085 \\
%  & (0.051) & (0.106) & (0.158) & (0.097) & (0.451) & (0.100) & (0.136) & (0.180) & \textbf{(0.225)} & (0.128) & (0.241) & (0.407) & (0.323) \\
% \multirow{2}{*}{\textbf{\textgreater{}55 years old}} & \textbf{0.065} & 0.102 & -0.035 & -0.056 & 0.079 & 0.057 & 0.010 & 0.162 & 0.193 & 0.078 & -0.040 & 0.087 & \textbf{0.640**} \\
%  & \textbf{(0.039)} & (0.080) & (0.119) & (0.073) & (0.340) & (0.075) & (0.103) & (0.136) & (0.170) & (0.096) & (0.182) & (0.307) & \textbf{(0.244)} \\
% \multirow{2}{*}{\textbf{Female}} & 0.027 & \textbf{0.217**} & 0.173 & -0.056 & \textbf{-0.700*} & 0.067 & -0.122 & -0.090 & 0.056 & 0.005 & -0.191 & 0.013 & 0.279 \\
%  & (0.036) & \textbf{(0.074)} & (0.111) & (0.068) & \textbf{(0.316)} & (0.070) & (0.095) & (0.126) & (0.158) & (0.089) & (0.169) & (0.285) & (0.227) \\
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Graduate\\ Level\end{tabular}}} & \textbf{-0.084} & -0.066 & -0.047 & -0.032 & 0.067 & 0.086 & -0.108 & -0.051 & 0.093 & 0.074 & -0.225 & 0.332 & 0.328 \\
%  & \textbf{(0.048)} & (0.100) & (0.148) & (0.091) & (0.423) & (0.094) & (0.128) & (0.169) & (0.211) & (0.120) & (0.227) & (0.382) & (0.304) \\
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Below\\ Secondary\end{tabular}}} & -0.064 & -0.085 & 0.066 & 0.025 & -0.106 & 0.015 & -0.040 & 0.141 & 0.153 & -0.085 & 0.070 & 0.156 & -0.106 \\
%  & (0.039) & (0.080) & (0.119) & (0.073) & (0.339) & (0.075) & (0.102) & (0.135) & (0.169) & (0.096) & (0.182) & (0.306) & (0.243) \\
% \multirow{2}{*}{\textbf{Divorced}} & -0.011 & 0.018 & 0.095 & -0.053 & 0.621 & 0.068 & \textbf{-0.219} & -0.113 & 0.161 & 0.129 & -0.056 & \textbf{0.657} & 0.208 \\
%  & (0.047) & (0.096) & (0.143) & (0.088) & (0.410) & (0.091) & \textbf{(0.124)} & (0.163) & (0.204) & (0.116) & (0.219) & \textbf{(0.369)} & (0.294) \\
% \multirow{2}{*}{\textbf{Married}} & 0.001 & -0.032 & 0.092 & \textbf{0.179*} & 0.225 & \textbf{-0.186*} & -0.036 & -0.074 & 0.071 & 0.100 & 0.024 & -0.077 & -0.168 \\
%  & (0.047) & (0.097) & (0.145) & \textbf{(0.089)} & (0.414) & \textbf{(0.092)} & (0.125) & (0.165) & (0.206) & (0.117) & (0.221) & (0.373) & (0.297) \\
% \multirow{2}{*}{\textbf{Widowed}} & -0.051 & -0.066 & 0.106 & 0.030 & 0.407 & -0.060 & \textbf{-0.308*} & -0.249 & -0.276 & 0.163 & 0.348 & 0.204 & -0.123 \\
%  & (0.048) & (0.100) & (0.149) & (0.091) & (0.425) & (0.094) & \textbf{(0.128)} & (0.169) & (0.212) & (0.120) & (0.228) & (0.383) & (0.305) \\
% \multirow{2}{*}{\textbf{Rural}} & 0.012 & 0.101 & 0.002 & 0.077 & -0.190 & -0.066 & 0.030 & -0.059 & \textbf{-0.258} & -0.078 & -0.066 & 0.010 & -0.305 \\
%  & (0.035) & (0.073) & (0.108) & (0.067) & (0.309) & (0.069) & (0.093) & (0.123) & \textbf{(0.154)} & (0.088) & (0.166) & (0.279) & (0.222) \\
% \multirow{2}{*}{\textbf{Asexual}} & 0.057 & \textbf{0.179} & -0.069 & 0.117 & -0.104 & 0.070 & -0.140 & -0.042 & -0.039 & -0.016 & 0.064 & 0.294 & \textbf{-0.574} \\
%  & (0.049) & \textbf{(0.101)} & (0.150) & (0.092) & (0.429) & (0.095) & (0.129) & (0.171) & (0.214) & (0.121) & (0.230) & (0.387) & \textbf{(0.308)} \\
% \multirow{2}{*}{\textbf{Bisexual}} & -0.023 & -0.144 & 0.161 & 0.101 & -0.080 & -0.134 & -0.181 & -0.095 & 0.305 & 0.085 & -0.105 & 0.620 & \textbf{-0.936**} \\
%  & (0.055) & (0.113) & (0.168) & (0.103) & (0.479) & (0.106) & (0.145) & (0.191) & (0.239) & (0.136) & (0.257) & (0.432) & \textbf{(0.344)} \\
% \multirow{2}{*}{\textbf{Homosexual}} & 0.024 & -0.073 & -0.010 & 0.134 & -0.171 & 0.037 & -0.184 & 0.058 & 0.142 & 0.098 & 0.285 & 0.458 & -0.230 \\
%  & (0.052) & (0.107) & (0.159) & (0.098) & (0.455) & (0.101) & (0.137) & (0.181) & (0.227) & (0.129) & (0.244) & (0.411) & (0.327) \\
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Physically\\ Disabled\end{tabular}}} & 0.019 & -0.018 & -0.168 & -0.036 & 0.312 & 0.033 & -0.040 & -0.118 & 0.054 & 0.107 & -0.033 & 0.190 & 0.323 \\
%  & (0.036) & (0.074) & (0.110) & (0.068) & (0.314) & (0.070) & (0.095) & (0.125) & (0.157) & (0.089) & (0.168) & (0.283) & (0.225) \\
% \multirow{2}{*}{\textbf{African}} & -0.016 & 0.062 & -0.133 & -0.038 & \textbf{0.802*} & -0.009 & 0.135 & -0.126 & -0.104 & 0.113 & 0.017 & -0.558 & 0.123 \\
%  & 0.044 & (0.092) & (0.137) & (0.084) & \textbf{(0.391)} & (0.087) & (0.118) & (0.156) & (0.195) & (0.111) & (0.209) & (0.353) & (0.280) \\
% \multirow{2}{*}{\textbf{Asian}} & -0.023 & 0.127 & -0.244 & -0.034 & 0.341 & -0.071 & 0.212 & -0.194 & -0.114 & 0.183 & 0.109 & \textbf{-1.107**} & \textbf{0.859**} \\
%  & 0.052 & (0.107) & (0.159) & (0.098) & (0.455) & (0.101) & (0.137) & (0.181) & (0.227) & (0.129) & (0.244) & \textbf{(0.410)} & \textbf{(0.326)} \\
% \multirow{2}{*}{\textbf{Hispanic}} & -0.012 & 0.023 & -0.049 & -0.018 & -0.001 & 0.027 & 0.051 & -0.056 & 0.028 & 0.038 & 0.059 & -0.442 & -0.151 \\
%  & (0.044) & (0.091) & (0.136) & (0.084) & (0.388) & (0.086) & (0.117) & (0.155) & (0.194) & (0.110) & (0.208) & (0.350) & (0.279) \\
% \multirow{2}{*}{\textbf{Atheist}} & -0.026 & -0.071 & \textbf{0.616***} & -0.024 & -0.114 & 0.142 & \textbf{0.233} & -0.035 & \textbf{-0.472} & -0.041 & 0.338 & 0.204 & 0.303 \\
%  & (0.051) & (0.106) & \textbf{(0.157)} & (0.097) & (0.449) & (0.099) & \textbf{(0.136)} & (0.179) & \textbf{(0.224)} & (0.127) & (0.240) & (0.405) & (0.322) \\
% \multirow{2}{*}{\textbf{Christian}} & -0.004 & -0.075 & 0.161 & -0.033 & -0.251 & 0.041 & 0.091 & -0.152 & -0.139 & -0.003 & 0.191 & 0.510 & 0.310 \\
%  & (0.048) & (0.100) & (0.149) & (0.092) & (0.426) & (0.094) & (0.129) & (0.170) & (0.213) & (0.121) & (0.228) & (0.384) & (0.306) \\
% \multirow{2}{*}{\textbf{Jewish}} & \textbf{-0.073} & -0.011 & 0.095 & 0.015 & 0.393 & -0.051 & \textbf{0.316**} & 0.022 & -0.139 & 0.000 & \textbf{0.388} & 0.534 & 0.067 \\
%  & \textbf{(0.042)} & (0.088) & (0.130) & (0.080) & (0.372) & (0.082) & \textbf{(0.112)} & (0.148) & (0.186) & (0.105) & \textbf{(0.199)} & (0.336) & (0.267) \\
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Obama\\ Supporter\end{tabular}}} & -0.004 & -0.033 & -0.051 & 0.146 & -0.255 & \textbf{0.178} & -0.160 & 0.102 & -0.022 & -0.021 & 0.289 & -0.102 & -0.341 \\
%  & (0.048) & (0.099) & (0.147) & (0.091) & (0.421) & \textbf{(0.093)} & (0.127) & (0.168) & (0.210) & (0.119) & (0.225) & (0.380) & (0.302) \\
% \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Trump\\ Supporter\end{tabular}}} & 0.054 & -0.068 & -0.121 & -0.085 & \textbf{-0.851} & \textbf{0.191} & \textbf{-0.413**} & 0.125 & 0.079 & 0.189 & -0.132 & 0.274 & -0.141 \\
%  & (0.051) & (0.105) & (0.156) & (0.096) & \textbf{(0.446)} & \textbf{(0.099)} & \textbf{(0.135)} & (0.178) & (0.223) & (0.126) & (0.239) & (0.402) & (0.320) \\
% \multirow{2}{*}{\textbf{Republican}} & -0.007 & -0.138 & -0.073 & -0.088 & -0.212 & -0.031 & -0.142 & 0.166 & 0.036 & 0.179 & -0.284 & 0.108 & 0.232 \\
%  & (0.046) & (0.095) & (0.142) & (0.087) & (0.404) & (0.090) & (0.122) & (0.161) & (0.202) & (0.114) & (0.217) & (0.365) & (0.290) \\
% \multirow{2}{*}{\textbf{Constant}} & 0.328 & 0.374 & 0.423 & 0.247 & 1.911 & 1.245 & 1.363 & 1.748 & 0.809 & 0.187 & 0.490 & 1.742 & 2.382 \\
%  & (0.074) & (0.154) & (0.229) & (0.140) & (0.653) & (0.145) & (0.197) & (0.260) & (0.326) & (0.185) & (0.350) & (0.589) & (0.468) \\ \hline
% \end{tabular}%
% }
% \label{tab:demo_cot}
% \begin{flushleft}
% \textit{Note}: The values in each cell represent the estimated coefficient, with the standard error provided in parentheses below. Bolded coefficients indicate statistical significance at the 90\% confidence level. The asterisks (*, **, ***) denote the levels of statistical significance as follows: *: \( p < 0.05 \) (significant at the 95\% confidence level), **: \( p < 0.01 \) (significant at the 99\% 
% confidence level), ***: \( p < 0.001 \) (significant at the 99.9\% confidence level).

% \end{flushleft}
% \end{table*}









\section{Conclusion and Discussion}
This study sets a new milestone in LLM strategic reasoning evaluation by assessing strategic capability while accounting for contextual influences, addressing the limitations of benchmarks focused solely on NE.
% Our findings highlight the need for comprehensive assessments, showing that performance on math or text understanding tests does not reliably predict reasoning capability.
We find that models that perform well on established benchmarks of text, math, etc., do not necessarily always have higher reasoning levels than others.
Our evaluation also uncovers CoT's limitations, showing that its benefits are not universal and can sometimes degrade reasoning performance. This insight contributes to ongoing discussions on enhancing LLM reasoning workflows, particularly in complex decision-making scenarios. Finally, our analysis of demographic sensitivity highlights an often-overlooked aspect of strategic reasoning—how biases can emerge even in high-performing models. Furthermore, we find that while model distillation can enhance the reasoning ability of smaller models in specific game types, it also introduces additional bias toward demographic features.
 % By embedding these insights into evaluation frameworks, we move toward a more nuanced understanding of fairness in AI reasoning, bridging the gap between capability assessments and ethical considerations in multi-agent interactions.



% This study introduces a novel evaluation scheme grounded in behavioral game theory, enabling advanced assessments of LLMs' strategic reasoning capabilities while accounting for contextual confounders. Through this framework, we evaluated 10 SOTA LLMs across baseline,  CoT, and demographic-feature-embedded settings using a diverse library of 13 games. The results highlight variability in reasoning capabilities across models and contexts. While advanced models like GPT-o1 and QwQ-32B-CoT demonstrate exceptional performance in structured scenarios, smaller models, such as Gemma-2-27b and Llama-3.1-8b, occasionally achieve comparable or superior results in different tasks. Importantly, the evaluation reveals significant sensitivities to demographic features, with CoT prompting amplifying these effects in some cases, raising critical questions about fairness and ethical alignment. 

% These findings validate the power and potential of our proposed framework to go beyond deterministic benchmarks like Nash Equilibrium, capturing the interplay of strategic reasoning, contextual alignment, and fairness considerations. By systematically evaluating reasoning in diverse scenarios, this work provides a robust foundation for advancing LLMs that are not only more capable but also more aligned with ethical and societal goals. By proposing the need for balanced reasoning calibration that prioritizes context-specific objectives and societal alignment over raw performance metrics, this study marks a critical step forward in understanding and developing fair, adaptable, and effective LLMs.


% \subsection*{Discussion}

\textbf{Beyond Scale and Benchmark Performance.} While larger models generally perform well, DeepSeek-R1 achieves competitive results despite its smaller scale. Its success, driven by reinforcement learning and fine-tuned optimization \citep{guo2025deepseek}, underscores the importance of training methodologies in shaping reasoning abilities. 

Although our approach provides a comprehensive evaluation of multi-faceted decision-making, further research is needed to clarify how factors such as mathematical and logical proficiency, post-training techniques, and model scale influence reasoning. These relationships remain unclear, but our work establishes an initial link. Building reliable AI agents requires understanding these dependencies, as improvements in isolated tasks do not necessarily enhance overall reasoning. Identifying and optimizing key hyperparameters is crucial to achieving robust reasoning capabilities.

% While larger models generally perform well, DeepSeek-R1 stands out by achieving competitive results despite smaller scale. Its success, driven by reinforcement learning and fine-tuned optimization strategies \citep{guo2025deepseek}, suggests that training methodologies play a crucial role in shaping reasoning abilities. 
% Moreover, excelling in benchmarks focused on math, logic, and content understanding does not necessarily translate to strong performance in complex strategic contexts. Many high-ranking models on traditional evaluations struggle when faced with multi-agent interactions, uncertainty, and adaptive decision-making. 
% While our approach offers a comprehensive evaluation of a model’s performance in abstract, multi-faceted decision-making, further research is needed to clarify the relationships between factors such as mathematical and logical proficiency, training data selection, post-training techniques, and model size in shaping reasoning ability. These connections remain unclear, but our work is the first to establish an initial link. Building reliable AI agents requires understanding these dependencies, as our results show that improvements in isolated tasks do not necessarily enhance overall reasoning. To achieve a desired level of reasoning, it is crucial to identify and optimize the key hyperparameters influencing it.

\textbf{CoT is Not Uniformly Effective. } While CoT prompting enhances reasoning in language models, its effectiveness depends on task comprehension and alignment with the intended goal. Our analysis reveals that when properly aligned, CoT scaffolds structured logic and improves performance; however, misaligned CoT prompts can amplify errors, akin to optimizing a flawed policy in reinforcement learning. This challenges the proposal that human-designed CoT is universally optimal for step-by-step reasoning, as LLMs may rely on latent reasoning mechanisms—such as parallel or hierarchical logic—that deviate from strict sequential processing. Empirical evidence from DeepSeek-R1 supports this view, as it successfully achieves SOTA performance through reinforcement learning, integrating CoT-like reasoning without enforcing explicit sequential structures. Instead, it dynamically adapts reasoning pathways through reward-driven optimization. Unlike static rule-based systems or predefined sequential reasoning processes, RL-based models iteratively refine their strategies based on feedback from the environment. This allows AI agents to develop flexible, context-aware reasoning mechanisms rather than relying solely on fixed logical structures. These findings highlight a fundamental open question: how to align external prompting strategies with a model’s intrinsic reasoning processes, a challenge crucial for advancing robust and interpretable AI systems.



\textbf{Fairness and Bias in AI Reasoning.} Superior strategic reasoning does not equate to ethical decision-making. When demographic cues are introduced, models adjust their strategies based on the given role, indicating that AI-driven decisions can encode and amplify societal biases. This is especially concerning in multi-agent interactions, where prioritizing strategic efficiency alone may lead to inequitable or adversarial outcomes. 

In interactive settings, LLMs that adjust strategies based on demographics risk perpetuating bias in negotiations and resource allocation, reinforcing real-world inequalities over time. This presents a critical challenge to balance strategic reasoning with fairness. Unconstrained optimization may encourage exploitative tactics, while rigid fairness constraints could hinder adaptability. Future development must move beyond performance metrics to integrate ethical safeguards, ensuring that LLMs engage in fair and justifiable decision-making in dynamic multi-agent environments.



% Through our experiments, we observed that CoT prompting can improve a model's reasoning performance, particularly at lower levels of reasoning capability. However, this improvement is constrained by factors such as model size, architecture, and training methodology. A crucial insight from our investigation is that a model's ability to understand its ultimate objective is essential for CoT to be effective. While CoT helps establish stronger sequential logic, it only proves beneficial when the model is aligned with the correct goal. Otherwise, it acts like fueling a car heading in the wrong direction—leading the model further from the correct answer.

% The strategic reasoning capabilities of LLMs are influenced by their performance on benchmarks related to mathematics, logic games, and content understanding. However, this does not guarantee that the top-ranked model on these benchmarks will consistently outperform others in every scenario. Our approach provides a comprehensive evaluation of a model’s performance in abstract, multi-faceted decision-making tasks. Nevertheless, further research is needed to explore the connections between factors such as model size, dataset selection, and post-training techniques, and their impact on reasoning ability.

% The experiments incorporating demographic features demonstrate how easily LLMs can be influenced and manipulated. When comparing CoT and non-CoT versions, we found that their biases and discriminatory tendencies toward decision-making roles varied significantly. This raises an important question: what level of reasoning should an LLM possess to ensure fairness and reliability? Interestingly, smaller models in our experiments exhibited less bias, but this appears to result from their limited understanding of the meaning and implications of demographic features in decision-making.

% Looking ahead, as researchers continue to advance LLMs, and these commercial models are increasingly deployed in applications like customer service, the stakes grow higher. If such models achieve reasoning levels far surpassing those of humans, it raises concerns about fairness and competition. For instance, would humans even stand a chance against LLMs in high-stakes scenarios, or would the benefits be disproportionately concentrated among businesses deploying these models? Evidence from our experiments already shows that advanced models like Qwen-QwQ-32B and GPT-o1 can outperform students from institutions like Caltech, where the original experiments were conducted. This trend is likely to intensify, underscoring the urgent need for regulations to establish clear boundaries on the development and deployment of these models to ensure fairness and equitable use.

% \subsection*{Limitations and Future Work}
\textbf{Limitations:} While we examine CoT prompting and links between individual and comprehensive benchmarks, the precise causal chain between prompts and final decision-making remains unexplored.  Future research could focus on digging into how individual skills, such as logical reasoning, mathematical ability, and contextual understanding, influence an LLM's overall reasoning capabilities.
Additionally, our evaluation framework is semi-dynamic, relying on non-real-time multi-agent scenarios. In real-time interactions, the same methodology can also apply, but results may differ due to dynamic rectifications. Such settings may reveal different reasoning levels or widen performance gaps between top-tier and lower-tier models due to the interactive nature of these evaluations. Future work may explore these scenarios for deeper insights into LLM behavior.





\bibliographystyle{splncs04}
\bibliography{main}

\clearpage
\newpage

\appendix


\section*{Appendices}
\addcontentsline{toc}{section}{Appendices}

\section{Additional Tables}
% \clearpage


\begin{table*}[h!]
\centering
\caption{LLMs Sensitivity to Demographic Features through Regression Analysis}
\label{tab:demo_base}
\begin{subtable}{\textwidth}
\centering
\caption{SOTA LLM models}
\label{tab:demo_base_A}
\hspace*{-0.1\textwidth}
\resizebox{1.2\textwidth}{!}{%
\begin{tabular}{lccccccccccccc}
\hline
\multicolumn{1}{c}{\textbf{Feature}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPT\\ 4o\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPT\\ o1\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPT\\ o3\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ MoE\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Claude\\ 3\\ Opus\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemma\\ V2\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemini\\ 2.0\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 405B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}InternLM\\ V2\end{tabular}} & \textbf{QwQ} & \textbf{\begin{tabular}[c]{@{}c@{}}GLM\\ 4\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{\textless{}25 years old}} & -0.296 & -0.051 & 0.020 & \textbf{0.140} & -0.017 & -0.071 & -0.106 & -0.090 & 0.177 & -0.001 & 0.026 & 0.027 & \textbf{0.138} \\
 & (0.184) & (0.260) & (0.130) & \textbf{(0.079)} & (0.207) & (0.067) & (0.140) & (0.084) & (0.338) & (0.088) & (0.175) & (0.090) & \textbf{(0.076)} \\
\multirow{2}{*}{\textbf{\textgreater{}55 years old}} & 0.094 & -0.226 & 0.036 & -0.098 & 0.246 & -0.009 & 0.083 & 0.054 & 0.190 & 0.030 & -0.055 & -0.101 & 0.027 \\
 & (0.138) & (0.197) & (0.098) & (0.060) & (0.156) & (0.051) & (0.106) & (0.063) & (0.255) & (0.066) & (0.132) & (0.068) & (0.058) \\
\multirow{2}{*}{\textbf{Female}} & \textbf{0.274*} & 0.263 & 0.118 & \textbf{0.190**} & 0.153 & \textbf{0.146**} & \textbf{0.167} & -0.057 & \textbf{-0.432} & 0.063 & 0.073 & 0.067 & -0.047 \\
 & \textbf{(0.126)} & (0.183) & (0.091) & \textbf{(0.056)} & (0.145) & \textbf{(0.047)} & \textbf{(0.098)} & (0.059) & \textbf{(0.237)} & (0.062) & (0.123) & (0.063) & (0.053) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Graduate\\ Level\end{tabular}}} & 0.208 & -0.283 & -0.029 & -0.070 & 0.255 & -0.069 & 0.020 & 0.053 & 0.131 & 0.034 & -0.076 & -0.122 & \textbf{0.141*} \\
 & (0.170) & (0.245) & (0.122) & (0.075) & (0.195) & (0.063) & (0.132) & (0.078) & (0.317) & (0.083) & (0.165) & (0.082) & \textbf{(0.069)} \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Below\\ Secondary\end{tabular}}} & \textbf{0.275*} & -0.154 & -0.011 & 0.095 & 0.128 & -0.077 & 0.030 & -0.090 & 0.323 & -0.064 & -0.120 & 0.006 & -0.004 \\
 & \textbf{(0.139)} & (0.196) & (0.098) & (0.060) & (0.156) & (0.050) & (0.106) & (0.063) & (0.254) & (0.066) & (0.132) & (0.066) & (0.056) \\
\multirow{2}{*}{\textbf{Divorced}} & 0.062 & 0.041 & -0.014 & -0.049 & \textbf{-0.336} & -0.024 & -0.179 & -0.066 & 0.211 & -0.032 & 0.166 & 0.050 & 0.023 \\
 & (0.163) & (0.237) & (0.118) & (0.072) & \textbf{(0.188)} & (0.061) & (0.128) & (0.076) & (0.307) & (0.080) & (0.159) & (0.078) & (0.066) \\
\multirow{2}{*}{\textbf{Married}} & 0.039 & -0.340 & -0.155 & -0.057 & \textbf{-0.401*} & -0.111 & -0.177 & -0.125 & 0.166 & 0.004 & 0.156 & -0.032 & \textbf{-0.129*} \\
 & (0.166) & (0.239) & (0.119) & (0.073) & \textbf{(0.190)} & (0.061) & (0.129) & (0.077) & (0.310) & (0.081) & (0.161) & (0.077) & \textbf{(0.065)} \\
\multirow{2}{*}{\textbf{Widowed}} & 0.097 & -0.081 & -0.034 & 0.043 & -0.321 & -0.016 & 0.024 & -0.063 & 0.016 & 0.022 & 0.203 & -0.019 & 0.059 \\
 & (0.175) & (0.246) & (0.122) & (0.075) & (0.195) & (0.063) & (0.132) & (0.079) & (0.319) & (0.083) & (0.165) & (0.078) & (0.066) \\
\multirow{2}{*}{\textbf{Rural}} & 0.076 & -0.285 & 0.086 & -0.028 & -0.009 & -0.001 & 0.099 & -0.036 & -0.289 & 0.048 & -0.011 & 0.037 & 0.052 \\
 & (0.126) & (0.179) & (0.089) & (0.055) & (0.142) & (0.046) & (0.096) & (0.057) & (0.232) & (0.060) & (0.120) & (0.058) & (0.049) \\
\multirow{2}{*}{\textbf{Asexual}} & -0.178 & 0.198 & -0.066 & -0.100 & 0.042 & 0.076 & \textbf{-0.224} & -0.022 & 0.230 & -0.010 & \textbf{0.333*} & \textbf{0.082} & 0.016 \\
 & (0.171) & (0.248) & (0.123) & (0.076) & (0.197) & (0.064) & \textbf{(0.134)} & (0.080) & (0.321) & (0.084) & \textbf{(0.167)} & \textbf{(0.080)} & (0.067) \\
\multirow{2}{*}{\textbf{Bisexual}} & -0.176 & 0.342 & 0.055 & -0.122 & -0.004 & 0.025 & -0.099 & \textbf{-0.162} & -0.254 & -0.145 & \textbf{0.438} & -0.075 & -0.017 \\
 & (0.190) & (0.277) & (0.138) & (0.084) & (0.220) & (0.071) & (0.149) & \textbf{(0.089)} & (0.359) & (0.094) & \textbf{(0.186)} & (0.092) & (0.077) \\
\multirow{2}{*}{\textbf{Homosexual}} & -0.262 & -0.111 & -0.076 & -0.002 & \textbf{-0.418*} & 0.086 & -0.160 & -0.057 & -0.044 & -0.143 & \textbf{0.345} & 0.109 & -0.035 \\
 & (0.183) & (0.263) & (0.131) & (0.080) & \textbf{(0.209)} & (0.068) & (0.142) & (0.084) & (0.341) & (0.089) & \textbf{(0.177)} & (0.086) & (0.072) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Physically\\ Disabled\end{tabular}}} & -0.169 & -0.144 & 0.047 & 0.036 & 0.045 & \textbf{0.096*} & -0.099 & -0.033 & -0.151 & 0.095 & 0.012 & 0.000 & -0.049 \\
 & (0.125) & (0.181) & (0.090) & (0.055) & (0.144) & \textbf{(0.047)} & (0.098) & (0.058) & (0.235) & (0.061) & (0.122) & (0.062) & (0.053) \\
\multirow{2}{*}{\textbf{African}} & 0.033 & 0.302 & 0.035 & 0.080 & -0.197 & -0.063 & -0.105 & 0.037 & -0.207 & 0.013 & -0.079 & -0.036 & 0.052 \\
 & (0.158) & (0.226) & (0.113) & (0.069) & (0.180) & (0.058) & (0.122) & (0.072) & (0.293) & (0.076) & (0.152) & (0.074) & (0.062) \\
\multirow{2}{*}{\textbf{Asian}} & 0.267 & 0.086 & -0.081 & 0.102 & -0.001 & -0.015 & -0.187 & 0.115 & -0.160 & 0.126 & 0.061 & 0.082 & 0.083 \\
 & (0.182) & (0.263) & (0.131) & (0.080) & (0.209) & (0.068) & (0.142) & (0.084) & (0.341) & (0.089) & (0.177) & (0.091) & (0.076) \\
\multirow{2}{*}{\textbf{Hispanic}} & 0.129 & 0.098 & -0.070 & 0.007 & -0.123 & -0.026 & 0.071 & -0.043 & -0.046 & 0.026 & -0.134 & -0.112 & 0.022 \\
 & (0.158) & (0.224) & (0.112) & (0.068) & (0.178) & (0.058) & (0.121) & (0.072) & (0.291) & (0.076) & (0.151) & (0.075) & (0.063) \\
\multirow{2}{*}{\textbf{Atheist}} & -0.132 & 0.000 & -0.034 & -0.086 & -0.265 & -0.036 & -0.017 & \textbf{0.220**} & -0.075 & -0.139 & 0.038 & 0.024 & 0.026 \\
 & (0.183) & (0.259) & (0.129) & (0.079) & (0.206) & (0.067) & (0.140) & \textbf{(0.083)} & (0.336) & (0.088) & (0.175) & (0.090) & (0.076) \\
\multirow{2}{*}{\textbf{Christian}} & 0.407 & 0.172 & -0.014 & -0.042 & \textbf{-0.381} & -0.004 & -0.019 & 0.062 & -0.357 & -0.052 & -0.059 & -0.069 & -0.022 \\
 & (0.169) & (0.246) & (0.123) & (0.075) & \textbf{(0.196)} & (0.063) & (0.133) & (0.079) & (0.319) & (0.083) & (0.166) & (0.085) & (0.072) \\
\multirow{2}{*}{\textbf{Jewish}} & -0.008 & 0.251 & 0.043 & -0.075 & -0.176 & 0.001 & -0.144 & -0.070 & -0.149 & -0.107 & -0.009 & -0.028 & -0.042 \\
 & (0.149) & (0.215) & (0.107) & (0.066) & (0.171) & (0.055) & (0.116) & (0.069) & (0.279) & (0.073) & (0.145) & (0.075) & (0.063) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Obama\\ Supporter\end{tabular}}} & -0.098 & -0.381 & -0.022 & -0.086 & -0.073 & -0.068 & -0.026 & 0.048 & -0.091 & -0.022 & -0.086 & -0.042 & -0.096 \\
 & (0.171) & (0.243) & (0.121) & (0.074) & (0.194) & (0.063) & (0.131) & (0.078) & (0.316) & (0.082) & (0.164) & (0.079) & (0.066) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Trump\\ Supporter\end{tabular}}} & -0.028 & 0.012 & 0.049 & \textbf{-0.187*} & \textbf{-0.533*} & 0.016 & 0.116 & 0.022 & -0.576 & -0.110 & -0.175 & 0.040 & -0.034 \\
 & (0.179) & (0.258) & (0.128) & \textbf{(0.079)} & \textbf{(0.205)} & (0.066) & (0.139) & (0.083) & (0.334) & (0.087) & (0.174) & (0.082) & (0.069) \\
\multirow{2}{*}{\textbf{Republican}} & -0.035 & -0.106 & 0.068 & \textbf{-0.152*} & -0.223 & -0.030 & 0.104 & -0.055 & -0.299 & -0.032 & -0.206 & -0.010 & 0.005 \\
 & (0.165) & (0.234) & (0.116) & \textbf{(0.071)} & (0.186) & (0.060) & (0.126) & (0.075) & (0.303) & (0.079) & (0.157) & (0.076) & (0.064) \\
\multirow{2}{*}{\textbf{Constant}} & 0.431 & 1.304 & 0.813 & 1.501 & 2.048 & 0.170 & 0.945 & 1.412 & 2.079 & 1.084 & 0.527 & 0.427 & 0.961 \\
 & (0.262) & (0.377) & (0.188) & (0.115) & (0.300) & (0.097) & (0.203) & (0.121) & (0.489) & (0.127) & (0.254) & (0.028) & (0.022) \\ \hline
\end{tabular}%
}
\end{subtable}

\end{table*}
\clearpage
\begin{table*}[t]
\ContinuedFloat
\centering
\caption*{(Continued) LLMs Sensitivity to Demographic Features through Regression Analysis}
\begin{subtable}{\textwidth}
\centering
\caption{DeepSeek family models}
\label{tab:demo_base_B}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\hline
\multicolumn{1}{c}{\textbf{Feature}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V2.5\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V3\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ R1\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ Qwen\\ 1.5B\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ Qwen\\ 7B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ Qwen\\ 14B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ Qwen\\ 32B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ LLaMA\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DSk-R1\\ LLaMA\\ 70B\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{\textless25 years old}} & -0.047 & 0.002 & 0.154 & 0.224 & 0.180 & 0.170 & 0.017 & -0.041 & -0.098 \\
 & (0.036) & (0.036) & (0.109) & (0.270) & (0.177) & (0.114) & (0.256) & (0.094) & (0.127) \\
\multirow{2}{*}{\textbf{\textgreater55 years old}} & -0.038 & -0.014 & 0.032 & 0.092 & \textbf{-0.235} & -0.003 & 0.042 & -0.039 & \textbf{-0.242*} \\
 & (0.028) & (0.027) & (0.082) & (0.204) & \textbf{0.133} & (0.086) & (0.194) & (0.071) & \textbf{0.095} \\
\multirow{2}{*}{\textbf{Female}} & 0.023 & -0.033 & 0.027 & -0.245 & 0.167 & -0.003 & 0.115 & -0.086 & -0.043 \\
 & (0.026) & (0.025) & (0.076) & (0.189) & (0.124) & (0.080) & (0.180) & (0.066) & (0.089) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Graduate\\ Level\end{tabular}}} & 0.005 & -0.006 & -0.026 & -0.417 & 0.016 & \textbf{0.201} & 0.139 & -0.137 & -0.048 \\
 & (0.034) & (0.034) & (0.102) & (0.254) & (0.166) & \textbf{0.107} & (0.241) & (0.088) & (0.119) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Below\\ Secondary\end{tabular}}} & 0.016 & 0.016 & \textbf{-0.179*} & \textbf{-0.371} & 0.063 & -0.029 & 0.272 & -0.026 & 0.133 \\
 & (0.027) & (0.027) & \textbf{0.082} & \textbf{0.203} & (0.133) & (0.086) & (0.193) & (0.071) & (0.095) \\
\multirow{2}{*}{\textbf{Divorced}} & \textbf{-0.070*} & -0.052 & -0.120 & -0.157 & \textbf{0.531**} & -0.144 & 0.277 & 0.046 & 0.066 \\
 & \textbf{0.033} & (0.033) & (0.099) & (0.245) & \textbf{0.161} & (0.104) & (0.233) & (0.085) & (0.115) \\
\multirow{2}{*}{\textbf{Married}} & \textbf{-0.057} & 0.013 & -0.102 & 0.273 & 0.152 & -0.125 & 0.035 & 0.113 & -0.106 \\
 & \textbf{0.033} & (0.033) & (0.100) & (0.248) & (0.162) & (0.105) & (0.235) & (0.086) & (0.116) \\
\multirow{2}{*}{\textbf{Widowed}} & \textbf{-0.059} & 0.004 & -0.040 & 0.046 & 0.060 & -0.242 & -0.170 & 0.029 & 0.050 \\
 & \textbf{0.034} & (0.034) & (0.103) & (0.255) & (0.167) & (0.108) & (0.242) & (0.088) & (0.119) \\
\multirow{2}{*}{\textbf{Rural}} & -0.014 & -0.019 & \textbf{0.157*} & -0.144 & -0.029 & -0.037 & -0.095 & -0.005 & -0.065 \\
 & (0.025) & (0.025) & \textbf{0.075} & (0.185) & (0.121) & (0.078) & (0.176) & (0.064) & (0.087) \\
\multirow{2}{*}{\textbf{Asexual}} & 0.038 & 0.018 & 0.016 & 0.164 & 0.070 & -0.081 & -0.094 & -0.007 & 0.167 \\
 & (0.035) & (0.034) & (0.104) & (0.257) & (0.168) & (0.109) & (0.244) & (0.089) & (0.120) \\
\multirow{2}{*}{\textbf{Bisexual}} & \textbf{0.078*} & \textbf{0.081*} & 0.109 & 0.338 & -0.109 & 0.095 & \textbf{-0.545*} & -0.071 & 0.109 \\
 & \textbf{0.039} & \textbf{0.038} & (0.116) & (0.287) & (0.188) & (0.122) & \textbf{0.273} & (0.100) & (0.135) \\
\multirow{2}{*}{\textbf{Homosexual}} & 0.029 & 0.012 & \textbf{0.303**} & 0.147 & -0.054 & -0.145 & -0.132 & \textbf{-0.163} & 0.141 \\
 & (0.037) & (0.037) & \textbf{0.110} & (0.273) & (0.179) & (0.115) & (0.259) & \textbf{0.095} & (0.128) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Physically\\ Disabled\end{tabular}}} & -0.024 & -0.003 & 0.081 & 0.138 & 0.089 & -0.073 & 0.062 & 0.020 & 0.051 \\
 & (0.025) & (0.025) & (0.076) & (0.188) & (0.123) & (0.080) & (0.178) & (0.065) & (0.088) \\
\multirow{2}{*}{\textbf{African}} & -0.037 & \textbf{0.067*} & 0.079 & 0.071 & -0.152 & -0.073 & -0.094 & 0.070 & -0.098 \\
 & (0.032) & \textbf{0.031} & (0.094) & (0.234) & (0.153) & (0.099) & (0.222) & (0.081) & (0.110) \\
\multirow{2}{*}{\textbf{Asian}} & -0.011 & 0.044 & 0.050 & -0.207 & \textbf{-0.359*} & -0.106 & 0.073 & 0.074 & \textbf{-0.298*} \\
 & (0.037) & (0.036) & (0.110) & (0.273) & \textbf{0.179} & (0.115) & (0.259) & (0.095) & \textbf{0.128} \\
\multirow{2}{*}{\textbf{Hispanic}} & 0.005 & 0.044 & -0.038 & -0.107 & \textbf{-0.302*} & -0.033 & 0.190 & 0.069 & -0.170 \\
 & (0.031) & (0.031) & (0.094) & (0.233) & \textbf{0.152} & (0.098) & (0.221) & (0.081) & (0.109) \\
\multirow{2}{*}{\textbf{Atheist}} & 0.017 & -0.001 & -0.036 & 0.255 & -0.026 & -0.077 & \textbf{0.445} & 0.005 & 0.108 \\
 & (0.036) & (0.036) & (0.108) & (0.269) & (0.176) & (0.114) & \textbf{0.255} & (0.093) & (0.126) \\
\multirow{2}{*}{\textbf{Christian}} & -0.022 & -0.028 & 0.005 & -0.051 & \textbf{-0.364*} & -0.003 & 0.227 & 0.142 & 0.012 \\
 & (0.034) & (0.034) & (0.103) & (0.255) & \textbf{0.167} & (0.108) & (0.242) & (0.089) & (0.120) \\
\multirow{2}{*}{\textbf{Jewish}} & -0.013 & 0.024 & 0.037 & 0.234 & -0.174 & 0.110 & -0.036 & 0.087 & 0.021 \\
 & (0.030) & (0.030) & (0.090) & (0.223) & (0.146) & (0.094) & (0.212) & (0.077) & (0.105) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Obama\\ Supporter\end{tabular}}} & -0.010 & \textbf{-0.068*} & 0.102 & 0.130 & -0.142 & -0.021 & -0.191 & -0.131 & 0.081 \\
 & (0.034) & \textbf{0.034} & (0.102) & (0.252) & (0.165) & (0.107) & (0.239) & (0.088) & (0.118) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Trump\\ Supporter\end{tabular}}} & -0.008 & -0.051 & 0.070 & 0.023 & -0.072 & 0.066 & -0.373 & -0.034 & 0.236 \\
 & (0.036) & (0.036) & (0.108) & (0.267) & (0.175) & (0.113) & (0.254) & (0.093) & (0.125) \\
\multirow{2}{*}{\textbf{Republican}} & -0.016 & -0.022 & 0.075 & 0.083 & -0.153 & 0.093 & \textbf{-0.456*} & -0.014 & 0.078 \\
 & (0.033) & (0.032) & (0.098) & (0.242) & (0.159) & (0.103) & \textbf{0.230} & (0.084) & (0.114) \\
\multirow{2}{*}{\textbf{Constant}} & 0.934 & 0.373 & 0.125 & 1.456 & 0.891 & 0.518 & 0.852 & 1.242 & 0.391 \\
 & (0.053) & (0.052) & (0.158) & (0.391) & (0.256) & (0.166) & (0.371) & (0.136) & (0.183) \\ \hline
\end{tabular}%
}
\end{subtable}

\begin{flushleft}
\textit{Note}: The values in each cell represent the estimated coefficient, with the standard error provided in parentheses below. Bolded coefficients indicate statistical significance at the 90\% confidence level. The asterisks (*, **, ***) denote the levels of statistical significance as follows: *: \( p < 0.05 \) (significant at the 95\% confidence level), **: \( p < 0.01 \) (significant at the 99\% 
confidence level), ***: \( p < 0.001 \) (significant at the 99.9\% confidence level).

\end{flushleft}
\end{table*}

\begin{table*}[]
\centering
\caption{LLMs Sensitivity to Demographic Features with Chain-of-Thought through Regression Analysis}
\hspace*{-0.05\textwidth} % Shift table slightly left
\resizebox{1.1\textwidth}{!}{%
\begin{tabular}{lccccccccccccc}
\hline
\multicolumn{1}{c}{\textbf{Feature}} & \textbf{\begin{tabular}[c]{@{}c@{}}GPT\\ 4o\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ MoE\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Granite\\ 3.1\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Claude\\ 3\\ Opus\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V2.5\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}DS\\ V3\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemma\\ V2\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Gemini\\ 2.0\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 8B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaMA\\ 405B\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}InternLM\\ V2\end{tabular}} & \textbf{QwQ} & \textbf{\begin{tabular}[c]{@{}c@{}}GLM\\ 4\end{tabular}} \\ \hline
\multirow{2}{*}{\textbf{<25 years old}} & -0.003 & -0.085 & 0.525 & 0.022 & 0.108 & 0.046 & -0.010 & -0.007 & -0.086 & -0.022 & \textbf{0.388} & -0.002 & -0.114 \\
 & (0.042) & (0.323) & (0.407) & (0.097) & (0.451) & (0.051) & (0.136) & (0.100) & (0.241) & (0.128) & \textbf{0.225} & (0.106) & (0.158) \\
\multirow{2}{*}{\textbf{>55 years old}} & 0.036 & \textbf{0.640**} & 0.087 & -0.056 & 0.079 & \textbf{0.065} & 0.010 & 0.057 & -0.040 & 0.078 & 0.193 & 0.102 & -0.035 \\
 & (0.031) & \textbf{0.244} & (0.307) & (0.073) & (0.340) & \textbf{0.039} & (0.103) & (0.075) & (0.182) & (0.096) & (0.170) & (0.080) & (0.119) \\
\multirow{2}{*}{\textbf{Female}} & -0.036 & 0.279 & 0.013 & -0.056 & \textbf{-0.700*} & 0.027 & -0.122 & 0.067 & -0.191 & 0.005 & 0.056 & \textbf{0.217**} & 0.173 \\
 & (0.029) & (0.227) & (0.285) & (0.068) & \textbf{0.316} & (0.036) & (0.095) & (0.070) & (0.169) & (0.089) & (0.158) & \textbf{0.074} & (0.111) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Graduate\\ Level\end{tabular}}} & -0.014 & 0.328 & 0.332 & -0.032 & 0.067 & \textbf{-0.084} & -0.108 & 0.086 & -0.225 & 0.074 & 0.093 & -0.066 & -0.047 \\
 & (0.040) & (0.304) & (0.382) & (0.091) & (0.423) & \textbf{0.048} & (0.128) & (0.094) & (0.227) & (0.120) & (0.211) & (0.100) & (0.148) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Below\\ Secondary\end{tabular}}} & -0.003 & -0.106 & 0.156 & 0.025 & -0.106 & -0.064 & -0.040 & 0.015 & 0.070 & -0.085 & 0.153 & -0.085 & 0.066 \\
 & (0.032) & (0.243) & (0.306) & (0.073) & (0.339) & (0.039) & (0.102) & (0.075) & (0.182) & (0.096) & (0.169) & (0.080) & (0.119) \\
\multirow{2}{*}{\textbf{Divorced}} & -0.043 & 0.208 & \textbf{0.657} & -0.053 & 0.621 & -0.011 & \textbf{-0.219} & 0.068 & -0.056 & 0.129 & 0.161 & 0.018 & 0.095 \\
 & (0.037) & (0.294) & \textbf{0.369} & (0.088) & (0.410) & (0.047) & \textbf{0.124} & (0.091) & (0.219) & (0.116) & (0.204) & (0.096) & (0.143) \\
\multirow{2}{*}{\textbf{Married}} & -0.018 & -0.168 & -0.077 & \textbf{0.179*} & 0.225 & 0.001 & -0.036 & \textbf{-0.186*} & 0.024 & 0.100 & 0.071 & -0.032 & 0.092 \\
 & (0.038) & (0.297) & (0.373) & \textbf{0.089} & (0.414) & (0.047) & (0.125) & \textbf{0.092} & (0.221) & (0.117) & (0.206) & (0.097) & (0.145) \\
\multirow{2}{*}{\textbf{Widowed}} & \textbf{-0.067} & -0.123 & 0.204 & 0.030 & 0.407 & -0.051 & \textbf{-0.308*} & -0.060 & 0.348 & 0.163 & -0.276 & -0.066 & 0.106 \\
 & \textbf{0.040} & (0.305) & (0.383) & (0.091) & (0.425) & (0.048) & \textbf{0.128} & (0.094) & (0.228) & (0.120) & (0.212) & (0.100) & (0.149) \\
\multirow{2}{*}{\textbf{Rural}} & -0.001 & -0.305 & 0.010 & 0.077 & -0.190 & 0.012 & 0.030 & -0.066 & -0.066 & -0.078 & \textbf{-0.258} & 0.101 & 0.002 \\
 & (0.029) & (0.222) & (0.279) & (0.067) & (0.309) & (0.035) & (0.093) & (0.069) & (0.166) & (0.088) & \textbf{0.154} & (0.073) & (0.108) \\
\multirow{2}{*}{\textbf{Asexual}} & -0.061 & \textbf{-0.574} & 0.294 & 0.117 & -0.104 & 0.057 & -0.140 & 0.070 & 0.064 & -0.016 & -0.039 & \textbf{0.179} & -0.069 \\
 & (0.040) & \textbf{0.308} & (0.387) & (0.092) & (0.429) & (0.049) & (0.129) & (0.095) & (0.230) & (0.121) & (0.214) & \textbf{0.101} & (0.150) \\
\multirow{2}{*}{\textbf{Bisexual}} & -0.035 & \textbf{-0.936**} & 0.620 & 0.101 & -0.080 & -0.023 & -0.181 & -0.134 & -0.105 & 0.085 & 0.305 & -0.144 & 0.161 \\
 & (0.043) & \textbf{0.344} & (0.432) & (0.103) & (0.479) & (0.055) & (0.145) & (0.106) & (0.257) & (0.136) & (0.239) & (0.113) & (0.168) \\
\multirow{2}{*}{\textbf{Homosexual}} & -0.003 & -0.230 & 0.458 & 0.134 & -0.171 & 0.024 & -0.184 & 0.037 & 0.285 & 0.098 & 0.142 & -0.073 & -0.010 \\
 & (0.042) & (0.327) & (0.411) & (0.098) & (0.455) & (0.052) & (0.137) & (0.101) & (0.244) & (0.129) & (0.227) & (0.107) & (0.159) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Physically\\ Disabled\end{tabular}}} & -0.010 & 0.323 & 0.190 & -0.036 & 0.312 & 0.019 & -0.040 & 0.033 & -0.033 & 0.107 & 0.054 & -0.018 & -0.168 \\
 & (0.029) & (0.225) & (0.283) & (0.068) & (0.314) & (0.036) & (0.095) & (0.070) & (0.168) & (0.089) & (0.157) & (0.074) & (0.110) \\
\multirow{2}{*}{\textbf{African}} & -0.044 & 0.123 & -0.558 & -0.038 & \textbf{0.802*} & -0.016 & 0.135 & -0.009 & 0.017 & 0.113 & -0.104 & 0.062 & -0.133 \\
 & (0.037) & (0.280) & (0.353) & (0.084) & \textbf{0.391} & 0.044 & (0.118) & (0.087) & (0.209) & (0.111) & (0.195) & (0.092) & (0.137) \\
\multirow{2}{*}{\textbf{Asian}} & -0.048 & \textbf{0.859**} & \textbf{-1.107**} & -0.034 & 0.341 & -0.023 & 0.212 & -0.071 & 0.109 & 0.183 & -0.114 & 0.127 & -0.244 \\
 & (0.043) & \textbf{0.326} & \textbf{0.410} & (0.098) & (0.455) & 0.052 & (0.137) & (0.101) & (0.244) & (0.129) & (0.227) & (0.107) & (0.159) \\
\multirow{2}{*}{\textbf{Hispanic}} & -0.034 & -0.151 & -0.442 & -0.018 & -0.001 & -0.012 & 0.051 & 0.027 & 0.059 & 0.038 & 0.028 & 0.023 & -0.049 \\
 & (0.036) & (0.279) & (0.350) & (0.084) & (0.388) & (0.044) & (0.117) & (0.086) & (0.208) & (0.110) & (0.194) & (0.091) & (0.136) \\
\multirow{2}{*}{\textbf{Atheist}} & -0.017 & 0.303 & 0.204 & -0.024 & -0.114 & -0.026 & \textbf{0.233} & 0.142 & 0.338 & -0.041 & \textbf{-0.472} & -0.071 & \textbf{0.616***} \\
 & (0.042) & (0.322) & (0.405) & (0.097) & (0.449) & (0.051) & \textbf{0.136} & (0.099) & (0.240) & (0.127) & \textbf{0.224} & (0.106) & \textbf{0.157} \\
\multirow{2}{*}{\textbf{Christian}} & -0.051 & 0.310 & 0.510 & -0.033 & -0.251 & -0.004 & 0.091 & 0.041 & 0.191 & -0.003 & -0.139 & -0.075 & 0.161 \\
 & (0.038) & (0.306) & (0.384) & (0.092) & (0.426) & (0.048) & (0.129) & (0.094) & (0.228) & (0.121) & (0.213) & (0.100) & (0.149) \\
\multirow{2}{*}{\textbf{Jewish}} & -0.004 & 0.067 & 0.534 & 0.015 & 0.393 & \textbf{-0.073} & \textbf{0.316**} & -0.051 & \textbf{0.388} & 0.000 & -0.139 & -0.011 & 0.095 \\
 & (0.035) & (0.267) & (0.336) & (0.080) & (0.372) & \textbf{0.042} & \textbf{0.112} & (0.082) & \textbf{0.199} & (0.105) & (0.186) & (0.088) & (0.130) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Obama\\ Supporter\end{tabular}}} & 0.025 & -0.341 & -0.102 & 0.146 & -0.255 & -0.004 & -0.160 & \textbf{0.178} & 0.289 & -0.021 & -0.022 & -0.033 & -0.051 \\
 & (0.039) & (0.302) & (0.380) & (0.091) & (0.421) & (0.048) & (0.127) & \textbf{0.093} & (0.225) & (0.119) & (0.210) & (0.099) & (0.147) \\
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Trump\\ Supporter\end{tabular}}} & -0.026 & -0.141 & 0.274 & -0.085 & \textbf{-0.851} & 0.054 & \textbf{-0.413**} & \textbf{0.191} & -0.132 & 0.189 & 0.079 & -0.068 & -0.121 \\
 & (0.041) & (0.320) & (0.402) & (0.096) & \textbf{0.446} & (0.051) & \textbf{0.135} & \textbf{0.099} & (0.239) & (0.126) & (0.223) & (0.105) & (0.156) \\
\multirow{2}{*}{\textbf{Republican}} & 0.019 & 0.232 & 0.108 & -0.088 & -0.212 & -0.007 & -0.142 & -0.031 & -0.284 & 0.179 & 0.036 & -0.138 & -0.073 \\
 & (0.038) & (0.290) & (0.365) & (0.087) & (0.404) & (0.046) & (0.122) & (0.090) & (0.217) & (0.114) & (0.202) & (0.095) & (0.142) \\
\multirow{2}{*}{\textbf{Constant}} & 4.183 & 2.382 & 1.742 & 0.247 & 1.911 & 0.328 & 1.363 & 1.245 & 0.490 & 0.187 & 0.809 & 0.374 & 0.423 \\
 & (0.058) & (0.468) & (0.589) & (0.140) & (0.653) & (0.074) & (0.197) & (0.145) & (0.350) & (0.185) & (0.326) & (0.154) & (0.229) \\ \hline
\end{tabular}%
}
\label{tab:demo_cot}
\begin{flushleft}
\textit{Note}: The values in each cell represent the estimated coefficient, with the standard error provided in parentheses below. Bolded coefficients indicate statistical significance at the 90\% confidence level. The asterisks (*, **, ***) denote the levels of statistical significance as follows: *: \( p < 0.05 \) (significant at the 95\% confidence level), **: \( p < 0.01 \) (significant at the 99\% 
confidence level), ***: \( p < 0.001 \) (significant at the 99.9\% confidence level).

\end{flushleft}
\end{table*}








\twocolumn
\section{Game Library Design} 
\label{app:lib}
\small
\sloppy
In this manuscript, we have developed and collected multiple games, the payoff matrices are below. 
The complete information includes:


\begin{table}[H]
\centering
\caption{Competitive (Zero-sum) Game - Base}

\vspace{5pt}
\resizebox{0.6\columnwidth}{!}{%
\begin{tabular}{|ccc|}
\hline
10, -10 & 0, 5 & -5, 8 \\
-10, 10 & 5, 0 & 8, -5 \\
0, 0 & 5, -5 & -5, 5 \\ \hline
\end{tabular}}

\label{tab:comp-base}
\end{table}

\begin{table}[H]
\centering
\caption{Competitive (Zero-sum) Game - High-stake}

\vspace{5pt}
\resizebox{0.6\columnwidth}{!}{%
\begin{tabular}{|ccc|}
\hline
20, -20 & 0, 10 & -10, 15 \\
-20, 20 & 10, 0 & 15, -10 \\
0, 0 & 10, -10 & -10, 10 \\ \hline
\end{tabular}}


\label{tab:comp-hs}
\end{table}

\begin{table}[H]
\centering
\caption{Competitive (Zero-sum) Game - Low-stake}
\vspace{5pt}
\resizebox{=0.5\columnwidth}{!}{%
\begin{tabular}{|ccc|}
\hline
3, -3 & 0, 1 & -1, 2 \\
-3, 3 & 1, 0 & 2, -1 \\
0, 0 & 1, -1 & -1, 1 \\ \hline
\end{tabular}}

\label{tab:comp-ls}
\end{table}

\begin{table}[H]
\centering
\caption{Cooperation Games (Stag Hunt) - Base}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
8, 8 & 0, 7 \\
7, 0 & 5, 5 \\ \hline
\end{tabular}}

\label{tab:coop-base}
\end{table}

\begin{table}[H]
\centering
\caption{Cooperation Games (Stag Hunt) - High-payoff}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
20, 20 & 0, 7 \\
7, 0 & 5, 5 \\ \hline
\end{tabular}}

\label{tab:coop-hp}
\end{table}

\begin{table}[H]
\centering
\caption{Cooperation Games (Stag Hunt) - Asymmetric-payoff}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
12, 8 & 0, 7 \\
7, 0 & 5, 5 \\ \hline
\end{tabular}}

\label{tab:coop-ap}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-Motive Games (Prisoner's Dilemma) - Base}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
3, 3 & 0, 5 \\
5, 0 & 1, 1 \\ \hline
\end{tabular}}

\label{tab:mixed-base}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-Motive Games (Prisoner's Dilemma) - High-punishment for Defection}
\vspace{5pt}
\resizebox{=0.35\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
10, 10 & 0, 15 \\
15, 0 & -5, 5 \\ \hline
\end{tabular}}

\label{tab:mixed-hp}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-Motive Games (Prisoner's Dilemma) - Low-punishment for Defection}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
3, 3 & 0, 4 \\
4, 0 & 2, 2 \\ \hline
\end{tabular}}

\label{tab:mixed-lp}
\end{table}

\begin{table}[H]
\centering
\caption{Sequential Games}
\vspace{5pt}
\resizebox{=0.5\columnwidth}{!}{%
\begin{tabular}{|ccc|}
\hline
0, 5 & 0, 3 & 0, 0 \\
5, 2 & 3, 3 & -1, -1 \\
2, 4 & 4, 3 & 0, -2 \\ \hline
\end{tabular}}

\label{tab:seq}
\end{table}


\begin{enumerate}[left=0pt]
    \item \textbf{Competitive (Zero-Sum) Games}: These games model adversarial interactions where one player's gain is exactly balanced by the other player's loss. The game consists of three variations:
    \begin{itemize}[left=0pt]
        \item \textbf{Baseline}: A standard zero-sum game where players choose strategies to maximize their own payoff while minimizing their opponent’s.
        \item \textbf{High-Stake}: A version where payoff differences are significantly larger, increasing the risk and reward of each decision.
        \item \textbf{Low-Stake}: A variant where payoff differences are minimized, reducing the overall impact of each decision and testing an agent’s ability to navigate lower-risk scenarios.
    \end{itemize}
    
    \item \textbf{Cooperation Games (Stag Hunt)}: These games examine the trade-off between individual risk and collective reward, highlighting the challenge of trust and cooperation. Three variations are included:
    \begin{itemize}[left=0pt]
        \item \textbf{Baseline}: The classic Stag Hunt game, where mutual cooperation leads to the highest payoff, but unilateral deviation results in significant losses.
        \item \textbf{High-Payoff}: A modified version where the rewards for successful cooperation are increased, testing whether agents are more willing to take cooperative risks.
        \item \textbf{Asymmetric-Payoff}: A variant where one player receives a higher payoff for cooperation than the other, introducing an imbalance that challenges fairness and trust dynamics.
    \end{itemize}
    
    \item \textbf{Mixed-Motive Games (Prisoner’s Dilemma)}: These games explore the tension between cooperation and self-interest, where defection offers short-term individual benefits but harms collective outcomes. Variations include:
    \begin{itemize}[left=0pt]
        \item \textbf{Baseline}: The standard Prisoner’s Dilemma setup, where mutual cooperation yields moderate rewards, but unilateral defection offers a higher individual payoff at the expense of the other player.
        \item \textbf{High-Punishment}: A version where the penalty for defection is significantly increased, discouraging selfish behavior.
        \item \textbf{Low-Punishment}: A variant where the cost of defection is minimal, encouraging more frequent defection and testing an agent's ability to recognize long-term cooperation benefits.
    \end{itemize}

    \item \textbf{Sequential Games}: These games test strategic planning and reaction-based decision-making by introducing turn-based interactions.
    \begin{itemize}[left=0pt]
        \item Player 1 makes the first move, setting the stage for Player 2's decision.
        \item The full payoff matrix is shown to Player 1, giving them a strategic advantage in planning their move.
        \item Player 2 must decide based on the observed action of Player 1, testing their ability to infer intent and optimize their response.
    \end{itemize}
    In our experiment, only Player 1's responses are collected for strategic reasoning evaluation.  
\end{enumerate}


\begin{table}[H]
\centering
\caption{Bayesian Coordination Games - Type 1 Matrix}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
10, 10 & 5, 2 \\
7, 5 & 3, 3 \\ \hline
\end{tabular}}

\label{tab:Bay-T1}
\end{table}

\begin{table}[H]
\centering
\caption{Bayesian Coordination Games - Type 2 Matrix}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
8, 8 & 6, 3 \\
5, 4 & 2, 2 \\ \hline
\end{tabular}}

\label{tab:Bay-T2}
\end{table}

\begin{table}[H]
\centering
\vspace{5pt}
\caption{Signaling Games - Sender Matrix}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
5, 5 & 2, 1 \\
3, 2 & 1, 0 \\ \hline
\end{tabular}}

\label{tab:Sig-send}
\end{table}

\begin{table}[H]

\centering
\caption{Signaling Games - Receiver Matrix}
\vspace{5pt}
\resizebox{=0.3\columnwidth}{!}{%
\begin{tabular}{|cc|}
\hline
4, 4 & 6, 3 \\
2, 3 & 1, 2 \\ \hline
\end{tabular}}

\label{tab:Sig-rec}
\end{table}
The incomplete information includes:
\begin{enumerate}
    \item \textbf{Bayesian Coordination Games}: These games test an agent's ability to make optimal decisions under uncertainty. Each round presents two payoff matrices, one of which is randomly selected based on a given probability distribution. The player must make a decision without knowing which matrix is active, relying solely on probabilistic reasoning. The challenge lies in balancing risk and expected utility while considering the likelihood of each scenario. This game type evaluates how well an agent can infer optimal strategies from incomplete information.

    \item \textbf{Signaling Games}: These games assess asymmetric information processing and strategic signaling. Player 1, the sender, has access to both a real and a fake payoff matrix. Player 2, the receiver, is only given the fake matrix and is aware that it does not reflect the true payoffs. Player 1 can send a signal to influence Player 2's decision, and Player 2 must determine whether to trust or disregard the signal. This game tests an agent’s ability to strategically communicate and interpret signals in an environment where deception and trust play key roles.
\end{enumerate}


\begin{table}[H]
\centering
\caption{S-W 10}
\vspace{5pt}
\resizebox{=0.6\columnwidth}{!}{%
\begin{tabular}{|ccc|}
\hline
47, 47 & 51, 44 & 28, 43 \\
44, 51 & 11, 11 & 43, 91 \\
43, 28 & 91, 43 & 11, 11 \\ \hline
\end{tabular}}
\label{tab:SW10}
\end{table}
S-W 10 matrix is the matrix we borrow from the human research in behavioral economics in \cite{stahl1994experimental, rogers2009heterogeneous}. 

\clearpage

\onecolumn

\section{Prompt Design}
In this section, we present the complete prompts used in our experiments. Three types of experiments were conducted in all cases.

The first type involves the vanilla version of the prompts, with specific variations for the Sequential Game, Bayesian Coordination Game, and Signaling Game, as detailed in Appendix \ref{app:lib}. The second type includes zero-shot CoT prompts, which add two additional sentences to the base prompts.

For the demographic feature-embedded prompts, a template is followed, where demographic information is incorporated either as part of the system context or as an initial section of the user content, explicitly stating the user's profile with demographic features.
\subsection{Vanilla Experiment Prompt}
The prompts used for the baseline evaluation of LLMs are listed in Table \ref{tab:prompt1}, \ref{tab:prompt2}, \ref{tab:prompt3}, and \ref{tab:prompt4}. These prompts are repeatedly sent using the LLMs' API or cloud services' API. 
\subsection{Demographic Feature Embedding}
The prompts used in the demographic-feature embedded experiment consist of two parts. The main body of the questions remains consistent with those in the vanilla prompt design. However, each prompt is augmented with a demographic component based on the template provided in Table \ref{tab:prompt-template}. This demographic information is added at the beginning of each prompt to ensure that the LLM retains all features throughout the interaction, preventing memory loss during long-term conversations.
\subsection{Chain-of-Thought Embedding}
The prompts used in the CoT embedding experiment are straightforward. Following the zero-shot CoT approach, a few additional sentences are appended to the end of each prompt to activate the CoT feature and enhance the reasoning capability of the model. Examples of the added CoT prompts are provided in Table \ref{tab:prompt-cot}.





\begin{table*}[h!]
\centering
\caption{User Prompt for Competitive, Cooperation, and Mixed-motive Game}
\vspace{5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c}
\hline
\textbf{User Prompt}                                                                                \\ \hline
You are an assistant who helps to make a choice in a game.                                 \\
You should make choices based on the idea of maximizing the number you get from a matrix.  \\
Now you are player one/two. You have to pick a row/column number x/y from row 0, 1, and 2. \\
Then there will be a player two/one who will pick column number y/x from column 0, 1, and 2.    \\
Your payoff will be the first/second value in location (x, y).                                    \\
Assume the matrix is \textcolor{red}{\textless{}A Matrix\textgreater{}}                                     \\
Please only give me a result of the row number you picked, do not include any thinking process. \\ \hline
\end{tabular}%
}

\label{tab:prompt1}
\end{table*}


\begin{table*}[h!]
\centering
\caption{User Prompt for Bayesian Coordination Game}
\vspace{5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c}
\hline
\textbf{User Prompt}                                                                                  \\ \hline
You are an assistant who helps to make a choice in a game.                                   \\
You should make choices based on the idea of maximizing the number you get from a matrix.    \\
Now you are player one/two. You have to pick a row/column number x/y from row 0 and 1.       \\
Then there will be a player two/one who will pick column/row number y/x from column 0 and 1. \\
Your payoff will be the first/second value in location (x, y).                               \\
With a \textcolor{red}{\textless{}p\textgreater} percent chance, you will be facing Matrix: \textcolor{red}{\textless{}A Matrix\textgreater{}}.   \\
With a \textcolor{red}{\textless{}1-p\textgreater} percent chance, you will be facing Matrix: \textcolor{red}{\textless{}B Matrix\textgreater{}.} \\
Please only give me a result of the row number you picked, do not include any thinking process.       \\ \hline         
\end{tabular}%
}

\label{tab:prompt2}
\end{table*}

\begin{table*}[]
\centering
\caption{User Prompt for Sequential Game}
\vspace{5pt}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{cl}
\hline
\multicolumn{2}{c}{\textbf{User Prompt}} \\ \hline
% \multicolumn{2}{c}{\multirow{2}{*}{Player one}} \\
\multicolumn{2}{c}{} \\
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Now you are player one. You are the first player to pick. \\ You have to pick a row number x from row 0, 1, and 2.\end{tabular}} \\
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Then there will be a player two who will pick column number y \\ from column 0, 1, and 2 based on your decision.\end{tabular}} \\
\multicolumn{2}{c}{Your payoff will be the first value in location (x, y).} \\
\multicolumn{2}{c}{Assume the matrix is \textcolor{red}{\textless{}A Matrix\textgreater{}.}} \\
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Please only give me a result of the row number you picked, \\ do not include any thinking process.\end{tabular}} \\ \hline
\end{tabular}%
}
\label{tab:prompt3}
\end{table*}


% \begin{table*}[]
% \centering
% \caption{User Prompt for Sequential Game}
% \vspace{5pt}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{cc}
% \hline
% \multicolumn{2}{c}{\textbf{User Prompt}}                                                                                                         \\ \hline
% \multicolumn{1}{c|}{Player one}                                              & Player two                                               \\
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}You are tasked with selecting a row in order to \\ maximize your payoff in a matrix game.\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}You are tasked with selecting a column in order to \\ maximize your payoff in a matrix game.\end{tabular} \\
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Now you are player one. You are the first player to pick. \\ You have to pick a row number x from row 0, 1, and 2.\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}Now you are player two. You have to pick a column number y \\ from column 0, 1, and 2.\end{tabular} \\
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Then there will be a player two who will pick column number y \\ from column 0, 1, and 2 based on your decision.\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}Then there will be a player one who will \\ pick row number x from row 0, 1, and 2.\end{tabular} \\
% \multicolumn{1}{c|}{Your payoff will be the first value in location (x, y).} & Your payoff will be the second value in location (x, y). \\
% \multicolumn{1}{c|}{Assume the matrix is \textcolor{red}{\textless{}A Matrix\textgreater{}.}} & Assume the matrix is \textcolor{red}{\textless{}A Matrix\textgreater{}}.  \\
% \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Please only give me a result of the row number you picked, \\ do not include any thinking process.\end{tabular}} &
%   \begin{tabular}[c]{@{}c@{}}Please only give me a result of the row number you picked, \\ do not include any thinking process.\end{tabular} \\ \hline
% \end{tabular}%
% }
% \label{tab:prompt3}
% \end{table*}



\begin{table*}[]
\centering
\caption{User Prompt for Signaling Game}
\vspace{5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc}
\hline
\multicolumn{2}{c}{\textbf{User Prompt}} \\ \hline
\multicolumn{1}{c|}{Player one} &
  Player two \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}You should make choices based on the idea of \\ maximizing the number you get from a matrix.\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}You should make choices based on the idea of \\ maximizing the number you get from a matrix.\end{tabular} \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Now you are player one. \\ You have to pick a row number x from row 0 and 1.\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Now you are player two. \\ You have to pick a column number y from column 0 and 1.\end{tabular} \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Then there will be a player two who will pick \\ column number y from column 0 and 1.\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Then there will be a player one who will pick \\ row number x from row 0 and 1.\end{tabular} \\
\multicolumn{1}{c|}{Your payoff will be the first value in location (x, y).} &
  Your payoff will be the second value in location (x, y). \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}The true matrix that determines the \\ payoff is Matrix: \textcolor{red}{\textless{}Matrix True\textgreater{}}.\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}The matrix you will be seeing is different from the true matrix, \\ but you have to make your best selection \\ based on your guess and the matrix you see.\end{tabular} \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}However, the matrix player two will be seeing is \\ Matrix: \textcolor{red}{\textless{}Matrix Fake\textgreater{}}.\end{tabular}} &
  The matrix is \textcolor{red}{\textless{}Matrix Fake\textgreater{}}. \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Please only give me a result of the row number you picked, \\ do not include any thinking process.\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}Please only give me a result of the column number you picked,\\ do not include any thinking process.\end{tabular} \\ \hline
\end{tabular}%
}

\label{tab:prompt4}
\end{table*}

\begin{table*}[]
\centering
\caption{Demographic Feature Prompt Template}
\vspace{5pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c}
\hline
Template \\ \hline
\begin{tabular}[c]{@{}c@{}}Imagine a \textcolor{red}{{[}'Age'{]}} year old \textcolor{red}{{[}'Gender'{]}} with a \textcolor{red}{{[}'Education'{]}} degree, \\ who is \textcolor{red}{{[}'Marital Status'{]}} and lives in a \textcolor{red}{{[}'Location'{]}} area. \\ This individual identifies as \textcolor{red}{{[}'Sexual Orientation'{]}} and is \textcolor{red}{{[}'Disability'{]}}, \\ of \textcolor{red}{{[}'Race'{]}} descent, adheres to \textcolor{red}{{[}'Religion'{]}} beliefs, and supports \textcolor{red}{{[}'Political Affiliation'{]}} policies. \\ Consider the risk preferences and decision-making processes of a person with these characteristics.\end{tabular} \\ \hline
\end{tabular}%
}

\label{tab:prompt-template}
\end{table*}

\begin{table*}[]
\caption{Chain-of-Thought version Prompt of Competitive Games}
\vspace{5pt}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c}
\hline
\textbf{User Prompt}                                                                                \\ \hline
You are an assistant who helps to make a choice in a game.                                 \\
You should make choices based on the idea of maximizing the number you get from a matrix.  \\
Now you are player one/two. You have to pick a row/column number x/y from row 0, 1, and 2. \\
Then there will be a player two/one who will pick column number y/x from column 0, 1, and 2.    \\
Your payoff will be the first/second value in location (x, y).                                    \\
Assume the matrix is \textcolor{red}{\textless{}A Matrix\textgreater{}}                                     \\
To decide, analyze the possible outcomes based on maximizing your payoff. \\ 
Explain your reasoning step by step and then provide only the row/column number you picked as the final answer. \\ \hline
\end{tabular}%
}
\label{tab:prompt-cot}
\end{table*}


\clearpage
\newpage



\end{document}