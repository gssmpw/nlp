\section{Related Work}
\label{related_work}

Graph Neural Networks (GNNs) have emerged as a powerful tool for simulating complex physical systems, particularly on unstructured meshes____. 
However, these methods predominantly rely on supervised training, which requires extensive annotated data. Common approaches involve generating data through analytical solvers like OpenFOAM____ and ArcSim____. Additionally, some works use real-world observations to train models____.
Early work, such as \MGN ____, adapts the Encoder-Process-Decode architecture____ to mesh data, with the Process module implemented as a GNN for effective message passing. 
Variants like EA-GNN and M-GNN____ introduce enhancements such as virtual edges and multi-resolution graphs to improve efficiency and handle long-range interactions. 
Additionally, the transformer architecture has been explored in mesh-based physical simulations. Hybrid models like the GMR-Transformer-GMUS____ and HCMT____ combine GNNs to learn local rules and transformers to capture global context and long-term dependencies over roll-out trajectories. 
Unlike most methods that directly predict future states from input data, C-GNS____ employs a GNN to model system constraints and computes future states by solving an optimization problem based on these learned constraints. 

Transfer learning, which transfers knowledge from a source domain to a target domain, has gained prominence in deep learning for improving performance and reducing the need for annotated data ____.
Strategies typically involve parameter control, either by sharing parameters between models or enforcing similarity through techniques like $l^2$-norm penalties____.
These approaches have proven effective in computer vision and natural language processing. However, the application of transfer learning to GNN-based physics simulations remains largely unexplored.