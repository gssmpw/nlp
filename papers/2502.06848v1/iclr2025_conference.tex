
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\iclrfinalcopy % !!!!comment out this line before submission!!!!!

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{natbib} 
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\ours}{\textsc{SguNet}\xspace}
\newcommand{\MGN}{MGN\xspace}
\newcommand{\PTData}{ABCD\xspace}

\title{Transfer learning in Scalable Graph Neural Network for Improved Physical Simulation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% !!! Change it to anoymous submission 
\author{Siqi Shen \thanks{This work was done during the author's internship at Apple.} \\
Peking University \\
\texttt{shensiqi1009@pku.edu.cn} \\
\And
Yu Liu \thanks {Equal Contribution} \\
Apple \\
\texttt{yliu66@apple.com} \\
\And
Daniel Biggs \footnotemark[2] \\
Apple \\
\texttt{dbiggs@apple.com} \\
\And
Omar Hafez \\
Apple \\
\texttt{ohafez@apple.com}
\And
Jiandong Yu \\
Apple \\ 
\texttt{jiandong\_yu@apple.com} \\
\And
Wentao Zhang \\
Peking University \\
\texttt{wentao.zhang@pku.edu.com} \\
\And
Bin Cui \\
Peking University \\
\texttt{bin.cui@pku.edu.cn} \\
\And 
Jiulong Shan \\
Apple \\
\texttt{jlshan@apple.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% (Scalable Graph Neural Network) - (Weights alignment) - (pre-training dataset) -> (Performance and impact)
In recent years, Graph Neural Network (GNN) based models have shown promising results in simulating physics of complex systems. However, training dedicated graph network based physics simulators can be costly, as most models are confined to fully supervised training, which requires extensive data generated from traditional physics simulators. To date, how transfer learning could improve the model performance and training efficiency has remained unexplored.
In this work, we introduce a pre-training and transfer learning paradigm for graph network simulators. 
We propose the scalable graph U-net (\ours). Incorporating an innovative depth-first search (DFS) pooling, the \ours is adaptable to different mesh sizes and resolutions for various simulation tasks. 
To enable the transfer learning between differently configured \textsc{SguNet}s, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance.
To pre-train our physics simulator we created a dataset which includes 20,000 physical simulations of randomly selected 3D shapes from the open source A Big CAD (ABC) dataset. 
We show that our proposed transfer learning methods allow the model to perform even better when fine-tuned with small amounts of training data than when it is trained from scratch with full extensive dataset.
On the 2D Deformable Plate benchmark dataset, our pre-trained model fine-tuned on 1/16 of the training data achieved an 11.05\% improvement in position RMSE compared to the model trained from scratch.

\end{abstract}


\section{Introduction}
% 3 subparts: 
% 1) Introduce the success adoption of Graph Neural network in physical simulations, mentioning U-Net (probably Transformer). Emphasis current approaches are all fully supervise trained.
% 2) Introduce the transfer learning. Mentioning the challenge in Transfer learning in our proposed context - weights alignement.
% 3) Introduce our work

Graph Neural Networks (GNNs) have shown promising results in simulating physics of complex systems on unstructured meshes~\cite{meshgraphnets, GraphNetworkDiscontinuous, GraphNetworkConstraint, allen2022FIG}. 
Existing works stack message passing (MP) blocks to model propagation of physical information. Different pooling operations ~\cite{li2020multipole, pmlr-v202-cao23a} and U-Net like architectures~\cite{gnnunet, DESHPANDE2024108055} have been introduced to better solve the multi-scale challenges in different simulation tasks.
However, despite their potential, current GNN-based methods rely heavily on supervised training approaches. Collecting extensive annotated data typically involves using traditional Finite Element Analysis (FEA) solvers~\cite{TemporalAttention, MultiScaleMGN, allen2022physical}, 3D engines~\cite{greff2022kubric}, and real-life video clips~\cite{lopez2024scalingFIG}. The substantial cost of acquiring the training data has constrained the scalability and practicality of GNN-based simulators.

On the other hand, transfer learning has revolutionized fields like computer vision (CV)~\cite{TLResNet1, TLResNet2} and natural language processing (NLP)~\cite{GPT2, GPT3, llama}, where models pre-trained on large datasets are fine-tuned for specific tasks, leading to remarkable improvements in training efficiency and the model's performance~\cite{TLsurvey}. 
However, for GNN-based simulators, network architecture hyperparameters, such as number of message passing steps and pooling ratios, must be specifically tailored to the mesh resolution of the target problem~\cite{MultiScaleMGN, pmlr-v202-cao23a}. As a result, pre-trained models are difficult to load and fine-tune directly for downstream GNN-based simulators. It remains unexplored how to apply transfer learning to GNN-based physics simulators.

In this work, we introduce a transfer learning paradigm applied to the proposed scalable graph U-net (\ours) for physical simulations. The \ours follows the Encoder-Processor-Decoder design and incorporates an innovative DFS pooling operation to handle various mesh receptive fields. It is designed to be modular and configurable, making it adaptable to different mesh sizes and resolutions across various simulation tasks. To enable transfer learning between differently configured \textsc{SguNet}s, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance. As there is no existing dataset available for pre-training, we created a dataset named ABC Deformable (\PTData) for pre-training. The dataset includes approximately 20,000 physical simulations of deformable bodies, whose shapes are sampled from the open-source ABC dataset~\cite{koch2019abc}.

We evaluated our proposed methods over two public datasets, namely the 2D Deformable Plate \cite{GGNS} and a more complex 3D Deforming Plate \cite{meshgraphnets}. We set \textsc{MeshGraphNet} (\MGN) \cite{meshgraphnets} training from scratch as the baseline. On 2D Deformable Plate, our model pre-trained by \PTData and fine-tuned on 1/16 of the training data could achieve an 11.05\% improvement in position RMSE compared to the model trained from scratch. On 3D Deforming Plate, our pre-trained model reached the same level of performance when fine-tuned with only 1/8 of the training data in 40\% of the training time. Applying the transfer learning approach to \MGN also lead to better performance with less training data and shorter training time. 

% To address these challenges, we propose a Scalable Graph Neural Network (\ours), which features a modular design tailored for different mesh sizes and simulation tasks. Specifically, \ours integrates a message passing mechanism for local information aggregation with a Graph U-Net~\cite{Graph-Unets} architecture to capture long-range dependencies. To facilitate transfer learning, we modularize these components, making the weight transfer strategy between the pre-trained and fine-tuned models flexible and adaptable to various mesh resolutions and simulation tasks.

% We pre-train our model on a large-scale synthetic dataset, which includes approximately 20,000 physical simulations based on the open-source ABC dataset~\cite{koch2019abc}. Our experimental results demonstrate that the fine-tuned model significantly outperforms models trained from scratch. Remarkably, the fine-tuned model achieves comparable performance even when trained on just $\frac{1}{16}$ of the original dataset size.

% Our contributions can be summarized as follows:
% \textbf{1) pre-training Framework:} To the best of our knowledge, we introduce the first pre-training and fine-tuning framework specifically designed for mesh-based physical simulations, addressing the inefficiencies associated with training models from scratch for each new task.
% \textbf{2) Hybrid Model Architecture:} We develop a hybrid model that integrates message passing for local information aggregation with a Graph Unet for capturing long-range dependencies, providing a robust solution for mesh-based simulations.
% \textbf{3) Scaling Techniques:} We implement novel scaling techniques that ensure effective knowledge transfer from the pre-trained model to the fine-tuned model, leading to improved performance metrics.
% \textbf{4) Synthetic Dataset:} We propose a large-scale synthetic dataset for pre-training, which empowers the model's ability to generalize across a wide range of simulation tasks.
% \textbf{5) Experimental Validation:} Our extensive experiments validate the efficacy and scalability of our approach, demonstrating faster convergence, lower loss, and reduced computational costs compared to existing methods.

\section{Related Work}
\label{related_work}

Graph Neural Networks (GNNs) have emerged as a powerful tool for simulating complex physical systems, particularly on unstructured meshes~\cite{meshgraphnets, GraphNetworkDiscontinuous, GraphNetworkConstraint, allen2022FIG, allen2022physical}. 
However, these methods predominantly rely on supervised training, which requires extensive annotated data. Common approaches involve generating data through analytical solvers like OpenFOAM~\cite{OpenFOAM} and ArcSim~\cite{arcsim}. Additionally, some works use real-world observations to train models~\cite{particle-based-RGB, allen2022FIG}.
Early work, such as \MGN ~\cite{meshgraphnets}, adapts the Encoder-Process-Decode architecture~\cite{GNS} to mesh data, with the Process module implemented as a GNN for effective message passing. 
Variants like EA-GNN and M-GNN~\cite{gnnunet} introduce enhancements such as virtual edges and multi-resolution graphs to improve efficiency and handle long-range interactions. 
Additionally, the transformer architecture has been explored in mesh-based physical simulations. Hybrid models like the GMR-Transformer-GMUS~\cite{TemporalAttention} and HCMT~\cite{HCMT} combine GNNs to learn local rules and transformers to capture global context and long-term dependencies over roll-out trajectories. 
Unlike most methods that directly predict future states from input data, C-GNS~\cite{GraphNetworkConstraint} employs a GNN to model system constraints and computes future states by solving an optimization problem based on these learned constraints. 

Transfer learning, which transfers knowledge from a source domain to a target domain, has gained prominence in deep learning for improving performance and reducing the need for annotated data ~\cite{TLResNet1, TLResNet2, GPT3, llama}.
Strategies typically involve parameter control, either by sharing parameters between models or enforcing similarity through techniques like $l^2$-norm penalties~\cite{TLsurvey, gouk2020distance, xuhong2018explicit}.
These approaches have proven effective in computer vision and natural language processing. However, the application of transfer learning to GNN-based physics simulations remains largely unexplored.

\section{Method}

\subsection{Overview}

In this section, we introduce our pre-training and fine-tuning framework. We begin by detailing the data format used by our model. Following this, we provide an in-depth explanation of the model architecture, discussing its fundamental networks, operators, and key modules. Finally, we describe the transfer learning mechanism, emphasizing two mapping functions that adjust the model size for optimal performance.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/hetero_graph.pdf}
        \caption{}
        \label{hetero_graph}
    \end{subfigure}
    \hspace{0.1\columnwidth}
    \begin{subfigure}[b]{0.4\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/sample_graph.pdf}
        \caption{}
        \label{sample_graph}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{(a) An illustration of the composition of mesh data $\mathcal{M}$ and its representation as a heterogeneous graph $\mathcal{G}^{hetero}$. (b) Example of down-sampled graphs with pooling ratios as 3 and 2.}
    \vspace{-0.2cm}
\end{figure}

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{0.8\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/model_simple.pdf}
        \caption{}
        \label{Model_detail}
    \end{subfigure}
    \vspace{0.25cm}

    \begin{subfigure}[b]{0.4\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/mapping_Uniform.pdf}
        \caption{Uniform mapping}
        \label{mapping_Uniform}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}[b]{0.4\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/mapping_firstN.pdf}
        \caption{First-N mapping}
        \label{mapping_firstN}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{(a) A detailed depiction of our proposed model, \ours, which includes four primary modules: the \textbf{Processor $Pr_i$} for information propagation; the \textbf{Encoder} for data transformation, the \textbf{GUnet} for graph pooling, and the \textbf{Decoder} for downstream tasks. (b) \& (c) Mapping functions for GUNet stages and GNBs for the case where pre-trained model has more stages and per-processor GNBs than the fine-tuned model.}
    \vspace{-0.2cm}
\end{figure}


\subsection{Problem Statement}
\label{problem_statement}
Given the physical state of a system, our task is to predict the subsequent state over a time interval and under specified boundary conditions. The system's current state at time $t$ is described by a discretized mesh $M^{t}$ and can be represented in 2D or 3D space. The mesh data $M^{t}$ is comprised of world coordinates, element connectivity, and physical parameters (stress/strain state and material properties). We use one-step prediction to find the subsequent mesh state $M^{t+1}$, but for sake of notational convenience, we will omit the superscript $t$, which indicates the timestamp in the subsequent expressions.

To facilitate the learning process, we transform the original mesh data $M$ into a heterogeneous graph $G^{hetero} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ denotes the set of nodes and $\mathcal{E}$ represents the set of edges. The node set $\mathcal{V}$ comprises two types of node: mesh nodes $\mathcal{V}^{M}$ and element nodes $\mathcal{V}^{E}$. The mesh vertices in $M$ are converted to graph nodes in $\mathcal{V}^{M}$, while the mesh faces are represented as nodes in $\mathcal{V}^{E}$. This heterogeneous graph structure is necessary to describe physical state variables like stress in a single-valued way when multiple materials are present in a system, as is typical for many physical simulations. The edge set $\mathcal{E}$ includes three groups of edges: (1) bidirectional edges $\mathcal{E}^{M,M}$ between adjacent mesh vertices $\mathcal{V}^{M}$, (2) bidirectional edges $\mathcal{E}^{E,E}$ between adjacent faces $\mathcal{V}^{E}$, and (3) directional edges $\mathcal{E}^{E,M}$ and $\mathcal{E}^{M,E}$ which connect each mesh face to its vertices. Figure~\ref{hetero_graph} provides a demonstration of mesh data derived from the Deformable Plate dataset and its corresponding heterogeneous graph.

Each type of node and edge has its own feature matrix. For example, the feature matrix of mesh nodes $\mathcal{V}^{M}$ is $\mathbf{X}^{M} \in \mathbb{R}^{|\mathcal{V}^{M}|\times h_{0}^{M}}$, where the feature vector of node $i \in \mathcal{V}^{M}$ is the $i$-th row vector of $\mathbf{X}^{M}$. We explain the composition of feature matrices in Appendix~\ref{info:hetero_graph}.

\subsection{Scalable Graph U-Net}
% State the target for the design first: To enable the transfer learning for trained graph simulators of different mesh size problems. Graph U-Net as backbone, changeable ratio dfs-pooling, modular design(?)

Our model comprises four main modules, as illustrated in Figure~\ref{Model_detail}. We adopt an Encoder-Process-Decoder~\cite{GNS} style model with several key extensions: (1) the framework is extended to handle the heterogeneous graph structure with multiple node and edge types, and (2) we add a staged U-Net with variable DFS pooling and unpooling that greatly expands the model's receptive field. In the following paragraphs, we will detail these networks and modules.

\textbf{Encoder:} The Encoder comprises a set of MLP models and three Processors. The MLP models map raw node and edge features of varying sizes into a unified latent space. The three Processors manage the message passing flows $\mathcal{V}^M \rightarrow \mathcal{V}^M$, $\mathcal{V}^E \rightarrow \mathcal{V}^E$, and $\mathcal{V}^M \rightarrow \mathcal{V}^E$, respectively. Consequently, the Encoder transforms the heterogeneous graph $\mathcal{G}^{\text{hetero}}$ into a homogeneous graph $\mathcal{G}^{E}$ by aggregating the mesh node representations into their neighboring element nodes.

\textbf{Processor:} The Processor (Pr) consists of $m$ identical Graph-Net blocks (GNBs), with the output of each block serving as the input to the subsequent block in the sequence. This sequential processing enables the model to gather and integrate information from nodes located up to $m$ hops away from the central node.

Each Graph-Net block operates as an independent message passing unit, with no shared parameters between blocks. We extend the concept of Graph-Net blocks from~\cite{GNB, meshgraphnets} to accommodate graphs with various types of nodes and edges. Specifically, for message passing flow from $\mathcal{V}^{\text{src}}$ to $\mathcal{V}^{\text{tgt}}$, where $\text{src}$ and $\text{tgt}$ $\in \{E, M\}$, the feature of the edge connecting nodes $i$ and $j$ is updated via 
$$\mathbf{X}^{\text{src},\text{tgt}}_{i,j}=f^{\text{src},\text{tgt}}\left(\mathbf{X}^{\text{src}}_i | \mathbf{X}^{\text{tgt}}_j | \mathbf{X}^{\text{src},\text{tgt}}_{i, j}\right) ,$$ 
where $f^{\text{src},\text{tgt}}$ is an MLP model with residual connections, and $|$ represents the concatenation operation.
After updating the edge features, the feature of the end-point node $j$ is updated via 
$$\mathbf{X}^{\text{tgt}}_j=f^{\text{tgt}}\left(\mathbf{X}^{\text{tgt}}_j | \sum_{i \in \mathcal{N}^{\text{src}}_j}\mathbf{X}^{\text{src},\text{tgt}}_{i, j}\right) ,$$
where $f^{\text{tgt}}$ is an MLP model with residual connections, and $\mathcal{N}^{\text{src}}_j$ are the neighborhoods of node $j$ of type $\text{src}$.

\textbf{GUnet Stage:} The GUnet (GU) is composed of $L$ stages, with each stage containing one pooling layer and two Processors. During the down-sampling phase, a Processor (Pr$_{i}^{E}$) and a down-sample operator (Down$_i$) are sequentially applied at stage $i$. This process allows graphs of varying sizes to undergo information propagation and aggregation. During the up-sampling phase, the procedure is reversed: the graph is first restored to its original size by an up-sampling operator (Up$_i$) before being processed by a Processor (Pr$_{i}^{D}$) at stage $i$. Unlike traditional graph network models such as those in \cite{meshgraphnets, GraphNetworkConstraint, GraphNetworkDiscontinuous}, which require many message passing steps to capture long-range dependencies, the GUnet is able to process long-range data with significantly fewer message passing steps. This can also help alleviate the difficulty of scaling message passing networks to a large number of nodes as the number of message passing steps needed would become intractable for GPU memory.

% for our pooling design, distinguish with others: changeable pooling ratio + pooling based on node proximity (compared with topk pooling, and bi-stride pooling).
We design a Depth First Search style pooling (DFS-pooling) which has two key advantages that distinguishes it from the existing work~\cite{Graph-Unets}: (1) changeable pooling ratios for different pooling stages and (2) pooling based on node proximity. We first select an un-visited node from the element node set as the starting node and initiate a DFS-style random walk to explore adjacent nodes of the same material. During this walk, nodes are clustered according to the pooling ratio, with nodes in each cluster pooled into a single node. We use an even-pooling scheme of averaging weights to update the node and edge features within each cluster. This process continues until all nodes in $\mathcal{V}^{E}$ have been visited. The pseudo-code for this clustering computation is provided in Algorithm~\ref{pooling_dfs}. Additionally, we provide an example of down-sampled graphs in Figure~\ref{sample_graph} with pooling ratios as 3 and 2, respectively. Here, nodes of different colors have different types of material. Note that this computation needs to be performed only once during the preprocessing stage, once the pooling ratio is determined.

\textbf{Decoder:} In line with \MGN~\cite{meshgraphnets}, we use an MLP model to project the latent features of the element nodes $\mathbf{X}^E$ into the output space. Additionally, we perform a 1-hop message passing operation from $\mathcal{V}^E$ to $\mathcal{V}^M$ to interpolate the features of the element nodes to those of mesh nodes. Finally, another MLP model decodes the interpolated latent features of the mesh nodes into the desired output space.


\subsection{Utilizing the pre-trained model}
% 2 parts:
% 0) Introduce the transfer learning challenge at first. Correlate to the receptive field definition?
% 1) Weights Alignment
% 2) Weights constraints

% this should be in related work
%The pre-training and fine-tuning paradigm has achieved significant success in natural language processing (NLP) and computer vision (CV)~\cite{kolesnikov2020big, hu2021lora}. 

How exactly the knowledge from a pre-trained model is instilled into a target task-specific model, particularly when their architectures are mismatched, is a non-trivial task. As stated in Section~\ref{related_work}, one of the widely-used approaches in transfer learning operates at the parameter level. We also adopt the parameter sharing and parameter restriction strategies and discuss these in more detail here. To our knowledge, this is the first time transfer learning has been adapted and applied to GNNs predicting physics simulations.

\subsubsection{Parameter Sharing}
The parameter sharing strategy involves initializing the downstream task-specific model with a pre-trained model. Typically, the pre-trained model is either the same as or more complex than the task-specific model due to resource constraints~\cite{xu2023initializing}. However, this is not the case for our mesh-based graph network model. This distinction arises because the number of stages and message passing steps in the GUnet are closely aligned with the data size and simulation settings. As a result, conventional weight initialization methods are not directly applicable. Therefore, we design alternative strategies to effectively transfer learned knowledge from the pre-trained model to the downstream model within our unique framework. 

We propose a scaling method at two levels — Processor and GUnet — to align the sizes of the pre-trained and fine-tuned models. We choose between two mapping functions for both the Processors and GUnet: Uniform and First-N. The details of these mapping functions are described below. 
For the Processors, we employ the mapping function on GNBs, which ensures that the alignment reflects the stages of message propagation through the blocks. For the GUnet, we employ the mapping function on the GUNet stages. 
In general, the mapping function chosen for the Processors and GUnet need not be the same, but for our present work we used the same mapping (First-N or Uniform) for both Processors and GUnet for a given experiment. 

Note that the Encoder and the Decoder do not contain generalizable knowledge as they are tailored to specific tasks. In our implementation, we randomly initialize the parameters of the Encoder and Decoders during fine-tuning.

\textbf{1) Uniform Mapping:} The first method of parameter sharing between pre-trained and fine-tuned models is the Uniform method. The goal of this mapping method is to achieve uniform division and alignment of weights. To align the weights of two Processors, we consider cases where the number of GNBs in the pre-trained model $m_{\text{pt}}$ is less than, greater than, or equal to that in the fine-tuned model $m_{\text{ft}}$. Formally, let $g_1(\mathbf{Pr}_{\text{pt}})$ be a function that maps a Processor $\mathbf{Pr}_{\text{pt}}$ in the pre-trained model to a Processor $\mathbf{Pr}_{\text{ft}}$ in the fine-tuned model. This mapping function uses uniform division to achieve the alignment:
\[ 
\scalebox{0.9}{$
    \mathbf{Pr}_{\text{ft}}^i = g_1^{\text{uni}}(i; \mathbf{Pr}_{\text{pt}}) =
    \begin{cases}
    \mathbf{Pr}_{\text{pt}}^{\lfloor i / \text{upN} \rfloor,}, & \text{if } m_{\text{pt}} < m_{\text{ft}} \\
    \text{MEAN}_{j=st(i)}^{st(i)+d(i)} \{\mathbf{Pr}_{\text{pt}}^j \}, & \text{if } m_{\text{pt}} > m_{\text{ft}} \\
    \mathbf{Pr}_{\text{pt}}^i, & \text{if } m_{\text{pt}} == m_{\text{ft}},
    \end{cases}
$}
\]
where,
$$
st(i)=
\begin{cases}
     (\text{dwN}+1) \times i,  & \text{if } i < \text{r}, \\ 
     \text{dwN} \times i + \text{r}, &  \text{if } i \geq \text{r},
\end{cases}
d(i)=
\begin{cases}
    \text{dwN} + 1,  & \text{if } i < \text{r}, \\ 
    \text{dwN} , &  \text{if } i \geq \text{r}
\end{cases},
$$
$$\text{upN}=\lceil m_{\text{ft}} / m_{\text{pt}}\rceil ,\;\;\; \text{dwN}=\lfloor m_{\text{pt}}/ m_{\text{ft}} \rfloor ,\;\;\; \text{r}=m_{\text{pt}} \bmod m_{\text{ft}} .$$ 

$\mathbf{Pr}^i$ represents the $i$-th Graph-Net block in the Processor, and $\text{MEAN}\{\cdot\}$ represents the averaging operation.
We can define the mapping function for the GUnet $g_2(\mathbf{GU}{\text{pt}})$ in a similar fashion. We leave the detailed discussion to Appendix~\ref{alignment:gunet}. An example of the Uniform mapping for both Processor and GUnet is shown in Figure~\ref{mapping_Uniform}.

\textbf{2) First-N Mapping:} The second method for parameter sharing between pre-trained and fine-tuned models is the First-N method. For Processor, the goal of this mapping method is to selectively share weights for the first set of Graph-Net blocks that are common between the pre-trained and fine-tuned models. Formally, 
\[ 
\scalebox{0.9}{$
    \mathbf{Pr}_{\text{ft}}^i = g_1^{\text{First-N}}(i; \mathbf{Pr}_{\text{pt}}) =
    \begin{cases}
     \mathbf{Pr}_{\text{pt}}^i, & \text{if } i \leq  m_{\text{pt}} \\
     \text{Randomly Initialized}, & \text{if } i >  m_{\text{pt}}. \\
    \end{cases}
$}
\]
We can also define the First-N mapping from a pre-trained GUnet $\mathbf{GU}_{\text{pt}}$ to a fine-tuned GUnet $\mathbf{GU}_{\text{ft}}$. An example of the First-N mapping of both pooling and message passing is shown in Figure~\ref{mapping_firstN}.

% \textbf{1) Scaling Processor:} Recall that each Processor consists of $m$ stacked Graph-Net blocks. To align a Processor with $m_{\text{pt}}$ blocks in the pre-trained model with a Processor of size $m_{\text{ft}}$ in the fine-tuned model, we construct a mapping function to correlate the Graph-Net blocks in the two models. Due to the isotropic nature of Graph-Net blocks, there is flexibility and variability in how this mapping can be done. In our design, we base the mapping function on the message passing stage, which ensures that the alignment reflects the stages of message propagation through the blocks. Mathematically, let $g_1(\mathbf{Pr}_{\text{pt}})$ be a function that maps a Processor $\mathbf{Pr}_{\text{pt}}$ in the pre-trained model to a Processor $\mathbf{Pr}_{\text{ft}}$ in the fine-tuned model. This mapping function utilizes Uniform division to achieve the alignment:
% \[ 
% \scalebox{0.9}{$
%     \mathbf{Pr}_{\text{ft}}^i = g_1^{\text{uni}}(i; \mathbf{Pr}_{\text{pt}}) =
%     \begin{cases}
%     \mathbf{Pr}_{\text{pt}}^{\lfloor i / \text{upN} \rfloor,}, & \text{if } m_{\text{pt}} < m_{\text{ft}} \\
%     \text{MEAN}_{j=st(i)}^{st(i)+d(i)} \{\mathbf{Pr}_{\text{pt}}^j \}, & \text{if } m_{\text{pt}} > m_{\text{ft}} \\
%     \mathbf{Pr}_{\text{pt}}^i, & \text{if } m_{\text{pt}} == m_{\text{ft}}.
%     \end{cases}
% $}
% \]
% We have also implemented a mapping function adopting the widely-used First-N strategy:
% \[ 
% \scalebox{0.9}{$
%     \mathbf{Pr}_{\text{ft}}^i = g_1^{\text{First-N}}(i; \mathbf{Pr}_{\text{pt}}) =
%     \begin{cases}
%      \mathbf{Pr}_{\text{pt}}^i, & \text{if } m_{\text{pt}} < m_{\text{ft}} \wedge i \leq  m_{\text{pt}} \\
%      \text{Randomly Initialized}, & \text{if } m_{\text{pt}} < m_{\text{ft}} \wedge i >  m_{\text{pt}} \\
%     \mathbf{Pr}_{\text{pt}}^i, & \text{if } m_{\text{pt}} \geq m_{\text{ft}}.
%     \end{cases}
% $}
% \]


% \textbf{2) Scaling GUnet:} In a GUnet, the DownSample and UpSample modules each comprise $L$ stages. To align a GUnet with $L_{\text{pt}}$ stages in the pre-trained model with a GUnet having $L_{\text{ft}}$ stages in the fine-tuned model, it is essential to construct a mapping function that connects the stages of the two models. We also employ a Uniform division strategy. Specifically, let $g_2(\mathbf{GU}{\text{pt}})$ denote the function that maps a GUnet $\mathbf{GU}{\text{pt}}$ in the pre-trained model to a GUnet $\mathbf{GU}_{\text{ft}}$ in the fine-tuned model. This function facilitates the alignment by distributing the stages Uniformly:
% \[
% \scalebox{0.9}{$
%     \mathbf{GU}_{\text{ft}}^i = g_2^{\text{uni}}(i; \mathbf{GU}_{\text{pt}})=
%     \begin{cases}
%     g_1(\mathbf{GU}_{\text{pt}}^{\lfloor i / \text{upN} \rfloor}), & \text{if } L_{\text{pt}} < L_{\text{ft}} \\
%     \text{MEAN}_{j=st(i)}^{st(i)+d(i)} \{g_1(\mathbf{GU}_{\text{pt}}^j) \}, & \text{if } L_{\text{pt}} > L_{\text{ft}} \\
%      g_1(\mathbf{GU}_{\text{pt}}^i), & \text{if } L_{\text{pt}} == L_{\text{ft}}.
%     \end{cases}
% $}
% \]
% In this approach, the calculations for $\text{upN}$, $\text{dwN}$, $st(i)$, and $ed(i)$ follow a similar procedure to those used in scaling the Processor, with $L_{*}$ replacing $m_{*}$. Here, $\mathbf{GU}^i$ denotes the Processor at the $i$-th layer of the GUnet. Prior to this alignment, the Processor is scaled using the function $g_1$ to ensure consistency at the GUnet level. We have also developed a mapping function that utilizes the First-N strategy. 

Regardless of the mapping methods, the stages from the GUnet must first be mapped from the pre-trained model to the fine-tuned model, then within each GUNet stage the GNBs for the Processor must be mapped to the fine-tuned model as well. In this way there is a hierarchical approach to the shared parameters.

In summary, the key difference between the Uniform and First-N mapping functions lies in the extent to which parameters from the GUnet modules are used by the fine-tuned model. The Uniform strategy may allow for more comprehensive parameter use, whereas the First-N approach might be more selective. The effectiveness of these mapping functions will be assessed in Section~\ref{sec:exp}. 

\subsubsection{Parameter Restriction}
Beyond parameter sharing, we implement a parameter restriction technique to enhance the generalization capabilities of the downstream models. Following Gouk et al.~\cite{gouk2020distance}, we calculate the Frobenius distance between the pre-trained and fine-tuned model weights to apply a regularization term that penalizes discrepancies between them. Let $\mathbf{W}_{\text{pt}}$ denote the weights of the pre-trained model, and $\mathbf{W}_{\text{ft}}$ represent the weights of the fine-tuned model. Then the Frobenius norm of the difference between these two sets of weights can be expressed as:
\[
\|\mathbf{W}_{\text{pt}} - \mathbf{W}_{\text{ft}}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \left( \mathbf{W}_{\text{pt}}^{(i,j)} - \mathbf{W}_{\text{ft}}^{(i,j)} \right)^2} .
\]
To incorporate Frobenius distance into the training process, the regularization term is added to the loss function. The regularized loss function $\mathcal{L}_{reg}$ can be written as: 
\[
L_{\text{reg}} = L_{\text{task}} + \lambda \|\mathbf{W}_{\text{pt}} - \mathbf{W}_{\text{ft}}\|_F^2 ,
\]
where $\lambda$ is a hyperparameter that controls the strength of the regularization term, and $\mathcal{L}_{task}$ represents the original task-related loss.

%------------------ Section 4 Experiments ------------------%

\section{Experiments}
\label{sec:exp}

We present an evaluation of our proposed pre-training and fine-tuning framework for mesh-based simulations. We begin by detailing the datasets used in our experiments, highlighting both the generalized dataset constructed for pre-training and the benchmark datasets employed for fine-tuning. Following this, we introduce the baseline models against which our approach is compared. We then present the results of our pre-training phase and evaluate the transfer learning performance.

\subsection{Datasets}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{pictures/fea/FEA_flow.pdf}
    \vspace{-0.2cm}
    \caption{Randomized FEA simulation dataset using geometry from ABC dataset.}
    \vspace{-0.2cm}
    \label{fea_flow}
\end{figure}

\textbf{1) For pre-training:} Since there is currently no existing work on pre-training for mesh-based physical simulations, and popular benchmark datasets contain at best a few thousands training samples, we constructed a larger and more-generalized pre-training dataset. The goal of this dataset was to have a wide variety of geometric shapes that are deformed after coming into contact with each other. We used the ABC dataset~\cite{koch2019abc}, which is a CAD model dataset used for geometric deep learning, to get a wide sample of parts and shapes to deform. To generate a simulation in our pre-training dataset, we first randomly select two CAD geometries, then auto-mesh them with the meshing tool Shabaka~\cite{hafez2023robust}. We then align the two meshed parts in 3D space and apply compressive boundary conditions to simulate the parts coming into contact. Figure~\ref{fea_flow} illustrates the workflow of the pre-training dataset construction process. 

In total, we generated a pre-training dataset of 20,000 simulations by drawing pairs of geometries from a set of 400 geometry samples. Figure~\ref{fea_examples} shows several example simulations and the modes of deformation achieved through contact. Here we can see examples of mechanical contact and stress around a hole.

% TODO: figure 3 showing the different modes, do we need a more detailed explaination for each mode showed in the figure?
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{pictures/fea/abcDataset_fea_image.pdf}
    \vspace{-0.2cm}
    \caption{The FEA simulation results using ABC CAD dataset highlight various deformation modes, including compression with associated tension around a hole, as well as plate and beam bending.}
    \vspace{-0.2cm}
    \label{fea_examples}
\end{figure}

% Consider moving the MSE definition to Appendix. Reader know the common definition for MSE, though we have minor mod, but not making too much difference for results.
\textbf{2) For Transfer Learning:} We selected two representative datasets for quasi-static mechanical simulations as benchmarks to evaluate model performance on downstream tasks: 2D Deformable Plate~\cite{GGNS} and 3D Deforming Plate~\cite{meshgraphnets}. These downstream task datasets represent a subspace of simulations relative to our generalized pre-training dataset, and thought to be good candidates to evaluate our fine-tuning framework.
For more detailed information about the datasets, please refer to Table~\ref{appendix:table:data-detail}. 

\subsection{Baseline and Metric}

To demonstrate the generalization and effectiveness of our pre-training and fine-tuning paradigm, we used \MGN~\cite{meshgraphnets}, a state-of-the-art model in the field of physics simulation, as the baseline for comparison. The model configurations are shown in Table~\ref{model_hyper} and more explanations can be found in Appendix~\ref{appendix:model_config}. We used the RMSE loss on the positions of mesh nodes $\mathcal{V}^{M}$ as the metric to evaluate model performance. We show the calculation of receptive field size in Appendix~\ref{calc_recep}.

\begin{table*}[htbp]
    \renewcommand\arraystretch{1.2}
    \caption{Hyperparameter configurations for the models.}
    \label{model_hyper}
    \centering
    \vspace{-0.2cm}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{clccccccccc}
        \toprule
        \textbf{Model} & \textbf{Dataset} & \multicolumn{3}{c}{\makecell[c]{\textbf{\# Message Passing} \\ (Encoder)}} & \multicolumn{3}{c}{\makecell[c]{\textbf{\# Message Passing} \\ (Processor)}} & \makecell[c]{\textbf{Mesh Receptive} \\ \textbf{Field Size}} & \textbf{\# Parameters} & \textbf{Batch Size} \\
        \hline
        \multirow{3}{*}{\MGN} & \PTData & \multicolumn{3}{c}{5} & \multicolumn{3}{c}{13} & 18 & 1053766 & 2 \\ % 4.22MiB
        \cline{2-11}
        & Deforming Plate & \multicolumn{3}{c}{2} & \multicolumn{3}{c}{15} & 17 & 891462 & 8 \\ % 3.57MiB
        \cline{2-11}
        & Deformable Plate & \multicolumn{3}{c}{2} & \multicolumn{3}{c}{6} & 8 & 553156 & 16 \\ % 2.21MiB
        \midrule
        \textbf{Model} & \textbf{Dataset} & \multicolumn{2}{c}{\textbf{Pooling Ratio}} & \multicolumn{2}{c}{\makecell[c]{\textbf{\# Message Passing} \\ (Encoder)}} & \multicolumn{2}{c}{\makecell[c]{\textbf{\# Message Passing} \\ (GUnet)}} & \makecell[c]{\textbf{Mesh Receptive} \\ \textbf{Field Size}} & \textbf{\# Parameters} & \textbf{Batch Size} \\
        \hline
        \multirow{3}{*}{\ours} & \PTData & \multicolumn{2}{c}{[4, 2, 2]} & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{1} & 35 & 719686 & 4 \\ % 2.88MiB
        \cline{2-11}
        & Deforming Plate & \multicolumn{2}{c}{[4, 2]} & \multicolumn{2}{c}{4} & \multicolumn{2}{c}{2} & 29 & 894918 & 16 \\ % 3.58MiB
        \cline{2-11}
        & Deformable Plate & \multicolumn{2}{c}{[2]} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{2} & 9 & 569540 & 16 \\ % 2.28MiB
        \bottomrule
    \end{tabular}}
\end{table*}

\subsection{pre-training Results}

We trained both the \MGN and \ours models on \PTData for 1 million training steps. The RMSE losses for \MGN on the training and validation datasets are 8.3205$\times 10^{-4}$ and 5.8018$\times 10^{-4}$, respectively. In comparison, \ours achieves RMSE losses of 4.2041$\times 10^{-4}$ on the training set and 4.2657$\times 10^{-4}$ on the validation set. These results demonstrate that \ours outperforms \MGN, reducing the training loss by nearly 50\%. Moreover, the validation loss shows that \ours generalizes better to unseen data while converging more effectively during training.

\subsection{Transfer Learning Performance}

To evaluate the effectiveness of our pre-training and fine-tuning framework, we performed experiments using the \PTData dataset for pre-training. Both the \MGN and \ours models are trained for a defined number of epochs. Subsequently, we fine-tuned these models on downstream tasks. 
For the Deformable Plate dataset, the models were fine-tuned for 20k steps. For the Deforming Plate dataset, the models were fine-tuned for 500k steps. We applied two parameter sharing strategies — Uniform and First-N — when loading the checkpoint of the pre-trained model. Additionally, we reduced the size of the training dataset to investigate whether our framework can decrease reliance on large volume of data. 
During this process, we recorded the minimum validation loss and saved the corresponding model checkpoint, which was later used to assess performance on the test dataset. All experiments are repeated 5 times with different random seeds.

\textbf{Deformable Plate:} We reduced the training set to $\frac{1}{8}$, and $\frac{1}{16}$ of its original size. 
The animations in Figure~\ref{dp2D_anim} provide an intuitive qualitative assessment. This figure presents an example from the test dataset. From these visualizations, we can observe a clear improvement in the handling of deformations at the contact area between the ball and the plate after fine-tuning. Specifically, the ball and plate no longer overlap, the plate’s deformation curve conforms more closely to the ball’s contour, and the deformation in areas farther from the contact point aligns more accurately with the ground truth.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP2D_84_all.pdf}
    \vspace{-0.2cm}
    \caption{Simulated meshes at various stages ($t$=30 at the top row, $t$=50 at the bottom row) for different models: \MGN, \MGN-FT (fine-tuned with Uniform and First-N strategies), \ours, \ours-FT (fine-tuned with Uniform and First-N strategies), and the ground truth. All models are trained on \textbf{1/8} of the original training size. The colors indicate displacement magnitude.}
    \label{dp2D_anim}
    \vspace{-0.2cm}
\end{figure}

Figures~\ref{valid_dp2D} and ~\ref{DP2D} compare the roll-out validation RMSE for different models across three data scales, offering insights from various perspectives. The results demonstrate that \ours consistently outperforms \MGN across all data scales, with lower RMSE values and faster convergence, particularly when fine-tuned. These observations are corroborated by the test dataset performance shown in Table~\ref{test_dp2D}. Notably, the RMSE of \ours fine-tuned with the Uniform strategy on \(\frac{1}{16}\) of the training data is comparable to that of the model fine-tuned on the full dataset, achieving an 11.05\% improvement compared to the model trained from scratch.

These results reveal that fine-tuning with the Uniform strategy further reduces the RMSE compared to the First-N strategy, demonstrating the effectiveness of this approach. Notably, \ours-FT with the Uniform strategy achieves the lowest RMSE even with significantly reduced datasets, highlighting the model’s robustness and efficiency in generalizing from limited data. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP2D_valid_loss_combined.pdf}
    \vspace{-0.2cm}
    \caption{The best validation loss of the two models on the Deformable Plate dataset when trained from scratch and when fine-tuned.}
    \vspace{-0.2cm}
    \label{valid_dp2D}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP2D_RI_two_models.pdf}
        \caption{}
        \label{dp2D_ri}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP2D_RIvsFT_MP.pdf}
        \caption{}
        \label{dp2D_rivstl_mp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP2D_RIvsFT_GUNv3.pdf}
        \caption{}
        \label{dp2D_rivstl_ours}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{Comparison of the best validation loss tendencies on the Deformable Plate. (a) Two models both trained from scratch. (b) \MGN trained from scratch and fine-tuned with the Uniform strategy. (c) \ours trained from scratch and fine-tuned with the Uniform strategy.}
    \vspace{-0.2cm}
    \label{DP2D}
\end{figure}

\textbf{Deforming Plate:} We reduced the training set to $\frac{1}{4}$ and $\frac{1}{8}$ of its original size. 
The images in Figure~\ref{dp_anim} offer a qualitative assessment by showing an example from the test dataset. The visualizations reveal that \MGN performs poorly when trained from scratch as the displacement of the plate was concentrated tightly around the ball. Although fine-tuning improved \MGN's predictions, the displacement area it predicts is still much smaller than the ground truth. In contrast, \ours, especially \ours-FT, delivered much more accurate predictions. This accuracy could be attributed to the larger receptive field size of \ours compared to \MGN.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP_150_all.pdf}
    \vspace{-0.2cm}
    \caption{Simulated meshes at various stages ($t$=200 at the top row, $t$=300 at the bottom row) for different models: \MGN, \MGN-FT (fine-tuned with Uniform strategy), \ours, \ours-FT (fine-tuned with Uniform strategy), and the ground truth. All models are trained on \textbf{1/8} of the original training size. The colors indicate displacement magnitude.}
    \vspace{-0.2cm}
    \label{dp_anim}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP_valid_loss_combined.pdf}
    \vspace{-0.2cm}
    \caption{The best validation loss of the two models on the Deforming Plate dataset when trained from scratch and when fine-tuned.}
    \vspace{-0.2cm}
    \label{valid_dp}
\end{figure}


Figures~\ref{valid_dp} and ~\ref{DP} compare the roll-out validation RMSE, while Table~\ref{test_dp} presents the performance of different models on the test dataset. The results reveal that — consistent with the findings on Deformable Plate — 
(1) Fine-tuned models consistently outperform those trained from scratch across all dataset scales. Notably, reduction on training data does not lead to much worse performance, especially on the proposed SGUNET.
(2) Fine-tuned models also exhibit faster convergence speeds. For instance, as shown in Figure~\ref{dp_rivstl_ours}, \ours-FT on \(\frac{1}{8}\) of the training data reaches the RMSE value that its counterpart requires 500k steps to achieve, in just 200k steps.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP_RI_two_models.pdf}
        \caption{}
        \label{dp_ri}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP_RIvsFT_MP.pdf}
        \caption{}
        \label{dp_rivstl_mp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{pictures/exp/DP_RIvsFT_GUNv3.pdf}
        \caption{}
        \label{dp_rivstl_ours}
    \end{subfigure}
    \vspace{-0.2cm}
    \caption{Comparison of the best validation loss tendencies on the Deforming Plate. (a) Two models both trained from scratch. (b) \MGN trained from scratch and fine-tuned with the Uniform strategy. (c) \ours trained from scratch and fine-tuned with the Uniform strategy.}
    \vspace{-0.2cm}
    \label{DP}
\end{figure}

%------------------ Section 5 Conclusion ------------------%

\section{Discussion}

In this paper, we introduce a novel pre-training and fine-tuning framework tailored specifically for mesh-based physical simulations. Our approach uses a scalable graph U-net (\ours), which is defined in a modular and configurable manner to facilitate the parameter sharing process for transfer learning. We constructed a dataset for pre-training, i.e. \PTData, and utilized it to pre-train the models. Through extensive experiments, we demonstrate that not only does \ours outperform \MGN, a SOTA model in this field, but also both models achieve improvements in performance across various dataset scales when fine-tuned. Notably, the fine-tuned models reduce their dependence on the training data.

Despite the promising results, there are some limitations that warrant further exploration. First, we have evaluated our framework only in the context of quasi-static simulations. Future work could extend it to a broader range of physical systems to assess its versatility and effectiveness in more dynamic scenarios. Second, our current transfer learning methods, which includes two strategies for parameter sharing and one for parameter restriction, have proven effective, exploring alternative and more advanced transfer learning techniques could offer valuable opportunities for future research.

\clearpage

%----------------------------------------------------%
%----------------------------------------------------%

%------------------ Bibliograph ------------------%

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

%------------------ APPENDIX ------------------%
\newpage

\appendix

\section{Dataset Details}
Three quasi-static datasets, ABCD, Deforming Plate, Deformable Plate are used in our experiments. All datasets are simulated as hyper-elastic deformations with linear elements. For ABCD, it includes 20,000 trajectories for pre-training. For Deforming Plate, it contains 1200 trajectories in total, we split it into 1000/100/100 for training, validation and testing. For Deformable Plate, we use a split of 675/135/135 for training, validation and testing. 

In the experiments, we repeated the fine-tuning on Deformable Plate and Deforming Plate 5 times. Each time, we shuffled the data splits for training, validation and testing while maintaining the ratio.

\begin{table*}[htbp]
    \renewcommand\arraystretch{1.2}
    \vspace{-0.2cm}
    \caption{Basic statistics for datasets.}
    \label{appendix:table:data-detail}
    \centering
    \vspace{-0.2cm}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{cccccccccc}
        \toprule
        \multirow{2}{*}{\textbf{pre-training}} & \makecell[c]{\textbf{Dataset}} & \makecell[c]{\# $|\mathcal{V}^M|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{V}^E|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{M,M}|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{E,E}|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{E,M}|$ \\ (avg.)} & \# Steps & Mesh Type & Dimension \\
        \cline{2-10}
        & \textbf{\PTData} & 4445 & 12944 & 42919 & 57052 & 51777 & 20 & Tetrahedral & 3 \\
        \hline
         \multirow{3}{*}{\textbf{Transfer Learning}} & \makecell[c]{\textbf{Dataset}} & \makecell[c]{\# $|\mathcal{V}^M|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{V}^E|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{M,M}|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{E,E}|$ \\ (avg.)} & \makecell[c]{\# $|\mathcal{E}^{E,M}|$ \\ (avg.)} & \# Steps & Mesh Type & Dimension \\
         \cline{2-10}
         & \textbf{Deforming Plate} & 1270 & 4038 & 12718 & 15648 & 16154 & 400 & Tetrahedral & 3 \\
         \cline{2-10}
         & \textbf{Deformable Plate} & 138 & 183 & 648 & 515 & 549 & 50 & Triangular & 2 \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
\end{table*}

\section{Method Details}
\subsection{DFS-Pooling}
We provide the pseudocode for generating the cluster index vector, which is utilized for graph pooling in the GUnet module.
\begin{algorithm}
\SetAlgoNoLine
\caption{Get nodes clustering index}
\label{pooling_dfs}
\KwData{adjacency matrix $\mathbf{A}$, pooling ratio $p$, material index vector $\mathbf{m}$, number of nodes $n$}
\KwResult{cluster index vector $\mathbf{c}$}

\SetKwFunction{MyFunction}{DFS} 
\SetKwProg{Fn}{Function}{:}{\KwRet}

\Fn{\MyFunction{$nid, mat$}}{
    $\mathbf{c}.\mathrm{get}(nid) \leftarrow cid$\;
    $cnt \leftarrow cnt + 1$\;
    \If{$cnt \geq p$} {
    $cid \leftarrow cid + 1$\;
    $cnt \leftarrow 0$\;
    }
    \For{$nnid \in [0, 1, \dots, n-1]$}{
    \If{$nnid \in left \wedge \mathbf{m}.\mathrm{get}(nnid) == mat$}{
    $left.\mathrm{remove}(nnid)$\;
    \MyFunction{$nnid, mat$}\; 
    }
    }
}
Let $cid\leftarrow -1, cnt\leftarrow 0$\;
Let $left\leftarrow\mathrm{set}(0,1,\dots, n-1)$, $\mathbf{c}\leftarrow[\mathbf{-1}]_n$\;
\While{$left$ is not empty} {
$nid \leftarrow left.\mathrm{pop}(0)$\;
$mat \leftarrow \mathbf{m}.\mathrm{get}(nid)$\;
\If{$\mathbf{c}.\mathrm{count}(cid) > 0$} {
$cid \leftarrow cid + 1$\;
$cnt \leftarrow 0$\;
}
\MyFunction{$nid, mat$}\;
}
\Return $\mathbf{c}$
\end{algorithm}

\subsection{Alignment of GUnet}
\label{alignment:gunet}

A GUnet module comprise $L$ stages. To align a GUnet with $L_{\text{pt}}$ stages in the pre-trained model with a GUnet having $L_{\text{ft}}$ stages in the fine-tuned model, it is essential to construct a mapping function that build the connection between stages of the two models. Similar to the alignment of the Processor module, we design two mapping methods. Let $g_2(\mathbf{GU}{\text{pt}})$ denote the function that maps a GUnet $\mathbf{GU}{\text{pt}}$ in the pre-trained model to a GUnet $\mathbf{GU}_{\text{ft}}$ in the fine-tuned model.
For \textbf{Uniform Mapping}, 
\[
\scalebox{0.9}{$
    \mathbf{GU}_{\text{ft}}^i = g_2^{\text{uni}}(i; \mathbf{GU}_{\text{pt}})=
    \begin{cases}
    g_1(\mathbf{GU}_{\text{pt}}^{\lfloor i / \text{upN} \rfloor}), & \text{if } L_{\text{pt}} < L_{\text{ft}} \\
    \text{MEAN}_{j=st(i)}^{st(i)+d(i)} \{g_1(\mathbf{GU}_{\text{pt}}^j) \}, & \text{if } L_{\text{pt}} > L_{\text{ft}} \\
     g_1(\mathbf{GU}_{\text{pt}}^i), & \text{if } L_{\text{pt}} == L_{\text{ft}}.
    \end{cases}
$}
\]
For \textbf{First-N Mapping},
\[ 
\scalebox{0.9}{$
    \mathbf{GU}_{\text{ft}}^i = g_2^{\text{First-N}}(i; \mathbf{GU}_{\text{pt}}) =
    \begin{cases}
     g_1(\mathbf{GU}_{\text{pt}}^i), & \text{if } i \leq  L_{\text{pt}} \\
     \text{Randomly Initialized}, & \text{if } i >  L_{\text{pt}}.
    \end{cases}
$}
\]

The calculations for $\text{upN}$, $\text{dwN}$, $st(i)$, and $ed(i)$ are nearly the same as those in scaling the Processor. The only difference is to replace $m_{*}$ with $L_{*}$ . Here, $\mathbf{GU}^i$ denotes the Processor at the $i$-th layer of the GUnet. Prior to the alignment between GUnets, the Processor should be aligned up using the function $g_1$ to ensure consistency.

\subsection{How to Calculate Receptive Field Size} 
\label{calc_recep}
The receptive field size is defined as the maximum distance from which the central node can aggregate information from other nodes. Let the receptive field in the $i$-th stage be denoted by $r_i$, the pooling ratio by $p_i$, and the number of message passing steps in the GUnet's Processors by $m^{\text{GU}}$. Due to the presence of a Processor at the bottom of the GUnet, we have $r_{L} = m^{\text{GU}}$. Given $r_i$, the receptive field for the $(i-1)$-th stage can be calculated as $r_{i-1} = (r_i + 1) \cdot p_i - 1$. By applying this recursively, the receptive field of the central node in the first stage is determined as $r_{0} = (m^{\text{GU}} + 1) \cdot \left(\prod_{i=0}^{L-1} p_i + 1\right) - 2$. Prior to reaching the first stage of the GUnet, the graph has already undergone $m^{\text{Enc}}$ steps of information aggregation through the Processor in the Encoder. Therefore, the overall receptive field size is $r = m^{\text{Enc}} + (m^{\text{GU}} + 1) \cdot \left(\prod_{i=0}^{L-1} p_i + 1\right) - 2$.

\subsection{More Information about Heterogeneous Graph}
\label{info:hetero_graph}

As described in Section~\ref{problem_statement}, each node and edge type has a distinct feature matrix. Specifically, the feature at time $t$ is constructed as follows: 1) for $v_{i} \in \mathcal{V}^{M}$, the feature is given by $n_i \big|\big| (\mathbf{x}_{i}^{t}-\mathbf{x}_{i}^{0})$; 2) for $v_{i} \in \mathcal{V}^{E}$, it is represented as $\lambda_i \big|\big| \mu_i \big|\big| (\mathbf{x}_{i}^{t}-\mathbf{x}_{i}^{0})$; 3) for $e_{i,j} \in \mathcal{E}^{\text{src}, \text{tgt}}$, the feature is $\mathbf{x}_{ij}^{0} \big|\big| |\mathbf{x}_{ij}^{0}| \big|\big| \mathbf{x}_{ij}^{t} \big|\big| |\mathbf{x}_{ij}^{t}|$. Here, $n_i$ is a binary indicator (0 or 1) representing whether the mesh node $v_i$ is a normal or boundary node, $\lambda_i$ and $\mu_i$ are mechanical properties of the material, $\mathbf{x}_i^{t}$ denotes the world coordinates of node $i$ at time $t$, and $\mathbf{x}_{ij}^{t}$ refers to the relative world position between nodes $i$ and $j$ at time $t$. The operator $\big|\big|$ denotes concatenation, while $|\cdot|$ refers to the $L_2$ norm.

Following the approach in previous work~\cite{meshgraphnets}, we construct edges between the plate and the ball in the Deforming Plate and Deformable Plate datasets based on the distance between endpoints. Specifically, the distances used for \PTData, Deforming Plate, and Deformable Plate are 0.0003, 0.003, and 0.05, respectively. Unlike \MGN, which connects mesh nodes directly, we establish edges between element nodes.

Previous work transforms the mesh data from the Deforming Plate and Deformable Plate datasets into homogeneous graphs, where mesh vertices are represented as graph nodes. As a result, these datasets only capture the physical information of the mesh vertices and lack material properties. To address these limitations, we 1) use the average position of the mesh nodes to represent the position of the corresponding element node, and 2) set the material properties $\lambda$ and $\mu$ to zero.


\section{Experiment Details}
\subsection{Model Configuration}
\label{appendix:model_config}

\textbf{\MGN:} For the Deforming Plate and Deformable Plate datasets, we adhere to the settings outlined in the original paper~\cite{meshgraphnets, GGNS}. For the \PTData dataset, which involves larger-scale mesh sizes, more message passing steps are required. Since increased message passing steps lead to higher memory consumption and longer training times, we balance effectiveness and efficiency by setting the message passing steps in the Encoder and Processor to 5 and 13, respectively.

\textbf{\ours:} We configure our model based on two key objectives: 1) achieving a larger mesh receptive field size and 2) maintaining a model size that is comparable to or smaller than that of \MGN. We evaluate the model performance across several configurations and select the one that performs best.

\textbf{Noise Std.:} As we adopt the next-step prediction approach, adding noise to the input data is essential to enhance robustness during inference. We follow the noise settings specified in the original paper~\cite{meshgraphnets, GGNS} for the Deforming Plate and Deformable Plate datasets, which are 0.003 and 0.05, respectively. For the \PTData dataset, given that it involves fewer roll-out steps, we use a smaller noise standard deviation value of 0.0003.

\subsection{Loss}

The task-related objective across all datasets is unified as the mean squared error (MSE) loss of the normalized delta displacement between nodes over two steps. This can be expressed as:
\[
\scalebox{0.9}{$
\mathcal{L}_{task}=\frac{1}{|\mathcal{V}^{M}|+|\mathcal{V}^{E}|} \sum_{v \in \mathcal{V}^{M}\cup \mathcal{V}^{E}} \left \| \widetilde{\dot{\mathbf{x}_v}}^{\text{pred}} - \widetilde{\dot{\mathbf{x}_v}}^{\text{GT}} \right \|^2,
$}
\]
where $\widetilde{\dot{\mathbf{x}_v}}^{\text{pred}}$ and $\widetilde{\dot{\mathbf{x}_v}}^{\text{GT}}$ represent the normalized predicted and ground truth displacements of node $v$ over two steps, respectively.

\subsection{Additional Experimental Results}

We present the experimental results of \MGN and \ours on the Deformable Plate and Deforming Plate datasets in Table~\ref{test_dp2D} and Table~\ref{test_dp}, respectively. These results are obtained by loading the checkpoint from the best-performing step on the validation set, followed by inference on the corresponding test datasets. To facilitate comparison, the best result for each training data size is highlighted in bold, while the second-best result is underlined.

\begin{table}[htbp]
    \begin{minipage}[b]{0.48\linewidth}
        \renewcommand\arraystretch{1.1}
        \caption{The performance of the two models on the test dataset for Deformable Plate when trained from scratch and when fine-tuned.}
        \label{test_dp2D}
        \centering
        \vspace{-0.2cm}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{ccc|c|c}
            \toprule
            \textbf{Model} & \textbf{Method} & \textbf{All Data} & \textbf{$\frac{1}{8}$ Data} & \textbf{$\frac{1}{16}$ Data} \\
            \hline
            \multirow{3}{*}{\MGN} & From scratch & 0.062391$\pm$0.0106 & 0.064114$\pm$0.0046 & 0.070436$\pm$0.0065 \\
            & Fine-tuned (First-N) & \underline{0.056409$\pm$0.0052} & \underline{0.057404$\pm$0.0015} & \underline{0.058858$\pm$0.0052} \\
            & Fine-tuned (uni) & \textbf{0.054644$\pm$0.0029} & \textbf{0.055432$\pm$0.0032} & 0.060024$\pm$0.0030 \\
            \hline
            \multirow{3}{*}{\ours} & From scratch & 0.059615$\pm$0.0005 & 0.063806$\pm$0.0085 & 0.064714$\pm$0.0096 \\
            & Fine-tuned (First-N) & 0.057769$\pm$0.0058 & 0.059909$\pm$0.0062 & 0.061929$\pm$0.0060 \\
            & Fine-tuned (uni) & 0.056966$\pm$0.0061 & 0.057517$\pm$0.0044 & \textbf{0.057560$\pm$0.0034} \\
            \bottomrule
        \end{tabular}}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\linewidth} 
        \renewcommand\arraystretch{1.1}
        \caption{The performance of the two models on the Deforming Plate dataset when trained from scratch and when fine-tuned.}
        \label{test_dp}
        \centering
        \vspace{-0.2cm}
        \resizebox{\linewidth}{!}{
        \begin{tabular}{ccc|c|c}
            \toprule
            \textbf{Model} & \textbf{Method} & \textbf{All Data} & \textbf{$\frac{1}{4}$ Data} & \textbf{$\frac{1}{8}$ Data} \\
            \hline
            \multirow{3}{*}{\MGN} & From scratch & 0.007058$\pm$0.0009 & 0.007068$\pm$0.0006 & 0.007477$\pm$0.0008 \\
            & Fine-tuned (First-N) & \textbf{0.005903$\pm$0.0008} & 0.006977$\pm$0.0008 & 0.006350$\pm$0.0005 \\
            & Fine-tuned (uni) & 0.006363$\pm$0.0006 & 0.006523$\pm$0.0007 & 0.006535$\pm$0.0009 \\
            \hline
            \multirow{3}{*}{\ours} & From scratch & 0.006402$\pm$0.0008 & 0.006585$\pm$0.0007 & 0.007045$\pm$0.0008 \\
            & Fine-tuned (First-N) & \underline{0.006071$\pm$0.0002} & \textbf{0.005993$\pm$0.0003} & \textbf{0.006006$\pm$0.0004} \\
            & Fine-tuned (uni) & 0.006173$\pm$0.0005 & \underline{0.006140$\pm$0.0005} & \underline{0.006272$\pm$0.0006} \\
            \bottomrule
        \end{tabular}}     
    \end{minipage}
\end{table}

Figure~\ref{dp2D_84_all} and Figure~\ref{dp_1200_all} provide supplementary animations, where the models are trained on the full training dataset. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP2D_675_all.pdf}
    \vspace{-0.2cm}
    \caption{Simulated meshes at various stages ($t$=30 at the top row, $t$=50 at the bottom row) for different models. All models are trained on the \textbf{full} training dataset. The colors indicate displacement magnitude.}
    \vspace{-0.2cm}
    \label{dp2D_84_all}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{pictures/exp/DP_1200_all.pdf}
    \vspace{-0.2cm}
    \caption{Simulated meshes at various stages ($t$=200 at the top row, $t$=300 at the bottom row) for different models. All models are trained on the \textbf{full} training dataset. The colors indicate displacement magnitude.}
    \label{dp_1200_all}
    \vspace{-0.2cm}
\end{figure}

\end{document}
