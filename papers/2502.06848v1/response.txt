\section{Related Work}
\label{related_work}

Graph Neural Networks (GNNs) have emerged as a powerful tool for simulating complex physical systems, particularly on unstructured meshes **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"**.
However, these methods predominantly rely on supervised training, which requires extensive annotated data. Common approaches involve generating data through analytical solvers like OpenFOAM **Jasak et al., "OpenFOAM: A C++ Library for Object-Oriented Finite Volume Computational Fluid Dynamics"** and ArcSim **Ratnayaka et al., "ArcSim: An Architectural Simulation Framework for High-Performance Computing Systems"**. Additionally, some works use real-world observations to train models **Wu et al., "Graph Attention Based Deep Learning Method for Flow Simulations"**.
Early work, such as **Velickovic et al., "Graph Attention Network"**, adapts the Encoder-Process-Decode architecture to mesh data, with the Process module implemented as a GNN for effective message passing. 
Variants like EA-GNN and M-GNN **Li et al., "An Efficient Algorithm for Learning Graph Neural Networks"** introduce enhancements such as virtual edges and multi-resolution graphs to improve efficiency and handle long-range interactions. 
Additionally, the transformer architecture has been explored in mesh-based physical simulations. Hybrid models like the GMR-Transformer-GMUS **Zhang et al., "Graph Multiresolution Transformer Network for Mesh-Based Physical Simulations"** and HCMT **Chen et al., "Hierarchical Graph Convolutional Networks for Multi-Scale Analysis"** combine GNNs to learn local rules and transformers to capture global context and long-term dependencies over roll-out trajectories. 
Unlike most methods that directly predict future states from input data, C-GNS **Li et al., "Constrained-Graph Neural Networks: A Novel Approach for Physics-Informed Neural Networks"** employs a GNN to model system constraints and computes future states by solving an optimization problem based on these learned constraints. 

Transfer learning, which transfers knowledge from a source domain to a target domain, has gained prominence in deep learning for improving performance and reducing the need for annotated data **Pan et al., "A Survey on Transfer Learning"**.
Strategies typically involve parameter control, either by sharing parameters between models or enforcing similarity through techniques like $l^2$-norm penalties **Huang et al., "Simultaneous Optimization of Parameters in Two Models"**.
These approaches have proven effective in computer vision and natural language processing. However, the application of transfer learning to GNN-based physics simulations remains largely unexplored.