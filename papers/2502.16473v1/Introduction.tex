\vspace{-2mm}
\section{Introduction}
\label{sec:intro}
Large Language Models (LLMs) have revolutionized AI with breakthrough capabilities. While state-of-the-art LLMs like ChatGPT\cite{chatgpt}, Claude\cite{claude}, and Google Gemini\cite{gemini} have hundreds of billions of parameters, a significant trend has emerged toward creating smaller yet highly capable models through advanced techniques like knowledge distillation~\cite{kd}. For instance, Meta's Llama3.2\cite{llama3.2} includes lightweight 1B and 3B models for high-performance edge applications like text generation. Similarly, GPT-4o mini\cite{gpt4omini} exceeds GPT-4's capabilities while reducing inference costs by 60\% compared to GPT-3.5 Turbo, and it is believed to be comparable in scale to Llama3-8B model\cite{gpt4omini_1}.

Most existing AI inference accelerators, such as GPUs, rely on off-chip DRAM due to limited on-chip memory capacity. However, accessing off-chip DRAM or HBM is less power-efficient and scaling the memory bandwidth suffers from the \textit{memory wall}~\cite{memory_wall} problem. As shown in Figure~\ref{fig:model size}, an increase in model parameters results in decreased inference throughput and greater energy consumption. Although on-chip memory offers significantly better power efficiency and higher throughput, its limited capacity hinders large model acceleration. However, the prevailing trend to develop smaller models reduces memory capacity requirements, allowing an entire model's weights to fit within on-chip memory, thereby improving overall system efficiency. Quantization techniques further reduce memory requirements~\cite{awq,quip,10705489,i-llm,binarybert,bitnet,bitnet1.58} wherein activations and/or weights are represented using reduced-precision data types. For example, binary and ternary quantizations represent weights using just two values \{-1, 1\} or three values \{-1, 0, 1\}, respectively.
Since a binary or ternary weight can be stored using less than 2 bits, such LLMs are known as `1-bit' LLMs. Despite low-precision weights, 1-bit LLMs have the potential to achieve similar accuracy of full-precision models~\cite{bitnet,bitnet1.58,scalable,vit1.58,1bittheory}. Low-bit quantizations open the door to fully on-chip inference for larger models at substantially higher throughput and lower energy as shown in Figure~\ref{fig:model size}. However, existing hardware, such as CPUs and GPUs, lacks the architectural support to unlock the potential of low-bit quantizations, underscoring the need for specialized hardware designs.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/insight.pdf}
    \caption{On-Chip vs. off-chip memory for inference: Throughput and energy trends with increasing model parameters}
    \label{fig:model size}
    \vspace{-3mm}
\end{figure} 

To address this gap, we propose \textit{\textbf{TerEffic}}, a custom architecture on FPGA to fully unlock the potential of on-chip inference.  We focus on ternary quantization due to its superior accuracy compared to binary approaches~\cite{bitnet, bitnet1.58}. Our design encompasses three key innovations: a {\bf 1.6-Bit Weight Compression} method that approaches the theoretical minimum of 1.58 bits per weight, a custom {\bf Ternary MatMul Unit (TMU)} optimized for ternary computations, and a {\bf Compute-Memory Alignment} strategy that maximizes the utilization of on-chip memory bandwidth and capacity. We propose two architectural variants. Our {\bf fully on-chip architecture} supports models of up to 1.3B parameters. For a 370M parameter model with single-batch inference, our design achieves a throughput of 12,700 tokens/sec (149× higher) and a power efficiency of 467 tokens/sec/W (19× better) compared to NVIDIA's Jetson Orin Nano. For multi-batch inference, our approach leverages {\bf pipeline parallelism}, yielding a 130,200 tokens/sec throughput (119× higher) and a 2,770 tokens/sec/W power efficiency (12× better). For larger models that exceed the on-chip memory capacity, we propose an \textbf{HBM-assisted architecture} that maintains high performance through {\bf full-resource parallelism}, minimizing the impact of off-chip memory access.



