\section{TerEffic Architecture design}
\label{sec:archi}
We design a ternary LLM accelerator architecture on FPGA to showcase the benefits of the aforementioned optimizations. We choose the MatMul-free LM model~\cite{scalable} as the representative ternary LLM for our design. Although \cite{scalable} also proposed an FPGA design, it was a simple and inefficient prototype as their main focus was on the model architecture. 

\begin{figure}[h]
    \centering
    \vspace{-3mm}
    \includegraphics[width=.9\linewidth]{figures/arch.pdf}
    \caption{TerEffic Hardware Architecture}
    \label{fig:model}
    \vspace{-6mm}
\end{figure}

\subsection{Architecture Overview}

TerEffic overall hardware architecture is shown in Fig.\ref{fig:model}, along with its two major components, the HGRN\cite{HGRN} and the GLU\cite{glu} module. HGRN is a powerful RNN-based alternative to the self-attention mechanism\cite{attention}, while GLU, widely utilized in models like Llama\cite{llama}, serves as a robust enhancement for feed-forward networks (FFNs).
In the model, addition, subtraction, and dot product (Dot) can be directly implemented using LUTs and DSPs, while the sigmoid function is approximated by piecewise linear equations~\cite{sigmoid}. Consequently, the primary implementation challenge lies in the BitLinear module consisting of a Root-Mean-Square Normalization (RMSNorm) module and a ternary MatMul module.
\subsubsection{RMSNorm module}
The RMSNorm\cite{RMSNorm} is more computationally efficient than the traditional LayerNorm\cite{attention} but maintains high accuracy, making it well-suited for FPGA. The algorithm for RMSNorm is presented below:
\vspace{-1mm}
\begin{equation}
        r = \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2 + \epsilon} \quad , \quad 
        \text{RMSNorm}(X) = \frac{X\odot W_n}{r}
\label{eq:RMSNorm}
\end{equation}
\vspace{-1mm}

where $X\in\mathbb{R}^{1 \times d}$ denotes the input activation, $W_n\in\mathbb{R}^{1 \times d}$ denotes the normalization weight, $r$ denotes the RMS result and $\epsilon$ is a small constant. As shown in the architecture in Fig. \ref{fig:RMSNorm}, the computation of r and $X\odot W_n$ can be executed in parallel. As the former has longer latency, the results of $X\odot W_n$ are temporarily stored in on-chip buffers. These results are then repeatedly retrieved from the buffers to match the multiple iterations required by the subsequent ternary MatMul modules. Moreover, as divisions incur high hardware cost and long cycle latency, we replace the divisions ($\div r$) with DSP-based multiplications ($\times \frac{1}{r}$), using r as an index to retrieve 1/r from an on-chip look-up table consisting of a small amount of SRAM. This method saves hardware resources and significantly increases maximum frequency.

\subsubsection{Ternary MatMul Module}
The Ternary MatMul Module serves as the primary computational core executing $X\times W$, where $X\in\mathbb{R}^{1 \times d}$ denotes the normalized activation and $W\in\mathbb{R}^{d \times d}$ denotes the ternary weight matrix. The module consists of submodules that perform the dot product between $X$ and one column of $W$. Each submodule is made up of TMUs for ternary MatMuls and a reduction tree to aggregate the TMU outputs. A detailed architecture is shown in Fig.\ref{fig:TMM}. To address the memory bandwidth limitations, we partition $X$ and each column of $W$ into x groups. Each submodule, which contains $\frac{d}{x}$ TMUs, computes the MatMuls for one group. Moreover, due to the limited computational capacity, the d columns of $W$ are divided into y sets and processed sequentially by $\frac{d}{y}$ submodules. The values of x and y are set based on the on-chip memory bandwidth and computing capacity, optimized by the alignment strategy discussed in Section \ref{sec:Compute-Memory Alignment}.


\begin{figure}[h]
    \vspace{-5mm}
    \centering
    \begin{subfigure}[b]{0.4\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/RMSNorm.pdf} 
        \caption{RMSNorm Module}
        \label{fig:RMSNorm}
    \end{subfigure} 
    \begin{subfigure}[b]{0.4\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/TernaryMatMulModule.pdf}  
        \caption{Ternary MatMul Module}
        \label{fig:TMM}
    \end{subfigure}
    \vspace{-2mm}
    \caption{BitLinear Module}
    \label{fig:bitlinear module}
    \vspace{-6mm}
\end{figure}


\subsection{Architecture Variants}
We propose two memory architectures--one aims to store all weights in on-chip memory, and another utilizes an HBM. Since a single card is insufficient to hold all the weights of a moderate-sized model, the fully-on-chip architecture may need to leverage multiple cards. For larger models, we switch to HBM-assisted architecture that requires only one card for weight storage. We explore different parallelism methods in both architectures to achieve higher energy efficiency. Though the methods differ from the batching method for GPU, we use terms like 'multi-batch' and 'batch size' for convenience.
\subsubsection{TerEffic On-Chip Architecture}
\label{sec:On-Chip Architecture}
The on-chip architecture corresponds to the orange line in Fig. \ref{fig:Alignment}, where we store all weights on-chip and benefit from the massive on-chip bandwidth.
The basic design is based on the 24-layer, 370M model from~\cite{scalable}. Each layer is segmented into 10 main stages, as shown in Fig. \ref{fig:stages} (Norm denotes RMSNorm and TMM denotes ternary MatMul), with the latency of each stage indicated in the red row. Through the optimizations discussed before, the latency per layer is 820 cycles, resulting in a total latency of $\approx$  20,000 cycles across all 24 layers.

After 1.6-Bit Weight Compression, the 370M model occupies $\approx$ 57.6MB of memory, exceeding the on-chip capacity of 42MB (see TABLE \ref{tab:SRAM}). Thus, a 2-card system is required, with each card holding the weights for 12 layers and processing them accordingly, as shown in Fig. \ref{fig:multi-card}. As the model size increases, more cards become necessaryâ€”for instance, the 1.3B model requires eight cards each storing three layers.

Though this design delivers the shortest latency, the power efficiency is relatively low as only a portion of the resources is utilized at any point in time. To better harness parallelism, we construct a \textbf{pipeline}. A simple 2-stage example is illustrated in Fig. \ref{fig:pipeline}, where a batch size of two is processed concurrently by different TMUs.
As shown by the purple row in Fig. \ref{fig:stages}, each pipeline stage takes 160 cycles, resulting in a single-layer latency of 1600 cycles. With each card handling up to a batch size of 10, the 2-card system can process a batch size of 20, boosting throughput by $10\times$ over the basic design.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/Stages.pdf}
    \caption{Stage latency of a layer for on-chip single-batch, on-chip multi-batch (with pipeline parallelism), and HBM-assisted multi-batch (with full-resource parallelism)}
    \label{fig:stages}
    \vspace{-4mm}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.23\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/Multi-Card.pdf} 
        \caption{On-Chip Architecture}
        \label{fig:multi-card}
        \vspace{-1mm}
    \end{subfigure} 
    \begin{subfigure}[b]{0.23\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/HBM_version.pdf}  
        \caption{HBM-Assisted Architecture}
        \vspace{-1mm}
        \label{fig:HBM}
    \end{subfigure}
    \caption{TerEffic Architecture Variants}
    \label{fig:single batch}
    \vspace{-6mm}
\end{figure}
\subsubsection{TerEffic  HBM-Assisted Architecture}
\label{sec:HBM-Assisted Architecture}
For larger models, an HBM-assisted architecture (Fig.\ref{fig:HBM}) is required. Xilinx U280 FPGA features 8GB HBM with 32 channels, providing a total maximum bandwidth of 8Kb. As discussed in Section \ref{sec:Compute-Memory Alignment}, the HBM's bandwidth is much lower than on-chip memory, resulting in a longer latency of about 2,400 cycles/layer. 

For this architecture, the pipeline parallelism is not suitable, as the latency after pipelining is shorter than the data-fetch latency from HBM. Moreover, pipelining requires two layers of weights stored on-chip in a ping-pong buffer, which is infeasible for large models. Hence, we introduce \textbf{full-resource parallelism} with a simplified example (2 stages, batch size=2) presented in Fig. \ref{fig:parallelism}. Unlike in the previous pipeline, all the TMUs are utilized for processing each stage. As the batch size increases, the TMUs available for each input decrease and the latency increases. We set the batch size to 15 to accommodate 2,400 cycle HBM data-fetch latency. The detailed latency is shown in the orange row in Fig. \ref{fig:stages}, where the latency of each TMM stage is proportional to the computational workload.


\begin{figure}[t]
    \vspace{-3mm}
    \centering
    \begin{subfigure}[b]{0.15\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/Pipeline_Parallelism.pdf} 
        \caption{Pipeline}
        \vspace{-2mm}
        \label{fig:pipeline}
    \end{subfigure}
    \hspace{10mm}  
    \begin{subfigure}[b]{0.15\textwidth} 
        \centering
        \includegraphics[width=\textwidth]{figures/Full-Resource_Parallelism.pdf}  
        \caption{Full-Resource}
        \vspace{-2mm}
        \label{fig:Full-Resource}
    \end{subfigure}
    \caption{Parallelism Methods}
    \label{fig:parallelism}
    \vspace{-5mm}
\end{figure}
