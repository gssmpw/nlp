\section{TerEffic Key Innovations}
%\vspace{-1.5mm}
To enable efficient on-chip inference of ternary LLMs, we introduce three key innovations:
\begin{enumerate}
\item \textbf{1.6-Bit Weight Compression}: Our method encodes ternary weights \{-1, 0, 1\} using only 1.6 bits per weight, approaching the theoretical minimum of 1.58 bits. This 20\% reduction in memory footprint compared to 2-bit encoding enables larger models to fit on-chip.
\item \textbf{Ternary MatMul Unit (TMU)}: We optimize ternary matrix multiplications (MatMuls) through specialized hardware units that reduce complex operations such as negation and leverage FPGA-native multiplexers. This customization reduces Look-up Table(LUT) usage by 40\% while maintaining high computational throughput.
\item \textbf{Compute-Memory Alignment}: Our hybrid URAM-BRAM memory architecture balances computational power, memory bandwidth, and capacity. By storing weights in URAM and intermediate values in BRAM, we achieve optimal resource utilization without bandwidth bottlenecks or wasted resources.
\end{enumerate}

\subsection{1.6-Bit Weight Compression}
\label{1.6bit}
\vspace{-0.5mm}
To encode ternary weights \{-1, 0, 1\} with binary bits, the theoretical lower bound is 1.58 bits\cite{bitnet1.58}. However, achieving exactly 1.58 bits is not feasible in hardware, while a straightforward 2-bit representation introduces 25\% redundancy. To overcome this challenge, we adapt and optimize the encoding scheme from~\cite{ternaryencoding} to create an efficient 1.6-bit compression method that closely approaches the theoretical limit while remaining hardware-friendly.

We observe that 5 ternary weights have 243 possible combinations whereas 8 binary bits can represent 256 distinct values. Hence, we encode 5 ternary weights using 8 bits, averaging 1.6 bits per weight. The encoding and decoding scheme is shown in Fig. \ref{fig:bit-compression}. For example, five original weights \{-1,0,0,1,1\} are encoded into 8 bits (10001100) when stored in memory. During inference, the 8-bit encoded weight is decoded into five 2-bit representations, where 01 corresponds to 1, 11 to -1, and 00 to 0. As the decoding involves only bitwise operations, such as + and \&, it incurs minimal hardware cost and latency. This method can store and transfer 25\% more data compared to the regular 2-bit representation, effectively improving throughput and energy efficiency.
\begin{figure}
    \centering
    \vspace{-1mm}\includegraphics[width=0.5\textwidth]{figures/bit-compression.pdf}
    \caption{1.6-Bit Weight Compression}
    \label{fig:bit-compression}
    \vspace{-7mm}
\end{figure}


\subsection{Ternary MatMul Unit}
MatMuls account for the largest portion (65\%-85\%) of the computational workload\cite{high_perf} in typical LLMs. To achieve the full potential of ternary LLMs, we replace costly MatMuls with optimized \textbf{Ternary MatMuls Units (TMUs)} that are much cheaper. 

A naive TMU design is shown in Fig. \ref{fig:TMU_ori}. Here, the partial sum ($S$) is either added or subtracted by the activation ($X$), or remains unchanged depending on the weight ($W \in \{1,-1,0\}$). We identify inefficiencies in this simple design and propose the corresponding optimizations. First, the activation ($X$) is broadcast to multiple TMUs in parallel (as elaborated in Section IV.A). Therefore, performing complex operations, such as negation, within each TMU harms efficiency. To address this, we pre-calculate the negation outside the TMUs, providing both the activation and its negation as TMU inputs. Secondly, as the default logic units, LUTs cannot directly implement conditional statements but decompose them into a series of mapping relations. This indirect mapping causes resource wastage. We resolve this by leveraging another basic FPGA resource, the 2-to-1 multiplexer (Mux), specialized for the selection logic. With each weight decoded to 2 bits (as presented in Section \ref{1.6bit}), the high bit (W[1]) serves as the select signal for the Mux, while the low bit (W[0]) determines whether the Mux's output should be retained or transformed into zero. By integrating the two optimizations, we introduce an improved TMU design illustrated in Fig. \ref{fig:TMU_new}, which achieves approximately 40\% LUT savings for the whole design.
\begin{figure}[h]
    \vspace{-1mm}
    \centering
    \begin{subfigure}[b]{0.14\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/TMU_ori.pdf}
        \caption{Naive TMU}
        \label{fig:TMU_ori}
    \end{subfigure}
    \hspace{15mm}
    \begin{subfigure}[b]{0.14\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/TMU_new.pdf}
        \caption{Improved TMU}
        \label{fig:TMU_new}
    \end{subfigure}
    \caption{Improvements in the TMU Design}
    \label{fig:TMU}
    \vspace{-4.5mm}
\end{figure}



\vspace{-1.5mm}
\subsection{Compute-Memory Alignment}
\label{sec:Compute-Memory Alignment}


\begin{figure}
    \vspace{-5mm}
    \centering
    \includegraphics[width=.95\linewidth]{figures/design_space.pdf}
    \caption{Compute-Memory Alignment}
    \label{fig:Alignment}
    \vspace{-7mm}
\end{figure}

Xilinx U280 FPGA offers two types of on-chip SRAM: BlockRAM (BRAM) and UltraRAM (URAM), as shown in Table \ref{tab:SRAM}. 
To align with the substantial computational capability offered by TMU, an intricate memory architecture is required to provide enough bandwidth while avoiding excessive bandwidth which will cause power wastage. 
At the same time, the memory architecture must offer sufficient capacity for on-chip weight storage while optimizing memory capacity utilization to minimize power wastage.

\begin{table}[h]
    \vspace{-3mm}
    \centering
    \caption{Attibutes of On-chip BRAM and URAM on U280}
    \label{tab:SRAM} 
    \resizebox{\linewidth}{!}{
    \begin{tabular}{|r|r|r|r|r|}
        \hline
        SRAM& Capacity/piece & Bandwidth/piece & Number & Total Capacity \\
        \hline
        BRAM & 36Kb & 72b & 2,016 & 8.85MB \\
        \hline
        URAM & 288Kb & 144b & 960 & 33.75MB \\
        \hline
    \end{tabular}
    }
    \vspace{-3mm}
\end{table}
Fig. \ref{fig:Alignment} illustrates memory choices where each point depicts the bandwidth and capacity provided by a certain memory architecture. The horizontal dotted line represents the required bandwidth to align with the maximum computational capability of the device, while the vertical dotted lines indicate the required memory capacity that increases with the model size. As shown by the blue line, a fully BRAM-based architecture lacks sufficient memory capacity. Conversely, a fully URAM-based architecture (the green line) suffers from low bandwidth, also resulting in power wastage.

Therefore, we propose a \textbf{hybrid fully on-chip architecture} (the orange line) storing weights in URAM and buffering intermediate values in BRAM. Our design falls approximately on the intersection of the dotted lines in Fig. \ref{fig:Alignment}, enabling full utilization of computational power, memory bandwidth, and capacity concurrently. However, it can be observed that as the model size increases, the on-chip memory capacity will eventually fail to meet the storage requirements. In such cases, the large capacity of off-chip HBM is still required, while its limited bandwidth bottlenecks the performance (shown in the purple line). To alleviate this issue, we introduce an \textbf{HBM-assisted architecture} with effective parallelization, proposed in Section \ref{sec:HBM-Assisted Architecture}.





