\section{Background \& Related work}
%\vspace{-2mm}
\subsection{Binary and Ternary Quantizations}
Quantization is a key strategy to accelerate inference as neural networks grow rapidly in parameter count. Binary and Ternary quantization were initially validated in CNN models such as BNN \cite{BNN}, XNOR-Net \cite{Xnor}, and TWN \cite{TWN}, achieving high efficiency with minimal accuracy loss.

In the era of LLMs, large model sizes and limited hardware resources necessitate low-precision quantization, especially in the context of edge devices. As a pioneer, BitNet\cite{bitnet} employed binary quantization and introduced BitLinear as a replacement for matrix multiplications(MatMuls) in linear projections. As its variant and improvement, BitNet 1.58b\cite{bitnet1.58} matched the performance of full-precision (fp16 or bf16) transformers using ternary quantization. Furthermore, the 1-bit model was extended to the Vision Transformer (ViT) domain \cite{vit1.58} and a scaling law \cite{1bittheory} was established, providing a theoretical basis for the impressive performance of these models. Advances in post-training reparameterization \cite{shiftaddllm} also paved the way for low-cost training of 1-bit LLMs.

\vspace{-1.5mm}
\subsection{FPGA-based Transformer Accelerators}
\vspace{-0.5mm}
Field-programmable gate arrays (FPGAs) are widely used for accelerating various tasks.

In the context of deep learning, several works have contributed toward accelerating transformer inference using FPGAs. FTRANs\cite{FTRANs} was the earliest accelerator specifically designed for Transformers featuring an enhanced block-circulant matrix-based weight representation. The NPE\cite{NPE} flexible overlay processor offered a common method for approximating different nonlinear functions. The HPTA\cite{HPTA} accelerator provided support for several transformer variants without needing FPGA reconfiguration. DFX\cite{DFX} presented an end-to-end design for Generative Pretrained Transformer (GPT) model inference. \cite{sparse_atten} proposed efficient algorithmâ€“hardware co-designs with sparse attention and dynamic pipelining.  \cite{spatial} built an HLS kernel library for FPGA-based LLM spatial acceleration. FlightLLM\cite{flightllm} introduced a sparse DSP chain to support different sparsity patterns and an always-on-chip decode scheme to reduce memory overhead. EdgeLLM\cite{edgellm} proposed a CPU-FPGA heterogeneous acceleration system with a unified and universal data format and customized FP16*INT4 computational units.

So far, on-chip inference on FPGA has been minimally explored, yet 1-bit quantization offers a promising avenue for this efficient paradigm. FPGAs provide an ideal platform to harness the advantages of 1-bit LLMs, thanks to their bit-level reconfigurability that enables customized memory architectures and computational units. While \cite{scalable} proposed a basic FPGA implementation for their ternary model, their prototype lacked effective hardware optimizations and failed to leverage the on-chip potential. 



