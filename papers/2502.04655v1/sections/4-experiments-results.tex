% !TeX root = ../camera.tex

\input{sections/fig-4-data-insights.tex}
%event, #opinions, users, posts, 
\section{Experiments and Results}
In this section, we present the experimental setup and the results we obtain; 
including datasets and data insights (\cref{subsec:datasets}), 
the baseline models we compare against (\cref{subsec:baselines-xp-setup}), and 
the results that address our research questions (\cref{subsec:results}).
\begin{table}[t]
    \caption{Dataset Statistics}
    % \begin{tabular}{lrrr}
    %     \toprule
    %     \textbf{Dataset} & \textbf{\#Opinions} & \textbf{\#users} & \textbf{\#posts} \\ \midrule
    %     Bushfire       & 15                  & 13,438           & 78,030           \\ 
    %     Climate Change & 24                  & 25,850           & 138,278          \\ 
    %     Vaccination    & 27                  & 34,652           & 178,894          \\ 
    %     COVID-19          & 17                  & 67,727           & 640,100          \\ \midrule
    %     DiN            & 9                   & 41               & 746,653          \\ 
    %     \bottomrule
    % \end{tabular}
    \setlength{\tabcolsep}{3pt}
    \centering
    \begin{tabular}{@{}lrrrr|r@{}}
        \toprule
        \textbf{Dataset} & \multicolumn{1}{c}{Bushfire} & \multicolumn{1}{c}{Climate Ch.} & \multicolumn{1}{c}{Vaccin.} & \multicolumn{1}{c|}{COVID-19} & \multicolumn{1}{c}{DiN} \\ \midrule
        \textbf{\#posts}                     & 78,030                       & 138,278                            & 178,894                         & 640,100                       & 746,653                 \\
        \textbf{\#users}                     & 13,438                       & 25,850                             & 34,652                          & 67,727                        & 41                      \\
        \textbf{\#opinions}                  & 15                           & 24                                 & 27                              & 17                            & 9                       \\ \bottomrule
    \end{tabular}
    \label{tab:data_statistics}
\end{table}
\subsection{Datasets}
\label{subsec:datasets}



%% MAR: moved here for figure placement


\paragraph{Datasets}
Our experiments use two Facebook datasets: the theme-focused SocialSense dataset~\citep{kong2022slipping} and the user-centric Disinformation Network (DiN) dataset.
For each post in our datasets, we collect historical engagement metrics (likes, shares, comments, emoji reactions) collected via CrowdTangle API\footnote{\url{https://www.crowdtangle.com/} before its termination in August 2024.}. 
\emph{SocialSense} contains posts and comments from four main themes during 2019-2021(see \cref{tab:data_statistics}) that attracted significant volumes of misinformation and conspiratorial discussions. 
The \emph{DiN dataset} comprises posts from $41$ accounts (2019-2024). Social science experts systematically analyzed and assigned narrative labels to these posts through comprehensive content evaluation to detect suspected coordinated information operations. 
The two datasets capture the dynamics of misinformation across diverse real-world events (SocialSense) and disinformation narrative spread by information operation networks (DiN).~\footnote{Note that, posts with fewer than four engagement intervals were excluded from model evaluation to ensure sufficient temporal depth. }
%Our initial analysis covered the complete dataset, while experimental results reflect posts with $\geq$4 intervals.
% : SocialSense enables the evaluation of engagement prediction across diverse real-world events, DiN facilitates the assessment of narrative classification within information operation networks.

\paragraph{Data Insights}
\cref{fig:data_insights}(a) and (b) present the Empirical Complementary Cumulative Distribution Functions (ECCDFs) for likes, shares, comments, and emoji reactions across DiN (a) and the Climate Change theme in SocialSense (b). The survival probability $P(X \geq k)$ measures the likelihood of achieving at least $k$ engagements \citep{clauset2009power}, and the power-law exponent $\alpha$ characterizes the decay rate \citep{newman2005power}. While Climate Change content rarely exceeds $10^4$ total engagements, DiN reaches $10^6$, indicating significantly broader reach.
In the low-engagement regime ($1 \leq k \leq 10$), DiN exhibits a higher survival probability ($\alpha \approx 2.1$) compared to Climate Change ($\alpha \approx 2.4$), suggesting stronger early visibility potential. The mid-range ($10 \leq k \leq 1000$) shows uniform decay across engagement types for Climate Change, reflecting organic interaction patterns. In contrast, DiN reveals marked stratification, especially in likes. Beyond $k > 1000$, Climate Change content plateaus near $10^3$ engagements, aligning with established social network theory regarding human-scale constraints -- approximately 150 stable connections, known as Dunbar’s number \citep{dunbar1992neocortex} -- while DiN content transcends these natural limits, reaching $10^6$ engagements.

\cref{fig:data_insights}(c) offers examines comment distributions over time windows ranging from one hour to seven days. The scale-invariant, power-law structure persists across all observation periods, though longer windows ($3$–$7$ days) exhibit slightly elevated survival probabilities beyond $10^3$. This self-similar temporal behavior distinguishes naturally diffusing, high-visibility content from artificially amplified patterns, underscoring the unique viral longevity of DiN.

%Engagement probabilities follow a consistent hierarchy (likes > emoji reactions > shares > comments) across datasets, with DiN (a) exhibiting larger inter-engagement gaps, especially in high-engagement regions (>100).
%\cref{fig:data_insights}(a) and (b) present the log-log scale Empirical Complementary Cumulative Distribution Functions (ECCDF) for four types of social media engagement: likes, shares, comments, and emoji reactions for DiN (a) and the Climate Change theme in SocialSense (b).
%(c) displays the temporal evolution of comment distributions across multiple time windows.
%We define survival probability $P(X \geq k)$ as the probability that content receives at least $k$ engagements \citep{clauset2009power}, with power-law exponent $\alpha$ characterizing the decay rate \citep{newman2005power}. 
%DiN content achieves engagements up to $10^6$, two orders higher than climate change content ($10^4$). In the low engagement region ($k \in [1,10]$), DiN shows higher survival probability ($\alpha \approx 2.1$ vs $\alpha \approx 2.4$), indicating greater potential for visibility growth. The mid-range ($k \in [10,1000]$) reveals distinct patterns: climate change content exhibits uniform decay across engagement types suggesting organic interactions, while DiN content shows stratification with anomalously high like probabilities. 
%In the viral region ($k > 1000$), climate change content respects natural network limits at $10^3$ comments, aligning with established social network theory regarding the constraints of human social networks -- 150 connections, known as Dunbar's number\cite{dunbar1992neocortex} -- while DiN content extends to $10^6$ engagements.

\input{sections/tab-3-engagement-prediction.tex}

\subsection{Baselines and Experimental Setup}
\label{subsec:baselines-xp-setup}

We compare our \icmamba model against the following state-of-the-art baselines, including generative models, transformer-based architectures and state space models:
% like the Mean Behaviour Poisson (MBP) process~\cite{rizoiu2022interval}, transformer-based architectures (Informer~\cite{zhou2021informer}, Autoformer~\cite{wu2021autoformer}, Transformer-Hawkes (TH), Interval-Censored Transformer Hawkes (IC-TH)~\cite{kong2023interval}~\cite{zuo2020transformer}, and TimeSeriesTransformer~\cite{li2019enhancing}), and the state space model Mamba~\cite{dao2024transformers}:
\begin{itemize}[leftmargin=*]
    \setlength\itemsep{0em}
    \item \textbf{TimeSeriesTransformer}~\cite{li2019enhancing}\footnote{\url{https://huggingface.co/docs/transformers/en/model_doc/time_series_transformer}} is a transformer-based model specifically adapted for time series forecasting. It applies self-attention mechanisms to capture temporal dependencies.
    \item \textbf{Informer}~\cite{zhou2021informer}\footnote{\url{https://huggingface.co/docs/transformers/en/model_doc/informer}} is a long sequence time-series forecasting model that uses a ProbSparse self-attention mechanism to handle long-term dependencies.
    \item \textbf{Autoformer}~\cite{wu2021autoformer}\footnote{\url{https://huggingface.co/docs/transformers/en/model_doc/autoformer}} is a decomposition-based architecture for long-term time series forecasting. It uses an auto-correlation mechanism to identify period-based dependencies and a series decomposition architecture for trend-seasonal decomposition.
    \item \textbf{Mean Behaviour Poisson (MBP)}~\cite{rizoiu2022interval} is a generative time series model that uses a compensator function to model non-linear engagement patterns. It treats each engagement as an event in a continuous time process and optimizes post-specific parameters to model the expected cumulative engagements over time to capture the growth patterns.
    \item \textbf{Transformer-Hawkes (TH)}~\cite{zuo2020transformer} is a model that combines the transformer architecture with the Hawkes process for modeling sequential events. It uses self-attention mechanisms to capture temporal dependencies in event sequences.
    \item \textbf{Interval-Censored Transformer Hawkes (IC-TH)}~\cite{kong2023interval} is a TH extension designed to handle interval-censored data. It adapts the transformer architecture to work with event data where exact occurrence times are unknown but bounded within intervals.
    \item \textbf{TS-Mixer}~\cite{chen2023tsmixer}\footnote{\url{https://github.com/google-research/google-research/tree/master/tsmixer}} is a model that combines MLPs and transformers for time series forecasting. It uses separate mixing operations across the temporal and feature dimensions, allowing it to capture both temporal patterns and feature interactions.
    \item \textbf{Mamba}~\cite{mamba2}\footnote{\url{https://huggingface.co/docs/transformers/en/model_doc/mamba2}} is a selective state space model, it uses selective algorithms instead of attention mechanisms for sequence modeling. It can handle long-range dependencies in sequential data for time series analysis tasks.
\end{itemize}
\noindent\textbf{Experimental settings.}
We use a temporal holdout evaluation protocol across all the datasets.
We chronologically order all posts and use the earliest $70\%$ for training, the next $15\%$ for validation, and the most recent $15\%$ for testing. 
This ensures no future information leaks into training and models are evaluated on their ability to generalize to future posts.
Models are implemented using PyTorch, with hyperparameters and other settings detailed in \cref{sec:exp_settings}.



% \subsection{Engagement Prediction (RQ1) and Opinion Identification}
\subsection{Engagement Prediction--RQ1}
\label{subsec:results}

We evaluate the performance of our models with two tasks: \emph{engagement forecasting} and \emph{opinion classification}. 
For \emph{engagement prediction}, we observe the first six hours of engagement metrics for each post and forecast the overall engagement metrics (i.e.,\ at $T = \infty$). 
%
For \emph{opinion classification}, we evaluate our model's classification performance at multiple granularities.
We perform a \emph{post-level opinion classification} across the four SocialSense themes (\textit{bushfire}, \textit{climate change}, \textit{vaccination}, and \textit{COVID-19}) -- that is, we predict if a given post expresses one of the predefined opinions.
For the DiN dataset, we perform a \emph{user-level opinion classification} --
% We extend this prediction task to the user level in the DiN dataset, 
classify the presence of opinions across multiple posts from the same user.

\noindent\textbf{Post-level Engagement Prediction Performance -- RQ1}
\cref{tab:postlevel_results} reports the performance metrics using three standard measures.
We evaluate the models using RMSE to assess absolute prediction errors (crucial for high-engagement posts), MAPE for scale-independent accuracy, and $R^2$ to measure explained variance in engagement predictions.
\icmamba outperforms all baselines on every metric (RMSE, MAPE, and $R^2$) and dataset, while the original Mamba architecture ranks consistently second, confirming the effectiveness of state space models. Among transformers, IC-TH improves upon TH, and TS-Mixer outperforms both Autoformer and Informer; TSTransformer lags behind. Interestingly, the lightweight MBP model, still competes well on some events (particularly \textit{bushfire} and \textit{climate change}).
All models exhibit performance degradation on the DiN dataset, reflecting the complexity of predicting engagement in coordinated campaigns. For models supporting dynamic prediction time points, additional results for next-time and next social engagement metrics are provided in Appendix~\ref{sec:next_token_pred}.

We conduct an ablation study to understand the contribution of different components by removing text, user, and temporal features from \icmamba (\cref{tab:postlevel_results}). 
Text features demonstrate a stronger influence on SocialSense datasets, where their removal leads to a 0.005 RMSE increase, compared to a smaller 0.002 RMSE increase in the DiN dataset. This difference highlights the crucial role of textual content in organic content spread versus coordinated campaigns. Temporal features, conversely, show greater impact on the DiN dataset, where their removal results in a 0.006 RMSE increase, compared to a 0.003 RMSE increase in SocialSense datasets. This may suggest the strategic temporal patterns in coordinated disinformation campaigns. User features maintain consistent importance across both datasets, with their removal causing similar performance degradation (0.002-0.003 RMSE increase) regardless of the dataset type. Even with text features removed, \icmamba still outperforms IC-TH, improving RMSE from 0.156 to 0.123 on the Bushfire dataset, demonstrating the fundamental strength of our model's architectural design.




\noindent\textbf{Opinion-level Classification Performance --RQ1}
In our classification settings, we tackled datasets of varying complexity: the \textit{bushfire} dataset contains $9$ opinions, \textit{climate change} and \textit{vaccination} each have 12 classes, the \textit{COVID-19} dataset includes $10$ classes, and the DiN (Disinformation Narrative) dataset comprises $9$ distinct narrative labels. 
We also include a random classification baseline with an expected F1 score of $1/N$ for each dataset, where $N$ is the number of classes.
Note that we removed opinions with less than $5,000$ posts in this experimental setting.

% The classification results in \cref{tab:cls_results} show the performance variations across models and datasets. 
\cref{tab:cls_results} presents the macro-averaged F1 scores for classification across models and datasets. IC-Mamba consistently outperforms all others, achieving F1 scores between $0.69$ and $0.75$. While BERT performs well on SocialSense (F1: $0.62$–$0.68$), both models see significant drops on DiN, with IC-Mamba scoring $0.52$ and BERT falling to $0.11$. This highlights the limitations of text-only analysis for DiN, where narrative elements demand more complex temporal or contextual understanding.
Informer, Autoformer, and Mamba struggle on SocialSense (F1 < $0.41$) but perform relatively better on DiN, with Mamba achieving its best score of $0.32$. This suggests that temporal and non-textual features are critical for narrative detection, contrasting with the outbreak event focus of SocialSense.
%\cref{tab:cls_results} presents the classification F1 scores across models and datasets, macro averaged to account for class imbalance.
%\icmamba consistently outperforms all other models, achieving F1 scores ranging from $0.69$ to $0.75$. 
%BERT shows strong performance on the SocialSense dataset, with F1 scores between $0.62$ and $0.68$. However, both models show a notable drop in performance on the DiN dataset, with \icmamba achieving an F1 score of $0.52$ and BERT performing poorly at $0.11$. This shows that the DiN classification task requires more than just textual analysis. Pure sequence models may capture narrative aspects that text-only models miss. Interestingly, while Informer, Autoformer, and Mamba struggle with SocialSense dataset on the classification task (F1 scores below $0.41$), they perform relatively better on the DiN dataset, with Mamba achieving its highest score of $0.32$. This indicates that temporal and non-textual features play important roles in narrative detection tasks, compared with outbreak events in SocialSense dataset. 

\begin{table}[t]
    \caption{
        Opinion Classification results; F1 scores are reported; higher is better; best results in boldface.}
    \centering
        \begin{adjustbox}{max width=1.0\linewidth}
            \begin{tabular}{l@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}c@{\;\;}c}
                \toprule
                \toprule
                Model                              & Bushfire       & Climate        & Vaccination    & COVID-19          & DiN            \\ \toprule
                Random                             & 0.111          & 0.083          & 0.083          & 0.083          & 0.111          \\
                \midrule
                Informer~\cite{zhou2021informer}   & 0.323          & 0.291          & 0.274          & 0.299          & 0.248          \\
                Autoformer~\cite{wu2021autoformer} & 0.342          & 0.313          & 0.278          & 0.324          & 0.255          \\
                \midrule
                BERT~\cite{vaswani2017attention}   & 0.676          & 0.652          & 0.621          & 0.644          & 0.107          \\
                \midrule
                Mamba~\cite{mamba2}   & 0.412          & 0.375          & 0.363          & 0.388          & 0.316          \\
                \icmamba                           & \textbf{0.751} & \textbf{0.724} & \textbf{0.687} & \textbf{0.705} & \textbf{0.508} \\
                \bottomrule
                \bottomrule
            \end{tabular}
        \end{adjustbox}
    \label{tab:cls_results}
\end{table}
\input{sections/fig-5-forcasting.tex}
\subsection{Early Engagement Prediction--RQ2}
\label{sec:early_pred}
We vary the length of the observed period in the temporal holdout setup (see \cref{subsec:baselines-xp-setup})
to assess how well different models can forecast engagement in the critical initial hours after a post is made.
\cref{fig:early_prediction} shows RMSE-based early prediction performance for the climate change theme in SocialSense, measured at intervals from 15 minutes to 6 hours after posting, across Informer, Autoformer, TS-Mixer, IC-TH, and \icmamba.

All models demonstrate substantial improvement in prediction accuracy over time, with error rates decreasing from 15 minutes to 6 hours. The most notable improvements occur in the first hour, particularly between 15-50 minutes, suggesting that the first hour of a post's life is crucial for accurate engagement forecasting.
\icmamba outperforms other models across all time points, and its performance advantage increases over time. While all models show similar patterns of improvement in the first hour, \icmamba continues to achieve increasingly better RMSE scores through the 6-hour mark, reaching the lowest RMSE of 0.118. 
IC-TH maintains second-best performance throughout most of the timeline, followed by Autoformer. The Informer and TS-Mixer models show higher error rates, with their performance plateauing more quickly than the interval-censored approaches. This performance gap may illustrate the benefits of interval-censored modeling in engagement prediction tasks on real-world social media platforms, while the widening gap in RMSE scores over time suggests that \icmamba's improvements go beyond just interval-censored modeling, potentially indicating better long-range dependency learning.

\subsection{Dynamic Opinion-level Prediction--RQ3}
\label{sec:dynamic_pred}

This section simulates a real-world monitoring and forecasting scenario.
We analyze the opinion ``Climate change is a UN hoax'' from the SocialSense climate dataset. 
\cref{fig:dynamic_pred}(b)(c) demonstrates our dynamic prediction approach at opinion-level across multiple interaction types (likes, comments, emojis, and shares) over a 28-day period. We showcase two scenarios of initial data windows -- 168 hours (1 week), and 240 hours (10 days). 
Our model first processes the initial historical data window to establish baseline engagement patterns. As time progresses beyond these initial periods (marked by "Predictions Start" lines), the model continuously incorporates new engagement data to refine its predictions. The shaded areas around each prediction line represent the $95\%$ confidence intervals -- obtained from all previous prediction for this time -- providing a measure of prediction uncertainty over time.
We see that the uncertainty reduces as more initial data is available, suggesting that increased historical data improves the model's predictive accuracy. 

