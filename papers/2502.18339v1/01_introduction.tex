\section{Introduction}

For decades, the field of natural language processing (NLP) has relied on academic benchmarks and automated metrics (e.g., Accuracy, Brier Score \citep{brier1950verification}, BLEU \cite{papineni2002bleu}) to evaluate the performance of language models (LMs). These NLP benchmarks provide a standardized and efficient way to measure model capabilities such as machine translation, text summarization, and question answering \citep{wang2018glue,wang2019superglue,srivastava2022beyond,eleuther2023evalharness,wang2023decodingtrust}. However, the recent emergence of highly capable chat LMs such as GPT \citep{ouyang2022training,openai2023gpt4}, Llama \citep{touvron2023llama1, touvron2023llama2,dubey2024llama3herdmodels}, Gemini \citep{team2023gemini,reid2024gemini} and Claude \citep{anthropic2023claude3} has prompted a re-evaluation of how we assess LMs, with a growing emphasis on assessing LMs based on their ability to interact with and assist human users in real-world scenarios \citep{zheng2023lmsyschat1m,reuel2024openproblemstechnicalai}.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/schematics/meta_humans_evals_vs_nlp_benchmarks_schematic.pdf}
    \caption{\textbf{Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing (NLP) Benchmarks.} We evaluate chat language models on conversational tasks with human pairwise evaluations and on standard NLP benchmarks with automated metrics, then study whether scores on computationally inexpensive and fast NLP benchmarks are correlated with and predictive of expensive and time-intensive human evaluations.}
    \label{fig:schematic}
\end{figure*}


This shift towards human evaluations raises important questions about the relationship between NLP benchmarks and human evaluations of chat LMs.
Human evaluations have long been considered the gold standard \citep{gatt2018survey,van2019best,celikyilmaz2020evaluation,roller2020opendomainconversationalagentscurrent,van2021human}, but are expensive, time-intensive and noisy, in contrast with computationally cheaper, faster and precise benchmarks.
We explore the relationship between human evaluations and NLP benchmarks in pursuit of understanding what role, if any, benchmarks should play in the era of chat LMs.
We seek to answer two key research questions (Fig.~\ref{fig:schematic}):
%
\begin{enumerate}
    \item To what extent are human evaluations and NLP benchmarks correlated with one another?
    \item How well can NLP benchmarks predict human evaluations?
\end{enumerate}
%
To answer these questions, we conducted a large-scale study comparing human evaluations and NLP benchmarks using four Llama 2 Chat language models (LMs) \citep{touvron2023llama2}. For human evaluations, we constructed a large-scale dataset of single-turn and multi-turn prompts from a diverse taxonomy (Fig. \ref{fig:human_eval_prompt_taxonomy}) and collected high quality pairwise human preference data against GPT 3.5 \citep{ouyang2022training} from paid human annotators. For NLP benchmarks, we evaluated the same four Chat Llama 2 models on standard NLP benchmarks under established evaluation processes (metrics, prompting, 0-shot/few-shot, etc.). We analyzed pairwise correlations between NLP benchmark and human evaluations to identify which NLP benchmarks correlate highly with human evaluations and which do not.
We identified which human evaluations, if any, are uncorrelated with any NLP benchmarks.
We then pivoted to predict human evaluations from NLP benchmarks using overparameterized linear regressions and leave-one-out cross-validation, answering the extent to which NLP benchmarks can predict human evaluations. See Appendix~\ref{app:sec:related_work} for Related Work.

