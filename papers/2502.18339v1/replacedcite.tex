\section{Related Work}
\label{app:sec:related_work}

The evaluation of language models has a rich and constantly evolving history. Human evaluations have long been considered the gold standard ____, despite serious objections raised regarding the collection, analysis, and interpretation of human evaluation scores ____. Many classic NLP benchmark metrics, such as BLEU ____, NIST ____, ROUGE ____, and METEOR ____, were introduced on the premise that they correlate with human judgments. However, subsequent studies revealed that the relationship between automated metrics and human evaluations is often complex and not straightforward ____. Another prominent class of evaluation methods are based on machine learning models, e.g., word mover distance ____ and BERT-Score ____ that have since evolved into using chat LMs themselves as evaluators ____, albeit with limitations, e.g., ____.

The earliest investigations into the general relationship between NLP benchmark scores and human evaluations date back to ____, ____, and ____. In the context of natural language generation, ____ found that embedding-based automated metrics (e.g., BERT-Score ____ and BLEURT ____) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE ____ and BLEU ____). In the domain of natural language inference, ____ found that automated metrics do not appear to correlate with human judgment scores. However, the majority of these works predate the current era of chat LMs, which exhibit significantly more advanced capabilities compared to their predecessors. This new era motivates our work to investigate the relationship between NLP benchmarks and human evaluations when evaluating chat LMs.