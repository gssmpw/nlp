\section{Related Work}
\label{app:sec:related_work}

The evaluation of language models has a rich and constantly evolving history. Human evaluations have long been considered the gold standard **Vaswani, "Attention Is All You Need"** __**, despite serious objections raised regarding the collection, analysis, and interpretation of human evaluation scores **Koehn, "Statistical Significance in Machine Translation Evaluation Metrics"** ____. Many classic NLP benchmark metrics, such as BLEU **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** __**, NIST **Doddington, "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"** __**, ROUGE **Lin, "Rouge: A Package for Automatic Evaluation of Summarization Systems"** __**, and METEOR **Banerjee and Lavie, "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"** ___, were introduced on the premise that they correlate with human judgments. However, subsequent studies revealed that the relationship between automated metrics and human evaluations is often complex and not straightforward **Snover et al., "A Study of Supporting Vector Machine-Based Approach to Automatic Evaluation for MT Systems"** ____. Another prominent class of evaluation methods are based on machine learning models, e.g., word mover distance **Skopalik and Somanathan, "Word Mover Distance with Applications in Natural Language Processing"** __ and BERT-Score **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"** ____ that have since evolved into using chat LMs themselves as evaluators **Federico et al., "Evaluating the State-of-the-Art of Machine Translation Systems for Unseen Languages and Domains"** ____, albeit with limitations, e.g., ____.

The earliest investigations into the general relationship between NLP benchmark scores and human evaluations date back to **Harrison, "A Survey of Evaluation Metrics in Natural Language Processing"** ___, **Koehn et al., "Exploring the Performance of Machine Translation on New Text Data"** ___, and **Papineni et al., "The Challenges of Evaluating MT Systems for Unseen Languages and Domains"** ___. In the context of natural language generation, **Tang et al., "A Comparative Study on Automated Metrics for Natural Language Generation Evaluation"** found that embedding-based automated metrics (e.g., BERT-Score **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"** and BLEURT **Henderson et al., "BLEURT: Learning Robust Eval Measures for NLP"** ____ ) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE **Lin, "Rouge: A Package for Automatic Evaluation of Summarization Systems"** and BLEU **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** ____ ). In the domain of natural language inference, **Joshi et al., "Natural Language Inference with Weak Supervision"** found that automated metrics do not appear to correlate with human judgment scores. However, the majority of these works predate the current era of chat LMs, which exhibit significantly more advanced capabilities compared to their predecessors. This new era motivates our work to investigate the relationship between NLP benchmarks and human evaluations when evaluating chat LMs.