\section{Discussion}

In this paper, we explored the relationship between human evaluations and NLP benchmarks of chat-finetuned language models (chat LMs). Our work is motivated by the recent shift towards human evaluations as the primary means of assessing chat LM performance, and the desire to determine the role that NLP benchmarks should play.

Through a large-scale study of the Chat Llama 2 model family on a diverse set of human and NLP evaluations, we demonstrated that NLP benchmarks are generally well-correlated with human judgments of chat LM quality. However, our analysis also reveals some notable exceptions to this overall trend. In particular, we find that adversarial and safety-focused evaluations, as well as language assistance and open question answering tasks, exhibit weaker or negative correlations respectively with NLP benchmarks. We also explored predicting human evaluation scores from NLP evaluation scores using overparameterized linear regression models. Our results suggest that NLP benchmarks can indeed be used to predict aggregate human preferences, although we caution that the limited sample size and the assumptions of our models may limit the generalizability of these findings. Our results suggest that NLP benchmarks can serve as fast and cheap proxies of slower and expensive human evaluations in assessing chat LMs.

Additionally, our work highlights the need for further research into NLP evaluations that can effectively capture important aspects of LM behavior, such as safety, robustness to adversarial inputs, and performance on complex, open-ended tasks. It is possible that new NLP benchmarks can provide signals on these topics, e.g., \citep{wang2023decodingtrust}. Of particular interest is developing human-interpretable and scaling-predictable evaluation processes, e.g., \citep{schaeffer2024emergent, ruan2024observational,schaeffer2024predictingdownstreamcapabilitiesfrontier}. Developing and refining such evaluation methods \citep{madaan2024quantifyingvarianceevaluationbenchmarks}, as well as detecting whether evaluations scores faithfully capture models' true performance \citep{oren2023proving,schaeffer2023pretrainingtestsetneed,roberts2023cutoff,jiang2024investigatingdatacontaminationpretraining,zhang2024careful,duan2024uncoveringlatentmemoriesassessing} will be crucial for ensuring that LMs are safe, reliable, and beneficial as they become increasingly integrated into society.

% In conclusion, our study provides insights into the relationship between human evaluations and NLP benchmarks of chat language models. By leveraging the complementary strengths of both human and NLP benchmarks, we can build a more complete understanding of LM capabilities and behaviors, ultimately enabling the development of models more capable, trustworthy, and beneficial to society.
