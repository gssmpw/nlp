\section{Related Work}
\label{app:sec:related_work}

The evaluation of language models has a rich and constantly evolving history. Human evaluations have long been considered the gold standard \citep{gatt2018survey,van2019best,celikyilmaz2020evaluation,roller2020opendomainconversationalagentscurrent,van2021human}, despite serious objections raised regarding the collection, analysis, and interpretation of human evaluation scores \citep{novikova2018rankme,howcroft2020twenty, bowman2021fixbenchmarkingnaturallanguage,karpinska2021perilsusingmechanicalturk, clark2021all,smith2022humanevaluationconversationsopen, gehrmann2023repairing,finch2023dontforgetabcsevaluating}. Many classic NLP benchmark metrics, such as BLEU \citep{papineni2002bleu}, NIST \citep{doddington2002nist}, ROUGE \citep{lin2004rouge}, and METEOR \citep{banerjee2005meteor}, were introduced on the premise that they correlate with human judgments. However, subsequent studies revealed that the relationship between automated metrics and human evaluations is often complex and not straightforward \citep{liu2016hownot, novikova2017we, reiter2018structured, karpinska2021perilsusingmechanicalturk}. Another prominent class of evaluation methods are based on machine learning models, e.g., word mover distance \citep{kusner2015word} and BERT-Score \citep{zhang2019bertscore} that have since evolved into using chat LMs themselves as evaluators \citep{wang2023chatgpt,zheng2024judging, chiang2023largelanguagemodelsalternative,chan2023chatevalbetterllmbasedevaluators,bavaresco2024llms,fu2024gptscore}, albeit with limitations, e.g., \citep{dorner2024limits,szymanski2024limitationsllmasajudgeapproachevaluating, thakur2024judging}.

The earliest investigations into the general relationship between NLP benchmark scores and human evaluations date back to \citet{bangalore2000evaluation}, \citet{belz2006comparing}, and \citet{liu2016hownot}. In the context of natural language generation, \citet{clinciu2021study} found that embedding-based automated metrics (e.g., BERT-Score \citep{zhang2019bertscore} and BLEURT \citet{sellam2020bleurt}) correlate more strongly with human judgments compared to word-overlap metrics (e.g., ROUGE \citep{lin2004rouge} and BLEU \citep{papineni2002bleu}). In the domain of natural language inference, \citet{schuff2021does} found that automated metrics do not appear to correlate with human judgment scores. However, the majority of these works predate the current era of chat LMs, which exhibit significantly more advanced capabilities compared to their predecessors. This new era motivates our work to investigate the relationship between NLP benchmarks and human evaluations when evaluating chat LMs.