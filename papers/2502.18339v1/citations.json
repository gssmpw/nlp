[
  {
    "index": 0,
    "papers": [
      {
        "key": "gatt2018survey",
        "author": "Gatt, Albert and Krahmer, Emiel",
        "title": "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation"
      },
      {
        "key": "van2019best",
        "author": "Van Der Lee, Chris and Gatt, Albert and Van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel",
        "title": "Best practices for the human evaluation of automatically generated text"
      },
      {
        "key": "celikyilmaz2020evaluation",
        "author": "Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng",
        "title": "Evaluation of text generation: A survey"
      },
      {
        "key": "roller2020opendomainconversationalagentscurrent",
        "author": "Stephen Roller and Y-Lan Boureau and Jason Weston and Antoine Bordes and Emily Dinan and Angela Fan and David Gunning and Da Ju and Margaret Li and Spencer Poff and Pratik Ringshia and Kurt Shuster and Eric Michael Smith and Arthur Szlam and Jack Urbanek and Mary Williamson",
        "title": "Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions"
      },
      {
        "key": "van2021human",
        "author": "van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Krahmer, Emiel",
        "title": "Human evaluation of automatically generated text: Current trends and best practice guidelines"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "novikova2018rankme",
        "author": "Novikova, Jekaterina and Du{\\v{s}}ek, Ond{\\v{r}}ej and Rieser, Verena",
        "title": "RankME: Reliable Human Ratings for Natural Language Generation"
      },
      {
        "key": "howcroft2020twenty",
        "author": "Howcroft, David M and Belz, Anya and Clinciu, Miruna and Gkatzia, Dimitra and Hasan, Sadid A and Mahamood, Saad and Mille, Simon and Van Miltenburg, Emiel and Santhanam, Sashank and Rieser, Verena",
        "title": "Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions"
      },
      {
        "key": "bowman2021fixbenchmarkingnaturallanguage",
        "author": "Samuel R. Bowman and George E. Dahl",
        "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?"
      },
      {
        "key": "karpinska2021perilsusingmechanicalturk",
        "author": "Marzena Karpinska and Nader Akoury and Mohit Iyyer",
        "title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation"
      },
      {
        "key": "clark2021all",
        "author": "Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A",
        "title": "All that's' human'is not gold: Evaluating human evaluation of generated text"
      },
      {
        "key": "smith2022humanevaluationconversationsopen",
        "author": "Eric Michael Smith and Orion Hsu and Rebecca Qian and Stephen Roller and Y-Lan Boureau and Jason Weston",
        "title": "Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents"
      },
      {
        "key": "gehrmann2023repairing",
        "author": "Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault",
        "title": "Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text"
      },
      {
        "key": "finch2023dontforgetabcsevaluating",
        "author": "Sarah E. Finch and James D. Finch and Jinho D. Choi",
        "title": "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "papineni2002bleu",
        "author": "Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing",
        "title": "Bleu: a method for automatic evaluation of machine translation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "doddington2002nist",
        "author": "Doddington, George",
        "title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lin2004rouge",
        "author": "Lin, Chin-Yew",
        "title": "Rouge: A package for automatic evaluation of summaries"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "banerjee2005meteor",
        "author": "Banerjee, Satanjeev and Lavie, Alon",
        "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2016hownot",
        "author": "Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle",
        "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation"
      },
      {
        "key": "novikova2017we",
        "author": "Novikova, Jekaterina and Du{\\v{s}}ek, Ond{\\v{r}}ej and Curry, Amanda Cercas and Rieser, Verena",
        "title": "Why We Need New Evaluation Metrics for NLG"
      },
      {
        "key": "reiter2018structured",
        "author": "Reiter, Ehud",
        "title": "A structured review of the validity of BLEU"
      },
      {
        "key": "karpinska2021perilsusingmechanicalturk",
        "author": "Marzena Karpinska and Nader Akoury and Mohit Iyyer",
        "title": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kusner2015word",
        "author": "Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian",
        "title": "From word embeddings to document distances"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2019bertscore",
        "author": "Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav",
        "title": "Bertscore: Evaluating text generation with bert"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2023chatgpt",
        "author": "Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie",
        "title": "Is chatgpt a good nlg evaluator? a preliminary study"
      },
      {
        "key": "zheng2024judging",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others",
        "title": "Judging llm-as-a-judge with mt-bench and chatbot arena"
      },
      {
        "key": "chiang2023largelanguagemodelsalternative",
        "author": "Cheng-Han Chiang and Hung-yi Lee",
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?"
      },
      {
        "key": "chan2023chatevalbetterllmbasedevaluators",
        "author": "Chi-Min Chan and Weize Chen and Yusheng Su and Jianxuan Yu and Wei Xue and Shanghang Zhang and Jie Fu and Zhiyuan Liu",
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"
      },
      {
        "key": "bavaresco2024llms",
        "author": "Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fern{\\'a}ndez, Raquel and Gatt, Albert and Ghaleb, Esam and Giulianelli, Mario and Hanna, Michael and Koller, Alexander and others",
        "title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks"
      },
      {
        "key": "fu2024gptscore",
        "author": "Fu, Jinlan and Ng, See Kiong and Jiang, Zhengbao and Liu, Pengfei",
        "title": "GPTScore: Evaluate as You Desire"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "dorner2024limits",
        "author": "Dorner, Florian E and Nastl, Vivian Y and Hardt, Moritz",
        "title": "Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data"
      },
      {
        "key": "szymanski2024limitationsllmasajudgeapproachevaluating",
        "author": "Annalisa Szymanski and Noah Ziems and Heather A. Eicher-Miller and Toby Jia-Jun Li and Meng Jiang and Ronald A. Metoyer",
        "title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks"
      },
      {
        "key": "thakur2024judging",
        "author": "Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke",
        "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "bangalore2000evaluation",
        "author": "Bangalore, Srinivas and Rambow, Owen and Whittaker, Steve",
        "title": "Evaluation metrics for generation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "belz2006comparing",
        "author": "Belz, Anja and Reiter, Ehud",
        "title": "Comparing automatic and human evaluation of NLG systems"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2016hownot",
        "author": "Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle",
        "title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "clinciu2021study",
        "author": "Clinciu, Miruna and Eshghi, Arash and Hastie, Helen",
        "title": "A Study of Automatic Metrics for the Evaluation of Natural Language Explanations"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2019bertscore",
        "author": "Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav",
        "title": "Bertscore: Evaluating text generation with bert"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "sellam2020bleurt",
        "author": "Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P",
        "title": "BLEURT: Learning robust metrics for text generation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lin2004rouge",
        "author": "Lin, Chin-Yew",
        "title": "Rouge: A package for automatic evaluation of summaries"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "papineni2002bleu",
        "author": "Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing",
        "title": "Bleu: a method for automatic evaluation of machine translation"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "schuff2021does",
        "author": "Schuff, Hendrik and Yang, Hsiu-Yu and Adel, Heike and Vu, Ngoc Thang",
        "title": "Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings"
      }
    ]
  }
]