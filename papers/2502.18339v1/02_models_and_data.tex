\section{Methods: Models, Human Evaluations and NLP Benchmarks}
\label{sec:methods}

We briefly outline our methodology here; for additional information, please see Appendix \ref{app:sec:experimental_methodology}.

\begin{figure*}[t!]
    \centering
    % \includegraphics[width=0.55\textwidth]{figures/correlations/correlation_by_human_eval_area_split_corrmethod.pdf}%
    \includegraphics[width=0.9\textwidth]{figures/correlations/correlation_by_human_eval_area_cat_split_corrmethod.pdf}
    \caption{\textbf{Distributions of Correlations between Human Evaluations and NLP benchmarks.} Macroscopically, for each human evaluation area, Chat LM scores are typically highly correlated with NLP benchmarks. Mesoscopically, human and NLP benchmarks remain positively correlated, with notable exceptions: Adversarial Dishonesty, Adversarial Harmfulness and Safety are anticorrelated with most NLP benchmarks, and Language Assistance and Open QA are uncorrelated.}
    \label{fig:corr:human_eval_area}
\end{figure*}

\textbf{Models.} We evaluated four Chat Llama 2 models with 7, 13, 34, and 70 billion parameters pre-trained on 2 trillion tokens and finetuned using supervised finetuning \citep{sanh2021multitask, chung2022scaling, longpre2023flan} and reinforcement learning from human feedback \citep{christiano2017deep, ziegler2019fine, stiennon2020learning}.
We chose the Llama 2 family because, at the time we collected our data, it contained leading open-access chat-finetuned models spanning multiple scales with minimal variations, ensuring a consistent foundation for our investigations.

\textbf{Human Evaluations: Single Turn \& Multi-Turn.} In this work, our aim was specifically to identify which NLP benchmark scores are predictive of human preferences on open-ended prompts representative of real-world chat model usage. We chose this approach to maximize the ecological validity and generalizability of the findings to real-world use cases. For a concrete example, we may want our chat language models (LMs) to excel at providing bespoke career advice; which NLP benchmarks provide useful signals for whether models are improving at such tasks? 



To answer such questions, we created a taxonomy of single-turn and multi-turn interactions (Fig. \ref{fig:human_eval_prompt_taxonomy}) between chat LMs and humans. For single-turn interactions, we generated a diverse set of prompts spanning common areas of interest: Factual Questions, Procedural Questions, Language Assistance, Writing \& Content Creation, Dialogue, Code, Reasoning, Recommendations / Brainstorming and Safety, with nested categories and subcategories. 
For multi-turn prompts, non-annotator humans were asked to have conversations (3 to 15 turns long) with all models on similar topics of interest: Factual Questions, Procedural Questions, Language Assistance, Writing \& Content Creation, Summarization \& Editing, General Dialogue, Reasoning and Recommendations / Brainstorming. This taxonomy was chosen to broadly cover common use-cases of Chat LMs.
Example prompts include: ``What is the tallest mountain in the world?" (Factual Question); ``How do I make minestrone soup?" (Procedural Question); ``Please make this sentence more friendly: I need you to stop parking in my space" (Language Assistance). 
% ``Write me a poem about getting to the weekend after a long day at work" (Writing \& Content Creation).
See Appendix \ref{app:sec:experimental_methodology:human_evals} for more information.

We paid human annotators to evaluate each of the four Chat Llama 2 models against ChatGPT 3.5 \citep{ouyang2022training} (gpt-3.5-0301) on a dataset of single-turn and multi-turn prompts (Fig \ref{fig:human_eval_prompt_taxonomy}). We chose gpt-3.5-0301 because, at the time this data was collected, gpt-3.5-0301 was a good balance of three desirable properties for our study: performant, cheap, and stable.
For each pair of conversations (one conversation with Chat Llama responses and the other with ChatGPT responses), at least three unique human annotators independently indicated which conversation was preferred using a \citet{likert1932technique} scale from 1 to 7, where 1 denotes the Chat Llama model was strongly preferred and 7 denotes gpt-3.5-0301 was strongly preferred.
Across the 11291 single-turn samples and 2081 multi-turn samples, we had at least 3 unique human annotators evaluate each per pairwise comparison, with 2104 unique annotators overall. 
We averaged the annotators' scores for each pairwise comparison to give us an average human evaluation score per datum.



\textbf{Natural Language Processing (NLP) Benchmarks.} We evaluated the four Chat Llama 2 models on large-scale and commonly-used NLP benchmarks: AGI Eval \citep{zhong2023agieval}, AI2 Reasoning Challenge (ARC; both Easy and Hard) \citep{clark2018arc}, BIG Bench Hard \citep{srivastava2022beyond,suzgun2022challenging} BoolQ \citep{clark2019boolq}, CommonSenseQA \citep{talmor2019commonsenseqa}, COPA \citep{roemmele2011choice}, DROP \citep{dua2019drop}, GSM8K \citep{cobbe2021training}, HellaSwag \citep{zellers2019hellaswag}, HumanEval \citep{chen2021evaluatinglargelanguagemodels}, InverseScaling \citep{mckenzie2022inverse,mckenzie2022round1,mckenzie2022round2}, MBPP \citep{austin2021program}, MMLU \citep{hendrycks2020measuring}, Natural Questions \citep{kwiatkowski2019naturalquestions}, OpenbookQA \citep{mihaylov2018openbookqa}, PIQA \citep{bisk2020piqa}, QuAC \citep{choi2018quac}, RACE \citep{lai2017race}, SIQA \citep{sap2019social}, SQUAD \citep{rajpurkar2016squad}, TLDR \citep{volske2017tl}, TriviaQA \citep{joshi2017triviaqa}, WinoGrande \citep{sakaguchi2021winogrande} and XSum \citep{narayan2018xsum}. Some of these benchmarks (e.g., MMLU) contain subsets (e.g., Jurisprudence) that we do not aggregate over.
These tasks cover commonsense reasoning, world knowledge, reading comprehension, coding and more. We used standard evaluation processes for all academic benchmarks including prompt formatting, metrics, 0-shot/few-shot, etc.
This structured approach facilitates an exhaustive examination of model performances across varied metrics.
For more information, see Appendix \ref{app:sec:experimental_methodology:nlp_benchmarks}.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/correlations/academic_benchmark_subset_by_pearson_avg_over_human.pdf}
    \caption{\textbf{NLP Benchmarks Ranked by Average Pearson Correlation over All Human Evaluations.} Certain benchmarks have higher correlations with human evaluations, including a subset of MMLU, a subset of BIG Bench Hard, HellaSwag, ARC, RACE, PIQA, NaturalQuestions, QuAC, and CommonSenseQA. Other benchmarks were weakly or uncorrelated with human evaluations: ETHOS, Kth Sentence, Inverse Scaling (with the exception of Resisting Correction Classification), OpenBookQA, COPA, SciBench and SIQA.}\label{fig:corr:academic_vs_correlation_split_corrmethod}
\end{figure*}

\textbf{Scores for Subsequent Analyses.} For each dataset and evaluation process, we average each model's scores across all samples, yielding two matrices of scores:
%
$$X_{\text{NLP}} \in \mathbb{R}^{160 \times 4} \quad \quad \quad \quad X_{\text{Human}} \in \mathbb{R}^{55 \times 4}$$
%
Here, $4$ is the number of models, $160$ is the number of NLP benchmarks per model and $55$ is the number of human evaluation area-category-subcategory scores per model. We subsequently study the correlations between $X_{\text{NLP}}$ and $X_{\text{Human}}$, then test how well $X_{\text{NLP}}$ can predict $X_{\text{Human}}$.

