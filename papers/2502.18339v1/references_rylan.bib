@inproceedings{Vaswani+2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{vivek2023anchor,
      title={Anchor Points: Benchmarking Models with Much Fewer Examples}, 
      author={Rajan Vivek and Kawin Ethayarajh and Diyi Yang and Douwe Kiela},
      year={2023},
      eprint={2309.08638},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{clark2018arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{likert1932technique,
  title={A technique for the measurement of attitudes.},
  author={Likert, Rensis},
  journal={Archives of psychology},
  year={1932}
}

@article{galton1877heredity,
  title = {Typical Laws of Heredity},
  author = {Francis Galton},
  journal = {Nature},
  volume = {15},
  number = {388},
  pages = {492--495},
  year = {1877},
  doi = {10.1038/015492a0},
  url = {https://doi.org/10.1038/015492a0}
}

@article{kendall1938new,
  title={A new measure of rank correlation},
  author={Kendall, Maurice G},
  journal={Biometrika},
  volume={30},
  number={1/2},
  pages={81--93},
  year={1938},
  publisher={JSTOR}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{hosking2023human,
  title={Human Feedback is not Gold Standard},
  author={Hosking, Tom and Blunsom, Phil and Bartolo, Max},
  journal={arXiv preprint arXiv:2309.16349},
  year={2023}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}




@article{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, MingWei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={NAACL},
  year={2019}
}

@misc{anthropic2023claude3,
  title = {Model Card and Evaluations for Claude Models},
  author = {Anthropic},
  year = {2023}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}



@article{kwiatkowski2019naturalquestions,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}


@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}


@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}


@article{joshi2017triviaqa,
       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},
                 Daniel and {Zettlemoyer}, Luke},
        title = "{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}",
      journal = {arXiv e-prints},
         year = 2017,
          eid = {arXiv:1705.03551},
        pages = {arXiv:1705.03551},
archivePrefix = {arXiv},
       eprint = {1705.03551},
}


@inproceedings{mihaylov2018openbookqa,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{sap2019social,
  title={Social IQa: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}


@inproceedings{choi2018quac,
    title = "{Q}u{AC}: Question Answering in Context",
    author = "Choi, Eunsol  and
      He, He  and
      Iyyer, Mohit  and
      Yatskar, Mark  and
      Yih, Wen-tau  and
      Choi, Yejin  and
      Liang, Percy  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1241",
    doi = "10.18653/v1/D18-1241",
    pages = "2174--2184",
    abstract = "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at \url{http://quac.ai}.",
}


@misc{zhong2023agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@inproceedings{talmor2019commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    archivePrefix = "arXiv",
    eprint        = "1811.00937",
    primaryClass  = "cs",
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI Spring Symposium Series},
  year={2011}
}

@misc{mckenzie2022inverse,
    title={The Inverse Scaling Prize},
    url={https://github.com/inverse-scaling/prize},
    author={McKenzie, Ian and Lyzhov, Alexander and Parrish, Alicia and Prabhu, Ameya and Mueller, Aaron and Kim, Najoung and Bowman, Sam and Perez, Ethan},
    year={2022}
}

@misc{mckenzie2022round1,
    title={Inverse Scaling Prize: First Round Winners},
    url={https://irmckenzie.co.uk/round1},
    author={McKenzie, Ian and Lyzhov, Alexander and Parrish, Alicia and Prabhu, Ameya and Mueller, Aaron and Kim, Najoung and Bowman, Sam and Perez, Ethan},
    year={2022}
}

@misc{mckenzie2022round2,
    title={Inverse Scaling Prize: Second Round Winners},
    url={https://irmckenzie.co.uk/round2},
    author={McKenzie, Ian and Lyzhov, Alexander and Parrish, Alicia and Prabhu, Ameya and Mueller, Aaron and Kim, Najoung and Bowman, Sam and Perez, Ethan},
    year={2023}
}

@article{narayan2018xsum,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08745}
}

@article{dua2019drop,
  title={DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs},
  author={Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1903.00161},
  year={2019}
}

@article{schaeffer2023double,
  title={Double descent demystified: Identifying, interpreting \& ablating the sources of a deep learning puzzle},
  author={Schaeffer, Rylan and Khona, Mikail and Robertson, Zachary and Boopathy, Akhilan and Pistunova, Kateryna and Rocks, Jason W and Fiete, Ila Rani and Koyejo, Oluwasanmi},
  journal={arXiv preprint arXiv:2303.14151},
  year={2023}
}

@inproceedings{schaeffer2023divergence,
  title={Divergence at the interpolation threshold: Identifying, interpreting \& ablating the sources of a deep learning puzzle},
  author={Schaeffer, Rylan and Robertson, Zachary and Boopathy, Akhilan and Khona, Mikail and Fiete, Ila and Gromov, Andrey and Koyejo, Sanmi},
  booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
  year={2023}
}


@inproceedings{schaeffer2024doubledescentdemystified,
  author = {Schaeffer, Rylan and Robertson, Zachary and Boopathy, Akhilan and Khona, Mikail and Pistunova, Kateryna and Rocks, Jason W. and Fiete, Ila R. and Gromov, Andrey and Koyejo, Sanmi},
  title = {Double Descent Demystified},
  abstract = {Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle},
  booktitle = {ICLR Blogposts 2024},
  year = {2024},
  date = {May 7, 2024},
  note = {https://d2jud02ci9yv69.cloudfront.net/2024-05-07-double-descent-demystified-54/blog/double-descent-demystified/},
  url  = {https://d2jud02ci9yv69.cloudfront.net/2024-05-07-double-descent-demystified-54/blog/double-descent-demystified/}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}


@inproceedings{kiela2021dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = {"https://aclanthology.org/2021.naacl-main.324"},
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124",
    abstract = "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
}

@inproceedings{volske2017tl,
  title={Tl; dr: Mining reddit to learn automatic summarization},
  author={V{\"o}lske, Michael and Potthast, Martin and Syed, Shahbaz and Stein, Benno},
  booktitle={Proceedings of the Workshop on New Frontiers in Summarization},
  pages={59--63},
  year={2017}
}



@inproceedings{bowman2021whatwill,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.385",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias.",
}


@article{curth2024u,
  title={A u-turn on double descent: Rethinking parameter counting in statistical learning},
  author={Curth, Alicia and Jeffares, Alan and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{lai2017race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1082",
    doi = "10.18653/v1/D17-1082",
    pages = "785--794",
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{touvron2023llama1,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2023ph1p5,
  title={Textbooks Are All You Need II: \textbf{phi-1.5} technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{penedo2023falcon,
  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal={arXiv preprint arXiv:2306.01116},
  eprint={2306.01116},
  eprinttype = {arXiv},
  url={https://arxiv.org/abs/2306.01116},
  year={2023}
}

@article{gunasekar2023phi1,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}


@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{mosaicml2023mpt,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    Commercially Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-05-05},
    urldate   = {2023-05-05}
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tow2023stablelmalphav2, 
      url={[https://github.com/Stability-AI/StableLM](https://github.com/Stability-AI/StableLMw)},
      title={StableLM Alpha v2 Models},
      year={2023},
      author={Tow, Jonathan}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}


@article{vania2021comparing,
  title={Comparing test sets with item response theory},
  author={Vania, Clara and Htut, Phu Mon and Huang, William and Mungra, Dhara and Pang, Richard Yuanzhe and Phang, Jason and Liu, Haokun and Cho, Kyunghyun and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2106.00840},
  year={2021}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{lalor2016building,
  title={Building an evaluation scale using item response theory},
  author={Lalor, John P and Wu, Hao and Yu, Hong},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing},
  volume={2016},
  pages={648},
  year={2016},
  organization={NIH Public Access}
}

@article{peyrard2021better,
  title={Better than average: Paired evaluation of NLP systems},
  author={Peyrard, Maxime and Zhao, Wei and Eger, Steffen and West, Robert},
  journal={arXiv preprint arXiv:2110.10746},
  year={2021}
}

@inproceedings{rodriguez2021evaluation,
  title={Evaluation examples are not equally informative: How should that change NLP leaderboards?},
  author={Rodriguez, Pedro and Barrow, Joe and Hoyle, Alexander Miserlis and Lalor, John P and Jia, Robin and Boyd-Graber, Jordan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4486--4503},
  year={2021}
}

@article{varshney2022ildae,
  title={ILDAE: Instance-level difficulty analysis of evaluation data},
  author={Varshney, Neeraj and Mishra, Swaroop and Baral, Chitta},
  journal={arXiv preprint arXiv:2203.03073},
  year={2022}
}

@article{varshney2022model,
  title={Model cascading: Towards jointly improving efficiency and accuracy of NLP systems},
  author={Varshney, Neeraj and Baral, Chitta},
  journal={arXiv preprint arXiv:2210.05528},
  year={2022}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@article{blackwell1947conditional,
  title={Conditional expectation and unbiased sequential estimation},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={105--110},
  year={1947},
  publisher={JSTOR}
}

@incollection{rao1992information,
  title={Information and the accuracy attainable in the estimation of statistical parameters},
  author={Rao, C Radhakrishna},
  booktitle={Breakthroughs in Statistics: Foundations and basic theory},
  pages={235--247},
  year={1992},
  publisher={Springer}
}

@article{nelson1987control,
  title={On control variate estimators},
  author={Nelson, Barry L},
  journal={Computers \& Operations Research},
  volume={14},
  number={3},
  pages={219--225},
  year={1987},
  publisher={Elsevier}
}

@article{rothery1982use,
  title={The use of control variates in Monte Carlo estimation of power},
  author={Rothery, P},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={31},
  number={2},
  pages={125--129},
  year={1982},
  publisher={Oxford University Press}
}

@article{lemieux2014control,
  title={Control variates},
  author={Lemieux, Christiane},
  journal={Wiley StatsRef: Statistics Reference Online},
  pages={1--8},
  year={2014},
  publisher={Wiley Online Library}
}

@article{botev2017variance,
  title={Variance reduction},
  author={Botev, Zdravko and Ridder, Ad},
  journal={Wiley statsRef: Statistics reference online},
  pages={1--6},
  year={2017},
  publisher={Wiley Hoboken, NJ, USA}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{thakur2024judging,
  title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges},
  author={Thakur, Aman Singh and Choudhary, Kartik and Ramayapally, Venkat Srinik and Vaidyanathan, Sankaran and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2406.12624},
  year={2024}
}


@misc{szymanski2024limitationsllmasajudgeapproachevaluating,
      title={Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks}, 
      author={Annalisa Szymanski and Noah Ziems and Heather A. Eicher-Miller and Toby Jia-Jun Li and Meng Jiang and Ronald A. Metoyer},
      year={2024},
      eprint={2410.20266},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2410.20266}, 
}

@inproceedings{fu2024gptscore,
  title={GPTScore: Evaluate as You Desire},
  author={Fu, Jinlan and Ng, See Kiong and Jiang, Zhengbao and Liu, Pengfei},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={6556--6576},
  year={2024}
}


@article{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating LLMs by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}


@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@inproceedings{schuff2021does,
  title={Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings},
  author={Schuff, Hendrik and Yang, Hsiu-Yu and Adel, Heike and Vu, Ngoc Thang},
  booktitle={Proceedings of the Fourth Blackbox NLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  pages={26--41},
  year={2021}
}


@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  booktitle={International Conference on Machine Learning},
  pages={22631--22648},
  year={2023},
  organization={PMLR}
}


@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}


@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}



@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950},
  publisher={American Meteorological Society}
}

@article{spearman1904proof,
  title={The Proof and Measurement of Association between Two Things},
  author={Spearman, C},
  journal={The American Journal of Psychology},
  volume={15},
  number={1},
  pages={72--101},
  year={1904}
}


@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@article{pace1997sparse,
  title={Sparse spatial autoregressions},
  author={Pace, R Kelley and Barry, Ronald},
  journal={Statistics \& Probability Letters},
  volume={33},
  number={3},
  pages={291--297},
  year={1997},
  publisher={Elsevier}
}

@article{efron2004least,
  title={Least angle regression},
  author={Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  year={2004}
}

 @misc{gochiashvili_2023_who,
	title={World Health Organization Life Expectancy (Fixed)},
    author={Gochiashvili, Lasha},
	url={https://www.kaggle.com/ds/3065197},
    year={2023},
	DOI={10.34740/KAGGLE/DS/3065197},
 }

 @article{marchenko1967distribution,
  title={Distribution of eigenvalues for some sets of random matrices},
  author={Marchenko, Vladimir Alexandrovich and Pastur, Leonid Andreevich},
  journal={Matematicheskii Sbornik},
  volume={114},
  number={4},
  pages={507--536},
  year={1967},
  publisher={Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}


@article{advani2020high,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Advani, Madhu S and Saxe, Andrew M and Sompolinsky, Haim},
  journal={Neural Networks},
  volume={132},
  pages={428--446},
  year={2020},
  publisher={Elsevier}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{spigler2018jamming,
  title={A jamming transition from under-to over-parametrization affects loss landscape and generalization},
  author={Spigler, Stefano and Geiger, Mario and d'Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1810.09665},
  year={2018}
}

@article{opper1995statistical,
  title={Statistical mechanics of learning: Generalization},
  author={Opper, Manfred},
  journal={The handbook of brain theory and neural networks},
  pages={922--925},
  year={1995},
  publisher={MIT Pres) (Cambridge, MA}
}


@article{rocks2022memorizing,
  title={Memorizing without overfitting: Bias, variance, and interpolation in overparameterized models},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical Review Research},
  volume={4},
  number={1},
  pages={013201},
  year={2022},
  publisher={APS}
}

@article{rocks2022bias,
  title={Bias-variance decomposition of overparameterized regression with random linear features},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={Physical Review E},
  volume={106},
  number={2},
  pages={025304},
  year={2022},
  publisher={APS}
}

@article{rocks2021geometry,
  title={The Geometry of Over-parameterized Regression and Adversarial Perturbations},
  author={Rocks, Jason W and Mehta, Pankaj},
  journal={arXiv preprint arXiv:2103.14108},
  year={2021}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}


@article{poggio2019double,
  title={Double descent in the condition number},
  author={Poggio, Tomaso and Kur, Gil and Banburski, Andrzej},
  journal={arXiv preprint arXiv:1912.06190},
  year={2019}
}

@article{henighan2023superposition,
  title={Double descent in the condition number},
  author={Henighan, Tom and Carter, Shan and Hume, Tristan and Elhage, Nelson and Lasenby, Robert and Fort, Stanislav and Schiefer, Nicholas and Olah, Christopher},
  journal={Transformer Circuits Thread},
  url={https://transformer-circuits.pub/2023/toy-double-descent/index.html},
  year={2023}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@inproceedings{bordelon2020spectrum,
  title={Spectrum dependent learning curves in kernel regression and wide neural networks},
  author={Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle={International Conference on Machine Learning},
  pages={1024--1034},
  year={2020},
  organization={PMLR}
}


@article{chen2021multiple,
  title={Multiple descent: Design your own generalization curve},
  author={Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8898--8912},
  year={2021}
}

@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}

@article{vallet1989hebb,
  title={The Hebb rule for learning linearly separable Boolean functions: learning and generalization},
  author={Vallet, F},
  journal={Europhysics Letters},
  volume={8},
  number={8},
  pages={747},
  year={1989},
  publisher={IOP Publishing}
}

@inproceedings{duin2000classifiers,
  title={Classifiers in almost empty spaces},
  author={Duin, Robert PW},
  booktitle={Proceedings 15th International Conference on Pattern Recognition. ICPR-2000},
  volume={2},
  pages={1--7},
  year={2000},
  organization={IEEE}
}

@article{ren2024safetywashing,
  title={Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?},
  author={Ren, Richard and Basart, Steven and Khoja, Adam and Gatti, Alice and Phan, Long and Yin, Xuwang and Mazeika, Mantas and Pan, Alexander and Mukobi, Gabriel and Kim, Ryan H and others},
  journal={arXiv preprint arXiv:2407.21792},
  year={2024}
}


@misc{zheng2023lmsyschat1m,
      title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric. P Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
      year={2023},
      eprint={2309.11998},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzmán and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Govind Thattai and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Maria Tsimpoukelli and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vítor Albiero and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{duan2024uncoveringlatentmemoriesassessing,
      title={Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Frontier AI Models}, 
      author={Sunny Duan and Mikail Khona and Abhiram Iyer and Rylan Schaeffer and Ila R Fiete},
      year={2024},
      eprint={2406.14549},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.14549}, 
}

@article{von1988moments,
  title={Moments for the inverted Wishart distribution},
  author={Von Rosen, Dietrich},
  journal={Scandinavian Journal of Statistics},
  pages={97--109},
  year={1988},
  publisher={JSTOR}
}



@article{liang2020just,
  title={Just Interpolate: Kernel “Ridgeless” Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={The Annals of Statistics},
  volume={48},
  number={3},
  pages={1329--1347},
  year={2020}
}

@misc{bach2023highdimensional,
      title={High-dimensional analysis of double descent for linear regression with random projections}, 
      author={Francis Bach},
      year={2023},
      eprint={2303.01372},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wang2023decodingtrust,
  title={Decodingtrust: A comprehensive assessment of trustworthiness in gpt models},
  author={Wang, Boxin and Chen, Weixin and Pei, Hengzhi and Xie, Chulin and Kang, Mintong and Zhang, Chenhui and Xu, Chejian and Xiong, Zidi and Dutta, Ritik and Schaeffer, Rylan and others},
  journal={arXiv preprint arXiv:2306.11698},
  year={2023}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@misc{eleuther2023evalharness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}



@article{belkin2020twomodels,
	doi = {10.1137/20m1336072},
	url = {https://doi.org/10.1137%2F20m1336072},
	year = 2020,
	month = {jan},
	publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
	volume = {2},
	number = {4},
	pages = {1167--1180},
	author = {Mikhail Belkin and Daniel Hsu and Ji Xu},
	title = {Two Models of Double Descent for Weak Features},
	journal = {{SIAM} Journal on Mathematics of Data Science}
}


@article{krogh1992generalization,
  title={Generalization in a linear perceptron in the presence of noise},
  author={Krogh, Anders and Hertz, John A},
  journal={Journal of Physics A: Mathematical and General},
  volume={25},
  number={5},
  pages={1135},
  year={1992},
  publisher={IOP Publishing}
}

@article{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  journal={arXiv preprint arXiv:1206.6389},
  year={2012}
}

@inproceedings{carlini2021poisoning,
  title={Poisoning the unlabeled dataset of $\{$Semi-Supervised$\}$ learning},
  author={Carlini, Nicholas},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={1577--1592},
  year={2021}
}

@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}

@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@inproceedings{schuster2021you,
  title={You autocomplete me: Poisoning vulnerabilities in neural code completion},
  author={Schuster, Roei and Song, Congzheng and Tromer, Eran and Shmatikov, Vitaly},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={1559--1575},
  year={2021}
}

@article{wallace2020concealed,
  title={Concealed data poisoning attacks on NLP models},
  author={Wallace, Eric and Zhao, Tony Z and Feng, Shi and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.12563},
  year={2020}
}

@incollection{kurakin2018adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC}
}


@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}


@article{carlini2021contrastive,
  title={Poisoning and backdooring contrastive learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  journal={arXiv preprint arXiv:2106.09667},
  year={2021}
}

@article{steinhardt2017certified,
  title={Certified defenses for data poisoning attacks},
  author={Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{athalye2018synthesizing,
  title={Synthesizing robust adversarial examples},
  author={Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
  booktitle={International conference on machine learning},
  pages={284--293},
  year={2018},
  organization={PMLR}
}

@article{geman1992neural,
  title={Neural networks and the bias/variance dilemma},
  author={Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal={Neural computation},
  volume={4},
  number={1},
  pages={1--58},
  year={1992},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}



@article{krogh1991simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John},
  journal={Advances in neural information processing systems},
  volume={4},
  year={1991}
}


@article{xie2022word,
  title={A word is worth a thousand dollars: Adversarial attack on tweets fools stock prediction},
  author={Xie, Yong and Wang, Dakuo and Chen, Pin-Yu and Xiong, Jinjun and Liu, Sijia and Koyejo, Sanmi},
  journal={arXiv preprint arXiv:2205.01094},
  year={2022}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}


@article{barreno2010security,
  title={The security of machine learning},
  author={Barreno, Marco and Nelson, Blaine and Joseph, Anthony D and Tygar, J Doug},
  journal={Machine Learning},
  volume={81},
  pages={121--148},
  year={2010},
  publisher={Springer}
}

@misc{jiang2024investigatingdatacontaminationpretraining,
      title={Investigating Data Contamination for Pre-training Language Models}, 
      author={Minhao Jiang and Ken Ziyu Liu and Ming Zhong and Rylan Schaeffer and Siru Ouyang and Jiawei Han and Sanmi Koyejo},
      year={2024},
      eprint={2401.06059},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06059}, 
}

@misc{schaeffer2023pretrainingtestsetneed,
      title={Pretraining on the Test Set Is All You Need}, 
      author={Rylan Schaeffer},
      year={2023},
      eprint={2309.08632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.08632}, 
}

@misc{reuel2024openproblemstechnicalai,
      title={Open Problems in Technical AI Governance}, 
      author={Anka Reuel and Ben Bucknall and Stephen Casper and Tim Fist and Lisa Soder and Onni Aarne and Lewis Hammond and Lujain Ibrahim and Alan Chan and Peter Wills and Markus Anderljung and Ben Garfinkel and Lennart Heim and Andrew Trask and Gabriel Mukobi and Rylan Schaeffer and Mauricio Baker and Sara Hooker and Irene Solaiman and Alexandra Sasha Luccioni and Nitarshan Rajkumar and Nicolas Moës and Jeffrey Ladish and Neel Guha and Jessica Newman and Yoshua Bengio and Tobin South and Alex Pentland and Sanmi Koyejo and Mykel J. Kochenderfer and Robert Trager},
      year={2024},
      eprint={2407.14981},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2407.14981}, 
}

@misc{madaan2024quantifyingvarianceevaluationbenchmarks,
      title={Quantifying Variance in Evaluation Benchmarks}, 
      author={Lovish Madaan and Aaditya K. Singh and Rylan Schaeffer and Andrew Poulton and Sanmi Koyejo and Pontus Stenetorp and Sharan Narang and Dieuwke Hupkes},
      year={2024},
      eprint={2406.10229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.10229}, 
}

@article{bavaresco2024llms,
  title={Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks},
  author={Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fern{\'a}ndez, Raquel and Gatt, Albert and Ghaleb, Esam and Giulianelli, Mario and Hanna, Michael and Koller, Alexander and others},
  journal={arXiv preprint arXiv:2406.18403},
  year={2024}
}

@inproceedings{belz2006comparing,
  title={Comparing automatic and human evaluation of NLG systems},
  author={Belz, Anja and Reiter, Ehud},
  booktitle={11th conference of the european chapter of the association for computational linguistics},
  pages={313--320},
  year={2006}
}

@inproceedings{doddington2002nist,
  title={Automatic evaluation of machine translation quality using n-gram co-occurrence statistics},
  author={Doddington, George},
  booktitle={Proceedings of the second international conference on Human Language Technology Research},
  pages={138--145},
  year={2002}
}

@article{van2021human,
  title={Human evaluation of automatically generated text: Current trends and best practice guidelines},
  author={van der Lee, Chris and Gatt, Albert and van Miltenburg, Emiel and Krahmer, Emiel},
  journal={Computer Speech \& Language},
  volume={67},
  pages={101151},
  year={2021},
  publisher={Elsevier}
}

@article{gehrmann2023repairing,
  title={Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text},
  author={Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={103--166},
  year={2023}
}

@inproceedings{kusner2015word,
  title={From word embeddings to document distances},
  author={Kusner, Matt and Sun, Yu and Kolkin, Nicholas and Weinberger, Kilian},
  booktitle={International conference on machine learning},
  pages={957--966},
  year={2015},
  organization={PMLR}
}

@misc{chan2023chatevalbetterllmbasedevaluators,
      title={ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate}, 
      author={Chi-Min Chan and Weize Chen and Yusheng Su and Jianxuan Yu and Wei Xue and Shanghang Zhang and Jie Fu and Zhiyuan Liu},
      year={2023},
      eprint={2308.07201},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.07201}, 
}

@inproceedings{clinciu2021study,
  title={A Study of Automatic Metrics for the Evaluation of Natural Language Explanations},
  author={Clinciu, Miruna and Eshghi, Arash and Hastie, Helen},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={2376--2387},
  year={2021}
}

@inproceedings{van2019best,
  title={Best practices for the human evaluation of automatically generated text},
  author={Van Der Lee, Chris and Gatt, Albert and Van Miltenburg, Emiel and Wubben, Sander and Krahmer, Emiel},
  booktitle={Proceedings of the 12th International Conference on Natural Language Generation},
  pages={355--368},
  year={2019}
}

@article{dorner2024limits,
  title={Limits to scalable evaluation at the frontier: LLM as Judge won't beat twice the data},
  author={Dorner, Florian E and Nastl, Vivian Y and Hardt, Moritz},
  journal={arXiv preprint arXiv:2410.13341},
  year={2024}
}



@inproceedings{novikova2017we,
  title={Why We Need New Evaluation Metrics for NLG},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Curry, Amanda Cercas and Rieser, Verena},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={2241--2252},
  year={2017}
}



@article{sellam2020bleurt,
  title={BLEURT: Learning robust metrics for text generation},
  author={Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2004.04696},
  year={2020}
}


@misc{chiang2023largelanguagemodelsalternative,
      title={Can Large Language Models Be an Alternative to Human Evaluations?}, 
      author={Cheng-Han Chiang and Hung-yi Lee},
      year={2023},
      eprint={2305.01937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.01937}, 
}

@article{wang2023chatgpt,
  title={Is chatgpt a good nlg evaluator? a preliminary study},
  author={Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
  journal={arXiv preprint arXiv:2303.04048},
  year={2023}
}


@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}



@article{liu2023g,
  title={G-eval: Nlg evaluation using gpt-4 with better human alignment},
  author={Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
  journal={arXiv preprint arXiv:2303.16634},
  year={2023}
}


@misc{roller2020opendomainconversationalagentscurrent,
      title={Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions}, 
      author={Stephen Roller and Y-Lan Boureau and Jason Weston and Antoine Bordes and Emily Dinan and Angela Fan and David Gunning and Da Ju and Margaret Li and Spencer Poff and Pratik Ringshia and Kurt Shuster and Eric Michael Smith and Arthur Szlam and Jack Urbanek and Mary Williamson},
      year={2020},
      eprint={2006.12442},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.12442}, 
}

@misc{finch2023dontforgetabcsevaluating,
      title={Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems}, 
      author={Sarah E. Finch and James D. Finch and Jinho D. Choi},
      year={2023},
      eprint={2212.09180},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09180}, 
}

@misc{smith2022humanevaluationconversationsopen,
      title={Human Evaluation of Conversations is an Open Problem: comparing the sensitivity of various methods for evaluating dialogue agents}, 
      author={Eric Michael Smith and Orion Hsu and Rebecca Qian and Stephen Roller and Y-Lan Boureau and Jason Weston},
      year={2022},
      eprint={2201.04723},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.04723}, 
}

@article{reiter2018structured,
  title={A structured review of the validity of BLEU},
  author={Reiter, Ehud},
  journal={Computational Linguistics},
  volume={44},
  number={3},
  pages={393--401},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}


@misc{karpinska2021perilsusingmechanicalturk,
      title={The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation}, 
      author={Marzena Karpinska and Nader Akoury and Mohit Iyyer},
      year={2021},
      eprint={2109.06835},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.06835}, 
}


@misc{roller2020recipesbuildingopendomainchatbot,
      title={Recipes for building an open-domain chatbot}, 
      author={Stephen Roller and Emily Dinan and Naman Goyal and Da Ju and Mary Williamson and Yinhan Liu and Jing Xu and Myle Ott and Kurt Shuster and Eric M. Smith and Y-Lan Boureau and Jason Weston},
      year={2020},
      eprint={2004.13637},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.13637}, 
}

@inproceedings{bangalore2000evaluation,
  title={Evaluation metrics for generation},
  author={Bangalore, Srinivas and Rambow, Owen and Whittaker, Steve},
  booktitle={INLG’2000 Proceedings of the First International Conference on Natural Language Generation},
  pages={1--8},
  year={2000}
}

@article{celikyilmaz2020evaluation,
  title={Evaluation of text generation: A survey},
  author={Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2006.14799},
  year={2020}
}

@inproceedings{novikova2018rankme,
  title={RankME: Reliable Human Ratings for Natural Language Generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={72--78},
  year={2018}
}


@article{gatt2018survey,
  title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
  author={Gatt, Albert and Krahmer, Emiel},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={65--170},
  year={2018}
}



@misc{bowman2021fixbenchmarkingnaturallanguage,
      title={What Will it Take to Fix Benchmarking in Natural Language Understanding?}, 
      author={Samuel R. Bowman and George E. Dahl},
      year={2021},
      eprint={2104.02145},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.02145}, 
}

@inproceedings{howcroft2020twenty,
  title={Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions},
  author={Howcroft, David M and Belz, Anya and Clinciu, Miruna and Gkatzia, Dimitra and Hasan, Sadid A and Mahamood, Saad and Mille, Simon and Van Miltenburg, Emiel and Santhanam, Sashank and Rieser, Verena},
  booktitle={13th International Conference on Natural Language Generation 2020},
  pages={169--182},
  year={2020},
  organization={Association for Computational Linguistics}
}

@article{liu2016hownot,
  title={How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation},
  author={Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
  journal={arXiv preprint arXiv:1603.08023},
  year={2016}
}


@article{clark2021all,
  title={All that's' human'is not gold: Evaluating human evaluation of generated text},
  author={Clark, Elizabeth and August, Tal and Serrano, Sofia and Haduong, Nikita and Gururangan, Suchin and Smith, Noah A},
  journal={arXiv preprint arXiv:2107.00061},
  year={2021}
}





@misc{schaeffer2024predictingdownstreamcapabilitiesfrontier,
      title={Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive?}, 
      author={Rylan Schaeffer and Hailey Schoelkopf and Brando Miranda and Gabriel Mukobi and Varun Madan and Adam Ibrahim and Herbie Bradley and Stella Biderman and Sanmi Koyejo},
      year={2024},
      eprint={2406.04391},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04391}, 
}

@article{schaeffer2024emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{roberts2023cutoff,
  title={To the cutoff... and beyond? a longitudinal perspective on LLM data contamination},
  author={Roberts, Manley and Thakur, Himanshu and Herlihy, Christine and White, Colin and Dooley, Samuel},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{zhang2024careful,
  title={A careful examination of large language model performance on grade school arithmetic},
  author={Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Slack, Dylan and Lyu, Qin and others},
  journal={arXiv preprint arXiv:2405.00332},
  year={2024}
}


@article{oren2023proving,
  title={Proving test set contamination in black box language models},
  author={Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and Ladhak, Faisal and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2310.17623},
  year={2023}
}


@article{ruan2024observational,
  title={Observational Scaling Laws and the Predictability of Language Model Performance},
  author={Ruan, Yangjun and Maddison, Chris J and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2405.10938},
  year={2024}
}

