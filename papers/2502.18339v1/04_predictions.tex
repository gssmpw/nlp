\section{Predicting Human Evaluations from NLP Benchmarks}
\label{sec:predictions}

Having established the existence of correlations between human evaluations and NLP benchmarks, we investigated the feasibility of predicting human evaluations from NLP benchmarks. Our goal is to build predictive models that accurately predict a language model's average human evaluation scores per areas and categories using the model's average scores on NLP benchmarks and tasks. However, we faced a significant challenge due to the overparameterized nature of our data: for each target human evaluation area or category, there are approximately 150 covariates (NLP benchmarks and tasks) but only four models.


\textbf{Predictive Modeling: Overparameterized Linear Regressions.}
To predict human evaluations from NLP benchmarks, we used overparameterized linear regression. In general, overparameterized linear regression is known to be capable of generalizing (App. Sec. \ref{app:sec:generalization_of_overparameterized_models}), although whether linear models would generalize in this setting was an empirical question. 
For each human evaluation area and category, we fit a linear model to predict a language model's average human evaluation score from its average scores on all NLP benchmarks and tasks. To assess the predictive accuracy of these overparameterized models, we employed leave-one-out cross validation: we fit four separate linear models, each time fitting on three of the chat LMS' scores and holding out the fourth to test the performance of the linear model. This approach allows us to estimate the models' performance on unseen data, albeit with limitations due to the small sample size.
Before fitting the models, we normalized all human evaluation scores to lie in $[0, 1]$ rather than $[-7, -1]$ (recalling that higher scores indicate the human evaluator prefers the Chat Llama 2 model compared to GPT-3.5). 

\textbf{Results.} Across human evaluation areas and categories, we found that the linear models' predicted average human evaluation scores generally align well with the actual average human evaluation scores, as evidenced by most points falling close to the identity line in the predicted score vs. actual score plane (Fig. \ref{fig:reg:overparameterized_regressions_leave_one_out}). This suggests that, despite the overparameterization, the linear models can capture meaningful relationships between NLP benchmarks and human evaluations. However, we caution against over-interpreting these results, as the small sample size and the assumption of linearity may limit the generalizability of these findings to other language models or evaluation settings.

To gain insight into which NLP benchmarks are most informative for predicting human evaluation scores, we examine the learned weights of the linear models (Fig. \ref{app:fig:linear_regression_coefficients}). NLP benchmarks with consistently high absolute weights across different human evaluation areas and categories are likely to be more predictive of human judgments. However, due to the overparameterized nature of the models, we refrain from drawing strong conclusions about the relative importance of individual benchmarks and instead focus on the overall predictive performance.
These results suggest that scaling up the number of chat LMs and human evaluation data could unlock highly predictive models of slow, noisy and expensive but valuable human evaluations using fast, precise and cheaper NLP benchmarks.