


\section{Correlating Human Evaluations with NLP Benchmarks}
\label{sec:correlations}

We began by computing correlations between human evaluations and NLP benchmarks over the 4 average scores per model with three standard correlations  --- Pearson \citep{galton1877heredity}, Spearman \citep{spearman1904proof} and Kendall \citep{kendall1938new} --- giving us three correlation matrices of shape $160 \times 55$ between every pair of NLP benchmark and human evaluation area-category-subcategory (Fig. \ref{fig:corr:microscopic}).
% Pearson correlation measures the linear relationship between two continuous variables, whereas Spearman and Kendall correlations assess the monotonic relationship between two variables; Spearman correlation is based on the rank order of the data points, whereas Kendall correlation is determined by the number of concordant and discordant pairs.
By using different correlation metrics, we aim to robustly characterize the relationships between human and NLP benchmarks.

Macroscopically, at the most coarse grouping of human evaluations in our taxonomy (i.e., areas) (Fig.~\ref{fig:human_eval_prompt_taxonomy}), we found that average NLP benchmark scores are highly correlated with average human scores for all human evaluation areas under all three correlation metrics (Fig. \ref{fig:corr:human_eval_area} top).
% Due to the small number of models ($N=4$), Spearman and Kendall correlations suffer discretization effects (Fig. \ref{app:fig:correlation_couplings}), inducing an illusion of undulations.
These strong correlations suggest that, at a high level, NLP benchmarks are reasonable proxies for human judgments of LM quality.

Mesoscopically, at the level of human evaluation areas and categories, NLP benchmarks remain highly correlated with human evaluations, with two notable types of exceptions (Fig. \ref{fig:corr:human_eval_area}). First, Adversarial Dishonesty, Adversarial Harmfulness, and Safety are anti-correlated with most NLP benchmarks, potentially indicating that these adversarial and safety-focused categories are more easily transgressed by more capable LMs; an alternative hypothesis could be that safety benchmarks simply are not especially good, as demonstrated by \citet{ren2024safetywashing}. Second, Language Assistance and Open Question Answering are uncorrelated with most NLP benchmarks, suggesting that these categories may require new NLP benchmarks. Open Question Answering was surprising given that some of our NLP benchmarks are open question answering datasets, e.g., OpenBookQA \citep{mihaylov2018openbookqa}. We found the three correlations metrics visually agreed with one another and were themselves tightly coupled (App. Fig. \ref{app:fig:correlation_couplings}), and so we present only one (Pearson) moving forward, with equivalent plots of the other two (Spearman, Kendall) deferred to the appendix.



\subsection{Which human evaluations have few-to-no correlated NLP benchmarks?}
\label{sec:correlations:subsec:nlp_benchmarks_no_correlations}

To the best of our ability to discern, none. Every human evaluation seemed to have at least some NLP benchmarks that were either correlated or anticorrelated with it. This result is promising because it suggests human evaluations might be predictable from NLP benchmarks (Sec.~\ref{sec:predictions}).  

\subsection{Which NLP benchmarks exhibit high correlations with human evaluations?}
\label{sec:correlations:subsec:nlp_benchmarks_highest_correlations}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/correlations/human_eval_area_cat_vs_academic_benchmark_subset_by_correlation_method=pearson}
    \caption{\textbf{Pearson Correlations Between Human Evaluations and NLP Benchmarks.} Rows: Human evaluation areas-categories-subcategories. Columns: NLP benchmarks. The heatmap is row-wrapped to fit on the page. \textcolor{red}{Large positive correlations (+1) are shown in red.} \textcolor{blue}{Large negative anticorrelations (-1) are shown in blue.} Low uncorrelations ($\sim$0) are shown in light-white-gray.}
    \label{fig:corr:microscopic}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/correlations/2D_biplot_correlation=Pearson.pdf}
    \caption{\textbf{Matrix Decomposition of Pairwise Pearson Correlations Between Human Evaluations and NLP Benchmarks.} The correlation matrix has 3 non-zero singular values (App. Fig. \ref{app:fig:academic_human_singular_value_spectra}). Bottom: \textcolor{Magenta}{Human evaluations} and \textcolor{Green}{NLP benchmarks} are plotted projected along the (dimension-scaled) first two singular modes of the Pearson correlation matrix. The bulk of evaluations live in one community (left), with smaller communities (top, bottom, right); for an in-depth interpretation, see Sec. \ref{sec:correlations:subsec:community_detection}.}
    \label{fig:corr:singular_modes}
\end{figure*}


To answer this question, we ordered NLP benchmarks based on their average correlation score with all human evaluation areas, categories and subcategories.
We found many NLP benchmarks have high average correlation with human evaluations (Fig. \ref{fig:corr:academic_vs_correlation_split_corrmethod}); the highest average correlation NLP benchmarks include a subset of MMLU (Nutrition, Human Aging, Sociology, Public Relations, Moral Scenarios, College Computer Science), a subset of BIG Bench Hard (Word Sorting, Reasoning About Colored Objects, Logical Deduction), HellaSwag, ARC, RACE, PIQA, NaturalQuestions, QuAC, CommonSenseQA, DROP and TriviaQA. Other benchmarks were less correlated or uncorrelated with human evaluations: ETHOS, Kth Sentence, Inverse Scaling (excluding Resisting Correction Classification), OpenBookQA, COPA, SciBench (excluding Fundamentals of Physics) and SIQA.
Upon investigating, some of the most highly correlated NLP benchmarks make sense. For instance, Inverse Scaling's Resisting Correction Classification ranked second highest for being correlated with human evaluations, and the task measures a highly desirable capability for human users: the LM's ability to follow user instructions that run counter to the LM's natural inclinations.


\subsection{Matrix Decomposition of Correlations between Human Evaluations and NLP Benchmarks} \label{sec:correlations:subsec:community_detection}


To gain a structured view of how human evaluations and NLP benchmarks interrelate, we analyze the $160 \times 55$ Pearson correlation matrix between them by computing the singular value decomposition of the correlation matrix $C = U \Sigma V^T$, where $U \in \mathbb{R}^{160 \times 3}$ and $V \in \mathbb{R}^{55 \times 3}$ are the left and right singular vectors. 
We observe only three non-zero singular values (Appendix Figure~\ref{app:fig:academic_human_singular_value_spectra}), indicating that most of the variance in correlations is captured by three underlying dimensions.
Interpreting each row of $U$ and $V$ as coordinates in the corresponding low-dimensional space provides a way to visualize how tasks and human evaluations group together based on their similarity in correlation patterns, as shown by plotting each NLP benchmark (\textcolor{Green}{green}) and each human evaluation category (\textcolor{Magenta}{magenta}) in the 2D space spanned by the first two singular vectors (scaled by their singular values). (Fig.~\ref{fig:corr:singular_modes}).
% Each correlation is computed across the four Chat Llama 2 models. Consequently, the matrix can have rank at most four. 



\textbf{Overall Alignment (Left Cluster).} The largest group of points, combining both \textcolor{Magenta}{human evaluations} and \textcolor{Green}{benchmarks}, sits on the left side. This indicates that many standard NLP benchmarks (e.g., language understanding, commonsense reasoning, factual QA) tend to move in tandem with broad human-judged performance (e.g., correctness, clarity) across our four models.

\textbf{Dialogue and Context-Oriented Tasks (Top-Left).} Certain dialogue-related human evaluations (e.g., \textcolor{Magenta}{Language Assistance, Coding Assistance}) appear near several benchmarks that emphasize context or discourse (e.g., \textcolor{Green}{OpenBookQA}, \textcolor{Green}{Kth Sentence}). Their proximity suggests a shared correlation pattern across the models, possibly reflecting reliance on social reasoning and contextual cues.

\textbf{Adversarial and Safety-Focused Tasks (Right \& Bottom-Right).} Evaluations tied to \textcolor{Magenta}{Adversarial Harmfulness/Dishonesty} and \textcolor{Magenta}{Safety} show distinct positions, often near benchmarks aimed at exposing errors or biases (e.g., \textcolor{Green}{Inverse Scaling tasks}, \textcolor{Green}{ETHOS} for hate speech). This segregation indicates that safety/adversarial capabilities differ in how they correlate with more conventional tasks.

\textbf{Open QA \& Domain Knowledge (Lower-Left).} Finally, \textcolor{Magenta}{Open QA} and some \textcolor{Magenta}{Writing} evaluations lie closer to benchmarks demanding specialized knowledge (\textcolor{Green}{MMLU.Electrical Engineering}, \textcolor{Green}{SciBench.Quantum Chemistry}), suggesting that open-ended user queries may align more with advanced domain-knowledge benchmarks than with simpler tasks.

Overall, this matrix decomposition shows that while there is a dominant “general ability” factor (represented by the first singular value) that aligns most tasks and evaluations, additional singular vectors capture subtler patterns. These include the safety/adversarial dimension and context-dependent or domain-specialized dimensions. Consequently, the correlations between human evaluations and NLP benchmarks exhibit a rich low-rank structure indicative of multiple underlying performance factors.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/regressions/pred_vs_true_human_scores_by_human_eval_split_by_left_out_model.pdf}
    \caption{\textbf{Leave-One-Out Cross Validated Linear Regression Predictions of Human Evaluations.} Linear regressions accurately predict human evaluation scores from all NLP benchmark scores. Each subfigure shows predicted human evaluation scores against actual human evaluation scores on each of the four left-out Chat Llama 2 models colored by human evaluation area, category and subcategory.}
    \label{fig:reg:overparameterized_regressions_leave_one_out}
\end{figure*}