\section{emprical study setup}
A thorough understanding and categorization of these problems is essential to address the pervasive login issues in Android apps. We followed the process adopted by existing work \cite{8999997,chen2024demystifyingdevicespecificcompatibilityissues,10.1145/3597926.3598138,7582761} to prepare the dataset for our empirical study. Our research began with an extensive examination of Android-related repositories on GitHub \cite{github} using specific keywords such as ``android-app, ``android" and ``android-library," among others, to select repositories likely to contain relevant data on login issues. All keywords used are available on our GitHub repository~\cite{code}.

\subsection{Data Collection}

Our study involved a meticulous selection process for identifying repositories that are both relevant and exhibit high quality based on the following criteria:
1) \textbf{Recent Activity:} Repositories must have had their last commit after January 1, 2023, indicating active development;
2) \textbf{Community Engagement:} Repositories were required to have more than 50 stars, reflecting a certain level of community endorsement;
3) \textbf{Substantial Contribution:} Each repository should have over 200 commits, demonstrating significant developer investment;
4) \textbf{Programming Language:} Repositories needed to predominantly use Java or Kotlin, which are commonly used for Android development.

To efficiently gather data, we utilized the GitHub Search API \cite{githubsearch}, which imposes a cap of 1,000 results per query. The query example is shown in Listing~\ref{lst:query}. To circumvent this limitation and ensure comprehensive data retrieval, we adopted an iterative querying approach\cite{8595172,chen2024demystifyingdevicespecificcompatibilityissues,10148722}. By sorting repositories by their most recent update and progressively refining our search with the `pushed' filter, we were able to compile a complete list of repositories fitting our criteria. This methodological rigor enabled us to amass a total of 2,675 repositories. Fig \ref{fig:data selection} shows the overview of the data collection and selection process.  
%\yuki{Personally speaking, redrawing this figure in tikz is recommended. Please ask me for help if you want to do so. I have some experiences in playing with the tikz magic stuffs, it's really time comsuming for new comers.}\zixu{Modified by tikz} \lili{I think this figure needs to be revised again. It's too loose.}\zixu{Compress the figure to one cm between nodes}\rufeng{Maybe revise "2569 Repositories and 50201 issues" to "50201 issues from 2569 Repositories"? }\zixu{Done}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=0.5cm, auto]
        \tikzset{
            startstop/.style={rectangle, rounded corners, minimum width=0.5\linewidth, minimum height=0.4cm, text centered, draw=black, fill=red!10, font=\small},
            result/.style={rectangle, minimum width=0.5\linewidth, minimum height=0.4cm, text centered, draw=black, fill=blue!10, font=\small},
            process/.style={rectangle, minimum width=0.8\linewidth, minimum height=0.4cm, text centered, draw=black, fill=orange!10, font=\small},
            arrow/.style={thick,->,>=stealth, shorten >=1pt, shorten <=1pt},
            description/.style={text width=5cm, align=left, midway, font=\small} 
        }

        \node (start) [startstop] {GitHub};
        \node (in1) [result, below=of start] {2675 Repositories};
        \node (in2) [result, below=of in1] {50201 Issues From 2569 Repositories};
        \node (in3) [result, below=of in2] {2398 Issues From 89 Repositories};
        \node (out1) [result, below=of in3] {361 Issues From 44 Repositories};

        \draw [arrow] (start) -- (in1) node[description] {Step 1:GitHub Search};
        \draw [arrow] (in1) -- (in2) node[description] {Step 2: Keywords Filter};
        \draw [arrow] (in2) -- (in3) node[description] {Step 3: TF-IDF Keywords Filter };
        \draw [arrow] (in3) -- (out1) node[description] {Step 4: Advanced Criteria Filter };
    \end{tikzpicture}
     \caption{The process of dataset collection}
\label{fig:data selection}
\end{figure}

The repositories' metadata and the complete list of repositories have been preserved and are accessible within our study's digital artifacts~\cite{code}. 
%\yuki{Make a cite to the dataset here. (Simply make a fake link for now)}\zixu{Added}
\begin{lstlisting}[language=SQL, caption={GitHub Search Query Example}, label=lst:query]
query := keyword
            + " stars:>50" 
            + " pushed:>=2023-01-01"
            + " language:java language:kotlin"
\end{lstlisting}

In the subsequent phase, our objective was to pinpoint login-specific issues within the amassed repositories. Initially, to refine our search and reduce extraneous data, we excluded repositories with names suggesting a focus on coding interviews or algorithmic challenges, such as those containing ``interview" or ``leetcode". After the filter, we obtained 2,569 repositories. 
%\yuki{How many repos are filtered out here?}\zixu{Added}

To enhance search performance and avoid triggering rate limits of the GitHub search API, we downloaded all closed issues from 2,569 repositories. We then conducted a keyword analysis using TF-IDF \cite{aizawa2003information} and stopwords \cite{stopwords} to identify and extract key terms frequently appearing in login-related issues. Initially, we manually selected 10 login-related issues and used TF-IDF in conjunction with stopwords to determine the top 10 most frequent terms within these issues. These keywords were then employed to search through all downloaded issues, focusing on the titles and descriptions. Subsequently, we applied TF-IDF analysis again to the issues that contained these top 10 terms, such as ``login". To keep the login relevance of results issues, we only extracted the top 100 terms, such as ``auth", ``sign in" and ``MFA" that were used as search keywords. We manually verified each term to ensure the relevance of the login process. This refined list of keywords guided our secondary search through the GitHub issues API, leading us to identify 2,398 issues across 189 repositories.
%\yuki{Describe more about the "keywords" we have? How did we do with the "TF-IDF"? A "wordcloud" about the "key terms" seems suitable here.}\zixu{add a process about TFIDF} \rufeng{Showing a few examples for keywords might be better. And, it might be better to use one sentence to explain why choosing top 10 or top 100. Some reviewers might care about the reasons for these criteria. For top 100, you can say that you manually checked each term to ensure the relevance of these terms.}\zixu{Added}

To ensure the scientific validity and reliability of our study, we applied rigorous filters to the identified issues: 1) \textbf{Fix Commit Link:} Each issue had to include a link to a commit that purportedly resolved the issue, ensuring that we could verify and analyze the resolution approaches; 2) \textbf{Accessibility of Commit Links:} The commit links associated with the issues had to be accessible, allowing us to review the actual code changes made.


After applying these filters, our final dataset comprised 361 issues from 44 repositories. As detailed in Table~\ref{tab:android-issues-significant}, which only contains the repositories that have more than 10 issues, our dataset includes highly popular Android apps, evidenced by significant GitHub stars and download numbers on Google Play. This diverse set encompasses a wide range of app categories, ensuring a comprehensive analysis across various user experiences and functionalities. For instance, WordPress~\cite{wordpress} serves as a content management app, NextCloud~\cite{nextcloud-android} is utilized for file synchronization and cloud storage, and Element~\cite{element-android} functions as a secure messaging platform. This breadth of app types, from productivity to social interaction, enhances the relevance and applicability of our findings to real-world usage scenarios.
%\yuki{During my review progress of the dataset, I did find that some cases only happen in the test environment, such as the CI, or the development environment. Consider that a developer opens the Project directly in the Android Studio, without correctly setting up some credentials. These cases may not be posed to real end-users. Should we rule them out? aka, some cases under the "Wrong Environment Dependency" category in RQ1.}\zixu{In the comments, there are some users reported they found the same issues in productive env}

\begin{table}
  \caption{Summary of Android Open-Source Repositories with Significant Login Issues(We only listed repositories that contain more than 10 issues) \lili{The number of issues in this table adds up to 258. Is it correct?}\zixu{added caption} %\rufeng{IMO, I would prefer a brief intro for some of the repos rather than a table, like some repos that focus on different aspects of Android or some well-known repos.}\zixu{add stars and downloads numbers, and sentences to show our repos are diverse}
  }
  \label{tab:android-issues-significant}
  \centering
  \begin{tabular}{lccc}
    \toprule
    Repository & Issues & Stars& Downloads\\
    \midrule
    thunderbird/thunderbird-android \cite{thunderbird-android} & 41 & 10.9k & 50k+ \\
    woocommerce/woocommerce-android \cite{woocommerce-android} & 37 & 277 & 1M+\\
    nextcloud/android \cite{nextcloud-android} & 35 & 4.3k &1M+ \\
    firebase/FirebaseUI-Android  \cite{firebaseui-android}& 25 & 4.6k & N/A \\
    fossasia/open-event-attendee-android \cite{fossasia} & 17 &2k& 500k+ \\
    element-hq/element-android \cite{element-android} & 16 & 3.4k & 1M+ \\
    home-assistant/android \cite{home-assistant-android}& 16 &2.3k & 1M+ \\
    tuskyapp/Tusky \cite{tusky}& 15 &2.5k &500k+ \\
    commons-app/apps-android-commons \cite{commons-android-commons} & 14 &1k &100k+ \\
    ankidroid/Anki-Android \cite{anki-android}& 11 & 8.7k &10M+ \\
    fossasia/phimpme-android \cite{phimpme-android}& 11 & 2.6k & 1M+ \\
    element-hq/element-x-android \cite{element-x-android}& 10 & 1.1k & 10k+ \\
    bitfireAT/davx5-ose \cite{davx5-ose} & 10 & 1.5k & 100k+ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Data Analysis}

 The study observes that precise categorizations of issues play a vital role in the development of Android apps~\cite{10.1145/3386685}. We employed an open coding methodology to address the classification of 361 identified login issues into root causes, symptoms, and trigger conditions. This approach facilitated a systematic and structured classification process, enhancing the depth and accuracy of our analysis. 

% \subsubsection{Overview of Open Coding}

%  It is particularly useful in exploratory studies where predefined categories may not exist. This methodology is characterized by its iterative and flexible nature, allowing researchers to refine and adapt categories as more data is analyzed. 
%\rufeng{Consider moving the overview section into stage 1, if we need more space.}\zixu{Remove Overview of Opencoding part}

\subsubsection{Stage I Initial Categorization}

The first stage involved a preliminary analysis where 10\% of the issues, totaling 36 distinct issues, were randomly selected for initial coding. Two researchers independently reviewed each issue’s description, developer replies, and fix commits. The objective was to identify broad categories under the three aspects: (1) \textbf{Root Causes}: Fundamental reasons that initiate the login issues; (2) \textbf{Symptoms}: Observable effects or behaviors resulting from the issues; (3) \textbf{Trigger Conditions}: Specific scenarios or conditions under which the issues occur. A combination of several conditions can trigger one issue. So, one issue can only have one label for the root cause and symptom but multiple for trigger conditions. %\rufeng{You can indicate here that an issue may have multiple triggering conditions.}\zixu{Reformat and add a sentence}


Each researcher independently assigned issues to preliminary categories, ensuring that the initial classification captured the diversity and complexity of the login issues.

Following the independent phase, the two authors convened to discuss their findings and reconcile any differences in category assignment. A third author resolved any conflicts between the initial two authors. 
% This collaborative discussion refined the categorization framework and achieved a consensus on the definitions and scopes of each category.\rufeng{Consider adding " A third author was involved to resolve any conflicts between the initial two authors’ classifications. "}\zixu{Add the third author} This process ensured the reliability and validity of the categories developed during this phase.
%\yuki{Do we need more details about the results of the Stage.I?}\zixu{Add some conflict example?}

\subsubsection{Stage II Extensive Categorization and Category Refinement}

With the foundational categories established, the second stage extended the categorization process to an additional 30\% of the issues. This larger sample allowed for a robust application of the categorization framework. The two authors continued to work independently, applying the categories to new issues and meeting regularly to discuss their findings. These discussions allowed for continuous refinement of the categories: \textbf{(1)} Identifying new categories that emerged from the data; \textbf{(2)} Modifying existing categories to reflect the data better; \textbf{(3)} Ensuring consistency and accuracy in the application of categories across all analyzed data.

This iterative categorization and discussion cycle was repeated until all issues in the sample had been classified. The open coding procedure facilitated a dynamic adaptation of the categorization framework, accommodating the complexities and nuances of the login issues encountered.

\subsubsection{Final Review and Consensus}

The final stage of the open coding process involved a comprehensive review and consensus discussion to confirm the accuracy of category assignments and finalize the taxonomy of login issues. The categorization's reliability was highlighted by high agreement levels among researchers, quantified by Cohen’s Kappa: 0.88 for Root Causes, 0.82 for Symptoms, and 0.55 for Trigger Conditions. The relatively lower Kappa for Trigger Conditions is attributed to the complexity and variability in identifying multiple conditions per issue, where exact agreement on the type and number of conditions is required, reflecting substantial inter-rater reliability. 