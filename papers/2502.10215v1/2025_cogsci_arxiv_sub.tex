% 
% Annual Cognitive Science Conference
%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass[twocolumn,10pt,letterpaper]{article}

\usepackage{cogsci}

\cogscifinalcopy % Uncomment this line for the final submission 
% colors
% \usepackage[table]{xcolor}
 
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\definecolor{citecolor}{HTML}{1F3B4D}

\usepackage{hyperref}
\hypersetup{
    unicode,                    % Use unicode for links
    pdfborder       = {0 0 0},  % Suppress border around pdf
    bookmarksdepth  = subsection,
    bookmarksopen   = true,     % Expand the bookmarks as soon as the pdf file is opened
    % bookmarksopenlevel = 4,   % What depth level of bookmarks to show.
    %linktoc         = all,      % Toc entries and numbers links to pages
    % linktocpage   = true,     % Only the page number links to pages
    breaklinks      = true,
    colorlinks      = true,
    linkcolor       = citecolor,
    citecolor       = citecolor,
    urlcolor        = citecolor,
}

% math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}

% enumeration
\usepackage{enumitem}


% figures and tables
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{dblfloatfix}
\usepackage{multirow}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{array}
\usepackage{rotating} % for sideways tables
\usepackage{float} % Roger Levy added this and changed figure/table
                   % placement to [H] for conformity to Word template,
                   % though floating tables and figures to top is
                   % still generally recommended!


\usepackage{tikz}

\usepackage{tabularx}

% Define colors
\definecolor{myorange}{RGB}{255,165,0}
\definecolor{myblue}{RGB}{0,0,128}
\definecolor{mygreen}{RGB}{0,128,0}
\definecolor{myred}{RGB}{255,10,10}
\definecolor{myviolet}{RGB}{138,43,226}


\definecolor{observed}{RGB}{170,170,170} 
\definecolor{inference}{RGB}{255,91,89}  
\definecolor{gptfouro}{HTML}{08306B}
\definecolor{claude}{HTML}{66C2A5} 
\definecolor{gemini}{HTML}{A6D854}
\definecolor{humans}{HTML}{E7298A}
\definecolor{gptthreefive}{HTML}{6BAED6}


%%%%%% % Tikz for figures


\DeclareRobustCommand{\colordot}[1]{\begin{tikzpicture}[baseline=(a.south)]
    \node[circle, scale=0.75,color=black, fill=#1] (a) {};
\end{tikzpicture}}

\DeclareRobustCommand{\dashedcircle}{%
    \begin{tikzpicture}[baseline=(a.south)]% Align baseline to the center
        \node[circle, scale=0.75, draw=black, dashed, dash pattern=on 1pt off 1pt, fill=white] (a) {};
    \end{tikzpicture}%
}

\DeclareRobustCommand{\colorsquare}[1]{\begin{tikzpicture}[baseline=(a.south)]
    \node[rectangle, scale=0.75, color=black, fill=#1, minimum width=0.6em, minimum height=0.6em] (a) {};
\end{tikzpicture}}



\DeclareRobustCommand{\colortriangle}[1]{\begin{tikzpicture}[baseline=(a.south)]
    \draw[fill=#1, thick] (0, 0) -- (0.3em, 0) -- (0.15em, 0.26em) -- cycle;
\end{tikzpicture}}


\DeclareRobustCommand{\colordotcircum}[1]{\begin{tikzpicture}[baseline=(a.south)]
    \node[circle, scale=0.75,color=black, fill=white] (a) {};
\end{tikzpicture}}



% Bibliography
% \usepackage{natbib}
\usepackage{pslatex}
\usepackage{apacite}

%\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off
%hyphenation for purposes such as spell checking of the resulting
%PDF.  Uncomment this block to turn off hyphenation.




\usepackage[
    capitalize,
    nameinlink,
    % noabbrev,
]{cleveref}


% \setlength\titlebox{4.5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 4.5cm (the original size).
%%If you do, we reserve the right to require you to change it back in
%%the camera-ready version, which could interfere with the timely
%%appearance of your paper in the Proceedings.

\newcommand{\todo}{\textbf{\textcolor{red}{TODO: }}}

\title{Do Large Language Models Reason Causally Like Us? Even Better?}



\author{
    {\large \bf Hanna M. Dettki} \\ New York University\\ \href{mailto:hmd8142@nyu.edu}{hmd8142@nyu.edu}
    \And 
    {\large \bf Brenden M. Lake } \\ New York University \\ \href{mailto:brenden@nyu.edu}{brenden@nyu.edu}
    \And 
    {\large \bf Charley M. Wu }\\ University of Tübingen \\ \href{mailto:charleymswu@gmail.com}{charleymswu@gmail.com}
    \And
    {\large \bf Bob Rehder } \\ New York University \\ \href{mailto:bob.rehder@nyu.edu}{bob.rehder@nyu.edu} 
  %\\
  % Department of Psychology\\
  % New York University (NYU)
}
\begin{document}

\maketitle







\begin{abstract} 


Causal reasoning is a core component of intelligence. 
Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns.
We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. 
We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-4o and Claude showed the most normative behavior, including ``explaining away,'' whereas Gemini-Pro and GPT-3.5 did not.
Although all agents deviated from the expected independence of causes --- Claude the least --- they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes.
These findings underscore the need to assess AI biases as they increasingly assist human decision-making.

  \textbf{Keywords:} 
  Large Language Models; Causal Inference; Human and Machine Reasoning
  \end{abstract}
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the remarkable capabilities of LLMs
Large Language Models (LLMs) have proven to be highly capable across a range of domains, including natural language understanding, answering questions, and engaging in creative tasks \cite{bubeck2023sparks,abdin2024phi,gunter2024apple}.
% are we entering an era of AGI?
In light of these recent advancements in LLMs, many believe that we are now truly entering an era of Artificial Intelligence \cite<AI;>[]{bottou2023borges}. The degree to which machines genuinely comprehend our environment carries significant implications for their reliability in various domains \cite{mitchell2023debate}, including the automatic generation of news content, policy recommendations \cite{kekiC2023evaluating}, 
knowledge discovery, disease diagnosis \cite{nori2023capabilities}, and autonomous driving. 
The impressive capability of LLMs to produce text resembling human language raises the question of whether these models possess some form of world understanding, and if they reason similarly to humans.

% Causal reasoning has a hallmark of intelligence
\emph{Causal reasoning} is widely regarded as a core aspect of  intelligence \cite{lake2017building}. It involves recognizing and inferring the causal relationships between variables, moving beyond mere correlations to uncover underlying mechanisms. Such capabilities are essential in practical applications, including the development of pharmaceutical drugs or the planning of public health strategies. Therefore, causal reasoning is considered an important milestone in the pursuit of Artificial General Intelligence \cite<AGI;>[]{obaid2023machine}.
Causal reasoning can be formalized using \textit{causal Bayes nets} (CBNs) providing a probabilistic calculus for reasoning about the probability of some variables given others that are causally related \cite{pearl1995bayesian}.
By comparing human reasoners to CBNs, CBNs can  serve as a normative benchmark \cite{glymour2003learning, waldmann2006beyond} and help reveal human biases that deviate from ideal causal reasoning \cite{rehder2017failures, bramley2015conservative}. 
% CBNS: a normative benchmark on how to evaluate human reasoning
% Biases in Human Reasoning
For instance, when reasoning about a simple collider graph $C_1 \rightarrow E \leftarrow C_2$, people exhibit biases such as \textit{weak explaining away} and \textit{Markov violations}  \cite<explained later;>[]{rehder2017failures}. These systematic deviations highlight the interplay between normative principles and cognitive heuristics in human causal reasoning.

% Concern: 
% Do LLMs rely on patterns in data?
A plethora of recent studies have assessed the capabilities of LLMs \cite<e.g.,>[]{kiciman2023causal}, 
and concerns have been raised regarding their reliance on learned patterns rather than genuine causal relationships \cite{willig2023causal,jiang2024peek}.

For example, \citeA{pmlr-v202-shi23a} and \citeA{mirzadeh2024gsm} demonstrated that introducing irrelevant context can drastically alter the outputs of LLMs. 
That even minor distractions influence their responses raises questions about the robustness of LLMs in high-stakes scenarios.

Indeed, a growing number of researchers have proposed that current LLMs are unable to generalize causal ideas beyond their training distribution and/or without strong user-induced guidance  \cite<e.g., chain-of-thought prompting;>[]{jin2023cladder, kiciman2023causal}. 
Thus, understanding the extent to which LLMs reason causally, and whether they show similar biases to people when they deviate from normative principles has practical importance in deploying AI systems.



  \begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/fig_1_cartoons.png}
    %\includegraphics[width=\columnwidth]{figures/fig_1_domain_viz_tasks_mock_up.png}
    \caption{\textbf{Visualization of Causal Mechanism per Domain.} The left most graph represents task X from the diagnostic inference group.  The  nodes are colored according to:  \colordot{inference} $\to$ latent; \colordot{observed} $\to$ observed $\in \{0,1\}$.
    %; and \dashedcircle{}   $\to$  no information on.} 
    }
    \label{fig:example_table}
  \end{figure}


To this end, \citeA{jin2023cladder} introduced the CLADDER dataset, comprising 10,000 causal reasoning questions designed to evaluate the formal causal reasoning abilities of LLMs.  
While they tested colliders in their dataset, 
they didn't  contrast LLMs with humans. In addition, while the dataset serves as a valuable benchmark to test whether LLMs honor the rules of probability, solving the tasks requires a substantial background in probability and statistics (college-level statistics class and pen and paper), making them less suitable for a direct comparison with human subjects.
\citeA{keshmirian2024biased} directly compared humans and LLMs by asking them to judge the strength of  a causal relationship $C \rightarrow B$ as a function of the context it appeared in. Human's strength judgments were highest when $C \rightarrow B$ was embedded in a chain ($A \rightarrow C \rightarrow B$) versus a fork ($A \leftarrow C \rightarrow B$) or in isolation, a pattern that LLMs matched with a sufficiently large temperature parameter. In contrast, the current work compares human's and LLM's causal inferences rather than their judgments of causal strength.

% Contributions
\textbf{Goals and Scope.}
As we increasingly rely on AI-supported decision making, our work aims to contribute to the investigation of biases in causal reasoning and compares those between LLMs and humans using human data previously collected in \citeA
{rehder2017failures}. 
We assess a collider graph where two independent causes influence a shared effect ($C_1 \rightarrow E \leftarrow C_2$). A collider gives rise to four inference types:
predictive inference (see \Cref{fig:comparison_agg_1}), 
unconditional independence  (\Cref{fig:comparison_agg_2}), 
diagnostic inference with both effect present (\Cref{fig:comparison_agg_3}) and absent  
(\Cref{fig:comparison_agg_4}), 
 from which more specific causal reasoning patterns emerge, such as explaining away.  
Using behavioral analyses and modeling with CBNs, we ask if LLMs reason like humans, if they reason normatively, and if their inferences reflect the use of domain knowledge that inheres in their training data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods} \label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Participants.}
We compare the human behavioral data collected in \citeA{rehder2017failures} (Experiment 1,  Model-Only condition, $N = 48$) with judgments gathered from four LLMs --- GPT-3.5 (\colorsquare{gptthreefive}), GPT-4o (\colorsquare{gptfouro}), Claude-3-Opus (\colorsquare{claude}), and Gemini-Pro (\colorsquare{gemini}) --- which were prompted with the same inference tasks as humans over their respective APIs.
The LLMs were tested with five temperature settings $\in \{0.0, .3, .5, .7, 1.0\}$ but we only report results for temperature 0.0 as this ensures consistent and reproducible outputs.

\textbf{Materials.}
The collider causal structure $C_1 \rightarrow E \leftarrow C_2$ was embedded in one of three cover stories from three different knowledge domains (meteorology, economics, and sociology), allowing for a natural language description of the causal structure. 
The three domains were chosen because the undergraduate subjects were expected to be relatively unfamiliar, such that their causal inferences would reflect the causal structure given to them and not idiosyncratic prior knowledge. Nevertheless, as an additional safeguard, the direction of each variable was counterbalanced  (e.g., in the domain of sociology, some subjects were told that high urbanization causes high socio-economic mobility, others that it causes \textit{low} socio-economic mobility, etc). In fact, \citeA{rehder2017failures} did not find significant effects of domain or the counterbalancing factor, suggesting that subjects' inferences were not strongly influenced by domain knowledge. An important question we ask here is whether this also holds for the LLMs.

Given a set of observations (a subset of the states of $C_1$, $C_2$, and $E$), both humans and LLMs were asked to provide a likelihood judgment on a continuous scale (0-100) for a specific \textit{query variable} \colordot{inference}.

Below is an \textit{example prompt}  from the sociology domain, matching the visualization in \Cref{fig:example_table} and diagnostic task X  in \Cref{fig:comparison_agg_4}, where the query node (\colordot{inference}) is $C_1=1$  and $C_2$ and the effect $E$ are known to be absent. Note that only the \textit{italicized text} following ``:'' was presented to LLMs in one piece. %The prompt describes a causal mechanism and an observation, followed by an inference task: $ p(C_2 = 1 \mid E = 0, C_1 = 0)$.




\small{ % decrease font size for the example prompt
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{Domain introduction:}
    \begin{itemize}[noitemsep, topsep=0pt]
        \item \textit{Sociologists seek to describe and predict the regular patterns of societal interactions. To do this, they study some important variables or attributes of societies. They also study how these attributes are responsible for producing or causing one another.}
    \end{itemize}
    \item \textbf{Causal mechanism:}
    \begin{itemize}[noitemsep, topsep=0pt]
        \item \textbf{\textit{Assume you live in a world that works like this:}}
        \begin{itemize}[noitemsep, topsep=0pt]
            \item \textbf{$C_1$ $\to E$:} \textit{High urbanization causes high socio-economic mobility.}
            \begin{itemize}[noitemsep, topsep=0pt]
                \item \textbf{Explanation:}
                \textit{Big cities provide many opportunities for financial and social improvement.}
            \end{itemize}
            \item \textbf{$C_2$ $\to E$:} \textit{Also, low interest in religion causes high socio-economic mobility.}
            \begin{itemize}[noitemsep, topsep=0pt]
                \item \textbf{Explanation:}
                \textit{Without the restraint of religion-based morality, the impulse toward greed dominates and people tend to accumulate material wealth.}
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item \textbf{Observation:}
    \begin{itemize}[noitemsep, topsep=0pt]
        \item \textit{Now suppose you observe the following: low socio-economic mobility and low urbanization.}
    \end{itemize}
    \item \textbf{Inference task, here X:}
    \begin{itemize}[noitemsep, topsep=0pt]
        \item \textit{Given the observations and the causal mechanism, how likely on a scale from 0 to 100 is low interest in religion? 0 means definitely not likely and 100 means definitely likely. Please provide only a numeric response and no additional information.}
    \end{itemize}
\end{itemize}}

\normalsize

To summarize how humans reason with colliders, the empirical findings reported by \citeA{rehder2017failures} are presented in \Cref{fig:main_comparison_agg} (\colorsquare{humans}) alongside the inferences drawn by the LLMs, which are discussed later. The eleven inference tasks (I-XI) are grouped into four types:

\emph{Predictive inferences} in a collider network involve inferring the state of the effect given information about one or more of the causes. Reasoners should judge, for example, that $p(E = 1 \mid C_1 = 0, C_2 = 0) < p(E = 1 \mid C_1 = 0, C_2 = 1) < p(E = 1 \mid C_1 = 1, C_2 = 1)$.
\Cref{fig:comparison_agg_1} reveals that human reasoners in fact exhibit this pattern, confirming that they made use of the causal knowledge on which they were instructed.

\emph{Independence of causes} is another property of colliders. Because in CBNs exogenous causes are stipulated to be uncorrelated, reasoners should judge that the presence of one cause should not affect the likelihood of the other: $p(C_1 = 1 \mid C_2 = 1) = p(C_1 = 1 \mid C_2 = 0)$. \Cref{fig:comparison_agg_2} reveals that humans judged instead that $p(C_1=1|C_2=1) > p(C_1=1|C_2=0)$. This is an instance of the well-known \textit{Markov violations} that characterize how humans reason with numerous causal network topologies involving generative relations \cite{davis2020mutation}. 
Markov violations have been characterized as an \textit{associative bias}
%Rehder \& Waldmann, 2017,
 \cite<or what>[referred to as a \textit{rich-get-richer} bias]{rehder2017failures}, where the presence of one causal variable makes another supposedly independent variable more likely. 
Both weak explaining away and Markov violations with collider graphs have been documented in multiple studies \cite<see>[for a review]{davis2020mutation}.

\emph{Diagnostic inferences} involve inferring the state of one of the causes given information about the effect and possibly the other cause. An important property of collider networks with independent causal relations is \textit{explaining away}, the phenomenon where observing one cause should decrease the likelihood of the other, when the effect is present: $p(C_1 = 1 \mid E = 1, C_2 = 1) < p(C_1 = 1 \mid E = 1) < p(C_1 = 1 \mid E = 1, C_2 = 0)$. Explaining away is one of the many ways that causal and associative knowledge differs, as it entails that the presence/absence of one variable lowers/raises the probability of another. \Cref{fig:comparison_agg_3} reveals that humans indeed exhibited the explaining away pattern. However, this effect is quite weak and theoretical analyses have revealed that explaining away is often weaker than is normatively warranted \cite{davis2020mutation, rehder2024inhibitory}. Note that when the effect $E$ is absent (\Cref{fig:comparison_agg_4}), explaining away is absent entirely.




% dataset description
%\\ \todo \textit{refer to counterbalance conditions in + and - like in Bob's original paper or as in the datasets where p is + and m is -? }\\
%\Cref{tab:variables} presents the conditions  senses of the variables, i.e., the adjective used to describe the "on-state" of each binary variable. For the off-state, the opposite sense was used. The four counterbalance conditions can then be shortened to \texttt{+++}, \texttt{+-{}-}, \texttt{-{}+-}, and \texttt{-{}-{}-}, where the first +/- presents $C_1$, the second $C_2$, and final one $E$ and encode the specific sense of the binary variables (e.g., high/low, small/large). For example, in the \texttt{+++} condition within the economics domain, \textit{low} interest rates, \textit{small} trade deficits, and \textit{high} retirement savings corresponded to the on-state. In the \texttt{-{}+{}-} condition, the senses for interest rates (C_1) and retirement savings (E) were flipped to high and low, respectively, while trade deficits (C_2) remained small.  Note that the  four counterbalance conditions represent only a subset of all eight possible combinations of variable states and were chosen because they were deemed to be the most plausible ones by subjects in a pilot study conducted by \citeA{rehder2017failures}.

% \begin{table}[htbp]
%   \centering
%   \caption{Variable Names and Senses Used in the Domains}
%   \resizebox{\columnwidth}{!}{%
%   \begin{tabular}{@{}llll@{}}
%       \toprule
%       \textbf{Variable} & \textbf{Economics}          & \textbf{Meteorology}      & \textbf{Sociology}              \\ \midrule
%       Cause $C_1=1$ & Interest rates (low+/high-) & Ozone levels (high+/low-) & Urbanization (high+/low-)       \\
%       Cause $C_2=1$ & Trade deficits (small+/large-) & Air pressure (low+/high-) & Interest in religion (low+/high-) \\
%       Effect $E=1$ & Retirement savings (high+/low-) & Humidity (high+/low-)     & Socio-economic mobility (high+/low-) \\ \bottomrule
%   \end{tabular}%
%   }
%   \label{tab:variables}
% \end{table}



\textbf{Procedure.} 
A key contribution of this work is the creation of a causal inference task dataset, 
enabling direct comparisons between human causal inference judgments  collected in \citeA{rehder2017failures} and LLMs.
The dataset is designed to closely replicate the experimental conditions of \citeA{rehder2017failures} (Experiment 1, Model-Only condition) with some  notable differences:

The procedure for humans consisted of two phases. In the learning phase subjects were presented and tested on the domain knowledge, including the causal mechanisms. In the testing phase they were presented with each of the inference tasks in random order. A graphical representation of the collider structure remained on the screen during testing. 

In contrast, for the LLMs each textual prompt included all the domain knowledge and a single inference task. 
% optional? take out "randomized order" as it shouldn't matter for API calls, but that's how I set up the script
The different domains and inference tasks were presented 
%in a randomized order 
within each of the four counterbalancing groups.
%and repeated five times to ensure balanced sample sizes between humans and each LLM for each experimental condition.
Whereas humans provided their probability judgments using a slider ranging from 0 to 100 with default setting=50.0, LLMs were instructed to provide a numerical answer $\in \{0.0, 100.0\}$. 
%%%% Note: include repeated 5 times ... when talking about other temperature settings than 0.0. since the same script was used for all temperature settings, also for temp=0.0 the query was repeated 5 times. Note that data per each of the 528 unitque experimental conditions (LLMxtaskxDomainxCounterbalance condition where temp=0.0) reveals about only have have the responses following the expected zero variation, partially confirming that temp=0.0 is indeed yielding deterministic ouputs. 235 of the above conditions had a std \neq 0.
%%%% end note


%%% Model fitting:
% Do LLMs reason consistently?
% include tex file
%\input{maybe_someday_useful_text/model_fit_text_hanna/model_fit_methods.tex}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}   %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Exploratory Analysis}
%   \Cref{fig:main_comparison_agg_violin,fig:group_1_violin,fig:group_2_violin,fig:group_3_violin,fig:group_4_violin}}\\
% \todo \textit{describe violin plots \Cref{fig:main_comparison_agg_violin,fig:group_1_violin,fig:group_2_violin,fig:group_3_violin,fig:group_4_violin}}\\
% \todo \textit{Report the main behavioral patterns comparing human and LLM performance \Cref{fig:main_comparison_agg}}


%\subsection*{Human Results}

    %We first review the empirical findings reported in \citeA{rehder2017failures} regarding how humans reason with a simple collider structure. The ratings for all agents are shown in Fig. \ref{fig:main_comparison_agg} where the four panels represent the four inference types described earlier. The red lines are the human responses. Starting with \Cref{fig:comparison_agg_1}, recall that a collider structure entails explaining away: When the effect $E$ is present, one cause should become less/more probable when the other cause is revealed to be present/absent. \Cref{fig:comparison_agg_1} reveals that humans exhibited a explaining away effect such that $p(C_1=1|E=1, C_2=0) > p(C_1=1|E=1, C_2=1)$. However, this effect is quite weak and research with collider networks have usually revealed that explaining away is weaker than is normatively warranted. Note that when the effect $E$ is stipulated to be absent (\Cref{fig:comparison_agg_2}), explaining away is absent entirely. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LINE PLOTS:  %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% aggregated across all domains
\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.1\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/graph_B.pdf}
        \caption{Reference Graph (task II)}
 
        \label{fig:graph}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/line_plots_by_task/all_domains/predictive_inference_all_domains_temp-0.0.pdf}
        \caption{Predictive Inference}
 
        \label{fig:comparison_agg_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/line_plots_by_task/all_domains/conditional_independence_all_domains_temp-0.0.pdf}
        \caption{Independence of $C_1, C_1$}
        \label{fig:comparison_agg_2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/line_plots_by_task/all_domains/effect-present_diagnostic_inference_all_domains_temp-0.0.pdf}
        \caption{Diagnostic Inference \\ \hspace*{11mm} $(E=1)$}
        \label{fig:comparison_agg_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/line_plots_by_task/all_domains/effect-absent_diagnostic_inference_all_domains_temp-0.0.pdf}
        \centering
        \caption{Diagnostic Inference \\\hspace*{11mm}  $(E=0)$}
        \label{fig:comparison_agg_4}
    \end{subfigure}
    \caption{\textbf{Aggregated across all domains:} Likelihood judgments that query node \colordot{inference} has value 1  $\in \{0,100\}$ with bootstrapped 95\% confidence intervals of humans \colorsquare{humans} and LLMs (GPT-3.5 \colorsquare{gptthreefive}, GPT-4o \colorsquare{gptfouro}, Claude \colorsquare{claude}, and Gemini \colorsquare{gemini})  for each inference task (I-XI), aggregated across counterbalancing conditions and domains for temperature value 0.0 (most deterministic). Graphs on the x-axis visualize the conditional probability of the inference tasks (I-XI) where the  nodes are colored according to:  \colordot{inference} $\to$ query node that the question is asked about; \colordot{observed} $\to$ observed $\in \{0,1\}$; and \dashedcircle{}   $\to$  no information on.} 
    \label{fig:main_comparison_agg}
\end{figure*}



\textbf{Comparison of LLMs and Humans.} As an initial assessment of the LLMs we computed the Spearman correlation between their inferences and those of humans in each domain. \Cref{tab:correlations} reveals correlations that are positive and substantial in magnitude, indicating the LLMs are exhibiting a degree of human-like performance on the causal reasoning task. The highest average correlations were displayed by Claude \colorsquare{claude} ($r_s = .631$) and GPT-4o \colorsquare{gptfouro} ($.626$), followed by GPT-3.5 \colorsquare{gptthreefive} ($.462$) and Gemini \colorsquare{gemini} ($.373$). This pattern was observed in all three domains.


\begin{table}[h]
    \centering
        \caption{Spearman correlations $r_s$ between human and LLM inferences in each domain.}
        \begin{small}
    \resizebox{\columnwidth}{!}{ 
    \begin{tabular}{lcccc}
        \toprule
         & \multicolumn{3}{c}{Domain} &  \\
        \cmidrule(lr){2-4}
        Model & Economy ($r_s$) & Sociology ($r_s$) & Weather ($r_s$) & Pooled \\
        \midrule
        Claude \colorsquare{claude}  & 0.557 & 0.637 & 0.698 & 0.631 \\
        GPT-4o \colorsquare{gptfouro}   & 0.662 & 0.572 & 0.645 & 0.626 \\
        GPT-3.5 \colorsquare{gptthreefive}  & 0.419 & 0.450 & 0.518 & 0.462 \\
        Gemini \colorsquare{gemini}  & 0.393 & 0.297 & 0.427 & 0.372 \\
        \bottomrule
    \end{tabular}
    }\end{small}
    \label{tab:correlations}
\end{table}


% Line plot description

\Cref{fig:main_comparison_agg} presents the LLMs' responses to the individual inference tasks averaged over conditions. The four inference types reveal distinct reasoning patterns across agents. 

 %While we tested outputs across a range of temperatures (0.0 - 1.0), model sensitivities varied—for instance, GPT-3.5 was invariant, while Gemini-pro had 32 significant variations. By limiting the analysis to temperature=0.0, we ensure a consistent and interpretable comparison, avoiding confounds from stochastic variability at higher temperatures.
 %The variability in LLM responses stems from the counterbalance conditions, which were aggregated over  in the analysis.

% - LLMs can do the task
\emph{Predictive inferences} (see \Cref{fig:comparison_agg_1}, I-III) for the LLMs were a monotonic increasing function of the number of causes present, similar to the human judgments. This indicates that the LLMs were sensitive to the most rudimentary aspect of the task, namely, that causes make their effects more likely. 

\emph{Independence of causes} (IV-V) means that the state of one cause should not affect the likelihood of the other. In fact, \Cref{fig:comparison_agg_2} shows that GPT-3.5 \colorsquare{gptthreefive} and Gemini \colorsquare{gemini} judged that $p(C_1=1|C_2=1) > p(C_1=1|C_2=0)$ even more egregiously than humans. Claude \colorsquare{claude} violated independence the least whereas GPT-4o \colorsquare{gptfouro} exhibited a small independence violation in the opposite direction. 



\emph{Effect-Present Diagnostic Inference} (\Cref{fig:comparison_agg_3}, VI-VIII) reveals
whether agents explain away, indicated by a positive slope.
GPT-4o \colorsquare{gptfouro} exhibits the strongest explaining away, followed by Claude \colorsquare{claude} and then humans \colorsquare{humans}. Gemini \colorsquare{gemini} and GPT-3.5 \colorsquare{gptthreefive} failed to  
explain away, judging instead that the presence of one cause would \textit{increase} the probabililty of the other.

\emph{Effect-Absent Diagnostic Inference} (\Cref{fig:comparison_agg_4}, IX-XI)
has  all agents produce lower ratings for the cause, with GPT-4o \colorsquare{gptfouro} and Claude \colorsquare{claude} producing the lowest ratings across all conditions and Gemini \colorsquare{gemini} seeming to be closest aligned with  humans  \colorsquare{humans}. 
While humans and Gemini \colorsquare{gemini} are more likely to assign ratings in the middle of the scale, 
GPT-4o \colorsquare{gptfouro} is most inclined to assign a rating of 0 
and treated the causal relations as closer to necessary and sufficient than any other agent. This conclusion ends up being supported by the model fitting that follows, which yielded especially large estimates of the strengths of the causal relations for GPT-4o \colorsquare{gptfouro} (see \Cref{fig:fitted_parameters}).


Note that the LLM responses in \Cref{fig:main_comparison_agg} were distributed more broadly than human's: Whereas the difference between the highest and lowest judgment was at least 78 for the LLMs (and was 100 for GPT-4o), it was only 66 for the humans. This tendency might stem from the experimental setup. Whereas LLMs were prompted to generate a single numeric value, humans responded using an interactive slider that defaulted to 50. This default could have introduced a motor bias that encouraged responses near the middle of the scale.

% summary
These inference patterns suggest LLMs capture core causal reasoning principles and are aligned with human responses to a considerable degree. 
Some LLMs' reasoning patterns in \Cref{fig:main_comparison_agg} reveal that causal relations were treated as deterministic, necessary, and sufficient (e.g., GPT-4o \colorsquare{gptfouro}), which is also supported later when we fit CBNs (see \Cref{fig:fitted_parameters}).
Inferences varied over domains only modestly but were more pronounced for the LLMs,
suggesting that whereas humans carried out more abstract, content-free reasoning, LLMs relied more on domain knowledge.


%% End Line plot description.
%%%%%%%%%%%%%%%%%%%%%%%%%%
% Likely drop / shorten significantly
% % Violin plot description
% \paragraph{Response distributions: LLMs respond on wider range of values than humans}
% \Cref{fig:main_comparison_agg_violin} shows the aggregated responses of humans and LLMs per domain across all inference tasks and temperature settings. 
% % Key observations:
% Across subjects, the violin plot reveals different response distributions across domains (economy, sociology, and weather) for all agents where GPT-4o and Claude appear most similar in shape. All agents have a similar median at around 50.
% Within subjects, the response distribution seems similar across domains. 
% % 
% This pattern stability across domains may indicate that agents reason similarly across domains, and may suggest that the agents are not leveraging domain-specific knowledge.
% To further investigate this, we conducted a statistical analysis to test for differences in reasoning across domains and agents.

%\todo \textit{Say something about the extreme values of likelihood judgements? LLMs seem to be much more willing to give extreme likelihood judgements (0 or 100) compared to humans.}

% \todo \textit{for domain specific plots: reorganize subfigures according to new label order: predicitive first, then cond. inpdep, then diagnostic with E=1 followed by E=0.}
% economy
% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
     
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/economy/predictive_inference_economy_temp-0.0.pdf}
%         \caption{Predictive Inference.}
%         \label{fig:comparison_agg_econ_1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/economy/conditional_independence_economy_temp-0.0.pdf}
%         \caption{Conditional Independence.}
        
%         \label{fig:comparison_agg_econ_2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
        
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/economy/effect-present_diagnostic_inference_economy_temp-0.0.pdf}
%         \caption{$E=1$ Diagnostic Inference.}
%         \label{fig:comparison_agg_econ_3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/economy/effect-absent_diagnostic_inference_economy_temp-0.0.pdf}
%         \caption{$E=0$ Diagnostic Inference.}
%         \label{fig:comparison_agg_econ_4}
%     \end{subfigure}
%     \caption{\textbf{Economy:} Likelihood judgments that query node \colordot{inference} has value 1  $\in \{0,100\}$ with bootstrapped 95\% confidence intervals of humans \colorsquare{humans} and LLMs (GPT-3.5 \colorsquare{gptthreefive}, GPT-4o \colorsquare{gptfouro}, Claude \colorsquare{claude}, and Gemini \colorsquare{gemini})  for each inference task (I-XI), aggregated across counterbalancing conditions and domains for temperature value 0.0 (most deterministic). Graphs on the x-axis visualize the conditional probability of the inference tasks (I-XI) where the  nodes are colored according to:  \colordot{inference} $\to$ query node that the question is asked about; \colordot{observed} $\to$ observed $\in \{0,1\}$; and \dashedcircle{}   $\to$  no information on.}
%     \label{fig:main_comparison_ecconomy}
% \end{figure*}

% % sociology
% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/sociology/predictive_inference_sociology_temp-0.0.pdf}
%         \caption{Predictive Inference.}
%         \label{fig:comparison_agg_socio_1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/sociology/conditional_independence_sociology_temp-0.0.pdf}
%         \caption{Conditional Independence.}
%         \label{fig:comparison_agg_socio_2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/sociology/effect-present_diagnostic_inference_sociology_temp-0.0.pdf}
%         \caption{$E=1$ Diagnostic Inference.}
%         \label{fig:comparison_agg_socio_3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/sociology/effect-absent_diagnostic_inference_sociology_temp-0.0.pdf}
%         \caption{$E=0$ Diagnostic Inference.}
%         \label{fig:comparison_agg_socio_4}
%     \end{subfigure}
%     \caption{\textbf{Sociology:} Likelihood judgments that query node \colordot{inference} has value 1  $\in \{0,100\}$ with bootstrapped 95\% confidence intervals of humans \colorsquare{humans} and LLMs (GPT-3.5 \colorsquare{gptthreefive}, GPT-4o \colorsquare{gptfouro}, Claude \colorsquare{claude}, and Gemini \colorsquare{gemini})  for each inference task (I-XI), aggregated across counterbalancing conditions and domains for temperature value 0.0 (most deterministic). Graphs on the x-axis visualize the conditional probability of the inference tasks (I-XI) where the  nodes are colored according to:  \colordot{inference} $\to$ query node that the question is asked about; \colordot{observed} $\to$ observed $\in \{0,1\}$; and \dashedcircle{}   $\to$  no information on.}
%     \label{fig:main_comparison_sociology}
% \end{figure*}



% % weather
% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/weather/predictive_inference_weather_temp-0.0.pdf}
%         \caption{Predictive Inference.}
%         \label{fig:comparison_agg_weather_1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering 
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/weather/conditional_independence_weather_temp-0.0.pdf}
%         \caption{Conditional Independence.}
%         \label{fig:comparison_agg_weather_2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/weather/effect-present_diagnostic_inference_weather_temp-0.0.pdf}
%         \caption{$E=1$ Diagnostic Inference.}
%         \label{fig:comparison_agg_weather_3}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.24\textwidth}
%         \centering 
%         \includegraphics[width=\textwidth]{figures/line_plots_by_task/weather/effect-absent_diagnostic_inference_weather_temp-0.0.pdf}
%         \caption{$E=0$ Diagnostic Inference.}
%         \label{fig:comparison_agg_weather_4}
%     \end{subfigure}
%     \caption{\textbf{Weather:} Likelihood judgments that query node \colordot{inference} has value 1  $\in \{0,100\}$ with bootstrapped 95\% confidence intervals of humans \colorsquare{humans} and LLMs (GPT-3.5 \colorsquare{gptthreefive}, GPT-4o \colorsquare{gptfouro}, Claude \colorsquare{claude}, and Gemini \colorsquare{gemini})  for each inference task (I-XI), aggregated across counterbalancing conditions and domains for temperature value 0.0 (most deterministic). Graphs on the x-axis visualize the conditional probability of the inference tasks (I-XI) where the  nodes are colored according to:  \colordot{inference} $\to$ query node that the question is asked about; \colordot{observed} $\to$ observed $\in \{0,1\}$; and \dashedcircle{}   $\to$  no information on.}
%     \label{fig:main_comparison_weather}
% \end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% VIOLIN PlOTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% overall comparison
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Violin Plots: all tasks %%%%%%
%include tex file:
%\input{maybe_someday_useful_text/results/violin_plots/all_tasks.tex}  
  

% %%% Violin Plots: effect present diagnostic inference %%%%%%
% %include tex file:
% \input{maybe_someday_useful_text/results/violin_plots/effect_present_diagnostic.tex}  
  

% %%% Violin Plots: effect absent diagnostic inference %%%%%%
% %include tex file:
% \input{maybe_someday_useful_text/results/violin_plots/effect_absent_diagnostic.tex}  

% %%% Violin Plots: conditional independence %%%%%%
% %include tex file:
% \input{maybe_someday_useful_text/results/violin_plots/cond_independence.tex}  



% %%% Violin Plots: predictive inference %%%%%%
% %include tex file:
% \input{maybe_someday_useful_text/results/violin_plots/predictive.tex}  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% END violin plots
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Temperature Analysis %%%%%%
%include tex file
%\input{maybe_someday_useful_text/results/temperature_analysis.tex}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Correlation Analysis %%%%%%
%include tex file
%\input{maybe_someday_useful_text/results/correlation_analysis.tex}
  
%%%%%%%% correlation figure
% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figures/results/correlation_results_by_domain_humans_vs_llms_barplot.png}
%     \caption{Comparison of human and LLM responses across inference tasks aggregated across all domains, temperature values (LLMs only), and counterbalancing conditions.}
%     \label{fig:corr}
%   \end{figure*}
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Regression Analysis (vanilla) %%%%%%
%include tex file
%\input{maybe_someday_useful_text/results/regression_analysis_vanilla.tex}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Regression Analysis: Mixed effects model %%%%%%
%include tex file
%\input{maybe_someday_useful_text/results/reg_mixed_effects_model.tex}  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Model fitting  %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{CBN Model Fitting.}
We evaluate LLMs and humans against normative inferences from a causal Bayes net (CBN). Since agents received only verbal descriptions, the CBN's parameters $\theta_M$ were treated as free parameters and fit to the data. These parameters include the prior probabilities over causes $p(C_1), p(C_2)$, the causal strength parameters $w_{C_1,E}, w_{C_2,E}$ representing the strength of each causal relationship $C_{1,2} \to E$, and the baseline parameter $w_E$ 
capturing the influence of any exogenous causal influence on $E$.
%In addition to comparing the LLMs to humans, we evaluate them relative to the normative inferences generated by a causal Bayes net (CBN). Because the agents were presented with only a verbal description of the collider CBN, to generate quantitative predictions,  the prior probabilities of the causes, the causal strengths, etc. were treated as free parameters.
%We evaluate LLMs and humans against normative inferences from a causal Bayes net (CBN). Since agents received only verbal descriptions, the CBN's parameters  were treated as free parameters and fit to the data."
% The judgments of each agent were fit according to $\mathrm{Judgment} (t_i) =100 * p(t_i|M,\theta_M)$ where $M$ is a CBN, $\theta_M$ are its parameters, and $t_i$ is a task (i.e., a conditional probability query). 
% %% updated version

% % todo: condense neöpw
% The judgments were modeled as:
% \begin{equation}
%     \mathrm{Judgment}(q_k) = 100 \cdot p(Q_k = 1 | X_k = x_k, M, \theta_M)
% \end{equation}
% where:
% \begin{itemize}
%     \item $k \in \{I,\ldots,XI\}$ indexes the inference task
%     \item $q_k$ is the queried variable \colordot{inference}
%     \item $X_k$ is the set of observed variables \colordot{observed} for task $k$
%     \item $x_k$ are their observed values (0 or 1)
%     \item $M$ is the collider CBN $C_1 \rightarrow E \leftarrow C_2$
%     \item $\theta_M = \{p(C_1), p(C_2), w_{C_iE}, w_{C_jE}, w_E\}$ are the model parameters
% \end{itemize}

% The judgments of each agent were fit according to 
% $\mathrm{Judgment}(i) = 100 \cdot p(Q_i = 1 | X_i = x_i, M, \theta_M)$
% %\mathrm{Judgment} (q_i) =100 \cdot p(q_i|M, X_i, \theta_M)$ 
% where $M$ is a CBN, $\theta_M$ are its parameters, $X_i$ are the observed variables \colordot{observed}, and $q_i$ is the query node \colordot{inference} specified by each task $i \in \{I,XI\}$.

The CBN is used to derive a joint probability distribution which was then used to derive the conditional probability appropriate for that task. For a collider causal graph $C_1 \rightarrow E \leftarrow C_2$, the joint distribution was derived assuming that $p(C_1,C_2,E) = p(E|C_1,C_2)p(C_1)p(C_2)$ and that $p(E=1|C_1,C_2) = 1/(1 + \exp(-(C_1w_{C_1,E} + C_2w_{C_2,E} + w_E)))$, where $w_{C_1,E}$ and $w_{C_2,E}$ represent the strength of $C_1 \rightarrow E$ and $C_2 \rightarrow E$, respectively, and $C_1$ and $C_2$ are each coded as 1 when present and $-1$ when absent.\footnote{Note in this literature it is common to assume ``noisy logical'' generating functions, such as the noisy-OR function introduced in the 
PowerPC theory  of causal learning by \citeA{cheng1997covariation}. We report fits using the logistic generating function as it consistently yielded better fits to these data sets.} The CBNs were fit to each agent's set of causal judgments by identifying parameters that minimized squared error. Fits were carried out via an initial grid search followed by optimization.

We fit two variants of the basic collider CBN that varied in their number of free parameters. The first variant had three: $w_C$ represented the prior probability of both $C_1$ and $C_2$, $w_{C,E}$ represented the strength of the two causal relations (thus for this model, $w_{C_1,E} = w_{C_2,E} = w_{C,E}$), and $w_E$ absorbs the influence of any exogenous causal influence on $E$. A 4-parameter variant allowed the strength of the causal relations to differ by having two causal strength parameters, $w_{C_1,E}$ and $w_{C_2,E}$. $w_C$ was constrained to the range [0, 1] whereas the causal strength parameters were constrained to the range [-3, 3]. For the human data these CBNs were fit to each subject. For the LLMs, they were separately fit to the judgments generated in each of the 3 domains $\times$ 4 counterbalancing $= 12$ conditions. 

Table \ref{ModelFits-3v4Params} presents the CBNs' best fitting parameters averaged over conditions/subjects. Several interesting trends emerge. First, the judgments of all LLMs exhibited substantial correlations with their fitted CBNs, spanning the range $.559-.882$. These results compare with the corresponding correlations for the human data (about .77). The best fitting LLMs were GPT-4o and Claude-3-Opus; for both the correlations associated with their 4-parameter fits exceeded .88 and their model loss (the average absolute difference between predicted and observed values) was less than 13 points on the 0-100 scale. Indeed, the responses of these LLMs were more correlated with their fitted CBNs than those of their human counterparts. In contrast, the fits of GPT-3.5 and Gemini-Pro were worse than the other LLMs and the human reasoners. The fits of Gemini-Pro were the poorest, exhibiting a model loss of 23.4 for even the 4-parameter CBN.

\begin{table*}[h]
\begin{small}
\centering
\caption{Fits of collider causal graphical models to five data sets.}
\begin{tabular}{lllllllllll}
\toprule 
    & &
    \multicolumn{5}{c}{Average Parameter Estimates} & 
    \multicolumn{3}{c}{Measures of Fit}\\
    Agent & $NP$ & $w_C$ & $w_{C,E}$ & $w_{C_1,E}$ & $w_{C_2,E}$ & $w_{E}$ & $R$ & $AIC$ & Loss \\
\midrule
   Humans \\ 
   \hspace{3mm}3-Parameter & 
   3 & .528 & 1.06 & & & 0.91 &  \textbf{.770}  & \textbf{114.4}  & \textbf{11.8} \\
   \hspace{3mm}4-Parameter & 
   4 & .529 & & 1.09 & 1.04 & 0.92 & .783 & 115.4 & 11.5\\
   Gemini-Pro \\ 
   \hspace{3mm}3-Parameter & 
   3 & .634 & 1.09 & & & 1.25 & .559 & 147.9 & 27.3 \\
   \hspace{3mm}4-Parameter & 
   4 & .672 & & 1.78 & 0.79 & 1.83 & \textbf{.674} & \textbf{145.6} &  \textbf{23.4}\\
   GPT-3.5-Turbo \\ 
   \hspace{3mm}3-Parameter & 
   3 & .553 & 1.10 & & & 1.45 & .655 & 138.1 & 21.3 \\
   \hspace{3mm}4-Parameter & 
   4 & .547 & & 1.41 & 0.96 & 1.50 & \textbf{.726} & \textbf{137.2} &  \textbf{19.4}\\
   GPT-4o \\ 
   \hspace{3mm}3-Parameter & 
   3 & .420 & 2.49 & & & 0.77 & .862 & 121.5 & 12.8 \\
   \hspace{3mm}4-Parameter & 
   4 & .422 & & 2.53 & 2.22 & 0.80 & \textbf{.882} & \textbf{120.4} &  \textbf{12.0}\\
   Claude-3-Opus \\ 
   \hspace{3mm}3-Parameter & 
   3 & .447 & 1.74 & & & 0.77 & .857 & 112.9 & 11.3 \\
   \hspace{3mm}4-Parameter & 
   4 & .448 & & 2.01 & 1.55 & 0.92 & \textbf{.893} & \textbf{112.1} &  \textbf{10.3}\\
\bottomrule
\end{tabular}%}
\label{ModelFits-3v4Params}
\centering
\caption*{\emph{Note}: $NP$ = Number of model parameters. $AIC$ = Akaike Information Criterion.}
\end{small}
\end{table*}


Regarding the contrast between the 3- and 4-parameter CBNs, all LLMs were better fit by the model that allowed the two causal relations to have different strengths (according to $AIC$, which corrects for the 4-parameter model's extra parameter). That the LLMs didn't necessarily assume that the two causal relations were the same strength provides additional evidence that the LLMs were making use of domain knowledge. In contrast, the fits to the human data did not benefit from the extra causal strength parameter. This result is broadly consistent with past analyses of this data set showing that neither domain nor the counterbalancing factors had a significant effect on subjects' judgments \cite{rehder2017failures}. In contrast, the LLM judgments were apparently more influenced by knowledge they had about economics, meteorology, and sociology.\footnote{We also fit CBNs in which the two causes $C_1$ and $C_2$ each had their own parameter representing their prior probability. Generally, these models did not yield a better fit than the models with a single $w_C$ parameter. The one exception was Gemini-Pro, but as this model yielded relatively poor fits, we do not discuss this result further.}

To provide additional insight into differences between agents, the parameter values from the 4-parameter CBN are presented \Cref{fig:fitted_parameters}. The figure also includes an additional quantity, the absolute difference between the two fitted causal strength parameters $w_{C_1,E}$ and $w_{C_2,E}$, averaged over domain and counterbalance conditions. The parameter representing the priors on the causes $w_C$ was about the same for all agents (and were all close to .5). In contrast, the causal strength parameters $w_{C_1,E}$ and $w_{C_2,E}$ estimated for the LLMs were generally much larger than those for the humans. The especially large causal strengths estimated for GPT-4o (averages in excess of 2.0) explains the greater range of responses exhibited by that model as compared to human (see \Cref{fig:main_comparison_agg}). The average absolute difference between the two causal strengths was also much larger for the LLMs: Whereas for the average human subject $w_{C_1,E}$ and $w_{C_2,E}$ differed by only .12, for the LLMs they differed by at least .45. Moreover, the error bars in the figure, which depict the standard deviation of each parameter, shows that the causal strength estimates for the LLMs varied much more widely across conditions than they did for the human subjects. This corroborates the claim that the LLMs were estimating the strengths of the causal relations on the basis of their domain knowledge to a greater degree than the human reasoners. 

\begin{figure}[h]
    \includegraphics[width=\columnwidth]{figures/bob_model_fit_plot.pdf}
    \centering
    \caption{The parameter values from the 4-parameter causal Bayes net fits. Error bars are standard deviations.}
    \label{fig:fitted_parameters}
\end{figure}
\textbf{Fitting a Psychological Model.} 
We also fit the LLM inferences with a model proposed as an account human causal reasoning, the \textit{mutation sampler} \cite{davis2020mutation}. The mutation sampler is an example of a \textit{rational process model}, an algorithm that yields normative responses when cognitive resources are unlimited but that produces errors when they are not \cite{johnson2016computational, lieder2012burn, vul2014one}. The mutation sampler carries out MCMC sampling over a causal graph's \textit{state space} and draws inferences on the basis of samples. But because sampling begins at one of the graph's \textit{prototypes states} (when causal relations are all generative, the states where variables are all present or all absent), errors are introduced when the number of samples drawn is limited. \citeA{davis2020mutation} showed that the mutation sampler accounts for the independence violations that arise in a wide variety of network topologies and the weak explaining away that arises when reasoning about collider graphs. 

The mutation sampler was fit to the human data and the four LLMs.  \Cref{ModelFits-MutSampler} presents the improvement for each data set relative to the best fitting CBN in  \Cref{ModelFits-3v4Params} realized by adding the mutation sampler's \textit{chain length} free parameter $\lambda$ representing the number of MCMC samples. Replicating past findings, the mutation sampler yielded a better fit to the human data compared to the 3-parameter CBN \cite{davis2020mutation}. The new result is that it also yielded more favorable $AIC$s for 3 of 4 LLMs; a better fit was not achieved only for GPT-4o. Given that the mutation sampler was designed to capture the associative reasoning processes that influence people's causal inferences, these results reinforce the conclusion that LLMs are often swayed by the same factor.


\begin{table}[ht]
\caption{Mutation sampler fits.}
\label{ModelFits-MutSampler}
\centering
\begin{small}
\begin{tabular}{lccccc}
\toprule 
    Agent & $NP$ & $\lambda$ & $R$ & $AIC$ & $Loss$\\
\midrule
   Human & 4 & 3.7 & \textbf{.810} & \textbf{113.0} & \textbf{10.8} \\
   Gemini-Pro & 5 & 2.9 & \textbf{.791} & \textbf{137.3} & \textbf{18.4} \\
   GPT-3.5-Turbo & 5 & 2.2 & \textbf{.853} & \textbf{128.1} & \textbf{13.9} \\
   GPT-4o & 5 & 5.2 & .886 & 121.0 & 11.8 \\
   Claude-3 & 5 & 4.3 & \textbf{.907} & \textbf{111.7} & \textbf{9.9} \\
\bottomrule 
\end{tabular}
\end{small}
\end{table}
\vspace{-.2em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We compared the causal reasoning abilities of large language models to those of people. In \citeA{rehder2017failures} undergraduates were taught hypothetical causal knowledge consisting of three variables that formed a collider causal graph and then were asked to draw simple causal inferences. Our first main finding is that given exactly the same information, LLMs can do the task. 
% predicitve inference results
That is, after being told that the presence of one variable $C$ causes the presence of another $E$, LLMs will judge the effect $E$ is more likely when a cause  $C$ is present versus absent (and vice versa). 
Indeed, across all domains and tasks, the Spearman correlation $r_s$ between LLM  and human inferences ranged from $.372$ to $.631$.


% explaining away
As discussed an important property of collider structures is that they entail explaining away. 
On one hand, GPT-4o and Claude-3 exhibited the basic explaining away pattern at least partially, just like people did. 
 GPT-4o exhibited the strongest explaining away (indicated by most positive slope in \Cref{fig:comparison_agg_3}), while Gemini-Pro and GPT-3.5 did not exhibit explaining away.  One interpretation of this result is that the two latter models were reasoning more ``associatively,'' that is, without regard to causal semantics of a collider network. At least on this one test of ``causal understanding,'' Gemini-Pro and GPT-3.5 failed.

 % conditional independence
A collider structure also entails that the two causes should be independent. As discussed, human reasoners often violate independence, which in a collider with generative causes means that the two causes are treated as positively correlated \cite{davis2020mutation}.
In other words, human causal inferences also exhibit a degree of ``associative thinking." Gemini-Pro and GPT-3.5's also treated the causes as positively correlated, consistent with the interpretation of their explaining away inferences offered above. Interestingly, whereas Claude treated the causes as uncorrelated, GPT-4o treated them as somewhat negatively correlated.


% model fits
In addition to humans, we compared LLMs to the normative inferences of fitted causal Bayes nets. The correlations between the LLM and normative inferences were between .559 and .882, as compared to about .77 for the humans. GPT-3.5 and especially Gemini-Pro exhibited the poorest correlations with the normative inferences and ones that were worse than for humans; GPT-4o and Claude exhibited the highest correlations and ones that were \textit{higher} than for humans. 

The parameters derived from the CBN fits also provided insight into how the agents were reasoning in the different domains. As mentioned, the materials were drawn from domains about which undergraduates were expected to know little. Consistent with this expectation, the standard deviations associated with the human's fitted CBN parameters were relatively low (e.g., for the average human reasoner the difference between estimated strength of the two causal relations differed by only .12). In contrast, the standard deviations for with the LLM's fitted model parameters were much higher (e.g., the difference between the two causal strengths was at least .45). These results provide evidence that the LLM inferences were affected by the their domain knowledge more than those of the human reasoners. 

\textbf{Future Work.} There are numerous avenues for future research. 
Here we compared human and LLM inferences on only one simple causal structure, whereas humans have been tested on causal networks with different topologies (e.g., forks, chain, etc.), causal relations (inhibitory vs. generative), integration functions (e.g., causes that combine conjunctively rather than independently), with more than three variables, and with continuous variables rather than binary ones. Besides the simple causal inferences examined here, there is a wealth of data on how humans intervene on causal systems, make causal attributions in cases of actual causation, and learn causal systems from observed data. Regarding LLMs, a deeper analysis of the effects of domain knowledge on their inferences is warranted as such knowledge can affect both independence (via inferred causal connections between the collider's causes) and explaining away \cite<via treating the two causal relations as interactive rather than independent;>[]{cruz2020explainingaway, morris1995one}.
It is also important to better understand how their inferences are affected by factors such as the temperature parameter.



Overall,  LLMs respond meaningfully to the same complex prompts used in research on  human causal reasoning. GPT-4o and Claude-3 aligned more with normative inferences than humans,  while Gemini-Pro and GPT-3.5 were often less normative than humans.
Rather than treating the task abstractly, LLMs drew inferences based on their domain knowledge.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{bibliography.bib}

%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{maybe_someday_useful_text/appendix.tex}  

\end{document}
