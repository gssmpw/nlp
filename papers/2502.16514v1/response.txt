\section{Related Work and Background}
\textbf{Methods in Detecting Hallucination.} 
% open-book vs closed-book? 
Recent fact-checking research  Zellers et al., "Revealing Flaws in NLP Models: Identifying Pitfalls in Machine Translation, Question Answering, and Summarization" use Retrieval-Augmented Generation (RAG)  Guu et al., "REALM: Towards Image Text Matching with Automated Realism Metrics" and external knowledge bases like DBpedia  Bizer et al., "DBpedia - A crystallization point for structured information extraction based on Wikipedia"  and Wikidata  Vrandečić et al., "Wikidata: a free collaborative knowledge base" to verify generated claims by retrieving structured or semi-structured data.
% These methods rely on structured or semi-structured data from external repositories to check factual errors by querying related entities and their relationships. 
Another line of research  Thorne et al., "FEVER: A Large-Scale Dataset for Stance Classification, Fact-Checking, and Open-Domain Question Answering" focuses on verifying factual consistency using LLMs with grounding documents. These approaches harness LLMs’ reasoning and language capabilities to fact-check claims against textual evidence. While effective for short texts, they often fail to capture fine-grained inconsistencies in longer documents, limiting their accuracy.
Our work builds on this second setting, aiming to improve fact-checking performance on long texts by enhancing LLMs with structured graph-based reasoning.
% These approaches leverage the reasoning and language capabilities of LLMs to verify claims against relevant knowledge sources. Although effective for short texts, these approaches often overlook detailed errors in longer texts, resulting in insufficient fact-checking accuracy. 

\begin{figure*}[t!]
    % \vspace{-4mm}
    \centering
    \includegraphics[width=1\textwidth]{graphcheck-framework-6.pdf}
    \caption{An illustration of the GraphCheck framework. Firstly, an LLM extracts entity-relation triples from both the claim and the document to construct KGs, respectively. 
    A GNN pre-trained with external text graph data is then used to obtain graph embeddings from both KGs. 
    These graph embeddings, combined with the text embeddings, are fed into an LLM for final fact-checking. 
    This approach enables the LLM to perform fine-grained fact-checking by leveraging key triples in the KG (highlighted) alongside the text information.}
    \label{fig:pipline}
    % \vspace{-3mm}
\end{figure*}

\noindent\textbf{Fact-Checking on Long Texts.}
To address the challenge of capturing detailed errors in long texts, recent methods have shifted towards using fine-grained units for fact-checking. 
Methods like FactScore  Wang et al., "Facts are not all that matters: a Study of Misleading Evidence and Its Role in NLP Tasks" MiniCheck  Bhardwaj et al., "MiniCheck: A Lightweight Tool for Evaluating Natural Language Processing Models" and ACUEval  Liu et al., "ACUEval: An Efficient Framework for Automated Evaluation of NLP Models" focus on extracting atomic units from the generated text to enable fine-grained fact verification. 
However, these fine-grained fact-checking methods often require multiple calls to verify each unit or triple, especially for long texts, which greatly increases computational cost and time.
In contrast, our approach uses KGs to model complex entity relationships in long texts, enabling fine-grained verification in a single call. This avoids repetitive calls and significantly improves efficiency.

\noindent\textbf{Graph-based Methods for Enhancing Factuality. }
Previous graph-based fact-checking methods have primarily focused on isolated triple evaluations or document-level encoding, often overlooking the global graph structure and topological information. 
GraphEval  Wadden et al., "GraphEval: A Graph-Based Framework for Evaluating NLP Models" extracts triples from claims and evaluates their factual consistency individually using a pretrained NLI model. 
However, it also relies on pairwise comparisons and does not incorporate the overall graph structure, limiting its ability to capture complex relationships. 
FactGraph  Fan et al., "FactGraph: A Graph-Based Framework for Fact-Checking with External Knowledge" employs graph encoders to process documents and summary semantic graphs extracted via OpenIE. It then combines text and graph embeddings through an MLP for the final prediction. 
However, as a pre-LLM method, it lacks the powerful contextual reasoning ability of modern models. 
AMRFact  Li et al., "AMRFact: An AMR-Based Framework for Fact-Checking with Graph Reasoning" leverages AMR graphs to represent document structures and guide factual summarization generation, focusing on structured summarization rather than direct fact verification.
Unlike previous methods, our approach integrates a trainable GNN with an LLM, combining long-form contextual understanding with structured knowledge from extracted KGs. By incorporating graph reasoning, our model captures complex entity relationships and logical structures, enabling fine-grained fact verification in a single comparison. 
This enhanced reasoning ability allows the model to generalize effectively to specialized domains.