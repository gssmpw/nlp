\section{Related Work and Background}
\textbf{Methods in Detecting Hallucination.} 
% open-book vs closed-book? 
Recent fact-checking research \cite{yuan2023zero, kim2023factkg} use Retrieval-Augmented Generation (RAG) \cite{Fan2024ASO,yang2025retrieval} and external knowledge bases like DBpedia \cite{lehmann2015dbpedia} and Wikidata \cite{vrandevcic2014wikidata} to verify generated claims by retrieving structured or semi-structured data.
% These methods rely on structured or semi-structured data from external repositories to check factual errors by querying related entities and their relationships. 
Another line of research \cite{manakul2023selfcheckgpt, mundler2023self} focuses on verifying factual consistency using LLMs with grounding documents. These approaches harness LLMsâ€™ reasoning and language capabilities to fact-check claims against textual evidence. While effective for short texts, they often fail to capture fine-grained inconsistencies in longer documents, limiting their accuracy.
Our work builds on this second setting, aiming to improve fact-checking performance on long texts by enhancing LLMs with structured graph-based reasoning.
% These approaches leverage the reasoning and language capabilities of LLMs to verify claims against relevant knowledge sources. Although effective for short texts, these approaches often overlook detailed errors in longer texts, resulting in insufficient fact-checking accuracy. 

\begin{figure*}[t!]
    % \vspace{-4mm}
    \centering
    \includegraphics[width=1\textwidth]{graphcheck-framework-6.pdf}
    \caption{An illustration of the GraphCheck framework. Firstly, an LLM extracts entity-relation triples from both the claim and the document to construct KGs, respectively. 
    A GNN pre-trained with external text graph data is then used to obtain graph embeddings from both KGs. 
    These graph embeddings, combined with the text embeddings, are fed into an LLM for final fact-checking. 
    This approach enables the LLM to perform fine-grained fact-checking by leveraging key triples in the KG (highlighted) alongside the text information.}
    \label{fig:pipline}
    % \vspace{-3mm}
\end{figure*}

\noindent\textbf{Fact-Checking on Long Texts.}
To address the challenge of capturing detailed errors in long texts, recent methods have shifted towards using fine-grained units for fact-checking. 
Methods like FactScore \cite{min2023factscore}, MiniCheck \cite{tang2024minicheck}, and ACUEval \cite{wan2024acueval} focus on extracting atomic units from the generated text to enable fine-grained fact verification. 
However, these fine-grained fact-checking methods often require multiple calls to verify each unit or triple, especially for long texts, which greatly increases computational cost and time.
In contrast, our approach uses KGs to model complex entity relationships in long texts, enabling fine-grained verification in a single call. This avoids repetitive calls and significantly improves efficiency.

\noindent\textbf{Graph-based Methods for Enhancing Factuality. }
Previous graph-based fact-checking methods have primarily focused on isolated triple evaluations or document-level encoding, often overlooking the global graph structure and topological information. 
GraphEval \cite{liu2024evaluating} extracts triples from claims and evaluates their factual consistency individually using a pretrained NLI model. 
However, it also relies on pairwise comparisons and does not incorporate the overall graph structure, limiting its ability to capture complex relationships. 
FactGraph \cite{ribeiro2022factgraph} employs graph encoders to process documents and summary semantic graphs extracted via OpenIE. It then combines text and graph embeddings through an MLP for the final prediction. 
However, as a pre-LLM method, it lacks the powerful contextual reasoning ability of modern models. 
AMRFact \cite{qiu_amrfact_2024} leverages AMR graphs to represent document structures and guide factual summarization generation, focusing on structured summarization rather than direct fact verification.
Unlike previous methods, our approach integrates a trainable GNN with an LLM, combining long-form contextual understanding with structured knowledge from extracted KGs. By incorporating graph reasoning, our model captures complex entity relationships and logical structures, enabling fine-grained fact verification in a single comparison. 
This enhanced reasoning ability allows the model to generalize effectively to specialized domains.


% GraphEval~\cite{liu2024evaluating} extracted triples from the claim and evaluated the factual consistency of each triple individually with a pre-trained NLI model. Also pairwised comparison and overlooked the global graph structure and graph topological information.
% FactGraph~\cite{ribeiro2022factgraph} uses graph encoders to encode document and summarization semantic graphs (extracted by OpenIE), then combine text embedding and graph embedding with a MLP to obtain final classification results. pre-LLM No LLM involved, no powerful context reasoning ability.
% AMRFact~\cite{qiu_amrfact_2024} use AMR graphs to represent document structure and guide factual summarization generaton. 
% Ours: trainable GNN and enhance with LLM long-form contextual understanding with extracted KGs structrued data.
% graph reasoning ability. This ability can generalize to specialized domains.