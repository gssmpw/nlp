
@misc{wang_openfactcheck_2024,
	title = {{OpenFactCheck}: {A} {Unified} {Framework} for {Factuality} {Evaluation} of {LLMs}},
	shorttitle = {{OpenFactCheck}},
	url = {http://arxiv.org/abs/2405.05583},
	abstract = {The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM’s factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers’ verification results using human-annotated datasets. OpenFactCheck is publicly released at https: //github.com/yuxiaw/OpenFactCheck.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Wang, Yuxia and Wang, Minghan and Iqbal, Hasan and Georgiev, Georgi and Geng, Jiahui and Nakov, Preslav},
	month = may,
	year = {2024},
	note = {arXiv:2405.05583 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 8 tables, 8 figures},
	file = {PDF:/Users/haoliu/Zotero/storage/QCGGLLUK/Wang et al. - 2024 - OpenFactCheck A Unified Framework for Factuality Evaluation of LLMs.pdf:application/pdf},
}

@article{shahi_fakekg_2023,
	title = {{FakeKG}: {A} {Knowledge} {Graph} of {Fake} {Claims} for {Improving} {Automated} {Fact}-{Checking} ({Student} {Abstract})},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{FakeKG}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27020},
	doi = {10.1609/aaai.v37i13.27020},
	abstract = {False information could be dangerous if the claim is not debunked timely. Fact-checking organisations get a high volume of claims on different topics with immense velocity. The efficiency of the fact-checkers decreases due to 3V problems volume, velocity and variety. Especially during crises or elections, fact-checkers cannot handle user requests to verify the claim. Until now, no real-time curable centralised corpus of fact-checked articles is available. Also, the same claim is fact-checked by multiple fact-checking organisations with or without judgement. To fill this gap, we introduce FakeKG: A Knowledge Graph-Based approach for improving Automated Fact-checking. FakeKG is a centralised knowledge graph containing fact-checked articles from different sources that can be queried using the SPARQL endpoint. The proposed FakeKG can prescreen claim requests and filter them if the claim is already fact-checked and provide a judgement to the claim. It will also categorise the claim's domain so that the fact-checker can prioritise checking the incoming claims into different groups like health and election. This study proposes an approach for creating FakeKG and its future application for mitigating misinformation.},
	language = {en},
	number = {13},
	urldate = {2024-09-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shahi, Gautam Kishore},
	year = {2023},
	note = {Number: 13},
	keywords = {News Media},
	pages = {16320--16321},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/X3K55WBX/Shahi - 2023 - FakeKG A Knowledge Graph of Fake Claims for Improving Automated Fact-Checking (Student Abstract).pdf:application/pdf},
}

@inproceedings{ribeiro_factgraph_2022,
	address = {Seattle, United States},
	title = {{FactGraph}: {Evaluating} {Factuality} in {Summarization} with {Semantic} {Graph} {Representations}},
	shorttitle = {{FactGraph}},
	url = {https://aclanthology.org/2022.naacl-main.236},
	doi = {10.18653/v1/2022.naacl-main.236},
	abstract = {Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15\%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.},
	urldate = {2024-09-22},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Leonardo F. R. and Liu, Mengwen and Gurevych, Iryna and Dreyer, Markus and Bansal, Mohit},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {3238--3253},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/EJN45DYR/Ribeiro et al. - 2022 - FactGraph Evaluating Factuality in Summarization with Semantic Graph Representations.pdf:application/pdf},
}

@misc{tang_minicheck_2024,
	title = {{MiniCheck}: {Efficient} {Fact}-{Checking} of {LLMs} on {Grounding} {Documents}},
	shorttitle = {{MiniCheck}},
	url = {http://arxiv.org/abs/2404.10774},
	abstract = {Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of "fact-checking" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Tang, Liyan and Laban, Philippe and Durrett, Greg},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: LLM-AggreFact benchmark, MiniCheck models, data generation code at https://github.com/Liyan06/MiniCheck},
	file = {PDF:/Users/haoliu/Zotero/storage/CI6IHHZ2/Tang et al. - 2024 - MiniCheck Efficient Fact-Checking of LLMs on Grounding Documents.pdf:application/pdf},
}

@misc{kim_factkg_2023,
	title = {{FactKG}: {Fact} {Verification} via {Reasoning} on {Knowledge} {Graphs}},
	shorttitle = {{FactKG}},
	url = {http://arxiv.org/abs/2305.06590},
	abstract = {In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Kim, Jiho and Park, Sungjin and Kwon, Yeonsu and Jo, Yohan and Thorne, James and Choi, Edward},
	month = may,
	year = {2023},
	note = {arXiv:2305.06590 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ACL 2023},
	file = {PDF:/Users/haoliu/Zotero/storage/HD87Z37N/Kim et al. - 2023 - FactKG Fact Verification via Reasoning on Knowledge Graphs.pdf:application/pdf},
}

@misc{min_factscore_2023,
	title = {{FActScore}: {Fine}-grained {Atomic} {Evaluation} of {Factual} {Precision} in {Long} {Form} {Text} {Generation}},
	shorttitle = {{FActScore}},
	url = {http://arxiv.org/abs/2305.14251},
	abstract = {Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost \$26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14251 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 25 pages; 7 figures. Published as a main conference paper at EMNLP 2023. Code available at https://github.com/shmsw25/FActScore},
	file = {PDF:/Users/haoliu/Zotero/storage/SA6AXUPT/Min et al. - 2023 - FActScore Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.pdf:application/pdf},
}

@misc{dmonte_claim_2024,
	title = {Claim {Verification} in the {Age} of {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Claim {Verification} in the {Age} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.14317},
	abstract = {The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems.1 Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Dmonte, Alphaeus and Oruche, Roland and Zampieri, Marcos and Calyam, Prasad and Augenstein, Isabelle},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14317 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/Users/haoliu/Zotero/storage/3EDSNL6P/Dmonte et al. - 2024 - Claim Verification in the Age of Large Language Models A Survey.pdf:application/pdf},
}

@misc{yuan_zero-shot_2023,
	title = {Zero-{Shot} {Fact}-{Checking} with {Semantic} {Triples} and {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2312.11785},
	abstract = {Despite progress in automated fact-checking, most systems require a significant amount of labeled training data, which is expensive. In this paper, we propose a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference. This allows it to generalize to adversarial datasets and domains that supervised models require specific training data for. Our empirical results show that our approach outperforms previous zero-shot approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being comparable or better than supervised models on the adversarial and the out-of-domain datasets.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Yuan, Zhangdie and Vlachos, Andreas},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11785 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/haoliu/Zotero/storage/4F4GLJIU/Yuan and Vlachos - 2023 - Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs.pdf:application/pdf},
}

@article{chang_communitykg-rag_nodate,
	title = {{CommunityKG}-{RAG}: {Leveraging} {Community} {Structures} in {Knowledge} {Graphs} for {Advanced} {Retrieval}-{Augmented} {Generation} in {Fact}-{Checking}},
	abstract = {Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, their effectiveness is often hindered by a lack of integration with entity relationships and community structures, limiting their ability to provide contextually rich and accurate information retrieval for fact-checking. We introduce CommunityKG-RAG (Community Knowledge Graph-Retrieval Augmented Generation), a novel zero-shot framework that integrates community structures within Knowledge Graphs (KGs) with RAG systems to enhance the fact-checking process. Capable of adapting to new domains and queries without additional training, CommunityKG-RAG utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval. Our experimental results demonstrate that CommunityKG-RAG outperforms traditional methods, representing a significant advancement in fact-checking by offering a robust, scalable, and efficient solution.},
	language = {en},
	author = {Chang, Rong-Ching and Zhang, Jiawei},
	file = {PDF:/Users/haoliu/Zotero/storage/UHNLK6MP/Chang and Zhang - CommunityKG-RAG Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmente.pdf:application/pdf},
}

@inproceedings{ma_kapalm_2023,
	address = {Singapore},
	title = {{KAPALM}: {Knowledge} {grAPh} {enhAnced} {Language} {Models} for {Fake} {News} {Detection}},
	shorttitle = {{KAPALM}},
	url = {https://aclanthology.org/2023.findings-emnlp.263},
	doi = {10.18653/v1/2023.findings-emnlp.263},
	abstract = {Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarse- and fine-grained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph and graph pooling layer. Finally, we integrate the coarse- and fine-grained entity knowledge representations with the textual representation for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines. In addition, it is competitive in the few-shot scenario.},
	urldate = {2024-10-06},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Jing and Chen, Chen and Hou, Chunyan and Yuan, Xiaojie},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {3999--4009},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/G85KYA3S/Ma et al. - 2023 - KAPALM Knowledge grAPh enhAnced Language Models for Fake News Detection.pdf:application/pdf},
}

@article{opsahl_fact_nodate,
	title = {Fact or {Fiction}? {Improving} {Fact} {Verification} with {Knowledge} {Graphs} through {Simplified} {Subgraph} {Retrievals}},
	abstract = {Despite recent success in natural language processing (NLP), fact verification still remains a difficult task. Due to misinformation spreading increasingly fast, attention has been directed towards automatically verifying the correctness of claims. In the domain of NLP, this is usually done by training supervised machine learning models to verify claims by utilizing evidence from trustworthy corpora. We present efficient methods for verifying claims on a dataset where the evidence is in the form of structured knowledge graphs. We use the FACTKG dataset, which is constructed from the DBpedia knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval process, from fine-tuned language models to simple logical retrievals, we are able to construct models that both require less computational resources and achieve better test-set accuracy.},
	language = {en},
	author = {Opsahl, Tobias A},
	file = {PDF:/Users/haoliu/Zotero/storage/TGMR4UDK/Opsahl - Fact or Fiction Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retri.pdf:application/pdf},
}

@article{gunjal_molecular_nodate,
	title = {Molecular {Facts}: {Desiderata} for {Decontextualization} in {LLM} {Fact} {Verification}},
	abstract = {Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology 1 for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings.},
	language = {en},
	author = {Gunjal, Anisha and Durrett, Greg},
	file = {PDF:/Users/haoliu/Zotero/storage/LNQMXATA/Gunjal and Durrett - Molecular Facts Desiderata for Decontextualization in LLM Fact Verification.pdf:application/pdf},
}

@misc{sansford_grapheval_2024,
	title = {{GraphEval}: {A} {Knowledge}-{Graph} {Based} {LLM} {Hallucination} {Evaluation} {Framework}},
	shorttitle = {{GraphEval}},
	url = {http://arxiv.org/abs/2407.10793},
	abstract = {Methods to evaluate Large Language Model (LLM) responses and detect inconsistencies, also known as hallucinations, with respect to the provided knowledge, are becoming increasingly important for LLM applications. Current metrics fall short in their ability to provide explainable decisions, systematically check all pieces of information in the response, and are often too computationally expensive to be used in practice. We present GraphEval: a hallucination evaluation framework based on representing information in Knowledge Graph (KG) structures. Our method identifies the specific triples in the KG that are prone to hallucinations and hence provides more insight into where in the response a hallucination has occurred, if at all, than previous methods. Furthermore, using our approach in conjunction with state-of-the-art natural language inference (NLI) models leads to an improvement in balanced accuracy on various hallucination benchmarks, compared to using the raw NLI models. Lastly, we explore the use of GraphEval for hallucination correction by leveraging the structure of the KG, a method we name GraphCorrect, and demonstrate that the majority of hallucinations can indeed be rectified.},
	language = {en},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Sansford, Hannah and Richardson, Nicholas and Maretic, Hermina Petric and Saada, Juba Nait},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10793 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 12 pages, to be published at KiL'24: Workshop on Knowledge-infused Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona, Spain},
	file = {PDF:/Users/haoliu/Zotero/storage/FNG7ZDYC/Sansford et al. - 2024 - GraphEval A Knowledge-Graph Based LLM Hallucination Evaluation Framework.pdf:application/pdf},
}

@article{kim__2024,
	title = {: {Evaluating} faithfulness and content selection},
	abstract = {While long-context large language models (LLMs) can technically summarize book-length documents ({\textgreater} 100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of \$5.2K USD, which allows us to rank LLM summarizers based on faithfulness: CLAUDE-3-OPUS significantly outperforms all closedsource LLMs, while the open-source MIXTRAL is on par with GPT-3.5-TURBO. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book. We release FABLES to spur further research on the evaluation of book-length summarization.},
	language = {en},
	author = {Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and Garimella, Aparna and Manjunatha, Varun and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
	year = {2024},
	file = {PDF:/Users/haoliu/Zotero/storage/WW78W6A5/Kim et al. - 2024 -  Evaluating faithfulness and content selection.pdf:application/pdf},
}

@misc{weng_extrinsic_2024,
	title = {Extrinsic {Hallucinations} in {LLMs}},
	url = {https://lilianweng.github.io/posts/2024-07-07-hallucination/},
	abstract = {Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:
In-context hallucination: The model output should be consistent with the source content in context.},
	language = {en},
	urldate = {2024-10-22},
	author = {Weng, Lilian},
	month = jul,
	year = {2024},
	note = {Section: posts},
	file = {Snapshot:/Users/haoliu/Zotero/storage/SZ3EXHJB/2024-07-07-hallucination.html:text/html},
}

@article{wei_long-form_nodate,
	title = {{LONG}-{FORM} {FACTUALITY} {IN} {LARGE} {LANGUAGE} {MODELS}},
	abstract = {Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for longform factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall).},
	language = {en},
	author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Huang, Jie and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da},
	file = {PDF:/Users/haoliu/Zotero/storage/Z8ZVE2ZS/Wei et al. - LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS.pdf:application/pdf},
}

@misc{kim_fables_2024,
	title = {{FABLES}: {Evaluating} faithfulness and content selection in book-length summarization},
	shorttitle = {{FABLES}},
	url = {http://arxiv.org/abs/2404.01261},
	abstract = {While long-context large language models (LLMs) can technically summarize book-length documents ({\textgreater} 100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of \$5.2K USD, which allows us to rank LLM summarizers based on faithfulness: CLAUDE-3-OPUS significantly outperforms all closedsource LLMs, while the open-source MIXTRAL is on par with GPT-3.5-TURBO. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book. We release FABLES to spur further research on the evaluation of book-length summarization.},
	language = {en},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and Garimella, Aparna and Manjunatha, Varun and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
	month = sep,
	year = {2024},
	note = {arXiv:2404.01261 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: preprint - 39 pages},
	file = {PDF:/Users/haoliu/Zotero/storage/KJL37AF4/Kim et al. - 2024 - FABLES Evaluating faithfulness and content selection in book-length summarization.pdf:application/pdf},
}

@inproceedings{qiu_amrfact_2024,
	address = {Mexico City, Mexico},
	title = {{AMRFact}: {Enhancing} {Summarization} {Factuality} {Evaluation} with {AMR}-{Driven} {Negative} {Samples} {Generation}},
	shorttitle = {{AMRFact}},
	url = {https://aclanthology.org/2024.naacl-long.33},
	doi = {10.18653/v1/2024.naacl-long.33},
	abstract = {Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.},
	urldate = {2024-10-28},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Haoyi and Huang, Kung-Hsiang and Qu, Jingnong and Peng, Nanyun},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {594--608},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/PK3D8A8T/Qiu et al. - 2024 - AMRFact Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation.pdf:application/pdf},
}

@misc{noauthor_active_llm_h_nodate,
	title = {active\_llm\_h… (auto-{E}) - {JupyterLab}},
	url = {https://d-lkx6oa6ougrt.studio.us-east-1.sagemaker.aws/jupyter/default/lab/workspaces/auto-E/tree/marketingfm-eval},
	urldate = {2024-10-28},
	file = {marketingfm-… (auto-E) - JupyterLab:/Users/haoliu/Zotero/storage/P6XJTL9N/marketingfm-eval.html:text/html},
}

@inproceedings{wan_acueval_2024,
	address = {Bangkok, Thailand},
	title = {{ACUEval}: {Fine}-grained {Hallucination} {Evaluation} and {Correction} for {Abstractive} {Summarization}},
	shorttitle = {{ACUEval}},
	url = {https://aclanthology.org/2024.findings-acl.597},
	doi = {10.18653/v1/2024.findings-acl.597},
	abstract = {The impressive generation capabilities of large language models (LLMs) have made it harder to detect the subtle hallucinations they make in abstractive summarization, where generated summaries consist of a blend of correct and incorrect information w.r.t. a given document. Recently-proposed LLM-based evaluation metrics attempt to capture this, but still face challenges: (1) they are biased towards summaries generated from the same underlying LLM, and (2) they lack interpretability, offering only a single score. In this work, we present ACUEval, a metric that leverages the power of LLMs to perform two sub-tasks: decomposing summaries into atomic content units (ACUs), and validating them against the source document. Compared to current strong LLM-based metrics, our two-step evaluation strategy improves correlation with human judgments of faithfulness on three summarization evaluation benchmarks by 3\% in balanced accuracy compared to the next-best metric, and also shows reduced preference bias towards LLM-generated summary. Further, we show that errors detected by ACUEval can be used to generate actionable feedback for refining the summary, improving the faithfulness scores by more than 10\%.},
	urldate = {2024-11-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Wan, David and Sinha, Koustuv and Iyer, Srini and Celikyilmaz, Asli and Bansal, Mohit and Pasunuru, Ramakanth},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {10036--10056},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/6VJCZYQA/Wan et al. - 2024 - ACUEval Fine-grained Hallucination Evaluation and Correction for Abstractive Summarization.pdf:application/pdf},
}

@inproceedings{zhong_reasoning_2020,
	address = {Online},
	title = {Reasoning {Over} {Semantic}-{Level} {Graph} for {Fact} {Checking}},
	url = {https://aclanthology.org/2020.acl-main.549},
	doi = {10.18653/v1/2020.acl-main.549},
	abstract = {Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.},
	urldate = {2024-11-11},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Wanjun and Xu, Jingjing and Tang, Duyu and Xu, Zenan and Duan, Nan and Zhou, Ming and Wang, Jiahai and Yin, Jian},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {6170--6180},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/LTBICRDV/Zhong et al. - 2020 - Reasoning Over Semantic-Level Graph for Fact Checking.pdf:application/pdf},
}

@inproceedings{qiu_amrfact_2024-1,
	address = {Mexico City, Mexico},
	title = {{AMRFact}: {Enhancing} {Summarization} {Factuality} {Evaluation} with {AMR}-{Driven} {Negative} {Samples} {Generation}},
	shorttitle = {{AMRFact}},
	url = {https://aclanthology.org/2024.naacl-long.33},
	doi = {10.18653/v1/2024.naacl-long.33},
	abstract = {Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.},
	urldate = {2024-11-11},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Haoyi and Huang, Kung-Hsiang and Qu, Jingnong and Peng, Nanyun},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {594--608},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/3D9BWPEF/Qiu et al. - 2024 - AMRFact Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation.pdf:application/pdf},
}

@misc{zhu_rageval_2024,
	title = {{RAGEval}: {Scenario} {Specific} {RAG} {Evaluation} {Dataset} {Generation} {Framework}},
	shorttitle = {{RAGEval}},
	url = {http://arxiv.org/abs/2408.01262},
	abstract = {Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics Completeness, Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhu, Kunlun and Luo, Yifan and Xu, Dingling and Wang, Ruobing and Yu, Shi and Wang, Shuo and Yan, Yukun and Liu, Zhenghao and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2024},
	note = {arXiv:2408.01262},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/CFBUGEC7/Zhu et al. - 2024 - RAGEval Scenario Specific RAG Evaluation Dataset Generation Framework.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/UU57DRYV/2408.html:text/html},
}

@misc{noauthor_openreview_nodate,
	title = {{OpenReview}},
	url = {https://openreview.net/notifications?email=liuhr99%40outlook.com},
	abstract = {Promoting openness in scientific communication and the peer-review process},
	language = {en},
	urldate = {2024-11-18},
	journal = {OpenReview},
	file = {Snapshot:/Users/haoliu/Zotero/storage/ELCGCSAQ/notifications.html:text/html},
}

@inproceedings{tang_understanding_2023,
	address = {Toronto, Canada},
	title = {Understanding {Factual} {Errors} in {Summarization}: {Errors}, {Summarizers}, {Datasets}, {Error} {Detectors}},
	shorttitle = {Understanding {Factual} {Errors} in {Summarization}},
	url = {https://aclanthology.org/2023.acl-long.650},
	doi = {10.18653/v1/2023.acl-long.650},
	abstract = {The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights.},
	urldate = {2024-11-16},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Liyan and Goyal, Tanya and Fabbri, Alex and Laban, Philippe and Xu, Jiacheng and Yavuz, Semih and Kryscinski, Wojciech and Rousseau, Justin and Durrett, Greg},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11626--11644},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/A9CGGLJX/Tang et al. - 2023 - Understanding Factual Errors in Summarization Errors, Summarizers, Datasets, Error Detectors.pdf:application/pdf},
}

@misc{dammu_claimver_2024,
	title = {{ClaimVer}: {Explainable} {Claim}-{Level} {Verification} and {Evidence} {Attribution} of {Text} {Through} {Knowledge} {Graphs}},
	shorttitle = {{ClaimVer}},
	url = {http://arxiv.org/abs/2403.09724},
	abstract = {In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.},
	urldate = {2024-11-24},
	publisher = {arXiv},
	author = {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Dewan, Mouly and Kim, YoungMin and Roosta, Tanya and Chadha, Aman and Shah, Chirag},
	month = sep,
	year = {2024},
	note = {arXiv:2403.09724},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/ZEIXQMWA/Dammu et al. - 2024 - ClaimVer Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Gr.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/UIVGU7ZD/2403.html:text/html},
}

@misc{zha_alignscore_2023,
	title = {{AlignScore}: {Evaluating} {Factual} {Consistency} with a {Unified} {Alignment} {Function}},
	shorttitle = {{AlignScore}},
	url = {http://arxiv.org/abs/2305.16739},
	doi = {10.48550/arXiv.2305.16739},
	abstract = {Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Zha, Yuheng and Yang, Yichi and Li, Ruichen and Hu, Zhiting},
	month = may,
	year = {2023},
	note = {arXiv:2305.16739 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 5 figures, ACL2023},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/BSW9JI2W/Zha et al. - 2023 - AlignScore Evaluating Factual Consistency with a Unified Alignment Function.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/7F462BQB/2305.html:text/html},
}
