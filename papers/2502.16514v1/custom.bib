% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{liu2024aligning,
  title={Aligning with logic: Measuring, evaluating and improving logical consistency in large language models},
  author={Liu, Yinhong and Guo, Zhijiang and Liang, Tianya and Shareghi, Ehsan and Vuli{\'c}, Ivan and Collier, Nigel},
  journal={arXiv preprint arXiv:2410.02205},
  year={2024}
}
@article{min2023factscore,
  title={Factscore: Fine-grained atomic evaluation of factual precision in long form text generation},
  author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2305.14251},
  year={2023}
}

@article{tang2024minicheck,
  title={MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents},
  author={Tang, Liyan and Laban, Philippe and Durrett, Greg},
  journal={arXiv preprint arXiv:2404.10774},
  year={2024}
}

@article{kim2023factkg,
  title={FactKG: Fact verification via reasoning on knowledge graphs},
  author={Kim, Jiho and Park, Sungjin and Kwon, Yeonsu and Jo, Yohan and Thorne, James and Choi, Edward},
  journal={arXiv preprint arXiv:2305.06590},
  year={2023}
}

@article{liu2024evaluating,
  title={Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs},
  author={Liu, Xiaoze and Wu, Feijie and Xu, Tianyang and Chen, Zhuo and Zhang, Yichi and Wang, Xiaoqian and Gao, Jing},
  journal={arXiv preprint arXiv:2404.00942},
  year={2024}
}

@inproceedings{wan2024acueval,
  title={ACUEval: Fine-grained hallucination evaluation and correction for abstractive summarization},
  author={Wan, David and Sinha, Koustuv and Iyer, Srini and Celikyilmaz, Asli and Bansal, Mohit and Pasunuru, Ramakanth},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={10036--10056},
  year={2024}
}

@article{he2024g,
  title={G-retriever: Retrieval-augmented generation for textual graph understanding and question answering},
  author={He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
  journal={arXiv preprint arXiv:2402.07630},
  year={2024}
}

@article{dammu2024claimver,
  title={ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs},
  author={Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Dewan, Mouly and Kim, YoungMin and Roosta, Tanya and Chadha, Aman and Shah, Chirag},
  journal={arXiv preprint arXiv:2403.09724},
  year={2024}
}

@article{ribeiro2022factgraph,
  title={FactGraph: Evaluating factuality in summarization with semantic graph representations},
  author={Ribeiro, Leonardo FR and Liu, Mengwen and Gurevych, Iryna and Dreyer, Markus and Bansal, Mohit},
  journal={arXiv preprint arXiv:2204.06508},
  year={2022}
}

@inproceedings{shahi2023fakekg,
  title={FakeKG: a knowledge graph of fake claims for improving automated fact-checking (student abstract)},
  author={Shahi, Gautam Kishore},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={13},
  pages={16320--16321},
  year={2023}
}

@article{zhu2024rageval,
  title={Rageval: Scenario specific rag evaluation dataset generation framework},
  author={Zhu, Kunlun and Luo, Yifan and Xu, Dingling and Wang, Ruobing and Yu, Shi and Wang, Shuo and Yan, Yukun and Liu, Zhenghao and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2408.01262},
  year={2024}
}

@inproceedings{liu-etal-2023-towards-interpretable,
    title = "Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation",
    author = "Liu, Yixin  and
      Fabbri, Alexander  and
      Zhao, Yilun  and
      Liu, Pengfei  and
      Joty, Shafiq  and
      Wu, Chien-Sheng  and
      Xiong, Caiming  and
      Radev, Dragomir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1018/",
    doi = "10.18653/v1/2023.emnlp-main.1018",
    pages = "16360--16368",
    abstract = "Interpretability and efficiency are two important considerations for the adoption of neural automatic metrics. In this work, we develop strong-performing automatic metrics for reference-based summarization evaluation, based on a two-stage evaluation pipeline that first extracts basic information units from one text sequence and then checks the extracted units in another sequence. The metrics we developed include two-stage metrics that can provide high interpretability at both the fine-grained unit level and summary level, and one-stage metrics that achieve a balance between efficiency and interpretability. We make the developed tools publicly available at https://github.com/Yale-LILY/AutoACU."
}

@inproceedings{tang-etal-2023-understanding,
    title = "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
    author = "Tang, Liyan  and
      Goyal, Tanya  and
      Fabbri, Alex  and
      Laban, Philippe  and
      Xu, Jiacheng  and
      Yavuz, Semih  and
      Kryscinski, Wojciech  and
      Rousseau, Justin  and
      Durrett, Greg",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.650/",
    doi = "10.18653/v1/2023.acl-long.650",
    pages = "11626--11644",
    abstract = "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights."
}

@article{10.1162/tacl_a_00373,
    author = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
    title = {SummEval: Re-evaluating Summarization Evaluation},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {391-409},
    year = {2021},
    month = {04},
    abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00373},
    url = {https://doi.org/10.1162/tacl\_a\_00373},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00373/1923949/tacl\_a\_00373.pdf},
}
@article{Kim2024CanLP,
  title={Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate},
  author={Kyungha Kim and Sangyun Lee and Kung-Hsiang Huang and Hou Pong Chan and Manling Li and Heng Ji},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.07401},
  url={https://api.semanticscholar.org/CorpusID:267627437}
}
@article{rouhizadeh2025large,
  title        = {Large Language Models Struggle to Encode Medical Concepts — A Multilingual Benchmarking and Comparative Analysis},
  author       = {Rouhizadeh, Hossein and Yazdani, Anthony and Zhang, Boya and Alvarez, David Vicente and Hüser, Matthias and Vanobberghen, Alexandre and Yang, Rui and Li, Irene and Walter, Andreas and Teodoro, Douglas},
  year         = {2025},
  doi          = {10.1101/2025.01.15.25320579},
  journal      = {medRxiv},
  url          = {https://www.medrxiv.org/content/10.1101/2025.01.15.25320579v1},
  note         = {Preprint posted January 15, 2025}
}
@inproceedings{Fan2024ASO,
  title={A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
  author={Wenqi Fan and Yujuan Ding and Liang-bo Ning and Shijie Wang and Hengyun Li and Dawei Yin and Tat-Seng Chua and Qing Li},
  booktitle={Knowledge Discovery and Data Mining},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269740933}
}
@article{Ahsan2023RetrievingEF,
  title={Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges},
  author={Hiba Ahsan and Denis Jered McInerney and Jisoo Kim and Christopher Potter and Geoffrey Young and Silvio Amir and Byron C. Wallace},
  journal={Proceedings of machine learning research},
  year={2023},
  volume={248},
  pages={
          489-505
        },
  url={https://api.semanticscholar.org/CorpusID:261681755}
}
@article{liu2023recprompt,
  title={Recprompt: A prompt tuning framework for news recommendation using large language models},
  author={Liu, Dairui and Yang, Boming and Du, Honghui and Greene, Derek and Lawlor, Aonghus and Dong, Ruihai and Li, Irene},
  journal={arXiv preprint arXiv:2312.10463},
  year={2023}
}
@article{Lee2024ASO,
  title={A Survey of Large Language Models in Finance (FinLLMs)},
  author={Jean Lee and Nicholas Stevens and Soyeon Caren Han and Minseok Song},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.02315},
  url={https://api.semanticscholar.org/CorpusID:267412025}
}
@article{Jiang2024LeveragingLL,
  title={Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling},
  author={Hang Jiang and Xiajie Zhang and Robert Mahari and Daniel Kessler and Eric Ma and Tal August and Irene Li and Alex 'Sandy' Pentland and Yoon Kim and Jad Kabbara and Deb Roy},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.17019},
  url={https://api.semanticscholar.org/CorpusID:268031822}
}
@article{malaviya2023expertqa,
  title={Expertqa: Expert-curated questions and attributed answers},
  author={Malaviya, Chaitanya and Lee, Subin and Chen, Sihao and Sieber, Elizabeth and Yatskar, Mark and Roth, Dan},
  journal={arXiv preprint arXiv:2309.07852},
  year={2023}
}

@inproceedings{vladika-etal-2024-healthfc,
    title = "{H}ealth{FC}: Verifying Health Claims with Evidence-Based Medical Fact-Checking",
    author = "Vladika, Juraj  and
      Schneider, Phillip  and
      Matthes, Florian",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.709/",
    pages = "8095--8107",
    abstract = "In the digital age, seeking health advice on the Internet has become a common practice. At the same time, determining the trustworthiness of online medical content is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance automated Natural Language Processing (NLP) solutions for this task, in this paper we introduce a novel dataset HealthFC. It consists of 750 health-related claims in German and English, labeled for veracity by medical experts and backed with evidence from systematic reviews and clinical trials. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for NLP tasks related to automated fact-checking, such as evidence retrieval, claim verification, or explanation generation. For testing purposes, we provide baseline systems based on different approaches, examine their performance, and discuss the findings. We show that the dataset is a challenging test bed with a high potential for future use."
}

@article{kotonya2020explainable,
  title={Explainable automated fact-checking for public health claims},
  author={Kotonya, Neema and Toni, Francesca},
  journal={arXiv preprint arXiv:2010.09926},
  year={2020}
}

@article{wadden2020fact,
  title={Fact or fiction: Verifying scientific claims},
  author={Wadden, David and Lin, Shanchuan and Lo, Kyle and Wang, Lucy Lu and van Zuylen, Madeleine and Cohan, Arman and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2004.14974},
  year={2020}
}

@article{saakyan2021covid,
  title={COVID-fact: Fact extraction and verification of real-world claims on COVID-19 pandemic},
  author={Saakyan, Arkadiy and Chakrabarty, Tuhin and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2106.03794},
  year={2021}
}

@article{manakul2023selfcheckgpt,
  title={Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models},
  author={Manakul, Potsawee and Liusie, Adian and Gales, Mark JF},
  journal={arXiv preprint arXiv:2303.08896},
  year={2023}
}

@article{mundler2023self,
  title={Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation},
  author={M{\"u}ndler, Niels and He, Jingxuan and Jenko, Slobodan and Vechev, Martin},
  journal={arXiv preprint arXiv:2305.15852},
  year={2023}
}

@inproceedings{mckenna-etal-2023-sources,
    title = "Sources of Hallucination by Large Language Models on Inference Tasks",
    author = "McKenna, Nick  and
      Li, Tianyi  and
      Cheng, Liang  and
      Hosseini, Mohammad  and
      Johnson, Mark  and
      Steedman, Mark",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.182/",
    doi = "10.18653/v1/2023.findings-emnlp.182",
    pages = "2758--2774",
    abstract = "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as {\textquotedblleft}indices' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation."
}

@article{zhang2023language,
  title={How language model hallucinations can snowball},
  author={Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A},
  journal={arXiv preprint arXiv:2305.13534},
  year={2023}
}

@article{ramprasad2024analyzing,
  title={Analyzing LLM Behavior in Dialogue Summarization: Unveiling Circumstantial Hallucination Trends},
  author={Ramprasad, Sanjana and Ferracane, Elisa and Lipton, Zachary C},
  journal={arXiv preprint arXiv:2406.03487},
  year={2024}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zha2023alignscore,
  title={AlignScore: Evaluating factual consistency with a unified alignment function},
  author={Zha, Yuheng and Yang, Yichi and Li, Ruichen and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.16739},
  year={2023}
}

@article{fu2023gptscore,
  title={Gptscore: Evaluate as you desire},
  author={Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  journal={arXiv preprint arXiv:2302.04166},
  year={2023}
}

@article{lehmann2015dbpedia,
  title={Dbpedia--a large-scale, multilingual knowledge base extracted from wikipedia},
  author={Lehmann, Jens and Isele, Robert and Jakob, Max and Jentzsch, Anja and Kontokostas, Dimitris and Mendes, Pablo N and Hellmann, Sebastian and Morsey, Mohamed and Van Kleef, Patrick and Auer, S{\"o}ren and others},
  journal={Semantic web},
  volume={6},
  number={2},
  pages={167--195},
  year={2015},
  publisher={IOS Press}
}

@article{yuan2023zero,
  title={Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs},
  author={Yuan, Zhangdie and Vlachos, Andreas},
  journal={arXiv preprint arXiv:2312.11785},
  year={2023}
}

@article{vrandevcic2014wikidata,
  title={Wikidata: a free collaborative knowledgebase},
  author={Vrande{\v{c}}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  journal={Communications of the ACM},
  volume={57},
  number={10},
  pages={78--85},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@inproceedings{Achiam2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haim-ing Bao and Mo Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Made-laine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Benjamin Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Sim'on Posada Fishman and Juston Forte and Is-abella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Raphael Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Jo-hannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Lukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Hendrik Kirchner and Jamie Ryan Kiros and Matthew Knight and Daniel Kokotajlo and Lukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Ma-teusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel P. Mossing and Tong Mu and Mira Murati and Oleg Murk and David M'ely and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Ouyang Long and Cullen O'Keefe and Jakub W. Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alexandre Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Pond{\'e} de Oliveira Pinto and Michael Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack W. Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario D. Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas A. Tezak and Madeleine Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cer'on Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll L. Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qim-ing Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{sansford2024grapheval,
  title={Grapheval: A knowledge-graph based llm hallucination evaluation framework},
  author={Sansford, Hannah and Richardson, Nicholas and Maretic, Hermina Petric and Saada, Juba Nait},
  journal={arXiv preprint arXiv:2407.10793},
  year={2024}
}
@inproceedings{Gao2023EvaluatingLL,
  title={Evaluating Large Language Models on Wikipedia-Style Survey Generation},
  author={Fan Gao and Hang Jiang and Rui Yang and Qingcheng Zeng and Jinghui Lu and Moritz Blum and Dairui Liu and Tianwei She and Yuang Jiang and Irene Li},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:267783118}
}
@article{yun2019graph,
  title={Graph transformer networks},
  author={Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{yang-etal-2024-kg,
    title = "{KG}-Rank: Enhancing Large Language Models for Medical {QA} with Knowledge Graphs and Ranking Techniques",
    author = "Yang, Rui  and
      Liu, Haoran  and
      Marrese-Taylor, Edison  and
      Zeng, Qingcheng  and
      Ke, Yuhe  and
      Li, Wanxin  and
      Cheng, Lechao  and
      Chen, Qingyu  and
      Caverlee, James  and
      Matsuo, Yutaka  and
      Li, Irene",
    editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
    booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.bionlp-1.13/",
    doi = "10.18653/v1/2024.bionlp-1.13",
    pages = "155--166",
    abstract = "Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves an improvement of over 18{\%} in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it realizes a 14{\%} improvement in ROUGE-L, showing the effectiveness and potential of KG-Rank."
}

@article{yang2023large,
  title={Large language models in health care: Development, applications, and challenges},
  author={Yang, Rui and Tan, Ting Fang and Lu, Wei and Thirunavukarasu, Arun James and Ting, Daniel Shu Wei and Liu, Nan},
  journal={Health Care Science},
  volume={2},
  number={4},
  pages={255--263},
  year={2023},
  publisher={Wiley Online Library}
}

@article{yang2024ascle,
  title={Ascle—a Python natural language processing toolkit for medical text generation: development and evaluation study},
  author={Yang, Rui and Zeng, Qingcheng and You, Keen and Qiao, Yujie and Huang, Lucas and Hsieh, Chia-Chun and Rosand, Benjamin and Goldwasser, Jeremy and Dave, Amisha and Keenan, Tiarnan and others},
  journal={Journal of Medical Internet Research},
  volume={26},
  pages={e60601},
  year={2024},
  publisher={JMIR Publications Toronto, Canada}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@misc{claude35sonnet,
  author       = {Anthropic},
  title        = {Claude 3.5 Sonnet},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-5-sonnet}},
  note         = {Retrieved February 12, 2025}
}

@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016}
}

@article{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@article{yang2024graphusion,
  title={Graphusion: a RAG framework for Knowledge Graph Construction with a global perspective},
  author={Yang, Rui and Yang, Boming and Feng, Aosong and Ouyang, Sixun and Blum, Moritz and She, Tianwei and Jiang, Yuang and Lecue, Freddy and Lu, Jinghui and Li, Irene},
  journal={arXiv preprint arXiv:2410.17600},
  year={2024}
}

@article{edge2024local,
  title={From local to global: A graph rag approach to query-focused summarization},
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  journal={arXiv preprint arXiv:2404.16130},
  year={2024}
}


@misc{wang_openfactcheck_2024,
	title = {{OpenFactCheck}: {A} {Unified} {Framework} for {Factuality} {Evaluation} of {LLMs}},
	shorttitle = {{OpenFactCheck}},
	url = {http://arxiv.org/abs/2405.05583},
	abstract = {The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM’s factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers’ verification results using human-annotated datasets. OpenFactCheck is publicly released at https: //github.com/yuxiaw/OpenFactCheck.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Wang, Yuxia and Wang, Minghan and Iqbal, Hasan and Georgiev, Georgi and Geng, Jiahui and Nakov, Preslav},
	month = may,
	year = {2024},
	note = {arXiv:2405.05583 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 8 tables, 8 figures},
	file = {PDF:/Users/haoliu/Zotero/storage/QCGGLLUK/Wang et al. - 2024 - OpenFactCheck A Unified Framework for Factuality Evaluation of LLMs.pdf:application/pdf},
}

@article{shahi_fakekg_2023,
	title = {{FakeKG}: {A} {Knowledge} {Graph} of {Fake} {Claims} for {Improving} {Automated} {Fact}-{Checking} ({Student} {Abstract})},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{FakeKG}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/27020},
	doi = {10.1609/aaai.v37i13.27020},
	abstract = {False information could be dangerous if the claim is not debunked timely. Fact-checking organisations get a high volume of claims on different topics with immense velocity. The efficiency of the fact-checkers decreases due to 3V problems volume, velocity and variety. Especially during crises or elections, fact-checkers cannot handle user requests to verify the claim. Until now, no real-time curable centralised corpus of fact-checked articles is available. Also, the same claim is fact-checked by multiple fact-checking organisations with or without judgement. To fill this gap, we introduce FakeKG: A Knowledge Graph-Based approach for improving Automated Fact-checking. FakeKG is a centralised knowledge graph containing fact-checked articles from different sources that can be queried using the SPARQL endpoint. The proposed FakeKG can prescreen claim requests and filter them if the claim is already fact-checked and provide a judgement to the claim. It will also categorise the claim's domain so that the fact-checker can prioritise checking the incoming claims into different groups like health and election. This study proposes an approach for creating FakeKG and its future application for mitigating misinformation.},
	language = {en},
	number = {13},
	urldate = {2024-09-22},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shahi, Gautam Kishore},
	year = {2023},
	note = {Number: 13},
	keywords = {News Media},
	pages = {16320--16321},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/X3K55WBX/Shahi - 2023 - FakeKG A Knowledge Graph of Fake Claims for Improving Automated Fact-Checking (Student Abstract).pdf:application/pdf},
}

@inproceedings{ribeiro_factgraph_2022,
	address = {Seattle, United States},
	title = {{FactGraph}: {Evaluating} {Factuality} in {Summarization} with {Semantic} {Graph} {Representations}},
	shorttitle = {{FactGraph}},
	url = {https://aclanthology.org/2022.naacl-main.236},
	doi = {10.18653/v1/2022.naacl-main.236},
	abstract = {Despite recent improvements in abstractive summarization, most current approaches generate summaries that are not factually consistent with the source document, severely restricting their trust and usage in real-world applications. Recent works have shown promising improvements in factuality error identification using text or dependency arc entailments; however, they do not consider the entire semantic graph simultaneously. To this end, we propose FactGraph, a method that decomposes the document and the summary into structured meaning representations (MR), which are more suitable for factuality evaluation. MRs describe core semantic concepts and their relations, aggregating the main content in both document and summary in a canonical form, and reducing data sparsity. FactGraph encodes such graphs using a graph encoder augmented with structure-aware adapters to capture interactions among the concepts based on the graph connectivity, along with text representations using an adapter-based text encoder. Experiments on different benchmarks for evaluating factuality show that FactGraph outperforms previous approaches by up to 15\%. Furthermore, FactGraph improves performance on identifying content verifiability errors and better captures subsentence-level factual inconsistencies.},
	urldate = {2024-09-22},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Leonardo F. R. and Liu, Mengwen and Gurevych, Iryna and Dreyer, Markus and Bansal, Mohit},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {3238--3253},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/EJN45DYR/Ribeiro et al. - 2022 - FactGraph Evaluating Factuality in Summarization with Semantic Graph Representations.pdf:application/pdf},
}

@misc{tang_minicheck_2024,
	title = {{MiniCheck}: {Efficient} {Fact}-{Checking} of {LLMs} on {Grounding} {Documents}},
	shorttitle = {{MiniCheck}},
	url = {http://arxiv.org/abs/2404.10774},
	abstract = {Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of "fact-checking" are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to LLMs to check a single response. In this work, we show how to build small models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact, collected from recent work on fact-checking and grounding LLM generations. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Tang, Liyan and Laban, Philippe and Durrett, Greg},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: LLM-AggreFact benchmark, MiniCheck models, data generation code at https://github.com/Liyan06/MiniCheck},
	file = {PDF:/Users/haoliu/Zotero/storage/CI6IHHZ2/Tang et al. - 2024 - MiniCheck Efficient Fact-Checking of LLMs on Grounding Documents.pdf:application/pdf},
}

@misc{kim_factkg_2023,
	title = {{FactKG}: {Fact} {Verification} via {Reasoning} on {Knowledge} {Graphs}},
	shorttitle = {{FactKG}},
	url = {http://arxiv.org/abs/2305.06590},
	abstract = {In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Kim, Jiho and Park, Sungjin and Kwon, Yeonsu and Jo, Yohan and Thorne, James and Choi, Edward},
	month = may,
	year = {2023},
	note = {arXiv:2305.06590 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ACL 2023},
	file = {PDF:/Users/haoliu/Zotero/storage/HD87Z37N/Kim et al. - 2023 - FactKG Fact Verification via Reasoning on Knowledge Graphs.pdf:application/pdf},
}

@misc{min_factscore_2023,
	title = {{FActScore}: {Fine}-grained {Atomic} {Evaluation} of {Factual} {Precision} in {Long} {Form} {Text} {Generation}},
	shorttitle = {{FActScore}},
	url = {http://arxiv.org/abs/2305.14251},
	abstract = {Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58\%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2\% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost \$26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	month = oct,
	year = {2023},
	note = {arXiv:2305.14251 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 25 pages; 7 figures. Published as a main conference paper at EMNLP 2023. Code available at https://github.com/shmsw25/FActScore},
	file = {PDF:/Users/haoliu/Zotero/storage/SA6AXUPT/Min et al. - 2023 - FActScore Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.pdf:application/pdf},
}

@misc{dmonte_claim_2024,
	title = {Claim {Verification} in the {Age} of {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Claim {Verification} in the {Age} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.14317},
	abstract = {The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems.1 Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Dmonte, Alphaeus and Oruche, Roland and Zampieri, Marcos and Calyam, Prasad and Augenstein, Isabelle},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14317 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/Users/haoliu/Zotero/storage/3EDSNL6P/Dmonte et al. - 2024 - Claim Verification in the Age of Large Language Models A Survey.pdf:application/pdf},
}

@misc{yuan_zero-shot_2023,
	title = {Zero-{Shot} {Fact}-{Checking} with {Semantic} {Triples} and {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2312.11785},
	abstract = {Despite progress in automated fact-checking, most systems require a significant amount of labeled training data, which is expensive. In this paper, we propose a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference. This allows it to generalize to adversarial datasets and domains that supervised models require specific training data for. Our empirical results show that our approach outperforms previous zero-shot approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being comparable or better than supervised models on the adversarial and the out-of-domain datasets.},
	language = {en},
	urldate = {2024-09-22},
	publisher = {arXiv},
	author = {Yuan, Zhangdie and Vlachos, Andreas},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11785 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/haoliu/Zotero/storage/4F4GLJIU/Yuan and Vlachos - 2023 - Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs.pdf:application/pdf},
}

@article{chang_communitykg-rag_nodate,
	title = {{CommunityKG}-{RAG}: {Leveraging} {Community} {Structures} in {Knowledge} {Graphs} for {Advanced} {Retrieval}-{Augmented} {Generation} in {Fact}-{Checking}},
	abstract = {Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, their effectiveness is often hindered by a lack of integration with entity relationships and community structures, limiting their ability to provide contextually rich and accurate information retrieval for fact-checking. We introduce CommunityKG-RAG (Community Knowledge Graph-Retrieval Augmented Generation), a novel zero-shot framework that integrates community structures within Knowledge Graphs (KGs) with RAG systems to enhance the fact-checking process. Capable of adapting to new domains and queries without additional training, CommunityKG-RAG utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval. Our experimental results demonstrate that CommunityKG-RAG outperforms traditional methods, representing a significant advancement in fact-checking by offering a robust, scalable, and efficient solution.},
	language = {en},
	author = {Chang, Rong-Ching and Zhang, Jiawei},
	file = {PDF:/Users/haoliu/Zotero/storage/UHNLK6MP/Chang and Zhang - CommunityKG-RAG Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmente.pdf:application/pdf},
}

@inproceedings{ma_kapalm_2023,
	address = {Singapore},
	title = {{KAPALM}: {Knowledge} {grAPh} {enhAnced} {Language} {Models} for {Fake} {News} {Detection}},
	shorttitle = {{KAPALM}},
	url = {https://aclanthology.org/2023.findings-emnlp.263},
	doi = {10.18653/v1/2023.findings-emnlp.263},
	abstract = {Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarse- and fine-grained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph and graph pooling layer. Finally, we integrate the coarse- and fine-grained entity knowledge representations with the textual representation for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines. In addition, it is competitive in the few-shot scenario.},
	urldate = {2024-10-06},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Jing and Chen, Chen and Hou, Chunyan and Yuan, Xiaojie},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {3999--4009},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/G85KYA3S/Ma et al. - 2023 - KAPALM Knowledge grAPh enhAnced Language Models for Fake News Detection.pdf:application/pdf},
}

@article{opsahl_fact_nodate,
	title = {Fact or {Fiction}? {Improving} {Fact} {Verification} with {Knowledge} {Graphs} through {Simplified} {Subgraph} {Retrievals}},
	abstract = {Despite recent success in natural language processing (NLP), fact verification still remains a difficult task. Due to misinformation spreading increasingly fast, attention has been directed towards automatically verifying the correctness of claims. In the domain of NLP, this is usually done by training supervised machine learning models to verify claims by utilizing evidence from trustworthy corpora. We present efficient methods for verifying claims on a dataset where the evidence is in the form of structured knowledge graphs. We use the FACTKG dataset, which is constructed from the DBpedia knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval process, from fine-tuned language models to simple logical retrievals, we are able to construct models that both require less computational resources and achieve better test-set accuracy.},
	language = {en},
	author = {Opsahl, Tobias A},
	file = {PDF:/Users/haoliu/Zotero/storage/TGMR4UDK/Opsahl - Fact or Fiction Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retri.pdf:application/pdf},
}

@article{gunjal_molecular_nodate,
	title = {Molecular {Facts}: {Desiderata} for {Decontextualization} in {LLM} {Fact} {Verification}},
	abstract = {Automatic factuality verification of large language model (LLM) generations is becoming more and more widely used to combat hallucinations. A major point of tension in the literature is the granularity of this fact-checking: larger chunks of text are hard to fact-check, but more atomic facts like propositions may lack context to interpret correctly. In this work, we assess the role of context in these atomic facts. We argue that fully atomic facts are not the right representation, and define two criteria for molecular facts: decontextuality, or how well they can stand alone, and minimality, or how little extra information is added to achieve decontexuality. We quantify the impact of decontextualization on minimality, then present a baseline methodology 1 for generating molecular facts automatically, aiming to add the right amount of information. We compare against various methods of decontextualization and find that molecular facts balance minimality with fact verification accuracy in ambiguous settings.},
	language = {en},
	author = {Gunjal, Anisha and Durrett, Greg},
	file = {PDF:/Users/haoliu/Zotero/storage/LNQMXATA/Gunjal and Durrett - Molecular Facts Desiderata for Decontextualization in LLM Fact Verification.pdf:application/pdf},
}

@misc{sansford_grapheval_2024,
	title = {{GraphEval}: {A} {Knowledge}-{Graph} {Based} {LLM} {Hallucination} {Evaluation} {Framework}},
	shorttitle = {{GraphEval}},
	url = {http://arxiv.org/abs/2407.10793},
	abstract = {Methods to evaluate Large Language Model (LLM) responses and detect inconsistencies, also known as hallucinations, with respect to the provided knowledge, are becoming increasingly important for LLM applications. Current metrics fall short in their ability to provide explainable decisions, systematically check all pieces of information in the response, and are often too computationally expensive to be used in practice. We present GraphEval: a hallucination evaluation framework based on representing information in Knowledge Graph (KG) structures. Our method identifies the specific triples in the KG that are prone to hallucinations and hence provides more insight into where in the response a hallucination has occurred, if at all, than previous methods. Furthermore, using our approach in conjunction with state-of-the-art natural language inference (NLI) models leads to an improvement in balanced accuracy on various hallucination benchmarks, compared to using the raw NLI models. Lastly, we explore the use of GraphEval for hallucination correction by leveraging the structure of the KG, a method we name GraphCorrect, and demonstrate that the majority of hallucinations can indeed be rectified.},
	language = {en},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Sansford, Hannah and Richardson, Nicholas and Maretic, Hermina Petric and Saada, Juba Nait},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10793 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 12 pages, to be published at KiL'24: Workshop on Knowledge-infused Learning co-located with 30th ACM KDD Conference, August 26, 2024, Barcelona, Spain},
	file = {PDF:/Users/haoliu/Zotero/storage/FNG7ZDYC/Sansford et al. - 2024 - GraphEval A Knowledge-Graph Based LLM Hallucination Evaluation Framework.pdf:application/pdf},
}

@article{kim__2024,
	title = {: {Evaluating} faithfulness and content selection},
	abstract = {While long-context large language models (LLMs) can technically summarize book-length documents ({\textgreater} 100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of \$5.2K USD, which allows us to rank LLM summarizers based on faithfulness: CLAUDE-3-OPUS significantly outperforms all closedsource LLMs, while the open-source MIXTRAL is on par with GPT-3.5-TURBO. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book. We release FABLES to spur further research on the evaluation of book-length summarization.},
	language = {en},
	author = {Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and Garimella, Aparna and Manjunatha, Varun and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
	year = {2024},
	file = {PDF:/Users/haoliu/Zotero/storage/WW78W6A5/Kim et al. - 2024 -  Evaluating faithfulness and content selection.pdf:application/pdf},
}

@misc{weng_extrinsic_2024,
	title = {Extrinsic {Hallucinations} in {LLMs}},
	url = {https://lilianweng.github.io/posts/2024-07-07-hallucination/},
	abstract = {Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.
There are two types of hallucination:
In-context hallucination: The model output should be consistent with the source content in context.},
	language = {en},
	urldate = {2024-10-22},
	author = {Weng, Lilian},
	month = jul,
	year = {2024},
	note = {Section: posts},
	file = {Snapshot:/Users/haoliu/Zotero/storage/SZ3EXHJB/2024-07-07-hallucination.html:text/html},
}

@article{wei_long-form_nodate,
	title = {{LONG}-{FORM} {FACTUALITY} {IN} {LARGE} {LANGUAGE} {MODELS}},
	abstract = {Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for longform factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall).},
	language = {en},
	author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Huang, Jie and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da},
	file = {PDF:/Users/haoliu/Zotero/storage/Z8ZVE2ZS/Wei et al. - LONG-FORM FACTUALITY IN LARGE LANGUAGE MODELS.pdf:application/pdf},
}

@misc{kim_fables_2024,
	title = {{FABLES}: {Evaluating} faithfulness and content selection in book-length summarization},
	shorttitle = {{FABLES}},
	url = {http://arxiv.org/abs/2404.01261},
	abstract = {While long-context large language models (LLMs) can technically summarize book-length documents ({\textgreater} 100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of \$5.2K USD, which allows us to rank LLM summarizers based on faithfulness: CLAUDE-3-OPUS significantly outperforms all closedsource LLMs, while the open-source MIXTRAL is on par with GPT-3.5-TURBO. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book. We release FABLES to spur further research on the evaluation of book-length summarization.},
	language = {en},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and Garimella, Aparna and Manjunatha, Varun and Lo, Kyle and Goyal, Tanya and Iyyer, Mohit},
	month = sep,
	year = {2024},
	note = {arXiv:2404.01261 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: preprint - 39 pages},
	file = {PDF:/Users/haoliu/Zotero/storage/KJL37AF4/Kim et al. - 2024 - FABLES Evaluating faithfulness and content selection in book-length summarization.pdf:application/pdf},
}

@inproceedings{qiu_amrfact_2024,
	address = {Mexico City, Mexico},
	title = {{AMRFact}: {Enhancing} {Summarization} {Factuality} {Evaluation} with {AMR}-{Driven} {Negative} {Samples} {Generation}},
	shorttitle = {{AMRFact}},
	url = {https://aclanthology.org/2024.naacl-long.33},
	doi = {10.18653/v1/2024.naacl-long.33},
	abstract = {Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.},
	urldate = {2024-10-28},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Haoyi and Huang, Kung-Hsiang and Qu, Jingnong and Peng, Nanyun},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {594--608},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/PK3D8A8T/Qiu et al. - 2024 - AMRFact Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation.pdf:application/pdf},
}

@misc{noauthor_active_llm_h_nodate,
	title = {active\_llm\_h… (auto-{E}) - {JupyterLab}},
	url = {https://d-lkx6oa6ougrt.studio.us-east-1.sagemaker.aws/jupyter/default/lab/workspaces/auto-E/tree/marketingfm-eval},
	urldate = {2024-10-28},
	file = {marketingfm-… (auto-E) - JupyterLab:/Users/haoliu/Zotero/storage/P6XJTL9N/marketingfm-eval.html:text/html},
}

@inproceedings{wan_acueval_2024,
	address = {Bangkok, Thailand},
	title = {{ACUEval}: {Fine}-grained {Hallucination} {Evaluation} and {Correction} for {Abstractive} {Summarization}},
	shorttitle = {{ACUEval}},
	url = {https://aclanthology.org/2024.findings-acl.597},
	doi = {10.18653/v1/2024.findings-acl.597},
	abstract = {The impressive generation capabilities of large language models (LLMs) have made it harder to detect the subtle hallucinations they make in abstractive summarization, where generated summaries consist of a blend of correct and incorrect information w.r.t. a given document. Recently-proposed LLM-based evaluation metrics attempt to capture this, but still face challenges: (1) they are biased towards summaries generated from the same underlying LLM, and (2) they lack interpretability, offering only a single score. In this work, we present ACUEval, a metric that leverages the power of LLMs to perform two sub-tasks: decomposing summaries into atomic content units (ACUs), and validating them against the source document. Compared to current strong LLM-based metrics, our two-step evaluation strategy improves correlation with human judgments of faithfulness on three summarization evaluation benchmarks by 3\% in balanced accuracy compared to the next-best metric, and also shows reduced preference bias towards LLM-generated summary. Further, we show that errors detected by ACUEval can be used to generate actionable feedback for refining the summary, improving the faithfulness scores by more than 10\%.},
	urldate = {2024-11-04},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Wan, David and Sinha, Koustuv and Iyer, Srini and Celikyilmaz, Asli and Bansal, Mohit and Pasunuru, Ramakanth},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {10036--10056},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/6VJCZYQA/Wan et al. - 2024 - ACUEval Fine-grained Hallucination Evaluation and Correction for Abstractive Summarization.pdf:application/pdf},
}

@inproceedings{zhong_reasoning_2020,
	address = {Online},
	title = {Reasoning {Over} {Semantic}-{Level} {Graph} for {Fact} {Checking}},
	url = {https://aclanthology.org/2020.acl-main.549},
	doi = {10.18653/v1/2020.acl-main.549},
	abstract = {Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.},
	urldate = {2024-11-11},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Wanjun and Xu, Jingjing and Tang, Duyu and Xu, Zenan and Duan, Nan and Zhou, Ming and Wang, Jiahai and Yin, Jian},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {6170--6180},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/LTBICRDV/Zhong et al. - 2020 - Reasoning Over Semantic-Level Graph for Fact Checking.pdf:application/pdf},
}

@inproceedings{qiu_amrfact_2024-1,
	address = {Mexico City, Mexico},
	title = {{AMRFact}: {Enhancing} {Summarization} {Factuality} {Evaluation} with {AMR}-{Driven} {Negative} {Samples} {Generation}},
	shorttitle = {{AMRFact}},
	url = {https://aclanthology.org/2024.naacl-long.33},
	doi = {10.18653/v1/2024.naacl-long.33},
	abstract = {Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.},
	urldate = {2024-11-11},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Qiu, Haoyi and Huang, Kung-Hsiang and Qu, Jingnong and Peng, Nanyun},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {594--608},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/3D9BWPEF/Qiu et al. - 2024 - AMRFact Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation.pdf:application/pdf},
}

@misc{zhu_rageval_2024,
	title = {{RAGEval}: {Scenario} {Specific} {RAG} {Evaluation} {Dataset} {Generation} {Framework}},
	shorttitle = {{RAGEval}},
	url = {http://arxiv.org/abs/2408.01262},
	abstract = {Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics Completeness, Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications.},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Zhu, Kunlun and Luo, Yifan and Xu, Dingling and Wang, Ruobing and Yu, Shi and Wang, Shuo and Yan, Yukun and Liu, Zhenghao and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2024},
	note = {arXiv:2408.01262},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/CFBUGEC7/Zhu et al. - 2024 - RAGEval Scenario Specific RAG Evaluation Dataset Generation Framework.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/UU57DRYV/2408.html:text/html},
}

@misc{noauthor_openreview_nodate,
	title = {{OpenReview}},
	url = {https://openreview.net/notifications?email=liuhr99%40outlook.com},
	abstract = {Promoting openness in scientific communication and the peer-review process},
	language = {en},
	urldate = {2024-11-18},
	journal = {OpenReview},
	file = {Snapshot:/Users/haoliu/Zotero/storage/ELCGCSAQ/notifications.html:text/html},
}

@inproceedings{tang_understanding_2023,
	address = {Toronto, Canada},
	title = {Understanding {Factual} {Errors} in {Summarization}: {Errors}, {Summarizers}, {Datasets}, {Error} {Detectors}},
	shorttitle = {Understanding {Factual} {Errors} in {Summarization}},
	url = {https://aclanthology.org/2023.acl-long.650},
	doi = {10.18653/v1/2023.acl-long.650},
	abstract = {The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights.},
	urldate = {2024-11-16},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tang, Liyan and Goyal, Tanya and Fabbri, Alex and Laban, Philippe and Xu, Jiacheng and Yavuz, Semih and Kryscinski, Wojciech and Rousseau, Justin and Durrett, Greg},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {11626--11644},
	file = {Full Text PDF:/Users/haoliu/Zotero/storage/A9CGGLJX/Tang et al. - 2023 - Understanding Factual Errors in Summarization Errors, Summarizers, Datasets, Error Detectors.pdf:application/pdf},
}

@misc{dammu_claimver_2024,
	title = {{ClaimVer}: {Explainable} {Claim}-{Level} {Verification} and {Evidence} {Attribution} of {Text} {Through} {Knowledge} {Graphs}},
	shorttitle = {{ClaimVer}},
	url = {http://arxiv.org/abs/2403.09724},
	abstract = {In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.},
	urldate = {2024-11-24},
	publisher = {arXiv},
	author = {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Dewan, Mouly and Kim, YoungMin and Roosta, Tanya and Chadha, Aman and Shah, Chirag},
	month = sep,
	year = {2024},
	note = {arXiv:2403.09724},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/ZEIXQMWA/Dammu et al. - 2024 - ClaimVer Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Gr.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/UIVGU7ZD/2403.html:text/html},
}

@misc{zha_alignscore_2023,
	title = {{AlignScore}: {Evaluating} {Factual} {Consistency} with a {Unified} {Alignment} {Function}},
	shorttitle = {{AlignScore}},
	url = {http://arxiv.org/abs/2305.16739},
	doi = {10.48550/arXiv.2305.16739},
	abstract = {Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.},
	urldate = {2025-01-12},
	publisher = {arXiv},
	author = {Zha, Yuheng and Yang, Yichi and Li, Ruichen and Hu, Zhiting},
	month = may,
	year = {2023},
	note = {arXiv:2305.16739 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, 5 figures, ACL2023},
	file = {Preprint PDF:/Users/haoliu/Zotero/storage/BSW9JI2W/Zha et al. - 2023 - AlignScore Evaluating Factual Consistency with a Unified Alignment Function.pdf:application/pdf;Snapshot:/Users/haoliu/Zotero/storage/7F462BQB/2305.html:text/html},
}

@article{yang2025retrieval,
  title={Retrieval-augmented generation for generative artificial intelligence in health care},
  author={Yang, Rui and Ning, Yilin and Keppo, Emilia and Liu, Mingxuan and Hong, Chuan and Bitterman, Danielle S and Ong, Jasmine Chiat Ling and Ting, Daniel Shu Wei and Liu, Nan},
  journal={npj Health Systems},
  volume={2},
  number={1},
  pages={2},
  year={2025},
  publisher={Nature Publishing Group UK London}
}