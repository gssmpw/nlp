\vspace{-3mm}
\section{Evaluation Setup}

\paragraph{Dataset.}
%描述数据集
We used the BIOSCAN-5M dataset \cite{gharaee2024bioscan5m}, the largest multi-modal resource available for genotype-to-phenotype prediction.
It contains over 5 million insect specimens with taxonomic labels, DNA barcode sequences, geographic coordinates (longitude and latitude), and phenotypic images. We preprocessed the phenotypic images by resizing and padding them to a resolution of 
$256 \times 256$. The seen-set images were then split into training and validation sets using a 90-10 ratio. Additionally, the dataset includes an unseen set, consisting of samples either lacking species labels or belonging to organisms without established scientific names.

% To ensure balanced species evaluation, we place samples in the test set with a flattened species distribution.
% Importantly, the DNA barcodes in the testing set were specifically chosen to be absent from the training set, ensuring that the model is evaluated on completely unseen data.


\vspace{-3mm}
\paragraph{Baselines.}
Since no direct baselines for genotype-to-phenotype image synthesis, we employ a comparative framework that adapts the leading conditional image generation methods to this specialized task. The baselines include GAN-based approaches such as DF-GAN \cite{liao2022text},  diffusion-based methods like Stable Diffusion \cite{rombach2022high}, and ControlNet \cite{zhang2023adding}.
% Each method was carefully adapted to the task of genotype-to-phenotype prediction, ensuring consistent training and testing conditions for a fair evaluation.

% and ControlNet \cite{zhang2023adding}. 

\vspace{2mm}
\textit{We introduce the following new metrics for the genotype-to-phenotype prediction task}:
\vspace{-3mm}
% While the proposed G2PDiffusion model approaches genotype-to-phenotype prediction from an image generation perspective, it is important to recognize that traditional image generation metrics, such as SSIM \cite{nilsson2020understanding}, are not well-suited for this task. This limitation arises because images of the same species, captured at different developmental stages or from varying angles, can introduce biases in simple image similarity measures. To address this, we introduce the following new metrics:
% This is because even images of the same species taken from different developmental stages or different angles can bias simple image similarity metrics. Therefore, 

% \vspace{-1em}
\paragraph{CLIBDScore.}
This metric is built on the pre-trained CLIBD model \cite{gong2024clibd} to measure the semantic similarity between the DNA and image, which uses CLIP-style \cite{radford2021learning} contrastive learning to align images and barcode DNA representations in a unified embedding space.
Similar to CLIPScore \cite{hessel2021clipscore}, a commonly-used metric for text-image alignment, \textbf{CLIBDScore} measures how well an image-based morphology is aligned with the corresponding DNA.

\begin{figure}[h]
    \centering
    % \vspace{-1em}
    \includegraphics[width=0.9\linewidth]{img/distribution_contrastive_line.pdf}
    \vspace{-2mm}
    \caption{Density Distribution of DNA-Image CLIBDScore.}
    \label{fig:CLIBDScore_Distribution}
    \vspace{-8mm}
\end{figure}

% \vspace{-2em}
\paragraph{Success Rate.}
Moreover, we compute the CLIBDScore for all DNA-image pairs in the training set, and the randomly shuffled pairs for comparison. 
The density distributions of these two sets are illustrated in Fig.~\ref{fig:CLIBDScore_Distribution}.
% The results show that the average CLIBDScore for true DNA-image pairs is significantly higher than that for the shuffled pairs (with values of 0.513 and 0.004, respectively).
% We note that a ground truth CLIBDScore of 0.513 is acceptable, as phenotype expression is influenced not only by genetic factors but also by environmental factors. 
The minimal overlap between the two distributions indicates that true DNA-image pairs are distinguishable from the random pairs. 
Building on this observation, we introduce an additional evaluation metric, \textbf{Success Rate}, which is based on the intersection line $x=x_0=0.255$ between the two distributions. If CLIBDScore exceeds this threshold $x_0$, the prediction is considered successful; otherwise, it is considered a failure.

\vspace{-5mm}
\paragraph{PES.} We introduce Phenotype Embedding Similarity (PES) as a metric to assess the biological relevance of generated images by comparing them to real images in a learned phenotype feature space. Specifically, we first train a species classification model using authentic phenotype images, with an intermediate embedding layer that captures species-specific visual characteristics. During evaluation, both real and generated images are processed through the classifier to extract their embeddings, and PES is calculated as the average cosine similarity between the embeddings of real and generated images corresponding to the same DNA. Higher PES values indicate that the generated images more accurately preserve species-level phenotypic features, providing a biologically meaningful measure of image quality.

% We introduce \textbf{Phenotype Embedding Similarity} (PES) to evaluate the biological relevance of generated images by assessing their similarity to real images in a learned phenotype feature space. Specifically, we first train a species classification model using real phenotype images, where the intermediate embedding layer captures species-specific visual characteristics. During evaluation, both the real and generated images are passed through the classifier to extract their embeddings, and PES is computed as the average cosine similarity between the embeddings of real and generated images for the same DNA.
% Higher PES values indicate that the generated images better preserve species-level phenotype features, providing a biologically meaningful measure for image quality assessment.


% \paragraph{FID.} Fréchet Inception Distance (FID) is a widely used metric for assessing the quality of generated images by comparing their distribution to that of real images. FID captures both perceptual quality and diversity by computing the Fréchet distance between feature distributions extracted from an Inception network. Lower FID values indicate higher similarity between generated and real distributions, implying better generation quality. 

\vspace{-5mm}
\paragraph{Implementation details.} All the models are trained on 8 NVIDIA-A100 GPUs using Adam optimizer \cite{kingma2014adam} up to 100k steps, with the learning rate of 1e-5, batch size of 128 and cosine annealing scheduler. 
During sampling, for each given DNA, we generate $n$ images, compute CLIBDScore, Success Rate, and PES metrics, and record the hignest score as the top-$n$ values ($n$ takes values of 1, 5, 10, 20, 50, and 100, as shown in the following experimental sections).



% The dataset contains 343,333 samples labeled with species that have established scientific names, representing the \textbf{closed-world setting}. 



