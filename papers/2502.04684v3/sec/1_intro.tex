\section{Introduction}
\label{sec:intro}





% 先讲清楚背景和问题的意义
One of the fundamental biology challenges is understanding how genes interact with environmental factors to determine phenotype \cite{lehner2013genotype}, which has profound implications for crop breeding \cite{araus2014field,danilevicz2022plant}, disease resistance \cite{weatherall2001phenotype}, and personalized therapeutics \cite{nebert2012personalized}. Phenotypes can be physiological, morphological, and behavioral, such as the resistance to toxins, wing shape, and foraging behavior. This paper focuses on morphological phenotypes, aiming to understand how genes influence phenotypes, how species evolve under natural selection, and how phenotypic diversity is formed.





Conventional genotype-to-phenotype prediction usually relies on statistical methods such as genome-wide association studies (GWAS) \cite{uffelmann2021genome, tam2019benefits, visscher2012five,gallagher2018post,harris2024genome} and quantitative trait locus (QTL) mapping \cite{kearsey1998qtl,mccough1995qtl,korstanje2002qtl}. Recent works \cite{wang2024lstm, andreu2024phenolinker, yelmen2024interpreting,dingemans2023phenoscore} apply deep learning models to decode the intricate genotype-phenotype interactions. 
However, the existing approaches are limited to individual species due to the expensive phenotype labeling process.  As the phenotypic features are located in high-dimensional space and measured using specified equipment, labeling large populations of individuals requires intensive effort. The labeling cost is even more enormous when studying complex genotype-phenotype relationships across different species.
To break through the limitation induced by high-dimensional phenotype space, we propose to solve the problem from a novel perspective. As shown in Fig.~\ref{fig:cover_idea}, we suggest taking images as phenotypic proxies and formulating the genotype-to-phenotype prediction problem as conditional image generation. By learning from millions of DNA-image pairs across diverse taxa, our framework facilitates efficient and scalable cross-species genotype-to-phenotype prediction.

\vspace{-3mm}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{img/intro.pdf}
    \vspace{-6mm}
    \caption{G2PDiffusion generates morphological images using advanced diffusion model and cross-species large data.}
    \label{fig:cover_idea}
\end{figure}
\vspace{-3mm}



We propose the first genotype-to-phenotype diffusion model (\textbf{G2PDiffusion}) that generates morphological images from DNA considering two evolutionary signals, i.e., multiple sequence alignments (MSA) and environmental contexts. MSA identify evolutionary conserved and variable regions in DNA sequences, revealing genetic variations across individuals or species that contribute to morphological diversity. However, phenotypic traits are not solely determined by genotype; they are also influenced by external factors such as climate, food sources, and social interactions. Considering these influences, we take latitude and longitude as environmental factors. Both MSA and environmental contexts are regarded as evolutionary signals, enhancing the accuracy and realism of phenotype prediction.



G2PDiffusion contains three novel components: a MSA retrieval engine, an environment-aware MSA conditioner, and a dynamic genotype-phenotype aligner. Firstly, the MSA engine retrieves DNA alignments from an external database to identify evolutionarily conserved and variable sequence regions. Secondly, the retrieved MSA and environmental contexts are fed into a conditional encoder, which leverages novel MSA attention modules to capture genotype-environment (GxE) interactions. Then, we build a diffusion model conditioned on the GxE representation to generate images capturing morphological features. During each denoising step, a dynamic phenomic alignment module is employed to refine phenotypic representations.


We rigorously assess the performance of our proposed approach by comparing it with competitive baselines across diverse species under both seen and unseen conditions. We employ a range of quantitative metrics—including alignment scores, success rates, and phenotype embedding similarities—to evaluate the accuracy, biological relevance, and consistency of the generated images with the underlying genotype information. Extensive experiments demonstrate that our method not only significantly outperforms traditional models but also effectively captures the intricate genotype-environment interactions, thereby establishing its robustness and generalizability for cross-species phenotype prediction.


In summary, our \textbf{contributions} are as follows: 
\begin{itemize}
    \item We redefine the genotype-to-phenotype prediction problem as a conditional image generation, offering a novel solution to address the challenges of modeling complex environment-genotype-phenotype interactions.
    \item We propose G2PDiffusion, a first-of-its-kind diffusion model for genotype-to-phenotype prediction, where a novel evolution-aware conditional mechanism and a dynamic alignment module are proposed.
    \item G2PDiffusion can predict phenotype from genotype with high accuracy and consistency (Figure \ref{fig:1_overview_result}), offering a valuable exploration into AI-assisted genomic analysis.
    % It offers a pioneering approach for AI-assisted genomic analysis, potentially laying a foundation for future research.
    
\end{itemize}





