\section{Literature Review}
Both supervised and unsupervised learning techniques have been applied to address the AC-OPF problem. In the supervised learning framework, traditional solvers are commonly used to generate input-output data pairs by finding the optimal solution to the given load demand. \cite{Fioretto2019} and \cite{chatzos2020} utilize fully connected neural networks (FCNNs) to output all decision variables. \cite{Chatzos2022} proposes a two-stage approach for scalable learning while \cite{Seonho2023} employs compact optimization learning to compress the space of optimal solutions. By contrast, \cite{Huang2021_v} and \cite{Lei2021} focus on outputting voltage phasors and recovering active/reactive power generation via power balance equations. Some studies leverage the power grid's topology information by using graph neural networks, convolutional neural networks (CNNs), and Chebyshev CNNs in place of FCNNs; see \cite{Owerko2020,falconer2022,Gao2023}. However, a major drawback of these approaches is the lack of constraint satisfaction. \cite{NELLIKKATH} proposes a physics-informed NN that outputs the primal and dual variables and adds the KKT condition violations in the loss term. To address the load mismatch issue, \cite{deepopf} and \cite{WANG2022} utilize FCNNs to predict a partial set of decision variables and reconstruct the remaining variables using power flow (PF) solvers. Furthermore, the time required for data pair preparation in existing works remains substantial due to the long solve times of conventional solvers generating the data. Frequent training on such data is important to ensure that the quality of NN solutions does not degrade~\cite{Zhou2023}. In the modern smart grid, the Internet of Things framework allows ISOs to rapidly acquire system states through the advanced communication network, such as monitoring the load demands and controlling distributed energy resources by smart devices \cite{ZhangIot}.

Recent research has used unsupervised learning as a tool to streamline the laborious process of data preparation. In unsupervised learning techniques, the training process of NNs itself ensures that its outputs are feasible and optimal solutions to AC-OPF without requiring explicit targets. \cite{huang2021} and \cite{Junfei2022} incorporate both the OPF objective function and a penalty for constraint violations in their training loss. Other approaches, such as \cite{parkaaai}, exploit duality theory to jointly train primal and dual networks with a loss function inspired by the augmented Lagrangian method. \cite{Cao} formulates the multi-period OPF as a stochastic nonlinear programming problem and solves the Markov decision process by employing reinforcement learning. To fulfill load balance, \cite{donti2021dc3} and \cite{KejunGlobal} employ a variable splitting (VS) scheme, which predicts a subset of decision variables and reconstructs the remaining variables using PF solvers (e.g., fast decoupled power flow (FDPF)). The unsupervised learning framework typically relies on bounded activation functions in the output layer to ensure the automatic satisfaction of inequality constraints. However, in the worst-case scenario, subsequent PF solvers may fail to find feasible solutions, which leads to unsuccessful training. However, variable splitting introduces new challenges when it comes to computing gradients. During each training iteration, the gradient of the reconstructed variables with respect to the predicted variables needs to be calculated. Based on the implicit function theorem, \cite{donti2021dc3} employs the inverse Jacobian matrix to derive the gradient. To alleviate the computational complexity, \cite{deepopf} and \cite{WANG2022} adopt the zeroth-order estimation method \cite{Liu2020}. Nevertheless, training can still be time-consuming, as it requires more iterations to converge to the optimal solution. Consequently, these existing schemes are ill-suited for timely and frequent updates of the NNs.