\section{Passthrough Modes and Metric Design}

\begin{figure*}[htbp]
    \includegraphics[width=\textwidth]{images/DistortionsAndScale.pdf}
    \caption{\textbf{\DP versus \GAP output images.} Two images are shown above taken from the headset placed at the same point in the scene. We observe that \DP enlarges all the objects, making the scene look closer to the user. The scale difference can be easily noticed when comparing the text on the bottom right of both images. While \GAP improves the scale, it also results in warping artifacts which can be observed on the edge of the door behind the hand.}
    \Description{We showcase the difference between DP and GAP captures through the headset. In the 4 sub-figures, the capture is that of a hand in front of a puzzle box. The top left capture of DP shows incorrect scale since the puzzle and hand look closer than they actually are. The top right capture of GAP shows warping on the door edge behind the hand. The bottom captures are the same as the top ones with the differences highlighted in squares.}
    \label{fig:artifacts}
\end{figure*}

In this section, we first describe the technical differences between the two passthrough approaches (i) \Directpassthrough Passthrough (DP) and (ii) \DepthPassthrough Passthrough (GAP) that we later evaluate in Section \ref{study}. 
We then provide a technical overview of \GAP and propose metrics for evaluating the geometric accuracy and warping artifacts.
\newline
\textbf{Hardware Details.} The studies were conducted using a mixed-reality headset powered by Android XR\footnote[1]{\url{https://www.android.com/xr/}}.  
We strive to keep all factors other than reprojection such as cameras, image processing, and latency constant across experiments. 
We also utilized the same device to run user studies for fair comparison. 
% We understand that while these factors can have a huge impact on the user comfort, we do not consider these parameters for evaluation in this work and hope that future work can further explore the impact of these factors.

\subsection{Direct Passthrough}

% {\color{red}\textbf{Mohit}}
% \begin{itemize}
%     \item Talk about Direct Passthrough, and problems with exaggerated head motion.
%     \item Emphasize the exaggerated head motion is more prominent on close objects and less on further away objects requiring a depth-based correction.
%     \item Then talk about how this can be seen as a planar re-projection where a fronto-parallel plane is assumed at infinite distance. We instead choose a plane at 1 meters to better reflect the geometry. Later, we will discuss some measure for evaluating geometry, where we will further see the impact of this parameter.
% \end{itemize}

\DP presents unprocessed video feeds directly from external VST HMD cameras, forgoing any form of view correction. This inherent simplicity, however, introduces a critical limitation: visual discrepancies in object positions and scales (see Figure \ref{fig:passthroughmodes}). The disparity between the physical camera locations and the user's eyes manifests, most notably, as an exaggerated perception of motion parallax, particularly pronounced when observing objects in close proximity. 
% The underlying cause of this phenomenon lies in the fundamental difference between the centers of projection for the camera and the human eye.
From a theoretical perspective, \DP can be interpreted as a simplified form of planar reprojection, where the scene is assumed to be geometrically planar (see Figure \ref{fig:passthroughmodes}), with the plane being fronto-parallel and situated infinitely far from the cameras. In our practical implementation, we modify the positioning of the projection plane at a distance of 2 meters from the cameras since most of the objects lie in the 1-2 meters range. This adjustment helps improve the perceived scale of the scene by \emph{better} reflecting the average depth of objects, specifically in context of the activities chosen for cybersickness analysis (see section \ref{subsec:tasks} for more details). Moreover, it can be implemented using a simple homography transformation with minimal computational cost, without doing any geometry based correction fundamental to \DP.
% Direct Passthrough approach directly displays the video captured by external VST HMD cameras without any modifications such as reprojection techniques. This approach does not account for the difference in physical positions of the external cameras and the eyes, and presents the video directly to the user without accounting for the visual errors introduced by camera-eye separation. The differences in center of projections for camera and eye can produce perceived exaggerated movement due to head motion The exaggerated head motion is especially prominent on close objects and less far objects, requiring a depth based correction. This method is analogous to a planar re-projection where a fronto-parallel plane is assumed at infinite distance. We instead choose a plane at 1 meters to better reflect the geometry. 
% Later, we will discuss some measure for evaluating geometry, where we will further see the impact of this parameter.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/GeometricalMetric.pdf}
  \caption{\textbf{Spatial Reprojection Error}. Evaluation of perceived scene geometry with reprojection error. We take the Estimated Depth and GT Depth for the same image, and reproject every pixel to the eye view. The differences in the reprojected locations is used as the evaluation criteria. On the right, we compare the effects of smoothing the depth. Since the estimated depth can have errors, warping and rotating this depth map to the right camera can further exacerbate these errors. Gaussian smoothing of the estimated depth helps reduce these errors on the right eye as shown above.}
  \label{fig:geometrical_metric}
  \Description{Geometrical Assessment Measure: On the left, we visualize how the spatial reprojection error is calculated. Heat maps of the estimated and ground truth depth are shown. We also visualize the distance between the projected ground truth and estimated points on a grid. This distance is the reprojection error. On the right, we visualize heat maps of depth with raw and smooth GAP.}
\end{figure*}

\subsection{\DepthPassthrough Passthrough}
% {\color{red}\textbf{Mohit}}
% \begin{itemize}
%     \item Talk about Depth Passthrough, and problems with more compute needs, added latency and quality of depth estimation. Discuss that even best geometry will introduce holes and some warping is therefore needed to fill those areas of disocclusion. (Look at Figure 1.)
%     \item For practicality, we estimate the depth at 10 fps for the left camera feed and then with known calibration re-render the depth for the right eye and use it for reprojection. 
%     \item Talk about errors in the eye space that are introduced here. We notice two dimensions that are predominant error modes, one is the geometry of scene (to capture any shifts or scaling artefacts) and then warping (inaccurate depth can cause warping on even planar surfaces).
% \end{itemize}

% As opposed to the Direct Passthrough, Geometry Aware Passthrough reprojects the external camera images to the eye viewpoint, minimizing the camera-eye separation errors. 
% However, since the reprojected view is computationally calculated, this approach can add complexities due to the need for additional compute resources, potentially adding latency to the VST system. Additionally even the best geometric estimation will introduce holes in the reprojected imagery, and some warping is therefore needed to fill those areas of disocclusion. 
% The errors in the eye space that can be potentially introduced here are predominantly in two dimensions, (i) Shift or scaling artefacts can impact the original Geometry of the scene, and (ii) Warping artifacts can occur depending on the quality of depth estimation.
In contrast to \DP, \GAP employs novel view synthesis techniques or reprojection to align external camera images with the user's viewpoint, thereby mitigating the inherent disparity between camera and eye positions. 
While this reprojection can potentially enhance visual fidelity, it requires more computational resources, which can introduce additional latency into the VST system. 
Furthermore, even under ideal conditions with perfect geometric estimations, the reprojection process may result in gaps or holes in the rendered imagery due to depth disocclusion. Disocclusions occur when objects closer to the camera obscure portions of the scene that would be visible from the userâ€™s perspective. 
\GAP is susceptible to two primary categories of artifacts. First, geometric artifacts can manifest due to inaccurate depth perception, leading to incorrect positioning and scale of objects. 
Second, warping artifacts can emerge as a consequence of filling the aforementioned gaps or holes by stretching the foreground or background at depth discontinuities (see Figure \ref{fig:artifacts}). 
% The severity of these warping artifacts is contingent upon the quality and accuracy of the depth estimation algorithms utilized.
In our specific implementation, we leverage a stereo depth estimation algorithm to obtain the depth information for the left camera. We then generate the corresponding depth maps for the right camera view using known camera calibration parameters. It is important to note that this choice of depth estimation method is not fundamental to GAP; it can be replaced with any other suitable depth estimation technique without compromising the overall methodology. We follow Chaurasia et al. \cite{chaurasia2020passthroughplus} as our approach is similar to their passthrough implementaiton ---  \emph{Passthrough+}.
% To address this, warping techniques are needed to fill these missing areas, but this can introduce additional visual artifacts. The potential errors in the eye space resulting from this approach are primarily two-dimensional in nature: (i) Shifts or scaling artifacts can distort the original geometry of the scene, and (ii) warping artifacts can arise depending on the accuracy of the depth estimation algorithms.

\input{geometrical_metrics}

\subsubsection{Geometry Evaluation}
\edits{Geometrical correctness of the rendered scene is fundamental to any AR/VR system \cite{de2024visual, deptheval2023, objectdepth22} that relies on the head tracking quality, depth estimation, device calibration and the overall rendering pipeline. Particularly for passthrough,} the main advantage of using geometrical knowledge of the scene before applying reprojection is to \edits{allow} for correctly projecting each point in the world exactly where the eye expects to see it on the display. However, geometry estimation is not perfectly accurate, and the estimate needs to be reused for multiple future images (to perform reprojection in real time), which further impacts its accuracy. Therefore it is crucial to evaluate the impact of errors in depth estimation on the display as seen by the user. We demonstrate that these errors cannot be trivially obtained by just examining geometry errors in 3D space, such as mean absolute error in depth estimated per pixel. To this end, we obtain ground-truth depth of several indoor scenes using 3D laser scanners and then calibrate our headset in those scenes using ArUco \cite{aruco} markers with a bundle adjustment algorithm to collect aligned benchmarking data. We collected 9 datasets in total, each containing 3000 images. This provides paired frames $D_{est}, D_{GT}$ for every input camera image $I_{cam}$ (our metric does not rely on $I_{cam}$ and we use it for visualization purposes only). Then, each pixel in $I_{cam}$ is reprojected into the eye view using the reprojection operator which uses the depth and the camera intrinsics for the input camera, followed by projection into the eye. We define the resulting pixel projections as $P_{est}$ and $P_{GT}$ when using $D_{est}$ and $D_{GT}$, respectively. Finally, the error is computed as the L-1 norm of the residual $|P_{est} - P_{GT}|$ (see Figure \ref{fig:geometrical_metric}). We refer to this as the spatial reprojection error, which we compute for both left and right eye. Additionally, we compute the mean absolute error in depth as $|D_{est} - D_{GT}|$ averaged over the image.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{images/DepthErrorsSingle.pdf}
    \caption{\textbf{Geometrical Errors.} Comparison of the errors in depth estimation for different \GAP variants and DP. We observe that smoothing increases the depth errors. DP on the other hand has the highest error among all approaches achieving minimum depth error at the 2m (2000mm) mark which is expected since we assume all the points are at 2m for DP.}
    \Description{Bar plot of GAP (raw), GAP (smooth), GAP (oversmooth), and DP depth error. The y-axis represents depth error (mm) while the x-axis represents depth range (mm). All GAP methods show an increasing depth error with increasing depth range. Smoothing increases depth errors. DP has the highest error among all approaches. The error with DP decreases until 2000 mm then increases again.}
    \label{fig:depth_errors}
\end{figure}

We compared \DP and \GAP using the spatial reprojection error and the results are summarized in Table \ref{tab:geometric_metrics}. We observed that DP, on average, has a 3.2 pixel error across the 9 datasets we evaluated on. In comparison, \GAP - Depth (Raw) gave a 4x improvement in the spatial reprojection error (see differences in perceived scale in Figure \ref{fig:artifacts}). However, we observed that the error on the right eye is much higher than that on the left eye since the depth is estimated on the left camera and then warped to the right camera. We believe that this depth warping can exacerbate the depth errors on objects, particularly at object boundaries due to depth discontinuities. This is consistent with our observation that \GAP - Depth (Smooth), which uses a Gaussian kernel, reduces the error on the right eye by 25\%. Since smoothing smoothes out changes in depth, we see that errors due to warping get reduced specifically on object edges in the right eye after smoothing (see spatial reprojection errors visualized on the right in Figure \ref{fig:geometrical_metric}). However, when we increased the smoothing of the estimated depth further, we observed a higher spatial reprojection error in both eyes. We emphasize that these insights, derived from spatial reprojection error, cannot be  understood by examining depth error alone in 3D space (as seen in Table \ref{tab:geometric_metrics}). We also observed the error in depth estimation at different scene depths for DP and different variants of GAP in Figure \ref{fig:depth_errors}. We noted that DP achieves  minimal depth error only for points approximately 2 meters in depth, which supports our findings. This further validates the choice of GAP over DP. For the remainder of the paper, we refer to Geometry Aware Passthrough with smooth depth as \GAP since it achieves the best spatial reprojection error for both eyes and Direct Passthrough as \DP.

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\linewidth]{images/WarpingMetric.pdf}
  \caption{\textbf{Warping Errors.} We consider planar targets with known hand-crafted textures to quantify warping artifacts in the passthrough images. Specifically, we take the passthrough reprojected image and obtain the crop around the reprojected texture of interest using ArUco \cite{aruco} marker detectors (see left tile). Then we obtain the correspondences between the known texture and the reprojected texture using ML-based keypoint matching techniques \cite{sarlin20superglue} (a). We then use a homography solver to find the residual errors which measures the deviation of the reprojected image from a planar surface. This residual error can be plotted on the image (b) indicating pixel locations where the warping is observed. We also observe that \depthpassthrough passthrough has a higher warping error than \directpassthrough passthrough (c).}
  \Description{The figure demonstrates how warping errors are calculated. A reference image as well as passthrough image with its corresponding cropped image are shown. We visualize the keypoints matched between the reference and cropped image. We also show a bar plot of the residual error. The y-axis represents frequency (number of points) and the x-axis represents residual error (pixels). The frequency for DP is high at a residual error of 1 pixel. The frequency for GAP is spread over the residual error range of 1 to 4 pixels.}
  \label{fig:warping_metric}
\end{figure*}

\subsubsection{Warping Evaluation}

While \GAP improves the perceived location of objects in the scene (as shown in Figure \ref{fig:passthroughmodes}), the errors in geometry estimation introduce warping artifacts. This manifests as bending artifacts where rigid objects can appear to deform, like jelly, altering their perceived shape. This issue becomes immediately noticeable to users when using GAP (more in Section 4 and see). Therefore, we propose a simple method to quantify these effects for any black-box system by directly using the synthesized imagery. While spatial reprojection metric uses depth estimates to compute reprojection errors, this metric uses the final image output by the passthrough device to quantify stretching on objects of know shape and texture. Specifically, we choose planar targets with hand crafted textures and compute correspondences between known and reprojected textures. For estimating correspondences, we employ SuperGlue \cite{sarlin20superglue} which uses deep neural networks to extract features and graph-based algorithms to match them across images. Then, we use RANSAC to estimate a homography between these correspondences as they represent a planar target. \edits{The accuracy of this formulation highly depends on the quality of keypoint matching and stability of homography estimation. To ensure high quality matching, we utilize the state of the art ML-based method for detecting keypoints. To stabilize the homography estimation, we adhere to two principles: obtaining a large number of matches and ensuring the matches are of high quality. To achieve this, we used a custom-designed target with substantial texture variation and applied a confidence threshold of 0.3 to obtain at least 100 high-quality matches, enabling stable homography estimation. Additionally, to further reduce noise, we averaged the residuals over 45 frames to calculate the final metric.} We finally report the residual error from the resulting fit output by the RANSAC algorithm (see Figure \ref{fig:warping_metric}).

\input{warping_metrics}


As shown in Table \ref{tab:warping_metrics}, we indeed observe that \GAP introduces significantly more warping compared to \DP. We did not find meaningful reduction in these artifacts on the pasted textures through different depth estimates (raw, smooth and oversmooth). While the proposed metric accurately quantifies bending introduced by GAP, we believe that counteracting and evaluating these issues require \edits{more complex models than just homography on planar targets and} more complex metrics that can capture not only for static images, but also their temporal nature.

\edits{\textbf{Impact of warping artifacts on user experience:} Through discussions with passthrough users, we identified that warping artifacts substantially affect their preferences and overall experience. We note that an important consideration of our approach is applicability to any passthrough system without requiring access to the internal state of the system. The subjective responses from participants, discussed in Subsection \ref{subsec:qualfeedback}, provide insight into the effect of warping artifacts on user experience and comfort.}

% Itâ€™s equally important to measure the warping artifacts induced in the Depth Passthrough approach, but directly measuring the depth errors is not enough to capture warping effects. Since generalizing to unknown scenes with unknown geometry is hard, we propose a simplification and estimate warping in the reprojected imagery on objects of known geometry and texture. We specifically choose planar targets with hand crafted textures and compute correspondences between known and reprojected textures. These correspondences are then validated to be in geometrical agreement to a planar surface. The residual errors (measured in pixels), if any, indicate the induced warping.