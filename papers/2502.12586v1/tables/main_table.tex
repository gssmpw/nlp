\begin{table*}[t]
\centering
\caption{Overall comparison in terms of Explainability and Stability. Superscripts ``P'', ``R'', and ``F1'' denote Precision, Recall, and F1-Score, respectively. The subscript ``std'' indicates the standard deviation of each metric. \textbf{Bold} indicates the best results, while \underline{underlined} denotes the second-best. ``7B'' and ``8B'' denote LLaMA 2-7B and LLaMA 3-8B, respectively.}
\label{tab:overall}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{l|ccccccc|cccccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multicolumn{7}{c|}{\textbf{Explainability $\uparrow$}} & \multicolumn{6}{c}{\textbf{Stability $\downarrow$}} \\
\cmidrule(lr){2-8} \cmidrule(lr){9-14}
& GPT${_\text{score}}$ & BERT${_\text{score}^\text{P}}$ & BERT${_\text{score}^\text{R}}$ & BERT${_\text{score}^\text{F1}}$ & BART${_\text{score}}$ & BLEURT & USR & GPT\textsubscript{std} & BERT${_\text{std}^\text{P}}$ & BERT${_\text{std}^\text{R}}$ & BERT${_\text{std}^\text{F1}}$ & BART\textsubscript{std} & BLEURT\textsubscript{std} \\
\midrule

\multicolumn{14}{c}{\textbf{Amazon-books}} \\
\midrule
NRT & 75.63 & 0.3444 & 0.3440 & 0.3443 & -3.9806 & -0.4073 & 0.5413 & 12.82 & 0.1804 & 0.1035 & 0.1321 & 0.5101 & 0.3104 \\
Att2Seq & 76.08 & 0.3746 & 0.3624 & 0.3687 & -3.9440 & -0.3302 & 0.7757 & 12.56 & 0.1691 & 0.1051 & 0.1275 & 0.5080 & 0.2990 \\
PETER & 77.65 & 0.\textbf{4279} & 0.3799 & 0.4043 & -3.8968 & -0.2937 & 0.8480 & 11.21 & 0.1334 & 0.1035 & 0.1098 & 0.5144 & 0.2667 \\
PEPLER & 78.77 & 0.3506 & 0.3569 & 0.3543 & -3.9142 & -0.2950 & 0.9563 & 11.38 & 0.1105 & 0.0935 & 0.0893 & 0.5064 & 0.2195 \\
% XRec (w/o $\mathcal{P}$) & 81.77 & \underline{0.4194} & 0.4004 & 0.4106 & -3.8218 & -0.1294 & \underline{\textbf{1.0000}} & 9.60 & \textbf{0.0819} & 0.0955 & \textbf{0.0786} & 0.4799 & \underline{0.1803} \\
XRec & 82.57 & \underline{0.4193} & 0.4038 & 0.4122 & -3.8035 & \textbf{-0.1061} & \underline{\textbf{1.0000}} & 9.60 & \textbf{0.0836} & 0.0920 & \textbf{0.0800} & 0.4832 & \textbf{0.1780} \\
\rowcolor{gray!10} \textbf{\model (7B)} & \underline{82.70} & 0.4076 & \underline{0.4476}  & \underline{0.4282} & \underline{-3.3358} & -0.1246 & \underline{\textbf{1.0000}} & \underline{9.04} & \underline{0.0937}  & \textbf{0.0845} & \underline{0.0820} & \underline{0.4009} & \underline{0.1893}  \\
\rowcolor{gray!10}\textbf{\model (8B)} & \textbf{82.82} & 0.4073 & \textbf{0.4494} \textcolor{red}{\scriptsize{\textbf{(+4.56\%)}}}  & \textbf{0.4289} \textcolor{red}{\scriptsize{\textbf{(+1.67\%)}}}  & \textbf{-3.3110} & \underline{-0.1203} & \underline{\textbf{1.0000}} & \textbf{8.95} & 0.0945  &  \underline{0.0855} & 0.0825 & \textbf{0.3983} & 0.1912  \\
\midrule

\multicolumn{14}{c}{\textbf{Yelp}} \\
\midrule
NRT & 61.94 & 0.0795 & 0.2225 & 0.1495 & -4.6142 & -0.7913 & 0.2677 & 16.81 & 0.2293 & 0.1134 & 0.1581 & 0.5612 & 0.2728 \\
Att2Seq & 63.91 & 0.2099 & 0.2658 & 0.2379 & -4.5316 & -0.6707 & 0.7583 & 15.62 & 0.1583 & 0.1074 & 0.1147 & 0.5616 & 0.2470 \\
PETER & 67.00 & 0.2102 & 0.2983 & 0.2513 & -4.4100 & -0.5816 & 0.8750 & 15.57 & 0.3315 & 0.1298 & 0.2230 & 0.5800 & 0.3555 \\
PEPLER & 67.54 & 0.2920 & 0.3183 & 0.3052 & -4.4563 & -0.3354 & 0.9143 & 14.18 & 0.1476 & 0.1044 & 0.1050 & 0.5777 & 0.2524 \\
% XRec (w/o $\mathcal{P}$) & 71.81 & \underline{0.3879} & 0.3427 & 0.3657 & -4.4035 & -0.2486 & \underline{\textbf{1.0000}} & 12.71 & 0.1087 & 0.1072 & 0.0919 & 0.5717 & 0.2272 \\
XRec & 74.53 & \textbf{0.3946} & 0.3506 & 0.3730 & -4.3911 & -0.2287 & \underline{\textbf{1.0000}} & 11.45 & \textbf{0.0969} & 0.1048 & \textbf{0.0852}& 0.5770 & 0.2322 \\
\rowcolor{gray!10}\textbf{\model (7B)} & \underline{74.91} & 0.3573 &  \underline{0.4264} & \underline{0.3922} & \underline{-3.7729} & \underline{-0.1451} & \underline{\textbf{1.0000}} & \underline{10.88} & \underline{0.1050} & \textbf{0.0952} & \underline{0.0862} & \underline{0.4815} & \underline{0.2197}  \\
\rowcolor{gray!10} \textbf{\model (8B)} & \textbf{75.16} & \underline{0.3629} & \textbf{0.4373} \textcolor{red}{\scriptsize{\textbf{(+8.67\%)}}}  & \textbf{0.4003} \textcolor{red}{\scriptsize{\textbf{(+2.73\%)}}} & \textbf{-3.6448} & -0.1336 & \underline{\textbf{1.0000}} & \textbf{10.76} & 0.1068 & \underline{0.0995} & 0.0885 & \textbf{0.4743 }& \textbf{0.2182}  \\
\midrule

\multicolumn{14}{c}{\textbf{Google-reviews}} \\
\midrule
NRT & 58.27 & 0.3509 & 0.3495 & 0.3496 & -4.2915 & -0.4838 & 0.2533 & 19.16 & 0.2176 & 0.1267 & 0.1571 & 0.6620 & 0.3118 \\
Att2Seq & 61.31 & 0.3619 & 0.3653 & 0.3636 & -4.2627 & -0.4671 & 0.5070 & 17.47 & 0.1855 & 0.1247 & 0.1403 & 0.6663 & 0.3198 \\
PETER & 65.16 & 0.3892 & 0.3905 & 0.3881 & -4.1527 & -0.3375 & 0.4757 & 17.00 & 0.2819 & 0.1356 & 0.2005 & 0.6701 & 0.3272 \\
PEPLER & 61.58 & 0.3373 & 0.3711 & 0.3546 & -4.1744 & -0.2892 & 0.8660 & 17.17 & \underline{0.1134} & 0.1161 & 0.0999 & 0.6752 & 0.2484 \\
% XRec (w/o $\mathcal{P}$) & 69.71 & \underline{0.4427} & 0.4187 & 0.4310 & -4.1142 & -0.2026 & 0.9997 & 14.09 & 0.1180 & 0.1171 & 0.1034 & 0.6465 & \underline{0.2439} \\
XRec & 69.12 & \textbf{0.4546} & 0.4069 & 0.4311 & -4.1647 & -0.2437 & 0.9993 & 14.24 & \textbf{0.0972} & 0.1163 & 0.0938 & 0.6591 & \underline{0.2452} \\
\rowcolor{gray!10} \textbf{\model (7B)} & \underline{71.47} & \underline{0.4253} & \underline{0.4873}  & \underline{0.4566} & \underline{-3.3857}  & \underline{-0.1561} & \underline{\textbf{1.0000}} & \underline{13.46} & 0.1184 & \textbf{0.0872} & \underline{0.0921} &\textbf{0.4739}& \textbf{0.2415}   \\
\rowcolor{gray!10} \textbf{\model (8B)} & \textbf{71.73} & 0.4245 & \textbf{0.4935} \textcolor{red}{\scriptsize{\textbf{(+7.48\%)}}}  & \textbf{0.4592} \textcolor{red}{\scriptsize{\textbf{(+2.81\%)}}} & \textbf{-3.3235} & \textbf{-0.1518}	 & \underline{\textbf{1.0000}} & \textbf{13.23}  & 0.1175 & \underline{0.0920} & \textbf{0.0916} & \underline{0.4761} & 0.2511  \\
\bottomrule
\end{tabular}
}
\end{table*}