\section{Related Works}

\subsection{Explainable Recommendation}
Explainable recommendation~\cite{zhang2020explainable,zhang2014explicit,peake2018explanation,chang2023knowledge,chang2024path,wang2024heterophilic} has demonstrated significant advantages in informing users about the logic behind recommendations, thereby increasing system transparency, effectiveness, and trustworthiness. 
Early works focus on generating explanations with predefined templates~\cite{li2021caesar} or extracting logic reasoning rules from recommendation models~\cite{shi2020neural,chen2021neural,zhu2021faithfully}. To provide more detailed and personalized explanations, recent works have explored generating explanations from graph structure, which contains abundant CF information for providing explanations~\cite{xian2019reinforcement,fu2020fairness,balloccu2023faithful}. For example, PGPR \cite{xian2019reinforcement} proposes a reinforcement learning-based method to find a path in the graph to explain the recommendation. However, the graph structure is often complex and hard for users to understand. To address this issue, some works have explored generating explanations in natural language~\cite{li2017neural,dong2017learning,chen2019co,li2021personalized,chen2021generate,li2023personalized,cheng2023explainable,ma2024xrec}. With the advance of LLMs, like ChatGPT \cite{chatgpt}, they can generate more fluent and informative explanations based on user's and item's profile \cite{li2023personalized}. 
Recently, researchers have tried to combine the advantages of graphs to enhance the explanation generated by LLMs \cite{qiu2024unveiling}. XRec \cite{ma2024xrec} adopts a graph neural network (GNN) to model the graph structure and generate embeddings. Then, the embeddings are fed into LLMs to generate explanations. This approach allows LLMs to produce more informative explanations by considering CF information within the graph structure. 
However, XRec represents CF information as hidden embeddings, leaving it unclear what specific CF information is considered when generating explanations. This ambiguity makes it difficult to verify the explanations generated by LLMs, which suffer from severe hallucination issues~\cite{zhang2023siren} and further diminishes the trustworthiness of the explainable recommendation.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{figures/efficiency.pdf}}
    \vspace{-6mm}
    \caption{Efficiency Analysis of \model.} 
    \label{fig:efficient}
    \vspace{-3mm}
\end{figure}


\subsection{Graph Retrieval-Augmented Generation}
Although retrieval-augmented generation (RAG) has been widely used to enhance the LLMs by incorporating external knowledge~\cite{gao2023retrieval,luollm_kg2024}, it has shown limitations in considering the graph structure. To address this issue, graph retrieval-augmented generation (GraphRAG) has been proposed to effectively incorporate graph information into the LLM generation process~\cite{peng2024graph,wang2024contrastive,wang2024large}. The GraphRAG typically involves two processes: \emph{graph retrieval} and \emph{graph-augmented generation}. In graph retrieval, we need to design a retrieval model to retrieve relevant graph information for the given query, which can be some simple non-parametric retrievers like $L$-hop neighbors. To better consider the graph structure, GNN-based \cite{gu2020implicit,chang2020restricted,li2022semi} retrievers have been proposed. For example, GNN-RAG \cite{mavromatis2024gnn} first encodes the graph, assigns a score to each entity, and retrieves entities relevant to the query. Instead of GNNs, RoG \cite{luoreasoning} proposes an LLM-based graph retriever by planning, retrieving, and reasoning on graphs. In the graph-augmented generation, the retrieved graph information is used to enhance the generation process of LLMs. Due to the unstructured nature of the LLMs, some works introduce an adapter layer to bridge the modality gap. GraphLLM \cite{chai2023graphllm} first encodes the graph as embeddings and then feeds them into LLMs with a graph adapter layer. However, the hidden embeddings used by these methods are difficult to interpret and understand. To improve the interpretability, recent research adopts some graph description language to translate the graph structure into natural language, such as edge list, node list, and syntax tree \cite{kim2023kg,zhao2023graphtext,luo2023chatrule}. In this way, the graph structure can be understood by both LLMs and humans. Thus, we adopt a simple prompt to describe the graph structure for the explainable recommendation.