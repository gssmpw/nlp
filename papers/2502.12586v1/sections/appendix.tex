\section{More Experiment Details}

\subsection{Details of Datasets}
\label{app:datasets}

\input{tables/datasets}

\begin{itemize}[leftmargin=*]
    \item \textbf{Amazon-books}~\cite{ni2019justifying} is a subset of the Amazon review dataset, specifically focused on book recommendations. This dataset encompasses user-book interactions, including both numerical ratings and textual reviews submitted by users after their purchases. 
    % It provides a rich source of information for analyzing user preferences and book characteristics in the context of recommendation systems.
    \item \textbf{Yelp} \cite{ma2024xrec} is a widely used dataset derived from Yelp\footnote{https://www.yelp.com/dataset/}, where local businesses such as restaurants and bars are considered items. It provides rich information about local businesses, including user reviews and ratings across various categories.
    \item \textbf{Google-reviews}~\cite{li2022uctopic,yan2023personalized} comprises restaurant reviews collected from Google Local\footnote{https://www.google.com/maps}. 
    This dataset incorporates both business metadata and user feedback, offering a broad perspective on dining establishments globally.
\end{itemize}

\subsection{Details of Metrics}
\label{app:metrics}

\begin{itemize}[leftmargin=*]
    \item \textbf{GPT\textsubscript{score}} \cite{wang2023chatgpt} leverages large language models to evaluate text quality, providing a context-aware assessment.
    
    \item \textbf{BERT\textsubscript{score}} \cite{zhang2019bertscore} computes the similarity between reference and generated texts using contextual embeddings from BERT. Given a reference sentence $x = \langle x_1, x_2, ..., x_n \rangle$ and a generated sentence $\hat{x} = \langle\hat{x}_1, \hat{x}_2, ..., \hat{x}_m\rangle$, A sequence of word embeddings are first generated using BERT:
    \begin{equation}
    \begin{split}
        & \text{BERT}(\langle x_1, x_2, ..., x_n \rangle) = \langle \mathbf{x_1}, \mathbf{x_2}, ..., \mathbf{x_n} \rangle \\
        & \text{BERT}(\langle \hat{x}_1, \hat{x}_2, ..., \hat{x}_m \rangle) = \langle \mathbf{\hat{x}_1}, \mathbf{\hat{x}_2}, ..., \mathbf{\hat{x}_m} \rangle
    \end{split}
    \end{equation}
    The similarity between two individual embeddings ($\mathbf{x_i}$, $\mathbf{\hat{x}_j}$) is measured using cosine similarity, which simply reduces to $\mathbf{x}_{\mathbf{i}}^{\top}\hat{\mathbf{x}}_{\mathbf{j}}$ since both embeddings are pre-normalized. With these definitions, the Precision, Recall, and F1-score are calculated as follows:
    \begin{equation}
    \text{BERT}{_\text{score}^\text{P}}=\frac{1}{|\hat{x}|} \sum_{\hat{\mathbf{x}}_{\mathbf{j}} \in \hat{x}} \underbrace{\max _{\mathbf{x}_{\mathbf{i}} \in x} \overbrace{\mathbf{x}_{\mathbf{i}}^{\top} \hat{\mathbf{x}}_{\mathbf{j}}}^{\text {cosine similarity }}}_{\text {greedy matching }}
    \end{equation}
    \begin{equation}
    \text{BERT}{_\text{score}^\text{R}}=\frac{1}{|x|} \sum_{\mathbf{x}_{\mathbf{i}} \in x} \underbrace{\max _{\hat{\mathbf{x}}_{\mathrm{j}} \in \hat{x}} \overbrace{\mathbf{x}_{\mathbf{i}}^{\top} \hat{\mathbf{x}}_{\mathbf{j}}}^{\text {cosine similarity }}}_{\text {greedy matching }}
    \end{equation}
    \begin{equation}
    \text{BERT}{_\text{score}^\text{F1}} = 2 \times \frac{\text{BERT}{_\text{score}^\text{P}} \times \text{BERT}{_\text{score}^\text{R}}}{\text{BERT}{_\text{score}^\text{P}} + \text{BERT}{_\text{score}^\text{R}}}
    \end{equation}
    \item \textbf{BART\textsubscript{score}} \cite{yuan2021bartscore} conceptualizes the evaluation as a text generation task, assigning scores based on the probability of regenerating reference texts using the BART model.

    \item \textbf{BLEURT} \cite{sellam2020bleurt} employs a novel language model pre-trained with synthetic data to assess the similarity between the generated and reference texts.

    \item \textbf{USR} \cite{li2021personalized} assesses the uniqueness of generated explanations by calculating the ratio of unique sentences to total sentences.
\end{itemize}

\subsection{Details of Baselines}
\label{app:baselines}

\begin{itemize}[leftmargin=*]
    \item \textbf{NRT} \cite{li2017neural} employs multi-task learning to predict ratings and generate tips for recommendations simultaneously based on user and item IDs. The generation component is a GRU.
    \item \textbf{Att2Seq} \cite{dong2017learning} implements an attention-based attribute-to-sequence model that generates reviews by leveraging attribute information. The generation component is a two-layer LSTM.
    \item \textbf{PETER} \cite{li2021personalized} is a personalized Transformer model that maps user and item IDs to generated explanations. It bridges IDs and words through a "context prediction" task. PETER is used instead of PETER+ due to the absence of word features in the datasets.
    \item \textbf{PEPLER} \cite{li2023personalized} proposes sequential tuning and recommendation as regularization strategies to bridge the gap between prompts (incorporating user and item ID vectors) and the pre-trained transformer model for generating explanations.
    \item \textbf{XRec} \cite{ma2024xrec} utilizes the encoded user/item embeddings from GNNs as implicit collaborative signals, which are then integrated into each layer of LLMs, enabling the generation of explanations.
\end{itemize}


\section{Case Study}
\label{app:case}

\input{tables/cases}

We present two cases in Table \ref{tab:case1} and Table \ref{tab:case2} to demonstrate the effectiveness of \model and illustrate how retrieved CF signals benefit the generated explanations. All retrieved knowledge is presented as human-readable text after graph translation for better user understanding. We also provide the ground truth explanations and explanations generated by XRec \cite{ma2024xrec} for comparison.

From Table \ref{tab:case1}, we can observe that the user profile contains no explicit drinking interests. However, node-level CF signals reveal that other users who have similar interests to the current user show a preference for drinking. This allows \model to infer that the current user might appreciate this restaurant specializing in ``crafted beers''. In contrast, XRec's explanation is notably generic, which relies on common phrases like ``delicious food'' and ``great service'', missing the key recommendation reasons, thus leading to a low Recall score. Table \ref{tab:case2} provides another case that leverages path-level retrieved CF information for more comprehensive explanations. While the user profile only indicates general preferences for comfort food and efficient services, path-level CF signals uncover broader connections between the current user and other users/items. Users along the retrieved path have an interest in ``frozen yogurt'', allowing us to infer that the current user might be interested in this restaurant due to its frozen yogurt offerings. In contrast, XRec incorrectly explains that the user might choose this restaurant for its Thai food, failing to capture the user's true interests. These cases show that node-level and path-level CF signals are complementary. While not equally effective in every situation, gathered CF signals can contribute to generating more accurate explanations in recommendation scenarios. The better explanations demonstrate \model has the ability to effectively retrieve, understand, and utilize such knowledge. 
