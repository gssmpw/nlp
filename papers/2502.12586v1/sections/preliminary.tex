\section{Preliminaries}

\subsection{Explainable Recommendation}
The explainable recommendation aims to unveil the rationale behind the recommendation results and provide human-understandable textual explanations.
%
In recommendation scenarios, the user-item interactions can be represented by a bipartite graph $\mathcal{G}=\{(u,i)|u\in\mathcal{U},i\in\mathcal{I}\}$, where $\mathcal{U}$ and $\mathcal{I}$ are the sets of users and items, respectively.
Each node in $\mathcal{G}$ is associated with textual information from either the user profile set $\mathcal{B}$ or the item profile set $\mathcal{C}$. 
Specifically, each user $u$ has a profile $b_u \in \mathcal{B}$ describing their preferences, and each item $i$ has a profile $c_i \in \mathcal{C}$ detailing attributes and descriptions. 

%
Given a recommendation $(u, i)$, which is a pair of user $u$ and recommended item $i$, the goal of an explainable recommendation is to generate a clear textual explanation, which is formulated as:
% \noindent \textbf{Explainable Recommendation.} 
% For a given user-item pair $(u, i)$, we aim to generate clear textual explanations that allow us to understand the rationale behind each recommendation. This process can be formulated as:
\begin{equation}
    explanation(u, i) = \texttt{generator}(u,i,\mathcal{G}, \mathcal{B}, \mathcal{C}).
\end{equation}
The graph models abundant structural CF information beneficial for explainable recommendations. Given the size of the entire graph, we commonly utilize the subgraph $\mathcal{G}{(u,i)} \subseteq \mathcal{G}$ as input~\cite{ying2018graph,zhang2023page}.
This subgraph represents the $L$-hop edge-centered ego-graph around the pair $(u, i)$.  It includes nodes that are at most $L$ hops away from either $u$ or $i$, effectively capturing the local connectivity structure for generating explanations.

% \begin{table}[th]
% \caption{Notations and meanings. }
% \label{tab:notation}
% {
% \begin{tabular}{ll}
%   \hline
%   {\bf Notation} & {\bf Definition and description}\\
%   \hline
%   $\mathcal{G}$, $\mathcal{P}$ & User-item bipartite graph and profile set \\
%   \hline
%   $\mathcal{U}$, $\mathcal{I}$ & User set and item set \\
%   \hline
%   $\mathcal{P}_u$, $\mathcal{P}_i$ & Profile of node $u$ and node $i$ \\
%   \hline
% \end{tabular}
% }
% \end{table}

% key is 协同过滤信息

\subsection{Graph Retrieval-Augmented Generation}
Retrieval-Augmented Generation (RAG) has achieved remarkable success in enhancing Large Language Models (LLMs) by retrieving external documents. However, existing RAGs fail to fully exploit the structural information in the graph to augment LLMs. The graph retrieval-augmented generation (GraphRAG), to address this issue, leverages structural information to enable more precise retrieval and facilitate context-aware generation \cite{peng2024graph}. Given a query $q$, GraphRAG aims to retrieve structural information from a graph $\mathcal{G}$ and generate responses $a$ conditioned by the retrieved information, which can be formulated as:
\begin{gather}
    G = \text{G-retriever}(q, \mathcal{G}), \\
    P(a|q,\mathcal{G}) = P(a|q,G,\theta),
\end{gather}   
where $\theta$ is the parameters of the LLMs, and $G$ is the structural information (e.g., nodes, paths, and subgraphs) retrieved by the graph retriever. 

% Large language models (LLMs) have demonstrated surprising reasoning ability via next-token prediction, which models probability distributions over sequences of tokens $\{x_1,...,x_n\}$:
% \begin{equation}
%       p(x_1,...,x_j) = \prod_{i=1}^n p(x_i|x_{<j}; \theta),
% \end{equation}
% where $x_{<j}:=\{x_1,...,x_{j-1}\}$ is the sequence of tokens before $x_j$, also known as its prefix. This autoregressive model is usually implemented via a learned transformer network \cite{vaswani2017attention} parameterized by $\theta$, which models the conditional probabilities $p_{\theta}(x_j|x_{<j})$ for $j\in[n]:=\{1, ..., n\}$ by employing a causal self-attention mask \cite{radford2018improving}. 

% To further enhance the factual knowledge and tailor the LLM outputs, \textit{Retrieval-augmented language modeling} \LH{Why not use the term RAG?} introduces an additional operation that retrieves one or more documents (could be other forms of text such as sentences and triplets) from an external corpus $\mathcal{C}$, and conditions the LLM predictions on these documents. Specifically, the retrieval for predicting $x_j$ is denoted as $\mathcal{R}_\mathcal{C}(x_{<j})$, where $x_{<j}$ represents the prefix. Drawing inspiration from in-context learning \cite{radford2021learning,dong2022survey}, a common approach simply prepends the retrieved documents to the input prior to the input sequence, which does not require modifying the LLM weights. The general formulation for retrieval-augmented language modeling is shown as follows:
% \begin{equation}
%  p(x_1,...,x_n) = 
%  \prod_{j=1}^n p(x_j|\left[\mathcal{R}_\mathcal{C}(x_{<i}) \oplus x_{<j}\right]; \theta),
% \end{equation}

% \noindent where $\left[a \oplus b\right]$ denotes the concatenation of strings $a$ and $b$. In this paper, we use the user-item bipartite graph as the external corpus. Given a user-item pair, we retrieve relevant paths and nodes from this graph to instruct LLM to generate explanations. \LH{I still not fully understand the purpose of introdcing RAG in the preliminary.}

% LLM 本身具有推理能力  通过retrieve

