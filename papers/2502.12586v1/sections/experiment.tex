\section{Experiments}

\input{tables/main_table}

We evaluate our model on real-world datasets to assess its performance in enhancing explainable recommendations. In particular, we aim to answer the following research questions: \textbf{Q1}: How effective is \model compared with state-of-the-art models? \textbf{Q2}: How do the main components of our model impact the performance? \textbf{Q3}: What is the impact of hyper-parameters? \textbf{Q4}: How does \model's training efficiency compare to state-of-the-art methods?  

% \textbf{Q3}: What is the impact of hyper-parameters such as pruning ratios and number of retrieved items on performance?


\subsection{Experimental Protocols}

\subsubsection{Datasets} We evaluate \model and leverage three prominent public datasets that offer distinct perspectives on user-item interactions, including \textbf{Amazon-books}~\cite{ni2019justifying}, \textbf{Yelp} \cite{ma2024xrec}, and \textbf{Google-reviews}~\cite{li2022uctopic,yan2023personalized}. Table \ref{tab:datasets} lists the statistics of three datasets. More details of datasets can be found in Appendix \ref{app:datasets}.


\subsubsection{Metrics} We follow XRec \cite{ma2024xrec} to utilize a suite of metrics aimed at assessing the semantic explainability and stability of the generated explanations. Traditional n-gram-based metrics like BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge} are not adequate for this purpose due to their inability to fully capture semantic meaning. Specifically, we use \textbf{GPT}\textsubscript{score} \cite{wang2023chatgpt}, \textbf{BERT}\textsubscript{score} \cite{zhang2019bertscore}, \textbf{BART}\textsubscript{score} \cite{yuan2021bartscore}, \textbf{BLEURT} \cite{sellam2020bleurt}, and \textbf{USR} \cite{li2021personalized} to measure the explainability. Notably, BERT\textsubscript{score} comprises the Recall score, which measures the completeness and quality of the retrieved CF information, allowing us to evaluate the performance of our graph retriever. More details of the used metrics can be found in Appendix \ref{app:metrics}. To evaluate quality consistency, we also report the standard deviations of these metrics. Lower standard deviation values indicate more stable performance.


\subsubsection{Baselines} We introduce five state-of-the-art baselines, including \textbf{NRT} \cite{li2017neural}, \textbf{Att2Seq} \cite{dong2017learning}, \textbf{PETER} \cite{li2021personalized}, \textbf{PEPLER} \cite{li2023personalized}, and \textbf{XRec} \cite{ma2024xrec}. These models are based on representative language models, such as GRU, LSTM, GPT, and LLaMA. More details of the compared baselines can be found in Appendix \ref{app:baselines}.

\subsubsection{Implementation Details} 

For path-level retrieval, we set the node embedding dimension to $128$, the maximum retrieved path length to $5$, and $m$ is set to $2$ for m-core pruning. We initialize the encoders in both the node-level retriever and the re-ranking mechanism with SentenceBERT \cite{reimers2019sentence}. The number of retrieved paths, as well as the number of retrieved nodes (including users and items), is set to $2$, and the pruning ratio $t$ is set to $70$\% across all datasets. For retrieval-augmented fine-tuning, we use the models from the open-source LLaMA family. Specifically, for a fair comparison, we utilize the same model, i.e., LLaMA 2-7B adopted by baselines \cite{ma2024xrec}. We also report results based on the advanced LLaMA 3-8B model. We set the learning rate, epochs, and max length as 2e-5, $2$, and $2048$ for RAFT, which is trained on 8 NVIDIA A100 GPUs. The total batch sizes are set to $32$ and $16$, respectively, for the 7B and 8B models. The rank of the LoRA adapter is set to $8$. For inference, we set the temperature as $0$ and the maximum output tokens as $256$, ensuring a stable and reasonable generation. In addition, we employ the GPT-3.5-turbo model for computing the GPTScore metric.

\subsection{Model Performance (RQ1)}

\subsubsection{Overall Performance}

We first compare the quality of generated explanations against baselines across three datasets, the results are summarized in Table \ref{tab:overall}. It is observed that \model demonstrates superior performance in both explainability and stability, outperforming baselines across semantic evaluators such as GPT, BERT, and BART. Compared to the most powerful baseline, XRec \cite{ma2024xrec}, which uses GNNs to implicitly capture CF information, our model shows improvements in BERT${_\text{score}^\text{F1}}$ across three datasets, with increases of 1.67\%, 2.73\%, and 2.81\% respectively. This indicates that the explicit CF information retrieved by our hybrid graph retriever is more accurate and better utilizes both structures and semantics of user-item interaction graphs compared to the implicit CF information captured by XRec. In addition, graph translation enables the adapter-free RAFT, allowing LLMs to better comprehend the structural inputs and leverage their powerful capabilities to generate human-understandable explanations. Notably, \model significantly outperforms all baselines in BERT${_\text{score}^\text{R}}$, with increases of 4.56\%, 8.67\%, and 7.48\% across the three datasets, while slightly decreasing in BERT${_\text{score}^\text{P}}$. The retrieved CF information enables generated explanations to include more key information (e.g., user topic modeling and interaction history). Although introducing retrieved knowledge inevitably introduces some noise, reducing the precision slightly, we consider this trade-off beneficial, as the completeness of explanations is crucial for user understanding compared to accurate but less informative expressions. Finally, we notice that \model's performance improvement on Amazon is relatively modest compared to other datasets. This can be attributed to the sparsity of its user-item interaction graph, particularly in the test set where the average node degree is only $2.76$. In such cases, even with retrieval, the effectiveness of CF information from the graph is limited. 

% Consequently, obtaining relevant semantic and structural knowledge through the user's previous interactions becomes challenging.

% \subsubsection{Zero-shot Transferablity}

\subsubsection{Human Evaluation} The ultimate goal of model explanation is to aid human understanding and decision-making. Human evaluation is thus the best way to evaluate the effectiveness of an explainer, which has been widely used in previous works \cite{ribeiro2016should,ghazimatin2020prince}. We conduct a human evaluation by randomly selecting $20$ user-item pairs from the test set of each dataset and generating explanations for each sample using XRec \cite{ma2024xrec} and \model. We designed a survey with single-choice questions and distributed it to five senior researchers, asking them to select the best explanations. As shown in Figure~\ref{fig:human}, explanations generated by \model were consistently favored across all datasets, especially for Yelp and Google-reviews, where our explanations were chosen in over 80\% of cases.

\subsubsection{Case Study} To demonstrate the effectiveness of \model and show how retrieved CF signals benefit the generated explanations, we provide several cases and give analysis in Appendix \ref{app:case}.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{figures/human.pdf}}
    \vspace{-6mm}
    \caption{Human evaluation comparing XRec and \model.} 
    \label{fig:human}
    \vspace{-3mm}
\end{figure}

\input{tables/ablation}

\subsection{Ablation Study (RQ2)}
\label{subsec:ablation}

% To investigate how each component affects the model performance, we conduct the following ablation studies in Table \ref{tab:ablation}.

\subsubsection{The Effects of Hybrid Graph Retriever.} 

From Table \ref{tab:ablation} we can observe that both path-level and node-level retrievers contribute to the final results, with their combination yielding the best performance. We observe that semantic information is more crucial for Yelp, while structural knowledge offers greater advantages for Google-reviews. This suggests that the importance of retrievers may differ based on dataset characteristics and highlights the complementarity of the two retrievers. 

% Notably, \model can outperform XRec \cite{ma2024xrec} even without any CF information, demonstrating the robust performance of our model.


\subsubsection{The Effects of Various GNNs.}

The GNN encoder in our path-retriever captures structural CF information and can be replaced with any advanced GNN-based recommender model. We compare LightGCN \cite{he2020lightgcn}, which is one of the representative GNN models for recommendation, with R-GCN \cite{schlichtkrull2018modeling} in the ablation study. The results show that both of the GNNs achieve competitive performance. This demonstrates their effectiveness in capturing structural information for recommendation and the flexibility of our path-level retriever, which could adapt to different graphs and recommendation scenarios using different GNN architectures.

\subsubsection{The Effects of Various LLMs with Different Scales.} 

We investigate the impact of LLM scales on \model's performance by comparing the advanced Qwen 2.5 family \cite{yang2024qwen2} of different scales. Firstly, we observe a clear performance scaling from 0.5B to 3B models. Interestingly, the 3B model achieves comparable results to the 7B model, indicating that with the introduction of retrieved knowledge, even a relatively smaller LLM can achieve impressive results through RAFT. This also demonstrates the necessity of fine-tuning with retrieved knowledge, which can significantly enhance the model's ability to generate high-quality explanations.


\subsubsection{The Effects of Knowledge Pruning.}

Training with the full dataset increases the number of samples by several times but does not improve performance proportionally, and may even lead to slight degradation (e.g., on Yelp). This is because some user-items are self-explanatory, and additional CF information may introduce noise. Thus, knowledge pruning is essential for removing these samples from training and improving model performance.


\subsection{Hyperparameter Study (RQ3)}

We explore the variation of \model's performance with respect to the number of retrieved elements $k$, including both paths and nodes. Figure~\ref{fig:hyper_k} illustrates the impact of $k$ (ranging from 1 to 5) on BERT${_\text{score}}$ precision and BERT${_\text{score}}$ recall (abbr. precision and recall in the following). The best precision is achieved at $k$=2, after which it decreases, showing that more retrieved paths and nodes can introduce noise and deteriorate the performance. Conversely, a low $k$ (i.e., $k$=1) fails to provide sufficient CF information, leading to lower results. We also notice that recall is not sensitive to the increased $k$, suggesting that a modest $k$ can effectively retrieve essential information for generating explanations.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \includegraphics{figures/hyper_k.pdf}}
    \vspace{-7mm}
    \caption{Performance of different retrieved number $k$.} 
    \label{fig:hyper_k}
    \vspace{-4mm}
\end{figure}

% We explore the variation of \model's performance with respect to two hyperparameters: $k$ (the number of retrieved elements, including both paths and nodes) and $t$ (the knowledge pruning ratio).


% \subsubsection{Retrieved Number $k$}
% Figure~\ref{fig:hyper_k} illustrates the impact of $k$ (ranging from 1 to 5) on BERT${_\text{score}}$ precision and BERT${_\text{score}}$ recall (abbr. precision and recall in the following). The best precision is achieved at $k$=2, after which it decreases, showing that more retrieved paths and nodes can introduce noise and deteriorate the performance. Conversely, a low $k$ (i.e., $k$=1) fails to provide sufficient CF information, leading to lower results. We also notice that recall is not sensitive to the increased $k$, suggesting that a modest $k$ can effectively retrieve essential information for generating explanations.

% \subsubsection{Pruning Ratio $t$}

% In Table \ref{tab:hyper_t}, we report \model's explainability and stability on three datasets with $t$ ranging from 90\% to 50\%. In terms of explainability, both Amazon-books and Yelp perform best at $t=70\%$, while Google-reviews at $t=50\%$. This suggests that a low $t$ value, which corresponds to retaining more training samples, does not always lead to improved model performance, as irrelevant samples can introduce noise. However, it generally enhances model stability, indicating that fine-tuning with more data can enhance the model's ability to generate consistent explanations.

% \input{tables/hyper_ratio}

\subsection{Efficiency Analysis (RQ4)}

In Figure~\ref{fig:efficient}, we analyze the efficiency of our model, including the time taken to train one epoch, performance results, and the number of parameters that require tuning for G-Refer, XRec \cite{ma2024xrec}, and full-set training. For a fair comparison, all experiments were conducted on a single GPU with a batch size of 1. Though requiring more learnable parameters, G-Refer achieves faster training and better performance compared to XRec. This can be attributed to (1) our knowledge pruning which significantly reduces training data; (2) XRec's speed limitation due to adapter insertion. In addition, full-set training reduces efficiency without performance gains, indicating the necessity of knowledge pruning.
