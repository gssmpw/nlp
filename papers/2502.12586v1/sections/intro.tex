\section{Introduction}


Recommendation systems (RS) play an essential role in our daily lives, as they affect how people navigate the vast array of products available in online services~\cite{lu2015recommender,he2017neural,wu2024survey,zhao2023cross1,zhao2023cross, zhang2022knowledge, chen2024shopping}. These systems have the ability to predict whether a user is interested in an item, \textit{e.g.}, by clicking on it or making a purchase. Recently, explainable recommendation~\cite{zhang2020explainable,zhang2014explicit,wang2018tem,peake2018explanation} has attracted more attention and demonstrated significant advantages in informing users about the logic behind their received recommendation results, thereby increasing system transparency, effectiveness, and trustworthiness. Specifically, this task aims to generate human-understandable textual explanations for each user-item recommendation. This enables us to understand the underlying reasons behind each recommendation and develop accountable RS as a result~\cite{chen2022measuring}. 

Due to the impressive generative and reasoning capabilities of large language models (LLMs)~\cite{li2024graph,li2023survey,li2024glbench,tang2024grapharena,li2024zerog,chen2024graphwiz,li2023unlocking}, existing explainable recommendation methods \cite{li2021personalized,ma2024xrec,li2023personalized} can now produce fluent and informative explanations in natural language based on user and item profiles, as well as their interactions. To provide more personalized explanations, \cite{ma2024xrec} have utilized user-item interaction graphs, which contain abundant collaborative filtering (CF) information from graph structure. The CF information \cite{herlocker2000explaining} reveals the complex patterns between users and items, which can be used to generate more accurate and informative explanations. Nevertheless, the graph structure is inherently noisy and complex, making it challenging to effectively extract CF information for explanations.
% By incorporating the CF information extracted from graphs, these approaches can leverage both users' historical behaviors and high-order patterns to generate more accurate and informative explanations. 

% which have demonstrated impressive generative and reasoning capabilities, existing explainable recommendation methods \cite{li2021personalized,ma2024xrec,li2023personalized} are now capable of generating fluent and informative explanations in natural language based on user and item profiles as well as user-item interactions. In addition, recent works~\cite{ma2024xrec} have recognized that user-item interaction graphs are also valuable in this situation, as they contain abundant collaborative filtering (CF) information from their graph structure for providing more detailed and personalized explanations. By incorporating collaborative evidence, these approaches can leverage users' historical behaviors and high-order patterns to make the generations more accurate and informative. Consequently, effectively extracting and harnessing CF information from graphs becomes important for improving the quality of explanations for LLM-based RS.

In order to better model the CF information from graph structure in RS, graph neural networks (GNNs) have been widely adopted and demonstrated exceptional performance in recommendation tasks~\cite{cao2019unifying,he2020lightgcn,wu2022graph,yang2021dagnn,chen2022learning}. GNNs capture CF information by learning hidden representations of both users and items through an iterative process of feature transformation and information aggregation from neighboring nodes. While GNNs excel at capturing CF information for recommendations, simply incorporating GNNs with LLM-based RS such as XRec~\cite{ma2024xrec} still faces several challenges in producing satisfactory explanations for recommendations: \textbf{(C1) Implicit CF signal.} The CF signals are injected into LLMs by feeding user and item embeddings generated by GNNs, where the CF signals are represented as implicit node embeddings. Given the vagueness of the explainability of GNNs themselves~\cite{yuan2022explainability}, it is difficult to interpret the CF signals contained in these embeddings. Therefore, more explicit evidence constructed from the CF signals contained in graphs, especially favored from the view of human-understandable text, is eagerly needed.  \textbf{(C2) Modality gap.} Since GNNs are primarily used to capture structural information, the rich semantics within user and item profiles are inevitably ignored. In addition, LLMs struggle to directly understand the structured CF signals captured by GNNs, because of their unstructured nature. This results in a significant modality gap between structured representations and natural language, making it difficult for LLMs to effectively leverage graph-derived CF information when generating explanations.

% Current efforts rely on the structural information alone, but ignore the rich semantics within user and item nodes, to facilitate LLMs in generating explanations, which can be inaccurate due to the noisy and incomplete nature of user-item interaction graphs. Therefore, it is crucial to retrieve semantics from graphs to uncover latent connections and provide more comprehensive explanations for recommendations.

% \textbf{(2) Lack of diversity.} GNNs use fixed node embeddings to represent users and items, failing to adapt to specific user-item interactions. For example, a user who enjoys both basketball and classical music requires different CF signals to explain interactions with sports-related versus music-related items. The static embeddings provided by GNNs lack this context-aware diversity.

\paratitle{Presented Work.} Motivated by these challenges, in this work, we propose \underline{\model}, a framework designed to use \underline{G}raph \underline{Re}trieval-augmented LLMs \underline{f}or \underline{e}xplainable \underline{r}ecommendation. We aim to address the limitations of existing methods and extract explicit, diverse, and semantically rich CF information to generate more accurate and personalized explanations. As shown in Figure~\ref{fig:framework}, given a user-item recommendation to be explained, we leverage a hybrid graph retrieval mechanism that combines multi-granularity retrievers to extract explicit \textit{structural} and \textit{semantic} CF information from the graphs (to address \textbf{C1}). 
%
Specifically, we employ a path-level retriever to capture structural CF information by identifying $k$ paths from graphs that account for recommendation. To utilize the rich semantics within nodes, we also employ a node-level retriever to retrieve semantic CF information from the graphs by finding the most relevant nodes to the recommendation. The retrieved CF information is then translated into human-understandable text with the graph translation module to facilitate LLMs in generating explanations.
%
To bridge the modality gap and enhance the understanding of LLMs (to address \textbf{C2}), we adopt a lightweight retrieval-augmented fine-tuning (RAFT) approach to instruct LLMs in understanding the retrieved CF information and generating explanations. To further improve the training efficiency and reduce noise, we introduce a knowledge pruning technique to filter out training samples with less relevant CF information. After the training, LLMs exhibit a greater ability to leverage both the retrieved CF information and rich semantics in profiles to generate accurate and contextually relevant explanations for recommendations.

% unstructured text pertaining .  first, translation. then pruning + raft. 

Our main contributions can be summarized as follows: 
\begin{itemize}[leftmargin=*]
    \item \textbf{Comprehensive Analysis.} We identify the challenges in existing explainable recommendation works, 
    as GNNs struggle to capture explicit and semantically rich CF information.
    \item \textbf{Architecture Design.} We propose \model, a model leveraging hybrid graph retrieval to capture both structural and semantic CF signals from user-item interaction graphs. We also incorporate knowledge pruning and retrieval-augmented fine-tuning to enhance LLMs' ability to utilize retrieved knowledge.
    \item \textbf{Superior Performance.} Extensive experiments on public datasets demonstrate the effectiveness of our proposed \model, surpassing a series of SOTA baselines by up to 8.67\%.
\end{itemize}

\begin{figure*}[t]
    \centering
    \resizebox{1\linewidth}{!}{
    \includegraphics{figures/framework.pdf}}
    \vspace{-4mm}
    \caption{Our proposed pipeline \model facilitates explainable recommendation with three key components: (1) Hybrid Graph Retrieval employs multi-granularity retrievers to retrieve explicit CF signals and formulated as human-readable text by the Graph Translation; (2) Knowledge Pruning eliminates noise and improves training efficiency; and (3) Retrieval-augmented Fine-tuning instructs LLMs to leverage retrieved CF information in generating informative explanation. } 
    \label{fig:framework}
    \vspace{-3mm}
\end{figure*}
