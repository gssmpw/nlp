\section{Methodology}

Our proposed G-Refer is designed to excavate the CF information from the graph to enhance explainable recommendations. 
% Figure \ref{} illustrates the pipeline of our model, which can be divided into three parts. 
First, we leverage a hybrid graph retrieval mechanism that combines multi-granularity retrievers to extract both structural and semantic CF information from the user-item interaction graph. In the second part, we employ knowledge pruning to filter out less relevant or redundant training samples, eliminating noise and improving training efficiency. Finally, a lightweight retrieval-augmented fine-tuning is conducted to bridge the modality gap, instructing LLMs to understand retrieved knowledge and generate accurate and contextually relevant explanations.

\subsection{Hybrid Graph Retrieval}

% structure + semantic

The user-item interaction graph encapsulates a wealth of knowledge, including user behaviors, preferences, and relationships among users and between items \cite{wu2024survey}. It can provide sufficient CF information from different perspectives, such as \textbf{structural} and \textbf{semantic} views, to explain the user behaviors, thereby enhancing the explainability of recommendations. 
For example, as illustrated in Figure~\ref{fig:framework}, we can explain why user $u_1$ might be interested in item $i_2$ from both structural and semantic perspectives. 
From the structural perspective, the path ($u_1 \rightarrow i_1 \rightarrow u_2 \rightarrow i_2$) illustrates the potentially shared preferences between user $u_1$ and $u_2$, both of whom have watched $i_1$, \textit{Iron Man}, suggests that user $u_1$ may also enjoy  $i_2$, \textit{Doctor Strange},  which user $u_2$ has watched, due to the overlapping Marvel fandom.
In addition, from the semantic view, retrieving items $i_2$ and $i_3$ could also lead to an explanation. Both items feature the same leading actor, Benedict Cumberbatch. This commonality suggests that user $u_1$, having shown interest in related actors through their viewing habits, might find $i_3$, \textit{Sherlock}, appealing as well.



According to \cite{peng2024graph}, retrievers based on different levels of granularity in the graph (e.g., nodes, paths, and subgraphs) have unique strengths for addressing various aspects of retrieval scenarios. Inspired by recent works \cite{luoreasoning,gao2024two}, we adopt a hybrid graph retrieval strategy that employs both \textbf{path-level} and \textbf{node-level} retrievers to respectively retrieve structural and semantic CF information from the user-item interaction graph, which is composed of purchase histories with enriched text attributes. The CF information retrieved from both views is then translated into human-understandable text, facilitating the generation of explanations.


\subsubsection{Path-level Retriever} The path-level retriever aims to retrieve the structural CF information from the graphs by identifying the most $k$ informative paths that connect users and items for interpretable recommendations. Typically, recommendations on graphs can be formalized as a link prediction (LP) problem, which predicts the connections between user $u$ and $i$ based on the graph structure \cite{li2013recommendation}. This can be formulated as 
\begin{equation}
    i = f_\phi(u, \mathcal{G}),
\end{equation}
where $f_\phi$ denotes a link prediction model parameterized by $\phi$. The $f_\phi$ captures complex patterns between users and items in the graph to provide the recommendation results.




\begin{figure}[t]
    \centering
    \resizebox{1\columnwidth}{!}{
    \includegraphics{figures/pathR.pdf}}
    \vspace{-6mm}
    \caption{The illustration of path-level retriever.} 
    \label{fig:pathR}
    \vspace{-4mm}
\end{figure}



\paratitle{Motivation.} GNNs \cite{wu2020comprehensive,DBLP:conf/icde/000200O024,wu2023billion,li2024gslb,li2024rethinking} have demonstrated exceptional performance in capturing graph structures and are widely used in recommender systems \cite{he2020lightgcn,wu2022graph}. While GNNs excel at extracting CF information for recommendations, they are often criticized for their lack of interpretability. GNNs struggle to provide clear explanations for their predictions \cite{ying2019gnnexplainer}, which significantly undermines transparency. To address this issue, inspired by recent GNN explanation methods \cite{zhang2023page}, we propose a path-level retriever that generates explanations for GNN predictions and reveals the structural CF information captured by GNNs as interpretable paths, which is illustrated in Figure \ref{fig:pathR}.

\paratitle{GNN Training.} To capture the structural CF information, we first train a GNN to learn the user-item interactions in the graph for recommendations. We follow a standard two-step pipeline, which involves obtaining user and item representations with an $L$-hop GNN encoder and applying a prediction head to get a recommendation prediction. We use R-GCN \cite{schlichtkrull2018modeling} as the GNN encoder to learn node embeddings on user-item graphs, and an inner product is adopted as the prediction head. We also explore other GNN variants, i.e., LightGCN \cite{he2020lightgcn}, in our experiments (Section \ref{subsec:ablation}). The forward-pass update of each user $u$ in R-GCN is formalized as follows:
\begin{equation}
    \mathbf{h}_u^{(l)} = \text{ReLU} \left( \sum_{i^\prime \in \mathcal{N}_u} \frac{1}{|\mathcal{N}_u|} \mathbf{W}_1^{(l-1)} \mathbf{h}_{i^\prime}^{(l-1)} + \mathbf{W}_0^{(l-1)} \mathbf{h}_u^{(l-1)} \right), 
\end{equation}
\noindent where {\small$\mathbf{h}_u^{(l)}$} is the hidden state of user $u$ in the $l$-th layer and $\mathcal{N}_u$ denotes the neighbors of $u$. The {\small $\mathbf{W}_0^{(l-1)}$} and {\small$\mathbf{W}_1^{(l-1)}$} are learnable weight matrices at layer $l$-1. Likewise, the item representation {\small$\mathbf{h}_i^{(l)}$} can be obtained in a similar manner.

%and $R_v$ indicates the set of relation types associated with $v$, with $R_u$ = \{[$u \rightarrow i$]\} for users and $R_i$ = \{[$i \rightarrow u$]\} for items.

\paratitle{The $m$-core Pruning.} To ensure concise and accurate explanations, we aim to extract paths that are short in length and avoid containing high-degree nodes in the graph \cite{zhang2023page}. Given a prediction ($u$, $i$), the corresponding $L$-hop edge-centered ego-graph is initially extracted, which includes nodes that are at most $L$ hops away from either $u$ or $i$. A $m$-core pruning process is then applied to remove spurious neighbors and improve speed. Specifically, the $m$-core pruning is a recursive algorithm that eliminates nodes with degrees less than $m$, until only those with degrees $\geq m$ remain, forming the $m$-core. 

\paratitle{Explanation Path Retrieval.} We leverage PaGE-Link \cite{zhang2023page} to perform mask learning on the learned GNN model to identify explanation paths from the user-item interaction graph via learning a mask over all edges, assigning high weights to important edges and low weights to others. Based on the mask, we apply the Dijkstra's shortest path algorithm \cite{dijkstra2022note} to retrieve explanation paths.

 Formally, let $\mathcal{E}$ denote all user-item interactions, $\tau(e)$ be the type of the edge $e$, and $\mathcal{M}$ be learnable masks of all edge types. To optimize $\mathcal{M}$, the objective function \( \mathcal{L} \) that needs to be minimized is composed of two loss terms, which can be formulated as follows:
\begin{equation}
    \mathcal{L}(\mathcal{M}) = \mathcal{L}_{pred}(\mathcal{M}) + \mathcal{L}_{path}(\mathcal{M}).
\end{equation}

\noindent The first loss term $\mathcal{L}_{pred}$ is to learn to select crucial edges for model prediction based on the perturbation-based explanation. Given a connected pair ($u$, $i$), the loss can be defined as follows: 
\begin{equation}
    \mathcal{L}_{pred}(\mathcal{M}) = - \log P (Y = 1 | \mathcal{G}= (\mathcal{U}, \mathcal{I}, \mathcal{E} \odot \sigma(\mathcal{M})), (u, i)),
\end{equation}

\noindent where $\sigma(\cdot)$ is the sigmoid function, $\odot$ is the element-wise product, and $Y$ is the original GNN prediction. Another loss term $\mathcal{L}_{path}$ is to learn to select path-forming edges. We first use the following score function to assign a score to each candidate path $p$ in the graph, prioritizing shorter paths and those without high-degree nodes:
\begin{equation}
    Score(p) = \prod_{\substack{e \in p}} \log \sigma (\mathcal{M}_{\tau(e)}) - \log(\text{deg}(e)),
\end{equation}

\noindent where $\mathcal{M}_{\tau(e)}$ denotes the mask corresponding to the type of the edge $e$ and $\text{deg}(e)$ is the degree of the target node of $e$. Using Dijkstra's shortest path algorithm, where path lengths are redefined according to the score function, a set of candidate edges considered concise and informative is selected, denoted by $\mathcal{E}_{path}$. Guided by $\mathcal{E}_{path}$, we define $\mathcal{L}_{path}$ accordingly:
\begin{equation}
    \mathcal{L}_{path}(\mathcal{M}) = - \sum_{r \in \mathcal{R}} (\sum_{\substack{e \in \mathcal{E}_{path}}} \mathcal{M}_{\tau(e)} - \sum_{\substack{e \in \mathcal{E}, e \notin \mathcal{E}_{path}}}  \mathcal{M}_{\tau(e)}).
\end{equation}

\noindent Upon convergence of mask learning, we can run the Dijkstra's shortest path to generate up to $k$ explanation paths based on $\mathcal{M}$. 

% \LH{$P$ has already been used to denote profile or probability. I suggest we use $w\in\mathcal{W}$ instead.}

\subsubsection{Node-level Retriever}

% Given a user-item pair ($u$, $i$) that requires explanation, \textit{the goal} of our node-level retriever is to identify at most $k$ relevant nodes \LH{abusively used $P$} from the graphs based on their semantic similarity to the user and item profiles, thereby extracting additional explanatory knowledge.

The node-level retriever aims to retrieve the semantic CF information from the graphs by selecting the $k$ most relevant user nodes and the $k$ most relevant item nodes.

\paratitle{Motivation.} Graphs are often known as noisy and incomplete, and the structural information alone may not be sufficient to provide accurate explanations. Additionally, graphs in recommendations typically contain rich semantics within user and item nodes derived from user interests and item properties. While the profiles provide some information, using only individual profiles for explanations may be inadequate, as it might not fully capture user topics or item features. Thus, retrieving semantics from graphs is crucial to uncovering latent connections and supplementing more comprehensive explanations for recommendations. Our node-level retriever incorporates a dense retrieval to calculate the semantic similarity between users and items to retrieve the most relevant neighboring nodes for the current recommendation ($u$, $i$) to unveil the semantic CF knowledge for explanations.

% capture user topics or item features. However, using only individual profiles for explanations may be inadequate, as it might not fully capture the user's topic or the item's features. Thus, it is essential to utilize the rich semantics from other relevant nodes as a supplement to uncover their latent connections for explanation.

% users demonstrate consistent patterns in their preferences, which can be captured through their historical data. Meantime, items consistently appeal to certain user types, allowing prediction of their attractiveness to similar users by analyzing past purchasers' characteristics. 
%
% Thus, it is essential to utilize the rich semantics contained in both nodes' and items' profiles to uncover their latent connections for explanation. Our node-level retriever leverages a pre-trained language model to calculate the semantic similarity between users and items to retrieve the most relevant neighboring nodes for the current recommendation ($u$, $i$) to unveil the semantic CF information for explanations.

% \LH{Above motivations do not show the contribution of attributes and their semantics.}

% To provide better explanations for user behavior, it is crucial to model user interests and analyze what types of users an item tends to attract. This is based on two observations: \ding{182} Users demonstrate consistent patterns in their interactions and preferences, which can be captured through their historical data. \ding{183} Items consistently appeal to certain user types, allowing prediction of their attractiveness to similar users by analyzing past purchasers' characteristics. Given that both user and item profiles contain rich semantics, our node-level retriever is designed to be \textbf{semantic-aware}. It leverages a pre-trained language model to calculate the semantic similarity between users and items to retrieve the most relevant neighboring users and items for the current ($u$, $i$) requiring explanation, thereby extracting additional explanatory knowledge and leading to more accurate explanations.

\paratitle{Dense Retrieval.} We employ a dual-encoder-based retriever architecture, which has demonstrated effectiveness across various retrieval tasks \cite{ni2022large} and efficiency at the inference stage \cite{lewis2020retrieval,izacardunsupervised}. Given a user-item pair $(u, i)$, a text encoder maps the user profile $b_u$ to an embedding $f(b_u) \in \mathbb{R}^d$, and the same encoder maps each item profile $c_i$ to an embedding $f(c_i) \in \mathbb{R}^d$, where $d$ is the hidden state dimension of the text encoder. The top-$k$ relevant users are retrieved based on the user-user semantic similarities, which are computed via cosine similarity:
\begin{equation}
sim(u, u') = \frac{f(b_u) \cdot f(b_{u'})}{\|f(b_u)\| \|f(b_{u'})\|}, \ \ u' \in \mathcal{N}_i,
\label{eq:node-level}
\end{equation}
\noindent where $\mathcal{N}_i$ is the set of users connected to item $i$. We also retrieve top-$k$ items based on the item-item semantic similarities. The retriever is training-free and a pre-trained language model specifically utilized to generate high-quality sentence embeddings.

\subsubsection{Graph Translation}

After the hybrid retrieval, the complex nature of graph-type knowledge, particularly the paths retrieved, presents a challenge since it cannot be directly integrated with profiles of users and items for input into the LLMs \cite{wang2024can,chen2024graphwiz}. To address this, it is necessary to employ graph translation techniques that convert the graph-type knowledge into a format compatible with LLMs, enabling LLMs to effectively process and utilize structured information. Considering that instructions are typically presented in natural language, we follow \cite{tan2024walklm,ye2023natural} to adopt a flatten-based method, which transforms retrieved paths and nodes into descriptive, easily comprehensible language. For each user-item pair $(u, i)$ requiring an explanation, along with $k$ retrieved paths, $k$ user nodes, and $k$ item nodes, we design the instruction prompt as follows:

\begin{tcolorbox}[
    enhanced,
    boxsep=2pt, 
    left=2pt, 
    right=2pt, 
    top=2pt, 
    bottom=2pt, 
    boxrule=0.5pt, 
    arc=2pt, 
    colback=white, 
    colframe=black, 
    fonttitle=\bfseries,
    fontupper=\footnotesize 
]
\small
Given the item title, item profile, and user profile, please explain why the user would enjoy this item. Item title: \textcolor[HTML]{4169E1}{[Item title]}. Item profile: \textcolor[HTML]{4169E1}{[Item summary]}. User profile: \textcolor[HTML]{4169E1}{[User summary]}.  For the user-item pair, here are some related users and items. Users: \textcolor[HTML]{4169E1}{[Top-$k$ similar user profiles]}. Items: \textcolor[HTML]{4169E1}{[Top-$k$ similar item profiles]}. For the given user-item pair, here are several related paths connecting users and items through their interactions. \textcolor[HTML]{4169E1}{[Top-$k$ explanation paths]} (each formatted as "<User profile> -> buys -> <Item profile> -> bought by -> <User profile> -> ...").
Explanations:
\end{tcolorbox}

\subsubsection{Discussion of Retrieval Granularity.} 

In \model, we consider node-level and path-level retrieval rather than subgraph-level based on the following observations. Firstly, both nodes and paths have a considerably smaller search space than subgraphs. As proven in \cite{zhang2023page}, compared to the expected number of edge-induced subgraphs, the expected number of candidate nodes and paths grows strictly slower and becomes negligible. Therefore, such explanations exclude many less-meaningful subgraph candidates, making the explanation generation much more straightforward and accurate. Secondly, subgraphs contain complex high-order information, making it difficult to describe a retrieved subgraph with hundreds or even thousands of nodes in a way that is easily understandable, especially when all nodes have text attributes \cite{liuone}. Consequently, we implement hybrid graph retrieval in the path and node levels.

\subsection{Knowledge Pruning}

\paratitle{Motivation.} After graph retrieval, our goal is to enable LLMs to effectively generate explanations based on the graph CF information of varying granularities retrieved. However, it is noticed that for some user-item pairs, a sufficient explanation can be derived solely from their profiles, without the need for additional CF information. For instance, if a user's profile shows a preference for \textit{science fiction} and the recommended item is a popular \textit{sci-fi film}, then explaining this interaction might only require referencing these profile details. In such cases, the retrieved knowledge is not only redundant but employing such training samples could also weaken the LLM's ability to utilize CF information. 

\paratitle{Re-ranking.} To enhance LLMs' focus on CF information, we propose knowledge pruning to filter out less relevant or redundant training samples \cite{peng2024graph}, building on the inherent ability of LLMs to generate explanations by understanding user and item profiles. It also improves training efficiency through reduced data volume, particularly beneficial for large graphs. To be specific, we use a re-ranking method \cite{li2022community,shen2021entity} that sorts all training samples based on their reliance on CF information for explanation, prioritizing those that require more additional knowledge. We measure such reliance via the semantic similarity between the profiles and the ground truth explanations, which is defined as follows:
\begin{equation}
sim((u, i), \text{Explain}_{(u, i)}) = \frac{f(b_u \oplus c_i) \cdot f(\text{Explain}_{(u, i)})}{\|f(b_u \oplus c_i)\| \|f(\text{Explain}_{(u, i)})\|},
\label{eq:reranking}
\end{equation}

\noindent where $\oplus$ represents the concatenation and $\text{Explain}_{(u, i)}$ is the ground truth explanation for ($u$, $i$). We introduce a pruning ratio $t$, which determines the portion of the total training data $\mathcal{D}$ to be filtered after re-ranking. The top ``$(1-t) \cdot |\mathcal{D}|$'' training samples are preserved, ensuring that samples that are most likely to benefit from CF-based explanations are prioritized, thereby improving the overall efficiency and effectiveness of the training process. 

\subsection{Retrieval-Augmented Fine-Tuning}

\paratitle{Motivation.} To improve the LLM's capacity for generating better explanations using retrieved CF information, we fine-tune it with a parameter-efficient pre-training strategy, i.e., LoRA \cite{hu2021lora}, on the pruned training set with in-context retrieval augmentation. Integrating retrieval results during fine-tuning offers two key advantages. (1) It adapts the LLM to better utilize retrieved CF information to generate explanation, especially for the requirement of domain-specific knowledge it has never seen before. (2) Even state-of-the-art retrievers can falter and return inaccurate results \cite{linra}. By training the LLM to generate ground-truth responses even when irrelevant CF information is given, we enable the LLM to ignore misleading retrieval content and lean into its internal knowledge to reduce hallucination. 

\paratitle{Fine-tuning.} We employ retrieval-augmented fine-tuning (RAFT) with a conventional language modeling loss \cite{zhang2024raft}. The model is trained to process input consisting of a user-item pair ($u$, $i$) with profiles $b_u$ and $c_i$, its associated retrieved knowledge $\mathcal{K}_{(u,i)}$, and a corresponding prompt question $Q$ to generate textual explanations $E$ as outputs. The RAFT loss is computed based on the discrepancy between the model's predicted explanations and the ground truth explanations in the dataset. Formally, the loss is defined as follows:
\begin{equation}
    \mathcal{L}_{\text{RAFT}} = -\sum_{(u, i)\in \mathcal{D}_{\text{prune}}} \log P(\text{Explain}_{(u, i)} | b_u, c_i, \mathcal{K}_{(u,i)}, Q; \theta),
\end{equation}
where $\mathcal{D}_{\text{prune}}$ represents the pruned training set and $\theta$ is the parameters associated with the LoRA model.
