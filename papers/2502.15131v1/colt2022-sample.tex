%\documentclass[anon,12pt]{colt2022} % Anonymized submission
\documentclass[final,12pt]{colt2022} % Include author names


\title[Angular Calibration and Platt Scaling in High Dimensions]{Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling}
\usepackage{times}



\usepackage{graphicx}    % for \includegraphics
%\usepackage{subcaption}  % for sub-figure environments
\usepackage{float}       % for [H] specifier (if really needed)




\usepackage{amsmath}


\usepackage{bm}
%\usepackage{natbib}
\RequirePackage[sort&compress,numbers]{natbib}

% \usepackage[plain,noend]{algorithm2e}
% Recommended, but optional, packages for figures and better typesetting:
%\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{physics}



% For theorems and such
\usepackage{amssymb}
\usepackage{mathtools}
% \usepackage{amsthm}
\usepackage{bm}
\usepackage{relsize}
\usepackage{appendix}

\usepackage{graphicx}
%\usepackage{subcaption}  % For subfigure support


\usepackage[capitalize,noabbrev]{cleveref}

\newcommand{\stmis}{\tilde{\bm{\beta}}^\star}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\yprebf}{\mathbf{y}_{\mathsf{pre}}}
\newcommand{\Xprebf}{\mathbf{X}_{\mathsf{pre}}}
\newcommand{\yhatbf}{\hat{\mathbf{y}}}
\newcommand{\that}{\hat{t}}
\newcommand{\what}{\widehat{{w}}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\xnewbf}{\mathbf{x}_{\mathrm{new}}}
\newcommand{\ynew}{{y}_{\mathrm{new}}}
\newcommand{\zerobf}{\bm{0}}
\newcommand{\betabf}{\bm{\beta}}
\newcommand{\alphabf}{\bm{\alpha}}
\newcommand{\alphahatbf}{\hat{\bm{\alpha}}}
\newcommand{\betahatbf}{\hat{\bm{\beta}}}
\newcommand{\epbf}{{\bm{\varepsilon}}}
\newcommand{\Sigmabf}{\mathbf{\Sigma}}
\newcommand{\betastar}{\mathbf{\beta}^\star}
\newcommand{\Bstar}{\mathbf{B}^\star}
\newcommand{\Bstarmis}{\tilde{\mathbf{B}}^\star}
\newcommand{\BBstar}{\mathbf{B}^\star {\mathbf{B}^\star}^\top}

\newcommand{\Bhat}{\widehat{\mathbf{B}}}
\newcommand{\Bhatprior}{\widehat{\mathbf{B}}^{\mathsf{avg}}}
\newcommand{\Dhat}{\widehat{\mathbf{D}}}
\newcommand{\Ohat}{\widehat{\mathbf{O}}}
\newcommand{\Qhat}{\widehat{\mathbf{Q}}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\qbf}{\hat{\mathbf{q}}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Khat}{\widehat{\mathbf{K}}}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\alphastar}{\bm{\alpha}^\star}
\newcommand{\qtrue}{q}
\newcommand{\krank}{k}
\newcommand{\st}{{\bm{\beta}^\star}}
\newcommand{\lambdaa}{\lambda_\mathbf{\alpha}}
\newcommand{\lambdab}{\lambda_\mathbf{\beta}}
\newcommand{\lambdaB}{\lambda}
\newcommand{\lambdazero}{\lambda_0}
\newcommand{\Gammahat}{\hat{\bm{\Gamma}}}
\newcommand{\Lambdahat}{\hat{\bm{\Lambda}}}
\newcommand{\BSC}{B_\mathsf{SC}}
\newcommand{\VSC}{V_\mathsf{SC}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Rcal}{\mathfrak{R}}
\newcommand{\cfrak}{\mathfrak{c}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\eigtop}{\sigma_{\mathrm{max}}}
\newcommand{\eigbot}{\sigma_{\mathrm{min}}}
\newcommand{\vs}{\varphi}
\newcommand{\xH}{\mathbf{x}_H}
\newcommand{\hind}{\tilde{h}}
\newcommand{\What}{\hat{\mathbf{W}}}
\newcommand{\Zj}{\mathbf{Z}^{\st}}
\newcommand{\Zhj}{\mathbf{Z}^{\st}_h}
\newcommand{\Th}{\hat{\mathbf{T}}_h}
\newcommand{\Ph}{\hat{\mathbf{P}}_h}
\newcommand{\Pbf}{\hat{\mathbf{P}}}
\newcommand{\upbf}{\bm{\upsilon}}
\newcommand{\Phpj}{\hat{\mathbf{P}}_{h,\perp}^{\st}}
\newcommand{\Phpinf}{\hat{\mathbf{P}}_{h,\perp}^{\infty}}
\newcommand{\upbfj}{\bm{\upsilon}^{\st}}
\newcommand{\upbfjtilde}{\tilde{\bm{\upsilon}}^{\st}}
\newcommand{\upbfjbreve}{\breve{\bm{\upsilon}}^{\st}}
\newcommand{\stj}{{\bm{\beta}^\star}}
\newcommand{\upbfjT}{\bm{\upsilon}^{\st\top}}
\newcommand{\upbfjtildeT}{\tilde{\bm{\upsilon}}^{\st\top}}
\newcommand{\upbfjbreveT}{\breve{\bm{\upsilon}}^{\st\top}}
\newcommand{\stjT}{{\bm{\beta}^\star}^{\st\top}}
\newcommand{\Pw}{\hat{\mathbf{P}}_{h, \perp}^{\varpi,\st}}
\newcommand{\Pwinf}{\hat{\mathbf{P}}_{h, \perp}^{\varpi,\infty}}
\newcommand{\Pz}{\hat{\mathbf{P}}_{h, \perp}^{z,\st}}
\newcommand{\Pzinf}{\hat{\mathbf{P}}_{h, \perp}^{z,\infty}}
\newcommand{\xibf}{\bm{\xi}}
\newcommand{\Xbfj}{{\Xbf}^{\st}}
\newcommand{\Xbfhj}{{\Xbf}^{\st}_h}
\newcommand{\XbfhjT}{{{\Xbf}^{\st}_h}^\top}
\newcommand{\rzero}{\rho_i^{(0)}}
\newcommand{\rone}{\rho_i^{(1)}}
\newcommand{\bigsum}{\mathlarger{\mathlarger{\sum}}}
\newcommand{\Lbf}{\mathbf{L}}

\newcommand{\normop}[1]{\norm{#1}_\mathrm{op}}

\newcommand{\ho}{\mathrm{ho}}
 
\usepackage{xcolor}
\definecolor{ps}{RGB}{0,0,200}
\newcommand{\ps}[1]{\textcolor{ps}{[PS: #1]}}

% Authors with different addresses:
\coltauthor{%
 \Name{Yufan Li} \Email{yufan\_li@g.harvard.edu}
 %\addr Address 1
 \AND
 \Name{Pragya Sur} \Email{pragya@fas.harvard.edu}\\
 \addr Department of Statistics, Harvard University\\ 1 Oxford Street, Cambridge, MA%
}

\begin{document}

 \maketitle

\begin{abstract}%


We study the fundamental problem of calibrating a linear binary classifier of the form \(\sigma(\hat{w}^\top x)\), where the feature vector \(x\) is Gaussian, \(\sigma\) is a link function, and \(\hat{w}\) is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative \emph{chance classifier}, we construct a well-calibrated predictor whose interpolation weight depends on the angle \(\angle(\hat{w}, w_\star)\) between the estimator \(\hat{w}\) and the true linear weight \(w_\star\). We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle \(\angle(\hat{w}, w_\star)\)  can be consistently estimated. Furthermore, the resulting predictor is uniquely \emph{Bregman-optimal}, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors.
Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions. 

\end{abstract}

\begin{keywords}%
  Calibration; Binary Classification; High Dimensions; Bregman Divergence%
\end{keywords}

\section{Introduction}

Calibration of predictive models is a fundamental problem in statistics and machine learning, especially in applications that require reliable uncertainty quantification. A well-calibrated model ensures that its predicted probabilities align closely with true event probabilities—a property essential in fields such as medical decision-making \citep{begoli2019need,jiang2012calibrating}, meteorological forecasting \citep{brocker2009reliability, gneiting2005weather,degroot1983comparison,murphy1977reliability,murphy1973new}, self-driving systems \citep{michelmore2018evaluating}, and natural language processing \citep{nguyen2015posterior,guo2017calibration}.

Numerous algorithms have been proposed for calibrating the outputs of a trained model, including classical methods such as Platt scaling \citep{platt1999probabilistic,boken2021appropriateness,phelps2024using,gupta2023online}, histogram binning \citep{zadrozny2001obtaining,sun2024minimum}, isotonic regression \citep{zadrozny2002transforming,jiang2011smooth,berta2024classifier}, and more recent approaches such as temperature scaling \citep{guo2017calibration,kull2019beyond}, ensemble-based methods \citep{lakshminarayanan2017simple,malinin2019ensemble,wen2020batchensemble,tran2020hydra}, and Bayesian strategies \citep{kristiadi2020being,clarte2023expectation}, among others. 
%works apply classical asymptotic theories to study calibration error 

While extensive prior work has studied calibration 
\cite{kumar2019verified,sun2024minimum,gupta2020distribution,shabat2020sample,jung2021moment}, this literature primarily focuses on traditional asymptotic theories or finite-sample learning theoretic arguments. These approaches often overlook the impact of problem dimensionality, which is particularly relevant for high-dimensional settings where the number of features may be substantial.
Alternatively, a separate line of research has explored calibration within a high-dimensional proportional asymptotic regime, where the sample size \(n\) and the feature dimension \(d\) both diverge, at a comparable rate. This  proportional scaling regime has gained significant traction in modern statistics and machine learning. In statistics, its popularity stems from the fact that theories derived under this regime  capture high-dimensional phenomena observed in moderate to large sized datasets unusually well \cite{johnstone2009statistical,bean2013optimal,donoho2009message,thrampoulidis2014gaussian,zdeborova2016statistical, barbier2019optimal,sur2019modern,jiang2022new,montanari2024friendly,malekibridge}. Consequently, this has spurred the creation of innovative methods displaying remarkable practical performance  \cite{mondelli2021approximate,feng2022unifying,bellec2022biasing,li2023spectrum,song2024hede,luo2024roti}.  In machine learning, this regime has proven exceptionally valuable and effective in analyzing the behavior of modern neural networks and other interpolation learners under overparametrization
%(ii) the asymptotics are known   reflects many contemporary data-analysis settings that involve a large number of features but a relatively limited sample size \cite{johnstone2009statistical,sur2019modern,verchand2024high};
\cite{liang2020just, hastie2022surprises,liang2022precise, mei2022generalization,adlam2020neural,song2024generalization,patil2024optimal}. For binary classification in this proportional regime, a substantial line of work \cite{sur2019likelihood,sur2019modern,zhao2022asymptotic} establishes that classical logistic regression yields seriously biased estimates; building upon these, \cite{bai2021don} shows that logistic regression tends to be inherently overconfident, while \cite{clarte2023theoretical} discusses the impact of regularization under the same model. %Moreover, \cite{clarte2022study} investigates the calibration of random feature models within a Bayesian context. 
Finally, \cite{clarte2023expectation} introduces expectation consistency and derives a limiting calibration error formula as a function of the signal prior and other problem parameters. 


Despite these advancements,  an approach that is provably calibrated in  high dimensions, without knowledge of the true signal prior, is missing. Moreover, there is a lack of principled understanding regarding optimal calibration strategies from among the available options.  Additionally, rigorous guarantees on the performance of classical calibration methods, such as Platt scaling, in modern high-dimensional scenarios is  notably absent from the literature. In this paper, we address these gaps. We consider the challenge of calibrating a binary linear predictor in a frequentist setting under a Gaussian design. Our contributions are three-fold: (i) we introduce a data-driven predictor that can provably calibrate in a broad class of high-dimensional binary classification problems; (ii) we show that our calibrated predictor is \emph{Bregman-optimal}, meaning it uniquely minimizes any Bregman divergence relative to the true label-generation probability; (iii) we establish conditions under which a classical Platt-scaled predictor converges to this Bregman-optimal calibrated solution, thereby formally showing that Platt scaling is both well-calibrated and Bregman optimal in our high-dimensional setting. Although we derive our theoretical results assuming Gaussian features, extensive recent universality results suggest that these should continue to hold for sufficiently light tailed distributions (see  Section \ref{sec:conc} for a discussion). We provide experiments that demonstrate this robustness to the Gaussian assumption (\Cref{fig:Rad} and \ref{fig:Unif}).


We construct our calibrated predictor by interpolating with an uninformative (“chance”) predictor, where the interpolation weight is determined by the angle \(\angle(\hat{w}, w_{\star})\) between the estimated linear weight \(\hat{w}\) and the true weight \(w_{\star}\). Our construction crucially leverages recent developments from the literature on observable estimation of unknown parameters in high dimensions. For instance, leveraging advances in \cite{javanmard2018,bellec2022observable,bellec2022biasing,bellec2023debiasing,celentano2023lasso,li2023spectrum}, we can show that the angle \(\angle(\hat{w}, w_{\star})\) is consistently estimable when \(n\) and \(d\) grow proportionally. To our knowledge, this is the first provable calibration method in a high-dimensional setting, and it uncovers a conceptual link between optimal calibration and \(\angle(\hat{w}, w_{\star})\): the poorer the alignment of $\what$ with $w_\star$, the greater the noise needed to be injected to prediction logits to ensure calibration.

\section{Setting}\label{Setting}
Suppose we observe i.i.d.~data \((y_i, x_i)\) satisfying  
\begin{equation}\label{lab}
    y_{i} \stackrel{\mathrm{iid}}{\sim} \operatorname{Bern}\left(\sigma\left(w_{\star}^{\top} x_{i}\right)\right), \quad i=1, \ldots, n,
\end{equation}
where \( \sigma: \mathbb{R} \to [0,1] \) denotes the link function and the covariates \( x_{i} \in \mathbb{R}^{d} \) are drawn independently as
$x_{i} \stackrel{\mathrm{iid}}{\sim} N(0, \Sigma)$, 
with \(\Sigma\) assumed to be known (say from a separate unlabeled dataset as in \cite{celentano2024correlation,celentano2023lasso}). The true linear weight \( w_\star \in \mathbb{R}^d \) is an arbitrary deterministic vector, and we assume without loss of generality that  
$
w_{\star}^{\top} \Sigma w_{\star} = \|w_{\star}\|_{\Sigma}^{2} = 1.
$
The training dataset is denoted as \( X = \left[x_{1}, \ldots, x_{n}\right]^{\top} \in \mathbb{R}^{n \times d} \) and \( y = \left[y_{1}, \ldots, y_{n}\right]^{\top} \in \mathbb{R}^{n} \).

To quantify the degree of miscalibration, we define the calibration error at level \( p \) for any predictor \( \hat{f} \) as  
\[
\Delta_{p}^{\mathrm{cal }}(\hat{f})=p-\mathbb{E}_{x_{\mathrm{new }}}\left[\sigma\left(w_{\star}^{\top} x_{\mathrm{new }}\right) \mid \hat{f}\left(x_{\mathrm{new }}\right)=p\right],
\]
where \( \mathbb{E}_{x_{\mathrm{new }}} \) denotes the expectation over \( x_{\mathrm{new }} \sim N(0,\Sigma) \). A predictor is said to be \emph{well-calibrated} if \( \Delta_{p}^{\mathrm{cal }}(\hat{f}) = 0 \) for all \( p \) in the range of \( \hat{f} \). Intuitively, this means that when the predictor assigns a probability \( p \) to label \( 1 \), the true probability of label \( 1 \) is indeed \( p \).

We consider the regularized M-estimator
\[
\widehat{w} = \underset{w}{\arg \min } \frac{1}{n} \sum_{i=1}^{n} \ell_{y_{i}}\left(w^{\top} x_{i}\right) + g(w),
\]
where \( g(\cdot) \) is a convex penalty and \( \ell(\cdot) \) is a convex loss function. In this setting, we consider a sequence of problem instances $\{y(d),X(d), w_{\star}(d)\}_{d\ge 1}$ such that $X(d)\in \R^{n(d)\times d}$ and $y(d)\in \R^{n(d)}$ generated from \eqref{lab}. It is well-known that in the special case where $\ell(\cdot)$ equals the logistic loss and $g(\cdot)$ is zero, the corresponding predictor \( \sigma(\hat{w}^\top x_{\mathrm{new}}) \) is grossly mis-calibrated in the high-dimensional regime \( \frac{n}{d} \to (0, +\infty) \) \cite{sur2019likelihood,sur2019modern,bai2021don, clarte2023theoretical}, even where it is well-defined and unique \cite{candes2020phase}. 

In what follows, we present, for the first time, a predictor that is provably well-calibrated  in this  regime (for general convex losses and penalties beyond the special case mentioned above). We achieve this through an angular calibration idea, and furthermore, establish 
that this is optimal in the sense that it minimizes any Bregman divergence to the  true label distribution. We conclude showing an interesting connection---Platt scaling converges to our angular predictor---and therefore is both provably well-calibrated and optimal in the aforementioned sense. 

\section{Introducing angular calibration}

Most calibration strategies \emph{adjust} a pre-trained predictor by learning a mapping \(F\colon u \mapsto F(u)\) of the logits \(\hat{w}^\top x_{\mathrm{new}}\). Platt scaling, for example, stipulates the parametric form \(F(u)=\sigma(Au+B)\), where \(A,B\in \R\) are fit on a holdout dataset. This raises a natural question:
\begin{center}
\emph{Among all well-calibrated predictors of the form \(F(\hat{w}^\top x_{\mathrm{new}})\), which one is ``the best''?}
\end{center}
In this section, we introduce a predictor with such an optimality property by interpolating between the prediction logits \(\hat{w}^\top x\) and an uninformative chance predictor. Specifically, we show that if the interpolation weight is determined by the angle between the estimator \(\widehat{w}\) and the true weight \(w_\star\), given by  
\begin{equation}\label{angledef}
    \theta_{*}=\operatorname{arccos}\left(\frac{\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}}{\|\widehat{w}\|_{\Sigma} \|w_{\star}\|_{\Sigma}}\right),
\end{equation}
then the resulting interpolated predictor minimizes any Bregman divergence to the true label distribution among all predictors of the form \(F(\hat{w}^\top x_{\mathrm{new}})\).
To the best of our knowledge, a predictor that is both provably calibrated and optimal (in the aforementioned sense) has not been previously introduced for high-dimensional problems.

Notably, our predictor uses the angle defined in \eqref{angledef}, thus to define a data-driven predictor, we require a consistent estimate of this angle. Fortunately, recent advances in the  high-dimensional literature (c.f.,~\cite{javanmard2018,bellec2022observable,bellec2022biasing,bellec2023debiasing,celentano2023lasso,li2023spectrum}) allow us to  estimate the inner product $\expval{\widehat{w}, w_\star}_\Sigma$, and therefore $\theta_*$,  when \(n\) and \(d\) grow proportionally. We discuss the details of this estimation scheme later in \Cref{mest}. For now, we present our angular calibration idea assuming that we have access to a consistent estimator
 $\hat{\theta}$ for $\theta_*$.

\begin{definition}(Angular Predictor)
Let
\begin{equation}\label{definterp}
    \hat{f}_{\mathrm{ang}}\left(\what^\top x_{\mathrm{new }}; \hat{\theta} \right)=\mathbb{E}_{Z}\left(\sigma\left(\cos \left(\hat{\theta}\right) \cdot\left( \frac{\what^\top x_{\mathrm{new }}}{\|\widehat{w}\|_{\Sigma}}\right)+\sin \left(\hat{\theta}\right) \cdot Z\right)\right)
\end{equation}
where $\hat{\theta}$ is a consistent estimator of $\theta_*$ defined as in \eqref{angest} and $\mathbb{E}_{Z}$ denotes expectation with respect to the \textit{Gaussian noise} $Z \sim N(0,1)$. We will later refer to $\hat{f}_{\mathrm{ang}}$ as the angular predictor for simplicity. 
\end{definition}


\Cref{mainThm} below shows that the angular predictor is  well-calibrated. We defer the proof to \Cref{pfmainThm} 
\begin{theorem}\label{mainThm}
    Assume the link function $\sigma$ is continuous. Then, the predictor $\hat{f}_{\mathrm{ang}}$ defined in \eqref{definterp} is well-calibrated as $d,n \rightarrow \infty, n/d \rightarrow (0,\infty)$. That is, for any $p$ contained in the range of $\sigma$,  we have that
    $$\Delta_{p}^{\mathrm{cal }}\left(\hat{f}_{\mathrm{ang}}\left(\cdot; \hat{\theta} \right) \right)=p-\mathbb{E}_{x_{\mathrm{new }}}\left[\sigma\left(w_{\star}^{\top} x_{\mathrm{new }}\right) \mid \hat{f}_{\mathrm{ang}}\left(\what^\top x_{\mathrm{new }}; \hat{\theta}\right)=p\right]\to 0,$$
    in probability where $\hat{\theta}$ is a consistent estimator for $\theta_\star$ (Cf. Proposition \ref{thmconsis}).
\end{theorem}

The above utilizes the result  that when $\hat{\theta}=\theta_*$ exactly, $\hat{f}_{\mathrm{ang}}\left(\cdot; \theta_* \right)$ is exactly well-calibrated (we state and  prove this formally in \Cref{mainThmPop}) and that $\hat{\theta}$ is consistent for $\theta_\star$. %We defer technical statement of this fact and the proof to \Cref{mainThmPop}. 
We will later show that $\hat{f}_{\mathrm{ang}}\left(\cdot; \hat{\theta} \right)\approx \hat{f}_{\mathrm{ang}}\left(\cdot; \theta_* \right)$ is in fact optimal in the sense that it minimizes any Bregman divergence to the true label distribution. The construction \eqref{definterp} admits an intuitive interpretation. By the basic trigonometric identity
$
\cos^2(\theta_*) + \sin^2(\theta_*) = 1,
$
we can see that the logits, that is, the argument of $\sigma(\cdot)$ in \eqref{definterp},  is an interpolation between the informative component $\what^\top x_{\mathrm{new}}$ and the noninformative Gaussian noise $Z$. Notice that when \(\widehat{w}\) is well aligned with \(w_\star\) (i.e., \(\cos^2(\theta_*) = 1\)), the angular predictor \(\hat{f}_{\mathrm{ang}}\) lies closer to the informative predictor \(\sigma(\widehat{w}^\top x_{\mathrm{new}})\). Conversely, when \(w_\star\) and \(\widehat{w}\) are orthogonal (i.e., \(\sin^2(\theta_*) = 1\)), \(\hat{f}_{\mathrm{ang}}\) defaults to the non-informative chance predictor \(\mathbb{E}[\sigma(Z)] = \mathbb{E}[\sigma(w_\star^\top x_{\mathrm{new}})]\). In other words,
\begin{center}
\emph{The poorer the alignment between \(w_\star\) and \(\widehat{w}\), the greater the magnitude of  noise  $Z$ required to maintain calibration.}

%needed to must be injected to prediction logits via interpolation with Gaussian noise \(Z\) to preserve calibration.}
\end{center}
We will show in the next section that this angular interpolation idea leads to a uniquely Bregman optimal calibrated predictor. This provides the first calibration procedure that is calibrated and optimal in high dimensions, provably.


\section{Main Result I: Calibrating Optimally using Angular Calibration}


Before formally stating our results on optimality of angular calibration, we first define the following random probability vectors for label distribution,
\begin{equation}\label{probvec}
    q_{ \star}:=\mqty(\sigma(w_\star^\top x_\mathrm{new}) \\ 1- \sigma(w_\star^\top x_\mathrm{new})), \;\; \hat{q}_{ F}:=\mqty(F(\widehat{w}^\top x_\mathrm{new})\\ 1- F(\widehat{w}^\top x_\mathrm{new})), \;\; 
 \hat{q}_{\mathrm{ang}}(\hat{\theta}):=\mqty(\hat{f}_{\mathrm{ang}}(\what^\top x_\mathrm{new}; \hat{\theta})\\ 1- \hat{f}_{\mathrm{ang}}(\what^\top  x_\mathrm{new}; \hat{\theta}))
\end{equation}
where $F: \R \to [0,1]$ is any measurable function. Here, $q_{ \star}$ corresponds to the ground-truth probability distribution of the new label, $\hat{q}_{ F}$ the prediction probability distribution of an $F$-calibrated predictor, and $q_{\mathrm{ang}}$ the prediction probability distribution of our angular predictor. 

Next, we define the Bregman loss function.
\begin{definition}[Bregman Loss Functions]
     Let $\phi: \mathbb{R}^2 \mapsto \mathbb{R}$ be a strictly convex differentiable function. Then, the Bregman loss function $D_\phi: \mathbb{R}^2 \times \mathbb{R}^2 \mapsto$ $\mathbb{R}$ is defined as
\begin{equation*}
D_\phi(x, y)=\phi(x)-\phi(y)-\langle x-y, \nabla \phi(y)\rangle .
\end{equation*}
\end{definition}
The Bregman loss function class covers common losses  such as the squared loss $D_\phi(x, y):=\norm{x-y}_2^2$ and Kullback-Liebler (KL) divergence $D_\phi(x, y)=\sum_{j=1}^2 x_j \log \left(x_j / y_j\right)$ between two probability vectors $x,y$.  

% \ps{Is there a reference for this? This is not how original KL is defined right?}

%We are now prepared to present the first theoretical result of this paper: 
Theorem \ref{optimalThm} states that the prediction probability \(\hat{q}_{\mathrm{ang}}\) generated by the angular predictor uniquely minimizes \textit{any} Bregman loss against the ground-truth probability vector \(\hat{q}_{ \star}\) within the class of \(q_{ F}\) for any \(F\). We defer the proof of the Theorem below to \Cref{proofoptimalThm}. 



\begin{theorem}[Optimality of angular predictor]\label{optimalThm}
Let $\phi: \mathbb{R}^2 \mapsto \mathbb{R}$ be any strictly convex differentiable function, and let $D_\phi$ be the corresponding Bregman loss function. Let $\E_{x_\mathrm{new}}[\phi(q_\star)]$ be finite. Then, the expected Bregman loss $\E_{x_\mathrm{new}}\left[D_\phi(q_{ \star}, \hat{q}_{ F})\right]$ admits a unique minimizes (up to a.s. equivalence) among all $q_{ F}, \forall F \in \mathcal{F}:=\{f: \R \to [0,1]\}$. Let this minimizer be $F_\star = \arg \min _{F\in \mathcal{F}} \E_{x_\mathrm{new}}\left[D_\phi(q_{ \star}, \hat{q}_{ F})\right]$. Further suppose that the link function $\sigma$ is continuous. We then have that as $n,d \to \infty$, we have
\begin{equation*}
\begin{aligned}
    \norm{\hat{q}_{\mathrm{ang}} (\hat{\theta})-F_*(\what^\top x_{\mathrm{new}})}_2^2 \to 0
\end{aligned}
\end{equation*}
in probability. That is, the label prediction probability vector from angular calibration converges to the optimal label prediction probability vector given by $F_*$. 
\end{theorem}

We note that as $\hat{\theta}=\theta_*$, $\hat{q}_{\mathrm{ang}} (\hat{\theta})$ precisely attains the optimal solution $F_*(\what^\top x_{\mathrm{new}})$. We defer the technical statement to \Cref{optimalThmPop} in \Cref{proofoptimalThm}. 


\section{Main Result II: Platt scaling is provably calibrated and Bregman-optimal}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{approx.pdf}
    \caption{Platt scaling of a logistic ridge predictor converges to angular calibration predictor, as holdout set size increases. The plot is generated with Gaussian data with covariance $\Sigma=\frac{1}{d} \bar{\Sigma}$ where $\bar{\Sigma}_{kl}=0.5^{|k-l|}, \forall k,l \in \{1,...,d\}$, sigmoid link function in a data deficient setting where $n=1000, p=2000$. See more details in \Cref{simulation}}
    \label{fig:asymp}
\end{figure}


Platt scaling is arguably the most widely used calibration method in modern machine learning, yet its theoretical properties in high-dimensional settings remain unexplored. In this section, we identify conditions under which Platt scaling converges to our angular predictor, and is therefore well-calibrated and Bregman-optimal in high dimensions.


 
 Platt scaling finds a mapping $F$ of the prediction logits $\what^\top x_{\mathrm{new}}$ by minimizing the log-likelihood on a holdout dataset. In this section, we specifically consider the setting where we have a holdout dataset $(x_{\ho, i}, y_{\ho, i})_{i=1}^{n_{\ho}}$ and the negative log-likelihood
\begin{equation}\label{sampleloss}
    \hat{\ell}_{n_{\ho}}(F) := \sum_{i=1}^{n_{\ho}}-y_{\ho,i}\log(F(\widehat{w}^\top x_{\ho, i}))-(1-y_{\ho,i})\log(1-F(\widehat{w}^\top x_{\ho, i})).
\end{equation}
The Platt calibration procedure then searches for a mapping $F$ within some hypothesis class $\mathcal{F}_{\mathrm{platt}}$ that minimizes the negative log-likelihood. Elementary asymptotic theory then shows that as $n_{\ho}\to\infty$ (i.e. the holdout set is sufficiently large), $\hat{L}(\theta)$ in \eqref{sampleloss} converges to the population loss 
\begin{equation}\label{poploss}
\ell^\star(F)=\E_{x_\mathrm{new}}\left[D_{\mathrm{KL}}\qty(\mqty(\sigma(w_\star^\top x_\mathrm{new}) \\ 1- \sigma(w_\star^\top x_\mathrm{new})) \bigg \| \mqty(F(\widehat{w}^\top x_\mathrm{new})\\ 1- F(\widehat{w}^\top x_\mathrm{new})))\right]
\end{equation}
almost surely up to a constant. This is exactly the argument of the Bregman loss $\E_{x_\mathrm{new}}\left[D_\phi(q_{ \star}, \hat{q}_{ F})\right]$ from \Cref{optimalThm} for $\phi$ specialized as the negative Shannon entropy. That is, such calibration procedures are essentially trying to optimizing the Bregman loss but within the restricted hypothesis class. 

This naturally raises the question of whether calibration procedures such as Platt scaling can achieve the optimal Bregman loss, say in the limit of a sufficiently large holdout set. \Cref{plattjust} shows that, if \(\sigma\) is a probit link function (or is closely approximated by one up to an affine transformation---for instance, \(\mathrm{sigmoid}(x) \approx \Phi(\sqrt{\pi/8}\,x)\)), and if the negative log-likelihood \eqref{sampleloss} is minimized over the hypothesis class of the form
$\sigma(Au+B), A,B\in \R$
then the resulting Platt-scaled predictor converges to our angular predictor as \(n_{\ho}\to\infty\). Combining this connection with our results for the predictor $\hat{f}_{\mathrm{ang}} (u; \theta_*)$ which is our angular predictor $\hat{f}_{\mathrm{ang}} (u; \hat{\theta})$ with $\hat{\theta}=\theta_*$ exactly. As mentioned previously (see also \Cref{mainThmPop} and \Cref{optimalThmPop} in Appendix), $\hat{f}_{\mathrm{ang}} (u; \theta_*)$ is exactly calibrated and Bregman-optimal, which shows that Platt scaling is both provably calibrated and Bregman optimal. This offers the first formal high-dimensional guarantees of this kind for the widely well-known Platt scaling procedure. We defer the proof of the Theorem below to \Cref{proofplattjust}.


\begin{theorem}\label{plattjust}
    Consider the predictor $\hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(u)$ calibrated by the Platt scaling procedure, that is,
    \begin{equation}\label{wer}
    \begin{aligned}
    &\hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(\widehat{w}^\top x_{\mathrm{new}})=\sigma (\hat{A}^{n_{\mathrm{ho}}}\cdot \widehat{w}^\top x_{\mathrm{new}}+\hat{B}^{n_{\mathrm{ho}}}), \\
    &\text{ with }\hat{A}^{n_{\mathrm{ho}}}, \hat{B}^{n_{\mathrm{ho}}} = \argmin_{(A,B)\in \mathcal{H}} \hat{\ell}_{n_{\ho}} \qty(u\mapsto \sigma(Au+B))
\end{aligned}
\end{equation}
for $\hat{\ell}_{n_{\ho}}(\cdot)$ defined in \eqref{sampleloss}. If the link function $\sigma$ satisfies $\sigma(x)=\Phi(a\cdot x+b)$ for some $a\in \R \setminus \{0\},b\in \mathbb{R}$ and the point $(A_*, B_*)$ defined in \eqref{ABstardef} is contained in a compact subset $\mathcal{H} \subset \R^2$, the angular predictor defined in \eqref{definterp} satisfies
    \begin{equation}\label{correctform}
        \hat{f}_{\mathrm{ang}} (u; \theta_*)=\sigma \qty(A_* \cdot u+B_*) \in \mathcal{F}_{\mathrm{platt}},
    \end{equation}
    where 
    \begin{equation}\label{ABstardef}
        A_*=\frac{\cos(\theta_*)}{\norm{\hat{w}}_\Sigma \sqrt{1+a^2 \sin^2(\theta_*)}}, \qquad  B_*=\frac{b}{a}\qty(\frac{1}{\sqrt{1+a^2 \sin^2(\theta_*)}}-1)
    \end{equation}
    and $\mathcal{F}_{\mathrm{platt}}=\{u \mapsto \sigma(Au+B): A,B\in \R \}$.
    Moreover, as $n_{\mathrm{ho}}\to \infty$, we have that $\hat{A}^{n_{\mathrm{ho}}} \to A_*, \hat{B}^{n_{\mathrm{ho}}} \to B_*$ in probability and 
    \begin{equation}\label{conv}
       \sup_{u \in \R} \abs{\hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(u) -\hat{f}_{\mathrm{ang}} (u; \theta_*)}\to 0
    \end{equation}
    in probability. Here, in-probability convergence is with respect to the randomness of $\{(x_{\ho, i}, y_{\ho, i})_{i=1}^{n_{\ho}}\}$. 
\end{theorem}
To be clear, the above theorem considers the asymptotics in the holdout set $n_{\ho}$ for a fixed sample size and dimension $n,d$ of the training dataset. We illustrate \Cref{plattjust} in \Cref{fig:asymp} (see \Cref{simulation} for detailed settings) where the solid red line plots our angular predictor $u\mapsto\hat{f}_{\mathrm{ang}}(u)$ defined in \eqref{definterp} and the dashed lines plot the predictor $u\mapsto \hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(u)$ calibrated by Platt scaling on increasingly large holdout sets $n_{\mathrm{ho}}$. We observe that the Platt scaling predictors indeed converge to our angular predictor as the holdout set sizes $n_{\mathrm{ho}}$ increase.



\section{Consistent angle estimation }\label{mest}
Observe that the angular predictor \(\hat{f}_{\mathrm{ang}}\) depends on the unobserved quantity \(\langle w_{\star}, \widehat{w} \rangle_{\Sigma}\). Using recent advancements from \Cref{bellec} we are able to provide a consistent estimator for this quantity. For simplicity, we outline the estimation procedure here for a twice-differentiable loss function $\ell$ and strongly convex, twice-differentiable penalty $g$; analogous results for unregularized M-estimation and other losses/penalties found in \cite{bellec2022observable}. We note that this result is part of a long line of development in observable estimation of unknown quantities in high dimensions  \cite{javanmard2018,bellec2022biasing,bellec2023debiasing,celentano2023lasso,li2023spectrum}.



A data driven estimator for $\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}^2$ proposed in \cite{bellec2022observable} is:
\begin{equation}\label{bellec}
    \hat{a}_{*}^2=\frac{\left(\frac{\hat{v}}{n}\|X \widehat{w}-\hat{\gamma} \hat{\psi}\|^{2}+\frac{1}{n} \hat{\psi}^{\top} X \widehat{w}-\hat{\gamma}\hat{r}^2\right)^{2}}{\frac{1}{n^{2}}\left\|\Sigma^{-\frac{1}{2}} X^{\top} \hat{\psi}\right\|^{2}+\frac{2 \hat{v}}{n} \hat{\psi}^{\top} X \widehat{w}+\frac{\hat{v}^{2}}{n}\|X \widehat{w}-\hat{\gamma} \hat{\psi}\|^{2}-\frac{d}{n}\hat{r}^2}
\end{equation}
where $\hat{\psi} \in \mathbb{R}^{n}$ is the vector with components $\hat{\psi}_{i}=-\ell_{i}^{\prime}\left(x_{i}^{\top} \widehat{w}\right)$, $\hat{v}=\frac{1}{n} \operatorname{Tr}\left(D-D X \hat{H} X^{T} D\right), \hat{\gamma}=$ $\operatorname{Tr}\left(X \hat{H} X^{\top} D\right)$ for $D=\operatorname{diag}\left(\ell_{y}^{\prime \prime}(X \widehat{w})\right)$ and $\hat{H}=\left(X^{\top} D X+n \nabla^{2} g(\widehat{w})\right)^{-1}$ and $ \hat{r}=(\frac{\norm{\hat{\psi}}^2}{n})^{1/2}$. It can be shown that $\abs{\hat{a}_*^2-\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}^2} \to 0$ in suitable high-dimensional sense. We refer the technical statement to \Cref{bellecthm} in \Cref{complecons}. 




To estimate $\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}$, we also need to estimate its sign. We require reserving a constant fraction $n_{\ho}=\alpha \cdot n$ of the $n$ training data for the sign estimator,
\begin{equation}\label{signest}
    \widehat{\operatorname{sgn}} := \operatorname{sign}\left( \sum_{i=1}^{n_{\mathrm{ho}}} \widehat{w}^{\top} x_{i}^{\mathrm{ho}} \cdot y_{i}^{\mathrm{ho}}\right).
\end{equation}
It can be shown that the probability of wrong sign identification using $\widehat{\operatorname{sgn}}$ decreases exponentially with $n_{\mathrm{ho}}$. We defer the proof to \Cref{signestsec}.
\begin{proposition}\label{consistThm}
Suppose that $\sigma^{\prime}(x)$ is well-defined and non-negative almost everywhere and $\sigma^{\prime}(x)>0$ on a set with non-zero Lebesgue measure. We then have for some absolute constant $c>0$,
$$
\mathbb{P}_{\mathrm{ho}}\left(\widehat{\operatorname{sgn}}=\operatorname{sign}\left(\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}\right)\right) \geq 1-2 \exp \left(-c n_{\mathrm{ho}}\left(\cos \left(\theta_{*}\right) \cdot \mathbb{E} \sigma^{\prime}(Z)\right)^{2}\right)
$$
where $Z\sim N(0,1)$ and $\mathbb{P}_{\mathrm{ho}}$ is with respect to the randomness in $\left(y_{i}^{\mathrm{ho}}, x_{i}^{\mathrm{ho}}\right)_{i=1}^{n_{\mathrm{ho}}}$. 
\end{proposition}








 Plugging \eqref{bellec} and \eqref{signest} into \eqref{angledef}, we have the following estimator $\hat{\theta}$ for $\theta_*$ in \eqref{angledef}
 \begin{equation}\label{angest}
    \hat{\theta}:=\arccos \qty(\norm{\what}_\Sigma^{-1} \widehat{\operatorname{sgn}} \cdot \sqrt{\hat{a}_*^2}).
\end{equation}
\Cref{thmconsis} below shows that $\hat{\theta}$ is consistent.
\begin{corollary}\label{thmconsis}
Under the assumption of Proposition \ref{consistThm} and \Cref{bellecthm}, as $n,d\to \infty$, we have that $|\hat{\theta}-\theta_*|\to 0$ in probability where $n_{\ho}=\alpha\cdot n$ for a fixed constant $\alpha>0$. 
\end{corollary}



\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{bar1.pdf}
    \includegraphics[width=0.49\linewidth]{bar2.pdf}
    \caption{Reliability plots for angular calibration and Platt scaling of a logistic ridge predictor. Left panel uses a small holdout set for Platt scaling with $n_{\ho}=100$; Right panel uses a large holdout set with $n_{\ho}=2000$. The plot is generated with Gaussian data with covariance $\Sigma=\frac{1}{d} \bar{\Sigma}$ where $\bar{\Sigma}_{kl}=0.5^{|k-l|}, \forall k,l \in \{1,...,d\}$, sigmoid link function in a data deficient setting where $n=1000, p=2000$. See \Cref{simulation} for more details.}
    \label{fig:combinedcalib}
\end{figure}


\section{Simulations}\label{simulation}
This section presents a simple simulation to demonstrate results in \Cref{mest}. We generate i.i.d.~samples $x_{i} \stackrel{\mathrm{ iid }}{\sim} N(0, \Sigma), i=1,...,n$ where $\Sigma=\frac{1}{d} \bar{\Sigma}$ and $\bar{\Sigma}_{kl}=0.5^{|k-l|}, \forall k,l \in \{1,...,d\}$; we also generate labels from \eqref{lab} with $\sigma(u)=\mathrm{sigmoid}(3u+1)=1/(1+\exp(-(3u+1)))$ and $w_\star \sim N(0, I_d)$ (normalized to $\norm{w_\star}_\Sigma=1$). We consider the case of ridge logistic regression with $
\ell_{y_i}(w^\top x)= -y_i \log(\hat{p}_w(x_i))-(1-y_i) \log(1-\hat{p}_w(x_i))
$ with $\hat{p}_w\left(x_i\right)=\frac{1}{1+\exp \left(-x_i w\right)}$ and $g(w)=\frac{\lambda}{2d} \norm{w}_2^2$ with $\lambda =0.5$. We assume that we are in a data deficient setting where $n=1000, p=2000$. 

The realizability plots in \Cref{fig:combinedcalib} are generated from a test set of size \(n_{\mathrm{test}}=20000\). To produce these plots, we bin the predicted probabilities for label 1 (on the x-axis) and then compute the average of the observed label within each bin (on the y-axis). Perfect calibration would align the binned points with the 45° line. In the left and right panels, Platt scaling is derived using holdout sets of \(n_{\mathrm{ho}}=100\) and \(n_{\mathrm{ho}}=20000\), respectively, whereas both the uncalibrated predictor and the angular predictor remain unchanged across the two panels.

From the reliability plots, we see that the uncalibrated predictor (blue) is poorly calibrated, while, as expected, the angular predictor (green) shows good calibration. Here, we have used the angle estimator \eqref{bellec} and the sign estimator \eqref{signest} to estimate the value of $\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}$. The estimated value for $\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}$ is 0.4356 while the true value is 0.4526. We also ran 5000 Monte Carlo trials where we found the probability of incorrect sign estimation to be 0.89\% with a holdout set of size $n_{\mathrm{ho}}=100$.

In contrast, the left panel of \Cref{fig:combinedcalib} shows that Platt scaling (orange) with a holdout set size of \( n_{\mathrm{ho}} = 100 \) fails to properly calibrate. However, when the holdout set size is increased to  \( n_{\mathrm{ho}} = 20000 \), Platt scaling also calibrates correctly. When \( n_{\mathrm{ho}} = 20000 \), the predictor calibrated from Platt scaling is found to be (using scikit-learn package's $\mathsf{CalibratedClassifierCV}$ routine \cite{scikit-learn})
\begin{equation*}
\hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(\widehat{w}^\top x_{\mathrm{new}})=\sigma (\hat{A}^{n_{\mathrm{ho}}}\cdot \widehat{w}^\top x_{\mathrm{new}}+\hat{B}^{n_{\mathrm{ho}}}), \text{ with } \hat{A}^{n_{\mathrm{ho}}}=0.3396, \hat{B}^{n_{\mathrm{ho}}}=-0.1521.
\end{equation*}
We now check if $\hat{A}^{n_{\mathrm{ho}}}$ and $\hat{B}^{n_{\mathrm{ho}}}$ are indeed close to $A_*, B_*$ as claimed in \Cref{plattjust}. Note that even though the link function is not strictly a probit function as required by \Cref{plattjust}, by approximating $\mathrm{sigmoid}(u)\approx \Phi(\sqrt{\pi/8}\cdot u)$, we have $\sigma(u)\approx \Phi(\sqrt{\pi/8}(3u+1))$. We then obtain $A_*=0.2991, B_*=-0.1597$ from \eqref{ABstardef} setting $a=3\sqrt{\pi/8}, b=\sqrt{\pi/8}$, which are indeed quite close to $\hat{A}^{n_{\mathrm{ho}}}=0.3396, \hat{B}^{n_{\mathrm{ho}}}=-0.1521$. 


\section{Limitations and future directions}\label{sec:conc}
We derived our theoretical results assuming the covariates to be Gaussian---although at first pass this might appear stylistic, recent universality results \cite{hu2022universality,liang2022precise,han2023universality,dudeja2024spectral,lahiry2024universality,montanari2022universality}
 demonstrate that these results should continue to hold as long as the covariates have sufficiently light tails. We demonstrate this with further experiments. In \Cref{fig:Rad} and \Cref{fig:Unif}, we reproduce \Cref{fig:asymp} and \ref{fig:combinedcalib} with non-Gaussian design matrices (iid Rademacher and uniform entries respectively) where we observe that our results remain robust. Formally establishing this by generalizing the leave-one-out and Stein's method arguments in \cite{hu2022universality,han2023universality,lahiry2024universality,montanari2022universality} would be an interesting avenue for future work. Additionally, we consider binary classification in this work---it would be interesting to extend our results to multi-class calibration \cite{shabat2020sample,jung2021moment} and more sophisticated models such as random feature models and two layer neural networks \cite{mei2022generalization,adlam2020neural,hu2022universality,clarte2022study}.


\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{bar1subG.pdf}
    \includegraphics[width=0.32\linewidth]{bar2subG.pdf}
    \includegraphics[width=0.315\linewidth]{approx_subG-2.pdf}
    \includegraphics[width=0.32\linewidth]{bar1subGRelu.pdf}
    \includegraphics[width=0.32\linewidth]{bar2subGRelu.pdf}
    \includegraphics[width=0.315\linewidth]{approx_subRelu.pdf}
    \caption{Reproduce \Cref{fig:asymp} (in the third column) and \ref{fig:combinedcalib} (in first two columns) for Rademacher entries. Upper Row: rerun simulations in \Cref{simulation} but with subGaussian designs $W\Sigma^{1/2}$ where $W_{ij}$ are sampled iid from Rademacher distribution, taking values $+1,-1$ with equal probability.  Bottom Row: we replace the sigmoid link function in \Cref{simulation} with a clipped relu link function $\sigma(x)=\mathrm{clip}(3x+0.5)$ where $\mathrm{clip}(x)=x,\forall x\in [0,1]$, $\mathrm{clip}(x)=0, \forall x<0$ and $\mathrm{clip}(x)=1,\forall x>1$. }
    \label{fig:Rad}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.32\linewidth]{bar1subGUnifsig.pdf}
    \includegraphics[width=0.32\linewidth]{bar2subGUnifsig.pdf}
    \includegraphics[width=0.315\linewidth]{approx_subGUnifsig.pdf}
    \includegraphics[width=0.32\linewidth]{bar1subGUnifrelu.pdf}
    \includegraphics[width=0.32\linewidth]{bar2subGUnifrelu.pdf}
    \includegraphics[width=0.315\linewidth]{approx_subGUnifRelu.pdf}
    \caption{Reproduce \Cref{fig:asymp} (the third column) and \ref{fig:combinedcalib} (first two columns) for uniform entries. Upper Row: rerun simulations in \Cref{simulation} but with non-Gaussian designs $W\Sigma^{1/2}$ where $W_{ij}$ are sampled iid from uniform distribution, taking values in interval $[-\sqrt{12}/2, \sqrt{12}/2]$ uniformly at random.  Bottom Row: we replace the sigmoid link function in \Cref{simulation} with a clipped relu link function $\sigma(x)=\mathrm{clip}(3x+0.5)$ where $\mathrm{clip}(x)=x,\forall x\in [0,1]$, $\mathrm{clip}(x)=0, \forall x<0$ and $\mathrm{clip}(x)=1,\forall x>1$. }
    \label{fig:Unif}
\end{figure}



\newpage
\appendix

\section{Proof of \Cref{mainThm}}\label{pfmainThm}
Before proving \Cref{mainThm}, we first show that $\hat{f}_{\mathrm{ang}}\left(\cdot; \theta_* \right) $ is exactly calibrated. 
\begin{theorem}\label{mainThmPop}
    The predictor $\hat{f}_{\mathrm{ang}}$ defined in \eqref{definterp} is well-calibrated at all $p \in[0,1]$ when . That is, for any $p\in [0,1]$ and any $d,n \in \mathbb{N}_{+}$
    $$\Delta_{p}^{\mathrm{cal }}\left(\hat{f}_{\mathrm{ang}}\left(\cdot; \theta_* \right) \right)=p-\mathbb{E}_{x_{\mathrm{new }}}\left[\sigma\left(w_{\star}^{\top} x_{\mathrm{new }}\right) \mid \hat{f}_{\mathrm{ang}}\left(\what^\top x_{\mathrm{new }}; \theta_*\right)=p\right]=0.$$
\end{theorem}

\begin{proof}[Proof of \Cref{mainThmPop}]
Let us define the following event
$$\mathcal{A}:=\qty{\hat{f}_{\mathrm{ang}}\left(w_\star^\top x_{\mathrm{new }}; \theta_*\right)=p}. $$
We have 
$$
\begin{aligned}
\mathbb{E}_{x_{\mathrm{new }}}&\left[\sigma\left(w_{\star}^{\top} x_{\mathrm{new }}\right) \mid \mathcal{A}\right] \\
&\stackrel{(i)}{=}\mathbb{E}_{x_{\mathrm{new }}}\left[\mathbb{E}_{x_{\mathrm{new }}}\left[\sigma\left(w_{\star}^{\top} x_{\mathrm{new }}\right) \mid x_{\mathrm{new }}^{\top} \widehat{w}\right] \mid \mathcal{A}\right] \\
&\stackrel{(ii)}{=}\mathbb{E}_{x_{\mathrm{new }}}\left[\mathbb{E}_{Z}\left[\sigma\left(\frac{1}{\|\widehat{w}\|_{\Sigma}} \cdot \cos \left(\theta_{*}\right) \cdot x_{\mathrm{new }}^{\top} \widehat{w}+\sin \left(\theta_{*}\right) \cdot Z\right) \right] \mid \mathcal{A}\right] \\
&=\mathbb{E}_{x_{\mathrm{new }}}\left[\hat{f}_{\mathrm{ang}}\left(w_\star^\top x_{\mathrm{new }} ; \theta_*\right) \mid \mathcal{A}\right]\\
& =p .
\end{aligned}
$$
where (i) follows from tower property of expectation and the fact that $\hat{f}_{\mathrm{ang}}\left(x_{\mathrm{new }}\right)$ depends on $x_{\mathrm{new}}$ only through $x_{\mathrm{new }}^{\top} \widehat{w}$, (ii) follows from conditional expectation of multivariate Gaussian distribution
\begin{equation}\label{conditionalid}
    \left[w_{\star}^{\top} x_{\mathrm{new }} \mid x_{\mathrm{new }}^{\top} \widehat{w}\right] \stackrel{L}{=} \frac{1}{\|\widehat{w}\|_{\Sigma}} \cdot \cos \left(\theta_{*}\right) \cdot x_{\mathrm{new }}^{\top} \widehat{w}+\sin \left(\theta_{*}\right) \cdot Z
\end{equation}
for some $Z\sim N(0,1)$. 
\end{proof} 



\begin{proof}[Proof of \Cref{mainThm}]
    Using result from \Cref{mainThmPop}, it suffices to show that as $|\hat{\theta}-\theta_*|\to 0$ in probability, we have that
    \begin{equation}\label{wsaf}
        \abs{\Delta_{p}^{\mathrm{cal }}\left(\hat{f}_{\mathrm{ang}}\left(\cdot; \theta_* \right) \right)- \Delta_{p}^{\mathrm{cal }}\left(\hat{f}_{\mathrm{ang}}\left(\cdot; \hat{\theta} \right) \right)}\to 0.
    \end{equation}
    
    Let us introduce the following notation for the ease of presentation:
    $$X:=\sigma(w_\star^\top x_{\mathrm{new}}), \quad \hat{Y}=\hat{f}_{\mathrm{ang}}\left(\what^\top x_{\mathrm{new }}; \hat{\theta}\right),\quad Y_* = \hat{f}_{\mathrm{ang}}\left(\what^\top x_{\mathrm{new }}; \theta_*\right).$$
    Then, we can write LHS of \eqref{wsaf} as 
    \begin{equation*}
\left|\mathbb{E}[X \mid \hat{Y}=p]-\mathbb{E}\left[X \mid Y_{\star}=p\right]\right|=\left|\frac{1}{f_{\widehat{Y}}(p)} \int_0^1 x f_{X, \hat{Y}}(x, p) \mathrm{d} x-\frac{1}{f_{Y_{\star}}(p)} \int_0^1 x f_{X, Y_{\star}}(x, p) \mathrm{d} x\right|
\end{equation*}
where $f_{\hat{Y}}, f_{Y_*}, f_{X, Y_{\star}}, f_{X, \hat{Y}}$ are the distribution density functions of $\hat{Y},Y_*$ and joint density functions of $(X, Y_{\star})$ and $(X, \hat{Y})$. We now show that the RHS of the above converges to 0. Firstly, $|\frac{1}{f_{Y_{\star}}(p)}-\frac{1}{f_{\hat{Y}}(p)}|\to 0$ because $|\hat{Y}-Y_\star|\to 0$ in probability (and thus in distribution) by continuous mapping theorem. Secondly, 
$$\left|\int_0^1 x f_{X, \hat{Y}}(x, p) \mathrm{d} x-\int_0^1 x f_{X, Y_{\star}}(x, p) \mathrm{d} x\right|\to 0$$
by bounded convergence theorem and the fact that $(X, \hat{Y})$ converges to $(X,Y_*)$ jointly. We conclude the proof.  
\end{proof}


\section{Proof of \Cref{optimalThm}}\label{proofoptimalThm}
We first state a result from \cite{banerjee2005optimality} for general random variables. 

\begin{proposition}[Theorem 1, \cite{banerjee2005optimality}]\label{bregprop}
Let $\phi: \mathbb{R}^d \mapsto \mathbb{R}$ be a strictly convex differentiable function, and let $D_\phi$ be the corresponding Bregman loss function. Let $X$ be an arbitrary random variable taking values in $\mathbb{R}^d$ for which both $\E[X]$ and $\E[\phi(X)]$ are finite. Then, among all functions of $Z$, the conditional expectation is the unique minimizer (up to a.s. equivalence) of the expected Bregman loss, i.e.,

\begin{equation*}
\arg \min _{Y \in \sigma(Z)} \E\left[D_\phi(X, Y)\right]=\E[X \mid Z] .
\end{equation*}
\end{proposition}

Using the above results, we show that angular calibration with $\hat{\theta}=\theta_*$ minimizes Bregman divergence to true label distribution among predictors of the form $F(\what^\top x_{\mathrm{new}})$. 

\begin{theorem}[Optimality of angular predictor]\label{optimalThmPop}
Let $\phi: \mathbb{R}^2 \mapsto \mathbb{R}$ be any strictly convex differentiable function, and let $D_\phi$ be the corresponding Bregman loss function. Let $\E_{x_\mathrm{new}}[\phi(q_\star)]$ be finite. Then, the expected Bregman loss $\E_{x_\mathrm{new}}\left[D_\phi(q_{ \star}, \hat{q}_{ F})\right]$ admits a unique minimizes (up to a.s. equivalence) among all $q_{ F}, \forall F \in \mathcal{F}:=\{f: \R \to [0,1]\}$. Let this minimizer be $F_\star = \arg \min _{F\in \mathcal{F}} \E_{x_\mathrm{new}}\left[D_\phi(q_{ \star}, \hat{q}_{ F})\right]$. We then have that almost surely
\begin{equation*}
\begin{aligned}
    \hat{q}_{\mathrm{ang}}(\theta_*)=F_*(\what^\top x_{\mathrm{new}})
\end{aligned}
\end{equation*}
where $\hat{q}_{\mathrm{ang}}(\theta_*)$ is the label prediction probability vector by angular calibration given in \eqref{probvec} with $\hat{\theta}$ replaced by $\theta_*$. 

\end{theorem}

\begin{proof}[Proof of \Cref{optimalThmPop}]
    Firstly, we set $X,Y,Z$ in \Cref{bregprop} as
    $$X\gets \mqty(\sigma(w_\star^\top x_\mathrm{new}) \\ 1-\sigma(w_\star^\top x_\mathrm{new})), \quad Y\gets \mqty(F(\widehat{w}^\top x_\mathrm{new}) \\ 1-F(\widehat{w}^\top x_\mathrm{new})), \quad Z\gets \widehat{w}^\top x_\mathrm{new}.$$
    The result then follows from \Cref{bregprop} and the following
    $$\E[X\mid Z]=\E \qty[\mqty(\sigma(w_\star^\top x_\mathrm{new}) \\ 1-\sigma(w_\star^\top x_\mathrm{new}))\mid x_{\mathrm{new }}^{\top} \widehat{w}]=\mqty(\hat{f}_{\mathrm{ang}}(x_\mathrm{new})\\ 1- \hat{f}_{\mathrm{ang}}(x_\mathrm{new}))$$
    where we used \eqref{conditionalid} and \eqref{definterp} for the last equality. 
\end{proof}


Now we are ready to state proof of \Cref{optimalThm}.
\begin{proof}
    Using result from \Cref{optimalThmPop}, it suffices to show that
    $$\norm{\hat{q}_{\mathrm{ang}} (\hat{\theta})-\hat{q}_{\mathrm{ang}} (\theta_*)}_2\to 0$$
    in probability. This is an immediate consequence of the continuous mapping theorem under the assumption $\sigma$ is continuous. 
\end{proof}



\section{Proof of \Cref{plattjust}} \label{proofplattjust}
Before proving \Cref{plattjust}, we first state two classic analysis results that we will later use. 

\begin{proposition}[Theorem 5.7, \cite{van2000asymptotic}]\label{VDV}
Let $\ell_n$ be random functions on $\mathcal{H}$, $\ell^{\star}$ be a fixed function on $\mathcal{H}$, and $\theta^{\star} \in \mathcal{H}$ such that (i) uniform convergence of $n^{-1} \ell_n$ to $\ell^{\star}$ holds: \begin{equation*}
\sup _{\theta \in \mathcal{H}}\left|\frac{1}{n} \ell_n(\theta)-\ell^{\star}(\theta)\right| \xrightarrow[n \rightarrow \infty]{\mathrm{in prob.}} 0,
\end{equation*} (ii) the mode of $\ell^{\star}$ is well-separated, i.e for all $\varepsilon>0$,
\begin{equation*}
\sup _{\theta \in \mathcal{H}: d\left(\theta, \theta^{\star}\right) \geq \varepsilon} \ell^{\star}(\theta)<\ell^{\star}\left(\theta^{\star}\right)
\end{equation*}
Then any sequence $\hat{\theta}_n$ maximizing $\ell_n$ converges in probability to $\theta^{\star}$.
\end{proposition}

\begin{proposition}[Theorem 10.8, \cite{rockafellar2015convex}]\label{rock}
Let $C$ be a relatively open convex set, and let $f_1, f_2, \ldots$, be a sequence of finite convex functions on $C$. Suppose that the sequence converges pointwise on a dense subset of $C$, i.e. that there exists a subset $C^{\prime}$ of $C$ such that its closure satisfies $\mathrm{cl} C^{\prime} \supset C$ and, for each $x \in C^{\prime}$, the limit of $f_1(x)$, $f_2(x), \ldots$, exists and is finite. The limit then exists for every $x \in C$, and the function $f$, where
\begin{equation*}
f(x)=\lim _{i \rightarrow \infty} f_i(x)
\end{equation*}
is finite and convex on $C$. Moreover the sequence $f_1, f_2, \ldots$, converges to $f$ uniformly on each closed bounded subset of $C$.
\end{proposition}

Now we are ready to prove \Cref{plattjust}. 
\begin{proof}[Proof of \Cref{plattjust}]
\eqref{correctform} is obtained from applying the well-known identity below for probit function $\Phi(\cdot)$
$$\mathbb{E} \Phi(\mu+\sigma \cdot Z)=\Phi\left(\frac{\mu}{\sqrt{1+\sigma^2}}\right), \qquad Z\sim N(0,1) $$ 
to \eqref{definterp}. 

To prove that $\hat{A}^{n_{\mathrm{ho}}} \to A_*, \hat{B}^{n_{\mathrm{ho}}} \to B_*$ as $n_{\mathrm{ho}}\to \infty$, we would like to apply \eqref{VDV} by setting $n\gets n_{\ho}, \theta \gets \{A,B\}, \theta^\star \gets \{A_*, B_*\}$,
\begin{equation}\label{samp}
    \ell_n(\theta) \gets \ell_{n_{\ho}}(A,B):= \sum_{i=1}^{n_{\ho}}-y_{\ho,i}\log(F_{A,B}(\widehat{w}^\top x_{\ho, i}))-(1-y_{\ho,i})\log(1-F_{A,B}(\widehat{w}^\top x_{\ho, i})),
\end{equation}
and
\begin{equation}\label{CEpop}
    \begin{aligned}
    \ell^\star(\theta) \gets \ell^\star(A,B):=
& \mathbb{E}_{x_{\text {new }}}\bigg[-\sigma\left(w_{\star}^{\top} x_{\text {new }}\right) \log \left(F_{A, B}\left(\widehat{w}^{\top} x_{\text {new }}\right)\right)\\
    &\qquad \qquad -\left(1-\sigma\left(w_{\star}^{\top} x_{\text {new }}\right)\right) \log \left(1-F_{A, B}\left(\widehat{w}^{\top} x_{\text {new }}\right)\right)\bigg].
\end{aligned}
\end{equation}
where we used the notation
$$F_{A,B}(u):=\sigma(Au+B)=\Phi(a(Au+B)+b).$$

Here, RHS of \eqref{CEpop} is up to an affine transform of the KL divergence \eqref{poploss} and, therefore, it follows from \eqref{correctform} and \eqref{optsol} that its minimizer is indeed $A_*, B_*$. 

To verify condition (i) of \Cref{VDV}, we first note that \eqref{samp} converges to \eqref{CEpop} point-wise in probability following from law of large number theorem. Furthermore, we note that the functions
$$f_1(u):= -\log(\Phi(u)), \qquad f_2(u):= -\log(1-\Phi(u))$$
are both strictly convex functions. To see this, one can show second derivatives are positive for all $u\in \R$ by utilizing the two elementary inequalities: $u \Phi(u)+\Phi'(u)>0$, $ u \Phi(u)+\Phi'(u)>u$. It then follows that $\ell_{n_{\ho}}(A,B)$ is a convex function of $(A,B)$ on the domain $\R^2$. Uniform convergence on $\mathcal{H}$ is then an application of \Cref{rock}.

To verify condition (ii) of \Cref{VDV}, we only need to show that $\ell^\star(A,B)$ is a strictly convex function in $(A,B)$. Let us write the inside of the expectation of \eqref{CEpop} as
$$f(A,B)=\sigma(H)f_1(aA\cdot H+aB+b)+(1-\sigma(H))f_2(aA\cdot H+aB+b)$$
where $H:=w_\star^\top x_{\mathrm{new}}\sim N(0,1)$. Then, we have that
$$\nabla^2 f(A,B)=\big(\sigma(H)\cdot f_1^{\prime \prime}(aA\cdot H+aB+b) \cdot +(1-\sigma(H)) f_2^{\prime \prime}(aA\cdot H+aB+b)\big) \cdot \mqty(a^2 H^2 & a^2 H\\a^2 H & a^2)$$
which is positive-definite almost surely when $a\neq 0$. Hence, we have that almost surely
$$f\left(t (A_1, B_1)+(1-t) (A_1, B_1)\right)<t f\left((A_1, B_1)\right)+(1-t) f\left((A_1, B_1)\right)$$
which implies that
$$\mathbb{E}_{x_{\text {new }}} f\left(t (A_1, B_1)+(1-t) (A_1, B_1)\right)<t\mathbb{E}_{x_{\text {new }}} f\left((A_1, B_1)\right)+(1-t) \mathbb{E}_{x_{\text {new }}}f\left((A_1, B_1)\right).$$
The claim that $\ell^\star(A,B)$ is strictly convex follows. 

It then follows from \Cref{VDV} that $\hat{A}^{n_{\mathrm{ho}}} \to A_*, \hat{B}^{n_{\mathrm{ho}}} \to B_*$ in probability as $n_{\mathrm{ho}}\to \infty$. The uniform convergence $\hat{f}_{\mathrm{platt}}^{n_{\mathrm{ho}}}(u) \rightarrow \hat{f}_{\mathrm{ang}}(u)$ follows immediately. 





\end{proof}

\section{Inner product estimation}\label{complecons}
We restate the following results from Theorem 4.4, \cite{bellec2022observable}. We note that the quantity $\frac{\hat{r}^{4}}{\hat{v}^{2} \hat{t}^{2}}$ in the error bound is observable and is typically of constant order in the proportional regime. See \cite{bellec2022observable} for details. 
\begin{theorem}\label{bellecthm}
Suppose $\ell$ is continuously differentiable and $g$ is strongly convex and twice differential penalty function. Assume also that $\frac{1}{2 \delta} \leq \frac{d}{n} \leq \frac{1}{\delta}$ for some $\delta>0$, for arbitrarily large probability $1-\delta$, the following holds
$$
\E \left|\hat{a}_{*}^2-\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}^2\right| \leq \frac{C\hat{r}^{4}}{\hat{v}^{2} \hat{t}^{2}} \cdot n^{-1 / 2}
$$
where $C$ is a constant depending only on $g$ and $\delta$. 
\end{theorem}


\section{Sign estimation}\label{signestsec}

\begin{proof}[Proof of \Cref{consistThm}]
    With respect to randomness in validation dataset (that is, $\widehat{w}$ is treated as deterministic), we have that $\widehat{w}^{\top} x_{i}^{\mathrm{ho}} \cdot y_{i}^{\mathrm{ho}}$ are iid across $i$ and satisfies that
$$
\begin{gathered}
\widehat{w}^{\top} x_{i}^{\mathrm{ho}} \cdot y_{i}^{\mathrm{ho}} \stackrel{L}{=} \widehat{w}^{\top} x_{i}^{\mathrm{ho}} \cdot \operatorname{Bern}_{i}\left(\sigma\left(\frac{\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}}{\widehat{w}^{\top} \Sigma \widehat{w}} \widehat{w}^{\top} x_{i}^{\mathrm{ho}}+\sin \left(\theta_{*}\right) \cdot Z_{i}\right)\right) \\
\qquad \stackrel{L}{=}\|\widehat{w}\|_{\Sigma} U_{i} \cdot \operatorname{Bern}_{i}\left(\sigma\left(\cos \left(\theta_{*}\right) U_{i}+\sin \left(\theta_{*}\right) \cdot Z_{i}\right)\right)=H_{i}
\end{gathered}
$$
where $Z_{i} \stackrel{i i d}{\sim} N(0,1)$ and we have used $w^{\top} x_{i}^{\mathrm{ho}} \stackrel{L}{=}\|\widehat{w}\|_{\Sigma} \cdot U_{i}$ for $U_{i} \stackrel{\mathrm{ iid }}{\sim} N(0,1)$. It follows from Gaussian integration by parts that
$$
\mathbb{E} H_{i}=\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \cdot \mathbb{E} \sigma^{\prime}\left(\cos \left(\theta_{*}\right) U_{i}+\sin \left(\theta_{*}\right) \cdot Z_{i}\right)=\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z).
$$
Meanwhile, $H_{i}$ is subGaussian with subGaussian norm
$$
\left\|H_{i}\right\|_{\psi_{2}}^{2} \leq\|\widehat{w}\|_{\Sigma}^{2}\left\|U_{i}\right\|_{\psi_{2}}^{2} \leq 3\|\widehat{w}\|_{\Sigma}^{2}.
$$
By theorem assumption, $\mathbb{E} \sigma^{\prime}(Z)>0$ and
$$
\operatorname{sign}\left(\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z)\right)=\operatorname{sign}\left(\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}\right)
$$
So the sign identification of $\widehat{\mathrm{sgn}}$ is correct if the following event holds
$$
\left|\frac{1}{n_{\mathrm{ho}}} \sum_{i=1}^{n_{\mathrm{ho}}} w^{\top} x_{i} \cdot y_{i}-\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z)\right|<\left|\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z)\right|.
$$
By Hoeffding's inequality,
$$
\begin{aligned}
\mathbb{P}_{\mathrm{ho}}&\left(\left|\frac{1}{n_{\mathrm{ho}}} \sum_{i=1}^{n_{\mathrm{ho}}} w^{\top} x_{i} \cdot y_{i}-\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z)\right|>\left|\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma} \mathbb{E} \sigma^{\prime}(Z)\right|\right) \\
& \leq 2 \exp \left(-\frac{c n_{\mathrm{ho}}\left(\mathbb{E} \sigma^{\prime}(Z)\right)^{2}\left\langle w_{\star}, \widehat{w}\right\rangle_{\Sigma}^{2}}{\|\widehat{w}\|_{\Sigma}^{2}}\right)=2 \exp \left(-c n_{\mathrm{ho}}\left(\cos \left(\theta_{*}\right) \cdot \mathbb{E} \sigma^{\prime}(Z)\right)^{2}\right).
\end{aligned}
$$
The theorem statement follows. 
\end{proof}


\newpage
\bibliographystyle{plain}
\bibliography{paper-ref}

\end{document}
