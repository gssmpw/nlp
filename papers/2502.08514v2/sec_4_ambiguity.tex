\section{Defining and Annotating Ambiguity}\label{sec:ambiguity}
\input{tables/tab4.0_coarse-ambiguity}
% \mk{need to come up with a better title for this section}
Faithfulness evaluation is usually done with a major underlying assumption: the summaries can \textsc{always} be definitely classified as either faithful or with some faithfulness errors. However this might not always be true. A summary can be interpreted in different ways, all plausible but where one interpretation can make the summary faithful whereas with a different interpretation, one might consider the summary as unfaithful. Given the example in Figure \ref{fig:ambiguity-motiv}, depending on how one would parse the summary, two interpretations can emerge; the first one would make the summary faithful but the second one would give the unfaithfulness perception.

We therefore define the notion of \textit{ambiguity} as follows:
an ambiguous summary can be correctly interpreted in multiple ways given the source document, leading to different faithfulness judgments depending on the underlying assumptions.
% 
The first point about this definition is that we define ambiguity \textit{in context of the given document}. That means a summary is considered ambiguous if it can have different interpretations with respect to the source document and not on its own.
The first point is necessary but not sufficient. The sufficient condition is stated in the second point of the definition which specifies that different interpretations should lead to \textit{different faithfulness judgments} for a summary to be considered ambiguous. 
% 
We argue that this ambiguity dimension plays a critical role in our understandings of faithfulness of the generated summaries and believe that it should be addressed before evaluating the summaries faithfulness so that evaluators would not be penalized solely based on the subjectivity of their interpretations. 
An ideal faithfulness evaluation framework should hence include an ambiguity detection module to filter out the ambiguous cases and perform faithfulness evaluation on non-ambiguous instances only (we depict an example under our multi-agent debate framework in the appendix Figure~\ref{fig:overview_w_ambiguity}).

To better help with identifying ambiguous cases as defined above, we first introduce a detailed taxonomy of such cases along with the definitions and examples of each category in Section \ref{sec:taxonomy}. Then, in a first attempt to identify ambiguous cases in summaries, we extend TofuEval MeetingBank \cite{tang2024tofueval} with ambiguity human annotations and present the details in Section \ref{sec:data_annotation}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/ambiguity2.pdf}
    \vspace*{-0.7cm}
    \caption{A summary with structural ambiguity which can be interpreted using two different parses where interpretation with Parse 1 makes the summary faithful whereas Parse 2 makes the summary unfaithful.}
    \label{fig:ambiguity-motiv}
    \vspace*{-0.4cm}
\end{figure}
\subsection{Ambiguity Taxonomy}\label{sec:taxonomy}
We used our definition of ambiguity as stated above and tried to classify the ambiguities into respective categories. We have looked into possible causes of ambiguity and come up with a fine-grained taxonomy that consists of $16$ different ambiguity types. 
On a coarse level, ambiguities can be grouped into three main categories summarized as follows (see examples in Table \ref{tab:coarse-ambiguity}):
%\textit{implicit reasoning phenomena}, \textit{meaning phenomena} and \textit{context phenomena} as shown in Table \ref{tab:coarse-ambiguity}. 

\noindent\textbf{Implicit reasoning phenomena.} This category refers to summary instances containing some type of implicit reasoning that can not be directly traced back to the document which would lead to difficulty in evaluating the summary faithfulness. The main sub-categories are deduction and inference.

\noindent\textbf{Meaning phenomena.}
This includes cases where there are multiple meanings associated with the summary which makes it ambiguous. The meaning phenomena can cover different semantic relations, linguistic ambiguity or vagueness. 

\noindent\textbf{Context phenomena.}
This category deals with summaries that are ambiguous as a result of challenges of representing the information of the source document as part of the summary. It includes decontextualization and conflation as the two main sub-categories.

The full taxonomy with fine-grained types and definitions and also a complete list of examples of each category can be found in Appendix \ref{app:taxonomy}.


% The proposed taxonomy was curated based on the data annotation that 

% \input{tables/tab4:ambiguity_taxonomy}

\subsection{Data Annotation}\label{sec:data_annotation}
Our ambiguity benchmark is constructed on top of the TofuEval MeetingBank \cite{tang2024tofueval} faithfulness dataset. 
Professional linguists as annotators are given a detailed instructions of the task (Appendix \ref{app:ambiguity-data-stats}), its goal and the desired output. 
% 
Next, they are provided with the document and the summary sentence and are asked to identify whether there is an ambiguity in the summary that would affect its evaluability and if so, what is the best category to describe the ambiguity using the fine-grained taxonomy in Table \ref{tab:ambiguity2}. They are also asked to write a description of the evaluability issue within the summary sentence.



Due to the inherent difficulty of the task and to ensure high inter-annotator agreement, we performed a final step to finalize the ambiguity annotations. For each instance, two experts (well-familiar with the taxonomy and the task) went over the responses by both annotators and made the final call on whether there is an ambiguity or not and if so, picked the best category from the taxonomy. 
The data statistics is shown in Table \ref{tab:ambiguity_stats}. The final dataset has an inter-annotator agreement (Cohen's Kappa) of $\approx0.73$ on binary labels.
More on IAA and the distribution of fine-grained sub-categories can be found in Appendix \ref{app:ambiguity-data-stats} and Table \ref{tab:ambiguity_stats_fine}.

% (with the initial stage having a value of $\approx0.40$) which highlights the importance of the adjudication step to achieve high-quality data.
% \sm{TODO  pull this table into main document and add relevant stats, IAA, fine/coars category distribution? other?}.

%\mk{more statistics on the annotated data will be added once the annotation effort is complete.}

\subsection{Ambiguity Detection}
Ambiguities as described earlier can lead to different assessments of faithfulness and should be addressed before evaluating the summaries so that models would not be penalized solely based on the subjectivity of the interpretations.
But how can we identify such ambiguities?
We propose an ambiguity detection approach based on \method, in which an ambiguity detector model would make a judgment call based on the arguments generated during debate. 
Formally, an ambiguity detector model predicts whether a summary sentence is ambiguous or not given the source document.
\[M_{a}(D,s,A,t) \in \text{\{ambiguous, non-ambiguous\}}\]
%
Where $D$ is the source document, $s$ is the summary sentence, $A$ is the arguments from agents involved in faithfulness evaluation in \method and $t$ is our proposed ambiguity taxonomy in Section \ref{sec:taxonomy}.
% 
The overview of the full faithfulness evaluation pipeline with ambiguity detection module is shown in Figure \ref{fig:overview_w_ambiguity}. 
%
Evaluator agents start with opposing views on the faithfulness of the summary and try to come up with arguments to support their decisions in multiple rounds of debate. Agents with different stances can have plausible arguments for their decisions showing the possibility of an inherent ambiguity in the summary. Therefore, our proposed ambiguity detection approach makes use of the generated arguments and check their plausibility to help with understanding the ambiguities as follows: if there are sound arguments both supporting the faithfulness of the summary as well as some sound arguments arguing for the unfaithfulness of the summary sentence, the summary will be deemed ambiguous by the ambiguity detector module.
% 
We later show how the presence of agents debate arguments can help with better identifying existing ambiguities in the summaries.

\input{tables/tab8_ambiguity_stats}