% \section{Appendix}
\section{Multi-agent Debate Approach Details}
The following sections describe more details of our proposed approach.

\subsection{Multi-round Debate}
The multi-round debate stage of \method is \textit{guideline-based} and will be stopped it meets the \textit{stopping criterion}. We describe guidelines that are used during the debate and the stopping conditions below.

\subsubsection{Guidelines}\label{app:guidelines}
LLMs have their own interpretations of concepts and similar to human evaluators might mix their perception with what is actually considered correct or plausible \cite{elangovan2024considersthehumanevaluationframeworkrethinking} which might be different from specific needs and requirements of certain tasks. Guidelines or rules can be established to clearly specify the dos and don’ts of the evaluation process such that evaluators can base their judgment on them and can easily refer to them during discussion leading to discussion efficiency \cite{chen2019cicero}. Guidelines can encourage a more structured debate and arguments referring to the guidelines can be verified based on whether the guidelines are used correctly or not. 

Guidelines can be generated manually and provided as part of the prompt. However, it might be difficult to come up with a comprehensive set of desirable guidelines at once and prior to the evaluation. 
Instead we can apply an alternative semi-automatic (possibly automatic) approach to generate guidelines in a learning phase using the following procedure. We start with a small subset of the annotated data (both positive and negative from dev sets) and use our debate approach for the evaluation with a minor tweak. We explicitly ask agents to provide the guidelines they have used to make their judgments and collect them. Agents might be either correct or wrong in their final judgements on whether the summary is faithful or not. If an agent is correct, the guidelines provided by it will be placed in the list of potential guidelines and if it is incorrect, the negated guidelines will be added to the pool. This process is done incrementally, meaning that after each evaluation the guidelines are updated and provided to the agents. Once, no more new guidelines are added to the pool (after a certain number of repetitions), the learning phase is stopped and the full set of guidelines will be curated for future evaluations.

Figure \ref{fig:guidelines} shows some of the generated guidelines during the learning phase. Some of these guidelines lead to correct label prediction whereas the other ones can result in an incorrect prediction. The later group should be negated and provided to the agents for future predictions.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/guidelines.jpg}
    \caption{Guidelines generated during learning phase.}
    \label{fig:guidelines}
\end{figure*}

\subsubsection{Stopping Criterion}\label{app:stop}
At any debate round $r_j$ if agents reach consensus and \textbf{\textit{all}} agree on the faithfulness label, the debate would be stopped and label $l$ would be assigned to the summary. 
However, it might be the case that even after rounds of debate, there would still be disagreement among agents. In such cases, once the debate reaches its predefined maximum number of rounds $n$, the debate will be stopped and the final decision would be made in the adjudication step.

If after multiple rounds of discussions, the agents still disagree, an intervention happens and agents are encouraged to be more open to accept each other’s opinion. This can be done by either updating the description of the task that has been assigned to them or through specifying a new goal.

Finally, after a fixed number of rounds, the debate will be stopped and the final decision would be made in the adjudication step.


\subsection{Evaluation Metrics}\label{app:metrics}
\subsubsection{Balanced Accuracy}
Following previous works, we evaluate the performance of our evaluation approach using Balanced Accuracy (BAcc). This metric takes into account the imbalance of consistent and inconsistent summaries with respect to the evaluation dimension over the test instances. 

\[ BAcc = 1 - 1/2(FPR+FNR)\]
where  $FPR=FP/(FP+TN)$ and $FNR=FN/(FN+TP)$.
FPR indicates the rate at which an evaluator incorrectly predicts that a summary sentence contains an error when it is actually correct and FNR represents the rate at which an evaluator incorrectly predicts that a summary sentence is correct when it actually contains an error.

Generally, positive shows there is a faithfulness error in the summary while negative means there is no error in the summary.   More specifically:
FP: instances where the ground truth label for the summary is 1 (faithful) but the predicted label is 0 (unfaithful)
FN: instances where the ground truth label for the summary is 0 (faithful) but the predicted label is 1 (unfaithful)


\subsubsection{Krippendorff alpha}
A good evaluator not only has to achieve high accuracy but it also has to be well-aligned with human annotations by scoring higher IAA. Hence, to measure this alignment, we use Krippendorff alpha (K-alpha) \cite{krippendorff2011computing} to measure the correlation between system and human evaluations. 

\subsection{Dataset Statistics}\label{app:data-stats}
We have used TofuEval \cite{tang2024tofueval} and AggreFact \cite{tang-etal-2023-understanding} with diaolgue and news domain instances respectively.
The statistics of the datasets we have used for our evaluations are presented in Table \ref{tab:dataset}. We report both the number of individual sentences as well as full summaries for TofuEval as we report results both on sentence-level and summary-level evaluation.
\input{tables/tab0_dataset}

\section{Ambiguity}\label{app:ambiguity}
An ideal faithfulness evaluation system should handle ambiguities first. This can be done by identifying the ambiguous summaries and filtering them out and then evaluating the non-ambiguous summaries. 
The overall view of a faithfulness evaluator with the ambiguity detection module is shown in Figure \ref{fig:overview_w_ambiguity}.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figs/overview_w_ambiguity.jpg}
    \caption{Faithfulness evaluator with ambiguity detection module.}
    \label{fig:overview_w_ambiguity}
\end{figure*}

A summarizer can additionally be evaluated on ambiguity dimension and be provided with feedback to avoid generating ambiguous summaries. This can be seen as a future direction and is out of scope of this work.

\subsection{Ambiguity Taxonomy}\label{app:taxonomy}
We provide a detailed taxonomy of ambiguities based on our definition of ambiguity in summaries which consists of $3$ main categories and overall $16$ fine-grained sub-categories to help with ambiguity detection. 
The detailed proposed ambiguity taxonomy with definitions and examples is presented in Table \ref{tab:ambiguity2}.
\input{tables/tab4_ambiguity_taxonomy}
\input{tables/tab4_ambiguity_taxonomy2}

\input{ambiguity_examples_tables/1.deduction_example}
\input{ambiguity_examples_tables/2.commonsense_example}
\input{ambiguity_examples_tables/3.value_based_example}
\input{ambiguity_examples_tables/4.other_reasoning_example}
\input{ambiguity_examples_tables/5.hypernymy_gen}
\input{ambiguity_examples_tables/6.hyponymy_example}
\input{ambiguity_examples_tables/7.synonymy_example}
\input{ambiguity_examples_tables/8.structural_example}
\input{ambiguity_examples_tables/9.lexical_example}
\input{ambiguity_examples_tables/10.vagueness_example}
\input{ambiguity_examples_tables/11.non_assertion_example}
\input{ambiguity_examples_tables/12.other_meaning_example}
\input{ambiguity_examples_tables/13.decontextualization_example}
\input{ambiguity_examples_tables/14.conflation_example}
\input{ambiguity_examples_tables/15.other_context_example}
\input{ambiguity_examples_tables/16.other_example}

\subsection{Data Annotation for Ambiguity}\label{app:ambiguity-data-stats}
We used an existing faithfulness dataset, TofuEval MeetingBank \cite{tang2024tofueval} and annotated the sentence summaries for ambiguity.
The instructions provided to the expert annotators are presented in Table \ref{tab:annotator_intsructs}.
\input{tables/tab9_annotation-instructions}

The fine-grained data statistics is shown in Table \ref{tab:ambiguity_stats_fine}. The final dataset has a high inter-annotator agreement (Cohen's Kappa) of $\approx0.73$ on binary labels.  
The initial stage (before the adjudication step) has an IAA of $\approx0.40$ which highlights the importance of the adjudication step to achieve high-quality data.
% \input{plots/plot:ambiguity_stats}

% The statistics of the annotated MeetingBank data for ambiguity is presented in Table \ref{tab:ambiguity_stats}.
\input{tables/tab8.1_ambiguity_stats_fine}

\section{Prompts}\label{app:prompts}
We listed all the prompts we have used for our experiments in Table \ref{tab:prompt_lists}.
\input{prompt_tables/tab0_list}
\input{prompt_tables/tab1_direct_prompt}
\input{prompt_tables/tab2_cot}
\input{prompt_tables/tab3.0_debate_first_round}
\input{prompt_tables/tab3_debate}
\input{prompt_tables/tab4_adjudicator}
\input{prompt_tables/tab5_ambiguity_baseline}
\input{prompt_tables/tab6_ambiguity_w_args}


% \section{Evaluation}
% \subsection{\method with simultaneous debates}\label{app:method}
% In this setup, instead of having a single debate session with evaluator agents, we start $m$ separate simultaneous sessions, each with the same number of agents ($4$ in our experiments). The sessions will continue independently to reach a final label. Once all sessions are over, the final label can be generated by aggregation over either the sessions conclusion or vote over individual responses.

% This setup can be seen as having more evaluator agents to perform the same task, except that since the sessions are independent, if there is an error propagation in one of the sessions, it will only affect the output of that session which would hopefully not affect the overall performance.
% Also, having more agents can increase the context size (specially in the final rounds) which given the context size of an LLM might not be feasible.


\section{Additional Results}\label{app:result}
\subsection{\method shows similar trends on sentence-level summaries.}\label{app:full_results}
Table \ref{tab:full} presents a mix of finetuned and LLM-based evaluators along with our debate variants both on sentence-level and summary-level faithfulness evaluation.
\input{tables/tab7_full_results_app}

\subsection{Initialization distribution effect on \method }\label{app:init}
We use a uniform distribution to assign initial stances to the evaluator agents. 
The reason behind doing so is that
since we the instances are random (without knowing what the correct label is) and to have a fair debate without one stance being stronger than the other (by having more agents start with that stance), we decided to have the same number of agents pro each stance.
\input{tables/tab10_number_of_agents}
We performed an analysis on how changing the balance of evaluators can affect the performance of \method in Table \ref{tab:number_of_agents}. 
As can be seen in Table \ref{tab:number_of_agents}, the uniform distribution performs the best. Having more agents with initial belief that the summary is unfaithful will result in the lowest FNR but highest FPR as this setup tends to identify more errors. On the other hand, more positive agents fail to identify errors (similar to the setup without initialization). 


\subsection{\method can improve FNR.}\label{app:results:fpr-fnr}
We compare the FPR and FNR of different approaches in Table \ref{tab:llama-main-fpr-fnr} and show the decrease in FNR using the debate approach.
\input{tables/tab1.1_main-fpr-fnr}
The debate approach can help with identifying more errors as shown by lower FNR in Table \ref{tab:llama-main-fpr-fnr}.  
However, since the debate approach is more sensitive to the errors, the FPR is also increased. 

There are a few hypothesis to describe this phenomena. First, the initialization would increase the evaluator sensitivity to the potential errors which could lead to labeling cases as erroneous for some superficial reasons as ``lack of context'' or ``omission of details''. One way to resolve this issue is to further curate the guidelines that are given to the evaluators during the debate.
% The second reason of high FPR can be the annotation errors in the datasets. It is usually more difficult to identify errors for human annotators and it might be the case that some of the errors were unnoticed by human annotators specially given the low inter-annotator agreement for these datasets. Therefore the higher FPR for the debate approach can be the result of identifying more errors that are labeled as correct in the data. \mk{a manual analysis of such cases and report the results}
Another cause for this increase is the ambiguities in the summaries that would lead to disagreement between human judgments and model judgments. As discussed earlier, ambiguity can be a major source of disagreement on faithfulness evaluation and has to be dealt with before faithfulness evaluation.  
We later show (\ref{app:results:fpr-fnr-filtered}) that filtering ambiguous cases would lower the gap in terms of FPR between the debate approach and other baseline settings.

%% REMOVING GPT-4O-MINI RESULTS
\subsection{\method is orthogonal to the underlying LLM.}\label{app:results:gpt4}
The comparison of \method with other baselines using GPT-4o-mini as the underlying LLM is shown in Table \ref{tab:gpt-main}.
\input{tables/tab2_gpt_main}
Though self-consistency on XSum dataset is the highest performing baseline, its performance is not even close to any debate settings for other datasets in Table \ref{tab:gpt-main}.

\subsection{\method works for smaller LLMs as well}\label{app:results:llama-small}
We also show in Table \ref{tab:llama-small-main} that \method can be superior to baselines even when a smaller size LLM is used.
Even though the debate setup for a smaller-size LLM does not reach the larger LLM performance in Table \ref{tab:llama-main}, but it can beat any other single LLM-based approaches using the larger LLM.
\input{tables/tab3_llama_small}

% \subsection{Debate approach can perform better even for sentence-level summary evaluation.}
% A fuller set of results with the addition of results of finetuned specialized fact checkers and the sentence-level summaries of TofuEval are presented along with our Llama3-70B variations in Table \ref{tab:full}.

\subsection{Ambiguity filtering can help with balancing FPR and FNR.}\label{app:results:fpr-fnr-filtered}
We previously observed that with \method we have lower FNR rate however, the FPR is also increased. 
Once the ambiguous cases are filtered, we can see the decrease in FPR as well. This further suggests that our assumption on how the ambiguous cases can lead to higher FPR is true.
Figure \ref{plot:ambiguity-filtering-fpr-fnr} shows the decline in both FPR and FNR. The FPR gap between the debate approach and different setups is lower once ambiguous cases are filtered.
\input{plots/plot_ambiguity-filtering-fpr-fnr}
% Our primary goal is to show how the multi-agent debate approach can outperform the baselines through initialization and debate orthogonal to the underlying LLM, so we have not tried benchmarking all exiting LLMs to find the best performing ones.