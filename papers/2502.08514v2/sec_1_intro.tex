\section{Introduction}
Summary evaluation has a long history, and over the years, different approaches have been applied to evaluate the quality of the generated summaries including n-gram based metrics \cite{lin2004rouge,papineni2002bleu}, representation-based approaches \cite{zhang2020bertscoreevaluatingtextgeneration}, finetuned specialized evaluators \cite{kryscinski2020evaluating,fabbri2022qafacteval,goyal2020evaluating,clark2023seahorse,tang2024minicheck} and human evaluation. 
With recent advancements in LLMs and their superior ability to generate fluent text, automatic summary evaluation has gained even more attention. In particular, assessing aspects like faithfulness has become more challenging due to the high fluency of LLM-generated text. 

While overlap-based metrics usually show weak correlation with human judgments \cite{liu2023g,tang2024tofueval} and finetuned approaches usually lack explainability, human evaluation is also costly with high turnaround time, low reproduciblity and low inter annotator agreement (IAA). With that said, efficient and accurate evaluation of summaries still remains a challenge. 

Automatic evaluation using LLMs have shown promising results, overcoming some of the major bottlenecks of traditional approaches in efficient evaluation of the generated summaries.
Different single-LLM and multi-LLM settings have been applied on a wide range of tasks and are shown to be strong automatic evaluators \cite{liu2023g,luo2023chatgptfactualinconsistencyevaluator,wang2023factcheck,chan2023chateval,song2024finesure}. 
% 
But even LLMs as evaluators fail to identify a large portion of the errors and are often fooled by the fluency of the LLM-generated summaries. 
Interestingly, when told that a given summary is unfaithful, LLMs can come up with correct reasoning and arguments that they couldn't otherwise, showing their inherent potential for error detection.
% \sm{TODO add citation}. 
To efficiently exploit the error detection capability of the LLMs to reason about the faithfulness of a given summary, we propose \textbf{\method}, a Multi-Agent Debate with Initial Stance for Summary Evaluation framework, in which LLM-based agents will be assigned opposing initial stances (either faithful or unfaithful) as their beliefs on the faithfulness quality of the summary. 
Forcing LLMs to come up with reasons to justify an initial stance might not always lead to correct prediction as the stances are random and might not be aligned with actual faithfulness labels.
Therefore, agents engage in multiple rounds of debate with each other, either support or refute othersâ€™ arguments with the aim of resolving any inconsistencies and reaching an agreement on the final label.

However, the main underlying assumption in faithfulness evaluation is that a summary \textsc{always} has a right answer and can either be classified as faithful or unfaithful which might not be the case. 
A summary can be interpreted in different correct and plausible ways and then depending on the interpretation can be seen as both faithful and unfaithful as shown in Figure \ref{fig:ambiguity-motiv}. This would lead to low IAA regardless of the quality of the evaluators as they might only think of one interpretation and base their evaluation on that. 
The possibility of a summary having multiple interpretations leading to different faithfulness evaluations can impact the conclusions regarding system performance and ranking.
% \sm{TODO check if we actually observe that? good to add to abstract if so}. 
We therefore introduce a new evaluation dimension, \textbf{\textit{ambiguity}}, and we define it as when a summary can have multiple correct interpretations in context of the given document leading to opposing beliefs about the faithfulness of the summary. 
%We believe that addressing ambiguities before faithfulness evaluation is a necessary step to better measure evaluators performance and increase IAA. 
An optimal faithfulness evaluator should address any ambiguities before evaluating faithfulness and the initial step in doing so is to identify such ambiguous summaries.
To facilitate this, we also provide a detailed taxonomy of ambiguities and a human annotated dataset by extending the TofuEval MeetingBank dataset \cite{tang2024tofueval} with ambiguity annotations.
% Our experiments show how the proposed approach can help with identifying more faithfulness errors compared to other LLM-based baselines and at the same time help with identifying ambiguities in the summaries and enhancing the correlation between agents and human judgments. 
% We also show that the existence of ambiguous cases can affect the evaluators performance.
% Our multi-agent setup with agents taking a stance and debating does not only help improve the detection of ambiguous cases but also enhances the correlation between agents and human judgments.

Our main contributions can be summarized as follows:
% \begin{itemize}
% \item 
(1) We propose \method, a multi-agent debate setup with initial stance for improved faithfulness evaluation leading to stronger performance compared to single-LLM and multi-LLM setups for non-ambiguous scenarios by identifying more errors;
% \item 
(2) We introduce a new evaluation dimension, ambiguity, a detailed taxonomy of ambiguity types and provide ambiguity annotation on TofuEval MeetingBank dataset;
% \item 
(3) We show how the debate approach can help with identifying ambiguous cases and furthermore can even have a stronger performance in terms of accuracy and increasing IAA, when evaluated on non-ambiguous summaries.
% \footnote{Data and code will be made public upon acceptance.}.
% \end{itemize}
