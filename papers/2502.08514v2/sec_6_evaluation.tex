\section{Evaluation}
% We conduct experiments using different evaluator settings to answer the following main questions. These evaluation questions are aimed at showing how our multi-agent debate approach is different from the existing works and how these differences can lead to improvements.

Our evaluation setup is focused on three main directions; First, 
showing the improvement of \method in terms of accuracy for faithfulness evaluation plus the added interpretability with generated explanations for faithfulness label.
Second, justifying our arguments on how ambiguity can affect the performance of faithfulness evaluators and how addressing them can help with better assessment of performance.
Finally, showing that \method does not only help with better faithfulness evaluation but it also helps with identifying ambiguity.

\paragraph{How does \method compare with other single and multi LLM-based baselines?}
% We measure the performance of different faithfulness evaluator models via a binary prediction task. For any evaluation model $M$, a document $D$, and a generated summary $s$, we ask $M$ to predict whether $s$ is faithful with respect to the corresponding document $D$. 
% We used two different LLM families for this experiment, Llama3-70B-instruct and GPT-4o-mini and 
We report BAcc and K-alpha of different models using Llama3-70B-instruct in Table \ref{tab:llama-main}.
% and Table \ref{tab:gpt-main} respectively.
%As can be seen from the table, 
Overall, \method improves performance on faithfulness evaluation task compared to all other baselines, and the predictions are better aligned with human annotations.
% Also, regardless of the underlying LLM, the debate approach can consistently outperform the other baselines. 
Moreover, our approach is orthogonal to the underlying LLM and we also observe similar trends for other LLMs as well (Appendix \ref{app:results:gpt4}, \ref{app:results:llama-small}). %We show how \method can almost always outperform other baselines regardless of the base LLM. These additional results and analysis are reported in the Appendix \ref{app:result}.
%
For a more complete set of results, both sentence-level and summary-level using different automatic evaluators, refer to Table \ref{tab:full} in Appendix \ref{app:full_results}.

\paragraph{How effective is the initial stance assignment?}
One of the key components of \method is the stance initialization stage where the evaluator agents are assigned opposing beliefs about the faithfulness of the summary before entering the debate stage as shown in Figure \ref{fig:overview}. 
Assigning initial stances to evaluator agents can significantly improve the performance of \method as this initialization encourages LLMs to think more thoroughly as to whether there exists a faithfulness error in the summary or not. As shown in Table \ref{tab:llama-main}, \method without initialization performs almost similarly to other baselines. But after assigning the random stances, a larger performance gap is observed as shown in the second chunk of Table \ref{tab:llama-main}, highlighting the importance of initialization to diversify the debate towards identifying more errors (for analysis on the effect of stance initialization distribution, please refer to Appendix \ref{app:init}).
% \sm{TODO add info what kind of more anaylsis?}).

\paragraph{Can \method identify more errors?}
% \input{tables/tab1.1:main-fpr-fnr}
Missing on a large portion of the errors in the summaries is a major issue with the existing evaluation approaches. This mainly happens due to the fact that evaluators are usually fooled by the fluency of the generated text and would fail to distinguish fluency from faithfulness. This might be even more problematic in domains where failure to identify an error in the text can be a critical issue (for instance medical domain).
% 
We report the false negative rate (FNR) and false positive rate (FPR) as described in Appendix \ref{app:metrics} in Table \ref{tab:llama-main-fpr-fnr}.
It is shown that \method is capable of achieving lower FNR by identifying more errors with the help of random stance initialization and debate.
However, since \method is more sensitive to the errors, the FPR is also increased. 
More on  why this might be the case and how it can be alleviated is described in Appendix \ref{app:results:fpr-fnr}.
% \sm{TODO bring the FPR/FNR table/info into main text. Suggestion: one column table, take one dataset, show FNR, FPR, BAcc, the message should be MADDISE improves FNR, with a small cost of FPR, but this is a better balance in terms of BAcc and especially for critical domains}.


\paragraph{Can \method help identify ambiguities?}
Ambiguities as described earlier can lead to different assessments of faithfulness and should be addressed before evaluating the summaries so that models would not be penalized solely based on the subjectivity of the interpretations. 
\input{tables/tab6_ambiguity_identification}

% Due to the ambiguities that might exist in the generated summaries, they can be interpreted in different ways, all plausible, where one interpretation can make the summary faithful whereas with a different interpretation, one might consider the summary as unfaithful. 
% We believe that this type of disagreement should be addressed before evaluating the summaries faithfulness so that models would not be penalized solely based on the subjectivity of the interpretations. 

But how can we identify such ambiguities? Using our proposed taxonomy and the MeetingBank annotated data on this dimension (as described in Section \ref{sec:data_annotation}), we tried different ways to automatically identify such cases given the document and the summary using Llama3-70B-instruct:
% \sm{TODO I agree with Han's comment here that currently the methods are explained in results, I would suggest to move some of the methods explanation into the methods section so it's easier to explain here}:
\textbf{1. Self-consistency variation:} 
In this baseline, LLM is asked multiple times ($41$ in our case) to identify whether the summary is faithful or not. Then, the ratio of the times the answer is faithful and the ratio of the times the summary is labeled as unfaithful will be measured. If the difference lies between some pre-define threshold ($<20$), the summary will be considered as ambiguous. The motivation using this approach is that if the evaluator is not sure of its decision, that can mean the summary can be interpreted in different ways, hence ambiguous.
\textbf{2. Zero-shot with ambiguity taxonomy:}
We provide our ambiguity taxonomy to LLM to identify whether the summary is ambiguous or not.
\textbf{3. Debate disagreement:}
Using \method, we consider cases for which even after $3$ rounds of debate, none of the agents changed their initial stances as ambiguous.
\textbf{4. Ambiguity detection with debate arguments:}
Using the arguments of the debates and ambiguity taxonomy, we ask the LLM to identify whether there exists an ambiguity or not. You can refer to prompts in Table \ref{tab:prompt_lists} in Appendix \ref{app:prompts}.
% \sm{TODO add prompt}.
% \[M_{a}(D,s,A,t) \in \text{\{ambiguous, non-ambiguous\}}\]

\noindent 
% We used the newly annotated data on ambiguity (\ref{sec:data_annotation}) and the above evaluators and measured the accuracy of different approaches on ambiguity detection task.
The accuracy numbers are reported in Table \ref{tab:ambiguity_results}. 
% \input{tables/tab6:ambiguity_identification}
The ambiguity taxonomy can help baselines with identifying the ambiguous cases. Our best performing ambiguity detection model is the one which uses the arguments from the debates on summary faithfulness. %This main takeaway form the results of ambiguity benchmarking, 
Our results suggest that not only does \method help with faithfulness evaluation but it can also serve as a means to identifying ambiguous cases and filtering them. 
These are the initial results on ambiguity detection however there is still a large room for improvement on the task which is left for future work.

\input{plots/plot_ambiguity-filtering}
\paragraph{How ambiguous cases can affect the evaluators performance?}
As can be seen from Table \ref{tab:llama-main}, even the best performing evaluators still fall very short in terms of k-alpha showing low agreement between models predictions and human annotations. Aside from the evaluators individual errors, the existence of ambiguities is a major contributing factor to low agreement and would lead to incorrect conclusions on models performance.


To remove the effect of ambiguous cases on model performance and have a more accurate estimate of evaluators performance, we filtered them out (the ones annotated as ambiguous by human annotators) from the evaluation subset (MeetingBank dataset with ambiguity annotation) and measured the performance of different models on both unfiltered/filtered data. 
As can be seen in Figure \ref{plot:ambiguity-filtering}, regardless of the setting, removing such ambiguous cases would lead to higher agreement between gold labels and the model-generated labels (with slightly larger gap for \method). Removing ambiguities can also improve FNR and FPR trends (\ref{app:results:fpr-fnr-filtered}).

% \sm{TODO currently the result here is straightforward, a more interesting result would be if removing ambiguity can result in a different model ranking, which would be important consideration for the community when evaluating evaluators}. 
% \mk{Unfortunately we don't have such results for MeetingBank. I myself annotated MediaSum and saw a flip for zero-shot VS self-consistency with and without ambiguities. Such flips can usually happen when there is not a large gap between systems in presence of ambiguities.}

% \input{tables/tab5:filtered_results_meetingbank}

