\section{Experimental Setting}
\input{tables/tab1_main}

\subsection{Datasets}
To evaluate our multi agent debate framework \method, we use a mix of summarization datasets, namely AggreFact \cite{tang-etal-2023-understanding} benchmark consisting of CNN and XSum datasets %(also part of LLM-AggreFact \cite{tang2024minicheck} benchmark) 
as well as TofuEval benchmark \cite{tang2024tofueval} consisting of an annotated subset of MediaSum \cite{zhu2021mediasum} and MeetingBank \cite{hu2023meetingbank}, for a mix of news and dialogue domains. The ambiguity annotation of MeetingBank (Section~\ref{sec:ambiguity} is additionaly used for ambiguity related experiments. 
% For TofuEval benchmark, we use full summaries for evaluating faithfulness instead of evaluating sentence-level summaries.
The statistics of the datasets are presented in Table \ref{tab:dataset}.
% \sm{this table is also in the appendix, could be merged with the ambiguities stats into one data stats table?}. 
We have used full summaries (instead of sentence-level) to measure faithfulness on TofuEval, as it was previously shown that asking the model to evaluate sentences at once or individually would not lead to any significant performance change \cite{tang2024minicheck}. However, we also report the sentence-level results in Appendix \ref{app:result}.

\subsection{Evaluators}
% The agents in our debate approach are LLMs mainly from two families: Meta Llama3 \cite{llama3modelcard} and OpenAI GPT-4o \cite{achiam2023gpt}. 
We use Meta Llama3 \cite{llama3modelcard} as our underlying LLM for our experiments and results reported in the main script. We also used other LLMs and reported the results in Appendix \ref{app:result}.
% \sm{TODO add info that we use other llms in the appendix}.
We have used different setups, including single and multi-LLM evaluators and compared their performance with variations of \method:
% our debate approach: 
%
\textbf{(1) Zero-shot Single LLM:} a single LLM agent which is directly asked to predict whether the given summary is faithful or not given the document. %:   
%\[M_{z}(D,s) \in \text{\{faithful, unfaithful\}}\]
%
\textbf{(2) Chain of thought:} an LLM is asked to first think step by step before providing its judgment on the summary \cite{wei2022chain}.
%\[M_{cot}(D,s,t) \in \text{\{faithful, unfaithful\}}\]
%where $t$ represents the LLM thought process before getting to the final answer.
%
\textbf{(3) Self-consistency:} the system is queried $n$ times \cite{wang2022self} to sample different paths, with the final judgment determined by the majority vote.
%The system is asked $n$ times \cite{wang2022self} to sample different paths and then selecting the majority vote as the final judgement.
%\[\text{Vote}(M^i_{cot}(D,s,t) \in \text{\{faithful, unfaithful\}})\]
%where $M^i_{cot}(D,s,t)$ is one of the responses out of $n$ repetitions.
%
%\paragraph{Multi-LLM single debate without initialization.} 
\textbf{(4) \method wo. initialization:} 
\method with $4$ evaluator agents participating in at most $3$ discussion rounds to evaluate the faithfulness of the summary as shown in Figure \ref{fig:overview} but without the stance initialization stage. 
%
%\paragraph{Multi-LLM single debate.} 
\textbf{(5) \method:} our proposed approach and evaluation framework as shown in Figure \ref{fig:overview} with $4$ evaluator agents and at most $3$ discussion rounds.
%
\textbf{(6) \method w. simultaneous debates: }%Similar to the previous setup, however, 
instead of having a single debate session, we initialize $3$ simultaneous debate sessions, each with $4$ evaluator agents, and the final label would be aggregated over the responses from different sessions as described in \ref{app:method}.
% The aggregation can be done in two ways: \emph{debate vote} -- the majority vote over labels assigned in debates. Each debate session concludes with a label as described in the single debate setting.
% The majority vote over these values is the final faithfulness label -- and \emph{agent vote} -- the majority vote over all participating agents in all debates. Regardless of the session to which agents belong, their individual responses are aggregated (with a majority vote) and reported as the final label.
% \sm{TODO let's move this into the method section and explain it there in a self contained manner without relying on appendix. Why? currently, the definition is confusing, it is referring to the appendix, while the result is presented in the main table. Also we need to explain why 3 simu. rather than just increasing the number of agents, I think there is some empirical evidence?}.
% 
All setups perform the evaluation in a zero-shot manner.
% without few-shot examples\sm{TODO add some info why? do we have a justification? we want to leave it to future work?}.
The prompts used for all these settings are presented in Appendix \ref{app:prompts}.

\subsection{Evaluation Criteria}
We have used two main metrics for our evaluation purposes, balanced accuracy (BAcc) which is used to measure the overall performance of evaluators in detecting the correct labels for summaries, and Krippendorff alpha (K-alpha) \cite{krippendorff2011computing} to measure how well system-generated labels align with the human annotations. More details on these metrics can be found in Appendix \ref{app:metrics}.

