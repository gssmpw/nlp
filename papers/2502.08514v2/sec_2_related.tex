\section{Related Work}

Evaluation of summary faithfulness has been extensively studied before. We present an overview of such works, with special attention to the recent LLM-based and multi-agent approaches.
% specifically.

\subsection{Summary Evaluation}

Automatic n-gram based metrics such as ROUGE \cite{lin2004rouge} and BLEU \cite{papineni2002bleu} or representation-based metrics such as BERTScore \cite{zhang2020bertscoreevaluatingtextgeneration} have long been used to measure the quality of a generated summary with respect to a given reference summary (or the document). However, they have been shown to have poor correlation with human judgments \cite{gao-wan-2022-dialsummeval,Tang2023.04.22.23288967}. The reason behind that is the arrival of LLMs which have proven to be extremely good at generating text of a high quality, relevance and at the same time of enough diversity to mislead the word overlap/distance-based metrics. Moreover, the LLMs' parametric knowledge would lead to new subtleties that cannot be easily directed with the traditional automatic metrics. One of the major issues with employing LLMs as summarizers is {\it hallucination}, when the LLM generates a fact solely using its parametric knowledge and without grounding it in the source document.
% To overcome these challenges is summary evaluation, two sets of approaches are being used for the task.
Many approaches were developed to overcome those challenges in summary evaluation, which we categorize into two.
% \begin{enumerate}
 % \item 
First, specialized error detectors which are trained to detect a specific type of error in the generated summary \cite{kryscinski2020evaluating, fabbri2022qafacteval, goyal2020evaluating, clark2023seahorse, tang2024minicheck}. However, these approaches require annotated data and only provide a single faithfulness label without localizing the error.
 % \item 
 Second, LLM-based evaluators through zero-shot prompting \cite{luo2023chatgptfactualinconsistencyevaluator, wang2023factcheck}. In these approaches, the LLMs are provided with the task description and are asked to evaluate the given text by either providing a label or a ranking. The final result can also be an aggregation of the responses from multiple LLMs that are instructed to do the same task \cite{verga2024replacing}.
 Though shown to be competitive with human evaluations, they still miss on a large portion of the errors \cite{tang2024minicheck,tang2024tofueval}.
% \end{enumerate}

\subsection{LLM-Based Multi-Agent Systems}

Single LLM agents have shown promising results in many tasks and applications, however, LLM-based multi-agents have been proposed to further expand their capabilities and to better leverage their expertise and skills.  
There are two main system categories: in the first category, different LLMs are asked to do the same task but either with a specific role in mind such as a critic or general public \cite{chan2023chateval} or are asked to do it using the feedback from other agents and try to modify their response with respect to other agents responses through rounds of debates \cite{du2023improving}. In this setting of peer-to-peer debaters with a judge, a known problem is the {\it degeneration of thought} when, having acquired some confidence in its stance, the debater will stick to it whether it's correct or not, making the potentially lengthy and costly further debate of little use. In this case, the diversity of the debaters' stances becomes important, and as such, \citet{DBLP:journals/corr/abs-2305-19118} assign roles (affirmative, disagreeing) to the agents in the prompts, having the judge combine all the debaters' arguments and come up with the final decision. \citet{DBLP:conf/icml/SmitGDBP24} also explore the {\it agreement modulation} technique in which they assign each debater the ratio with which it agrees with others' points of view, leading to notable performance improvements. \citet{DBLP:conf/acl/ZhangX0LHD24} explore both personality traits of the agents (easy going / overconfident) and thinking patterns (self-reflection / debating) and their contribution to the debate outcome. In the second category, multiple LLMs can collaborate together through a set of guidelines to do a task with each agent only doing a part of the job \cite{mandi2024roco, qian2024chatdev, hong2023metagpt, lan2024stance}. In this setup, a task is broken into smaller sub-tasks that require different skill set and all agents work towards reaching the broader goal by realizing their specified tasks. 
Our approach is similar to the first category in which multiple evaluators with different initial instances engage in a debate to reach a conclusion on the faithfulness of a given summary.
% \sm{TODO add how do we differ}
