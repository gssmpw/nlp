\section{Related work}
\subsection{Diffusion model for the super-resolution task.}
\begin{figure*}[tb]
    \centering
    \includegraphics[width=1\linewidth]{figures/unet.drawio.png}
    \caption{The backbone of our MSSR approach. The UNet architecture ____ is an encoder-decoder structure. 
    % The input of the UNet usually consists of noise-corrupted data. The output is the prediction of the noise added to the input.
    The model consists of a series of blocks for downsampling and upsampling, which form a U-shape.  In our MSSR approach, the noisy high-resolution spectrum and the low-resolution spectrum are first concatenated and fed to the first layer of the model and then forwarded sequentially through different blocks until the final outputs. The upscaling factor and timestep embeddings are summed and inputted to each block independently.}
    \label{fig:unet}
\end{figure*}

The explosion of interest in generative models came in the 2010s, as deep learning techniques became more mature. 
Generative models learn the underlying patterns and structures of the data, enabling them to produce novel instances. These models can generate a wide range of data types, such as images ____, text ____, and audio ____, and are capable of tasks like data augmentation ____, content creation ____, and style transfer ____.
This period saw the development of key generative models such as Generative Adversarial Networks (GANs) ____, Variational Autoencoders (VAEs) ____, and more recently Diffusion models ____.


Diffusion models are probabilistic models designed to learn a data distribution ____. 
They are widely used in computer science, majorly in computer vision and visual-language models ____.
The model operates by gradually adding noise (generally, Gaussian noise) to data in the forward process (called diffusion process) and then learning to reverse this noise through a backward process (called denoising process) to recover the original data distribution.
From 2020, the diffusion model family has gained attention due to its effectiveness in generating high-quality samples, initially in the context of image generation such as Repaint ____, then audio generation such as Diffwave ____, and more recently image-text generation such as Stable diffusion ____, GLIDE____, and Dall-E 2 ____.


The backward process, which is the core of the diffusion model, aims to denoise the corrupted data in an iterative process. 
This reverse diffusion process in diffusion models generally relies on deep neural networks. As shown in Fig.~\ref{fig:unet}, UNet, a well-established architecture originally designed for biomedical image segmentation is widely used in diffusion models due to its multi-scale nature that facilitates the integration of fine and coarse features ____. 
The UNet or its variants ____ is regarded as the backbone of the diffusion model.


Diffusion models are in principle capable of modeling
conditional distributions ____.
% of the form $p(x|y)$ where $x$ is the input data and $y$ is the conditional input of the UNet.
% $\epsilon_{\theta}(x, t, y); t=1 \dots T$ where t refers to the timesteps of the 
This can be implemented with a conditional UNet and paves the way to control the denoising process through auxiliary information (i.e., conditioning inputs) such as class labels ____, low-resolution data ____ and texts ____.
By conditioning on this information, the model can generate outputs that adhere to desired constraints or specifications.

In the context of super-resolution, the conditioning input typically consists of a low-resolution version of the target data. The diffusion model learns to enhance the spatial resolution by iteratively refining the details while preserving the overall structure and content of the input. This can be achieved by concatenating or integrating the low-resolution input with the noise-corrupted data at each step of the reverse diffusion process, or by integrating the auxiliary information into the model through additional channels or attention mechanisms ____. For more information, please refer to the UNet with attention modules ____.
In the super-resolution task, the diffusion model usually takes the following inputs during the denoising process:
\begin{itemize}
    \item A low-resolution data, which serves as the conditioning input. This is often a downsampled version of the target high-resolution data.
    \item The current time step \( t \) provides the information to the model on how much noise has been added to the original spectrum.
    \item Optional auxiliary conditions, such as a class label or other features describing specific output characteristics. These conditions help guide the model to generate outputs aligned with the desired specifications.
\end{itemize}

% The model learns to iteratively refine the noisy input \( \mathbf{y}_t \) while incorporating the conditioning information to reconstruct the high-resolution output \( \mathbf{y}_0 \). This process is expressed mathematically as:
% \[
% \mathbf{\epsilon}_\theta(\mathbf{x}_t, x_{LR}, t, \mathbf{c}_{\text{emb}}) = \text{UNet}([\mathbf{x}_t; x_{LR}], \text{MLP}(\mathbf{e}_t), \mathbf{c}_{\text{class}}),
% \]
% where \( [\mathbf{y}_t; \mathbf{y}_{\text{low}}] \) denotes the concatenation of the noisy input and the low-resolution conditioning input along the channel dimension, \( \mathbf{e}_t \) is the time embedding, and \( \mathbf{c}_{\text{class}} \) represents the class label embedding.

% The objective of the backbone is the learning of the conditional distribution ____. 
% As such, the low-resolution data is the condition integrated into the backward process (i.e., denoising process). 
% A principle known as "guidance" can mitigate this issue by weighting the conditioning information, thus allowing high-quality super-resolution ____. 

Most super-resolution techniques involve processing image data or multimodal data that includes images ____. SR3 ____ and SRdiff ____ have achieved high-quality super-resolution by manipulating the pixel domain. 
However, unlike previous works that process the pixel-wise input, in this paper, we apply the conditional diffusion model in the frequency-based domain. Furthermore, our approach offers multi-scale functionality (see Section \ref{Condition mechanism}).

\subsection{NMR super-resolution}\label{nmr super-reso}
% \begin{figure*}[btp]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figures/MSSR.drawio.png}
%     \caption{MSSR pipeline: we leverage the Diffusion Model to reconstruct the high-spectrum spectrum via auxiliary information (i.e., conditioning inputs): the upscaling factor and the low-resolution spectrum.}
%     \label{training}
% \end{figure*}

The application of AI models for NMR super-resolution tasks is still in its early stages. To the best of our knowledge, prior work in this area has primarily focused on improving resolution through data acquisition (i.e., experimental optimization) rather than post-acquisition processing.

Mulleti et al. ____ applied finite-rate-of-innovation sampling (FRI) ____ to achieve super-resolution in NMR spectroscopy. By reconstructing signals with fewer measurements, their approach accurately resolves overlapping or broadened peaks, enabling precise estimation of chemical shifts and enhancing the analysis of complex systems.

Wenchel et al. ____ focused on optimizing the experimental process itself. Their method dynamically increases the number of scans over time to counteract signal decay, reducing peak linewidth and enhancing resolution. However, this approach is limited to improving resolution during the data acquisition process and does not address the enhancement of pre-existing low-resolution spectra.

\begin{figure*}[btp]
    \centering
    \includegraphics[width=\linewidth]{figures/MSSR_draft.drawio.png}
    \caption{Our MSSR pipeline. In the diffusion process, the original NMR spectrum $x_0$ (high-resolution spectrum) is gradually corrupted by the Gaussian noise. After a total number of time steps $T$ in the diffusion process, the output is a noisy high-resolution spectrum denoted by $x_T$. $t$ represents the current time step.
    In the denoising process, an equally weighted sequence of UNets $p_{\theta}$, is trained to predict a denoised variant of their input $x_t$, where $x_t$ is a noisy version of the input $x_0$. $x_{LR}$ represents the low-resolution spectrum. $f$ is the upscaling factor representing the ratio of high resolution to low resolution.  At the end of the denoising process, the high-resolution spectrum denoted by $\widehat{x_0}$ is reconstructed. 
    The input of the UNet is the noisy spectrum $x_T$ at time step $T$ or the intermediate denoised spectrum (denoted by $\widehat{x_t}$) at time step $t$ where $t \in \{1,\dots,T-1\}$. The conditional inputs are the upscaling factor $f$, the time step $t$, and the low-resolution spectrum $x_{LR}$.
    Note that the architecture of the conditional UNet is detailed in Fig. \ref{fig:unet}.
    }
    \label{mssr}
\end{figure*}

Unlike the aforementioned works ____, we propose a novel AI-driven approach called the Multi-Scale Super-Resolution model (MSSR) shown in Fig.~\ref{fig:unet} and Fig.~\ref{mssr}. MSSR directly enhances the resolution of NMR spectra without requiring any improvements in the data acquisition process. By leveraging a conditional diffusion model, our method achieves super-resolution in post-acquisition processing, enabling the generation of high-resolution spectra from low-resolution spectra. This eliminates the reliance on costly high-field NMR instruments, providing an accessible and efficient alternative for improving spectral resolution.