\section{Experiments}
\paragraph{Implementation Details}
To build \ours{}, we fine-tune Image-to-Video Stable Video Diffusion (I2V-SVD) \cite{svd}, which adopts the LDM framework~\cite{ldm} with the EDM~\cite{edm} diffusion mechanism.
We use the REDS dataset~\cite{reds} to train our model. Following previous work \cite{chan2022investigating,yang2023mgldvsr}, we merge 240 training videos and 30 test videos, reorganizing them into 266 training videos and 4 test videos, and refer to the latter as REDS4.
We refer the reader to the supplementary material for more implementation details.

\paragraph{Evaluation Datasets}
We use the REDS4 and UDM10 \cite{udm10} datasets for VSR evaluation.
We construct HR-LR video pairs from the datasets using the real-world degradation pipeline of Chan \etal~\shortcite{chan2022investigating}, which applies random blur, resizing, noise, JPEG compression, and video compression.
Additionally, we use the VideoLQ dataset~\cite{chan2022investigating} as a real-world LR dataset.


\subsection{Comparison with Previous SR and VSR Approaches}
To evaluate the performance of {\ours}, we compare it with various previous methods across different categories.
Specifically, our evaluation includes non-generative image super-resolution (ISR) methods: bicubic interpolation and SwinIR~\cite{liang2021swinir}; two generative ISR methods: Real-ESRGAN~\cite{real-esrgan} and StableSR~\cite{stablesr}; two non-generative VSR methods: RealBasicVSR~\cite{chan2022investigating} and RealViformer~\cite{realviformer}; and two generative VSR methods: Upscale-A-Video (UAVideo)~\cite{zhou2024upscaleavideo} and MGLD~\cite{yang2023mgldvsr}.

For quantitative evaluation on synthetic datasets that provide ground-truth HR videos, we use PSNR, SSIM, DISTS~\cite{dists}, tOF, and tLP~\cite{tOFtLP}.
PSNR, SSIM, and DISTS measure the quality of each frame in super-resolution results, while tOF and tLP measure the temporal consistency by comparing optical flows estimated from super-resolution results with ground-truth optical flows.
We also conduct a quantitative evaluation on a real-world dataset, VideoLQ~\cite{chan2022investigating}, using no-reference quality metrics: MUSIQ~\cite{musiq} and DOVER~\cite{dover}.
MUSIQ measures perceptual image quality, while DOVER is a video quality metric that quantifies aesthetic quality and technical artifacts such as compression or blur.

\cref{table:1} presents a quantitative comparison with previous methods. It is important to note that quantitatively measuring VSR quality is challenging, especially for generative-prior-based approaches, due to the synthesized details that may not align with the ground-truth details. Nevertheless, on both synthetic and real-world datasets, \ours{} consistently demonstrates superior or comparable results across various metrics. For PSNR and SSIM, non-generative methods generally score higher because they do not synthesize high-frequency details that may not align with ground-truth videos. However, \ours{} achieves PSNR scores that are competitive and SSIM scores that surpass previous generative VSR methods. When considering DISTS, a perceptual metric, \ours{} ranks second among all methods. Regarding temporal consistency, measured by tOF and tLP, \ours{} outperforms all other methods, including non-generative ones, highlighting the effectiveness of video generative prior and our {\tfiabb} scheme. Finally, evaluating video quality independent of ground-truth data using MUSIQ and DOVER, \ours{} outperforms other methods in most cases on both synthetic and real-world datasets.

\cref{fig:spatial_qualitative} presents a qualitative comparison focusing on spatial consistency. 
Among the compared methods, RealViformer~\cite{realviformer} processes the entire spatial area of video frames without splitting them into multiple spatial tiles. However, it lacks mechanisms, such as spatial attention, to ensure consistency between distant regions, resulting in spatially inconsistent artifacts. The other methods, including ours, adopt tile-based approaches. 
For all tile-based approaches, the regions marked by red squares in the LR frames belong to different tiles.
As shown in the results, our method produces high-quality, spatially-consistent results despite our tile-based approach, surpassing the other approaches.

\cref{fig:temporal_qualitative} presents a qualitative comparison focusing on temporal consistency. In the figure, all previous approaches exhibit challenges in producing temporally consistent HR details for both distant and neighboring frames. StableSR~\cite{stablesr}, an image super-resolution (ISR) approach, handles each video frame separately, which inherently limits its temporal consistency.
RealViformer~\cite{realviformer} propagates information bidirectionally between neighboring frames, but this method still struggles with ensuring consistency across longer temporal sequences.
Moreover, due to the lack of generative prior, its results show less realistic details.
Meanwhile, MGLD~\cite{yang2023mgldvsr} and UAVideo~\cite{zhou2024upscaleavideo}, both VSR approaches, rely on image diffusion priors and process temporal tiles, but struggle to maintain temporal consistency.
Finally, our results show superior temporal consistency and higher-quality HR details than the results of previous methods. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/temporal_ablation.pdf}
    \vspace{-2mm}
    \caption{(a)\&(b) are VSR results of image diffusion prior-based methods. (c)\&(d) show the effects on the proposed {\tfiabb}.}
    \label{fig:temporal_ablation}
    \vspace{-2mm}
\end{figure}

\cref{fig:fo1} presents a qualitative comparison highlighting the restoration quality of a single frame, demonstrating the superiority of {\ours} over previous methods by a significant margin. For example, {\ours} excels at restoring building windows, adding plausible details to degraded regions, as seen in VideoLQ 020. Notably, it surpasses other VSR methods in restoring characters, a particularly challenging task for generative models, as seen in  VideoLQ 008 and 049. These results indicate that {\ours} not only ensures spatio-temporal consistency but also achieves outstanding restoration quality.

The superior performance of {\ours}, demonstrated in \cref{fig:spatial_qualitative,fig:temporal_qualitative,fig:fo1}, stems from the video diffusion prior and our proposed components.
Specifically, the video diffusion prior leverages spatially and temporally distant information within a spatio-temporal tile, ensuring not only spatio-temporal consistency but also enhanced HR details.
Furthermore, our SAP and TAP schemes allow exploiting information from distant tiles, further improving spatio-temporal consistency and HR details.
Finally, our DSSAG scheme additionally improves the quality of HR details.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/spatial_ablation.pdf}
    \vspace{-2mm}
    \caption{Visualization of the effects on the proposed {\ssiabb}.}
    \label{fig:spatial_ablation}
    \vspace{-4mm}
\end{figure}


\subsection{Component Evaluation}
\label{ssec:ablation_study}

\paragraph{Effect of Video Diffusion Prior}
\cref{fig:temporal_ablation}(a-b) and (c) show the VSR results of previous methods utilizing image diffusion priors and our method employing video diffusion priors, respectively. While image diffusion prior-based methods incorporate additional techniques to enhance temporal consistency, they produce blurry and inconsistent outcomes. In contrast, our approach, which directly applies video diffusion priors without additional temporal consistency techniques, shows significantly clearer and more temporally consistent results.

\paragraph{Ablation Study on {\tfiabb} \& {\ssiabb}} 
\cref{fig:temporal_ablation}(c) and (d) present the VSR results without and with {\tfiabb}, respectively. By applying {\tfiabb}, temporal consistency between distant frames is effectively preserved. Specifically, in frame 77, a vertical line in the number ``1'' appears in (c) but remains absent in (d), maintaining the original consistency. 
% This highlights the effectiveness of {\tfiabb} in ensuring temporal coherence across frames.
\cref{fig:spatial_ablation} illustrates the VSR results of two distant brick regions in a frame, comparing outcomes without and with {\ssiabb}, respectively. Without {\ssiabb}, the brick patterns appear mismatched between the regions. In contrast, after applying {\ssiabb}, the distant brick patterns are restored with coherent details.
Additionally, the first three rows in \cref{tab:ablation} present ablation studies on {\ssiabb} and {\tfiabb}. Both schemes show improvements in the DOVER. In contrast, since MUSIQ measures image quality, no improvement is observed for {\tfiabb}. For a detailed visualization of the enhancements brought by SAP and TAP, please refer to the supplemental video.

\paragraph{Comparisons with Guidance Methods}
\cref{fig:ablation_dssag} presents qualitative comparisons of {\drgabb} with previous guidance approaches, where our method shows the clearest image quality and most accurate character shape.
The last three rows in \cref{tab:ablation} provide quantitative comparisons, showing that {\drgabb} achieves the best performance in DOVER and the second-best in MUSIQ, while being $1.5\times$ faster than previous methods. Moreover, since these guidance methods can be generally applicable beyond VSR, we provide additional comparisons for image generation in the supplemental document.

\input{tables/ablation1}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/qual_dssag_sag_pag.pdf}
    \vspace{-2mm} 
    \caption{Qualitative comparison with previous guidance approaches.
    }
    \vspace{-3mm}
    \label{fig:ablation_dssag}
\end{figure}