We have attached a video named ``SuppleVideo.mp4'' as Supplementary Material, containing VSR results and comparisons that include the contents mentioned in the main paper.

\section{Implementation Details}
We fine-tune Image-to-Video Stable Video Diffusion (I2V-SVD) \cite{svd} with $8\times$ downsampling VAE \cite{ldm}, trained on 14-frame sequences at a resolution of $576\times1024$.
We train {\ours} using AdamW optimizer \cite{loshchilov2019decoupled} with a learning rate of $1 \times 10^{-4}$ and a batch size of 256 on 4 NVIDIA A100-80GB GPUs. Due to the memory limit, we use gradient accumulation to match the size of the batch. The training data is cropped into spatio-temporal tiles with width, height, and sequence length set to 512, 512, and 14, respectively. The denoising U-Net receives motion magnitude and frame rate, fixed at 127 and 8 respectively, as conditioning inputs. We initialize VAE and U-Net network parameters to pre-trained I2V-SVD \cite{svd} parameters. During training, we freeze the VAE and fine-tune all U-Net parameters for the first 5K iterations. Afterward, we fix the temporal attention and temporal residual block layers in the U-Net and fine-tune for an additional 5K iterations. 

\section{Sampling Strategy for {\tfiabb}}
In this section, we show the effectiveness of our sampling strategy for {\tfiabb}. Within the {\tfiabb} scheme, we reduce computational cost by selecting 4 frames from the previous spatial tile, which consists of 14 frames. A straightforward approach is to select frames randomly. However, we opt for a more informed strategy: selecting frames whose keys exhibit large standard deviations. This choice is based on the observation that frames with more varied and sharper details tend to produce distinct keys, resulting in larger standard deviations. \Cref{fig:tfi_sampling} illustrates this trend by comparing random sampling with maximum standard deviation (max SD) sampling. As shown, max SD sampling produces consistent and detailed brick patterns, in contrast to random sampling. This demonstrates that selecting detail-rich frames based on their standard deviations enhances the quality and consistency of temporal tiles.

\section{Ablation Study of {\drgabb}}
\Cref{fig:ablation_drg} demonstrates the effectiveness of {\drgabb} by comparing VSR results with and without {\drgabb}. Without {\drgabb}, the car grille and the bases of the windows exhibit significant distortions. However, after applying {\drgabb}, these elements are restored with clear and precise details. These results highlight the effectiveness of {\drgabb} in enhancing visual quality and structural fidelity in VSR.

\section{Image Generation Comparison of {\drgabb}}
The guidance methods, such as SAG \cite{sag}, PAG \cite{pag}, and our {\drgabb}, are generally applicable to the diffusion sampling process. To demonstrate the general effectiveness of {\drgabb}, we compare the proposed method with SAG and PAG in the image generation task. Specifically, we generate 10K images using Stable Diffusion \cite{sd} and evaluate them using the Inception Score (IS) \cite{is} and Fr\'echet Inception Distance (FID) \cite{fid} metrics. The MS-COCO 2014 validation dataset \cite{coco} is used for these measurements. \cref{tab:image_synthesis} presents the comparison. The proposed method achieves the best FID score and a comparable IS score, highlighting the general effectiveness of {\drgabb}. Notably, this performance is achieved with a $1.5\times$ faster inference speed. \cref{fig:qual_dssag_sd} shows visualization of previous diffusion guidance approaches and {\drgabb}. {\drgabb} effectively improves image synthesis quality and integrates well with CFG.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/tfi_sup_random_maxstd.pdf}
    \caption{Comparison of two different {\tfiabb} sampling strategy.}
    \label{fig:tfi_sampling}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/dssag_ablation.pdf}
    \caption{Effects of {\drgabb}. The first row and second row 
     are the results of {\ours} without and with {\drgabb}. The proposed method enhances the generation quality of video frames.}
    \Description{ablation figures}
    \label{fig:ablation_drg}
\end{figure}

\input{tables/image_synthesis}

\section{Image-to-Video Stable Video Diffusion}
I2V-SVD employs LDM \cite{ldm} structure with EDM \cite{edm} diffusion mechanism, which downsamples (x8) input with VAE encoder $\mathcal{E}$ and proceeds continuous time diffusion denoising process in latent space. After the denoising process ended, it upsamples (x8) latent vectors with VAE decoder $\mathcal{D}$ to the original RGB space. For training EDM with the LDM structure, diffusion loss can be defined as:
\begin{equation}
  L(\theta) = \mathbb{E}_{\sigma,\bm{y},\bm{n}} \left[ \lambda(\sigma) || D_{\theta}(\bm{y+n}, \bm{c}, \sigma) - \bm{y} ||^{2}_{2} \right],
  \label{eq:1}
\end{equation}
where $D_{\theta}$ is a denoiser network, $\bm{c}$ is additional conditions such as text prompt, motion magnitude, and frame rate, $\sigma$ is sampled noise level from predefined $p_{train}(\sigma)$, $\bm{y} \sim p_{\mathcal{E}(data)}$, $\bm{n} \sim \mathcal{N}(\bm{0},\sigma^2\bm{I})$ and $\lambda(\sigma)$ is adaptive loss weight. For stable and consistent input and output signal magnitudes, $D_{\theta}$ is derived from network $F_{\theta}$ in the following form:
 \begin{equation}
     D_{\theta}(\bm{x}, \bm{c}, \sigma)=c_{\text{skip}}(\sigma) \ \bm{x}+c_{\text{out}}(\sigma) \ F_{\theta}(c_{\text{in}}(\sigma) \ \bm{x}, \bm{c}, c_{\text{noise}}(\sigma)),
     \label{eq:2}
 \end{equation}
where $c_\text{skip}(\sigma)$, $c_\text{out}(\sigma)$, $c_\text{in}(\sigma)$ and $c_\text{noise}(\sigma)$ are parameterized function over the noise level $\sigma$ and $\bm{x}=\bm{y}+\bm{n}$. In the sampling stage, noise levels $\sigma_{0\sim T}$ are selected in the range $[0.002, 700]$ in descending order, and the deterministic diffusion backward process is calculated by solving ordinary differential equations from $\bm{x}_{0} \sim \mathcal{N}(\bm{0}, 700^2\bm{I})$ to $\bm{x}_{T}$, where $T$ is the number of sampling time steps.

\section{tLP and tOF}
In \cite{tOFtLP}, pixel-level video flows tOF and perceptual video flows tLP are respectively defined as:
\begin{align}
    \text{tOF} &= \sum_{\text{i=1}}^{\text{L}} ||OF(\hat{g}_{{i-1}}, \hat{g}_{{i}})-OF(g_{{i-1}}, g_{{i}})||_1, \\
    \text{tLP} &= \sum_{\text{i=1}}^{\text{L}} ||LPIPS(\hat{g}_{{i-1}}, \hat{g}_{{i}})-LPIPS(g_{{i-1}}, g_{{i}})||_1,
\end{align}
where $g_\text{i}$ is \textit{i}-th frame of ground truth video, $\hat{g_\text{i}}$ is \textit{i}-th frame of restored video, $OF(\cdot)$ is optical flow estimator and $LPIPS(\cdot)$ is LPIPS score estimator. We use the RAFT \cite{raft} model to estimate $OF(\cdot)$. 

\begin{figure*}[t]
    \centering
    \scalebox{0.8}{
    \includegraphics[width=\textwidth]{figures/qual_diffusion_guidance.pdf}
    }
    \caption{Visualization of previous diffusion guidance approaches and our {\drgabb}. {\drgabb} combines well with CFG, while 1.5$\times$ faster than SAG~\shortcite{sag} and PAG~\shortcite{pag}}
    \label{fig:qual_dssag_sd}
\end{figure*}