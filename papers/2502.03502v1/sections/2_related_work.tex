\input{figures_folder/pipeline}

\section{Related Work}
\paragraph{Generative Prior for Video Super-Resolution}
VSR methods can broadly be classified into two main categories based on the use of generative prior. VSR models without generative prior \cite{Youk_2024_CVPR, chan2021basicvsr, chan2022basicvsrpp, chan2022investigating, fastrealVSR, realviformer} are typically trained using reconstruction losses, such as the mean squared error (MSE), augmented with additional techniques designed to enhance temporal consistency. Although they restore HR videos closely matching the input LR videos, they fail to generate complex details, producing blurry results due to the significant ill-posed nature of the problem. To achieve VSR with rich details, generative prior-based VSR methods \cite{rota2024stablevsr, xu2024videogigagan, chen2024sateco, zhou2024upscaleavideo, yang2023mgldvsr} have recently emerged. These methods fine-tune GANs \cite{gan} or diffusion-based image generation models by modifying them to take LR inputs as conditions and incorporating additional modules to consider temporal consistency. However, maintaining consistency across frames with detailed textures is still challenging \cite{blattmann2023stablevideodiffusionscaling,blattmann2023videoldm}, leading to temporal flickering artifacts. To address this challenge, we propose to leverage video generative priors instead of image generative priors for the first time. By fine-tuning a high-quality video diffusion model~\cite{svd}, we successfully achieve not only rich details but also improved temporal consistency in a simple yet effective manner.

\paragraph{Image Super-Resolution}
As deep learning techniques have advanced, various image super-resolution (ISR) methods have been proposed. SRCNN~\cite{srcnn} was the first to introduce a CNN-based SR method. Subsequent studies have applied various network architectures, such as residual networks~\cite{vdsr,srgan}, dense networks~\cite{rdn,esrgan,real-esrgan}, Laplacian pyramid networks~\cite{lapsrn}, back-projection networks~\cite{dbpn}, recursive structures~\cite{drcn,drrn}, and channel attention~\cite{rcan}. While these methods have demonstrated promising SR results, they often struggle to recover high-frequency details. To address this, approaches leveraging the generative prior of pretrained generative models have been introduced exploiting GANs~\cite{glean,pulse} or Diffusion models~\cite{stablesr,supir,diffbir,pasd,seesr,ccsr,wu2024one}. Since the application of these methods to videos leads to severe flickering artifacts due to the absence of temporal consistency, VSR research has been active in addressing this issue.

\paragraph{Tile-based Generation}
Diffusion models face two main challenges in generating higher spatial or temporal resolutions: quality degradation from discrepancies between training and inference conditions, and substantial computational costs. To address these challenges, tile-based generation approaches have been proposed~\cite{multidiffusion2023,syncdiffusion2023,demofusion2024,zhou2024upscaleavideo,yang2023mgldvsr}. These methods divide the input into overlapping tiles, perform diffusion sampling on each tile individually, and subsequently merge the tiles. However, their approaches do not consider inter-tile consistency, often generating inconsistent details for the same content across split tiles in an image or distant frames~\cite{zhou2024upscaleavideo,yang2023mgldvsr}.
Our proposed methods, SAP and TAP, propagate information across spatial and temporal tiles, producing spatially and temporally consistent VSR results.