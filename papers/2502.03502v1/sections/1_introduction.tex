\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\begin{teaserfigure}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/teaser.pdf}
  \vspace{-2mm}
  \caption{Real-world video super-resolution results of our proposed methods and state-of-the-art models: RealViformer~\cite{realviformer}, Upscale-A-Video~\cite{zhou2024upscaleavideo}, MGLD~\cite{yang2023mgldvsr}. \ours{} shows outstanding super-resolution performance, in respect of not only quality but also spatial and temporal consistency.}
  \Description{teaser}
  \label{fig:teaser}
\end{teaserfigure}

\maketitle

\section{Introduction}
\label{sec:intro}
Video super-resolution (VSR) is a task to restore a high-resolution (HR) video from a low-resolution (LR) counterpart, which has a vast array of applications, such as enhancing old footage and improving streaming video quality on limited bandwidths.
However, VSR is particularly challenging due to its severely ill-posed nature of the problem, primarily because of the missing high-frequency information in LR videos caused during the sampling process.
Furthermore, real-world videos face a myriad of unknown degradation factors beyond just sampling issues, including noise, compression artifacts, and various other distortions, making the task even more ill-posed.

To achieve successful VSR, it is essential to generate realistic HR details while maintaining both spatial and temporal consistency.
To this end, over the past few decades, numerous approaches have been proposed, ranging from traditional methods such as interpolation techniques and model-based optimization approaches to recent neural network-based ones~\cite{realviformer}.
Nevertheless, most of these approaches struggle to produce realistic HR details due to a lack of effective priors that model high-frequency details as shown in \cref{fig:teaser}(b).

Recently, diffusion models that provide powerful generative priors for natural images, have been exploited for VSR to achieve high-quality results with realistic textures.
Yang \etal~\shortcite{yang2023mgldvsr} and Zhou \etal~\shortcite{zhou2024upscaleavideo} utilize image diffusion models as their generative priors and successfully restore detail-rich HR videos from real-world LR videos.
However, adopting image diffusion models poses challenges in maintaining spatial and temporal consistency due to the design of image diffusion models, which typically target single images with limited spatial sizes due to significant memory usage.
To achieve temporally-consistent VSR using image diffusion models, these approaches adopt additional temporal layers, frame information propagation with motion compensation, and overlapping of temporal windows.
Furthermore, to handle large video frames, frames are split into overlapping tiles, processed individually, and then merged.
Despite these efforts, the resulting videos often suffer from spatially and temporally inconsistent results due to the inherent randomness of image diffusion models and their tile-based approach (\cref{fig:teaser}(c) and (d)).


In this paper, we propose \emph{\ours{} (Diffusion-based Consistent VSR)}, a novel VSR approach that, for the first time, leverages a video diffusion prior.
Our approach produces spatially and temporally consistent results with realistic textures, given an LR video of arbitrary length and spatial size (\cref{fig:teaser}(e)).
We leverage Stable Video Diffusion (SVD)~\cite{svd} as a video diffusion prior, which provides powerful generative capabilities for restoring high-quality and temporally-consistent details.
However, exploiting an existing video diffusion model for consistent VSR over long video clips with large frames is not straightforward, since existing models are designed to synthesize a limited number of small  frames, similar to image diffusion models.

To achieve spatial and temporal consistency for a long video with large frames, \ours{} introduces a novel Spatial Attention Propagation (SAP) scheme and Temporal Attention Propagation (TAP) scheme.
Specifically, \ours{} decomposes an input LR video into multiple spatio-temporal tiles, and processes them separately.
To achieve spatial consistency across tiles, SAP introduces a subsampled feature map representing the entire area of a video frame and uses it to process tiles at different spatial locations.
On the other hand, TAP enhances temporal consistency across tiles by propagating information between temporally consecutive tiles.
Both schemes are realized by extending the self-attention layers of a video diffusion model, enabling information on HR details to be effectively propagated across tiles without losing the generative capability of a pretrained diffusion model.

Additionally, we propose Detail-Suppression Self-Attention Guidance (DSSAG), a novel diffusion guidance scheme to improve high-frequency details in synthesized HR video frames.
Similar to Self-Attention Guidance (SAG)~\cite{sag} and Perturbed Attention Guidance (PAG)~\cite{pag}, DSSAG guides the diffusion process by amplifying high-frequency details in the latent representation.
However, unlike the previous methods, DSSAG provides more flexible control over the guidance scale, and can seamlessly integrate with classifier-free guidance (CFG) for high-quality synthesis without incurring additional computational overhead.

We demonstrate the effectiveness of our approach on real-world VSR tasks, where input LR videos contain various unknown degradations. Our experimental results show that \ours{} achieves spatially and temporally consistent, high-quality VSR results, outperforming previous approaches. Our contributions are summarized as follows:
\begin{itemize}
    \item We introduce DC-VSR, a novel VSR approach based on a video diffusion prior, which produces spatially and temporally consistent results with realistic textures. Our approach is the first to exploit a video diffusion prior in VSR.
    \item We propose a Spatial Attention Propagation (SAP) scheme that injects subsampled features representing the entire area of a video frame to different tiles, ensuring spatial consistency.
    \item We propose a Temporal Attention Propagation (TAP) scheme for sharing information across temporally distant frames, achieving temporal consistency.
    \item We introduce Detail-Suppression Self-Attention Guidance (DSSAG), which enhances the quality of synthesized video frames without any additional computational overhead.
\end{itemize}