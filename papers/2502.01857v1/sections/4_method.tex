Effective human-robot cooperation in CoNav-Maze hinges on efficient communication. Maximizing the human’s information gain enables more precise guidance, which in turn accelerates task completion. Yet for the robot, the challenge is not only \emph{what} to communicate but also \emph{when}, as it must balance gathering information for the human with pursuing immediate goals when confident in its navigation.

To achieve this, we introduce \emph{Information Gain Monte Carlo Tree Search} (IG-MCTS), which optimizes both task-relevant objectives and the transmission of the most informative communication. IG-MCTS comprises three key components:
\textbf{(1)} A data-driven human perception model that tracks how implicit (movement) and explicit (image) information updates the human’s understanding of the maze layout.
\textbf{(2)} Reward augmentation to integrate multiple objectives effectively leveraging on the learned perception model.
\textbf{(3)} An uncertainty-aware MCTS that accounts for unobserved maze regions and human perception stochasticity.
% \begin{enumerate}[leftmargin=*]
%     \item A data-driven human perception model that tracks how implicit (movement) and explicit (image transmission) information updates the human’s understanding of the maze layout.
%     \item Reward augmentation to integrate multiple objectives effectively leveraging on the learned perception model.
%     \item An uncertainty-aware MCTS that accounts for unobserved maze regions and human perception stochasticity.
% \end{enumerate}

\subsection{Human Perception Dynamics}
% IG-MCTS seeks to optimize the expected novel information gained by the human through the robot’s actions, including both movement and communication. Achieving this requires a model of how the human acquires task-relevant information from the robot.

% \subsubsection{Perception MDP}
\label{sec:perception_mdp}
As the robot navigates the maze and transmits images, humans update their understanding of the environment. Based on the robot's path, they may infer that previously assumed blocked locations are traversable or detect discrepancies between the transmitted image and their map.  

To formally capture this process, we model the evolution of human perception as another Markov Decision Process, referred to as the \emph{Perception MDP}. The state space $\mathcal{X}$ represents all possible maze maps. The action space $\mathcal{S}^+ \times \mathcal{O}$ consists of the robot's trajectory between two image transmissions $\tau \in \mathcal{S}^+$ and an image $o \in \mathcal{O}$. The unknown transition function $F: (x, (\tau, o)) \rightarrow x'$ defines the human perception dynamics, which we aim to learn.

\subsubsection{Crowd-Sourced Transition Dataset}
To collect data, we designed a mapping task in the CoNav-Maze environment. Participants were tasked to edit their maps to match the true environment. A button triggers the robot's autonomous movements, after which it captures an image from a random angle.
In this mapping task, the robot, aware of both the true environment and the human’s map, visits predefined target locations and prioritizes areas with mislabeled grid cells on the human’s map.
% We assume that the robot has full knowledge of both the actual environment and the human’s current map. Leveraging this knowledge, the robot autonomously navigates to all predefined target locations. It then randomly selects subsequent goals to reach, prioritizing grid locations that remain mislabeled on the human’s map. This ensures that the robot’s actions are strategically focused on providing useful information to improve map accuracy.

We then recruited over $50$ annotators through Prolific~\cite{palan2018prolific} for the mapping task. Each annotator labeled three randomly generated mazes. They were allowed to proceed to the next maze once the robot had reached all four goal locations. However, they could spend additional time refining their map before moving on. To incentivize accuracy, annotators receive a performance-based bonus based on the final accuracy of their annotated map.


\subsubsection{Fully-Convolutional Dynamics Model}
\label{sec:nhpm}

We propose a Neural Human Perception Model (NHPM), a fully convolutional neural network (FCNN), to predict the human perception transition probabilities modeled in \Cref{sec:perception_mdp}. We denote the model as $F_\theta$ where $\theta$ represents the trainable weights. Such design echoes recent studies of model-based reinforcement learning~\cite{hansen2022temporal}, where the agent first learns the environment dynamics, potentially from image observations~\cite{hafner2019learning,watter2015embed}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/ICML_25_CNN.pdf}
    \caption{Neural Human Perception Model (NHPM). \textbf{Left:} The human's current perception, the robot's trajectory since the last transmission, and the captured environment grids are individually processed into 2D masks. \textbf{Right:} A fully convolutional neural network predicts two masks: one for the probability of the human adding a wall to their map and another for removing a wall.}
    \label{fig:nhpm}
    \vskip -0.1in
\end{figure}

As illustrated in \Cref{fig:nhpm}, our model takes as input the human’s current perception, the robot’s path, and the image captured by the robot, all of which are transformed into a unified 2D representation. These inputs are concatenated along the channel dimension and fed into the CNN, which outputs a two-channel image: one predicting the probability of human adding a new wall and the other predicting the probability of removing a wall.

% Our approach builds on world model learning, where neural networks predict state transitions or environmental updates based on agent actions and observations. By leveraging the local feature extraction capabilities of CNNs, our model effectively captures spatial relationships and interprets local changes within the grid maze environment. Similar to prior work in localization and mapping, the CNN architecture is well-suited for processing spatially structured data and aligning the robot’s observations with human map updates.

To enhance robustness and generalization, we apply data augmentation techniques, including random rotation and flipping of the 2D inputs during training. These transformations are particularly beneficial in the grid maze environment, which is invariant to orientation changes.

\subsection{Perception-Aware Reward Augmentation}
The robot optimizes its actions over a planning horizon \( H \) by solving the following optimization problem:
\begin{subequations}
    \begin{align}
        \max_{a_{0:H-1}} \;
        & \mathop{\mathbb{E}}_{T, F} \left[ \sum_{t=0}^{H-1} \gamma^t \left(\underbrace{R_{\mathrm{task}}(\tau_{t+1}, \zeta)}_{\text{(1) Task reward}} + \underbrace{\|x_{t+1}-x_t\|_1}_{\text{(2) Info reward}}\right)\right] \label{obj}\\ 
        \subjectto \quad
        &x_{t+1} = F(x_t, (\tau_t, a_t)), \quad a_t\in\Ocal \label{const:perception_update}\\ 
        &\tau_{t+1} = \tau_t \oplus T(s_t, a_t), \quad a_t\in \Ucal\label{const:history_update}
    \end{align}
\end{subequations} 

The objective in~\eqref{obj} maximizes the expected cumulative reward over \( T \) and \( F \), reflecting the uncertainty in both physical transitions and human perception dynamics. The reward function consists of two components: 
(1) The \emph{task reward} incentivizes efficient navigation. The specific formulation for the task in this work is outlined in \Cref{appendix:task_reward}.
(2) The \emph{information reward} quantifies the change in the human’s perception due to robot actions, computed as the \( L_1 \)-norm distance between consecutive perception states.  

The constraint in~\eqref{const:history_update} ensures that for movement actions, the trajectory history \( \tau_t \) expands with new states based on the robot’s chosen actions, where \( s_t \) is the most recent state in \( \tau_t \), and \( \oplus \) represents sequence concatenation. 
In constraint~\eqref{const:perception_update}, the robot leverages the learned human perception dynamics \( F \) to estimate the evolution of the human’s understanding of the environment from perception state $x_t$ to $x_{t+1}$ based on the observed trajectory \( \tau_t \) and transmitted image \( a_t\in\Ocal \). 
% justify from a cognitive science perspective
% Cognitive science research has shown that humans read in a way to maximize the information gained from each word, aligning with the efficient coding principle, which prioritizes minimizing perceptual errors and extracting relevant features under limited processing capacity~\cite{kangassalo2020information}. Drawing on this principle, we hypothesize that humans similarly prioritize task-relevant information in multimodal settings. To accommodate this cognitive pattern, our robot policy selects and communicates high information-gain observations to human operators, akin to summarizing key insights from a lengthy article.
% % While the brain naturally seeks to gain information, the brain employs various strategies to manage information overload, including filtering~\cite{quiroga2004reducing}, limiting/working memory, and prioritizing information~\cite{arnold2023dealing}.
% In this context of our setup, we optimize the selection of camera angles to maximize the human operator's information gain about the environment. 

\subsection{Information Gain Monte Carlo Tree Search (IG-MCTS)}
IG-MCTS follows the four stages of Monte Carlo tree search: \emph{selection}, \emph{expansion}, \emph{rollout}, and \emph{backpropagation}, but extends it by incorporating uncertainty in both environment dynamics and human perception. We introduce uncertainty-aware simulations in the \emph{expansion} and \emph{rollout} phases and adjust \emph{backpropagation} with a value update rule that accounts for transition feasibility.

\subsubsection{Uncertainty-Aware Simulation}
As detailed in \Cref{algo:IG_MCTS}, both the \emph{expansion} and \emph{rollout} phases involve forward simulation of robot actions. Each tree node $v$ contains the state $(\tau, x)$, representing the robot's state history and current human perception. We handle the two action types differently as follows:
\begin{itemize}
    \item A movement action $u$ follows the environment dynamics $T$ as defined in \Cref{sec:problem}. Notably, the maze layout is observable up to distance $r$ from the robot's visited grids, while unexplored areas assume a $50\%$ chance of walls. In \emph{expansion}, the resulting search node $v'$ of this uncertain transition is assigned a feasibility value $\delta = 0.5$. In \emph{rollout}, the transition could fail and the robot remains in the same grid.
    
    \item The state transition for a communication step $o$ is governed by the learned stochastic human perception model $F_\theta$ as defined in \Cref{sec:nhpm}. Since transition probabilities are known, we compute the expected information reward $\bar{R_\mathrm{info}}$ directly:
    \begin{align*}
        \bar{R_\mathrm{info}}(\tau_t, x_t, o_t) &= \mathbb{E}_{x_{t+1}}\|x_{t+1}-x_t\|_1 \\
        &= \|p_\mathrm{add}\|_1 + \|p_\mathrm{remove}\|_1,
    \end{align*}
    where $(p_\mathrm{add}, p_\mathrm{remove}) \gets F_\theta(\tau_t, x_t, o_t)$ are the estimated probabilities of adding or removing walls from the map. 
    Directly computing the expected return at a node avoids the high number of visitations required to obtain an accurate value estimate.
\end{itemize}

% We denote a node in the search tree as $v$, where $s(v)$, $r(v)$, and $\delta(v)$ represent the state, reward, and transition feasibility at $v$, respectively. The visit count of $v$ is denoted as $N(v)$, while $Q(v)$ represents its total accumulated return. The set of child nodes of $v$ is denoted by $\mathbb{C}(v)$.

% The goal of each search is to plan a sequence for the robot until it reaches a goal or transmits a new image to the human. We initialize the search tree with the current human guidance $\zeta$, and the robot's approximation of human perception $x_0$. Each search node consists consists of the state information required by our reward augmentation: $(\tau, x)$. A node is terminal if it is the resulting state of a communication step, or if the robot reaches a goal location. 

% A rollout from the expanded node simulates future transitions until reaching a terminal state or a predefined depth $H$. Actions are selected randomly from the available action set $\mathcal{A}(s)$. If an action's feasibility is uncertain due to the environment's unknown structure, the transition occurs with probability $\delta(s, a)$. When a random number draw deems the transition infeasible, the state remains unchanged. On the other hand, for communication steps, we don't resolve the uncertainty but instead compute the expected information gain reward: \philip{TODO: adjust notation}
% \begin{equation}
%     \mathbb{E}\left[R_\mathrm{info}(\tau, x')\right] = \sum \mathrm{NPM(\tau, o)}.
% \end{equation}

\subsubsection{Feasibility-Adjusted Backpropagation}
During backpropagation, the rewards obtained from the simulation phase are propagated back through the tree, updating the total value $Q(v)$ and the visitation count $N(v)$ for all nodes along the path to the root. Due to uncertainty in unexplored environment dynamics, the rollout return depends on the feasibility of the transition from the child node. Given a sample return \(q'_{\mathrm{sample}}\) at child node \(v'\), the parent node's return is:
\begin{equation}
    q_{\mathrm{sample}} = r + \gamma \left[ \delta' q'_{\mathrm{sample}} + (1 - \delta') \frac{Q(v)}{N(v)} \right],
\end{equation}
where $\delta'$ represents the probability of a successful transition. The term \((1 - \delta')\) accounts for failed transitions, relying instead on the current value estimate.

% By incorporating uncertainty-aware rollouts and backpropagation, our approach enables more robust decision-making in scenarios where the environment dynamics is unknown and avoids simulation of the stochastic human perception dynamics.
