\section{Task Reward Composition}\label{appendix:task_reward}
The \textit{task reward} $R_{\mathrm{task}}$ in our implementation incentivizes effective navigation and consists of three components:
\begin{equation}
    R_{\mathrm{task}}(\tau, \zeta) = R_{\mathrm{env}}(\tau) + R_{\mathrm{guidance}}(\tau, \zeta) + R_{\mathrm{smooth}}(\tau).
\end{equation}

Suppose we rewrite $\tau$ as a state sequence $(s_1, s_2, \cdots, s_n)$, we can formally define each reward as follows:

1. The navigation environment assigns a fixed reward $r_g > 0$ when the robot reaches a goal location for the first time:
   \begin{equation}
   R_{\mathrm{env}}(\tau) = \begin{cases} 
   r_g, & s_n \in \mathcal{G}, \\
   -1, & \text{otherwise.}
   \end{cases}
   \end{equation}

2. A step incurs an additional cost when the robot wanders away from the human's guidance.:
   \begin{equation}
   R_{\mathrm{guidance}}(\tau) = \begin{cases} 
   0, & s_n \in \zeta, \\
   -\log(n), & \text{otherwise.}
   \end{cases}
   \end{equation}

3. The smoothness reward penalizes unnecessary revisits to previously visited states:
   \begin{equation}
   R_{\mathrm{smooth}}(\tau) = -\sum_{i=1}^{n-1} \mathbbm{1}[s_i = s_n].
   \end{equation}

\paragraph{Additional implementation-specific design:} To reduce the search horizon, lower estimation variance, and improve computational efficiency, we terminate an MCTS rollout when the agent either (a) reaches the goal or (b) performs a communication action. However, this modification introduces a bias toward shorter paths. To correct for this, we impose a communication cost of  $c = 10 + n$ , where  n  represents the number of unfulfilled states in the human guidance.

\newpage
\section{Algorithm Pseudocode}\label{appendix:pseudocode}
\begin{algorithm}[htbp]
\caption{IG-MCTS}
\label{algo:IG_MCTS}
\begin{algorithmic}[1]
    \STATE \textbf{Input:} human guidance $\zeta$, previous state-visitation history $\tau_0$, current human perception state $x_0$
    \STATE \textbf{Parameters:} iterations $n=100$, exploration constant $k=\sqrt{2}$, discount factor $\gamma=0.99$, depth $d=100$
    \STATE Create root node $v_0$ with $(\tau_0, x_0)$, initialize $Q(v_0) \gets 0$, $N(v_0) \gets 0$, $\mathbb{C}(v_0) \gets \emptyset$
    \FOR{each iteration $i$ from 1 to $n$}
        \STATE Set $v \gets v_0$, $\mathrm{stopping} \gets \text{False}$
        \WHILE{$v$ is not terminal and $\mathrm{stopping} = \text{False}$}
            \IF{$v$ is fully expanded}
                \STATE $v \gets \arg\max_{v' \in \mathbb{C}(v)} \left( \frac{Q(v')}{N(v')} + k \sqrt{\frac{\log N(v)}{N(v')}} \right)$
            \ELSE
                \STATE $v, \mathrm{stopping} \gets \textsc{Expand}(v)$
            \ENDIF
        \ENDWHILE
        \STATE $q \gets \textsc{Rollout}(v)$
        \STATE \textsc{BackPropagate}($v, q$)
    \ENDFOR
    \STATE \textbf{Return} action of best child $c^\star = \arg\max_{c \in \mathbb{C}(v_0)} N(c)$
\end{algorithmic}
\end{algorithm}

\begin{minipage}{0.45\textwidth}
    \begin{algorithm}[H]
        \captionsetup{labelformat=default,labelsep=colon,name=Procedure}
        \caption{\sc Expand($v$)}
        \label{algo:expand}
            \begin{algorithmic}[1]
            % \STATE \textbf{Input:} node $v$ containing $(\tau, x)$, action space $A=U\cup O$
            \STATE Choose an untried action $a\in \Acal$
            \IF{$a\in \Ucal$}
                \STATE $s=\mathrm{last}(\tau)$
                \STATE $\tau'\gets \tau\oplus T(s,a)$
                \STATE Create node $v'$ with $(\tau', x)$
                \STATE $\mathbb{C}(v) \gets \mathbb{C}(v) \cup \{v'\}$
                \STATE \textbf{Return} $v'$, False
            \ELSIF{$a\in \Ocal$}
                \STATE $x'\gets F(x, (\tau,a))$
                \STATE Create node $v'$ with $(\tau, x')$
                \STATE \textbf{Return} $v'$, True
            \ENDIF
            \end{algorithmic}
    \end{algorithm}
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \begin{algorithm}[H]
        \captionsetup{labelformat=default,labelsep=colon,name=Procedure}
        \caption{\sc Rollout($v$)}
        \label{algo:rollout}
        \begin{algorithmic}[1]
            % \STATE \textbf{Input:} node $v$ containing $(\tau, x)$
            \STATE Initialize $q \gets 0$, depth $d \gets 0$
            \WHILE{$d < T$ and $(\tau, x)$ not terminal}
                \STATE Sample $a \in A$
                \IF{$a\in U$}
                    \STATE $\tau\gets \tau\oplus T(s,a)$ where $s=\mathrm{last}(\tau)$
                \ELSIF{$a\in O$}
                    \STATE $x'\gets F(x, (\tau,a))$
                \ENDIF
                \STATE $q \gets q + \gamma^d R(\tau, \zeta, x, x')$
                \STATE $x\gets x'$
                \STATE $d \gets d + 1$
            \ENDWHILE
            \STATE \textbf{Return} $q$
        \end{algorithmic}
    \end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{\textwidth}
    \centering
    \begin{algorithm}[H]
        \captionsetup{labelformat=default,labelsep=colon,name=Procedure}
        \caption{\sc BackPropagate($v, q$)}
        \label{algo:backprop}
        \begin{algorithmic}[1]
            \STATE Initialize $q_{\text{sample}} = q$ and $\delta=1$
            \WHILE{$v$ is not null}
                \STATE Current value estimate $w = \frac{Q(v)}{N(v)}$ if $N(v)>0$ else $0$
                \STATE $q_{\text{sample}} \gets r(v) + \gamma\left[\delta q_{\text{sample}} + (1-\delta) w\right]$
                \STATE $Q(v) \gets Q(v) + q$
                \STATE $N(v) \gets N(v) + 1$
                \STATE $\delta\gets \delta(v)$
                \STATE $v \gets$ parent of $v$
            \ENDWHILE
        \end{algorithmic}
    \end{algorithm}
\end{minipage}


\newpage
\section{User Study Details}
\subsection{Baseline Pupil Diameter Measurement}\label{appendix:baseline_pupil}
To account for individual differences in pupil sizes, we ask each participant to read a brief text paragraph at the beginning of each session. The eye-tracking data from this period is used to calculate the mean pupil diameter as the baseline for the session's PCPD. We drafted these paragraphs to ensure comparable length and maintain neutral content.

\begin{small}
\begin{tcolorbox}[colback=gray!5, colframe=black!80, title=Text Before Method A (Teleoperation)]
Making a sandwich begins by picking your favorite type of bread. You can spread butter, mayonnaise, or other condiments before adding a layer of vegetables, meat, or cheese. Once the ingredients are in place, press the slices together gently. Preparing a sandwich is a simple task, but it’s also a quick and satisfying way to create a meal.
\end{tcolorbox}
\begin{tcolorbox}[colback=gray!5, colframe=black!80, title=Text Before Method B (Instruction-Following)]
Washing dishes starts by filling the sink with warm, soapy water. Plates, bowls, and utensils are scrubbed clean with a sponge to remove food residue. Once clean, they are rinsed under running water and placed on a rack to dry. While it’s a routine chore, it’s also a small step toward keeping the kitchen tidy and organized.
\end{tcolorbox}
\begin{tcolorbox}[colback=gray!5, colframe=black!80, title=Text Before Method C (IG-MCTS)]
Sitting in a chair can be a relaxing moment during a busy day. You adjust your position to get comfortable, letting your body rest as you settle in. Sometimes, it’s a chance to pause and think quietly. Whether you’re sitting to read, work, or simply take a break, it’s a small but familiar part of daily life.
\end{tcolorbox}
\end{small}

\subsection{PCPD Calculation}\label{appendix:pcpd_calculation}
The percent change in pupil diameter (PCPD) is calculated as:
\begin{equation}
    \text{PCPD}_t = \frac{\text{pupil diameter}_t - \text{baseline diameter}}{\text{baseline diameter}}
\end{equation}
Here, \( \text{baseline diameter} \) is the average pupil diameter recorded during the baseline period, as detailed in \Cref{appendix:baseline_pupil}. For each recording, we compute the mean and standard deviation of PCPD over time. These statistics are then aggregated, and we report the averages in \Cref{tab:eye_metrics}.

\newpage
\subsection{Maze Layouts}\label{appendix:maze_layouts}
The three maze layouts used in the user study were generated from fixed seeds on a Linux system (MuJoCo's randomization differs across operating systems). The layouts are visualized below:

\begin{minipage}{0.32\textwidth}
    \begin{tcolorbox}[colback=gray!5, colframe=black!80, boxrule=0.5mm, 
        sharp corners,  
        title={
            \centering
            \textbf{Demo Layout (Seed: 234)} \\
            \small Similarity: $0.860$ 
        }, width=\textwidth]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/layout1_gt.png}
        \\
        \small Robot Groundtruth
        \\[1em]
        \includegraphics[width=0.8\textwidth]{figures/layout1_human.png}
        \\
        \small Initial Human Map
    \end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
    \begin{tcolorbox}[colback=gray!5, colframe=black!80, boxrule=0.5mm, 
        sharp corners,
        title={
            \centering
            \textbf{Layout 1 (Seed: 666)} \\
            \small Similarity: $0.843$ 
        },
        width=\textwidth]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/layout2_gt.png}
        \\
        \small Robot Groundtruth
        \\[1em]
        \includegraphics[width=0.8\textwidth]{figures/layout2_human.png}
        \\
        \small Initial Human Map
    \end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
    \begin{tcolorbox}[colback=gray!5, colframe=black!80, boxrule=0.5mm, 
    sharp corners,
    title={
            \centering
            \textbf{Layout 2 (Seed: 9)} \\
            \small Similarity: $0.876$ 
        },
        width=\textwidth]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/layout3_gt.png}
        \\
        \small Robot Groundtruth
        \\[1em]
        \includegraphics[width=0.8\textwidth]{figures/layout3_human.png}
        \\
        \small Initial Human Map
    \end{tcolorbox}
\end{minipage}