% \paragraph{Human Perception Modeling.}
% Modeling how humans organize and update their knowledge of the environment is critical for effective human-robot collaboration, especially in settings with incomplete information~\cite{hiatt2017human,tabrez2020survey}.
% % optimal bayesian inference
% Optimal Bayesian inference provides a normative framework for maintaining and updating probability distributions over possible world states~\cite{ross2011bayesian}, assuming that humans process information in a statistically optimal manner.
% However, empirical evidence suggests that humans often deviate from optimality due to cognitive limitations~\cite{simon1955behavioral,miller1956magical}. 
% % Boltzmann rationality
% To address these deviations, Boltzmann rationality has been proposed to model human actions as probabilistic choices based on expected rewards~\cite{luce1959individual,luce1977choice,ziebart2008maximum}.
% Despite recent efforts to extend Boltzmann rationality to account for systematic suboptimality across different states, it remains fundamentally tied to the repeated bandit problem framework, where action utilities are influenced by Gumbel noise~\cite{train2009discrete}. Therefore, it is inadequate for our problem as it fails to account for the influx of information that influences action utilities.
% % our: information processing theory inspired
% Inspired by mental models~\cite{staggers1993mental} and information processing theory, our approach learns a human perception dynamics model from behavioral data, capturing cognitive constraints that may deviate from normative models.


\paragraph{Human Perception Modeling.}
Human perception has been extensively studied in psychophysics~\cite{prins2016psychophysics}, which examines how physical stimuli influence human sensations and perceptions.
% psychometric function
A key model is the psychometric functions, which characterize the probabilistic relationship between stimulus intensity and observer responses~\cite{wichmann2001psychometric,klein2001measuring}. The logistic psychometric function is a widely used variant that captures the gradual transition from detecting to not detecting a stimulus as intensity changes~\cite{treutwein1999fitting}. While psychometric functions provide a structured framework for modeling perception, their reliance on predefined mathematical forms may limit their ability to capture more nuanced perceptual phenomena~\cite{morgan2012observers,garcia2013shifts}.
% For instance, shifts in the psychometric function can result from both perceptual changes and decision biases, making it challenging to distinguish between the two . Additionally, observers can voluntarily adjust their response criteria without affecting sensitivity, indicating that psychometric functions may not fully account for strategic factors in perceptual decision-making .


\paragraph{Teleoperation.}
Teleoperation allows human operators to remotely control robots, facilitating tasks in hazardous or inaccessible environments. Advances in immersive interfaces and reduced latency have improved human-robot interaction~\cite{moniruzzaman2022teleoperation}, but challenges remain, including reliance on stable communication links and maintaining operator situational awareness under high cognitive load. The cognitive burden required for direct control in teleoperation often limits the human-to-robot ratio~\cite{murphy2004human}, as it becomes challenging to effectively manage multiple robots simultaneously.


% \paragraph{Information-Theoretic Navigation Techniques.}
% Information-theoretic techniques are powerful tools for enhancing robot navigation and perception. 
% % active navigation
% Active navigation optimizes robot movements and sensor usage to maximize environmental information gain~\cite{davison1999mobile, burgard1997active}. In contrast, the robot in our approach aims to maximize its human partner's information gain to enhance collaborative decision-making.
% % viewpoint selection
% viewpoint selection employs information theory to choose the most informative camera positions or perspectives~\cite{dufek2021best}, facilitating better collaboration and more effective remote manipulation for both mobile robots~\cite{ito2015optimal} and drones~\cite{senft2022method}. 
% (not ours) Our problem can be interpreted as a viewpoint selection task within a sequential decision-making framework, where the robot aims to assist human partners in better understanding the environment rather than supporting another robot.


% \paragraph{Vision-Language Navigation.}
% % what is VLN
% Vision-and-Language Navigation (VLN)~\cite{anderson2018vision} is a challenging interdisciplinary problem requiring agents to interpret human instructions and navigate unseen environments by executing a sequence of actions.
% % impactful VLN works
% Recent works have made significant progress via synthetic data generation, imitation learning with cross-modal alignment, and reinforcement learning with look-ahead planning~\cite{fried2018speaker,wang2019reinforced,wang2018look}
% % VLN vs our problem
% However, many VLN frameworks rely on simplifying assumptions, such as complete-information, static environments~\cite{kolve2017ai2,puig2023habitat,xia2018gibson}, panoramic action spaces~\cite{fried2018speaker}. In contrast, (this paper/outcome) our research emphasizes human-robot coordination through multimodal communication in environments with incomplete information and actions constrained by local SLAM observations.
% Additionally, most existing approaches assume a complete instruction set given a priori~\cite{tan2019learning}. (remove, talk about papers instead) Recent efforts aim to improve interactivity and reduce uncertainty but still constrain human input to predefined multiple-choice options~\cite{ren2023robots}. In contrast, our approach allows users to specify any route in the environment, which can change as new information is gathered, which is neither given a priori nor constrained to a predefined set.

\paragraph{Vision-and-Language Navigation (VLN).}
VLN tasks require agents to interpret human instructions and navigate through 3D environments by executing action sequences~\cite{anderson2018vision}. 
% impactful VLN works
Methods have leveraged synthetic data generation, cross-modal imitation learning, and reinforcement learning with look-ahead planning~\cite{fried2018speaker,wang2019reinforced,wang2018look}. 
% VLN vs our problem
However, many of these methods rely on simplifying assumptions, such as complete information, static environments~\cite{kolve2017ai2,puig2023habitat,xia2018gibson}, or panoramic action spaces~\cite{fried2018speaker}. Recently, KnowNo~\cite{ren2023robots} explores the use of conformal prediction to quantify uncertainty in robotic scene recognition, but it restricts human input to predefined multiple-choice options, limiting flexibility in real-world scenarios.
In contrast, our approach focuses on human-robot coordination in incomplete and dynamic environments, allowing users to specify flexible routes that evolve as new information is gathered.
