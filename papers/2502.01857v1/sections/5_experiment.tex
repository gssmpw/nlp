\subsection{Human Perception Dynamics Evaluation}
We evaluate the effectiveness of the proposed Neural Human Perception Model (NHPM) in predicting how humans perceive environmental information based on a robot’s movement and transmitted images.

\subsubsection{Baseline: Grid-Based Logistic Psychometric Function}
We compare our method to the Logistic Psychometric Function (LPF), a standard model that relates human observer performance (e.g., detection or discrimination) to stimulus intensity. LPF fits a curve that captures thresholds, slopes, and response variability but lacks the expressive power to model complex spatial dependencies.

To adapt LPF to our setting, we extend it to operate at the grid level, treating each cell in the maze independently. This adaptation, referred to as the Grid-Based LPF (GLPF), models the probability of perception updates as a function of stimulus intensity:
\begin{equation}
P(y = 1 \mid x) = \gamma + \frac{1 - \gamma - \lambda}{1 + e^{-\beta (x - \alpha)}},
\end{equation}
where $P(y = 1 \mid x)$ is the probability of a human updating their perception of a grid cell given stimulus intensity $x$. The slope parameter $\beta$ governs sensitivity to stimulus changes, while $\alpha$ defines the middle point of the logistic curve. $\gamma$ represents the guessing rate---the likelihood of a response in the absence of a stimulus---while $\lambda$ accounts for lapses where the signal is missed even at maximum intensity.

In GLPF, stimulus intensity $x$ is modeled as an exponentially decaying function of the distance $d$ between the grid cell and the robot: $x(d) = e^{-\alpha d}$. We assume correctly labeled grids provide no stimulus.

\subsubsection{Quantitative Evaluation}

We split the dataset into training and test sets and consider three distinct test settings:
\begin{enumerate}[leftmargin=*]
    \item GLPF-Train: The psychometric function is fit on the training set to evaluate how well it generalizes to unseen environments based on prior human data.
    \item GLPF-Test: To establish an upper performance bound, we fit the GLPF directly to the test set. This removes the generalization gap, revealing the best-case scenario for an LPF-based approach.
    \item NHPM: We train our neural network model using backpropagation, optimizing parameters by minimizing the binary cross-entropy loss between predicted and ground-truth human edits to the map.
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_visualization126.pdf}
    \includegraphics[width=\linewidth, trim=0 0 0 30, clip]{figures/comparison_visualization47.pdf}
    \begin{minipage}{0.38\linewidth}
        \centering
        \vspace{-10pt}
        \includegraphics[width=\linewidth]{figures/comparison_visualization_legend_left.pdf}
    \end{minipage}
    \hspace{0.02\linewidth}
    \begin{minipage}{0.57\linewidth}
        \centering
        \vspace{3pt}
        \includegraphics[width=\linewidth]{figures/comparison_visualization_legend_right.pdf}
    \end{minipage}
    \caption{Visualization of human perception models. The left two columns show the inputs, including the human's current map, the robot's path, and the visible grids communicated by the robot. The models predict how the human will update the maze based on this information. \textbf{Top:} The human correctly adds a distant wall instead of a nearby one, a behavior accurately predicted by NHPM. \textbf{Bottom:} The human mistakenly marks a nearby wall at the wrong location. Despite never encountering this exact scenario, NHPM successfully anticipates the error by generalizing from similar training examples.}
    \label{fig:GLPF_vs_NHPM}
    \vskip -0.1in
\end{figure*}


\begin{table}[ht]
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}c@{\hskip 8pt}r}
\toprule
Method & \makecell{Train Loss \\(MBCE)} & \makecell{Test Loss \\(MBCE)} & \makecell{Test Accuracy \\ (IOU @ $\Gamma$)} \\
\midrule
GLPF-Train & $1.15 \cdot 10^{-1}$ & $9.10 \cdot 10^{-2}$  & $0.335 \, @ \, 0.38$ \\
GLPF-Test & N/A & $8.98 \cdot 10^{-2}$ & $0.335 \, @ \, 0.21$ \\
NHPM (Ours)  & $1.36 \cdot 10^{-2}$ & $\mathbf{1.43 \cdot 10^{-2}}$ & $\mathbf{0.352}$  \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Our neural human perception dynamics model (NHPM) achieves lower mean binary cross entropy error (MBCE) and higher prediction accuracy in the test set compared to a grid-based logistic psychometric function (GLPF). The advantage holds even when GLPF is fit directly on the test set and an optimal decision boundary $\Gamma$ is searched for highest accuracy.}
\label{tab:perception_dynamics_evaluation}
% \vskip -0.1in
\end{table}

The test loss and accuracy in \Cref{tab:perception_dynamics_evaluation} highlight NHPM's advantage over GLPF, demonstrating that incorporating spatial structure and contextual awareness improves human perception prediction. Even when fit on test data, GLPF remains limited by its lack of spatial expressiveness, whereas the CNN generalizes effectively from the training set.

\subsubsection{Model Prediction Visualization}

In \Cref{fig:GLPF_vs_NHPM}, we visualize the outputs of human perception models and highlight two representative scenarios where NHPM outperforms GLPF. In the top row, the robot captures an image down a hallway, and the human adds a distant wall instead of a nearby one, likely because it appears in the center of the image and aligns with an existing wall on the map. NHPM accurately predicts this behavior, while the psychometric function assigns a low probability due to the wall's distance. In the bottom row, the human mistakenly marks a wall close by, misjudging its distance from the first-person view image. Despite never encountering this exact test scenario, NHPM correctly anticipates the error by generalizing from similar patterns in the training set.

\subsection{User Study}
% \paragraph{Implementation Details.}
% We implement the algorithm with trained checkpoint, anonymous repo link: []
% we set max SLAM distance to $3$. 
We are interested in the question: Can IG-MCTS, with reduced communication, lower human cognitive load while maintaining task performance comparable to teleoperation and instruction-following?
% \subsubsection{Experimental Design}
To answer this question, we design and conduct a within-subject user study.\footnote{The user study is approved by IRB \#****.}

\paragraph{Independent Variables.}  
The study compares IG-MCTS to two baseline interaction methods:

\emph{Teleoperation:} Participants manually control the robot’s low-level movements by providing actions \(u_t^\mathrm{low} \in \Ucal^\mathrm{low}\) at each timestep \(t\) using keyboard arrow keys. The robot deterministically executes these actions based on the environment’s low-level transition function $T_\mathrm{low}$. The robot streams all incoming local observations \(o_t\) as RGB images to the human operator, providing real-time visual feedback. 

\emph{Instruction-following:} Participants issue guidance in the form of a trajectory $\zeta = \langle s_t, \dots, s_{t+n} \rangle$, specifying the desired sequence of states. The robot autonomously executes this trajectory using its internal model and the transition function \(T\), moving step by step until the trajectory is either completed or blocked. If blocked, the robot pauses execution and prompts humans for updated guidance. Same as in the teleoperation setting, the robot streams RGB images from a front camera view.

% task metrics summary table
\begin{table*}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    Method & Communication (MB) & \#Robot Step & \#Human Guidance & Mapping Accuracy (\%) \\
    \midrule
    Teleoperation & $86.69\pm 32.11$ & $481.60\pm178.40$ & N/A & $18.77\pm21.95$\\
    Instruction-following & $75.94\pm \phantom{0}3.97$ & $421.90\pm\phantom{0}22.08$ & $13.40\pm4.28$ & $42.53\pm\phantom{0}7.92$\\
    IG-MCTS  & $\phantom{0}2.25\pm \phantom{0}0.60$ & $407.10\pm107.80$ & $11.50\pm3.33$ & $49.26\pm12.73$\\
    \bottomrule
    \end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Average task metrics reported as mean $\pm$ standard deviation. Communication assumes $180$ KB/image.}
\label{tab:task_metrics}
% \vskip -0.1in
\end{table*}

\paragraph{Dependent Measures.}  
The study measures the following dependent variables. First, task performance metrics include the number of robot steps, the instances of human guidance provided, and mapping accuracy. 
Second, eye-tracking metrics serve as physiological indicators of cognitive load, including pupil diameter measurements, blink rate, and fixation shifts between areas of interests (AOIs).

% justify eye tracking metric choices
\emph{Why do we choose these eye tracking metrics?}
Task-evoked pupillary responses have long been established as reliable indicators of mental effort~\cite{beatty1982task,hess1964pupil}, with increased cognitive demand leading to greater pupil dilation. In this study, we choose the mean pupil diameter and the percent change in pupil dilation (PCPD)~\cite{kruger2013measuring}.
Blink rate, on the other hand, is inversely correlated with cognitive load, with higher rates indicating reduced mental effort~\cite{zagermann2018studying,zagermann2016measuring}.
Fixation shifts between AOIs reflect visual and cognitive resource allocation during tasks~\cite{joseph2020potential}. Two AOIs are defined: the robot’s ego-centric view (left) and the top-down global maze map (right). A lower fixation shift rate suggests reduced cognitive effort needed to integrate information across the AOIs.


\paragraph{Hypotheses.} We hypothesize that IG-MCTS, compared to teleoperation and instruction-following, will (\textbf{H1}) achieve better or comparable task performance and (\textbf{H2}) yield eye-tracking metrics indicative of lower cognitive load.

% \subsubsection{Conducting the Study}
% The study adopts a within-subject design in which each participant interacts with all three independent variables.

\paragraph{Participants.}
The study recruited $10$ graduate students, with a demographic breakdown of $80\%$ male, $20\%$ female. The participants' average age was $25.9$ years (SD = $1.91$). 
% In response to the background question, ``How familiar are you with keyboard controls (e.g., arrow keys or WASD)?", $X\%$ reported as Novice (little to no experience), $Y\%$ as Intermediate (occasional use), and $Z\%$ as Expert (frequent or proficient use).

\paragraph{Procedure.}
Participants begin the study by wearing the Pupil Labs Core~\cite{kassner2014pupil} eye tracker and completing a 5-point calibration to ensure accurate data collection. After giving consent, they complete three sessions, each corresponding to one of the interaction types. The order of these sessions is randomized to minimize potential ordering effects, such as learning or fatigue~\cite{martin2007doing}.
To isolate the effects of cognitive processing on pupil responses, the experiment was conducted in a controlled lighting environment with emotionally neutral content, minimizing the influence of the two main confounding factors known to affect pupilometry responses.

% to account for individual differences in pupil size.
Each session starts with a baseline pupil diameter measurement lasting up to 30 seconds, during which participants read a brief, neutral paragraph (see \Cref{appendix:baseline_pupil}).
% demo maze (layout 1)
Next, participants familiarize themselves with the task and practice the controls specific to the current method in a demo maze layout.
% actual mazes (layout 2 & 3)
Subsequently, participants complete tasks in two distinct maze layouts, each generated using different seeds but maintaining a comparable level of similarity: $0.843$, and $0.876$ (see \Cref{appendix:maze_layouts}). 
% control maze task difficulty
The same set of maze layouts is used across sessions to standardize task difficulty, with $90^\circ$ or $180^\circ$ rotations to prevent boredom or memorization and maintain participant engagement.

\subsubsection{Results}
\paragraph{On H1 (Task Performance Metrics).} 
\Cref{tab:task_metrics} summarizes the average task metrics across all participants for two maze layouts. The results indicate that IG-MCTS requires significantly less communication compared to teleoperation and instruction-following, as it selectively transmits images at specific angles and times rather than streaming continuously. 
The results in \Cref{tab:task_metrics} show that IG-MCTS results in the fewest robot steps, indicating more efficient task execution. Additionally, IG-MCTS requires less human guidance than instruction-following, demonstrating reduced reliance on human intervention. Finally, IG-MCTS achieves the highest mapping accuracy, outperforming both baselines. 
While these observed trends cannot be concluded as statistically significant due to the small sample size\footnote{The eye-tracking requirement necessitated in-person recruitment, limiting the sample size. We leave recruiting larger sample sizes to future work.}, the results suggest that IG-MCTS achieves at least comparable task performance to the baselines despite significantly reduced communication, providing preliminary support for \textbf{H1}.

% pupil diameter comparison plot
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/pupil_diameter_left_eye.pdf}
    \vskip -0.1in
    \caption{Aggregated mean pupil diameter with a $95\%$ confidence interval (Interpolation: $1000$, Smoothing: $5$).}
    \label{fig:pupil_diameter_plot}
\end{figure}

% eye metrics summary table
\begin{table}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l@{\hskip 3pt}c@{\hskip 3pt}c@{\hskip 3pt}c@{\hskip 3pt}r}
\toprule
Method & \makecell{PCPD (\%)} & \makecell{Blink Rate \\(/min)} & \makecell{Fixation Shift \\ Rate (/min)} \\
\midrule
Teleoperation & $32.74\pm7.95$ & $\phantom{0}8.86\pm5.88$  & $45.69\pm11.50$ \\
\makecell{Instruction-following} & $20.29\pm7.17$ & $10.43\pm4.46$ & $35.18\pm\phantom{0}6.78$ \\
IG-MCTS  & $17.11\pm7.95$ & $12.32\pm7.85$ & $33.30\pm\phantom{0}7.39$ \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Eye-tracking metrics for cognitive load: PCPD (lower is better), blink rate (higher is better), and fixation shift rate (lower is better).}
\label{tab:eye_metrics}
\vskip -0.1in
\end{table}

% aggregate gaze heatmaps
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/gaze_heatmap_legend.pdf}
\begin{minipage}[b]{0.32\textwidth} % use columnwidth if single-column
    \centering
    \includegraphics[width=\linewidth]{figures/gaze_heatmap_teleop_icml.png}
    \vskip -0.1in
    \caption*{(a) Teleoperation}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/gaze_heatmap_monitor_icml.png}
    \vskip -0.1in
    \caption*{(b) Instruction-following}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/gaze_heatmap_coord_icml.png}
    \vskip -0.1in
    \caption*{(c) IG-MCTS}
\end{minipage}
\vskip -0.1in
\caption{Aggregate heatmaps showing gaze point distributions. Dashed red box: robot view AOI; dashed blue box: global map AOI. Fixation rates in each AOI are labeled in white text above the boxes.}
\label{fig:gaze_heatmaps}
\end{center}
\vskip -0.15in
\end{figure*}

\paragraph{On H2 (Eye tracking for Cognitive Load).}
% pupil diameter metrics
\Cref{fig:pupil_diameter_plot} shows the mean pupil diameter for each method, normalized to a $0\text{-}$ time scale, interpolated at $1000$ points, and smoothed with a window size of $5$.
The plot reveals that pupil diameters are overall smallest in IG-MCTS, followed by instruction-following, and largest in teleoperation. This qualitative trend aligns with the percent change in pupil diameter (PCPD) statistics in \Cref{tab:eye_metrics} (see calculation details in \Cref{appendix:pcpd_calculation}), which also show the lowest value for IG-MCTS, followed by instruction-following and teleoperation.
% blink
\Cref{tab:eye_metrics} also shows that IG-MCTS results in a higher blink rate than instruction-following and teleoperation.
% fixation shift
Additionally, \Cref{fig:gaze_heatmaps} presents the aggregate gaze heatmaps from all participants for each method, showing the distribution of attention between the robot ego view and the global map. 
The heatmaps reveal a clear trend in gaze allocation across methods. Teleoperation divides attention between the ego view and the global map due to the demand for continuous monitoring and low-level control. Instruction-following alleviates the need for low-level control, thus shifting more focus to the global map. IG-MCTS concentrates gaze primarily on the global map, as it automates low-level control and provides selective ego-view snapshots.
This observed trend is supported quantitatively by the fixation rates of each AOI, labeled in white text above the AOI boxes in \Cref{fig:gaze_heatmaps}.
This pattern is also reflected in the fixation shift rate differences listed in \Cref{tab:eye_metrics}, where IG-MCTS results in the lowest rate compared to the two baselines.
These results collectively support \textbf{H2}.

\subsubsection{Discussion}
We observe several notable IG-MCTS behaviors during interactions with participants that are reflected as helpful:

\emph{Goal Reaching.}
IG-MCTS accumulates SLAM observations, enabling it to recognize and move toward visible goals within its field of view when it is confident in reaching them. This occurs even if the human has not extended the guided trajectory to the exact goal position, thereby improving overall efficiency.

\emph{Strategic Guidance Compliance.}
When the human guidance is suboptimal, e.g., when the human specifies a route bypassing an alley entrance, unaware of an opening, IG-MCTS robots resolve such situations by assessing the information gained following each action before complying. In this case, rather than blindly following the specified route, the robot stops at the alley entrance and turns toward it to capture and share a snapshot, ensuring the human does not miss important information.

\emph{Efficient Communication.}
Unlike instruction-following robots that take snapshots directly facing walls when blocked, IG-MCTS often angles itself 45 degrees toward the corner of a wall. This allows the human to gather information about two walls within the same snapshot, enhancing communication efficiency.


\paragraph{Implication for Multitasking.}
IG-MCTS robots alternate between execution and ego-view communication phases. While our study’s setup does not fully showcase this, in real-world scenarios with longer execution phases, this approach opens the possibility of a single human operator managing multiple robots by receiving interleaved updates. Although this introduces scheduling challenges, it highlights the potential to improve the human-to-robot ratio, a critical aspect in domains like search and rescue~\cite{murphy2004human}.

% \paragraph{Room for UX improvement.}
% Limitations of the current interaction implementation point to several ways to improve user experience in the future:
% The IG-MCTS system's reliance on a single final ego-view snapshot, while efficient, may lack the continuity needed for effective spatial understanding. Replacing static snapshots with short video clips could provide richer spatial context and help users better grasp movement trends.
% Additionally, the fixed top-down global map may not align well with human spatial cognition, as humans often struggle with spatial reorientation. Adopting a dynamic ego-centric representation of the map could make it easier for users to interpret the robot’s environment and provide guidance without the mental burden of perspective transformation.

% the extend to which/ trade-off between compliance to human guidance vs autonomous exploration (robot confidence/trust in itself vs human) something we can tune but need to be careful since humans have weird varying tolerance about robot disobeying their guidance.
% implication for XAI
