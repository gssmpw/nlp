%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-nature]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}
\usepackage{microtype}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

% %% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
% \newtheorem{theorem}{Theorem}%  meant for continuous numbers
% %%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
% %% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
% \newtheorem{proposition}[theorem]{Proposition}% 
% %%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
% \newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[AI Safety for Everyone]{AI Safety for Everyone}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Balint} \sur{Gyevnar}}\email{balint.gyevnar@ed.ac.uk}
\equalcont{These authors contributed equally to this work.}
\author*[2]{\fnm{Atoosa} \sur{Kasirzadeh}}\email{atoosa.kasirzadeh@gmail.com}
\equalcont{These authors contributed equally to this work.}

\affil[1]{\orgdiv{School of Informatics}, \orgname{University of Edinburgh}, \orgaddress{\street{10 Crichton Street}, \city{Edinburgh}, \postcode{ EH8 9AB}, \country{United Kingdom}}}


\affil*[2]{\orgdiv{Departments of Philosophy \& Software and Societal Systems}, \orgname{Carnegie Mellon University}, \orgaddress{\street{Baker Hall 161
5000 Forbes Avenue}, \city{Pittsburgh}, \postcode{15213}, \country{United States}}}



%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.}



%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{AI Safety, AI Ethics, Safe AI, AI Governance}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{intro}


The rapid development and deployment of AI systems has made questions of safety increasingly urgent, demanding immediate attention from policymakers and governance bodies as they face critical decisions about regulatory frameworks, liability standards, and safety certification requirements for AI systems. Recent discourse has narrowed to focus primarily on AI safety as a project of minimizing existential risks from future advanced AI systems \citep{kasirzadeh2024two,lazar2023ai,ahmed2023building}.\footnote{The focus on existential risks from AI has been particularly connected to normative theories and movements such as rationalism, effective altruism, or longtermism \citep{bostrom2014superintelligence,ord2020precipice}. While these theories offer valuable perspectives on long-term challenges, their specific institutional articulations have faced substantial criticism \citep{Boston2015EffectiveAltruism,gabriel2017effective,eisikovits2023ai}. As the relationship between AI safety and these normative theories and movements is quite complex, the association between AI safety and the conventional conception of AI existential risk is commonly presented as a defining characteristic \cite{CenterforAISafety2023,AISafetyTraining,wiki:aisafety,ahmed2023building,lazar2023ai}. This situation is particularly concerning to researchers and practitioners who work on the development and deployment of safe and responsible AI, but do not align with these normative theories or existential risk narratives posed by AI \citep{aguera2023artificial, roose2023ai, bcs2023pmethics, gilardi2024we}. This disparity has led to a divisive and sometimes unhealthy atmosphere, with some researchers \emph{even} questioning the unique contribution of ``AI safety'' community \cite{bender2023schism}.} This concentrated attention on existential risk has emerged despite --- and perhaps overshadowed --- decades of engineering and technical progress in building robust and reliable AI systems.



Historically, technological and system safety research has evolved alongside each major industrial and computational advance. From ensuring aircraft structural integrity \citep{krause2003aircraft,boyd2017review} to pharmaceutical security \citep{pifferi2003safety,leveson2012applying}, and later expanding to cyber and internet safety \citep{de2019internet,salim2014cyber}, the field of technological and system safety has consistently evolved in reaction to new technological paradigms. This evolution has produced robust engineering practices and governance frameworks \citep{leveson2016engineering,varshney2016engineering,rismani2023plane} that could remain relevant to the challenges of AI development and deployment \citep{dobbe2022system,rismani2023plane,rismani2023beyond}.

In this paper, we conduct a systematic review of primarily peer-reviewed research on AI safety to contextualize AI safety within broader technological and systems safety practice. Our analysis examines two key questions: (1) What categories of safety risks are addressed at different stages of an AI system lifecycle, from development through deployment? and (2) What concrete strategies and methodologies have researchers proposed to mitigate these identified risks? Our findings show that peer-reviewed research on AI safety has included a broad spectrum of concerns throughout AI development and deployment. The mathematical methods, computational tools, and algorithms developed for safe AI address fundamental challenges in current systems, from adversarial robustness to interpretability.

%\textcolor{blue}{While recent years of research on AI safety have seen increased focus on the vision of minimizing existential risks posed by advanced AI systems like artificial general intelligence or artificial superintelligence \cite{ahmed2023building,lazar2023ai,nissenbaum2024ai}, our analysis suggests that this emphasis may have inadvertently overshadowed other pressing concerns with immediate societal impact. The focus on existential risk has been particularly influenced and connected to normative theories and movements such as rationalism, effective altruism, and longtermism \citep{bostrom2002existential,bostrom2014superintelligence,ord2020precipice}. While these theories offer valuable perspectives on long-term challenges, their specific articulations have faced substantial criticism \citep{Boston2015EffectiveAltruism,gabriel2017effective,eisikovits2023ai}. As the relationship between AI safety and these normative theories and movements is quite complex, the association between AI safety and the conventional conception of AI existential risk is commonly presented as a defining characteristic \cite{CenterforAISafety2023,AISafetyTraining,wiki:aisafety,ahmed2023building,lazar2023ai}. This situation is particularly concerning to researchers and practitioners who work on the development and deployment of safe and responsible AI, but do not align with these normative theories or existential risk narratives posed by AI \citep{aguera2023artificial, roose2023ai, bcs2023pmethics, gilardi2024we}.  This disparity has led to a divisive and sometimes unhealthy atmosphere, with some researchers \emph{even} questioning the unique contribution of ``AI safety'' community \cite{bender2023schism}.}





Previous attempts to bridge different safety concerns have often distinguished between concrete, near-term problems and broader, long-term existential challenges \cite{amodei2016concrete,raji2023concrete}. However, our literature review reveals that this dichotomy may oversimplify the rich landscape of AI safety research. The shared vocabulary and overlapping technical challenges—particularly evident in reinforcement learning paradigms where concepts like corrigibility and adversarial robustness span multiple time horizons—suggest the value of a more integrated approach.

Our findings suggest that grounding AI safety discussions in the broader foundations of technological and system safety research could both enrich current debates and provide more robust guidance for policy and governance decisions. While existential and extreme risks from AI remain an important consideration, the field's historical roots in systems engineering and safety practices offer valuable views for addressing immediate challenges in order to build longer-term solutions. This suggests the need for an expanded discourse that reintegrates traditional safety engineering approaches with contemporary AI challenges, rather than treating existential risk as the primary lens through which to view AI safety. By reconnecting with these foundational conceptions, the field may be better equipped to address both immediate safety challenges and longer-term concerns, while maintaining the rigorous technical and theoretical standards that characterize effective safety research.


%\textcolor{red}{Narratives about AI safety have seen escalating growth and influence in recent years, igniting significant and often divisive debate among researchers, policymakers, and the broader public \citep{kasirzadeh2024two,lazar2023ai,ahmed2023building}. The research on AI safety has expanded rapidly, yet the notion of ``AI safety" itself carries an inherent ambiguity, leading to at least two often conflicting interpretations. These competing narratives not only shape public discourse but also significantly impact policy decisions and resource allocation in the field of AI development and regulation.}


%\textcolor{red}{(I) \textbf{AI safety as an organic evolution of technological safety research}. This narrative positions AI safety as a direct descendant of established technological and systems engineering safety practices \citep{leveson2016engineering,varshney2016engineering,rismani2023plane}. Historically, these practices have centred on mitigating technological safety risks, such as ensuring the structural integrity of aircraft \citep{krause2003aircraft,boyd2017review} or the security of pharmaceuticals \citep{pifferi2003safety,leveson2012applying}. With the rise of the internet and its associated threats, the field expanded to encompass cyber and internet safety, developing tools to protect users \citep{de2019internet,salim2014cyber}. AI safety, in this narrative, represents the next logical step, addressing the multifaceted challenges associated with creating, deploying, and managing AI and machine learning (ML) systems responsibly.} 


%\textcolor{red}{(II)~\textbf{AI safety as a distinct field with a specific community and focus on existential catastrophes from advanced AI.} This narrative positions AI safety as a niche field closely associated with a particular group of researchers primarily focused on mitigating existential risks posed by advanced AI like artificial general intelligence or artificial superintelligence \cite{ahmed2023building,lazar2023ai,nissenbaum2024ai}.\footnote{The concept of existential risk, conventionally, has referred to the potential for events or developments that could cause the permanent and drastic curtailment of humanity's potential, such as human extinction~\citep{bostrom2002existential,bostrom2014superintelligence,ord2020precipice}. In the context of AI, existential risk is often associated with the development of advanced AI systems that could directly cause existential catastrophes through unintended consequences, power-seeking AIs, or deliberate misuse \citep{yudkowsky2008artificial,hendrycks2022x,carlsmith2022power}. Within Interpretation II, AI safety research is positioned as the primary solution for mitigating these existential threats.} This community has drawn upon normative theories such as rationalism, effective altruism, or longtermism to frame their approach to AI safety, positioning the mitigation of existential risks posed by advanced AI as a central priority and advocating for the allocation of research resources to \emph{AI safety for minimizing existential risk threats} \cite{ahmed2023building}.\footnote{While certain theoretical principles of effective altruism and longtermism, such as emphasizing the identification and prioritization of the most effective ways to do good and concerns for future generations, are widely accepted, the \emph{actual} instantiation of these theories is peculiar and have faced substantial criticism \citep{Boston2015EffectiveAltruism,gabriel2017effective, eisikovits2023ai}.
%} The emphasis on existential risk often leads to specific prescriptions about global priorities and research resource allocation, potentially overshadowing other \emph{non-existential} pressing concerns with widespread societal impact, such as algorithmic injustice, the proliferation of misinformation, the weaponization of AI, and pervasive surveillance. While the relationship between AI safety and these philosophical movements is complex, the association between AI safety and the conventional conception of AI existential risk is commonly presented as a defining characteristic \cite{CenterforAISafety2023,AISafetyTraining,wiki:aisafety,ahmed2023building,lazar2023ai}, portraying AI safety research as inherently tied to concerns about AI existential catastrophes as formulated by the mentioned normative theories. This situation is particularly concerning to researchers and practitioners who work on the development and deployment of safe and responsible AI, but may not align with these normative theories or existential risk narratives posed by AI \citep{aguera2023artificial, roose2023ai, bcs2023pmethics}.  This disparity has led to a divisive and sometimes unhealthy atmosphere, with some researchers \emph{even} questioning the unique contribution of ``AI safety'' community \cite{bender2023schism}.}


%\textcolor{red}{These conflicting narratives raise a fundamental question: What truly constitutes AI safety research? Is it a comprehensive approach to mitigating all potential safety-related harms associated with AI, or is it primarily concerned with the existential threats posed by artificial general or super intelligence?}


%\textcolor{red}{In response, efforts have been made to refine the characterization of AI safety. One approach involves distinguishing between two categories of AI safety problems: concrete, near-term problems and broader, long-term existential challenges \cite{amodei2016concrete,raji2023concrete}. While this distinction provides some clarity, it does not entirely resolve the issue, as a shared vocabulary exists between these two categories. This overlap is particularly evident in the reinforcement learning paradigm, where challenges like corrigibility (ensuring AI systems remain controllable) and adversarial robustness (resisting malicious attacks) are relevant to both immediate safety concerns and long-term existential risks.}


%\textcolor{red}{To assess the scope of AI safety research with narrative (II), we conduct a systematic literature review, primarily of peer-reviewed research on safe AI. Our analysis sought to answer two questions: (1) What types of risks are addressed in safety studies across the AI system lifecycle? (2) What mitigation strategies are proposed for these identified risks? Our findings challenge the interpretation (II) of AI safety.} 

%\textcolor{red}{We find that research on AI safety extends beyond existential risk concerns, including a broad range of risks throughout AI development and deployment. The mathematical methods, computational tools, and algorithms developed for safe AI are fundamental to current systems, regardless of any future existential threats directly posed by advanced AI. Moreover, we identified diverse mitigation strategies addressing ``non-existential'' risks.}


%\textcolor{red}{Given that AI safety research actively addresses a wide range of risks beyond existential threats, we argue that the discourse surrounding narrative II needs to become more epistemically inclusive. This means embracing the diverse concerns and aspirations that characterize the ongoing discussion about (the future of) safe AI, rather than focusing solely on existential risk framing of AI safety work. Our empirical evidence aligns more closely with interpretation (I) on AI safety: AI safety research is a direct descendant of established technological and systems engineering safety practices. Given that our findings demonstrate AI safety research actively addressing a wide range of risks independent of existential catastrophes, we argue that the discourse surrounding its scope and relevance needs to become more inclusive. This means embracing a broader conception of AI safety that encompasses both immediate and long-term risks, while potentially reframing existential risk mitigation as one important component within this larger framework, rather than the defining feature of the field.}

\textbf{Limitations.} We follow a systematic review methodology for our analysis, but certain limitations must be considered when interpreting our results. First, AI safety is a dynamically evolving field with continuous developments across numerous research venues. New research entities are emerging, conducting daily studies within and outside organizations, including research outputs from AI companies like Anthropic, OpenAI, and other relevant institutions. A one-off literature review, such as ours, captures the state of the art only at the moment of querying, which, in our case, was on November 1, 2023. This means our findings may not fully reflect the most recent advancements in AI safety research. Second, our focus on peer-reviewed research, while ensuring a standard of quality and rigor, inherently excludes a significant portion of the AI safety landscape. Pre-print repositories like arXiv often serve as the first outlet for AI safety research, and their exclusion from our review may result in overlooking important contributions. While we attempted to mitigate this limitation through snowball sampling of high-impact arXiv papers, this approach is not comprehensive and carries the risk of missing relevant publications. Third, our review process involved annotating each selected paper with relevant keywords beyond those provided by the authors. This approach inevitably introduces some degree of annotator bias. However, given the substantial volume of papers selected (383 in total), we believe the impact of this bias on our overall conclusions is minimal. Finally, our review does not encompass the extensive discussions and non-peer-reviewed work found in online forums like LessWrong or the AI Alignment Forum, which are central platforms for discussing AI safety topics within certain communities. A truly comprehensive analysis of AI safety research would ideally include a detailed content analysis of these platforms and other non-peer-reviewed documents. Therefore, it is important to interpret our findings with caution, acknowledging that they provide a valuable but not exhaustive overview of the current state of AI safety research. Future research could expand upon our work by incorporating a wider range of sources, including other pre-print sources and non-peer-reviewed literature, to gain a more comprehensive understanding of the evolving landscape of AI safety. Despite these limitations, our findings provide a valuable snapshot of peer-reviewed AI safety research and motivates the need for a more inclusive understanding of the field's perceived scope and motivations. The novelty of our paper is leveraging empirical investigations in clarifying contested discussions about AI safety. Our review offers insights into research outputs, communities, and potential avenues for fostering healthy research development. In the concluding section, we outline key next steps and open questions to guide future research.



%The AI safety narrative often depicts its challenges as unique and complex, particularly concerning AGI or ASI safety. However, given that methodologies for AGI/ASI largely derive from current machine learning technologies – techniques that have been widely utilized and progressively refined – it is imperative to reevaluate the supposed uniqueness of these safety challenges. Our comprehensive review aims to assess the contributions of AI safety researchers to date, determining whether the identified safety issues are merely adaptations of existing challenges for more advanced AI models, or if they represent new categories of safety research warranting the 'existential AI safety' label. We propose exploring the possibility of replacing the vague and contentious term 'existential AI safety' with a more grounded approach, emphasizing specific problems in ML/Reinforcement Learning (RL) safety pertinent to cutting-edge AI advancements.


%Our proposed comprehensive literature review aims to address this by examining the efforts of AI safety researchers to date. We find that several safety issues are in fact extensions of existing problems, adapted to more advanced models. Consequently, we suggest reframing whatever researchers mean by 'existential safety' issues within technically-grounded and well-defined notions, for example tied to the realm of 'Reinforcement Learning (RL) safety', focusing on cutting-edge AI developments while avoiding overly sensationalized terminology.

%The discourse on AI safety frequently portrays its challenges as highly complex and distinct, especially in the context of AGI or ASI safety. Yet, given that the methodologies for AGI/ASI are largely extrapolated from existing machine learning technologies --- techniques that have been extensively used and incrementally improved --- it is crucial to reassess the uniqueness of these purported safety challenges. Our comprehensive literature review seeks to explore the contributions of AI safety researchers so far. We would like to explore whether many of the identified safety concerns are, in reality, adaptations of pre-existing issues, tailored to more advanced AI models, or whether there are genuinely new categories of safety research that demand the use of the term ``existential AI safety''. We would like to explore whether we can replace the ambiguous and contested notion of ``existential AI safety'' with more grounded approach, emphasizing specific problems with ML/Reinforcement Learning (RL) safety and its relevance to cutting-edge AI advancements.

%The recent surge of dialogues surrounding AI safety has been overshadowed by speculative and philosophical discussions concerning the feasibility and implications of approaching Artificial General Intelligence (AGI) or Artificial SuperIntelligence (ASI). In contrast, some view AI safety as a straightforward extension of traditional technological safety research, focusing on practical concerns. 

%The discourse on AI safety often seems to demand highly complex and isolated solutions, especially in the context of Advanced General Intelligence (AGI) or Artificial Superintelligence (ASI) safety. However, considering that current AGI/ASI methodologies largely derive from existing machine learning tools—which have been widely used and progressively made safer—it's pertinent to reevaluate the novelty of these safety challenges. 

%Our argument starts from the position that AI safety should be viewed as part of an ongoing series of challenges in technological safety. In light of this, we suggest a reevaluation of AI safety's conceptual framework, striving for a more unified approach. This might involve rebranding certain aspects, such as existential AI safety, to better reflect their scope and significance in the broader context of AI development.
%We start our exploration from the historical understanding that the evolution of AI systems has been a continual process, marked by new paradigms and approaches. AI safety, thus, continues to be a critical concern, now presenting updated challenges and novel predicaments for sophisticated Reinforcement Learning (RL) agents. In our empirical investigation, we explore answering the following questions: (1) What sources of risks related to the \textit{design/development}, \textit{deployment}, \textit{operation}, and \textit{decommissioning} of ML systems are addressed in recent AI safety research? (2) What mitigation strategies -- e.g., concrete methods, design principles, governance recommendations, etc. -- are proposed in recent AI safety research that directly address one or more of the above sources of AI risk? (3) How general are the proposed mitigation strategies in terms of the various sources of risk that they address and the various ML models they can be applied to?

% normative suggestions maybe --- We need to keep these two levels independant. Problems should be accessible to everyone, even those who do not believe in existential risks from AI, or longtermism, or effective altruism. of course, companies have many corporate motivations to label their future technologies as AGI or whatever.


%Robustness Create models that are resilient to adversaries, unusual situations, and Black Swan events.

%Monitoring Detect malicious use, monitor predictions, and discover unexpected model functionality.

%Alignment Build models that represent and safely optimize hard-to-specify human values.

%Systemic Safety Use ML to address broader risks to how ML systems are handled, such as cyberattacks

%The discourse on AI safety often seems to demand highly complex and isolated solutions, especially in the context of Advanced General Intelligence (AGI) or Artificial Superintelligence (ASI) safety. However, considering that current AGI/ASI methodologies largely derive from existing machine learning tools—which have been widely used and progressively made safer—it's pertinent to reevaluate the novelty of these safety challenges. Our proposed comprehensive literature review aims to address this by examining the efforts of AI safety researchers to date. We find that many proposed safety issues are in fact extensions of existing problems, adapted to more advanced models. Consequently, we suggest reframing 'existential safety' issues within the realm of 'Reinforcement Learning (RL) safety', focusing on cutting-edge AI developments while avoiding overly sensationalized terminology related to existential risk and superintelligence.




%Subsequently, we propose a framework for developing a more cohesive understanding of AI safety. This framework aims to integrate the varying perspectives into a unified conception, which acknowledges both the unique challenges presented by advanced AI systems and the continuity of safety concerns that have been prevalent in technology for an extended period.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systematic Review Methodology}\label{sec:methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We conduct a systematic literature review of primarily peer-reviewed AI safety research.\footnote{We will release publicly the selected papers with annotations and the code used to analyze this data after the review process.}
Our review follows the guidelines outlined by Kitchenham and Charters~\cite{kitchenhamGuidelinesPerformingSystematic2007} and complemented by snowball sampling as recommended by Wohlin~\cite{wohlinGuidelinesSnowballingSystematic2014}.
This approach combines a structured and reproducible methodology for mapping the field of AI safety research, as represented in peer-reviewed and indexed papers, with a targeted technique to capture emerging research not yet fully represented in peer-reviewed literature.

We conducted our review and analysis to investigate two key Research Questions~(RQ) in relation to the peer-reviewed published research on AI safety:

\begin{enumerate}
    \item[\textbf{RQ1}] What types of risks related to the lifecycle (design, development, deployment, operation, and decommissioning) of AI systems are addressed in AI safety research?
    \item[\textbf{RQ2}] What mitigation strategies -- e.g., concrete methods, design principles, governance recommendations -- are proposed in recent AI safety research that directly address one or more of the above sources of AI risk? 
\end{enumerate}

\subsection{Search and Review Process}

To conduct our systematic review, we perform a multi-stage query process to identify relevant papers from the Web of Science (WoS) and Scopus indexing databases, as these include not only computer science but also other peer-reviewed research for capturing interdisciplinary work.
We develop a hierarchy of increasingly refined queries for each research question, targeting the title, abstract, and author keywords of papers.
The querying process was finalized on November 1, 2023.  We outline the query hierarchy below, using WoS notation:


\begin{itemize}
    \item $q_1:$ AI $\vee$ AGI $\vee$ frontier AI $\vee$ artificial intelligence $\vee$ artificial (general $\vee$ super) intelligence $\vee$ (machine $\vee$ supervised $\vee$ unsupervised $\vee$ semi-supervised $\vee$ reinforcement) learning;
    \item $q_2:$ safe* $\vee$ robust* $\vee$ align*;
    \item $q_3:q_1 \wedge q_2$;
    \item $q_4:q_3 \wedge$ ($q_{4a} \vee q_{4b} \vee q_{4c} \vee q_{4d}$):
    \begin{itemize}
        \item[$\circ$] $q_{4a}:$ design $\vee$ develop* $\vee$ architecture $\vee$ model* $\vee$ framework $\vee$ system %$\wedge$ algo*
        \item[$\circ$] $q_{4b}:$ deploy* $\vee$ distribut* $\vee$ data* $\vee$ train* $\vee$ fine-tun*
        \item[$\circ$] $q_{4c}:$ operat* $\vee$ interact* $\vee$ online
        \item[$\circ$] $q_{4d}:$ decomission* $\vee$ remov* $\vee$ eras* $\vee$ delet*
    \end{itemize}
\end{itemize}

We start with a broad query ($q_1$) that includes various terms related to artificial intelligence (AI) and machine learning (ML), such as ``AI'', ``AGI'', ``frontier AI'', ``artificial intelligence'', ``machine learning'', ``supervised learning'', ``unsupervised learning'', etc (see $q_1$ above for full details). 
This initial query aims to capture all papers potentially related to AI research. 
Next, we introduce a second query ($q_2$) that incorporates high-level safety-related keywords of ``safe*'', ``robust*'', and ``align*''.
The asterisks act as wildcards to capture variations of these terms (e.g., ``safety'', ``safer'', ``safest'', ``robustness'', etc.). 
We then combine $q_1$ and $q_2$ to create $q_3$, which selects papers relevant to AI or ML research while ensuring they also address safety-related concepts. 
This step narrows down the results to publications specifically focusing on AI safety. 
To further refine the selection and align it with the specific focus areas of our RQ1, we introduce $q_4$. 
This query includes four sub-queries, each corresponding to terms related to the four areas of RQ1: design, deployment, operation, and decommissioning of AI systems. 
To ensure that we do not miss important non-peer-reviewed contributions, we supplement our systematic search with snowball sampling, starting with 11 seed papers identified as highly influential in the AI safety field~\cite{irvingAISafetyDebate2018,ngAlgorithmsInverseReinforcement2000,hendrycksAligningAIShared2021,yampolskiyArtificialIntelligenceSafety2016,amodei2016concrete,hadfield-menellCooperativeInverseReinforcement2016,xuMachineUnlearningSurvey2023,russellResearchPrioritiesRobust2016,willersSafetyConcernsMitigation2020,mohseniTaxonomyMachineLearning2022,hendrycksUnsolvedProblemsML2022}. 
This allows us to identify additional relevant papers that are not captured by the initial queries.

After removing duplicates, our query process yielded 2,666 papers from the database search and 117 papers from snowball sampling.
We then conducted a two-stage review process, first filtering based on titles and abstracts, followed by a comprehensive full-text review. 
This resulted in a final set of 383 papers for our analysis.
We applied the following exclusion criteria during both the title/abstract screening and full-text review stages to determine a paper's eligibility for inclusion in our analysis: (A) focus: the paper does not primarily focus on AI or a directly related subfield; (B) motivation: the paper's stated motivation does not include a clear need for developing safe AI algorithms; (C): generalizability: the paper's contributions are specific to a very particular application domain and do not offer broader insights or methodologies applicable to AI safety in general.

\subsection{Annotation Process}

As part of our review process, we recorded important metadata about each paper, which included the publication year, author affiliations, and Google Scholar citation count (as of January 27, 2024). 
We then performed a thorough inductive coding process~\cite{boyatzis1998transforming} across the selected papers.
We enriched each paper's author-written keywords by adding relevant terms (i.e., codes) based on a thorough reading of the full text. 
This expanded set of keywords encompassed both problem- and method-specific terms, as well as broader categorizations such as ``algorithm'' (for papers that propose an algorithm), ``theoretical'' (for purely theoretical contributions), and ``framework''.
We also gradually refined and consolidated our set of keywords as we progressed through the papers.
After gaining a holistic understanding of the selected publications, we further categorized each paper based on its assigned keywords according to its methodological approach, the specific safety risks it addressed, the types of risks mitigated by its proposed methods, and the overarching category of its methodology.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical findings}\label{sec:findings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/bibliometric/wc-tfidf.pdf}
    \caption{World cloud of morphologically standardised terms occurring in the \textit{abstracts} and \textit{titles} of selected papers weighted by their \textit{tf-idf} score. We excluded stop words from the analysis.}\label{fig:wordcloud}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/bibliometric/keywords-10-60p.pdf}
    \caption{Graph of \textit{term} co-occurrence in abstracts with binary counting, a minimum term frequency of 10, and a relevance score of at least 60\%. The figure was produced using the \textit{VOSViewer} tool~\cite{vaneckSoftwareSurveyVOSviewer2010}. Due to the large number of nodes, not all labels are shown in the figure.}\label{fig:keywords-abstract}
\end{figure*}

To begin, we present a high-level bibliometric overview of the trends and concepts prevalent in the AI safety literature we reviewed. 
Looking at the total number of publications per year in~\cref{fig:pub-years}, we observe increasing growth since 2016, which we assume is partially caused by the extensive development and deployment of deep learning models.
Nevertheless, this observation reinforces the need to look more deeply into the state of the field.
We first look at a word cloud of salient terms to understand the most important concepts among our selected papers. We then analyse patterns that emerge from the co-occurrences of different terms in abstracts and titles. 
% This hypothesis is further supported by a comparison to the initial unfiltered set of retrieved papers, which similarly demonstrates a surge in publications from 2016.


\subsection{Word cloud of salient terms} 
\Cref{fig:wordcloud} illustrates a word cloud of the most salient terms found in the abstracts of the selected papers, after morphological standardization. 
The terms are ranked by their \textit{tf-idf} score, a metric that emphasizes both the frequency and distinctiveness of a term within the corpus. 
This allows us to highlight important, but potentially less frequent, terms while de-emphasizing common or redundant words. The word cloud highlights several prominent themes within the AI safety literature. 
A significant portion of papers focus on safe reinforcement learning (RL), evidenced by terms like ``robust'', ``control'', ``agent'', and ``explore''.
 Additionally, there is a strong emphasis on adversarial attacks, as indicated by the prevalence of terms such as ``adversarial'' and ``attack.''
 Finally, domain adaptation emerges as another significant area of concern, with terms like ``domain'', ``distribution'', and ``adapt'' appearing frequently. 


\subsection{Term co-occurrence graph} 
To gain deeper qualitative insights into the landscape of peer-reviewed AI safety research, we constructed a co-occurrence graph of terms appearing in the abstracts and titles of selected papers using the \textit{VOSViewer} tool~\cite{vaneckSoftwareSurveyVOSviewer2010}. 
\Cref{fig:keywords-abstract} depicts this graph.
Nodes represent terms and edges connect terms that co-occur within the same abstract or title. 
The size of each node reflects its relevance score as calculated by \textit{VOSViewer}. 
We observe four distinct clusters.

The \textbf{motivations and concerns driving AI safety} research is shown in the largest, red cluster. 
We observe a clear emphasis on both the development and deployment of AI systems, especially in relation to humans, health, and society. 
Here, the focus is human- and society-centric aspects of AI safety, including trust, accountability, and safety assurance.

\textbf{Safe reinforcement learning} and related terms are aggregated in the blue cluster. 
Research in this cluster primarily centres on the safe control of agents under constraints in uncertain dynamic environments. 
While most studies test methods in simulated environments, there is also considerable work on the safety of non-linear systems employing RL for optimal control. 
Overall, the focus tends towards mathematically well-defined problems like optimality, convergence and stability, sample efficiency, and constraint satisfaction.

\textbf{Supervised learning}, primarily classification tasks, is the predominant focus of the green cluster.
This cluster features research on methods such as neural networks, extreme learning machines, support vector machines, and random forests.
Studies prioritize addressing issues like robustness to noise and outliers, generalization performance, and accuracy.

Finally, \textbf{adversarial attacks and defence} is highlighted in the yellow cluster, especially as they relate to deep neural networks.
The existence of a separate cluster for this class of problems emphasizes the importance of this method.
Methods here focus on robust-to-outlier optimization, adversarial training through synthetic datasets, and learning robust representations via semi-supervised learning.


The four clusters identified in the co-occurrence graph offer a strong reason for viewing AI safety research as a natural extension of traditional technological safety practices. The centrality of human and societal well-being in the red cluster aligns with the core purpose of technological safety: safeguarding human life and minimizing harm. This focus echoes traditional safety concerns in fields like aviation \citep{oster2013analyzing,boyd2017review} or medicine \citep{donaldson2000err,bates2023safety}, with an emphasis on protecting users and the public. The emphasis on mathematically rigorous control of agents under constraints in uncertain environments in cluster 2 directly mirrors the principles of control theory and systems engineering, which are foundational to ensuring the safety of complex systems \citep{leveson2016engineering,marais2004beyond}. The focus on optimality, stability, and constraint satisfaction reflects the fundamental goals of traditional safety engineering, such as preventing failures and ensuring predictable behavior. In the green cluster, the emphasis on robustness, generalization, and accuracy in supervised learning reflects the core concerns of reliability engineering. Just as engineers strive to build robust and reliable physical systems, AI safety researchers are focused on creating machine learning models that can perform consistently and accurately in diverse real-world scenarios. The yellow cluster shows the growing recognition of cybersecurity as a critical component of technological safety \citep{griffor2016handbook,prasad2020cyber}. The focus on adversarial attacks and defense strategies mirrors the evolving challenges of protecting digital infrastructure and data integrity. %This area of research is essential for ensuring the resilience of AI systems against malicious actors and preventing unintended harm.




\section{RQ1: Types of Safety Risks}\label{sec:risks}


\begin{table*}
    \centering
    \begin{tabular}{@{}p{2.35cm}lp{9cm}@{}}       
        \toprule
        \textbf{Risk Source} & \textbf{Ref.} & \textbf{Contribution} \\
        \midrule 

        Undesirable  & \cite{soaresCorrigibility2015} & Corrigibility of rational, utility-based agents  \\
        behavior & \cite{ringDelusionSurvivalIntelligent2011} & What happens when an agent can modify its own code?  \\        
        \midrule

        Non-stationary & \cite{ganinDomainAdversarialTrainingNeural2017} & Domain-adversarial training of robust neural networks \\
        distributions  & \cite{balajiMetaRegDomainGeneralization2018}  & Domain generalization via meta-regularization \\       
        \midrule

        Adversarial  & \cite{goodfellowExplainingHarnessingAdversarial2015} & Qualitative analysis of adversarial perturbations \\
        attack       & \cite{madryDeepLearningModels2019} & Adversarial robustness as robust optimization  \\
        \midrule 

        Unsafe      & \cite{rayBenchmarkingSafeExploration2023}  & Benchmarking safe exploration in deep RL \\
        exploration & \raggedright\cite{turchettaSafeExplorationInteractive2019}   &  Safe exploration for interactive machine learning \\
        \midrule           
        
        Lack of control & \cite{ouyangTrainingLanguageModels2022} & LLMs with human feedback to follow instructions \\
        enforcement     & \cite{abbeelApprenticeshipLearningInverse2004} &  Imitation learning via inverse reinforcement learning \\
        \midrule

        System           & \cite{izzoApproximateDataDeletion2021} & Efficient machine unlearning  \\
        misspecification & \cite{wuDeltaGradRapidRetraining2020} & Rapid retraining of ML models  \\ 
        \midrule  

        Lack of      & \cite{adebayoSanityChecksSaliency2018} & Sanity checks for interpreting saliency maps \\
        monitoring   & \cite{kimInterpretabilityFeatureAttribution2018} & Concept activation vectors in CNNs for interpretability \\ 
        \midrule      

        Noise and & \cite{hendrycksBaselineDetectingMisclassified2018} & Baseline for detecting misclassified/OOD samples  \\
        outliers  & \cite{hendrycksBenchmarkingNeuralNetwork2019}      & Benchmarking robustness to corruptions/perturbations \\

        \bottomrule
    \end{tabular}
    \caption{Representative and influential contributions with concrete methods addressing problems for each risk type. Publications were picked by citation count.}
    \label{tab:influential-papers}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/annotations/risks.pdf}
    \caption{The number and percentages of the selected papers that address various risk types.}
    \label{fig:risks}
\end{figure}


%\begin{table*}
%    \centering
%    \begin{tabular}{lp{12.3cm}}
%        \toprule
%        \textbf{Risk type} & \textbf{Examples sources of risk} \\
%        \midrule
%        Undesirable \newline behavior & wireheading, corrupt reward signal/transition function, hallucination \\
%        Non-stationary \newline distributions & lack of generalization, domain uncertainty, non-stationary distribution \\
%        Adversarial attack & adversarial sample, model/label poisoning, backdoor attack\\
%        Unsafe exploration & unsafe actions, constraint violation, unconstrained exploration \\
%        Lack of control \newline enforcement & misalignment, lack of systemic safety, unverifiable decisions, watermarking \\
%        System misspecification & incorrect requirements, bad modelling choices, wrong hyper-parameters, domain change \\
%        Lack of monitoring & black-box system, bias, privacy violation, unfairness \\
%        Noise and outliers & label noise, input perturbation, OOD samples \\
%        \bottomrule
%    \end{tabular}
%    \caption{Subdivision of identified risk types into various sources of risk as found during our inductive coding process.}\label{tab:risk-subdivision}
%\end{table*}


To better understand the motivations behind research on AI safety, it is important to examine the various types of risks addressed in research on AI safety. 
Building on DeepMind's categorization of AI safety risks~\citep{ortega2018building}, our review identifies eight overarching risk types, as depicted in~\cref{fig:risks}.

These risks in increasing order of frequency are: undesirable behavior, non-stationary distributions, adversarial attacks, unsafe exploration, lack of control enforcement, system misspecification, lack of monitoring, and noise and outliers.


%Each of these risk types may be further subdivided into more specific subtypes, as shown in~\cref{tab:risk-subdivision}.
%We derived this more granular view of risk types through an inductive coding process during the annotation of the selected papers.
%We also collate a list of relevant and influential papers in~\cref{tab:influential-papers} that gives a view of the origins of the study of each risk type.

The largest group of papers focuses on risk stemming from \textbf{noise and outliers}.
Recurring challenges in this area include brittle representations~\cite{mengDistantlySupervisedNamedEntity2021}, low classification performance~\cite{wangRobustAutomatedMachine2021}, a lack of generalization in the presence of noisy labels~\cite{cappozzoRobustApproachModelbased2020}, outliers~\cite{liRobustSupervisedSubspace2021}, or input perturbations and corruptions~\cite{curiCombiningPessimismOptimism2021}.

The current literature also studies the significant \textbf{lack of monitoring} of AI systems. Research addressing this risk ranges from theoretical violations of ethical and safety principles ~\cite{dobbeHardChoicesArtificial2021} to privacy violations~\cite{dworkPrivacypreservingPrediction2018}. The ``black-box'' nature of popular deep learning systems has also drawn considerable attention due to its potential to diminish human agency and obstruct understanding of the system's internal workings and memory. Consequently, there has been a surge of interest in reverse engineering ML models~\cite{elhageMathematicalFrameworkTransformer2021}, developing interpretable representation learning~\cite{kimDisentanglingFactorising2019}, and advancing explainable AI~\cite{wardAssuranceCasePattern2020,gyevnar2023transparencyGap}.


\textbf{System misspecification} or misunderstanding the requirements and purpose of AI-based systems, has also garnered significant attention. 
Potential risks include incorrectly eliciting requirements for AI systems' capabilities~\cite{reimannSafeDSDomainSpecific2023}, making suboptimal modelling choices~\cite{deyMultilayeredCollaborativeFramework2023}, or choosing inappropriate hyperparameters~\cite{weiModelSelectionApproach2022}.
Additionally, methods may fail to adapt to the changing requirements of their domain~\cite{ghoshDeploymentRobustCooperative2020a}, particularly due to the slow pace of retraining AI models~\cite{wuDeltaGradRapidRetraining2020}.
Privacy violations due to inadequate ethical data management also fall under this risk category~\cite{izzoApproximateDataDeletion2021}.


Significant attention has also been given to risks originating from a \textbf{lack of control enforcement}.
A primary research focus here is the misalignment of agent goals with human preferences~\cite{hadfield-menellCooperativeInverseReinforcement2016}, which can lead to wireheading~\cite{everittAvoidingWireheadingValue2016}, mesa-optimization (i.e., an optimizer creating another optimizer)~\cite{mesaoptimisation2019}, and other undesirable emergent behaviors~\cite{pistonoUnethicalResearchHow2016}. There is also a focus on the lack of systemic safety, where the goal is to ensure that the AI system is safe in the broader context of its deployment~\cite{picardiAssuranceArgumentPatterns2020,hendrycksAligningAIShared2021}.


A significant portion of research focuses on the \textbf{unsafe exploration} of autonomous agents, primarily in the context of reinforcement learning~\cite{wabersichProbabilisticModelPredictive2022}, constraint violations~\cite{wenConstrainedCrossEntropyMethod2021}, unintended behavior from incorrect domain or reward specification~\cite{weiModelSelectionApproach2022}, and incorrect behavior due to continuous deployment and learning~\cite{zanella-beguelinAnalyzingInformationLeakage2020,wangDirichletProcessMixture2022}.



\textbf{Adversarial attacks} constitute another significant risk, addressed by methods that aim to create or detect such attacks~\cite{zouUniversalTransferableAdversarial2023} or leverage adversarial samples for more robust training~\cite{ilyasAdversarialExamplesAre2019a}.
The risk of poisoned training data is also a concern~\cite{heNotAllParameters2022,aghakhaniBullseyePolytopeScalable2021}. 
Adversarial attacks not only compromise the robustness and generalization performance of AI systems but can also introduce a backdoor~\cite{liuBackdoorDefenseMachine2022}, which can potentially be exploited by malicious actors.



The challenges of \textbf{non-stationary distributions}, where distributions change over time, have also been explored.
These studies address issues such as behavior in the presence of out-of-distribution samples~\cite{meinkeNeuralNetworksThat2020}, non-stationary environments in RL~\cite{abdelfattahRobustPolicyBootstrapping2020}, partial information~\cite{djeumouTaskGuidedInverseReinforcement2021}, and domain adaptation~\cite{ghoshDeploymentRobustCooperative2020a}.
The overarching goal is to ensure safe behavior and limit the consequences of unsafe actions, even in novel or unforeseen situations.



The final group of risk types is concerned with \textbf{undesirable behavior} of AI systems.
Research has looked into mathematically proving certain unfavourable outcomes of utility-maximising rational agents, where issues include self-modifying~\cite{ringDelusionSurvivalIntelligent2011} and wireheading agents that by-pass reward signals to maximise their own utility~\cite{everittAvoidingWireheadingValue2016}, and corrigibility of uncooperative agents~\cite{soaresCorrigibility2015}. 
Additionally, there are some papers that directly discuss the existential risk of AI systems as a fundamental problem of AI safety.
These papers present theoretical arguments regarding the emergence, design, and containment of malevolent and superintelligent AI~\cite{yampolskiyLeakproofingSingularityArtificial2012,yampolskiyArtificialIntelligenceSafety2013,yampolskiyTaxonomyPathwaysDangerous2015}.



These identified risk types in AI safety research closely mirror those found in established fields of technological safety research. In AI safety, the focus on ensuring the reliability and predictability of AI systems, including robustness to noise and outliers, generalization performance, and adaptability, mirrors concerns in engineering fields where reliability is of core concern, such as aviation \citep{boyd2017review} or nuclear power \citep{wheatley2016reassessing}.

Furthermore, the research on adversarial attacks, control enforcement, and safe exploration in AI safety directly translates to broader concepts of system robustness and control. The goal of designing systems that can withstand unexpected perturbations, resist malicious attacks, and operate safely within defined boundaries is shared by both AI safety and traditional safety engineering. For example, the use of formal verification methods to ensure the safety of AI systems has roots in the verification of software and hardware systems. 

%It is also worth noting, that though some works mention longtermist concerns as motivation~\cite[e.g.,][]{irvingAISafetyDebate2018,ilyasAdversarialExamplesAre2019a,bills2023language,hendrycksAligningAIShared2021}, they focus on addressing concrete technical research problems. 
%For example, \citet{bills2023language} note that ``particularly important [for interpretability] is detecting examples of goal misgeneralization or deceptive alignment'' with~\citet{ilyasAdversarialExamplesAre2019a} adding that ``adversarial examples are a fundamentally \textit{human} phenomenon. [\dots] we should not be surprised that classifiers exploit highly predictive features that happen to be non-robust under a human-selected notion of similarity''.
%Additionally, \citet{falcoParticipatoryAIReducing2019} note that if the consequences of a safety breach are high then it is essential for ``the agent to recognize [undesirable behavior] and cease pursuit of its primary goal.''


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/bibliometric/pub-years.pdf}
    \caption{The number of publications over time since 2015 for the different risk types that were identified in the paper as a result of a systematic literature review.}\label{fig:pub-years}
\end{figure}



\section{RQ2: Developed AI Safety Methodologies}\label{ssec:methods}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/annotations/framework_algo.pdf}
    \caption{Comparison of papers that provide only theoretical results without significant empirical testing against those that give sufficient evaluation of their proposed system. Papers are further grouped by whether they propose a concrete algorithm. Note, that the number of papers overlap as some propose solutions in more than one category.}
    \label{fig:framework-algos}
\end{figure}

To address RQ2, we now examine the concrete mitigation strategies proposed in recent AI safety research to directly address the aforementioned sources of AI risk: ``What mitigation strategies – e.g., concrete methods, design principles, governance recommendations – are proposed in recent AI safety research that directly address one or more of the above sources of AI risk?''


We see a dynamic landscape of AI safety research, encompassing both theoretical and applied approaches (\cref{fig:framework-algos}). We consider a work as theoretical if its primary conclusions stem from theoretical analysis, mathematical proofs, philosophical arguments, literature reviews, or conceptual frameworks that are not empirically validated. Conversely, a work is classified as applied if its conclusions are derived from empirical evidence and supported by data.


Our analysis shows the following trend: theoretical works in AI safety predominantly offer general frameworks and recommendations~\cite[e.g.,][]{tayStudyRealtimeArtificial1998,hibbardAvoidingUnintendedAI2012,sezenerInferringHumanValues2015,elmhamdiWhenNeuronsFail2017,freieslebenGeneralizationTheoryRobustness2023,sannemanTransparentValueAlignment2023}, while applied works primarily focus on developing and testing concrete algorithms~\cite[e.g.,][]{abbeelApprenticeshipLearningInverse2004,murphyLearningEffectiveInterpretable2012,subramanianSPINESParseInterpretable2017,shahamUnderstandingAdversarialTraining2018,wuPUMAPerformanceUnchanged2022,zouUniversalTransferableAdversarial2023}. This dichotomy highlights a key distinction: theoretical research often prioritizes conceptual and methodological foundations, while applied research emphasizes practical implementation and testing. However, the applied algorithms frequently lack the rigorous guarantees that accompany analytical results found in theoretical work. 



Beyond the theoretical/applied divide, we investigate the specific methodologies employed in AI safety research. \cref{fig:methods} illustrates the broad categories of methods proposed in the selected papers, each accompanied by representative citations based on citation counts to highlight their prevalence in the literature.



\textit{Applied algorithms} research focuses on developing and empirically evaluating novel algorithms to enhance the safety of ML systems, such as supervised and unsupervised learning. These constitute the largest group within AI safety research and cover a wide range of techniques, including: robust classification algorithms resilient to noise~\cite{wangRobustAutomatedMachine2021,jingRobustExtremeLearning2020,ganDualLearningBasedSafe2018}, adversarial attacks~\cite{ilyasAdversarialExamplesAre2019a,engstromAdversarialRobustnessPrior2019a,shahamUnderstandingAdversarialTraining2018}, machine unlearning techniques for deep neural networks~\cite{izzoApproximateDataDeletion2021,brophyMachineUnlearningRandom2021,chundawatZeroShotMachineUnlearning2023}, and methods for improving domain generalisaion~\cite{chenATOMRobustifyingOutofdistribution2021,lakkarajuIdentifyingUnknownUnknowns2016,zhuoDeepUnsupervisedConvolutional2017}. 


\textit{Agent simulations}, complementary to applied algorithms, focus on designing and evaluating safer agent training algorithms, predominantly drawing on existing reinforcement learning literature.
While these simulations could be subsumed under the category of applied algorithms, we distinguish between the two to highlight the significant attention that safer agent learning receives. Research in this area encompasses various approaches, including: constrained Markov Decision Processes~\cite{wenConstrainedCrossEntropyMethod2021,bossensExplicitExploreExploit2023,massianiSafeValueFunctions2023}, enforcement of hard constraints~\cite{shiNearOptimalAlgorithmSafe2023,huntVerifiablySafeExploration2021}, model-based reinforcement learning for safe exploration~\cite{maConservativeAdaptivePenalty2022,curiCombiningPessimismOptimism2021,zwaneSafeTrajectorySampling2023}, reward learning and inverse reinforcement learning~\cite{ngAlgorithmsInverseReinforcement2000,abbeelApprenticeshipLearningInverse2004,fischerSamplingbasedInverseReinforcement2021}, multi-agent reinforcement learning with a focus on safety and cooperation~\cite{zhouRobustMeanFieldActorCritic2023,bazzanAligningIndividualCollective2019,christoffersenGetItWriting2023}, and reinforcement learning with human oversight or feedback mechanisms for enhanced safety~\cite{christianoDeepReinforcementLearning2017,kaushikLearningDifferenceThat2020,irvingAISafetyDebate2018}.


Notably, earlier research in agent simulations was primarily motivated by technical and computational challenges of reinforcement learning. However, recent studies have expanded their focus to explicitly address value alignment, aiming to align the reward functions of machines with human goals. Interestingly, despite the potential for real-world impact, research on \textit{real-world testing} of embodied AI systems remains relatively limited. These studies address crucial safety concerns that may be overlooked in the design of unembodied AI systems, ranging from contact-safe continuous control to collaborative robotics~\cite{liBridgingModelbasedSafety2022,zhuContactSafeReinforcementLearning2022,terraSafetyVsEfficiency2020}.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/annotations/methods.pdf}
    \caption{Distribution of categories of research undertaken to categorise and address the sources of risks presented in~\cref{fig:risks}.}
    \label{fig:methods}
\end{figure}


%\textcolor{red}{Evaluation frameworks might be a better term than analysis framework/if we choose evaluation framework, the figure needs to be updated to include this term (rather than "analysis".}
\textit{Analysis frameworks} is the third most prevalent category.  These works are predominantly concerned with offering frameworks for exploring and evaluating the vulnerabilities of existing AI systems~\cite{carliniEvaluatingRobustnessNeural2017,adebayoSanityChecksSaliency2018,nguyenDeepNeuralNetworks2015,kaufmannTestingRobustnessUnforeseen2019}, proposing benchmark tasks and environments for assessing AI safety~\cite{rayBenchmarkingSafeExploration2023,hendrycksWhatWouldJiminy2021,gardnerEvaluatingModelsLocal2020}, and safety verification processes~\cite{spearsAssuringBehaviorAdaptive2006,wozniakSafetyCasePattern2020,picardiAssuranceArgumentPatterns2020}. Notably, we include \textit{datasets} in this category~\cite{gholampourAdversarialRobustnessPhishing2023,hendrycksAligningAIShared2021,hendrycksBenchmarkingNeuralNetwork2019} as they provide standardized frameworks for evaluating AI systems, even though they are not frameworks in the strictest sense.



\textit{Mechanistic interpretability} methods, alongside evaluation frameworks, are a related category which investigates the inner workings of deep learning models to uncover the causal mechanisms behind their decision-making processes~\cite{nandaProgressMeasuresGrokking2023,olssonIncontextLearningInduction2022,elhageMathematicalFrameworkTransformer2021}. This often aims to develop automated interpretability tools for auditing and ensuring safety. Although the term ``mechanistic interpretability'' has gained recent prominence, various forms of interpretability methods have been present since the rise of deep learning, often under the umbrella of explainable AI (XAI) techniques~\cite{goodfellowExplainingHarnessingAdversarial2015,karpathyVisualizingUnderstandingRecurrent2016,kimInterpretabilityFeatureAttribution2018,morcosImportanceSingleDirections2018,gyevnarCausalExplanationsSequential2024}.


Approximately 10\% of the selected papers presented \textit{theoretical algorithms} with analytical proofs, often foregoing empirical evaluation. These studies typically approached AI safety from the perspective of verification, using variance or error bounds~\cite{weiModelSelectionApproach2022,okawaAutomaticExplorationProcess2020,ganRiskDegreebasedSafe2016}, constraint satisfiability~\cite{huntVerifiablySafeExploration2021}, or by demonstrating desirable properties such as metric goodness or Lyapunov functions within the problem setup~\cite{zhangBarrierLyapunovFunctionBased2022,nicolaeAlgorithmicRobustnessSemisupervised2015}. A notable portion of the papers also focused on proposing various \textit{design frameworks}. These included methodologies for eliciting safety and system requirements~\cite{khanNoHarmNovel2023,schumegProposedVModelVerification2023,deyMultilayeredCollaborativeFramework2023}, hierarchical integration of ML systems~\cite{kammConceptDynamicRobust2023,costaRobustLearningMethodology2023,aksjonovSafetyCriticalDecisionMakingControl2023}, and actionable ethical design processes~\cite{antikainenDeploymentModelExtend2021,vakkuriECCOLAMethodImplementing2021,zhangFairRoverExplorativeModel2021}.



A substantial portion of AI safety research consists of literature reviews, examining both well-established areas~\cite{costonValidityPerspectiveEvaluating2023,gittensAdversarialPerspectiveAccuracy2022,amodei2016concrete} and emerging or hypothetical issues~\cite{taylorAlignmentAdvancedMachine2016,hendrycksUnsolvedProblemsML2022,sotalaResponsesCatastrophicAGI2014}.


\textit{Theoretical frameworks} provide high-level analyses of AI safety issues, often by characterizing desirable properties of human-AI interactions~\cite{sannemanTransparentValueAlignment2023,johnsonMetacognitionArtificialIntelligence2022,hatherallResponsibleAgencyAnswerability2022}, advocating for alternative approaches~\cite{stahlEmbeddingResponsibilityIntelligent2023,samarasingheCounterfactualLearningEnhancing2023}, or formalizing existing but vaguely defined concepts~\cite{diemertSafetyIntegrityLevels2023,freieslebenGeneralizationTheoryRobustness2023,wangDataBanzhafRobust2023}. Within this category, a significant body of work employs mathematical reasoning to explore the safety of artificial general intelligence in the context of rational agents ~\cite{everittSelfModificationPolicyUtility2016,soaresCorrigibility2015,hibbardAvoidingUnintendedAI2012,ringDelusionSurvivalIntelligent2011}.


The final category includes \textit{purely philosophical} research, exploring diverse questions and perspectives often related to value alignment~\cite{badeaMoralityMachinesInterpretation2022,umbrelloBeneficialArtificialIntelligence2019,sezenerInferringHumanValues2015} and the theorized risks of artificial general intelligence~\cite{pistonoUnethicalResearchHow2016,yampolskiySafetyEngineeringArtificial2012,weldFirstLawRobotics2009}, responsible AI deployment~\cite{dobbeHardChoicesArtificial2021,matthiasResponsibilityGapAscribing2004,tayStudyRealtimeArtificial1998}, and the legal personhood of AIs~\cite{farinaArtificialIntelligenceSystems2022}.





% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/bibliometric/wc-tfidf.pdf}
%     \caption{World cloud of terms occurring in abstracts weighted by their TF-IDF score.}
%     \label{fig:wordcloud-tfidf}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/bibliometric/wc-count.pdf}
%     \caption{World cloud of terms occurring in abstracts weighted by their occurrence count.}
%     \label{fig:wordcloud-count}
% \end{figure}

% \begin{figure*}
%     \centering
%     \includegraphics[height=0.4\textheight]{figures/bibliometric/keywords-bibliographic.pdf}
%     \caption{Graph of \textit{keyword} co-occurrence in bibliographic data with a minimum term frequency of 15 and excluding the top 8 keywords corresponding to names of research fields (e.g., machine learning and reinforcement learning).}
%     \label{fig:keywords-bibliography}
% \end{figure*}




The distribution of methodologies in AI safety research positions AI safety as an organic evolution of traditional technological safety research. The abundance of research on developing and empirically evaluating algorithms for robust classification, adversarial defense, and machine unlearning directly reflects the core of engineering practice: creating practical solutions to real-world problems. This emphasis on empirical validation and the focus on improving existing machine learning techniques resonate with the iterative and improvement-oriented nature of traditional safety engineering. Research on safer agent training algorithms, drawing heavily from reinforcement learning literature, aligns with the established practice of using simulations to test and refine safety-critical systems in controlled environments \citep{lee2017flight}. This approach allows researchers to explore potential risks and develop mitigation strategies before deploying AI systems in real-world scenarios, similar to flight simulators used in aviation safety. Although limited in comparison to other categories, the research on real-world testing of embodied AI systems shows the importance of validating theoretical models in practical settings. This emphasis on real-world application is consistent with traditional safety engineering practices that prioritize testing and validation in operational environments to ensure the safety and reliability of complex systems. The prevalence of research on analysis frameworks, datasets, and mechanistic interpretability highlights the growing emphasis on transparency, explainability, and accountability in AI safety. This focus on understanding the inner workings of AI systems and developing tools for evaluating their behavior mirrors the rigorous analysis and testing protocols used in traditional safety engineering to identify and mitigate safety risks.



\section{Discussion and future research}

%\textcolor{red}{According to narrative (II), AI safety is primarily concerned with mitigating the existential risks posed by advanced AI, such as artificial general intelligence or artificial super intelligence. While these concerns are theoretically important and warrant consideration,}


In this paper, we conducted a systematic review of primarily peer-reviewed literature to unpack the diverse technical and practical challenges encompassed by AI safety. Our empirical analysis shows a broad landscape of motivations and outcomes driving AI safety research. The significance of these motivations and research outcomes stems from a desire to ensure that the AI systems we are building are reliable, trustworthy, and beneficial for society. By examining a diverse body of peer-reviewed literature, we have found that AI safety research addresses a wide range of risks across the entire lifecycle of AI systems. 

These risks echo a variety of concerns, including design misspecification, lack of robustness, inadequate monitoring, and potential biases embedded within AI systems. Design misspecification can lead to AI systems that behave in unintended and potentially harmful ways, while a lack of robustness can make them vulnerable to errors and malfunctions. Inadequate monitoring can prevent the detection and correction of issues. 


The breadth and depth of AI safety research, as evidenced by our analysis, challenge the narrative that associates AI safety primarily with mitigating existential risks from advanced AI. We hence propose framing of AI safety research within the broader context of technological safety. Just as other fields of engineering and technology have developed robust safety practices to mitigate risks and ensure the safe operation of complex systems, so too can AI safety research be viewed as an integral part of the progression within the broader domain of technological safety.

This framing has important implications for the field of AI safety. First, by recognizing the relevance of AI safety to a diversity of risks, we can expand the circle of stakeholders involved in the discourse about AI safety. This expanded engagement can lead to increased funding and support for AI safety research, as well as a shift in the discourse towards more practical and inclusive solutions. Second, we think that a more episemically-inclusive research environment helps discussions to demistify existential risks from fresh perspectives.

While our argument advocates for a broader perspective on AI safety, it is crucial to emphasize that we do not intend to diminish the importance of critically engaging with existential risk concerns. These risks remain a significant area of research and policy debate, and our findings do not negate the need for continued investigation and dialogue within this domain.  However, by acknowledging the full breadth of AI safety research, which extends beyond existential threats, we can foster a more inclusive and productive discourse that encompasses the diverse range of risks associated with AI systems, ultimately leading to a more comprehensive and effective approach to ensuring AI safety.




Several research questions remain open. We highlight two of them below.

First, the concept of ``sociotechnical AI safety'' and its implications for the field of AI safety require further investigation \citep{lazar2023ai}. Although recent work \citep{weidinger2023sociotechnical} has begun to clarify this notion in the context of AI evaluation practices, more research is needed to examine the implications of sociotechnical approaches for AI safety research and practice as a whole. This includes exploring how to effectively integrate social, ethical, and political considerations into technical safety research and development.


Second, while our review focused on peer-reviewed literature, future research should expand its scope to include non-peer-reviewed sources, such as more pre-print papers, technical reports, and substantive research content on online forums and workshops. This broader analysis would provide a more comprehensive understanding of the diverse perspectives and approaches within AI safety and identify areas for future research and collaboration. Our empirical analysis is just a first step in understanding a complex practice.


% \textcolor{red}{This is a repeat of the previous para} Third, while our review primarily focused on peer-reviewed literature, future research should expand its scope to include non-peer-reviewed sources, such as arXiv papers, technical reports, and substantive research content on online forums and workshops. This broader analysis would provide a more comprehensive understanding of the diverse perspectives and approaches within the AI safety community and identify potential areas for future research and collaboration. Our empirical analysis is just a first step in understanding a complex discourse and practice.

%\textcolor{red}{Finally, given the current state of AI safety research, a crucial question arises: If the existential threat posed by AI is not imminent, should not we cultivate a moral and economic culture that prioritizes the development of AI systems that are safe? This question raises questions about the relationship between interpretation (II) and (I). Is interpretation (II), which ties AI safety to concerns about existential risks from AI, a subset of the broader interpretation (I), or are they fundamentally distinct? We hope this paper serves as a foundation for further investigation and democratic discourse regarding the perception and scope of AI safety.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\backmatter



\bmhead{Acknowledgments}

This research was made possible through the generous support of several funding bodies. Primary support came from the AI2050 program at Schmidt Sciences (Grant 24-66924), which supported Atoosa Kasirzadeh's contributions. Additional funding was provided by the UKRI Arts and Humanities Research Council (grant AH/X007146/1). Bálint Gyevnár's work was supported by the UKRI Centre for Doctoral Training in Natural Language Processing through a joint grant (EP/S022481/1) from UK Research and Innovation and the University of Edinburgh's School of Informatics and School of Philosophy, Psychology \& Language Sciences.

%Acknowledgements are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

%Please refer to Journal-level guidance for any specific requirements.

%\bmheadsection*{Competing Interest}

%The authors declare no competing interest.

% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval and consent to participate
% \item Consent for publication
% \item Data availability 
% \item Materials availability
% \item Code availability 
% \item Author contribution
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

% \begin{appendices}

% \section{Section title of first appendix}\label{secA1}

% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{refs/main,refs/seeds,refs/lit,refs/all} % common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
