\section{Introduction}

Large language models~\citep{grattafiori2024llama3herdmodels, openai2024gpt4technicalreport} have demonstrated remarkable capabilities, especially in following complex instructions. While modeling such ability is crucial, the technical details and the instruction datasets used in state-of-the-art LLMs remain mysterious. 
For example, LLaMA3 \citep{grattafiori2024llama3herdmodels} reportedly leverages instruction-following data at the tens of millions scale but has not been open-sourced. This lack of transparency has resulted in a significant gap between the research community and the leading companies.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/intro.pdf}
    \caption{Instruction-following performance comparison of \method against baselines on IFEval and MultiIF.}
    \label{fig:intro_performance}
    \vspace{-3mm}
\end{figure}

Recent efforts in aligning LLMs to follow instructions have focused on creating high-quality instruction-following data. 
On the one hand, \citet{wei2021finetuned, no_robots,jiang2023followbench} involve human annotators in developing instructions and manually crafting corresponding responses. While effective, these methods are label-intensive, heavily reliant on human expertise, and face challenges in scalability and cost efficiency.
On the other hand, \citet{xu2023wizardlm,wang-etal-2023-self-instruct,sun2024conifer,dong2024self} attempt to leverage LLMs to automatically construct high-quality instruction data.
Specifically, \citet{xu2023wizardlm,sun2024conifer} guide LLMs to generate constraints and evolve initial instructions into more complex forms.
% (e.g., `How to prove 1+1=2 in the Goldbach Conjecture?'). 
However, these LLMs-driven methods heavily rely on models' instruction-evolving capability and overemphasize instruction complexity, ultimately hindering the diversity of evolved instructions and the correctness of generated responses.
To improve this, \citet{wang-etal-2023-self-instruct,dong2024self} introduce handcrafted constraints inspired by human priors to guide LLMs.
For instance, \citet{dong2024self} introduces constraints that can be verified by code execution to ensure response correctness.
However, these handcrafted constraints introduce rigidity, leading to homogeneous instructions and making it narrow in encompassing more complex or diverse instructions (e.g., \textit{write in Shakespeare's tone}).
As a result, scaling such instruction with correct responses remains a significant challenge, limiting the applicability of modeling the distribution of instructions from real-world users.



In this paper, we propose \method, a simple and scalable method which synthesizes high-quality instruction-following data.
The core idea of \method is to decompose real-world user instructions for both constraints and evaluation questions, then train a composer model to synthesize diverse and complex instructions with verification questions.
To achieve this, we first utilize LLM to decompose human instructions into simplified instructions and their associated constraints. 
For each constraint, the LLM further generates the corresponding evaluation question to verify whether the upcoming response meets the requirement.
With these components, we train \textbf{\textit{\composer}}, which takes the simplified instruction as input and outputs the original instruction along with its evaluation question.
In this way, the composer learns to evolve instructions with verifiable constraints, and benefits from the generalization ability of LLMs rather than handcrafted rules.
With the composer, \method could make any instruction more complicated to synthesize a large-scale and diverse dataset. The evaluation questions further help with quality control in rejection sampling and preference learning~\citep{rafailov2024direct,chen2024noise}.


Through comprehensive experiments, we demonstrate that \method significantly enhances the instruction-following capabilities of LLMs with high scalability and cost efficiency. Our evaluation, conducted on the LLaMA-3.1-8B model across five instruction-following datasets, confirms \method's strong alignment with general instructions. Notably, as shown in Figure \ref{fig:intro_performance}, by scaling up the training data, we achieve a milestone, optimizing the LLaMA-3.1-8B-Base model to match the instruction-following ability of its instruct version. Additionally, we assess the generability of \method by evaluating it on mathematical, reasoning, coding, and general conversation domains. Furthermore, we explore the potential of self-alignment in \method by further optimizing the LLaMA-3.1-8B-Instruct model, and achieve substantial improvement. 
The main contributions of our paper are as follows:
\vspace{-3mm}
\begin{itemize}[nolistsep,leftmargin=*]
    \setlength\itemsep{0mm}
    \item We introduce \method, a simple and scalable approach that leverages real-world user instructions to train a composer model, \composer, enabling the synthesis of complex and diverse instructions with correct responses.
    % \ganqu{do we have exp on cost?}
    \item Our experiments demonstrate the strong performance of \method in handling complex instructions, surpassing all baselines under the same data budget while retaining general capabilities in domains such as mathematics, coding, and conversational tasks.
    \item We reach a new milestone by optimizing the LLaMA-3.1-8B-Base model to match the instruction-following abilities of its Instruct counterpart with only 200k data, and showcase the self-alignment potential by further optimizing the LLaMA-3.1-8B-Instruct model on it own.

\end{itemize}