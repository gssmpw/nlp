\section{Datasets and Baselines}
\label{sec:dataset}
\textbf{ShareGPT} \footnote{We use the version from \href{https://huggingface.co/datasets/shibing624/sharegpt\_gpt4}{https://huggingface.co/datasets/shibing624/sharegpt\_gpt4.}} is an open-source and multi-turn conversation dataset that contains over 52K user-shared chatting histories with GPT-4. We decompose the human instructions from ShareGPT into around 200K data pairs to train our UltraComposer. For main experiment, we randomly select 10K human instructions to generate augmented instructions.

\textbf{OpenHermes} \citep{OpenHermes} is a large-scale, diverse, and high-quality compilation consisting of around 1M synthetically generated instruction and chat samples. We randomly select a subset of 150K instructions from OpenHermes-2.5 for the scale-up experiment.

\textbf{No Robots} \citep{no_robots} is a high-quality dataset of 10k instructions and demonstrations created by skilled human annotators. We use all instructions from No Robots for the scale-up experiment.


\section{Evaluation Benchmarks}
\label{sec:benchmark}
\textbf{IFEval} \citep{zhou2023instruction} is an easy-to-produce benchmark designed to evaluate the instruction-following capability of LLMs. IFEval constructs around 500 prompts that contain 25 types of verifiable instructions. We use both loose and strict accuracy metrics at prompt and instruction levels in our evaluation. 

\textbf{Multi-IF} \citep{he2024multi} is a benchmark designed to assess LLMs' proficiency in following multi-turn and multilingual instructions. Based on IFEval, Multi-IF contains 4,501 multilingual conversations, where each has three turns. We report the average accuracy across all languages for each of the three rounds in the experiment.

\textbf{InfoBench} \citep{qin2024infobench} is a benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories, adopting a new
metric Decomposed Requirements Following Ratio (DRFR) for evaluating LLM's ability to follow instructions. We use GPT-4-1106-preview as the evaluator in our assessments.


\textbf{FollowBench} \citep{jiang2023followbench} is a multi-level fine-grained constraints following benchmark for LLMs.  FollowBench incorporates five distinct fine-grained constraint types (Content, Situation, Style, Format, and Example) and underscores multi-level mechanism when building instruction prompts. In our experiment, we prompt the GPT-4-1106-preview to assess whether LLM's outputs have satisfied each individual constraint.

\textbf{LiveBench} \citep{white2024livebench} is a LLM benchmark that contains a wide variety of challenging tasks (math, coding, reasoning, language, instruction-following, and data analysis) and automatically scores answers according to objective ground-truth values. When assessing the instruction-following skills of LLMs, we employ the instruction-following subset, while the entire dataset is utilized to gauge their overall capabilities.

\textbf{GSM8K} \citep{cobbe2021training} comprises 8.5K high-quality, multilingual grade school math word problems, specifically designed to assess the multi-step mathematical reasoning proficiency of language models. We report the overall accuracy in the experiment.

\textbf{HumanEval} \citep{chen2021evaluating} consists of 164 programming problems with function signatures, docstrings, bodies, and unit tests, averaging 7.7 tests per problem. It is utilized to evaluate the coding abilities of LLMs. HumanEval assesses the capability of LLMs in program synthesis from docstrings, testing language comprehension, reasoning, algorithms, and elementary mathematics skills. We report Pass@1 on HumanEval in the experiment.

\textbf{BBH} \citep{suzgun2022challenging} is a clean, challenging and tractable subset benchmark filtered from Big Bench, which includes 23 types of difficult tasks and 6,511 evaluation examples in total. BBH primarily assesses the models' reasoning capacities and problem-solving skills comprehensively. In the experiment we report the accuracy metrics on BBH.

\textbf{Arena Hard} \citep{li2024crowdsourced} is an automatic  LLM benchmark consisting of 500 challenging challenging user queries, which is curated to evaluate the comprehensive performance of LLMs in user dialogue scenarios. In the experiment, we adopt GPT-4-1106-preview as the judge model and report the win rate of our models against the baseline model (GPT-4-0314).


\section{Experimental Details}
\subsection{Implementation Details}
\label{sec:experiment}

In the SFT stage, we perform full fine-tuning with a learning rate of 1e-5. The maximum token length is set to 8192 and variable-length packing is enabled. We use AdamW \citep{loshchilov2017decoupled} as the optimizer with a warmup ratio of 0.03 and employ a LinearLR scheduler at the beginning, transitioning to CosineAnnealingLR towards the end.

In the DPO stage, the configuration is similar, with the only difference being a lower learning rate of 5e-7. Additionally, the beta parameter of DPO loss is set to 0.1.

\subsection{Prompts of \method}
\label{sec:prompt}
To train our \textbf{\composer}, we prompt LLM to perform \textbf{Instruction Decomposition} and \textbf{Eval Question Generation}.

We use the following prompt template to decompose human instructions:
\begin{tcolorbox}[title = {Prompt Template of Instruction Decomposition}, breakable]
You are an expert in extracting instruction constraints from a given query. \\
\textbf{Definition of Constraint:} The smallest unit of restriction or requirement that the instruction imposes on the task.

\textbf{Query:} \{query\}

\begin{itemize}
    \item If the query is not a question, or is simple or straightforward without any constraints, please only respond with the following JSON, indicating that no constraints are present.
    \begin{verbatim}
    {
        "Complex": False
    }
    \end{verbatim}
    \item If constraints are present, follow these steps:
    \begin{enumerate}
        \item Identify the Basic Query: Clearly understand the primary goal of the query, stripping away any constraints. The Basic Query should be the essential task without any added conditions or restrictions.
        \item Extract and Categorize Constraints: Identify and classify constraints based on the following types:
        \begin{itemize}
            \item Content Constraints:
            \begin{itemize}
                \item Specific Terms or Symbols: Mandatory use of certain terms or symbols with their exact placement (e.g., must include the word 'beautiful').
                \item Required Elements or Concepts: Mandates for including specific elements or concepts in responses, reflecting a scenario or object (e.g., highlights the Great Wall).
                \item Thematic Directives: Instructions related to thematic content, perspective, or tone, emphasizing response significance (e.g., write a poem about London).
            \end{itemize}
            \item Numerical Constraints:
            \begin{itemize}
                \item Constraints on quantities related to the content, such as the number of points, sentences, paragraphs, response length, or examples (e.g., within a single paragraph with three sentences).
            \end{itemize}
            \item Stylistic Constraints:
            \begin{itemize}
                \item Desired tone and style for the response (e.g., formal, informal, conversational).
                \item Specific language or terminology to be used or avoided (e.g., encyclopedic style).
            \end{itemize}
            \item Format Constraints:
            \begin{itemize}
                \item Required structure or format for the response (e.g., list, JSON, bullet points, Java language).
                \item Presentation styles or formatting requirements (e.g., electronic medical record format).
            \end{itemize}
            \item Linguistic Constraints:
            \begin{itemize}
                \item Language use in specific contexts, such as discourse, dialects, sociolects, and language policies (e.g., in English).
                \item Sentence structure, including phrases, constituents, and the use of imperatives (e.g., with nouns and verbs).
                \item Internal structure of words, including roots, affixes, and morphological changes (e.g., lowercase, single-rhyme).
            \end{itemize}
        \end{itemize}
        \item Response Format:
        \begin{itemize}
            \item Do not consider details that are part of the content itself, such as those used in descriptions, scenarios, or examples, unless they directly impose a restriction of the response.
            \item The Basic Query should represent the queryâ€™s core goal, free from any constraints. 
            \item Ensure that extracted constraints do not overlap with the Basic Query.
            \item Present each constraint as a dictionary within a list, where each dictionary contains:
            \begin{itemize}
                \item \texttt{'constraint'}: The specific restriction or requirement.
                \item \texttt{'simplified query'}: The query after removing this constraint, polished for coherence and correctness.
            \end{itemize}
            \item Exclude any constraint types not present in the query.
        \end{itemize}
    \end{enumerate}
    \end{itemize}

\begin{verbatim}
{
    "Complex": True,
    "Basic Query": ...,
    "Content Constraints": [
        {
            "constraint": "...",
            "simplified query": "..."
        },
        {
            "constraint": "...",
            "simplified query": "..."
        },
    ],
    ...
}
\end{verbatim}
Please only provide the response in JSON format.
\end{tcolorbox}

We use the following prompt template to generate evaluation questions for instructions:
\begin{tcolorbox}[title = {Prompt Template of Eval Question Generation}, breakable]
You are an expert in crafting questions to evaluate whether a response to a query adheres to specific constraints.

For the given constraint, please design a question that human evaluators can use to assess if the response meets the specified constraint. The question should focus solely on the given constraint and not other constraints present in the original query.

Specifically, if the given constraint is meaningless or is a part of the content itself, such as those used in descriptions, scenarios, or examples, you can respond with an empty string.

\textbf{Query:} \{query\} \\
\textbf{Constraint:} \{constraint\}

Please design a question for the specified constraint for the given query, and respond in the JSON format without explanation.

\begin{verbatim}
{
    "question": "string",
}
\end{verbatim}
\end{tcolorbox}

\newpage

For \textbf{Generate-then-Evaluate} process, we prompt LLM to perform \textbf{Response Generation} and \textbf{Response Evaluation}.

First we use the following prompt template to generate responses for the augmented instructions:
\begin{tcolorbox}[title = {Prompt Template of Response Generation},breakable]
You are an expert tasked with answering the given query. Please provide a clear and concise response directly, without introductory phrases such as 'What a great question,' 'Here is the answer,' or similar expressions. Focus solely on addressing the query.

Now please answer the given query while strictly following its inside constraints.

\textbf{[Query]} \{query\}
\end{tcolorbox}

Then we use the following prompt template to evaluate the quality of those generated responses:
\begin{tcolorbox}[title = {Prompt Template of Response Evaluation}, breakable]
You are an expert that is good at judging whether the response to a given query meets the specified evaluator questions. \\
Your task is to carefully examine the response to determine if it adheres to each requirement outlined in the evaluator questions.

\textbf{[Query]} \{query\} \\
\textbf{[Response]} \{response\} \\
\textbf{[Evaluator Question]} \{question\}

For each question, please provide a justification for your evaluation, explaining how the response does or does not satisfy the criteria and a score (\texttt{'YES'} or \texttt{'NO'}) indicating whether the answer satisfies each constraint.

You should only respond in the following JSON format:
\begin{verbatim}
{
    "Question 1": {
        "explanation": "",
        "score": "YES" or "NO"
    },
    "Question 2": {
        "explanation": "",
        "score": "YES" or "NO"
    },
}
\end{verbatim}
\end{tcolorbox}

% \subsection{Additional Experimental Results}

\subsection{Case Study}

\begin{table}
\caption{Examples of \method's data pair.}
\label{tab:t6}
\begin{tabular}{|p{5cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Original Query} & \textbf{Augmented Instruction} & \textbf{Eval Question} \\
\hline 
 Explain merkle tree in blockchain. & Explain merkle tree in blockchain \textbf{to a 10 years old}. & Is the explanation of a Merkle tree in the context of blockchain presented in a way that a 10-year-old can understand? \\
\hline
We are driving in a car. It is cold outside, windshield is frozen, and we are on a secluded road to a small village. This is scene from a horror movie, and it's just starting. Describe a scene in great detail. & We are driving in a car. It is cold outside, windshield is frozen, and we are on a secluded road to a small village. This is scene from a horror movie, and it's just starting. Describe a scene in great detail, and \textbf{write it in the style of a gothic horror author}. & Does the response evoke a dark, eerie, and ominous atmosphere, characteristic of gothic horror? \\
\hline
 Design a html form with form tags. & Design a html form with form tags \textbf{for the following 3 user inputs: first\_name, last\_name, date\_of\_birth}. & Does the HTML form include form tags for exactly three user inputs: first\_name, last\_name, and date\_of\_birth? \\
\hline
 I'm planning to visit Okinawa Japan from April 7th to April 10th. Do you have any recommendation on what to do while I'm there?& I'm planning to visit Okinawa Japan from April 7th to April 10th. Do you have any recommendation on what to do while I'm there? \textbf{I'd like to focus on nature, food, and local culture.}&Does the response recommend activities in Okinawa that focus on nature, food, and local culture? \\
\hline
 What is the meaning of life?&What is the meaning of life? \textbf{Explain it in 5 paragraphs}. &Is the response to the question explained in exactly 5 paragraphs? \\
\hline
Write a homepage for translation business.& Write me a homepage for translation business \textbf{in wordpress}.&Is the homepage for the translation business designed using WordPress? \\
\hline
\end{tabular}
\end{table}

Table \ref{tab:t6} shows some examples of augmented instructions and evaluation questions generated by \method. The original queries come from ShareGPT dataset.