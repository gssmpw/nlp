\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/ultraif-framework.pdf}
    \caption{The framework of \method. Specifically, \method begins by training the \textbf{\textit{\composer}}, which decomposes real-world user instructions and evaluation questions. For (a), the given instruction can be decomposed into several pairs, such as the numeric constraint `ten books' and content constraint `Chinese books'. Next, \method adopts a \textbf{\textit{Generate-then-Evaluate}} process, where the composer iteratively adds multiple constraints to each collected instruction and then applies the evaluation questions for rejection sampling.
    % \ganqu{we can describe more here}
    }
    \label{fig:ultraif}
\end{figure*}

\section{\textsc{UltraIF}}
\subsection{Overview}
% We propose \method, a simple yet scalable method designed to improve the instruction-following abilities of LLMs. In this section, we outline the construction of the \textit{\textbf{\composer}} in \S\ref{sec:composer}, detail the \textit{\textbf{Generate-then-Evaluate} }framework in \S\ref{sec:framework}, which synthesizes high-quality instruction-following data.

\method synthesizes high-quality instruction-following datasets through a two-stage process. As shown in Figure \ref{fig:ultraif}, \method first constructs the \textbf{\textit{\composer}} by decomposing user instructions into simplified ones and constraints, along with corresponding evaluation questions, as detailed in \S\ref{sec:composer}. This specialized composer facilitates the synthesis of instructions with more complex and diverse constraints, while the evaluation questions ensure the correctness and reliability of the generated responses.
Then, we introduce the \textbf{\textit{Generate-then-Evaluate}} process, described in \S\ref{sec:framework}. This framework first uses \composer to incorporate constraints into instructions and then evaluates the generated responses using corresponding evaluation questions covering various quality levels.
% This process enables to capture the distribution of constraints in real-world instructions and provides a foundation for response verification.
% Then, we introduce the \textit{\textbf{Generate-then-Evaluate}} framework in \S\ref{sec:framework}, which iteratively complicates instructions and generates responses at varying levels of quality. 
Additionally, we introduce our two training strategies in \S\ref{sec:strategy}.



\subsection{UltraComposer}
\label{sec:composer}
% Since instructions in real-world scenarios are highly diverse, it is more effective to learn from real-world data rather than relying on human priors \citep{wang-etal-2023-self-instruct,dong2024self}, or exclusively leveraging LLMs for query augmentation \citep{xu2023wizardlm,katz2024evolutionary}. The primary goal of our \composer is to train a smaller model capable of capturing the distribution of real-world instructions, allowing it to seamlessly and dynamically integrate appropriate and diverse constraints into instructions.

Previous studies \citep{xu2023wizardlm,sun2024conifer} that rely solely on LLMs are limited by the models' instruction-evolving ability, which restricts the diversity of synthetic instructions and compromises response accuracy. While \citet{wang-etal-2023-self-instruct,dong2024self} address response correctness through handcrafted constraints, this approach further limits instruction diversity.
In contrast, \method focuses on generating diverse, complex instructions with correct responses. To achieve this, we propose the \textit{\composer}, a specialized model to synthesize diverse instructions and generate corresponding evaluation questions. Building this composer model involves three key steps: instruction decomposition, evaluation question generation, and \composer training.
% to better capture how real-world instructions are typically structured. 
% By decomposing instructions into simplified queries and constraints, \method trains the \composer. 
% Additionally, unlike \citet{xu2023wizardlm,wang-etal-2023-self-instruct}, which overlook response quality, \method incorporates an additional step to generate evaluation questions, enabling effective response verification. 
We provide the prompts about instruction decomposition and evaluation question generation in Appendix \ref{sec:prompt}.


% As illustrated in Figure \ref{fig:ultraif}, this process begins by leveraging real-world instructions, such as those sourced from ShareGPT \citep{vicuna2023}, and utilizing LLMs to decompose the original instructions into simplified queries and their associated constraints. For each constraint, the LLM then generates an evaluation question to verify if the constraint is satisfied. We provide the prompts about instruction decomposition and evaluation question generation in Appendix \ref{sec:prompt}.

\paragraph{Instruction Decomposition}
The decomposition process leverages LLMs to decompose complex instructions into simplified components, such as those sourced from ShareGPT \citep{vicuna2023}. These components consist of a set of simplified instructions paired with constraints that represent the underlying requirements of the original instruction. For example, as shown in Figure \ref{fig:ultraif} (a), the instruction ($X$) \textit{``In Shakespeare's tone, recommend me ten Chinese books.''} can be decomposed into the simplified instruction ($x_1$) \textit{``Recommend me ten Chinese books.''} and the paired constraint ($c_1$) \textit{``In Shakespeare's tone.''}, etc. This step is essential for disentangling intricate objectives into more structured elements, extending beyond basic format or content constraints \citep{dong2024self,wang-etal-2023-self-instruct}, and forming a foundation to model the distribution of real-world user instructions effectively.
\begin{equation}
    \small
    X \to \{(x_1, c_1, q_1),\ ...\ , (x_n, c_i,q_i)\},\ \ i\in \mathbb{N} 
\end{equation}

% \ganqu{is this equation better placed under the next paragraph, after introducing Q?}
\paragraph{Evaluation Question Generation}
While \citet{xu2023wizardlm,wang-etal-2023-self-instruct} focus on improving the complexity of instructions, omitting the quality of generated responses often leads to low-quality samples. Inspired by \citet{qin2024infobench}, we utilize LLM to generate an evaluation question for each identified constraint. For the above example, the evaluation question ($q_1$) would be \textit{``Is the response written in Shakespeare's tone?''}. These questions are designed to be precise, enabling reliable assessment of generated responses for adherence to the constraints. This mechanism not only addresses the limitation of verifying only constraints that can be checked programmatically \citep{dong2024self} but also enhances the reliability of the generated data and ensures alignment with the intent of the original instruction.




\paragraph{\composer Training}
With the decomposed instructions and evaluation questions, we train our \composer to take a simplified query  ($x_i$) as input and generate the original instruction ($X$) with its evaluation question ($q_i$), denoted as Eq. \ref{eq:2}. The training process is shown in Figure \ref{fig:ultraif} (b).
In this way, the \composer can directly complicate instructions, streamlining the generation process into a single step. Moreover, it enriches the diversity of constraints, extending beyond LLM's intrinsic knowledge to encompass the distribution of real-world scenarios.

\begin{equation}
    \small
    \label{eq:2}
    UltraComposer(x_i) \to (X,\ q_i),\ \ i\in \mathbb{N}
\end{equation}





\subsection{Generate-then-Evaluate}
\label{sec:framework}
As shown in Figure \ref{fig:ultraif}, with \composer, \method efficiently generates high-quality instruction-following data through the Generate-then-Evaluate process, supporting both Supervised Fine-tuning and Preference Learning.
% \ganqu{think preference learning more precise} 
This process comprises two key components, instruction generation and response evaluation. It begins by collecting user instructions from existing datasets, after which \composer iteratively incorporates multiple constraints into each instruction, and then the corresponding evaluation questions are used to assess the responses generated. Thus, \method could make any instruction more complex, facilitating a diverse and large-scale instruction-following dataset.
% Unlike \citet{dong2024self}, \method eliminates the need to check the alignment between added constraints and instructions, thus reducing overhead.
 
\paragraph{Instrucion Generation}
The \composer adapts the augmentation process fully automated and aligns with human preferences. We start by collecting user instructions from existing datasets \citep{vicuna2023,OpenHermes,no_robots}, and then use the Composer to augment these instructions. As shown in Eq. \ref{eq:3}, this process can be conducted iteratively, enabling the generation of more complex and realistic instructions ($\bar{x}$) with multiple constraints, paired with corresponding evaluation questions ($\bar{q}$).

\begin{equation}
\label{eq:3}
\begin{aligned}
    \small
    UltraComposer(x^{(n)}) \to (\bar{x}^{(n)},\ \bar{q}^{(n)}),\ \ n \in \mathbb{N} \\
    x^{(n+1)} = \bar{x}^{(n)}, \quad \bar{q}^{(n+1)} = \bar{q}^{(n+1)} \cup \bar{q}^{(n)}
\end{aligned}
\end{equation}


\paragraph{Response Evaluation}
Next, we prompt LLMs to generate $K$ responses for each augmented instruction. The quality of the generated responses is then assessed by evaluating them against evaluation questions. This results in a dataset $\mathcal{D}_{data}$ comprising  $(\bar{x},\ \bar{q}, y_{chosen}, y_{rejected})$.
Ideally, this process requires only three to four calls to the LLM, significantly reducing the computational cost. 
Since the added constraints are aligned with human instructions, \method eliminates the need to verify whether the constraints are consistent with the original instructions \citep{dong2024self,katz2024evolutionary}. Additionally, the evaluation questions replace the need for a separate score-filtering stage. Consequently, \method achieves greater efficiency and incurs minimal costs when constraining large-scale datasets compared to previous research \citep{xu2023wizardlm, dong2024self}.


\subsection{Training Strategies}
\label{sec:strategy}

\method offers flexible training strategies for aligning model with instruction following capabilities. To thoroughly evaluate the effectiveness, we provide two approaches:

\paragraph{Supervised Finetuing (SFT).}
Given the dataset $\mathcal{D}_{data}$, we apply standard Supervised Finetuning (SFT) objective on vanilla model $\pi$ with parameters $\theta$, as shown in Eq. \ref{eq:4}: 

\begin{equation}
\label{eq:4}
    \small
    \mathcal{L}_{SFT}(\pi_\theta)=\sum_{(\bar{x}, y_{c})\in \mathcal{D}_{data}} \log\ \pi_{\theta}(y_{c}|\bar{x})
\end{equation}
% \ganqu{$L(\theta)$ here but $L(\pi_\theta)$ in Eq 5. Unify them.}
where $\bar{I}$ represents the augmented instruction, and $r_{c}$ denotes the corresponding chosen response.

\paragraph{SFT + Iterative Online DPO.} As \method is equipped with evaluation questions, it facilitates quality control by enabling the generation of pairwise responses with varying quality levels. This property makes it particularly suitable for the application of Direct Perference Optimization (DPO, \citet{rafailov2024direct}) to refine the fine-tuned model, $\pi_{ref}$. The DPO objective is formulated as Eq. \ref{eq:5}:

\begin{equation}
\label{eq:5}
\small
\begin{aligned}
    \mathcal{L}_{DPO}(\pi_\theta, \pi_{ref}) = - \mathbb{E}_{(\bar{x}, y_{c}, y_{r}) \in \mathcal{D}_{data}} \log\ \sigma(\beta\ \cdot \Delta)\\
    \Delta = (\log \frac{\pi_\theta(y_c|\bar{x})}{\pi_{ref}(y_c|\bar{x})} - \log\frac{\pi_\theta(y_r|\bar{x})}{\pi_{ref}(y_r|\bar{x})})
\end{aligned}
\end{equation}

% where $\mathcal{D}_{data}$ represents the dataset of pairwise responses, 
where $\beta$ is a scaling hyperparameter, $\sigma$ denotes the sigmoid function, and $\pi_\theta$ is initialized from $\pi_{\text{ref}}$ and further optimized during the DPO stage.


In the context of \method, the \composer enables an iterative augmentation of instructions, transitioning from simpler to more complex tasks. This allows the DPO process to be formulated as an iterative curriculum. At each iteration, the model $\pi_{\text{ref}}$ is replaced with the latest optimized model from the previous stage. Concurrently, more challenging instruction-following datasets are generated and utilized for further training. This iterative approach ensures continuous improvement in model performance and adaptability across increasingly complex scenarios.

Moreover, during the iterative process, as observed by \citep{chen2024noise}, the DPO objective primarily focuses on optimizing the margin between the chosen and rejected samples, rather than directly maximizing the probability of chosen samples and minimizing that of the rejected ones. To address this, we employ the Noise Contrastive Estimation (NCA, \citet{chen2024noise}) loss in the final iteration, and the objective is defined in Eq. \ref{eq:6}:

\begin{equation}
\label{eq:6}
\small
\begin{aligned}
    \mathcal{L}_{NCA}(\pi_\theta, \pi_{ref}) &= \\
    &\hspace{-1.5cm} - \mathbb{E}_{(\bar{x}, y_{c}, y_{r}) \in \mathcal{D}_{data}}  \bigg[ 
    \log\ \sigma (\beta\log \frac{\pi_\theta(y_c|\bar{x})}{\pi_{ref}(y_c|\bar{x})} ) \\
    &\hspace{-1.5cm}  + \frac{1}{2} \sum_{y \in \{y_c, y_r\}} 
    \log\ \sigma (-\beta\log \frac{\pi_\theta(y|\bar{x})}{\pi_{ref}(y|\bar{x})} ) \bigg]
\end{aligned}
\end{equation}
