
\section{Analysis}
\subsection{Impact of the Iterative DPO Process}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/dpo_reward.pdf}
    \caption{Reward trajectories for the chosen and rejected samples across the steps of each iteration during the Iterative DPO process.}
    \label{fig:dpo_loss}
\end{figure}

As UltraComposer allows for iteratively adding constraints to instructions, we introduce the Iterative DPO process during training and gradually increase instruction complexity. Figure \ref{fig:dpo_loss} illustrates the reward trajectories for the chosen and rejected samples during the DPO process. The reward margin progressively broadens with the training steps of each iteration. However, by iteration 3, the optimization objective deviates, focusing on maximizing the margin between the chosen and rejected samples rather than enhancing the reward for the chosen samples and minimizing the rejected ones. This shift causes rewards for both samples to fall below zero, resulting in performance degradation on Multi-IF and LiveBench benchmarks, as shown in Table \ref{tab:iterative_dpo}.  To mitigate this issue, we replace the DPO objective with NCA \citep{chen2024noise},  stabilizing the training process and yielding more consistent and optimal results.



\subsection{Scalability of \method}
By capturing the distribution of real-world instructions, \method facilitates the instruction augmentation process while reducing the risk of inconsistencies between added constraints and the original instructions. Table \ref{tab:cost} reports the pass rate during dataset synthesis, where \method significantly outperforms AutoIF. This indicates that for generating an equivalent amount of data, \method reduces costs by a factor of three to five. Furthermore, during the rejection sampling stage, while AutoIF necessitates rigorous function-based filtering for instruction synthesis and response generation, \method achieves this with a single LLM call, making it far more scalable and cost-efficient.

\begin{table}[h]
    \centering
    \small
    \caption{The pass rate of data synthesis.}
    \begin{tabular}{lcc}
        \toprule
         \textbf{Method} & \textbf{SFT Pass Rate} & \textbf{DPO Pass Rate}  \\
         \midrule
         AutoIF & 20\%  & 26\% \\
         \method & 85\% & 60\% \\
         \bottomrule
    \end{tabular}
    \label{tab:cost}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/scaling.pdf}
    \caption{Scaling the training set on SFT stage.}
    \label{fig:scale}
\end{figure}


To further assess the scalability of \method, we conduct experiments under the self-alignment setting. Figure \ref{fig:scale} illustrates the effect of different training data sizes during the SFT stage. With about 175k, \method demonstrates powerful performance compared to baselines, highlighting \method's capacity to scale effectively with larger datasets.




\subsection{Extension of Self Alignment}
In our main experiments, we distill knowledge from the Instruct version model to enhance the vanilla model, demonstrating the effectiveness of \method. However, the potential for \method to independently enhance a strong model like \citet{cheng2024spar} has not yet been explored. In this section, we conduct experiments to investigate the self-improvement capabilities of \method. Under the Self-Alignment setting, we use data generated by LLaMA-3.1-8B-Instruct to enable the model to train itself. As shown in Figure \ref{fig:self_evolve}, \method significantly boosts the performance of the strong model across different size of training data, even without a more powerful supervisor. Specifically, \method improves performance on IFEval by 2.4\%-5.9\%, on Multi-IF by 3.74\%-5.38\%, on LiveBench by 2.5\%, and on FollowBench by 2.29\%, further validating the effectiveness of our approach.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/evolve.pdf}
    \caption{The performance of exploring the potentiality of \method on self-alignment.}
    \label{fig:self_evolve}
\end{figure}

\begin{table}[h]
    \centering
    \small
    \caption{The performance comparison of incorporating multi-turn data during the SFT stage.}
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Method} & \textbf{Multi-IF} & \textbf{InfoBench} & \textbf{LiveBench} \\
    \midrule
    \method + SFT & 40.12 & 77.78 & 46.60 \\ 
    \hspace{3pt} $w.\ multi\ turn$ & 43.13 & 79.86 & 54.20 \\
    \hdashline[2pt/3pt]
    \rowcolor{blue!5} \hspace{20pt} $\Delta$ & +3.01 & +1.18 & +5.10 \\
    \bottomrule
    \end{tabular}
    \label{tab:multiturn}
\end{table}


\subsection{Analysis of Multi-Turn Data}
Building on prior work that emphasizes enhancing multi-turn instruction-following capabilities \citep{sun-etal-2024-parrot, he2024multi}, our analysis reveals that incorporating multi-turn data during the SFT stage significantly improves \method's performance across various benchmarks. As shown in Table \ref{tab:multiturn}, the inclusion of multi-turn data results in performance gains of 3.01\% on Multi-IF, 1.18\% on InfoBench, and 5.10\% on LiveBench, compared to the baseline SFT model without such data. These improvements highlight the critical role of multi-turn interactions in training, allowing the model to better understand conversational context and dependencies, thereby enhancing its instruction-following capabilities. Therefore, we incorporate multi-turn data in our scaling experiments.


\subsection{Ablation Studies on \method}
The iterative augmentation capability of our \composer raises a critical question for the SFT stage: should simple or complex instructions be prioritized for training? Figure \ref{fig:ablation_constraint} presents the results of using varying levels of instruction complexity during the SFT stage. The results demonstrate that as instruction complexity increases, performance correspondingly improves, reaching the peak after three iterations with an improvement ranging from 0.73\% to 1.66\%. Furthermore, we evaluate the effectiveness of our evaluation questions. Without filtering out low-quality responses, performance deteriorates significantly over 3.35\%-5.36\%. This mechanism becomes increasingly critical as instruction complexity grows, with the performance gap widening alongside the increasing complexity, underscoring the importance of this module in maintaining high-quality training data.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/ablation_constraint.pdf}
    \caption{Analysis of the number of added constraints and the evaluation question filter module during the SFT stage.}
    \label{fig:ablation_constraint}
\end{figure}
