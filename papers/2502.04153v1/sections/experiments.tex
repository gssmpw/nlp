\section{Experiments}
\subsection{Experimental Setup}

\paragraph{Datasets and Baselines}
To train \composer, we decompose instructions from ShareGPT \citep{vicuna2023} and generate corresponding evaluation questions by LLaMA-3.1-70B-Instruct.
In our experiments, we collect human instructions from existing open-source datasets, including ShareGPT, OpenHermes2.5, and No Robots \citep{OpenHermes,no_robots,vicuna2023}, and employ \composer to complicate instructions and then generate responses.
For baselines, we reimplement existing methods using either public datasets \citep{sun2024conifer,xu2023wizardlm} or available implementations \citep{dong2024self}, and include a series of currently open and closed-source LLMs. More details are in Appendix \ref{sec:dataset}.


\paragraph{Experimental Settings}
We first fine-tune LLaMA-3.1-8B-Instruct to build our UltraComposer.
Subsequently, we explore two settings to implement our training strategies as written in \S\ref{sec:strategy}.
\begin{itemize}[nolistsep,leftmargin=*]
    \setlength\itemsep{0mm}
    \item \textbf{Strong-to-Weak.} In this setting, knowledge is distilled from a larger model to a smaller one. For \method, we leverage LLaMA-3.1-70B-Instruct for response generation and evaluation and then train LLaMA-3.1-8B-Base.
    \item \textbf{Self-Alignment.} We replace the supervision model with Llama-3.1-8B-Instruct and then train the Base model.
\end{itemize}


Our experiments are conducted on 8$\times$A100 GPUs (80GB) using mixed precision with bf16, DeepSpeed ZeRO Stage 3 \citep{rasley2020deepspeed}, and FlashAttention 2 \citep{dao2023flashattention}. And we choose Xtuner \citep{2023xtuner} as our training framework to implement experiments. More details can be found in Appendix \ref{sec:experiment}.




\begin{table*}[!t]
    \centering
    % \small
    \caption{The main results on five instruction-following benchmarks. Pr. and Ins. stand for prompt and instruction levels, respectively. S and L represent strict and loose metrics for IFEval. For LiveBench, we only report the performance on the subset of instruction-following data. Results marked with $\dag$ are sourced from the original benchmarks, and $\ddag$ represents we reimplement the methods.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lc*{7}{>{\centering\arraybackslash}p{1cm}}*{3}{>{\centering\arraybackslash}p{1.7cm}}}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{\#Data}} &  \multicolumn{4}{c}{\textbf{IFEval}} & \multicolumn{3}{c}{\textbf{Multi-IF}} & \textbf{InfoBench} & \textbf{LiveBench} & \textbf{FollowBench} \\ 
    \cmidrule(r){3-6}  \cmidrule(r){7-9} \cmidrule(r){10-10}  \cmidrule(r){11-11} \cmidrule(r){12-12}   
    & & Pr(S) & Pr(L) & Ins(S) & Ins(L) & Turn1 & Turn2 & Turn3 & DRFR & Score & SSR \\ 
    \midrule
    GPT-4$^\dag$ & - & 76.90  & 79.30  & 83.60  & 85.40  & 81.50  & 70.50  & 60.90  & 89.40  & 69.40 &  78.60 \\ 
    \rowcolor{green!5}  LLaMA-3.1-8B-Instruct$^\dag$ & - & 69.13  & 74.86  & 77.46  & 81.65  & 68.54  & 59.63  & 51.26  & 81.33  & 57.10 & 63.41 \\ 
    \midrule
    \rowcolor{pink!13}  \multicolumn{12}{c}{\textbf{\textit{Strong-to-Weak}    (Supervisor: LLaMA-3.1-70B-Instruct)}} \\ 
    \midrule
    LLaMA-3.1-8B (ShareGPT) & 10k & 43.99  & 54.34  & 54.32  & 64.39  & 44.69  & 25.11  & 18.50  & 81.56  & 33.20 & 59.59 \\ 
    Evol-Instruct \citeyearpar{xu2023wizardlm}$^\ddag$ & 10k & 41.96 & 45.66 & 54.44 & 58.03 & 39.03 & 24.34 & 19.14 & 75.74 & 44.90 & 43.87 \\ 
    Conifer \citeyearpar{sun2024conifer}$^\ddag$ & 13k & 46.40 & 51.02 & 58.51 & 62.59 & 44.91 & 25.83 & 17.95 & 75.73 & 45.60 & 52.42 \\
    \textsc{AutoIF} \citeyearpar{dong2024self}$^\ddag$ & 10k & 47.13  & 56.93  & 57.55  & 67.02  & 47.63  & 27.53  & 20.53  & 80.62  & 40.50  & \textbf{60.41}  \\ 
    \rowcolor{gray!13} \method & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
     + SFT & 10k & 53.97  & 58.59  & 64.15  & 68.82  & 52.55  & 29.34  & 22.29  & 81.91  & 42.20  & 59.50  \\ 
     \rowcolor{blue!5} \hspace{6pt} + Iterative DPO & 8k & \textbf{58.22}  & \textbf{65.25}  & \textbf{68.11} & \textbf{74.22}  & \textbf{58.14}  & \textbf{35.65}  & \textbf{26.55} & \textbf{83.56}  & \textbf{49.50} & 59.99   \\ 
    \midrule
    \rowcolor{pink!13} \multicolumn{12}{c}{\textbf{\textit{Self-Alignment}    (Supervisor: LLaMA-3.1-8B-Instruct)}} \\ 
    \midrule
    \rowcolor{gray!13} \method & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\ 
     + SFT & 10k & 55.82 & 58.78 & 66.18  & 69.54 & 55.59 & 36.72 & 28.07 & 77.78 & 46.60 & 55.88\\ 
     \hspace{6pt} + Iterative DPO & 8k & 56.93 & 64.14 & 66.66 & 73.02 & 58.63 & 42.04 & 31.20 & 79.86 & 54.20 & 58.56\\
     \hdashline[2pt/3pt]
     + SFT $scale\ up$ & 175k & 69.87  & 72.46  & 77.46  & 80.22  & 66.24  & 53.66  & 42.19  & 79.20 & 51.40  & 59.93 \\ 
     \rowcolor{blue!5} \hspace{6pt} + Iterative DPO & 20k & \textbf{71.35}  & \textbf{75.42}  & \textbf{79.38}  & \textbf{83.09} & \textbf{69.63} & \textbf{58.28} & \textbf{46.86} & \textbf{80.70} & \textbf{56.00} & \textbf{62.55 } \\ 
    \bottomrule
    \end{tabular}
    }
    \label{tab:main_instruction}
\end{table*}


\paragraph{Evaluation}
We evaluate \method on five instruction-following benchmarks, including IFEval \citep{zhou2023instruction}, Multi-IF \citep{he2024multi}, InfoBench \citep{qin2024infobench}, FollowBench \citep{jiang2023followbench}, and LiveBench \citep{white2024livebench}. While IFEval and Multi-IF focus on testing verifiable instructions using functions, the others extend to more general instructions that need to be evaluated by LLMs. The details about benchmarks are provided in Appendix \ref{sec:benchmark}.

In addition, we further test the general ability of \method such as mathematical \citep{chen2021evaluating}, reasoning \citep{suzgun2022challenging}, coding \citep{cobbe2021training}, and general interaction capabilities \citep{li2024crowdsourced}.



\subsection{Main Results}

Table \ref{tab:main_instruction} shows the performance of \method on five instruction-following benchmarks.

\paragraph{\method Outperforms All Previous Methods}
In the Strong-to-Weak setting, \method demonstrates performance that is comparable to or exceeds previous methods across all datasets. By fine-tuning on our generated data, \method achieves substantial improvements, particularly on IFEval and Multi-IF. When compared to strong baselines like AutoIF \citep{dong2024self}, \method achieves scores of 53.97 (Pr(S)) and 64.15 (Ins(S)) on IFEval and 81.91 (DRFR) on InfoBench, surpassing AutoIF by margins ranging from 1.29\% to 6.84\%. These results underscore \method's capability to effectively follow instructions, even with lower training data, representing a significant advancement over state-of-the-art approaches.


\paragraph{Iterative DPO Boosts Performance Effectively}
As shown in Table \ref{tab:main_instruction}, the iterative DPO process substantially enhances alignment with complex instructions. Specifically, in comparison to SFT, iterative DPO achieves an average improvement of 5\% in the Strong-to-Weak setting and 3.8\% in the Self-Alignment setting for multi-turn instruction-following tasks. Furthermore, this process enables \method to surpass state-of-the-art methods in three benchmarks that require LLM-based evaluation, with an improvement of 1.5\% on InfoBench, 4.6\% on LiveBench, and 2.62\% on FollowBench, demonstrating the importance of \method in handling diverse instructions.



\paragraph{Smaller Supervisor Yields Better Performance}
A comparison between the self-alignment and strong-to-weak settings reveals that the self-alignment setting, which employs a smaller model as a supervisor, achieves superior performance. This observation is consistent with the findings of \citet{hui2024smaller}. Notably, it is particularly evident during the SFT stage, where self-alignment outperforms strong-to-weak on Multi-IF and LiveBench. Although the improvements introduced by DPO are relatively minor, self-alignment continues to exhibit superior performance on these two benchmarks, further emphasizing the advantages of leveraging a smaller supervisory model.


\paragraph{\method Achieves A New Milestone}
By scaling up the training data, \method achieves a new milestone in instruction-following alignment. With 175k data in the SFT stage and 20k data in the DPO stage, \method reaches impressive performance, with 71.35 (Pr(S)) and 79.38 (Ins(S)), while the LLaMA-3.1-8B-Instruct model only achieves 69.13 (Pr(S)) and 77.46 (Ins(S)), and comparable across the left benchmarks. This demonstrates that \method, when optimized and trained on larger datasets, not only improves instruction-following capabilities but also comes closest to matching the performance of LLaMA-3.1-8B-Instruct, marking a significant leap forward in model performance.

\begin{table*}[!ht]
    \centering
    \small
    \caption{The general performance on mathematical, reasoning, coding, and conversational domains. We report Pass@1 on HumanEval, Acc on BBH and GSM8k, and Win Rate on Arena Hard.}
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \textbf{Code} & \textbf{Reasoning} & \textbf{Math} & \textbf{Conversation} & \textbf{General }\\ 
        \cmidrule(r){2-2}  \cmidrule(r){3-3}  \cmidrule(r){4-4}  \cmidrule(r){5-6} \cmidrule(r){6-6}
        ~ & \textbf{HumanEval} & \textbf{BBH} & \textbf{GSM8k} & \textbf{Arena Hard}  & \textbf{LiveBench [All]} \\ 
        \midrule
        \rowcolor{green!5} LLaMA-3.1-8B-Instruct & 65.24  & 68.54  & 80.80  & 18.30  & 25.90 \\ 
        AutoIF \citeyearpar{dong2024self} & 46.34  & 67.18  & 51.50  & 9.20 & 17.50  \\
        \midrule
        \method + SFT & 43.90 & 67.33 & 48.60 & 12.20 & 21.30  \\ 
        \hspace{15pt} + Iterative DPO & 47.56 &  68.03 & 48.10 & 16.00 & 21.70 \\ 
        \hdashline[2pt/3pt]
        + SFT $scale\ up$  & 52.44  & 67.26  & 66.70  & 16.00   & 22.80  \\ 
        \rowcolor{blue!5}  \hspace{15pt} + Iterative DPO & 55.49  & 68.44  & 68.00  & 31.40  & 23.10   \\ 
        \bottomrule
    \end{tabular}
    \label{tab:main_cross}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \caption{The performance and its difference compared to the SFT model across each iteration during the Iterative DPO process.}
    \begin{tabular}{lcccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Iteration}} &  \multicolumn{4}{c}{\textbf{IFEval}} & \multicolumn{3}{c}{\textbf{Multi-IF}} & \textbf{LiveBench} \\ 
    \cmidrule(r){2-5}  \cmidrule(r){6-8} \cmidrule(r){9-9} 
    & Pr(S) & Pr(L) & Ins(S) & Ins(L) & Turn1 & Turn2 & Turn3 & Score \\ 
    \midrule
    \textit{Iter 1} & 55.45$_{+1.48}$ & 61.55$_{+2.96}$ & 65.10$_{+0.95}$ & 70.74$_{+1.92}$ & 56.13$_{+3.58}$ & 32.11$_{+2.77}$ & 24.38$_{+2.09}$ & 42.20$_{+0.00}$ \\
    \textit{Iter 2} & 55.08$_{+1.11}$ & 62.66$_{+4.07}$ & 65.47$_{+1.32}$ & 71.82$_{+3.00}$ & 57.26$_{+4.71}$ & 34.92$_{+5.58}$ & 26.28$_{+4.00}$ & 47.20$_{+5.00}$ \\
    \textit{Iter 3} & 56.75$_{+2.78}$ & 63.03$_{+4.44}$ & 66.79$_{+2.64}$ & 72.42$_{+3.60}$ & 57.10$_{+4.55}$ & 34.87$_{+5.53}$ & 26.11$_{+3.82}$ & 45.70$_{+3.50}$ \\
    \textit{Iter 3$_{w. NCA}$}  & 58.22$_{+4.25}$ & 65.25$_{+6.66}$ & 68.11$_{+3.96}$ & 74.22$_{+5.40}$ & 58.14$_{+5.59}$ & 35.65$_{+6.31}$ & 26.55$_{+4.26}$ & 49.50$_{+7.30}$ \\
    \bottomrule
    \end{tabular}
    \label{tab:iterative_dpo}
\end{table*}


\subsection{Cross-Domain Validation}
To verify the generalizability of \method, we evaluate \method across four general domains, including coding, reasoning, mathematical problem solving, and conversational tasks. Table \ref{tab:main_cross} presents the performance of \method compared to AutoIF and LLaMA-3.1-8B-Instruct. While \method shows slightly lower performance than AutoIF in the mathematical domain, it achieves significant improvements in coding and conversational tasks. Additionally, scaling the training data leads to remarkable performance gains, and the DPO stage consistently improves results across all domains. Notably, \method significantly improves the general capabilities of models, particularly on the comprehensive LiveBench benchmark \citep{white2024livebench} and the ArenaHard \citep{li2024crowdsourced} conversational task. Specifically, it outperforms AutoIF by a notable margin of 4.2\% on LiveBench, while achieving an impressive 15.4\% improvement in conversational performance on ArenaHard. These results suggest that \method facilitates the development of more general and versatile models, capable of addressing a wide range of tasks effectively.



