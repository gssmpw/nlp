\section{Related Work}
\subsection{Instruction Following}

Instruction following has become a key area in LLMs, with a focus on improving the ability to understand and execute complex human instructions. Early approaches \citep{wei2021finetuned,no_robots,jiang2023followbench} have relied heavily on curated datasets involving human-generated instructions and their corresponding responses. Recent work has turned to leveraging LLMs to automate the process. For example, \citet{xu2023wizardlm} prompts LLMs to evolve instructions through in-depth or in-breadth transformations. \citet{sun2024conifer} directly guides LLMs to complicate them. But relying solely on LLMs leads to the inclusion of low-quality samples, as this is limited by LLM's instruction-evolving ability. \citet{wang-etal-2023-self-instruct,dong2024self} introduce human priors such as verifiable constraints to ensure the quality of generated responses. However, they restrict the diversity of instructions compared to real-world scenarios.

In contrast, \method decomposes real-world user instructions for both constraints and evaluation questions, then trains a composer model to synthesize diverse and complex instructions with correct responses, presenting an effective mechanism for generating instruction-following data.

% utilizes the instruction decomposition and self-evaluation abilities of LLMs to train the \composer, which seamlessly ands dynamically add constraints into instructions. This allows \method to generate high-quality instruction data that is both diverse and scalable while minimizing human labor and expertise.

\subsection{Perference Learning with Instruction Following}

Perference learning has gained prominence as a method to enhance instruction-following capabilities by refining models to generate high-quality responses based on feedback \citep{NEURIPS2022_b1efde53, dong2024self, sun2024conifer,gao2024unifiedviewpreferencelearning}. In this context, it is typically employed to optimize models that have been finetuned on instruction datasets, thus improving the accuracy and alignment in generating high-quality outputs. A common approach leverages reward signals, which may be human-provided or automatically generated, to guide the model's learning process. For instance, reinforcement learning from human feedback (RLHF) often employs Proximal Policy Optimization (PPO) to optimize models, but this approach requires ranked responses, which can be resource-intensive and dependent on human labor. Recent work by \citet{rafailov2024direct} and \citet{chen2024noise} introduces direct optimization of preferences, offering a solution to these challenges and reducing the need for extensive human feedback.

\method enables the generation of evaluation questions to guide preference learning in a more structured and efficient manner. It complements direct preference optimization by providing a scalable approach to generating instruction-following data, reducing costs and enhancing the scalability for instruction-following tasks.