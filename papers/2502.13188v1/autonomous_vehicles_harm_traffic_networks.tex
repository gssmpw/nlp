\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{paralist}
\usepackage{multirow}



\usepackage{hyperref}
\usepackage{soul}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage{comment}
\usepackage{bbm}
\usepackage{subcaption}
% We added the last one

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{}%Submission and Formatting Instructions for ICML 2025}
\newcommand{\ignore}[1]{}
\begin{document}

\twocolumn[



\icmltitle{Autonomous Vehicles Using Multi-Agent Reinforcement Learning for Routing Decisions Can Harm Urban Traffic}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anastasia Psarou*}{yyy}
\icmlauthor{Ahmet Onur Akman}{yyy}
\icmlauthor{Łukasz Gorczyca}{sch}
\icmlauthor{Michał Hoffmann}{sch}
\icmlauthor{Zoltán György Varga}{yyy}
\icmlauthor{Grzegorz Jamróz}{sch}
\icmlauthor{Rafał Kucharski}{sch}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{yyy}{Doctoral School of Exact and Natural Sciences, Jagiellonian University, Kraków, Poland}
\icmlaffiliation{sch}{Faculty of Mathematics and Computer Science, Jagiellonian University, Kraków, Poland}

\icmlcorrespondingauthor{Anastasia Psarou}{anastasia.psarou@doctoral.uj.edu.pl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Multi-agent reinforcement learning, Route choice, Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
    


\begin{abstract}
Autonomous vehicles (AVs) using Multi-Agent Reinforcement Learning (MARL) for simultaneous route optimization may destabilize traffic environments, with human drivers possibly experiencing longer travel times. We study this interaction by simulating human drivers and AVs. Our experiments with standard MARL algorithms reveal that, even in trivial cases, policies often fail to converge to an optimal solution or require long training periods. The problem is amplified by the fact that we cannot rely entirely on simulated training, as there are no accurate models of human routing behavior. At the same time, real-world training in cities risks destabilizing urban traffic systems, increasing externalities, such as $CO_2$ emissions, and introducing non-stationarity as human drivers adapt unpredictably to AV behaviors. Centralization can improve convergence in some cases, however, it raises privacy concerns for the travelers' destination data. In this position paper, we argue that future research must prioritize realistic benchmarks, cautious deployment strategies, and tools for monitoring and regulating AV routing behaviors to ensure sustainable and equitable urban mobility systems.

\end{abstract}



\section{Introduction}


\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/story.png}}
\caption{\textbf{Overview.} We demonstrate how MARL fails to find optimal routing policies on a simple topology of two routes: shorter with no priority and longer with priority (a). We simulate 22 human drivers who reach two equilibria: (b) optimal, when all drivers use the shorter route 0, (c) and suboptimal, when all use the longer route 1. If we replace a single human driver with an AV, any standard RL algorithm will quickly find the optimal routing policy (d). However, when multiple AVs learn simultaneously, many \textbf{MARL algorithms fail to converge to the optimal policy} (e) or require hundreds of days (episodes) until they find it (f), even for such a trivial case.}
\label{fig:overview}
\end{center}
\vskip -0.1in
\end{figure}

In urban traffic networks, humans (drivers) every day make routing decisions \cite{arnott1990departure} to arrive at their destinations as fast as possible \cite{bovy2005modelling}. With the advent of autonomous vehicles (AVs), these routing decisions may be delegated to algorithms, aiming to maximize the reward by selecting the optimal action in the current state of the network. Classically, this problem was formulated as a game-theoretical problem \cite{correa2004selfish}, where humans independently maximize their perceived payoffs. 


 Traditional methods that address the route choice problem often struggle to adapt to the complex and dynamic nature of mixed traffic scenarios involving AVs and human drivers \cite{BAMDADMEHRABANI2024}. A plausible extension is to employ ML-equipped AVs and specifically use \emph{reinforcement learning}, where each agent (vehicle) learns optimal policies to select the best route in the currently observed state of the urban road network. AVs can leverage approaches such as multi-agent reinforcement learning (MARL) to optimize their routing decisions for faster arrivals, benefiting fleet operators or individual clients. MARL has been used to solve this problem \cite{regret_route_choice, Thomasini+2023, AgentRewardShaping}, but from different perspectives, as discussed later.
 

 As AVs will start sharing the roads with humans in a mixed system, at least for a while until human driving ceases, they will be influencing the complex social dynamics of individual, rational yet non-deterministic driving behavior \cite{influence_of_AVs_on_car_following_behavior}. However, as we argue  machine intelligence will not have (as of today) a sufficiently detailed model of urban mobility capable of training routing algorithms. Furthermore, as we demonstrate, specifically during training, the joint actions of AVs may lead to suboptimal solutions, resulting in a cost to all users in the capacitated system with limited resources. Moreover, AVs are likely to take different actions than human drivers, which will likely trigger adaptations of humans. Humans then will change their routing policies in response to AVs' actions. This impact is up to some level negligible and single or few AVs routing recklessly during training will not disequilibrate the system. Yet the critical mass of AVs can be quickly reached as the AVs become broadly available (as little as 15\% of AVs in our experiment disequilibrate the system, \cref{fig:different-fleet-number}). There exist projections that by 2030, the number of commercial AVs for ridesharing will reach a few million \cite{goldman2024partiallyautonomous} and \citet{placek2024autonomousVehicles} projects that the number of AVs will rise significantly, from 17,000 in 2022 to 126,000 by 2030.





\textbf{This position paper argues that even in trivial cases, multiple AVs simultaneously learning optimal MARL routing policies will either destabilize the road networks and fail to find optimal solutions or learn long enough to affect the system's performance. } We support this position with experiments on a toy network (\cref{fig:overview}a) with a portfolio of MARL algorithms, discuss and show that:

\begin{compactenum}
\item Human drivers, by maximizing their payoffs, stabilize the system into two equilibria (\cref{fig:overview}b, \ref{fig:overview}c). 
\item RL efficiently finds the optimal route for a single AV (\cref{fig:single-agent}). However, with multiple human drivers replaced by AVs, MARL either \textbf{fails to converge or needs lengthy training} to find the optimal solution (\cref{fig:marl-proportions}).
\item The simulators of urban mobility are not ready to serve as virtual environments to train MARL, and training in the real-world will be at the cost of the system's performance (\cref{sec:traffic_models}).
\item Optimal equilibrium state can easily transition to suboptimal (with worse performance for each agent) when human adaptation is added as another source of non-stationarity (\cref{fig:adapt-centr} top).
\item Centralization can, in some cases, accelerate the convergence to optimal policies, but at the cost of our privacy (\cref{fig:adapt-centr} bottom).


\end{compactenum}




This hinders the massive potential of AVs to contribute to sustainability \cite{taiebat2018review}, efficiency \cite{talebpour2016influence}, and optimality \cite{zhou2024modeling} of urban mobility. Connected and Autonomous Vehicles (CAVs) offer potential as they promise novel routing strategies that allow the reduction of total and individual costs (travel times) and system costs (like total delays) \cite{jamróz2024socialimpactcavs} and its externalities ($\text{CO}_\text{2}$, $\text{NO}_\text{x}$, safety, noise, etc.) \cite{kopelias2020connected}. 

Based on findings from our illustrative experiments, to safely and reliably exploit the opportunities that AVs and MARL offer to the future urban traffic systems, \textbf{we call for}:

\begin{compactenum}
    \item \textbf{Broad experimental studies} demonstrating the potential impact of MARL-enabled AVs on urban systems.
    \item \textbf{Benchmarks} to test proposed MARL solutions in the standard set of networks, demand patterns, rich metrics, and efficient, rapidly converging \textbf{MARL algorithms} to simulate AVs' decision-making.
    \item \textbf{Data-driven development of urban traffic simulators}, realistically reproducing human route choice (in the presence of real-time information) and its adaptation to dynamic changes in the environment.
    \item \textbf{Careful deployment of MARL routing systems} in future AVs.
 
\end{compactenum}






\section{Background}


\subsection{Related work}
\label{sec:related_work}

MARL has been applied in many real-world applications such as finance, transportation, manufacturing \cite{Zhou_2024} as well as chemistry and biology \cite{NING202473}. For instance, \citet{mi2024taxaidynamiceconomicsimulator} introduced a simulation environment to model dynamic interactions among governments, households, firms, and financial intermediaries. \citet{Ma2020FeudalMD} utilized a cooperative MARL approach to control the traffic signals in different road networks. 


MARL has also been used to solve the route choice problems considering different aspects of the problem. \citet{AgentRewardShaping} explored Q-learning for route selection, emphasizing reward-shaping techniques aiming to eliminate traffic congestion. Another study approached route choice as a congestion game aiming for a collective equilibrium in the traffic network \cite{ZHOU2020124895}. \citet{regret_route_choice} proposed a regret-minimization approach \cite{Blum_Mansour_2007} that relies on external traffic data, which may limit its applicability in scenarios where such information is incomplete or inaccurate. In a different vein, \citet{Thomasini+2023} addressed route choice in a centralized multi-agent setting using a macroscopic traffic simulation. \citet{lazar2021learningdynamicallyrouteautonomous} address the route assignment problem to develop policies for CAVs that prevent unbounded delays in traffic networks. \citet{akman2024impact} introduced AV-specific behavioral reward formulations in mixed-traffic environments.




%%% Why RL is a natural thing to use here.
\subsection{Multi-agent reinforcement learning}
\label{sec:marl-motivation}

The route assignment problem is a combinatorial optimization problem. A natural approach to this problem is to leverage an ML method capable of adapting to evolving and dynamic environments, such as urban networks. MARL is well-suited for representing AVs in traffic as it enables decentralized decision-making, adaptability to dynamic environments, and scalable multi-agent coordination.  


MARL involves multiple agents interacting within a shared environment, where each agent's actions can influence the state of the environment. This makes the environment non-stationary from a single agent's perspective. Employing a single-agent RL algorithm to learn value functions of \textit{joint} actions would eliminate the non-stationarity \cite{centralization, ejal}. However, such a training setting does not scale well when the size of the action space grows exponentially with the number of agents \cite{lu2024centralized} and requires a coordination mechanism. 

It is also possible to train a set of \textit{independent learners} (IL) where each learner treats every other agent as a part of the environment. Our experiments start with Independent Q-Learning (IQL) as the initial baseline \cite{iql}, followed by actor-critic methods such as the Independent SAC (ISAC), which is the multi-agent version of the Soft Actor-Critic (SAC) algorithm \cite{sac} and the Independent Proximal Policy Optimization (IPPO) algorithm, as it has demonstrated strong benchmark performance across a variety of problems \cite{yu2022surprisingeffectivenessppocooperative, papoudakis2021benchmarkingmultiagentdeepreinforcement}.


Existing literature widely utilizes the \textit{Centralized Training and Decentralized Execution} (CTDE) structure, in which agents learn decentralized policies in a centralized manner, to be used independently during the execution phase \cite{lowe2020multiagentactorcriticmixedcooperativecompetitive}. This structure tackles partial observability, while maintaining feasible compute times.

Following this structure, \citet{vdn} introduced Value Decomposition Networks (VDN), a value-based algorithm that decomposes complex learning problems into sub-problems. \citet{qmix} proposed a similar algorithm named QMIX, which instead adopts a mixing network with a monotonicity constraint, enabling more complex coordination. Alternative to these value decomposition methods, \citet{mappo} introduced a multi-agent version of the Proximal Policy Optimization (PPO) algorithm \cite{ppo}, Multi-Agent PPO (MAPPO). Similarly, Multi-Agent SAC (MASAC) represents the multi-agent version of the SAC algorithm \cite{sac}. All these algorithms were employed to demonstrate our position in the experiments.



\subsection{Partially Observable Markov Game (POMG)}
We abstract the daily (repeated) route choices made by humans (and AVs in the future) in capacitated networks (limited resources) to the so-called routing game. This, in turn, can be formalized with a POMG as a MARL problem. We denote a finite-horizon multi-agent general-sum POMG as a tuple $(\mathcal{S}, \{\mathcal{O}_i\}_{i \in \mathcal{N}}, \{\mathcal{A}_i\}_{i \in \mathcal{N}}, \mathcal{P}, \{r_i\}_{i \in \mathcal{N}}, \gamma)$, where $\mathcal{N} = \{ 0, \ldots, N-1 \}$ represents the set of N agents. $\mathcal{S}$ represents a finite state space, partially observable by the agents. The global state $s \in  \mathcal{S}$ includes the joint agent observation (i.e., $o_{1} \times o_{2} \times ... \times o_{N}$) and additional environmental information. Each agent $i$ can apply an action $a_{i}^t \in \mathcal{A}_i$ at each timestep $t$ from its own action space $\mathcal{A}_i \subseteq \mathcal{A}$ to the environment and consequently receive a reward $r_{i}^t$ from the environment as a function of the state and the joint action. The state transition probability is defined by $\mathcal{P}(s' \mid s, a): \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, 1]$ and indicates the probability of transitioning to state $s' \in \mathcal{S}$ after a joint action $a = (a_1, a_2, \dots, a_N) \in \mathcal{A}$.  A policy $\pi_i : \mathcal{O}_i \times \mathcal{A}_i \to [0, 1]$ specifies the probability distribution over the actions of agent $i$. The goal of each agent $i$ is to learn a $\pi_i$ that maximizes its expected cumulative reward $G = \mathbb{E}_{\pi_i} \left[ \sum_{t=0}^{\infty} \gamma^t \cdot r_i^t \right]$, where $\gamma \in [0, 1)$ is the discount factor.


\subsection{Are traffic models ready to train RL algorithms?}
\label{sec:traffic_models}
We illustrate our position with a numerical simulation designed to replicate the real world and its complexity. However, the actual conditions in which AVs will be deployed are much more complex.
Namely, at the level of: 
\begin{compactenum}
\item \textbf{Traffic flow}. We use Simulation of Urban MObility (SUMO) \cite{SUMO2018}, which applies Intelligent Driver Model (IDM) \cite{Treiber_2000} accurate, but not perfect microscopic model of traffic. Real traffic is less predictable and noisy and admits rare events like accidents \cite{chen2018exploring}.
\item \textbf{Demand patterns}. In this paper, we assume a fixed commute pattern every day. %Each morning, the same number of drivers depart from the same origins to the same destinations at the same time.
Namely, each agent has a fixed origin from which it departs every day at the same time to a fixed destination. However, in real systems, humans change departure times, work irregularly, and occasionally stay at home or travel to different destinations \cite{Gonz_lez_2008, horni2016matsim, Bhat1999}. 
\item \textbf{Route choices}. Humans are non-deterministic decision makers, and their choices are probabilistic with a significant impact of variation and heterogeneity on the choice probabilities (like in the Logit model \cite{ben1999discrete}).
\item \textbf{Action space}. In real networks, the number of feasible routes explodes quickly, reaching $10^{40}$ in many real-world examples \cite{frejinger2009sampling}. 
\end{compactenum}
Unfortunately, all of the above are only roughly approximated with state-of-the-art transport system models. Our simple case involves 22 vehicles, two routes, and two equilibria that can be derived analytically. Although this setup is simple, it already gives rise to issues (like increased travel time for humans), and one can only expect them to worsen as the system scales to bigger networks with irregular demand patterns and non-deterministic traffic flows, which cannot be presented with state-of-the-art simulation models of urban mobility.




\section{Methodology}
\label{sec:components_of_simulation}
We illustrate our position on a simple two-route network (\cref{fig:overview}a), which seems abstract but can be identified in many complex urban topologies in the world (see \cref{sec:two-route-nets} for examples). Each agent selects from two possible routes (0 or 1) to reach the destination. Then, the reward is collected from the environment to update choices for the next episode. Each episode is interpreted as a day on which each of the 22 drivers commutes through our network. For clarity, the setting is unrealistically static. Each human driver follows the same route every day, departing at the same time, and the travel times do not vary day-to-day (none of the above holds in real networks as discussed in \cref{sec:traffic_models}). By design, the system has two Nash equilibria achievable by humans, one optimal (System Optimal - User Equilibrium, \cref{fig:overview}b) and one suboptimal (Suboptimal - User equilibrium, \cref{fig:overview}c). Humans \emph{mutate} into AVs, which will use any MARL algorithms to find optimal policies. To simulate this scenario RouteRL \cite{RouteRL2024} environment is employed (\cref{fig:routerl}), which models mixed-traffic scenarios, where AVs are RL-enabled agents and human agents behave according to a given human-behavior model.


\paragraph{SUMO.} SUMO \cite{SUMO2018} is a state-of-the-art, microscopic, agent-based traffic simulator, in which each vehicle navigates the road network according to the IDM.

\paragraph{Action.} The action space of the agents corresponds to the set of available routes connecting
their given origin with their destination and is, thus, discrete with value two, on this network (See \cref{fig:overview}a).

\paragraph{Reward.}
The reward $r_{i}^t(a_{i}^t | s_{i}^t)$ is the negative travel time of each agent $i$ to reach from its origin to its destination, as calculated by SUMO.

\paragraph{Observation.}
We assume, plausibly for the future systems, that AVs observation \(o_{i}^t\) is composed of their departure time and the number of agents that departed before them and chose routes 0 and 1, respectively. 


\paragraph{Human agents.}
\label{par:human_agents}
We follow the classical representation of human route-choice behavior from transportation research. Human drivers are rational decision-makers aiming to maximize their perceived utility \cite{Cascetta} by selecting actions that minimize expected travel times. Their expectations are updated based on experienced travel times (from SUMO). In scenarios with adaptation, humans shift to an alternative route with $10\%$ probability. 


\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth, keepaspectratio]{images/RouteRL.png}}
\caption{We model the routing "game" between humans and AVs on urban traffic networks using \textbf{RouteRL} \cite{RouteRL2024}, a custom \textit{PettingZoo} \cite{terry2021pettingzoo} environment, communicating with \textit{SUMO} as it trains optimal routing policies with standard MARL algorithms via torchRL \cite{bou2023torchrl}.
Every day, agents (humans or/and AVs) select a route and their actions are fed into SUMO to generate travel times (rewards), which are then used to update the policy for the next episode (day).}

\label{fig:routerl}
\end{center}
\vskip -0.1in
\end{figure}



\section{Experiments}


\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{images/single-AVs.png}}
\caption{\textbf{Single AV} replicates human choices, and each tested RL algorithm finds the optimal solution to the binary choice problem. During the training, humans are barely affected by suboptimal actions, and when trained, the system is back in its original equilibrium state. The line represents the mean across realizations and the error band accounts for variations in agent start times and application of different algorithms, as described in Section \ref{sec:single_agents}.}

\label{fig:single-agent}
\end{center}
\vskip -0.1in
\end{figure}


\subsection{Human system and its equilibria.}
\label{sec:equilibria}

We consider two plausible equilibria resulting from human collective actions: first, when all humans select the shorter route ($[\{0\}^{22}]$, \cref{fig:overview}b) and second when they all opt for the longer route ($[\{1\}^{22}]$, \cref{fig:overview}c).
Both meet Nash criteria for User Equilibrium \cite{Wardrop1952ROADPS} (common paraphrase of Nash equilibrium for the route choice context), with the latter being also System Optimal (minimizing total travel time in the system \cite{merchant1978optimality}). None of the drivers is inclined to change their route, as it would reduce individual rewards (travel longer and arrive later). For stability, we fix the route for the first two agents to stabilize the early loading of the system. After 200 days of simulation, the system is stable (the next day the rational drivers will replicate today's choices), fair (travel times are equal among drivers), and either globally optimal (\cref{fig:overview}b where total travel costs are minimized) or suboptimal (\cref{fig:overview}c). The individual and system costs (travel times) are reported in \cref{tab:costs}.

\subsection{Single AV routing with RL.}
\label{sec:single_agents}

In the equilibrated human system, we first replace a random human vehicle with an RL-controlled AV. Its traffic properties (reaction, acceptance gap, acceleration profile, and other IDM  parameters \cite{Treiber_2000}) remain intact. AVs are indistinguishable from humans by all but routing decisions: they may use any RL (MARL) algorithm to converge to the optimal policy.

We demonstrate that RL quickly finds an optimal solution, and AV agents follow the crowd (replicate the decision of the human that drove that vehicle before). Unsurprisingly, since the problem is a trivial binary decision in a static environment, this is true for all suitable RL algorithms, including Deep Q-Learning (DQN), PPO, and SAC as we demonstrate in \cref{fig:single-agent}. In any equilibrated system, any RL algorithm training AVs with the same reward formulation, and action space as their human predecessors will replicate the optimal strategy, which can be derived directly from conditions of Nash equilibrium. Moreover, in a dense traffic environment, the impact of a single vehicle is unlikely to be noticed by other humans. The marginal cost of actions taken by a single AV to other humans will fall within what is known as the indifference band \cite{di2017indifference}, making them indistinguishable from the traffic stochastic noise \cite{neun2023traffic4cast}. \textbf{Our position, that AVs will disequilibrate the traffic networks, does not hold for a single AV in a static environment}, which will converge to a solution that neither deviates from its human predecessor nor impacts other human drivers.



\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/proportions.png}}
\caption{\textbf{8000} episodes (that correspond to over 20 days of commute) of training for selected MARL algorithms. The trivial solution is found only by IPPO and MAPPO, other algorithms fail to converge to optimal policy. Suboptimal equilibrium is reproduced after 1000 days but System optimal requires much more training, far exceeding  the patience threshold for the remaining 12 human drivers in the system. 
The first 200 episodes represent the human learning phase, followed by the training and testing phases of AVs. See \cref{sec:experiments_details} for the experiments' setup.}
\label{fig:marl-proportions}
\end{center}
\vskip -0.1in
\end{figure}

\subsection{Simultaneous learning of multiple AVs with MARL.}
Already in our simple scenario, serious issues arise when multiple vehicles learn optimal policies simultaneously. Most likely, more than one AV will be introduced into our cities (whenever they become ready to operate reliably), each solving the same problem of identifying optimal routing policies to reach its destination. Such a multi-agent setting allows a significant alteration to the initial problem: the AVs may communicate (becoming the so-called CAVs) and share data.


\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/plot5_many_algos.png}}
\caption{With an increasing number of agents, a fraction of those, who learn optimal policy decreases, achieving similar efficiency to random choice (route 0: system optimal - user equilibrium, route 1: suboptimal user equilibrium). This appears to happen with as few as four agents for some of the tested algorithms (ISAC, IQL) that failed to converge in \cref{fig:marl-proportions}. See \cref{sec:experiments_details} for the experiments' setup.}
\label{fig:different-fleet-number}
\end{center}
\vskip -0.1in
\end{figure}


First, we demonstrate the natural first stage of AV introduction: with no communication and reward formulation identical to the one of the selfish human drivers. Now, the environment has become non-stationary \cite{jiang2024blackbox}, as the state transitions and rewards of an agent are influenced by the evolving policies of other agents. With as few as 10 AVs (where non-convergence appears after the introduction of just 4 AV agents discussed in Section \ref{para:fleet_size} and \cref{sec:smaller_av_fleets}) our setting is sufficient to argue for our \emph{position}. 

\textbf{Despite the very small size of joint action space, some algorithms fail to find the optimal solutions after thousands of iterations. Others need hundreds of policy updates to find the trivial solution}. Specifically, the trivial solutions are $[\{1\}^{22}]$ or $[\{0\}^{22}]$ depending on the human equilibrium from which we start, and the joint action space is $2^{10}$.

In \cref{fig:marl-proportions}, we present two arguments supporting our position. First is the class of algorithms that failed to converge after sufficiently long training (QMIX, VDN, IQL, MASAC, ISAC). Specifically, the choices are far from optimal, as reflected in the noisy profiles that fluctuate around 0.5. QMIX and VDN probably do not find optimal solutions due to the cooperative nature of their implementation. The difficulty of achieving stability and convergence in ISAC and MASAC can be attributed to their off-policy nature combined with the nonlinear function approximation used in neural networks \cite{DBLP:journals/corr/abs-1812-05905, NIPS2009_3a15c7d0}. In such a setting, the consequences of introducing MARL routing algorithms to AVs are negative to all parties: not only the travel times are longer for all agents (humans and AVs) but they also become variable, as we report in \cref{tab:costs}. IPPO and MAPPO, however, converged after a lengthy training.



\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/proportions-adapt.png}}
\caption{\textbf{Top}. IPPO and MAPPO still converge if the adaptation term is added to the model (dotted lines). However, the system diverts from the optimal state to the suboptimal, where the total costs are higher. {\textbf{Bottom}.} The centralized version of IPPO can, in some cases, accelerate convergence, reducing othe time required to reach an optimal solution. However, this reduction is limited to a few days. See \cref{sec:experiments_details} for the experiments' setup.}
\label{fig:adapt-centr}
\end{center}
\vskip -0.1in
\end{figure}

\subsection{Training in virtual traffic environment.} Some MARL algorithms converged and successfully managed to identify optimal policies (the small joint action space of 1024 can be easily enumerated to explicitly identify optimal solutions). Eventually, the 10 AVs managed to stabilize their actions and reliably make optimal choices (\cref{fig:marl-proportions}). After that, the negative impact on the system and the humans diminishes (\cref{tab:costs}). Nonetheless, the learning process was lengthy, with thousands of episodes (days).


\begin{table*}[t]
\caption{Average travel times (rewards) (in seconds) for each subgroup (AVs, humans, and both), with standard deviations within each subgroup in parentheses. ‘Human system’ refers to rollouts up to the 200th episode, before the introduction of AVs. The remaining values (MARL, Centralized) are calculated from aggregated results during the testing phase and averaged across repeated experiment folds. The lowest travel times for each subgroup in each experimental setting are highlighted in \textbf{bold}.}
\label{tab:costs}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}llcccccc@{}}
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{System Optimum \& User Equilibrium} & \multicolumn{3}{c}{Suboptimal \& User Equilibrium} \\ 
%\midrule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & AVs & Humans & \multicolumn{1}{c}{All} & AVs & Humans & All \\ 
\midrule
{Human system} & \multicolumn{1}{c|}{} & - & 53.1 (13.1) & \multicolumn{1}{c|}{53.1 (13.1)} & - & 65.9 (15.5) & 65.9 (15.5) \\ 
\midrule
\multirow{7}{*}{MARL}
 & \multicolumn{1}{l|}{IPPO} & 59.1 (13.3) & 55.9 (21.1) & \multicolumn{1}{c|}{57.4 (18.2)} & \textbf{69.9 (13.3)} & \textbf{62.5 (16.4)} & \textbf{65.9 (15.5)} \\
 & \multicolumn{1}{l|}{MAPPO} & \textbf{57.4 (12.0)} & \textbf{51.2 (15.6)} & \multicolumn{1}{c|}{\textbf{54.0 (14.4)}} & \textbf{69.9 (13.3)} & 62.6 (16.4) & \textbf{65.9 (15.5)} \\
 & \multicolumn{1}{l|}{ISAC} & 72.1 (17.4) & 71.5 (26.9) & \multicolumn{1}{c|}{71.8 (23.1)} & 82.0 (16.7) & 61.0 (15.4) & 70.6 (19.5) \\
 & \multicolumn{1}{l|}{MASAC} & 69.3 (15.3) & 70.1 (24.7) & \multicolumn{1}{c|}{69.7 (21.1)} & 84.2 (18.0) & 60.7 (15.5) & 71.3 (20.4) \\
 & \multicolumn{1}{l|}{QMIX} & 67.9 (14.8) & 66.2 (24.1) & \multicolumn{1}{c|}{67.0 (20.5)} & 77.5 (17.8) & 60.9 (15.2) & 68.5 (18.5) \\
 & \multicolumn{1}{l|}{VDN} & 69.1 (16.3) & 67.1 (27.4) & \multicolumn{1}{c|}{68.0 (23.0)} & 82.2 (17.1) & 60.2 (14.6) & 70.2 (19.4) \\
 & \multicolumn{1}{l|}{IQL} & 68.7 (16.4) & 67.9 (25.0) & \multicolumn{1}{c|}{68.3 (21.8)} & 80.0 (16.5) & 61.1 (15.2) & 69.7 (18.5) \\ 
 \midrule
\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}MARL\\ (Adaptation)\end{tabular}}
 & \multicolumn{1}{l|}{IPPO} & \textbf{65.3 (12.6)} & 74.1 (30.4) & \multicolumn{1}{c|}{70.1 (24.4)} & \textbf{69.7 (13.4)} & 65.4 (18.9) & 67.4 (16.8) \\
 & \multicolumn{1}{l|}{MAPPO} & 65.6 (12.2) & 78.0 (29.9) & \multicolumn{1}{c|}{72.3 (24.4)} & 69.8 (13.4) & 65.5 (18.9) & \textbf{67.4 (16.7)} \\
 & \multicolumn{1}{l|}{ISAC} & 70.2 (16.8) & 66.4 (23.9) & \multicolumn{1}{c|}{68.1 (21.1)} & 79.4 (17.9) & 63.5 (17.8) & 70.7 (19.7) \\
 & \multicolumn{1}{l|}{MASAC} & 71.6 (17.4) & 68.8 (26.8) & \multicolumn{1}{c|}{70.0 (23.1)} & 84.4 (17.9) & 62.9 (17.8) & 72.7 (20.9) \\
 & \multicolumn{1}{l|}{QMIX} & 68.5 (16.2) & \textbf{63.2 (23.2)} & \multicolumn{1}{c|}{\textbf{65.6 (20.5)}} & 80.6 (18.8) & 63.2 (18.1) & 71.1 (20.4) \\
 & \multicolumn{1}{l|}{VDN} & 72.2 (17.6) & 72.1 (26.0) & \multicolumn{1}{c|}{72.2 (22.6)} & 83.9 (17.3) & \textbf{62.7 (17.8)} & 72.4 (20.7) \\
 & \multicolumn{1}{l|}{IQL} & 68.6 (16.8) & 63.6 (24.2) & \multicolumn{1}{c|}{65.9 (21.3)} & 82.4 (17.0) & 63.4 (17.9) & 72.0 (20.2) \\ 
 \midrule
\multirow{2}{*}{Centralized}
 & \multicolumn{1}{l|}{IPPO} & \textbf{64.7 (10.6)} & \textbf{83.9 (30.3)} & \multicolumn{1}{c|}{\textbf{75.2 (25.3)}} & \textbf{41.9 (8.0)} & \textbf{37.5 (9.8)} & \textbf{39.5 (9.3)} \\
 & \multicolumn{1}{l|}{MAPPO} & \textbf{64.7 (10.6)} & \textbf{83.9 (30.3)} & \multicolumn{1}{c|}{\textbf{75.2 (25.3)}} & 69.9 (13.3) & 62.5 (16.4) & 65.9 (15.5) \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


The training can be interpreted twofold: within a virtual environment or in a real system. The former is neutral to the real system and its users since AVs can train their policies virtually and deploy them only after training is complete and the policies have converged to optimal solutions. Humans will not be affected and iterations remain only virtual. This, however, requires \emph{virtual environment} suitable to train a policy, which, is not available to date, as we argued in \cref{sec:traffic_models}. 
If so, the \textbf{learning period needs to be treated physically} and algorithmic iterations are not abstract episodes anymore, but physical days of real disturbances. Eventually, disturbances diminish (as for IPPO and MAPPO), yet the negative impact on the system and its users accumulates (to values presented in \cref{sec:cumulative_time_differences}). Alternatively, RL can be inaccurately trained on imperfect models and collected data and later fine-tuned in reality, as demonstrated in \citet{nair2021awacacceleratingonlinereinforcement}. However, \textit{sim2real} transfer presents significant challenges, as discussed in \citet{zhao2020sim}. Real-world complexity exceeds any simulation’s capabilities.



\subsection{Critical fleet size analysis.}\label{para:fleet_size}
As demonstrated, the problem behind our position is in multiple simultaneously learning agents. To find how many agents can learn at the same time without negatively affecting the system, we did simulations with a gradually increasing number of agents and reported when convergence issues arose. In each simulation, the specific AVs were chosen at random, with the condition that no two AVs are consecutive (there is always a human agent between AVs), and the first two vehicles never mutate. The algorithms start failing to converge even with 3-5 agents, as shown in \cref{fig:different-fleet-number} and \cref{sec:smaller_av_fleets}. 
\textbf{This reveals a systemic issue that becomes increasingly prevalent as the number of agents grows, which will pose a serious threat to traffic performance when more AVs participate in our daily commute.}













\subsection{Human adaptation.}

As the system gets disequilibrated, humans will naturally seek ways to improve their payoffs (to arrive faster) \cite{watling2013modelling}. This behavior is similar to the process of finding the equilibrium \cite{bie2010stability}, but even less predictable \cite{he2012modeling}. This brings another source of non-stationarity to the system. Now, two paradigms are applied to a single system: humans, rational decision-makers naturally adapt and AVs' algorithms explore and exploit the environment to identify optimal policies. We incorporate a probabilistic adaptation formula (to the human decisions) and observe that the previously optimal system now shifts to the suboptimal state \cref{fig:adapt-centr} top). 
\textbf{Decisions of adapting humans may shift towards suboptimal equilibria during the AV training period - leading to worse performance for all users.}



\subsection{Privacy of personal data in centralized systems.}

The final aspect of our position lies in the communication, collaboration, and/or centralization \cite{schwarting2019social} - which makes AVs the \textbf{C}AVs. This allows us to better exploit the potential of autonomous driving, presumably at the cost of sharing private information with others or with central agents \cite{nayak2021autonomous}. Can we trade our private destination and origin to resolve the non-stationarity issues? To some extent, centralization may speed up the convergence, yet nowhere close to solving the problem and leading to issues with combinatorically growing search space (see \cref{fig:adapt-centr} bottom). 





\section{Conclusion}


AVs have begun appearing in traffic networks worldwide, and their presence is expected to increase in the upcoming years. As AVs become more prevalent, MARL can be applied to optimize their route choices, and this optimization can improve urban efficiency and reduce congestion and pollution in our cities. 
In practice, our MARL algorithms need hundreds of episodes to land in optimal policies, even in trivial cases. To realistically simulate human route-choice behavior we miss theories, data to verify them, and simulation tools. Moreover, 
realistic urban mobility simulators are lacking, and the training episodes would probably need to be deployed directly on real traffic systems, disrupting the traffic networks. To tackle this problem, we are developing a benchmark tailored to test different road networks, and demand patterns to address the MARL challenges identified in this paper. We hope this will spark discussions within the MARL community encouraging the further development of benchmarks and the exploration of MARL algorithms that can effectively enhance urban mobility. \textbf{Such an experimental research program is needed to ensure we fully exploit the opportunities of the new technology (AVs) and algorithms (MARL) to help us improve the traffic in future cities.}



\section{Alternative Views}
\label{sec:alternate_views}

This section covers some alternative perspectives that challenge our position.

\paragraph{What if the limitations of the virtual environment are specific to this study and could be resolved with better design or tools?}

As discussed in \cref{sec:traffic_models}, accurately representing dynamic, multi-agent systems like traffic networks requires highly complex simulation environments. Modeling realistic traffic demand patterns involves acquiring and handling data about real-world driver behavior, which is often private and sensitive.

\paragraph{Can we adjust MARL approaches to converge faster?}

Although improvements in algorithm design or parameter tuning may improve convergence speed, our analysis (drawn from a broad spectrum of training configurations) suggests that we shall not expect to improve our results by a significant margin. Our trivial problem with a small action space and fleet size already required the equivalent of years of commute time to converge. Training algorithms for real-world tasks will converge orders of magnitude longer. 
Nevertheless, the following covers a selection of approaches proposed before our study which may improve the convergence rate and stabilize the learning process. 


\citet{teaching_on_budget} shows that off-policy learning using action advisement between agents can improve training efficiency. However, this approach has only been evaluated with 3 agents, which is insufficient for our study. \citet{DPIQN} shows that learning about other agents' policies through observations can address non-stationarity. Yet, their method relies on the amount of information embedded in the agent's raw observations, which is often limited. Alternatively, \citet{stable_MARL} shows that if an agent learns how the strategies of other agents evolve with its behavior, defining a stability reward can stabilize learning. As promising as this notion is, it considers only pairwise stability, rendering it inapplicable for bigger systems.

\paragraph{What if reinforcement learning is not suitable for vehicle routing?}

RL, as we discussed in \cref{sec:related_work}, is already regarded as a viable solution for routing optimization, offering the ability to learn complex coordination and solve dynamic problems. Existing literature often frames this problem as a Traffic Assignment Problem (TAP), which seeks to satisfy a travel demand (number of travelers) navigating between origin-destination pairs within limited route capacities while minimizing costs, such as reducing system travel times. An approach to solve this problem includes methods based on the Frank-Wolfe (FW) algorithm \cite{frankwolfe}, which is a general-purpose non-linear optimization method that is considered state-of-the-practice \cite{Bar-Gera2002} but suffers from a slow convergence rate, as discussed in detail in \citet{patriksson}. 


\paragraph{Can AVs lead to more efficient urban traffic systems?}

This study examines a future where AVs operate within today’s traffic network alongside human drivers. One could argue, in contrast to our position, that future cities will benefit from  AV deployment. Specifically, \citet{richter2022smart} show that AV deployment can offer advantages (including reduced congestion and shorter commutes), but in varying scales depending on the type of city. \citet{futuretransp4020017} state that AV introduction can contribute to sustainability by improving air quality and reducing $NO_2$ concentrations. Lastly, \citet{VAHIDI2018822} mention the energy-efficient driving opportunities that CAVs can offer. However, \citet{fagnant2015preparing} highlights the need for strict regulations in the deployment of AVs. %

\section*{Impact Statement}
\textbf{Our position aims to shape the research focus on creating and developing algorithms that can optimize the collective routing of AVs.} It demonstrates the potentially negative consequences for our future urban systems and their sustainability that are likely to occur if actions are not taken. We list gaps in the state-of-the-art that could render the large-scale deployment of AVs using MARL for urban routing decisions harmful to the overall system performance. To address these challenges, we propose research directions to leverage the capabilities of CAVs.  




\clearpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Screenshot sumo
\newpage
\appendix
\onecolumn

\section{Examples of real-world urban topologies corresponding to our two-route network}
\label{sec:two-route-nets}
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_cities/croatia.png}
        \caption{Split, Croatia.}
        \label{fig:split}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=3.7cm]{images_cities/poland.png}
        \caption{Bochnia, Poland.}
        \label{fig:bochnia}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=8.38cm, height=6.29cm]{images_cities/portugal.png}
        \caption{Lisbon, Portugal.}
        \label{fig:lisbon}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=5.92cm, height=6.29cm]{images_cities/poland_2.png}
        \caption{Tarnow, Poland.}
        \label{fig:lisbon}
    \end{subfigure}
    
    \caption{Occurrence of two-route networks similar to the one presented in this study across cities worldwide (source: \citet{OpenStreetMap}).}
    \label{fig:two-route-net-screenshots}
    
\end{figure}



\section{Experiments}
\label{sec:experiments_details}
The training for the experiments shown in \cref{fig:marl-proportions}, \cref{fig:different-fleet-number}, \cref{fig:adapt-centr},  and \cref{fig:multiple-agent-adaptation} involves 200 policy updates and each update includes 40 frames collected per agent (episodes or days). The epochs are 10, the minibatch size is 2, and the activation function is the Tanh. In \cref{fig:marl-proportions} each algorithm experiment was conducted for ten independent runs, five runs in \cref{fig:different-fleet-number} and \cref{fig:adapt-centr} and three runs in  \cref{fig:multiple-agent-adaptation}. The error bars indicate the standard deviation across these experiments. For clarity, data points are recorded every 10 episodes and smoothed using the moving average method with a window size of value 10. Hyperparameter optimization was performed for each algorithm separately.



\begin{table}[H]
\caption{Hyperparameters used in the experiments.}
\label{tab:hyperparameters}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccccc}
\toprule
Hyperparameter & IQL & VDN & QMIX & MASAC & ISAC & MAPPO & IPPO & IPPO/MAPPO - Centr\\
\midrule
Learning rate       & 1e-4 & 1e-2 & 1e-3 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-4\\
Memory size         & 1600 & 1600 & 1600 & 1600 & 1600 & -    & -   & - \\
Max gradient norm   & 1e-3 & 2    & 1.5  & 0.5  & 0.5  & 0.5  & 0.5 & 1.0 \\
Tau                 & 1    & 1e-2 & 1e-2 & 5e-3 & 5e-3 & -    & -   & - \\
Gamma               & 0.9  & 0.85 & 0.95 & 0.98 & 0.99 & 0.99 & 0.99 & 0.85 \\
Lambda              & -    & -    & -    & -    & -    & 1    & 1 & 0.9\\
Clip epsilon        & -    & -    & -    & -    & -    & 0.2  & 0.2 & 0.2  \\
Entropy coefficient & -    & -    & -    & -    & -    & 1e-4 & 1e-4 & 1e-3\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\section{SUMO}
In this study, we consider that SUMO operates under deterministic conditions. Specifically, if all vehicles select the same routes across consecutive iterations, their travel times will remain identical to those observed in the previous iterations. The videos from where the screenshots were taken are included in the supplementary material.


\begin{figure}[ht]
    \centering
    % Row 1
    \begin{subfigure}[b]{\textwidth}
        \centering
        %\includegraphics[height=2.2cm]{images/sumo_screenshot_eq0_2.png}
        \includegraphics[width=\linewidth]{images/screenshot_sy_ue.png}
        \caption{System optimal user equilibrium.}
        \label{fig:figure2}
    \end{subfigure}
    \hfill
    \hfill
    \begin{subfigure}[b]{\textwidth}
        \centering
        %\includegraphics[height=1.95cm]{images/sumo_screenshot_eq1_2.png}
        \includegraphics[width=\linewidth]{images/screenshot_us.png}
        \caption{Suboptimal user equilibrium.}
        \label{fig:figure1}
    \end{subfigure}
    \caption{SUMO screenshots illustrate the network under the two distinct equilibria. Red vehicles represent human drivers and yellow ones represent AVs.}
    \label{fig:sumo_screenshots}
\end{figure}




\section{Cumulative time difference}
\label{sec:cumulative_time_differences}

\begin{table}[H]
\caption{\textbf{Cumulative travel time differences} (in hours) with standard deviations between experiment replications for each user equilibrium and algorithm. The lowest cumulative travel time differences are highlighted in \textbf{bold}.}
\label{tab:cumtimedif}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}llcc@{}}
\toprule
\multicolumn{1}{l|}{Algorithm} & \multicolumn{1}{c}{System Optimum \& User Equilibrium} & \multicolumn{1}{c}{Suboptimal \& User Equilibrium} \\ 
\midrule
\multicolumn{1}{l|}{IPPO} & \multicolumn{1}{c|}{12.87 (7.17)} & -0.21 (0.09) \\
\multicolumn{1}{l|}{MAPPO} & \multicolumn{1}{c|}{\textbf{11.80 (3.72)}}  & -0.19 (0.16) \\
\multicolumn{1}{l|}{ISAC} & \multicolumn{1}{c|}{33.33 (0.19)} & -4.77 (0.04) \\
\multicolumn{1}{l|}{MASAC} & \multicolumn{1}{c|}{33.27 (0.39)} & \textbf{-4.78 (0.04)} \\
\multicolumn{1}{l|}{QMIX} & \multicolumn{1}{c|}{36.20 (12.66)}  & -3.95 (1.47) \\
\multicolumn{1}{l|}{VDN} & \multicolumn{1}{c|}{32.88 (0.92)} & -4.72 (0.07) \\
\multicolumn{1}{l|}{IQL} & \multicolumn{1}{c|}{33.67 (1.53)} & -3.94 (0.30) \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
Positive cumulative travel time differences, i.e. \(c_t > 0\), mean that on average, human agents after mutation experience longer travel times, thus the impact of AVs introduction on them is negative. This occurred in the system optimal user equilibrium scenario for each algorithm tested. In the suboptimal user equilibrium, \(c_t < 0\) for each algorithm. This means that human agents decrease their travel times compared to the ones they were experiencing before the AVs introduction. This can be attributed to the fact that humans follow the route that has priority (route 1) and these values are negligible.


Let \(\mathcal{H}\) be the set of all agents that do not mutate to AVs. In addition, let \(Ex\) denote the number of experiment replications and \(Eps\) the number of episodes from the moment of mutation till the end of each experiment. For each \(i\in Ex\), \(e\in Eps\) and agent \(h\in\mathcal{H}\), we annotate travel time by \(t^i_{e,h}\). Let \(\bar{t}^i_e\) describe the average travel time of agents from \(\mathcal{H}\) for each episode \(e \in Eps\) and experiment \(i\in Ex\):
\[
    \bar{t}^i_e = \frac{1}{|\mathcal{H}||Ex|}\sum_{h\in\mathcal{H}}t^i_{e,h}.
\]
For each MARL algorithm and user equilibrium, the cumulative travel time difference \(c_t\) is given by the following formula:
\[
    c_t =\sum_{i\in Ex} \sum_{e\in Eps} \left( \bar{t}_e^i - \bar{t}_{\text{lh}}^i\right),
\]
where \(\bar{t}_{\text{lh}}^{i}\) stands for the average travel time of agents \(\mathcal{H}\) in the last episode before the mutation in the \(i\)-th experiment, i.e. 
\[
    \bar{t}^i_{\text{lh}} = \frac{1}{|\mathcal{H}||Ex|}\sum_{h\in\mathcal{H}}t^i_{\text{lh},h},
\]
where lh stands for the last episode before introducing AVs. For each algorithm and each equilibrium, there are 10 experiment replications, set \(\mathcal{H}\) contains 12 human agents, mutation started in the 201-st episode and experiments took place until the 8300 episode. The values in the \cref{tab:cumtimedif} are cumulative travel time differences and the standard deviations over experiments.


\section{Travel times}
\begin{figure}[H]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{images/travel_times_appendix.png}}
\caption{\textbf{Mean travel times} of AVs, human agents, and both in the simulation. The two plots on the left show that IPPO and MAPPO algorithms achieve shorter travel times for both AVs and human agents, as they converge to more optimal solutions, according to \cref{fig:marl-proportions}.  In the two plots on the right, that represent the scenario where humans adapt their options (\cref{par:human_agents}), AVs achieve smaller travel times using the same algorithms. Human drivers gain no benefit. The error bars represent the results of three distinct experiments conducted for each algorithm in each equilibrium.}
\label{fig:travel_times}
\end{center}
\vskip -0.2in
\end{figure}

\section{Adaptation}
\begin{figure}[H]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/adaptation_appendix.png}}
\caption{\textbf{8000} episodes (days) of training for selected MARL algorithms while humans adapt their choices. The trivial solution is found only by IPPO and MAPPO in the suboptimal user equilibrium. Other algorithms remain probabilistic after and fail to converge to optimal policy. Suboptimal equilibrium is reproduced after 1000 days.}

\section{Results for smaller AV fleets}
\label{sec:smaller_av_fleets}

 \begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth, keepaspectratio,  clip]{images_supplementary_material/all_algos_2_agents.png}
        \caption{Performance of 2 AV fleet}
        \label{fig:2_AVs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth, keepaspectratio,  clip]{images_supplementary_material/all_algos_3_agents.png}
        \caption{Performance of 3 AV fleet}
        \label{fig:3_AVs}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed

    % Row 2
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth, keepaspectratio,  clip]{images_supplementary_material/all_algos_4_agents.png}
        \caption{Performance of 4 AV fleet}
        \label{fig:4_AVs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, height=0.9\textwidth, keepaspectratio, clip]{images_supplementary_material/all_algos_5_agents.png}
        \caption{Performance of 5 AV fleet}
        \label{fig:5_AVs}
    \end{subfigure}

    \caption{Results (last 100 episodes - testing phase) of the randomly sampled simulations for smaller fleet sizes. Several algorithms failed to find the optimal solutions and performed worse as the number of AVs in the system increased.}
    \label{fig:appendix}
\end{figure}
\label{fig:multiple-agent-adaptation}
\end{center}
\vskip -0.1in
\end{figure}




\section{Mean rewards}


% Mean travel time zero
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_iql_zero.png}
        \caption{Independent Q-learning (IQL)}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_vdn_zero.png}
        \caption{Value-decomposition networks (VDN)}
        \label{fig:figure2}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed

    % Row 2
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_qmix_zero.png}
        \caption{Monotonic value function factorisation (QMIX)}
        \label{fig:figure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_mappo_zero.png}
        \caption{Multi-agent proximal policy optimization (MAPPO)}
        \label{fig:figure4}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 3
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_ippo_zero.png}
        \caption{Independent proximal policy optimization (IPPO)}
        \label{fig:figure5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_isac_zero.png}
        \caption{Independent soft actor-critic (ISAC)}
        \label{fig:figure6}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 4
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_masac_zero.png}
        \caption{Multi-agent soft actor-critic (MASAC)}
        \label{fig:figure7}
    \end{subfigure}

    
    \caption{\textbf{Mean rewards} of AV agents, human agents, and both combined under the system optimum user equilibrium scenario. The initial 200 episodes of the simulation represent the human learning phase. The mutation event at episode 200 initiates the training of AV agents using different MARL algorithms and the end of the human learning phase. The testing phase corresponds to the last 100 episodes. Notably, IPPO and MAPPO algorithms converge after 6000 episodes to the lowest average reward, while other algorithms either stabilize at higher reward values or exhibit oscillatory behavior during training. The episodes in these plots do not correspond to policy updates (more details in Appendix Section B).}
    \label{fig:mean_rewards_zero}
\end{figure}



% Mean travel times ones
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_iql_one.png}
        \caption{IQL}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_vdn_one.png}
        \caption{VDN}
        \label{fig:figure2}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed

    % Row 2
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_qmix_one.png}
        \caption{QMIX}
        \label{fig:figure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_mappo_one.png}
        \caption{MAPPO}
        \label{fig:figure4}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 3
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_ippo_one.png}
        \caption{IPPO}
        \label{fig:figure5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_isac_one.png}
        \caption{ISAC}
        \label{fig:figure6}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 4
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images_supplementary_material/rewards_masac_one.png}
        \caption{MASAC}
        \label{fig:figure7}
    \end{subfigure}

    
    \caption{\textbf{Mean rewards} of AV agents, human agents, and both combined in the suboptimal user equilibrium. IPPO and MAPPO achieve the lowest average rewards in the testing phase and during the training their reward is less oscillatory than the other algorithms. The rewards in this scenario are higher than those in \cref{fig:mean_rewards_zero}, as the system is under the suboptimal use equilibrium.}
    \label{fig:mean_rewards_one}
\end{figure}



\section{Action shifts}

% Action shifts zero
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_iql_zero.png}
        \caption{IQL}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_vdn_zero.png}
        \caption{VDN}
        \label{fig:figure2}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed

    % Row 2
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_qmix_zero.png}
        \caption{QMIX}
        \label{fig:figure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_mappo_zero.png}
        \caption{MAPPO}
        \label{fig:figure4}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 3
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_ippo_zero.png}
        \caption{IPPO}
        \label{fig:figure5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_isac_zero.png}
        \caption{ISAC}
        \label{fig:figure6}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 4
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_masac_zero.png}
        \caption{MASAC}
        \label{fig:figure7}
    \end{subfigure}

    
    \caption{\textbf{Action shifts} of AV and human agents under the system optimum user equilibrium. The plots depict the number of human agents and AVs that choose routes 1 and 0. Among the algorithms, IPPO converges the fastest to the optimal solution, where all AV agents select route 0. MAPPO also converges to this solution but requires additional episodes. The remaining algorithms settle on suboptimal solutions.}
    \label{fig:action_shifts_zero}
\end{figure}



% Action shifts ones
\begin{figure}[H]
    \centering
    % Row 1
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_iql_one.png}
        \caption{IQL}
        \label{fig:figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_vdn_one.png}
        \caption{VDN}
        \label{fig:figure2}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed

    % Row 2
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_qmix_one.png}
        \caption{QMIX}
        \label{fig:figure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_mappo_one.png}
        \caption{MAPPO}
        \label{fig:figure4}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 3
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_ippo_one.png}
        \caption{IPPO}
        \label{fig:figure5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_isac_one.png}
        \caption{ISAC}
        \label{fig:figure6}
    \end{subfigure}

    \vspace{1em} % Adjust spacing between rows if needed
    
    % Row 4
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim=0cm 0cm 15cm 0cm, clip]{images_supplementary_material/actions_shifts_masac_one.png}
        \caption{MASAC}
        \label{fig:figure7}
    \end{subfigure}

    
    \caption{\textbf{Action shifts} of AV and human agents in the suboptimal user equilibrium. The plots illustrate the number of human agents and AVs selecting routes 0 and 1. Among algorithms, IPPO and MAPPO converge even faster to the optimal solution, compared to the system optimum user equilibrium scenario, as this equilibrium is more stable. The rest of the algorithms converge to suboptimal solutions.}
    \label{fig:action_shifts_one}
\end{figure}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
