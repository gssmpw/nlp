% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsfonts}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{TransMLA: Multi-Head Latent Attention Is All You Need}
%\title{TransMLA: Equivalently Transforms Group Query Attention into Multi-head Latent Attention.}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Fanxu Meng$^{1,2}$\thanks{These authors contributed equally to this work.}, Zengwei Yao$^{3\ast}$ , Muhan Zhang$^{1,2}$\thanks{Corresponding author: <muhan@pku.edu.cn>}\\
$^{1}$Institute for Artificial Intelligence, Peking University\\
$^{2}$State Key Laboratory of General Artificial Intelligence, Peking University\\
$^{3}$Xiaomi Corp., Beijing, China\\
\centering\href{https://github.com/fxmeng/TransMLA}{https://github.com/fxmeng/TransMLA}}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Modern large language models (LLMs) often face communication bottlenecks on current hardware, rather than purely computational limitations. Multi-head Latent Attention (MLA) addresses this issue by utilizing low-rank matrices in the key-value layers, enabling the caching of compressed latent key-value (KV) states. This design significantly reduces the KV cache size compared to traditional multi-head attention, thus accelerating inference. Furthermore, MLA incorporates an up-projection matrix to enhance expressiveness, effectively trading extra computation for reduced communication overhead.
Despite its demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers continue to rely on Group Query Attention (GQA), with no public plans to adopt MLA. This paper shows that GQA can always be represented by MLA with the same KV cache overhead—the reverse does not hold. To encourage broader adoption, we introduce TransMLA, a post-training method that converts widely used GQA-based pre-trained models (e.g. LLaMA, Qwen, Mixtral) into MLA-based models. After this conversion, further training is performed to boost the model’s expressiveness without increasing the KV cache size. Additionally, we plan to develop MLA-specific inference acceleration strategies to maintain low latency in transformed models, thereby facilitating more effective distillation of Deepseek R1.
\end{abstract}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{imgs/transmla_1.pdf}
        \caption{Group Query Attention (GQA)}
        \label{subfig:transmla_gqa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{imgs/transmla_2.pdf}
        \caption{Multi-Head Attention (MHA)}
        \label{subfig:transmla_inter}
    \end{subfigure}
        \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{imgs/transmla_3.pdf}
        \caption{Multi-head Latent Attention (MLA)}
        \label{subfig:transmla_mla}
    \end{subfigure}
    \caption{The process of equivalently converting GQA to MLA. (a) In GQA, $K$ is repeated to match the number of $Q$ heads before computing attention. (b) Instead, $W_K$ is repeated; after multiplying   with $X$, it directly produces $K'$, which is equivalent to GQA. (c) Decomposing $W_K'$ yields an $r$-dimensional representation that matches the KV cache dimension in GQA while maintaining equivalence.}
    \label{fig:clover}
\end{figure}

\section{Introduction}
In recent years, Large Language Models (LLMs) have become indispensable tools for productivity \cite{gpt4o, claude35sonnet, team2024gemini}, with open-source models \cite{llama3, mixtral8x22b, qwen2_5, dsviii, team2024gemma, abdin2024phi} narrowing the performance gap to closed-source alternatives. The effectiveness of LLMs is primarily due to Next Token Prediction \cite{radford2018improving, brown2020language}, where tokens are predicted sequentially, with attention computed between each token and its preceding ones. To avoid redundant calculations, key-value (KV) pairs are cached. However, as model sizes grow, caching overhead increases, leading to memory and communication bottlenecks. For example, LLaMA-65B \cite{llama}, with 8-bit key-value quantization, requires over 86GB of GPU memory to store 512K tokens, surpassing the capacity of a single H100-80GB GPU \cite{sun2024you}.  

Several attention modification methods have been proposed to address this issue. Multi-Query Attention (MQA) \cite{shazeer2019fast} reduces the number of attention heads by using a single head for both the Key and Value layers. Another approach, Group-Query Attention (GQA) \cite{ainslie2023gqa}, groups Query heads, with each group sharing a single Key and Value head. While both methods reduce KV Cache requirements, they sacrifice performance. Additionally, techniques like post-training pruning \cite{yu2024effectively} and KV compression \cite{liu2024kivi, hooper2024kvquant} can reduce KV size but may harm model accuracy, often necessitating fine-tuning to recover performance. Multi-head Latent Attention (MLA), introduced in Deepseek V2 \cite{dsvii} and extended in Deepseek V3 \cite{dsvii} and Deepseek R1 \cite{guo2025deepseek}, strikes a balance between speed and effectiveness. However, these studies did not offer theoretical proof or perform ablation experiments comparing MLA’s performance with that of GQA.

This paper presents a theoretical proof showing that \textbf{for the same KV Cache overhead, MLA consistently offers greater expressive power than GQA}. We then convert several popular GQA-based models—such as LLaMA-3 \cite{llama3}, Qwen-2.5 \cite{qwen2_5}, Mistral \cite{jiang2023mistral}, Mixtral \cite{mixtral8x22b}, Gemma-2 \cite{team2024gemma}, and Phi-4 \cite{abdin2024phi}—into equivalent MLA models with the same KV Cache overhead. Through fine-tuning experiments, we demonstrate that the enhanced expressive power of MLA leads to superior performance on downstream tasks. This work aims to shift the focus of mainstream LLM attention design from GQA to MLA, offering a low-cost migration strategy that enables model developers to maximize the potential of existing models while reducing both resource consumption and carbon emissions.


\section{Related Work}
In large language models (LLMs), the key-value (KV) cache becomes a significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each token's KV pair must be recomputed for every new token, the memory demands of the cache grow quickly, limiting the ability to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs.

One approach is linear attention, as seen in methods like Linear Transformer \cite{katharopoulos2020transformers,wang2020linformer}, RWKV \cite{peng2023rwkv}, and Mamba \cite{gu2023mamba}, which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory demands, it can reduce model expressivity, leading to performance degradation in tasks requiring complex, long-range token dependencies.

Another technique is dynamic token pruning, employed by LazyLLM \cite{fu2024lazyllm}, A2SF \cite{jo2024a2sf}, and SnapKV \cite{li2024snapkv}. These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring a detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining.

Pruning head dimensions, seen in approaches like SliceGPT \cite{ashkboos2024slicegpt}, Sheared \cite{xia2023sheared}, and Simple Pruning \cite{sun2023simple}, reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the model’s ability to capture important token relationships. 

Sharing KV representations across layers, as in YONO \cite{sun2024you}, MiniCache \cite{liu2024minicache}, and MLKV \cite{zuhri2024mlkv}, reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.

Finally, KV quantization techniques like GPTQ \cite{frantar2022gptq}, Kivi \cite{liu2024kivi}, and KVQuant \cite{hooper2024kvquant} reduce the precision of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computational overhead, enabling longer contexts and faster inference with minimal performance loss. 

\newpage
\section{Preliminary}
We will first introduce Multi-Head Attention (MHA) \cite{vaswani2017attention}, Group Query Attention (GQA) \cite{ainslie2023gqa}, and the special case of GQA where the group is equal to 1, namely Multi-Query Attention (MQA)  \cite{shazeer2019fast}, as well as Multi-Head Latent Attention (MLA) \cite{dsvii}.


\subsection{Multi-Head Attention (MHA)}
Let the input sequence be $\mathbf{X} \in \mathbb{R}^{T \times D}$, where $T$ represents the sequence length and $D$ is the hidden dimension. Let $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{D \times (n_h \cdot d_h)}$, and $\mathbf{W}_O \in \mathbb{R}^{(n_h \cdot d_h) \times D}$. First, MHA computes 

\begin{align}
    \mathbf{Q} & = \mathbf{X} \mathbf{W}_Q \in \mathbb{R}^{T \times (n_h \cdot d_h)}, \nonumber\\
    \mathbf{K} & = \mathbf{X} \mathbf{W}_K \in \mathbb{R}^{T \times (n_h \cdot d_h)}, \nonumber\\
    \mathbf{V} & = \mathbf{X} \mathbf{W}_V \in \mathbb{R}^{T \times (n_h \cdot d_h)}. \nonumber
\end{align}

These are then split into $n_h$ attention heads: 
\begin{align}
    [\mathbf{Q}_1; \mathbf{Q}_2; \dots; \mathbf{Q}_{n_h}] &= \mathbf{Q}, \nonumber\\
    [\mathbf{K}_1; \mathbf{K}_2; \dots; \mathbf{K}_{n_h}] &= \mathbf{K}, \nonumber\\
    [\mathbf{V}_1; \mathbf{V}_2; \dots; \mathbf{V}_{n_h}] &= \mathbf{V}. \nonumber
\end{align}

where $\mathbf{Q}_i, \mathbf{K}_i, \mathbf{V}_i \in \mathbb{R}^{T \times d_h}$. The matrix $\mathbf{W}_O$ is applied to the outputs of each attention head: 
\[
[{\mathbf{W}_O}_1; {\mathbf{W}_O}_2; \dots; {\mathbf{W}_O}_{n_h}] = {\mathbf{W}_O},
\] 
where ${\mathbf{W}_O}_i \in \mathbb{R}^{d_h \times D}$. Finally, the output of each attention head is computed as: 
\[
\mathbf{O}_i = \text{softmax}\left(\frac{\mathbf{Q}_i K_i^T}{\sqrt{d_h}}\right) \mathbf{V}_i {\mathbf{W}_O}_i \in \mathbb{R}^{T \times D},
\] 
and the final output is the sum of all head outputs:
\(
\mathbf{O} = \sum_{i=1}^{n_h} \mathbf{O}_i.
\)
In MHA, each $\mathbf{Q}_i$ corresponds to a $\mathbf{K}_i$, and each $\mathbf{O}_i$ corresponds to a $\mathbf{V}_i$.

\subsection{Group Query Attention (GQA)}
To reduce the overhead of the KV cache, GQA divides the Query heads into $g_h$ groups, with each group sharing a single Key and Value for $\frac{n_h}{g_h}$ heads. Specifically, let $\mathbf{W}_Q \in \mathbb{R}^{D \times (n_h \cdot d_h)}$, $\mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{D \times (g_h \cdot d_h)}$, and $\mathbf{W}_O \in \mathbb{R}^{(n_h \cdot d_h) \times D}$. First, compute 
\begin{align}
    \mathbf{Q} &= \mathbf{X} \mathbf{W}_Q \in \mathbb{R}^{T \times (n_h \cdot d_h)},\nonumber\\
    \mathbf{K} &= \mathbf{X} \mathbf{W}_K \in \mathbb{R}^{T \times (g_h \cdot d_h)},\nonumber\\
    \mathbf{V} &= \mathbf{X} \mathbf{W}_V \in \mathbb{R}^{T \times (g_h \cdot d_h)}.\nonumber
\end{align}

Then, split $\mathbf{K}$ and $\mathbf{V}$ into $g_h$ heads, and split $\mathbf{Q}$ into $g_h$ groups, with each group containing $\frac{n_h}{g_h}$ attention heads: 
\begin{align}
    [Q_1; \dots; Q_{\frac{n_h}{g_h}}; \dots; Q_{n_h-\frac{n_h}{g_h}}; \dots; Q_{n_h}] &= Q, \nonumber\\
    [K_1; K_2; \dots; K_{g_h}] &= K, \nonumber\\
    [V_1; V_2; \dots; V_{g_h}] &= V. \nonumber
\end{align}

where $Q_i, K_i, V_i \in \mathbb{R}^{T \times d_h}$. The matrix $\mathbf{W}_O$ is then applied to each Query head: 
\[
[{W_O}_1; {W_O}_2; \dots; {W_O}_{n_h}] = {W_O}, 
\]
where ${W_O}_i \in \mathbb{R}^{d_h \times D}$.
For the $j$-th group and the $k$-th head in that group, the output is computed as: 
\[
O_i = \text{softmax}\left(\frac{Q_i K_j^T}{\sqrt{d_h}}\right) V_j {W_O}_i \in \mathbb{R}^{T \times D},
\]
where $i=j\frac{n_h}{g_h}+k$ and the final output is the sum of all head outputs: $O = \sum_{i=1}^{n_h} O_i$.

In GQA, each $K_j$ corresponds to $\frac{n_h}{g_h}$ $Q_k$ heads, and each $V_j$ corresponds to $\frac{n_h}{g_h}$ $O_k$ heads. When $g_h = n_h$, GQA becomes MHA, and when $g_h = 1$, GQA becomes MQA.

\subsection{Multi-Head Latent Attention (MLA)}
To make it easier to understand and align with the forms of MHA and GQA, we omit the RoPE and allow each head to compute with its own latent vectors, and do not apply compression to the query.

Let $\mathbf{W}_Q \in \mathbb{R}^{D \times (n_h \cdot d_h)}$, $\mathbf{W}_K^a, \mathbf{W}_V^a \in \mathbb{R}^{D \times r}$, $\mathbf{W}_K^b, \mathbf{W}_V^b \in \mathbb{R}^{r \times (n_h \cdot d_h)}$, and $\mathbf{W}_O \in \mathbb{R}^{(n_h \cdot d_h) \times D}$, where $r$ is the KV compression dimension. MLA computes 
\begin{align}
    \mathbf{Q} &= \mathbf{X} \mathbf{W}_Q \in \mathbb{R}^{T \times (n_h \cdot d_h)}, \nonumber\\
    \mathbf{K} &= \mathbf{X} \mathbf{W}_K^a \mathbf{W}_K^b \in \mathbb{R}^{T \times (n_h \cdot d_h)}, \nonumber\\
    \mathbf{V} &= \mathbf{X} \mathbf{W}_V^a \mathbf{W}_V^b \in \mathbb{R}^{T \times (n_h \cdot d_h)}. \nonumber
\end{align}
then splits $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ into $n_h$ attention heads and computes attention:
\[
O = \sum_{i=1}^{n_h} \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_h}}\right) V_i {W_O}_i \in \mathbb{R}^{T \times D}.
\]
For saving the KV cache, only the intermediate latent representations need to be stored: $\mathbf{K}^c = \mathbf{X} \mathbf{W}_K^a \in \mathbb{R}^{T \times r}$, $\mathbf{V}^c = \mathbf{X} \mathbf{W}_V^a \in \mathbb{R}^{T \times r}$, where $r$ is much smaller than $n_h \cdot d_h$. During inference, MLA uses an absorb operation to merge $\mathbf{W}_K^b$ into $\mathbf{W}_Q$ and $\mathbf{W}_V^b$ into $\mathbf{W}_O$, thus avoiding increasing the KV latent dimension. At this point, MLA can be viewed as an MQA with each head having a dimension of $r$.



\section{TransMLA}

This section begins by presenting the following theorem:

\begin{theorem}\label{thm:mla_greater_than_gqa} The expressiveness of MLA is greater than that of GQA when both have the same size of KV cache. \end{theorem}

\textbf{Proof:} Sections \ref{sec:gqa_replication}, \ref{sec:replicate_parameters}, and \ref{sec:low_rank_factorization} show that any GQA configuration can be equivalently transformed into MLA with the same size of KV cache. In Section \ref{sec:exist_mla_not_representable_by_gqa}, we demonstrate that there are cases where MLA cannot be represented by GQA.

\subsection{Replicating the Keys in GQA}
\label{sec:gqa_replication}

Figure~\ref{subfig:transmla_gqa} illustrates the typical structure of Group Query Attention (GQA). 
In GQA, the query $\mathbf{Q}$ is split into $n_q = \tfrac{D}{d_h}$ heads, each of dimension $d_h$. 
In order to reduce the number of heads for the keys and values, the key $\mathbf{K}$ is defined to have 
$n_k$ heads (with $n_k < n_q$), each of dimension $d_h$. 

Let $\mathbf{X} \in \mathbb{R}^{T \times D}$ be the input sequence of length $T$ and hidden dimension $D$, 
and let $\mathbf{W}_K \in \mathbb{R}^{D \times (n_k\,d_h)}$ be the projection matrix for the keys. Then,
\[
  \mathbf{K} \;=\; \mathbf{X} \,\mathbf{W}_K 
  \;\in\; \mathbb{R}^{T \times (n_k\,d_h)}.
\]

Because standard multi-head attention requires $\mathbf{Q}$ and $\mathbf{K}$ (as well as $\mathbf{V}$) 
to have the same number of heads, we must expand $\mathbf{K}$ from $n_k$ heads to $n_q$ heads. 
Define the replication factor $s = \tfrac{n_q}{n_k}$. 
Partition $\mathbf{K}$ along its columns into $n_k$ blocks, each corresponding to one head:
\[
  \mathbf{K}^{(i)} \;=\; \mathbf{K}_{[:,\, i\,d_h : (i+1)\,d_h]},
  \quad i = 0, 1, \dots, n_k - 1,
\]
where each block $\mathbf{K}^{(i)} \in \mathbb{R}^{T \times d_h}$. 
Replicating each $\mathbf{K}^{(i)}$ by $s$ times and concatenating yields the expanded matrix 
$\mathbf{K}' \in \mathbb{R}^{T \times D}$:
\small{\[
  \mathbf{K}' 
  = \Bigl[
  \underbrace{\mathbf{K}^{(0)}, \dots, \mathbf{K}^{(0)}}_{s\ \text{times}},
  \;\dots,
  \underbrace{\mathbf{K}^{(n_k-1)}, \dots, \mathbf{K}^{(n_k-1)}}_{s\ \text{times}}
  \Bigr].
\]}
\normalsize
\subsection{Moving Replication to the Parameter Side}
\label{sec:replicate_parameters}

Figure~\ref{subfig:transmla_inter} shows an alternative approach to replicating keys. 
Instead of computing $\mathbf{K}$ and then replicating each head, we can replicate the projection matrix 
$\mathbf{W}_K$ \emph{before} computing $\mathbf{K}$. 

First, split $\mathbf{W}_K$ along its columns into $n_k$ parts, where each 
$\mathbf{W}_K^{(i)} \in \mathbb{R}^{D\times d_h}$ corresponds to one original key head:
\[
  \mathbf{W}_K^{(i)} 
  = 
  {\mathbf{W}_K}_{[:,:,\,i\,d_h : (i+1)\,d_h]},
  \quad i = 0, 1, \dots, n_k - 1.
\]
Then, replicate each $\mathbf{W}_K^{(i)}$ $s = \tfrac{n_q}{n_k}$ times and concatenate them in sequence 
to form a new projection matrix $\mathbf{W}_K' \in \mathbb{R}^{D \times D}$:

\small{\[
  \mathbf{W}_K' 
  =
  \Bigl[
  \underbrace{\mathbf{W}_K^{(0)}, \dots, \mathbf{W}_K^{(0)}}_{s\ \text{times}}, 
  \;\dots,
  \underbrace{\mathbf{W}_K^{(n_k-1)}, \dots, \mathbf{W}_K^{(n_k-1)}}_{s\ \text{times}}
  \Bigr].
\]}
\normalsize
Applying $\mathbf{W}_K'$ to $\mathbf{X}$ directly yields
\[
  \mathbf{K}' \;=\; \mathbf{X}\,\mathbf{W}_K'.
\]
This approach is mathematically equivalent to first computing $\mathbf{K}$ and then replicating its heads.

\subsection{Low-Rank Factorization Form MLA}
\label{sec:low_rank_factorization}

Figure~\ref{subfig:transmla_mla} demonstrates that 
$\mathbf{W}_K'$, formed by replicating $\mathbf{W}_K$, has at most $n_k\,d_h$ degrees of freedom. 
Consequently, its rank is at most $n_k\,d_h$. 
To see this more formally, we factorize $\mathbf{W}_K'$ using the Singular Value Decomposition (SVD):
\[
  \mathbf{W}_K' \;=\; \mathbf{U}_K \;\mathbf{S}_K \;\mathbf{V}_K^\top,
\]
where $\mathbf{U}_K$ and $\mathbf{V}_K$ are $D \times D$ orthogonal matrices, 
and $\mathbf{S}_K$ is a $D \times D$ diagonal matrix of singular values. 
Only the top $n_k\,d_h$ (or fewer) of these singular values can be nonzero. 
Hence, we can truncate the SVD to keep just the top-$r$ singular values, with $r \leq n_k\,d_h$:
\[
  \mathbf{W}_K'
  \;=\;
  {\mathbf{U}_K}_{[:, :r]} \; {\mathbf{S}_K}_{[:r, :r]} \; {\mathbf{V}_K^\top}_{[:r, :]}.
\]
Define
\[
  \mathbf{W}_K^a
  \;=\;
  {\mathbf{U}_K}_{[:, :r]} \;\sqrt{{\mathbf{S}_K}_{[:r, :r]}}
  \;\in\;\mathbb{R}^{D \times r},
\]
\[
  \mathbf{W}_K^b
  \;=\;
  \sqrt{{\mathbf{S}_K}_{[:r, :r]}} \;{\mathbf{V}_K^\top}_{[:r,:]}
  \;\in\;\mathbb{R}^{r \times D}.
\]
Then
\[
  \mathbf{W}_K' \;=\; \mathbf{W}_K^a \;\mathbf{W}_K^b
  \quad\text{and}\quad
  \mathbf{K}' \;=\; \mathbf{X}\,\mathbf{W}_K^a \;\mathbf{W}_K^b.
\]

This construction shows how the GQA ``repeat KV'' scheme can be interpreted as a 
\emph{low-rank factorization} reminiscent of the MLA approach. 
In practice, when caching the key and value matrices, one only needs to store the low-rank representation 
$\mathbf{X}\,\mathbf{W}_K^a$. 
During the actual attention computation, the representation can be ``expanded'' by multiplying with 
$\mathbf{W}_K^b$, thus regaining the full dimensionality and enhancing expressiveness.

\subsection{MLA Not Representable in GQA}
\label{sec:exist_mla_not_representable_by_gqa}

Consider a scenario where the vectors in $\mathbf{W}_K^b$ are orthogonal. In this case, the output of each channel, after multiplying $\mathbf{X} \mathbf{W}_K^a$ with $\mathbf{W}_K^b$, remains distinct across the channels. However, in GQA, within each group, the heads are replicated, meaning that the output for all heads within a group is identical. This difference in structure implies that there are certain cases in MLA that cannot be captured by GQA, as MLA allows for more diversity in the outputs across the channels.

Based on the steps outlined above, we have proven Theorem \ref{thm:mla_greater_than_gqa}. By transforming GQA into an equivalent MLA representation, we can enhance the expressiveness of the model. The next section will present experimental results to validate this claim.

\section{Experiment}
\subsection{Experiment Setting}
This section demonstrates how to convert a GQA-based model into an MLA model using the Qwen2.5 framework, and compares the training performance of the two models on downstream tasks. The Qwen2.5-7B model has 28 query heads and 4 key/value heads per layer, with each head having a dimension of 128 and a KV cache dimension of 1024. Similarly, the Qwen2.5-14B model features 40 query heads and 8 key/value heads per layer, each with a dimension of 128, and a KV cache dimension of 2048.

Upon transforming the Qwen2.5-7B model into the MLA model, the output dimensions of the weight matrices $W_K^a$ and $W_V^a$ are adjusted to 512, while the KV cache dimension remains unchanged at 1024. Unlike the original setup of the GQA model, the TransMLA approach projection the dimensions of $W_K^b$ and $W_V^b$ from 512 to $28 \times 128 = 3584$ dimensions. Since the 28 query heads can now interact with 28 distinct queries, forming diverse functional representations, this adjustment significantly enhances the model's expressive power.

This transformation allows TransMLA to improve the GQA model's expressive ability without increasing the KV cache size. Notably, the increase in parameters is minimal. Specifically, for the Q-K pair, an additional $512 \times 3584$ matrix is introduced, while the original matrix had dimensions of $3584 \times 3584 + 512 \times 3584$. Therefore, the added parameters account for only 1/8 of the original matrix size. Similarly, for the V-O pair, the additional parameters also represent just 1/8 of the original. As a result, the total parameter count of the model increases slightly from 7.6 billion to 7.7 billion, which is a negligible rise.

\subsection{Comparing The Fine-Tuning Performance}
To evaluate the performance improvement of the transformed MLA model, we train both the original GQA-based Qwen model and the TransMLA model on an instruction fine-tuning dataset, SmolTalk \cite{allal2025smollm2}. The SmolTalk dataset includes a rich set of instruction-tuning data, including mathematical tasks (MetaMathQA \cite{yu2023metamath}) and coding tasks (Self-OSS-Starcoder2-Instruct \cite{wei2024selfcodealign}). The training was conducted using the torchtune \cite{torchtune} framework, with a batch size of 16, a learning rate of 2e-5, and 2 epochs. To minimize the impact on the original model, only the Key-Value layers were trained. For the GQA model, only $W_K$ and $W_V$ were trained, while for the TransMLA model, $W_K^a$, $W_K^b$, $W_V^a$, and $W_V^b$ were trained. The loss during training and the model's performance after training are shown in Figure \ref{fig:transmla_qwen}.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{imgs/transmla_qwen_25_7b_loss.pdf}
        \caption{Training Loss over Steps}
        \label{subfig:transmla_qwen_25_7b_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{imgs/transmla_qwen_acc.pdf}
        \caption{Test Accuracy}
        \label{subfig:transmla_qwen_acc}
    \end{subfigure}
    \caption{Comparison of Training Loss and Accuracy between TransMLA and GQA-based Qwen Models}
    \label{fig:transmla_qwen}
\end{figure}


As shown in Figure \ref{subfig:transmla_qwen_25_7b_loss}, the transformed MLA model (TransMLA) exhibits a significantly lower loss during training, indicating stronger data fitting capabilities. Figure \ref{subfig:transmla_qwen_acc} further demonstrates that the TransMLA model outperforms the original GQA-based model in terms of accuracy across both 7B and 14B model settings, particularly in math and code tasks. This suggests that TransMLA not only enhances the model's expressiveness but also leads to notable performance improvements on specific tasks.

The performance boost is attributed not only to the enlarged Key-Value dimensions but also to the use of orthogonal decomposition. To further validate this, a comparison experiment was conducted in which the dimensionality expansion was implemented using an identity map initialization, without the orthogonal decomposition. The resulting model, trained on the GSM8K dataset, achieved an accuracy of 82.11\%, which was only 0.15\% higher than the original GQA-based model (81.96\%). This marginal improvement suggests that merely adding trainable parameters does not explain the significant performance boost; the orthogonal decomposition method plays a crucial role in enhancing the model's effectiveness.

Further experiments are ongoing to explore the underlying causes of this phenomenon and to validate the contribution of orthogonal decomposition to model performance.


\section{Conclusion and Future Work}
In this paper, we demonstrate that the expressive power of Multi-Head Linear Attention (MLA) surpasses that of Group Query Attention (GQA). We prove that any GQA model can be equivalently transformed into an MLA model. Through extensive experiments, we validate that the MLA model, once transformed and fine-tuned, exhibits significantly enhanced performance. This work fills the gap left by the DeepSeek paper, which lacked both a theoretical and empirical comparison between GQA and MLA.

In future work, we aim to extend our approach by adapting large-scale models such as LLaMA, Qwen, and Mistral to MLA-based architectures. Additionally, we plan to employ DeepSeek R1 distillation to further optimize the performance of the converted models, ultimately achieving even greater results.


\bibliography{custom}

%\appendix
%\section{Example Appendix}
%\label{sec:appendix}
\end{document}
