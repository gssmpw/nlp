[
  {
    "index": 0,
    "papers": [
      {
        "key": "katharopoulos2020transformers",
        "author": "Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\\c{c}}ois",
        "title": "Transformers are rnns: Fast autoregressive transformers with linear attention"
      },
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "peng2023rwkv",
        "author": "Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others",
        "title": "Rwkv: Reinventing rnns for the transformer era"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fu2024lazyllm",
        "author": "Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar",
        "title": "Lazyllm: Dynamic token pruning for efficient long context llm inference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jo2024a2sf",
        "author": "Jo, Hyun-rae and Shin, Dongkun",
        "title": "A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024snapkv",
        "author": "Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming",
        "title": "Snapkv: Llm knows what you are looking for before generation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ashkboos2024slicegpt",
        "author": "Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James",
        "title": "Slicegpt: Compress large language models by deleting rows and columns"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xia2023sheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared llama: Accelerating language model pre-training via structured pruning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "sun2024you",
        "author": "Sun, Tian and Zhang, Li and Wu, Shuang",
        "title": "You Only Need One: Efficient KV Sharing Across Transformer Layers"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024minicache",
        "author": "Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan",
        "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zuhri2024mlkv",
        "author": "Zuhri, Zayd Muhammad Kawakibi and Adilazuarda, Muhammad Farid and Purwarianti, Ayu and Aji, Alham Fikri",
        "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "frantar2022gptq",
        "author": "Frantar, Jamie and Pustokhina, Irina and Kotik, Artem and Kuzmin, Sergey",
        "title": "GPTQ: Efficient Quantization for Transformers in the GPT Family"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2024kivi",
        "author": "Liu, Zhiyang and Zhang, Dong and Li, Xinyi and Wu, Ji",
        "title": "Kivi: Quantized Key-Value Representation for Efficient Long-Context Transformers"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hooper2024kvquant",
        "author": "Hooper, James and Dai, Li and Zhang, Zhen and Lee, Seung-Hwan",
        "title": "KVQuant: Quantization for Efficient Key-Value Caching in Transformer Models"
      }
    ]
  }
]