\section{Related Work}
In large language models (LLMs), the key-value (KV) cache becomes a significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each token's KV pair must be recomputed for every new token, the memory demands of the cache grow quickly, limiting the ability to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs.

One approach is linear attention, as seen in methods like Linear Transformer \cite{katharopoulos2020transformers,wang2020linformer}, RWKV \cite{peng2023rwkv}, and Mamba \cite{gu2023mamba}, which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory demands, it can reduce model expressivity, leading to performance degradation in tasks requiring complex, long-range token dependencies.

Another technique is dynamic token pruning, employed by LazyLLM \cite{fu2024lazyllm}, A2SF \cite{jo2024a2sf}, and SnapKV \cite{li2024snapkv}. These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring a detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining.

Pruning head dimensions, seen in approaches like SliceGPT \cite{ashkboos2024slicegpt}, Sheared \cite{xia2023sheared}, and Simple Pruning \cite{sun2023simple}, reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the modelâ€™s ability to capture important token relationships. 

Sharing KV representations across layers, as in YONO \cite{sun2024you}, MiniCache \cite{liu2024minicache}, and MLKV \cite{zuhri2024mlkv}, reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.

Finally, KV quantization techniques like GPTQ \cite{frantar2022gptq}, Kivi \cite{liu2024kivi}, and KVQuant \cite{hooper2024kvquant} reduce the precision of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computational overhead, enabling longer contexts and faster inference with minimal performance loss. 

\newpage