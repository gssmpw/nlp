\section{Related Work}
In large language models (LLMs), the key-value (KV) cache becomes a significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each token's KV pair must be recomputed for every new token, the memory demands of the cache grow quickly, limiting the ability to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs.

One approach is linear attention, as seen in methods like Linear Transformer ____, RWKV ____, and Mamba ____, which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory demands, it can reduce model expressivity, leading to performance degradation in tasks requiring complex, long-range token dependencies.

Another technique is dynamic token pruning, employed by LazyLLM ____, A2SF ____, and SnapKV ____. These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring a detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining.

Pruning head dimensions, seen in approaches like SliceGPT ____, Sheared ____, and Simple Pruning ____, reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the modelâ€™s ability to capture important token relationships. 

Sharing KV representations across layers, as in YONO ____, MiniCache ____, and MLKV ____, reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.

Finally, KV quantization techniques like GPTQ ____, Kivi ____, and KVQuant ____ reduce the precision of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computational overhead, enabling longer contexts and faster inference with minimal performance loss. 

\newpage