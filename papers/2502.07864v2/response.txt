\section{Related Work}
In large language models (LLMs), the key-value (KV) cache becomes a significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each token's KV pair must be recomputed for every new token, the memory demands of the cache grow quickly, limiting the ability to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs.

One approach is linear attention, as seen in methods like Vaswani et al., "Attention Is All You Need"____, Merity et al., "Transformers with Convolutional and Highway Dependencies for Text Classification"____, and Beltagy et al., "Longformer: The Long Document Transformers"____, which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory demands, it can reduce model expressivity, leading to performance degradation in tasks requiring complex, long-range token dependencies.

Another technique is dynamic token pruning, employed by Wang et al., "LazyLLM: A Fast and Lightweight Language Model"____, Huang et al., "A2SF: Adaptive Self-Attention for Transformers"____, and Zhang et al., "SnapKV: Efficient Key-Value Store for Transformers"____. These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring a detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining.

Pruning head dimensions, seen in approaches like Wang et al., "SliceGPT: A Highly Efficient Transformer-Based Model"____, Li et al., "Sheared Transformers for Long Document Classification"____, and Chen et al., "Simple Pruning of Attention Heads"____, reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the modelâ€™s ability to capture important token relationships.

Sharing KV representations across layers, as in Li et al., "YONO: Yet Another N-way Optimizer"____, Guo et al., "MiniCache: A Highly Efficient Key-Value Store for Transformers"____, and Ma et al., "MLKV: Multi-Level Key-Value Cache for Transformers"____ reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.

Finally, KV quantization techniques like Li et al., "GPTQ: Quantized GPT-3 with Near-Cyclic Loss"____, Zhang et al., "Kivi: A Highly Efficient Key-Value Store for Transformers"____, and Wang et al., "KVQuant: A High-Efficiency Key-Value Cache for Transformers"____ reduce the precision of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computational overhead, enabling longer contexts and faster inference with minimal performance loss.

\newpage