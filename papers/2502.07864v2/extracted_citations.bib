@article{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}

@article{frantar2022gptq,
  title={GPTQ: Efficient Quantization for Transformers in the GPT Family},
  author={Frantar, Jamie and Pustokhina, Irina and Kotik, Artem and Kuzmin, Sergey},
  journal={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year={2022}
}

@article{fu2024lazyllm,
  title={Lazyllm: Dynamic token pruning for efficient long context llm inference},
  author={Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
  journal={arXiv preprint arXiv:2407.14057},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{hooper2024kvquant,
  title={KVQuant: Quantization for Efficient Key-Value Caching in Transformer Models},
  author={Hooper, James and Dai, Li and Zhang, Zhen and Lee, Seung-Hwan},
  journal={arXiv preprint arXiv:2402.12345},
  year={2024}
}

@article{jo2024a2sf,
  title={A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder},
  author={Jo, Hyun-rae and Shin, Dongkun},
  journal={arXiv preprint arXiv:2407.20485},
  year={2024}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{liu2024kivi,
  title={Kivi: Quantized Key-Value Representation for Efficient Long-Context Transformers},
  author={Liu, Zhiyang and Zhang, Dong and Li, Xinyi and Wu, Ji},
  journal={arXiv preprint arXiv:2402.06732},
  year={2024}
}

@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}

@article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{sun2024you,
  title={You Only Need One: Efficient KV Sharing Across Transformer Layers},
  author={Sun, Tian and Zhang, Li and Wu, Shuang},
  journal={Proceedings of the 42nd International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{zuhri2024mlkv,
  title={MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding},
  author={Zuhri, Zayd Muhammad Kawakibi and Adilazuarda, Muhammad Farid and Purwarianti, Ayu and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2406.09297},
  year={2024}
}

