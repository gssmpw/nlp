\label{sec:method}



\label{s:data_construction}
In this section, we introduce the detailed methodology for constructing \datasetname,  focusing on persona-based behavior chain extraction, distractors generation and presents the multi-choice and generation task formulations.

\subsection{Overview}
% \label{}
% simulating their behaviors, emotions, decision-making, and language use. While character role-playing demonstrates potential,

While LLMs offer potential for creating digital twins of individuals, current benchmarks inadequately evaluate their ability to simulate human behavior, especially within dynamic, continuous contexts. Thus, we introduce \datasetname. 
As illustrated in Figure \ref{fig:workflow}, each \datasetname instance is composed of four key components: a persona profile $p$, a historical narrative $h$, a behavior chain $B=\{b_1,b_2,\dots,b_n\}$ of the specific persona, and the contextual setting for each behavior $C=\{c_1,c_2,\dots,c_n\}$.
For the multi-choice task setup, we 
augment each behavior \( b_i \)
with three distractor behaviors
\( D_i = \{d_1, d_2, d_3\} \).
Example data samples are provided in Appendix \ref{sec:app_instance}.
% a new benchmark comprising character information and their behavior chains extracted from real-world books.
% For initial data preparation, we used chapter summaries of fiction and biography works from SupperSummary (ordered by publication date, newest to oldest) as character histories and used them to generate character profiles, These summaries were rewritten using GPT-3.5 to address any copyright concerns. and obtained free (Gutenberg) and paid e-books as sources for behavior chains.
% as character histories. These summaries,
Historical narratives were constructed by refining chapter summaries from SuperSummary\footnote{\url{https://www.suppersummary.com}} using GPT-3.5.
Persona profiles were generated from character analyses, also sourced from SuperSummary.  Novels and biographies from Project Gutenberg and purchased e-books served as the source material for extracting high-quality behavior chains.  
In the following sections, we will detail the scalable strategy employed for behavior chain and distractor generation, applicable to any character-centric literature. 
% All generation prompts are provided in Appendix \ref{sec:app_prompt}.
More detailed information and prompts about the dataset construction is provided in Appendix \ref{sec:app_con} and \ref{sec:app_prompt}.



\subsection{Dataset Construction}
% \label{}
The dataset's construction proceeds in three steps: (1) chapter selection and paragraph segmentation based on the frequency of a representative character's presence; (2) paragraph-by-paragraph extraction of behaviors and their corresponding contexts; and (3) distractor generation for each behavior within a given context.
% Next, we elaborate on each step in detail.

% \paragraph{Prelude: Data Preparation}
% We collected raw materials from two sources. First, 

\paragraph{Chapter Selection and Segmentation}

To ensure behavioral continuity and consistency with established character traits, we extracted the main character's behaviors from consecutive chapters in the latter half of each target book.  
Chapters with the highest character name frequencies were selected to maximize extractable behaviors.  
These chapters were then evenly divided into paragraphs, from which one behavior per paragraph was extracted.


\paragraph{Behavior and Context Extraction}


This step presents two challenges: the non-uniqueness and meaninglessness of extracted behaviors, and contextual information leakage. 
To address these challenges, we propose a hierarchical context-behavior extraction method based on segmented paragraphs $P = \{p_1, p_2, ..., p_m\}$.
Our framework begins by extracting the first node ${(c_1, b_1)}$ of the chain, where $c_1$ donates the initial context and  $b_1$ donates the most key behavior extracted from the first paragraph $p_1$.
For subsequent paragraphs $p_i$ (where $i > 1$), we instruct the model to extract $(c_i, b_i)$ by considering the preceding paragraph $p_{i-1}$, the preceding behavior $b_{i-1}$ , and the current paragraph $p_i$.  
Context-behavior isolation is maintained through post-refinement.
To ensure valid behavior extraction, we use a dual-validation mechanism: similarity and meaningfulness checks.
If either check fails, the current behavior $b_i$ is discarded, and the current paragraph $p_{i}$ is concatenated with the next paragraph $p_{i+1}$ to begin a new behavior extraction round.  
Validated $(c_i, b_i)$ nodes are appended to the chain, and the process continues until all paragraphs are processed or the iteration limit is reached.
Detailed algorithm flow is shown in \ref{algo:gen}.
% Context-behavior isolation is maintained by incorporating the scene and plot transition $\Delta p$  between  $b_{i-1}$ and $b_i$ into $c_i$ and post-refining.


% We request the model to extract only the most meaningful behavior it considers in the current paragraph, and that the behavior is not dissimilar to the previous behavior, and post-process the extracted behavior.








\paragraph{Distractor Generation}
For the multi-choice tasks, we generate three distractors, $D=\{d_1,d_2,d_3\}$, for each extracted context-behavior node $(c_i,b_i)$.  
We instruct the model to first identify the core personality trait reflected in the original behavior $b_i$.
This trait serves as a psychological anchor for subsequent distractor generation.
The model is then tasked with generating three context-constrained adversarial distractors, each exhibiting a distinct personality trait. This process ensures that the distractors are meaningfully related to the behavior while also being distinctly different in terms of personality.



\paragraph{Manual Examination}


To ensure data quality, ten native English-speaking undergraduate annotators were recruited and compensated at regional minimum wage rates.
These annotators, under expert guidance, evaluated the meaningfulness of each behavior and the logical coherence between context and behavior, flagging low-quality entries.
After iterative refinement, we curated 1,001 behavior chains of fictional and non-fictional characters, each uniquely mapped to its source monograph (with no cross-book duplicates).


% receive literary analysis guidelines co-authored by professional critics and domain experts, 



% \begin{algorithm}[t]
% \caption{Chain Generation}
% \label{algo:gen}
% \SetKwData{Initialization}{\textbf{Initialization}}
% \DontPrintSemicolon
% \SetAlgoLined
% \KwIn {Paragraph Set $P=\{p_1,p_2,\dots,p_m\}$, Iteration Count $I$}
% \KwOut {chain $\{(c_1,b_1),(c_2,b_2),\dots,(c_n,b_n)\}$}
% \Initialization \\
% $c_1, b_1 \gets \text{Generate\_First\_Chain\_Node}(p_1)$ \\
% $chain \gets \{(c_1, b_1)\}$ \\ 
% $i \gets 2$\\
% \While{$i \leq m$}{ 
%     $c_i, b_i \gets \text{Generate\_Next\_Chain\_Node}(p_{i-1}, p_i, b_{i-1})$ \\
%     $s_i \gets \text{is\_Similar}(b_i, b_{i-1})$ \\
%     $m_i \gets \text{is\_Meaningful}(b_i)$ \\ 
%     \If{$s_i = 1 \textbf{or} m_i=1$ }{
%         $p_i \gets \text{Concatenate}(p_{i-1}, p_i)$ \\
%         $P \gets P \setminus p_{i+1}$ \\ 
%         \textbf{Continue} \\
%     }
%     $c_i \gets \text{Refine\_for\_Isolation}(c_i)$\\
%     $chain \gets chain \cup \{(c_i, b_i)\}$ \\ % Add to the chain
%     $i \gets i+1$\\
%     \If{ $i$ \geq $I$}{
%        \textbf{Break}
%     }
% }
% \Return {chain}
% \end{algorithm}


\begin{algorithm}[t]
\caption{Chain Generation}
\label{algo:gen}
\SetKwData{Initialization}{\textbf{Initialization}}
\DontPrintSemicolon
\SetAlgoLined
\KwIn {Paragraph Set $P=\{p_1,p_2,\dots,p_m\}$, Iteration Count $I$}
\KwOut {chain $\{(c_1,b_1),(c_2,b_2),\dots,(c_n,b_n)\}$}
\Initialization \\
$c_1, b_1 \gets \text{Generate\_First\_Chain\_Node}(p_1)$ \\
$chain \gets \{(c_1, b_1)\}$ \\
$i \gets 2$ \\
\While{$i \leq m$}{
    $c_i, b_i \gets \text{Generate\_Next\_Chain\_Node}(p_{i-1}, p_i, b_{i-1})$ \\
    $s_i \gets \text{is\_Similar}(b_i, b_{i-1})$ \\
    $m_i \gets \text{is\_Meaningful}(b_i)$ \\
    \If{$s_i = 1 \textbf{ or } m_i = 1$}{
        $p_i \gets \text{Concatenate}(p_{i-1}, p_i)$ \\
        \If{$i + 1 \leq m$}{ 
            $P \gets P \setminus p_i$ \\
            $m \gets m - 1$ \\
        }
        \textbf{Continue} \\
    }
    $c_i \gets \text{Refine\_for\_Isolation}(c_i)$ \\
    $chain \gets chain \cup \{(c_i, b_i)\}$ \\
    $i \gets i + 1$ \\
    \If{$i \geq I$}{
        \textbf{Break}
    }
}
\Return {chain}
\end{algorithm}




\subsection{Task Formulation}
\label{task_formulation}

% We framework multi-choices and generation tasks to assess model ability in fine-grained behavior understanding/reasoning in both close-domain and open-domain with long-range consistency.

We designed multiple-choice and generation tasks to evaluate LLMs' ability to simulate persona-based continuous behavior in closed- and open-domain settings.



 
\paragraph{Multi-choice Task}
% Given the input \( x_i = (p, h, \text{chain}_{<i}, c_i, O) \), where \( \text{chain}_{<i} \) represents the sequence of previous context-behavior pairs \(\{(c_1, b_1), (c_2, b_2), \dots, (c_{i-1}, b_{i-1})\} \),
% and $O$ contains the correct answer $b_i$ and highly confusing distractors $D=\{d_1,d_2,d_3\}$, the target LLM needs to identify the correct choice $b_{i}$.
% By providing candidate behaviors, the multi-choice setting provides a more structured environment to test the model's fine-grained understanding of persona-based behaviors and its ability to reason across spatiotemporal evolving contexts.
The input to the LLM is defined as \( x_i = (p, h, \text{chain}_{<i}, c_i, O) \), where \( \text{chain}_{<i} \) denotes the historical sequence of context-behavior pairs \(\{(c_1, b_1), (c_2, b_2), \dots, (c_{i-1}, b_{i-1})\}\), and \( O \) comprises the correct answer \( b_i \) alongside a set of highly plausible distractors \( D = \{d_1, d_2, d_3\} \). The model is tasked with identifying \( b_i \) from \( O \). This multi-choice framework provides a structured mechanism to evaluate the LLM’s capacity for fine-grained, persona-based behavioral reasoning and its ability to synthesize information across spatiotemporal-evolving contexts.  


\paragraph{Generation task} 
% Unlike the multiple-choice task, the generation task removes predefined options, giving the model the freedom to generate behaviors.
% Given  $x_i = (p,h,chain_{<i},c_{i})$ as input, the target model is tasked with generating behavior $y$, conditioned on the current context.
% Instead of strict matching generated behaviors $y$ with ground-truth behavior $b_i$, we use GPT-4o to evaluate if the $y$ consistent with the context and character's personality.
% Generation setting challenges the target model's ability to generate open-domain behaviors, better reflecting real-world scenarios where future digital twins are expected to autonomously generate behaviors, rather than selecting from limited options.
In contrast to the multiple-choice format, the generation task eliminates predefined options, affording the model greater flexibility to produce contextually relevant behaviors. For a given input \( x_i = (p, h, \text{chain}_{<i}, c_i) \), the model is tasked with generating a behavior \( y \) conditioned on the current context. Rather than relying on strict matching between generated outputs \( y \) and ground-truth behaviors \( b_i \), we employ ChatGPT-4o-latest to assess whether \( y \) aligns with the contextual constraints and the persona's predefined personality traits. This task evaluates the model’s capacity to synthesize open-domain behaviors, thereby more closely mirroring real-world applications where future digital twins are anticipated to operate autonomously, generating contextually appropriate behaviors rather than selecting from a finite set of predefined choices.  




