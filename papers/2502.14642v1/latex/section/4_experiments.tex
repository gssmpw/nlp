\label{sec:exp}

\begin{table*}[t]
\centering
\newcolumntype{C}{>{\columncolor{mc-color}}c}
\newcolumntype{G}{>{\columncolor{gen-color}}c}
\resizebox{1\textwidth}{!}{
\begin{tabular}{lCGCGCGCGCGCGCG}

\toprule
& \multicolumn{4}{c}{\textbf{Fictional Persona}} & \multicolumn{4}{c}{\textbf{Nonfiction Persona}} & \multicolumn{4}{c}{\textbf{Average}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
& \multicolumn{2}{c}{\textbf{Multi-choice}} & \multicolumn{2}{c}{\textbf{Generation}} & \multicolumn{2}{c}{\textbf{Multi-choice}} & \multicolumn{2}{c}{\textbf{Generation}}& \multicolumn{2}{c}{\textbf{AvgAcc}} & \multicolumn{2}{c}{\textbf{ChainAcc}} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
\cellcolor{model-color}\textbf{Model} & \cellcolor{white}\textbf{AvgAcc} & \cellcolor{white}\textbf{ChainAcc} & \cellcolor{white}\textbf{AvgAcc} & \cellcolor{white}\textbf{ChainAcc} & \cellcolor{white}\textbf{AvgAcc} & \cellcolor{white}\textbf{ChainAcc} & \cellcolor{white}\textbf{AvgAcc} &
\cellcolor{white}\textbf{ChainAcc}&
\cellcolor{white}\textbf{AvgAcc} & \cellcolor{white}\textbf{ChainAcc} & \cellcolor{white}\textbf{AvgAcc} & \cellcolor{white}\textbf{ChainAcc} \\ \midrule
\multicolumn{13}{c}{\cellcolor{dark-grey}\textbf{Closed-Source Models}} \\ \midrule % Added line for closed-source
\cellcolor{model-color}\textbf{gpt-3.5} & 0.276 & 0.047 & 0.273 & \textbf{0.056} & 0.328 & 0.066 & 0.366 & 0.120 & 0.302 & 0.057 & 0.320 & 0.088 \\
\cellcolor{model-color}\textbf{gpt-4o} & \textbf{0.515} & \textbf{0.128} & \textbf{0.366} & \textbf{0.082} & \textbf{0.602} & \textbf{0.188} & \textbf{0.575} & \textbf{0.297} & \textbf{0.559} & \textbf{0.158} & \textbf{0.471} & \textbf{0.189} \\
\cellcolor{model-color}\textbf{claude-3-5-haiku} & 0.433 & 0.094 & 0.133 & 0.025 & 0.497 & 0.116 & 0.323 & 0.117 & 0.465 & 0.105 & 0.228 & 0.071 \\
\cellcolor{model-color}\textbf{qwen-plus} & 0.435 & 0.094 & 0.093 & 0.015 & 0.520 & 0.135 & 0.206 & 0.048 & 0.478 & 0.115 & 0.149 & 0.032 \\
\cellcolor{model-color}\textbf{deepseek-chat} & 0.474 & 0.109 & 0.094 & 0.014 & 0.560 & 0.154 & 0.220 & 0.062 & 0.517 & 0.132 & 0.157 & 0.038 \\

\multicolumn{13}{c}{\cellcolor{dark-grey}\textbf{Open-Source Models}} \\ \midrule % Added line for open-source

\cellcolor{model-color}\textbf{llama-3p1-8b} & 0.331 & 0.059 & 0.132 & 0.028 & 0.350 & 0.068 & 0.129 & 0.022 & 0.341 & 0.064 & 0.131 & 0.025 \\
\cellcolor{model-color}\textbf{llama-3p1-70b} & \textbf{0.531} & \textbf{0.137} & 0.221 & 0.038 & \textbf{0.617} & \textbf{0.206} & 0.326 & 0.068 & \textbf{0.574} & \textbf{0.172} & 0.274 & 0.053 \\
\cellcolor{model-color}\textbf{mixtral-8x7b} & 0.159 & 0.025 & 0.043 & 0.006 & 0.200 & 0.035 & 0.252 & 0.070 & 0.179 & 0.030 & 0.148 & 0.038 \\
\cellcolor{model-color}\textbf{qwen2.5-72b} & 0.403 & 0.082 & \textbf{0.230} & 0.044 & 0.479 & 0.114 & \textbf{0.420} & \textbf{0.151} & 0.441 & 0.098 & \textbf{0.325} & \textbf{0.098} \\
\cellcolor{model-color}\textbf{yi-1p5-34b} & 0.274 & 0.049 & 0.031 & 0.006 & 0.292 & 0.062 & 0.293 & 0.089 & 0.283 & 0.056 & 0.162 & 0.048 \\
\bottomrule
\end{tabular}
}
\label{tab:main}
\caption{Models performance on \datasetname, showing AvgAcc and ChainAcc results for simulating fictional and nonfictional characters' behaviors in multi-choice and generation settings, along with overall AvgAcc and ChainAcc.}
\end{table*}



\subsection{Experiments Setup}

\subsubsection{Models}

We selected ten LLMs for our experiments, encompassing closed-source models Claude-3.5-Haiku-20241022, DeepSeek-Chat~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, GPT-3.5-0613~\citep{brown2020language}, GPT-4o-2024-11-20~\citep{openai2024gpt4o} and Qwen-Plus-Latest, and open-source models consisted of Meta-Llama-3p1-8B-Instruct~\citep{dubey2024llama}, Meta-Llama-3p1-70B-Instruct~\citep{dubey2024llama}, Mixtral-8x7B-Instruct-v0.1~\citep{jiang2023mixtral}, Qwen2.5-72B-Chat~\citep{qwen2025qwen25technicalreport}, and Yi-1.5-34B-Chat~\citep{yi2023yi}.


\subsubsection{Evaluation Metrics}
\label{metric}


% To evaluate the xxx ability of LLMs, 
% We introduce two metrics, the Average Accuracy and the Chain Accuracy, to comprehensively quantify the correctness of LLMsâ€™ reasoning outputs. 
% to both quantifying the precision of individual behavior predictions and capturing the model's ability to discern dynamic relationships within behavioral sequences and maintain consistency in complex environments.

To quantify LLM performance in persona-based behavior chain simulation, we introduce two persona-level metrics: Average Accuracy (AvgAcc) and Chain Accuracy (ChainAcc). Overall benchmark performance is then computed by aggregating persona-level scores using expectation.

% , and their ability to discern dynamic relationships and maintain consistency within behavioral sequences.
% establishes a more rigorous and holistic evaluation for behavioral simulation quality in digital twin systems.
% We introduce two metrics for comprehensive evaluation of behavioral chain simulation. This xxx comprises Average Accuracy, quantifying the precision of individual behavior predictions, and Chain Accuracy, capturing the model's ability to discern dynamic relationships within behavioral sequences and maintain consistency in complex environments. This dual validation standard, balancing local accuracy and global coherence, establishes a more rigorous and holistic evaluation for behavioral simulation quality in digital twin systems.



\paragraph{Average Accuracy (AvgAcc)} AvgAcc measures a model's ability to capture discrete behaviors by calculating the node-wise accuracy of behavior identification or generation within a chain of length $n$. 
$A_i$ denotes the correctness of the model's response at node $i$. 
The criteria for correctness vary by task (see Section \ref{task_formulation}).
\begin{equation}
\text{AvgAcc} = \frac{1}{n}\sum_{i=1}^n A_i
\end{equation}




\paragraph{Chain Accuracy (ChainAcc)} 
ChainAcc measures models's higher-order ability to continuously capture behaviors consistent with both persona and context throughout a behavior chain.
ChainAcc is calculated by tabulating the lengths of all $m$ consecutively correct behavioral sequences in the chain and normalizing the result. 
\( L_k \) represents the length of the \( k \)-th consecutively correct sequence, and \( n \) represents the total number of nodes.
\begin{equation}
\text{ChainAcc}_{\text{norm}} = \frac{\sum_{k=1}^m L_k}{\sum_{i=1}^n i}
\end{equation}


% maintain the continuity of behaviors and capture dependencies within a spatiotemporal context 





% \subsubsection{Implementation Details}


\subsection{Main Result}





Based on \datasetname, we systematically evaluated the behavior chain simulation capabilities of current leading LLMs across \textit{multi-choice} and \textit{generation} tasks. 
More experimental details are provided in Appendix \ref{sec:app_exp}.
% Performance in both individual behavior (AvgAcc) and, especially, behavior chain consistency (ChainAcc, below 30\%) revealing significant room for improvement (see Table \ref{tab:main}). This highlights a critical technical bottleneck in digital twin development.


% \subsubsection{Multi-Choice Setting vs Generation Setting}

% % The Multi-Choice setting focuses on evaluating a model's ability to discern behaviors and reason within a given context. 
% By providing candidate behaviors, including the correct answer $b_i$ and highly confusing distractors $D=\{d_1,d_2,d_3\}$, the multi-choice setting provides a more structured environment to test the model's fine-grained understanding of person-consistent behaviors and context-based reasoning across spatiotemporal evolving contexts.

% Generation setting challenges the target model's ability to generate open-domain behaviors. This setup better reflects real-world scenarios where future digital twins are expected to autonomously generate behaviors, rather than choose from limited options.



% This setting reduces generation noise and, by using highly confusing distractors, isolates the model's grasp of the intrinsic logic and xxx of behavioral chains.
% (e.g., distinguishing between "polite refusal" and "tactful evasion")
% (e.g., inferring subsequent actions based on causal logic of preceding behaviors)
% (e.g., excluding anomalous behavior options inconsistent with a character's personality).



% It challenges models to autonomously construct reasonable behaviors based on character background (integrating personality traits and current context and maintain dynamic coherence across behavior chains.
 % in the absence of pre-set options,
 % (e.g., adapt to sudden situational shifts);
 % (e.g., adapt to sudden situational shifts); and 3) avoid logical breaks and out-of-character (OOC) behavior.





% in real-world person-around scenarios, and [x\%] and [x\%] in fictional person scenarios,


% As shown in Figure \ref{fig:models}, a cross-model comparison reveals that leading models like GPT-4o and Llama-3p1-70b achieve average accuracies of []\% and []\%, respectively, 
% significantly outperforming other models (ranging from [x\%] to [x\%]). 
% This trend aligns with the general capabilities of existing models. 
% Notably, GPT-4o's chain accuracy in fictional character settings ([x\%]) drops [x\%] compared to its nonfictional character simulation performance ([x\%]), revealing an inherent weakness in modeling fictional persona-around behaviors.

\paragraph{Multi-choice vs Generation}
Models generally perform better in the multi-choice task compared to the generation task, with \meone scores ranging from 0.159 to 0.474. 
For example, GPT-4o achieves an \meone score of 0.515 in the multi-choice task, but its score drops to 0.366 in generation for fictional settings.
These results highlight that models struggle more with generating consistent, contextually accurate behaviors across dynamic contexts in open domains.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{latex/picture/model2.png}
    \caption{Different model family's AvgAcc (dark bars) and ChainAcc (light bars) performance on \datasetname.}
    \label{fig:models}
\end{figure}




\paragraph{Model Family Comparison} 
As shown in Figure \ref{fig:models}, a cross-model comparison reveals that leading models like GPT-4o and Llama-3p1-70B achieve \meone scores of 0.515 and 0.531, respectively, in the multi-choice task, significantly outperforming other models (ranging from 0.159 to 0.474). This trend aligns with the general capabilities of existing models. In the generation task, GPT-4o also maintains its advantage, outperforming other models by at least 5\%â€“30\% in simulating fictional personas and by 10\%â€“40\% in simulating nonfiction personas.


% Notably, GPT-4o's chain accuracy in fictional character settings (12.8\%) drops 6.0\% compared to its nonfictional character simulation performance (18.8\%), revealing an inherent weakness in modeling fictional persona-around behaviors.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.75\linewidth]{scale.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}

\paragraph{Scaling Laws in Behavior Simulation}
The positive correlation between model scale and behavior chain simulation performance is clear, particularly within the Llama-3 family.
For instance, the Llama-3p1-70B outperforms its 8B parameter counterpart in nonfiction setting by a substantial 76.5\% in AvgAcc scores (0.617 vs. 0.350) and an impressive 205\% in ChainAcc scores (0.206 vs. 0.068).



% demonstrate significantly enhanced behavior simulation capabilities


% These results support the application of scaling laws to behavioral modeling.

% â€”increased model capacity enhances the simulation of continuous and complex behavioral patterns, with larger models exhibiting stronger contextual integration advantages, especially when dealing with long-range dependencies.


\paragraph{Nonfiction vs. Fiction}
% Performance varies notably between nonfiction and fiction scenarios.
% For example, Qwen-plus achieves a 19.4\% higher average accuracy in simulating nonfiction character (51.96\%) compared to fiction character (43.51\%). 
% This difference likely stems from two key factors: 
% (i) the stronger causal links in biographical narratives, facilitating deterministic reasoning in the behavior chain, and 
% (ii) the unconventional psychological motivations of fictional characters, requiring more creative inference.
Performance varies notably between nonfiction and fiction settings. 
For example, Qwen-plus achieves a 0.194 higher average accuracy simulating nonfiction personas (0.520) compared to fiction personas (0.435). 
This performance difference is generally observed across models. 
This disparity likely stems from two key factors:(i) the stronger causal links in biographical narratives, facilitating deterministic reasoning in the behavior chain, and (ii) the unconventional psychological motivations of fictional personas, requiring more creative inference.




\paragraph{Models Struggle with Simulating Behavior Chains}
Even the top-performing models, the closed-source GPT-4o and the open-source Llama-3p1-70b-instruct, achieve AvgAcc scores below 0.62 and ChainAcc scores below 0.21 for the multi-choice task, with even lower performance in the generation task. 
The low scores demonstrate that existing models still struggle with accurately simulating person-based behavior chains, particularly in maintaining long-range consistency across these chains.



% This indicates that these models, while strong in other areas, struggle to accurately predict person-around behavior chains. 
% these models, while strong in other areas, struggle to accurately predict person-around behavior chains. 

% \textbf{A fundamental limitation of current methods is the pervasive challenge of behavior chain coherence.} 
% Chain accuracy is significantly lower than average behavior accuracy across all models (with an average decrease of 73.2\%).  For instance, even the top-performing Llama-3-70B achieves a nonfiction chain accuracy (20.60\%) less than one-third of its average accuracy. This suggests: 1) single behavior prediction relies heavily on local pattern matching, while chain reasoning requires global state tracking; and 2) models lack explicit modeling of continuous behavioral state machines.



\subsection{Analysis}
\label{sec:analysis}

\subsubsection{Key Behavior vs Sub-Key Behavior}




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/picture/keyvssub.png}
    \caption{AvgAcc performance across the entire dataset for multiple models at key behavior and sub-key behavior.}
    \label{fig:level}
\end{figure}

% covered by the chapter summary
% This resulted in a 3:7 ratio of key behaviors to sub-key behaviors.
% in behavior chain simulation
Not all behaviors are equally important.
To investigate the models' ability to capture key behaviors, we categorized behaviors based on their presence in the chapter summaries collected from SuperSummary.
Each behavior chain was thus divided into sets of key nodes $B_k$ and sub-key nodes $B_{sub}$, yielding a 3:7 ratio.
We then examined the models' AvgAcc scores on these two sets. 
Results in Figure \ref{fig:level} indicate that models prioritize capturing important behaviors, as all models achieved higher AvgAcc scores on key behaviors than on sub-key behaviors in both fiction and nonfiction settings.
This also reveals models' shortcomings in simulating sub-key behaviors.

% though the AvgAcc score remains constrained by the dynamic complexity of the behavioral chain.  
% A Wilcoxon signed-rank test confirmed that all models achieved significantly higher accuracy on important behaviors (Î¼ = 58.3\%) compared to subordinate behaviors (Î¼ = 41.7\%, p < 0.001). For example, Llama-3-70B achieved 68.2\% accuracy on important behaviors and 53.1\% on subordinate behaviors in nonfiction scenarios.


\subsubsection{History Length \& Context Length}


% A character's history, encompassing the background, the overarching story context, and all prior relationships, behaviors, events, and dialogues, shapes the character's current state and provides the foundation for their present behavior.
% % In our literary-based behavior simulations, characters' historical contexts are intrinsically encoded in the literary's textual fabric.
% We evaluated two history configurations: \textbf{original chapter text}, containing the full original chapter for detailed historical information, and \textbf{chapter summary}, a concise distillation of the chapters.  
% % By comparing model performance across these settings, we investigated their ability to utilize different history formats and the impact of information redundancy. 
% Results in \ref{tab:ori_vs_sum} revealed models' superior performance with \textit{original chapter text} as history.
% % , suggest that LLMs benefit from richer, more detailed historical context when simulating character behavior.  
% This implies that the more detailed, more authentic
% information present in the full chapter text, even if some of it is redundant or seemingly irrelevant, contributes positively to the model's ability to predict coherent and consistent behavior.

% % models struggle with lengthy text and benefit from concise, key information.

% \begin{table}[]
%     \centering
%     \begin{tabular}{lcc}
%     \toprule
%      & \textbf{AvgAcc} & \textbf{ChainAcc} \\
%     \midrule
%     \textbf{Original Text} & 0.556 & 0.156 \\
%     \textbf{Summary} & 0.528 & 0.143 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Original Text vs. Summary Performance}
%     \label{tab:ori_vs_sum}
% \end{table}
% % Original Text:
% % avg_ac: 0.555786401164139
% % normalized_chain_ac: 0.15599306496126603
% % Summary
% % avg_ac: 0.5281744032854387
% % normalized_chain_ac: 0.14331458465308755


\paragraph{History Length}



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/picture/line_back_and_reverse.png}
    \caption{Impact of number of deleted summaries on AvgAcc and ChainAcc (forward and backward directions).}
    \label{fig:length}
\end{figure}




Using chapter summaries as the unit of history, we investigate the impact of history completeness by truncating history summaries both chronologically (forward) and in reverse (backward).
\textit{Forward truncation} progressively removes the earliest history, while \textit{backward truncation} removes the most recent.
We calculated AvgAcc and ChainAcc scores for each truncation strategy.


As shown in Figure \ref{fig:length}, backward truncation leads to a gradual decline in AvgAcc, suggesting that recent historical information contributes crucial contextual information for predicting subsequent behaviors. Forward truncation exhibits a more resilient performance, with AvgAcc scores remaining relatively stable with the removal of early history. 
This implies that the core narrative thread and contextual cues are often established lately.
% allowing the model to maintain a reasonable understanding of the latest situation updates. 
However, beyond a certain threshold of removed recent history (around 10 chapters' summaries in this average case), we observe a more pronounced drop in AvgAcc performance. This indicates that a severe lack of character history drastically impairs LLMs' ability to accurately simulate character behaviors.





\paragraph{Context Length}



\begin{table}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{NODE ID} & \textbf{AvgAcc} & \textbf{ChainAcc} \\
\midrule
\textbf{$\#$1 - $\#$5} & 0.402 & 0.210 \\
\textbf{$\#$6 - $\#$10} & 0.440 & 0.243 \\
\textbf{$\#$11 - $\#$15} & 0.416 & 0.229 \\
\textbf{$\#$16 - $\#$20} & 0.405 & 0.213 \\
\bottomrule
\end{tabular}
\caption{AvgAcc and ChainAcc at different stages of behavior chains.}
\label{tab:node}
\end{table}


We investigated the impact of context length on model performance to explore whether LLMs exhibit a demonstration enhancement akin to in-context learning (ICL). To this end, we selected 20-step behavior chains and temporally segmented them into four phases. We then examined the distribution of correct behavior nodes. As shown in Table \ref{tab:node}, the distribution of correct behavior nodes exhibited a peak between nodes 6 and 10. This localized performance peak may indicate a form of short-term in-context learning or a priming effect, suggesting the model is better equipped to handle behavior prediction within this middle window.  However, this peak is not sustained throughout the entire chain, indicating that accurately simulating character behaviors becomes increasingly difficult as the chain unfolds.



\subsubsection{Snowball Effect in the Behavior Chain}

\begin{table}
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{History Behavior Type} & \textbf{AvgAcc} & \textbf{ChainAcc} \\
        \midrule
        \textbf{Ground Truth} & 0.528 & 0.143 \\
        \textbf{Chosen Behavior} & 0.420 & 0.100 \\
        \bottomrule
    \end{tabular}
    \label{tab:my_label}
    \caption{Comparison of model performance on AvgAcc and ChainAcc with ground truth behavior vs. chosen behavior history}
\end{table}


We examined the snowball impact of incorrect behavior correction on subsequent simulation in behavior chains using a multi-choice task setup.
% Our experiments employed a multi-choice task setup. 
At $i$-th node, the model received input $x_i = (p, h, chain_{<i}, c_i, O)$, where $chain_{<i} = \{(c_1, b_1), (c_2, b_2), \dots, (c_{i-1}, b_{i-1})\}$ represents the preceding context and behavior, and $O$ contains the correct behavior $b_i$ and distractors ${d_1, d_2, d_3}$. 
The model is tasked with selecting the next behavior, denoted as $\hat{b}_i$.
% Model's task is to identify the correct choice $o_{right}$ that aligns with the original.
To simulate the snowball effect of incorrect choices, we iteratively replaced the ground truth behavior $b_i$ with the chosen behavior $\hat{b}_i$ at each node $i$ throughout the chain.
As a result, for subsequent nodes, the $chain_{<i}$ in input $x_i$ becomes $\{(c_1, \hat{b}_{1}), (c_2, \hat{b}_{2}), \dots, (c_{i-1}, \hat{b}_{{i-1}})\}$.  
By comparing the model's AvgAcc and ChainAcc under both the normal (ground truth) and replacement settings, we observed a substantial performance decline in the latter.
This confirms the accumulation of errors, where incorrect behavior choices at earlier nodes exacerbate the likelihood of errors at later nodes.
This snowball effect suggests that in more realistic scenarios, where ground-truth is unavailable, simulating continuous behavior becomes more challenging. Using the model's previous behavior records as history significantly impacts subsequent behavior simulation, causing the model to deviate further from the character.


\subsubsection{Temporal Bias}



% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\linewidth]{year.pdf}
%     \caption{3D Visualization of Model Accuracy: AveAcc Scores (z-axis) for various models (x-axis) plotted against the publication year of the works used for behavior extraction (y-axis).}
%     \label{fig:year}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{year.png}
%     \caption{AveAcc Scores (z-axis) for various models (x-axis) plotted against the publication year of the works used for behavior extraction (y-axis).}
%     \label{fig:year}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{latex/picture/year.png}
    \caption{AveAcc Scores (z-axis) for various models (x-axis) plotted against the publication year of the works used for behavior extraction (y-axis).}
    \label{fig:year}
\end{figure}


% 3D Visualization of Model Accuracy:

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{year2.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}

We categorized \datasetname by the publication year of its source literature (Figure \ref{fig:bing}). We observed a negative correlation between model performance and publication year. 
As shown in Figure \ref{fig:year}, all models performed better on characters from older publications than on those from more recent ones. 
This is likely due to pre-training data bias, as LLMs are typically trained on a large corpus of classic texts, with relatively fewer contemporary works, contributing to the models' stronger grasp of earlier character behaviors. Therefore, we propose the use of recent literature (published in 2024) as a hard set for evaluation.

% Further results analysis is provided in Appendix \ref{sec:app_result}.



% This effect is particularly pronounced when comparing novels published in 2024 with those classified as classic fiction.






