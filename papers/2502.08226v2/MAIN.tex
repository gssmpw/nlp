%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
% \usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb,mathtools,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{svg}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{TRISHUL}
% ----- NEW CODE: Override the conference notice -----
% Remove the conference notice (i.e. "Proceedings of the ...") by redefining \Notice@String to be empty.
% \renewcommand{\Notice@String}{}

% ----- NEW CODE: Override the affiliations and notice macro -----
\makeatletter
\renewcommand{\printAffiliationsAndNotice}[1]{%
    {\let\thefootnote\relax\footnotetext{%
        \\
        \textsuperscript{*}denotes equal contribution (alphabetical order). \\
        \textsuperscript{1} Fractal AI Research, India. \\
        \ifdefined\icmlcorrespondingauthor@text
          \textsuperscript{†} Correspondence to: \icmlcorrespondingauthor@text.
        \else
          {\bf AUTHORERR: Missing \textbackslash icmlcorrespondingauthor.}
        \fi
    }}%
}
\makeatother
\begin{document}

\twocolumn[
\icmltitle{TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents}

\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corr}{†}

\begin{icmlauthorlist}
\icmlauthor{Kunal Singh}{equal,comp,corr}
\icmlauthor{Shreyas Singh}{equal,comp}
\icmlauthor{Mukund Khanna}{comp}
\end{icmlauthorlist}

\icmlaffiliation{comp}{Fractal AI Research, Mumbai, India}

\icmlkeywords{Machine Learning, ICML}
\icmlcorrespondingauthor{Kunal Singh}{kunal.singh@fractal.ai}
\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{} % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent advancements in Large Vision Language Models (LVLMs) have led to the emergence of LVLM-based Graphical User Interface (GUI) agents developed under various paradigms. Training-based approaches, such as CogAgent and SeeClick, suffer from poor cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, utilize Set-of-Marks (SoM) for action grounding; however, obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Additionally, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL’s superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.
\end{abstract}

\section{Introduction}

% \begin{figure*}
%     \centering
%     \includesvg[width=1\linewidth]{main_figs/fig1}
%     \caption{Screen parsing results showing detected GUI elements and their function descriptors leveraging our HSP and SEED modules}.
%     \label{fig:prompt_groi}
% \end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig1.png}
    \caption{Screen parsing results showing detected GUI elements and their function descriptors leveraging our HSP and SEED modules}.
    \label{fig:prompt_groi}
\end{figure*}

\label{sec:intro}
% Developing agents that could operate digital devices by following natural language commands has been an area of keen interest in AI for a while \cite{pmlr-v70-shi17a, Liu2018ReinforcementLO, Gur2018LearningTN}. These autonomous device-control AI agents hold the potential of greatly enhancing productivity by performing simple/complex/monotonous/day-to-day tasks by interacting with digital devices, similar to how humans operate. While the initial efforts \cite{pmlr-v70-shi17a, Liu2018ReinforcementLO, Gur2018LearningTN} tried this in simplified and restricted settings, multiple studies \cite{Li2020MappingNL, Wang2021Screen2WordsAM, Li2020WidgetCG, He2020ActionBertLU, Bai2021UIBertLG, Wu2021ScreenPT, Zhang2021ScreenRC, Chen2020ObjectDF, Chen2020UnblindYA, Li2020MappingNL} started focusing on GUI (Graphical User Interface) understanding to enable its incorporation paving the way for GUI Agents and expanding the usefulness of the device-control digital agents. More recent efforts to build GUI navigation agents \cite{Yao2022WebShopTS, Gur2023ARW, Deng2023Mind2WebTA, zhou2023webarena, Sridhar2023HierarchicalPA} have tried to leverage LLMs (large language models) that rely on additional structured representations(GUI-metadata) such as HTML source code, DOM tree for web pages, and View Hierarchy for Android interfaces, for comprehending the content on the GUI. 




% Owing to the recent advances in LVLMs (Large Vision Language Models), efforts \cite{zheng2023seeact, Deng2023Mind2WebTA, He2024WebVoyagerBA, Zhang2023AppAgentMA, Furuta2023MultimodalWN} have been made to augment visual perception as well along with leveraging GUI-metadata leading to improved performance on benchmarks such as Mind2Web \cite{Deng2023Mind2WebTA} and WebAreana \cite{zhou2023webarena}. LVLMs have poor visual grounding abilities \citep{yang2023setofmark}, often resulting in GUI agents struggling with action grounding. SeeAct \cite{zheng2023seeact} paper highlights that GUI action-grounding in GPT-4V \cite{GPT-4V} is significantly improved with set-of-marks (SoM) \cite{yang2023setofmark} on interactive elements and icons identified using HTML source. However, this dependence on metadata and structured text-based GUI representations introduces multiple limitations. Firstly, they are not easily available across platforms/apps/ and often require permissions or backend access. Secondly, the data is too noisy, often not aligned and too extensive ( e.g HTML source) to be effectively used for GUI navigation.


%  Recent research has focused on developing agents that rely solely on visual perception to interact with GUIs in a human-like manner. \cite{Hong2023CogAgentAV, You2024FerretUIGM, Cheng2024SeeClickHG, Wang2024MobileAgentAM, Bai2024DigiRLTI, shaw2023pixels} have trained pure vision focused agents for specialized GUI navigation tasks such as action grounding or GUI referring. There also has been growing interest in utilising visual perception modules during test-time to assist generalist LVLMs such as GPT4V. While MM-Navigator \cite{Yan2023GPT4VIW} and MobileAgent \cite{Wang2024MobileAgentAM} utilize it for action grounding, Tree-of-lens (ToL) Agent \citep{fan2024readpointedlayoutawaregui} leverages it for GUI referring task. However, these approaches face two major limitations. First, these methods are often optimized for a single task to improve performance. As a result, they either perform poorly on or are incompatible with other GUI navigation tasks, making them inadequate for comprehensive GUI understanding. Second, they either struggle to generalize to unseen environments or achieve suboptimal accuracy on their targeted tasks.




% To this end, we propose TRISHUL, a training-free, agentic framework designed for comprehensive GUI screen understanding, which equips LVLMs with the capabilities required to perform diverse GUI interaction tasks. TRISHUL utilizes foundational models to parse and build a rich hierarchical understanding of the GUI screens,to enhance their action grounding and GUI referring capabilities. Our agent consists primarily of two modules:


% \textbf{Hierarchical Screen Parsing (HSP):} The HSP module organizes GUI elements across two distinct levels of granularity:  broad regions called Global Regions of Interest (GROIs) which cluster related components and local elements like icons, text, and images. This hierarchical structuring captures spatial and semantic relationships between different GUI components, providing a multi-layered comprehensive GUI screen understanding. This organization establishes functional zones within the GUI, enhancing the model's ability to understand both high-level and fine-grained structural relationships across the interface. 
    
 
% \textbf{Spatially Enhanced Element Description (SEED):} To deepen the model's GUI comprehension, we propose a novel prompting framework called SEED. SEED generates contextually aware, spatially informed descriptions for local elements by analyzing their relative positioning with respect to other elements in the GUI. By associating nearby icons and text, SEED enables the generation of high-fidelity functionality descriptions for GUI elements, facilitating a more nuanced understanding of each element's role.
    

% We benchmark TRISHUL's grounding effectiveness on the ScreenSpot \cite{Cheng2024SeeClickHG} and VisualWebBench \cite{Liu2024VisualWebBenchHF} datasets. We further evaluate its performance on episodic web tasks in the Mind2Web \cite{Deng2023Mind2WebTA} dataset and Android-based tasks in the AITW \cite{rawles2023androidwildlargescaledataset} dataset. We showcase how large Generalist LVLMs like GPT-4V \cite{GPT-4V} and GPT-4o \cite{gpt4o} using  TRISHUL can significantly outperform prior State-of-The-Art training-based methods on single action grounding and episodic instruction following tasks. 
% We further validate TRISHUL’s effectiveness in GUI referring task, by benchmarking on the Screen PR dataset where, the model generates layout and content descriptions based on user-indicated points. This supports essential screen-reading capabilities for accessibility applications and provides accurate feedback on user interactions, enhancing both layout comprehension and task-specific outcome assessment.
Developing AI agents capable of operating digital devices through natural language commands has been a longstanding research goal \cite{pmlr-v70-shi17a, Liu2018ReinforcementLO, Gur2018LearningTN}. These agents can enhance productivity by automating tasks through Graphical User Interface (GUI). Early studies explored simplified settings \cite{pmlr-v70-shi17a, Liu2018ReinforcementLO, Gur2018LearningTN}, while later efforts \cite{Li2020MappingNL, Wang2021Screen2WordsAM, Li2020WidgetCG, He2020ActionBertLU, Bai2021UIBertLG, Wu2021ScreenPT, Zhang2021ScreenRC, Chen2020ObjectDF, Chen2020UnblindYA, Li2020MappingNL} leveraged GUI understanding to build more sophisticated agents. Recent approaches \cite{Yao2022WebShopTS, Gur2023ARW, Deng2023Mind2WebTA, zhou2023webarena, Sridhar2023HierarchicalPA} incorporate LLMs alongside structured GUI representations (e.g., HTML, DOM trees, View Hierarchy) to enhance comprehension.

With advances in LVLMs, studies \cite{zheng2023seeact, Deng2023Mind2WebTA, He2024WebVoyagerBA, Zhang2023AppAgentMA, Furuta2023MultimodalWN} have integrated visual perception to improve performance on benchmarks like Mind2Web \cite{Deng2023Mind2WebTA} and WebArena \cite{zhou2023webarena}. However, these models struggle with visual grounding \cite{yang2023setofmark}, relying heavily on structured metadata, which is often unavailable, noisy, or misaligned. SeeAct \cite{zheng2023seeact} improves action grounding in GPT-4V \cite{GPT-4V} via set-of-marks (SoM) \cite{yang2023setofmark}, but its dependency on structured data introduces limitations.

% Recent work has explored vision-only agents for GUI interaction \cite{Hong2023CogAgentAV, You2024FerretUIGM, Cheng2024SeeClickHG, Wang2024MobileAgentAM, Bai2024DigiRLTI, shaw2023pixels}. Approaches like MM-Navigator \cite{Yan2023GPT4VIW} and MobileAgent \cite{Wang2024MobileAgentAM} assist LLMs with visual grounding, while Tree-of-Lens \cite{fan2024readpointedlayoutawaregui} aids GUI referring. However, these methods often specialize in single tasks or struggle with generalization across environments.

\subsection{Related Works \& Motivation}

Recent research has focused on developing agents that rely solely on visual perception to interact with GUIs in a human-like manner. These works on purely vision-based GUI agents using LVLMs have evolved along 2 main approaches:

\textbf{End to End Training based GUI Agents}: Multiple studies \cite{Hong2023CogAgentAV, You2024FerretUIGM, Cheng2024SeeClickHG, Bai2024DigiRLTI, shaw2023pixels} have trained LVLMs on GUI navigation tasks for various platforms/device-types. 

% These models are usually specialized for tasks such as grounding and for platforms such as web or mobile. 

% /A concurrent work to us, Omniparser \citep{}, trains an icon detection model. In addition to that, they also train BLIP \citep{} to generate icon description. 

\textbf{Test-time assistance with visual perception tools}: Studies have leveraged visual perceptions tools to assist generalist LVLMs like GPT-4V. MM-Navigator \citep{Yan2023GPT4VIW} leverages pre-trained icon detector module. A concurrent work to ours, Omniparser \citep{OmniParser}, trains a YOLO-v8 \citep{yolov8_ultralytics} based icon detection \& BLIPv2 \citep{blip2} based icon captioner modules for action grounding. Tree-of-Lens (ToL) Agent \cite{fan2024readpointedlayoutawaregui} trains a perception module for GUI referring task of generating region description based on user selected point. 

Multiple GUI navigation-related benchmarks \cite{Liu2024VisualWebBenchHF, Xie2024OSWorldBM} and studies \cite{zheng2023seeact, Cheng2024SeeClickHG} have highlighted two major weaknesses among pure vision-based GUI navigation agents. Firstly, the performance of these methods trained on certain distribution of user interfaces don't generalize well across platforms/device types. Given the rapid pace with which new user interfaces are introduced every day, the generalizability of training based approaches to Out-Of-Distribution samples remains a challenge. Secondly, most of the GUI agents such as DigiRL \cite{Bai2024DigiRLTI}, SeeClick \cite{Cheng2024SeeClickHG},  MM-Navigator \cite{Yan2023GPT4VIW} are optimized for specialized GUI related tasks (majorly action prediction \& grounding), and often evaluate on diversely sourced but thematically similar tasks and metrics, hence they lack proper GUI comprehension capabilities across different tasks and interfaces.

% Our approach, TRISHUL, demonstrates how a multi-tiered global and local semantic understanding can greatly enhance comprehensive GUI understanding and navigation abilities of generalist LVLMs in a training free setup.
\begin{algorithm}
\caption{Hierarchical Screen Parsing}\label{alg:groi}
\scriptsize  % Reduce font size for the algorithm
\begin{algorithmic}[1]
    \Require Image $I$, $A_{\textit{thresh-GROI}}$, $A_{\textit{thresh-Icon}}$, $IOU_{\textit{thresh}}$, \textit{SAM}, \textit{OCR}
   
    \State Initialize: \textit{SAM}, \textit{OCR}, $A_{\textit{thresh}}$, $IOU_{\textit{thresh}}$
    \State Sample N points $\mathcal{P} \gets \mathcal{U}(0, W) \times \mathcal{U}(0, H)$ \Comment{Image Size ($W$, $H$)}
    \State $\mathcal{B} \gets \textit{SAM}(I, \mathcal{P}), \quad \mathcal{T} \gets \textit{OCR}(I)$ \Comment{SAM boxes $\mathcal{B}$ and OCR boxes $\mathcal{T}$}
    
    \State Initialize $\mathcal{G} \gets \emptyset, \mathcal{I} \gets \emptyset$ \Comment{GROI candidates and Icon candidates}
    
    \For{each $b \in \mathcal{B}$}
        \If{Area$(b) > A_{\textit{thresh-GROI}}$}
            \State $\mathcal{G} \gets \mathcal{G} \cup \{b\}$  \Comment{Add to GROI candidates}
        \EndIf
        \If{Area$(b) < A_{\textit{thresh-Icon}}$}
            \State $\mathcal{I} \gets \mathcal{I} \cup \{b\}$ \Comment{Add to Icon candidates}
        \EndIf
    \EndFor
    
    \State Initialize $\mathcal{S} \gets \emptyset$ \Comment{Information Scores for Non Max Suppression (NMS)}
    \State $\mathcal{I}_{\text{filtered}}, \mathcal{T}_{\text{filtered}} \gets$ \textit{Overlap Removal and Filtering}($\mathcal{I}, \mathcal{T}$)
    
    \For{each $b \in \mathcal{G}$}
        \State $\mathcal{N}_{\text{inside}} = |\{ \mathcal{T}_b^\text{inside} \}| + |\{ \mathcal{I}_b^\text{inside} \}|$ \Comment{Number of boxes  inside $b$}
        
        \State $\mathcal{N}_{\text{inter}}  = |\{ \mathcal{T}_b^\text{intersect} \}| + |\{ \mathcal{I}_b^\text{intersect} \}|$
        \Comment{Number of boxes intersecting $b$}
        
        \State $\mathcal{S} \gets \mathcal{S} \cup \left\{ 
        \frac{\mathcal{N}_{\text{inside}}}
        {\sqrt{1 + \mathcal{N}_{\text{inter}} \cdot \text{Area}(b)}} 
        \right\}$ \Comment{Information Score for $b$}    
    \EndFor

    \State $\mathcal{G}_{\text{filtered}} \gets$ \textit{NMS}($\mathcal{G}, \mathcal{S}, IOU_{\textit{thresh}}$) 
    \Comment{Apply NMS to get Filtered GROIs}

    \State \textbf{return} $\mathcal{G}_{\text{filtered}}, \mathcal{I}_{\text{filtered}}, \mathcal{T}_{\text{filtered}}$
\end{algorithmic}
\end{algorithm}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{main_figs/fig2.png}
    \caption{TRISHUL: Agentic Action Grounding Framework, Pink arrow, denotes our Hierarchical Screen Parsing (HSP) method, to generate GROIs and local element annotations, Green arrows represent our Spatially Enhanced Element Descriptor (SEED) workflow, Blue arrows represent our GROI proposal framework and Magenta Arrow shows, the Set of Marks (SoM) based  Grounding workflow.}
    \label{fig:main_fig}
\end{figure*}



\subsection{Contribution}

To address these challenges, we introduce TRISHUL, a training-free, agentic framework for comprehensive GUI screen understanding. TRISHUL equips LVLMs with the capabilities required to perform diverse GUI interaction tasks, it utilizes foundational models to parse and build a rich hierarchical understanding of the GUI screens,to enhance their action grounding and GUI referring capabilities.

\textbf{Hierarchical Screen Parsing (HSP):} The HSP module organizes GUI elements across two distinct levels of granularity: broad regions called Global Regions of Interest (GROIs) which cluster related components and local elements like icons, text, and images. This hierarchical structuring captures spatial and semantic relationships between different GUI components, providing a multi-layered comprehensive GUI screen understanding.
% This organization establishes functional zones within the GUI, enhancing the model's ability to understand both high-level and fine-grained structural relationships across the interface. 
    
 
\textbf{Spatially Enhanced Element Description (SEED):} SEED generates contextually aware and spatially informed functionality descriptions for local elements by analyzing their relative positioning with respect to other elements in the GUI. By associating nearby icons and text, SEED enables the generation of high-fidelity functionality descriptions for GUI elements, facilitating a more nuanced understanding of each element's role.

We evaluate TRISHUL on ScreenSpot \cite{Cheng2024SeeClickHG}, VisualWebBench \cite{Liu2024VisualWebBenchHF}, Mind2Web \cite{Deng2023Mind2WebTA}, and AITW \cite{rawles2023androidwildlargescaledataset}, demonstrating that GPT-4V \cite{GPT-4V} and GPT-4o \cite{gpt4o} using TRISHUL surpass prior state-of-the-art methods in action grounding and episodic instruction-following tasks. Additionally, we validate TRISHUL’s effectiveness in GUI referring via the Screen PR dataset, improving accessibility applications and user interaction feedback









% However, there has been growing interest in broader GUI understanding. GUI agents such as Ferret-UI \cite{You2024FerretUIGM} and CogAgent \cite{Hong2023CogAgentAV}, and benchmarks such as VisualWebBench \cite{Liu2024VisualWebBenchHF} have focused on comprehensive GUI understanding beyond action prediction and grounding capabilities. 

% Although  efforts \cite{Wang2024MobileAgentAM, Yan2023GPT4VIW} leverage generalist LVLMs such as GPT-4V to overcome these shortcomings, the performance still remains subpar due to the lack of focus on comprehensive GUI screen understanding.


% Here, referring requires the agent to utilize particular regional image information on the screen. 
% Interestingly, Screen Point and Read (ScreenPR) \cite{fan2024readpointedlayoutawaregui} introduces an overlooked yet important GUI referring task of generating local screen description based on user selected point. Currently, this task is predominantly handled by rigid accessible screen reading tools. This work highlights the poor performance across multiple LVLMs such as GPT-4o \cite{gpt4o} and CogAgent \cite{Hong2023CogAgentAV}.

% TODO

\section{Methodology}


This section outlines the design of our training-free screen comprehension modules, HSP and SEED, and sheds light on their integration into our action grounding and GUI referring agent.

% # Mention each module 
% # intution behind each module 
% # HSP intution 

% In this section we detail our training free, agentic framework for screen understanding that utilizes foundational models to not only parse but also interpret GUI elements hierarchically, ultimately enhancing visual grounding and referring capabilities of generalist LVLMs, while also being adept at instruction following task. The framework introduces a Layout Structure Tree (LST) to systematically represent GUI elements across three levels of granularity: the entire interface, larger regions termed Global Regions of Interest (GROIs), and individual elements like icons and text. The LST captures spatial and semantic relationships, creating a tree structure where the top layer provides a global view, the middle layer organizes related components into GROIs, and the leaf nodes represent detailed local elements. This organization aids in establishing functional zones within the GUI, improving the model’s ability to understand both broad and fine-grained structural relationships across the interface.

% To deepen the model's GUI comprehension, we propose the Spatially Enhanced Element Description (SEED) prompting framework, that generates precise, context-based descriptions for local elements by analyzing their spatial relationships. By associating nearby icons and text, SEED enables the generation of high fidelity functionality descriptions for GUI elements. 

% We further demonstrate how the the GROIs can be leveraged for enhanced visual grounding ability of LVLMs. We build a GROI proposal module, which ranks GROIs by their relevance to the provided instruction, thereby reducing the context window for LVLMs, consequently increasing the accuracy of the element descriptions  and the grounding accuracy by focusing attention on the most relevant areas of the interface.

% In addition, we demonstrate the applicability of the LST in tasks like Screen Point and Read, where the model generates layout descriptions based on user-indicated points. This functionality supports accessibility and action verification, providing crucial screen-reading capabilities for accessibility use cases and assisting with layout understanding for tasks that require accurate feedback on action outcomes.

% We also propose an end to end system that enables the leveraging of the HLT for visual grounding.
% We use a GROI proposal module that ranks GROIs based on their relevance to a given instruction, reducing the context window for LVLMs to work with, consequently improving both description quality and grounding accuracy. 
% Additionally, we demonstrate that similar to CITE ScreenPR our HLT can be leveraged  for the  Screen Point and Read task ie: Screen reading and Layout description generation  based on user indicated points
% essential for accessibility and action verification.
% We propose a novel agentic framework, for screen understanding, that leverages foundational models to develop an in-depth understanding of GUIs .
% Sections:
% 1.  Hierarchical Screen Parsing

% We propose a novel, training-free, hierarchical screen parsing method designed to enhance screen understanding for GUI-referring and action grounding tasks. 
% The key insight behind our approach is to leverage the inherently hierarchical structure of GUIs, where spatial and semantic relationships—such as containment, overlap, and proximity of different elements determine the functional organization of the interface. To represent this hierarchy, we introduce a Hierarchical Layout Tree (HLT), which captures both the content and spatial relationships among GUI components, enabling a more nuanced and comprehensive understanding of screen layout.

% The HLT is constructed as a fixed-depth tree with three layers that represent different levels of GUI granularity. At the top level, the root node corresponds to the entire screenshot, providing a global context for the interface. The middle layer consists of Global Regions of Interest (GROIs), which group together related elements or functionalities into coherent, larger regions, such as taskbars, sidebars, or grouped navigation menus. These GROIs simplify the GUI by consolidating elements that serve a unified purpose, making it easier to interpret high-level functional zones within the interface. Finally, the leaf nodes represent fine-grained, local elements, primarily text, icons, and pictures embedded in the GUI.


% 2. SEED 
% Building on the hierarchical layout, the SEED (Spatially Enhanced Element Description) framework extends the model's capability by generating detailed functionality descriptors for local UI elements based on their spatial relationships. By associating icons with nearby text and leveraging context, SEED produces precise descriptions that enable the model to infer the purpose of each element accurately. The grounding process is then integrated into the agentic framework, where GROIs are proposed in alignment with an instruction, and SEED-generated descriptors help finalize bounding box predictions for the target element. This dual-stage process—GROI proposal followed by detailed element-level grounding—enables LVLMs to perform visual grounding tasks effectively, facilitating accurate and contextually aware interaction with screen elements for various GUI navigation tasks.


% 3. 
% We also develop a GROI proposal module that ranks the different sections of the GROI based on the relvance to a given instruction. 
% We use the GROI to generate descriptions using SEED and then Use SoM to perform grounding on the GROI. 
% Using GROI is helpful becase it shortens the context window for LVLMs and greatly enhances their Element functionality description and consequentlty action grounding capability. 
% We also demonstrate how using how our Hierarichal Layout Tree helps in the Screen Point and Read task. 
% ie: Screen reading based on user indicated points is a
% crucial task for helping blind people understand UIs and also as an agent to verify actions.
% To incorporate our ToL
% agent for verification of MagicWonder’s output actions, we design a simple pipeline.  the layout and content descriptions are
% generated by our ToL agent for each action-pointscreenshot triplet and sent to the GPT-4 model.
% The GPT-4 model, based on the task goal, instructions, and descriptions from the ToL agent, decides
% whether the last action from the MagicWonder is
% correct.

% identified using a semantic segmentation model (SAM) and an OCR model (EasyOCR). A significant advantage of our training free approach is its adaptability to new and evolving UI components, unlike \textbf{(CITE ToL and Omniparser)} 

% methods which finetune models for downstream UI component detection tasks, that will require retraining when new components or design updates are introduced, our method can generalize to detect new or modified elements without retraining, making it ideal for dynamic UI environments.
%Out of Distribution Icons, Changing icons


\subsection{Hierarchical Screen Parsing}
\label{sec:HSP}


The hierarchical screen parsing process is formalized in Algorithm \ref{alg:groi}. Initially, the screen image \( I \) is passed through SAM \cite{SAM} and EasyOCR \cite{EasyOCR}. The generated bounding boxes are filtered based on predefined area thresholds \( A_{\text{thresh-GROI}} \) and \( A_{\text{thresh-LE}} \) to generate  GROI candidates and Local Elements (LE). Local Elements collectively refer to bounding boxes for text, icon, buttons and images in the GUI. We then apply an overlap removal and filtering function to refine the icon and text bounding boxes by removing redundant and unwanted local elements. 

For each GROI candidate, the number of boxes inside and intersecting with the GROI is calculated. An Information Score \( \mathcal{S} \) is then computed for each candidate based on the ratio of the number of bounding boxes inside, to the area of the GROI, adjusted by the number of intersecting boxes. This score provides a measure of the GROI's information content, helping the system to prioritize larger and more informative regions for inclusion in the hierarchical tree.

Finally, a Non-Max-Suppression (NMS) algorithm is applied to the GROI candidates based on their Information Scores. The resulting filtered set of GROIs, icons, and text boxes are returned as the final hierarchical structure, which contains all the relevant GUI elements grouped together through GROIs. For specific details on the Overlap Removal, Filtering and NMS algorithm refer to Appendix \ref{app: HSP_details}




\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{main_figs/fig3.png}
    \caption{TRISHUL: Agentic GUI Referring Framework, the 2 Lenses created using our HSP module for local and global context. Lens-1 contains the local element (blue) in the cropped GROI (red), Lens-2 contains the GROI (blue) in the full input screenshot (red).The selected point is represented as the black dot. Both lenses are fed to the LVLM to generate Layout and Task description.}
    \label{fig:ungabunga}
\end{figure*}



\subsection{SEED: Spatially Enhanced Element Description Generation}
\label{sec:SEED}


Accurately describing the functionality of local GUI elements is essential for effective understanding of GUI and action grounding. Relying solely on visual appearance is unreliable since identical icons can serve different purposes in different contexts, and distinct icons may represent similar functions, leading to ambiguity.  Textual and semantic cues around GUI elements help clarify functionality. Pairing icons with nearby text enables precise descriptions, while semantic associations (e.g., text linked to input fields or buttons) aid in identifying actionable elements.  

We introduce SEED (Spatially Enhanced Element Description), a prompting framework that employs Chain of Thought (CoT) \cite{wei2023chainofthoughtpromptingelicitsreasoning} and In-Context Learning (ICL) \cite{Brown2020LanguageMA} to generate spatially and semantically informed functional descriptions for all GUI elements. SEED processes an image \( I \) annotated with SoM-style ID tags, and a prompt with bounding boxes for detected elements (via our HSP module), and OCR-extracted text descriptors:  

\begin{equation}
    \mathcal{B}_\text{icon} = \{ (i, b_{\text{icon},i}) \}_{i=1}^{N_\text{icon}} 
\end{equation}
\begin{equation}
    \mathcal{B}_\text{text} = \{ (i, b_{\text{text},i}, d_i) \}_{i=N_\text{icon}}^{N_\text{total}},
\end{equation}  

where \( b_{\text{icon},i} \) and \( b_{\text{text},i} \) are bounding boxes, and \( d_i \) represents OCR-derived text descriptors.  

SEED outputs a spatially enhanced descriptor set \( \mathcal{A} \):  

\begin{equation}
\mathcal{A} = \left\{ b, \ell, a, d \mid b \in \mathcal{B}_\text{icon} \cup \mathcal{B}_\text{text} \right\}
\end{equation}  

Each element's attributes include bounding box \( b \), label \( \ell \in \{ paired, standalone, picture, actionable-text \} \), set of associated elements \( a \), and a spatially enhanced functional description \( d \).  

SEED classifies elements as paired or standalone based on semantics and positioning. Paired elements combine descriptors from nearby text/icons for a unified description, while standalone elements rely on visual cues alone. Text elements linked to interactive components (e.g., input fields, search bars, buttons) are labeled as actionable, and embedded icons are classified as \{picture\}.  

We use ICL \cite{Brown2020LanguageMA} with six examples from the ScreenSpot \cite{Jurmu2008ScreenSpotMR} dataset,  The full SEED prompt with specific details about the SEED module is available  in Appendix \ref{sec:appendix}.  





\subsection{Agentic Formulation of Action Grounding}
\begin{table}[h!]
\label{GROI_prop_acc}
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{Platform} & \multicolumn{2}{c}{ScreenSpot} & \multicolumn{2}{c}{VisualWebBench} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & GPT-4o & GPT-4V & GPT-4o & GPT-4V \\
        \midrule
        Mobile & 0.91 & 0.81 & - & - \\
        Web & 0.96 & 0.83 & 0.93 & 0.86 \\
        PC & 0.92 & 0.83 & - & - \\
        Overall & 0.93 & 0.82 & 0.93 & 0.86 \\
        \bottomrule
    \end{tabular}
    \caption{GROI proposal accuracy.}
    \label{groi_prop}
\end{table}

\label{sec:agent_framework}

This section explains how the hierarchical nature of GUIs is leveraged for enhanced SoM style action grounding in LVLMs as explained in fig. \ref{fig:main_fig}. Given an image \( I \) with Global Regions of Interest (GROIs) \( \mathcal{G} \), bounding boxes for icons \( \mathcal{B}_{\text{icon}} \) and text \( \mathcal{B}_{\text{text}} \), OCR-derived text descriptors \( d_j \), and an instruction \( I_s \), the task is to identify the bounding box \( \mathcal{B} \) corresponding to the correct element required to complete the instruction in a single step. 

TRISHUL performs action grounding in two stages. First, it proposes the most relevant GROI by passing the full annotated image \( I_{\text{annotated}} \), cropped GROIs \( \mathcal{G}_{\text{cropped}} \), and instruction \( I_s \) to the LVLM. The model outputs descriptions \( \mathcal{D}_\mathcal{G} \) for each GROI and the ID of the most relevant one:  

\begin{equation}
    \{ I_{\text{annotated}}, \mathcal{G}_{\text{cropped}}, I_s \} \longrightarrow \left\{ \mathcal{D}_\mathcal{G}, \, \text{ID}_{\text{GROI}} \right\}.
\end{equation}

GROI proposal accuracy is evaluated by checking if the ground truth bounding box midpoint lies inside the proposed GROI. Results with GPT-4o and GPT-4V on ScreenSpot \cite{Jurmu2008ScreenSpotMR} and VisualWebBench \cite{Liu2024VisualWebBenchHF} (Table \ref{groi_prop}) confirm the effectiveness of our GROI ranking module.  

Next, we use SEED (Section \ref{sec:SEED}) to generate functionality descriptors for all local elements in the proposed GROI. The annotated image and descriptors are then used in a Set of Marks \cite{yang2023setofmark} framework to predict the bounding box for grounding the instruction.  

\subsection{Agentic Formulation of GUI referring task}


In this section we describe how the hierarchical screen parsing module can be leveraged to increase the ability of LVLMs on the GUI referring task as explained in fig. \ref{fig:ungabunga}. Given the input GUI screenshot \textit{I}, the task involves describing the content and layout of any point \( P_{\text{i}} \) on the screen as input by a user, we use the input screenshot to detect all local elements and corresponding GROI candidates. We then identify the bounding box of the local element containing the selected point, and then the GROI encompassing this local element. Following the prompting approach of the ToL agent in \cite{fan2024readpointedlayoutawaregui},  we curate two “lenses” or images to illustrate this hierarchy. The first lens consists of only the GROI region cropped from the original image, highlighting the local element with a labeled bounding box and marking the input point. The second lens shows the complete screenshot, highlighting the GROI with a labeled bounding box. Both lenses, along with the point coordinate \( P_{\text{i}} \) and input prompt, are sent to an LVLM, to generate the content description \( \hat{D}_{\text{c}} \) and the layout description \( \hat{D}_{\text{l}} \). 
% For testing the LVLM baselines, they receive a text prompt with \( P_{\text{i}} \) and the full screenshot as inputs. Detailed prompt is provided in the Appendix in Figure \ref{fig:prompt_screenpnr}. For consistency, we use a temperature setting of 0.1.

% Given the input image \( I \) along with its associated GROIs \( \mathcal{G} \), filtered bounding boxes for icons \( \mathcal{B}_{\text{icon}} \) and text \( \mathcal{B}_{\text{text}} \), OCR-derived text descriptors \( d_j \), and the instruction \( I_s \). Our visual grounding task can be formulated as: 
% \[
% \{ I, \mathcal{G}, \mathcal{B}_{\text{icon}}, \mathcal{B}_{\text{text}}, d_j, I_s \} \longrightarrow \mathcal{B}_{\text{correct element}},
% \]

% 4 stage pipeline , we generate GROIs find the GROI most relevant to the instruction and use it for grounding. 
% Idea, Shorter context window helps SEED generate high quality and better descriptions, to prevent the loss of global context we provide a functional description of the GROI in context of the full image. Here is a detailed break down.


% 1. We generate GROIs and Local element annotations for the Image I using the Hierarichal Screen Parsing algorithm detailed in Setion1 .


% 2. The second task in our pipeline is the GROI ranking, where we aim to rank the generated GROIs based on their relevance to the instruction. We achieve this by feeding the annotated image \( I_{\text{annotated}} \) and the cropped GROIs \( \mathcal{G}_{\text{cropped}} \) into a multi-modal language model (LVLM) alongside the instruction \( I_s \). The model generates a description \( \mathcal{D}_\mathcal{G} \) for each GROI and outputs the ID of the most relevant GROI.
% \[
% \{ I_{\text{annotated}}, \mathcal{G}_{\text{cropped}}, I_s \} \longrightarrow \left\{ \mathcal{D}_\mathcal{G}, \, \text{ID}_{\text{relevant GROI}} \right\},
% \]
% where the relevant GROI ID is determined by contextualizing the descriptions within the scope of the full image and instruction. 


% 3. We Generate spatially aware semantic descriptors for all the local elements within the GROI using SEED section 3.2


% 4. Finally, visual grounding is performed by combining the spatially aware local descriptors \( \mathcal{A}_{\text{spatially aware}} \) with the cropped GROI and the instruction \( I_s \) to predict the correct bounding box. This step ensures that the grounding process accounts for both visual features and semantic understanding of the instruction, leading to the correct identification of the UI element. This task is represented as
% \[
% \{ \mathcal{G}_{\text{cropped}}, \mathcal{A}_{\text{spatially aware}}, I_s \} \longrightarrow \mathcal{B}_{\text{correct element}},
% \]


   

% \subsection{Formulation of Screen Point and Read Task}

% Our visual grounding task is formulated as 
% Given an Image I and Filtered GROIs G Filtered Icon boxes Filtered Text bboxes, Filtered OCR Text Descriptors, and an Instruction Is we have to output a Bbox most relevant to the instruction. 

% 4 stage pipeline 
% 1. We generate GROIs and Local element annotations for the IMage I.

% 2. Once the GROIs are generated we give cropped GROIs to LVLM along with the full image annotated with GROI bboxes and IDs SoM stype and prompt the model to output a brief description of each groi in cotext of the full image and the ID of the GROI most relevant to the instruction.

% 3. We Generate spatially aware semantic descriptors for all the local elements within the GROI.

% 4. We use the cropped groi, groi description in cotext of the full imahe  the SEED local descritpros to predict the correct UI element for the given instruction.



% GROI ranking , SEED description , SoM Prediction 

% We formulate our visual grounding task as follows: Given an image \( I \), filtered Global Regions of Interest (GROIs) \( G \), filtered icon bounding boxes, filtered text bounding boxes, filtered OCR text descriptors, and an instruction \( I_s \), the objective is to output the bounding box most relevant to the instruction. This task is tackled through a three-stage pipeline, leveraging hierarchical screen parsing and the SEED (Spatial Aware Region Description) module, along with a GROI ranking module to rank and select the most relevant UI element.

% The pipeline consists of the following stages:

% \textbf{1. GROI and Local Element Annotation Generation:}
% In this stage, we generate the Global Regions of Interest (GROIs) and local element annotations for the image \( I \). This process involves detecting the key areas of the UI that are most likely to contain actionable elements or relevant content for the instruction. Simultaneously, local annotations are created for all icon and text elements within the image, identifying their positions and functional associations.

% \textbf{2. GROI Description Generation Using LVLM:}
% Once the GROIs are generated, cropped versions of these GROIs are fed into a Multimodal Large Language Model (LVLM), along with the full image annotated with GROI bounding boxes and IDs in SoM style. The model is prompted to generate a brief description for each GROI in the context of the entire image. This description includes a detailed explanation of the visual content within the GROI and the ID of the GROI that is most relevant to the given instruction \( I_s \). The model’s output thus provides a ranked set of GROIs, each with an associated description.

% \textbf{3. Spatially Aware Semantic Descriptor Generation:}
% After the GROI descriptions are generated, the next step involves generating spatially aware semantic descriptions for all local elements within the selected GROI. These local elements include both icons and text components, which are classified and described in terms of their semantic functionality. The SEED module is employed here to provide accurate and context-aware descriptions for these elements based on spatial and semantic relationships.

% \textbf{4. Visual Grounding Using SEED and GROI Descriptions:}
% Finally, using the cropped GROI, the GROI description in the context of the full image, and the spatially aware semantic descriptors generated for the local elements within the GROI, the model predicts the correct UI element most relevant to the instruction \( I_s \). The spatially aware context provided by SEED helps refine the model's understanding of the functionality of each element within the GROI, enabling more accurate grounding to the instruction.

% In summary, our pipeline combines hierarchical screen parsing, the SEED module, and a GROI ranking mechanism to effectively ground visual elements in a UI to a given natural language instruction. This approach ensures that the correct UI element is selected by considering both global contextual information (from the full image) and local semantic cues (from the GROIs and local element descriptors).


% Ideas: Smaller context window helps generate better descritions using hence use GROIs.


% The generation of precise functionality descriptions for local icon elements is crucial for effective UI understanding and visual grounding tasks using Set of Marks (SoM) prompting. Icon elements in UIs often exhibit visually similar designs, which complicates the inference of their functionality based solely on appearance. While similar icons may serve different purposes, distinct functions can also share similar visual cues, leading to ambiguity and inconsistent interpretations.

% We observe that most icon elements in UIs are accompanied by nearby textual labels or cues, which, when leveraged effectively, can significantly clarify their intended functionality. By combining the semantic interpretation of an icon’s visual form with textual information from spatially proximate elements, a more context-aware and accurate functionality description can be generated.
% % SEED classifies each icon and text element in a UI screenshot as either paired or static.

% To address this challenge, we introduce SEED (Spatial Aware Region Description), a prompting framework that utilizes Chain of Thought(CoT) prompting, In Context Learning (ICL) and Set Of Marks (SoM) to generate detailed functional descriptions for all local elements (Text and Icon boxes) in an Image. 
% SEED takes as input an image annotated with IDs SoM style and an input prompt giving ocr text descriptors as filtered ocr bboxes and filtered icon bboxes as input and outputs spatially aware semantic and functional description of all the tagged UI elements. 


% Input: \( \{ I, \mathcal{B}_\text{icon}, \mathcal{B}_\text{text}, \mathcal{D} \} \) \\
% Output: \( \mathcal{A} = \{ (b, \text{label}, \text{description}) \mid b \in \mathcal{B}_\text{icon} \cup \mathcal{B}_\text{text} \} \)

% SEED classfies each local element (Text 
% \& Icon) as paired/ standalone. 


% Then for each paired element SEED outputs the IDs of the associated elements in the UI in a Chain of Thought manner, and uses the semantic cues and textual descriptions for all those elements to generate detailed functioal descriptions for those icons. 
% For Icons classifed as standalone SEED only relies semantic cues to generate functional description.
% Additionally we also classifiy text elements as actionable, if they are associated with an input field, search bar, buttons etc.

% In Section (REsults) we demonstrate the power of spatially aware prompting with tLarge Multimodal models in generating high quality functioanal descriptors for all UI elements. 



% The generation of precise functionality descriptions for local icon elements is essential for effective UI understanding and visual grounding in user interfaces (UIs). Icons in UIs often exhibit visually similar designs, which complicates the inference of their functionality based solely on appearance. This visual similarity can lead to functional ambiguity, as identical icons may represent different actions, while distinct functionalities may also share similar visual cues, resulting in inconsistent interpretations.

% In most UIs, icon elements are frequently accompanied by nearby text labels or cues, which, when leveraged, can significantly clarify the intended functionality of each icon. To address this challenge, we introduce \textbf{SEED} (Spatial Aware Region Description), a prompting framework that combines spatial awareness, Chain of Thought (CoT) prompting, In-Context Learning (ICL), and Set of Marks (SoM) techniques to accurately interpret and describe both icon and text elements within a UI image. 

% SEED takes as input,
% an image \( I \) annotated with SoM-style ID tags, and an input prompt consisting of filtered bounding boxes and IDs of all Icon elements and bounding boxes IDs and and OCR descriptors for all text elements:
%     \[\mathcal{B}_\text{icon} =
%     \{ (i, b_{\text{icon},i}), \}_{i=1}^{N_\text{icon}} \quad, \quad \mathcal{B}_\text{text} = \{ (i, b_{\text{text},i}, d_i) \}_{i=N_\text{icon}}^{N_\text{total}}
%     \]
%     where \( b_{\text{icon},i} \) and \( b_{\text{text},i} \) represent bounding boxes and \( d_i \) provides OCR-derived descriptors for text elements. The output of SEED is a spatially aware semantic and functional descriptor set \( \mathcal{A} \) for all tagged UI elements, formulated as:
% \[
% \mathcal{A} = \{ (b, \text{label}, \text{description}) \mid b \in \mathcal{B}_\text{icon} \cup \mathcal{B}_\text{text} \}
% \]
% where each element \( (b, \text{label}, \text{description}) \) consists of:
% \begin{itemize}
%     \item \textbf{Bounding box} \( b \): the location of the icon or text element,
%     \item \textbf{Label} \( \in \{ \text{Paired }, \text{Standalone}, \text{Picture}, \text{Actionable}\} \): 
%     classification based on spatial and semantic relationships,
%     \item \textbf{Description}: spatially aware functionality descriptor for each element.
% \end{itemize}

% SEED classifies each local element (text and icon) as either paired or standalone based on their spatial and semantic features. Paired elements consist of an icon and its associated textual label, while standalone elements are either isolated icons or independent text elements. For paired elements, SEED utilizes the descriptors of the associated elements (i.e., the icon and its neighboring text) to generate a unified functionality description. This process leverages both visual cues from the icon and the textual context from the nearby text element to produce a detailed and accurate description of the icon's function. For standalone icons, SEED relies solely on the visual cues from the icon itself to generate the functionality description. Additionally, text elements are classified as actionable if they are associated with interactive components such as input fields, search bars, or buttons and Icon bboxes that are part of images embedded in the UI are classified as picture. 


% #Ideas: 
% 1. Identifying Key UI elements is important for screen understanding and grounding tasks, using SoM prompting technique.
% 2. Some methods train detectors problems: cannot identify new unseen icons and will not generailze well.  
% 2. UI componets have an inherently hierarichal nature, a model that is aware of the semantic and spatial nature of the hierarchy wil perform better on screen understanding GUI-referring and grounding tasks. a Hierarchical Layout
% Tree. Based on the tree, our ToL agent not only
% comprehends the content of the indicated area but
% also articulates the layout and spatial relationships
% between elements. Such layout information is crucial for accurately interpreting information on the
% screen, distinguishing our ToL agent from other
% screen reading tools.
% We propose a training free method that utiulizes semantic SAM and OCR to construct a hierarichal Screen tool, that generalizes to all types of UI. 
% We generate bboxes from SAM and classify them Global Region of Interest (GROI) canidates or Icon candidates and text boxes from OCR. This obviously has a lot of false positives. 

% We proposed 2 step filteration algorithm 
% 1. to parse Text bboxes and Icon bboxes togetether and remove overlaps b/w them.
% It first filters out OCR text boxes that match unwanted characters or words, then removes redundant icon boxes that overlap heavily with text boxes. It further refines icons by removing nested or redundant boxes and finally filters out redundant button boxes using aspect ratio and overlap checks. The filtered lists of button, icon, and text bboxes, along with the text, are returned.
% 2. Modifed Non Max supression algorithm based on our Novel Information score metric to remove  redundant and useless GROIs.


\section{Experiments}

\subsection{ScreenSpot and VisualWebBench}
\label{subsec:screenspot_vwb}

\begin{table*}[ht]
\small
  \centering
  \begin{tabular}{@{}lcccccccc@{}}
    \toprule
    \textbf{Method} & \multicolumn{2}{c}{\textbf{Mobile (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Desktop (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Web (ScreenSpot)}} & \textbf{ScreenSpot} & \textbf{VisualWebbench} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
     & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Overall} & \textbf{Overall} \\
    
    \midrule
    \multicolumn{9}{l}{\textbf{Training Based}} \\
    SeeClick & 78.0 & 52.2 & 72.2 & 30.0 & 55.7 & 32.5 & 53.4 & 31.0 \\
    CogAgent & 67.0 & 24.0 & 74.2 & 20.0 & 70.4 & 28.6 & 47.4 & 59.0 \\
    OmniParser (GPT-4V) & 90.1 & 54.1 & 88.6 &  60.0 & 73.4 & 27.1 & 66.9 & 58.3 \\
    OmniParser$^*$(GPT-4V)  & 92.1 & 55.2 & 90.1 &  61.1 & 77.4 & 30.1 & 69.5 & 63.1 \\
    OmniParser (GPT-4o) & \textbf{93.9} & \textbf{57.0} & \textbf{91.3} & \textbf{63.6} & \textbf{\textit{81.3}} & \textbf{51.0} & \textbf{72.6} & \textbf{68.9} \\
    OmniParser$^*$(GPT-4o)& \textbf{\textit{94.8}} & \textbf{\textit{66.3}} & \textbf{\textit{95.4}} & \textbf{\textit{64.2}} & \textbf{80.8} & \textbf{32.0} & \textbf{\textit{73.7}} & \textbf{\textit{69.9}} \\

    \midrule
    \multicolumn{9}{l}{\textbf{Training Free}} \\
   
    GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 & 16.2 & 6.0 \\
    GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 & 18.2 & 6.7 \\
   
    TRISHUL$^\dagger$ (GPT-4V) & 75.8 & 38.4 & 66.3 & 25.4 & 69.5 & 31.2 & 53.4 & 56.3 \\
    TRISHUL$^*$ (GPT-4V) & 88.6 & 37.9 & 82.9 & 23.5 & 72.6 & 29.1 & 59.0 & 58.1 \\
    TRISHUL$^{*\dagger}$ (GPT-4V) & 86.0 & {43.7} & 77.3 & {32.8} & {75.2} & {40.8} & {61.9} & \textbf{{68.0}} \\
   
    TRISHUL$^\dagger$ (GPT-4o) & 92.1 & \textbf{63.4} & 83.7 & 38.2 & 80.2 & \textbf{42.1} & 69.3 & 60.2 \\
    TRISHUL$^*$ (GPT-4o) & \textbf{92.7} & 62.0 & \textbf{\textit{90.2}} & \textbf{39.2} & \textbf{\textit{84.8}} & 40.8 & \textbf{71.1} & 62.1 \\
    TRISHUL$^{*\dagger}$ (GPT-4o) & \textbf{\textit{93.8}} & \textbf{\textit{64.6}} & \textbf{85.6} & \textbf{\textit{45.7}} & \textbf{83.5} & \textbf{\textit{44.7}} & \textbf{\textit{72.2}} & \textbf{\textit{68.0}} \\
   
    \bottomrule
  \end{tabular}
  \caption{Performance across platforms and methods on ScreenSpot (Mobile, Desktop, Web) and VisualWebbench datasets. 
  $^*$ denotes the usage of SEED module to improve the element functionality descriptors generated using OCR (for TRISHUL) / BLIPv2 (for OmniParser). 
  $^\dagger$ represents GROI-based action grounding instead of using the full image. 
  $^{*\dagger}$ represents our proposed end-to-end framework for action grounding that uses GROIs and SEED descriptors. 
  Refer to Sec. \ref{subsec:screenspot_vwb} for detailed discussion.}
  \label{tab:screenspot}
\end{table*}


\textbf{Dataset and Experiments}- We evaluate the action grounding capability of TRISHUL agent on the ScreenSpot \cite{Jurmu2008ScreenSpotMR} dataset. ScreenSpot consists of 610 interface screenshots from mobile (iOS, Android), desktop (macOS, Windows), and web platforms, paired with 1,276 task instructions corresponding to actionable GUI elements. Traditional training-based methods, which are often trained on datasets like Screenspot, tend to perform poorly on out-of-distribution samples such as those from VisualWebBench due to domain shift. Therefore, to assess the generalization capability of our approach, we also utilize the VisualWebBench \cite{Liu2024VisualWebBenchHF} dataset's action grounding subset, which consists of 103 pairs of images and their corresponding instruction. 

\textbf{Implementation Details:}  
The formulation of the action grounding tasks for the datasets used in our experiments is discussed in detail in Section \ref{sec:agent_framework}. The specific prompts employed for these tasks are provided in the Appendix (Figure \ref{fig:prompt_screenspot}).

Unfortunately, we were unable to replicate the results reported by OmniParser in their study on the ScreenSpot benchmark using the publicly available weights and codebase. In Table \ref{tab:screenspot}, we present the performance metrics for OmniParser as obtained from our own experiments on the ScreenSpot and VisualWebBench datasets. Due to the non-reproducibility of their results as observed above and limited resources, we were unable to verify their results on the AiTW and Mind2Web benchmarks hence we have chosen to exclude their results for these benchmarks from our analysis.
% Furthermore, since OmniParser has not been peer-reviewed or accepted as a formal publication, we have chosen to exclude their results for these benchmarks from our analysis.





\textbf{Evaluation and Results}: As shown in Table \ref{tab:screenspot}, the TRISHUL agent, when paired with LVLMs (GPT-4V \cite{GPT-4V} and GPT-4o \cite{gpt4o}), significantly outperforms the baseline GPT-4V and GPT-4o. Our approach also surpasses task-specific models such as SeeClick \cite{Cheng2024SeeClickHG} and CogAgent \cite{Hong2023CogAgentAV}, achieving an overall accuracy of 61.9\% with GPT-4V and 72.2\% with GPT-4o on the ScreenSpot benchmark. This performance exceeds SeeClick’s 53.4\%, CogAgents 47.4\% and closely rivals OmniParser’s 72.6\%. On VisualWebBench \cite{Liu2024VisualWebBenchHF}, unlike SeeClick, which suffers a sharp drop in accuracy on out-of-distribution data with 31\% accuracy, TRISHUL maintains strong generalization, achieving a robust 68.0\% accuracy with both GPT-4V and GPT-4o closely matching the performance of OmniParser which achieves 68.9\%.

We further present ablations in Table \ref{tab:screenspot} to assess the impact of the SEED module and GROI-based action grounding in TRISHUL. Removing SEED (TRISHUL$^\dagger$) results in a notable accuracy drop of 8.5\% for GPT-4V and 2.9\% for GPT-4o on ScreenSpot. Similarly, eliminating GROI-based action grounding (TRISHUL$^*$) reduces accuracy by 2.9\% for GPT-4V and 1.1\% for GPT-4o. These results highlight the critical role of these components in TRISHUL’s performance.  

Additionally, we demonstrate TRISHUL’s modularity by integrating its components into existing grounding pipelines. In Table \ref{tab:screenspot}, we show that augmenting OmniParser’s BLIPv2-derived icon descriptors—originally lacking local semantic context—with TRISHUL’s SEED module (OmniParser$^*$) yields the best performance among training-based methods.  

Our GROI-based action grounding proves particularly effective for web and desktop platforms, where hierarchical and content-dense GUIs benefit from structured decomposition. However, its impact is less pronounced in mobile interfaces, where regions have minimal semantic separation. Further details can be found in Appendix \ref{sec6.3}.  Lastly, we observe that GPT-4o outperforms GPT-4V significantly when paired with SEED, suggesting that improved reasoning capabilities in LVLMs enhance the accuracy of SEED-generated descriptions. 

% \begin{table*}[ht]
% \small
%   \centering
%   \begin{tabular}{@{}lccccccc@{}}
%     \toprule
%     \textbf{Method } & \multicolumn{2}{c}{\textbf{Mobile}} & \multicolumn{2}{c}{\textbf{Desktop}} & \multicolumn{2}{c}{\textbf{Web}} & \textbf{Overall} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      & \textbf{Text} & \textbf{Icon/Widget} & \textbf{Text} & \textbf{Icon/Widget} & \textbf{Text} & \textbf{Icon/Widget} & \\
%     \midrule
%     GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 & 16.2 \\
%     GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 & 18.2 \\
%     SeeClick & 78 & 52.2 & 72.2 & 30 & 55.7 & 32.5 & 53.4 \\
%     CogAgent & 67 & 24 & 74.2 & 20 & 70.4 & 28.6 & 47.4 \\
%     \midrule
%     TRISHUL (GPT-4V) & 73.6 & 36.2 & 64.9 & 23.6 & 68.3 & 30 & 52.2 \\
%     TRISHUL (GPT-4V) + SEED & \textbf{88.6} & 37.9 & \textbf{82.9} & 23.5 & 72.6 & 29.1 & 59 \\
%     TRISHUL (GPT-4V) + SEED + GROI & 86 & \textbf{43.7} & 77.3 & \textbf{32.8} & \textbf{75.2} & \textbf{40.8} & \textbf{61.9} \\
%     \midrule
%     TRISHUL (GPT-4o) & 91.9 & 65 & 82.5 & 37.8 & 79.5 & 41.7 & 69.3 \\
%     TRISHUL (GPT-4o) + SEED & 92.7 & 62 & \textbf{90.2} & 39.2 & 84.8 & 40.8 & 71.06 \\
%     TRISHUL (GPT-4o) + SEED + GROI & \textbf{93.8} & \textbf{64.6} & 85.6 & \textbf{45.7} & \textbf{83.5} & \textbf{44.7} & \textbf{72.2} \\
%     \bottomrule
%   \end{tabular}
%   \caption{Performance results across different platforms and methods on screenspot dataset.}
%   \label{tab:screenspot}
% \end{table*}
% \begin{table*}[ht]
% \small
%   \centering
%   \begin{tabular}{@{}lccccccc@{}}
%     \toprule
%     \textbf{Method } & \multicolumn{2}{c}{\textbf{Mobile}} & \multicolumn{2}{c}{\textbf{Desktop}} & \multicolumn{2}{c}{\textbf{Web}} & \textbf{Overall} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      & \textbf{Text} & \textbf{Icon/Widget} & \textbf{Text} & \textbf{Icon/Widget} & \textbf{Text} & \textbf{Icon/Widget} & \\
%     \midrule
%     GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 & 16.2 \\
%     GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 & 18.2 \\
%     SeeClick & 78 & 52.2 & 72.2 & 30 & 55.7 & 32.5 & 53.4 \\
%     CogAgent & 67 & 24 & 74.2 & 20 & 70.4 & 28.6 & 47.4 \\
%     \midrule
%     \multicolumn{8}{l}{\textbf{TRISHUL (GPT-4V)}} \\
%     \midrule
%     +LE & 75.8 & 38.4 & 66.3 & 25.4 & 69.5 & 31.2 & 53.6 \\
%     +LE +SEED & \textbf{88.6} & 37.9 & \textbf{82.9} & 23.5 & 72.6 & 29.1 & 59 \\
%     +LE +SEED +GROI & 86 & \textbf{43.7} & 77.3 & \textbf{32.8} & \textbf{75.2} & \textbf{40.8} & \textbf{61.9} \\
%     \midrule
%     \multicolumn{8}{l}{\textbf{TRISHUL (GPT-4o)}} \\
%     \midrule
    
%     +LE & 92.1 & 63.4 & 83.7 & 38.2 & 80.2 & 42.1 & 69.9 \\
%     +LE +SEED & 92.7 & 62 & \textbf{90.2} & 39.2 & 84.8 & 40.8 & 71.06 \\
%     +LE +SEED +GROI & \textbf{93.8} & \textbf{64.6} & 85.6 & \textbf{45.7} & \textbf{83.5} & \textbf{44.7} & \textbf{72.2} \\
%     \bottomrule
%   \end{tabular}
%   \caption{Performance results across different platforms and methods on the screenspot dataset.}
%   \label{tab:screenspot}
% \end{table*}
% \begin{table}[ht]
% \footnotesize
%   \centering
%   \begin{tabular}{@{}lcccccc@{}}
%     \toprule
%     \textbf{Method} & \multicolumn{2}{c}{\textbf{Mobile}} & \multicolumn{2}{c}{\textbf{Desktop}} & \multicolumn{2}{c}{\textbf{Web}} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} \\
%     \midrule
%     GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 \\
%     GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 \\
%     SeeClick & 78.0 & 52.2 & 72.2 & 30.0 & 55.7 & 32.5 \\
%     CogAgent & 67.0 & 24.0 & 74.2 & 20.0 & 70.4 & 28.6 \\
%     \midrule
%     \multicolumn{7}{l}{\textbf{TRISHUL (GPT-4V)}} \\
%     +LE & 75.8 & 38.4 & 66.3 & 25.4 & 69.5 & 31.2 \\
%     +LE+SEED & \textbf{88.6} & 37.9 & \textbf{82.9} & 23.5 & 72.6 & 29.1 \\
%     +LE+SEED+GROI & 86.0 & \textbf{43.7} & 77.3 & \textbf{32.8} & \textbf{75.2} & \textbf{40.8} \\
%     \midrule
%     \multicolumn{7}{l}{\textbf{TRISHUL (GPT-4o)}} \\
%     +LE & 92.1 & 63.4 & 83.7 & 38.2 & 80.2 & 42.1 \\
%     +LE+SEED & 92.7 & 62.0 & \textbf{90.2} & 39.2 & 84.8 & 40.8 \\
%     +LE+SEED+GROI & \textbf{93.8} & \textbf{64.6} & 85.6 & \textbf{45.7} & \textbf{83.5} & \textbf{44.7} \\
%     \bottomrule
%   \end{tabular}
%   \caption{Performance across platforms and methods on ScreenSpot dataset. Icon/widget: Icon/Widget. LE stands for Local Element detection, SEED is short for Spatially Enhanced Element Description, and GROI is short for Global Region of Interest }
%   \label{tab:screenspot}
% \end{table}
% \begin{table*}[ht]
% \small
%   \centering
%   \begin{tabular}{@{}lcccccccc@{}}
%     \toprule
%     \textbf{Method} & \multicolumn{2}{c}{\textbf{Mobile (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Desktop (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Web (ScreenSpot)}} & \textbf{ScreenSpot} & \textbf{VisualWebbench} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Overall} & \textbf{Overall} \\
%     \midrule
%     GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 & 16.2 & 6.0 \\
%     GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 & 18.2 & 6.7 \\
%     SeeClick & 78.0 & 52.2 & 72.2 & 30.0 & 55.7 & 32.5 & 53.4 & 31.0 \\
%     CogAgent & 67.0 & 24.0 & 74.2 & 20.0 & 70.4 & 28.6 & 47.4 & 59.0 \\
%     \midrule
%     \multicolumn{9}{l}{\textbf{TRISHUL (GPT-4V)}} \\
%     +LE & 75.8 & 38.4 & 66.3 & 25.4 & 69.5 & 31.2 & 53.4 & 56.3 \\
%     +LE+SEED & \textbf{88.6} & 37.9 & \textbf{82.9} & 23.5 & 72.6 & 29.1 & 59.0 & 56.3 \\
%     +LE+SEED+GROI & 86.0 & \textbf{43.7} & 77.3 & \textbf{32.8} & \textbf{75.2} & \textbf{40.8} & \textbf{61.9} & \textbf{68.0} \\
%     \midrule
%     \multicolumn{9}{l}{\textbf{TRISHUL (GPT-4o)}} \\
%     +LE & 92.1 & 63.4 & 83.7 & 38.2 & 80.2 & 42.1 & 69.3 & 60.2 \\
%     +LE+SEED & 92.7 & 62.0 & \textbf{90.2} & 39.2 & 84.8 & 40.8 & 71.1 & 62.1 \\
%     +LE+SEED+GROI & \textbf{93.8} & \textbf{64.6} & 85.6 & \textbf{45.7} & \textbf{83.5} & \textbf{44.7} & \textbf{72.2} & \textbf{68.0} \\
%     \bottomrule
%   \end{tabular}
%   \caption{Performance across platforms and methods on ScreenSpot (Mobile, Desktop, Web) and VisualWebbench datasets. LE stands for Local Element (detected using our HSP module), SEED is short for Spatially Enhanced Element Description, and GROI is short for Global Region of Interest.}
%   \label{tab:screenspot}
% \end{table*}

% \begin{table*}[ht]
% \small
%   \centering
%   \begin{tabular}{@{}lcccccccc@{}}
%     \toprule
%     \textbf{Method} & \multicolumn{2}{c}{\textbf{Mobile (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Desktop (ScreenSpot)}} & \multicolumn{2}{c}{\textbf{Web (ScreenSpot)}} & \textbf{ScreenSpot} & \textbf{VisualWebbench} \\
%     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%      & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Text} & \textbf{Icon/widget} & \textbf{Overall} & \textbf{Overall} \\
    
    
%     \midrule
%     \multicolumn{9}{l}{\textbf{Training Based}} \\
%     SeeClick & 78.0 & 52.2 & 72.2 & 30.0 & 55.7 & 32.5 & 53.4 & 31.0 \\
%     CogAgent & 67.0 & 24.0 & 74.2 & 20.0 & 70.4 & 28.6 & 47.4 & 59.0 \\
%     OmniParser (GPT-4V) & 90.1 & 54.1 & 88.6 &  60.0 & 73.4 & 27.1 & 66.9 & 58.3 \\
%     OmniParser$^*$(GPT-4V)  & 92.1 & 55.2 & 90.1 &  61.1 & 77.4 & 30.1 & 69.5 & 63.1 \\
%     OmniParser (GPT-4o) & \textbf{93.9} & \textbf{57.0} & \textbf{91.3} & \textbf{63.6} & \textbf{81.3} & \textbf{51.0} & \textbf{72.6} & \textbf{68.9} \\
%     OmniParser$^*$(GPT-4o)& \textbf{94.8} & \textbf{66.3} & \textbf{95.4} & \textbf{64.2} & \textbf{80.8} & \textbf{32.0} & \textbf{73.7} & \textbf{69.9} \\

%     \midrule
%     \multicolumn{9}{l}{\textbf{Training Free}} \\
   
%     GPT-4V & 22.6 & 24.5 & 20.2 & 11.8 & 9.2 & 8.8 & 16.2 & 6.0 \\
%     GPT-4o & 20.2 & 24.9 & 21.1 & 23.6 & 12.2 & 7.8 & 18.2 & 6.7 \\
   
%     TRISHUL$^\dagger$ (GPT-4V) & 75.8 & 38.4 & 66.3 & 25.4 & 69.5 & 31.2 & 53.4 & 56.3 \\
%     TRISHUL$^*$(GPT-4V)& 88.6 & 37.9 & 82.9 & 23.5 & 72.6 & 29.1 & 59.0 & 58.1 \\
%     TRISHUL$^*^\dagger$ (GPT-4V)& 86.0 & {43.7} & 77.3 & {32.8} & {75.2} & {40.8} & {61.9} & {68.0} \\
   
  
   
    
    
%     TRISHUL$^\dagger$ (GPT-4o) & 92.1 & 63.4 & 83.7 & 38.2 & 80.2 & 42.1 & 69.3 & 60.2 \\
%     TRISHUL$^*$ (GPT-4o)& 92.7 & 62.0 & \textbf{90.2} & 39.2 & 84.8 & 40.8 & 71.1 & 62.1 \\
%     TRISHUL$^*^\dagger$ (GPT-4o)& \textbf{93.8} & \textbf{64.6} & \textbf{85.6} & \textbf{45.7} & \textbf{83.5} & \textbf{44.7} & \textbf{72.2} & \textbf{68.0} \\
   
     
%     \bottomrule
%   \end{tabular}
%   \caption{Performance across platforms and methods on ScreenSpot (Mobile, Desktop, Web) and VisualWebbench datasets. $^*$ denotes the usage of SEED module to improve the element functionality descriptors generated using OCR / BLIPv2 (for OmniParser). $^\dagger$ reporesents GROI based action grounding on a reduced context window instead of using the full image.}
%   \label{tab:screenspot}
% \end{table*}




\begin{table*}[ht]
  \centering
  
  \begin{tabular}{@{}lccccccc@{}}
    \toprule
    \textbf{Method} & \textbf{General} & \textbf{Install} & \textbf{GoogleApps} & \textbf{Single} & \textbf{WebShopping} & \textbf{Overall} \\
    \midrule
    ChatGPT-CoT & 5.9 & 4.4 & 10.5 & 9.4 & 8.4 & 7.7 \\
    Palm2-CoT & - & - & - & - & - & 39.6 \\    GPT-4V + Image & 41.7 & 42.6 & 49.8 & 72.8 & 45.7 & 50.5 \\
    MM-Navigator (GPT-4V) & 43 & 49.2 & 46.1 & \textbf{\textit{78.3}} & 48.2 & 53.0 \\
    MM-Navigator (GPT-4o) & \textbf{\textit{55.8}} & 58.2 & 48.2 & 76.9 & 52.1 & 57.8 \\
    SeeClick (Qwen-VL) & \textbf{54.0} & \textbf{\textit{66.4}} & \textbf{54.9} & 63.5 & \textbf{\textit{57.6}} & \textbf{59.3}\\
    \midrule
    TRISHUL (GPT-4V) & 47.5 & 50.7 & 50.7 & 66.7 & 49.5 & 54.5 \\
    TRISHUL (GPT-4o) & 52.9 & \textbf{60.7} & \textbf{\textit{55.0}} & \textbf{78.2} & \textbf{52.6} & \textbf{\textit{60.0}} \\
    \bottomrule
  \end{tabular}
  \caption{Results on the different categories on the AITW dataset. TRISHUL (GPT-4V) outperforms all prior GPT-4V  baselines that use IconNet's  element detections. TRISHUL (GPT-4o) outperforms TRISHUL (GPT-4V) by 5.55\% achieving State of the Art performance.}
  \label{tab:AITW}
\end{table*}


\begin{table*}[ht]
\centering
\small
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Methods} & \textbf{Modality} & \multicolumn{3}{c}{\textbf{Cross-Website}} & \multicolumn{3}{c}{\textbf{Cross-Domain}} & \multicolumn{3}{c}{\textbf{Cross-Task}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
& & \textbf{Ele.Acc} & \textbf{Op.F1} & \textbf{Step SR} & \textbf{Ele.Acc} & \textbf{Op.F1} & \textbf{Step SR} & \textbf{Ele.Acc} & \textbf{Op.F1} & \textbf{Step SR} \\
\midrule
MindAct (gen) & HTML & 13.9 & 44.7 & 11.0 & 14.2 & 44.7 & 11.9 & 14.2 & 44.7 & 11.9 \\
MindAct & HTML & 42.0 & 65.2 & 38.9 & 42.1 & 66.5 & 39.6 & 42.1 & 66.5 & 39.6 \\
GPT-3.5-Turbo & HTML & 19.3 & 48.8 & 16.2 & 21.6 & 52.8 & 18.6 & 21.6 & 52.8 & 18.6 \\
GPT-4 & HTML & 35.8 & 51.1 & 30.1 & 37.1 & 46.5 & 26.4 & 41.6 & 60.6 & 36.2 \\
GPT-4V+Text & HTML, Image & 38.0 & 67.8 & 32.4 & 42.4 & 69.3 & 36.8 & 46.4 & 73.4 & 40.2 \\
\midrule
GPT-4V+SOM & Image & - & - & 32.7 & - & - & 23.7 & - & - & 20.3 \\
CogAgent & Image &18.4 & 42.2 & 13.4 & 20.6 & 42.0 & 15.5 & 22.4 & 53.0 & 17.6 \\
Qwen-VL & Image & 13.2 & \textbf{83.5} & 9.2 & 14.1 & 84.3 & 12.0 & 14.1 & 84.3 & 12.0 \\
SeeClick & Image & 21.4 & 80.6 & 16.4 & 23.2 & \textbf{84.8} & 20.8 & 28.3 & \textbf{87.0} & 25.5 \\
\midrule
TRISHUL (GPT-4V) & Image & 33.91 & 74.33 & 27.98 & 36.49 & 76.60 & 31.71 & 34.04 & 71.88 & 29.76 \\ 
TRISHUL (GPT-4o) & Image & \textbf{31.43} & 81.52 & \textbf{24.53} & \textbf{37.12} & 82.96 & \textbf{32} & \textbf{37.58} & 83.78 & \textbf{32.52} \\ 
\bottomrule
\end{tabular}

\caption{Results for Cross-Website, Cross-Domain, and Cross-Task scenarios with Element Accuracy, Operational F1, and Step Success Rate metrics on the Mind2Web benchmark. TRISHUL (GPT-4o) consistently gives better Element Accuracy and Step Success Rate in all three scenarios on Image modality, its performance
trails state-of-the-art HTML-based method like MindAct}
\label{tab:mind2web}
\end{table*}


\begin{table}[ht]
  \centering
  \small % Reduce font size
  \setlength{\tabcolsep}{2pt} % Reduce horizontal padding
  \begin{tabular}{@{}llcccc@{}}
    \toprule
    \textbf{LVLM} & \textbf{Method} & \textbf{Desc. Acc.} & \textbf{Cont. Acc.} & \textbf{BERT} & \textbf{ROUGE} \\
    \midrule
    \multirow{3}{*}{GPT-4V} & Baseline & 8 & 0.92 & 0.7130 & 0.1462 \\
                            & ToL & 31.84 & 14.24 & \textbf{0.7230} & 0.1527 \\
                            & \textbf{TRISHUL} & \textbf{32.64} & \textbf{17.07} & 0.7220 & \textbf{0.1534} \\
    \midrule
    \multirow{3}{*}{Claude-3.5} & Baseline & 16.04 & 7.43 & 0.7274 & 0.1134 \\
                            & ToL & 60.56 & 43.02 & 0.7306 & 0.1462 \\
                            & \textbf{TRISHUL} & \textbf{60.91} & \textbf{49.74} & \textbf{0.7336} & \textbf{0.1495} \\
    \midrule
    \multirow{3}{*}{GPT-4o} & Baseline & 18.82 & 5.64 & 0.6948 & 0.1843 \\
                            & ToL & 71.30 & 42.46 & 0.7147 & 0.1869 \\
                            & \textbf{TRISHUL} & \textbf{71.58} & \textbf{43.59} & \textbf{0.7151} & \textbf{0.1871} \\
    \bottomrule
  \end{tabular}
  \caption{Evaluation of description and content accuracy, BERT score, and ROUGE-L score across different methods on the Screen Point-and-Read benchmark. Desc. Acc. - Description Accuracy, Cont. Acc. - Content Accuracy}
  \label{tab:screenpr}
\end{table}





% \begin{table}[h]
%     \centering
%     \small
%     \setlength{\tabcolsep}{2 pt}
%     \begin{tabular}{@{\hspace{15 pt}}l|ccc|ccc|ccc@{\hspace{ 15 pt}}}
%     \hline
%     {\textbf{Method} ($\rightarrow$)}& \multicolumn{3}{c|}{RawNeRF~\cite{rawnerf}} & \multicolumn{3}{c|}{Raw3DGS} & \multicolumn{3}{c}{\textbf{HDRSplat}} \\
%     \cline{2-10}
%     {\textbf{Scene} ($\downarrow$)}& PSNR & SSIM & LPIPS & PSNR & SSIM & LPIPS & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ \\
%     \hline\hline
%     Bikes  & \cellcolor{black!35}\textbf{29.29} &  \cellcolor{black!25}0.76 &  \cellcolor{black!35}\textbf{0.29} & \cellcolor{black!10}27.10 &  \cellcolor{black!25}0.65 & \cellcolor{black!10}0.38 & \cellcolor{black!25}28.49 & \cellcolor{black!35}\textbf{0.80} & \cellcolor{black!35}\textbf{0.29} \\
%     Stove  & \cellcolor{black!25}29.42 &  \cellcolor{black!35}0.92 & \cellcolor{black!35}0.11 & \cellcolor{black!10}27.72 &  \cellcolor{black!25}0.82 & \cellcolor{black!10}0.13 & \cellcolor{black!35}\textbf{31.42} & \cellcolor{black!35}\textbf{0.94} & \cellcolor{black!35}\textbf{0.09} \\
%     Parkstatue & \cellcolor{black!10}29.00 &  \cellcolor{black!25}0.79 & \cellcolor{black!25}0.23 & \cellcolor{black!25}29.07 &  \cellcolor{black!25}0.73 & \cellcolor{black!10}0.28 & \cellcolor{black!35}\textbf{30.75} & \cellcolor{black!35}\textbf{0.85} & \cellcolor{black!25}0.21 \\ 
%     Sharpshadow   & \cellcolor{black!35}\textbf{27.94} &  \cellcolor{black!35}\textbf{0.80} & \cellcolor{black!35}\textbf{0.21} & \cellcolor{black!10}25.75 &  \cellcolor{black!25}0.74 & \cellcolor{black!10}0.29 & \cellcolor{black!25}26.72 & \cellcolor{black!25}0.77 & \cellcolor{black!25}0.26 \\ 
%     Candlefiat  &  \cellcolor{black!25}31.19 & \cellcolor{black!35}\textbf{0.86} & \cellcolor{black!25}0.28 & \cellcolor{black!10}27.48 &  \cellcolor{black!25}0.80 & \cellcolor{black!10}0.29 & \cellcolor{black!35}\textbf{31.96} & \cellcolor{black!35}\textbf{0.86} & \cellcolor{black!25}0.24 \\ 
%     Notchbush  & \cellcolor{black!25}27.87 &  \cellcolor{black!35}\textbf{0.76} & \cellcolor{black!10}0.40 & \cellcolor{black!10}27.58 &  \cellcolor{black!25}0.72 & \cellcolor{black!10}0.41 & \cellcolor{black!35}\textbf{29.18} & \cellcolor{black!25}0.75 & \cellcolor{black!25}0.36 \\ 
%     Nightstreet   & \cellcolor{black!10}28.07 &  \cellcolor{black!25}0.75 & \cellcolor{black!25}0.20 & \cellcolor{black!25}28.12 &  \cellcolor{black!25}0.75 & \cellcolor{black!10}0.25 & \cellcolor{black!35}\textbf{31.50} & \cellcolor{black!35}\textbf{0.87} &\cellcolor{black!25}0.20 \\  
%     Morningkitchen  & \cellcolor{black!25}29.55 &  \cellcolor{black!25}0.77 & \cellcolor{black!10}0.28 & \cellcolor{black!10}28.77 &  \cellcolor{black!25}0.77 & \cellcolor{black!10}0.30 & \cellcolor{black!35}\textbf{29.90} & \cellcolor{black!35}\textbf{0.83} & \cellcolor{black!25}0.26 \\  
%     Livingroom  & \cellcolor{black!35}\textbf{30.61} &  \cellcolor{black!35}\textbf{0.87} &\cellcolor{black!25} 0.19 & \cellcolor{black!25}29.39 &  \cellcolor{black!25}0.85 & \cellcolor{black!10}0.22 & \cellcolor{black!10}29.12 & \cellcolor{black!25}0.86 & \cellcolor{black!25}0.18 \\  
%     Gardenlights  & \cellcolor{black!35}\textbf{24.43} &  \cellcolor{black!35}\textbf{0.53} & \cellcolor{black!10}0.46 & \cellcolor{black!10}23.08 &  \cellcolor{black!10}0.51 & \cellcolor{black!10}0.46 & \cellcolor{black!10}24.35 & \cellcolor{black!35}\textbf{0.53} & \cellcolor{black!35}\textbf{0.42} \\  
%     Scooter   & \cellcolor{black!25}35.10 &  \cellcolor{black!35}\textbf{0.88} & \cellcolor{black!25}0.28 & \cellcolor{black!25}33.22 &  \cellcolor{black!25}0.79 & \cellcolor{black!10}0.33 & \cellcolor{black!35}\textbf{35.36} & \cellcolor{black!35}\textbf{0.88} & \cellcolor{black!35}\textbf{0.26} \\  
%     Streetcorner  & \cellcolor{black!25}31.79 & \cellcolor{black!35}\textbf{0.84} & \cellcolor{black!25}0.24 & \cellcolor{black!10}27.87 & \cellcolor{black!25}0.73 & \cellcolor{black!10}0.30 & \cellcolor{black!35}\textbf{32.16} & \cellcolor{black!35}\textbf{0.84} & \cellcolor{black!35}\textbf{0.23} \\  
%     \hline \hline
%     \textbf{Average} & \textbf{29.52} & \textbf{0.79} & \textbf{0.26} & \textbf{27.92}  & \textbf{0.73}  & \textbf{0.30} &  \textbf{30.07} & \textbf{0.82} & \textbf{0.25} \\  
%     \hline 
%     \end{tabular}
%     % \captionsetup{skip=3pt, justification=justified}
%     \caption{\textbf{Quantitative Comparison} of all models rendering in the 14-bit linear HDR space. We demonstrate superior fidelity of our model over RawNeRF (4\% LPIPS($\downarrow$), 4\% SSIM($\uparrow$), 2\% PSNR($\uparrow$)) and our baseline Raw3DGS (17\% LPIPS($\downarrow$), 12\% SSIM($\uparrow$), 8\% PSNR($\uparrow$)).}
%     \label{tab:quant_comp}
% \end{table}

\subsection{AITW}

\textbf{Dataset and Experiments} To evaluate TRISHUL on the mobile navigation benchmark AITW\cite{rawles2023androidwildlargescaledataset}, which consists of 30,000 instructions and 715,000 trajectories, we use the same train/test split as defined in \cite{Cheng2024SeeClickHG}. This split retains only one trajectory per instruction, ensuring no overlap between the train and test sets.

\textbf{Implementation details}- We adopt a similar prompt format to that used in MM-Navigator \cite{Yan2023GPT4VIW}, where we label the detected elements on the screen using SoM prompting and present the model with the annotated image and the clean image. However, we replace IconDet's bounding boxes (as used in MM-Navigator)  with local element boxes generated from our Hierarchal Screen Parsing method, and also provide our spatially enhanced element descriptions (Section \ref{sec:SEED}) for all the local elements in our input prompt. The exact prompt is mentioned in the Appendix in Figure \ref{fig:prompt_aitw}

\textbf{Evaluation and Results} In Table \ref{tab:AITW}, we report the baselines as presented in MM-Navigator\cite{Yan2023GPT4VIW}. The best performing baseline incorporates action history and uses only image modality for navigation. MM-Navigator presents baselines with GPT-4V only, we also run MM-navigator's best configuration (Image+History) with GPT-4o to contrast it with TRISHUL's GPT-4o performance.
We observe that TRISHUL with GPT-4V outperforms all prior GPT-4V-based baselines, achieving an overall accuracy of 54.5\%. With GPT-4o model, TRISHUL achieves an average accuracy of 60\%, surpassing MM-Navigator's GPT-4o baseline by over 2.2\% to become the state of the art. 

\subsection{Mind2Web}
\textbf{Dataset and Experiments}- 
To test on the web-navigation task we use the Mind2Web \cite{Deng2023Mind2WebTA} dataset. The test set consists of three different categories - Cross Task, Cross Website, and Cross Domain having 252, 177, and 912 tasks respectively.  


\textbf{Implementation details} -  We use the pre-processed test set provided by \cite{Yan2023GPT4VIW}. During inference, we feed the detected local elements outputs from our Hierarchical Screen Parsing (HSP) module along with the clean image. Additionally, our input prompts are augmented with the descriptions of local elements from our SEED module. The prompt is mentioned in the Appendix in Figure \ref{fig:prompt_mind2web}


\textbf{Evaluation and Results} - The results are presented in Table \ref{tab:mind2web} where we compare multiple baselines across two modalities HTML and image. GPT-4V+SoM and GPT-4V+Text correspond to SeeAct \cite{zheng2023seeact} with image annotations and text choice grounding methods respectively. Without using any parsed HTML information, TRISHUL is able to outperform all the approaches relying on only GUI screenshots in almost every sub-category. Compared to other baselines we surpass them in Element accuracy and Step success rate, while remaining competitive in Operational F1. This indicates that the local elements detected by our HSP module and SEED descriptions provide highly valuable information for web navigation tasks. Although we provide better Operational F1 than HTML-based methods, we still falter when it comes to element accuracy and step success rate as predicting bounding boxes is a more complex task than selecting HTML elements.




\subsection{Screen Point-and-Read}
\label{sec:screenpnr}

\textbf{Dataset and experiments}- We use the Screen Point and Read\cite{fan2024readpointedlayoutawaregui} benchmark to evaluate TRISHUL's performance on the GUI referring task. It evaluates the accuracy of the generated content description \( \hat{D}_{\text{c}} \) and layout description \( \hat{D}_{\text{l}} \) for the region marked by the user over the interface. This benchmark comprises of 650 screenshots across three domains: web, mobile, and operating systems. To validate our method, we run experiments using GPT-4o \cite{gpt4o}, GPT-4V \cite{GPT-4V}, and Claude-3.5-Sonnet \cite{claude3.5}, enabling us to examine performance across multiple LVLMs.


\textbf{Evaluation and Results} - To assess the quality of the generated content description and layout description we employ the cycle consistency evaluation following the screen point-and-read \cite{fan2024readpointedlayoutawaregui} paper. The agent outputs (\( \hat{D}_{\text{c}} \) , \( \hat{D}_{\text{l}} \)) are fed into an auxiliary model, which is asked to complete a downstream task, with its performance indicating description quality. We benchmark our approach against baseline GPT-4o, Claude, and the ToL agent from Screen point-and-read, using GPT-4o, GPT-4V , and Claude-3.5-Sonnet as the primary models. We also compute language similarity metrics like BERT \cite{Zhang2019BERTScoreET} score and ROUGE-L \cite{Rouge} to evaluate alignment with human-verified ground truth.

To further validate quality, we conduct two rounds of human evaluation: the first compares our approach against baseline GPT-4o, while the second compares our approach with the ToL agent, both using GPT-4o as the primary LLM. We employ 10 human annotators from \cite{Indika} and ask them to choose between the description generated by our approach and the alternative approach. Each evaluator is presented with the labeled image and asked a single question \textit{“Given the image with the labeled point, which description do you prefer?”}.The majority vote is used to select the preferred description. To ensure unbiased evaluation the annotators are unaware of which model generates which descriptions. The annotators are compensated at minimum wage. 

% TRISHUL consistently outperforms both the baseline and the ToL agent across all evaluation metrics for GPT-4V, Claude, and GPT-4o models, despite being zero-shot, as can be observed from Table \ref{tab:screenpr}. Additionally, TRISHUL achieves the highest Bert Score and Rouge Score. The human evaluation results, presented in Figure \ref{fig:human_eval} further prove the efficacy of our approach. TRISHUL-generated descriptions are preferred by human annotators 73\% of the time compared to GPT-4o and 62.8\% of the time over ToL agent. TRISHUL generated description tie with GPT-4o  0.9\% of the time, and ties with ToL agent 0.6\% of the time. 
TRISHUL consistently outperforms both the baseline and the ToL agent across all evaluation metrics for GPT-4V, Claude, and GPT-4o models (Table \ref{tab:screenpr}). Human evaluation results (Figure \ref{fig:human_eval}) further validate TRISHUL's efficacy, with descriptions generated by TRISHUL being preferred by annotators 73\% of the time over GPT-4o and 62.8\% of the time over ToL. TRISHUL ties with GPT-4o 0.9\% of the times and with ToL agent 0.6\% of the times.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig4.png}
    \caption{Human evaluation results on ScreenPR benchmark. TRISHUL is preferred by human annotators 63\% of the time over ToL agent and 73\% of the time over baseline GPT-4o}
    \label{fig:human_eval}
\end{figure}

\begin{figure}
    \centering
    % Bottom row: One image centered
    
        \centering
        \includegraphics[width=\linewidth]{main_figs/fig5.png}
        \caption{Local Element Exhaustiveness Score for ScreenSpot, Visual WebBench, AITW and Mind2Web}
        \label{fig:LEE}
\end{figure}

\section{Discussion}


\begin{table}[h!]
\small
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{2}{*}{Benchmark} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Accuracy (\%)} \\
        \cmidrule(lr){3-5}
        & & Pass@1 & Pass@2 & Pass@3 \\
        \midrule
        \multirow{3}{*}{VisualWebBench} & GPT-4o & 68.0 & 81.6 & 83.5 \\
                                        & GPT-4V & 56.3 & 69.9 & 71.8 \\
                                        & SeeClick & 31.0 & 36.0 & 36.0 \\
        \midrule
        \multirow{3}{*}{ScreenSpot} & GPT-4o & 72.2 & 77.8 & 80.0 \\
                                     & GPT-4V & 59.0 & 67.2 & 70.6 \\
                                     & SeeClick & 55.0 & 55.0 & 59.0 \\
        \bottomrule
    \end{tabular}
    \caption{Pass@1, Pass@2, and Pass@3 Accuracy (\%) for VisualWebBench and ScreenSpot using GPT-4o, GPT-4V,(with the TRISHUL framework) and SeeClick models.}
    \label{tab:pass_accuracy}
\end{table}



\subsection{Analysis on sampling multiple candidates}
% Lately, the primary approach in building these GUI agents has been to avoid dependence on additional metadata and, in a human like manner, leverage only visual perception to perform GUI interactions. These agents are evaluated using pass@1 or top@1 metric for success rate or  calculation. However, humans often operates GUIs in exploratory manner especially when using a new or a content dense app/platform/device. Similarly, this effort \cite{koh2024treesearchlanguagemodel} highlights improvements on web related tasks by first sampling multiple potential trajectories to reduce uncertainty and then pruning those trajectories using a value model.

% ScreenPR work highlights that the ToL agent can also be used as a verification agent for Mobile Agent actions and report promising accuracies for identification of correct/incorrect action candidates. 

% Leveraging these insights, we examine the Pass@2 and Pass@3 accuracy on the action grounding task for ScreenSpot and VisualWebBench datasets, which reveal an average gain of >10\% across datasets and models. 
% But to successfully deploy the multi click agent with Pass@K accuracy we need to be able to track back to the current state if a wrong action is exected first. 
% We posit that similar to ToL agent TRISHUL can be deployed as an action verification agent to catch incorrect actions and allowing backtracking, which can lead to the deployment of multi click grounding agents with greatly imporved accuracies. 

% we conduct a study to assess the quality of output candidates generated by TRISHUL agent. Ideally, lesser the number of samples it takes to achieve maximum accuracy, easier it is for the verification model. 
% \subsection{Formalizing the Concept of ToL as a Verification Agent for GUI Agents}
 % In GUI-based interactions, agents often rely solely on visual perception, minimizing the need for additional metadata and aiming to mirror human exploratory behavior, particularly in unfamiliar or content-rich interfaces.
LVLM-based GUI agents that rely solely on visual perception aim to mirror human like interface interaction. Humans often explore multiple paths when interacting with novel/complicated GUIs. Traditional metrics like pass@1 (top@1), may not fully reflect an agent's success in tasks that benefit from exploration. Recent research \cite{koh2024treesearchlanguagemodel}, shows that sampling and evaluating multiple potential action paths, then filtering them with a value model, improves success rates by reducing decision uncertainty.

The ToL agent has proven effective as a verification layer for mobile agents \cite{fan2024readpointedlayoutawaregui}, accurately identifying correct and incorrect action paths. Leveraging this insight, we propose utilizing TRISHUL as a verification agent in a GUI agent system to enable multi-click grounding with enhanced accuracy. Our findings in Table \ref{tab:pass_accuracy} indicate that multi-sampling metrics like pass@2 and pass@3 improve grounding accuracy by over 10\% across models on tasks in the ScreenSpot and VisualWebBench datasets. Here, pass@k highlights top K action-grounding candidates generated by TRISHUL. 

% This result highlights that TRISHUL can be used for both generating and verifying actions.

% \subsubsection{Deploying TRISHUL for Action Verification and Backtracking}

% For multi-click agent systems to perform reliably with pass@K metrics, they must retain the ability to revert to a known state if an initial action path is incorrect. We posit that, similar to ToL, TRISHUL can be deployed as a verification agent to identify errors in action paths, allowing agents to backtrack and explore alternative trajectories. This verification and backtracking process enhances action accuracy and robustness, significantly improving performance in complex GUI environments by aligning with human exploratory behavior and ensuring high success rates in dynamic, unstructured tasks.
\subsection{Failure Analysis}

% In Figure \ref{fig: Local Element Exhaustivness} we report the evaluate the Local Element Exhaustiveness (LEE) metric for different datasets and their splits.
% The LEE score for an Image is binary 1 or 0, 1 if the midpoint of the GT bbox lies in any one of the bounding boxes for local elements detected by our HSP module. 
% It is trivial to observe that the LEE score is directly correlated to TRISHUL's performance on any dataset. Significantly low LEE score on the Min2Web dataset indicate that the lack of exhaustiveness in the local elements proposed by HSP is the main bottleneck to the TRISHUL's performance on web navigation tasks. 
In Figure \ref{fig:LEE}, we evaluate the Local Element Exhaustiveness (LEE) metric across various datasets and their splits. The LEE score for an image is binary: it is set to 1 if the midpoint of the ground truth (GT) bounding box falls within any bounding box of local elements detected by our Hierarchical Screen Parsing (HSP) module; otherwise, it is set to 0. Thus, the LEE score shows the bottleneck that happens in our pipeline after the LE detection stage.

The results show a clear correlation between LEE scores and TRISHUL's performance across datasets. In particular, the low LEE scores in the Mind2Web dataset highlight that the limited exhaustiveness of local elements detected by the HSP module is a key factor constraining TRISHUL's effectiveness in web navigation tasks.

\section{Conclusion}
% Our TRISHUL agent leverages LVLMs to achieve superior GUI grounding and referring performance across diverse datasets, outperforming specialized works and cross-modal baselines. By using GROIs and SEED descriptions, TRISHUL excels in content-dense and hierarchical GUI environments, particularly on desktop and web interfaces, while achieving competitive performance on mobile. Unlike HTML-reliant approaches, TRISHUL demonstrates strong generalization and robustness with only GUI screenshots, leading to marked gains in accuracy, especially in complex tasks like bounding box prediction. Human evaluation further validates TRISHUL's efficacy, with a significant preference for its outputs. Our approach sets a new benchmark for multi-modal GUI agents.


% In this paper, we introduced TRISHUL  a training free agentic framework which enables LVLMs to develop comprehensive GUI screen understanding by leveraging two key modules: HSP and SEED. HSP module effectively obtains and organizes pool of GUI elements into hierarchical structure of Global Regions of Interest (GROIs) and local elements. The SEED module helps the agent understand local GUI elements through spatial context aware reasoning. Extensive experiments across diverse benchmarks comprising of action grounding and GUI referring tasks on the ScreenSpot, VisualWebBench, AITW, Mind2Web, and ScreenPR datasets demonstrate that TRISHUL consistently outperforms all training free methods and paralles training based methods while maintaing superior cross task and cross platform generalizability. 
% In this paper, we introduced TRISHUL, a training-free agentic framework that enables LVLMs to achieve comprehensive GUI screen understanding by leveraging two key modules: HSP and SEED. The HSP module effectively organizes a  GUI elements into a multi-granular hierarchical structure, between Global Regions of Interest (GROIs) and local elements. Meanwhile, the SEED module enhances the agent's ability to interpret local GUI elements through spatial context-aware reasoning. Extensive experiments across diverse benchmarks—including action grounding and GUI referring tasks on the ScreenSpot, VisualWebBench, AITW, Mind2Web, and ScreenPR datasets—demonstrate that TRISHUL consistently outperforms all training-free methods while rivaling training-based approaches. Moreover, it maintains superior cross-task and cross-platform generalizability, highlighting its robustness and adaptability in real-world GUI understanding scenarios.
In this paper, we introduced TRISHUL, a training-free agentic framework that enables LVLMs to achieve comprehensive GUI screen understanding using two key modules: HSP and SEED. The HSP module organizes GUI elements into a multi-granular hierarchical structure, distinguishing Global Regions of Interest (GROIs) from local elements, while the SEED module enhances spatial context-aware reasoning. Experiments on ScreenSpot, VisualWebBench, AITW, Mind2Web, and ScreenPR demonstrate that TRISHUL outperforms all training-free methods and rivals training-based approaches while maintaining superior cross-task and cross-platform generalizability.














\section*{Impact Statement}

This work advances machine learning methods for comprehensive graphical user interface (GUI) comprehension, enabling more intuitive and automated interactions. A key positive impact lies in enhancing accessibility for visually challenged individuals through our GUI referring agent, helping them navigate digital environments more effectively. Potential ethical concerns primarily revolve around privacy and control, if such automation tools are misused. Overall, while this framework promises to streamline user experiences and empower those with visual impairments, continued vigilance is advised to safeguard responsible, transparent, and privacy-oriented deployment.


\bibliography{MAIN}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix.}
\label{sec:appendix}

\subsection{Model Specifications and Endpoints}
\label{app:subsection_label}
Since all our work leverages closed-source models like GPT-4V, GPT-4o, and Claude, we mention the model identifiers that we use for our API calls for clarity. For GPT-4V - "gpt-4-vision-preview", For GPT-4o - "gpt-4o-2024-08-06", and for Claude - "claude-3-5-sonnet-20241022". Unless otherwise noted, all experiments are conducted with a temperature setting of 0.0.

\subsection{Hierarchical Screen Parsing Details}
\label{app: HSP_details}

\subsubsection{IoS Score} Similar to IoU score we define an IoS score as:
\lstset{
  language=Python,
  backgroundcolor=\color{white},
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{green!60!black},
  stringstyle=\color{red},
  showstringspaces=false,
}
\begin{lstlisting}
def IoS(boxA, boxB):
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    ios = interArea / float(boxAArea + 1e-3)
    return ios
\end{lstlisting}
The IoS (Intersection over Size) score is a measure used to evaluate the overlap between two bounding boxes, typically in the context of object detection. It calculates the ratio of the intersection area between two boxes to the area of the first box. IoS(A, B) (also written as $IOS_{A}$), a score of 0.5 means 50\% of A intersects with B.

\subsubsection{Filtering Redundant Bounding boxes}
\label{sec:filtering}
The output of EasyOCR + SAM model combined is extremely cluttered (see \ref{fig:candidate_ex}) and contains numerous overlaps and false positive detections from both models. We deploy the following steps to parse the outputs of SAM and OCR (local elements as referred to in the main script) together. 
\begin{itemize}
   
    
    \item \textbf{Generate  GROI, Icon and Button Candidate Proposals} 
    Classify all SAM boxes based on $A_{thresh-GROI}$, $A_{thresh-icon}$, $A_{thresh-button}$. Let \( B \) represent the set of bounding boxes detected in the GUI. 

    Global Region of Interest (GROI) Candidates: The set of boxes with an area greater than the GROI threshold is given by:
       \[
       \text{GROI} = \{ b \in B \mid \text{Area}(b) > A_{thresh-GROI} \}
       \]
    
    Icon Candidates:The set of boxes with an area between the Button and Icon thresholds is defined as:
       \[
       \text{Icon} = \{ b \in B \mid A_{thresh-button} < \text{Area}(b) < A_{thresh-icon} \}
       \]
    
    Button Candidates: The set of boxes with an area less than the Button threshold is:
       \[
       \text{Button} = \{ b \in B \mid \text{Area}(b) < A_{thresh-button} \}
       \]

 
    
   \item \textbf{Remove False Positive Text Bounding Boxes:} 
    Remove text boxes using a predefined dictionary,  that are likely OCR mis-detections for icons. These texts usually contain  only special characters or short, meaningless words. If a word contains one of these characters and has a length of less than < 5 that text bbox is ignored.
    
    Characters/Words to ignore:
      
    \begin{itemize}
        \item \texttt{"@", "\#", "x", "?", "\{", "\}", "<", ">", "\&", "`", "\textasciitilde{}", "\\", "=", "C", "Q", "88", "83", "98", "15J", "\textasciicircum{}", "0e", "n", "E", "ya", "ch", "893"}
    \end{itemize}




    \item \textbf{Remove Icons Inside or Overlapping Text Bounding Boxes:} 
    Remove icon bounding boxes that are either inside or intersect with text boxes, as they are likely to be text misidentified as icons by SAM.

    \item \textbf{Filter Square-like Icon Bounding Boxes:} 
    Keep only icons that are roughly square-shaped, based on a specific aspect ratio range of [0.7, 1.3].

    \item \textbf{Remove Redundant Icon and Button Bounding Boxes:} 
    Remove icon bounding boxes that are redundant, i.e., those that are inside or significantly overlap with $ IoS > 0.6$ with other icons or text boxes.


  
\end{itemize}

\subsubsection{Non Max Suppression for GROIs}
\begin{itemize}
     \item \textbf{Reject  boxes with low Information score$S$} 
    If the current bounding box has an information  score $S < S_{thresh}$ it is rejected. $S_{thresh}$ is set to 10 for the ScreenPoint and Read task and 25 for action grounding task.
    
    \item \textbf{Reject Overlapping BBoxes:} 
    If the current bounding box intersects with a previously selected bounding box with a higher Infrormation score and $IoS_{current} > IoS_{overlap-thresh}$ it is rejected. $IoS_{overlap-thresh}$ thresh is set to 0.5 for visual grounding task and 0 for ScreenPR task.

    \item \textbf{Reject Contained BBoxes (Smaller GROIs Inside Larger):} 
    If the current bounding box is inside a previously selected bounding box with a higher Information Score and if $IoS_{current} > IoS_{inside-thresh}$ it is rejected. $IoS_{overlap-thresh}$ thresh is set to 0.5 for visual grounding task and 0 for ScreenPR task.

    \item \textbf{Reject Engulfing Bboxes (Larger GROIs Inside Smaller):} 
    If the current bounding box completely engulfs a bounding box with a higher Information Score then it is rejected.
    
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig6.png}
    \caption{Candidate bounding boxes generated from SAM + OCR to the left and the corresponding  HSP results (Icon + text + picture) to the right}
    \label{fig:candidate_ex}
\end{figure*}

\subsection{GROI analysis for ScreenSpot and VisualWebBench}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig7-spnr-p.png}
    \caption{Prompt for Screen Point-and-Read}
    \label{fig:prompt_screenpnr}
\end{figure*}


\begin{figure*}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{main_figs/fig8.png}
        \caption{Distribution of Number of GROIs per image for ScreenSpot and Visual WebBench}
        \label{fig:image1}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{main_figs/fig5.png}
        \caption{Distribution of Total GROI area / Image area for ScreenSpot and Visual WebBench}
        \label{fig:image2}
    \end{minipage}
\end{figure*}

\label{sec6.3}
We plot two additional statistics for the detected GROI's through our HSP block. In Figure \ref{fig:image1}
we plot the average number of GROIs per image, across the three different sub-categories of ScreenSpot and the full dataset of VisualWebBench. We observe that GUI screenshots from mobiles have the lowest average number of GROIs per image. This is due to the fact that mobile regions are not semantically coherent, therefore lesser number of GROIs are generated.

In Figure \ref{fig:image2} we plot the average of the total area covered by all GROIs in an image to the total area of the image. Mobile GUI screenshots have the least dense GROI coverage, due to the fact that we also detect fewer GROIs in mobile screenshots. These studies further validate the fact that GROIs are not as useful for mobile GUI's however they offer more benefit for PC and web based GUIs. 
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig10-gp-p.png}
    \caption{Prompt for instruction guided GROI Proposal generation}
    \label{fig:prompt_groi}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig11-seed-p.png}
    \caption{Prompt for SEED}
    \label{fig:prompt_seed}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig12-vgp.png}
    \caption{SoM grounding Prompt for ScreenSpot and VisualWebBench}
    \label{fig:prompt_screenspot}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig13-aitw-p.png}
    \caption{Agentic task following prompt for AITW}
    \label{fig:prompt_aitw}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{main_figs/fig14-mind2web-p.png}
    \caption{Agentic task following prompt for Mind2Web}
    \label{fig:prompt_mind2web}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
