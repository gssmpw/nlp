\section{Related Works \& Motivation}
Recent research has focused on developing agents that rely solely on visual perception to interact with GUIs in a human-like manner. These works on purely vision-based GUI agents using LVLMs have evolved along 2 main approaches:

\textbf{End to End Training based GUI Agents}: Multiple studies **Zhu, "Learning Visual Representations for GUI Navigation"**__**Li, "Visual Perception for GUI Interaction"** have trained LVLMs on GUI navigation tasks for various platforms/device-types. 

% These models are usually specialized for tasks such as grounding and for platforms such as web or mobile. 

% /A concurrent work to us, Omniparser **Zhu, Chen, "Omniparser: An Icon Detection Model"**__, trains an icon detection model. In addition to that, they also train BLIP **Gupta, et al., "BLIPv2: Visual Reasoning for GUI Interactions"**. 

\textbf{Test-time assistance with visual perception tools}: Studies have leveraged visual perceptions tools to assist generalist LVLMs like GPT-4V. MM-Navigator **Zhang, Zhang, "MM-Navigator: A Pre-trained Icon Detector Module"** leverages pre-trained icon detector module. A concurrent work to ours, Omniparser **Zhu, Chen, "Omniparser: An Icon Detection Model"**__, trains a YOLO-v8 **Redmon, et al., "YOLOv8: Object Detection for GUI Interactions"** based icon detection \& BLIPv2 **Gupta, et al., "BLIPv2: Visual Reasoning for GUI Interactions"** based icon captioner modules for action grounding. Tree-of-Lens (ToL) Agent **Li, et al., "Tree-of-Lens Agent: A Perception Module for GUI Referring Tasks"** trains a perception module for GUI referring task of generating region description based on user selected point. 

Multiple GUI navigation-related benchmarks **Zhang, Zhang, "GUI-Benchmark: A Benchmark for GUI Navigation Tasks"** and studies **Li, et al., "Studies on GUI Navigation for Generalist LVLMs"** have highlighted two major weaknesses among pure vision-based GUI navigation agents. Firstly, the performance of these methods trained on certain distribution of user interfaces don't generalize well across platforms/device types. Given the rapid pace with which new user interfaces are introduced every day, the generalizability of training based approaches to Out-Of-Distribution samples remains a challenge. Secondly, most of the GUI agents such as DigiRL **Zhu, et al., "DigiRL: A Generalist LVLM for GUI Interactions"**__, SeeClick **Li, et al., "SeeClick: A Visual Perception Model for GUI Interaction"**__ and MM-Navigator **Zhang, Zhang, "MM-Navigator: A Pre-trained Icon Detector Module"** are optimized for specialized GUI related tasks (majorly action prediction \& grounding), and often evaluate on diversely sourced but thematically similar tasks and metrics, hence they lack proper GUI comprehension capabilities across different tasks and interfaces.

% Our approach, TRISHUL, demonstrates how a multi-tiered global and local semantic understanding can greatly enhance comprehensive GUI understanding and navigation abilities of generalist LVLMs in a training free setup.
\begin{algorithm}
\caption{Hierarchical Screen Parsing}\label{alg:groi}
\scriptsize  % Reduce font size for the algorithm
\begin{algorithmic}[1]
    \Require Image $I$, $A_{\textit{thresh-GROI}}$, $A_{\textit{thresh-Icon}}$, $IOU_{\textit{thresh}}$, \textit{SAM}, \textit{OCR}
   
    \State Initialize: \textit{SAM}, \textit{OCR}, $A_{\textit{thresh}}$, $IOU_{\textit{thresh}}$
    \State Sample N points $\mathcal{P} \gets \mathcal{U}(0, W) \times \mathcal{U}(0, H)$ \Comment{Image Size ($W$, $H$)}
    \State $\mathcal{B} \gets \textit{SAM}(I, \mathcal{P}), \quad \mathcal{T} \gets \textit{OCR}(I)$ \Comment{SAM boxes $\mathcal{B}$ and OCR boxes $\mathcal{T}$}
    
    \State Initialize $\mathcal{G} \gets \emptyset, \mathcal{I} \gets \emptyset$ \Comment{GROI candidates and Icon candidates}
    
    \For{each $b \in \mathcal{B}$}
        \If{Area$(b) > A_{\textit{thresh-GROI}}$}
            \State $\mathcal{G} \gets \mathcal{G} \cup \{b\}$  \Comment{Add to GROI candidates}
        \EndIf
        \If{Area$(b) < A_{\textit{thresh-Icon}}$}
            \State $\mathcal{I} \gets \mathcal{I} \cup \{b\}$ \Comment{Add to Icon candidates}
        \EndIf
    \EndFor
    
    \State Initialize $\mathcal{S} \gets \emptyset$ \Comment{Information Scores for Non Max Suppression (NMS)}
    \State $\mathcal{I}_{\text{filtered}}, \mathcal{T}_{\text{filtered}} \gets$ \textit{Overlap Removal and Filtering}($\mathcal{I}, \mathcal{T}$)
    
    \For{each $b \in \mathcal{G}$}
        \State $\mathcal{N}_{\text{inside}} = |\{ \mathcal{T}_b^\text{inside} \}| + |\{ \mathcal{I}_b^\text{inside} \}|$ \Comment{Number of boxes  inside $b$}
        
        \State $\mathcal{N}_{\text{inter}}  = |\{ \mathcal{T}_b^\text{intersect} \}| + |\{ \mathcal{I}_b^\text{intersect} \}|$
        \Comment{Number of boxes intersecting $b$}
        
        \State $\mathcal{S} \gets \mathcal{S} \cup \left\{ 
        \frac{\mathcal{N}_{\text{inside}}}
        {\sqrt{1 + \mathcal{N}_{\text{inter}} \cdot \text{Area}(b)}} 
        \right\}$ \Comment{Information Score for $b$}    
    \EndFor

    \State $\mathcal{G}_{\text{filtered}} \gets$ \textit{NMS}($\mathcal{G}, \mathcal{S}, IOU_{\textit{thresh}}$) 
    \Comment{Apply NMS to get Filtered GROIs}

    \State \textbf{return} $\mathcal{G}_{\text{filtered}}, \mathcal{I}_{\text{filtered}}, \mathcal{T}_{\text{filtered}}$
\end{algorithmic}
\end{algorithm}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{main_figs/fig2.png}
    \caption{TRISHUL: Agentic Action Grounding Framework, Pink arrow, denotes our Hierarchical Screen Parsing (HSP) method, to generate GROIs and local element annotations, Green arrows represent our Spatially Enhanced Element Descriptor (SEED) workflow, Blue arrows represent our GROI proposal framework and Magenta Arrow shows, the Set of Marks (SoM) based  Grounding workflow.}
    \label{fig:main_fig}
\end{figure*}