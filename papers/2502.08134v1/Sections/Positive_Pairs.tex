\section{Crafting Effective Positive Pairs
}
\label{sec: Positive_Pair}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/positive_pairs.pdf}
    \caption{\textbf{Positive Pair Curation Techniques: } Positive pair selection can utilize single-instance and multi-instance techniques. (a) Single-instance curation applies augmentations to a single sample. On the other hand, multi-instance positive pair generation can be further classified into several category of techniques. (b) Embedding-based retrieves the top-K nearest neighbors of the anchor sample's augmentation in the embedding space and pairs them with other augmentations of the anchor. (c) Synthetic pairs generate data conditioned on the input, which is then augmented and paired with the augmented real sample (d) Supervised pairs use external sources (human labels, oracles, or annotations) to fetch another sample from the same category and create positive pairs. (e) Attributed-based: These methods group samples by shared attributes (e.g., golden retrievers paired with golden labrador retrievers based on fur color) and pair their respective augmentations. (f) Cross-modal: This involves creating semantically aligned pairs across multiple modalities. The figure shows image-text and speech-image pairing.}
    \label{fig:positive}
\end{figure*}

\subsection{Single Instance Positives}

This technique creates positive pairs using augmentations of a single sample without explicit curation, as shown in Fig. 2(a).  The negative pairs are also randomly sampled from the dataset and are uncurated. %Data augmentation plays a crucial role by introducing various transformations to the original data, prompting the learning algorithm to identify consistent underlying patterns.

A common contrastive learning technique using this type of data curation is \textbf{SimCLR}\cite{chen2020simple}. SimCLR maximizes agreement between augmented views of the same data point using the InfoNCE loss, relying on large batch sizes to sample enough negatives. \textbf{MoCo} \cite{he2020momentum} addresses a drawback of SimCLR, which requires large negative samples that can be computationally expensive by using a momentum encoder and memory bank to maintain a queue of negatives dynamically. \textbf{SimSiam} \cite{chen2021exploring} eliminates the need for negative examples, using a stop-gradient mechanism to prevent representation collapse in its Siamese architecture. \textbf{BYOL} \cite{grill2020bootstrap} simplifies learning by aligning predictions from an online network with a momentum-maintained target network, achieving strong performance without negatives. \textbf{Barlow Twins} \cite{zbontar2021barlow} focuses on redundancy reduction by aligning embeddings and decorrelating feature dimensions, avoiding collapse naturally without negatives or momentum. \textbf{DINO} \cite{caron2021emerging} combines self-supervised learning with knowledge distillation using a teacher-student framework, producing generalized embeddings. \textbf{VicReg} \cite{bardes2021vicreg} introduces regularization to balance variance, invariance, and decorrelation in embeddings, ensuring quality without negatives or momentum encoders. \textbf{SwAV} \cite{caron2020unsupervised} uses clustering to align augmentations by mapping them to shared cluster assignments without direct contrastive loss. \textbf{CPC} \cite{oord2018representation} leverages contrastive loss in a latent space to predict future data segments, making it particularly effective for time-series tasks. Finally, \textbf{SEED} \cite{fang2021seed} simplifies training by using teacher-student distillation with pseudo-labels, reducing computational complexity. %while maintaining strong performance.





\subsection{Multi Instance Positives}
Multi-instance positive pair curation creates pairs from different samples rather than augmented views of the same sample.

\subsubsection{Embedding-Based Techniques}
Given an input candidate sample, this class of techniques first retrieves the K-nearest neighbors of one of the augmentations using a similarity metric in embedding space, as shown in Figure 2(b). Next, it uses the K-retrieved samples and the other augmentations of the candidate sample as positive pairs. 

Nearest-Neighbour Contrastive Learning of Visual Representations (NNCLR)\cite{dwibedi2021little} samples the nearest neighbors from the dataset in the latent space and treats them as positives. This provides
more semantic and intra-class variations to learn representations that are invariant to different viewpoints, deformations, and variations. The NNCLR framework relies entirely on a single nearest neighbor, limiting its potential. Mean Shift for Self-Supervised Learning (MSF) \cite{koohpayegani2021mean} addresses this limitation by proposing the use of \(k\) nearest neighbors to increase the diversity in the positive pairs. MSF shifts the embedding of each image to be closer to the \textit{mean} of the neighbors of its augmentation.
However, MSF is computationally expensive because the objective function must be computed \(k\) times for each neighbor. To address MSF's computational inefficiency, All4One \cite{estepa2023all4one} contrasts information from multiple neighbors by compiling information from the extracted \(k\) neighbors to create a pair of representations, called \textit{centroids}, which contain contextual information about all the neighbors. 

These techniques can be used when semantic clustering is needed for downstream applications. For instance, if the downstream task involves clustering similar faces, these techniques allow different views of the same person to be closer together, unlike single-instance positive techniques, which treat all other images as negatives.

\subsubsection{Synthetic Data Generation for Positive Pairs}

This class of techniques creates synthetic samples using a generative process conditioned on the candidate input sample. A positive pair is formed by combining the augmented generated sample with the augmentation of the original input sample, which is then processed by the encoder, as illustrated in Fig. 2(c).

Contrastive Learning with Synthetic Positives (CLSP) \cite{zeng2024contrastive} incorporates synthetic positives generated via a diffusion model. By interpolating Gaussian noise with diffusion-based features, CLSP creates images that resemble the anchor while varying the context and background, increasing diversity while preserving semantic meaning. Similarly, \cite{wu2023synthetic} introduces a GAN-based framework that dynamically generates hard positive pairs by jointly optimizing the GAN and contrastive model. However, this simultaneous training introduces instability and quality control challenges. These approaches are particularly useful in data-sparse scenarios, rare modalities, or domains where obtaining real data is challenging, such as cross-modal medical applications (e.g., speech-image pairs).



\subsubsection{Supervised Pairing Techniques}

These techniques use external data sources, such as human preferences, privileged information from an oracle, or an annotated dataset, to derive meaningful metadata and semantic categories and create positive pairs, as shown in Fig. 2(d).


Supervised contrastive learning (SupCon) \cite{khosla2020supervised} leverages ground truth labels to enhance representation learning by incorporating category-level supervision. Instead of defining positive pairs through augmentations of a single instance, SupCon creates positive pairs from multiple samples of the same category as the anchor, ensuring that representations capture category-level semantic similarities rather than just instance-specific features.

Building upon this, \cite{ghose2023tailoring} propose a method to create positive pairs on the fly by passively observing humans provide limited positive examples while working collaboratively with a robot without explicitly marking negatives. This aligns with Positive-Unlabeled (PU) Learning \cite{bekker2020learning}, where only positives are known, and the model infers meaningful distinctions. Contrastive learning then clusters these examples, ensuring representations align with human expectations in a task-adaptive manner.
%Contrastive learning is then applied to cluster human-selected examples, ensuring that representations align with human expectations in a task-adaptive manner.  
Similarly, Oracle-guided Contrastive Clustering (OCC) \cite{wang2022oracle} uses a deep clustering framework designed to create positive pairs for contrastive loss by incorporating oracle feedback into the clustering process, ensuring that learned representations align with user-specific clustering preferences. Instead of relying purely on instance similarity in the embedding space, OCC actively queries an oracle (human or predefined rule) to determine whether two samples should belong to the same cluster.

These techniques are useful in scenarios when labeled data is available, and the goal is to cluster semantically similar items within the same class or the downstream task requires discrimination between subtle intra-class variations. Leveraging labels and semantic information to create pairs enables us to generate embeddings that are better aligned to the downstream application. Intuitively, these techniques should eliminate false negatives (through semantic clustering) and potentially reduce noise in the generated embeddings. 

\subsubsection{Attribute-based Pairing Techniques}

Attribute-based pairing entails selecting positive pairs based on task-specific criteria, as shown in Fig. 2(e). Attributes can be generic, such as \textit{``a golden colored object"} or more specific such as \textit{``dog with golden colored coat"}

For instance, Geography-aware self-supervised learning \cite{ayush2021geography} leverages spatial and temporal attributes to create temporal positive pairs from images of the same geographical location taken at different times. They demonstrate their approach in the Remote sensing domain because it is easy to obtain multiple geo-located images of the exact location over time. Similarly, \cite{pantazis2021focus} leverages the natural variations in sequential images from static cameras, utilizing contextual information such as spatial and temporal relationships to identify high-probability positive pairs—images likely depicting the same visual concept. Yet another approach proposed by \cite{panambur2022self} leveraged domain-specific attributes like geological structures, terrain textures, and spatial and scientific properties to form positive pairs for terrain categorization in Martian terrain.

These techniques are useful when domain-specific contextual attributes are known. However, uneven attribute distribution can lead to over-representation of certain pair types and hinder performance on unseen variations, so careful attribute selection and balancing are essential.

\subsubsection{Cross-modal Positive Pairing Techniques}
Cross-modal contrastive learning aims to learn meaningful representations across different data modalities  to improve performance in various tasks that involve multi-modal data,
as shown in Fig. 2(f). 

\noindent
\textbf{Image-Text Pairing: }
Image-text pairing aims to align visual and textual information to learn effective representations. 
CLIP \cite{radford2021learning}, CLOOB \cite{furst2022cloob}, ALIGN \cite{jia2021scaling} employs contrastive learning to learn shared representations by aligning visual and textual data which facilitate downstream tasks like zero-shot image classification and cross-modal retrieval. BEiT-3 \cite{wang2022image}, VisualBERT \cite{li1908visualbert}, FLAVA \cite{singh2022flava}, LXMERT \cite{tan2019lxmert} are a class of methods that introduce a unified masked data modeling objective. Given a partially masked caption, the objective is to predict the masked words based on the corresponding image. They learn representations that capture the relationships between images and texts by masking parts of the input and training the model to predict the missing information. MAPL \cite{manas2022mapl} and Flamingo \cite{alayrac2022flamingo} keep the pre-trained vision encoder and language model frozen to learn a lightweight mapping between their representation spaces, enabling few-shot learning with minimal parameter updates.


\noindent
\textbf{Audio-Image-Text Pairing: }
AudioCLIP \cite{guzhov2022audioclip}, Wav2CLIP \cite{wu2022wav2clip} learns audio representations by distilling knowledge from the CLIP model to jointly learn a shared representation of audio data alongside image and text modalities. CLAP \cite{wu2023large} trains a dual-encoder model to align audio and text embeddings. %within a shared space.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.97\linewidth]{Figures/negative_pairs.pdf}
    \caption{\textbf{Negative Pair Curation Techniques}: 
    % Most positive pair creation techniques rely on uncurated negative data and do not fully utilize the available data's potential. 
    This figure shows three categories of techniques for negative pair curation. (a). Hard Negative Selection prioritizes negatives that are semantically similar to the anchor sample, such as a different cat breed, instead of an unrelated category like an airplane. The negatives are then augmented and fed into the encoder. (b). False Negative Elimination removes or reclassifies negatives that are highly similar to the anchor sample, preventing the model from mistakenly separating highly similar samples. The remaining negatives are then augmented before encoding. Hard negatives improve discrimination but risk overfitting, while false negative elimination reduces noise but may mistakenly remove challenging yet valid negatives, weakening the representations. (c). Synthetic negative pairs are created by feeding the positive and negative samples(dataset) into a generative process and conditioned on the anchor sample to create realistic but distinct negatives. The generated samples then undergo augmentation and are fed with the positive pairs to the downstream encoder.}
    \label{fig:negative}
\end{figure*}

% \subsubsection{Speech-Audio-Text Pairing}

% Wav2Vec \\cite{baevski2020wav2vec} represents a category of models designed for self-supervised representation learning from raw audio waveforms. The wav2vec model is trained by predicting speech units for masked parts of speech audio. CLAPSpeech \cite{ye2023clapspeech} introduces a framework that explicitly learns prosody variations of text tokens under different contexts using cross-modal contrastive pre-training. By aligning text context with corresponding prosody patterns in a joint multi-modal space and employing a multi-scale pre-training pipeline, CLAPSpeech enhances prosody prediction in text-to-speech systems.

\noindent
\textbf{Audio-Visual Pairing: }
Audio-Visual Instance Discrimination (AVID) \cite{morgado2021audio} and \cite{alwassel2020self} emphasize cross-modal discrimination, aiming to align audio and visual features effectively. They use cross-modal clustering, where shared semantic clusters are learned across modalities by mapping audio and video representations into a joint embedding space. Building upon this, \cite{yariv2023audiotoken} adapts pre-trained text-conditioned diffusion models, like Stable Diffusion, by converting audio inputs into text-like embeddings through a learnable adapter. These embeddings serve as prompts for the diffusion model to generate audio-aligned images.