\section{Taxonomy of Approaches for Positive and Negative Pair Curation}
\label{sec: Taxonomy}



In contrastive learning, a commonly used loss function is the InfoNCE loss defined below. It pulls similar (positive) pairs together while pushing dissimilar (negative) pairs apart in the embedding space and computes the similarity between an anchor and its positive counterpart, using a softmax over similarity scores. %The InfoNCE loss is defined as follows:


\begin{math}
\hspace{-1em}
\centering
 \mathcal{L} = 
- \frac{1}{N} \sum_{i=1}^{N} \log 
\frac{
\exp\left(\text{sim}\left(\mathbf{z}_i, \mathbf{z}_i^+\right)/\tau\right)
}{
\exp\left(\text{sim}\left(\mathbf{z}_i, \mathbf{z}_i^+\right)/\tau\right) 
+ \sum_{j=1}^{N} 
\exp\left(\text{sim}\left(\mathbf{z}_i, \mathbf{z}_j\right)/\tau\right)}
\end{math}


where \( \mathbf{z}_i \) is the representation of the anchor sample and \( \mathbf{z}_i^+ \) is the representation of the positive sample obtained through augmenting the same instance or using a criterion to select another instance. \( \mathbf{z}_j \) represents all samples in the batch (including negatives). \( \text{sim}(\cdot, \cdot) \) denotes the similarity function (commonly cosine similarity). \( \tau \) is the temperature scaling parameter and \( N \) is the number of samples in the batch.


\subsection{Positive Pair Creation Taxonomy}
The taxonomy of Positive Pair Creation can be categorized into two main groups: \textbf{single-instance positives} and \textbf{multi-instance positives}, as shown in Fig. 1.

%\subsubsection{Single-instance Positives}

% Single-instance positives can be grouped into a single family of technique, as shown in Figure 1. 
Single-instance positive pair creation generates pairs by applying augmentations (e.g., cropping, color changes, geometric transformations) to a single sample \cite{chen2020simple}.  However, this approach limits diversity, as random augmentations fail to capture viewpoint changes, object deformations, or semantically similar instances within the same class. As a result, the model's generalization depends heavily on the augmentation strategy, which may not fully capture the intrinsic variations needed for learning robust embeddings.


% \subsubsection{Multi-instance Positives}

To overcome the limitations of single-instance pairs, multi-instance positive pair curation creates pairs from different data samples rather than augmented views of the same sample, leading to greater diversity \cite{dwibedi2021little}. As shown in Fig. 1, multi-instance curation techniques include: (1) Embedding-based, which selects semantically similar instances in embedding space; (2) Synthetic, which generates positive pairs using generative models; (3) Supervised, which uses human or oracle-labeled data; (4) Attribute-based, which groups samples based on spatial, temporal, or other object-based attributes.  and (5) Cross-modal, which associates samples across different modalities.  By ensuring higher diversity and semantic alignment, multi-instance positive pairs improve representation learning and align embeddings more effectively with downstream tasks.

%comment from here
% \noindent \textbf{Embedding-based similarity} identifies semantically similar samples in the embedding space to form positive pairs. For example, Nearest-Neighbour Contrastive Learning of visual Representations (NNCLR) \cite{dwibedi2021little} retrieves the nearest neighbor of a sample as its positive pair. Similarly, MSF \cite{koohpayegani2021mean} proposes to use the first and $k$ nearest neighbors as the multiple instance positives. All4One \cite{estepa2023all4one} improves MSF by incorporating a centroid contrastive objective to learn contextual information from multiple neighbors using a transformer network. \textbf{Synthetic data generation} employs generative models such as generative adversarial networks (GAN) \cite{wu2023synthetic} or diffusion models \cite{zeng2024contrastive} to create synthetic data points that are semantically similar but distinct from the original, serving as positive pairs. \cite{wu2023synthetic} is jointly trained with the main model to dynamically customize hard samples based on the training state of the main model. \cite{zeng2024contrastive} replaces the features of the intermediate layers in the diffusion model with the semantic features extracted from an anchor image during a random reversed diffusion process. This results in the generation of images possessing similar semantic content to the anchor image but differing in background and context due to the randomness of features in other layers. \textbf{Supervised pairing} utilizes label information to create positive pairs from samples of the same class, as seen in Supervised Contrastive Learning (SupCon) \cite{khosla2020supervised}. Another method \cite{ghose2023tailoring} proposes to create pairs and train the model in an online manner by using human-guided feedback. \cite{wang2022oracle} incorporates human or oracle feedback for a subset of samples to extend the set of positive instance pairs. \textbf{Attribute-based pairing} leverages specific attributes such as spatial location or temporal proximity to form positive pairs. For instance, geographically aligned images captured at different times can be paired \cite{ayush2021geography}. The attributes used to generate optimal views for contrastive representation learning are task-dependent. \textbf{Cross-modal positives} align samples across different modalities (e.g., images and text, audio, speech) that correspond to the same semantic content \cite{radford2021learning}, \cite{wang2022image}, \cite{baevski2020wav2vec}, \cite{li2020unimo}, \cite{morgado2021audio}.
% %comment till here

\subsection{Negative Pair Creation Taxonomy}

In typical contrastive learning approaches, negative pairs are created from samples not used to create the positive pair without considering their semantic content. However, recent work \cite{huynh2022boosting} suggests that uncurated negatives may lead to false negatives, where semantically similar samples are incorrectly treated as negatives. An effective negative sample selection strategy should balance easy and hard negatives while maintaining representativeness. Based on these principles, negative pair curation can be categorized into three main approaches (Fig. 1): (1) Hard Negative Selection, which prioritizes difficult negatives close to the anchor in embedding space; (2) False Negative Elimination, which removes or reclassifies semantically similar false negatives; and (3) Synthetic Negatives, where generative models create diverse, controlled negative samples. There is a subtle trade-off between (1) and (2). Hard negatives improve discrimination but risk overfitting, while false negative elimination reduces noise but may mistakenly remove challenging yet valid negatives, weakening the representations.

% \noindent \textbf{Hard negative selection} involves identifying samples that are particularly challenging for the model to distinguish. \cite{Hardnegativemixing} extends the MoCo v2 framework \cite{chen2020improved} by adding two sets of hard and harder negatives into the queue. The first set is a convex linear combination of pairs of its hardest existing negatives, whereas the second set is created by mixing the negatives with the query. \cite{unremix} introduces UnReMix, a method designed to enhance contrastive learning by selecting hard negative samples based on three key factors: anchor similarity, model uncertainty, and representativeness, ensuring that negative samples are similar to the anchor point, making them challenging for the model to distinguish. \textbf{Removal of false negatives} addresses negative pairs from the same semantic category. \cite{huynh2022boosting} introduces methods to identify these false negatives and propose two strategies to mitigate their impact: elimination and attraction. False Negative Elimination identifies potential false negatives and excludes them from the negative sample set, preventing the model from learning misleading distinctions. In False Negative Attraction, instead of excluding false negatives, this strategy reclassifies them as positives, encouraging the model to learn representations that acknowledge their semantic similarity. \textbf{Synthetic hard negatives} can be created using various techniques, including generative models, feature space interpolation, or rule-based algorithms that modify existing data. \cite{dong2024synthetic} proposes an approach that involves mixing existing negative samples in the feature space to create more challenging negatives, encouraging the model to learn more discriminative representations. It proposes a novel feature-level sampling method to generate more and harder negative samples by mixing them through linear combination and ensuring their reliability by debiasing.


% Next, we dive into details of the most commonly used techniques for crafting effective positive and negative pairs.






