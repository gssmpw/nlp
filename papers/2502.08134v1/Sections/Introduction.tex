\section{Introduction}
\label{sec: Intro}

Contrastive learning has emerged in recent times as the dominant approach to self-supervised learning. 
The main idea of contrastive learning is to leverage the fact that similar data samples (positive pairs) should be positioned closer together in the embedding space, while dissimilar data samples (negative pairs) should be pushed further apart. 
The construction of the set of positive and negative pairs, referred to as \mbox{\emph{data curation}} in this paper, directly influences the informativeness of the embeddings \cite{khosla2020supervised,unremix,huynh2022boosting}. 
% The set of positive and negative pairs used directly influence the informativeness of the embeddings \cite{khosla2020supervised,unremix,huynh2022boosting}. 
% Moreover, effective data curation also helps in training inference-efficient models \cite{udandarao2024active,evans2024data}, and accelerates training and model convergence \cite{xu2023cit,zhou2021cupid}.

% One major challenge with contrastive learning is data curation, 
Data curation is a major challenge for contrastive learning as selecting ineffective data pairs may result in suboptimal embeddings, leading to poor generalization on downstream tasks. It also impedes the training process, resulting in higher training time and computational overhead, especially for large datasets. Effective data curation helps in training inference-efficient models \cite{udandarao2024active,evans2024data}, and accelerates training and model convergence \cite{xu2023cit,zhou2021cupid}.
Data curation can thus alleviate the issue of suboptimal representations and model convergence in the following ways: (1) Increasing diversity in data pairs, ensuring optimality of the learned embeddings that are invariant to intra-class variations, (2) Ensuring the selected data pairs are relevant and semantically aligned to prevent noisy samples from hindering training and enabling faster model convergence, (3) Enabling better alignment between the learned representations and the downstream task.

\input{Figures/taxonomy_figure}

Data curation for contrastive learning can be approached in two ways: positive pair curation and negative pair curation. %Recent studies describe techniques for both, ensuring that the embeddings are diverse, relevant, and aligned with the downstream task. 
In the positive data curation direction, several recent works \cite{dwibedi2021little,wu2023synthetic,ayush2021geography} show that carefully designed positive pairs introduce diverse variations leading to generalized and informative embeddings. These techniques create positive pairs from different data samples (as opposed to different views of the same data sample) that are semantically aligned. This semantic alignment is defined through some similarity metric in embedding space \cite{koohpayegani2021mean,estepa2023all4one}, label, or available category information which can either be pre-defined \cite{khosla2020supervised}, or queried from an oracle or human in an online manner \cite{ghose2023tailoring}.  This semantic alignment reduces noise in the learning process and ensures relevance of pairs, while diversity ensures robustness, and together they optimize representation learning and improve model convergence. Similarly, some methods \cite{ayush2021geography} enable better alignment through leveraging predefined attributes based on the input domain or downstream task. Yet other works \cite{wu2023synthetic,zeng2024contrastive} propose using synthetically generated positives to generate diverse pairs that can potentially help with category imbalance, rare scenarios and modalities. 
%and provide data instances for rare scenarios and modalities. 

Negative pairs in contrastive learning are often randomly sampled without explicit curation, but recent research highlights the importance of curating negatives to ensure semantic alignment and diversity through techniques like hard negative selection \cite{unremix}, synthetic negative generation \cite{giakoumoglou2024synco}, and false negative elimination \cite{huynh2022boosting}.
%Several techniques have been proposed, including hard negative selection \cite{unremix}, synthetic negative generation \cite{giakoumoglou2024synco}, and false negative elimination \cite{huynh2022boosting}.
If negative pairs are too different from positives (easy negatives), they provide little learning signalâ€”for example, distinguishing dogs from aircraft and ships is trivial. Instead, curating hard negatives that are closer to positives in embedding space \cite{robinson2020contrastive}, or incorporating adversarial negatives \cite{hu2021adco}, can improve representation learning. Additionally, synthetically generated negatives \cite{giakoumoglou2024synco,dong2024synthetic} enhance diversity, while methods like false negative elimination \cite{huynh2022boosting,chuang2020debiased} prevent semantically similar negatives from introducing conflicting learning signals. 
%Effective negative pair selection ensures that models learn discriminative yet meaningful representations without unnecessary noise. 

Thus, given the widespread use of contrastive pre-training, it is crucial to carefully select the data fed into the learning process to obtain robust and informative representations. 
Most existing surveys (\cite{gui2024survey,jaiswal2020survey,giakoumoglou2024review}) on visual contrastive learning focus on comparing architectural choices or learning objectives such as momentum encoders in MoCo \cite{he2020momentum} or stop-gradient mechanisms in SimSiam \cite{chen2021exploring}. In this survey, we take a complementary perspective by examining the role of data curation in contrastive learning. We discuss the tradeoffs between popular techniques, and finally pose some open questions for researchers interested in this direction.






