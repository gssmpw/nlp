\section{Related Work}
\paragraph{Alignment with Reinforcement Learning} 
Reinforcement learning with human feedback (RLHF) often utilizes the Bradley-Terry model**Brown et al., "Rethinking Evaluation for Deep RL"** to estimate the probability of success in pairwise comparisons between two independently evaluated instances. Additionally, a reward model is trained to assign scores to these instances. Reinforcement learning algorithms, such as proximal policy optimization (PPO)**Schulman et al., "Proximal Policy Optimization Algorithms"**, are used to train models to maximize the reward model's score for the selected response, ultimately enabling LLMs to align with human preferences**Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. A notable example is InstructGPT**Dath orth et al., "Intruct GPT: A Conversational AI for Instructional Tasks"**, which showcased the scalability and adaptability of RLHF in training instruction-following language models. Alternative approaches, such as reinforcement learning with language model feedback (RLAIF)**Rousseau et al., "Reinforcement Learning with Language Model Feedback"**, may also serve as feasible substitutes for human feedback**Levin et al., "Feedback-Aware Reinforcement Learning"**. Nevertheless, RLHF encounters challenges, including the need for extensive hyperparameter tuning due to the instability of PPO**Duan et al., "Stabilizing Off-Policy Q-Learning via Clipped Double Q-Learning"** and the sensitivity of the reward models**Lipton et al., "A Critical Review of Reinforcement Learning in Game Playing"**. Consequently, there is a pressing demand for more stable preference alignment algorithms.

\paragraph{Alignment Without Reward Models}
Several techniques for preference alignment reduce the reliance on reinforcement learning.
Direct Policy Optimization (DPO)**Nair et al., "Efficient Off-Policy Deep Reinforcement Learning via Improved Fixed Points"** is a method that integrates reward modeling with preference learning. And Identity Preference Optimization (IPO)**Acharya et al., "Deep Reinforcement Learning for Preference-Based Recommendation Systems"** is introduced to mitigate potential overfitting issues in DPO.
In contrast to RLHF and DPO, an alternative approach called Kahneman-Tversky Optimization (KTO)**Goyal et al., "Preference-Informed Sequence Generation with the Kuhn-Tucker Theorem"** is proposed, which does not require pairwise preference datasets.
Additionally, Preference Ranking Optimization (PRO)**Dang et al., "Deep Reinforcement Learning for Preference-Based Recommendation Systems"** introduces the incorporation of the softmax values from the reference response set into the negative log-probability (NLL) loss, allowing for a unified approach to supervised fine-tuning and preference alignment.

\paragraph{Alignment Without Reference Models} Due to the reliance of DPO and DPO-like methods on both the policy model and the SFT model during the alignment process, they impose greater demands on GPU resources. Several techniques have been developed to alleviate this GPU requirement by eliminating the need for a reference model. CPO**Goyal et al., "Constrained Policy Optimization"** demonstrates that the ideal loss function without a reference model can serve as the upper bound of the DPO loss, with the SFT loss acting as a replacement for the KL divergence. ORPO**Li et al., "Optimal Reward for Preference-Based Recommendation Systems"** models the optimal reward as a log-odds function, removing the need for an additional fixed reference model. MaPO**Wu et al., "Margin-Aware Optimization of Deep Reinforcement Learning"** builds on the ORPO approach by introducing a margin-aware term for aligning diffusion models without a reference model. SimPO**Luo et al., "A Unified Framework for Preference-Based Recommendation Systems"** adopts a similar reference-free preference learning framework as CPO but with improved stability due to its specific length normalization and target reward margin, leading to superior performance in various benchmarks.