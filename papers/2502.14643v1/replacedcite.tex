\section{Related Work}
\paragraph{Alignment with Reinforcement Learning} 
Reinforcement learning with human feedback (RLHF) often utilizes the Bradley-Terry model____ to estimate the probability of success in pairwise comparisons between two independently evaluated instances. Additionally, a reward model is trained to assign scores to these instances. Reinforcement learning algorithms, such as proximal policy optimization (PPO)____, are used to train models to maximize the reward model's score for the selected response, ultimately enabling LLMs to align with human preferences____. A notable example is InstructGPT____, which showcased the scalability and adaptability of RLHF in training instruction-following language models. Alternative approaches, such as reinforcement learning with language model feedback (RLAIF____), may also serve as feasible substitutes for human feedback____. Nevertheless, RLHF encounters challenges, including the need for extensive hyperparameter tuning due to the instability of PPO____ and the sensitivity of the reward models____. Consequently, there is a pressing demand for more stable preference alignment algorithms.

\paragraph{Alignment Without Reward Models}
Several techniques for preference alignment reduce the reliance on reinforcement learning.
Direct Policy Optimization (DPO)____ is a method that integrates reward modeling with preference learning. And Identity Preference Optimization (IPO)____ is introduced to mitigate potential overfitting issues in DPO.
In contrast to RLHF and DPO, an alternative approach called Kahneman-Tversky Optimization (KTO)____ is proposed, which does not require pairwise preference datasets.
Additionally, Preference Ranking Optimization (PRO)____ introduces the incorporation of the softmax values from the reference response set into the negative log-probability (NLL) loss, allowing for a unified approach to supervised fine-tuning and preference alignment.

\paragraph{Alignment Without Reference Models} Due to the reliance of DPO and DPO-like methods on both the policy model and the SFT model during the alignment process, they impose greater demands on GPU resources. Several techniques have been developed to alleviate this GPU requirement by eliminating the need for a reference model. CPO____ demonstrates that the ideal loss function without a reference model can serve as the upper bound of the DPO loss, with the SFT loss acting as a replacement for the KL divergence. ORPO____ models the optimal reward as a log-odds function, removing the need for an additional fixed reference model. MaPO____ builds on the ORPO approach by introducing a margin-aware term for aligning diffusion models without a reference model. SimPO____ adopts a similar reference-free preference learning framework as CPO but with improved stability due to its specific length normalization and target reward margin, leading to superior performance in various benchmarks.