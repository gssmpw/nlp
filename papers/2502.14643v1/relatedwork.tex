\section{Related Work}
\paragraph{Alignment with Reinforcement Learning} 
Reinforcement learning with human feedback (RLHF) often utilizes the Bradley-Terry model~\citep{bradley1952rank} to estimate the probability of success in pairwise comparisons between two independently evaluated instances. Additionally, a reward model is trained to assign scores to these instances. Reinforcement learning algorithms, such as proximal policy optimization (PPO)~\citep{schulman2017proximal}, are used to train models to maximize the reward model's score for the selected response, ultimately enabling LLMs to align with human preferences~\citep{stiennon2020learning,ziegler2019fine}. A notable example is InstructGPT~\citep{ouyang2022training}, which showcased the scalability and adaptability of RLHF in training instruction-following language models. Alternative approaches, such as reinforcement learning with language model feedback (RLAIF~\citep{lee2023rlaif}), may also serve as feasible substitutes for human feedback~\citep{bai2022constitutional,sun2023salmon}. Nevertheless, RLHF encounters challenges, including the need for extensive hyperparameter tuning due to the instability of PPO~\citep{rafailov2024direct} and the sensitivity of the reward models~\citep{wang2024secrets}. Consequently, there is a pressing demand for more stable preference alignment algorithms.

\paragraph{Alignment Without Reward Models}
Several techniques for preference alignment reduce the reliance on reinforcement learning.
Direct Policy Optimization (DPO)~\citep{rafailov2024direct} is a method that integrates reward modeling with preference learning. And Identity Preference Optimization (IPO)~\citep{azar2024general} is introduced to mitigate potential overfitting issues in DPO.
In contrast to RLHF and DPO, an alternative approach called Kahneman-Tversky Optimization (KTO)~\citep{ethayarajh2024kto} is proposed, which does not require pairwise preference datasets.
Additionally, Preference Ranking Optimization (PRO)~\citep{song2024preference} introduces the incorporation of the softmax values from the reference response set into the negative log-probability (NLL) loss, allowing for a unified approach to supervised fine-tuning and preference alignment.

\paragraph{Alignment Without Reference Models} Due to the reliance of DPO and DPO-like methods on both the policy model and the SFT model during the alignment process, they impose greater demands on GPU resources. Several techniques have been developed to alleviate this GPU requirement by eliminating the need for a reference model. CPO~\citep{xu2024contrastive} demonstrates that the ideal loss function without a reference model can serve as the upper bound of the DPO loss, with the SFT loss acting as a replacement for the KL divergence. ORPO~\citep{hong2024reference} models the optimal reward as a log-odds function, removing the need for an additional fixed reference model. MaPO~\citep{hong2024margin} builds on the ORPO approach by introducing a margin-aware term for aligning diffusion models without a reference model. SimPO~\citep{meng2024simpo} adopts a similar reference-free preference learning framework as CPO but with improved stability due to its specific length normalization and target reward margin, leading to superior performance in various benchmarks.