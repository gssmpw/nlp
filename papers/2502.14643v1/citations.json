[
  {
    "index": 0,
    "papers": [
      {
        "key": "bradley1952rank",
        "author": "Bradley, Ralph Allan and Terry, Milton E",
        "title": "Rank analysis of incomplete block designs: I. The method of paired comparisons"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bai2022constitutional",
        "author": "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others",
        "title": "Constitutional ai: Harmlessness from ai feedback"
      },
      {
        "key": "sun2023salmon",
        "author": "Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang",
        "title": "Salmon: Self-alignment with principle-following reward models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024secrets",
        "author": "Wang, Binghai and Zheng, Rui and Chen, Lu and Liu, Yan and Dou, Shihan and Huang, Caishuang and Shen, Wei and Jin, Senjie and Zhou, Enyu and Shi, Chenyu and others",
        "title": "Secrets of rlhf in large language models part ii: Reward modeling"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "song2024preference",
        "author": "Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng",
        "title": "Preference ranking optimization for human alignment"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xu2024contrastive",
        "author": "Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin",
        "title": "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hong2024reference",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Reference-free monolithic preference optimization with odds ratio"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hong2024margin",
        "author": "Hong, Jiwoo and Paul, Sayak and Lee, Noah and Rasul, Kashif and Thorne, James and Jeong, Jongheon",
        "title": "Margin-aware Preference Optimization for Aligning Diffusion Models without Reference"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      }
    ]
  }
]