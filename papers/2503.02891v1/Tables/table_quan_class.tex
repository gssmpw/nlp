\renewcommand{\arraystretch}{0.7}
\begin{table}[]
\centering
\caption{Results of different post-training quantization(classification) techniques proposed for ViTs. \textbf{MP} denotes mixed precision; \textbf{W-bit} refers to weight bit-widths and \textbf{A-bit} refers to activation bit-widths. Here, Baseline refers to the closest comparable results for classification tasks.}
\label{quantization_tech_classification}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|cccc}
\hline
\multirow{2}{*}{\textbf{Algorithm}} &
  \multirow{2}{*}{\textbf{Key point}} &
  \multirow{2}{*}{\textbf{Backbone}} &
  \multirow{2}{*}{\textbf{Dataset}} &
  \multicolumn{4}{c}{\textbf{Results}} \\ \cline{5-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{\textbf{Baseline}} &
  \multicolumn{1}{c|}{\textbf{W-bit}} &
  \multicolumn{1}{c|}{\textbf{A-bit}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Top-1\\ accuracy\end{tabular}} \\ \hline
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}PTQ\\ ~\cite{liu2021post}\end{tabular}} &
  \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Similarity-aware quantization \\ for linear layers ranking -aware\\ quantization for self-attention layers \\ Mixed-precision quantization \\ to retain performance\end{tabular}} &
  \multirow{6}{*}{ViT-B} &
  \multirow{2}{*}{CIFAR-10} &
  \multicolumn{1}{c|}{\multirow{6}{*}{Percentile}} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  96.83 ($+3.35$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8 MP} &
  \multicolumn{1}{c|}{8 MP} &
  97.79 ($+2.03$) \\ \cline{4-4} \cline{6-8} 
 &
   &
   &
  \multirow{2}{*}{CIFAR-100} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  83.99 ($+3.15$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8 MP} &
  \multicolumn{1}{c|}{8 MP} &
  85.76 ($+2.48$) \\ \cline{4-4} \cline{6-8} 
 &
   &
   &
  \multirow{2}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  75.26 ($+3.68$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8 MP} &
  \multicolumn{1}{c|}{8 MP} &
  76.98 ($+2.88$) \\ \cline{3-8} 
 &
   &
  DeiT-B &
  ImageNet-1k~\cite{5206848} &
  \multicolumn{1}{c|}{Bit-Split} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  74.58 ($+0.54$) \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}PTQ4ViT\\ ~\cite{yuan2022ptq4vit}\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Twin uniform method to reduce the quantization \\ error on activation values \& analyse Hessian guided\\ metric to determine the scaling factors of each layer\end{tabular}} &
  ViT-B &
  \multirow{4}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{4}{*}{Base-PTQ}} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  85.82 ($+0.52$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  82.97 ($+.64$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{Swin-B} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{\multirow{2}{*}{8}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{8}} &
  \multirow{2}{*}{86.39 ($+0.23$)} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{} &
   \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}APQ-ViT\\ ~\cite{ding2022towards}\end{tabular}} &
  \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Solve for extremely low-bit representation; \\ BBC to apply quantization in a blockwise \\ manner to perceive the loss in adjacent layers .\& \\ Matthew-effect preserving quantization for the \\ softmax to maintain power-law distribution\end{tabular}} &
  \multirow{2}{*}{ViT-B} &
  \multirow{6}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{6}{*}{PTQ4ViT~\cite{yuan2022ptq4vit}}} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  82.21 ($+0.56$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  41.41 ($+10.72$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{Swin-B/384} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  85.60 ($+0.16$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  80.84 ($+2.0$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{DeiT-B} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  80.42 ($+0.17$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  67.48 ($+3.09$) \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}NoisyQuant\\ ~\cite{liu2023noisyquant}\end{tabular}} &
  \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}A quantizer-agnostic enhancement for \\ the post-training activation quantization \\ of ViT \& adding a fixed Uniform noisy \\ bias to the values being quantized \\ for a given quantizer\end{tabular}} &
  \multirow{2}{*}{ViT-B} &
  \multirow{6}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{6}{*}{PTQ4ViT~\cite{yuan2022ptq4vit}}} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  81.90 ($+6.24$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  84.10 ($+0.71$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{DeiT-B} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  79.77 ($+.99$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  81.30 ($+0.36$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{Swin-S} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  84.57 ($+1.22$) \\ \cline{6-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  85.11 ($+0.32$) \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}FQ-Vit\\ ~\cite{lin2021fq}\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Efficient PTQ method for achieving accurate \\ quantization on LayerNorm inputs with one layerwise \\ quantization scale named PTF. Propose LIS for \\ performing 4-bit quantization on attention maps\end{tabular}} &
  DeiT-T &
  \multirow{4}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{4}{*}{Percentile}} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  71.61 ($+0.14$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  79.17 ($+2.6$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  81.20 ($+1.83$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{8} &
  \multicolumn{1}{c|}{8} &
  82.97 ($+42.04$) \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}I-ViT\\ ~\cite{li2022vit}\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Performing the entire inference with \\ integer arithmetic \& bit-shifting. integer \\ approximations for non-linear operations\end{tabular}} &
  ViT-B &
  \multirow{4}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{4}{*}{I-BERT~\cite{kim2021bert}}} &
  \multicolumn{1}{c|}{-} &
  \multicolumn{1}{c|}{-} &
  84.76 ($+1.06$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{-} &
  \multicolumn{1}{c|}{-} &
  81.74 ($+0.95$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  \multirow{2}{*}{Swin-S} &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{\multirow{2}{*}{-}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{-}} &
  \multirow{2}{*}{83.01 ($+1.15$)} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{} &
   \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}RepQ-ViT\\ ~\cite{li2023repq}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Apply channel-wise quantization on  post-LayerNorm \\ activations \& $\log \sqrt{2}$ for post-softmax activations\end{tabular}} &
  ViT-B &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{APQ-ViT~\cite{ding2022towards}}} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  83.62 ($+1.41$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  81.27 ($+0.85$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{4} &
  \multicolumn{1}{c|}{4} &
  82.79 ($+0.12$) \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MPTQ-ViT\\ ~\cite{tai2024mptq}\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Introduce SmoothQuant~\cite{xiao2023smoothquant} with bias term \\ to solve asymmetric issue \& minimize clamping loss\end{tabular}} &
  ViT-B &
  \multirow{2}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{TSPTQ-ViT~\cite{10096817}}} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  82.70 ($+0.41$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  81.25 ($+0.64$) \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}LRP-QViT\\ ~\cite{ranjan2024lrp}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Assigning precision bit for \\ individual layers based on layer's \\ importance\end{tabular}} &
  ViT-B &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{RepQ-ViT~\cite{li2023repq}}} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  83.87 ($+0.25$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  81.44 ($+0.17$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6 MP} &
  \multicolumn{1}{c|}{6 MP} &
  82.86 ($+0.07$) \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}ERQ\\ ~\cite{zhong2024erq}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Introduced weight quantization \\ error reduction metrics to minimize \\ the weight quantization error\end{tabular}} &
  ViT-B &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{AdaRound~\cite{nagel2020up}}} &
  \multicolumn{1}{c|}{5} &
  \multicolumn{1}{c|}{5} &
  82.81 ($+0.81$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{5} &
  \multicolumn{1}{c|}{5} &
  80.65 ($+0.47$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{5} &
  \multicolumn{1}{c|}{5} &
  82.44 ($+0.32$) \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Adalog\\ ~\cite{wu2025adalog}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Proposed adaptive log based \\ non-uniform quantization for post-Softmax\\ \& post GELy activations\end{tabular}} &
  ViT-B &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{RepQ-ViT~\cite{li2023repq}}} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  84.80($+1.18$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  DeiT-B &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  81.55 ($+0.28$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  83.19 ($+0.40$) \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CLAMP-ViT\\ ~\cite{ramachandran2025clamp}\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Leverage contrastive learning layer-wise evolutionary \\ search for fixed and mixed-precision quantization\end{tabular}} &
  DeiT-S &
  \multirow{2}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{LRP-QViT~\cite{ranjan2024lrp}}} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  79.43 ($+0.40$) \\ \cline{3-3} \cline{6-8} 
 &
   &
  Swin-S &
   &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{6} &
  \multicolumn{1}{c|}{6} &
  82.86 ($+0.00$) \\ \hline
\end{tabular}%
}
\vspace{-5mm}
\end{table}