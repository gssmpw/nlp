% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
% \renewcommand{\arraystretch}{0.65}
%  \newgeometry{hmargin=3cm,vmargin=3cm,landscape}
\renewcommand{\arraystretch}{0.74}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage{lscape}
% \begin{landscape}
% \begin{table}[]
% \centering
% \caption{Results of different KD (classification) techniques proposed for Vision transformers.}
% \label{tab:KD_result_classification}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{c|c|c|c|c|cc|cc}
% \hline
% \multirow{2}{*}{\textbf{Algorithm}} &
%   \multirow{2}{*}{\textbf{Key points}} &
%   \multirow{2}{*}{\textbf{Method}} &
%   \multirow{2}{*}{\textbf{Loss function}} &
%   \multirow{2}{*}{\textbf{Dataset}} &
%   \multicolumn{2}{c|}{\textbf{Teachers}} &
%   \multicolumn{2}{c}{\textbf{Students}} \\ \cline{6-9} 
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{\textbf{Models}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Top-1\\ (\%)\end{tabular}} &
%   \multicolumn{1}{c|}{\textbf{Models}} &
%   \textbf{\begin{tabular}[c]{@{}c@{}}Top-1\\ (\%)\end{tabular}} \\ \hline
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Fine-grained\\ manifold ~\cite{hao2022learning}\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Teach the student layer\\  having the same patch-\\ level manifold structure \\ as the teacher layer\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Patch-level manifold\\  space method\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Manifold distillation\\  loss (MD Loss)\end{tabular}} &
%   \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
%   \multicolumn{1}{c|}{CaiT-S24} &
%   83.4\% &
%   \multicolumn{1}{c|}{DeiT-T} &
%   \begin{tabular}[c]{@{}c@{}}76.5\\ ($\uparrow$4.3\%)\end{tabular} \\ \cline{6-9} 
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{CaiT-S24} &
%   83.4\% &
%   \multicolumn{1}{c|}{DeiT-S} &
%   \begin{tabular}[c]{@{}c@{}}82.2\\ ($\uparrow$2.3\%)\end{tabular} \\ \cline{6-9} 
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{Swin-S} &
%   83.2\% &
%   \multicolumn{1}{c|}{Swin-T} &
%   \begin{tabular}[c]{@{}c@{}}82.2\\ ($\uparrow$1.0\%)\end{tabular} \\ \hline
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Target-aware \\ Transformer ~\cite{lin2022knowledge}\end{tabular}} &
%   \multirow{2}{*}{Hierarchical distillation} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}One-to-all spatial\\  matching KD\end{tabular}} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Vanilla distillation +\\  L\textsubscript2 + Task loss\end{tabular}} &
%   \multirow{2}{*}{ImageNet-1k~\cite{5206848}} &
%   \multicolumn{1}{c|}{\multirow{2}{*}{ResNet34}} &
%   \multirow{2}{*}{72.4\%} &
%   \multicolumn{1}{c|}{\multirow{2}{*}{ResNet18}} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}72.1\\ ($\uparrow$+2.0\%)\end{tabular}} \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\ \hline
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Cross Inductive \\ Bias Distillation ~\cite{ren2022co}\end{tabular}} &
%   \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Enables student \\ transformers to emulate \\ the performance of various\\  inductive bias teachers\end{tabular}} &
%   \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Co-advising the student\\  models with lightweight\\  teacher model\end{tabular}} &
%   \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Kull back divergence +\\ Cross entropy loss\end{tabular}} &
%   \multirow{4}{*}{ImageNet-1k~\cite{5206848}} &
%   \multicolumn{1}{c|}{\multirow{4}{*}{ResNet18}} &
%   \multirow{4}{*}{83.4\%} &
%   \multicolumn{1}{c|}{\multirow{4}{*}{Transformer-Ti}} &
%   \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}88.0\\ ($\uparrow$+1.5\%)\end{tabular}} \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\ \hline
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention \\ probe ~\cite{9747484}\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Utilizing the intermediate\\  information for transferring  \\ the embedding features\\ between teacher and \\ student directly\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Probe distillation \\ \& Knowledge distillation\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Probe distillation + \\ cross-entropy\end{tabular}} &
%   CIFAR-100 &
%   \multicolumn{1}{c|}{DeiT-XS} &
%   76.30\% &
%   \multicolumn{1}{c|}{DeiT-XTiny} &
%   \begin{tabular}[c]{@{}c@{}}71.82\\ ($\uparrow$+6.36\%)\end{tabular} \\ \cline{5-9} 
%  &
%    &
%    &
%    &
%   CIFAR-10 &
%   \multicolumn{1}{c|}{DeiT-XS} &
%   96.65\% &
%   \multicolumn{1}{c|}{DeiT-XTiny} &
%   \begin{tabular}[c]{@{}c@{}}93.95\\ ($\uparrow$+7.64\%)\end{tabular} \\ \cline{5-9} 
%  &
%    &
%    &
%    &
%   MNIST &
%   \multicolumn{1}{c|}{DeiT-XS} &
%   99.39\% &
%   \multicolumn{1}{c|}{DeiT-XTiny} &
%   \begin{tabular}[c]{@{}c@{}}99.07\\ ($\uparrow$+0.01\%)\end{tabular} \\ \hline
% \multirow{3}{*}{MiniViT ~\cite{zhang2022minivit}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Prediction-Logit Distillation \&\\ Hidden-State Distillation\end{tabular}} &
%   \multirow{3}{*}{Weight distillation} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Self-attention distillation + \\ Hidden-state distillation + \\ prediction loss\end{tabular}} &
%   \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
%   \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}RegNet-\\ 16GF\end{tabular}}} &
%   \multirow{3}{*}{82.9\%} &
%   \multicolumn{1}{c|}{\multirow{3}{*}{DeiT-B}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}83.2\\ ($\uparrow$+1.4\%)\end{tabular}} \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\ \hline
% \multirow{3}{*}{TinyViT ~\cite{wu2022tinyvit}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Applying distillation during \\ pretraining to transfer \\ knowledge\end{tabular}} &
%   \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Reusing the teachers' \\ prediction \& data \\ augmentation for student\end{tabular}} &
%   \multirow{3}{*}{Cross entropy loss} &
%   \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
%   \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}CLIP-\\ ViT-L\end{tabular}}} &
%   \multirow{3}{*}{84.8\%} &
%   \multicolumn{1}{c|}{Swin-T} &
%   \begin{tabular}[c]{@{}c@{}}83.4\\ ($\uparrow$+2.2\%)\end{tabular} \\ \cline{8-9} 
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{\multirow{2}{*}{DeiT-S}} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}82.0\\ ($\uparrow$+2.1\%)\end{tabular}} \\
%  &
%    &
%    &
%    &
%    &
%   \multicolumn{1}{c|}{} &
%    &
%   \multicolumn{1}{c|}{} &
%    \\ \hline
% DearKD ~\cite{chen2022dearkd} &
%   \begin{tabular}[c]{@{}c@{}}Representational kD based \\ on intermediate features \&\\  response based KD\end{tabular} &
%   Self-generative data &
%   \begin{tabular}[c]{@{}c@{}}MSE distillation +\\ Cross entropy+ Intra\\ -divergence distillation loss\end{tabular} &
%   ImageNet-1k~\cite{5206848} &
%   \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ResNet-\\ 101\end{tabular}} &
%   77.37\% &
%   \multicolumn{1}{c|}{DeiT-Ti} &
%   \begin{tabular}[c]{@{}c@{}}71.2\\ ($\downarrow$+1.0\%)\end{tabular} \\ \hline
% \end{tabular}%
% }
% \end{table}
% \end{landscape}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
\caption{Results of different KD (classification) techniques proposed for vision transformers. Here, MSE loss means mean square error loss. }
\label{tab:KD_result_classification}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Algorithm}} &
  \multirow{2}{*}{\textbf{Method}} &
  \multirow{2}{*}{\textbf{Loss function}} &
  \multirow{2}{*}{\textbf{Dataset}} &
  \multicolumn{2}{c|}{\textbf{Teachers}} &
  \multicolumn{2}{c}{\textbf{Students}} \\ \cline{5-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{\textbf{Models}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Top-1\\ (\%)\end{tabular}} &
  \multicolumn{1}{c|}{\textbf{Models}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Top-1\\ (\%)\end{tabular}} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Fine-grained\\ manifold ~\cite{hao2022learning}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Patch-level manifold\\  space method\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Manifold distillation\\  loss (MD Loss)\end{tabular}} &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{CaiT-S24} &
  83.4\% &
  \multicolumn{1}{c|}{DeiT-T} &
  \begin{tabular}[c]{@{}c@{}}76.5\\ ($\uparrow$4.3\%)\end{tabular} \\ \cline{5-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{CaiT-S24} &
  83.4\% &
  \multicolumn{1}{c|}{DeiT-S} &
  \begin{tabular}[c]{@{}c@{}}82.2\\ ($\uparrow$2.3\%)\end{tabular} \\ \cline{5-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{Swin-S} &
  83.2\% &
  \multicolumn{1}{c|}{Swin-T} &
  \begin{tabular}[c]{@{}c@{}}82.2\\ ($\uparrow$1.0\%)\end{tabular} \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Target-aware \\ Transformer ~\cite{lin2022knowledge}\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}One-to-all spatial\\  matching KD\end{tabular}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Vanilla distillation +\\  L\textsubscript2 + Task loss\end{tabular}} &
  \multirow{2}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{ResNet34}} &
  \multirow{2}{*}{72.4\%} &
  \multicolumn{1}{c|}{\multirow{2}{*}{ResNet18}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}72.1\\ ($\uparrow$+2.0\%)\end{tabular}} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Cross Inductive \\ Bias Distillation ~\cite{ren2022co}\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Co-advising the student\\  models with lightweight\\  teacher model\end{tabular}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Kull back divergence +\\ Cross entropy loss\end{tabular}} &
  \multirow{4}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{4}{*}{ResNet18}} &
  \multirow{4}{*}{83.4\%} &
  \multicolumn{1}{c|}{\multirow{4}{*}{Transformer-Ti}} &
  \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}88.0\\ ($\uparrow$+1.5\%)\end{tabular}} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention \\ probe ~\cite{9747484}\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Probe distillation \\ \& Knowledge distillation\end{tabular}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Probe distillation + \\ cross-entropy\end{tabular}} &
  CIFAR-100 &
  \multicolumn{1}{c|}{DeiT-XS} &
  76.30\% &
  \multicolumn{1}{c|}{DeiT-XTiny} &
  \begin{tabular}[c]{@{}c@{}}71.82\\ ($\uparrow$+6.36\%)\end{tabular} \\ \cline{4-8} 
 &
   &
   &
  CIFAR-10 &
  \multicolumn{1}{c|}{DeiT-XS} &
  96.65\% &
  \multicolumn{1}{c|}{DeiT-XTiny} &
  \begin{tabular}[c]{@{}c@{}}93.95\\ ($\uparrow$+7.64\%)\end{tabular} \\ \cline{4-8} 
 &
   &
   &
  MNIST &
  \multicolumn{1}{c|}{DeiT-XS} &
  99.39\% &
  \multicolumn{1}{c|}{DeiT-XTiny} &
  \begin{tabular}[c]{@{}c@{}}99.07\\ ($\uparrow$+0.01\%)\end{tabular} \\ \hline
\multirow{3}{*}{MiniViT ~\cite{zhang2022minivit}} &
  \multirow{3}{*}{Weight distillation} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Self-attention distillation + \\ Hidden-state distillation + \\ prediction loss\end{tabular}} &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}RegNet-\\ 16GF\end{tabular}}} &
  \multirow{3}{*}{82.9\%} &
  \multicolumn{1}{c|}{\multirow{3}{*}{DeiT-B}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}83.2\\ ($\uparrow$+1.4\%)\end{tabular}} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\ \hline
\multirow{3}{*}{TinyViT ~\cite{wu2022tinyvit}} &
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Reusing the teachers' \\ prediction \& data \\ augmentation for student\end{tabular}} &
  \multirow{3}{*}{Cross entropy loss} &
  \multirow{3}{*}{ImageNet-1k~\cite{5206848}} &
  \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}CLIP-\\ ViT-L\end{tabular}}} &
  \multirow{3}{*}{84.8\%} &
  \multicolumn{1}{c|}{Swin-T} &
  \begin{tabular}[c]{@{}c@{}}83.4\\ ($\uparrow$+2.2\%)\end{tabular} \\ \cline{7-8} 
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{\multirow{2}{*}{DeiT-S}} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}82.0\\ ($\uparrow$+2.1\%)\end{tabular}} \\
 &
   &
   &
   &
  \multicolumn{1}{c|}{} &
   &
  \multicolumn{1}{c|}{} &
   \\ \hline
DearKD ~\cite{chen2022dearkd} &
  Self-generative data &
  \begin{tabular}[c]{@{}c@{}}MSE distillation +\\ Cross entropy+ Intra\\ -divergence distillation loss\end{tabular} &
  ImageNet-1k~\cite{5206848} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}ResNet-\\ 101\end{tabular}} &
  77.37\% &
  \multicolumn{1}{c|}{DeiT-Ti} &
  \begin{tabular}[c]{@{}c@{}}71.2\\ ($\downarrow$+1.0\%)\end{tabular} \\ \hline
\end{tabular}%
}
\vspace{-5mm}
\end{table}