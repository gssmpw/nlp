\section{Model Compression}\label{model_com}
Model compression is a key technique for deploying a model on edge devices with limited computational power and memory while maintaining model performance regarding accuracy, precision, and recall. It mainly focuses on lowering latency or reducing memory and energy consumption during inference. However, model compression on ViT still needs extensive exploration due to complex architecture and high resource usage tendencies. In this section, we will discuss prominent compression techniques for ViT models: pruning (Section~\ref{prun}), knowledge distillation (Section~\ref{know}), and quantization (Section~\ref{quan}). 

\subsection{Pruning}\label{prun}
% Static Pruning and Dynamic Pruning are two major pruning methodologies used to reduce the size of neural networks.
Pruning is used for reducing both memory and bandwidth. Most of the initial pruning techniques based on biased weight decay~\cite{hanson1988comparing},
second-order derivatives ~\cite{hassibi1992second}, and channels ~\cite{molchanov2016pruning}. Early days pruning techniques reduce the number of connections based on the hessian of the loss function~\cite{cheng2017survey,wu2021evolutionary}. In general, pruning removes redundant parameters that do not significantly contribute to the accuracy of results. The pruned model has fewer edges/connections than the original model. Most early pruning techniques are like brute force pruning, where one needs to manually check which weights do not cause any accuracy loss. Pruning techniques in deep learning became prominent post-2000 as neural networks (NNs) grew in size and complexity. The following subsections will discuss different pruning types and recent pruning techniques applied to ViT-based models.
% \begin{figure}
%   \centering
%   \includegraphics[scale=0.4]{assets/pruning.png}
%   \caption{A sample overview after pruning}
%   \label{fig:pruning}
% \end{figure}
\subsubsection{Types}\hfill\\ 
In recent studies, various pruning techniques have been utilized to optimize ViT models, categorizing them into different methods based on their approach and application timing. For example, unstructured pruning targets individual weights for removal, whereas structured pruning removes components at a broader scale, like layers or channels. On the one hand, static pruning is predetermined and fixed, which is ideal for environments with known constraints. On the other hand, dynamic pruning offers real-time adaptability, potentially enhancing model efficiency without sacrificing accuracy. Another pruning method, cascade pruning, is highlighted as a hybrid approach, integrating the iterative adaptability of dynamic pruning with the structured approach of static pruning. The following subsections will be a detailed discussion of different pruning types.
% For a detailed discussion about pruning types, please refer to Appendix B. 
\newline\hfill\break
\noindent \textbf{Unstructured vs Structured Pruning} Unstructured pruning removes individual weights or parameters from the network based on certain criteria. Unstructured pruning can result in highly sparse networks. However, it often does not lead to computational efficiency as the sparsity is not aligned with the memory access patterns or computational primitives of hardware accelerators. However, structured pruning applies to the specific components of the network, especially in layers, neurons, or channels. Structured pruning leads to more hardware-friendly sparsity patterns but often at the cost of higher accuracy loss. Cai et al. ~\cite{10.1145/3543622.3573044} proposed a two-stage coarse-grained/fine-grained structured pruning method based on top-K sparsification and reduces 60\% overall computation in the embedded NNs. In a recent survey~\cite{he2023structured}, He et al. discussed a range of SOTA structured pruning techniques, covering topics such as filter ranking methods, regularization methods, dynamic execution, neural architecture search (NAS), the lottery ticket hypothesis, and the applications of pruning. \\
\input{Tables/table_pruning}
    % Each technique is summarized and compared and the differences between structured pruning and its unstructured counterpart are highlighted.
\noindent \textbf{Static vs. Dynamic Pruning} Figure~\ref{fig:staticvsdynamic} demonstrates the workflow of the static and dynamic pruning. Static pruning works at the offline inference level, whereas dynamic pruning performs at the runtime level. Moreover, static pruning applies during training, where a fixed portion of a NN's components, such as neurons, channels, or weights, is removed or pruned. In static pruning, the decision on which components to prune is typically made before the training begins, and the pruning schedule remains constant throughout training. Static pruning is helpful in scenarios where the hardware constraints are well-defined. One of the most used static pruning techniques is Magnitude-based pruning~\cite{han2015learning}. In magnitude-based pruning, given a pruning rate \textbf{r}, weights whose absolute value is among the smallest \textbf{r\%} are pruned. In other words, for a weight matrix \textbf{W} of a layer, weights \textbf{w} in \textbf{W} are pruned if \textbf{\textbar w\textbar $\leq$ threshold}, where the \textbf{threshold} is determined such that the proportion of \textbf{\textbar w\textbar $\leq$ threshold} is \textbf{r}.
\begin{figure*}[h]
  \centering
  \includegraphics[scale=0.30]{assets/pruning_dynamic.png}
  \caption{Static Pruning vs Dynamic Pruning}
  \label{fig:staticvsdynamic}
\end{figure*}
In contrast, dynamic pruning applies during the runtime based on specific criteria, such as the importance of neurons or weights. One of the significant drawbacks of static pruning is that it relies on a fixed pruning schedule and rate, which is determined before training begins. That means that static pruning does not adjust to the network's learning progress or the changing importance of neurons or weights during training. Dynamic pruning evaluates and adjusts the pruning criteria during training based on real-time importance assessments to overcome the limitation of static pruning. Most of the recent pruning techniques use dynamic pruning techniques~\cite{song2022cp,bai2022dynamically,kong2022spvit} to get the accuracy without loss of any information.
\newline\hfill\break
\noindent \textbf{Cascade Pruning} Cascade pruning combines the iterative nature of dynamic pruning with predefined aspects resembling static pruning. Cascade pruning operates through multiple sequential iterations, also known as stages. Each stage selects a predefined portion of the network's components, such as neurons, channels, or weights, for pruning. The criteria for choosing which components to prune can vary between iterations. The ability to adjust pruning criteria between iterations makes cascade pruning adaptable to evolving training data, tasks, or hardware constraints.
% \subsubsection{Types}

% \paragraph{1. Unstructured vs. Structured Pruning.} \hfill \break
% \noindent{\bf Unstructured pruning} removes individual weights or parameters from the network based on certain criteria. Unstructured pruning can result in highly sparse networks. However, it often does not lead to computational efficiency as the sparsity is not aligned with the memory access patterns or computational primitives of hardware accelerators.

% \noindent{\bf Structured pruning} applies to the specific component of the network, especially in layers, neurons, or channels. Structured pruning leads to more hardware-friendly sparsity patterns but often at the cost of higher accuracy loss. Cai et al. ~\cite{10.1145/3543622.3573044} proposed a two-stage coarse-grained/fine-grained structured pruning method based on top-K sparsification and reduces 60\% overall computation in the embedded NN. In a recent survey ~\cite{he2023structured}, He et al. discussed surveys of a range of SOTA structured pruning techniques, covering topics such as filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. 
%     % Each technique is summarized and compared and the differences between structured pruning and its unstructured counterpart are highlighted.
% \paragraph{2. Static vs Dynamic Pruning.} As shown in figure \ref{fig:staticvsdynamic}, static pruning works at the offline inference level, whereas dynamic pruning performs at the runtime level.
% % ~\cite{liang2021pruning}
% \begin{figure}[htp]
%   \centering
%   \includegraphics[scale=0.4]{assets/pruning_dynamic.png}
%   \caption{Static Pruning and Dynamic Pruning}
%   \label{fig:staticvsdynamic}
% \end{figure}

% \noindent {\bf Static pruning} applies during training, where a fixed portion of a neural network's components, such as neurons, channels, or weights, is removed or pruned. In static pruning, the decision on which components to prune is typically made before the training begins, and the pruning schedule remains constant throughout training. Static pruning is helpful in scenarios where the hardware constraints are well-defined. However, this technique has drawbacks with iterative pruning methods, where the process involves a cycle of pruning some neurons or weights, retraining the pruned network, and then pruning again. One of the most used static pruning techniques is Magnitude-based pruning~\cite{han2015learning}. In magnitude-based pruning, given a pruning rate \textbf{r}, weights whose absolute value is among the smallest \textbf{r\%} are pruned. In other words, for a weight matrix \textbf{W} of a layer, weights \textbf{w} in \textbf{W} are pruned if \textbf{|w| <= threshold}, where the \textbf{threshold} is determined such that the proportion of \textbf{|w| <= threshold} is \textbf{r}.

% \noindent {\bf Dynamic pruning} applies during the runtime based on specific criteria, such as the importance of neurons or weights. One of the significant drawbacks of static pruning is that it relies on a fixed pruning schedule and rate, which is determined before training begins. That means that static pruning does not adjust to the network's learning progress or the changing importance of neurons or weights during training. Dynamic pruning evaluates and adjusts the pruning criteria during training based on real-time importance assessments to overcome the limitation of static pruning. Most of the recent pruning techniques use dynamic pruning techniques~\cite{song2022cp,bai2022dynamically,kong2022spvit} to get the accuracy without loss of any information.

% \noindent {\bf Cascade pruning} combines the iterative nature of dynamic pruning with predefined aspects resembling static pruning. Cascade pruning operates through multiple sequential iterations, also known as stages. Each stage selects a predefined portion of the network's components, such as neurons, channels, or weights, for pruning. The criteria for choosing which components to prune can vary between iterations. The ability to adjust pruning criteria between iterations makes cascade pruning adaptable to evolving training data, tasks, or hardware constraints.


\subsubsection{Pruning Techniques for Vision Transformer}
\hfill\\
Pruning methods for ViT-based models remain an underexplored area, with only a handful of studies in recent years. This section provides a brief overview of the current SOTA pruning techniques for ViTs.\\ 
 
\noindent \textbf{Important-based Pruning} Zhu et al.~\cite{zhu2021vision} pioneered a ViT pruning approach that removes dimensions with lower importance scores, achieving a high pruning ratio without sacrificing accuracy. Their study observed that a significant portion of ViT's computational cost comes from multi-head self-attention (MSA) and multi-layer perceptron (MLP). To address this, they introduced visual transformer pruning (VTP)—the first dedicated pruning algorithm for ViTs. VTP operates in three key steps: (1) L1 sparse regularization is applied during training to identify less significant channels, (2) channel pruning eliminates redundant computations, and (3) finetuning. This VTP approach managed to preserve the robust representative capability of the transformer while reducing the model's computational cost. Another recent study~\cite{yu2022width} utilized learning a unique saliency score and threshold for each layer to implement width pruning. This learning saliency score allows for a more effective, non-uniform allocation of sparsity levels across different layers. Additionally, The proposed model utilized supplementary plug-in classifiers to prune the transformer's trailing blocks. This approach enabled the construction of a sequential variant of the pruned model, capable of removing blocks within a single training epoch, thereby simplifying the control of the trade-off between the network's performance and the rate of pruning ~\cite{yu2022width}. Moreover, another study by Tang et al.~\cite{tang2022patch} proposed a patch-slimming approach that reduced unimportant patches in a top-down manner. The authors calculated their importance scores for the final classification feature to identify unimportant patches. The proposed method also identified the important patches in the last layer of the blocks and then utilized them to select the previous layer patches.\\ 

\noindent \textbf{Token Pruning} Kong et al.~\cite{kong2022spvit} introduced a latency-aware soft token pruning framework, SP-ViT. This framework was implemented on vanilla transformers such as data-efficient image transformers (DeiT)~\cite{touvron2021training} and swin transformers~\cite{liu2021swin}. The authors proposed a dynamic attention-based multi-head token selector for adaptive instance-wise token selection. Later, they incorporated a soft pruning method that consolidated less informative tokens into a package token instead of entirely discarding them identified by the selector module. The authors deployed their proposed method on ImageNet-1k with baseline models Swin-S, Swin-T, PiT-S, and PiT-Xs.\\ 

\noindent \textbf{Structure Pruning} Recently, many studies on ViT pruning have embraced structure pruning techniques to optimize model efficiency. Yu et al.~\cite{yu2022unified} proposed a structure pruning in a ViT named UVC where they pruned the head's number and dimension inside each layer. Experiments of this paper were conducted in various ViT models (e.g., DeiT-Tiny and T2T-ViT-14) on ImageNet-1k~\cite{krizhevsky2012imagenet} datasets. DeiT-Tiny~\cite{touvron2021training} cut down to 50\% of the original FLOPs while not dropping accuracy much in this study. Another study~\cite{zheng2022savit} proposed structure pruning on MSA attention and feedforward neural network (FFN) by removing unnecessary parameter groups. Other studies on structure pruning named NViT~\cite{yang2023global} proposed hessian-based structure pruning criteria comparable across all layers and structures. Moreover, it incorporated latency-aware regularization techniques to reduce latency directly. Another study on the structure pruning in ViT called S\textsuperscript{2}ViT~\cite{chen2021chasing} removed submodules like self-attention heads by manipulating the weight, activations \& gradients.\\

\noindent \textbf{Cascade Pruning} Cascade pruning combines multiple pruning techniques to reduce parameters and GFLOPs while preserving accuracy. A standout method named VTC-LFC~\cite{wang2022vtc} aimed to improve the identification of informative channels and tokens in a model, leading to better accuracy preservation. This approach introduced a bottom-up cascade (BCP) pruning strategy that gradually prunes tokens and channels, starting from the first block and advancing to the last. The pruning process is controlled by a hyper-parameter called a \textbf{global allowable drop}, ensuring the performance drop remains within an acceptable range. Additionally, BCP guarantees efficient compression without sacrificing model performance by pruning each block and immediately stopping the compression process when the performance drop reaches a predefined threshold. Another cascade ViT pruning ~\cite{song2022cp} utilized the sparsity for pruning PH-regions (containing patches and heads) in the MSA \& FFN progressively and dynamically. The authors conducted experiments on three different types of datasets: ImageNet-1k~\cite{krizhevsky2012imagenet}, CIFAR-10~\cite{cifar10}, and CIFAR-100~\cite{cifar10}.\\


 \noindent \textbf{Miscellaneous Approaches in Pruning} Hou et al.~\cite{hou2022multi} introduced a multi-dimensional pruning strategy for ViTs, leveraging a statistical dependence-based criterion to identify and remove redundant components across different dimensions. Beyond this, several pruning techniques have been developed to accelerate ViTs, particularly for edge devices, including column balanced block pruning ~\cite{9424344}, end-to-end exploration ~\cite{chen2021chasing}, gradient-based learned runtime pruning ~\cite{li2022accelerating}. These techniques have shown stability in applying pruning on ViT models without compromising accuracy.\\
% Pruning methods applied to ViT-based models still need to be explored, with only a limited number of studies conducted in recent years. Hou et al. ~\cite{hou2022multi} proposed multi-dimensional techniques for ViT and a statistical dependence-based pruning criterion for different dimensions to identify deleterious components. In addition, Zhu et al. ~\cite{zhu2021vision} proposed a ViT-based pruning approach, where the number of dimensions with less important scores was omitted to achieve a high pruning ratio without compromising accuracy. The paper observed that a significant portion of computation was allocated to multi-head self-attention (MSA) and multi-layer perceptron (MLP) components. Consequently, the authors concentrated on diminishing the floating-point operations per second (FLOPs) of MSA and MLP. The paper introduced the visual transformer pruning (VTP) algorithm, marking the first pruning method specifically designed for ViTs. The algorithm proceeds in three steps: firstly, L1 sparse regularization is employed during training, followed by channel pruning, and finally, fine-tuning. This VTP approach managed to preserve the robust representative capability of the transformer while dramatically cutting down the model's calculation volume and parameter count. Alternate pruning techniques on ViT-based pruning mitigate both width and depth dimensions simultaneously ~\cite{yu2022width}. The method involved learning a unique saliency score and threshold for each layer to implement width pruning. This learning saliency score allows for a more effective, non-uniform allocation of sparsity levels across different layers. Additionally, The proposed model utilized supplementary plug-in classifiers for the pruning of the transformer's trailing blocks. This approach enabled the construction of a sequential variant of the pruned model, capable of removing blocks within a single training epoch, thereby simplifying the control of the trade-off between the network's performance and the rate of pruning ~\cite{yu2022width}.

% Given the computational complexity, the intrinsic data patterns of ViT, and the development requirements of edge devices, Kong et al.~\cite{kong2022spvit} introduced a latency-aware soft token pruning framework, SP-ViT. This framework was implemented on vanilla transformers such as data-efficient image transformers (DeiT)~\cite{touvron2021training} and swin transformers~\cite{liu2021swin}. The authors proposed a dynamic attention-based multi-head token selector for adaptive instance-wise token selection. Later, they incorporated a soft pruning method that consolidated less informative tokens into a package token instead of entirely discarding them identified by the selector module. The authors deployed their proposed method on ImageNet-1k with baseline models Swin-S, Swin-T, PiT-S, and PiT-Xs. The authors evaluated their method and achieved  top-1 accuracy with minimum accuracy loss.

% Moreover, another study by Tang et al.~\cite{tang2022patch} proposed a patch-slimming approach that mitigates useless patches in a top-down manner. The authors calculated their importance scores to the final classification feature to identify the useless patches. The proposed method also identified the useful patches in the last layer of the blocks and then used them to select the previous layer patches. The authors implemented their method based on the official ViT pre-trained models, and the results of their experiment are documented in Table  \ref{tab:pruning_result}.

% Furthermore, many recent papers on ViT pruning follow structure pruning methods. Yu et al.~\cite{yu2022unified} proposed a structure pruning in a ViT named UVC where they pruned the head's number and dimension inside each layer. Experiments of this paper were conducted in various ViT models (e.g., DeiT- Tiny and T2T-ViT-14) on ImageNet-1k~\cite{krizhevsky2012imagenet} datasets. DeiT-Tiny~\cite{touvron2021training} cut down to 50\% of the original FLOPs while not dropping accuracy much in this study. Another study~\cite{zheng2022savit} proposed structure pruning on MSA attention and feedforward neural network (FFN) by removing unnecessary parameter groups. Other studies on structure pruning named NViT~\cite{yang2023global} proposed hessian-based structure pruning criteria comparable across all layers and structures. Moreover, it incorporated latency-aware regularization techniques to reduce latency directly. Another study on the structure pruning in ViT called S\textsuperscript{2}ViT removed submodules like self-attention heads by manipulating the weight, activations \& gradients. 

% Moreover, there is another type of pruning method applied on ViT architectures named cascade pruning, where one or more pruning methods were combined to get better accuracy. A proposed method named VTC-LFC~\cite{wang2022vtc} aimed to improve the identification of informative channels and tokens in a model, leading to better accuracy preservation. It achieved this by introducing a bottom-up cascade (BCP) pruning strategy. It gradually prunes tokens and channels, starting from the first block and progressing toward the last block. The pruning is controlled by a hyper-parameter called a global allowable drop, which ensures that the performance drop remains within an acceptable range. The BCP process ensures that the compression is completed for each block when the performance drop reaches the threshold. This approach enabled precise pruning and compression, ultimately enhancing the model's efficiency while maintaining its performance. Another cascade ViT pruning ~\cite{song2022cp} utilized the sparsity for pruning PH-regions (containing patches and heads) in the MSA \& FFN progressively and dynamically. The authors conducted experiments on three different types of datasets: ImageNet-1k~\cite{krizhevsky2012imagenet}, CIFAR-10~\cite{cifar10}, and CIFAR-100~\cite{cifar10}. In recent times, there are pruning techniques used in  ViT for different accelerations on edge devices such as column balanced block pruning ~\cite{9424344}, end-to-end exploration ~\cite{chen2021chasing}, gradient-based learned runtime pruning ~\cite{li2022accelerating}. These techniques have shown stability in applying pruning on ViT models without compromising accuracy.
\subsubsection{Discussion}\hfill\\
Pruning is utilized as a fundamental way to reduce the computation of the pre-trained ViT models. For ViT, the development of the pruning methods has systematically covered each perspective of model design, making the current pruning methods more flexible and well-organized for ViT models. We summarized all the core information about pruning techniques on ViT in comparison to GFlops reductions with the percentage of reduction from baseline, parameters reductions, and top-1 accuracy in Table~\ref{tab:pruning_result}. Top-1 accuracy shows the accuracy loss with the proposed methodology from baseline backbone ViT architecture. Recent pruning studies on ViT models, as shown in Table~\ref{tab:pruning_result}, have predominantly focused on the ImageNet-1k dataset~\cite{5206848}, with the exception of CP-ViT~\cite{song2022cp}, which conducted experiments on the CIFAR dataset~\cite{cifar10}. However, the training/finetuning cost is one of the critical points to consider in the hardware-inefficient ViT models in the pruning methods. Therefore, training-efficient or fine-tuned free pruning techniques need more attention in the near future for efficient deployment on the edge. This necessitates a more precise estimation of parameter or block sensitivity using limited data, as well as a deeper exploration of the information embedded within hidden features during training and inference.
\input{Tables/table_KD_class}

\subsection{Knowledge Distillation}\label{know}
Knowledge distillation (KD) is another model compression technique in machine learning where a smaller model (the "student") is trained to reproduce the behavior of a larger model (the "teacher"). The purpose is to transfer the "knowledge" from the larger model to the smaller one, thereby reducing computational resources without significantly losing accuracy.
% \input{table_pruning}
% ~\cite{gou2021knowledge}
% \begin{figure}[H]
%   \centering
%   \includegraphics[scale=0.5]{assets/knowledge_dis.png}
%   \caption{Overview of Knowledge distillation}
%   \label{fig:knowledge}
% \end{figure} 
% Figure \ref{fig:knowledge} is the general overview of applying knowledge distillation, enabling the student model to achieve comparable performance to the teacher model while being more computationally efficient.
Pruning is a direct way to reduce the complexity of the original model, whereas KD involves training a new, more compact model that is easy to deploy. By using KD during finetuning, the pruned model can benefit from the insights and information captured by the larger teacher model. KD helps to regain even surpass the original performance and compensate for the accuracy loss during the other compression techniques (e.g., pruning, quantization). 
\subsubsection{KD techniques for vision transformer}\hfill\\
Touvron et al.~\cite{touvron2021training} leveraged KD to train the transformer with a significantly smaller dataset than the traditionally required dataset. The authors introduced a distillation token, an additional learnable vector used alongside the class token during training. The proposed method achieved 84.5\% top-1 accuracy on the ImageNet-1k~\cite{dosovitskiy2020image} dataset, requiring fewer training data resources and computing power than ViT without KD.

Another study by Hao et al. ~\cite{hao2022learning} utilized every patch information to introduce a fine-grained manifold distillation method. In the manifold distillation method, the authors considered ViT as a feature projector that sets image patches into a sequence of manifold space layer by layer. The authors then teach the student layers to generate output features having the same patch-level manifold structure as the teacher layer for manually selected teacher-student layers. These output features are normalized and reshaped to compute a manifold relation map, a representation of the manifold structure of the features. However, the manifold relation map computation is resource-consuming and needs to simplify the computation. To solve the computational issue, the authors decoupled the manifold relation map into an intra-image relation map, an inter-image relation map, and a randomly sampled relation map. The authors utilized manifold distillation loss (MD Loss), the sum of individual loss from all three decoupled manifold relation maps.
\input{Tables/table_kD_ob}
Moreover, Lin et al. ~\cite{lin2022knowledge} proposed a one-to-all spatial matching KD technique. The proposed approach involves distilling the knowledge from each pixel of the teacher feature to all spatial locations of the student features based on their similarity. This similarity is determined using a target-aware transformer. By leveraging this target-aware transformer, the teacher's feature information can be effectively transferred and distilled to different spatial locations in the student's features.

Furthermore, Wang et al. ~\cite{9747484} proposed another KD method called attention probes. The main idea of this paper was to streamline ViTs through a two-step process using unlabelled data gathered from varied sources. The authors developed an 'attention probe' in the initial phase to discern and select significant data. The authors then used the selected data to instruct a compact student transformer by applying a probe-based KD algorithm. This algorithm was designed to optimize the resemblance between the resource-intensive teacher model and the more efficient student model, considering both the final outputs and intermediate features. The proposed method used cross-entropy (CE) and probe distillation functions for distilling intermediate features for calculating loss. Another study named DearKD ~\cite{chen2022dearkd} proposed the KD methods on self-generative data and used representational KD on intermediate features with response-based KD. The proposed paper used mean square error (MSE) distillation loss for hidden features, CE loss for hard label distillation, and intra-divergence distillation loss function to calculate the loss. It was noteworthy to see DearKD surpass the performance of the baseline transformer that trained with all ImageNet data, even though it only used 50\% of the data. The authors then evaluated the proposed technique on the ImageNet dataset and achieved 74.8\% top-1 accuracy in a tiny version, which is 2\% better than the DeiT-Tiny ~\cite{touvron2021training}. Moreover, TinyViT~\cite{wu2022tinyvit} highlights that smaller ViTs can benefit from larger teacher models trained on extensive datasets, such as distilling the student model on ImageNet-21k and finetuning on ImageNet-1k. To optimize computational memory, TinyViT introduced a strategy that pre-stores data augmentation details and logits for large teacher models, reducing memory overhead. Additionally, MiniViT~\cite{zhang2022minivit} argues by introducing a weight multiplexing strategy to reduce parameters across consecutive transformer blocks. Moreover, they employed weight distillation on self-attention mechanisms to transfer knowledge from large-scale ViT models to the smaller, weight-multiplexed MiniViT models. 

% Further research ~\cite{wu2022tinyvit,zhang2022minivit} explored kD as a model compression technique to improve the data efficiency required by transformers and showed better improvement than CNN-based models.
\input{Tables/table_KD_seg}


% Although KD in medical imaging tasks with ViT is quite an unexplored area, one of the recent studies by Park et al. ~\cite{park2022self} proposed distillation for self-supervision and self-train Learning (DISTL). DISTL was one of the first attempts of KD in medical images, inspired by the learning process of radiologists. DISTL enhanced the performance of ViT by combining self-supervision and self-training through knowledge distillation. In external validation across three hospitals for tuberculosis, pneumothorax, and COVID-19 diagnosis, DISTL exhibited progressive performance improvements with increasing amounts of unlabeled data. This work was one of the earliest to show that KD can be applied to medical imaging tasks.
\subsubsection{Discussion}\hfill\\
A key strength of ViT models lies in their scalability to high parametric complexity; however, this demands significant computational resources and incurs substantial costs. KD offers a way to transfer knowledge into more compact student models, yet challenges remain, particularly in the vision domain. One primary challenge involves the high training costs, as logits-based and hint-based KD methods necessitate extensive GPU memory for the distillation process. 

\noindent To show the results of the discussed techniques, we divide all the published results into three different Tables (\ref{tab:KD_result_classification}--\ref{tab:semantic_segmentation}). The results are divided based on popular CV tasks named image classification (Table~\ref{tab:KD_result_classification}), object detection (Table \ref{tab:KD_object}), and semantic segmentation (Table~\ref{tab:semantic_segmentation}). We observe that most of the papers are tested for image classification tasks, whereas a limited number of papers are evaluated for object detection and semantic segmentation. Table~\ref{tab:KD_result_classification} documents all the applied KD in image classification on the ViT backbone. As it is crucial to see the top-1 for classification problems, we summarize these accuracies for teacher and student models from the proposed papers. Moreover, it is essential to calculate the AP\textsuperscript{box} in object detection, and we document the AP\textsuperscript{box} results for both without applying KD and with KD for students model to compare the scenario in Table \ref{tab:KD_object} better. Lastly, Table~\ref{tab:semantic_segmentation} illustrates the semantic segmentation results for KD methods in ViT. All the current papers in Table~\ref{tab:semantic_segmentation} used mean intersection over union (mIoU) as a metric to calculate the accuracy. We separately document results with teacher models, student models, and proposed methods to better understand the improvement after applying KD.

\subsection{Quantization}\label{quan}
Quantization is used to reduce the bit-width of the data flowing through a NN model. So, it is used primarily for memory saving, faster inference times, and simplifying the operations for compute acceleration. That makes quantization essential for deploying NN on edge devices with limited computational capabilities. Quantization can be applied to different aspects of techniques. We organize our quantization discussion into two subsections. Firstly, we categorize different quantization techniques applied according to different aspects in Section~\ref{types_quanti}. Lastly, we discuss different quantization techniques in ViT in Section~\ref{vit_quanti}.

\subsubsection{Taxonomy of Quantization Methods} \label{types_quanti}
\hfill\\
Table~\ref{table:quan_classification} gives the overview of different quantization techniques and their pros and cons for computer vision tasks. Table~\ref{table:quan_classification} divides the quantization method from the aspects of quantization schemes, quantization approaches, calibration methods, granularity, and other independent techniques. Each aspect has multiple types of quantization techniques applied in different studies. In general, most quantization methods drop accuracy after applying quantization and need finetuning to regain the accuracy.\\
% For a detailed discussion about pruning types, please refer to Appendix C. 

\noindent \textbf{Quantization Schemes} Quantization schemes are broadly categorized into uniform and non-uniform techniques. Any weight or activation values in an NN can follow either a uniform or non-uniform distribution. Uniform quantization maps continuous weight and activation values to discrete levels with equal spacing between quantized values. However, non-uniform quantization sets different quantization steps for different parts of the data based on their distribution and importance to the final performance of the model. Non-uniform quantization can improve accuracy but is often more complex to implement in hardware. \\
 
\noindent \textbf{Quantization Approaches} Quantization approaches can be broadly classified based on whether they require retraining. Quantization-aware training (QAT) incorporates quantization into both forward and backward passes during training, allowing the model to adapt to lower-precision representations. However, QAT is resource-intensive due to the need for retraining. In contrast, Post-training quantization (PTQ) is a more efficient approach that applies quantization after a model has been fully trained in floating point (FP) precision, reducing the precision of weights and activations without additional training. While PTQ is less resource-intensive, it typically results in a higher accuracy drop compared to QAT~\cite{nagel2021white}.\\

\noindent \textbf{Calibration Methods} Calibration is a needed process of determining the appropriate scaling factors during finetuning methods that map the continuous range of FP values to discrete integer values. Calibration mainly ensures that the range of the quantized values matches the range of the original FP values as closely as possible. There are two types when choosing the range. One is dynamic quantization, and another is static quantization. The weights are quantized statically in dynamic quantization, but activations are quantized dynamically at runtime. Static quantization is quantized post-training. Unlike dynamic quantization, static quantization applies to weights and activations before deploying the model.\\

\noindent \textbf{Granularity} Another aspect of quantization techniques is the granularity of the clipping range of an NN. Layer-wise quantization is one of the granularity techniques where all weights and activations within a layer are quantized using the same scale. Channel-wise quantization, also referred to as per-channel quantization, is another granularity technique applied during the quantization of NN. Different scaling factors are computed for each channel of the weights in channel-wise quantization, meaning different layers can use different quantization parameters. \\

\noindent \textbf{Others} There are other techniques, such as mixed-precision quantization and hardware-aware quantization. Mixed-precision uses different parts (e.g., channels, layers) of the model that are quantized to different numerical precisions. Unlike uniform quantization, where the entire model is quantized to the same bit-width (like INT8), mixed-precision involves carefully choosing the bit-width for each layer or even each channel within a layer based on their sensitivity and contribution to the final performance of the model. Hardware-aware quantization is another technique that tailors a neural network's precision reduction process to the specific hardware to deploy on. This technique optimizes the model for the target hardware by adjusting the quantization parameters to match the hardware's operations~\cite{wang2019haq}, such as latency and throughput.
\input{Tables/table}
\subsubsection{Quantization Techniques for Vision Transformer} \label{vit_quanti}\hfill\\
% Applying quantization in ViT models is quite new in the neural network sectors. ViT consists of multiple layers, including a self-attention mechanism and a feed-forward neural network. As data passes through these layers, different layers learn to focus on different features of the input data. Applying quantization to ViT can be challenging due to its complexity. Additionally, the loss from quantization can significantly impact the self-attention mechanisms, potentially reducing the model's overall performance. Table \ref{quantization_tech_classification} \& \ref{tab:quantization_object} overview the quantization techniques for image classification and object detection in ViT models. From our observation, most of the quantization-related papers in ViT explored the PTQ rather than QAT. Table \ref{quantization_tech_classification} shows the top-1 accuracy of the proposed methods in image classification on ViT architectures and the improvement from their baseline method.\\
Applying quantization in ViT models is quite new in the neural network sectors. ViT consists of multiple layers, including a self-attention mechanism and a feed-forward neural network. As data passes through these layers, different layers learn to focus on different features of the input data. Applying quantization to ViT can be challenging due to its complexity. Additionally, the loss from quantization can significantly impact the self-attention mechanisms, potentially reducing the model's overall performance. Quantization methods on ViT can be broadly categorized into two main approaches based on their reliance on training or finetuning: PTQ and QAT.
\noindent \paragraph{PTQ Techniques for Vision Transformer}\hfill \\In the current scenario, PTQ is widely used for ViT because it offers an efficient way to meet the computational requirements on edge without additional training or finetuning, making it ideal for resource-constrained deployments. PTQ works can be divided into two categories~\cite{niu2023improving,zheng2022leveraging}: statistic-based PTQ and learning-based PTQ. Statistic-based PTQ methods focus on finding optimal quantization parameters to reduce quantization errors. In contrast, learning-based PTQ methods involve finetuning both the model weights and quantization parameters for improved performance~\cite{lv2024ptq4sam}. \\

\noindent \textbf{Statistic-Based PTQ methods} Most of the current PTQ works on ViT follow \textbf{statistic-based methods}. PTQ4ViT~\cite{yuan2022ptq4vit}, one of the first search-based PTQ methods, addressed two key issues with base-PTQ for ViTs: 1) unbalanced distributions after softmax and asymmetric distributions after GELU. 2) traditional metrics are ineffective for determining quantization parameters. To solve the first problem, the authors proposed twin uniform quantization, which quantizes values into two separate ranges. Additionally, to solve the second problem, they introduced a Hessian-guided metric for improved accuracy instead of mean square error (MSE) and cosine distance. 

Building on advancements in PTQ on ViT, Ding et al.~\cite{ding2022towards} proposed APQ-ViT solved two problems of the existing quantization techniques. The authors first proposed a unified bottom-elimination blockwise calibration scheme to solve the inaccurate measurement during quantization value calculation for extremely low-bit representation. This blockwise calibration scheme enables a more precise evaluation of quantization errors by focusing on block-level disturbances that impact the final output. For the second challenge, they observed the "matthew-effect" in the softmax distribution, where smaller values shrink further, and larger values dominate. However, the existing quantizers ignored the mattew-effect of the softmax function, which costs information loss from the larger values. In response, the authors proposed matthew-effect preserving quantization (MPQ) for Softmax to maintain the power-law character to solve the second limitation, ensuring balanced information retention during quantization. Additionally, Liu et al. proposed NoisyQuant~\cite{liu2023noisyquant}, where they focused on adding a noisy bias to each layer to modify the input activation distribution before quantization reduced the quantization error. The noisy bias is a single vector sampled from a uniform distribution. The authors removed the impact of noisy bias after the activation-weight multiplication in the linear layer with a denoising bias so that the method could retrieve the correct output. Surprisingly, the experiment showed that adding a noisy bias improved top-1 accuracy compared to the PTQ4ViT~\cite{yuan2022ptq4vit} on the ViT-B, DEiT-B, and Swin-S model.
\begin{figure}[htb]
  \centering
  \includegraphics[scale=0.25]   {assets/fq_vit_1.png}
  \caption{Comparison of using full precision Softmax and log-int-softmax in quantized MSA inference in FQ-vit ~\cite{lin2021fq}}
  \label{fig:quanwsoft}
\end{figure}

However, recent advancements in statistic-based PTQ for ViTs have moved beyond converting FP32 precision (dequantization) during inference, pioneering integer-only fully quantized methods~\cite{lin2021fq,li2022vit,li2023repq}. Moreover, Lin et al. first introduced fully quantized PTQ techniques named  FQ-ViT~\cite{lin2021fq} leveraging the power-of-two factor (PTF) method to minimize performance loss and inference complexity. To solve the non-uniform distribution in attention maps and avoid the dequantizing to FP32 before softmax, they proposed log-int-softmax (LIS) replacing softmax. Additionally, they streamline inference further using 4-bit quantization with the bit-shift operator. Figure~\ref{fig:quanwsoft} (left) illustrated the traditional approach (left) where the traditional approach dequantized INT8 query (Q) and key (K) matrices to FP before softmax, re-quantizing afterward for attention computations. In contrast, the proposed method (Figure~\ref{fig:quanwsoft} (right)) introduced matrix multiplication followed by integer-based exponential (i-exp). The authors then utilized $Log_{2}$ quantization scale in the softmax function and converted the MatMul to BitShift between the quantized attention map and values (V). This fully integer workflow, including LIS in INT4 format, significantly reduces memory usage while maintaining precision. Extended from FQ-ViT, Li et al. introduced I-ViT~\cite{li2022vit}, the first integer-only PTQ framework for ViT, enabling inference entirely with integer arithmetic and bit-shifting, eliminating FP operations. In this framework, the authors utilized an integer-only pipeline named dyadic anthemic for non-linear functions such as dense layers. In contrast, non-linear functions, including softmax, GELU, and LayerNorm, were approximated with lightweight integer-based methods. The key contribution of this work is that Shiftmax and ShiftGEU replicated the behavior of their FP counterparts using integer bit-shifting. Despite I-ViT's reduction in bit-precision for parameters and its emphasis on integer-only inference, it retained its accuracy. For example, when I-ViT applied to DeiT-B, it achieved 81.74\% top-1 accuracy with 8-bit integer-only inference, outperforming I-BERT~\cite{kim2021bert} by 0.95\% (see Table~\ref{quantization_tech_classification}).

However, the current studies consider quantizers and hardware standards always antagonistic, which is partially true. RepQ-ViT~\cite{li2023repq} decouples the quantization and inference process to explicitly bridged via scale reparameterization between these two steps. The authors applied channel-wise quantization for the post-LayerNorm activations to solve the interchannel variations and $\log \sqrt{2}$ quantization for the post-softmax activations. In the inference, the reparameterized the layer-wise quantization and $\log 2$ quantization with minimal computational cost for respective activations. Using integer-only quantization for all layers lessened the computational cost dramatically and made them highly suitable for edge devices.

Additionally, recent studies have further advanced statistics-based PTQ for ViTs by incorporating \textbf{mixed-precision techniques}. Liu et al. ~\cite{liu2021post} first explored a mixed precision PTQ scheme for ViT architectures to reduce the memory and computational requirements. The authors estimated optimal low-bit quantization intervals for weights and inputs, used ranking loss to preserve self-attention order, and analyzed layer-wise quantization loss to study mixed precision using the L1-norm~\cite{wu2018l1} of attention maps and outputs. Using calibration datasets from CIFAR-10~\cite{cifar10}, ImageNet-1k~\cite{5206848}, and COCO2017~\cite{lin2015microsoft}, their method outperformed percentile-based techniques\cite{li2019fully} by 3.35\% on CIFAR-10 with ViT-B model. Recently. Tai et al.~\cite{tai2024mptq} and Ranjan et al.~\cite{ranjan2024lrp} both extended the mixed precision PTQ techniques on ViT. MPTQ-ViT~\cite{tai2024mptq} utilized the smoothQuant~\cite{xiao2023smoothquant} with bias term (SQ-b) to address the asymmetry in activations, reducing clamping loss and improving quantization performance. The authors proposed a search-based scaling factor ratio (OPT-m) to determine the quantization parameters. Later, they incorporate SQ-b and OPT-m to propose greedy mixed precision PTQ techniques for ViT by allocating layer-wise bit-width. Additionally, Ranjan et al.~\cite{ranjan2024lrp} proposed LRP-QViT~\cite{ranjan2024lrp}, an explainability-based approach by assessing each layer's contribution to the model's predictions, guiding the assignment of mixed-precision bit allocations based on layer importance. The authors also clipped the channel-wise quantization to eliminate the outliers from post-LayerNorm activations, mitigating severe inter-channel variations and enhancing quantization robustness. Zhong et al. proposed ERQ~\cite{zhong2024erq} to mitigate the error arising during quantization from weight and activation quantization separately. The authors introduced activation quantization error reduction to reduce the activation error, which is like a ridge regression problem. The authors also proposed weight quantization error reduction in an interactive approach by rounding directions of quantized weight coupled with a ridge regression solver.\\

\noindent \textbf{Learning-Based PTQ Methods} While most current PTQ methods for ViTs are statistic-based, there are only a few that utilize learning-based approaches. Existing PTQ methods for ViTs face challenges with inflexible quantization of post-softmax and post-GELU activations, which follow power-law-like distributions. To solve this problem, Wu et al. proposed Adalog~\cite{wu2025adalog}. The authors optimized the logarithmic base to better align with the power-law distribution of activations while ensuring hardware-friendly quantization. The authors applied their proposed methods to post-softmax and post-GELU activations through bias reparameterization. Additionally, a fast progressive combining search strategy is proposed to efficiently determine the optimal logarithm base, scaling factors, and zero points for uniform quantizers. Moreover, a recently proposed by Ramachandran et al. named CLAMP-ViT~\cite{ramachandran2025clamp} adopted a two-stage approach between data generation and model quantization. The authors introduced a patch-level constrastive learning scheme to generate meaningful data. The authors also leveraged contrastive learning in layer-wise evolutionary search for fixed and mixed-precision quantization to identify optimal quantization parameters. In conclusion, the learned-based PTQ techniques on ViT are largely explored for low-bit quantization.

% \noindent \textbf{PTQ techniques for vision transformer} PTQ techniques are popular because no training is required. Liu et al. ~\cite{liu2021post} first explored a mixed precision post-training quantization scheme for ViT architectures to reduce the memory and computational requirements. First, the authors estimated the optimal low-bit quantization intervals for weights and inputs. Then, they introduced a ranking loss to keep the relative order of the self-attention results after quantization. Lastly, the authors analyzed the relationship between the quantization loss of each layer and studied mixed-precision quantization using the L1-norm~\cite{wu2018l1} of each attention map and output feature. For evaluation, the authors randomly selected 100 images from CIFAR-10 and CIFAR-100 datasets~\cite{cifar10} and 1000 images from ImageNet~\cite{5206848} and COCO2017~\cite{lin2015microsoft} datasets from the training dataset as the calibration dataset. The proposed technique outperformed percentile-based methods ~\cite{li2019fully}  by 3.35\% and 2.07\% on the CIFAR-10 dataset for ViT-base and ViT-large, respectively. Yuan et al. extended the base-PTQ (see Table \ref{quantization_tech_classification}) for ViT and proposed a framework named  PTQ4ViT~\cite{yuan2022ptq4vit}. The authors identified two major problems of dropping accuracy using base-PTQ for ViT. The authors identified that the first problem was the difference between distribution values after softmax and GELU and Gaussian distribution. The authors also found that distribution values after softmax are unbalanced, and distribution values after GELU are asymmetric. To solve these problems, the authors proposed twin uniform quantization. 
% % So, it is challenging to quantify well with uniform quantization, and the authors proposed twin uniform quantization. 
% In the twin uniform quantization, the authors quantized values in two ranges. The second problem was the metric calculation of base-PTQ for ViT. Although base-PTQ used various metrics, including mean square error (MSE) and cosine distance between the layer outputs before and after quantization, the metrics generated inaccurate results while determining the quantization parameters. The authors proposed to use the Hessian-guided metric to determine the quantization parameters.

% Moreover, another recent study on quantization by Ding et al.~\cite{ding2022towards} identified two problems of the existing quantization techniques in ViT as follows: 1) Calibration metric is inaccurate for measuring quantization for extremely low-bit representation. 2) The existing quantization paradigm is unfavorable to the power-law distribution of softmax. Based on their findings, the authors proposed an accurate post-training quantization framework for Vision Transformer, named APQ-ViT ~\cite{ding2022towards}. To solve the first limitation, the authors introduced a unified bottom-elimination blockwise calibration scheme, which focused on optimizing the calibration metric to assess quantization disturbance on a blockwise basis. This blockwise calibration scheme allowed them to prioritize and address the crucial quantization errors that impact the final output. Before solving the second limitation, the authors observed that the probability distribution of the softmax function in ViT makes the smaller values even smaller and the larger values even larger; that is typically called the matthew-effect. However, the existing quantizers ignored the mattew-effect of the softmax function that costs information loss from the larger values. Then, the authors proposed matthew-effect preserving quantization (MPQ) for Softmax to maintain the power-law character to solve the second limitation. MPQ maintained the matthew-effect of Softmax output during the quantization process and did not purely pursue the maximization of mutual information before and after quantization. Another study called NoisyQuant~\cite{liu2023noisyquant} proposed a quantizer-agnostic enhancement for the post-quantization of ViT. The proposed method added a noisy bias to each model layer to actively modify the input activation distribution before quantization, aiming to reduce quantization error. The noisy bias is a single vector sampled from a uniform distribution. The authors removed the impact of noisy bias after the activation-weight multiplication in the linear layer with a denoising bias so that the method could retrieve the correct output. NoisyQuant largely improved the performance of ViT architecture with minimal computation overhead. The experiment on ViT architecture showed that the proposed technique improved top-1 accuracy compared to the PTQ4ViT~\cite{yuan2022ptq4vit} on ViT-B, DEiT-B, and Swin-S architecture.

% Some other proposed recent quantization methods, like fully differentiable quantization method Q-Vit ~\cite{li2022q} based on head-wise bit-width used switchable scale to resolve the convergence issue during joint training of quantization. The proposed method limited quantization to 3-bit without an accuracy drop. The authors additionally analyzed the quantization robustness of every architecture component of ViT and explained that MSA and GELU are the key components during quantization. Comprehensive testing on various ViT models, including DeiT and Swin Transformer, demonstrated the effectiveness of their proposed quantization technique. Notably, their approach surpassed the uniform quantization method by a margin of 1.5\% on DeiT-T.
% \vspace{2pt}

% Another recent study by Lin et al. proposed Fq-vit ~\cite{lin2021fq} where the authors used the power-of-two factor (PTF) method to reduce the performance degradation and inference complexity of fully quantized vision transformers. Moreover, upon noticing a highly non-uniform distribution in attention maps, the authors introduced Log-Int-Softmax (LIS) to maintain the non-uniform distribution. The authors further streamline inference using 4-bit quantization combined with the bit-shift operator. The distinctions between the conventional MSA and the proposed technique for softmax are illustrated in Figure \ref{fig:quanwsoft} where the left one is for MSA with full precision (traditional) and the right one is for MSA with LIS (proposed). Figure \ref{fig:quanwsoft} (left) illustrated the traditional approach where the multiplication of the query (Q) and key (K) matrices, both in INT8 format, must be dequantized to the full precision (floating-point or FP) before the Softmax operation. After the softmax, the data is quantized to INT8 for the succeeding attention-weighted matrix multiplication with the value (V) matrix. However, figure \ref{fig:quanwsoft} (right) performed the matrix multiplication followed by an integer-based exponential function (i-exp). The authors then utilized $Log_{2}$ quantization scale in the softmax function and converted
% the MatMul to BitShift between the quantized attention
% map and values (V). The proposed method operated only with integer data types, including LIS. The result of the LIS was in INT4 format, indicating a more aggressive quantization that reduced memory usage.

% Moreover, the other study introduced I-ViT~\cite{li2022vit}, a quantization approach tailored for ViTs that allowed the execution of the entire inference computational graph solely using integer arithmetic and bit-shifting, eliminating the need for floating-point operations. This proposed method was the first work on integer-only quantization for ViTs. In this framework, linear functions, such as matrix multiplication and dense layers, strictly adhere to an integer-only pipeline leveraging dyadic arithmetic. Conversely, non-linear functions like Softmax, GELU, and LayerNorm were emulated using the suggested lightweight integer-only arithmetic techniques. I-ViT incorporated the novel Shiftmax and ShiftGELU methods, strategically developed to mimic their floating-point counterparts using integer bit-shifting. The study presented the accuracy outcomes of I-ViT alongside various baseline methods across several benchmark models on the ImageNet dataset. Despite I-ViT's reduction in bit-precision for parameters and its emphasis on integer-only inference, it retained a competitive accuracy level.

% In some cases, it surpassed the floating-point baseline and underscored the efficacy and reliability of the proposed approximation techniques. For example, when I-ViT was applied on DeiT-B, it achieved a top-1 accuracy of 81.74\% using 8-bit integer-only inference, a result that is notably 0.95\% (see table \ref{quantization_tech_classification}) higher than that of I-BERT ~\cite{kim2021bert}.

% Most quantization methods were experimented with for image classification with limited object detection tasks. However, some proposed methods showed great mAp values in object detection. All the results are summarized for different ViT models shown in Table \ref{tab:quantization_object}.
\noindent \paragraph{QAT Techniques for Vision Transformer}\hfill \\ Compared to PTQ techniques, QAT methods for ViTs remain relatively underexplored. Existing QAT approaches can be broadly classified into two categories: leveraging KD to optimize the quantized model and standalone independent frameworks. \\
\input{Tables/table_quan_class}

\noindent \textbf{Leveraging KD in QAT} Q-Vit~\cite{li2022q}  first proposed an information rectification module based on information theory to resolve the convergence issue during joint training of quantization. The authors then proposed distributed guided distillation by taking appropriate activities and utilizing the knowledge from similar matrices in distillation to perform the optimization perfectly. However, Q-ViT lacks other CV tasks, such as object detection. Another recent work, Q-DETR~\cite{xu2023q}, is introduced to solve the information distortion problem. The authors explored the low-bits quantization of DETR and proposed a bi-level optimization framework based on the information bottleneck principle. However, Q-DETR failed to keep the attention activations less than 4 bits and resulted in mixed-precision quantization, which is hardware-inefficient in the current scenario. Both Q-ViT and Q-DETR explored the lightweight version of DETR apart from modifying the MHA. AQ-DETR~\cite{aqvit} focused on solving the problem that exists for low bits of DETR in previous studies. The authors introduced an auxiliary query module and layer-by-layer distillation module to reduce the quantization error between quantized attention and full-precision counterpart. All the previously discussed works are heavily dependent on the data. Li et al. ~\cite{li2023psaq} proposed PSAQ-ViT, aiming to achieve a data-free quantization framework by utilizing the property of KD. The authors introduced an adaptive teacher-student strategy enabling cyclic interaction between generated samples and the quantized model under the supervision of the full-precision model, significantly improving accuracy. The framework leverages task- and model-independent prior information, making it universal across various vision tasks such as classification and object detection.\\
\input{Tables/table_quan_ob}

\noindent \textbf{Standalone QAT techniques} Although most works utilized the KD in QAT works, there are limited standalone studies without KD. PackQViT~\cite{dong2024packqvit} proposed activation-aware sub-8-bit QAT techniques for mobile devices. The authors leveraged $\log 2$ quantization or clamping to address the long-tailed distribution and outlier-aware training to handle the channel-wise outliers. Furthermore, The authors utilized int-2\textsuperscript{n}-softmax, int-LayerNorm, and int-GELU to enable integer-only computation. They designed a SIMD-based 4-bit packed multiplier to achieve end-to-end ViT acceleration on mobile devices. Another recent study named QD-BEV~\cite{zhang2023qd} explored QAT on BEVFormer~\cite{li2022bevformer} by leveraging 
image and BEV features. The authors identified that applying quantization directly in BEV tasks makes the training unstable, which leads to performance degradation. The authors proposed view-guided distillation to stabilize the QAT by conducting a systematic analysis of quantizing BEV networks. This work will open a new direction for autonomous vehicle research, applying QAT to reduce computational costs.
\input{Tables/table_quan_qat}
\subsubsection{Discussion}\hfill \\ Tables~\ref{quantization_tech_classification} and~\ref{tab:quantization_object} provide an overview of PTQ techniques for image classification and object detection on ViT models. A notable trend is the dominance of post-training quantization (PTQ) methods, with fewer studies exploring training-phase QAT. Table~\ref{quantization_tech_classification} highlights the top-1 accuracy improvements achieved by PTQ methods on ViT architectures for the classification tasks, while Table~\ref{tab:quantization_object} summaries the mean average precision (mAP) and AP\textsuperscript{box} improvement achieved from the baseline techniques for the object detection task. Moreover, Table~\ref{quantization_qat_classification} summarizes QAT techniques for image classification and object detection. Interestingly, most QAT methods for ViTs, such as those leveraging knowledge distillation~\cite{li2022q,xu2023q,aqvit,li2023psaq}, focus on optimizing quantized models but lack experimental validation on edge devices. In contrast, approaches like PackQViT~\cite{dong2024packqvit} have experimented with their QAT frameworks on mobile devices, pushing the boundaries of practical deployment. However, these quantization techniques broadly experimented on datasets like ImageNet-1k and COCO 2017, raising questions about their generalizability to specialized domains such as medical imaging or autonomous driving. This gap underscores the need for future research to explore versatile quantization strategies that cater to diverse application areas and resource-constrained edge devices.