\section{ViT based medical imaging models}\label{vit}
Transformers were introduced by Vaswani et al. ~\cite{vaswani2017attention} as a new attention-driven building block for machine translation. As we focus on only ViT in this survey, Dosovitskiy et al.~\cite{dosovitskiy2020image} first proposed ViT that is built on a vanilla Transformer model by combining multiple transformer layers to capture the global context of an input image. The authors presented a novel approach where images were interpreted as sequences of patches and then processed using standard transformer encoders commonly used in NLP. By eliminating hand-crafted visual features and inductive biases, ViT architecture takes advantage of large datasets and increases computational capacity to achieve impressive results. In particular, ViT has garnered significant interest in the medical imaging community, leading to the development of various approaches that build upon and extend the ViT architecture for diverse medical imaging applications. Shamshad et al.~\cite{shamshad2022transformers} showed that most of the medical imaging tasks focused on medical imaging classification, medical imaging segmentation, and medical imaging detection. In our survey, we only focus on these three medical imaging tasks.
\subsection{Medical Image Classification}\label{classi} Medical image classification refers to categorizing medical images into predefined classes based on specific features or abnormalities in the image. Through image classification, medical professionals can identify and diagnose various health conditions, from tumors to fractures, by categorizing and analyzing these images. Medical image classification involves preparing the images, extracting relevant features or patterns from them, and then using these features or patterns to categorize the images into specific classes. Figure~\ref{fig:classification_img} illustrates a sample heatmap for medical image classification output from ResNet~\cite{he2016deep} \& MedViT~\cite{manzari2023medvit} on the MedMNIST-v2 dataset~\cite{yang2023medmnist} which classifying two organs: the Derma and Oct. Most of the ViT architectures in image classification are used for COVID-19 and tumor classifications. Most COVID-19 image classification papers are divided into 2D and 3D image classification categories. Tumors in organs like the brain, lungs, breast, stomach, and Kidneys are used for tumor classification.

A recent study named Transmed~\cite{dai2021transmed} is the first ViT-based multi-modal medical image classification approach. The authors addressed that the performance of ViT in multi-modal medical image classification was not significant due to the small size of medical datasets. So, the authors used both CNN and transformers to extract low-level features of images and capture the mutual information between modalities. The authors evaluated their proposed model with the PGT and MRNet datasets to classify the parotid tumors in the multi-modal MRI medical image classification. The PGT dataset has two MRI modalities (T1 and T2). The MRNet dataset has three MRI modalities (T1-weighted images, T2-weighted images, and proton density-weighted). The proposed method achieved a 10.1\% improvement in average accuracy compared to BoTNet ~\cite{srinivas2021bottleneck}. In recent studies, Vit architecture has been successfully applied to diagnose and predict COVID-19, which showed good performance. Perera et al.~\cite{perera2021pocformer} proposed Pocformer~\cite{perera2021pocformer} that diagnosed COVID-19 that relies on ultrasound clips using the COVID-19 lungs POCUS ~\cite{born2020pocovid} dataset. The authors utilized Linformer ~\cite{wang2020linformer} to transform the self-attention's space and time complexity from quadratic to linear. POCFormer possessed two million parameters, achieving an average accuracy of 91\% with a speed of 70 frames per second. Another study named Covid-Vit~\cite{gao2021covid} proposed ViT-based architecture to classify COVID-19 from CT lung images in the MIA-COV19 competition. Two deep learning techniques were examined: ViT, which relied on attention mechanisms, and DenseNet~\cite{huang2017densely}, which is grounded in the traditional CNN approach. Preliminary assessments using validation datasets, where the true values are established, suggest that ViT outperforms DenseNet~\cite{huang2017densely}, yielding F1 scores of 0.76 compared to DenseNet's 0.72. As COVID-19 disease classification was necessary due to the pandemic in recent times, another study named PneuNet~\cite{wang2023pneunet} where multi-head attention is applied on channel patches rather than feature patches. The authors used their datasets, combining datasets from multiple open sources. Their COVID-19 dataset was a combination of normal pneumonia chest X-rays and chest X-rays of healthy people and divided into three sub-datasets out of 33920 chest X-ray images.

Moreover, ViT architecture also gained popularity in cancer classification. Using different data augmentation strategies, Gheflati el al.~\cite{gheflati2022vision} applied ViT architecture for the first time in breast cancer classification (BUC). The authors used the same parameter and training procedure described in ~\cite{steiner2021train}. Eventually, they trained their dataset with different ViT architectures that can compare with SOTA CNN models. The ViT architectures used by the authors achieved almost the same results: 86\% accuracy and 0.95 corresponding AUC from two datasets ~\cite{al2020dataset,yap2017automated}. Rah et al.~\cite{raj2023strokevit} proposed a combined model using CNN, ViT, and AutoML to classify whether the stroke is hemorrhagic or ischemic. The authors used the CNN component to capture the local features within CT slices and the transformer component to recognize the long-range dependencies between
\begin{landscape}
%\input{Tables/table_segmentation}
\begin{table}
\input{Tables/table_classify}
 \vspace{1cm} %space between tables
\input{Tables/table_object}
\end{table}
\end{landscape} \noindent
sequences. The combination aimed to enhance the accuracy of slice-level predictions. Feature extracted from each slice-level prediction is later used to make a patient-wise prediction via AutoML. The proposed architecture helped to reduce the radiologist's manual effort to select the most imperative CT from the original CT volume and achieved 0.92 accuracy with AutoML.

Lastly, Table~\ref{classification} illustrated the different organ classification results from different ViT-based architectures. There are already some existing surveys published about ViT-based medical imaging~\cite{shamshad2023transformers,henry2022vision}. The main agenda of picking the papers here is to show that ViT can be applied for different organ classifications and achieve good accuracy. The discussed ViT-based models for image classification tasks can be extended for edge application after applying model compression techniques.

\begin{figure}[btp]
  \centering
  \includegraphics[scale=0.28]{assets/classification.png}
  \caption{A medical image classification sample heatmap output applied from ResNet~\cite{he2016deep} \& MedViT~\cite{manzari2023medvit} on the MedMNIST-v2 dataset~\cite{yang2023medmnist}. The image classified four organs: the Blood, Breast, Oct, and Pneumonia.}
  \label{fig:classification_img}
\end{figure}
\vspace{-10pt}
\subsection{Medical Image Detection}\label{detec}
Object detection in medical imaging involves identifying and localizing specific organs or abnormalities within medical images, such as X-rays, CT scans, MRI images, and ultrasounds. It plays a crucial role in tasks like tumor and anomaly detection, assisting healthcare professionals in making more informed decisions. Unlike general object detection tasks in images (like identifying cars or pedestrians in a street photo), medical image object detection focuses on healthcare applications and requires high precision and sensitivity. Nevertheless, this task often demands significant time from medical professionals. Thus, there is a pressing need for a precise Computer-aided diagnosis (CAD) system to serve as an auxiliary reviewer, potentially speeding up the procedure. Recent research shows that works like tumor detection, anatomical structure identification, abnormality detection, and blood vessel abnormality are primarily used in object detection sectors in medical imaging. Figure \ref{fig:object_image} shows a sample example of detection containing multiple objects and 

blood impurity~\cite{leng2023deep}. The figure detects the blood cell abnormality for four different deep-learning architectures. However, it is not easy to detect an organ, tumor, or any other body organ or abnormality because of data imbalance, noise, and the need for high precision. Most of the transformer-based models used 

DETR~\cite{zhu2020deformable} as baseline models because DETR is one of the first object detection architectures with transformers.\\

\begin{figure}[ptb]
  \centering
  \vspace{-10pt}
  \includegraphics[scale=0.45]{assets/object_det_up.png}
  \vspace{-10pt}
  \caption{A sample comparison of the detection results on the image containing multiple objects and impurity: (a) DETR-TL; (b) Improved-DETR-TL; (c) Improved-DETR-FS; (d)
Faster R-CNN-TL; (e) YOLO v3-TL; (f) the original image ~\cite{leng2023deep}}
  \label{fig:object_image}
\end{figure}
Zhang et al.~\cite{zhang2022lightweight} first proposed lightweight ViT-based medical image detection architecture where the authors modified the baseline ViT architecture on image feature patches, enhancing the backbone for tumor detection more robustly. The authors reconstructed the feature patches with the original shape of feature maps generated by ResNet. The authors evaluated their proposed architecture on the BCS-DBT~\cite{buda2021detection} dataset, using mAP and AP as metrics. The authors improved ~7.2\% AP value compared to R-CNN and swin transformer. Another study by Lin et al.~\cite{lin2021aanet} introduced the Adaptive Attention Network (AANet), a framework designed to dynamically extract the distinct radiographic markers of COVID-19 from infection zones that display varying scales and appearances. The proposed model consists of two primary modules: an adaptive deformable ResNet and an attention-driven encoder. Initially, the adaptive deformable ResNet was crafted to automatically adjust its receptive fields to grasp feature representations in line with the shape and scale of the infected regions, catering to the multifaceted radiographic features of COVID-19. Subsequently, the author created an attention-based encoder to capture nonlocal interactions using a self-attention mechanism. This approach allowed the model to understand complex shapes in lesion regions by incorporating rich context information.

Moreover, MST~\cite{shou2022object} considered low resolution and high noise images to detect small organs showed great potential on DeepLesion~\cite{yan2018deeplesion} and BCDD~\cite{BCCD_Dataset} datasets. The proposed model used self-supervised learning to perform random masking on the input image and then reconstruct input features to filter out noise. The authors introduced a hierarchical transformer model to detect small objects in the paper. Then, the authors used a sliding window with a local self-attention mechanism to assign high attention scores to these diminutive objects targeted for detection. Finally, they utilized a single-stage object detection framework to predict object classes and bounding box locations. Another study by Leng et al.~\cite{leng2023deep}  proposed a pure transformer-based end-to-end object detection network based on DETR~\cite{zhu2020deformable} model for leukocyte detection. The authors used a pyramid vision transformer as a backbone to improve the performance of the DETR model and evaluated their proposed architecture on Raabin~\cite{kouzehkanan2022large} datasets.

Another study~\cite{liu2022sfod} proposed  SFOD-Trans, a semi-supervised framework for fine-grained object detection, mainly focusing on hepatic portal vein detection. The authors developed a fusion module, termed normalized ROI fusion (NRF), to facilitate information transfer from labeled and unlabeled images. This study showed great potential for semi-supervised learning and evaluated on PASCAL VOC~\cite{everingham2015pascal} CT image dataset. Moreover, Wittmann et al. utilized ViT for 3D medical image detection and introduced a novel feature extraction backbone called SwinFPN~\cite{wittmann2022swinfpn}. The proposed model utilized the concept of shifted window-based self-attention. SwinFPN is integrated with the head networks of Retina U-Net, resulting in notably improved detection performance.

Table \ref{object_detection} summarized all the discussed architecture key points, datasets, and performance to give an overview of medical image detection. From our observation, medical image detection using ViT is still an explorable area and needs more elaborate study to achieve a better response in this sector.
\subsection{Medical Image Segmentation}\label{segm}Medical image segmentation is a fundamental task in the medical domain, playing a crucial role in various clinical applications. Segmentation involves dividing a digital image into multiple segments to simplify its representation or to make it more meaningful. Unlike image classification, which assigns a label to an entire image, segmentation aims to assign a label to each pixel. This granularity is essential in medical contexts where it is essential to differentiate between tissues, organs, tumors, and other anatomical structures. The outcome of this process provides a detailed map of these structures, aiding in diagnosis, treatment planning, and further medical research. Figure \ref{fig:segmentation_image} illustrates output for medical imaging segmentation for prostate organ segmentation on the dataset from the PROSTATEx-2 challenge~\cite{Litjens2014ComputerAidedDO}. The red shape indicates the prostate segmentation after applying nnunet~\cite{isensee2018nnu}. In recent research, we have observed that the majority of papers have used hybrid structures combining transformer architecture and CNN. As ViT gained much attention from the medical community for its performance, the community explored ViT for medical image segmentation. One of the previous surveys ~\cite{shamshad2022transformers} discussed most of the ViT-based architectures for segmentation tasks in detail, and the authors collected the paper until 2021. We will discuss published papers that were not included in ~\cite{shamshad2022transformers} or published after the survey except Unetr ~\cite{hatamizadeh2022unetr} in this survey because Unetr achieved better dice score compared to other transformer-based models ~\cite{shamshad2022transformers}.

Hatamizadeh et al.~\cite{hatamizadeh2022unetr} proposed a U-shaped volumetric medical image segmentation named Unetr that utilized a transformer as the encoder to learn sequence representations of the input volume and successfully captured the global multi-scale information. The proposed method justified their architecture with Synapse~\cite{landman2015miccai} dataset for spleen segmentation, and  Medical Segmentation Decathlon(MSD)~\cite{antonelli2022medical} dataset for brain tumor segmentation. Another study presented Hiformer~\cite{heidari2023hiformer}, an approach that merges CNN and a transformer for medical image segmentation tasks. The authors utilized multi-scale feature representations from the Swin Transformer module and a CNN encoder. To seamlessly blend global and local features from these sources, a unique Double-Level Fusion (DLF) module is incorporated within the encoder-decoder skip connection. The authors evaluated their proposed architecture with 2D datasets using dice, mIoU, and accuracy metrics. Furthermore, another study named H2Former~\cite{he2023h2former}  addressed an issue that the direct merger of CNN and self-attention is computationally challenging, especially for high-resolution images. To solve this issue, H2former used a hierarchical hybrid ViT-based model combining CNNs, multi-scale channel attention, and transformers. This data-efficient approach outperforms existing models in 2D and 3D tasks. For instance, on the KVASIR-SEG~\cite{jha2020kvasir} dataset, H2Former outpaces TransUNet~\cite{chen2021transunet} in IoU scores while being more computationally efficient. The authors evaluated their proposed model for multi-organ and skin lesion segmentation and used dice score,mIoU, accuracy, and HD as evaluation metrics. Another recent study named dilated transformer (D-former)~\cite{wu2023d} proposed a U-shaped hierarchical encoder-decoder structure for 3D medical image segmentation. The encoder has a patch embedding layer to convert 3D images into sequences, followed by four D-Former blocks for feature extraction with three interspersed down-sampling layers. The first, second, and fourth blocks contain one local scope module (LSM) and one global scope module (GSM).
In contrast, the third block has three of each, arranged alternately. The decoder has the same structure as the encode with its own D-Former blocks, up-sampling layers, and a patch-expanding layer. The author evaluated D-former for multi-organ segmentation with Synapse~\cite{landman2015miccai} dataset and three critical parts for heart segmentation with ACDC~\cite{bernard2018deep} dataset using dice metrics for both.
\begin{figure}
  \centering
      \includegraphics[scale=0.3]{assets/segmentation_prostate_up.png}
  \caption{A sample output for medical imaging segmentation for prostate on the dataset from the SPIE-AAPM-NCI Prostate MR Gleason Grade Group Challenge~\cite{Litjens2014ComputerAidedDO}}
  \label{fig:segmentation_image}
  \vspace{-5pt}
\end{figure}
Hatamizadeh et al.~\cite{hatamizadeh2022unetformer} proposed a U-shaped hybrid transformer-based model named UNetFormer where the authors used a 3D swin-transformer~\cite{liu2021swin} as the encode combining with both CNN and transformer decoders. Additionally, the authors introduced a self-supervised pre-training technique where the encoder learns to predict randomly masked volumetric tokens using contextual information of visible tokens. The authors evaluated their method with MSD~\cite{antonelli2022medical} and BraTS-21~\cite{baid2021rsna} dataset for liver organ and tumor segmentation. Zhou et al.~\cite{zhou2023nnformer} proposed nnFormer to utilize sufficiently the potential of transformers, a specialized 3D transformer tailored for volumetric medical image segmentation. The authors proposed local and global volume-based self-attention mechanisms to learn volume representations. nnFormer proposed a new skip attention technique replacing the traditional concatenation/summation operations in skip connections in U-net~\cite{ronneberger2015u} like architecture. The authors showed its potential with three different public datasets~\cite{antonelli2022medical,landman2015miccai,bernard2018deep} for a multi-organ, brain tumor, and cardiac segmentation. Another recent study named swin transformer boosted U-Net (ST-Unet)~\cite{zhang2023st} utilized the low-level features to enhance the global features and mitigate the semantic gaps between encoder and decoders. The proposed model used swin transformer as an encoder and CNN as a decoder. The authors introduced a cross-layer feature enhancement (CLFE) module to capture multi-scale feature representations. The Spatial and Channel Squeeze \ and excitation (SCSE) module utilized the channel and spatial information data to find meaningful features and diminish the less important features. By integrating features via the CLFE module and processing them through CNNs, the authors regained low-level details and \input{Tables/table_segmentation}\noindent refined local features, achieving precise semantic segmentation results. The proposed model was evaluated with Synapse~\cite{synapse} dataset using dice and Hausdorff distance (HD) metrics.

However, some works focused on edge organ segmentation, improving image data, and proposing a pipeline for the pre-trained and fine-tuning phase. One study by Wang et al.~\cite{wang2023swinmm} proposed a multi-view pipeline for accurate and efficient self-supervised medical image segmentation called SwinMM. The authors utilized multi-view data through two main components: a masked multi-view encoder trained on diverse proxy tasks during pre-training and a cross-view decoder aggregated the multi-view information using a cross-attention block in the fine-tuning phase. Compared to the benchmark Swin UNETR~\cite{hatamizadeh2021swin}, SwinMM significantly improved medical segmentation tasks, enhancing model accuracy and data efficiency. The authors used dice and IoU as evaluation metrics and evaluated with ACDC~\cite{bernard2018deep} and the Whole Abdominal Organ (WORD)~\cite{luo2022word} datasets. Another study by Liet et al.~\cite{li2023lvit} proposed  LViT (Language meets Vision Transformer), integrating medical text annotations to boost the quality of image data. The model utilized text to generate exponential pseudo-labels in a semi-supervised context, aided by an Exponential Pseudo-label Iteration mechanism (EPI). The authors also designed language-vision loss to supervise the training of unlabeled images using text information directly. The proposed methodology experimented with 2D datasets and used dice and mean intersection of union (mIoU) for evaluation. Recognizing the importance of fine-grained information for organ edge segmentation and the computational challenges of processing high-resolution 3D features, Yang et al. introduced an encoder-decoder network named EPT-Net. EPT-Net fused edge perception with Transformer architecture for precise medical image segmentation. EPT-Net mainly focused on bolstering 3D spatial positioning, while an edge weight guidance module extracted edge details without increasing network parameters. Tests on three datasets, including SegTHOR 2019~\cite{lambert2020segthor}, Multi-Atlas Labeling Beyond the Cranial Vault~\cite{landman2015miccai}, and a modified KiTS19 dataset named KiTS19-M~\cite{heller2019kits19}, revealed that EPT-Net outperforms leading medical image segmentation techniques.

Table \ref{segmentation} shows all the papers after Shamshad et al. ~\cite{shamshad2022transformers} surveys for medical image segmentation, summarize the model architecture, and results of evaluation metrics. All the proposed models in recent times used hybrid (CNN and transformer) because CNN can excel at capturing local spatial hierarchies, and the transformer can handle long-range dependencies in data through their self-attention mechanism. Most of the proposed models in Table \ref{segmentation} are evaluated with 3D datasets, showing that researchers are more interested in studying those 3D medical data because 3D data offers context in three planes (axial, sagittal, and coronal). Transformers can leverage this multi-planar context better than traditional 2D approaches for medical image segmentation.

% \input{table_segmentation}
% multi-organ  on the Synapse dataset ~\cite{heidari2023hiformer}
% \input{table_segmentation}





