\section{Tools for Efficient Edge Deployment}\label{tools_for_edge}
Efficient edge deployment of ViT requires a combination of software tools, evaluation tools, and advanced optimization techniques for different hardware architectures. Software tools streamline model deployment by providing optimized libraries and frameworks tailored for edge environments. Optimization techniques, such as memory optimization and pipeline parallelism, enhance performance by leveraging hardware-specific optimizations. Finally, heterogeneous platforms, including CPUs, GPUs, FPGAs, and custom accelerators, offer the flexibility to balance power, performance, and cost for various applications. In this section, we explore these essential pillars of edge deployment.
% \subsection{Hardware Platforms for Vision Transformer Inference}

%% Will add hardware CPU<GPU and FPGA based on different parameters.

\subsection{Software Tools}
 Deploying deep learning models on heterogeneous platforms demands specialized software tools that bridge the gap between cutting-edge artificial intelligence (AI) research and real-world applications. These tools empower developers to optimize, accelerate, and seamlessly integrate AI models across different hardware architectures. Table~\ref{softwaretools} illustrates the most popularly used software tools/libraries/engines to deploy the deep learning models on different hardware architectures. The software libraries are divided into three hardware architectures: FPGA, GPU, and CPU. As FPGAs offer highly parallel and reconfigurable hardware capabilities,  deploying AI models on FPGAs requires specialized software tools for efficient hardware mapping, optimization, and deployment. Both Vivado Design Suite~\footnote{\label{vivado}Vivado design suite. Retrieved January 18, 2025, from \url{https://www.amd.com/en/products/software/adaptive-socs-and-fpgas/vivado.html}} (from Xilinx) and the Quartus Prime Design Software~\footnote{\label{quartus}Intel Quartus Prime. Retrieved January 18, 2025, from \url{https://www.intel.com/content/www/us/en/products/details/fpga/development-tools/quartus-prime.html}} (from Intel) offer advanced synthesis, converting high-level languages to hardware description language (HDL)  and preoptimized AI accelerators IP cores (such as Xilinx DPU or Intel AI Suite) that help to accelerate the inference task. Vitis AI~\footnote{\label{vitis}Vitis AI. Retrieved January 18, 2025, from \url{https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html}} is the software platform for Xilinx FPGA while OpenVINO~\footnote{\label{Openvinotoolkit}Openvinotoolkit. Retrieved January 18, 2025, from \url{https://github.com/openvinotoolkit/openvino}} designed for Intel FPGA and other hardware architectures, including GPU and CPU. However, It is possible/likely that each of these environments is highly modified only for their hardware family, which means developing applications on one would make it very difficult to port to the other. Besides those two software tools, Hls4ml~\footnote{\label{hls4ml}Hls4ml. Retrieved January 18, 2025, from \url{https://fastmachinelearning.org/hls4ml/index.html} } and FINN~\footnote{\label{finn}FINN. Retrieved January 18, 2025, from \url{https://xilinx.github.io/finn/} } designed to explore deep neural network inference on FPGAs efficiently and swiftly. However, both these libraries are still in the experimental phase. In the software tools for GPU, NVIDIA has a wide range of libraries/engines for edge devices. NVIDIA Triton Inference Server~\footnote{\label{triton}NVIDIA Triton Inference Server. Retrieved January 18, 2025, from \url{https://github.com/triton-inference-server/server}} is the most prominent open-source and scalable inference-serving software engine that simplifies the deployment of deep learning models at scale across all NVIDIA GPUs, x86, and Arm CPUs from major frameworks, including TensorFlow, PyTorch, and NVIDIA TensorRT. Additionally, TensorRT~\footnote{\label{tensorrt}TensorRT. Retrieved January 18, 2025, from \url{https://developer.nvidia.com/tensorrt}} uses as a popular inference optimizer and runtime library for NVIDIA GPU-based edge devices. oneDNN~\footnote{\label{onednn}oneDNN. Retrieved January 18, 2025, from \url{https://github.com/oneapi-src/oneDNN}} is a widely used open-source, cross-platform performance tool for deep learning models. It is optimized for Intel processors, graphics, and ARM-based processors and is in the experimental stage for NVIDIA GPU, AMD GPU, and RISC-V processors. Each software tool is mostly designed to optimize performance and efficiency for specific hardware architectures. However, ONNX Runtime~\footnote{\label{onnxrun}ONNX Runtime. Retrieved January 18, 2025, from \url{https://onnxruntime.ai/}} is one of the few inference engines that supports a wide range of hardware, including CPUs, GPUs, and FPGAs.
 \input{Tables/table_softwaretools}
 % Deploying deep learning models on heterogeneous platforms demands specialized software tools that bridge the gap between cutting-edge AI research and real-world applications. These tools empower developers to optimize, accelerate, and seamlessly integrate AI models across different hardware architectures. The following software tools are popular for processing deployment on edge devices. 
% \subsubsection{Software tools for FPGA deployment}
% \noindent \textbf{OpenVINO~\footnote{Openvinotoolkit. Openvinotoolkit/openvino. Retrieved January 18, 2025, from \url{https://github.com/openvinotoolkit/openvino}}:} OpenVINO is an open-source software toolkit provided by Intel corporation to boost the different deep learning models. It can utilize trained models directly from popular frameworks such as PyTorch~\cite{paszke2019pytorch}, TensorFlow~\cite{tensorflow2015whitepaper}, ONNX~\cite{onnxruntime}, Keras~\cite{chollet2015keras}, and JAX~\cite{jax2018github}, including direct integration of transformer models. OpenVINO supports diverse hardware architectures, including CPU (x86, ARM), GPU, and AI accelerators (Intel neural processing unit).

% \noindent \textbf{Vitis AI~\footnote{Vitis AI. Retrieved January 18, 2025, from \url{https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html} }:} An AI software platform by Xilinx that provides comprehensive tools and optimized architectures for deploying AI inference on Xilinx FPGAs. Vitis-AI mainly compresses the models using compression techniques and generates the xmodel to deploy into FPGA. Vitis AI includes a set of optimized neural network tools, libraries for evaluation, and models. Moreover, Xilinx offers a "model zoo" of pre-trained and optimized models for common AI tasks that can be directly deployed on FPGA using the Vitis AI platform.
% % \noindent \textbf{OpenCL~\cite{Trevett_Richards_Butler_McVeigh_Bhat_Calidas_Hindriksen_Dai_2013}:} A framework for writing programs that execute across heterogeneous platforms. Many FPGA vendors offer OpenCL support for programming FPGAs.

% \noindent \textbf{Hls4ml~\footnote{Hls4ml. Retrieved January 18, 2025, from \url{https://fastmachinelearning.org/hls4ml/index.html} }:} Hls4ml strives to efficiently and swiftly transform machine learning models from open-source platforms (such as Keras and PyTorch). It produces high-level synthesis (HLS) code that can be converted to FPGA firmware using the HLS compilers for different FPGA vendors. HLS4ml utilized Keras, PyTorch, Brevitas, and ONNX as the front, while Vivado/Vitis, oneAPI, and Quantus can be used as a backend for different FPGA manufacturers. However, HLs4ml is still under development and causes HLS synthesis issues, such as stopping the unrolling loop.

% \noindent \textbf{FINN~\footnote{FINN. Retrieved January 18, 2025, from \url{https://xilinx.github.io/finn/} }
% :} FINN is an experimental framework from Xilinx Research Labs to explore deep neural network inference on FPGAs. It provides end-to-end inference solutions for deploying quantized models on FPGA. It relies on co-design and design space exploration for quantization and parallelization tuning rather than just a generic deep learning acceleration solution. FINN utilized templated Vitis HLS and register transfer level (RTL) modules that implement neural network layers as streaming components.

% \noindent \textbf{TensorRT~\footnote{TensorRT. Retrieved January 18, 2025, from \url{https://developer.nvidia.com/tensorrt}}} TensorRT is a popular inference optimizer, and runtime library focused on low latency and high throughput performance for AI models deployed on NVIDIA GPUs. It offers various optimization techniques to reduce precision and supports plugins for custom layers to handle non-standard operations and convert models for deployment on GPU-based edge devices.

% \noindent \textbf{NVIDIA Triton Inference Server~\footnote{NVIDIA Triton Inference Server. Retrieved January 18, 2025, from \url{https://github.com/triton-inference-server/server}}} NVIDIA Triton Inference Server is open-source and scalable inference-serving software engine that simplifies the deployment of deep learning models at scale across all NVIDIA GPUs, x86, and Arm CPUs from major frameworks, including TensorFlow, PyTorch, and NVIDIA TensorRT. Additionally, it provides a model analyzer that reduces the time needed to find the optimal model deployment configuration. This software engine can also be utilized for any GPU- or CPU-based infrastructure (cloud, data center, or edge).

% \noindent \textbf{oneDNN~\footnote{oneDNN. Retrieved January 18, 2025, from \url{https://github.com/oneapi-src/oneDNN}}} OneDNN is a popular open-source cross-platform performance tool for deep learning models. It is optimized for Intel processors, graphics, and ARM-based processors. Additionally, it has experimental support for NVIDIA GPU, AMD GPU, and RISC-V processors.

% \noindent \textbf{ONNX Runtime~\footnote{ONNX Runtime. Retrieved January 18, 2025, from \url{https://github.com/oneapi-src/oneDNN}}} ONNX Runtime is an open-source inference engine that supports a wide range of hardware, including CPUs, GPUs, and FPGAs. It works with ONNX models and is optimized for various platforms through hardware-specific execution providers.
\subsection{Evaluation Tools}
Evaluating the performance of ViT acceleration techniques on edge platforms requires specialized tools and metrics to evaluate power consumption, energy efficiency, accuracy, and latency. Fortunately, most hardware vendors offer built-in tools and libraries to facilitate precise measurement of these key performance indicators.
\subsubsection{Latency}\hfill\\ Latency and frame per second (FPS) can be calculated as follows:
\[
\text{FPS} = \frac{1}{Latency}
\]
For GPU-based evaluations, PyTorch provides \texttt{torch.cuda.Event(enable\_timing=True)} for GPU-based evaluations, which accurately measures latency during inference. On NVIDIA EdgeGPU platforms, the \textbf{TensorRT Profiler} offers a detailed latency breakdown for Jetson boards. For AMD FPGAs, the \textbf{Vitis AI Profiler~\footnote{\label{vitisAI}Vitis AI Profiler. Retrieved February 10, 2025, from \url{https://github.com/Xilinx/Vitis-AI/tree/master/examples/vai_profiler}}} enables profiling during deployment, ensuring optimized execution. Additionally, \textbf{Intel's OpenVINO benchmark tool~\footref{Openvinotoolkit}} supports latency and throughput measurements across Intel CPUs and FPGAs, providing a standardized evaluation framework.

\subsubsection{Power}\hfill\\ Measuring power consumption is critical yet challenging in evaluating ViT acceleration techniques. Standard tools for general-purpose platforms (GPPs) like CPUs and GPUs include \textbf{Intel Power Gadget} for Intel CPUs and \textbf{NVIDIA-SMI} for NVIDIA GPUs. Power can be measured on edge GPU platforms, such as NVIDIA Jetson boards, using \textbf{tegraStats}, which provides real-time power monitoring, GPU utilization, and temperature. For FPGAs and ACAPs, AMD Xilinx offers \textbf{Xilinx Power Estimator (XPE)~\footnote{\label{xpe}Xilinx Power Estimator. Retrieved January 18, 2025, from \url{https://www.amd.com/en/products/adaptive-socs-and-fpgas/technologies/power-efficiency/power-estimator.html}}} for power estimation based on hardware configurations, while \textbf{Vaitrace} enables runtime power profiling for FPGA and adaptive compute acceleration platforms (ACAP). These tools provide essential insights into power efficiency, thermal behavior, and overall performance trade-offs across edge hardware platforms.
\subsubsection{Energy}\hfill\\ Energy consumption in ViT acceleration can be estimated through throughput per joule (GOP/J) and FPS per watt (FPS/W). However, accurately measuring energy on general-purpose platforms (GPPs) is challenging due to background processes affecting power readings. In contrast, edge devices provide a more controlled environment where only one primary task is executed simultaneously, making energy estimation more reliable. Several tools facilitate energy measurement: \textbf{Xilinx Vivado Power Analyzer~\footref{vivado}} estimates energy efficiency for FPGAs by profiling dynamic power, while \textbf{RAPL} tracks CPU-level energy consumption on x86 architectures. Additionally, energy efficiency can be derived using power measurements combined with latency, enabling a deeper evaluation of acceleration techniques.

\subsubsection{Resosurce Utlization}\hfill\\ Resource utilization is primarily analyzed in FPGA-based acceleration techniques to optimize hardware efficiency and minimize resource usage. Currently, Intel and AMD are two FPGA vendors. AMD offers \textbf{Vivado Design suite~\footref{vivado}} for synthesis evaluation of the FPGA before deployment. Similarly, Intel provides the \textbf{Quartus Prime~\footref{quartus}} software, which facilitates FPGA synthesis, resource utilization monitoring, and performance evaluation. Both vendors offer additional AI optimization frameworks—AMD’s \textbf{Vitis AI~\footref{vitis}} and Intel’s \textbf{FPGA SDK for OpenCL (AOCL)}—to enhance the efficiency of ViT acceleration on FPGA platforms.
\subsection{Common Optimization Techniques}
\textbf{Memory Optimization} Techniques like Huffman coding can be used to compress the weights. On-chip memory utilization efficiently uses the FPGA's Block RAMs (BRAMs) to store weights and intermediate feature maps, reducing the need for off-chip memory accesses, which can be slow and power-hungry.

\noindent \textbf{Pipeline Parallelism} The technique splits the model into stages and simultaneously processes different inputs at each stage, which helps in maximizing the throughput.

\noindent \textbf{Loop Unrolling} This FPGA-specific optimization involves unrolling loops in the FPGA design to speed up the processing. For instance, when performing matrix multiplications in the transformer layers.

\noindent \textbf{Layer Fusion} Layer fusion combines multiple layers into a single computational unit, reducing memory access between layers and improving the overall throughput.

\noindent \textbf{Hardware-friendly Activation Functions} Replace complex activation functions with simpler, hardware-friendly alternatives. For example, using piecewise linear approximations for non-linearities.
 
\noindent \textbf{Optimized Matrix Operations} ViT involves many matrix multiplications (in the attention mechanisms). Optimizing these matrix operations for FPGA leads to significant speed-ups. Techniques like systolic arrays or optimized linear algebra cores are employed.

\noindent \textbf{Dynamic Precision} In recent studies, some work uses mixed precision computations where certain parts of the model use lower precision (e.g., 8-bit). In comparison, other parts use higher precision (e.g., 16-bit or 32-bit). There are some works in which the authors introduced fixed point and PoT precision and optimally balanced accuracy and performance.