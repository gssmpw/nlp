\section{Challenges and Future Directions of ViT on Edge Devices}\label{cha_fu}
ViT models are computation-intensive, and their deployment on resource-constrained edge devices has been a big challenge. However, with the advancement of edge AI, this is now changing, and the efficient and cost-effective implementation of ViT models is possible directly on edge hardware. This increases accessibility for end users and reduces reliance on cloud infrastructure, which lowers latency, improves privacy, and reduces operational costs. However, some areas, such as real-world scenarios and software-hardware co-design still need to be explored for ViT on edge devices. In this section, we will discuss the current challenges and future opportunities of ViT on edge devices.

\subsection{Software-Hardware Co-design} The lightweight ViT model and compression techniques should be considered the hardware architecture. The SW-HW co-design can reduce the current dilemma between model and hardware architectures. Additionally, different edge hardware platforms (e.g., CPUs, GPUs, and FPGAs) have varying capabilities in handling precision, memory bandwidth, and computational efficiency. Often, accelerators support a uniform bit-width tensor, and this distinct bit-width precision needs zero padding, incurring inefficient memory usage. It is so hard to optimize the ViT for each type of hardware. Leveraging hardware-aware compression techniques can improve the efficiency of edge deployment. Frameworks such as DNNWeaver~\cite{7783720}, VAQF~\cite{sun2022vaqf}, M$^3$ViT~\cite{fan2022m3vit} have been developed for different hardware platforms FPGA, GPU accelerators for efficient edge inference. However, most of the current framework can not handle the sparsity caused by model compression. Therefore, the advancement of reconfigurability of software-hardware co-design for handling sparsity can be a future problem to solve.
\subsection{Utlizing NAS for Inference} NAS is currently used to find the optimized model. The limitations are the computational load and time required for the NAS algorithms, which are high. Consequently, improving NAS algorithms to obtain optimized ViT models targeting the edge can be a future research opportunity. Moreover, frameworks like HAQ~\cite{wang2019haq} and APQ~\cite{wang2020apq} utilized NAS for automatically generating pruning quantization strategies through reinforcement learning or evolutionary search methods. However, this work is highly customized for specific hardware (e.g., HAQ for FPGA). This problem arises because different edge hardware platforms or neural accelerators have distinct properties and processing capabilities. ProxylessNAS~\cite{cai2018proxylessnas} is one of the works that can find a model to fit the hardware but is limited to only CNN models. Therefore, utilizing NAS for searching hardware-independent optimization techniques for ViT can be the future direction to explore. 
\subsection{Acclerators to Handle Sparsity} Traditional processors such as GPU, CPU, or even FPGA cannot handle the space, irregular tensor. For example, although mixed-precision quantization techniques for ViT have been developed, their deployment on edge devices remains limited due to the inefficiency of current hardware architectures and accelerators in handling mixed-precision formats~\cite{lin2024awq}. Therefore, optimized accelerators to handle mixed precision format on edge devices still need to be explored. In addition, combining multiple compression techniques for optimum hardware performance is a promising research direction. Current hardware accelerators are not inherently designed to process sparse tensors efficiently, as they require fetching zero values from memory to processing elements (PEs). Thus, specialized techniques are needed to optimize the storage and computation of nonzero values in ViT.
\subsection{Automated Edge Aware Model Compression} Most of the current model compression techniques require manual adjustment of hyperparameters such as quantization bit width, pruning ratio, or layer-wise sparsity. Compression hyperparameters must be adjusted automatically or adaptively within the resource budget with minimum degradation of accuracy. From our observation, few works explore adjusting the compression parameters automatically. SparseViT~\cite{chen2023sparsevit} is one of few works that effectively reduced computation by targeting less-important regions with dynamically chosen pruning ratios in the images for the ViT model, achieving significant latency reductions. As a result, developing a hardware-efficient automaticity compression technique can be an interesting research domain in the future. 

Another drawback we observe from section~\ref{vit_quanti} is that most of the current work on ViT is post-training. The most interesting reason, perhaps the interactive nature of the training process, is that implementing compression techniques during training requires cost and time. However, QAT techniques on ViT are promising~\cite{li2023psaq,dong2024packqvit,zhang2023qd,aqvit}, but there is limited work on other compression techniques. Thus, the automated exploration of compression techniques during training can improve hardware realization. However, efficient compression techniques for faster convergence during training with reduced computation need to be explored in the future.
\subsection{Developing Benchmarks} Proper benchmark standards to evaluate the performance of the edge devices are important. The different stages of the model to deploying edge, including compression and accelerators, require a universal and comprehensive set of metrics to compare different proposed solutions. However, benchmarking datasets and models from the system perspective are limited. For example, most of the compression techniques for ViT were evaluated on the ImageNet-1k~\cite{5206848} dataset for classification tasks and COCO-2017~\cite{lin2015microsoft} datasets for object detection tasks. However, expanding that knowledge for different real-world application areas is still limited due to the lack of dataset and model benchmarks. Thus, more benchmarking datasets and ViT models for evaluating the proposed system/framework need to be developed for different application areas, such as medical imaging and autonomous driving. In addition, making one compression technique universal for different CV tasks is challenging. Making task-independent universal compression techniques can be an interesting research domain in the future.
\subsection{Real-world Case Studies} The deployment of ViT on edge devices has gained significant progress in recent years. However, most of the compression techniques, frameworks, and accelerators are limited to evaluation in an academic environment. For instance, there are numerous ViT model on medical imaging datasets for different CV tasks such as image classification~\cite{dai2021transmed,perera2021pocformer,wang2023pneunet,raj2023strokevit,gheflati2022vision}, segmentation~\cite{hatamizadeh2021swin,li2023lvit,heidari2023hiformer,he2023h2former,yang2023ept}, object detection~\cite{shou2022object,leng2023deep,wittmann2022swinfpn,liu2022sfod,lin2021aanet}. However, few studies have evaluated compression and accelerator techniques on those  ViT-based medical imaging models. Exploring ViT on edge for medical imaging can be challenging because of the unique nature of the data (3D ultrasounds or MRIs).

Maintaining accuracy, latency, and precision is critical in real-world applications, particularly in critical fields like medical imaging. In medical imaging, a significant challenge to the major compressing of the ViT models is maintaining the spatial resolution and feature details since even small degradations in accuracy will affect the diagnostic outcome significantly~\cite{hou2019high}. Compression techniques can reduce excessive feature abstraction, potentially discarding vital low-level details essential for accurate diagnoses. Furthermore, transfer learning in medical imaging adds another layer of complexity—determining which pre-trained layers to retain or modify without losing critical learned representations is a significant challenge~\cite{peng2022rethinking,vrbanvcivc2020transfer}. Therefore, it is an open research direction to achieve a balance between model efficiency and diagnostic reliability for ViT models on edge for real-world scenarios such as healthcare applications.
\subsection{Seamless Model-to-Edge Integration} The conversion from a trained model into a hardware-compatible version for inference requires extensive time, cost, and, most importantly, manual in each step. Additionally, there is a high knowledge gap between the research community. For example, training new models requires extensive software knowledge, while compressing and developing accelerating strategies require deep hardware knowledge. It is challenging to find an expert in both directions in the research community. These difficulties create a significant research gap in developing tools that automatically map the models on hardware. However, FPGA can overcome some limitations with the ability to reconfigure new operations and modules. However, the available tools are insufficient for the automatic mapping of models and are more limited for ViT.

The current deep learning frameworks for edge deployment help researchers quickly prototype the models to deploy on edge. However, it lacks support with the rapid growth of different model architectures. Additionally, most of the current frameworks are evaluated for CNN models, while those frameworks are still in the experimental phase for ViT. For instance, Xilinx provides quantization support through the FINN-R framework for inference realization on FPGA~\cite{blott2018finn}, limiting only standard techniques for ViT. Therefore, the automatic mapping from model to edge, precisely a one-click solution for deployment on edge based on the budget or auto-generated compression techniques, can be an interesting domain in the future.
\subsection{Robustness to Diverse Data Modality}In real-world scenarios, data sources change from sensor to sensor or vendor to vendor. For example, medical imaging includes X-rays and other modalities like magnetic resonance imaging (MRI), computed tomography (CT) scans, and ultrasounds. Each modality has its characteristics, and a compression technique effective for one might not be for another. So, using a generic model compression technique for all modalities is always tricky. Such heterogeneity may cause inconsistency in data, imposing a challenge on edge performance. Federated learning, multi-modal fusion, and adaptive data calibration techniques can be promising research directions to mitigate the data inconsistency problem at the edge device.