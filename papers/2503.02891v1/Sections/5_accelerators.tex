\section{Accelerating Strategies For ViT on Edge}\label{acce_tech}
This section explores acceleration strategies for non-linear operations, discusses SOTA ViT acceleration techniques, and provides a comprehensive performance analysis in terms of both hardware efficiency and accuracy.
\subsection{Accelerating Non-linear operations} ViT models in CV can mainly be split into two types of operations: linear and non-linear. Optimizing non-linear operations in quantized ViT is as crucial as optimizing linear operations. While low-bit computing units significantly reduce computational complexity and memory footprint, They are primarily designed for linear operations such as matrix multiplications and convolutions. However, non-linear functions, including softmax, GELU, and LayerNorm, are the essential components of ViT architectures yet unexplored largely in these low-bit computing environments. The lack of support for non-linear operations on hardware creates computational bottlenecks during the edge deployment, as non-linear functions often require FP32 operations. This results in latency and increases power and energy consumption, ultimately lessening the quantization's full benefits on edge deployment. Moreover, during inference on quantized ViT models on edge devices, frequent quantization and dequantization operations surrounding non-linear layers add further inefficiencies, slowing inference and reducing throughput~\cite{stevens2021softermax}. Researchers developed integer-based approximations for non-linear operations to solve these issues, eliminating the need for frequent FP32 computations in those layers.

As illustrated in Table~\ref{nonlinearoperation}, we introduce integer-only approximations for different hardware platforms to enhance ViT inference efficiency. As we discussed details in section~\ref{quan}, FQ-ViT~\cite{lin2021fq} utilized LIS for integer-only variants of the softmax that approximate the exponential component using second-order polynomial coupled with $\log 2$ quantization. For LayerNorm, the authors applied PTF to shift the quantized activations and later computed mean and variance using integer arithmetic. However, this approach seems hardware efficient; their methods of practical deployment on edge platforms are unclear. Additionally, I-ViT~\cite{li2022vit} calculates the square root in LayerNorm using an integer-based iterative method. The authors then introduced ShiftGELU, which used sigmoid-based approximations for GELU approximations. I-ViT utilized NVIDIA RTX 2080 Ti GPU as a hardware platform to evaluate their method. 

PackQViT~\cite{dong2024packqvit} is an extended version of the FQ-ViT concept that also leverages second-order polynomial approximations more straightforwardly, replacing the usual constant $e$ with 2 in the softmax. Although PackQViT requires training, the simplification ensures no accuracy loss. EdgeKernel~\cite{zhang2023practical} addresses precision challenges in softmax computations by optimizing the selection of the bit shift parameter on Apple A13 and M1 chips, ensuring high accuracy while minimizing significant bit truncations. Additionally, it employs asymmetric quantization for LayerNorm inputs, converting them to a uint16 format to enhance computational efficiency while maintaining data integrity.

SOLE~\cite{wang2023sole} and SwiftTron~\cite{marchisio2023swifttron} both are utilized application-specific integrated circuit (ASIC) platforms. SOLE optimizes the software perspectives, introducing E2Softmax with $\log 2$ quantization to avoid traditional FP32 precision in softmax layers. Additionally, SOLE designed a two-stage LayerNorm unit using PTF factors. However, SwiftTron focused on designing customized hardware for ASIC to efficiently use non-linear operations, even in FP32, which accounts for diverse scaling factors in performing correct computations.
\input{Tables/table_nonlinearoperation}
\subsection{Current Accelerating Techniques on ViT}
Efficient acceleration techniques for seamless deployment on edge devices are essential where computational and energy constraints limit performance. Various techniques have been developed to optimize ViT execution, balancing throughput, latency, and energy efficiency while ensuring minimal loss in accuracy. These techniques can be broadly classified into SW-HW co-design and hardware-only acceleration. SW-HW co-design integrates algorithmic optimizations with hardware-aware modifications to ensure efficient deployment of ViTs on edge devices, enhancing throughput and energy efficiency. On the other hand, hardware-only approaches push efficiency even further by designing architectures specifically optimized for transformer workloads, utilizing systolic arrays, spatial computing, and near-memory processing to eliminate bottlenecks caused by data movement and external memory access. Additionally, alternative approaches, such as distributing multiple tiny edge devices by partitioning the model into submodels, offer promising directions to rethink ViT acceleration from a fundamentally different perspective. 
\input{Tables/table_hwsw_co}
\subsubsection{Software Optimization in Software-Hardware Co-design}\hfill \\
Recent advancements in software part in SW-HW co-design for ViT acceleration encompass a range of optimization techniques, including quantization-based acceleration~\cite{sun2022vaqf,lit2022auto,10745859,dong2023heatvit}, sparse and adaptive attention mechanisms~\cite{you2023vitcod,dong2023heatvit}, adding mixture of experts (MoE) layers~\cite{fan2022m3vit}, analyzing kernel profiling and execution scheduling~\cite{10745859}, custom hardware softmax and LayerNorm to replace traditional softmax and LayerNorm~\cite{wang2023sole,stevens2021softermax}.\\ 

\noindent \textbf{Quantization-Based Software Acceleration} VAQF~\cite{sun2022vaqf} utilized binary quantization for weights and low-precision for the activations in the software part. VAQF automatically outputs the efficient quantization parameters based on the model structure and the expected frame per second (FPS) to meet the hardware specifications. The primary purpose of this work was to achieve high throughput on hardware while maintaining model accuracy. Moreover, Li et al.~\cite{lit2022auto} used a mixed-scheme (fixed+PoT) ViT quantization algorithm that can fully leverage heterogeneous FPGA resources for a target FPS. Both frameworks utilized targeted FPS as their input to achieve maximum hardware efficiency during inference. Additionally, both VAQF and AutoViT-Acc~\cite{lit2022auto} utilized PTQ methods as quantization. EQ-ViT~\cite{10745859} combined latency and accuracy requirements to decide the final quantization strategy leveraging the QAT approach. Additionally, EQ-ViT implemented a profiling-based execution scheduler that dynamically allocates workloads across hardware accelerators.\\

\noindent \textbf{Pruning-Based Software Acceleration} ViT's self-attention has quadratic complexity concerning input sequence length, leading to high memory bandwidth consumption. Sparse attention mechanisms aim to reduce redundant computation by focusing on pruning. For instance, ViTCoD~\cite{you2023vitcod} efficiently employed structured pruned and polarized the attention maps to remove redundant attention scores, creating a more memory-efficient execution. Moreover,  HeatViT~\cite{dong2023heatvit} utilized image-adaptive token pruning, identifying and removing unimportant tokens before transformer blocks using a multi-token selector, dynamically reducing computational complexity. The proposed method is highly inspired by SP-ViT ~\cite{kong2022spvit} and uses a similar token selector like SP-ViT.\\

\noindent \textbf{Other Approaches} Additionally, MoE improves model efficiency by activating only the most relevant expert networks per input instance, reducing unnecessary computations. One of the first studies named M\textsuperscript{3}ViT~\cite{fan2022m3vit} implemented MoE layers where a router dynamically selects the appropriate experts for processing. In this work, the authors conveyed training dynamics to balance large capacity and efficiency by selecting only a subset of experts using the MoE router. Beyond traditional quantization and attention optimizations, some co-design approaches target the computational bottlenecks of softmax and LayerNorm. SOLE~\cite{wang2023sole} proposed E2Softmax and AILayerNorm, hardware-aware modifications for non-linear operations that replace FP32 with integer-only approximation. This integer-only computation improved the latency significantly during the inference.
\subsubsection{Hardware Optimization in Software-Hardware Co-design}\hfill \\
Hardware optimization plays a crucial role in the software-hardware co-design of ViTs, ensuring efficient execution across different accelerators. Key techniques include C++ based hardware descriptions, high-level synthesis (HLS), and accelerator bitstream generation~\cite{sun2022vaqf,lit2022auto,dong2023heatvit}. Additionally, frameworks leverage AI engine (AIE) kernels~\cite{10745859} and custom hardware units (e.g., SOLE) to optimize execution for edge deployment.\\

\noindent \textbf{FPGA-Based Hardware Acceleration} FPGA is the pioneer for hardware accelerating strategies because of its reconfigurable characteristics. VAQF~\cite{sun2022vaqf} adopted the quantization schemes from the software part and applied them to the accelerator on the hardware side. Figure~\ref{fig:vaqf} illustrates the overview of the VAQF framework. The accelerator's C++ description was synthesized using the Vivado HLS tool. Initial accelerator parameters focused on maximizing parallelism, but there were adjustments due to Vivado's placement or routing problems. Successful implementations produced a bitstream file for FPGA deployment. Moreover, as illustrated in Figure~\ref{fig:aut-vcc}, Auto-ViT Acc~\cite{lit2022auto} first used the "FPGA Resource Utilization Modeling" module to give performance analysis and calculate the FPS of the FPGA ViT accelerator in software part. Inspired by VAQF, the authors implemented the FPGA accelerator using a C++ hardware description, synthesized through Vitis HLS to generate the final accelerator bitstream. 

In ViTs, the computational bottleneck often arises from General Matrix Multiply (GEMM) operations, which form the core of self-attention and feed-forward layers. HeatViT~\cite{dong2023heatvit} optimizes ViT execution by dynamically selecting tokens and loading each layer from off-chip DDR memory to on-chip buffers before processing via the GEMM engine. This approach, inspired by \cite{lit2022auto}, minimizes redundant computations and improves memory efficiency. The proposed HeatViT addressed two main challenges of hardware implementation in their proposed architecture, as follows.
\begin{enumerate}
\item The GEMM loop tiling must be adjusted to factor in an extra dimension from multi-head parallelism.
\item ViTs have more non-linear operations than CNNs; these must be optimized for better quantization and efficient hardware execution while maintaining accuracy.
\end{enumerate}

\begin{figure}[]
  \centering
  % First image
  \begin{minipage}[b]{0.47\linewidth}
    \includegraphics[width=\linewidth]{assets/VAQF_new_up.png}
   \caption{Overflow of VAQF acclerator~\cite{sun2022vaqf}. Using different colors in the architecture differentiates between types of processes within the overall workflow. The light gray boxes represent settings that are input/output to the process. The light blue boxes denote active processing steps or software tools within the workflow, like Vivado HLS. The lavender box signifies a platform/library used in the process, like "PyTorch". The light purple box indicates decision-making points or critical stages in the architecture.}
  \label{fig:vaqf}
  \end{minipage}
  \hspace{0.5cm} % Space between the images
  % Second image
  \begin{minipage}[b]{0.47\linewidth}
    \includegraphics[width=\linewidth]{assets/auto_vcc.png}
    % \includegraphics[scale=0.5]{assets/heatvit.png}
  \caption{The overview of Auto-ViT-Acc framework ~\cite{lit2022auto}. The "FPGA resource utilization modeling" was utilized for performance analysis and the estimated FPS rate for ViT accelerator with assigned bit-width for mixed schemes and lessening the bit-width until achieving the target FPS. The proposed mixed-scheme quantization then utilized mixed ratio (k\textsubscript{pot}) results to implement on FPGA through "C++ Description for accelerator," "Vitis HLS" and "Accelerator bitstream"}
  \label{fig:aut-vcc}
  \end{minipage}
\end{figure}

\noindent \textbf{MoE Execution for Hardware Efficiency} While MoE optimization techniques dynamically select experts, leading to an unpredictable computing pattern that makes hardware execution difficult. M\textsuperscript{3}ViT~\cite{fan2022m3vit} reordered computations to process tokens expert-by-expert rather than token-by-token, reducing irregular memory access and improving hardware parallelism. However, frequent off-chip memory accesses in MoE layers introduce a latency bottleneck. The authors utilized a ping-pong buffering technique for continuous processing without memory stalls, where one buffer fetches expert weights while another buffer performs computations. Additionally, the per-expert token queueing system groups tokens per expert, limiting the underutilization of compute units due to various expert demands.\\

\noindent \textbf{Optimizing Non-Linear Operations with Custom Hardware Units} Recent studies, such as EQ-ViT, separated matrix multiply (MM) and non-MM by efficiently mapping batch MM (BMM) and convolutions to AIE vector cores. Memory-bounded and non-linear operations are executed within the FPGA's programmable element (PE). Additionally, EQ-ViT leveraged fine-grained pipeline execution to overlap computation with memory transfers,  maximizing resource utilization. A key strength of this framework lies in its hardware mapping methodology, where execution is formulated as a mixed-integer programming (MIP) optimization problem, ensuring that latency and resource constraints are satisfied while maximizing throughput. Likewise, SOLE~\cite{10745859} designed separate units for the proposed E2Softmax  and AILayerNorm to perform non-linear operations efficiently. The E2Softmax unit included Log2Exp and an approximate Log-based divider that is implemented in a LUT-free and multiplication-free manner. Additionally, the AILayerNorm unit operates in two stages: the first stage performs statistical calculations, while the second stage applies the affine transformation. Similar to M\textsuperscript{3}ViT~\cite{fan2022m3vit}  also utilized ping pong buffer to pipeline the AILayerNorm unit. \\

\noindent \textbf{Pure Hardware Accelerators} Although most ViT acceleration techniques rely on SW-HW co-design—where software optimization plays a crucial role in achieving high efficiency and accuracy—the potential of pure hardware optimization has gained attention in recent studies. A recent study named ViA~\cite{wang2022via} addressed issues during data and computations flow through the layers in ViT. The authors utilized a partitioning strategy to reduce the impact of data locality in the image and enhance the efficiency of computation and memory access. Additionally, by examining the computing flow of the ViT, the authors also utilized the half-layer mapping and throughput analysis to lessen the effects of path dependency due to the shortcut mechanism and to maximize the use of hardware resources for efficient transformer execution. The study developed two reuse processing engines with an internal stream, distinguishing them from previous overlaps or stream design patterns drawing from the optimization strategies.

Moreover, ViTA~\cite{nag2023vita} used two sets of MAC units to minimize the off-chip memory accesses. The first set of MAC units representing the hidden layers was broadcasted to the second set of MAC units through a non-linear activation function. That broadcasting approach helped to compute the partial products corresponding to the output layer. The authors allocated these resources to maintain the pipeline technique as if the hidden layer value computations and the output layer partial product computation took equal time. This approach enabled the integration of several mainstream ViT models by only adjusting the configuration.\\

\noindent \textbf{Sparse Attention Optimization for ViTs}  Recent studies such as ViTCoD~\cite{you2023vitcod} introduced a sparser engine to process the sparse attention metrics. The authors structured the multiply-accumulate (MAC) into the encoder and decoder MAC lines to optimize the matrix multiplications.

\input{Tables/table_hwsw_co_performance}
\subsubsection{Other Techniques}\hfill \\
 ED-ViT~\cite{liu2024ed} utilized distributed workloads approaches to deploy the ViT mode utilizing multiple tiny edge devices such as Raspberry Pi-4B. The authors partitioned the model into multiple submodels, mapping each submodel to a separate edge device. This distributed execution strategy enables ViTs to achieve efficiency comparable to single powerful edge accelerators like EdgeGPUs or FPGAs while leveraging cost-effective and scalable edge computing resources. Similarly, COSA Plus, proposed by Wang et al.~\cite{10612833}, capitalized on high inherent parallelism within ViT models by implementing a runtime-configurable hybrid dataflow strategy. This method dynamically switches between weight-stationary and output-stationary dataflows in a systolic array, optimizing the computational efficiency for matrix multiplications within the attention mechanism. COSA Plus enhances processing element (PE) utilization by adapting data movement patterns to workload variations.
% From our observations, most of the SW-HW co-design proposed for FPGA or ACAP design due to their reconfigurable nature where the authors baselined GPU, CPU, or other FPGA platforms. Table~\ref{hwswco} provides a comprehensive overview of the SOTA SW-HW co-design strategies for ViTs on various edge platforms. The analysis focuses on the hardware devices, optimization techniques, and performance metrics, including energy efficiency, throughput, and speedup.

% Recent studies, such as SOLE and ViTCoD, demonstrate a shift towards ASIC accelerators, which offer superior energy efficiency and computational speedup compared to FPGA and GPU-based techniques. For example, ViTCoD~\cite{you2023vitcod} achieves a 33.8$\times$ speedup using an ASIC-optimized sparse attention engine, outperforming HeatViT on the DeiT-Tiny (T) model. 

% Additionally, the quantization-based acceleration has shown significant improvements in energy efficiency. However, Table~\ref{hwswco} highlights a key challenge in MoE-based architectures such as M\textsuperscript{3}ViT~\cite{fan2022m3vit}: despite achieving high throughput (1183.4 GOPs), their energy efficiency remains moderate (0.845 FPS/W). This suggests MoE introduces irregular memory access patterns, leading to suboptimal hardware utilization. While ping-pong buffering is a well-established technique to mitigate memory stalls, further optimizations are required to exploit hardware parallelism and memory efficiency.

% Additionally, Table~\ref{hwswco} shows that ASIC designs dominate speedup, but FPGA offers better energy efficiency. This suggests that ASIC accelerators provide raw computational power, while FPGA solutions are more power-efficient but slightly lower in absolute speedup.
\subsubsection{Discussion}\hfill \\
From our observations, most of the SW-HW co-design proposed for FPGA or ACAP design due to their reconfigurable nature where the authors baselined GPU, CPU, or other FPGA platforms. Table~\ref{hwswco} provides a comprehensive overview of the SOTA SW-HW co-design strategies for ViTs on various edge platforms. The analysis focuses on the hardware devices and optimization techniques. A key observation from this table is that different hardware platforms support distinct optimization levels. While GPUs and EdgeGPUs are widely used due to their parallel processing capabilities, FPGAs and ACAP platforms provide customized acceleration, often resulting in lower latency and energy-efficient execution. 

VAQF and Auto-ViTAcc apply PTQ optimization to reduce precision while maintaining accuracy, while HeatViT and EQ-ViT optimize workload distribution through kernel-level techniques and adaptive token pruning.
Additionally, the quantization-based acceleration has shown significant improvements in energy efficiency. However, MoE introduces irregular memory access patterns, leading to suboptimal hardware utilization. While ping-pong buffering is a well-established technique to mitigate memory stalls, further optimizations are required to exploit hardware parallelism and memory efficiency.

Additionally, Table~\ref{hwswco} shows that ASIC designs dominate speedup, but FPGA offers better energy efficiency. This suggests that ASIC accelerators provide raw computational power, while FPGA solutions are more power-efficient but slightly lower in absolute speedup.

Most current acceleration techniques for ViTs are highly customized for specialized hardware, such as FPGAs and ASICs, limiting their deployment flexibility. Table~\ref{performance_analysis} highlights the trade-offs between accuracy, adaptability, and hardware compatibility in SW-HW co-design techniques for ViTs. However, few approaches support cross-platform compatibility, making their extension to diverse edge devices challenging. Among the surveyed techniques, PIVOT~\cite{moitra2024pivot} emerges as the most flexible solution, as it maintains high accuracy while being compatible with GPPs, unlike ViTCoD, HeatViT, and VAQF, which are hardware-specialized acceleration techniques. Additionally, Table~\ref{performance_analysis} highlights how different acceleration techniques prioritize computations, offering insights into their adaptability across various deployment scenarios.
\subsection{Performance Analysis for Accelerating Techniques}
This section provides a performance analysis of state-of-the-art (SOTA) accelerating techniques, focusing on key metrics such as power consumption, energy efficiency, resource utilization, and throughput.
\input{Tables/table_performance_edge}
\subsubsection{Resource Utilization}\hfill \\ 
Resource utilization measures the hardware efficiency of various ViT acceleration techniques in terms of the use of block RAM (BRAM), digital signal processing units (DSP), Kilo lookup tables (KLUT), and Kilo flip-flops (KFF) from available resources. As illustrated in Table~\ref{Hardware_performance}, different acceleration techniques exhibit varying resource consumption patterns, reflecting their optimization strategies and deployment constraints.

From the observation of Table~\ref{Hardware_performance}, HeatViT (2066 DSPs)~\cite{dong2023heatvit} and Zhang et al. (2147 DSPs)~\cite{zhang2024109} demonstrate the highest DSP consumption, indicating their reliance on intensive parallel processing to accelerate transformer computations although both used different FPGA variants. In terms of on-chip memory usage, ViA (1002 BRAMs)~\cite{wang2022via} and VAQF (565.5 BRAMs)~\cite{sun2022vaqf} exhibit significant BRAM consumption, emphasizing a design strategy that prioritizes data locality to minimize off-chip memory access latency. In contrast, EQ-ViT (16 BRAMs)~\cite{10745859} utilizes remarkably low BRAM, likely due to its two-level optimization kernels—leveraging both single artificial intelligence engines (AIEs) and AIE array levels. Additionally, Auto-ViT-Acc~\cite{lit2022auto} (186 KLUTs) and HeatViT~\cite{dong2023heatvit} (161.4 KLUTs) indicate that they need to perform significant logical operations to implement their mixed precision quantization and token pruning to deploy on edge.
\subsubsection{Energy Efficiency}\hfill \\
Table~\ref{Hardware_performance} also indicates the energy efficiency of the ViT accelerating techniques. In the current studies, energy efficiency was measured in two ways: using throughput (GOP/J) and using FPS (FPS/watt). We also include the power consumption of the accelerating techniques for better accountability. It is perhaps difficult to conclude about energy efficiency when different edge targets are used for the CV task. From Table~\ref{Hardware_performance}, VAQF~\cite{sun2022vaqf}, Auto-ViT-Acc~\cite{lit2022auto}, and HeatViT~\cite{dong2023heatvit} utilized the same FPGA AMD ZCU102 board with the same number of resources and frequency. We can observe that VAQF outperforms Auto-ViT-Acc and HeatViT regarding energy and power usage. However, we were unable to find any energy comparison from the original paper for ASIC-based accelerating techniques (e.g., ViTCoD, SOLE).
\subsubsection{Throughput}\hfill \\
Table~\ref{Hardware_performance} illustrates the throughput of the accelerating techniques. We observe significant variations in throughput, influenced by factors such as hardware architecture, precision, and optimization strategies. Zhang et al. (2330.2 GOPs) achieve the highest throughput, leveraging an FPGA-based implementation with optimized parallel execution. Auto-ViT-Acc (1181.5 GOPs) and M³ViT (1217.4 GOPs) also report high throughput, suggesting effective hardware utilization. However, several studies, such as ViTCoD, do not report the throughput. Additionally, EQ-ViT and VAQF balance throughput with power efficiency, offering a more energy-efficient alternative.
\subsubsection{Accuracy}\hfill \\ Table~\ref{accuracy_performance} presents a comparative accuracy analysis across various ViT acceleration techniques on different edge platforms. To ensure a fair and structured comparison, we categorize our evaluation into four widely used ViT-based models: DeiT-Base, DeiT-Tiny, ViT-Base, and ViT-Small. While most acceleration techniques focus on classification tasks using the ImageNet-1K dataset~\cite{5206848}.

Among the methods analyzed, Auto-ViT-Acc achieves the highest Top-1 accuracy (81.8\%) on DeiT-Base, significantly surpassing VAQF (77.6\%) for classification tasks, demonstrating its effectiveness in preserving model accuracy while accelerating inference. For DeiT-Tiny, accuracy varies significantly across different methods: HeatViT (72.1\%), EQ-ViT (74.5\%), ViTCoD (70.0\%), and SOLE (71.07\%). Notably, EQ-ViT achieves the highest accuracy among these, highlighting the effectiveness of its attention-based optimizations. However, its energy consumption is significantly higher, indicating a trade-off between accuracy and efficiency.

M\textsuperscript{3}ViT\cite{fan2022m3vit} extends its evaluation to segmentation tasks on PASCAL-ContextNYUD-v2\cite{silberman2012indoor}. For segmentation tasks, M\textsuperscript{3}ViT delivers strong performance on PASCAL-Context (72.8 mIoU) but exhibits a noticeable decline on NYUD-v2 (45.6 mIoU), suggesting that its model compression techniques may be dataset-sensitive.

\input{Tables/table_accuracy_acc}








