\section{Privacy Risks in Asynchronous FL}
\label{sec03} 
The existing literature has extensively investigated FL's privacy leakage risks, particularly those arising from the exposure of gradient updates during the model training~\cite{picromveg2023Perfectly,pasfraate2022Eluding,ZhaShaElk2024Large-scale,melsondec2019exploiting}. While current aggregation techniques effectively mitigate such privacy leakages in SyncFL~\cite{NaHyeJun2022Closing,liuligao2023privacy-encoded,weilima2023personalized,HuGuoGon2023Federated}, they are not directly applicable in asynchronous settings, leading to numerous privacy and security concerns. 
As such, in this section, we focus on gradient reconstruction attacks in AsyncFL, targeting vision models, in which an adversary leverages leaked gradient updates to infer sensitive information about the original training data, reconstructing images or other private inputs with high fidelity.
\subsection{Threat Model and Security Assumptions}
\label{subsec03-01}
We assume an honest-but-curios adversary who could be present either at the server or the client. 
The aggregation server receives asynchronous model updates from clients, who update these model weights using their private training datasets. We assume that the clients' datasets have no overlap, a standard assumption in both SyncFL and AsyncFL literature~\cite{CheNinYue20,Mam21}. The server updates the aggregated model upon receipt of each client's model or on expiration of a countdown timer. 

The adversary is an honest-but-curious party, adhering to the training algorithm without introducing any modifications. The adversary has access to the global model's weights at any time step during the training phase and uses this information and attempts to utilize these weights to reconstruct clients private data. The aggregator employs an existing secure aggregation scheme, preventing the adversary from directly accessing client updates, but we assume that the adversary can access metadata in each training round to uniquely identify the participants participating in the corresponding round of training. 
We note that the adversary does not need to conduct an online attack and can conduct it in a post-training phase by leveraging the recorded global weight updates. This approach avoids any resource utilization or latency anomalies, making the attack stealthier and more difficult to detect.

We use different priors commonly utilized in the literature to analyze \sysname's privacy. These priors range from no prior to an identically distributed distribution to the target distribution. 



\subsection{Attack Design} 
\label{subsec03-02}
We consider adversaries with four different types of prior knowledge for our reconstruction attack. The default and weakest attack involves an adversary with {\it \textbf{no prior knowledge (No Prior)}}~\cite{deepLeakage}. The second type consists of adversaries with priors that include the {\it \textbf{total variation (TV)}} of the dataset~\cite{invertGradiants}. The third type involves adversaries with knowledge of the dataset's {\it \textbf{fidelity regularization (Fidelity)}}~\cite{batchRecovery}. Finally, the fourth and strongest type includes adversaries employing a {\it \textbf{deep prior}}, where it has access to an Independent and Identically Distributed (IID) dataset, which it uses to train an autoencoder~\cite{gradientObfuscation}. 



Although secure aggregation techniques render access to individual client gradients impractical in synchronous FL, this becomes a practical threat in asynchronous settings.
The increased likelihood of a single client update resulting in a global update enables the adversary to directly exploit the update and extract the individual clientâ€™s gradient.
Thus, an adversary with no prior knowledge can leverage the global weights at time step $t$ ($\mathcal{W}^{t}$) and the global weights from the previous time step ($\mathcal{W}^{t-1}$) to compute client $k$'s gradients:
\vspace{-0.15in}
\begin{align*}
\mathcal{W}^{t}  = \mathcal{W}^{t-1} -  \frac{1}{K}*\nabla_{\mathcal{\theta}}^k\mathcal{L}(x^{Priv},y^{Priv})
\\
\nabla_{\mathcal{\theta}}^k\mathcal{L}(x^{Priv},y^{Priv}) = (\mathcal{W}^{t} - \mathcal{W}^{t-1})*{K},
\vspace{-0.05in}
\end{align*}
where $K$ is the total number of clients, and $\nabla_{\mathcal{\theta}}^k\mathcal{L}(x^{Priv},y^{Priv})$ represents the gradient update computed by client $k$ using their private data sample ($x^{Priv}$) and the corresponding private label ($y^{Priv}$). 
Upon successful extraction of the client's local gradients from the global model, the adversary initializes a dummy input and its associated dummy label. By passing these dummy values through the global model update from a previous time step--used by the client to compute its gradient updates--the adversary calculates the gradient of the dummy loss function with respect to the model parameters \( \nabla_{\mathcal{\theta}}^k \mathcal{L}(x^{\text{dummy}}, y^{\text{dummy}}) \). If the adversary possesses no prior about the client, it minimizes the Euclidean distance between the dummy gradients and the client's private gradients by iteratively updating the dummy input and label, by using gradient descent, until they converge to the private input and label. After reconstructing \( x^{\text{Priv}} \), the adversary leverages metadata from the aggregation round, \eg participant ID, to identify the owner of the reconstructed sample.
\begin{figure}[t]
\centering
  \includegraphics[width=0.9\columnwidth]{Figures/reconstruction_attempts.pdf} 
  \vspace{-0.15in}
  \caption{Client data reconstruction by an adversary with deep prior knowledge~\cite{gradientObfuscation} across two neural network architectures. These results demonstrate the leakage of sensitive data that takes place in AsyncFed.}
  \label{fig:deep_prior_reconstruction} 
  \vspace{-0.25in}
\end{figure}

For more complex architectures, due to increased complexity and higher parameter count, minimization without any prior becomes infeasible. The second type of adversary (\ie TV), while using a similar setup, uses a cosine similarity loss function, to make optimization less sensitive to local optimas. The adversary utilizes its knowledge of the datasets total variation to add a TV regularization as a loss term. This leads to reduced search space and a higher fidelity reconstruction. 
Stronger adversaries can further utilize their insight into the data distribution to construct stronger regularization, called fidelity regularization, in order to reduce the search space to realistic images. This allows for the elimination of datapoints that have similar variance and parameter updates but lack the fidelity and structure of the target dataset.
\begin{figure*}[t]
\centering
  \includegraphics[width=0.95\textwidth]{Figures/DcTC_Overview_Final1.pdf} 
  \vspace{-0.1in}
  \caption{The architectural overview of \sysname framework, including its three main building blocks: {\it \textbf{Gradient Obfuscation via Orthogonal Distribution}}, {\it \textbf{TEE Mesh-based Secure Aggregation}}, and {\it \textbf{Data-centric Attestation}}.}
  \label{fig:overview}
  \vspace{-0.1in}
\end{figure*}


Our final adversary, by utilizing higher resources and an IID distribution, trains autoencoders with the goal of reducing the latent space dimensionality. This allows the adversary to perform reconstruction by searching the reduced space and then using the decoded output to calculate the reconstruction loss. While the attack shows a high success rate, we consider this the strongest prior because the adversary needs access to an identically distributed dataset, which may not be available in a federated learning setting. This also makes the deep prior a good worst-case reconstruction test in the FL setting. Figure~\ref{fig:deep_prior_reconstruction} illustrates the outcome of our reconstruction attack, using CNN and ResNet18 architectures with deep prior knowledge, on a set of low-res images, demonstrating attack practicality even in the most unlikely attack scenario. The full attack results will be detailed in Section~\ref{sec08}.











