\section{Preliminaries}
We begin by presenting the definition of differential privacy, which is a constraint on an algorithm \alg that processes a dataset $\ds = (\ds_1, \ldots, \ds_n)$ of user records, one per user. Two datasets are called \emph{neighbors} if they differ on one person's record. At a high level, differential privacy requires that for any pair of neighboring datasets, the algorithm's output distributions are similar when run on each dataset.

\begin{definition}[Differential Privacy~\citep{DworkMNS06, dwork2016calibrating}]\label{def:DP} A randomized algorithm $\alg: \universe^{n} \rightarrow \mathcal{Y}$ is {\em $\eps$-differentially private} if for every pair of neighboring datasets $\ds, \ds'\in \universe^n$, and for all outputs $y \in \mathcal{Y}$,
 \begin{equation*}
    \Pr[\alg(\ds) =y] \leq e^\eps \cdot \Pr[\alg(\ds') = y] + \delta,
 \end{equation*}
 where the probability is taken over the internal coins of \alg.
 \end{definition}
 
 The differential privacy guarantee is parameterized by $\eps>0$, where algorithms with lower values have less privacy leakage and higher values of epsilon denote more privacy leakage from the algorithm's output. DP gives a worst-case guarantee (over the algorithm's inputs and outputs) on how much information an algorithm leaks about its input.


\subsection{Prior Work}\label{sec:prior_work}

\paragraph{GAN-based methods for DP synthetic data}

Many prior works have proposed synthetic data mechanisms based on generative adversarial networks.  See \cite{yang2024tabular} for a nice survey of these and other approaches.  These mechanisms generally work by fitting the parameters of the model via DP-SGD, and then using the model to generate synthetic data after training.  These techniques are typically best suited for unstructured data like images or text.

\paragraph{Marginal-based methods for DP synthetic data}

Many mechanisms for DP synthetic data generation work by adding noise to low-dimensional marginals of the data distribution \cite{mckenna2021winning,mckenna2022aim,cai2021data,aydore2021differentially,fuentes2024joint,vietri2022private,liu2021iterative,liu2021leveraging,zhang2021privsyn}.  Some mechanisms in this space are also designed to leverage public data when it's available \cite{fuentes2024joint,liu2021iterative,liu2021leveraging}.  Benchmarks have confirmed these approaches work very well in tabular data settings \cite{tao2021benchmarking}.

% \paragraph{DP synthetic data methods that use foundation models} A number of recent works use foundation models trained on ``public data''\footnote{The extent to which data used to train LLMs is considered \emph{public} and compatible with privacy goals is hotly contested~\citep{tramer2022position}. We sidestep this question and assume public models are fair to treat as non-private, but acknowledge it remains an important question.} to improve differentially private synthetic data generation. Among these are two broad categories of methods: those which use DP finetuning on a foundation model, and API access only. \citet{sablayrolles2023privately},  \citet{tran2024differentially}, and \citet{afonja2024dp2stageadaptinglanguagemodels} use private finetuning on generative language models to generate private tabular synthetic data, and \citet{kurakin2023harnessing} similarly do private LoRA finetuning on an LLM to generate synthetic text data. Similarly, \citet{ghalebikesabi2023differentially} employ DP finetuning of diffusion models for generating DP synthetic images. 

% These finetuning methods, while quite powerful, require access to model weights---something which may not be available for state-of-the-art proprietary models. A series of works in the synthetic image \citep{lin2023differentially} and text \citep{xie2024differentially} domains use only API access to foundation models, combining queries with a genetic algorithm to privately generate synthetic data; these methods were further extended to the federated setting by \citet{hou2024pre}. Yet a different approach \citep{amin2024private} uses private prediction combined with other privacy budget saving tricks on the foundation model to generate DP synthetic text.

