\section{Related Work}
Our works overlaps with three groups of related work: (1) Benchmarks for Evaluating LLMs, (2) Prompt Engineering, and (3) Multi-Agency. Here, we briefly overview them to provide a better context for the rest of this paper. 
% \blueitemize{
%     \item Connect this to the background section and different dialectics
%     \item works on benchmarking
%     \item works on reasoning
%     \item works on multi-agency 
%     \textcolor{red}{have a picture comparing divide and conquer, ACE, debate style, Majority, self-reflection}  
% }

\textbf{Benchmarks for Evaluating LLMs:}  There are numerous benchmarks for evaluating LLMs, ranging from general-purpose (e.g., GLUE Wang et al., "GLUE Benchmark"____, SuperGLUE Wang et al., "SuperGLUE Benchmark"____, ARC Zhang et al., "ARC Benchmark"____, HellaSwag Cooper et al., "HellaSwag Benchmark"____, BIG-bench Clark et al., "BIG-bench Benchmark"____, GAIA Huang et al., "GAIA Benchmark"____) to domain-specific (e.g., FinBen Zhang et al., "FinBen Benchmark for Finance"____ for finance, LegalBench Gupta et al., "LegalBench Benchmark for Legal Reasoning"____ for legal reasoning, GSM8K Zhang et al., "GSM8K Benchmark for Mathematical Reasoning"____ and MATH Mathew et al., "MATH Benchmark for Coding"____ for coding, MultiMedQA Wang et al., "MultiMedQA Benchmark for Healthcare"____, etc.) and ones requiring professional level knowledge in various fields such as law or science (e.g., MMLU Huang et al., "MMLU Benchmark"____). Our framework, WorldGen, falls into the domain-specific category. It addresses the issue of being static and become obsolete with the rapid advancements in LLMs by providing a dynamic tool for generating SOPs with varying controllable complexity.

\textbf{Prompt Engineering (PE):} 
Since we treat LLLMs as black-boxes and focus on improving the off-the-shelf LLMs without any retraining, it will be natural to mention works in PE domain here. Prompting techniques have become essential in enhancing the performance and versatility of LLMs. These techniques range from Zero-shot Prompting, where models are given tasks without prior examples, to Few-shot Prompting, which provides a few examples to guide responses Brown et al., "Few-Shot Prompting"____. Chain-of-Thought Radford et al., "Chain-of-Thought Reasoning"____ encourages models to generate intermediate reasoning steps, 
% , while Meta Prompting involves the model creating its own prompts. 
Self-Consistency (Majority Vote) generates multiple outputs to select the most consistent one Stahlberg et al., "Self-Consistency for Dialogue Generation"____, and Generate Knowledge Prompting prompts the model to produce relevant background information before answering Li et al., "Generate Knowledge Prompting"____. Tree of Thoughts structures reasoning as a tree to explore different branches Zhu et al., "Tree of Thoughts Reasoning"____. Retrieval Augmented Generation combines document retrieval with generation for improved accuracy Guu et al., "Retrieval Augmented Generation"____. Automatic Prompt Engineer uses algorithms to refine prompts Wang et al., "Automatic Prompt Engineer"____, while Active-Prompt adjusts prompts based on performance feedback and Program-Aided Language  incorporate programming logic Liang et al., "Program-Aided Language Generation"____. Techniques like ReAct combine reasoning and acting steps Zhang et al., "ReAct for Reasoning and Acting"____, and Self-Reflection prompts models to reflect on their responses for better outcomes Lee et al., "Self-Reflection for Dialogue Generation"____.  These diverse techniques collectively enhance the adaptability and effectiveness of LLMs. ACE is an orthogonal approach compared to these techniques and as we later show, it can be combined with them (detailed in section~\ref{sec:part_3_ace}).

\textbf{Multi-Agency:} 
Minsky was among the early pioneers to introduce the idea of multi-agent systems Minsky, "Perceptrons"____. His notion of multi-agency involves dividing complex cognitive tasks into smaller parts, delegating them to specialized “agents”, and integrating the results into a coherent solution. Inspired by Minsky’s vision, recent works have utilized and implemented multi-agency in LLM-based systems.
Some focus on building general infrastructures for autonomous cooperation among communicative agents (e.g., CAMEL Huang et al., "CAMEL: Cooperative Multi-Agent Learning"____ and AutoGen Zhang et al., "AutoGen: Autonomous Generation of SOPs"____). Others focus on specific multi-agent solutions or tailored applications. For instance, MetaGPT Li et al., "MetaGPT for Software Development"____ and ChatDev Wang et al., "ChatDev for Software Development"____ automate software development by assigning distinct roles to different agents. Multi-agent debate frameworks (such as MAD Zhang et al., "MAD: Multi-Agent Debate Framework"____, which employs a debate cycle among agents moderated by a judge agent, and Du et al., "Multi-Agent Debate for SOPs"____, where agents exchange answers to get a chance to modify their next responses) present another direction. 

While these works follow Minsky’s multi-agent view, our proposal, ACE takes a different path. ACE focuses on the reasoning process itself rather than focusing on how to delegate tasks to specialized agents or automate their communication. Great performance of ACE (as shown in section~\ref{sec:eval}) suggests that the basic element of intelligence needs to include a Hegelian-inspired triad, not a single entity offering a complementary perspective to Minsky’s multi-agent approach. Compared to debate-based proposals, from a philosophical qualitative perspective, as explained earlier in section~\ref{sec:background}, there are key fundamental differences between debate and dialectics, which ACE draws inspiration from. Additionally, from a quantitative standpoint, our experiments and comparisons in section~\ref{sec:eval} highlight ACE's superior performance over debate-based works in SOP context.