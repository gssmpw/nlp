\section{Related Work}
Our works overlaps with three groups of related work: (1) Benchmarks for Evaluating LLMs, (2) Prompt Engineering, and (3) Multi-Agency. Here, we briefly overview them to provide a better context for the rest of this paper. 
% \blueitemize{
%     \item Connect this to the background section and different dialectics
%     \item works on benchmarking
%     \item works on reasoning
%     \item works on multi-agency 
%     \textcolor{red}{have a picture comparing divide and conquer, ACE, debate style, Majority, self-reflection}  
% }

\textbf{Benchmarks for Evaluating LLMs:}  There are numerous benchmarks for evaluating LLMs, ranging from general-purpose (e.g., GLUE~\cite{wang2018glue}, SuperGLUE \cite{wang2019superglue}, ARC \cite{clark2018think}, HellaSwag \cite{zellers2019hellaswag}, BIG-bench \cite{srivastava2022beyond}, GAIA~\cite{gaia}) to domain-specific (e.g., FinBen \cite{xie2024finben} for finance, LegalBench~\cite{guha2024legalbench} for legal reasoning, GSM8K \cite{gsm8k} and MATH \cite{mmlu} for mathematical reasoning, HumanEval \cite{chen2021evaluating} and MBPP~\cite{austin2021program} for coding, MultiMedQA \cite{singhal2023large} for healthcare, etc.) and ones requiring professional level knowledge in various fields such as law or science (e.g., MMLU \cite{mmlu}). Our framework, WorldGen, falls into the domain-specific category. It addresses the issue of being static and become obsolete with the rapid advancements in LLMs by providing a dynamic tool for generating SOPs with varying controllable complexity.

\textbf{Prompt Engineering (PE):} 
Since we treat LLMs as black-boxes and focus on improving the off-the-shelf LLMs without any retraining, it will be natural to mention works in PE domain here. Prompting techniques have become essential in enhancing the performance and versatility of LLMs. These techniques range from Zero-shot Prompting, where models are given tasks without prior examples, to Few-shot Prompting, which provides a few examples to guide responses \cite{brown2020fewshot}. Chain-of-Thought encourages models to generate intermediate reasoning steps \cite{wei2022cot},
% , while Meta Prompting involves the model creating its own prompts. 
Self-Consistency (Majority Vote) generates multiple outputs to select the most consistent one \cite{wang2022majority,lewkowycz2022majority2}, and Generate Knowledge Prompting prompts the model to produce relevant background information before answering~\cite{liu2021generated}. Tree of Thoughts structures reasoning as a tree to explore different branches~\cite{yao2024tree,long2023tree2}. Retrieval Augmented Generation combines document retrieval with generation for improved accuracy \cite{lewis2020retrieval}. Automatic Prompt Engineer uses algorithms to refine prompts~\cite{zhou2022prompteng}, while Active-Prompt~\cite{diao2023active} adjusts prompts based on performance feedback and Program-Aided Language  incorporate programming logic~\cite{gao2023pal}. Techniques like ReAct combine reasoning and acting steps \cite{yao2022react}, and Self-Reflection prompts models to reflect on their responses for better outcomes~\cite{madaan2024selfreflection, shinn2024reflexion}.  These diverse techniques collectively enhance the adaptability and effectiveness of LLMs. ACE is an orthogonal approach compared to these techniques and as we later show, it can be combined with them (detailed in section~\ref{sec:part_3_ace}).

\textbf{Multi-Agency:} 
Minsky was among the early pioneers to introduce the idea of multi-agent systems~\cite{minsky1988society}. His notion of multi-agency involves dividing complex cognitive tasks into smaller parts, delegating them to specialized “agents”, and integrating the results into a coherent solution. Inspired by Minsky’s vision, recent works have utilized and implemented multi-agency in LLM-based systems.
Some focus on building general infrastructures for autonomous cooperation among communicative agents (e.g., CAMEL~\cite{li2023camel} and AutoGen~\cite{wu2023autogen}). Others focus on specific multi-agent solutions or tailored applications. For instance, MetaGPT~\cite{hong2023metagpt} and ChatDev~\cite{qian2023chatdev} automate software development by assigning distinct roles to different agents. Multi-agent debate frameworks (such as MAD~\cite{liang2023mad}, which employs a debate cycle among agents moderated by a judge agent, and Du et al.~\cite{du2023debate}, where agents exchange answers to get a chance to modify their next responses) present another direction. 

While these works follow Minsky’s multi-agent view, our proposal, ACE takes a different path. ACE focuses on the reasoning process itself rather than focusing on how to delegate tasks to specialized agents or automate their communication. Great performance of ACE (as shown in section~\ref{sec:eval}) suggests that the basic element of intelligence needs to include a Hegelian-inspired triad, not a single entity offering a complementary perspective to Minsky’s multi-agent approach. Compared to debate-based proposals, from a philosophical qualitative perspective, as explained earlier in section~\ref{sec:background}, there are key fundamental differences between debate and dialectics, which ACE draws inspiration from. Additionally, from a quantitative standpoint, our experiments and comparisons in section~\ref{sec:eval} highlight ACE's superior performance over debate-based works in SOP context.