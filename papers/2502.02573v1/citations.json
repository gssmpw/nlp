[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2018glue",
        "author": "Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel Bowman",
        "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2019superglue",
        "author": "Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel Bowman",
        "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "clark2018think",
        "author": "Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind",
        "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zellers2019hellaswag",
        "author": "Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin",
        "title": "Hellaswag: Can a machine really finish your sentence?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gaia",
        "author": "Mialon, Gr\u00e9goire and Fourrier, Cl\u00e9mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas",
        "title": "GAIA: A Benchmark for General AI Assistants"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xie2024finben",
        "author": "Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and others",
        "title": "The finben: An holistic financial benchmark for large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "guha2024legalbench",
        "author": "Guha, Neel and Nyarko, Julian and Ho, Daniel and R{\\'e}, Christopher and Chilton, Adam and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and others",
        "title": "Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "gsm8k",
        "author": "Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John",
        "title": "Training Verifiers to Solve Math Word Problems"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mmlu",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring Massive Multitask Language Understanding"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2021evaluating",
        "author": "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others",
        "title": "Evaluating large language models trained on code"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "austin2021program",
        "author": "Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others",
        "title": "Program synthesis with large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "singhal2023large",
        "author": "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others",
        "title": "Large language models encode clinical knowledge"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "mmlu",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring Massive Multitask Language Understanding"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "brown2020fewshot",
        "author": "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wei2022cot",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H and Le, Quoc and Zhou, Denny",
        "title": "Chain of thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wang2022majority",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed H and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      },
      {
        "key": "lewkowycz2022majority2",
        "author": "Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others",
        "title": "Solving quantitative reasoning problems with language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "liu2021generated",
        "author": "Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh",
        "title": "Generated knowledge prompting for commonsense reasoning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      },
      {
        "key": "long2023tree2",
        "author": "Long, Jieyi",
        "title": "Large language model guided tree-of-thought"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\u00fcttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\u00e4schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhou2022prompteng",
        "author": "Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy",
        "title": "Large language models are human-level prompt engineers"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "diao2023active",
        "author": "Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Pan, Rui and Liu, Xiang and Zhang, Tong",
        "title": "Active prompting with chain-of-thought for large language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "gao2023pal",
        "author": "Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham",
        "title": "Pal: Program-aided language models"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "yao2022react",
        "author": "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Cao, Shixiang Shane and Yu, Ziyi and Narasimhan, Karthik and Cao, Yuan",
        "title": "ReAct: Synergizing reasoning and acting in language models"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "madaan2024selfreflection",
        "author": "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others",
        "title": "Self-refine: Iterative refinement with self-feedback"
      },
      {
        "key": "shinn2024reflexion",
        "author": "Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu",
        "title": "Reflexion: Language agents with verbal reinforcement learning"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "minsky1988society",
        "author": "Minsky, Marvin",
        "title": "Society of mind"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "li2023camel",
        "author": "Li, Guohao and Hammoud, Hasan and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard",
        "title": "Camel: Communicative agents for\" mind\" exploration of large language model society"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "wu2023autogen",
        "author": "Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi",
        "title": "Autogen: Enabling next-gen llm applications via multi-agent conversation framework"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "hong2023metagpt",
        "author": "Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others",
        "title": "Metagpt: Meta programming for multi-agent collaborative framework"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "qian2023chatdev",
        "author": "Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong",
        "title": "Communicative agents for software development"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "liang2023mad",
        "author": "Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng",
        "title": "Encouraging divergent thinking in large language models through multi-agent debate"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "du2023debate",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor",
        "title": "Improving factuality and reasoning in language models through multiagent debate"
      }
    ]
  }
]