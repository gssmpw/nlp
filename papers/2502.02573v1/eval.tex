\section{Evaluation}
\label{sec:eval}
% \soheilb{Interactive Cycle}
% \soheilb{Problem Generator}
\begin{table*}[t]
\caption{Success rates (\%) of different schemes in 3-D worlds with L1 and L2 complexity levels using various base LLMs}
\label{table:main}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
Scheme & \multicolumn{6}{c}{Success Rate $\uparrow$}  \\
\hline
& \multicolumn{3}{c|}{L1} & \multicolumn{3}{c}{L2}\\
\cline{2-7}
\multirow{2}{*}{} & GPT-4-32k & Llama-3-70B-Inst & GPT-3.5-Turbo & GPT-4-32k & Llama-3-70B-Inst & GPT-3.5-Turbo \\
\cline{2-7}
LLM$^+$ (Default)       & 36\% & 16\% & 7\% & 4\% & \textbf{3\%} & 1\%\\ 
Self-Reflection      & 30\% & 16\% & 8\% & 7\% & \textbf{3\%} & \textbf{3\%}\\
Debate               & 39\% & 16\% & 7\% & 7\% & 2\% & 1\%\\
Majority             & 38\% & 27\% & 10\% & 7\% & \textbf{3\%} & 2\%\\
ACE                  & \textbf{88\%} & \textbf{29\%} & \textbf{22\%} & \textbf{9\%} & 2\% & \textbf{3\%}\\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

\subsection{Overall Results}
\textbf{Settings:}
To put the performance improvements of ACE in proper context, we implemented several recent related proposals including Self-Reflection~\cite{madaan2024selfreflection}, Majority Vote~\cite{wang2022majority,lewkowycz2022majority2}, and Debate~\cite{du2023debate} and compared them with ACE. To have a fair comparison, for the Majority and Debate schemes we set the total number of agents to three and two agents, respectively, to roughly match the token usage of ACE. Later, in section~\ref{sec:eval:deepdive}, we perform more evaluations with higher number of agents for them.
We accompany all these schemes with the additional prompting techniques appeared in LLM$^+$ to have a fair comparison (check Appendix~\ref{sec:app:prompts} for more details). As for the LLMs, we use 3 different models: GPT-4-32K~\cite{gpt4}, Llama-3-70B-Instruct~\cite{dubey2024llama3}, and GPT-3.5-Turbo~\cite{openai2025gpt35}. As before, we repeat evaluations 100 times and report the success rate of different schemes.

\textbf{Results:} Table~\ref{table:main} presents the findings. A comparison of the general performance of LLMs in L1 and L2 complexity levels reveals that LLMs struggle to solve SOPs as problem complexity increases. This aligns with the objective of WorldGen, which is designed to generate progressively complex SOPs to rigorously evaluate model performance. The results also demonstrate that ACE significantly enhances performance, boosting the capabilities of base language models across various scenarios. For instance, ACE achieves a remarkable success rate of 88\% with GPT-4-32K in L1 complexity, more than doubling the next best approach.
When compared to alternative methods, ACE emerges as the superior solution, showcasing its effectiveness in enhancing LLM performance. However, ACE is not without limitations. In scenarios where the base model’s inherent capabilities are limited, such as in L2 complexity with GPT-3.5-Turbo, performance gains are modest. This is because ACE operates by treating LLM as a black-box, relying on the existing abilities of the LLM without retraining or modifying its weights. Consequently, ACE cannot compensate for a model’s lack of foundational knowledge. For example, a model achieving a 0\% success rate indicates it lacks the necessary baseline understanding to solve the problem, rendering ACE's dialectical method—rooted in the same model—ineffective. That explains why ACE's effectiveness varies with the capabilities of the underlying LLM\footnote{A sample dialectical process in ACE is shown in Appendix~\ref{sec:app:dialectical_sample}}.

\subsection{Deep Dive}
\label{sec:eval:deepdive}
% \textbf{Cost Comparison:} 
% Here, we report the cost of using ACE in terms of total tokens used and compare it with other schemes. Table~\ref{table:cost} presents the average total number of tokens used in each scenario, normalized to the default single-agent scheme, LLM$^+$ and GPT-3.5-Turbo as the base model. On average, ACE consumes $2.27\times$ tokens compared to LLM$^+$, yet compared to schemes with multiple agents, Debate and Majority, it uses fewer tokens.  
\textbf{Cost Comparison:}
We evaluate the cost of using ACE by analyzing the total number of tokens consumed and comparing it with other approaches. Table~\ref{table:cost} provides the average total token usage for one run of the experiment for various schemes, and their corresponding normalized values (to the default single-agent scheme, LLM$^+$) using GPT-3.5-Turbo as the base model. On average, ACE consumes $2.27\times$ the total tokens compared with LLM$^+$; however, it remains more efficient compared to multi-agent schemes like Debate and Majority, which require higher token usage.

\begin{table}[t]
\caption{Averaged total consumed tokens of different schemes (with GPT-3.5-Turbo as base) in one instance of an experiment}
\label{table:cost}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcccr}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lcc}
\toprule
Scheme & Normalized $\downarrow$ & Total Tokens$\downarrow$ \\
\midrule
LLM$^+$            & 1  &  6912 \\
Self-Reflection & 1.24$\times$   &  8581 \\
ACE             & 2.27$\times$   &  16211 \\
Debate          & 2.66$\times$   &  18396 \\
Majority        & 3.29$\times$   &  22733 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

\textbf{ACE vs. Multi-Agent Schemes with More Agents:} 
A natural assumption might be that increasing the number of agents in schemes like Debate and Majority would enhance performance. So, we conducted additional experiments, scaling the number of agents to seven (referred to as Debate$^\star$ and Majority$^\star$) and comparing their performance and token costs with ACE. These experiments were carried out using GPT-4-32K and GPT-3.5-Turbo as base models in the L1 3-D world sample. The results are summarized in Table~\ref{table:7agents}.
As shown in Table~\ref{table:7agents}, increasing the number of agents in these schemes results in higher costs without corresponding performance improvements. In fact, the performance of Debate$^\star$ and Majority$^\star$ degrades when compared to their counterparts with fewer agents. Schemes like Debate$^\star$, which require exchanging responses among all agents in each round, can experience exponential growth in token consumption as the number of agents increases. This inefficiency is particularly evident in SOP settings, where sequential decision-making processes require multiple rounds of interaction, as illustrated by the token cost data in Table~\ref{table:7agents}.
The key takeaway is that simply increasing the number of agents in tasks involving sequential decision-making, such as those in SOP settings, does not necessarily yield better results. Instead, it often introduces inefficiencies and performance degradation in solutions like Debate$^\star$ and Majority$^\star$.

\begin{table}[h]
\caption{Success rates (\%) and averaged total token cost of Debate$^\star$ and Majority$^\star$ (using 7 agents) and ACE, normalized to tokens used by LLM$^+$ (with the same base LLM) in L1 3-D world scanerio}
\label{table:7agents}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{l|cc|cc}
\toprule
Scheme & \multicolumn{2}{c|}{GPT-4-32k} & \multicolumn{2}{c}{GPT-3.5-Turbo} \\
\hline
 & Success $\uparrow$ & Cost $\downarrow$ & Success $\uparrow$ & Cost $\downarrow$  \\
\hline
% & \multicolumn{3}{c|}{L1} & \multicolumn{3}{c}{L2}\\
% \cline{2-7}
% LLM$^+$        & 36\% & 1 & 4\% & 1 \\
Debate$^\star$     & 32\% & 32.07$\times$ & 3\% &  11.34$\times$\\
Majority$^\star$   & 27\% & 6.34$\times$ & 6\% & 6.83$\times$ \\
ACE         & \textbf{88}\% & \textbf{3.64$\times$} & \textbf{22\%} & \textbf{2.27$\times$} \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{ACE in Static Settings:}
Our primary domain and targeted setting (SOPs) possess an important characteristic: the ability of the World to provide feedback during task execution. This feedback plays a pivotal role in ACE’s architecture, as the generation of Antithesis relies not only on the Thesis but also on the feedback provided by the World after executing the Thesis. This feedback serves as a ground truth, guiding the generation of a more refined Antithesis. 
To explore how ACE performs in scenarios without access to such feedback, we use a modified version of ACE, referred to as ACE$^\ast$, which operates in a general static task where real-time feedback from the World is unavailable. So here, we utilize the MMLU~\cite{mmlu} benchmark, focusing on multiple-choice question answering. For our experiments, we select three datasets: (1) a set of 100 randomly chosen questions (Set \#1), (2) a college-level Physics test (Set \#2), and (3) a high-school-level Statistics test (Set \#3). 
Table~\ref{table:mmlu} summarizes the results. While ACE$^\ast$ outperforms other schemes like Debate and Majority in Sets \#1 and \#2, the performance gap is less pronounced than in SOP context where feedback from the World is available. Additionally, ACE$^\ast$ struggles with Set \#3, showing lower performance levels compared to other benchmarks. These results can highlight the critical importance of World feedback in effective Antithesis generation.

\begin{table}[t]
\caption{Averaged Score of Different Schemes (with GPT-4-32K as base model) in MMLU benchmarks}
\label{table:mmlu}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcccr}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccc}
\toprule
Scheme & MMLU (set \#1) & MMLU (set \#2) & MMLU (set \#3)\\
\midrule
Default         & 87.1 $\pm$ 0.74 & 80.2 $\pm$ 2.1 & 179.6 $\pm$ 2.67\\
Majority        & 87.3 $\pm$ 0.95 & 81.1 $\pm$ 1.1 & \textbf{180.7} $\pm$ 1.7\\
Debate          & 87.3 $\pm$ 1.64 & 69.6 $\pm$ 3.1 & 160.0 $\pm$ 3.23\\
ACE$^\ast$             & \textbf{88.4} $\pm$ 2.32 & \textbf{85.0} $\pm$ 2.21 & 178.1 $\pm$ 3.07\\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.3in
\end{table}

