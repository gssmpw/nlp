\section{Introduction}

% \textbf{Setting the Context:}
\textbf{Setting the Context:}
Optimization is fundamental to decision-making across diverse domains such as engineering, science, economics, healthcare, and even nature. The essence of decision-making lies in choosing the best option from a set of alternatives, driven by objectives such as efficient resource allocation, cost minimization, profit maximization, or performance enhancement of systems and infrastructures~\cite{opt_intro}. However, solving optimization problems is often intricate, requiring specialized expertise and addressing practical challenges like high dimensionality, nonlinearity, and the dynamic, stochastic nature of real-world environments~\cite{opt_complex}. Consequently, there is a continuous quest across various fields to simplify the optimization process.

\textbf{A New Opportunity:}
Simultaneously, Large Language Models (LLMs), such as GPT-4~\cite{gpt4}, have shown impressive capabilities in both general assistance and specialized domains, including mathematics, coding, and law~\cite{gpt4_results}. These advancements present a compelling opportunity to revolutionize our approach to solving optimization problems by leveraging LLMs to automate solution processes. This emerging potential prompts a fundamental scientific inquiry: How proficient are current LLMs in handling optimization problems?

\textbf{Assessing LLMs in a New Context:}  
Although various benchmarks exist to evaluate LLM performance in general and specialized tasks (e.g., coding, mathematics), their capabilities in solving optimization problems, particularly Sequential Optimization Problems (SOPs), remain underexplored. SOPs involve making a series of decisions over time, where each decision affects subsequent options and outcomes, creating a complex web of interdependencies. Different challenges contribute to this issue, primarily the need for defining a representative set of SOPs and ensuring that observed performance is not influenced by data contamination or prior exposure during the LLMs' training~\cite{data_contamination_1,gaia,hellaswag}.


\textbf{On-Demand SOP Generation:}
Motivated by these opportunities and challenges, in the first half of this paper we aim to addresse this research question: How can we evaluate the performance of LLMs in SOPs? We introduce a straightforward yet effective framework, WorldGen, capable of generating unseen SOPs with controllable complexities on demand. WorldGen contrasts with most existing static benchmarks (e.g., MMLU~\cite{mmlu}, GLUE~\cite{wang2018glue}, SuperGLUE~\cite{wang2019superglue}, GSM8k~\cite{gsm8k}, etc.) which become obsolete as LLMs evolve~\cite{gaia}. Utilizing this dynamic framework, we have made two key observations: (1) For relatively simple optimization tasks, with a single global maximum and no local maxima (i.e., simple surfaces and scenarios), current LLMs can solve them efficiently. (2) As the complexity of the optimization problems increases, the performance of LLMs degrades significantly and becomes unsatisfactory.

\textbf{Improving LLMs with Roots in Philosophy:}
Inspired by these observations and the poor performance of LLMs in SOPs, in the second half of this paper, we propose a systematic approach to enhance the performance of off-the-shelf LLMs (in the context of SOPs) without necessitating any retraining, treating the LLM as a black box. Specifically, we aim to transcend existing prompt engineering techniques and discover a more structured method to improve LLM performance, grounded in explainable and reasoned principles. This led us to revisit the fundamentals of reasoning, as it appeared crucial to the low performance of LLMs in our targeted scenarios. Consequently, we delved into centuries of existing literature in philosophy and drew inspiration from well-established philosophical frameworks that describe the reasoning process in a structured manner. Notably, we explored how Hegelian Dialectics~\cite{hegel_science_logic}, a renowned philosophical hypothesis proposed by Hegel, could enhance the capabilities of existing LLMs. By integrating concepts from Hegelian Dialectics, we designed ACE, a novel framework that can significantly improve the performance of off-the-shelf LLMs in SOPs.
% \footnote{Since it is debatable whether ACE improves the actual reasoning capability of the LLMs or enhances how they mimic human reasoning processes more effectively, throughout the paper, we intentionally  avoid saying that ACE improves the reasoning of LLMs. Instead, we choose to say their performance is improved.}.

\textbf{Main Contributions:}
In summary, this paper makes the following key contributions:
\begin{enumerate}
    \item We design WorldGen, a framework to assess the performance of LLMs in SOP settings, addressing the data contamination issues typically associated with general LLM benchmarks (detailed in section~\ref{sec:worldgen}). WorldGen allows for the growth of evaluation complexity in line with the advancement of LLMs.
    \item Using this framework, we provide initial observations on the poor performance of current LLMs in SOPs, motivating the need for developing further techniques to enhance LLM capabilities (detailed in section~\ref{sec:part_2_evaluations}).
    \item We propose ACE, a solution inspired by one of the most successful dialectical hypotheses in philosophy for explaining reasoning and its improvement. We show that ACE can greatly enhance the performance of LLMs in SOPs, potentially paving the way for more structured frameworks rooted in established philosophical works to improve LLMs (detailed in section~\ref{sec:part_3_ace}).
\end{enumerate}

