%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usetikzlibrary{fit, positioning, backgrounds}


\newpage
\appendix
\onecolumn
\section{More on the Evaluations and the Prompt Templates Used}
\label{sec:app:prompts}
\textbf{Prompt Templates:}
Figure~\ref{fig:prompts:llm+} shows the main template used for the LLM$^+$ scheme. We use the same template for the main agent of other schemes compared in this paper, including ACE's Actor. Additionally, Figure~\ref{fig:prompts:critic} and~\ref{fig:prompts:critic:transitional} demonstrate the initial and transitional prompts used for ACE's Critic, respectively. The task of the Synthesizer, the Actor of the previous and next steps, will be identified through a transitional prompt, as shown in Figure~\ref{fig:prompts:synth}.
% \input{synth_prompt}

\textbf{Majority Scheme:}
To implement the Majority scheme and automate the solution, we use another agent called the poll worker. The poll worker checks different agents' responses and identifies the Majority response, which is the one with the highest consensus. Figure~\ref{fig:prompts:pollworker} shows the prompt template used for the poll worker. 
Unlike taking the majority vote after every agent completes the task in general scenarios, in our sequential decision-making problems, we need to take the majority vote in every round. Therefore, the poll worker processes the agents' responses at each round of interaction with the World, identifies the response with the majority consensus at each round so that the World can execute it and provide the feedback.

\section{A Couple of Samples for LLM$^+$ in Action} 
\label{sec:app:llm+}
Figures~\ref{fig:llm+:sample1} and~\ref{fig:llm+:sample2} demonstrate two samples of strategies used by LLM$^+$ in separate runs, utilizing the GPT-4-32K base model in an L1 world. In the first run (Figure~\ref{fig:llm+:sample1}), the agent begins with a broad grid search strategy, aiming to cover the entire search space and identify regions with high values. This initial phase can form a foundation for subsequent searches. However, the agent does not adapt its strategy in the following rounds, remaining static and failing to find the optimum point by the end of the 16 rounds.

In the second example (Figure~\ref{fig:llm+:sample2}), the agent attempts to adapt its strategy based on the feedback it receives from the World. Initially, the agent starts with a coarse exploration to identify promising regions. It then refines the search around one promising area identified in the initial phase and continues with further refinements. However, it still cannot find the optimum point. This time, the issue lies in the agent spending a significant number of queries exploring around the local maximum found. The search strategy was not sufficiently adaptive to balance between local exploitation and global exploration. Nevertheless, the fact that it found a local maximum is noteworthy, as it demonstrates some degree of understanding about the notion of a maximum point in a region, the direction of increase or decrease of a sequence of observations, and their relation to the actual curve modeling the unknown World function, \( f \).

\section{A Sample of the Dialectical Process in ACE} 
\label{sec:app:dialectical_sample}
Figures~\ref{fig:ace:sample:part1} and~\ref{fig:ace:sample:part2} demonstrate a sample of a dialectical progress with ACE, utilizing the GPT-4-32K base model in an L1 world. The iterative process of ACE's dialectical method, which involves a cycle of thesis, antithesis, and synthesis allows for systematic refinement and improvement of strategies based on feedback and critique. 
One of the key advantages of ACE is its ability to adapt and improve through structured feedback. For instance, the initial grid search strategy (Thesis 1 in Figure~\ref{fig:ace:sample:part1}) provided a broad understanding of the search space. However, the corresponding antithesis highlighted the need for refinement in promising areas, the incorporation of adaptive techniques, and a balance between exploration and exploitation. This critical feedback led to a more refined and effective strategy (Synthesis 1 in Figure~\ref{fig:ace:sample:part2}), which combined grid search with adaptive methods like simulated annealing.

The dialectical method also ensured that the agent did not become overly focused on a single approach. By evaluating and adjusting strategies through a dialectal cycle, ACE attempted to balance the thorough exploration of high-value areas with the need to investigate less explored regions. This is evident in the transition from Thesis 2 to Synthesis 2 (Figure~\ref{fig:ace:sample:part2})), where the agent broadened its exploration based on feedback, expanding the use of simulated annealing to uncover potential peaks outside the heavily focused regions.

Moreover, the dialectical approach fostered a dynamic and flexible search process. The agent's ability to incorporate feedback and adjust its methods in real-time allowed for efficient use of queries and increased the chances of identifying the global maximum. This adaptability is crucial in complex search spaces where the landscape can vary significantly.

In short, the Hegelian Dialectics aspect of ACE offered a mechanism for continuous improvement. By leveraging structured feedback and iterative refinement, ACE enhanced the agent's ability to navigate search spaces effectively. The figures illustrate this process, showcasing how each cycle of thesis, antithesis, and synthesis leads to progressively better strategies and outcomes.

\input{actor_prompt}
\input{critic_prompt}
\input{synth_prompt}
% \input{intermidiate_prompts}
\input{poll_worker}

\input{llm_plus_sample}

\input{ACE_sample}

% \blueitemize{
%     % \item how the prompting happens and notion of each keeping the context
%     % \item role assignment, objective descriptions, ...
%     \item Deep Dive
%     \begin{itemize}
%         \item Analysis of the main eval
%         \item Distribution of the algorithms used, etc --> appendix
%         % \item sample of the prompts --> appendix
%         % \item cost/token comparison stuff
        
%         % \item \textcolor{red}{\# of agents/antithesis $=>$ this may not be anymore valid as we are not doing debate style job}
        
%         % \item \textcolor{red}{\# of agents and majority solution $=>$ not valid anymore maybe} 
%     \end{itemize}
%     \item debate style vs ACE, importance of the world feedback being in the loop
%     % \item
%     %  We analyzed the algorithms that the agents used in L1 scenarios. Here are ...
%     % sample figures of queries made on the 2-d map for L1 and L2?
%     \item
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


