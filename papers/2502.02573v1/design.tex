% \section{LLMs \& Sequential Optimization Tasks}
\section{Assessing \& Improving LLMs in SOPs}
\label{sec:part1_evaluatingLLMs}
% \textit{The Complexity of SOPs:} 
SOPs are pervasive across diverse domains, ranging from logistics and resource allocation to machine learning and operations research. Requiring specialized knowledge to address practical issues such as high dimensionality, nonlinearity, and the dynamic, unpredictable nature of real-world settings make SOPs complex in their nature~\cite{opt_complex}. Automating the solution of such problems is highly desirable, as it can lead to efficiency gains and innovative solutions to complex challenges. On the other hand, LLMs have demonstrated remarkable capabilities, including proficiency in coding and exceptional context-awareness, making them promising candidates for tackling SOPs. 

So, naturally, exploring the performance of LLMs in addressing these crucial tasks is a significant step forward in understanding their broader applicability and the opportunity to solve these problems automatically. However, evaluating LLMs in this context comes with it own set of challenges. Key concerns include managing data contamination, ensuring that the problems and their solutions were not inadvertently exposed during training, and consequently distinguishing between genuine reasoning and mere memorization. Additionally, access to a set of representative optimization problems is vital to effectively assess LLMs' capabilities in solving sequential tasks. Addressing these challenges will enable a deeper understanding of the role LLMs can play in advancing optimization methodologies.

% \begin{itemize}
%     \item a long horizon task with inter-dependencies of step-wise decisions as opposed to myopic tasks and rewards
%     \item requires efficient context Handling
%     \item requires proper reasoning capabilities   
% \end{itemize}

% \subsection{Evaluating LLMs in Sequential Optimization Tasks}
% \blueitemize{
%     \item mastery in primary functionalities such as coding, and context-awareness potentials make LLMs good candidates to perform sequential optimization tasks
%     \item so it would be natural to see how they perform in these crucial tasks
    
%     \item challenges of choosing sequential optimization tasks and assessment of LLMs' performance
%     \begin{itemize}
%         \item handling data contamination
%         \item how to ensure the problem and/or its answer were not seen during the training? 
%         \item distinguish memorization vs actual reasoning
%         \item access to a large set of representative optimization problems
%     \end{itemize}
% }

% \subsection{WorldGen: Having Flexibility to Generate SOPs}
\subsection{WorldGen}
\label{sec:worldgen}
% \blueitemize{
%     \item any optimization problem at the end is about finding the optimum point(s) in an n-dimensional world represented by $f(x_1,x_2,...,x_{n-1})$
%     \item so, instead of attempting to find the optimization problems, we pay attention to the worlds representing those problems. 
%     \item iow, instead of defining an optimization problem with the risk of data contamination, we directly generate an n-dimensional world that can represent the space corresponding to various optimization problems.
%     \item and then, the optimization problem itself will be defined as finding the maximum point(s) in this n-dimensional world. 
%     \item this will provide us will great guarantee that the LLM agent did not have the opportunity to see the problem or its solution during the training. Iow, the scenario becomes more like giving a new optimization problem to an optimization expert and asking her to solve it using any technique pleased.
%     \item moreover, the world generator provides a great potential to generate complicated problems to test the future more capable LLMs as oppose to using static benchmarks that will be obsolete in near future. iow, the setting and benchmark can grow with the LLMs! 
% }

\textbf{Core Idea:} 
At the heart of optimization lies the task of finding the optimum point(s) in an \(n\)-dimensional world, mathematically expressed as \(f(x_1, x_2, \dots, x_{n-1})\). So, instead of focusing on specific optimization problems, we shift our attention to the worlds that represent these problems. In other words, rather than trying to come up with specific optimization problems, which may inadvertently introduce biases or contamination from training data, we focus on directly generating \(n\)-dimensional worlds that can represent the solution spaces for a wide range of SOPs.
% \textbf{Framing the Optimization Problem:} 
That said, we do not predefine the optimization problem. Instead, the task naturally emerges as finding the maximum (or other extrema) in the generated \(n\)-dimensional world. This setup allows us to define flexible problems while preserving the integrity of the test environment.
% \textbf{Ensuring Data Integrity:} 

\textbf{Benefits and Advantages:} This approach ensures that neither the optimization problem nor its solution was exposed to the LLM during training. By doing so, we can mimic a real-world scenario where an optimization expert is asked to tackle a newly faced optimization problem using any techniques or strategies they prefer. Utilizing this approach brings some advantages. It offers generative flexibility, allowing the \(n\)-dimensional world to represent an infinite variety of optimization problems, from simple to highly complex ones. By abstracting the problem into a generated world, it ensures unbiased evaluation, reducing contamination from known problem-solution pairs and providing a more acceptable measure of the LLM's capability. Our world generator, WorldGen, enables the creation of increasingly complex worlds that test the limits of learning agents and provides a platform for benchmarking them under controlled yet dynamic conditions. 
% The world generator block only has one input, complexity index, which determines how complicated the target world should be. It is scalable, enabling the creation of increasingly complex worlds and provides a platform for benchmarking them under controlled yet dynamic conditions. 
Figure~\ref{fig:worlds} shows samples of generated 3-D worlds with different complexity levels.
% \todo{having two samples: low, high should be good enough}

\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{figs/worlds.pdf}
% \includegraphics{example-image-duck}
}
\caption{Samples of generated 3-D worlds for different SOP complexities: very simple (left), simple (middle), and medium (right). The top plots display the 3-D graphs of the worlds, while the bottom plots show their 2-D heatmap versions.}
\label{fig:worlds}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{LLM and Accessing the World}
% \blueitemize{
%     \item To enable LLM agent perform its task, we provide it with access to the generated world through an interactive cycle. 
%     \item In particular, in each iteration, LLM agent can interact with the world by asking it to reveal the values corresponding to certain chosen points. After the world reveals those values, LLM will attempt to come up with next set of chosen points and so on so forth.
%     \item we let the LLM agent to write python codes and use any interested library.
%     \item As part of the World's responsibility, it should execute the code and provides the results to the LLM agent.
% }
\textbf{An Interactive Cycle:}
To enable the LLM agent to perform its task effectively, we provide it with access to the generated world through an interactive cycle as shown in Figure~\ref{fig:interactive_cycle}. This ensures a dynamic sequential process where the agent iteratively learns and refines its approach based on the information it gathers. 
Simply put, in each iteration, the LLM agent is allowed to interact with the world by selecting a batch of interested points where each point is a vector, $v_i$ of size $n-1$. Then, the world responds by revealing the corresponding values of $f(v_i)$ to the LLM agent. This will end one iteration/round of the interaction. In the next round, the LLM utilizes this feedback to determine the next set of points to query. 
% This iterative process continues until the optimization objective is achieved or a predefined stopping criterion is met.

\textbf{Supporting Coding \& Providing Flexibility:}
Due to the complexity of SOPs, the LLM agent is permitted to provide a Python code as part of its response and therefore utilize any library it deems necessary for solving the problem. This freedom ensures that the agent can employ a diverse range of tools and techniques to explore and analyze the generated world. As part of its role, the World is responsible for executing the Python code generated by the LLM agent. Once the code is executed, the World provides the results back to the agent as part of its feedback, enabling it to adapt and refine its strategy in subsequent iterations. In case of errors that may arise during execution, the World is responsible to provide the details of the errors and return meaningful feedback. Moreover, the LLM agent is not constrained by a fixed set of queries or techniques. Instead, it has the freedom to decide how to approach the problem, including leveraging mathematical models, heuristics, or machine learning techniques. This flexibility mimics the conditions under which human experts operate when solving optimization problems.
The interaction between the LLM and the World creates a real-time feedback loop. The LLM continually refines its understanding of the world based on the revealed data, while the World executes the agent’s strategies and provides results.

\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=.8\columnwidth]{figs/cycle_v10.pdf}
% \includegraphics{example-image-duck}
}
\caption{Interaction loop between an LLM agent and the World}
\label{fig:interactive_cycle}
\end{center}
\vskip -0.4in
\end{figure}


\subsection{Notion of Efficiency}
\textbf{To Solve or to Efficiently Solve:}
Exhaustively searching through all possible solutions, or brute force, is always a solution to any optimization problem; however, it represents the most inefficient approach. Thus, merely solving an optimization problem is not the primary goal; solving it efficiently is what truly matters. To formalize this, we require the notion of an efficient solution. But how can we define efficiency in a meaningful and practical way here? 

\textbf{The Expert Solution:}
To address this question, we designed a baseline referred to as the Expert Solution. This baseline serves as a reference point for assessing the efficiency of the LLM agent's performance. The Expert Solution is crafted using different optimization techniques, including a combination of Monte Carlo search methods, Bayesian optimization, and Active Learning strategies. 
An important aspect of the Expert Solution is the introduction of a \soheili{query budget}. This budget represents the number of queries required by the Expert Solution to reliably solve the optimization problem. It provides an upper bound on the number of interactions with the environment that are necessary to achieve a solution. That said, alongside the optimization problem, the LLM agent is provided with the query budget and instructed to not only solve the optimization problem but also do so within the given query budget.

\textbf{Incentivizing Efficiency:}
This setup ensures that the agent is incentivized to prioritize efficiency. It must strategize its queries, balancing exploration and exploitation to maximize the information gained from each interaction. By enforcing a query budget, we can objectively evaluate LLM's efficiency and effectiveness in solving the problem. Ultimately, the notion of efficient solutions pushes the LLM agent beyond simple problem-solving, encouraging it to adopt creative and resource-conscious strategies that align with real-world optimization challenges.

\subsection{Evaluating LLMs Performance in SOPs}
\label{sec:part_2_evaluations}
% \blueitemize{
% \item Randomly generate worlds (with low complexity index)
% \item Let the expert solution try the optimization problem associated with that world 
% \item Ask the LLM agent to solve the optimization problem given the <query budget> acquired from the expert solution
% \item Repeat the problem for multiple times and measure the success rate of the agent 
% \item We utilize GPT-4-32k model as the baseline model (for now!)
% }

\textbf{The Setup:}
To evaluate the performance of the LLM agent, we follow a structured approach based on repeated experiments. We begin by generating worlds, characterized by a complexity index. In particular, to simplify experiments, visualizations, and keep overall token usage manageable, we focus on 3-D worlds and three levels of complexity: very simple (L0), simple (L1), and medium (L2). Next, we apply the Expert Solution to solve the corresponding SOPs associated with the generated world. As a result, we find a query budget required to achieve this reliably. Then, LLM agent is asked to solve the problem constrained by the query budget.
We repeat this process 100 times and measure the success rate of the LLM agent—defined as the proportion of trials where the agent successfully finds the optimal solution within the given query budget\footnote{We apply a relaxation here, treating any value found within 5\% of the optimum as the optimum point}. This success rate provides a quantitative measure of the agent's effectiveness and efficiency. In these experiments, we utilize the GPT-4-32k model as a capable baseline model. 

\textbf{The Default Scheme:}
Without involving any prompting techniques, the success rates become very low (close to 0\%). Therefore, we borrowed ideas from few-shot learning~\cite{brown2020fewshot}, Chain of Thought (CoT)~\cite{wei2022cot}, and added other techniques such as proper role assignment~\cite{karpathy2023stateofgpt} to improve the performance of the LLM agent. We name the resulting scheme \textit{LLM$^+$} and treat it as our default scheme from now on.
In particular, the prompt given to the LLM agent includes: [Role Assignment], followed by [Problem Definition \& Examples], [General Helpful Notes], and [Required Response Format]. The [Required Response Format] itself consists of [Plain Description of Current Strategy], [Python Code Implementation of the Strategy], and [Maximum Value Found So Far] fields (check Appendix~\ref{sec:app:prompts} for more details).

\textbf{Results:}

Table 1 summarizes the results of LLM$^+$ in various scenarios. These results optimistically suggest that LLMs understand SOP settings and are familiar with optimization techniques. As expected, the success rate of LLM$^+$ depends on the complexity of the underlying world. However, it is somewhat surprising that even in relatively simple scenarios (L1), LLM$^+$ does not achieve a high success rate. In straightforward settings where the world representing the SOP has only one global optimum with no other local optimum points, LLM$^+$ effectively utilizes general optimization techniques such as gradient ascent. But, when the world exhibits some complexity, with a few local optimum points in non-trivial parts of the space—common in real-world scenarios—LLMs struggle to strategize properly and find the global optimum (check Appendix~\ref{sec:app:llm+} for more details).

\begin{table}[t]
% \caption{Success rate of the Expert Solution and LLM agent}
\caption{The success rates of LLM$^+$ (with GPT-4-32k base model) in different worlds and complexity levels}
\label{table:primary_results}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c}
\toprule
World Complexity & Success Rate $\uparrow$\\
\hline
% \cline{2-4}
L0 (Very Simple) & 100\% \\
L1 (Simple) & 36\% \\
L2 (Medium) & 4\% \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.3in
\end{table}


\subsection{A Dialectical Perspective to Enhance LLMs in SOPs}
\label{sec:part_3_ace}


% \textbf{Enhancing Off-The-Shelf LLMs:}
Motivated by the unsatisfactory performance of vanilla LLMs in non-straightforward SOPs, we aim to address a natural follow-up question: Can we enhance the performance of LLMs without relying on retraining, fine-tuning, or post-training modifications? 

\textbf{The Core Idea:}
To that end, we propose a framework inspired by the well-established Hegelian Dialectics, offering a formal structure for improving LLM performance through dynamic dialectical reasoning processes.
Using the terminology of Hegelian Dialectics, we can conceptualize a general LLM agent as a Thesis Generator entity—a block that observes a given problem and generates a corresponding solution or response to it\footnote{Note that this remains valid even within Minsky's multi-agent system, where the main problem is divided into smaller pieces, each delegated to different agents communicating with one another.}. However, inspired by Hegel's framework, we suggest that confining to a single Thesis Generator block is not an appropriate model for solving problems or generating ideas. Instead, a more robust model should include additional components: an Antithesis Generator and a Synthesis Block. Together, these three components, as Hegel formally describes~\cite{hegel_phenomenology_spirit,hegel_science_logic}, form a dynamic reasoning cycle, enabling the system to iteratively refine and improve its outputs.

\textbf{ACE:}
The Antithesis Generator plays a critical role by challenging the solutions produced by the Thesis Generator. It identifies potential flaws, contradictions, or alternative perspectives that may have been overlooked. This counterbalance forces the system to evaluate its assumptions critically and consider a broader range of possibilities. The Synthesis Block then reconciles the Thesis and Antithesis, combining their insights to produce a more refined and coherent solution. This iterative interplay between Thesis, Antithesis, and Synthesis ensures that the system continuously evolves its understanding and response, ultimately arriving at a more suitable outcome.
This dialectical structure led us to introduce our solution, \textit{ACE}\footnote{ACE stands for \textbf{A}ct, \textbf{C}ritique, and \textbf{E}volve}, embodying three components: (1) Actor, (2) Critic, and (3) Synthesizer. The relationship between these components is shown in Fig.~\ref{fig:ace}.
The iterative cycle of solving a problem starts with the Actor creating an initial thesis. This thesis is then implemented and executed in the world, and the corresponding outcomes and results (called observations) are gathered. Next, the Critic examines the initial thesis and the corresponding observations to generate an antithesis. The thesis, antithesis, and corresponding observations are then fed into the Synthesizer. The Synthesizer creates an evolved thesis, completing an iteration/round. The cycle continues by treating the evolved thesis as the next initial thesis in the cycle.
As we later show in section~\ref{sec:eval}, ACE significantly improves the performance of LLMs with no modification to their architecture and no extra post-training or fine-tuning. By embedding a dialectical reasoning process into the system, ACE creates a more adaptive process that is better equipped to handle complex and nuanced sequential optimization tasks (Appendix~\ref{sec:app:dialectical_sample} provides samples and details of ACE's dialectical process).

\begin{figure}[!t]
% \vskip 0.2in
\begin{center}
\centerline{
\includegraphics[width=\columnwidth]{figs/spiral_of_thoughts_v4.pdf}
% \includegraphics{example-image-duck}
}
\caption{ACE and the spiral of thoughts}
\label{fig:ace}
\end{center}
\vskip -0.4in
\end{figure}





