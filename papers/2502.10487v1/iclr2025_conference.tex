
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[capitalise]{cleveref}
\usepackage{todonotes}
\usepackage{amssymb} % Load for \blacklozenge
\usepackage{mathabx}
\usepackage{tikz}
\usepackage{placeins} 
\usepackage{enumitem}

\DeclareRobustCommand{\instructIcon}{%
  \scalebox{0.3}{\tikz\draw[fill=black] (90:0.3) -- (234:0.3) -- (18:0.3) -- (162:0.3) -- (306:0.3) -- cycle;}%
}
\DeclareRobustCommand{\safetyIcon}{%
  \scalebox{0.25}{\tikz\draw[fill=black] (0:0.3) -- (60:0.3) -- (120:0.3) -- (180:0.3) -- (240:0.3) -- (300:0.3) -- cycle;}%
}
\DeclareRobustCommand{\advIconUp}{%
  \scalebox{0.7}{$\blacktriangle$}%
}
\DeclareRobustCommand{\advIconDown}{%
  \scalebox{1.1}{$\blacktriangledown$}%
}
\DeclareRobustCommand{\circuitIcon}{%
  \scalebox{0.7}{$\blacklozenge$}%
}
\DeclareRobustCommand{\capabilityCircle}{%
  \tikz\draw[black,fill=black] (0,0) circle (.4ex);%
}
\DeclareRobustCommand{\capabilityIcon}{%
  \scalebox{0.25}{\tikz\draw[fill=black] (90:0.3) -- (162:0.3) -- (234:0.3) -- (306:0.3) -- (18:0.3) -- cycle;}%
}


 
\title{Fast Proxies for LLM Robustness Evaluation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Tim Beyer, Jan Schuchardt, Leo Schwinn, Stephan G\"unnemann \\
Technical University of Munich \& Munich Data Science Institute \\
\texttt{\{tim.beyer,j.schuchardt,l.schwinn,s.guennemann\}@tum.de} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\vspace{-8pt}

\begin{abstract}
Evaluating the robustness of LLMs to adversarial attacks is crucial for safe deployment, yet current red-teaming methods are often prohibitively expensive. 
We compare the ability of fast proxy metrics to predict the real-world robustness of an LLM against a simulated attacker ensemble.
This allows us to estimate a model's robustness to computationally expensive attacks without requiring runs of the attacks themselves.
Specifically, we consider gradient-descent-based embedding-space attacks, prefilling attacks, and direct prompting.
Even though direct prompting in particular does not achieve high ASR, we find that it and embedding-space attacks can predict attack success rates well, achieving $r_p=0.87$ (linear) and $r_s=0.94$ (Spearman rank) correlations with the full attack ensemble while reducing computational cost by three orders of magnitude.
\end{abstract}

\vspace{-8pt}

\section{Introduction}
\label{submission}
As the capabilities of large language models advance, ensuring their robustness and reliability becomes increasingly critical. 
To this end, frontier models undergo extensive adversarial testing and red-teaming to identify vulnerabilities before deployment \citep{openai_red_teaming_network,dubey2024llama}.

However, state-of-the-art red-teaming methods are computationally expensive, as finding adversarial prompts is a challenging combinatorial optimization problem over discrete natural language. 
Here, model-agnostic approaches require prohibitive computational resources~\citep{zou2023universal, chao2023jailbreaking}, whereas more efficient attack algorithms tend to be model-specific and struggle to transfer across architectures~\citep{liao2024amplegcg}. 
Moreover, reliable red-teaming with strong attacks still demands significant manual effort in tailoring the attack algorithm to a specific model~\citep{andriushchenko2024jailbreaking,li2024llm}. 
As a result, large-scale red teaming approaches require thousands of GPU hours~\citep{samvelyan2024rainbow}, making thorough safety evaluations prohibitively expensive in most research settings.

To address this problem, we propose a scalable alternative: low-cost proxies for real-world threat models. 
These proxies enable LLM robustness evaluation without needing to run highly expensive automated attack suites against the model.
As an example of such an attack suite, we use a "synthetic red-teamer" ensemble comprising six distinct LLM attack methods,  which we evaluate on 33 open-source models across 300 harmful prompts. 
We leverage substantial computational resources and aggregate more than 7M jailbreak attempts. 
The data suggest that model robustness in adversarial settings can be predicted through inexpensive approaches. 


Our main contributions are as follows:

\begin{itemize}[nosep,noitemsep]
    \item We investigate whether inexpensive proxies --- including direct prompting, prefilling, and embedding space attacks --- can predict robustness against strong adversarial red teaming.
    \item We demonstrate that robustness can be predicted within model families (e.g., different Llama 3 versions) and across model families (e.g., Llama and Mistral).
    \item Finally, we show that by estimating the most robust model checkpoint during training, proxy attacks can aid adversarial model alignment across different training regimes(e.g., circuit breaking or adversarial training).
\end{itemize}

    % \label{fig:combined_correlations}
    % \caption{Relation between synthetic red-teamer attack success rate (worst case ASR) and attack success rate of proxy methods (direct, prefilling, and embedding space ASR), for different model families, different stages of robust training, and XYZ.
    % Strength of the relation is additionally quantified by Spearman ($r_s$), Pearson ($r_p$), and Kendall rank ($\tau$) correlation  coefficients.
    %Direct attacks have the largest $r_s$, i.e., order of two models w.r.t. direct ASR is most predictive of their order w.r.t. worst case ASR.}
% \newpage
\vspace{-2.5pt}
\section{Synthetic Red-Teamer}\label{section:synethetic_red_teamer}
To emulate a strong attacker, we create a \emph{synthetic red-teamer} by ensembling six common attack algorithms (listed in \cref{tab:ensemble}). All attacks are run using the recommended hyperparameters  (see also
\newpage
\begin{wraptable}{h}{0.455\textwidth}
    \centering
    \setlength{\tabcolsep}{0pt}  % Adjust column padding
    % \vskip -0.3cm
    % \caption{Attacks in synthetic red-teamer ensemble \& their jailbreak candidate count per prompt. }
    \caption{Attacks in synthetic red-teamer ensemble \& how many jailbreak candidates they generate per prompt. }
    \begin{tabular}{lr}
        \toprule
        \textbf{Attack Name} & \textbf{Candidates} \\
        \midrule
        AmpleGCG \citep{liao2024amplegcg} & 200 \\
        AutoDAN \citep{liu2023autodan} & 100 \\
        BEAST \citep{sadasivan2024fast} & 40 \\
        GCG \citep{zou2023universal} & 250 \\
        HumanJB \citep{mazeika2024harmbench} & 112 \\
        PAIR \citep{chao2023jailbreaking} & 25  \\
        \midrule
        Total & 727 \\
        \bottomrule
    \end{tabular}
    \vskip -0.35cm
    \label{tab:ensemble}
\end{wraptable}
 \cref{sec:hyperparameters}) and simulate a strong red-teamer with significant computational resources ($\approx$ 30 H100-minutes per prompt).
We evaluate each algorithm in the \textit{many-trial} setting, where all candidate prompts (including intermediate steps) are tried on the victim model. 
Thus, for each harmful prompt in the dataset, a model is attacked by 727 different input prompts.
If \emph{any} of the prompts succeed, we count the attack as successful.
While some algorithms (e.g., AmpleGCG and PAIR) perform many-trial attacks by default, others, such as GCG and BEAST, generally only use the final attack prompt to generate a harmful response.   
The many-trial setting makes attacks strictly more powerful, at the cost of increased compute.

\section{Proxy Methods}

We aim to find an inexpensive and fast approach that can reliably predict a model's real-world robustness.
Finding such a proxy for robustness could dramatically reduce the cost of robustness evaluations, make it easier to compare models across and within families, and efficiently select promising checkpoints during defense training.
To this end, we consider three candidate approaches:

\textbf{Embedding Space Attacks.} 
\citet{schwinn2023adversarial,schwinn2024soft} recently proposed a white box attack that operates in continuous token embedding space, rather than the discrete input vocabulary. 
This framework---while impractical for real-world attacks, where most threat models assume a black box setting with string-level input---provides an extremely fast way to attack models in a white box setting, and can be used e.g., to adversarially train LLMs \citep{xhonneux2024efficient}.

\textbf{Prefilling.} 
Prefilling attacks \citep{vega2023bypassing,andriushchenko2024jailbreaking} rely on injecting a prefix to the beginning of the victim model's response to the harmful prompt - typically using an affirmative response prefix. 
As this level of access is also provided by some private models (e.g., the Claude family \citep{TheC3}), it represents a realistic attack vector even for hosted models.

\textbf{Direct.} \emph{Direct} prompting is the simplest possible baseline: 
We simply use an unmodified harmful prompt from the dataset and sample a single greedy generation, which is then judged.


\section{Experimental Evaluation}

We conduct experiments to determine how well the attack success rates of inexpensive proxy methods (direct ASR, prefilling ASR, embedding-space ASR) predict robustness against real-world red-teaming approaches, which we simulate using our strong synthetic red-teamer from~\cref{section:synethetic_red_teamer} across various training and attack scenarios.
In addition to directly comparing the different ASRs, we compute Pearson correlation ($r_p$) to quantify linear correlation between proxy ASR and ensemble ASR. 
We further compute Spearman ($r_s)$ and Kendall rank ($\tau$) correlation to understand whether the order of any two models w.r.t.\ proxy ASR is predictive of their order w.r.t.\ ensemble ASR.
For the full details of our experimental setup, see \cref{sec:hyperparameters}. For additional results see \cref{sec:additional-results}.


%\newpage % needed to put figure below heading
\subsection{Comparing Within-Family Models}
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{figures/llama3-family-correlations.pdf}
    \vskip -0.35cm
    \caption{Attack success rates for different variants of Llama-3-8B. We include instruct versions (\instructIcon) as a baseline and compare to safety-tuned (\safetyIcon), adversarially trained (\advIconUp,\advIconDown), circuit breaker (\circuitIcon), and capability-optimized (\capabilityCircle,\capabilityIcon) models.}
    \label{fig:combined_correlations_individual_family_llama3}
    \vskip -0.5cm
\end{figure}
Popular base models are often fine-tuned for particular use cases, such as chatting \citep{tunstall2023zephyr}, helpfulness \citep{starling2023}, or tool use \citep{teknium2024hermes}.
We are interested in comparing the safety of several post-trained model versions.
In Figure \ref{fig:combined_correlations_individual_family_llama3}, we evaluate different derivatives of Llama 3 8B Instruct.
Spearman and Kendall rank correlation coefficients $r_s$ and $\tau$ of direct prompting are greater or equal than those of the other proxy attacks.
We observe that direct ASR is close to $0$ for multiple models, which impedes a good linear fit ($r_p$ of $0.62$) between direct ASR and ensemble ASR. This $r_p$ is smaller than those of prefilling and embedding space attacks. 
Thus, even for within-family comparisons, the simplest and fastest attack appears like a suitable choice as a proxy for computationally expensive red-teaming.

\subsection{Comparing Across Model Families}\label{section:comparing_families}

We also investigate whether proxy methods can be used to predict the success rate of expensive red-teaming attacks on newly introduced model families.
%\cref{fig:combined_correlations_family} shows pairs of proxy ASR and worst case ASR for each of the three proxy attacks when using 300 harmful prompts. 
In~\cref{fig:combined_correlations_family}, 
each point corresponds to a specific model from one of six model families (Gemma 2 \citep{team2024gemma}, Mistral \citep{jiang2023mistral}, Qwen \citep{bai2023qwen}, Phi-3 \citep{abdin2024phi}, Llama 3 \citep{dubey2024llama}, Llama 2 \citep{touvron2023llama}).
%As may be expected,
Prefilling and embedding space attacks often have much higher ASR than direct prompting, which na{\"i}vely use the harmful prompt without any modification.
Direct ASR is generally below $5\%$, except for models that are extremely unrobust (ensemble ASR close to $100\%$).
Thus, the pairs of direct and ensemble ASR do not admit a linear fit and the Pearson correlation $r_p$ is small.
However, the rank correlation coefficients of direct prompting ($r_s=0.94$, $\tau=0.83$) are higher than those of the other two proxy methods ($r_s=0.79$, $\tau=0.61$) and ($r_s=0.90$, $\tau=0.73$). 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/cross-family-correlations.pdf}
    \vskip -0.35cm
    \caption{
        Attack success rates for models from different families.
        Direct ASR has the largest $r_s$ and $\tau$, i.e., 
        the order of two models w.r.t. direct ASR is most predictive of order w.r.t. ensemble ASR.
    }
    \label{fig:combined_correlations_family}
\end{figure}


\subsection{Assessing Effectiveness of Robustness Fine-Tuning}

A standard method for increasing model robustness is via post-training/fine-tuning approaches, e.g., via circuit breaker training~\citep{zou2023universal} or continuous adversarial training~\citep{sheshadri2024targeted, xhonneux2024efficient}.
%A crucial hyper-parameter is the number of fine-tuning steps.
In~\cref{fig:combined_correlations_training_cb} \& \ref{fig:combined_correlations_training_capo}, we assess whether proxy ASR can potentially be used to predict ensemble ASR after fine-tuning for a specific number of steps, rather than performing computationally expensive red-teaming for every possible value of this hyper-parameter.
Specifically, we apply circuit breaker training to Llama-3-8B-Instruct and vary the number of training steps between $1$ and $300$.
Again, while the relation between proxy ASR and ensemble ASR is generally monotonic and linear for all three proxies, direct prompting achieves significantly higher ranking correlations $r_s$ and $\tau$.
% Put differently, all three proxy methods are similarly suitable for choosing an appropriate number of training steps for robustness fine-tuning.

\begin{figure}[h]
        \includegraphics[width=\textwidth]{figures/llama3_cb-family-correlations.pdf}
        \vskip -0.35cm
        \caption{Attack success rates for different number of robustness fine-tuning steps using Circuit Breakers \citep{zou2024improving}. We include the base instruct model and the officially released circuit breaker model.
        Despite varying success rate, all proxy methods have similar correlation coefficients, i.e., are similarly predictive of fine-tuning effectiveness. Arrows indicate training progression.
        }
        \label{fig:combined_correlations_training_cb}
        % \vskip -0.5cm
\end{figure}


\subsection{Scaling Trends}
We find that the effectiveness of different proxy methods varies with the amount of prompts used (\cref{fig:scaling_trends}).
Prefilling and embedding space attacks attain universally higher Pearson correlation, i.e., admit a better linear fit irrespective of the number of prompts. 
They can also reach higher Spearman and Kendall ranking correlation --- but only when using few prompts.
For $50$ or more prompts, direct prompting yields higher ranking correlation coefficients.
This can be explained as follows:
Since direct ASR is generally small for robust models, there is a high chance that our sample estimate will incorrectly indicate a direct ASR of exactly $0$ when using few prompts, making the observed relation to ensemble ASR very erratic.
Using more prompts provides a better estimate of the small but non-zero population success rate of direct prompting, thus eliminating this issue and making direct ASR a good predictor of whether one model will be more robust than another to our synthetic red-teamer. 
As increasingly robust models will decrease ASR, we expect to see an increase in the number of prompts required to effectively use direct prompting as a proxy.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/prompt_scaling.pdf}
    \vskip -0.3cm
    \caption{Correlation coefficients between proxy attack success rate and ensemble attack success rate under varying number of prompts. When using fewer than 50 prompts, PGD yields higher Spearman and Kendall ranking correlations, however the direct prompting scales better with more prompts. Prefilling and PGD achieve higher linear/Pearson correlations at any prompt count.}
    \label{fig:scaling_trends}
    % \vskip -0.5cm
\end{figure}




% \subsection{Loss vs. ASR}
% We also consider the loss on the target suffix as target suffix for ASR.
% However, we find that it is not comparable across models. Many models have idiosyncratic preferences regarding the answer format, yielding significantly different baseline losses on the standard affirmative response targets.


\subsection{Limitations}
While we conducted an exhaustive and computationally intensive evaluation using six attacks and 33 models from the sub-10B parameter class, our experiments should be further validated to ensure they generalize to other attack algorithms and model sizes.

\section{Conclusion}
We investigated the effectiveness of inexpensive proxy attacks in predicting LLM robustness against adversarial red-teaming.
Our results highlight key trade-offs between different proxy methods.
Direct prompting is a strong baseline for ranking models by robustness across diverse scenarios (within-family, cross-family, safety fine-tuning), provided that enough ($>50$) prompts are used.
Embedding-space attacks provide better ranking at low prompt count and better linear fits, while prefilling attacks are generally inferior to the two alternatives.
Overall, our results showcase that efficient proxy attacks are a promising direction for future research
towards making foundation models more responsible without incurring unjustifiable computational overhead.


% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\newpage
\section{Model Zoo}
\label{sec:model-zoo}

\begin{table}[h!]
    \centering
    \begin{tabular}{ll}
    \toprule
        \textbf{HuggingFace Model ID} & \textbf{Model Family} \\
        \midrule
        google/gemma-2-2b-it & Gemma 2 \\
        mistralai/Mistral-7B-Instruct-v0.3 & Mistral 7B \\
        berkeley-nest/Starling-LM-7B-alpha & Mistral 7B \\
        cais/zephyr\_7b\_r2d2 & Mistral 7B \\
        HuggingFaceH4/zephyr-7b-beta & Mistral 7B \\
        ContinuousAT/Zephyr-CAT & Mistral 7B \\
        GraySwanAI/Mistral-7B-Instruct-RR & Mistral 7B \\
        mistralai/Mistral-Nemo-Instruct-2407 & Mistral Nemo \\
        mistralai/Ministral-8B-Instruct-2410 & Ministral \\
        meta-llama/Llama-2-7b-chat-hf & Llama 2 \\
        ContinuousAT/Llama-2-7B-CAT & Llama 2 \\
        lmsys/vicuna-7b-v1.5 & Llama 2 \\
        NousResearch/Hermes-2-Pro-Llama-3-8B & Llama 3 \\
        meta-llama/Meta-Llama-3-8B-Instruct & Llama 3 \\
        LLM-LAT/robust-llama3-8b-instruct & Llama 3 \\
        GraySwanAI/Llama-3-8B-Instruct-RR & Llama 3 \\
        meta-llama/Meta-Llama-3.1-8B-Instruct & Llama 3.1 \\
        allenai/Llama-3.1-Tulu-3-8B-DPO & Llama 3.1 \\
        meta-llama/Llama-3.2-1B-Instruct & Llama 3.2 \\
        meta-llama/Llama-3.2-3B-Instruct & Llama 3.2 \\
        qwen/Qwen2-7B-Instruct & Qwen2 7B \\
        ContinuousAT/Phi-CAT & Phi 3 \\
        microsoft/Phi-3-mini-4k-instruct & Phi 3 mini \\
        microsoft/phi-4 & Phi 4 \\
        \bottomrule
    \end{tabular}
    \caption{List of models with their short names and base models.}
    \label{tab:models}
\end{table}

In addition, we fine-tune Llama-3-8B-Instruct using the circuit breaker methodology \citep{zou2024improving} using $N=\{1,10,20,50,100,200,300,500,1000\}$ steps, and with the CAPO version of continuous adversarial training \citep{xhonneux2024efficient} and $N=\{75,150,225\}$ steps.
We use \texttt{bfloat16} quantization for all models.

\newpage
\section{Hyperparameters \& Experimental Details}
\label{sec:hyperparameters}
We run all attacks on all 300 harmful prompts from AdvBench \citep{zou2023universal}, as included in HarmBench. 
A jailbreak attempt is counted as successful if both HarmBench's finetuned Llama-2-13B classifier \citep{mazeika2024harmbench} and LlamaGuard 3 8B \citep{dubey2024llama} flag the model's response as harmful.


The hyperparameters for the attacks used in the ensemble and the proxy attacks are shown below. 
Where possible, attack implementations were sourced from the original authorsâ€™ GitHub repositories; otherwise, we integrated a HarmBench implementation into our pipeline. 
In some cases, we consulted authors directly to obtain reference implementations and verify correctness. For all attacks, we evaluate a single greedy generation per prompt-candidate.

\begin{itemize}
    \item AmpleGCG \cite{liao2024amplegcg}: We use \texttt{osunlp/AmpleGCG-llama2-sourced-} \texttt{llama2-7b-chat} to generate 200 attack suffixes with diversity penalty 1 and generate completions for all 200 of the attack candidates. 
    \item AutoDAN \cite{liu2023autodan}: We use 100 steps and initialize using the 128 seed prompts from HarmBench's implementation. We use the attacked model itself as mutator model and set $N_\text{elites}=0.05$, $\text{crossover}=0.5$, $N_\text{points}=5$, and $P_\text{mutation}=0.01$.
    \item BEAST \cite{sadasivan2024fast}: We use $k1=k2=15$ and set the temperature to 1 to sample $N=40$  suffix tokens.
    \item GCG \cite{zou2023universal}: We use a modified version of nanoGCG with a corrected token filtering algorithm to remove special tokens \& only allow ASCII-representable characters. We set $N_\text{steps}=250$, use a batch size of 512, $\text{Top-K}=256$, and initialize using the string ``x x x x x x x x x x x x x x x x x x x x" as it tokenizes to exactly 20 tokens for all tested models.
    \item HumanJailbreaks: We use the \href{https://github.com/centerforaisafety/HarmBench/blob/main/baselines/human_jailbreaks/jailbreaks.py}{114 human-designed jailbreak templates} in HarmBench \cite{mazeika2024harmbench} to prompt the model.
    \item PAIR \cite{chao2023jailbreaking}: We use \texttt{lmsys/vicuna-13b-v1.5} as attacker model and generate up to 512 tokens per attack prompt. Sampling attacks is done with with temperature 1 and top-p of 0.9, setting $N_\text{streams}$ to 5 and $N_\text{iterations}$ to 5. During the attack, the victim model generates up to 256 tokens using greedy generation. If the conversations grow longer than the model's context, we truncate the first non-system messages from the conversation until the conversation fits into the context window.
    \end{itemize}
        
The proxy attacks use the following settings: 
\begin{itemize}
    \item Direct: We simply use the harmful prompt without any modification and sample a greedy generation.
    \item Embedding-space  \cite{schwinn2024soft}: We initialize the attack using the suffix ``x x x x x x x x x x x x x x x x x x x x" as it tokenizes to exactly 20 tokens for all tested models, and run signed gradient descent optimization for 100 steps. We use a learning rate of $\alpha=0.01$ and constrain the optimization to an $L_2$ ball with radius 1 around the initialization for each token. To normalize across model families, we normalize both step size and $L_2$ constaint by the average $L_2$ embedding norm across the input vocabulary.
    \item Prefilling: We use the unmodified harmful prompt and pre-fill the beginning of the model's response using the affirmative target sequence from the dataset.
\end{itemize}

Running the attack ensemble on an Nvidia H100 GPU for a single prompt requires 1,731 seconds on average, while direct prompting and prefilling can be easily batched and is completed in a single second.
Batched embedding space attacks require approximately 5 seconds per prompt.

\newpage
\section{Additional Experimental Results}
\label{sec:additional-results}
\subsection{Mistral Variants}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/mistral-family-correlations.pdf}
    \caption{Attack success rates for different variants of Mistral 7B Instruct. We include instruct versions (\instructIcon) as a baseline and compare to safety-tuned (\safetyIcon), adversarially trained (\advIconUp), circuit breaker (\circuitIcon), and capability-optimized (\capabilityIcon) models.}
    \label{fig:combined_correlations_individual_family_mistral}
\end{figure}

\subsection{Llama 3 Variants}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/llama3-family-correlations.pdf}
    \caption{Attack success rates for different variants of Mistral 7B Instruct. We include instruct versions (\instructIcon) as a baseline and compare to safety-tuned (\safetyIcon), adversarially trained (\advIconUp), circuit breaker (\circuitIcon), and capability-optimized (\capabilityIcon) models.}
\end{figure}

These model families were selected due to their popularity and resulting large number of versions.



\subsection{Continuous Adversarial Training}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/llama3_capo-family-correlations.pdf}
    \caption{Attack success rates for different number of robustness fine-tuning steps using Continuous Adversarial Training \citep{xhonneux2024efficient} on Llama 3 8B Instruct. All methods are highly correlated with the synthetic red-teamer. Due to resource and time constraints we only compare four training checkpoints. Arrows indicate training progress.}
    \label{fig:combined_correlations_training_capo}
\end{figure}

\end{document}
