\begin{algorithm}[t]
\setcounter{AlgoLine}{0} %
\small
    \SetAlgoLined
    \KwIn{Initial policy parameters for actor network $\zeta_a$, critic parameters $\zeta_c$, target network parameters $\zeta_a', \zeta_c'$\\
Training parameters: $\mathit{actionNoise}$, $\policyGuidanceRate$, $\mathit{bufferSize}$, $\mathit{batchSize}$, maximum iterations: $M$,  training steps: ${\it trainStep}$; target network update steps: ${\it updateStep}$
}
    \KwOut{Trained policy $\pi_{\zeta_a}$}
    Initialize replay buffer $\Buffer$;  ${\it step}\gets 0$\\
    % Initialize target network weights $\zeta_a' \leftarrow \zeta_a$, $\zeta_c' \leftarrow \zeta_c$ \\
    \For{$1$ \KwTo $M$}{
        Input a sample into simulator to generate initial state $s_0$ 
        % Initialize a random process $N$ for action exploration 
        
        \For{each time slot $T_j \in \mathcal{T}$}{
        % \If{$T_j$ is during non-peak hours}{
        % Get action A(T_j) using h
        %}
        \tcp{Introducing policy guidance stochastically.}
            {\color{black} randomValue $\leftarrow randomBetween(0,1) $
            
            \If
            %(\tcp*[h]{Adding policy guidance stochastically}) $$
            { randomValue $\leq \policyGuidanceRate$}{
        Get action $A(T_j)$ by rerunning the MILP solver: $A(T_j)\leftarrow\MILP(S(T_j), {\it remainEpisode})$
                }
            \Else{
                % $A(T_j)\gets  \pi(S(T_j) | \zeta_a) + \mathit{actionNoise}$
                Get masked action using current policy,  $\mathit{actionNoise}$: \\
                $A(T_j) \gets \Mask\left(S(T_j), \pi(S(T_j) | \zeta_a)) + \mathit{actionNoise} \right)$
                % \tcp{Add action masking}
            }}
             State transition $S(T_{j+1})\leftarrow {\it Trans}(S(T_j), A(T_j))$ 
            %in Algorithm~\ref{alg: stateTrans}. 

            Get the action reward $R(T_j)\leftarrow {\it Reward}(S(T_j), A(T_j))$
            %by Algorithm~\ref{alg: reward}. 
            
            Store transition $(S(T_j), A(T_j), R(T_j), S(T_{j+1}))$ in $\Buffer$
            
            \If{${\it step} \bmod {\it trainStep} == 0$}{
            Sample batch $(S(T_i), A(T_i), R(T_i), S(T_{i+1})$ from $\Buffer$ 

           {\color{black} Get masked actions using target actor network:\\
            $A(T_{i+1})\leftarrow \Mask(S(T_{i+1}), \pi'(S(T_{i+1}) | \zeta_a'))$}  
            %using Algorithm~\ref{alg: mask}  \tcp{Add action masking}

            Set target $y_i\leftarrow R(T_j) + \gamma Q'(S(T_{i+1}), A(T_{i+1})
            | \zeta_c')$ 
            
            Update critic network by minimizing the loss: $L 
            \leftarrow \frac{1}{N} \sum_i (y_i - Q(S(T_i), A(T_i) | \zeta_c))^2$ 
            
             {\color{black} Get masked actions $A(T_i)$ at $S(T_i)$ using actor network: $A(T_i)\leftarrow \Mask( S(T_i), \pi(S(T_i) | \zeta_a))$ }%using Algorithm~\ref{alg: mask}.  
             % \tcp{Add action masking}
           % Mask action $A(T_i)\leftarrow \Mask(S(T_i), a_{i})$ 
           
            Update the actor policy using policy gradient:\\
            $\nabla_{\zeta_a} J \leftarrow \frac{1}{N} \sum_i \nabla_a Q(S(T_i), A(T_i) | \zeta_c) | \nabla_{\zeta_a} \pi(s | \zeta_a) |_{S(T_i)}$}
            
        \If{${\it step} \bmod {\it updateStep} == 0$}{
             Update the target networks: 
            $\zeta_a' \leftarrow \tau \zeta_a + (1 - \tau) \zeta_a'$;\quad 
            $\zeta_c' \leftarrow \tau \zeta_c + (1 - \tau) \zeta_c'$ \\
            }${\it step}\gets {\it step}+1$
        }
    }
    \caption{Improved DDPG with Action Masking and Policy Guidance.}
\label{alg:DDPG} 
\end{algorithm}