\subsection{Markov Decision Process Model}
\label{ssec:MDP}

%We model the V2B problem as a Markov Decision Process (MDP). This framework enables optimal decision-making regarding charger power rates and load management, allowing for effective responses to uncertainties in user behaviors and building loads in our working scenario. 
 % \color{black} We define the MDP as a tuple $(\mathcal{S}, \mathcal{A}, {\it Trans}(S(T_j),A(T_j))$, ${\it Reward}(S(T_j)$, $A(T_j))$, where $\mathcal{S} = \{S(T_j) \mid T_j \in \mathcal{T}\}$ is the set of states representing the system's state at the beginning of each time slot $T_j$, $\mathcal{A} = \{A(T_j) \mid T_j \in \mathcal{T}\}$ is the set of actions, with each action $A(T_j) \in \mathcal{A}$ representing a decision made at time slot $T_j$ and including continuous values indicating the power rate of all chargers. The state transition function $\text{Trans}(S(T_j), A(T_j))$ describes the transition from state $S(T_j)$ to the next state based on action $A(T_j)$, while the reward function $\text{Reward}(S(T_j), A(T_j))$ assigns a reward for taking action $A(T_j)$ in state $S(T_j)$. The key notations in the MDP are provided as follows: 

%We briefly describe the state, actions and transitions for the problem. 
% The key notations in the MDP for the V2B problem are as follows:
% \rishav{add small \\description of MDP, \\as a tuple\\ (S,A,P,T,$\gamma$, \\describing each \\in few words}
%Our objective is to determine the optimal power rate sequence $\mathcal{P}$ across all time slots $T_j \in \mathcal{T}$. 
%To address the uncertainly in our working scenario, we model the V2B problem as a Markov Decision Process (MDP), which effectively tracks changing conditions like fluctuating SoC, building load, and unpredictable EV arrivals. MDPs provide a framework for making optimal decisions regarding charger power rates and load management, ensuring effective responses to variations in user behavior and demand.    We model the V2B charging control process as a MDP, aiming to find the optimal power rate sequence \(\mathcal{P}\) for all time slots \(T_j \in \mathcal{T}\). Our goal is to derive the optimal policy for determining $P(C_i, T_j)$ for each time slot, minimizing the total cost while ensuring that the EVs' SoC requirements are met throughout the billing period.     




{\bf State.}
%At each time slot $T_j\in\mathcal{T}$, we define state $S(T_j) \in \mathcal{S}$ by using a combination of features that we identified through a mix of domain expertise and experiments.
The complete state space for the problem can be described using features that provide historical, current, and future estimation at a given time $T_j$.
%These features provide historical as well as current context and include estimated value of peak power that is used to calculate demand charge.  The complete state features of our V2B problem at each decision-making time 
This includes key input parameters for each vehicle such as the current SoC, required SoC, departure time, and battery capacity for each EV, along with SoC boundaries across 15 chargers. Additionally, the current building load, time slot, day of the week, past historical building load, and long-term peak power estimation value are included, resulting in approximately 100 features. While this state space is complete, it is not tractable to be used for the learning process. Therefore, we had to reduce the state space. For this,
%For improving practical performance, 
we leveraged domain-specific knowledge to abstract key information, reducing the state space to 37 essential state elements without compromising crucial data. We describe them below.
%abstracting the following features providing historical and current building load and arrival EVs, as well as features for future estimation:
    % \item The current building load is $\Building(T_j)$. 
    % \item The power gap between the current builidng load to the estimated peak power for the billing period is $\PrdPeak(T_j) - \Building(T_j)$. By incorporating this long-term peak power estimation, we provide the RL model with a feature that aids in estimating the optimal peak power for demand charge reduction.
    % \item The historical building load information which constrains the mean peak building load$\mu(B^H(T_j))$ and variance $\sigma^2(B^H(T_j))$  over the previous 7 days. This data can also inform estimations of future building load.  
    % \item The current time slot $T_j$ and day of the week are included to help the RL model distinguish daily patterns and manage feature diversity, enhancing generalization across different weekdays. 
    % \item Number of EV arrivals upto time slot $T_j$ for the current day, represented as $|\{V| V\in \mathcal{V}, A(V)\leq T_j \}|$, tracking the current EVs and their status. 
    % \item We include the status of all chargers $\CS(T_j) = \{(\PowerNeed(C_i, T_j)$, $\ReTime(C_i, T_j))\}_{C_i \in \mathcal{C}}$, where each charger status is defined by two components: (i) $\PowerNeed(C_i, T_j)$ representing the energy gap between the required SoC and the current SoC of the EV, where, 
    % \begin{equation*} 
    %  \PowerNeed(C_i, T_j) = ( \SOCR(V) - \SOC(V, T_j) ) \times {\it CAP}(V)
    %  \label{eq:chargerstate}
    % \end{equation*} 
    % with $V=\phi(C_i, T_j)$ indicating the EV connected to $C_i$ at time slot $T_j$
    % and, (ii) $\ReTime(C_i, T_j) = D(\phi(C_i, T_j)) - T_j$, specifying the time remaining till the departure of the EV.
   
    \begin{enumerate}[leftmargin=*]
    \item The current time slot, $T_j$.
    \item The current building load, denoted as ${B}(T_j)$.
    \item The power gap between the current building load and the estimated peak power for the billing period, $ \PrdPeak(T_j) - {B}(T_j)$. It aids the RL model in estimating optimal peak power for demand charge reduction.
    \item The mean peak building load over the previous 7 days, $\mu(B^H(T_j))$.
    \item The variance of the peak building load over the previous 7 days, $\sigma^2(B^H(T_j))$. It informs the model about the future building load.  
    \item The day of the week for the current time slot, $T_j$. It helps the RL model distinguish daily patterns and enhance generalization.
    \item The number of EV arrivals up to time slot $T_j$, represented as $|\{V | V \in \mathcal{V}, A(V) \leq T_j \}|$. This helps in tracking the number of EVs currently present.
    \item The energy needed by each EV that is connected to a charger at time slot $T_j$, given by $[\PowerNeed(C_i, T_j)]_{C_i \in \mathcal{C}}$, and is initialized to $0$. This represents the energy gap between the required SoC and the current SoC of the EV connected to charger $C_i$ at time slot $T_j$, defined as: 
    \begin{equation} 
     \PowerNeed(C_i, T_j) = (\SOCR(V) - \SOC(V, T_j)) \times \text{CAP}(V)
     \label{eq:chargerstate}
    \end{equation}
    where $V=\phi(C_i, T_j)$ indicating the EV connected to $C_i$ at time slot $T_j$. 
    \item The remaining time until the departure of each EV connected to the chargers is given by $[\ReTime(C_i, T_j)]_{C_i \in \mathcal{C}}$, and is set to 0 when no cars are connected. Each term is computed as $\ReTime(C_i, T_j) = \DepartureTime(\phi(C_i, T_j)) - T_j$. 
\end{enumerate} 
   % where $RT(C_i,T_j)$ represents the remaining time before the departure of the EV, i.e.,
    %The energy gap $\PowerNeed(C_i, T_j)$ is calculated based on the difference between the target SoC and the current SoC at time $T_j$.
   % Status of all chargers is given by $\CS(T_j) = \{ CS(C_i, T_j) \}_{C_i \in \mathcal{C}, T_j \in \mathcal{T}}$. Each charger status is specified by $CS(C_i, T_j) = (\PowerNeed(C_i, T_j)$ , $RT(C_i, T_j))$. Here, $\PowerNeed(C_i, T_j)$ represents the energy gap between the required SoC and the current SoC for the EV connected to $C_i$, calculated by: 
 %We set $CS_t^i = (0,0)$ if no EV is connected to charger $C^i$.

% {\bf Initial State.} The initial state $S(T_0)$ is defined at the beginning of the billing period. It consists of the starting building load $B(T_0)= 0$, the estimated peak power $\PrdPeak(T_0)$, which may be derived from historical data or forecasts (as detailed in Section~\ref{ssec:pipeline}), and historical load values from the preceding 7 days, represented as $B^{H}(T_0)$. The initial time is set to $T_0$, corresponding to the day of the week. At this initial time, the number of EV arrivals is $0$. Additionally, all parameters of the initial status of all chargers $\CS(T_0)$ are set to 0.  
%the initial status of all chargers, denoted as $\CS(T_0) = \{ CS(C_i, T_0) \}_{C^i \in \mathcal{C}}$, is initialized with $CS(C_i, T_0) = (0,0)$ for all chargers.  

{\bf Actions.} The actions $A(T_j) \in \mathcal{A}$ in this MDP are continuous and specify the power rates of all chargers at time $T_j$, where $A(T_j) = [P(C_i, T_j)]_{C^i \in \mathcal{C}}$.

{\bf State Transition.} We utilize a discrete event simulator to track the state transitions. \ad{Need to write a bit about the simulator.} The simulator is given a ``chain'' empirically sampled from the data provided by our partner representing a day in the monthly billing period. 
\jpt{Will connect it to the new subsection in Approach}
Each chain is accompanied by the estimate of peak power over the entire billing period, $\PrdPeak(T_0)$, based on the full sequence of daily chains across a month. This peak is generated by solving a MILP program that gives all the monthly chains as an input, and during training, this ``optimal peak'' across the month is used as an input for a given daily chain \ad{the whole concept of daily and monthly chains is a bit confusing and has to be cleared up. Basically how we reduce the monthly problem into a sequence of daily problems has to be described here clearly.}.


% Given the complexity of modeling state transition probabilities in the V2B problem, we conduct simulations for each billing period, referred to as a ``sample''. The simulator includes:
% \begin{itemize}[leftmargin=*] 
%     \item {The estimated peak power over entire billing period, $\PrdPeak(T_0)$ based on training samples.}
%     \item Electricity prices, $\theta_E(T_j)$ for $T_j \in \mathcal{T}$.
%     \item Building load, ${B}(T_j)$ for $T_j \in \mathcal{T}$.
%     \item EV arrival and departure schedules with SoC requirements, $\forall V \in \mathcal{V}: \SOCI, \SOCR, SOC, CAP$.
%     \item Available chargers and charging limits $\{C_i \in \mathcal{C}\}$.
% \end{itemize} 
The simulator updates the status of the building and chargers based on actions taken at each time slot. 
%The state transition function is defined as ${\it Trans}: \mathcal{S} \times \mathcal{A} \leftarrow \mathcal{S}$, with $(S(T_j), A(T_j)) \mapsto S(T_{j+1})$,  
%\rishav{change \\this and simulator\\ steps to use \\ ${T_j}$ \&$T_{j-1}$} 
%indicating how state features are updated.
This process includes the following steps: 

\begin{enumerate}[leftmargin=*]
    \item Initialize the estimated peak power, $\PrdPeak(T_0)$, which can be derived from historical data \ad{does not make sense. We need to clearly describe the process of how this is estimated for training as well as inference. Please update. There is no mention in section 4.2} (detailed in Section~\ref{ssec:pipeline}),
    , and update it by
    $
    \PrdPeak(T_{j+1}) = \max(\PrdPeak(T_j)$, $ \Building(T_j) + \sum_{C^i \in \mathcal{C}} P(C_i, T_j)),
    $
    which updates the estimated peak power depending on the previous estimate and the current peak power.
    \item Update SoC of EVs connected to all chargers: $\SOC(\phi(C_i,T_j), T_{j+1})$ using action $A(T_j)$ according to Equation~(\ref{eq: soc}). 
    %Additionally, we apply action post-processing to keep EV SoCs within valid boundaries by adjusting the power rate for stopping charging or discharging when they exceed $SoC^{\text{max}}(\phi(C_i, T_j))$ or drop below $SoC^{\text{min}}(\phi(C_i, T_j))$. 
   \item Update the EV charger assignment $\phi(C_i, T_j)$ and $\eta(V_k)$ by first releasing chargers with departing EVs in the next time slot $T_{j+1}$ and then assigning new arrival EVs to idle chargers, following the FIFO procedure and prioritizing the bi-directional chargers first. 
   \item Update the energy requirement of all EVs connected to a charger: $[\PowerNeed(C_i, T_{j+1})]_{C_i \in \mathcal{C}}$ (by Equation~(\ref{eq:chargerstate})) based on EV's current SoCs.
   \item Update the remaining time of all EVs connected to chargers: $[\ReTime(C_i, T_{j+1})]_{C_i \in \mathcal{C}}$ at time slot $T_{j+1}$.   
\end{enumerate}


{\bf Action Reward.} We define the function ${\it Reward}: \mathcal{S} \times \mathcal{A} \rightarrow \Re$, where ${\it Reward}(S(T_j), A(T_j))$ evaluates the reward for actions taken in a specific state, focusing on minimizing the total bill while satisfying SoC requirements. This function is expressed by: \ad{is the subscript index for actions is correct? }
\begin{align}
   & \mathit{Reward}(S(T_j), A(T_j)) = \lambda_{S} \cdot \mathit{Reward}_1 + \lambda_{E} \cdot \mathit{Reward}_2 + \lambda_{D} \cdot \mathit{Reward}_3
\end{align}
where, 
\begin{align*}
\begin{aligned}
    \mathit{Reward}_1 &=  \sum\limits_{C^i\in\mathcal{C}} \max(0, \min(\PowerNeed(C_i, T_j), P(C_i, T_j) \cdot \delta)) \\
    \mathit{Reward}_2 &= - P(C_i, T_j) \cdot \delta  \cdot \theta_E(T_j) \\
    \mathit{Reward}_3 &= - \max(0, \Building(T_j) + \sum\limits_{C^i \in \mathcal{C}} P(C_i, T_j) - \PrdPeak(T_j)) \cdot \theta_D
\end{aligned}
\end{align*}
% \begin{align}
%    & \mathit{Reward}(S(T_j), A(T_j)) = \nonumber \\
%    & \sum\limits_{C^i\in\mathcal{C}} \max(0, \min(\PowerNeed(C_i, T_j), P(C_i, T_j) \times\delta)) \times \lambda_{S} \nonumber \\
%    & - P(C_i, T_j) \times\delta  \times \theta_E(T_j) \times \lambda_{E} \nonumber \\
%    & - \max(0, \Building(T_j) + \sum\limits_{C^i \in \mathcal{C}} P(C_i, T_j) - \PrdPeak(T_j)) \times \theta_D \times \lambda_{D} 
% \end{align}
where $\mathit{Reward}_1$ promotes actions that charge EVs to reach their required SoC, as outlined in Eq. (\ref{eq: soc}). $\mathit{Reward}_2$ penalizes the energy costs generated by this action, and $\mathit{Reward}_3$ penalizes the increase in demand charges caused by rising peak power, aligning with our objective in Eq. (\ref{eq: billing}). These functions use three reward coefficients, $\lambda_{S}$, $\lambda_{E}$, and $\lambda_{D}$ to balance trade-offs between these reward factors. 
%The coefficients are adjustable during training to optimize model performance. 

% {\bf Policy}: Our objective is to develop the policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$, where $\pi(S(T_j))$ generates the optimal charging actions for a given state, maximizing the overall action reward function throughout the billing period $T_j \in \mathcal{T}$.
 
% \rishav{Add discount factor \\short description}