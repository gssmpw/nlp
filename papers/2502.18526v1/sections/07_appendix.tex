\section{Appendix}

\input{sections/related_table}

\subsection{Related Work}

We provide a more detailed review of prior work here.~\Cref{tab:comparison} describes the key papers and summarizes their gaps.

\noindent\textbf{Uncertainty of vehicles and SoC requirements.} Prior work has taken \citeauthor{MJG2015} defines this taking into consideration different mobility aspects such as the arrival/departure time of an EV at/from a charging station, trip history of EVs, and unplanned departure of EVs~\cite{MJG2015}. Empirical studies, such as those by \citeauthor{richardson2011electric}, have analyzed EV charging strategies and their impact on grid stability, which are closely related to V2B systems~\cite{richardson2011electric}. 
The challenges of optimizing V2B systems, especially given the dynamic nature of energy pricing and vehicle usage patterns, have also been addressed by \citeauthor{8274175}, who proposed a demand response framework for smart grids~\cite{8274175}. Additionally, \citeauthor{oconnell2010integration} applied optimization algorithms, such as Mixed Integer Linear Programming (MILP), to integrate renewable energy sources into grid systems~\cite{oconnell2010integration}. 
{\color{black} Other approaches, including meta-heuristics and Model Predictive Control (MPC), have been explored to optimize the smart EV charging process for electric vehicles (EVs), focusing on energy cost and user fairness in single-site or vehicle-to-grid (V2G) systems~\cite{AORC2013, 5986769, 9409126, MJG2015}.} 
However, many of these methods focus on unidirectional chargers and fail to fully account for uncertainty including, vehicle arrivals and departures~\cite{MJG2015}. 
% Other approaches, including meta-heuristics and Model Predictive Control (MPC), have been proposed to address V2B and V2G challenges~\cite{AORC2013, 5986769, 9409126, MJG2015}.



\noindent\textbf{Time of use pricing, demand charge, and long-term rewards.} 
% Time of use pricing and demand charge, Long-term rewards and planning horizon
% AORC2013, MMN2019, SNDJ2020
Optimizing V2B is a complex problem, made more complex when the lengths of billing periods set by TSOs are considered. Several approaches~\cite{AORC2013, MMN2019, SNDJ2020} only optimize and plan for single-day horizons. ~\citeauthor{9409126} are able to achieve one-month planning horizons while considering demand charge~\cite{9409126}. However, they assume a homogeneous set of chargers. Thus preventing them from fully realizing the effect of EVs on potential savings.

\noindent\textbf{Heterogeneous chargers and continuous action spaces.} Approaches that solve EV charging without considering the ability of EVs to discharge ignore even more potential savings. However, addressing this introduces further complexity to the system.
\citeauthor{NNM2024} works around the limitations of charger homogeneity by using Deep RL~\cite{NNM2024}. They consider SoC requirements and address the mobility-aware needs of EVs. However, their approach does not consider long-term rewards, instead limiting their planning to a single day. 
Improving upon these initial approaches, \citeauthor{ZJS2022} investigated federated RL for EV charger control, aiming to maximize user benefits~\cite{ZJS2022}, and minimize electricity prices. Their approach explores the continuous action space of charging power and extends their planning horizon to an entire week. While their approach includes both discharging and charging actions, they fail to capture the idea of demand charge into their problem, which is critical in the industrial context. 
% Additionally, when we consider demand charge the planning horizons have to increase to a month.

\noindent\textbf{Tracking real-world state and transition.}
Existing approaches validate their approaches using simulations that have limited interface with the real world. \citeauthor{9409126} utilizes an existing Adaptive Charging Network (ACN) EV charging testbed along with a mobile application to capture EV telemetry and charger behavior~\cite{9409126}. Thus, they capture the complexity of real charging systems, including battery charging behaviors.


\input{results/car_arrivals}

\input{results/car_arrival_departure_hours_distribution}


\subsection{Complementary Figures}
\label{ss:appendix_figures}
The characteristics of the training set are shown in~\Cref{fig:arrival_counts} and~\Cref{fig:car_distributions}.~\Cref{fig:arrival_counts} show the variations in arrival counts (top) and peak building draw (bottom) across the different days of the week across 8 months. Most cars arrive on Wednesdays and Fridays while the peak building draw is the least on Fridays.~\Cref{fig:car_distributions} (top) show distributions of car arrivals and departures hours against the EVs arrival SoC and required SoC upon departure.~\Cref{fig:car_distributions} (bottom) show the distribution of peak power draw and the hour of day. The line in red signifies the TOU rates across the day. All of the cars arrive within peak hours, while the majority of them leave within the peak hours. Finally, all off-the-peak power draws occur at peak hours. The intersection of these arrivals, departures, and peak power draw hours represents the potential space in which our policy can operate.

\textbf{Environment Simulator.} We process these training data into input samples for our digital twin/simulator: Optimums~\cite{JP2024}. We model a digital twin for the target environment and provide several interfaces that allow both simulated and real-world components to leverage our proposed approach. This allows us to investigate how any action or decision can potentially impact the real world. 
% % Decisions are taken at the end of each set of events for any given time period. 
% %
There are two main decisions that must be taken when solving the V2B charging problem. (1) charger assignments and (2) charger actions.

\input{tables/charger_assignment_table}

\textbf{Charger assignment.} We consider a first-in, first-out policy that assigns EVs to bidirectional chargers first, breaking ties assigning to later departing cars; a comparison of different policy combinations is shown in~\Cref{table:charger_assignment_policies}. We observe that bidirectional charging assignments outperform all other policies. Tie-breaking strategies that prioritize later-departing vehicles show a marginal advantage. While these assignment policies could be further optimized, we chose to follow this heuristic and focus on the second decision problem: determining charger actions.
% %

\textbf{Charger actions.} We provide several policies with our simulator to compare our proposed approach. Charger action policies receive a state of the environment for a particular time and generate actions based on this. 




% \input{results/charger_assignments} 
\input{algorithms/overall_algo1}
% \input{results/month_demand_charge} 
% \input{results/month_total_bill} 

% \subsection{Complementary Algorithms } 


\subsection{Additional Details on the Approach}
\label{ss:appendix_approach}
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.68\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0.2cm 0.2cm 0.2cm 0.2cm,clip]{figures/framework.pdf}
%         % \vspace{-0.1in}
%         \caption{Reinforcement Learning Framework.}
%        \label{fig:framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.28\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0cm 0.5cm 0cm 0.5cm,clip]{figures/inference_pipeline.pdf}
%         \vspace{-0.15in}
%         \caption{Pipeline for Inference.}
%         \label{fig:pipeline}
%     \end{subfigure}
%     \caption{\color{black}(a) Our framework relies on daily samples along with an estimated monthly peak power. We use reinforcement learning, i.e., DDPG, and extend it with policy guidance and action masking, to learn a near-optimal policy, A reinforcement learning policy based on DDPG is trained with policy guidance and action masking. (b) At inference time, the model ingests data of connected cars, charger states, building power reading, and the estimated monthly peak power to make decisions.}
%     \label{fig:framework_and_pipeline}
% \end{figure*}

Our proposed approach, as outlined in Algorithm~\ref{alg:DDPG}, is based on the DDPG algorithm~\cite{lillicrap2015continuous}, 
which utilizes an actor network to generate actions. Tuples of state, action, reward, and next state are stored as transitions in the replay buffer (lines 9 to 12). 
%
During training, we interact with the environment simulator. We provide input from the training dataset.~\Cref{tab:training_testing_data} shows the characteristics of this dataset. The environment simulator abstracts state features for the RL models and manages state transitions based on the function described in Section~\ref{ssec:MDP}. 
%
In each training iteration, we batch state transitions from the replay buffer for model training (line 13). Specifically, DDPG maintains target networks for both the actor and critic, which are used to generate the next state and compute Q-values essential for calculating the critic loss during training. The critic network is trained using gradient descent by minimizing the mean squared error between predicted Q-values and target Q-values derived from the Bellman equation (lines 14-16). The critic learns Q-values for state-action pairs, which are then used to train the actor network through a policy gradient approach (lines 17 and 18). The updates to the target networks are delayed to stabilize the training process (lines 18-19). 

% However, traditional DDPG is limited in its ability to reach global optima for particularly long horizons and large state-action spaces.
To improve RL performance in handling the limitations associated with large continuous action spaces and long-term reward optimization, we introduce action masking and policy guidance techniques. 
%
Action masking, denoted as $\Mask(S(T_j), A(T_j))$, refines the raw actions generated by the actor network by enforcing action validity and utilizing domain-specific knowledge, thereby improving policy performance (lines 9, 14, 17). Meanwhile, policy guidance incorporates the MILP solver to provide optimal actions through $\MILP(S(T_j), {\it remainEpisode})$, based on current and future information (lines 5-9). These optimal actions are stochastically introduced during RL training into the replay buffer, mixing high-quality actions with the raw RL actions to enhance the training transition quality and guide the RL training in a beneficial direction. 


\input{tables/hyperparameters_table}
\input{tables/training_statistics_table} 
% \input{results/demand_charge_shave_mean_std}

\input{tables/simulation_parameters_table}


\subsection{Complementary Experimental Results}
\label{appendix:results}
We evaluate our approach on an environment with the parameters shown in~\Cref{tab:simulation_parameters}. We use the same parameters across hyperparameter tuning, training, evaluation, and comparison.

\noindent \textbf{Additional Details on Hyper-Parameter Tuning:} This methodology enables us to monitor and assess the RL model's performance throughout the training process. 
An early stopping procedure is implemented, terminating training if the total reward on the 20 evaluation samples does not improve after a specified number of iterations. Finally, we select the optimal combination of hyperparameters based on the results from the 3-fold cross-validation. The new RL model is then trained using the full set of training samples with the identified best parameter combination. Finally, we test the trained 9 RL models on 50 unseen monthly samples for each month from May 2023 to January 2024 to evaluate their generalization performance. ~\Cref{tab:hyperparameters} show selected values for each hyperparameter after tuning.


\input{algorithms/heuristictrickle}

\input{algorithms/heuristicchargefirst}

\input{results/add_ablation} 

\input{results/demand_charge_shave_mean_std}


\noindent \textbf{Heuristics:}~\Cref{alg:charge_first_llf} and~\Cref{alg:llf} show the exact algorithms used for Charge First LLF and Trickle LLF, respectively.

\noindent \textbf{Peak Shaving Results:}~\Cref{table:peak_shaving_demand} shows the peak shaving performance of all approaches. 

\noindent \textbf{Ablation results:} Finally,~\Cref{table:ablation_study} shows a breakdown of the ablation results per month.




% \subsection{Baselines}

% \begin{itemize}[leftmargin=*]
%     \item {\bf Optimal MILP Solver}: We use CPLEX~\cite{cplex2009v12} to solve each MILP instance. As discussed before, the solution assumes complete knowledge of future events (in the current chain) and can provide the optimal solution for the entire billing period, while minimizing the total cost and SoC deficit. This serves as the upper bound of performance for each sample.
%     \item {\bf Fast Charge (Baseline)}: The baseline approach simulates a real-world charger charging procedure. It charges all connected EVs as quickly as possible upon connection, till $\SOCMAX$.
%     \item {\bf Trickle Charging (Trickle)}: The trickle charging approach utilizes the trickle charging rate, defined as the minimum required charge at each time slot: $P(C_i, T_j) = \frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)}$, to charge all EVs until they reach their required SoC. Here, $\PowerNeed(C_i, T_j)$ represents the remaining power needed for the EV connected to charger $C^i$ at time slot $t$ to reach the required SoC, while $\ReTime(C_i, T_j)$ denotes the remaining time the EV will stay before departure, as defined in Section~\ref{ssec:MDP}. 
%    %{\color{black} Note that we obtain MILP solutions through an oracle simulation, which are not implementable in the real world.}
%     % \item {\bf Least Laxity First (LLF)}: Least Laxity First is a dynaimc priority driven algorithm designed for scheduling multiprocessor real time tasks~\cite{leung1989new}. Laxity or slack time refers to the amount of time a task can be delayed without causing it to miss its deadline. In the context of EV charging, we define laxity as the difference between the amount of time remaining for a car before it departs and the amount of time it takes to meet the EV required SoC at a constant rate of charge~\cite{xu2016dynamic}. Specifically, for EV connected with charger $C^i$ at time slot $t$, we compute its laxity value  by $(T_\DepartureTime(\phi(C_i, T_j))-t)-(\SOCR(\phi(C_i, T_j))-SoC_t(\phi(C_i, T_j)))\times {\it CAP}(\phi(C_i, T_j))C^{max}_i$. 
%     % Additionally, we limit the amount of EVs charging at any given time interval by allocating a capacity at each step. The capacity is based on the difference between a set a power threshold and the current building power at that time. 
%     % Only EVs connected to chargers whose aggregate power rates fall within this limit are able to be charged for that time interval. LLF will provide trickle charge, charging the EVs to the minimum required power to reach required SoC before departure, at each interval. 


% \item \textbf{Trickle Least Laxity First (Trickle LLF)}: We define Trickle LLP algorithm, detailed in Algorithm~\ref{alg:llf} in the Appendix, based on the classic scheduling approach: the Least Laxity First algorithm, a dynamic priority-driven method for scheduling multiprocessor real-time tasks~\cite{leung1989new}. Laxity, or slack time, is the time a task can be delayed without missing its deadline. In EV charging, we define laxity as the difference between the remaining time before departure and the time required to reach the desired SoC at a constant charging rate~\cite{xu2016dynamic}.   For an EV connected to charger $C^i$ at time slot $t$, the laxity value is computed based on $ (\DepartureTime(\phi(C_i, T_j)) - t) - \PowerNeed(C_i, T_j)/C^{max}_i $.
% At each time slot, we compute the available power gap till we cross the estimated peak power using the current building power as, $ \PrdPeak(T_j) - \Building(T_j) $. This power gap is allocated to all EVs by distributing the trickling charger action, calculated as $ \frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)} $, to EVs prioritized by their laxity values until they reach the required SoC. 

% % \input{algorithms/heuristictrickle}
    
% \item {\bf Trickle Early Deadline First (Trickle EDF)}: We propose the Trickle EDF algorithm in a similar manner to Trickle LLF, with the only difference being the prioritization method. Trickle EDF follows the Early Deadline First approach, which was originally designed as a dynamic scheduling algorithm for real-time systems~\cite{stankovic_EDF}. 
% In Trickle EDF, the deadline for each EV is defined as the remaining time before departure, i.e., $\DepartureTime(\phi(C_i, T_j)) - t$. The algorithm computes the available power gap and allocates the trickle charging rate to each EV based on the earliest deadline first until they reach their required SoC.
% % We only consider EVs below the required SoC in the scheduling. Similar to LLF, this approach utilizes a 
% % capacity to limit the amount of vehicles that can be charged for any time interval. 
% %Charge First policies behaves similarly to EDF and LLF. However, the key difference is how much charge is provided to EVs. Charge First policies will prioritize charging EVs connected to  bidirectional chargers, as much as possible given the capacity. The potentially excess energy that these EVs will hold will then be used during intervals where there is a need for more capacity to charge critical EVs.
%     \item {\bf Charge First Least Laxity First (Charge First LLF)}: 
%    We also propose the Charge First LLF algorithm detailed in Algorithm~\ref{alg:charge_first_llf} in the Appendix. Similar to Trickle LLF, at each step, we compute the gap between the estimated peak power and the building power to determine the available power gap for EV charging: $\hat{P} = \PrdPeak(T_j) - \Building(T_j)$. 
% The key difference in Charge First LLF is that we first calculate the sum of the trickle charging rates for all EVs at the current time slot: $S = \sum_{i} {\PowerNeed(C_i, T_j)}/{\ReTime(C_i, T_j)}$. If this sum is less than the available power gap, $S < \hat{P}$, it indicates that we have excess power available for charging. In this case, we first charge all EVs using their trickle charging rates: $P(C_i, T_j)={\PowerNeed(C_i, T_j)}/{\ReTime(C_i, T_j)}$. Then, we prioritize charging EVs connected to bi-directional chargers as much as possible to reach their maximum State of Charge (SoC), thereby consuming the extra available power. This charging process follows the reverse order of their laxity. 
% If the sum of the trickle charging rates is greater than the available power gap, $S \geq \hat{P}$, we order all EVs connected to bi-directional chargers based on the reverse of their laxity. We then determine the excess power for each EV that exceeds its required SoC: $P(C_i, T_j) = \max\left(C^{min}_i, \frac{(\SOCR(V) -\SOC(V,T_j)) \times CAP(V)}{\delta}\right)$, with $V=\phi(C_i, T_j)$. This excess power is added to the available power gap for later trickle charging power allocation. The trickle charging rates are then allocated to EVs based on their laxity order. 
% %$P(C_i, T_j) \gets \min\left(\frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)}, \hat{P}\right)$.

% % \input{algorithms/heuristicchargefirst}

% \item {\bf Charge First Deadline First (Charge First EDF)}: Similar to Charge First LLF, Charge First EDF follows the same procedure but utilizes a different prioritization metric, focusing on the remaining time before EV departure to prioritize charging or discharging EVs. 

% % \item {\bf Trickle Charging (Trickle)}: Trickle charging approach simply uses the minimum required charge for each time interval. The minimum required charge is given as $\frac{MaxRate}{departure-arrival}$. This results in a trickle charge for all EVs that guarantee them leaving with the required charge, given that their request is feasible.

% % {\color{red} \item {\bf Online MCTS}: Takes too long to get the result.} Hierarchical and decentralized approach to charging EVs. Actions are discrete. 



%     %Two variations of the baseline are implemented: (1) EVs are charged to the maximum allowed SoC (often higher than the required SoC of the user) and (2) 
%     \color{black}
%     % \item Variations on Scheduling Algorithms: Trickle Charging, Charge First, and Machine Learning-based threshold.
%     \color{black}
% \end{itemize}


% \input{results/all_total_bills}


\iffalse
An outlier was observed in June, where the RL model did not outperform all heuristics, leading to bills that were $\$28$ higher than the best-performing heuristic, Charger First LLF. 
Thus, we propose a procedure for evaluating RL performance on training data relative to heuristics to determine which policy to use. We define the performance metric as:
\begin{equation}  
\bar{\Theta}^{\it policy} = \frac{1}{N} \sum_{i=1}^{N} \frac{\Theta(\mathcal{P})^{{\it policy}}_i - \Theta_i^*}{\Theta_i^*}
\label{eq:training_metrice}
\end{equation}
where $ \Theta(\mathcal{P})^{\text{policy}}_i $ represents the final total bill of a specific policy ${\it policy}$ for the $i$-th sample, and $\Theta_i^* $ is the optimal total bill generated by the MILP solver. This metric allows for comparing the total costs of different policies relative to the optimal solution, providing insights into their effectiveness in minimizing costs in the V2B problem.

By comparing this metric across all training samples, we can evaluate the performance of RL and heuristic policies and identify the policy that provides the smallest gap relative to the optimal solution.  
Table~\ref{table:performance_metrics_training_data} presents the normalized performance metrics for the training data across all heuristic policies. 
The results indicate that in June, the Charger First LLF heuristic had lower performance metric values than the RL model. Therefore, we recommend using Charger First EDF as the preferred policy for this month.
% Upon analyzing the training data for December, we observed a significantly higher number of car arrivals, about 20 per day, especially during peak hours, compared to around 10 per day in other months (Table~\ref{tab:training_testing_data}). We hypothesize that this high volume and diversity of car arrivals introduced additional challenges for the RL model's generalization, resulting in a locally optimal policy. 
% Based on this observation, 
\input{results/performance_metrics_training_data}
\fi

