\section{Approach}
\label{sec:approach} 



% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.68\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0.2cm 0.2cm 0.2cm 0.2cm,clip]{figures/framework.pdf}
%         % \vspace{-0.1in}
%         \caption{Reinforcement Learning Framework.}
%        \label{fig:framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.28\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0cm 0.5cm 0cm 0.5cm,clip]{figures/inference_pipeline.pdf}
%         \vspace{-0.15in}
%         \caption{Pipeline for Inference.}
%         \label{fig:pipeline}
%     \end{subfigure}
%     \caption{\color{black}(a) Our framework relies on daily samples along with an estimated monthly peak power. A reinforcement learning policy based on DDPG is trained with policy guidance and action masking. These techniques improve the performance of the model. Finally, it generates charger actions for every given state. (b) At inference time $T_j$, the model requires data of connected cars, charger states and building load reading. It then uses a monthly peak power estimator to forecast the peak power for the entire billing period. The result are the charger actions for time $T_{j+1}$.}
%     \label{fig:framework_and_pipeline}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% % \includegraphics[width=0.95\textwidth]{figures/framework_only.pdf}
% \includegraphics[width=0.95\textwidth]{figures/framework_v2.pdf}
% \caption{(Left) Data Processing Pipeline, (right) Complete Framework of our RLApproach.}
% \label{fig:framework_and_pipeline}
% \Description{}{}
% \end{figure*}


%We employ the Deep Deterministic Policy Gradient (DDPG) algorithm~\cite{lillicrap2015continuous} to train the policy $\pi(S(T_j))$ for the V2B problem. DDPG is ideal for handling our continuous action space and supports off-policy training, allowing the RL model to learn from diverse experiences across various scenarios, thereby enhancing model generalization. To enhance the policy performance, we propose integrating action masking and policy guidance techniques alongside DDPG, as outlined below. Figure~\ref{fig:framework_and_pipeline}.  

%. This framework enables optimal decision-making regarding charger power rates and load management, allowing for effective responses to uncertainties in user behaviors and building loads in our working scenario. We aim to derive a policy for each time slot that minimizes total costs while ensuring that the SoC requirements of the EVs are met throughout the billing period. 

%Considering the uncertainty in the V2B problem, we model it as a sequential decision-making problem using a Markov Decision Process (MDP) to simulate state transitions and utilize an RL-based approach to train a policy for this MDP. 

In this section, we first outline the data pipeline, focusing on how we handle the monthly training data, generate input features, and execute the policy (Figure~\ref{fig:framework_and_pipeline}) and then detail the MDP formulation. \ad{while the figure talks about delayed update, its discussion clearly in a highlighted paragraph is missing. This is an important part and hence must be emphasized.}
%, then discuss the steps of the data processing pipeline, 
 

\subsection{Data Pipeline}
\label{ssec:pipeline}
%To generate training and testing data for the RL models in the V2B problem, we follow these steps for data collection and processing. 
% % \begin{enumerate}[leftmargin=*] 
% \textbf{Data Collection}: We gather data from \nissan{}'s EV fleet, including arrival and departure times, initial SoC, and required SoC. We also collect historical smart building load demand across different seasons from May 2023 to January 2024. Additionally, data from EV chargers is collected, including charging/discharging rates.
    
% \textbf{Modeling for User Behavior and Building Load Fluctuation}: Based on the collected data, we model multiple features. For instance, we use the Poisson distribution to model EV arrivals at each hour, considering inputs such as current time, month, and weather information. Similarly, we train models using random forest to generate estimateed EV stay duration, initial SoC, and required SoC, effectively modeling EV driver behaviors. To model the building load fluctuation, we utilize 9 months of real building energy consumption data collected from \nissan{}'s smart building, recorded at 15-minute intervals. The building load is modeled using a normal distribution, with the real building load as the mean and introducing a standard error of 2 kW.

% \textbf{Generating Sample Chains for Training and Testing}: We then generate training samples for one-month billing period episodes, including EV arrival/departure schedules along with their initial SoC and required SoC, using the developed models for user behaviors. Building load data is generated at every 15-minute interval over a month. Additionally, we incorporate data for 15 \nissan{} EV chargers, consisting of 5 bidirectional and 10 unidirectional chargers. Time-of-Use (ToU) electricity rate schedules and demand charge data from utility providers are also integrated to simulate realistic billing scenarios. We generate 110 samples for each month from May 2023 to January 2025. Each sample contains all the above information for each day of the month, and we use 60 samples for training and 50 for testing.  
{\color{black} 
We outline the data pipeline in Figure~\ref{fig:framework_and_pipeline}, which encompasses the preliminary processes for RL training, detailing data collection, sample generation, peak power estimation, and training episode segmentation. \ad{clarify how much data exists. Also, clarify how we reduce the monthly problem to daily problem}
% These are all essential steps for effective RL training. 
 %  \textbf{Data Collection.} We first gather comprehensive data from \nissan{}'s EV fleet, including arrival and departure times, initial and required State of Charge (SoC), as well as historical smart building load demand across seasons from May 2023 to January 2024. Additionally, we collect charging and discharging rates from EV chargers. Multiple features are modeled to capture these distributions using the Poisson distribution for EV arrivals, SoC requirements, and building fluctuations based on historical data.

% \textbf{Generating Sample Chains for Training and Testing}. We create training samples for one-month billing episodes that encompass EV schedules, initial SoC, and required SoC. Building load flows are generated at 15-minute intervals based on historical data. We also integrate Time-of-Use (ToU) electricity rate schedules and demand charge data from utility providers to simulate realistic billing scenarios. 
% , as well as historical smart building load demand across seasons from May 2023 to January 2024. 
% Additionally, we collect telemetry data from EVs and EV chargers.
{\bf Sample generation.}
Data was collected and provided by \nissan{} from their fleet and chargers at their office building over the period from May 2023 to January 2024. The dataset includes EV arrival and departure times and initial and required State of Charge (SoC) as well as building power demand readings at 15 minute intervals. Since the location is categorized as an industrial building, it is subject to time-of-use (TOU) electricity rates and demand charges. TOU introduces variations in the electricity rates, with higher prices during peak hours (6:00 AM to 10:00 PM). Demand charge adds a flat rate multiplier to the highest average power delivery across 15-minute intervals over the billing period, and is added to the total bill.
To capture the distributions of these features, we utilize the Poisson distribution for modeling EV arrivals, SoC requirements, and building fluctuations based on historical data. We then empirically sampled $1000$ one-month billing episodes samples for each month from May 2023 to January 2024.  
% The building did not have data before May 2023 and data collection stopped in Jan 2024.

%Additionally, we integrate Time-of-Use (ToU) electricity rate schedules and demand charge data from utility providers to simulate realistic billing scenarios. Using these data models, we generate 1,000 samples for each month from May 2023 to January 2024. 

\textbf{Splitting data into daily episodes.}
\jpt{Can we explain a bit more on how splitting monthly to daily help with the training?}
To enhance training efficacy, we address the challenge of lengthy state-action episodes by splitting the monthly training dataset into daily episodes.
We focus only on weekdays since employees are often not present at the office, resulting in minimal EV arrivals and low building loads, both of which do not significantly impact the overall monthly demand charge. We incorporate the estimated monthly peak power as an input feature and penalize only those peak power values exceeding this estimated value in each daily episode. This approach incentivizes the policy to minimize the monthly peak power while effectively training with daily episodes.

{{\bf Down-sampling Strategy}.
During training, we varied the amount of samples used for training and observed that utilizing a larger number of training episodes results in longer convergence time and overall worse performance. We show the results of this in~\Cref{ssec:ablation}.
% During training, we implement a down-sampling strategy from the 1,000 monthly samples. We observe that utilizing the complete dataset results in excessive training iterations, significantly increasing the training duration (up to 3 days on our computing systems) and complicating convergence, motivating us to perform down-sampling. 
Thus, we utilize a k-means clustering approach to down-sample. We used $k=5$ and clustered using the optimal demand charge derived from the MILP solution for each sample. 
% This optimal demand charge is closely correlated with sample characteristics, such as building load distribution and EV user's behavior, and significantly influences the total bill. 
% Specifically, We clustered 1,000 samples into five groups based on optimal demand charge using k-means. 
From each cluster, we select 60 and 50 samples for the final training and testing datasets respectively, ensuring that these datasets are mutually exclusive. 
% This method allows us to train and evaluate our RL model using fewer but representative training samples, thereby mitigating the loss of diversity and distribution among the samples.
}

\textbf{Peak power estimation.} 
% A key aspect of our state definition is the monthly peak power estimation. 
We employ a MILP solver to determine the minimum demand charge derived from optimal action sequences over the one-month billing period across all training samples. From the distribution of peak power from the MILP solutions, we set the lower bound of the 95\% confidence interval as the estimated peak power for the month, providing a conservative initial estimate.



%{\bf Data Collection}: We gather data from \nissan{}'s EV fleet, including arrival and departure times, initial and required State of Charge (SoC). Historical smart building load demand is collected across seasons from May 2023 to January 2024, along with charging and discharging rates from EV chargers. Multiple features are modeled to capture these distributions, employing the Poisson distribution for EV arrivals, SoC requirements, and building fluctuations based on historical data.
%{\bf Generating Sample Chains for Training and Testing}: We create training samples for one-month billing episodes, encompassing EV schedules, initial SoC, and required SoC, using the developed models. Building load flows are generated at 15-minute intervals based on historical data. Additionally, Time-of-Use (ToU) electricity rate schedules and demand charge data from utility providers are integrated to simulate realistic billing scenarios.

% \textbf{Feature Generation}: Based on the cleaned data, we generate input features such as building load profiles, EV arrival schedules, initial SoC, and departure SoC targets. These features serve as the input states for the RL model training and testing phases. 
% This process ensures that the RL models are trained on accurate and diverse data, providing robustness and generalization across various V2B scenarios.

%{\bf Generating samples.} To assess our RL-based V2B charging optimization approach, we used real-world data collected {\color{black} from an \nissan{}'s office building, along with charger meter readings in Santa Clara, California.} This data was used to derive statistics and develop generative models for generating samples for training and testing the RL model, which also serves as input to our simulator.

%\textbf{Monthly Peak Power estimation:} Our state definition includes an initial peak power estimation over the whole billing period as an input feature. We employ a Mixed-Integer Linear Programming (MILP) solver to determine the minimum demand charge derived from optimal action sequences over a one-month period across all training samples. For each month, we analyze the distribution of peak power from the MILP solutions and use the lower bound of the 95\% confidence interval as the estimated monthly demand charge, providing a conservative initial estimate. 
 
%{\bf Splitting Training Data into Daily Episodes:} During our training process, we encountered a challenge: utilizing a one-month billing period resulted in lengthy state-action episodes, hindering the policy's ability to learn the Q-value effectively. To address this, we reduced the training episodes to daily segments, focusing on weekdays. This decision was informed by our observation that weekends often have infrequent EV arrivals and low buidling load, which do not significantly impact the overall monthly demand charge.  To ensure that monthly peak power considerations are integrated, we incorporate the estimated monthly demand charge as an input feature. Additionally, the reward function penalizes only those demand charges that exceed this estimated value in each daily episode. This approach incentivizes the policy to minimize the monthly demand charge while effectively training with daily episodes.

{\bf Model inference.} Our RL-based policy operates at $delta$ intervals to determine current policy actions. To generate these actions, we require a set of input data, including the charger status (connected EV's current SoC, expected departure time, current building load, and charger information), the charging rate limits for each charger, and the estimated peak power. 
%{which can be derived from historical data or machine learning-based predictive models}.
In this paper, we utilize the estimated peak power based on training samples. With this current information, we abstract the input features for the RL model, enabling it to determine the power rate control actions for the upcoming time interval.
} 



\subsection{Environment Simulator}
Our approach uses stateless discrete event simulator that serves as the digital twin of our target environment. It holds a state that represents the entirety of the world. This includes information on EVs, building, and the grid. This allows us to investigate how any action or decision can potentially impact the real world. Decisions are taken at the end of each set of events for any given time period. There are two main decisions that must be taken when solving the V2B charging problem. (1) Charger assignments and (2) Charger actions. We address the charger assignment decision below and provide information on the charger action in~\Cref{sec:RL}.

\textbf{Environment updates.} The input episodes serve as the entirety of the simulator's world view. Each event includes an event type and time, matching their real world trigger and occurrence. We identify several critical events in the episodes to serve as the triggers for the simulator. These include (1) EV arrivals, (2) EV departures, (3) building power readings, and (4) TOU rate changes. Events are placed in a queue, with each event triggering an update to the environment which modifies the state. Updates to the state, which include the charging or discharging of EVs, are based on the elapsed time between events. Only EV arrival events trigger charger assignment decisions, while all events trigger a charger action decision.

\input{results/charger_assignments}
\textbf{Charger assignment.} In our approach we consider a first-in, first-out policy that prioritizes the assignment of EVs to bidirectional chargers. If multiple EVs arrive at the same time, then we break the ties randomly. \Cref{table:charger_assignment_policies} shows the different charger assignment and tie breaking policies tested. Bidirectional charging assignments outperform any other assignment policy. Tie breaking policies that favor latest departing cars have marginal advantage over others. While the assignment policies can be further optimized, we elected to follow this heuristic, focusing instead on the second decision problem of charger action.

\textbf{Charger actions.} We provide several policies with our simulator to contrast and compare with our proposed approach. Charger action policies receive a state of the environment for a particular time and generate actions based on this. The simulator is stateless. Thus, it provides only a current representation of the world at that specific time to each policy.


\input{sections/02_2_MDP}
