% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.68\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0.2cm 0.2cm 0.2cm 0.2cm,clip]{figures/framework.pdf}
%         % \vspace{-0.1in}
%         \caption{Reinforcement Learning Framework.}
%        \label{fig:framework}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.28\linewidth}
%         % \centering
%         \includegraphics[height=2.2in,keepaspectratio,trim=0cm 0.5cm 0cm 0.5cm,clip]{figures/inference_pipeline.pdf}
%         \vspace{-0.15in}
%         \caption{Pipeline for Inference.}
%         \label{fig:pipeline}
%     \end{subfigure}
%     \caption{\color{black}(a) Our framework relies on daily samples along with an estimated monthly peak power. We use reinforcement learning, i.e., DDPG, and extend it with policy guidance and action masking, to learn a near-optimal policy, A reinforcement learning policy based on DDPG is trained with policy guidance and action masking. (b) At inference time, the model ingests data of connected cars, charger states, building power reading, and the estimated monthly peak power to make decisions.}
%     \label{fig:framework_and_pipeline}
% \end{figure*}


\section{Our Approach}
\label{sec:our_approach}
In this section, we discuss the different components in our  framework, shown in~\Cref{fig:framework}. 
%We present how our approach targets every critical issue we presented in~\Cref{sec:related_work}. 
% We begin by defining the simulator then we provide a description of the MDP.
%
% and our approach targets every critical issue we presented in~\Cref{sec:related_work}.
% , shown in~\Cref{fig:framework}. We present how our approach targets every critical issue we presented in~\Cref{sec:related_work}.
% 
% \noindent\textbf{Uncertainty of vehicles and SoC requirements.}
% %{\jpnote{
% % we first collect data across almost an year -- buildings and car, we use this data to empirically sample chains that can be used to train and validate our policy. This was used by our partners to train a poisson and conditional distribution models from which we were able to sample more chains.  Clearly define what a sample means...}}  
% To address the uncertainty and diversity of V2B scenarios, we collect real-world historical data 
% from \nissan{} from their fleet and chargers at their office building, covering the period from May 2023 to January 2024.  
% The dataset includes EV arrival and departure times, initial and required State of Charge (SoC), and building power demand readings at 15-minute intervals. Since the location is categorized as an industrial building, it is subject to time-of-use (TOU) electricity rates and demand charges during peak hours (6:00 AM to 10:00 PM). Demand charge adds a flat rate multiplier to the highest average power delivery across 15-minute intervals over the billing period and is added to the total bill.
% % . TOU introduces variations in the electricity rates, with higher prices during peak hours (6:00 AM to 10:00 PM). Demand charge adds a flat rate multiplier to the highest average power delivery across 15-minute intervals over the billing period, and is added to the total bill.
% Then, we use the data to learn a Poisson distribution for modeling EV arrival counts, and random forest models for SoC requirements, and building fluctuations based on historical data.
% % We then empirically sampled $1000$ one-month billing episodes samples for each month from May 2023 to January 2024. 
% 
% \noindent\textbf{Time of use pricing, demand charge, and long-term rewards.} 
% %\jpnote{so we break a month into daily episodes and utilize peak power prediction generated across chains using the optimal policy which is MILP. but why daily? constant environment daily, mininmal variance weekly}  
% To enhance training efficacy, we address the challenge of lengthy state-action episodes by splitting the monthly training dataset into daily episodes. This approach captures the diverse conditions observed across different weekdays, enabling the model to learn effectively from shorter episodes, which facilitate quicker adaptation to daily variations.  
% In addition, to incorporate monthly peak power considerations, we use an estimated monthly demand charge as an input feature, penalizing only daily demand charges that exceed this predicted value. This incentivizes the policy to minimize the monthly demand charge while training with daily episodes. We determine the minimum demand charge from optimal action sequences over the one-month billing period generated by MILP optimization (detailed in~\Cref{subsec:milpsolver}). By analyzing the peak power distribution from the MILP solutions, we estimate the lower bound of the 99\% confidence interval as the monthly demand charge, providing a conservative initial estimate. This input feature is further tuned by increasing it by 0\%, 5\%, and 10\% during RL training. 
% % We incorporate the estimated monthly peak power as an input feature and penalize only those peak power values exceeding this estimated value in each daily episode. This approach incentivizes the policy to minimize the monthly peak power while effectively training with daily episodes.   
% 
% During training, we varied the amount of samples used for training and observed that utilizing a larger number of training episodes results in longer convergence time and overall worse performance. We show the results of this in~\Cref{ssec:ablation}. Thus, we utilize a k-means clustering approach to down-sample. We used $k=5$ and clustered using the optimal demand charge derived from the MILP solution for each sample. From each cluster, we select 60 and 50 samples for the final training and testing datasets respectively, ensuring that these datasets are mutually exclusive. 
% 
% \noindent\textbf{Heterogeneous chargers and continuous action spaces.}
% % \jpnote{we model as MDP; for tractability we divide into 15 minute intervals. But our system can work with other discretizations. We use DDPG; but to help with training we use policy guidance (action masking and ILP); its too difficult when you consider heterogeneous and continuous, you need guidance and action masking for RL. only mention briefly. why we need, what we need.}  
% To address uncertainty in the V2B environment, we model the problem as a Markov Decision Process (MDP), dividing the time horizon into 15-minute intervals for tractability, although our system can accommodate other discretizations. We solve this problem using a  RL-based approach built on the Deep Deterministic Policy Gradient (DDPG) algorithm~\cite{lillicrap2015continuous}, which is well-suited for continuous actions and supports off-policy training. This capability allows the model to learn from diverse experiences across various scenarios, enhancing generalization. 
% However, traditional DDPG is limited in reaching global optima for particularly long horizons and large state-action spaces. To address these limitations, we introduce action masking~\cite{huang2020closer,kanervisto2020action}, ensuring that policy actions generated by the actor network are valid and reasonable during DDPG training. This constraint enhances training efficiency by preventing the actor network from exploring invalid actions, optimizing resource usage. 
% Additionally, we incorporate policy guidance techniques~\cite{pmlr-v28-levine13} into the RL training process, which aim to introduce optimal state-action transitions and improve local optima in DDPG. 
% 
% \noindent\textbf{Tracking real-world state and transition.}  
% % - first we downsample to help with generalization using a clustering approach. % why do we need do that. Is there a paper that did something similar.. 
% % \jpnote{we model a digital twin and provide rest APIs -- it has state and transitions and tracks the MDP that we will discuss in the next section. The simulator also is able to generate rewards that are used for training and are also discussed in the next section.} 
% We model a digital twin for the target environment and provide several interfaces that allows both simulated and real-world components to leverage our proposed approach. 
% We model the environment using a digital twin that holds a state representation of the world with transitions representing V2B behavior. 
% % This includes information on EVs, building, and the grid. This allows us to investigate how any action or decision can potentially impact the real world. 
% % Decisions are taken at the end of each set of events for any given time period. 
% %
% There are two main decisions that must be taken when solving the V2B charging problem. (1) Charger assignments and (2) Charger actions.
% % We address the charger assignment decision below and provide information on the charger action in~\Cref{sec:RL}.
% % \textbf{Environment updates and transitions.} The input episodes dictate the simulator's world view. Each event includes an event type and time, matching their real world trigger and occurrence. We identify several critical events in the episodes to serve as the triggers for the simulator. These include (1) EV arrivals, (2) EV departures, (3) building power readings, and (4) TOU rate changes. Events are placed in a queue, with each event triggering an update to the environment which modifies the state. Updates to the state, which include the charging or discharging of EVs, are based on the elapsed time between events. At each decision epoch, there are two decisions to be made, charger assignment and charger actions.
% % % \input{results/charger_assignments} 
% % \textbf{Charger assignment.} 
% We consider a first-in, first-out policy that assigns EVs to bidirectional chargers first, breaking ties assigning to later departing cars, \textit{a comparison of other approaches are available in the appendix}. 
% % %shows the different charger assignment and tie breaking policies tested.  
% % By comparing various charger assignment and tie-breaking policies (as shown in \Cref{table:charger_assignment_policies} in the Appendix), we observe that bidirectional charging assignments outperform all other policies. Tie-breaking strategies that prioritize later-departing vehicles show a marginal advantage. While these assignment policies could be further optimized, we chose to follow this heuristic and focus on the second decision problem: determining charger actions.
% %
% % \textbf{Charger actions.} 
% % We provide several policies with our simulator to contrast and compare with our proposed approach. 
% Charger action policies receive a state of the environment for a particular time and generate actions based on this. 
% % The simulator is stateless. Thus, it provides only a current representation of the world at that specific time to each policy. 
% We expand this idea and model the problem as an MDP.

% % \jpnote{Explain again that there are two decisions to be made; charger assignment and charging rates; but we use a heuristic to solve the charger assignment; we show quick results across the months in table 2 and we pick bidrection first and departure as our basic policy. Connect to next subsection on MDP}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.68\linewidth}
        % \centering
        \includegraphics[height=2.1in,keepaspectratio,trim=0.2cm 0.2cm 0.2cm 0.2cm,clip]{figures/framework.pdf}
        % \vspace{-0.1in}
        \caption{Reinforcement Learning Framework.}
       \label{fig:framework}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.3\linewidth}
        % \centering
        \includegraphics[height=2.1in,keepaspectratio,trim=0cm 0.5cm 0cm 0.5cm,clip]{figures/inference_pipeline.pdf}
        \caption{Pipeline for Inference.}
        \label{fig:pipeline}
    \end{subfigure}
    \caption{\color{black}(a) Our framework relies on daily samples and an estimated monthly peak power. We use RL, i.e., DDPG, and extend it with policy guidance and action masking, to learn a near-optimal policy. (b) At inference time, the model ingests data of connected cars, charger states, building power, and the estimated monthly peak power to make decisions.}
    \label{fig:framework_and_pipeline}
\end{figure*}

\subsection{Markov Decision Process Model}
\label{ssec:MDP}

We model the V2B problem as the following MDP. 

{\bf State.}
The complete state space for the problem can be described using features that capture historical, current, and future estimation at a given time $T_j$, which includes parameters for each vehicle, such as the current SoC, required SoC, departure time, and battery capacity for each EV, along with SoC boundaries across all chargers. Additionally, the current building power, time slot, day of the week, historical building power, and long-term peak power estimation value are included, resulting in approximately $100$ features. 
% While this state space is complete, it is not tractable to be used for the learning process. Therefore, 
We leverage domain-specific knowledge to abstract key information from these features, reducing the state space to the $37$ essential state elements.

These features are:
\textbf{1)} The current time slot, $T_j$. \textbf{2)} The current building power, denoted as ${B}(T_j)$. \textbf{3)} The power gap between the current building power and the estimated peak power for the billing period, given by $ \PrdPeak(T_j) - B(T_j)$, where $\PrdPeak(T_j)$ indicates the estimated peak power at $T_j$, initialized from a value derived from training data. This gap aids the RL model in estimating the optimal peak power for demand charge reduction. \textbf{4)} The mean peak building power over the previous 7 days, $\mu(B^H(T_j))$, where $B^H(T_j)$ represents the list of peak building power for the previous 7 days. \textbf{5)} {The variance of the peak building power over the previous 7 days, $\sigma^2(B^H(T_j))$, helps inform the model about the future building power use}. \textbf{6)} The day of the week for the current time slot, $T_j$, which helps the model distinguish daily patterns and enhance generalization. \textbf{7)} The number of EV arrivals up to time slot $T_j$, represented as $|\{V | V \in \mathcal{V}, A(V) \leq T_j \}|$ for tracking EV arrival status. \textbf{8)} The energy needed by each EV connected to a charger at time slot $T_j$, given by $[\PowerNeed(C_i, T_j)]_{C_i \in \mathcal{C}}$, which is initialized to $0$. This quantity represents the energy gap between required SoC ($\SOCR$) and current SoC ($\it{SOC}$) of the EV $V = \phi(C_i, T_j)$, defined as $\PowerNeed(C_i, T_j) = (\SOCR(V) - \SOC(V, T_j)) \times \text{CAP}(V)$.
    % \begin{equation} 
    %  \PowerNeed(C_i, T_j) = (\SOCR(V) - \SOC(V, T_j)) \times \text{CAP}(V)
    %  \label{eq:chargerstate}
    % \end{equation}
    % where $V=\phi(C_i, T_j)$ indicating the EV connected to $C_i$ at $T_j$. 
\textbf{9)} The remaining time until the departure of each EV connected to the chargers is given by $[\ReTime(C_i, T_j)]_{C_i \in \mathcal{C}}$, and is set to 0 when no cars are connected. Each term is computed as $\ReTime(C_i, T_j) = \DepartureTime(\phi(C_i, T_j)) - T_j$. 

% \begin{enumerate}[leftmargin=*]
%     \item The current time slot, $T_j$.
%     \item The current building power, denoted as ${B}(T_j)$.
%     \item The power gap between the current building power and the estimated peak power for the billing period, given by $ \PrdPeak(T_j) - B(T_j)$, where $\PrdPeak(T_j)$ indicates the estimated peak power at $T_j$, initialized from a value derived from training data. This gap aids the RL model in estimating the optimal peak power for demand charge reduction.
%     \item The mean peak building power over the previous 7 days, $\mu(B^H(T_j))$, where $B^H(T_j)$ represents the list of peak building powers from the previous 7 days. 
%     \item The variance of the peak building power over the previous 7 days, $\sigma^2(B^H(T_j))$. It informs the model about the future building power.  
%     \item The day of the week for the current time slot, $T_j$. It helps the model distinguish daily patterns and enhance generalization.
%     \item The number of EV arrivals up to time slot $T_j$, represented as $|\{V | V \in \mathcal{V}, A(V) \leq T_j \}|$ for tracking EV arrival status. 
%     \item The energy needed by each EV that is connected to a charger at time slot $T_j$, given by $[\PowerNeed(C_i, T_j)]_{C_i \in \mathcal{C}}$, and is initialized to $0$. This represents the energy gap between the required SoC and the current SoC of the EV connected to charger $C_i$ at time slot $T_j$, defined as: 
%     \begin{equation} 
%      \PowerNeed(C_i, T_j) = (\SOCR(V) - \SOC(V, T_j)) \times \text{CAP}(V)
%      \label{eq:chargerstate}
%     \end{equation}
%     where $V=\phi(C_i, T_j)$ indicating the EV connected to $C_i$ at $T_j$. 
%     \item The remaining time until the departure of each EV connected to the chargers is given by $[\ReTime(C_i, T_j)]_{C_i \in \mathcal{C}}$, and is set to 0 when no cars are connected. Each term is computed as $\ReTime(C_i, T_j) = \DepartureTime(\phi(C_i, T_j)) - T_j$. 
% \end{enumerate}

{\bf Actions.} We define the set of actions $\mathcal{A}$, which includes all actions at each time slot $T_j$ with $T_j \in \mathcal{T}$. In this MDP, $\mathcal{A}$ is continuous and specifies the power of all chargers at each time slot $T_j$, where $A(T_j) = [P(C_i, T_j)]_{C_i \in \mathcal{C}} $.
 

{\bf State Transition.} 
%The states of the building and chargers are updated based on actions taken at each time slot. 
% Building and charger states are updated based on actions and EV arrivals/departures at each time slot. To simulate state transitions, we designed an environment simulator that provides state features and manages these transitions. 
States are updated based on actions and EV arrivals/departures at each time slot. To simulate these transitions, we designed an environment simulator that provides and updates states. The state transition function is given as:
${\it Trans}(S(T_{j-1})$, $A(T_{j-1})) \mapsto S(T_j)$, with the following steps: 
\begin{enumerate}[leftmargin=*]
    \item Initialize the estimated peak power, $\PrdPeak(T_0)$, which can be derived from historical data 
    %\ad{does not make sense. We need to clearly describe the process of how this is estimated for training as well as inference. Please update. There is no mention in section 4.2} 
    (detailed in ~\Cref{sec:our_approach})
    , and update it by
    $
    \PrdPeak(T_{j}) = \max(\PrdPeak(T_{j-1})$, $ \Building(T_{j-1}) + \sum_{C_i \in \mathcal{C}} P(C_i, T_{j-1})),
    $
    which updates the estimated peak power depending on the previous estimate and the last peak power.
    \item Update SoC of EVs connected to all chargers: $\it{SOC}(\phi(C_i,T_{j}), T_{j})$ using action $A(T_{j-1})$ according to Equation~(\ref{eq: soc}). 
   \item Update the EV charger assignment $\phi(C_i, T_j)$ and $\eta(V)$ by first releasing chargers with departing EVs in the current time slot $T_j$ and then assigning new arrival EVs to idle chargers. 
   \item Update the energy requirement of all EVs connected to a charger: $[\PowerNeed(C_i, T_j)]_{C_i \in \mathcal{C}}$ by based on EV's current SoCs.
   \item Update the remaining time of all EVs connected to chargers: $[\ReTime(C_i, T_{j})]_{C_i \in \mathcal{C}}$ at time slot $T_{j}$.   
\end{enumerate}


{\bf Reward.} We define the function ${\it Reward}: \mathcal{S} \times \mathcal{A} \rightarrow \Re$, where ${\it Reward}(S(T_j), A(T_j))$ evaluates the reward for actions taken in a specific state, focusing on minimizing the total bill while satisfying SoC requirements. We express reward as $\lambda_{S} \cdot \mathit{r}_1 + \lambda_{E} \cdot \mathit{r}_2 + \lambda_{D} \cdot \mathit{r}_3$ where $\mathit{r}_1 =  \sum_{C_i\in\mathcal{C}} \max(0, \min(\PowerNeed(C_i, T_j), P(C_i, T_j) \times \delta))$, $\mathit{r}_2 = - P(C_i, T_j) \cdot \delta  \cdot \theta_E(T_j)$, and $\mathit{r}_3 = - \max(0, \Building(T_j) + \sum_{C_i \in \mathcal{C}} P(C_i, T_j) - \PrdPeak(T_j)) \cdot \theta_D$
% \begin{align*}
%    & \mathit{Reward}(S(T_j), A(T_j)) = \lambda_{S} \cdot \mathit{r}_1 + \lambda_{E} \cdot \mathit{r}_2 + \lambda_{D} \cdot \mathit{r}_3
% \end{align*}
.
% \begin{align*}
% \begin{aligned}
%     \mathit{r}_1 &=  \sum_{C_i\in\mathcal{C}} \max(0, \min(\PowerNeed(C_i, T_j), P(C_i, T_j) \cdot \delta)) \\
%     \mathit{r}_2 &= - P(C_i, T_j) \cdot \delta  \cdot \theta_E(T_j) \\
%     \mathit{r}_3 &= - \max(0, \Building(T_j) + \sum\limits_{C_i \in \mathcal{C}} P(C_i, T_j) - \PrdPeak(T_j)) \cdot \theta_D
% \end{aligned}
% \end{align*}
In this reward structure, $\mathit{r}_1$ promotes actions that charge EVs to reach their required SoC, as intended in Equation~(\ref{eq: soc_penalty}), while $\mathit{r}_2$ penalizes the energy cost for the charging actions taken. The third component, $\mathit{r}_3$, penalizes the increase in demand charges if peak power increases, aligning with our objective in Eq. (\ref{eq: billing}). These functions use three  coefficients, $\lambda_{S}$, $\lambda_{E}$, and $\lambda_{D}$ to balance trade-offs.
%between these reward factors.

% \subsection{MILP Solver}
% \label{subsec:milpsolver}



% When solving for a month, it has, on average, 20000 variables and 32000 constraints and takes 3.7 seconds to solve. However, the MILP solution is limited by its requirement to know the future completely, not accounting for any stochasticity in the future.



% We implement an optimization framework to generate optimal actions and use it to provide effective guidance to steer the search toward the global optimum. To get the optimal actions, we formulate the V2B problem using mixed-integer linear programming (MILP) and refer to the MILP framework as $\mathit{MILP(S(T_j), events_{future})}$. Here, $S(T_j)$ is the current state information, including the status of connected EVs, building's power, and $events_{future}$ covers all the future events from the input sample, which are EV arrivals and departures, building's power consumption, and electricity prices from the current time to the end of the billing period. The MILP solution provides the optimal power for each charger from the current time to the end of the billing period, maximizing the multi-objective weighted sum of the total cost (detailed in Equation~(\ref{eq: billing})) and penalties for missing SoC requirements (defined in Equation~(\ref{eq: soc_penalty})). 
% % It is designed to follow the multiple constraints related to the EV SoC update function following Equation~(\ref{eq: soc}) - (\ref{eq:building_power}).   

% As we discussed earlier, our problem deals with futures for up to a month, while some subproblems may need to be solved only for a day. 


 

% \input{algorithms/overall_algo1}
% \input{algorithms/actionmasking}

\subsection{Reinforcement Learning Approach}
\label{sec:RL}
In this section, we describe the entire reinforcement learning pipeline. We introduce the network structure, discuss how we use a simulator to gather state features and describe the different techniques, such as action masking and policy guidance, used to improve the performance of the V2B problem.  

To improve training efficiency, we address the challenge of long state-action sequences by splitting the monthly dataset into daily episodes. This allows the model to capture variations across different weekdays and learn more effectively from shorter episodes, adapting more quickly to daily changes. By incorporating estimated monthly peak power into the state features and reward function, the approach still accounts for monthly demand charges, helping to minimize long-term costs while staying aligned with our objective. 
% - make sure key hyperparameters are known.
% - make sure the confidence bound is explained as a hyperparameter.
% \jpnote{Will have to copy paste sections from~\Cref{sec:RL}. Talk about how the previous components are used to create our policy/model.}

%which is well-suited for continuous action spaces and supports off-policy training, allowing the model to learn from diverse experiences across various scenarios, thereby improving generalization.
%
\subsubsection{Enhanced Deep Deterministic Policy Gradient}
Our approach based on the DDPG framework~\cite{lillicrap2015continuous} uses an actor network for continuous actions.
During training, we interact with the simulator that provides state abstractions and transitions.
To improve RL performance in handling the limitations associated with large continuous action spaces and long-term reward optimization, we introduce action masking and policy guidance techniques. Details of the enhanced approach are in Algorithm~\ref{alg:DDPG} in the appendix. 
Action masking, denoted as $\Mask(S(T_j), A(T_j))$, refines the raw actions generated by the actor network by enforcing action validity and utilizing domain-specific knowledge, thereby improving policy performance. Additionally, policy guidance incorporates the MILP solver discussed earlier to provide optimal actions based on current and future information. These optimal actions are stochastically introduced during RL training into the replay buffer (i.e., tossing a biased coin) to mix high-quality actions given a deterministic trajectory with exploratory actions).  


\subsubsection{Action Masking}
Action masking ensures that the policy actions generated by the actor network are feasible during DDPG training. Findings from \cite{huang2020closer,kanervisto2020action} confirm that differentiable action masking does not interfere with the policy gradient backpropagation process. As a result, the learning process remains effective, while the imposed constraints on the action space prevent the policy from exploring invalid actions, thereby improving training efficiency and optimizing resource usage. 
\input{algorithms/actionmasking}

This procedure takes the RL raw action $A(T_j)$, an array of charging power $[P(C_i, T_j)]_{C_i\in\mathcal{C}}$ for all chargers, processes it through the following masking steps, and outputs the masked actions $A'$. Before starting the procedure, we need to obtain the following state features: the remaining power needed to reach the required SoC for all connected EVs ($\PowerNeed$), the time remaining for each EV ($\ReTime$), and the maximum ($C^{\max}$) and minimum ($C^{\min}$) power of all chargers (line 1 in Algorithm~\ref{alg: action_masking}). Also, for our case, since we work with both unidirectional and bidirectional, we denote ${\it uniIdx}$ and ${\it biIdx}$ as the indices for unidirectional and bidirectional chargers, respectively. All of the masking techniques referenced below are from Algorithm~\ref{alg: action_masking}.
% \jpnote{Shorten the descriptions and make it very concise.} 
\begin{itemize}[leftmargin=*]
    \item \textbf{Mask 1.} 
    We set the charging power $P(C_i, T_j)$ of charger $C_i$ to 0 if no EV is connected, i.e., $\ReTime(\phi(C_i, T_j))=0$. (line 2)
    %%%
     \item \textbf{Mask 2.} Overcharging unidirectional chargers is not beneficial since excess energy cannot be discharged. Thus, we limit the charging power to ensure the SoC of EVs connected to a unidirectional charger remains within their required SoC. 
    For each connected EV, the actions are masked to the minimum of the current charging power and the power needed to reach its required SoC $\left(\frac{\PowerNeed}{\delta}\right)$ (line 3). 
     
    \item \textbf{Mask 3.} 
    If necessary, we want to adjust actions such that it forces charging to the required SoC before departure to minimize missing SoC, as in Equation~(\ref{eq: soc_penalty}).
    We compute the critical power $\overline{\Power^*(T_j)}$, which is the minimum power required for all chargers at time $T_j$ to reach the required SoC of the connected EVs before departing (assuming maximum power $C^{max}$ is utilized in subsequent time slots). The raw action is adjusted if it falls below this value, especially in time slots leading up to the EV's departure (line 4).    
    \item \textbf{Mask 4.} 
    This mask is symmetrical to Mask 3 for force discharging.
    Overcharging bidirectional EVs is only advantageous if excess energy can be discharged during peak hours, but there is no benefit to overcharging just before departure. Using this mask, we force discharge EVs connected to bidirectional chargers, which have excess energy, and they reach the required SoC by departure.  Here, $\Power^*(T_j)$ denotes the minimum power to discharge for all chargers $C_i \in \mathcal{C}$ at time $T_j$ to guarantee EV can reduce to required SoC when departing (assuming the maximum discharging power $C^{min}$ is utilized subsequently) (lines 5, 6).    
    %%%
    \item \textbf{Mask 5.} 
    We increase charging power while ensuring the masked action stays within the estimated peak power $\PrdPeak(T_{j})$. This aims to charge EVs as much as possible towards their required SoC without raising demand charges, thereby avoiding forced charging just before departure, which could elevate peak power. 
    We calculate the ``power gap'' between estimated peak power and current building power, $\PrdPeak(T_j) - \Building(T_j)$. If the current power sum ($\Building(T_{j-1}) + \sum_{C_i \in \mathcal{C}} P(C_i, T_{j-1})$) is below this ``power gap'', we boost the current actions using the available ``power'' gap, constrained by $\min \left(\frac{\PowerNeed}{\delta}, C^{max}\right)$. (lines 7 to 9). 
    %%%
    \item \textbf{Mask 6.} We adjust the discharging power to prevent cumulatively discharging below the current building power $\Building(T_j)$, to satisfy Constraint~\ref{eq:building_power} by reducing the discharging power based on the current actions (lines 10 to 11).
\end{itemize} 
All of the action masking procedures utilize array computations and differentiable operations, such as ReLU \cite{rasamoelina2020review} and maximum/minimum operations, and the PyTorch framework~\cite{paszke2017automatic}. 

\subsubsection{Policy Guidance with MILP Solver}
Note that for a fixed sample, i.e., a fixed set of EV arrivals and departures, the V2B problem can be modeled as a single-shot mathematical program, i.e., a mixed-integer linear program (MILP), which can solved efficiently (at least, for our problem size) to retrieve the optimal actions. The objective of the MILP is maximizing the multi-objective weighted sum of the total rewards (detailed in Equations~\ref{eq: billing}, \eqref{eq: soc_penalty}), and the other properties of the V2B problem can be encoded as constraints. The fixed sample of arrivals and departures can be extracted from historical data. Naturally, this modeling paradigm does not solve the V2B problem in general---EV arrivals and departures are not known ahead of time---however, this strategy provides a set of optimal actions that the learning module can \textit{learn to imitate}. For our use case, the MILP problem can be solved reasonably fast. For example, for a planning horizon of a day with 15 cars, the problem size averages 800 variables and 1400 constraints and takes $0.05$ seconds to solve. 

We integrate a MILP solver based on CPLEX~\cite{cplex2009v12} as a policy guidance subroutine~\cite{pmlr-v28-levine13} in the RL training process. The solver, given the current state and future events, provides optimal charging actions.
{\color{black} Each training dataset contains complete episode data, enabling the MILP solver to account for future dynamics. During RL training, it generates optimal actions based on the current state and full future information of the episode (i.e., a full-month billing period).} The solver is stochastically triggered, and its outputs are added to the replay buffer with a predefined coefficient, $\policyGuidanceRate$ (see Algorithm~\ref{alg:DDPG} in the appendix). The next optimal action is computed as $\mathit{MILP(S(T_j), {\it remainEpisode})}$, considering factors such as EV arrivals, SoC requirements, and building power.
By blending MILP-generated actions with those from the RL actor network, the agent explores a more effective action space, improving its ability to handle large continuous action spaces and long-term rewards.% During training, the MILP solver is stochastically triggered to generate optimal actions based on the current state. These state-action transitions are added to the replay buffer with a predefined coefficient, $\policyGuidanceRate$ (see Algorithm~\ref{alg:DDPG} in the appendix). The next optimal action is obtained using $\mathit{MILP(S(T_j), {\it remainEpisode})}$, which accounts for remaining events like EV arrivals, SoC requirements, and building power. By blending MILP and RL actor network actions during training, the RL agent explores a more effective action space, improving performance in handling large continuous action spaces and maximizing long-term reward.

% \jpnote{Confirm with rishav that MILP above, talks about commented lines below}
%We add the optimal actions to the replay buffer, providing effective guidance to steer the search towards global optima. 
%
%
% We give the MILP solver the current state information, including the current EV status, charge usage, and all future events from the input sample (EV arrival/departure, building power flow, and electricity prices from the current time to the end of the billing period. The MILP solution provides the charging power for each charger from the current time to the end of the billing period, maximizing the multi-objective weighted sum of total cost (detailed in Equation~(\ref{eq: billing})) and penalties for missing SoC requirements (defined in Equation~(\ref{eq: soc_penalty})). Following multiple constraints related to the EV SoC update  function following Equation~(\ref{eq: soc}) and Constraints~(\ref{eq:charging_rate}) to (\ref{eq:building_power}). 
%
% During training, optimal actions are stochastically incorporated into the state transition process based on a predefined coefficient, denoted as $\policyGuidanceRate$, and stored in the replay buffer (see Algorithm~\ref{alg:DDPG}). We use $\mathit{MILP(S(T_j), {\it remainEpisode})}$ to get the next optimal action. Here, {\it remainEpisode} considers the remaining events in the fixed sample, such as upcoming EV arrivals, SoC requirements, and building power, to invoke the MILP solver. Using the MILP solver helps us maximize the long-term reward. 

\subsubsection{Actor-Critic Network Structure} 
Both the actor and critic networks are fully connected, having two hidden layers with 96 neurons each. Both feature a ReLU activation layer at the end. The critic network outputs a single Q-value estimate, while the actor network outputs the action, which represents the charging power of each charger.
%
To enhance convergence and improve generalization, we normalize all state variables to be within $[0, 1]$ before feeding them into neural networks. Time slot $T_j$ is normalized by division with the number of time slots in a day ($\frac{24}{\delta}$), while power-related variables such as building power $\Building(T_j)$, estimated peak power $\PrdPeak(T_j)$ are scaled by their respective statistical values from training data. Furthermore, we normalize the energy capacity $CAP(V)$ of each car by division with the maximum capacity among EVs, $\max(CAP(V))$.
For the action $A(T_j)=[P(C_i, T_j)]_{C^{i}\in \mathcal{C}}$, we constrain the output within the range $[-1, 1]$ using the $\tanh$ activation function. It is finally translated into the 
charging power range $[C_i^{min}, C_i^{max}]$ by scaling the value using a constant factor.

%We normalize the action values to the range of $[-1, 1]$ based on the charging power range $[C_i^{min}, C_i^{max}]$.
% by: 
% \begin{equation}
%     \hat{P}(C_i, T_j)=\frac{2\times(P(C_i, T_j)-C_i^{min})}{C_i^{max} - C_i^{min}}-1. 
% \label{eq: normalize}
% \end{equation} 
%The output of the actor network is constrained within the range $[-1, 1]$ using a $\tanh$ activation function, from which the unnormalized charging values are retrieved.
% The original charging power values can be obtained by computing the inverse of the normalization equation in Equation~(\ref{eq: normalize}). 

\subsubsection{Heuristics and Action Post Processing}

% for tractability the RL model focused on weekday and peak- billing period. For others including off-peaks and weekend we use heuristics. Here are the list of heuristics we use.  We will show later the results comparing to using heuristics for all periods as well.
% For tractability, the RL model focuses only on weekdays and peak-time billing periods. For off-peaks and weekends we use heuristics. We observe that off-peak hours offer lower electricity prices enabling EV charging at higher charging powers, also off-peak hours are not considered in the calculation of demand charge. Thus, heuristics can be used to optimize EV charging during these periods. Similarly, weekends often experience far fewer users than weekdays leading to low building power demand. Also, TSOs do not consider in the demand charge calculation, leading to minimal opportunities for optimization. During RL training, we use a charge first heuristic based on a least laxity task scheduling algorithm (Charge First LLF described in Section~\Cref{ssec:alternatives}) during off-peak hours and weekends that guarantees that EVs reach the required SoC before departure time. 

To enhance the ease of learning in this complex decision space, we use the RL model on weekdays and the peak hours of TOU price within each billing period (for both training and inference). For off-peak hours and weekends, we use a heuristic based on the least laxity task scheduling algorithm (described in \Cref{sec:experiments_and_results}) to ensure EVs achieve the required SoC before departure, calculating the minimum charge needed for each time slot. Off-peak hours offer lower electricity prices, allowing for higher EV charging rates, and are excluded from demand charge calculations, making heuristics effective for optimization. Similarly, weekends see fewer EV arrivals and lower power demand, with Transmission System Operators excluding them from demand charge assessments. 
% During training (and inference), we use 
% \jpnote{Cite paper for this}
% Finally, we condense the state features while accounting for the common minimum and maximum SoC boundaries of all EVs (with $\SOCMIN=0$ and $\SOCMAX=90\%$), we do not include SoC boundaries in the state representation, limiting the policy's ability to control actions. Thus, to maintain valid SoC boundaries, we apply a post-processing procedure~\cite{XXXX}, which differs from action masking. Post-processing is not integrated with the actor network and influenced by training backpropagation. This adjusts policy-generated actions before they are given to the environment, ensuring that charging EVs do not exceed $SoC^{\text{max}}$ and discharging prevent it to go below $SoC^{\text{min}}$. By employing this approach, we ensure that all policy-generated charging powers for charging EVs remain within the defined SoC boundaries, thereby satisfying Constraints~(\ref{eq:soc_min}) and~(\ref{eq:soc_max}).  
% Finally, we condense the state features while accounting for the common minimum and maximum SoC boundaries of all EVs, following the guidelines set by the EV manufacturer. To avoid limiting the policy's flexibility by including SoC boundaries in the state representation, we apply a post-processing procedure. This is a common approach in RL, utilized in AlphaGo~\cite{silver2016mastering}, where illegal actions are filtered before interacting with the environment to save training resources that would otherwise be spent on learning valid actions, which is not the final objective. Similarly, we adjust policy-generated actions before passing them to the environment, ensuring that EVs' SOC stays within the limits. Specifically, we stop discharging if the EV's SoC is below $SoC^{\text{min}}$, and stop charging if it exceeds $SoC^{\text{max}}$, satisfying Constraints~(\ref{eq:soc_min}) and~(\ref{eq:soc_max}). 
% Following the guidelines set by the EV manufacturer, we need to limit charging EVs to their SoC boundaries. We apply a post-processing procedure, a common approach in RL, utilized in AlphaGo~\cite{silver2016mastering}, where actions are adjusted to avoid illegal actions before interacting with the environment. This saves training resources that would otherwise be spent on learning valid actions, which is not the final objective. Similarly, considering the exceeding SoC boundary rarely happens during training and learning the validity of matining SoC limit is not our final objective which is to minimize total bill,  we adjust policy-generated actions before passing them to the environment, ensuring that the EVs' SoC stays within the specified limits. Specifically, we stop discharging if the EV's SoC is below $SoC^{\text{min}}$, and stop charging if it exceeds $SoC^{\text{max}}$, thereby satisfying Constraints~(\ref{eq:soc_min}) and~(\ref{eq:soc_max}).  
Following the EV manufacturer guidelines, we limit charging to SoC boundaries by clipping the actions of the learned policy within $[SoC^{\text{min}}, SoC^{\text{max}}]$ through post-processing to satisfy Constraints~(\ref{eq:soc_min}) and~(\ref{eq:soc_max})
% In this work, we assume all EVs share common minimum and maximum SoC limits. A post-processing procedure, similar to AlphaGo~\cite{silver2016mastering}, adjusts actions to prevent illegal steps during environmental interaction, conserving training resources. Since exceeding SoC boundaries is rare and maintaining SoC limits is not our main goal, which is minimizing the total bill, we adjust policy-generated actions after they are passed to the environment simulator. Specifically, we stop discharging if SoC is outside $[SoC^{\text{min}},SoC^{\text{max}}]$.

\subsection{Inference}
During execution, our RL-based policy, which is a trained actor network with the action masking procedure, operates at $\delta$ time intervals to determine the charging power for all chargers. At each time slot, the state features are generated from data captured from the environment, including charger status (connected EV's current SoC, expected departure time, and SoC), the building's current power and charging rate limits.
% , which are obtained through the simulator interface. 
% Additionally, we apply the heuristic approach: Fast Charge LLF (detailed in \Cref{ssec:results}) during off-peak hours and weekends. 
While we use the estimated peak power $\hat{P}^{max}$ as the state feature based on training samples, as shown in~\Cref{fig:pipeline}, it can be replaced by any data-driven forecasting or prediction model. Then, we input all the normalized state features, as described in~\Cref{ssec:MDP}, into the trained RL model to get the charging actions for the next time interval.

%{which can be derived from historical data or machine learning-based predictive models}.


% \subsection{Inference in Real-time}
% \jpnote{Write this in terms of how this will be used in the real world. Reference the digital twin.}
% write about inference diagram from workflow. we use the digital twin to track what the real-system state. We take a full chain of a month; we use the best heuristic for non-peak and weekend and for weekday and peak period we use RL policy. 

