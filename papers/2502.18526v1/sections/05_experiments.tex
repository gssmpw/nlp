\section{Experiments and Analysis}
\label{sec:experiments_and_results}
To demonstrate the performance of our proposed approach, we use data collected from our \nissan{}'s research laboratory. We evaluate our approach against several baselines in terms of total bill and peak shaving (demand charge savings).


% \input{results/car_arrivals}
% \input{results/car_arrival_departure_hours_distribution}
% \input{results/building_weekday_peaks}

\noindent \textbf{Data Collection}
% \label{ssec:data}
We collected real-world data from \nissan{}'s research laboratory in Santa Clara, California, including building power, EV charger usage, and EV telemetry, over a nine-month period from May 2023 to January 2024. To model the distributions of EV arrivals, SoC requirements, and building power fluctuations, we used Poisson distribution based on historical data. Characteristics of the datasets are shown in~\Cref{ss:appendix_figures}.
% The number of EVs arriving at the office on weekdays is illustrated in~\Cref{fig:arrival_counts} in the Appendix, highlighting the inherent uncertainties of the use case. Arrival hours relative to SoC and departure hours concerning required SoC are shown in~\Cref{fig:car_distributions}. The EVs have an equal probability of being one of two types, differing only in battery capacity (40 kWh or 62 kWh). Additionally,~\Cref{fig:car_distributions} presents the distribution of peak power draw and the corresponding hour.
% The main environment parameters are provided in Table~\ref{tab:simulation_parameters} in the Appendix.
% We sampled 1000 billing episodes for each month in May 2023 to January 2024.  
{\color{black} The number of EVs arriving at the office on weekdays varies daily, illustrating the inherent uncertainties. Arrival and departure hours relative to SoC are depicted in~\Cref{fig:car_distributions} in the appendix, which also presents the distribution of peak power draw and corresponding hours. Main environment parameters are provided in Table~\ref{tab:simulation_parameters} (appendix).} We sampled 1000 billing episodes for each month. 

% The number of EVs arriving to the office per weekday is shown in~\Cref{fig:arrival_counts} in the Appendix. The samples inherently capture the uncertainties present in the use-case. The distribution of arrival hours compared to arrival SoC and departure hours compared to required SoC at departure are shown in~\Cref{fig:car_distributions} in the Appendix. The EVs have an equal probability of being one of two vehicle types. The only applicable difference between the two vehicle types is the battery capacity, which can be either 40 kWh or 62 kWh. Finally, the distribution of peak power draw and the corresponding hour is shown in~\Cref{fig:car_distributions} in the Appendix. These traces are used as training and testing samples for the RL training.
% We illustrate the main characteristics of the training and testing data for each month in~\Cref{tab:training_testing_data} in the Appendix, which 
% including information about daily car arrivals, building power distribution, estimated peak power during training, and the weekdays used for training. The simulation parameters are provided in Table~\ref{tab:simulation_parameters} in the Appendix. 
% }
% To assess our RL-based V2B charging optimization approach, we used real-world data collected {\color{black} from a \nissan{} office building, along with charger meter readings, in Santa Clara, California.} This data was used to derive statistics and develop generative models for generating samples for training and testing the RL model, which also serves as input to our simulator. We outline all inputs as follows: {\color{red} Need modified to the old dataset?}
% \begin{itemize}[leftmargin=*]
%     \item \textbf{building power}: Real building power data from the \nissan{} office spanning {\color{black} May 2023 to January 2024 }was collected. These readings were logged at approximately 15-minute intervals. To introduce variability, we applied random noise to the real data at each time interval. Specifically, we added noise within the range of $(-5\%, 5\%)$ to each real building power at each time interval. 
%     \item \textbf{Car Arrival/Departure Time}:  {\color{black} Charging session data was collected from the \nissan{} office building's parking lot from January to December 2022, which included the starting time, ending time, starting SoC, and ending SoC for each session. To generate arrival count predictions for each hour, a Poisson regression model was fit to the starting times. The regressor was conditioned on factors including time (the hour, month, day of week) and weather (precipitation, temperature). The fit regressor has a Log-Likelihood of {\color{red} -860.03}. \ava{How are hourly-counts turned into specific arrival times?} {\color{red} Hourly-counts are converted to specific arrival times by sampling a minute within the hour uniformly at random.}
%     To generate stay durations for each session, a random forest regression model was trained using the same set of features. The model was trained using a randomized 80\% of the session data, with the other 20\% used as a hold-out validation set. The trained model has an RMSE of {\color{red} 149.70 minutes}.}. 
%     \ava{TODO for Ava: condense the model descriptions and move some information to appendix}
%     \item \textbf{Car Arrival and Required SoC at Departure:} Arrival and departure SoC models were trained using a random forest regression model. The model was trained on several temporal features such as month, day of week, and hour of day, as well as the car's battery capacity and current weather. Similarly, a required SoC upon departure model was trained with additional features to capture the state at the time of departure including hour of day, weather at departure. 
%     % \item \textbf{Electricity price and Demand price}: Peak hours, defined as 6 a.m. to 10 p.m. daily, determined the power price, while the demand charge price utilized real-world prices provided by \nissan{}. 
%    % Below is the new model description
%     % \item \textbf{Car Arrival and Required SoC at Departure:} From the charging session data collected above, a Beta Model was fit to the EV SoC upon arrival. The fit regressor has a Log-Likelihood of {\color{red} 128.40}. The model was conditioned on hour of day and day of week. A required SoC upon departure model was trained in the same manner with the addition of the arrival SoC as a feature. The fit regressor has a Log-Likelihood of {\color{red} 909.26}.
% \end{itemize}
% % \input{results/simulation_parameter} 
% \begin{table*}[htp]
% \centering
% \small 
% \caption{Training and testing data information for each month.} 
% \begin{tabular}{|p{1.0cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|p{1.0cm}|p{1.0cm}|p{1.5cm}|p{1.7cm}|p{1.7cm}|}
% \hline
% \multirow{2}{*}{\textbf{Month}} & \multicolumn{5}{p{3.75cm}|}{\textbf{Training (60 Samples)}} & \multicolumn{3}{p{3.75cm}|}{\textbf{Testing (50 Samples)}} \\ 
% \cline{2-9} & {Car Arrival Number (per day)} & {Monthly Peak building power (kW)} & {Daily Peak building power (kW)} & {estimated peak power (kW)} & {Number of Weekdays} & {Car Arrival Number (per day)} & {Monthly Peak building power (kW)} & {Daily Peak building power (kW)} \\ \hline 
% \textbf{MAY} & $9.35\pm2.19$ & $125.89\pm1.72$ & $96.22\pm13.76$ & 119 & 22 & $9.31\pm2.2$ & $125.62\pm1.65$ &$96.18\pm13.73$ \\ \hline

% \textbf{JUN} & $11.4\pm2.53$ & $141.04\pm1.83$ & $109.86\pm14.0$ & 125 & 21 & $11.49\pm2.51$ & $140.34\pm2.21$ &$109.75\pm13.97$\\ \hline

% \textbf{JUL} & $10.86\pm2.45$ & $148.08\pm1.57$ & $111.79\pm35.39$ & 145 & 20 & $10.97\pm2.4$ & $148.12\pm1.72$ &$111.94\pm35.49$\\ \hline

% \textbf{AUG} & $6.87\pm1.97$ & $221.02\pm3.86$ & $160.5\pm27.92$ & 202 & 23 & $6.92\pm1.93$ & $221.17\pm3.77$ &$160.69\pm28.14$\\ \hline

% \textbf{SEP} & $14.99\pm2.9$ & $145.91\pm1.48$ & $127.2\pm11.86$ & 143 & 20 & $14.94\pm2.88$ & $146.23\pm1.08$ &$127.2\pm11.85$\\ \hline

% \textbf{OCT} & $14.55\pm2.91$ & $186.54\pm1.72$ & $118.58\pm30.12$& 174 & 21 & $14.54\pm2.86$ &$185.17\pm2.77$ &$118.5\pm30.13$ \\ \hline 

% \textbf{NOV} & $9.59\pm2.31$ & $144.9\pm3.28$ & $113.09\pm19.18$ & 130 & 19 & $9.57\pm2.29$ & $144.2\pm2.92$&  $113.09\pm19.2$\\ \hline 

% \textbf{DEC} & $20.36\pm3.4$ & $116.49\pm2.25$ & $98.72\pm8.0$ & 104 & 16 & $20.32\pm3.35$ & $116.33\pm2.01$ &$98.69\pm7.95$\\ \hline 

% \textbf{JAN} & $13.62\pm2.73$ & $137.77\pm2.47$ & $91.56\pm19.46$ & 127 & 21 & $13.53\pm2.76$ & $137.64\pm2.5$ &$91.72\pm19.47$\\ \hline 
% \end{tabular}
% \label{tab:training_testing_data}
% \end{table*}   
% \begin{table}[h]
%     \centering
%     \small
%     \caption{Simulation Parameters.}
%      \label{tab:simulation_parameters}
%     \begin{tabular}{|p{1.3cm}|p{6.3cm}|}
%     \hline
%     \textbf{Parameter} & \textbf{Value} \\
%     \hline
%     $\mathcal{C}$ & 15 chargers (5 bi-directional, 10 uni-directional) \\\hline
%     ${\it C}^i_{min}, {\it C}^i_{max}$ & 
%     [-20 kW, 20 kW] for bi-directional, [0, 20 kW] for uni-directional chargers \\\hline
%     $\delta$ & Time interval: 0.25 hours \\\hline
%     % ${\it SOC}^i_{min}, {\it C}^i_{max}$ & 
%     % [-20 kW, 20 kW] for bi-directional, [0, 20 kW] for uni-directional chargers \\\hline
%      % $\mathcal{T}$, $\mathcal{T}_P$ & Total time slots in one month, time slots in peak hours (6 am to 10 pm) daily\\\hline 
%     $\theta_{D}, \theta_{E}(T_j)$ & 9.62 \$/kW (Demand), 0.11271 \$/kWh (off-peak), 0.1466 \$/kWh (peak) \\\hline
%     $CAP(V)$ & EV battery capacity: 40 or 62 kWh \\\hline
%     $\SOCMIN(V)$, $\SOCMAX(V)$ & Minimum and maximum SoC: 0\% and 90\% of capacity \\\hline
%     \end{tabular}
% \end{table} 
\noindent \textbf{Downsampling.} 
% The data was provided by \nissan{} from their fleet and chargers at their office building, covering the period from May 2023 to January 2024.
%This feature is further fine-tuned by adjusting it by 0\%, 5\%, and 10\% during RL training.  
% We varied the number of training samples and found that exceeding a certain limit increased computational demands and worsened performance (see ablation study  in~\Cref{ssec:ablation}). To mitigate this, we applied k-means clustering \cite{ikotun2023k} with \(k=5\) based on the optimal demand charge from the MILP solution, selecting 60 training samples and 50 testing samples per cluster to ensure exclusivity. 
% %This approach is further evaluated in the ablation study. 
% {\color{black} \Cref{tab:training_testing_data} in the appendix illustrates the key characteristics of the training and testing data, where datasets capturing variations in daily car arrivals, peak building loads, and estimated peak power demands. The average daily car arrivals range from 6.87 (August) to 20.36 (December), reflecting seasonal demand fluctuations. Similarly, monthly peak building loads vary significantly, from 116.49 kW (December) to 221.02 kW (August), illustrating the diverse energy consumption patterns that impact charging strategies.} 
We found that increasing training samples beyond a certain limit raised computational demands and worsened performance (see ablation study in~\Cref{ssec:ablation}). To address this, we applied \textit{k}-means clustering \cite{ikotun2023k} with \( k=5 \), using optimal demand charges from the MILP solution to select 60 training samples and 50 testing samples per cluster, ensuring exclusivity.   
{\color{black} As shown in \Cref{tab:training_testing_data} (appendix), the training and testing datasets span nine months, capturing variations in daily EV arrivals, peak building loads. Daily arrivals range from 6.87 (August) to 20.36 (December), reflecting seasonal demand shifts, while monthly peak building loads vary from 116.49 kW (December) to 221.02 kW (August), demonstrating diverse energy consumption patterns affecting charging strategies.}  
%We trained an RL model for each month using the selected samples, covering all weekdays, and evaluated performance using the corresponding testing samples.

\noindent \textbf{Estimated Peak Power.} To enhance training efficacy, we split the monthly dataset into daily episodes for the model to learn from varying weekday conditions. We include a monthly peak power estimate for each month as an input feature derived from optimal action sequences generated by the MILP solver, using the lower bound of the 99\% confidence interval from training data as a conservative demand charge estimate. This input feature is further tuned during RL training. 

\noindent \textbf{Hyperparameter Tuning.} 
% \label{ssec:generation}
% {\color{red} Add text here. Nothing is written in this subsection. I think this should discuss the break down of training and testing and validation splits. How models are evaluated etc...} 
%We trained an RL model for each month using 60 samples, covering all weekdays in each month. The model's performance was then evaluated using 50 testing samples. 
Hyperparameter tuning is performed on the parameters outlined in Table \ref{tab:hyperparameters} in the Appendix, which also shows the parameters of the best models selected for each of the nine months. 
To evaluate the model's performance, we employ a 3-fold cross-validation approach, dividing the 60 monthly training samples into 40 samples for training and 20 samples for evaluation. 

%\rishav{Do we justify somewhere why 50 and 60 respectively?} 
% Both the training and testing samples were generated based on the generative models described in Section~\ref{ssec:data}.  
%Table~\ref{tab:training_testing_data} illustrates the main characteristics of the training and testing data for each month, including information about daily car arrivals, building power distribution, estimated peak power during training, and the weekdays used for training. The simulation parameters are provided in Table~\ref{tab:simulation_parameters}.  
% \begin{table}[h]
% \centering
% \small
% \caption{Hyperparameters and selected values.}
% \begin{tabular}{|>{\raggedright}p{1.7cm}|p{4cm}|p{1.0cm}|}
% % \begin{tabular}{|@{}c|c|c@{}|}
% \hline
% Parameter & Description & Range \\ 
% \hline
% Actor network & Number of units at each layer & [96, 96] \\\hline
% Critic network & Number of units at each layer & [96, 96] \\\hline
% $\Gamma$ & Discount factor for future reward & 1 \\\hline 
% Actor\&Critic learning rate &Learning rate for updating actor and critic networks & $10^{-5}$, $10^{-3}$ \\\hline 
%  % & Tuning parameter  & 1e-3 \\\hline 
% Batch size & Batch size for fetching transitions from replay buffer & 64 \\\hline 
% $\policyGuidanceRate$ & Probability to introduce policy guidance & 0.5 or 0.7 \\\hline  
% % Max episodes & Total iterations for training RL model & 5000 \\\hline
% % Collect steps & Number of steps run before training actor/critic network once & 10 \\\hline 
% % Action noise & Noise added to action generated by actor-network & 0.2 \\\hline  
% $\lambda_{S}$, $\lambda_{B}$, $\lambda_{D}$ & Penalty coefficients for SoC requirement, bill cost, and demand charge &1,1, 3\\\hline 
% Random seed & Random seed for actor and critic network initialization  &0-5\\\hline  
% Adjustment of $\PrdPeak$& Lower bound of the 99\% confidence interval for the optimal monthly peak power based on training data & Increased by 0\%, 5\%, 10\% \\\hline  

% \end{tabular}
% \label{tab:hyperparameters}
% \end{table} 

% {\color{red} Add RL performance using training data}
\noindent \textbf{Baseline Approaches.}
% \label{ssec:alternatives} 
% We transform training data into input samples for our digital twin, Optimums~\cite{JP2024}, which models the EV charging scenario. 
% To evaluate the performance of our RL approach, we compare it with various methods, including an optimal oracle solution, a real-world charging procedure (baseline approach), and several proposed heuristic approaches. We provide brief descriptions of the baselines here and present the detailed descriptions in~\Cref{ss:appendix_approach}. 
{\color{black} We transform training data into input samples for our digital twin/simulator, Optimus~\cite{JP2024}, which simulates the EV charging scenario. To evaluate our RL approach, we compare it with an optimal oracle, a real-world charging baseline, and several heuristics. Brief baseline descriptions are provided here, with details in~\Cref{ss:appendix_approach}.}

% All approaches ensure that each EV connected to a charger reaches the required SoC by the time of departure. 
%This is achievable as long as the car stays connected long enough to charge feasibly. Additionally, all methods forcibly charge the car when it is about to leave, regardless of the available battery capacity.
%  % Each approach guarantees that each car that has connected to a charger, will reach the required SoC at the time of departure. 
% This holds true when a car stays long enough to feasibly charge to the required SoC. All approaches accomplish this by forcibly charging the car when its about to leave regardless of the available battery capacity. 
%
% {\color{black} Update the equations and variables}
\begin{itemize}[leftmargin=*]
    \item {\bf Optimal MILP Solver (MILP)}: We model deterministic sequences of EV arrivals and departures and solve the problem using the MILP formulation with IBM ILOG CPLEX Optimization Studio~\cite{cplex2009v12}. \textit{The results serve as an upper bound for comparison, as they utilize an oracle for optimality.}
    \item {\bf Fast Charge (FC)}: This approach simulates current real-world charging procedures, charging all connected EVs as quickly as possible to $\SOCMAX$.
    \item {\bf Trickle Charging (Trickle)}: The trickle charging approach utilizes the trickle charging rate, defined as the minimum required charge at each time slot: $P(C_i, T_j) = \PowerNeed(C_i, T_j)/\ReTime(C_i, T_j)$, to charge all EVs until they reach their required SoC. 
    %Here, $\PowerNeed(C_i, T_j)$ denotes the energy needed by the EV connected to charger $C^i$ at time slot $T_j$ to reach its required SoC.
    % , while $\ReTime(C_i, T_j)$ denotes the remaining time the EV will stay before departure, as defined in Section~\ref{ssec:MDP}. 
   %{\color{black} Note that we obtain MILP solutions through an oracle simulation, which are not implementable in the real world.}
    % \item {\bf Least Laxity First (LLF)}: Least Laxity First is a dynaimc priority driven algorithm designed for scheduling multiprocessor real time tasks~\cite{leung1989new}. Laxity or slack time refers to the amount of time a task can be delayed without causing it to miss its deadline. In the context of EV charging, we define laxity as the difference between the amount of time remaining for a car before it departs and the amount of time it takes to meet the EV required SoC at a constant rate of charge~\cite{xu2016dynamic}. Specifically, for EV connected with charger $C^i$ at time slot $t$, we compute its laxity value  by $(T_\DepartureTime(\phi(C_i, T_j))-t)-(\SOCR(\phi(C_i, T_j))-SoC_t(\phi(C_i, T_j)))\times {\it CAP}(\phi(C_i, T_j))C^{max}_i$. 
    % Additionally, we limit the amount of EVs charging at any given time interval by allocating a capacity at each step. The capacity is based on the difference between a set a power threshold and the current building power at that time. 
    % Only EVs connected to chargers whose aggregate power rates fall within this limit are able to be charged for that time interval. LLF will provide trickle charge, charging the EVs to the minimum required power to reach required SoC before departure, at each interval. 
\item \textbf{Trickle Least Laxity First (T-LLF)}: We define the Trickle LLF algorithm (detailed in the Appendix) based on the Least Laxity First approach, a dynamic priority-driven method for scheduling multiprocessor real-time tasks~\cite{leung1989new}. 
% Laxity, or slack time, is the time a task can be delayed without missing its deadline. 
In EV charging, we define laxity as the difference between the remaining time before departure and the time required to reach the desired SoC at a constant charging rate~\cite{xu2016dynamic}. 
%For an EV connected to charger $C^i$ at time slot $T_j$, the laxity value is computed as $ (\DepartureTime(V) - T_j) - \PowerNeed(C_i, T_j)/C^{max}_i $, with $\phi(C_i, T_j)$ is the EV connected to $C_i$. 
At each time slot, we compute the ``power gap'' (as $\PrdPeak(T_j) - \Building(T_j)$), using the estimated peak power and the current building power.
%and the ``power gap'' is distributed among all EVs.
% ~\Cref{alg:llf} in appendix shows the pseudocode.
% as, $ \PrdPeak(T_j) - \Building(T_j) $. 
This power gap is allocated to all EVs by distributing the trickling charger rate %, calculated as $ \frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)} $, 
to those prioritized by their laxity. 
%until they reach required SoC. 
% \input{algorithms/heuristictrickle}    
\item {\bf Trickle Early Deadline First (T-EDF)}: We propose the Trickle EDF algorithm in a similar manner to Trickle LLF, with the only difference being the prioritization method. Trickle EDF follows the Early Deadline First approach (based on time of departure of an EV), which was originally designed as a dynamic scheduling algorithm for real-time systems~\cite{stankovic_EDF}. 
% In Trickle EDF, the deadline for each EV is defined as the remaining time before departure, i.e., $\DepartureTime(\phi(C_i, T_j)) - t$. The algorithm computes the available power gap and allocates the trickle charging rate to each EV based on the earliest deadline first until they reach their required SoC.
% We only consider EVs below the required SoC in the scheduling. Similar to LLF, this approach utilizes a 
% capacity to limit the amount of vehicles that can be charged for any time interval. 
%Charge First policies behaves similarly to EDF and LLF. However, the key difference is how much charge is provided to EVs. Charge First policies will prioritize charging EVs connected to  bidirectional chargers, as much as possible given the capacity. The potentially excess energy that these EVs will hold will then be used during intervals where there is a need for more capacity to charge critical EVs.
    \item {\bf Charge First Least Laxity First (CF-LLF)}: We compute the available ``power gap'', as in Trickle LLF. 
    Then we calculate the sum of the trickle charging rates for all EVs at the current time slot; if this sum is less than the available ``power gap'', we have capacity for overcharging.  We first assign the charging rate for all EVs to be their trickle charging rates, and then, we charge EVs connected to bi-directional chargers to reach their maximum SoC, following the reverse order of their laxity until the power gap is consumed. 
    If the trickle sum exceeds the power gap, bidirectional EVs are discharged, also based on reverse laxity, to fill the negative gap before resuming the trickle charging.  See \Cref{alg:charge_first_llf} in the appendix.
%hen the rates for EVs connected to bidirectional chargers are increased to their maximum, depending on the ``power gap'', till they reach their maximum SoC. 
%$P(C_i, T_j) \gets \min\left(\frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)}, \hat{P}\right)$.
 % \item {\bf Charge First Least Laxity First (Charge First LLF)}: 
 %   We also propose the Charge First LLF algorithm detailed in Algorithm~\ref{alg:charge_first_llf}. 
   %Similar to Trickle LLF, at each step, we compute the gap between the estimated peak power and the building power to determine the available power gap for EV charging: $\hat{P} = \PrdPeak(T_j) - \Building(T_j)$. 
% The key difference in Charge First LLF is that we first calculate the sum of the trickle charging rates for all EVs at the current time slot: $S = \sum_{i} {\PowerNeed(C_i, T_j)}/{\ReTime(C_i, T_j)}$. 
% If this sum is less than the available power gap, $S < \hat{P}$, it indicates that we have excess power available for charging. 
% In this case, we first charge all EVs using their trickle charging rates: $P(C_i, T_j)={\PowerNeed(C_i, T_j)}/{\ReTime(C_i, T_j)}$. 
% Then, we prioritize charging EVs connected to bi-directional chargers as much as possible to reach their maximum SoC, thereby consuming the extra available power. 
% This charging process follows the reverse order of their laxity. 
% If the sum of the trickle charging rates is greater than the available power gap, $S \geq \hat{P}$, we order all EVs connected to bi-directional chargers based on the reverse of their laxity. We then determine the excess power for each EV that exceeds its required SoC: $P(C_i, T_j) = \max\left(C^{min}_i, \frac{(\SOCR(V) -\SOC(V,T_j)) \times CAP(V)}{\delta}\right)$, with $V=\phi(C_i, T_j)$. 
% This excess power is added to the available power gap for later trickle charging power allocation. The trickle charging rates are then allocated to EVs based on their laxity order. 
%$P(C_i, T_j) \gets \min\left(\frac{\PowerNeed(C_i, T_j)}{\ReTime(C_i, T_j)}, \hat{P}\right)$.



% \input{algorithms/heuristicchargefirst}

\item {\bf Charge First Deadline First (CF-EDF)}: This follows the same procedure as Charge First LLF but utilizes a different prioritization metric, focusing on the remaining time before EV departure. 

% \item {\bf Trickle Charging (Trickle)}: Trickle charging approach simply uses the minimum required charge for each time interval. The minimum required charge is given as $\frac{MaxRate}{departure-arrival}$. This results in a trickle charge for all EVs that guarantee them leaving with the required charge, given that their request is feasible.

% {\color{red} \item {\bf Online MCTS}: Takes too long to get the result.} Hierarchical and decentralized approach to charging EVs. Actions are discrete. 



    %Two variations of the baseline are implemented: (1) EVs are charged to the maximum allowed SoC (often higher than the required SoC of the user) and (2) 
    \color{black}
    % \item Variations on Scheduling Algorithms: Trickle Charging, Charge First, and Machine Learning-based threshold.
    \color{black}
\end{itemize}

\subsection{Results}
\label{ssec:results}
% \input{results/month_total_bill}
\input{results/total_bill_mean_std}

% \input{results/month_demand_charge}  
% \input{results/demand_charge_shave_mean_std} 
% \input{results/performance_metrics_training_data}

% \input{results/add_ablation} 
\input{results/ablation_study_mean_std} 
% \input{results/all_total_bills}

% \input{results/all_demand_charge}
% \input{results/training_data_analysis}
\noindent We evaluate all approaches using two metrics:  1) \textbf{Total Bill:} The sum of electricity cost and demand charge over the billing period, computed by Eq.~(\ref{eq: billing}) and  2) \textbf{Peak Shaving:} It is the difference in demand charge between (i) the building's power usage (without any charging) and (ii) by adding charging the EVs under the respective policies. Positive values indicate that the policy reduced the demand charge by controlling the charging actions. 
% \item \textbf{Missing SoC:} The sum of the SoC not meeting the required SoC before EV departure during the charging period.
% Note that \textbf{missing SoC}—the energy shortfall between required and actual SoC at departure for each EV—is a key metric in the V2B problem. Our RL model, with action masking, applies force charging and discharging in \textbf{Mask 2} and \textbf{Mask 3} to ensure all EVs reach the required SoC before departure, minimizing missing SoC. For fairness, we apply these force procedures across all proposed heuristics, so we will not report this metric separately, as missing SoC is minimized for all EVs. 
Additionally, missing SoC—the energy shortfall between required and actual SoC at departure—is critical in the V2B problem. Our RL model, with action masking, ensures all EVs reach their required SoC before departure by applying force charging and discharging in \textbf{Mask 2} and \textbf{Mask 3}. For fairness, these force procedures are applied across all proposed heuristics, effectively minimizing missing SoC. Therefore, we do not report this metric separately.

% {\bf RL Performance on Single Day Billing Period.} 
% We begin by evaluating the RL model's performance on minimizing single-day demand charge and total bill. We select 25 samples per month from May to December 2023 and January 2024, including all Monday and Tuesday weekdays. Of these, 20 samples are used for training and 5 for testing from each month, to evaluate the model's performance in reducing demand charge and total bill in a single-day episode during the testing phase. 

% {\bf Generalization for Monthly Demand Charge Reduction. } 
%During training, we apply a 3-fold cross-validation approach to select the best model, evaluating RL performance in reducing the monthly demand charge, total bill, and minimizing the missing SoC. 
%Figure~\ref{fig:all_total_bills} compares the total bill generated by all comparative policies over a one-month period. The results indicate that the trained RL model provides the lowest final total bill in all months except December. {\color{black} Note, MILP serves as an oracle simulation and is not implementable in the real world. } 
We assess the RL model's long-term performance from May 2023 to January 2024, comparing it against baseline approaches on 50 testing samples. 
{\color{black}~\Cref{table:monthly_total_bill} compares the total bill over nine months across different policies. While MILP offers an oracle-based optimal solution, it is impractical for real-world use and serves as a performance upper bound. The results show that the trained RL model consistently achieves the lowest total bills from May 2023 to January 2024 (except June 2023), outperforming other real-time policies in eight of the nine months and significantly reducing costs compared to the real-world Fast Charge procedure as detailed in Table~\ref{table:monthly_total_bill}.
Additionally, heuristic approaches using the First Charge logic, like First Charge LLF or EDF, consistently result in relatively lower total bills and demand charges compared to other heuristics. This indicates that the First Charge approach is effective in balancing the charging and discharging process, offering better overall performance across all heuristics.
\color{black}
\Cref{table:peak_shaving_demand}  in Appendix~\ref{appendix:results} illustrates the peak shaving performance in all approaches, showing that our RL approach achieved peak shaving in six months (indicated by positive values), demonstrating its effectiveness in reducing demand charges by charging EV.
}
% However, an outlier was observed in June, where the RL model did not outperform all heuristics, leading to bills that were $\$28$ higher than the best-performing heuristic, Charger First LLF. 
% % Upon analyzing the training data for December, we observed a significantly higher number of car arrivals, about 20 per day, especially during peak hours, compared to around 10 per day in other months (Table~\ref{tab:training_testing_data}). We hypothesize that this high volume and diversity of car arrivals introduced additional challenges for the RL model's generalization, resulting in a locally optimal policy. 
% % Based on this observation, 
% Thus, we propose a procedure for evaluating RL performance on training data relative to heuristics, to determine which policy to use. We define the performance metric as:
% \begin{equation}  
% \bar{\Theta}^{\it policy} = \frac{1}{N} \sum_{i=1}^{N} \frac{\Theta(\mathcal{P})^{{\it policy}}_i - \Theta_i^*}{\Theta_i^*}
% \label{eq:training_metrice}
% \end{equation}
% where $ \Theta(\mathcal{P})^{\text{policy}}_i $ represents the final total bill of a specific policy ${\it policy}$ for the $i$-th sample, and $\Theta_i^* $ is the optimal total bill generated by the MILP solver. This metric allows for comparing the total costs of different policies relative to the optimal solution, providing insights into their effectiveness in minimizing costs in the V2B problem.

% By comparing this metric across all training samples, we can evaluate the performance of RL and heuristic policies and identify the policy that provides the smallest gap relative to the optimal solution.  
% Table~\ref{table:performance_metrics_training_data} presents the normalized performance metrics for the training data across all heuristic policies. 
% The results indicate that in June, the Charger First LLF heuristic has lower performance metric values than the RL model. Therefore, we recommend using Charger First EDF as the preferred policy for this month.

%However, there was an outlier in December, where the RL performance did not outperform all heuristics, resulting in a bill that was \$66 higher than the best-performing heuristic.
% Upon analyzing the training data for December, we observed a significantly high number of car arrivals—about 20 per day—especially during peak hours, compared to around 10 car arrivals per day in other months as shown in Table~\ref{tab:training_testing_data}. We hypothesize that the high volume and diversity of car arrivals introduced additional challenges for RL generalization, causing the RL model to provide only a locally optimal policy. 
% Based on this observation, we propose a procedure for evaluating RL performance on training data compared to heuristics to determine which policy to use. We define a performance metric as: 
% $$
% \bar{C}^{\it policy} = \frac{1}{N} \sum_{i=1}^{N} \frac{\hat{Cost}(\mathcal{P})^{{\it policy}}_i - \hat{Cost}_i^*}{\hat{Cost}_i^*}
% $$ 
% where $ \hat{Cost}(\mathcal{P})^{\text{policy}}_i $ represents the total bill of a specific policy ${\it policy}$ in the $i$-th sample, and $ \hat{Cost}_i^* $ is the optimal total bill generated by MILP solver. This metric enables us to compare the total bills of different policies relative to the optimal solution, providing insight into their effectiveness in minimizing cost in V2B problem.    
% By comparing this metric across all training samples, we can assess the performance of RL and heuristic policies and select the best policy that provides the smallest gap relative to the optimal solution.  Table~\ref{table:performance_metrics_training_data} presents the normalized performance metrics for the training data across all heuristic policies. The results show that in June and December, the Charger First LLF heuristic has lower performance metric values compared to the RL model. Therefore, we recommend using Charger First EDF as the preferred policy for these months. 
%Figure~\ref{fig:all_peak_shaving} illustrates the performance of all policies regarding peak shaving, showing that the RL model achieves peak shaving for certain test samples and leads to the lowest shaved peak demand (excluding the optimal solution) in seven months from May to November 2023.   

% % \input{results/month_total_bill}
% \input{results/total_bill_mean_std}

% % \input{results/month_demand_charge}  
% \input{results/demand_charge_shave_mean_std}


% % \input{results/performance_metrics_training_data}

% % \input{results/add_ablation} 
% \input{results/ablation_study_mean_std} 


\subsection{Ablation Study}
\label{ssec:ablation}

We evaluate the contributions of key techniques in our approach through ablation. For the ablation studies, we trained RL models on monthly samples of three months, May to July 2023, and tested their performance on the total bill. The ablations explored are: 
1) \rlcluster, RL training with more (500) training samples. 
2) \rlrandom, RL training using 60 randomly selected samples from 1000 generated samples.
3) \rlmorefeature, RL models trained using the complete set of 100 state features defined in ~\Cref{ssec:MDP}. 
4) \rlnoe, RL training where the monthly estimated peak power is set to 0, removing the influence of long-term peak power estimation. 
5) \rlnop, RL training without policy guidance. 
6) \rlnoa, RL training without action masking, except for forced charging and discharging (Masks 2 and 3), which are retained to minimize missed SoC. 
7) \random, where actions are randomly selected instead of using a trained actor network, followed by action masking.
%8) {\bf PPO}, a policy trained using the traditional PPO approach~\cite{NNM2024}. 
We present the sum of the monthly total bills from May to July 2023 for all approaches in the ablation study in Table~\ref{table:ablation} and~\Cref{ss:appendix_approach}. 
%Next, we discuss the significance of each ablated feature.
% illustrates the model's performance across various ablation approaches. The results show that removing key components leads to a decline in performance. Below, we detail the impact of each aspect. 
 
% \noindent {\bf Downsampling.} 
We evaluate the impact of downsampling using k-means clustering to generate 60 training samples from a pool of 1000. The \rlcluster approach, which uses 500 samples, showed no improvement in performance but increased computational burden during training. We also tested \rlrandom, where samples were randomly selected instead of clustered, resulting in a performance drop. These findings confirm that our downsampling method maintains RL performance while improving efficiency.

% \noindent{\bf State abstraction.} 
We then examine the \rlmorefeature approach, which performs worse, suggesting that condensing state features with domain-specific knowledge improves training and leads to better outcomes. 
% \noindent{\bf Policy guidance.}
The \rlnop\  approach, which removes policy guidance, results in decreased performance, highlighting its importance in optimizing actions during training. This guidance narrows down the action exploration space, directing the model toward better solutions.
 
% \noindent{\bf Long-term prediction}. 
The \rlnoe\  approach shows worse results, highlighting the importance of accurate long-term peak power estimation during training. This value is used in action masking to improve the charging actions without increasing the monthly peak power and influences the reward function by penalizing actions that raise peak power. When set to $0$, the RL model fails to converge to a good global optimum, emphasizing the critical role of peak power estimation in achieving optimal performance. 

% \noindent {\bf Action Masking.} 
Training without the action masking procedure in \rlnoa \ leads to a significant performance drop, demonstrating its importance in improving RL performance. This also highlights the challenge of training RL models with 15 chargers in a continuous action space. Action masking incorporates heuristics to guide actions, resulting in significant improvements.

% \noindent {\bf RL actor network.} 
% To assess the impact of the actor network, we replaced it with a random policy in the \random approach. It generates random charging actions before passing through the action masking. The poor performance shows that action masking alone is insufficient, emphasizing the critical role of the actor network in achieving optimal outcomes. Despite all proposed heuristics (except FC and Trickle) adhering to action masking heuristics, including forced charging and power allocation based on estimated peak power, the RL approach consistently delivers better results, underscoring the importance of the actor network. 
To assess the impact of the actor network, we replaced it with a random policy in the \random approach, where random charging actions are generated before applying action masking. Its poor performance highlights that action masking alone is insufficient, emphasizing the actor network’s critical role in achieving optimal outcomes. While all proposed heuristics (except FC and Trickle) adhere to action masking constraints, including forced charging and power allocation based on estimated peak power, the RL approach consistently outperforms them, reinforcing the importance of the actor network.


% To evaluate the actor network’s impact, we replaced it with a random policy in the \random approach, generating random charging actions before action masking. Its poor performance highlights that action masking alone is insufficient, underscoring the actor network’s critical role. Despite all heuristics (except FC and Trickle) adhering to action masking constraints, the RL approach consistently outperforms them, further demonstrating its effectiveness.
% Additionally, the results generated by the model trained using traditional PPO appaorh  indicated by {\bf PPO} shows worse performance, further underscoring the effectiveness of our approach. 
%{\color{black} Additionally, the results from the model trained using the traditional PPO approach ({\bf PPO}) show worse performance, further highlighting the effectiveness of our approach.}