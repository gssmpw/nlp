% \begin{algorithm}[t]
% \setcounter{AlgoLine}{0} %
% \small
%     \SetAlgoLined
%     \KwIn{Initial policy parameters for actor network $\zeta_a$, critic parameters $\zeta_c$, target network parameters $\zeta_a', \zeta_c'$\\
% Training parameters: $\mathit{actionNoise}$, $\policyGuidanceRate$, $\mathit{bufferSize}$, $\mathit{batchSize}$, maximum training iterations: $M$}
%     \KwOut{Trained policy $\pi_{\alpha}$}
%     Initialize replay buffer $\Buffer$ \\
%     % Initialize target network weights $\zeta_a' \leftarrow \zeta_a$, $\zeta_c' \leftarrow \zeta_c$ \\
%     \For{$1$ \KwTo $M$}{
%         $s_0$ \gets initial state from the simulator 
%         % Initialize a random process $N$ for action exploration 
        
%         \For{each time slot $T_j \in \mathcal{T}$}{
%         % \If{$T_j$ is during non-peak hours}{
%         % Get action A(T_j) using h
%         %}
%         \tcp{Introducing policy guidance stochastically.}
%             {\color{black} randomValue $\leftarrow randomBetween(0,1) $
            
%             \If
%             %(\tcp*[h]{Adding policy guidance stochastically}) $$
%             { randomValue $\leq \policyGuidanceRate$}{
%         Get action $A(T_j)$ by rerunning the ILP solver: $A(T_j)\leftarrow\MILP(S(T_j), {\it remainEpisode})$
%                 }
%             \Else{
%                 % $A(T_j)\gets  \pi(S(T_j) | \zeta_a) + \mathit{actionNoise}$
%                 Get masked action $A(T_j) \gets \Mask\left(S(T_j), \pi(S(T_j) | \zeta_a)) + \mathit{actionNoise} \right)$ by current policy and $\mathit{actionNoise}$  
%                 % \tcp{Add action masking}
%             }}
%              State transition $S(T_{j+1})\leftarrow {\it Trans}(S(T_j), A(T_j))$. 
%             %in Algorithm~\ref{alg: stateTrans}. 

%             Get the action reward $R(T_j)\leftarrow {\it Reward}(S(T_j), A(T_j))$. 
%             %by Algorithm~\ref{alg: reward}. 
            
%             Store transition $(S(T_j), A(T_j), R(T_j), S(T_{j+1}))$ in $\Buffer$

%             Sample a minibatch $(S(T_i), A(T_i), R(T_i), S(T_{j+1})$ from $\Buffer$ 

%            {\color{black} Get masked actions $A(T_{i+1})$ at $S(T_{i+1})$ using target actor network: $A(T_{i+1})\leftarrow \Mask(S(T_{i+1}), \pi'(S(T_{i+1}) | \zeta_a'))$}  
%             %using Algorithm~\ref{alg: mask}  \tcp{Add action masking}

%             Set target $y_i\leftarrow R(T_j) + \gamma Q'(S(T_{i+1}), A(T_{i+1})
%             | \zeta_c')$ 
            
%             Update critic network by minimizing the loss: $L 
%             \leftarrow \frac{1}{N} \sum_i (y_i - Q(S(T_i), A(T_i) | \zeta_c))^2$ 
            
%              {\color{black} Get masked actions $A(T_i)$ at $S(T_i)$ using actor network: $A(T_i)\leftarrow \Mask( S(T_i), \pi(S(T_i) | \zeta_a))$ }%using Algorithm~\ref{alg: mask}.  
%              % \tcp{Add action masking}
%            % Mask action $A(T_i)\leftarrow \Mask(S(T_i), a_{i})$ 
           
%             Update the actor policy by policy gradient: 
%             $\nabla_{\zeta_a} J \leftarrow \frac{1}{N} \sum_i \nabla_a Q(S(T_i), A(T_i) | \zeta_c) | \nabla_{\zeta_a} \pi(s | \zeta_a) |_{S(T_i)}$
            
%             Delayed Update the target networks: 
%             $\zeta_a' \leftarrow \tau \zeta_a + (1 - \tau) \zeta_a'$; 
%             $\zeta_c' \leftarrow \tau \zeta_c + (1 - \tau) \zeta_c'$ \\
%         }
%     }
%     \caption{Improved DDPG with Action Masking and Policy Guidance.}
% \label{alg:DDPG} 
% \end{algorithm}

%%%%%%%%%%%% ACTION MASKING %%%%%%%%%%%%
\begin{algorithm}[ht]
    \SetAlgoNlRelativeSize{-1}
    \KwIn{$\textit{state}, \textit{action: } A(T_j) $}
    \KwOut{Masked action: $\MaskAction$} 
\small
$\PowerNeed \gets [\PowerNeed(C_i, T_j)]_{C_i \in \mathbf{C}};$
$\ReTime \gets [\ReTime(\phi(C_i, T_j))]_{C_i \in \mathbf{C}};$
$\epsilon \gets 10^{-5};$
$C^{max} \gets [C^{max}_i]_{C_i \in \mathbf{C}};$
$C^{min} \gets [C^{min}_i]_{C_i \in \mathbf{C}};$

\tcp{Mask 1: Set action = 0 if no car is connected}
    $ \MaskAction \gets \frac{\ReTime}{\ReTime + \epsilon} \times A(T_j)$\;
    
    \tcp{Mask 2: Stop charging when required SoC is reached for uni-directional chargers}
    $ \MaskAction_{tmp} \gets \MaskAction$; 
    $\MaskAction_{tmp} \gets \min(\MaskAction_{tmp}, \frac{\PowerNeed}{\delta})$\;
    $ \MaskAction[\textit{uniIdx}] \gets \MaskAction_{tmp}[\textit{uniIdx}]$\;

    \tcp{Mask 3: Enforce charging to the required SoC before departure. }
    $\overline{\Power(T_j)} \gets \frac{ \PowerNeed- (\ReTime - 1) \times C^{max} \times \delta }{\delta}$\;
    $\overline{\Power(T_j)} \gets \min(\overline{\Power(T_j)}, C^{max})$\;
    $ \MaskAction \gets \max(\MaskAction, \overline{\Power(T_j)})$\;
    \tcp{Mask 4: Ensure bidirectional chargers discharge to the required SoC before departure.}
    $\Power^*(T_j) \gets \frac{\PowerNeed- (\ReTime - 1) \times P_{min} \times \delta }{\delta}$\;
    $\Power^*(T_j) \gets \max(\Power^*_t, P_{min})$\;
    
    $\MaskAction_{tmp}\gets \MaskAction$; 
    $ \MaskAction[\textit{biIdx}] \gets \min(\MaskAction_{tmp}, \Power^*_t)[\textit{biIdx}]$\;

    \tcp{Mask 5: Power improvement strategy}
    $ \textit{powerGap} \gets \Building(T_j) - \PrdPeak(T_j)$\;
    $ \textit{canIncrease} \gets \textit{RELU}\left(\max\left(\frac{\PowerNeed}{\delta}, C^{max}\right) - \MaskAction \right)$\;
    
    $ \textit{toImprove} \gets \min\left(\textit{RELU}(\textit{powerGap} - \sum \MaskAction), \sum \textit{canIncrease}\right)$
    
    $ \MaskAction \gets \MaskAction + \frac{\textit{toImprove} \times \textit{canIncrease}}{\sum(\textit{canIncrease}) + \epsilon}$\;

    \tcp{Mask 6: Do not discharge below building load}
    $ \textit{toImprove} \gets \max(-\Building(T_j) - \sum(\MaskAction), 0)$\;
    $ \textit{negAction} \gets \textit{RELU}(\MaskAction \times -1) \times -1$\;
    
    % $ \textit{toIncrease} \gets \frac{\textit{toImprove} \times \textit{tmpAction}}{\sum(\textit{tmpAction}) + \epsilon}$\;
    $ \MaskAction \gets \MaskAction +  \frac{\textit{toImprove} \times \textit{negAction}}{\sum(\textit{negAction}) + \epsilon}$\;

    \caption{Action Masking: $\Mask(S(T_j), A(T_j))$.} 
    \label{alg: action_masking}
\end{algorithm} 

% Based on the formulated MDP, we next use a RL approach to train a policy that maximizes long-term rewards by interacting with a custom environment simulator, which processes all data samples and handles state transitions.  
% \subsection{Environment Simulator}
% \jpnote{I will add this, just talk about how the transition is represented by the simulator. See Mike's paper.}



\section{Reinforcement Learning Policy}
\label{sec:RL}
% Figure~\ref{fig:framework_and_pipeline}. We input the the training data into our designed environment simulator, which handle the state generation and state transformations based on actions following our state transition function described in Section~\ref{ssec:MDP}.

% Based on the formulated MDP, we discuss the RL-based framework used for policy training to maximize the long-term reward of this MDP, as shown in \Cref{fig:framework}. This paper utilizes the Deep Deterministic Policy Gradient (DDPG) algorithm~\cite{lillicrap2015continuous} for policy training. DDPG is well-suited for handling continuous action spaces and supports off-policy training, enabling the model to learn from diverse experiences across various scenarios, thus improving generalization. 

% To further enhance policy performance, we integrate action masking and policy guidance techniques with DDPG. An overview of the RL-based framework shows that we input the training data into our designed environment simulator, which generates state transitions and transformations based on actions according to the state transition function described in Section~\ref{ssec:MDP}. In traditional DDPG, the actor network generates state and action trajectories through interaction with the environmental simulator, which are then stored in the replay buffer for training the critic and actor networks.  
% In this work, we enhance this process with a policy guidance approach. Instead of solely relying on the actor network for training trajectories, we incorporate a MILP solver to provide optimal actions based on current and future information, guiding the RL training away from local optima. Additionally, we implement an action masking procedure, represented as $\Mask(S(T_j), A(T_j))$, which refines the raw actions from the actor network by considering constraints for action validity and utilizing domain-specific knowledge to limit the exploration range of training actions and improve policy performance.  
% Our advanced DDPG method is described as follows. Algorithm~\ref{alg:DDPG} depicts our customized DDPG approach, incorporating action masking and policy guidance. This approach builds upon the classic DDPG algorithm by introducing policy guidance (lines 5 to 9) and action masking (lines 9, 14, and 17).
%Based on the formulated MDP, we present an RL-based framework for policy training aimed at maximizing long-term rewards, as illustrated in \Cref{fig:framework}. 
During training, data samples are input into our designed environment simulator \ad{as mentioned before we do not describe simulator enough, perhaps more information and statement that we will cite the simulator after blind review}, which provides the environment that abstracts state features for the RL models and manages state transitions based on the function described in Section~\ref{ssec:MDP}. The simulator operates based on actions (power rates) generated by the policies and dynamic events such as EV arrivals and departures. 
Our core approach is based on  Deep Deterministic Policy Gradient (DDPG) algorithm~\cite{lillicrap2015continuous}, which  is well-suited for continuous action spaces and supports off-policy training, allowing the model to learn from diverse experiences across various scenarios, thereby improving generalization. To enhance the policy performance, we integrate action masking and policy guidance techniques with DDPG as outlined in Algorithm~\ref{alg:DDPG}.  

Traditional DDPG relies on the actor network to generate actions (power rates in this work). The tuples of state, action, action reward, and next state are stored as state transitions in the replay buffer (see lines 9 to 12). In each training iteration, we batch state transitions from the replay buffer for model training (see line 13). Specifically, DDPG maintains target networks for both the actor and critic, which are used to generate the next state and compute Q-values essential for calculating the critic loss during training. The critic network is trained using gradient descent by minimizing the mean squared error between predicted Q-values and target Q-values derived from the Bellman equation (see lines 14 to 16). The critic learns Q-values for state-action pairs, which are then used to train the actor network through a policy gradient approach (see lines 17 and 18). These target networks are updated less frequently to stabilize the training process (see lines 18 and 19). 

To address the large state and action spaces in this RL model, we enhance DDPG by integrating an action masking procedure, denoted as $\Mask(S(T_j), A(T_j))$. This procedure refines the raw actions generated by the actor network by enforcing action validity and utilizing domain-specific knowledge, thereby improving policy performance. The action masking is applied after the actor network produces raw actions, acting as an additional layer (see lines 9, 14, 17). 
Additionally, we implement a policy guidance procedure (see lines 5 to 9) by incorporating a MILP solver to provide optimal actions through the function $\MILP(S(T_j), {\it remainEpisode})$, based on current and future information. These optimal actions are stochastically introduced during RL training into the replay buffer, mixing high-quality actions with the raw RL actions to enhance the training transition quality and guide the RL training in a beneficial direction. 

Next, we detail our RL framework by outlining the design of the environment simulator, the data normalization and network structure, the action masking and policy guidance procedures, and the heuristic approaches used to simplify our RL training. 
%which incorporates both policy guidance (lines 5 to 9) and action masking (lines 9, 14, and 17). 


% During training, optimal actions are stochastically incorporated into the process based on a predefined ratio coefficient, denoted as $\policyGuidanceRate$, and stored in the replay buffer. The function $\MILP(S(T_j), {\it remainEpisode})$ takes the current state as input and outputs the next optimal action. It can also access remaining events in the episode, including upcoming EV arrivals, SoC requirements, and building load, to invoke a MILP solver. This solver generates a sequence of optimal actions starting at timestamp $t$, returning the first action in the sequence, which serves as the optimal action for the current state to maximize long-term reward (lines 5 to 9).



\subsection{Actor-Critic Network Structure} 
%\jpnote{Add sentence description of the number of layers we are considering for the Actor Critic models}
To enhance convergence and improve generalization, we preprocess all state variables to be within the range of 0 and 1 before feeding them into neural networks. Timestamps are normalized by dividing the number of minutes in a day, while power-related variables such as building load $\Building(T_j)$, estimated peak power $\PrdPeak(T_j)$ are scaled by their respective statistical factors. Furthermore, we normalize the energy capacity $CAP(V_k)$ of each car by dividing it by the maximum capacity among EVs. 
For the action $A(T_j)=[P(C_i, T_j)]_{C^{i}\in \mathcal{C}}$. We normalize the action values to the range of $[-1, 1]$ based on the power rate range $[C_i^{min}, C_i^{max}]$ by Equation~(\ref{eq: normalize}).
\begin{equation}
    \hat{P}(C_i, T_j)=\frac{2\times(P(C_i, T_j)-C_i^{min})}{C_i^{max} - C_i^{min}}-1. 
\label{eq: normalize}
\end{equation}
% In DDPG, two neural networks are employed: the critic network evaluates state Q-values, while the actor network generates deterministic actions as the policy.
% The critic network processes state and action inputs through hidden layers (2 layers used in final model) with ReLU activation, outputting a single Q-value estimate.
Both critic and actor network have two hidden layers with 96 neurons each. Both feature a ReLU activation layer at the end. The critic network outputs a single Q-value estimate while the actor network outputs the action which represent the power rate of each charger.
% The exact size used in the model are described in~\Cref{tab:hyperparameters}.
% Simultaneously, the actor network maps the input state through multiple hidden layers (2 layers used in final model) with ReLU activations to produce the action output, representing the power rate of each charger.
This output is constrained within the range $[-1, 1]$ using a $\tanh$ activation function. The original charging power values can be obtained by computing the inverse of the normalization equation in Equation~(\ref{eq: normalize}).  \ad{this section needs clear revision. It should just tell how the network is structured and how data is normalized and input into the network. Also, clear description of the size and architecture is required.}
\jpt{I modified the text, it still needs the normalization part.}



\subsection{Action Masking}
\label{sec: DDPG} 

%\jpnote{Move the algorithm ahead of this and summarize your points while referring to the algorithm. You can add label commands to the lines of the algorithm to refer to it. Also rename your heuristic and constraints in the algorithm to just action masking N}

We propose action masking approach addresses the challenge of large continuous action spaces (detailed in Algorithm~\ref{alg: action_masking}), by ensuring that the policy actions generated by the actor network are valid and reasonable during DDPG training. This technique, based on findings from \cite{huang2020closer,kanervisto2020action}, confirms that differentiable action masking does not interfere with the policy gradient backpropagation process. As a result, the learning process remains effective, while the imposed constraints on the action space prevent the actor network from exploring invalid actions, thereby improving training efficiency and optimizing resource usage.

This procedure takes the RL raw action $A(T_j)$, an array of power rates $[P(C_i, T_j)]_{C_i\in\mathcal{C}}$ for all chargers, processes it through the following steps, and outputs the masked actions $A'$. Before computing, we first obtain the state features formatted as arrays: the remaining power needed to reach the required SoC for all connected EVs ($\PowerNeed$), their remaining time ($\ReTime$), and the maximum ($C^{\max}$) and minimum ($C^{\min}$) power rates of all chargers (see line 1). Also, we denote ${\it uniIdx}$ and ${\it biIdx}$ as the indices for unidirectional and bidirectional chargers, respectively.    
% \jpnote{Shorten the descriptions and make it very concise.} 
\begin{itemize}[leftmargin=*]
    \item \textbf{Mask 1.} 
    %We masks the action by setting the powe rate $P(C_i, T_j)$ of charger $C_i$ to 0 if no EV is plugged in. This method effectively masks all actions for idle chargers (where $\ReTime(\phi(C_i, T_j))=0$ indicates no EV is connected to charger $C_i$ at time slot $T_j$).
    We set the power rate $P(C_i, T_j)$ of charger $C_i$ to 0 if no EV is plugged in, effectively masking actions for idle chargers (where $\ReTime(\phi(C_i, T_j))=0$).
    %%%
    \item \textbf{Mask 2.} Overcharging unidirectional chargers is unbeneficial since excess energy cannot be discharged. Thus we limit the power rates to ensure the SoC of unidirectional chargers remains within required SoCs. 
    %by masking the actions using the minimum of the current power rate and the rate required for EVs to reach the required SoC after this time slot, calculated as $\PowerNeed/\delta$. 
    Actions are masked to the minimum of the current power rate and the rate needed for EVs to reach required SoC, calculated as $\PowerNeed/\delta$ (see lines 3 and 4).
    % The rationale behind this approach is that overcharging unidirectional chargers offers no benefit, as the excess energy cannot be discharged 
    %%%
    \item \textbf{Mask 3.} 
    %Action are masked to minimize the missing SoC for all EVs at the time of departure, as outlined in objective function~(\ref{eq: soc_penalty}).
    Actions are adjusted to forced charging to required SoC before departure if necessary, to minimize missing SoC as per objective function~(\ref{eq: soc_penalty}).  
    We compute the critical power rate $\overline{\Power^*(T_j)}$, representing the minimum required power rate for all chargers at time $T_j$ to their required SoC before departing (assuming maximum power is subsequently utilized). The raw action is adjusted if it falls below this rate, especially in final time slots (see lines 5 to 7).
    %At each time slot, we compute the critical power rate $\overline{\Power^*(T_j)}$, which represents the minimum power rate required for all chargers at time $T_j$ to ensure that all connected EVs reach the required SoC before departing (assuming maximum power is subsequently utilized). The raw action is adjusted if it falls below this critical power rate. 
    
    \item \textbf{Mask 4.} 
    Overcharging bidirectional EVs is only advantageous if excess energy can be discharged during peak hours; thus, thereâ€™s no benefit to overcharging just before departure. Thus, we adjust actions to prevent discharging EVs if the raw action would cause exceeding the required SoC before departure.  Here, $\Power^*(T_j)$ denotes the minimum power rate needed for all chargers $C_i \in \mathcal{C}$ at time $T_j$ (see lines 8 to 10).
    %We mask action to discharge EVs to reduce their SoC if the raw action would result in exceeding the required SoC before departure. The intuition behind this is that overcharging EVs in bidirectional chargers is advantageous only when the excess energy can be discharged during peak hours to mitigate demand charges; thus, there is no benefit to overcharging them just before departure. Here, $\Power^*(T_j)$ denotes the minimum power rate required for all chargers $C_i \in \mathcal{C}$ at time $T_j$ to ensure that all EVs connected to bidirectional chargers can discharge to the required SoC before leaving (see lines 8 to 10). 
    
    %%%
    \item \textbf{Mask 5.} 
    %We mask action to increase charging power rates while ensuring that the masked action remains within the estimated peak power limit. The goal is to encourage charging to the required SoC without raising the demand charge, thereby avoiding forced charging just before departure, which could elevate peak power. In this step, we calculate the power gap between the estimated peak power and the current building load, represented as $\PrdPeak(T_j) - \Building(T_j)$. If the current sum of action power is below this gap, we utilize the available power gap to enhance the current action by allocating the additional power needed. This allocation is constrained by the maximum power rate each entity can increase and the power required to reach the required SoC of the connected EVs, computed using ${\it canIncrease}$ (see lines 11 to 14). 
    We increase charging power rates while ensuring the masked action stays within the estimated peak power limit. This aims to charge EVs to their required SoC without raising demand charges, thereby avoiding forced charging just before departure, which could elevate peak power. 
    We calculate the power gap between estimated peak power and current building load, $\PrdPeak(T_j) - \Building(T_j)$. If the current power sum is below this gap, we enhance the action using the available power gap, constrained by the maximum rate each EV can increase to reach the required SoC, computed using ${\it canIncrease}$ (see lines 11 to 14). 
    %constrained by the maximum power increase and the required SoC of connected EVs (see lines 11 to 14).
    
    %%%
    \item \textbf{Mask 6} We adjust the discharging power rate to prevent discharging below the current building load $\Building(T_j)$ by increasing negative actions based on their current values (see lines 15 to 17). 
\end{itemize} 

All above action masking procedures utilize array computations and differentiable operations, such as ReLU \cite{rasamoelina2020review} and maximum/minimum operations, from the PyTorch library \cite{paszke2017automatic}. 







 % We then employ \textbf{Heuristic 2}, which ensures that all EVs' charging needs are met by enforcing minimum power rates $P(C_i, T_j)$ when the remaining time is insufficient to reach the required SoC ${\it SoC}^{\text{req}}(v)$ using the maximum power rate.

% We then employ \textbf{Heuristic 2}, which ensures that all EVs reach their required SoC at departure. This is achieved by enforcing maximum power rates \( P(C_i, T_j) \) when the remaining time is insufficient to reach the required SoC $ {\it SoC}^{\text{req}}(v) $ using the maximum power rate. 
% This step ensures that all chargers charge the EVs to the required SoC before departure, if feasible. Additionally, this forced charging is applied only in the final time slots when it is required.

%We denote ${\it uniIdx}$ and ${\it biIdx}$ as the indices of the uni-directional and bi-directional chargers, respectively.   
%We introduce \textbf{Heuristic 3}, which bounds discharging to allow EVs to reduce their SoC if it exceeds the required levels. The intuition is that there is no benefit to overcharging in bidirectional chargers before departure.






\subsection{MILP Policy Guidance} 

To address the challenge of local optima in DDPG, we  integrate a policy guidance approach~\cite{pmlr-v28-levine13} into the RL training process. This aims to improve performance by providing optimal actions to guide the training toward better outcomes. We implement an optimization framework to generate optimal actions and add them to the replay buffer, providing effective guidance to steer the search towards global optima. Specifically, we formulate the V2B problem using mixed-integer linear programming (MILP). We give the MILP solver the current state information, including the current EV status, charge usage, and all future events from the input sample (EV arrival/departure, building load flow, and electricity prices from the current time to the end of the billing period. The MILP solution provides the power rate for each charger from the current time to the end of the billing period, maximizing the multi-objective weighted sum of total cost (detailed in Equation~(\ref{eq: billing})) and penalties for missing SoC requirements (defined in Equation~(\ref{eq: soc_penalty})). Following multiple constraints related to the EV SoC update  function following Equation~(\ref{eq: soc}) and Constraints~(\ref{eq:charging_rate}) to (\ref{eq:building_power}).   

% During training, optimal actions are stochastically incorporated into state transitions process based on a predefined ratio coefficient, denoted as $\policyGuidanceRate$, and stored in the replay buffer (as shown in Algorithm~\ref{alg: DDPG}. The function $\MILP(S(T_j), {\it remainEpisode})$ takes the current state as input and outputs the next optimal action. It can also access remaining events in the episode, including upcoming EV arrivals, SoC requirements, and building load, to invoke a MILP solver. This solver generates a sequence of optimal actions starting at timestamp $t$, returning the first action in the sequence, which serves as the optimal action for the current state to maximize long-term reward (lines 5 to 9). 
During training, optimal actions are stochastically incorporated into the state transition process based on a predefined coefficient, denoted as $\policyGuidanceRate$, and stored in the replay buffer (see Algorithm~\ref{alg:DDPG}). The function $\MILP(S(T_j), {\it remainEpisode})$ takes the current state as input and outputs the next optimal action. It considers remaining events in the episode, such as upcoming EV arrivals, SoC requirements, and building load, to invoke a MILP solver. This solver generates a sequence of optimal actions starting at time slot $T_j$ and returns the optimal action for the next time slot, serving to maximize the long-term reward. 

%SoC requirement,
% which ensure EVs must reach their required SoC before departure.
% \[
% SoC_{T_d(v)}(v) \geq So
% \]   
% Using the MILP solution, we can generate actions based on any state to maximize the long-term reward. 
% {\color{black} 
% The MILP aims to minimize the total energy cost, including both consumption and demand charges, while ensuring that all EVs meet their SoC requirements before departure. The objective function is:
% \[
% \min \Cost^{\it EN}(\mathcal{P}) + Cost^{\it DC}(\mathcal{P})
% \]
% as shown in Equations~\ref{eq: objective_1, eq: objective_2}. 
% where \( g_t \) represents the energy usage cost at time slot \( t \in \mathcal{T} \), \( P_{\text{max}} \) is the maximum power consumed during peak hours.  
% The model includes binary variables for charger assignments (\( a_{v,cp,s} \)), and continuous variables for charging rates (\( c^v_s \)) and battery levels (\( e^v_s \)). Key constraints ensure that: 
% \begin{enumerate}
% \item \textbf{Charger Assignments}: Each EV \( v \) is assigned to at most one charger \( cp \) at any time slot \( s \), and charger capacities are not exceeded, and is denoted by the assignment variable $a \in \mathcal{A}$.
% \[
%  \sum_{C_i \in \mathcal{C}} a_{v,i,s} \leq 1
% \]
        
% \item \textbf{SoC Requirements}: EVs must reach their required SoC before departure.
% \[
% SoC_{T_d(v)}(v) \geq SoC^(v)
% \]  
% \item \textbf{Energy Balance}: The energy in the EV battery evolves according to the charging rate, where $C_i$ is the charger attached to car $v$ indicated by Equation~(\ref{eq: SoC}). 
%     % \[
%     % SoC_{t+1}(v) = SoC_{t}(v) + P(C_i, T_j)
%     % \]
% \item \textbf{Demand Charges}: Demand charges are incorporated by modeling the maximum power consumption, as indicated by Equation~\eqref{eq: objective_2}.
% \end{enumerate}
% By solving this MILP using current state information and future events (e.g., EV arrivals/departures, building load, electricity prices), we generate optimal action sequences that maximize the long-term reward. These optimal actions are then stochastically introduced into the replay buffer during DDPG training, effectively guiding the RL agent towards global optima and improving training outcomes.
% } 
% The final DDPG approach, incorporating action masking and policy guidance, is depicted in Algorithm~\ref{alg: DDPG}. 
% {\color{black} This approach builds upon the classic DDPG algorithm by introducing policy guidance (lines 5 to 9) and action masking (lines 9, 14, and 17). 
 %This procedure ensures valid and efficient action selection, leading to improved training outcomes in DDPG. 
%The lines in {\color{black}black} highlight the enhancements over the classic DDPG. These colored the sections involve policy guidance and action masking. During DDPG training, optimal actions are stochastically introduced based on a predefined ratio coefficient $\policyGuidanceRate$ and stored in the replay buffer. We define the $\MILP(S(T_j))$ function to invoke the MILP solver and generate a sequential optimal action sequence starting at timestamp $t$, returning the first generated action which represents the optimal action for the current state (see lines 5 to 9). This procedure ensures both valid and efficient action selection, leading to improved training outcomes in DDPG.  


% {\color{red} Add text connecting two subsections. }
% \begin{itemize}[leftmargin=*]
%     \item {\bf Least Laxity First (LLF)}: Least Laxity First is a dynaimc priority driven algorithm designed for scheduling multiprocessor real time tasks~\cite{leung1989new}. Laxity or slack time refers to the amount of time a task can be delayed without causing it to miss its deadline. In the context of EV charging, we define laxity as the difference between the amount of time remaining for a car before it departs and the amont of time it takes to meet the user's required SoC at a constant rate of charge~\cite{xu2016dynamic}. Additionally, we limit the amount of cars charging at any given time interval by allocating a capacity at each step. The capacity is based on the difference between a set a power threshold and the current building load at that time. 
%     Only cars connected to chargers whose aggregate power rates fall within this limit are able to be charged for that time interval. LLF will provide trickle charge, charging the cars to the minimum required power to reach required SoC before departure, at each interval. 
% \end{itemize} 
\subsection{Heuristic Approach and Post Processing}
% \jpnote{Clarify what this section is, is this in the figure 2A?}
% We observe that off-peak hours typically feature lower electricity prices, allowing for charging EVs at a higher power rate without impacting the final demand charge. This insight leads us to maximize EV charging during off-peak hours, thereby reducing energy costs and alleviating pressure on charging during peak hours. To implement this strategy, we employ a greedy approach that directs the charging of all EVs to their required SoC at the maximum power rate. Additionally, discharging to the required SoC is performed when entering the off-peak duration after peak hours.
% We observe that off-peak hours typically offer lower electricity prices, enabling EV charging at a higher power rate without affecting the final demand charge. This insight allows us to maximize EV charging during off-peak hours, reducing energy costs and alleviating pressure on charging during peak periods. To implement this strategy, we adopt a greedy approach to manage off-peak charging, replacing the RL models. This approach directs the charging of all EVs until they reach their required SoC at the maximum power rate during off-peak hours. Additionally, discharging occurs when entering off-peak periods after peak hours if the current SoC of the EV exceeds the required level.  
% Additionally, we apply action post-processing to keep EV SoCs within valid boundaries by adjusting the power rate for stopping charging or discharging when they exceed $SoC^{\text{max}}(\phi(C_i, T_j))$ or drop below $SoC^{\text{min}}(\phi(C_i, T_j))$.  
We observe that off-peak hours offer lower electricity prices, enabling EV charging at higher power rates without affecting the final demand charge. This helps optimize EV charging during these periods, reducing energy costs and alleviating pressure during peak times. To implement this approach, we utilize a simple greedy method during off-peak hours, foregoing RL training during these times. EVs charge at maximum power until they reach their required SoC. Specifically, assuming the current time slot $T_j$ is within non-peak hours and EV $V' = \phi(C_i, T_j)$ is connected to charger $C_i$, if $\SOCR(V') < \SOC(V', T_j)$, then $P(C_i, T_j) = \min(C^{max}_i, (\SOCR(V') - \SOC(V', T_j)) \times {CAP}(V') / \delta)$. Discharging occurs if the SoC exceeds the required level, calculated as $P(C_i, T_j) = \max(C^{min}_i, (\SOCR(V') - \SOC(V', T_j)) \times {CAP}(V') / \delta)$.


To condense the state features while accounting for the common maximum and minimum SoC boundaries of all EVs (with $\SOCMIN=0$ and $\SOCMAX=90\%$), we do not include SoC boundaries in the state representation, limiting the policy's direct access to them for action control. To maintain valid SoC boundaries, we apply a post-processing procedure, which differs from action masking that is integrated with the actor network and influenced by training backpropagation. This action post-processing adjusts policy-generated actions before they are input to the environment, ensuring that charging stops when SoC exceeds $SoC^{\text{max}}$ and discharging halts if it drops below $SoC^{\text{min}}$. 
By employing this approach, we ensure that all policy-generated power rates for charging EVs remain within the defined SoC boundaries, thereby satisfying Constraints~(\ref{eq:soc_min}) and~(\ref{eq:soc_max}).
%This method allows the RL policy to concentrate on the objective of total bill reduction without needing to learn the validity of actions concerning SoC limits, thereby enhancing the effectiveness of the training process. 

