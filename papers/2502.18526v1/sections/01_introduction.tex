\section{Introduction}
\label{section:introduction}

%In the global transition to sustainable energy solutions, electric vehicles (EVs) are becoming increasingly vital in energy management systems \cite{kempton2005vehicle}, particularly in smart buildings.
The concept of vehicle-to-building (V2B)  charging \cite{kempton2005vehicle, lund2008integration} leverages the ability of battery electric vehicles (EVs) to operate as both energy consumers and temporary storage units \cite{tomic2007using}. V2B systems are particularly relevant in large office buildings, where EVs can be aggregated to optimize energy consumption and reduce peak power demand. By strategically controlling the charging and discharging cycles of EVs, these systems ensure that vehicles meet users' expected state-of-charge (SoC) requirements while minimizing the energy bought during peak time-of-use (ToU) periods  \cite{tse2014use, zhao2023research} and reducing the building's peak power demand over a billing cycle.
Implementing this optimization process in practice becomes complex due to the heterogeneity of charging infrastructures \cite{park2024exact}, the uncertainty of EV arrival and departure times, and the need for a careful balance between energy cost savings and ensuring that the expected final state of charge (SoC) is kept close to user expectation.  Additionally, aligning V2B frameworks with complex electricity pricing policies, including both energy and demand charges, adds to the challenge \cite{zhang2018optimal, 8274175}. While prior work has largely modeled this problem as a single-shot mixed-integer linear program~\cite{AORC2013, 5986769, 9409126, MJG2015}, such approaches fail to capture the intricacies of real-time decision-making in dynamic environments.

This sequential decision process can be modeled as a Markov Decision Process (MDP); however, solving the MDP presents several difficulties, including delayed and sparse rewards, a continuous action space, and the need for effective long-term decision-making under uncertainty. 
% Traditional optimization methods, such as offline Mixed Integer Linear Programming (MILP) and heuristic-based strategies ~\cite{oconnell2010integration, AORC2013, 5986769, 9409126, MJG2015}, struggle to handle real-time decision-making in dynamic environments.
% Online search methods such as MCTS also are difficult to implement because of the large action space. In contrast, reinforcement Learning (RL) approaches  have been explored in similar contexts~\cite{MMN2019, SNDJ2020, ZJS2022}, and perform better. However, they often limit the size of the state and action spaces or overlook long-term rewards \cite{NNM2024, 9409126}, i.e., the monthly billing period and demand charge. 
To address these challenges, we propose a novel approach to solve this problem that combines the Deep Deterministic Policy Gradient (DDPG) with two key enhancements: action masking and policy guidance through a mixed-integer linear program (MILP). The DDPG algorithm allows us to optimize continuous action spaces while accounting for uncertainties in EV arrival times, SoC requirements, and fluctuating building energy demands. By leveraging action masking, we adjust neural network actions during training using domain-specific knowledge, limiting exploration and guiding the RL agent toward more efficient and feasible policies.  The MILP component provides policy guidance during training, steering the RL agent toward near-optimal solutions and enhancing convergence in complex environments. 
% A critical contribution of our work is the introduction of a set of criteria to proactively detect when RL models are unlikely to generalize to unseen conditions, particularly in the presence of out-of-distribution data. This ability to anticipate performance issues enables proactive adaptation of the model, ensuring robustness in real-world scenarios. 
Our approach demonstrates strong generalization across diverse conditions and offers a scalable solution for V2B energy management.
Our team includes a major EV manufacturer with access to a smart building that has 15 heterogeneous chargers (~\Cref{fig: EV chargers} shows some of them). We use real-world charging and energy data to validate our approach, showing its effectiveness in reducing energy costs over nine months (May 2023 – Jan 2024). The summary of our contributions is as follows:  
\begin{figure}[t]
    \centering
    \includegraphics[width=0.58\linewidth]{images/fermata_close.pdf}
    \caption{EVs and bidirectional chargers at the research site.}
    \label{fig: EV chargers}    
    % \vspace{-0.1in}
\end{figure}  
%demonstrating its effectiveness in minimizing monthly energy bills over 9 months (May 2023 - Jan 2024).
% are challenged by large state-action spaces, multi-agent settings, and the necessity to optimize for long-term rewards, such as minimizing monthly demand charges. 

% In contrast, RL-based approaches have been explored in similar contexts~\cite{MMN2019, SNDJ2020, ZJS2022}, and perform better. However, they often limit the size of the state and action spaces or overlook long-term rewards \cite{NNM2024, 9409126}, i.e., the monthly billing period and demand charge. 



% In the context of the global transition to sustainable energy solutions, electric vehicles (EVs) have emerged as pivotal components in energy management systems \cite{kempton2005vehicle}. This transition necessitates comprehensive management of EV energy dynamics within smart building infrastructures, exemplified by the concept of vehicle-to-building (V2B) charging \cite{lund2008integration}.
% V2B charging harnesses EVs' ability to charge at varying rates and support bidirectional charging, enabling them to function as temporary energy storage units \cite{tomic2007using}. This facilitates the implementation of energy resilience and demand-response strategies in smart buildings \cite{richardson2011electric}. For instance, EVs can charge during low-cost periods and supply energy back during high-cost periods, thereby optimizing energy usage and promoting savings through strategic charging schedules \cite{tse2014use, zhao2023research}.

% Practical implementation of V2B systems presents several complexities \cite{nguyen2012optimal}. While incentivizing EV owners with lower charging costs is straightforward, ensuring a minimum charge level is crucial to prevent inconvenience. Managing numerous EVs in large buildings, each with unique charging requirements, and varied charger types complicates the optimization process \cite{he2022optimal}. Additionally, aligning V2B frameworks with complex electricity pricing policies, including both energy and demand charges, adds to the challenge \cite{alizadeh2014demand}.

% This paper focuses on the V2B scenario, aiming to develop an online policy for controlling charger charging powers by charging or discharging EVs to reduce the long-term electricity bill. The goal is to minimize both energy costs for smart building powers and EV charging under Time-of-Use (ToU) rate policies, while also reducing demand charges over a monthly period, all while meeting the charging requirements (SoC) of EV users. Given that our V2B scenario involves real-time online decision-making and uncertainty from unpredictable EV arrivals, varying SoC requirements, and fluctuating building powers, traditional solutions such as offline MILP optimization, scheduling approaches, and meta-heuristics~\cite{oconnell2010integration, AORC2013, 5986769, 9409126, MJG2015} struggle to provide real-time decisions or may fall short of optimal solutions. Recent RL-based approaches have been explored for similar problems~\cite{MMN2019, 9409126, SNDJ2020, NNM2024, ZJS2022}  , but they are often limited by the scale of the state and action spaces. Existing work tends to focus on simpler scenarios without accounting for long-term billing periods or demand charges, and often only considers limited decision spaces, such as discrete charge charging powers or decentralized training for individual chargers, missing opportunities for cooperation among chargers to optimize costs.  
% In collaboration with \nissan{}, we develop an RL-based approach to address practical V2B challenges in a smart building by optimizing charging power for diverse EV chargers (including bi-directional and unidirectional)
% , shown in~\Cref{fig: EV chargers},
% at each time interval. The objective is to minimize overall costs, including energy bills and demand charges, while ensuring EVs reach their required State of Charge (SoC) before departure, all within real-world energy pricing constraints. 

% {\color{black} 
% %This paper focuses on the V2B problem, where teams at Vanderbilt University and \nissan{} aims to develop an RL-based approach for optimizing charger charging powers in a smart building equipped with both unidirectional and bidirectional chargers, as shown in~\Cref{fig: EV chargers}. The objective is to minimize long-term electricity costs, including both building energy bills and EV charging costs under Time-of-Use (ToU) rate policies, along with monthly demand charges. Additionally, the policy ensures that EVs meet their required State of Charge (SoC) before departure. 

% This paper focuses on the V2B problem, where teams at Vanderbilt University and \nissan{} aim to develop an online approach for optimizing charger charging powers in a smart building equipped with both unidirectional and bidirectional chargers, as shown in~\Cref{fig: EV chargers}. We account for uncertainties in the process, such as unpredictable EV arrivals, varying State of Charge (SoC) requirements, and fluctuating building powers. Our objective is to minimize long-term electricity costs, encompassing both building's energy use and EV charging costs under Time-of-Use (ToU) electricity rate, along with demand charge which is levied on monthly peak power usage. We leverage the use of reinforcement learning (RL) techniques to solve this problem.
 
% %The V2B problem involves real-time, online decision-making and faces uncertainties such as unpredictable EV arrivals, varying SoC requirements, and fluctuating building powers. Traditional solutions, such as offline MILP optimization, scheduling methods, control-based approaches, and meta-heuristics~\cite{oconnell2010integration, AORC2013, 5986769, 9409126, MJG2015}, are not designed to deliver real-time decisions or achieve optimal outcomes in dynamic environments. While RL-based approaches have been explored in similar contexts~\cite{MMN2019, 9409126, SNDJ2020, NNM2024, ZJS2022}, they often limit the scale of the state and action spaces or overlook long-term billing periods and demand charges. Furthermore, many focus on discrete charging rates or decentralized control for individual chargers, missing opportunities for cost optimization through charger coordination. 

% %The V2B problem involves real-time, online decision-making and faces uncertainties such as unpredictable EV arrivals, varying SoC requirements, and fluctuating building powers. 
% Traditional solutions, such as offline MILP optimization, scheduling methods, control-based approaches, and meta-heuristics~\cite{oconnell2010integration, AORC2013, 5986769, 9409126, MJG2015}, are not effective for real-time decisions or optimal outcomes in dynamic environments. In contrast, RL-based approaches have been explored in similar contexts~\cite{MMN2019, SNDJ2020, ZJS2022}, and perform better. However, they often limit the size of the state and action spaces or overlook long-term rewards \cite{NNM2024, 9409126}, i.e., the monthly billing period and demand charge. 
% %Additionally, many focus on discrete charging rates or decentralized control for individual chargers, missing opportunities for cost optimization through charger coordination. 
% % To manage uncertainties in V2B scenarios, we employ a generative model trained on data collected from charger meters in the smart building (as illustrated in Figure~\ref{fig: EV chargers}), enabling the RL model to generalize across various scenarios.  
% % Our solution addresses real-world application challenges such as multi-agent decision-making, centralized control of up to 15 chargers, and continuous charging rate decisions to optimize costs while adapting to diverse user requirements. The model also aims to minimize total energy bills over a one-month period, adhering to demand charge policies and balancing multiple objectives. 

% Our solution addresses real-world application challenges, allowing continuous control of up to 15 heterogeneous chargers (unidirectional and bidirectional) to optimize costs. To enhance robustness, we integrate policy guidance into the RL training process, leveraging an efficient MILP solution to generate optimal actions that steer the policy in a favorable direction. Furthermore, we introduce action masking, which aims to limit the action exploration space and improve policy performance by utilizing domain-specific knowledge. Finally, we evaluate our proposed approach using real-world data, demonstrating its effectiveness in minimizing monthly energy bills over 9 months (May 2023 - Jan 2024).

% } 



%We summarize the contributions of this work as follows: 
\begin{itemize}[leftmargin=*]
% \vspace{-0.2in}
\item \textbf{Modeling the V2B problem as an MDP with continuous action space}: We model the V2B problem as a Markov Decision Process (MDP) that captures the dynamics of EV SoC levels, varying arrival and departure times, and time-dependent electricity pricing. This formulation addresses delayed and sparse rewards, continuous action spaces, and long-term goals to reduce the monthly peak demand charge and energy costs. 
\item \textbf{Solving the V2B sequential decision-making problem}: We present a novel RL framework based on the Deep Deterministic Policy Gradient (DDPG). We combine DDPG with i) action masking that leverages domain knowledge and the structure of the V2B problem and ii) policy guidance based on solving a deterministic MILP to aid the learning of the optimal policy.
%This  enables real-time decision-making in dynamic environments, optimizing the trade-off between minimizing energy costs and satisfying user SoC requirements.

%\item \textbf{Developing out-of-distribution generalization criteria}: We establish a set of criteria to identify when the RL model may fail to generalize, especially when encountering out-of-distribution data. This proactive mechanism allows for model adaptation, ensuring reliability and robustness in real-world applications. 
\item \textbf{Validating with real-world data}: We validate our proposed approach using real-world data from a major electric vehicle manufacturer. The model achieved significant cost savings
% of approximately \$1,270 
over nine months (May 2023–January 2024), meeting all user charging demands. Our approach outperforms heuristics and prior work.
\item \textbf{Ablation Study:} We conduct a detailed ablation study to assess the impact of each technique and demonstrate the model's effectiveness.

% \item We formulate the V2B problem as a Markov Decision Process (MDP) integrated with simulators that simulate real-world user behaviors, capturing historical and current states for buildings and chargers. The action-reward function is tailored to minimize energy costs while ensuring user requirements are met.

% \item We apply the Deep Deterministic Policy Gradient (DDPG) reinforcement learning approach to train a policy for the proposed MDP which we use to form the online V2B charging decisions. 
% %To enhance performance, we incorporate techniques such as action masking, policy guidance and importance sampling, facilitating valid actions and avoiding local optima. 
% To enhance performance, we incorporate techniques such as action masking, policy guidance, and importance sampling, facilitating valid actions and steering the model toward better solutions.

% \item We evaluate the DDPG model's performance across various real-world test datasets.
% The test data is generated using a generative model based on real-world statistics from \nissan{} and a smart building operator, covering a period of nine months from May 2023 to January 2024. 
% Additionally, we benchmark our process against multiple baselines and perform an ablation study to assess the impact of individual techniques which help us demonstrate the model's effectiveness. 
\end{itemize} 

 % The rest of the paper is organized as follows: Section \ref{sec:problem_statement} introduces the V2B problem and how we model it as a MDP. We discuss related work and state-of-the-art in Section~\ref{sec:related_work}. Section \ref{sec:approach} defines our approach to solving the problem based on the stated assumptions. Finally, the experimental setup and results are discussed in Section \ref{sec:experiments_and_results} followed by the conclusion in Section~\ref{sec:conclusion}.

