\begin{table}[hpt]
\centering
\small
\caption{Hyperparameters and selected values.}
\begin{tabular}{|>{\raggedright}p{1.7cm}|p{4cm}|p{1.3cm}|}
% \begin{tabular}{|@{}c|c|c@{}|}
\hline
Parameter & Description & Range \\ 
\hline
Actor network & Number of units at each layer & [96, 96] \\\hline
Critic network & Number of units at each layer & [96, 96] \\\hline
$\Gamma$ & Discount factor for future reward & 1 \\\hline 
Actor\&Critic learning rate &Learning rate for updating actor and critic networks & $10^{-5}$, $10^{-3}$ \\\hline 
 % & Tuning parameter  & 1e-3 \\\hline 
{\it bufferSize} & Batch size for fetching transitions from replay buffer & 64 \\\hline 
{\it batchSize} & Size of the replay buffer & $10^6$ \\\hline 
{\it actionNoise} & Noise added during action exploration & normal(0,0.2)\\\hline 
$\policyGuidanceRate$ & Probability to introduce policy guidance & 0.5 or 0.7 \\\hline  
% Max episodes & Total iterations for training RL model & 5000 \\\hline
% Collect steps & Number of steps run before training actor/critic network once & 10 \\\hline 
% Action noise & Noise added to action generated by actor-network & 0.2 \\\hline  
$\lambda_{S}$, $\lambda_{B}$, $\lambda_{D}$ & Penalty coefficients for SoC requirement, bill cost, and demand charge &1,1, 3\\\hline 
Random seed & Random seed for actor and critic network initialization  &0-5\\\hline  
Adjustment of $\PrdPeak$& Lower bound of the 99\% confidence interval for the optimal monthly peak power based on training data & Increased by 0\%, 5\%, 10\% \\\hline  
${\it trainStep}$, ${\it updateStep}$& Training steps and steps per update of target networks& 5, 5\\\hline  


\end{tabular}
\label{tab:hyperparameters}
\end{table} 
