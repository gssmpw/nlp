%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
% \documentclass[manuscript,screen,review]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

% \documentclass[sigplan,authordraft]{acmart}
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix}
\usepackage{titling}
\usepackage{tikz}
\usepackage{multirow} 
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algpseudocode}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{tabularx}
\usepackage{authblk}
% \usepackage{draftwatermark}
% \usepackage{titling}
\microtypecontext{spacing=nonfrench}

% \SetWatermarkText{Unpublished working draft \\ Not for distribution}
% \SetWatermarkText{}
% \SetWatermarkScale{0.4}
% \SetWatermarkLightness{0.8}

\usepackage{enumitem} % Add this line to your preamble
\usepackage{pifont}
\usepackage{placeins}
% Define commands for tick and cross
\newcommand{\tick}{\ding{51}} % Tick symbol ✔
\newcommand{\cross}{\ding{55}} % Cross symbol ✘
\newcommand{\mixed}{\ding{51}\kern-0.62em\ding{55}} % Mixed symbol (tick and cross overlay)

\usepackage{tikz}
\newcommand*\circled[1]{%
  \scalebox{0.78}{\begin{tikzpicture}[baseline=-3pt]
    \node[draw,circle,inner sep=0.5pt, fill=black] {\textcolor{white}{\textsf{\textbf{#1}}}};
  \end{tikzpicture}}}

\usepackage{xcolor}
\newcommand{\yeqi}[1]{\textcolor{orange}{Yeqi: #1}}
\newcommand{\cj}[1]{\textcolor{blue}{Congjie: #1}}

% reduce space below title
 \usepackage{etoolbox}
 
 \makeatletter
 \patchcmd{\maketitle}
 	{\@maketitle}
 	{\vspace{-7em}\@maketitle\vspace{-5em}}% change the value as needed
 	{}
 	{}
 \makeatother

\newcommand{\todo}[1]{{\textcolor{red}{[~TODO:~#1~]}}}
\newcommand{\ncite}[1]{{\textcolor{blue}{[~Cite]}}}


% \makeatletter
% \def\UrlAlphabet{%
%       \do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
%       \do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
%       \do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
%       \do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
%       \do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
%       \do\Y\do\Z}
% \def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}
% \g@addto@macro{\UrlBreaks}{\UrlOrds}
% \g@addto@macro{\UrlBreaks}{\UrlAlphabet}
% \g@addto@macro{\UrlBreaks}{\UrlDigits}
% \makeatother

\algdef{SE}[LOOP]{Loop}{EndLoop}[1]{\textbf{loop} #1}{}

\setlength{\droptitle}{-2cm}

\begin{document}

\newcommand{\meshtp}{MeshTP\xspace}
\newcommand{\meshpp}{MeshPP\xspace}
\newcommand{\gemm}{MeshGEMM\xspace}
\newcommand{\gemv}{MeshGEMV\xspace}
\newcommand{\sys}{WaferLLM\xspace}


\input{utils}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\sys: A Wafer-Scale LLM Inference System}
\date{}


% \author{\#203}
% \author[1]{Congjie He\thanks{congjie.he@ed.ac.uk}}
\author{
    Congjie He\textsuperscript{1}, 
    Yeqi Huang\textsuperscript{1}, 
    Pei Mu\textsuperscript{1}, 
    Ziming Miao\textsuperscript{2}, 
    Jilong Xue\textsuperscript{2}, 
    Lingxiao Ma\textsuperscript{2}, 
    Fan Yang\textsuperscript{2}, 
    Luo Mai\textsuperscript{1}
}

\affil{\textsuperscript{1}University of Edinburgh \hspace{1cm} \textsuperscript{2}Microsoft Research}

\maketitle

\begin{abstract}
Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh-based architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to fully exploit these accelerators.

We introduce \sys, the first wafer-scale LLM inference system. \sys is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, \sys pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.

Evaluations show that \sys achieves 200$\times$ better wafer-scale accelerator utilization than state-of-the-art systems. On a commodity wafer-scale accelerator, \sys delivers 606$\times$ faster and 22$\times$ more energy-efficient GEMV compared to an advanced GPU. For LLMs, based on 16-bit data type, \sys achieves 2700 toks/sec/req decode speed on Llama3-8B model and 840 toks/sec/req decode speed on Qwen2-72B model, which enables 39$\times$ faster decoding with 1.7$\times$ better energy efficiency. We anticipate these numbers will grow significantly as wafer-scale AI models, software, and hardware continue to mature.

\end{abstract}

\input{sections/introduction}

\input{sections/background_motivation}

\input{sections/device_mode}

\input{sections/model_parallelism}

\input{sections/meshgemm}

\input{sections/meshgemv}

\input{sections/evaluation}

\input{sections/conclusion}

\clearpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{plain}
\bibliography{main}

%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
