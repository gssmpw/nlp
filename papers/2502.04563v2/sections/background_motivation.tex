\section{Background and Motivation}\label{sec:motivation}

\subsection{LLM inference and its key constraint}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{imgs/General-LLM.pdf}
    \vspace{-0.3cm}
    \caption{Key components in LLM inference}
    \label{fig:llm-inference-overview}
    \vspace{-0.3cm}
\end{figure}

An LLM inference system typically performs auto-regressive token-by-token generation, as illustrated in Figure~\ref{fig:llm-inference-overview}. The model comprises multiple transformer layers, dominated by self-attention and feedforward blocks. Inference operates in two phases: prefill and decode. The total cycles of the prefill phase are dominated by GEMM operations (shown by \circled{1}). Similarly, the total cycles of the decode phase are dominated by GEMV operations (shown by \circled{2}).

LLM inference is memory bandwidth-bound. Model weights (10–100 GB) must be repeatedly fetched from external memory during inference, as GPUs typically have only ~100 MB of on-chip memory. Generating a single token requires transferring tens of GBs, and producing thousands of tokens per second demands hundreds of TB/s bandwidth, far exceeding the capabilities of HBM on current GPUs. 

While tensor parallelism across GPUs can increase bandwidth, mitigating communication bottlenecks in a large GPU cluster remains challenging. Also, adding GPUs improves throughput for concurrent queries but does not reduce response time, as each query is still memory bandwidth-limited.

\vspace{-3mm}
\subsection{Reasons for wafer-scale accelerators}
\vspace{-1mm}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[t]
\resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|}
\hline
 & System-on-Die & System-on-Wafer \\ \hline
Chip Area & 400 $\sim$ 800 mm$^2$ & 30,000 $\sim$ 160,000 mm$^2$~\cite{dojo} \\ \hline
\# Transistors & 10s Billions & Trillions \\ \hline
\# Cores & 1,000 $\sim$ 10,000s & 100,000 $\sim$ 1,000,000s \\ \hline
% AI Compute & Hundreds teraFLOPS & Hundreds petaFLOPS \\ \hline
On-Chip Memory & 100s MBs  & 10s GBs  \\ \hline
Memory Bandwidth & TBs/s & 10s PBs/s \\ \hline
Attached HBM & 10s GBs  & TBs (via TSMC SoW)  \\ \hline
Die-to-Die Bandwidth & 100s GBs/s (via PCB) & 10s TB/s (via Wafer) \\ \hline
Die-to-Die Power & 10s pJ/bit (via PCB) & 0.1s pJ/bit (via Wafer) \\ \hline
\end{tabular}%
}
    \vspace{-2mm}
\caption{System-on-Die vs. System-on-Wafer}
    \vspace{-3mm}
\label{tab1:die-scale-vs-wafer-scale}
\end{table}


To increase memory bandwidth, accelerator designers are increasingly adopting system-on-wafer integration~\cite{tsmc-advanced-packaging} for several reasons:

\mypar{Performance advantages} System-on-wafer technology allows trillions of transistors to be integrated into a single wafer-scale chip—100$\times$ more than a typical GPU die, shown in Table~\ref{tab1:die-scale-vs-wafer-scale}. This enables millions of AI-optimized cores, providing tens of GBs of on-chip memory and up to tens of PB/s memory bandwidth—1,000$\times$ higher than a standard GPU’s several TB/s. Future wafer-scale chips can also attach 40–80$\times$ more HBM chips to their edge compared to a standard die~\cite{tsmc-advanced-packaging}.

\mypar{Integration efficiency} System-on-wafer excels at integrating massive parallel cores, with wafer-based die-to-die connections offering up to 10$\times$ more bandwidth per unit area and nearly 100$\times$ better power efficiency per bit than conventional PCB-based I/O (e.g., NVIDIA NVLink), as shown in Table~\ref{tab1:die-scale-vs-wafer-scale}.

\mypar{Lower cost} Wafer-scale integration can lower the manufacturing cost, since the significant fraction of the cost of fabrication (typically 30-50\%) is related to testing and packaging the individual chips~\cite{wiki:waferscale}. 
Additionally, wafer-scale integration has made notable progress in yield improvement. Companies such as TSMC are also developing techniques to integrate fully tested dies on a single wafer, further enhancing yield.
\vspace{-0.2cm}
\subsection{Challenges for wafer-scale LLM inference}

The key challenge in leveraging wafer-scale accelerators for LLM inference is their shift to a distributed, non-uniform memory architecture on a single chip. Current LLM systems are optimized for shared memory (single chip) or fully connected architectures (e.g., GPU pods), as shown in Figure~\ref{fig:mesh-architecture}(a). However, as on-chip memory size grows, these architectures face exponential manufacturing costs and performance degradation, driving the need for a distributed on-chip architecture.

AI accelerator designers predominantly use a \textbf{mesh-like network-on-chip (NoC)} to connect \textbf{massive cores} (ranging from hundreds of thousands to millions), as shown in Figure~\ref{fig:mesh-architecture}(b). The mesh topology is favored for its efficiency in core arrangement, enabling effective cooling\cite{cooling}, power delivery~\cite{powerdelivery}, and cost-efficient wiring~\cite{chip-cost-1, chip-cost-2}, with each core communicating only with nearby neighbors, as shown in Figure\ref{fig:mesh-architecture}(b). Alternative topologies, such as 3D torus or tree structures, are impractical due to high on-chip wiring costs. Therefore, wafer-scale chip makers such as Cerebras WSE\cite{wse} and Tesla Dojo\cite{dojo} adopt massive-scale mesh architectures. Even non-wafer-scale accelerators such as Meta MTIA~\cite{mtia}, Tenstorrent~\cite{tenstorrent}, and others~\cite{amd-xdna, azure-maia} use mesh to scale cores on a chip.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{imgs/die_on_wafer.pdf}
    \vspace{-3mm}
    \caption{Massive-scale mesh-based memory architecture}
    \vspace{-3mm}
    \label{fig:mesh-architecture}
\end{figure}

The massive-scale mesh architecture presents challenges for several LLM operations due to their high data movement demands: (i)~managing LLM models and KV cache, (ii)~GEMM operations during the prefill phase, and (iii)~GEMV operations during decoding. Other operations, such as element-wise computations such as dot-product and activation functions, require no data movement and naturally benefit from parallelism. Operations needing allreduce, such as RMSNorm and Softmax, can leverage GEMV solutions.