\vspace{-0.3cm}
\section{Device Model for Wafer-Scale Accelerators}
\vspace{-0.2cm}
% that differentiate wafer-scale accelerators from traditional shared-memory and NUMA architectures.

\subsection{The PLMR model}
\vspace{-0.1cm}
We develop the PLMR model to capture the unique hardware properties of wafer-scale accelerators and to motivate system requirements needed for utilizing this emerging hardware.

\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]
    \item \textbf{Massive Parallelism (P)}:
A wafer-scale accelerator can easily be equipped with millions of parallel cores, compared to thousands in GPUs. Each core features a local hardware pipeline that overlaps data ingress, egress, computation, and memory access at the cycle level.
This requires the computation to be partitioned at a massive scale and a fine-grained schedule to overlap computation, memory access, and NoC communication.

\item \textbf{Highly non-uniform memory access Latency (L)}:
Accessing memory on other cores in a mesh exhibits highly non-uniform latency. In a mesh with \(N_w \times N_h\) cores, the maximum NoC hops to a remote core is \(\max(N_w, N_h)\). For a million-core mesh, this can reach 1000 hops, causing a 1000$\times$ latency difference between local and remote memory access.
Therefore, it is crucial for the computation to minimize long-range communication whenever possible.

\item \textbf{Constrained local Memory (M)}: Each core has a small local memory (tens of KBs to several MBs), as performance and energy efficiency decline with larger capacities~\cite{sram-wiki}.
As a result, computation data must be explicitly partitioned into fine-grained chunks to fully fit within the constraints of each core's local memory.

\item \textbf{Constrained Routing resources (R)}: 
The message size in the NoC of a wafer-scale accelerator is extremely limited (e.g., a few bytes). This constraint requires message headers (e.g., address encoding) to be restricted to just a few bits, maximizing the capacity for actual data transfer. Consequently, only limited routing paths can be used, and the software system must carefully plan these paths.
\end{enumerate}

We expect these properties to remain relevant, as they are rooted in the fundamental characteristics of hardware and its manufacturing process. The PLMR model applies to both current (Cerebras WSE) and future (Tesla Dojo) wafer-scale devices. Even some non-wafer-scale devices with mesh-based NoC architectures, such as Tenstorrent Blackhole~\cite{tenstorrent}, can be represented by PLMR with adjusted parameters for parallelism (P), the size of the mesh (L), or relaxed constraints on local memory (M) and routing resources (R).


    \vspace{-3mm}
\subsection{Limitations of state-of-the-art approaches}
    \vspace{-1mm}
    
Leveraging the PLMR model, we analyze why existing AI systems fail to fully utilize wafer-scale accelerators.
To run an LLM model on a wafer-scale accelerator, we generally have two choices: (i) abstract the distributed local memory in each core as a shared memory and directly access data placed in a remote core through NoC; and (ii) explicitly partition computation into distributed cores and use message passing to exchange necessary data. 
We analyze two types of representative systems: LLM runtime or DNN compilers for shared memory architecture such as GPUs, e.g., Ladder~\cite{ladder}; and the SOTA compiler for distributed on-chip memory architectures, e.g., T10~\cite{t10} for GraphCore IPU.

\mypar{Shared-memory system} 
A shared-memory-based DNN compiler such as Ladder usually assumes a uniform memory access pattern within the underlying memory hierarchy, which cannot tolerate the 1000$\times$ latency variance in wafer-scale accelerators when accessing data from remote memory (failing in L).
Moreover, these compilers~\cite{tvm,rammer,ansor,flextensor,roller,welder,ladder} often focus primarily on partitioning computation, with less emphasis on optimizing data partitioning. This approach can easily lead to significant data duplication and violate the memory constraint requirements (failing in M).
Finally, these compilers are unaware of the communication distance of each core, poorly addressing the constraint of routing resources.

\mypar{Distributed-memory system} The T10 system~\cite{t10} is designed for AI accelerators with an on-chip crossbar which ensures a constant hop of memory access to other cores on the same chip. T10 handles small local memory and balances communication loads, addressing memory constraints (M) and routing resource limits (R). However, on a PLMR device, it fails to account for varying hop distances (failing in L) and scales to thousands, not millions, of cores (failing in P).