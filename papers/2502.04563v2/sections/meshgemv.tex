\vspace{-0.3cm}
\section{Wafer-Scale GEMV}\label{sec:gemv}
\vspace{-0.2cm}

In this section, we describe \gemv, a scalable GEMV algorithm for PLMR devices.


\subsection{PLMR compliance in distributed GEMV}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{imgs/reduce-algo.pdf}
    \caption{PLMR compliance in distributed GEMV}
    \label{fig:gemv-plmr-compliance}
\end{figure}

The completion time of a distributed GEMV is primarily determined by an allreduce operation that aggregates partial results from all selected cores and broadcasts the aggregated results back to all cores. So, we define the number of add-operations (hops) in the longest aggregation path as the \emph{critical path} in GEMV. Below, we analyze common distributed GEMV implementations in LLM systems and demonstrate that \gemv is the only approach fully compliant with the PLMR model.

\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]

 \item \textbf{GEMV with pipeline allreduce} is commonly used in TPU pod systems~\cite{google2023efficiently} and as the default in Cerebras demo~\cite{cerebrasgemv}. As shown by \circled{1} in Figure~\ref{fig:gemv-plmr-compliance}, it bounds routing resource usage to $O(1)$ per core (meeting R in PLMR). However, its longest aggregation path is from tail to head cores, as shown in the red line, and spans the critical path at $O(N)$, violating the L property.

\item \textbf{GEMV with ring allreduce} is commonly used in GPU pod systems, where it is the default configuration. As shown by \circled{2} in Figure~\ref{fig:gemv-plmr-compliance}, it bounds routing resource usage to $O(1)$ (meeting R in PLMR). However, it spans $O(N)$ hops in the critical path, violating the L property.

\item \textbf{GEMV with two-way K-tree allreduce (Ours)}. As shown by \circled{4} in Figure~\ref{fig:gemv-plmr-compliance}, we build a balanced K-tree to reduce from two-way; its longest aggregation path is from the head or tail core to the tree root core. The critical path is $O(\sqrt[K]{N}K)$ which can address the L. The max number of communication paths at each root core is $O(K)$, and can meet the R limitation by adjusting the K.

\end{enumerate}


\subsection{The \gemv algorithm} 

We will outline the key steps of \gemm below:

\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]
\item \textbf{Initialization:} Consider $C = A \times B$ and $A$ is a vector. \gemv will partition $B$ into tiles $B_{sub}$ along two dimensions, forming $N \times N$ tiles and distributed across the cores. For $A$, \gemv will partition it along the vector length, forming $N$ tiles distributed on one axis and replica $A$ on another axis. Each core receives one tile of $A_{sub}$ and one of $B_{sub}$. Then we determine which cores form a group to obtain aggregated results in each phase based on the K-tree.

\item \textbf{Parallel computation:} In this stage, each core performs a local GEMV $A_{\text{sub}} \times B_{\text{sub}}$ to obtain $C_{sub}$ partial sum.

\item \textbf{Aggregation:} The aggregation step primarily involves using the two-way K-tree allreduce we design. The key steps as follows: (i)~In the 1st-phase, each group performs group reduction and obtains the partial sum of $C_{sub}$ at the root core of each group. (ii)~In the $k$th-phase, the results from the $(k-1)$~th-phase are reduced to the root cores of each group in the $k$th-phase. After K times repeating, $C$ can be obtained by concatenating the $C_{sub}$ from all K-tree root cores. (iii)~ Optionally, a broadcast operation from the root core of the $K$-tree may follow, depending on whether continuous GEMV is required.
\end{enumerate}

\mypar{Scalability Analysis}  
As shown in \circled{1} of Figure~\ref{fig:gemv-plmr-compliance}, this method scales efficiently with parallelism and meets the L property by selecting an appropriate $K$. It requires $K+1$ paths at the tree root core but allows flexible adjustment of $K$ to address R based on hardware limitations.  

However, a larger $K$ is not always better, as it depends on $N$ and R constraints. Additionally, larger $K$ increases routing complexity and overhead. Considering these factors, we have chosen $K=2$ for our current implementation evaluated in the following sections.
