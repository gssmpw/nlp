    \vspace{-3mm}
\section{Wafer-Scale GEMM} \label{sec:gemm}
    \vspace{-1mm}

In this section, we introduce MeshGEMM, a scalable distributed GEMM for massive-scale, mesh architectures. 

    \vspace{-3mm}
\subsection{PLMR compliance in distributed GEMM}
    \vspace{-1mm}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/gemm-analysis.pdf}
    \vspace{-3mm}
    \caption{PLMR compliance in distributed GEMM}
    \vspace{-3mm}
    \label{fig:distributed-gemm-analysis}
\end{figure}

To identify an scalable distributed GEMM for PLMR devices, we define the following metrics: 
(i)~\emph{Paths per core}: The number of routing paths per core, with fewer paths ensuring compliance with the R property.
(ii)~\emph{critical path}: The longest communication path in each step to transmit submatrix (as the red lines in Figure~\ref{fig:distributed-gemm-analysis}), with fewer hops adhering to the L property.
(iii)~\emph{Memory per core}: The memory required per core, with lower usage ensuring the M property.

We analyze current distributed GEMM methods and show how MeshGEMM meets these metrics:

\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]
\item \textbf{GEMM via Allgather} is commonly used in GPU and TPU pods for distributed GEMM~\cite{google2023efficiently, tensorrt-llm, megatron}. Its longest communication path in each step is one core gathering data from the farthest cores, shown as the red line in Figure~\ref{fig:distributed-gemm-analysis}~\circled{1}, and $N$ steps to complete the allgather. Each core creates $N$ communication paths to neighbors in its row and column (violating R). The gather in each step spans the critical path with $O(N)$ hops (violating L), and each core uses $O(1/N)$ memory due to inflated working buffers, far exceeding the $O(1/N^2)$ for local submatrices (violating M).

\item \textbf{SUMMA} is Cerebras' default choice for distributed GEMM on its wafer-scale engine~\cite{cerebrasgemm}. Its longest communication path in each step is where one core broadcasts data to the farthest core along the column or row, shown by the red line in \circled{2} of Figure~\ref{fig:distributed-gemm-analysis}. Every core creates $N$ communication paths (violating R) and spans the critical path with $O(N)$ hops (violating L) in the longest path. While SUMMA improves memory usage compared to AllGather, requiring only a working set equal to the size of locally partitioned submatrices, it still doubles memory usage.

\item \textbf{Cannon} is mesh-optimized choice for distributed GEMM~\cite{cannon}, popular in supercomputers. 
Its longest communication path in each step is the head cores send data to the tail cores.
% It needs $N$ steps submatrices shifting to complete GEMM on cores. 
As shown in \circled{3} of Figure~\ref{fig:distributed-gemm-analysis}, each core communicates with two neighbours in a 2D torus, and only needs $O(1)$ communication paths and optimal memory usage of $O(1/N^2)$. However, it incurs the critical path with $O(N)$ hops as the red line, violating L.

\item \textbf{MeshGEMM (Ours)} is a distributed GEMM which complies with the PLMR model. Its longest communication path in each step is shown as the red line in \circled{4} of Figure~\ref{fig:distributed-gemm-analysis}. Each core communicates with two neighbors, two hops away (proven in later sections to be scalable for mesh architectures). This design achieves $O(1)$ communication paths per core needed and optimal memory usage of $O(1/N^2)$, similar to Cannon. Crucially, it bounds the critical path to 2 hops with $O(1)$ complexity, making it uniquely capable of addressing the L property.

\end{enumerate}

\vspace{-0.2cm}
\subsection{Design intuitions and scalability analysis} 

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{imgs/gemm_intuition.pdf}
    \caption{Design intuitions and scalability analysis.}
    \label{fig:gemm_intuition}
\end{figure}

Our design involves two steps: (i)~We ensure algorithm correctness using a cyclic shifting process for GEMM, and (ii)~We prove that two-hop communication on this cycle is the minimal distance required to satisfy the L property.

\mypar{Cyclic shifting} Cyclic shifting enables \gemm to satisfy the M and R properties by limiting communication to two neighbors and minimizing memory usage. It ensures correct GEMM results, following reasoning similar to Cannon~\cite{cannon}. As illustrated in \circled{3} of Figure~\ref{fig:distributed-gemm-analysis}, a logical circle of 5 cores is flattened into the physical communication mapping, with a critical path from head core to tail core. 

\input{algorithms/interleave_func}

\mypar{Interleaving} For the flatten communication plan, we would like to minimize the length of the critical path further, thus satisfying the L property. Our key intuition here is to introduce an INTERLEAVE operation to find the mapping relationship from logical to physical, defined in Algorithm~\ref{alg:interleave}. As shown by \circled{1} of Figure~\ref{fig:gemm_intuition}, MeshGEMM first insert core 1 in between core 0 and 4 and core 2 in between core 4 and 3 to form a logical mapping, and then call the INTERLEAVE operation to get the send to and receive from neighbours' index, resulting in a permutated, equivalent communication plan as shown by \circled{2} in Figure~\ref{fig:gemm_intuition}. For example, there are 5 cores total (N=5), so physical core 2 (index=2) sends data to physical core 4 (send\_index=4) and receives from physical core 0 (recv\_index=0).


\mypar{Scalability analysis}  We can prove that the two-hop distance created by INTERLEAVE cannot be further reduced. The proof relies on the fundamental properties of sequential arrangements: if we attempt to create a circular sequence where each number differs from its neighbors by exactly one hop, we encounter a mathematical impossibility. This can be understood by visualizing the numbers as points on a line - while adjacent numbers can be connected, the endpoints of the sequence cannot simultaneously maintain single-hop differences with their neighbors while forming a circle.

Note that our discussion, based on a 1D array, naturally extends to a 2D mesh, as the 1D array corresponds to the mesh's X-axis and Y-axis due to their symmetry.

\vspace{-0.3cm}
\subsection{The \gemm algorithm} \label{subsec:meshgemm} 

We outline the key steps of \gemm below:

\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]
\item \textbf{Initialization:} Consider $C = A \times B$. \gemm will partition $A$ and $B$ into tiles $A_{sub}$ and $B_{sub}$ along two dimensions, forming $N \times N$ tiles, which are distributed across the cores. Each core receives one tile of $A_{sub}$ and one of $B_{sub}$. \gemm will then use INTERLEAVE to initialize the neighbor's positions for each core. 

\item \textbf{Alignment:}  
Each core will then align with neighbors to align the input submatrices in a way that ensures every core in the distributed system begins with the appropriate operands for the matrix multiplication process. 

\item \textbf{Compute-shift loop:}  
Each core operates with a compute-shift loop involving $N$ steps of communication and computation. In each step, every core computes the partial sum of its corresponding $C_{sub} = A_{sub} \times B_{sub} + C_{sub}$. Meanwhile, shift $A_{sub}$ along the X-axis and $B_{sub}$ along the Y-axis to get new $A'_{sub}$ 
 and $B'_{sub}$ for the next step computation as \circled{3} we shown in Figure~\ref{fig:gemm_intuition}. After $N$ steps, the accumulated $C_{sub}$ is returned.
\end{enumerate}

\vspace{-0.3cm}
\subsection{Implementation details}

\mypar{Handling non-square mesh} For a non-square mesh $N_h \times N_w$ ($N_h \neq N_w$), the $A$ and $B$ matrices can be logically partitioned into $N_{lcm} \times N_{lcm}$ cores, where $N_{lcm}$ is the least common multiple of $N_h$ and $N_w$.

\mypar{Transposed distributed GEMM} The above algorithm key steps can be applied to the computation of $C = A \times B^T$, the dist-GEMM-T in Figure~\ref{fig:prefill-partition} to avoid transposing $B$ on mesh. It does not require alignment before computation and only necessitates $N$ steps two-hop compute-shift for the right matrix $B$ along the Y-axis. After each shift step, each core computes $C_{\text{sub}} = A_{\text{sub}} \times B_{\text{sub}}$, followed by a ReduceAdd of $C_{\text{sub}}$ along the X-axis. After $N$ steps, the final matrix $C$ is obtained.