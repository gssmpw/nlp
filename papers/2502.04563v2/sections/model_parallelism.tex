
    \vspace{-3mm}
\section{Wafer-Scale LLM Parallelism}\label{sec:meshmp}
    \vspace{-1mm}
    
We present wafer-scale LLM parallelism, featuring new designs across prefill, decode and KV cache management.


    \vspace{-3mm}
\subsection{Prefill parallelism}
    \vspace{-1mm}

The parallelism for LLM prefill must ensure compliance with the PLMR model. Key challenges include: (i)~Handling multiple large matrices during prefill, requiring effective dimension partitioning to achieve million-core parallelism (P); (ii)~Optimizing GEMM operations, which involve further partitioning and overlapping computation and communication, to minimize long-range communication overhead (L), respect local memory constraints (M), and account for limited routing resources (R); and (iii)~Handling matrix transposes, which are costly on a NoC (L) but often required for sequential GEMM operations.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.46\textwidth]{imgs/prefill-partition.pdf}
    \vspace{-2mm}
    \caption{Prefill parallelism plan. $E_xF_y$ represents a matrix of shape $EF$, where the $E$ dimension is partitioned along the $x$-axis of cores, and $F$ along the $y$-axis of cores on a mesh.}
    \vspace{-5mm}
    \label{fig:prefill-partition}
\end{figure}

\mypar{Designing fine-grained partitioning for million-core parallelism} To achieve high chip utilization, we propose partitioning two dimensions of the input activation and weight matrices along both the $X$- and $Y$-axes of cores. This approach enables finer-grained, million-scale parallelism compared to existing methods~\cite{flashattn2, flashdecoding++, megatron, google2023efficiently}, which typically partition only the embedding dimension, resulting in insufficient parallelism on PLMR devices. 

We illustrate this partitioning using self-attention and feedforward, as shown in Figure~\ref{fig:prefill-partition}. For this discussion, we define the following annotations: the input activation $A$ and weight $W$ are multi-dimensional tensors during the prefill process. $B$ represents the batch size, $L$ the sequence dimension, $E$ the embedding dimension, $H$ the head dimension, and $F$ the hidden dimension in the feedforward block.
As shown by \myc{1}, the partitioning layout of $A$ is represented as $BL_yE_x$, where the $L$ dimension is partitioned along the $Y$-axis of cores, and the $E$ dimension along the $X$-axis of cores. Similarly, all weight matrices ($W_Q$, $W_K$, $W_V$, $W_{in}$, and $W_{out}$) are partitioned across both dimensions. 


\mypar{Designing PLMR-compliant distributed GEMM} 
We propose replacing conventional GEMM operators, designed for shared memory architectures, with a newly designed PLMR-compliant distributed GEMM during the prefill phase (as shown in \myc{2} of Figure~\ref{fig:prefill-partition}). Unlike TPU and GPU systems that primarily rely on allgather operations for GEMM, PLMR-compliant distributed GEMM algorithms achieve high NoC bandwidth utilization while respecting local memory and routing constraints, ensuring compliance with the L, M, and R properties. This PLMR-compliant distributed GEMM is fully described in Section~\ref{sec:gemm}.

\mypar{Using transposed distributed GEMM to avoid matrix transpose}
We propose a transpose-free parallelism plan for prefill to avoid matrix transpose, a common operation in LLM systems designed for shared memory architectures. The L property in PLMR highlights that matrix transposition is particularly costly on a wafer-scale device. It requires a core on one corner of the mesh to send data to the opposite diagonal corner, creating a long-range communication path.

Our transpose-free parallelism plan leverages transposed distributed GEMM (denoted as dist-GEMM-T)~\cite{summa, trans-dist} to compute $Q@K^T$ during LLM prefill, as shown by \myc{3} in Figure~\ref{fig:prefill-partition}. Specifically, the intermediate $Q$ and $K$ tensors, generated by multiplying $X$ with $W_Q$ and $W_K$, require transposing $K$ before proceeding with dist-GEMM operations due to the on-chip partition shape.


    \vspace{-3mm}
\subsection{Decode parallelism}
    \vspace{-1mm}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.46\textwidth]{imgs/decode-partition.pdf}
    \vspace{-2mm}
    \caption{Decode parallelism plan. $E^yF_x$ indicates the $E$ dimension is replicated along the $y$-axis, and $F$ is partitioned along the $x$-axis.}
    \vspace{-5mm}
    \label{fig:decode-partition}
\end{figure}

The parallelism strategy for LLM decode must address its memory-bandwidth-intensive nature, presenting several challenges: (i)~Decode uses smaller matrices than prefill due to limited input sequences and batch sizes, requiring careful parallelization when dimensions are insufficient for partitioning; (ii)~The phase heavily relies on GEMV operations, which are less compute-intensive than GEMM, resulting in short computation phases with limited overlap with communication, making GEMV vulnerable to long-range communication overhead on a NoC (L) and requiring adherence to local memory and routing constraints (M and R); and (iii)~Sequential GEMV operations introduce costly matrix transpose on a NoC, risking violation of the L property.

\mypar{Designing fine-grained replication to enable parallelism at minimal communication cost} When tensor dimensions are insufficient to achieve the high parallelism required for decode, we propose fine-grained replication of tensors in LLMs, specifically replicating the sequence dimension, where the sequence length equals the prompt length during prefill phase and equals 1 during the decode phase. This approach offers two key advantages: (i)~It improves parallelism and ensures balanced loads across all cores, and (ii)~It avoids additional communication operations such as allreduce. As shown by \circled{1} in Figure\ref{fig:decode-partition}, the $E$ dimension is partitioned along the $y$-axis, and the $L$ dimension is replicated along the $x$-axis, represented as $BE_yL^x$. Weight matrices $W$ are partitioned across both dimensions, consistent with the prefill phase.

Our fine-grained replication differs from recent work on long-context/sequence inference systems~\cite{loongserve, distserve}, which selectively replicate certain dimensions during the prefill phase rather than the decode phase.

\mypar{Designing PLMR-compliant distributed GEMV} We found that existing GEMV implementations fail to fully comply with PLMR requirements due to long-range communication and excessive routing resource consumption at each core. To address this, we propose a PLMR-compliant distributed GEMV, utilizing this new implementation throughout the decode phase (as detailed in \myc{2} of Figure~\ref{fig:decode-partition}). A comprehensive description of this GEMV design is provided in Section~\ref{sec:gemv}.

\mypar{Pre-optimizing model weight placement to avoid matrix transpose}
To avoid matrix transpose during decode, we propose pre-optimizing the model weight layout for decode, particularly for the distributed GEMV operation, to eliminate matrix transpose. While this introduces re-placement overhead between prefill and decode phases, the overhead is far smaller than that of sequential matrix transpose during token generation.

Figure~\ref{fig:decode-partition} illustrates this proposal, detailed in \myc{3}. Specifically, we optimize the placement of weights such as $W_{O}$ and $W_{out}$ for distributed GEMV in decode, differing from their layout in the prefill phase. This approach also removes the need for transpose operations in calculating $Q@K^T$ during decode self-attention.

    \vspace{-3mm}
\subsection{Shift-based KV cache management}\label{sec:parallel:kvcache}
    \vspace{-1mm}
    
\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{imgs/KVManage.pdf}
    \vspace{-9mm}
    \caption{KV cache concatenation vs. KV cache shift}
    \vspace{-5mm}
    \label{fig:kv_cache_update}
\end{figure}

KV cache management on PLMR devices is challenging as it requires storing large data across distributed cores while adhering to local memory constraints (M) and distributing KV cache computations to achieve high parallelism (P). To address these, we have the following insights:

\mypar{Existing concatenate-based management causes skewed core utilization} Current KV cache management methods primarily concatenate newly generated KV vectors to the existing cache. While efficient in shared memory architectures, this concatenate operation leads to highly skewed core utilization on PLMR devices, as shown in \circled{1} of Figure~\ref{fig:kv_cache_update}, where only core in a row is responsible for storing and computing over the newly generated KV vector. After several token generation steps, this only core quickly becomes the bottleneck, as depicted in \circled{2} of Figure~\ref{fig:kv_cache_update}, causing skewed memory usage and violating the M in PLMR. Moreover, the imbalanced KV cache distribution across cores results in inefficient parallelism, violating the P property.


\mypar{Proposing shift-based management for balanced core utilization} We propose a shift-based KV cache management strategy that evenly distributes cache data across all cores. Instead of concatenating new KV cache vectors at the end, this method performs a balancing shift operation, where each row transfers the oldest KV cache data to the row above, as shown in \circled{3} of Figure~\ref{fig:kv_cache_update}. When new KV data arrives, each core checks its local capacity against its neighbors. If equal, upward shifts are triggered, with each row receiving data from below and passing some to the row above. As illustrated in \circled{4}, this ensures even KV cache distribution across all cores.

The upward shifts utilize all NoC links in parallel, maintaining high performance and satisfying the P property. The physical placement of KV cache aligns with logical continuity, adhering to the L property. This method also fully resolves the M violation issue observed in the last row of cores with the concatenate-based approach.

    \vspace{-3mm}
\subsection{Implementation details}
    \vspace{-1mm}

We outline several implementation details below:

\mypar{Prefill and decode transition} Prefill and decode require distinct strategies. To handle the transition efficiently, we reshuffle KV cache and weights through the fast NoC which often provides 100s Pbits/s aggregated bandwidth, completing instantly without relying on slower off-chip memory.

\mypar{Parallelism configuration} We empirically determine the scalable parallelism for LLM operators. Automatic parallelism configuration is left for future work.

\mypar{Variations of self-attention} \sys supports variations of Self-Attention, including Grouped Query Attention~\cite{gqa}, Multihead Attention~\cite{mha}, and Multi-query Attention~\cite{mqa}. These differ by performing dist-GEMM, dist-GEMV and dist-GEMM-T locally after grouping by head dimensions.

