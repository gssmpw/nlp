\vspace{-0.3cm}
\section{Current Limitations and Future Directions}

We discuss the current limitations of \sys and wafer-scale accelerators and envision their future solutions:

% \mypar{Software maturity} The software stack for Cerebras WSE is less mature than NVIDIA CUDA, with a number of numerical operators performing significantly slower. However, over the past year, some operators have significantly improved, nearing CUDA’s performance. This progress is expected to accelerate as more developers and researchers work on new AI accelerator platforms.

\mypar{Execution bubbles} The performance of \sys is currently constrained by execution bubbles caused by the need for pipeline parallelism. Increasing a core's compute efficiency and local memory could mitigate the need for pipeline parallelism, potentially allowing the wafer-scale chip to decode over 10,000 tokens per second with LLMs. 

% Wafer-scale chip designers are already moving in this direction. Cerebras WSE-3 retains the same NoC configuration but improves core efficiency, while Tesla’s Dojo incorporates larger local memory on each core.

\mypar{LLM model design} The limited bandwidth of HBM has constrained today’s LLM model designs from using large tensors in transformer layers\cite{kaplan2020scaling, brown2020language}. With the advent of wafer-scale accelerators, we anticipate new LLM architectures adopting larger tensors (for example, the architectures with significantly wider layers than those today), free from hardware limitations, and avoiding the bubble issues when supporting current LLMs in \sys.

\mypar{Beyond Cerebras WSE} Although evaluated with Cerebras WSE, the PLMR model applies to future devices such as Tesla Dojo, featuring hundreds of thousands of cores connected via a mesh, each with MBs of local memory and limited NoC routing. Future devices may adopt different mesh-like architectures with shorter NoC paths, such as 2D torus or hybrids with on-chip switches, all consistent with the PLMR model. Our wafer-scale LLM parallelism will continue to support these architectures effectively. MeshGEMM and MeshGEMV address the worst-case 2D mesh scenario and will remain better, at least not worse, than baseline methods.

TSMC’s newer System-on-Wafer integration, expected in 2027, could boost chip density by 40$\times$ on a wafer while still aligning with the PLMR model, ensuring the long-term relevance of our contributions. 

\vspace{-0.2cm}
\section{Related Work}
\vspace{-0.2cm}

\mypar{Deep learning frameworks and compilers}
Current deep learning frameworks and compilers, such as PyTorch, TensorFlow, and XLA~\cite{pytorch, tensorflow, xla,flextensor, tvm, ansor, roller, rammer, welder, ladder}, are designed for shared memory architectures and use a tile-based “load-compute-store” computation model. While effective for shared memory, this model ignores the unique characteristics of PLMR devices, making it inefficient for wafer-scale AI chips. LLM frameworks such as vLLM and TensorRT-LLM~\cite{vllm,tensorrt-llm} have emerged to support modern LLMs but rely on frameworks and compilers designed for shared memory architectures (e.g., PyTorch~\cite{pytorch}), inheriting similar limitations on wafer-scale chips.

\mypar{Distributed GPU and TPU systems} The on-chip distributed memory architecture could theoretically be treated as a distributed LLM system, as studied in prior works~\cite{alpa, alpaserver, vllm, google2023efficiently, tensorrt-llm, loongserve}. However, such systems, designed for GPU and TPU pods (up to thousands of nodes), rely on more capable routers and lack local memory constraints, making them misaligned with the PLMR model. These approaches are complementary to our focus on on-chip scaling.

\mypar{Systolic array} Systolic array architectures~\cite{resa}, used in AI accelerators such as Amazon Trainium and Google TPU, focus on the design of small cores rather than larger wafer-scale accelerators. With limited processing elements (usually up to hundreds) in a core, they are not PLMR devices but complement \sys. For example, a Cerebras WSE core could employ a systolic array to accelerate local GEMM operations.

\mypar{Dataflow architectures}
Prior research has explored computation on dataflow architectures that account for inter-core connections. TENET~\cite{tenet} maps computation spatially and temporally to connected cores in a dataflow pattern. DISTAL~\cite{distal} enables scheduling over distributed clusters using a dataflow approach. SambaNova~\cite{sambanova} combines model and pipeline parallelism for DNN execution. However, none of these works scale computation to wafer-scale chips.

\mypar{Wafer-scale allreduce} Recent research~\cite{wafer-reduce} has investigated wafer-scale allreduce, but a single allreduce cannot fully parallelize GEMV or support full LLM inference as achieved by \sys. Additionally, this prior work is a specific instance of the two-way K-tree allreduce proposed in \sys.

\vspace{-0.3cm}
\section{Conclusion}

We envision this paper as a foundational step in exploring the potential of wafer-scale computing for LLMs. The simple yet effective PLMR model has revealed significant opportunities, guiding the development of the first wafer-scale LLM parallelism solution and scalable GEMM and GEMV algorithms for wafer-scale accelerators. Despite the limitations of the current software stack for wafer-scale devices, our approach achieves orders-of-magnitude improvements in both performance and energy efficiency. We hope this work inspires greater focus on wafer-scale computing and advances the path toward a more sustainable future for AI.