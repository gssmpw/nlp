\section{Introduction}
\vspace{-0.3cm}
Large Language Model (LLM) inference is a rapidly growing workload. It has two phases~\cite{flashdecoding++}: (i) the \textit{prefill phase}, which processes input tokens (the prompt) and spends most of its cycles on General Matrix Multiply (GEMM); and (ii) the \textit{decode phase}, which generates tokens one by one in an autoregressive manner, primarily performing General Matrix-Vector Product (GEMV). Decode requires repeatedly loading the entire LLM model into on-chip memory, with GEMV dominating its cycles. Since LLMs generate many tokens, inference is constrained by GEMV latency, making it inherently memory-bandwidth-bound.

To address memory bandwidth bottlenecks, AI accelerators are increasingly adopting system-on-wafer integration~\cite{tsmc-advanced-packaging}. This approach scales chip area to a full wafer, up to 100$\times$ larger than a typical GPU die, enabling significantly more on-chip cores, memory and bandwidth. Examples include Cerebras WSE~\cite{wse} and upcoming Tesla Dojo~\cite{dojo}. The Cerebras WSE-2, for instance, integrates 850,000 cores with 40GB of on-chip memory—1000$\times$ more than GPUs—and provides 22PB/s memory bandwidth, 7000$\times$ higher than GPUs. TSMC predicts widespread adoption of system-on-wafer integration due to its performance advantages, energy efficiency in connecting dies, and lowering cost, with IEEE forecasting a wave of wafer-scale computers by 2027~\cite{tsmc-advanced-packaging}.

Unlocking the potential of wafer-scale accelerators is challenging because current LLM systems rely on \emph{shared memory architectures} typical of GPUs and TPUs. Wafer-scale accelerators, however, adopt \emph{network-on-chip} (NoC) designs that interconnect millions of cores with local memory in a \emph{massive-scale, mesh-based memory architecture}. This architecture far exceeds the scale of on-chip crossbars (e.g., one-hop NUMA such as GraphCore IPU), multi-socket NUMA~\cite{amd2023numa}, and high-density AI clusters (hundreds of GPUs per pod)\cite{tpu}. Without fully addressing this fundamental shift in memory architecture, directly applying designs from state-of-the-art systems like T10\cite{t10} and Ladder~\cite{ladder} to wafer-scale devices often results in extremely poor performance.

To address these challenges, we propose a \emph{device model} that captures the critical hardware properties of wafer-scale accelerators, highlighting key differences from shared-memory devices. This model enables us to evaluate current LLM inference design principles, identify non-compliant areas, and pinpoint where new approaches are required. Guided by this model, we can achieve an ambitious system design: \emph{running complete LLM inference on a single chip}, minimizing costly off-chip communication and maximizing on-chip memory bandwidth utilization.

The above idea drives the design of \sys, the first wafer-scale LLM inference system, yielding several contributions:

\mypar{(1) Device model for wafer-scale accelerators}
We propose the PLMR model\footnote{PLMR model can be pronounced as “Plummer” }, which captures the following hardware properties of wafer-scale accelerators:
(i) Massive \textbf{P}arallel cores (P): Millions of cores can be integrated on a large wafer, requiring systems to effectively partition LLMs and their operations.
(ii) Highly non-uniform memory access \textbf{L}atency (L): Inter-core data access exhibits significant variation, with latency differences up to 1000$\times$, necessitating system to mitigate this.
(iii) Constrained local \textbf{M}emory (M): Each core has limited memory (tens of KBs to several MBs), requiring efficient memory usage.
(iv) Limited hardware-assisted \textbf{R}outing (R): The NoC routing hardware supports small messages (e.g., a few bytes), with headers and address encoding limited to a few bits, restricting routing paths. Careful communication path planning is crucial to avoid falling back to slower software-based routing.

\mypar{(2) Wafer-scale LLM parallelism}
We propose an effective LLM parallelism method for wafer-scale accelerators, fully compliant with the PLMR model. In the prefill phase, we design fine-grained partitioning to achieve million-core parallelism. For the decode phase, where tensor dimensions are insufficient for partitioning, we design fine-grained replication to enable parallelism with minimal communication costs. As a result, \sys achieves larger-scale and finer-grained parallelism (satisfying P in PLMR) than GPU-based approaches. Additionally, we replace conventional GPU-based GEMM and GEMV operators in existing LLM models with new versions designed for the PLMR model (satisfying L, M and R) and propose tensor placement strategies that eliminate matrix transpositions, costly with a mesh NoC (satisfying L). 

We also design a scalable KV-cache management method for wafer-scale devices. This approach features a novel KV cache shift method to ensure balanced core usage (satisfying P and M), avoiding skewed utilization of cores caused by KV cache concatenation methods common on GPUs.

\mypar{(3) Wafer-scale GEMM}
We propose MeshGEMM, a scalable GEMM algorithm for wafer-scale accelerators, enabling \sys to fully accelerate its prefill phase. Unlike conventional distributed GEMM algorithms, MeshGEMM achieves full PLMR compliance by leveraging two key operations: \emph{cyclic shifting} and \emph{interleaving}. Cyclic shifting ensures algorithm correctness while maintaining bounded usage of local memory (satisfying M) and routing resources (satisfying R). The interleaving operation minimizes multi-hop communication in the mesh NoC, effectively reducing the overhead of highly non-uniform memory latency (satisfying L).

\mypar{(4) Wafer-scale GEMV} 
We propose MeshGEMV, a scalable GEMV algorithm for wafer-scale devices, enabling \sys to effectively accelerate its decode phase. Unlike existing GEMV implementations, MeshGEMV uses a novel \emph{two-way K-tree allreduce} algorithm to aggregate local GEMV results across massive cores. This algorithm ensures minimal routing resource usage (satisfying R) and reduced communication paths (satisfying L).

\vspace{0.2cm}
\noindent
We implemented \sys on the Cerebras WSE engine using approximately 7,000 lines of CSL (a C-like programming language) for LLM parallelism, MeshGEMM, and MeshGEMV, and 2,000 lines of Python for loading LLM checkpoints and launching inference.

We conducted end-to-end LLM inference experiments with various models, including full LLaMA3-8B and LLaMA3-13B, as well as subsets of layers of LLaMA3-70B, CodeLLaMA-34B, and QWen2-72B. By combining wafer-scale LLM parallelism, GEMM and GEMV, \sys outperforms state-of-the-art (SOTA) systems:
(i) 100-200$\times$ faster than T10~\cite{t10}, the SOTA system for massive cores with a distributed on-chip memory architecture, and
(ii) 200-400$\times$ faster than Ladder~\cite{ladder}, the SOTA system for shared-memory architectures.

Micro-benchmarks further show that MeshGEMM is 2–3$\times$ faster than SUMMA~\cite{summa}, the default optimized GEMM for Cerebras WSE, and Cannon~\cite{cannon}, the SOTA GEMM for supercomputers with large-scale mesh architectures. MeshGEMV achieves 4–8$\times$ speedups over Cerebras’s optimized GEMV. Additionally, \sys’s cache shift method is up to 400$\times$ more scalable than the KV cache SOTA on GPUs, such as PagedAttention~\cite{vllm}.

Combining these benefits, \sys (using Cerebras WSE-2) outperforms vLLM (using A100) by 606$\times$ in GEMV operations and achieves 22$\times$ better energy efficiency. This comparison is fair, as both WSE-2 and A100 are manufactured using TSMC’s 7nm process. For full LLM inference, \sys delivers a 38$\times$ faster decode rate (tokens/s) and is 1.7$\times$ more energy-efficient (token/J) than vLLM. The reduced gains from GEMV to LLM are due to current limitations in Cerebras’s software, hardware, and existing LLM model designs. We anticipate stronger performance as wafer-scale AI computing matures and these limitations are addressed.