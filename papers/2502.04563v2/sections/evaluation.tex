\section{Evaluation}
We extensively evaluated \sys against various state-of-the-art methods and systems. Our results show that:
\begin{enumerate}[label=(\arabic*), leftmargin=0.5cm, noitemsep,topsep=0pt]
\item \sys{} achieves orders of magnitude speedup over T10 and Ladder in LLM inference (\S\ref{sec:eval:e2e});
\item \sys{}'s MeshGEMM and MeshGEMV achieve strong performance and scalability over state-of-the-arts (\S\ref{sec:eval:meshgemm});
\item \sys{}'s shift-based KV cache management enables over 360$\times$ more token capacity (\S\ref{sec:eval:kvcache});
\item \sys{} on Cerebras WSE-2 achieves up to 38.6$\times$ throughput and 1.7$\times$ energy efficiency compared to vLLM on A100 in LLM inference (\S\ref{sec:eval:gpu}).
\end{enumerate}

\mypar{Experiment setup}
We evaluate \sys on a server with Cerebras WSE-2. WSE-2 has 850,000 Cores, each with a Compute Engine (CE) operating at a maximum 1.1 GHz. Each clock cycle can fetch two 32-bit operands from SRAM, perform a multiply-accumulate operation, and then write back to SRAM. Each core also has a fabric router that can send or receive 32-bit messages from neighbouring cores with a single clock cycle. Additionally, each core contains 48KB of SRAM, with the chip totalling 40GB of aggregated SRAM~\cite{wse}.

We compare \sys{} with two DNN compilers: (i)T10\cite{t10}, the state-of-the-art compiler for AI accelerators with inter-core connections and distributed on-chip memory, and (ii)Ladder\cite{ladder}, the state-of-the-art compiler for shared memory architectures. For T10, we implemented it on WSE-2, treating each core as part of a distributed memory system interconnected by a crossbar, despite the actual mesh topology. T10 maps data to core IDs and fetches data from local SRAM as required. For Ladder, we treated the distributed memory architecture of the chip, interconnected by mesh, as unified memory, requiring collective communication over the NoC to access data.

\mypar{LLM models}
Our evaluation includes various representative LLMs of different sizes and architectures. Specifically, LLaMA3-8B and LLaMA2-13B are widely used open-source LLMs, with LLaMA3 using group-query attention instead of multi-head attention to reduce KV cache usage. CodeLLaMA-34B is a specialized LLM for coding tasks, while QWen2-72B, another popular LLM, is renowned for its high model quality.

\vspace{-0.2cm}
\subsection{LLM inference} \label{sec:eval:e2e}

We first report the end-to-end performance of \sys{} compared to T10 and Ladder. To provide deeper insights, we further analyze the performance by breaking down the execution into prefill and decode phases.



\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
    \hline
    Model & Seqlen In/Out & 2048/128 & 4096/128 & 2048/2048 & 4096/4096 \\
    \hline
    \multirow{3}[0]{*}{LLaMA3-8B} & \sys & 764.4 & 604.38 & 2370.33 & 2480.4 \\
          & T10   & 4.6   & 4.5   & 58.3  & 94.6 \\
          & Ladder & 1.18  & 1.05  & 7.4   & 8.72 \\
    \hline
    \multirow{3}[0]{*}{LLaMA2-13B} & \sys & 473.9 & 413.98 & 1690.28 & 1848 \\
          & T10   & 2.6   & 2.51  & 35    & 58.27 \\
          & Ladder & 0.7   & 0.69  & 4.93  & 6.14 \\
    \hline
    \end{tabular}%
    }
  \caption{End-to-end LLM inference throughput (tokens/s)}
  \label{tab:e2e_table}%
\end{table}%

\mypar{End-to-end throughput} Table~\ref{tab:e2e_table} shows the inference throughput (i.e., tokens per second) of LLaMA3-8B and LLaMA2-13B on different configurations of input and output sequence length. \sys{} uses core configurations optimized for the best performance with each model.
In LLaMA3-8B, we use 660$\times$660 cores for prefill and 360$\times$360 for decode.
In LLaMA2-13B, we use 750$\times$750 cores for prefill and 375$\times$375 for decode.
CodeLLaMA-34B and QWen-72B are not included due to the memory constraint of WSE-2.

Compared to T10, \sys{} achieves 160$\times$ speedup on average, up to 180$\times$, for short sequence generation tasks such as 4096 and 2048 input context lengths with 128 tokens output. For longer tasks, with input context lengths of 4096 and 2048 tokens and output lengths of 4096 and 2048 tokens, \sys{} achieves 36$\times$ on average and up to 48$\times$. Although T10 designs the compute-shift model that considers the memory constraints (M) and routing resource limits (R) of a PLMR device, it does not account for the cores interconnected by a mesh NoC. thus failing to address varying hop distances (L) and scale to millions of cores (P), highlighting the need for new system designs in massive-scale NUMA architectures.

Compared to Ladder, \sys{} achieves 625$\times$ speedup on average, up 677$\times$, for short sequence generation tasks such as 4096 and 2048 input context lengths with 128 tokens output. For longer sequence generation tasks, with input context lengths of 4096 and 2048 tokens and output lengths of 4096 and 2048 tokens, \sys{} achieves 312$\times$ on average and up to 342$\times$. That is because Ladder is designed for shared memory architecture and does not consider the characteristics of the PLMR device, resulting in failure in partitioning LLMs across millions of cores (P), incurring costly long-range NoC communication (L), failure in handling local memory constraints (M) and limited routing resources (R).


\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c}
    \hline
    Model & Core Config & 480$\times$480 & 600$\times$600 & 720$\times$720 \\
    \hline
    \multirow{3}[0]{*}{LLaMA3-8B} & \sys & 20320.6 & 25037.22 & 27686.45 \\
& T10 & 175.01 & 156.62 & 132.82 \\
& Ladder & 61.82 & 42.31 & 31.32 \\
    \hline
    \multirow{3}[0]{*}{LLaMA2-13B} & \sys & 13685.10 & 16854.21 & 17498.28 \\
& T10 & 121.02 & 100.53 & 81.28 \\
& Ladder & 47.25 & 33.14 & 24.23 \\
    \hline
    \multirow{3}[0]{*}{CodeLLaMA-34B} & \sys & 5471.43 & 7540.13 & 8526 \\
& T10 & 49.06 & 46.77 & 41.23 \\
& Ladder & 30.01 & 23.14 & 17.67 \\
    \hline
    \multirow{3}[0]{*}{QWen2-72B} & \sys & 2785.19 & 3775.53 & 4421.58 \\
& T10 & 24.89 & 23.48 & 21.50 \\
& Ladder & 16.77 & 12.80 & 10.12 \\
    \hline
    \end{tabular}%
    }
  \caption{Prefill throughput (tokens/s)}
  \vspace{-0.4cm}
\label{tab:prefill_throughput}
\end{table}%

\mypar{Prefill throughput} Table~\ref{tab:prefill_throughput} shows the prefill throughput (i.e., input tokens processed per second) for an input sequence length of 4096, using core configurations from 480$\times$480 to 720$\times$720. For CodeLLaMA-34B and QWen2-72B, which exceed the memory capacity of WSE-2, we evaluate a subset of layers and scale the results proportionally due to their uniform layer structure.

\sys achieves significant speedups over T10 and Ladder by effectively addressing all PLMR properties. As discussed in \S\ref{sec:motivation}, GEMM is the primary bottleneck, and MeshGEMM substantially enhances \sys’s prefill performance, analyzed in detail in \S\ref{sec:eval:meshgemm}.

Additionally, \sys scales throughput with increasing cores across all models. For instance, \sys achieves a 1.6$\times$ scaleup on QWen2-72B and a 1.4$\times$ scaleup on LLaMA3-8B when scaling from 480$\times$480 to 720$\times$720 cores.
This improved scalability is primarily because larger models better utilize the device’s compute resources, reducing the relative impact of NoC communication. In contrast, T10 and Ladder fail to scale effectively, with throughput even declining as more cores are added, due to persistent communication latency bottlenecks.


\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c}
    \hline
    Model & Core Config & 420$\times$420 & 540$\times$540 & 660$\times$660 \\
    \hline
    \multirow{3}[0]{*}{LLaMA3-8B} & \sys & 2699.94 & 2501.54 & 2243.25 \\
          & T10   & 418.27 & 339.43 & 265.12 \\
          & Ladder & 14.6  & 13.09 & 11.42 \\
    \hline
    \multirow{3}[0]{*}{LLaMA2-13B} & \sys & 2039.22 & 1899.4 & 1739.78 \\
          & T10   & 341.83 & 270.79 & 233.72 \\
          & Ladder & 11.01 & 9.93  & 9.07 \\
    \hline
    \multirow{3}[0]{*}{CodeLLaMA-34B} & \sys & 1450.77 & 1407.68 & 1359.18 \\
          & T10   & 278.24 & 222.41 & 222.41 \\
          & Ladder & 6.07  & 6.15  & 5.77 \\
    \hline
    \multirow{3}[0]{*}{QWen2-72B} & \sys & 839.71 & 824.3 & 787.08 \\
          & T10   & 168.5 & 132.97 & 114.56 \\
          & Ladder & 3.23  & 3.29  & 3.38 \\
    \hline
    \end{tabular}%
    }
  \caption{Decode throughput (tokens/s)}
  \vspace{-0.3cm}
\label{tab:decode_throughput}
\end{table}%

\mypar{Decode throughput} Table~\ref{tab:decode_throughput} shows decode throughput for core configurations from 420$\times$420 to 660$\times$660. For CodeLLaMA-34B and QWen2-72B, we evaluate a subset of layers and scale the results.

By addressing all PLMR properties, \sys achieves an average speedup of 5.7$\times$ (up to 6.5$\times$) over T10 and 217$\times$ (up to 260$\times$) over Ladder with 420$\times$420 cores.

Unlike prefill, decode throughput may decrease with more cores due to increased NoC communication latency, which impacts GEMV performance. Additionally, decode’s 6.5X improvement over T10 is less than prefill’s 160X because it involves less data transfer, limiting \sys’s communication advantage.

\vspace{-0.2cm}
\subsection{MeshGEMM}\label{sec:eval:meshgemm}
\vspace{-0.2cm}
% including non-square

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/gemm_micro_benchmark_combined.pdf}
    \caption{MeshGEMM vs. SUMMA vs. Cannon}
    \vspace{-0.5cm}
    \label{fig:eval:meshgemm}
\end{figure}

We compare MeshGEMM with Cannon~\cite{cannon} and SUMMA~\cite{summa} across different core scales and matrix sizes.

\mypar{Scaling the number of cores} 
Figure~\ref{fig:eval:meshgemm} shows MeshGEMM with a fixed matrix size while scaling the number of cores. MeshGEMM achieves the lowest execution cycles by leveraging the INTERLEAVE operation to minimize communication overhead. It demonstrates stronger scalability, maintaining over 70\% computational efficiency even near the hardware limit. In contrast, SUMMA and Cannon exhibit poor scalability, with computational efficiency falling below 50\% with 720$\times$720 cores, primarily due to high communication overhead.

Additionally, increasing the cores does not consistently yield benefits. In GEMM 2K, scaling cores from 540$\times$540 to 720$\times$720 demonstrates diminishing returns: while computational cycles decrease, the additional communication overhead from more rounds of shifting negates these benefits. As a result, the total execution cycles for Cannon and SUMMA worsen significantly. In contrast, MeshGEMM mitigates these effects effectively, allowing further core scaling if required to address other constraints, such as the need for larger aggregate memory to accommodate data.

An interesting observation in Figure~\ref{fig:eval:meshgemm} is that for GEMM 8K, communication cycles decrease as core count increases. This occurs because GEMM 8K processes large data volumes and has lengthy computations, allowing full overlap with communication. In this scenario, communication becomes bandwidth-bound rather than latency-bound, and increasing core count boosts aggregated network bandwidth, resolving the bottleneck.

\mypar{Scaling matrix size}  
We also evaluate MeshGEMM with larger matrix sizes, transforming GEMM into a more compute-intensive operation. At large scales, though the cost of communication becomes less significant, MeshGEMM maintains its scalability and outperforms SUMMA and Cannon by a wide margin, reducing total cycles by around 17\%. 

\vspace{-0.2cm}
\subsection{MeshGEMV}
\vspace{-0.2cm}

We evaluate MeshGEMV and the default GEMV implementation on Cerebras (pipeline allreduce) across various core scales and matrix shapes.

\mypar{Scaling the number of cores}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imgs/gemv_micro_benchmark_combined.pdf}
    \caption{MeshGEMV vs. GEMV-Cerebras}
    \vspace{-0.3cm}
    \label{fig:eval:meshgemv}
\end{figure}
Figure~\ref{fig:eval:meshgemv} shows MeshGEMV with a fixed matrix size across an increasing number of cores. Compared to the baseline method, MeshGEMV significantly reduces communication cycles due to the efficient Two-way K-tree AllReduce, saving communication time and improving overall execution time by up to 4.6$\times$. 

As the number of cores increases, the communication cost for MeshGEMV increases only slightly. In contrast, the baseline method's linear reduce steps become increasingly costly as the number of cores increases, resulting in substantial performance degradation with more cores.

\mypar{Scaling matrix size}  
We also evaluate MeshGEMV with larger matrix sizes. At larger scales such as $16K$, MeshGEMV maintains great scalability, with total execution cycles continuing to decrease as more cores are added. This contrasts with the baseline method, where a clear transition is observed: the savings in computational cycles diminish with increasing cores, and eventually result in negative gains.

\vspace{-0.2cm}
\subsection{Shift-based KV cache management}\label{sec:eval:kvcache}
\vspace{-0.2cm}

\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|c|c}
    \hline
    Model & LLaMA3-8B & LLaMA2-13B \\
          \hline
    Concat-based (PagedAttention) & 382   & 16 \\
    \hline
    Shift-based (\sys{}) & 137548 & 6168 \\
    \hline
    \end{tabular}%
    }
  \caption{Maximum tokens in generation}
  \label{tab:eval:kvcache}%
\end{table}%


We also compare the shift-based KV cache management with the concat-based KV cache management implemented in PagedAttention. We evaluate KV cache capacity on LLaMA3-8B and LLaMA2-13B using the same settings as the end-to-end inference evaluation in \S\ref{sec:eval:e2e}. Table~\ref{tab:eval:kvcache} shows that \sys’s shift-based KV cache management supports 360$\times$ and 385$\times$ more tokens than the concat-based method for LLaMA3-8B and LLaMA2-13B, respectively. This improvement results from balanced core utilization and the resolution of skewed data issues achieved by the shift-based approach.
\vspace{-0.2cm}
\subsection{Comparison with GPUs}\label{sec:eval:gpu}
\vspace{-0.2cm}

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c}
    \hline
    GEMV & [1,16K]$\times$[16K,16K] & [1,32K]$\times$[32K,32K] \\
          \hline
    MeshGEMV(WSE-2) Time (ms) & 0.0012 & 0.00203 \\
    cuBLAS(A100) Time (ms) & 0.336 & 1.231 \\
    WSE-2/A100 Energy Ratio & \textbf{10.37}  & \textbf{22.46} \\
    \hline
    \end{tabular}%
    }
  \caption{Comparing MeshGEMV(WSE-2) with cuBLAS(A100) in GEMV latency and energy.}
  \vspace{-0.3cm}
  \label{tab:mv_gpu}%
\end{table}%

\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c}
    \hline
    GEMM & [16K,16K]$\times$[16K,16K] & [32K,32K]$\times$[32K,32K] \\
          \hline
    MeshGEMM(WSE-2) Time (ms) & 4.8 & 34 \\
    cuBLAS(A100) Time (ms) & 34.4 & 282.1 \\
    WSE-2/A100 Energy Ratio & \textbf{0.265}  & \textbf{0.307} \\
    \hline
    \end{tabular}%
    }
  \caption{Comparing MeshGEMM(WSE-2) with cuBLAS(A100) in GEMM latency and energy.}
  \label{tab:mm_gpu}%
\end{table}%

Finally, we compare \sys against the SOTA LLM inference system on GPUs. For this experiment, we use Cerebras WSE-2 and NVIDIA A100, both manufactured on TSMC’s 7nm process, ensuring a fair comparison. To compare against the H100 fairly, we would need access to the WSE-3 which is unavailable for us.

\mypar{GEMV} We compare \gemv with GEMV on GPUs, isolating the difference in numerical operators that Cerebras has yet to optimize fully compared to CUDA. Shown by Table~\ref{tab:mv_gpu}, for a cuBLAS implementation GEMV~\cite{nvidia_cublas}, \gemv outperforms GPU by 606$\times$ in completion time, showcasing the advantages of providing substantial memory bandwidth through wafer-scale devices. This translates to 22$\times$ greater energy efficiency, reflecting the benefits of wafer-based connections (connecting on-chip memory) over PCB-based ones (connecting off-chip HBM) in GPUs.

Despite these advantages, \gemv does not achieve the theoretical 7,000$\times$ improvement. Profiling identifies three contributing factors: (i)~WSE-2 cores, still in their second generation, cannot fully overlap memory access and computation; (ii)~edge cores are underutilized; and (iii)~NoC long-range communication overhead persists, despite \gemv mitigating it effectively. We anticipate these gaps will continue to narrow as wafer-scale accelerators mature.

\mypar{GEMM} Cerebras WSE-2 offer limited benefits for GEMM, since energy-efficient wafer-based connections do not address its compute bottleneck. Performance ultimately hinges on the density of transistors and the efficiency of the core design, which is reflected by the specification of Cerebras WSE-2 (i.e., 50\% less energy efficient than A100 GPU). 

In absolute performance, \gemm is 8.3$\times$ faster than CUDA GEMM, as shown in Table~\ref{tab:mm_gpu}, largely due to its larger on-chip area. However, this speedup does not translate into energy savings. \gemm is observed to be 70\% less energy efficient than GPU GEMM, a gap expected to narrow with improved core designs, as seen in Cerebras WSE-3, which increases core efficiency by 100\%. 


\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c}
    \hline
    Sequence Input 4096 / Output 4096 & LLaMA3-8B & LLaMA2-13B \\
          \hline
    \sys{}(WSE-2) Throughput (tokens/s) & 2480.4 & 1848 \\
    vLLM(A100) Throughput (tokens/s) & 78.36 & 47.86 \\
    WSE-2/A100 Energy Ratio & \textbf{1.41}  & \textbf{1.71} \\
    \hline
    \end{tabular}%
    }
  \caption{Comparing \sys{}(WSE-2) with vLLM(A100) in end-to-end throughput and energy.}
  \vspace{-0.3cm}
  \label{fig:llm_gpu}%
\end{table}%

\mypar{LLM inference} Finally, we compare \sys with vLLM for running LLM inference, shown as Table~\ref{fig:llm_gpu}. Unlike isolated GEMV and GEMM comparisons, full LLM inference include numerous system components that \sys does not optimize and are limited by the current maturity of the Cerebras software stack (e.g., less optimized numerical operators compared to NVIDIA CUDA). Despite these constraints, \sys achieves 2480 tokens/s and 1848 tokens/s throughput for LLaMA3-8B and LLaMA2-13B, respectively, translating to 31.6$\times$ and 38.6$\times$ speedup and approximately 1.4$\times$ and 1.7$\times$ better energy efficiency in tokens per joule.

The reduction in energy efficiency from GEMV’s 22$\times$ to LLM’s 1.7$\times$ stems from two factors: (i)~WSE-2 cores have limited local SRAM (48KB), preventing efficient tensor parallelism as in vLLM and necessitating pipeline parallelism, which introduces execution bubbles and reduces chip utilization by 5$\times$; and (ii)~LLaMA models are optimized for GPU architectures, with narrow layers designed to minimize off-chip overhead. This limits layer placement across WSE-2 cores, exacerbating bubble issues. 