\section{Results}
\label{sec:results}
The potential for a malware process to run indefinitely in a real-world setting underscores the critical importance of early detection, providing security researchers with a crucial advantage.
Therefore, we run our experiment at different API call counts to determine the best count that gives us the best balance between early prediction and performance.
Our experiment was comprehensive, considering a wide range of lengths: {50, 100, 150, 200, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 20000, 100000}.
Running the experiment for different maximum API calls allows us to understand an executable's minimum required number of API calls to identify it as malicious or benign software.
We plot this against the F1-Score, the harmonic mean of precision and recall. 
We do not consider accuracy in this case as this is a very unbalanced dataset, and simply predicting the majority class gives us an accuracy value of above~$96\%$.

We see from~\autoref{fig:f1_score}~that the metrics are inferior for the initial value of fifty maximum API calls.
Remarkably, the performance sees a substantial leap with just one hundred API calls, highlighting the potential for significant improvement.
It's clear from this analysis that with only a few hundred API calls per sample, we can confidently identify the sample's malicious nature, demonstrating the power of our analysis without the need for temporal information.
Fifty-nine unique API calls to the ntdll.dll library correspond to the fifty-nine functions we mapped.
Using these fifty-nine unique function calls, we find 2540 unique~(where we consider two consecutive API calls)~API calls and 5483 unique trigrams ~(where we consider three consecutive API calls)~API calls using a sliding window approach.
An exciting aspect from~\autoref{fig:f1_score}~is in the trigram-based model~(where we consider a window of three consecutive API calls).
The F1-Score drops off after 200 API calls, and this is due to the large number of unique sequences, which spreads the count out along the feature vector. 
Due to the large number of unique occurrences, the number of calls tends to be unique and the feature vector is a smear of mostly individual unique API calls.
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/f1-score.png}
    \caption{F1-Score for all our models at different max API call counts. The X-axis is on a logarithmic scale. The results are the average of four different runs.}
    %\Description[a short description]{F1-Score for max API calls}
    \label{fig:f1_score}
\end{figure}
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Metric & Unigram & Bigram & Trigram & Combined \\
        \hline
        Accuracy & 99.24 & 99.21 & 98.50 & 98.50 \\
        Precision & 91.04 & 92.47 & 86.71 &  90.91 \\
        Recall & 82.05 & 79.45 & 57.12 & 76.79 \\
        F1-Score & 86.31 & 85.46 & 68.87 & 83.25 \\
        ROC AUC & 0.9843 & 0.9881 & 0.9495 & 0.9812 \\
        \hline
    \end{tabular}
    \caption{Metric values at a maximum of 2500 API calls. We chose 2500 API calls as the maximum limit as it provided the best results across the board.}
    %Unigram refers to only individual API calls, bigram refers to results when considering two consecutive API calls, and trigram refers to the model that considers three consecutive API calls. Combined refers to the model which is trained on the concatenated feature vector input of the unigram, bigram and trigram models. 
    %Our GitHub repository contains the entire set of results.}
    \label{tab:metrics}
\end{table}
In our case, we set the benign class to "1" and the malware class to "0". 
We do this due to highly imbalanced data favoring the malware class. 
In a real-world scenario, we do not want malware samples identified as benign.
Therefore, we need a very precise model so that no malware is identified as benign.
Although having a higher recall is good, in our case, it should not be at the expense of precision.
If our model has false negatives, it means that some benign software was classified as malware and could warrant a closer look.
However, having lower precision might have drastic consequences.
A precision-recall curve can help us understand the model's confidence in test data predictions. 
From~\autoref{fig:pr_curve}, we see that the Unigram model~(only single function call irrelevant of order)~performs very well along with the Bigram, Trigram and combined models.
The only exception is the Trigram model, where the precision drops drastically. 
From this, having a Unigram model should be sufficient for identifying malware from benign software. 
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/pr_curve.png}
    \caption{Precision Recall curve for the Unigram, Bigram, Trigram, and combined models. We see that the Trigram model confidence drops very quickly compared to the other models.}
    %\Description[a short description]{PR Curve for the models}
    \label{fig:pr_curve}
\end{figure}
All our results and plots are given in our GitHub repository\footnote{\url{https://github.com/cfellicious/api-based-malware-detection}}.

\iffalse
We also take a look at the~ROC Curve in~\autoref{fig:tpr}~which gives us an idea of the rate of the predictions of true positives to false positives by a model. 
We see that the trigram-based model performs worse than the other models. 
The AUC values in~\autoref{tab:metrics}~also point to the trigram model being the worst.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/tpr.png}
    \caption{ROC curve for the unigram, bigram, trigram and combined models. We see that the trigram model performance drops when compared to the other models. AUC values are 0.9843, 0.9881, 0.9495, and 0.9812 for the unigram, bigram, trigram, and combined models respectively.}
    %\Description[a short description]{TPR Curve for the models}
    \label{fig:tpr}
\end{figure}
\fi
From the different metrics and plots, we see that the Bigram model and the Unigram model provide the best performance overall. However, the Unigram model could be favored more simply due to the lower memory requirements and faster execution. 

