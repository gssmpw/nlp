\subsection{Dataset}
\label{subsec:dataset}
%With the dataset, we aim to address a few gaps in the existing landscape of malware-based API call datasets. 
%\todo{This is the direction we want to go}
%We wanted to create a dataset that, 
A popular method to detect malware includes tracing the Application Program Interface~(API)~calls.
An API, or application programming interface, is a set of rules or protocols that enables software applications to communicate with each other to exchange data, features, and functionality~\footnote{\url{https://www.ibm.com/topics/api}}.
We created this dataset to address the gaps in the existing landscape of the current malware-based API call datasets with the following properties,
\begin{itemize}
    \item up to date with the current malware
    \item large and varied enough to cover most modern malware
    \item does not restrict the labels to a single category. Malware might not always fall into a specific category alone. From a cybersecurity perspective, it makes more sense to group samples by the malware family.
    \item a dataset collected from the real-world machines. We worked closely with~\gdata~for data collection. 
    Working directly with a cybersecurity firm gives us the advantage of knowing the malware is from the real world.
\end{itemize}

%A popular method to detect malware includes tracing the Application Program Interface~(API)~calls.
%An API, or application programming interface, is a set of rules or protocols that enables software applications to communicate with each other to exchange data, features, and functionality~\footnote{\url{https://www.ibm.com/topics/api}}.
In a Microsoft Windows environment, user processes interact with the Operating System using dynamically linked libraries~(DLL). 
The Windows~\emph{ntdll.dll}~library is one such library.
"The NTDLL runtime consists of hundreds of functions that allow native applications to perform file I/O, interact with device drivers, and perform interprocess communications"~\cite{ms_ntdll}. 
Any malware or, for that matter, the user process will need to communicate with the NTDLL library, which makes it the perfect library for hooking our API call logger.
This library contains hundreds of functions related to different functionalities, such as semaphores, threads, creating events or objects, and so forth.
The library belongs to the Native API and can call functions in user or kernel mode~\cite{chappell2024}.
Therefore, we decided to trace the function calls to the ntdll.dll library.
Tracing all the functions to the ntdll.dll is cumbersome as documentation only exists for a few functions.
Therefore, we select a subset of~$59$ proven valuable functions of the NTDLL library~\cite{inside_ntdll}.
We obtained this set of function calls in cooperation with the cybersecurity experts from~\gdata.%~\cite{gdata}.

We wanted to keep up with the latest malware trends, so we collected samples for approximately six months between 01.05.2023 and 01.11.2023.
%The meticulous collection of these samples will provide us with a comprehensive understanding of the trends and ensure we have a diverse range of malware samples.
During this period, we undertook the monumental task of observing approximately $1,000,000$ malicious samples.
%We further truncated the list of these samples to around~\textbf{330,000}~samples, as we did not want to oversaturate the dataset by a few classes. 
Malware occurs in waves in the real world. If a specific malware type is successful, it spreads like wildfire across different networks.
This, in turn, caused a spike in a few classes in our dataset, and we removed such samples to avoid biasing any classifier to a few classes.
We collected these samples~(approximately 330k), and we grouped the samples into different malware families.
%These malware samples were then collected and split into families.
Labeling this data proved to be a challenge as well.
However, the most common form of labeling data for malware is grouping it into predefined classes such as trojan, virus, backdoor, rootkit, and so on.
But newer malware could have overlapping patterns with multiple labels and having a single label might not capture the complete behavior of the malware.
Therefore, rather than group them into a single category, such as a virus, backdoor, or trojan, we group them into distinct malware families based on source code analysis from~\gdata.
This means that malware belonging to the same label in our dataset will exhibit the same properties and this should allow for better grouping and analysis by cybersecurity researchers.

For a malware detection dataset, we also require data from benign software as well.
Acquiring benign software is not a problem at all, as there are millions of verified benign software from different organizations all over the world.
Our problem was that most benign software required user interaction to install or had a Graphical User Interface~(GUI).
The drawback of having such benign software in the dataset was that delineating between malware and benign software would be easy as most malware does not have a GUI.
We needed benign software that did not have a GUI and required no user interaction to execute.
This criterion alone made it similar to some types of malware as some malware genres also require no input from the user~\cite{gopinath2023comprehensive,bilot2024survey}.
%Our cybersecurity partner firm maintains a whitelist of benign software which we used in addition to Microsoft service executables to generate the API traces for the benign samples.
Our cybersecurity partner firm maintains a comprehensive whitelist of benign software, which includes a variety of trusted applications and services regularly used in enterprise environments. This whitelist was instrumental in ensuring the integrity and reliability of our benign sample set. In addition to this whitelist, we included Microsoft service executables, which are widely recognized as baseline components of the Windows operating system. We aimed to cover a broad spectrum of typical, non-malicious software behavior by incorporating these executables.
%To generate the API traces for our benign samples, we executed each application and service in a controlled environment, meticulously recording all API calls made to the ntdll.dll library. This process involved monitoring the software in real-time to capture an accurate representation of its interaction with the operating system. By doing so, we ensured that our dataset reflected genuine benign activity, providing a robust foundation for distinguishing between normal and malicious behavior.
Including a diverse set of benign software, sourced from our partner's whitelist and Microsoft services, significantly enhances the robustness of our dataset. This diversity is not just a factor, but a key element for training machine learning models that need to accurately differentiate between benign and malicious API call patterns. Our approach ensures that the benign samples are representative of real-world software environments, thereby improving the reliability and effectiveness of the malware detection system we developed.

%Each malicious and benign sample in our dataset has a unique SHA value, which identifies the executable.
%Malware belonging to the same family can have different executable versions, determined by their SHA values, based on various factors, such as different versions, code rewrites, or slight changes to defeat detections via SHA analysis.
%This differentiation of executables based on their SHA values is important as it allows us to keep track of mutating malware samples.
%We execute each malicious sample in a virtual environment and monitor the process.
%The tools used for monitoring and logging are proprietary to our cybersecurity partner.
We uniquely identify each malicious and benign sample in our dataset using a SHA value, which serves as a digital fingerprint for each executable.
This SHA value is crucial for distinguishing between different versions and variants of software, particularly for malware samples that belong to the same family but exhibit variations in their code. 
These differences can arise from factors such as version updates, code rewrites, or intentional modifications designed to evade detection mechanisms that rely on SHA analysis.
By assigning a unique SHA value to each executable, we can accurately track and manage the diversity of samples within our dataset. 
This differentiation is significant for mutating malware, which frequently changes its code to avoid detection. 
The ability to identify and catalog these variations ensures that our dataset reflects the dynamic nature of real-world malware, enhancing the robustness of our detection models.
We utilize a controlled virtual environment to execute and monitor each malicious sample. 
This approach allows us to observe the malware's behavior in isolation, preventing any potential damage to actual systems. 
During execution, we meticulously monitor and log all API calls made by the malware to the ntdll.dll library. 
This process's monitoring and logging tools are proprietary to our cybersecurity partner, ensuring precise and secure data collection.
These proprietary tools are specifically designed to capture detailed API call traces, providing a comprehensive view of the malware's interaction with the operating system. 
We then use the resulting data to construct a detailed profile of each sample's behavior, which is integral to developing our machine learning-based malware detection system. 
By leveraging unique SHA values and advanced monitoring techniques, we ensure that our dataset is extensive and precise, forming a solid foundation for accurate and reliable malware detection.

We simulate an internet connection using an open source library~\footnote{\url{https://github.com/prskr/inetmock}}.
Some malware samples, after unpacking themselves, request the download of an executable. We simulate the executable download by sending a predefined harmless executable whose functionalities are inert and whose behavior is predictable.
We also monitor any direct child, determined by~\textbf{InheritedFromUniqueProcessId}, from the~\textbf{EPROCESS}~struct~\cite{eprocess}.
Therefore, if the malicious sample forks, we will still monitor all the child processes and log their API calls.
Each such sample, both malware and benign, is traced over a runtime of $360$ seconds.
The execution environment is a Microsoft Windows 10 21H2 virtual machine with 4GB RAM and one vCPU.
The logger then dumps all the calls and call parameters into a JSON file named SHA.
We show the sample trace of a single API call in~\autoref{fig:json_log}.
\input{tables/data_format}
\iffalse
\subsubsection{Dataset Statistics}
\label{subsubsec:dataset_statistics}
We iterate over all instances in the dataset to collect statistics. 
We give the distribution of API calls per malware file in~\autoref{fig:api_calls_per_file}.
Although a few malware samples contain more than twenty thousand API calls, for clarity, we clip the calls to $20000$.
This clipping explains the small peak at twenty thousand API calls. 
Most samples have less than five thousand API calls during the logging process.
Identifying a lower threshold of API call count could help us better distinguish between malware and benign samples. 
We also look at the most frequent API calls among the traced functions in~\autoref{fig:api_calls}. From~\autoref{fig:api_calls}~, it is evident that the samples call some functions more than others. 
The difference between them is also significant, with the top five being  'close,' 'ntprotectvirtualmemory,' 'ntopenkeyex,' 'ntqueryvaluekey,' and 'ldrgetdllhandle' functions.
\begin{figure}
    \centering
    \includegraphics[width = 0.99\linewidth]{figures/API_calls_per_file.png}
    \caption{Histogram of total API Calls per malware sample. We see that most of the malware samples have less than 5000 total API calls. For the sake of presentation, we clip samples to counts lower than twenty thousand API calls.}
    \label{fig:api_calls_per_file}
    %\Description[a short description]{Distribution of API calls}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width = 0.99\linewidth]{figures/fn_call_statistics.png}
    \caption{Histogram of each traced API function call for the whole dataset. The Y-axis is in 10e8. The integer values are functions mapped to integer values based on the function names sorted in the alphabetic order. The top-5 called functions are 'ntclose', 'ntprotectvirtualmemory', 'ntopenkeyex', 'ntqueryvaluekey', 'ldrgetdllhandle'.}
    \label{fig:api_calls}
    %\Description[a short description]{Distribution of API calls}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width = 0.99\linewidth]{figures/api_call_instances.png}
    \caption{Files to label distribution in the dataset. The Y-axis represents the number of instances or files in the dataset and is in~$log_{10}$~scale. The X-axis is the label which is alphabetically sorted class names. The different class names could be obtained from the "shas\_by\_families.json" file in the repository.}
    \label{fig:label_distribution}
    %\Description[a short description]{Distribution of API calls}
\end{figure*}
\begin{table}[!ht]
    \centering
    
    \begin{tabular}{|c|c|}
         \hline
         Statistic & Value \\
         \hline
         Mean & 1284.67 \\
         Std. Dev & 5692.11 \\
         Median & 162 \\
         \hline
    \end{tabular}
    \caption{Statistics for the label distribution in the dataset. The discrepancy between~\textbf{mean}~and~\textbf{median}~is due to the few classes that contribute significantly to the mean.}
    \label{tab:dataset_statistic}
\end{table}
To see the label distribution, we map the label names in the \textit{shas\_by\_families.json} file into integers.
We do this by alphabetically sorting the families and assigning the index of each family as its integer mapping.
We then count how many SHA values are present for each family and plot it as a bar graph as shown in~\autoref{fig:label_distribution}.
We also compute the different statistic values in~\autoref{tab:dataset_statistic}.
\fi
We created an online repository containing all information about the dataset and our code along with the names of the traced functions\footnote{\url{https://github.com/cfellicious/api-based-malware-detection}}.
Overall, the uncompressed size of the dataset is approximately 572GB in total. We already published the dataset on Zenodo\footnote{\url{https://zenodo.org/records/11079764}}, and the dataset is currently public access.



\iffalse
\begin{figure}
    \centering
    \includegraphics[width = 0.99\linewidth]{figures/API_calls_per_file.png}
    \caption{Distribution of API Calls per malware sample}
    \label{fig:api_calls_per_file}
    %\Description[a short description]{lorem ipsum dolor}
\end{figure}

We created an online repository containing all information about the dataset and our code along with the names of the traced functions\footnote{\url{https://github.com/cfellicious/api-based-malware-detection}}.
\fi