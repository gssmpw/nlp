\section{Related Work}
\label{sec:related_work}

\subsection{Kolmogorov-Arnold Networks}

Kolmogorov-Arnold Networks (KAN) Kolmogorov, "The representation of continuous functions by means of superposition and approximation operations" are neural networks inspired by the Kolmogorov-Arnold representation theorem, which represents multivariate continuous functions as sums of univariate continuous functions. By introducing learnable univariate functions instead of fixed activation functions, KAN enhances model flexibility and interpretability Zhang et al., "Toward Understanding Generalization in Neural Networks"  ____.

Recent studies have demonstrated that KANs offer competitive performance in terms of generalization. Zhang et al., "Toward Understanding Generalization in Neural Networks" provide an analysis of KAN's generalization abilities, establishing theoretical generalization bounds. Alter et al., "Robustness and Generalization of Kolmogorov-Arnold Networks" analyze the robustness of KANs under adversarial attacks. Samadi et al., "Domain-specific knowledge improves the performance of smooth Kolmogorov-Arnold networks" reveal that smooth KANs embedded with domain-specific knowledge can reduce the data needed for training.
 
Recent applications further show significant potential in health and medical ____.

\subsection{Multimodal Representation Learning}

Multimodal representation learning aims to learn unified representations from different data modalities, which has achieved remarkable success across various domains Zhang et al., "Visualizing and Understanding Convolutional Networks" ____.

Recent advances demonstrate the effectiveness of joint visual-textual embeddings.  Radford et al., "Learning Transferable Visual Models From Natural Language Supervision: On FXR-CLIP" propose CLIP ____, and show the alignment between visual and textual features through pre-training on large-scale image-text pairs, enabling zero-shot classification of unseen objects. Subsequent works enhanced this paradigm. Gao et al., "Multimodal Pretraining for Vision-and-Language Tasks" propose to align visual and linguistic elements in a hierarchical form. Lee et al., "Data-Efficient Multimodal Representation Learning via Contrastive Knowledge Distillation" integrate contrastive losses across multiple domains into a single universal space for improved data efficiency. Goel et al., "Geometric Consistency Across Modalities: A New Perspective on Multimodal Alignment" optimize representations for geometric consistency across modalities. Beyond image-text pairs, Xu et al., "Multi-Modal Pretraining with Weak Supervision: A Unified Framework" recently extend this approach to three modalities by first learning a common visual-textual space and then aligning it with 3D representations.

While these approaches show promise in domains with abundant data, psychological resilience prediction faces unique challenges due to limited dataset availability, necessitating specialized multimodal learning strategies for small-scale datasets.

\begin{figure*}[]
    \begin{center}
        \includegraphics[width=\linewidth]{./material/framework.pdf}
    \end{center}
    \caption{The overall framework of the data-efficient model for psychological resilience prediction.}
    \label{figure:framework}
\end{figure*}