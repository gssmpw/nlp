\section{Related Work}
\label{sec:related_work}

\subsection{Kolmogorov-Arnold Networks}

Kolmogorov-Arnold Networks (KAN) \cite{liu2024kan,liu2024kan} are neural networks inspired by the Kolmogorov-Arnold representation theorem, which represents multivariate continuous functions as sums of univariate continuous functions. By introducing learnable univariate functions instead of fixed activation functions, KAN enhances model flexibility and interpretability \cite{somvanshi2024survey}.

Recent studies have demonstrated that KANs offer competitive performance in terms of generalization. Zhang et al. \cite{zhang2024generalization} provide an analysis of KAN's generalization abilities, establishing theoretical generalization bounds. Alter et al. \cite{alter2024robustness} analyze the robustness of KANs under adversarial attacks. Samadi et al. \cite{samadi2024smooth} reveal that smooth KANs embedded with domain-specific knowledge can reduce the data needed for training.
 
Recent applications further show significant potential in health and medical \cite{tang20243d,jahin2024kacq,aghaomidi2024ecg}. However, the application of KANs to EEG and fMRI remains unexplored.

\subsection{Multimodal Representation Learning}

Multimodal representation learning aims to learn unified representations from different data modalities, which has achieved remarkable success across various domains \cite{manzoor2023multimodality}.

Recent advances demonstrate the effectiveness of joint visual-textual embeddings.  Radford et al. propose CLIP \cite{radford2021learning}, and show the alignment between visual and textual features through pre-training on large-scale image-text pairs, enabling zero-shot classification of unseen objects. Subsequent works enhanced this paradigm. Gao et al. \cite{gao2022pyramidclip} propose to align visual and linguistic elements in a hierarchical form. Lee et al. \cite{lee2022uniclip} integrate contrastive losses across multiple domains into a single universal space for improved data efficiency. Goel et al. \cite{goel2022cyclip} optimize representations for geometric consistency across modalities. Beyond image-text pairs, Xu et al. \cite{xue2023ulip} recently extend this approach to three modalities by first learning a common visual-textual space and then aligning it with 3D representations.

While these approaches show promise in domains with abundant data, psychological resilience prediction faces unique challenges due to limited dataset availability, necessitating specialized multimodal learning strategies for small-scale datasets.

\begin{figure*}[]
    \begin{center}
        \includegraphics[width=\linewidth]{./material/framework.pdf}
    \end{center}
    \caption{The overall framework of the data-efficient model for psychological resilience prediction.}
    \label{figure:framework}
\end{figure*}