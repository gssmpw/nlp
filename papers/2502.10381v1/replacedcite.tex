\section{Related Work}
\label{app:related-work}

This section provides an expanded discussion of related work on class
imbalance in machine learning.

The class imbalance problem, defined by a significant disparity in the
number of instances across classes within a dataset, is a common
challenge in machine learning applications
____.  This issue is prevalent
in many real-world binary classification scenarios, and arguably even
more so in multi-class problems with numerous classes. In such cases,
a few majority classes often dominate the dataset, leading to a
``long-tailed'' distribution.  Classifiers trained on these imbalanced
datasets often struggle, performing similarly to a naive baseline that
simply predicts the majority class.

The problem has been widely studied in the literature
____. It includes numerous methods
including standard Softmax, class-sensitive learning, {Weighted
  Softmax}, weighted 0/1 loss
____, size-invariant
metrics for Imbalanced Multi-object Salient Object Detection studied
by ____ as well as {Focal loss}
____, {LDAM} ____, {ESQL}
____, {Balanced Softmax}
____, {LADE} ____),
logit adjustment ({UNO-IC} ____, {LSC} ____), transfer
learning ({SSP} ____), data augmentation ({RSG}
____, {BSGAL} ____, {ELTA} ____, {OT} ____), representation learning ({OLTR}
____, {PaCo} ____, {DisA} ____, {RichSem} ____, {RBL} ____, {WCDAS} ____),  classifier
design ({De-confound} ____, ____,  {LIFT} ____, {SimPro} ____), decoupled training
({Decouple-IB-CRT} ____, {CB-CRT}
____, {SR-CRT} ____,
      {PB-CRT} ____, {MiSLAS}
      ____), ensemble learning ({BBN}
      ____, {LFME} ____, {RIDE}
      ____, {ResLT} ____, {SADE}
      ____, {DirMixE} ____).  An interesting recent study
      characterizes the asymptotic performances of linear classifiers
      trained on imbalanced datasets for different metrics
      ____.
      
Due to space restrictions, we
cannot give a detailed discussion of all these methods. Instead, we
will describe and discuss several broad categories of existing methods
to tackle this problem and refer to reader to a recent survey of
____ for more details.
%
These methods fall into the following broad categories.

\textbf{Data modification methods.} These include methods such as
oversampling the minority class ____, undersampling
the majority class
____, or
generating synthetic samples (e.g., SMOTE
____), aim to
rebalance the dataset before training
____.

\textbf{Cost-sensitive techniques.} These techniques, including
cost-sensitive learning and the incorporation of class weights assign
different penalization costs to losses on different classes. They
include cost-sensitive SVM ____
and other cost-senstive methods
____. The weights are often determined by the
relative number of samples in each class or a notion of effective
sample size ____.

These two method categories are very related and can actually be shown
to be equivalent in the limit.  Cost-sensitive methods can be viewed
as more efficient, flexible and principled techniques for implementing
data sampling methods.  However, these methods often risk overfitting
the minority class or discarding valuable information from the
majority class.  Both methods inherently bias the input training data
distribution and suffer from Bayes inconsistency (in Section, we prove
that cost-sensitive methods do not admit Bayes consistency). While
they have been both reported to be effective in various instances,
this varies and depends on the problem, the distribution, the choice
of predictors, and the performance metric adopted and they have been
reported not to be effective in all cases ____.
Additionally, cost-sensitive methods often resort to careful tuning of
hyperparameters.  Hybrid approaches attempt to combine the strengths
of data modification and cost-sensitive methods but often inherit
their respective limitations.

\textbf{Logistic loss modifications.}  A family of more recent methods
rely on logistic loss modifications.  They consist of modifying the
logistic loss by augmenting each logit (or predicted score) with an
additive hyperparameter.  They can be equivalently described as a
cost-sensitive modification of the exponential terms appearing in the
definition of the logistic loss.  They include the Balanced Softmax
loss ____, the Equalization loss
____, and the \LDAM\ loss
____. Other similar additive change methods use
quadratically many hyperparameters with a distinct additive parameter
for each pair of logits. They include the logit adjustment methods of
____ and ____.
____ argue that their specific choice of the
hyperparameter values is Bayes-consistent.
%
A multiplicative modification of the logits, with one hyperparameter
per class label is advocated by ____. This can be
equivalently viewed as normalizing scoring functions (or feature
vectors in the linear case) beforehand, which is a standard method
used in many learning applications, irrespective of the presence of
imbalanced classes. The Vector-Scaling loss of ____
combines the additive modification of the logits with this
multiplicative change. These authors further present an analysis of
this method in the case of linear predictors, underscoring the
specific benefits of the multiplicative changes.  As already pointed
out, the multiplicative changes coincide with prior rescaling or
renormalization of the feature vectors, however.

\textbf{Other methods.} Additional approaches for tackling imbalanced
datasets (see ____) include post-hoc
correction of decision thresholds ____ or
weights ____], as well as information and data
  augmentation via transfer learning, or
  distillation ____.

Despite significant advances, these techniques face persistent
challenges.

First, most existing solutions are heuristic-driven and lack a solid
theoretical foundation, making their performance difficult to predict
across varying contexts.  In fact, we are not aware of any analysis of
the generalization guarantees for these methods, with the exception of
that of ____. However, as further discussed in
Section~\ref{sec:existing-methods}, the analysis presented by these
authors is limited to the \emph{balanced loss}, that is the uniform
average of the misclassification on each class.  More specifically,
their analysis is limited to binary classification and only for the
separable case.
%
The balanced loss function differs from the target misclassification
loss. It has been argued, and that is important, that the balanced
loss admits beneficial fairness properties when class labels correlate
with demographic attributes as it treats all class errors equally.
The balanced loss is also the metric considered in the analysis of
several of the logistic loss modifications papers
____.
However, class labels do not alway relate to demographic attributes.
Furthermore, many other criteria are considered for fairness purposes
and in many machine learning applications, the misclassification
remains the key target loss function to minimize.  We will show that,
even in the special case of the analysis of ____,
the solution they propose is the opposite of the one corresponding to
our theoretical analysis for the standard misclassification loss. We
further show that their solution is empirically outperformed by ours.

Second, the evaluation of these methods is frequently biased toward
alternative metrics such as F1-measure, AUC, or other metrics
weighting false or true positive rate differently, which may obscure
their true effectiveness on standard misclassification.  Additionally,
these methods often seem to struggle with extreme imbalances or when
the minority class exhibits high intra-class variability.

We refer to ____ for more details about
work related to learning from imbalanced data.