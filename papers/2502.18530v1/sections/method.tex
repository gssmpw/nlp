\label{sec:method}
\subsection{Overview}
Traditional machine learning (ML) pipeline optimization requires deep ML and domain expertise, as practitioners iteratively adjust data processing, model selection, and training configurations based on training performance. While effective, this traditional workflow is labor-intensive, requiring constant supervision from ML experts at every stage of development.

Recent LLM-based automation methods~\citep{grosnit2024kaggleagent, trirat2024automlagent, hong2024datainterpreter, li2024autokaggle, guo2024dsagent} aim to replace this process by autonomously generating ML pipelines. These methods typically incorporate data-driven reasoning and employ optimization techniques to iteratively refine the pipeline for improved performance. However, most existing approaches optimize the entire pipeline as a single entity and treat performance improvements globally all at once, diverging from human workflows, where ML practitioners refine individual components in an iterative manner.

Global optimization strategies present a number of fundamental challenges that hinder its effectiveness. First, modifying multiple pipeline components simultaneously reduces interpretability, making it difficult to attribute performance changes to specific adjustments. Second, due to this lack of interpretability and the vast search space, performance improvements often occur through chance-based trial and error rather than systematic refinement, leading to instability in results. Finally, while these strategies may initially yield rapid improvements in early iterations, they tend to plateau as they struggle to make meaningful refinements at later stages when the overall performance is already high, resulting in an overall slow convergence.

\subsection{Iterative Refinement}
To address these limitations, we introduce \textbf{Iterative Refinement}, a structured optimization strategy inspired by how human ML practitioners systematically adjust individual pipeline components based on empirical feedback. Applying Iterative Refinement in an LLM agent framework requires dividing the target pipeline into distinct components based on the problem domain. Each component must be generated independently yet easily integrated into a complete pipeline. Following the initial training and evaluation of the pipeline, one single component is selected for modification in an attempt to improve while the remaining components remain unchanged. The updated pipeline undergoes training and evaluation, and modifications are retained only if they result in performance improvements. Otherwise, the system reverts to the previous configuration. The process repeats for a fixed number of times, allowing the framework to refine the pipeline progressively.

Compared to existing methods that apply holistic optimization methods, Iterative Refinement offers three key advantages:
\begin{itemize}
    \item \textbf{Improved Interpretability}: By modifying only one component at a time, performance changes can be directly attributed to specific adjustments.
    \item \textbf{Stable Improvement}: The framework does not rely on finding great combinations of parameters and methods based on chance. Instead, it achieves gradual and systematic improvement across the given iterations.
    \item \textbf{Accelerated Convergence}: Targeted refinements prevent redundant modifications and ensure a structured optimization trajectory, leading to faster and more efficient performance gains.
\end{itemize}

\subsection{IMPROVE}
To evaluate the Iterative Refinement strategy, we developed \textbf{IMPROVE}, a multi-agent end-to-end LLM framework that autonomously generates and optimizes an object classification pipeline. IMPROVE requires only a dataset as input and independently manages the entire process, from pipeline generation to model training, eliminating the need for any human intervention. We provide a diagram of the workflow of IMPROVE in Figure~\ref{fig:improve}. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\textwidth]{images/improve.png}
    \caption{The IMPROVE framework employing Iterative Refinement. Users provide a dataset, and IMPROVE autonomously executes the model development process. The Project Architect designs the pipeline, which is implemented by specialized agents. The Performance Analyst iteratively refines components based on empirical feedback, optimizing the pipeline over multiple iterations. After a set number of iterations, a high-quality model is produced.}
    \label{fig:improve}
\end{figure*}

IMPROVE consists of the following key agents:
\begin{itemize}
    \item \textbf{Project Architect}: This agent is responsible for the initial analysis of the dataset, evaluating its size, structure, and any supplementary information provided. Based on this analysis, the Project Architect generates a comprehensive technical design that serves as the foundation for the entire pipeline.
    \item \textbf{Data Engineer}: This agent handles the implementation of the data processing component, which involves tasks such as data standardization, augmentation, and transformation.
    \item \textbf{Model Engineer}: This agent is tasked with selecting and configuring the model architecture by choosing the most appropriate base model, applying necessary modifications, and incorporating techniques such as regularization if needed.
    \item \textbf{Training Engineer}: This agent is responsible for configuring the model training process, where key tasks include setting the hyperparameters, such as learning rate and batch size, and selecting the best optimization algorithms.
    \item \textbf{Performance Analyst}: This agent is responsible for reviewing pipeline configurations and training logs after each training run is completed. It then identifies one area for improvement and provides targeted feedback to one selected engineer agent.
\end{itemize}
Each agent in the IMPROVE framework is equipped with knowledge of its role, the teamâ€™s goals, and how to collaborate effectively with other agents. Also, each agent operates with domain-specific expertise embedded through prompt engineering. For example, the Model Engineer is aware of effective pre-trained models like ConvNeXt~\citep{liu2022convnext}, while the Data Engineer is familiar with advanced data augmentation techniques such as MixUp~\citep{zhang2018mixup} and CutMix~\citep{yun2019cutmix}. Although state-of-the-art LLMs implicitly understand these techniques and can implement them when asked to, they rarely apply them to code proactively unless explicitly instructed to do so. Thus, we specify these techniques in the prompts to ensure the agents only select and experiment with highly effective methods when generating the training pipeline.


The framework applies Iterative Refinement to optimize three primary pipeline components that we identified for object classification: \textbf{data augmentation}, \textbf{model architecture}, and \textbf{training procedure}. The development process begins with the \textbf{Project Architect}, who reviews the dataset and additional context to produce a technical design. This design is then passed to the \textbf{Data Engineer}, \textbf{Model Engineer}, and \textbf{Training Engineer}, who collaboratively build their respective components according to the plan. Once the data pipeline, model architecture, and training configurations are in place, the system integrates these components and trains the model. The \textbf{Performance Analyst} then reviews all past and current pipeline configurations, along with their associated training logs, to identify areas for improvement. It then provides targeted feedback to a selected engineer, who refines their component accordingly. The updated pipeline undergoes retraining and re-evaluation, ensuring that only changes leading to performance gains are retained, while ineffective modifications are logged for future reference but discarded from the active codebase. This iterative cycle continues for a predefined number of iterations, progressively refining the pipeline until the best-performing configuration and corresponding model checkpoint are identified as the final output.

\subsection{Implementation}
We now highlight some other design choices and innovations in IMPROVE that enhance its effectiveness beyond the Iterative Refinement strategy.

\textit{Dataset Information Utilization.}
A key advantage of the LLM agents framework is its ability to understand any natural language description of the dataset, enabling the system to make more informed decisions. Optionally, the user can supply a brief description of the dataset, detailing aspects such as the image domain (e.g., real-life objects, digits, synthetic images), whether the images are grayscale or colored, their dimensions (same or varying sizes), and their quality (high or low). This information is passed to both the Project Architect and Performance Analyst and is factored into the initial technical design and performance analysis. The impact of including this dataset information is discussed further in Section~\ref{sec:dataset}.

\textit{Autonomous Code Execution.}
The IMPROVE framework is capable of autonomously generating, executing, and refining model pipeline code over multiple iterations to improve model performance. To achieve this, IMPROVE first needs to ensure that the code produced by each engineer agent is coherent and compatible. This is facilitated by a predefined constant interface that specifies the required input arguments and expected return values for each component. The code-generating agents (i.e., Data Engineer, Model Engineer, and Training Engineer) have flexibility in their implementations but must adhere to these interface guidelines for input and output consistency. To automate the training process, a built-in workflow takes the agents' responses, and then parses and extracts the code into Python files. The system utilizes the predefined interface to integrate these components into a coherent pipeline and initiates a terminal shell script that executes the full model training process. Training logs and other outputs are captured at the shell level and stored in a text file for analysis.

\textit{Unified Pipeline Initialization.}
At the start of the IMPROVE workflow, the Project Architect agent leverages the strong zero-shot generation capabilities of LLMs to generate a single, coherent pipeline in one step before dividing it into three components. In Section~\ref{sec:ablation}, we discuss how generating the data, model, and training components separately and then combining them instead can lead to suboptimal initial performance. Using unified pipeline initialization ensures greater efficiency by starting with a well-integrated baseline, reducing the need for extensive corrections and minimizing wasted iterations on poorly constructed initial configurations.

\textit{Summarized History.}
To ensure informed decision-making and prevent redundancy, all previous pipeline configurations and their corresponding training logs are passed to the Performance Analyst. However, passing the full history of code and logs could quickly exceed the LLMâ€™s maximum context length. To mitigate this issue, after each iteration, the system makes a call to the LLM to generate a summary of the key aspects of the data, model, and training code, highlighting information such as the methods and hyperparameters used. Then, instead of passing the entire code from all past iterations, only the summarized configuration along with the training log is provided to the Performance Analyst to guide its reasoning.