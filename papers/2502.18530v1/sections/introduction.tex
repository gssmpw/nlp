\label{sec:intro}
Computer vision has emerged as a powerful tool for addressing many complex real-world problems, from plant monitoring in agriculture to handwriting classification in digital systems. However, the process of training computer vision models has grown increasingly complex, involving many different steps such as data augmentation, architecture selection, and hyperparameter tuning. As a result, developing high-performing models is labor-intensive and often demands machine learning (ML) expertise, as well as domain-specific knowledge. Each component must be manually calibrated based on prior experience and detailed analysis of training statistics.

To simplify this process, tools such as Weights \& Biases have been developed, offering ML practitioners the ability to track, organize, and optimize model training workflows. Similarly, researchers have been investigating approaches, such as hyperparameter optimization~\citep{bergstra2012hpo,bergstra2011hpo,snoek2012hpo,springenberg2016hpo,falkner2018hpo} and neural architecture search~\citep{elsken2017nas,kandasamy2018bo}, to automate parts of the ML pipeline. While these platforms and methods provide valuable support, they tend to address isolated aspects of the model development process and still require substantial involvement from human experts.

Recent advances in large language models (LLMs) have opened up new possibilities for automation by enabling AI agents to generate code and reason about design choices. Inspired by this capability, researchers have begun leveraging LLM agents to automate the construction and optimization of ML models~\citep{grosnit2024kaggleagent, trirat2024automlagent, hong2024datainterpreter, li2024autokaggle, guo2024dsagent}. Approaches in this emerging field vary widely in terms of planning, reasoning, and evaluation strategies. However, one common feature of most methods is to optimize the entire pipeline simultaneously before each evaluation attempt, rather than refining each component in isolation. While this "all-at-once" approach can help with rapid early-stage improvements, our experiments reveal a key limitation: as model performance approaches its maximum potential, this strategy struggles to make further progress. The inability to attribute performance changes to specific components makes it difficult to refine the pipeline and push improvements beyond a certain threshold.

To address these limitations, we introduce Iterative Refinement, a new strategy for LLM-driven ML system design inspired by how human experts approach model development. Rather than attempting to optimize an entire pipeline at once, experienced ML practitioners typically analyze performance, adjust specific components, and iteratively refine the design based on training feedback. For example, if they determine that data augmentation needs improvement, they will experiment with different augmentation techniques while keeping the training procedure and model architecture unchanged. Only after evaluating its impact will they refine other aspects of the pipeline. Following this intuition, Iterative Refinement systematically updates one component at a time, evaluates its impact, and refines further using real training feedback. This structured approach enables more stable, interpretable, and controlled improvements by isolating the effects of each change and precisely identifying what drives performance gains.

We implement this strategy in IMPROVE (Iterative Model Pipeline Refinement and Optimization leveraging LLM agents), a fully autonomous framework for designing and optimizing image classification pipelines. IMPROVE emulates the workflow of a team of human ML engineers, taking a dataset as input and autonomously constructing, training, and iteratively refining a model through its multi-agent system. It efficiently manages key tasks such as data preprocessing, architecture selection, and hyperparameter tuning.

Our experiments demonstrate that Iterative Refinement enables IMPROVE to generate higher-performing models consistently compared to modifying entire pipelines at once. IMPROVE also greatly outperforms zero-shot LLM-generated training pipelines and achieves near-human-level performance on standard datasets, including CIFAR-10, TinyImageNet, and various Kaggle competition datasets. Notably, it maintains computational efficiency comparable to that of human practitioners, making it both effective and scalable. Our findings validate Iterative Refinement as a practical strategy for LLM-driven ML automation but also establish IMPROVE as an accessible tool for developing state-of-the-art image classification models without requiring deep ML expertise.

Overall, our work introduces a novel strategy for LLM-based ML system design along with a concrete framework that automates the generation and optimization of object classification pipelines. Specifically, our contributions are:
\begin{itemize}
    \item We introduce Iterative Refinement, a new design approach in LLM agent frameworks inspired by how human experts iteratively refine models, optimizing individual components incrementally for more stable and controlled improvements.
    \item We design IMPROVE, a novel framework that applies Iterative Refinement to create and optimize object classification pipelines using specialized LLM agents.
    \item Through extensive experiments on standard and real-world datasets, we demonstrate that Iterative Refinement ensures more consistent performance gains in pipeline optimization, helping IMPROVE to outperform zero-shot LLM-generated pipelines and achieves human expert-level results.
\end{itemize} 