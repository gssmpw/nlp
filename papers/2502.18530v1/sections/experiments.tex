\label{sec:experiments}
\subsection{Experimental Setting}
\paragraph{Datasets.} 
We evaluated IMPROVE using two widely recognized classification datasets: CIFAR-10~\citep{krizhevsky2009cifar} and TinyImageNet~\citep{deng2009imagenet}. CIFAR-10 contains 60,000 32x32 color images across 10 distinct classes, while TinyImageNet, a subset of ImageNet1K, includes 100,000 images across 200 classes, resized to 64x64 pixels. Both datasets feature commonly seen objects such as vehicles and animals.

To test IMPROVE's robustness, we also incorporated CIFAR-10-C~\citep{hendrycks2019cifar10c}, a variant of CIFAR-10 designed to assess model performance under common corruptions. CIFAR-10-C introduces 15 corruption types, grouped into four categories: noise, blur, weather, and digital distortions. These corruptions are applied at two severity levels (1 and 5), resulting in 30 distinct versions of the dataset. For experimental efficiency, we created two subsets, CIFAR-10-C-1 and CIFAR-10-C-5, where the suffix denotes the severity level. These subsets were generated by uniformly sampling one image from each corrupted dataset for every test image.

Additionally, we evaluated IMPROVE on two smaller datasets from different domains: SVHN and dSprites. SVHN is a digit classification dataset containing real-world images of house numbers, with variations in resolution and background complexity. dSprites is a synthetic dataset of 2D shapes, with latent factors controlling the shapes’ properties. Specifically, we used the dSprites Orientation dataset, where each shape is rotated into one of 40 possible orientations within the $[0, 2\pi]$ range. Both datasets are part of the Visual Task Adaptation Benchmark (VTAB)~\citep{zhai2020vtab}.

To further assess IMPROVE’s versatility across diverse real-world domains, we also evaluated it on four real-world datasets from Kaggle, a leading platform for machine learning competitions. These datasets span a range of sizes and represent various data domains, including:
\begin{itemize}
    \item \textbf{Cassava Leaf Disease Classification}~\citep{2020cassava}: This dataset consists of 21,367 photos of cassava plants in Uganda, mostly taken using inexpensive cameras. The model must identify whether the plant has one of the four diseases: Mosaic Disease, Green Mottle Brown Streak Disease, Bacterial Blight Disease, or no disease, with 5 classes in total.
    \item \textbf{4 Animal Classification}~\citep{lee2022animal}: This dataset comprises a total of 3,529 images, categorized into four distinct animal classes: cats, deer, dogs, and horses. Each class represents a diverse range of images capturing various poses, environments, and lighting conditions.
    \item \textbf{Arabic Letters Classification}~\citep{khalil2023arabic}: This dataset contains 53,199 images of 65 written Arabic letters, each exhibiting positional variations based on their occurrence within a word with four possible forms: isolated, initial, medial and final. These letters were collected from 82 different users.
    \item \textbf{Kitchenware Classification}~\citep{ololo2022kitchenware}: This dataset has 9,367 photos of 6 types of common kitchenware, including cups, glasses, plates, spoons, forks and knives. These photos are taken in households with various lighting and scene conditions.
\end{itemize}

We used three key criteria to guide our selection of Kaggle datasets to ensure an efficient and fair evaluation. First, we excluded datasets with more than a million images to maintain computational feasibility. Second, we only selected datasets used for public competitions, allowing for a direct comparison between IMPROVE's performance and that of human ML practitioners. Third, we chose competitions that used top-1 accuracy as the primary evaluation metric, avoiding those that relied on metrics such as the area under the ROC curve, F1 score, or multiclass log loss, to be consistent across our experiments. After applying these filters, these four datasets were among the few that met all the criteria and aligned with the goals of our evaluation.

We always use the provided test set when available. If no test set is provided, we designate the validation set as the test set. In cases where only a single unsplit dataset is available, we manually split the dataset into training and test sets using an 80-20 ratio.

\textit{Baseline.} 
For VTAB datasets, namely SVHN and dSprites Orientation, we compare our results with that of Visual Prompt Tuning (VPT)~\citep{jia2022vpt}, a widely recognized method in the domain of fine-tuning. For all four Kaggle datasets, we benchmark IMPROVE against the top existing leaderboard submissions. To further assess the practicality of the IMPROVE framework, we compare the accuracy of its generated model pipelines with that of a straightforward zero-shot prompting approach, which represents the most likely method a non-ML expert would use to generate a model. For this comparison, we provide the LLM with a simple prompt in the format: "Generate code for training a model on a dataset with X classes." The generated script is then executed, and the resulting model is trained and evaluated on the test set manually.

\textit{Experimental Setup.}
We used two commercial LLMs, GPT-4o and o1, developed by OpenAI, for all experiments. The experiments were conducted over three trials, with the average accuracy and their standard deviation reported. For all IMPROVE runs, we performed 20 iterations to balance optimization and experimental efficiency. For the VTAB datasets (SVHN and dSprites Orientation), we instructed the Model Engineer to use the Vision Transformer (ViT), specifically the ViT-B/16 model, to ensure a fair comparison with the VPT paper, which utilized the same model as its baseline.

\subsection{Main Results}
The results in Table~\ref{tab:zeroshot} demonstrate that IMPROVE consistently outperforms models generated by zero-shot prompting LLMs using both GPT-4o and o1, confirming its effectiveness as a superior alternative for non-ML experts seeking to train high-performing models. For simpler datasets like CIFAR-10, IMPROVE achieves slightly higher accuracy compared to zero-shot prompting, with the difference being roughly between 3\% to 20\%. However, its strength becomes more evident on more challenging datasets like TinyImageNet and the two variants of CIFAR-10 with corrupted images. On these datasets, IMPROVE at most achieves a roughly 40\% higher accuracy than the zero-shot prompting baseline. 

Another notable distinction between the results of IMPROVE and the zero-shot baseline is the significantly lower standard deviation observed with IMPROVE. While zero-shot results occasionally achieve accuracy closer to that of IMPROVE, they are highly inconsistent, even when using the same prompt. For example, the highest recorded standard deviation was approximately ±17\% for GPT-4o on CIFAR-10-C-5. This inconsistency means that inexperienced users might achieve decent results at times, but they could just as easily get terrible models. In contrast, this variability is lowered with IMPROVE, as all runs eventually converge to a consistently strong performance.

\begin{table*}[ht]
\centering
\caption{Average classification accuracy for IMPROVE-generated models and zeroshot prompting LLMs on four standard datasets. The best accuracy on each dataset is bolded.}
\label{tab:zeroshot}
\begin{tabular}{@{}ccccc@{}}
\toprule
Dataset & IMPROVE (o1) & \multicolumn{1}{l}{Zero-shot (o1)} & IMPROVE (GPT-4o) & Zero-shot (GPT-4o) \\ \midrule
CIFAR-10     & \textbf{0.9825±0.0018} & 0.7940±0.0287 & 0.9626±0.0311 & 0.9290±0.0331 \\
CIFAR-10-C-1 & \textbf{0.9621±0.0026} & 0.7654±0.0282 & 0.9476±0.0158 & 0.5425±0.1007 \\
CIFAR-10-C-5 & \textbf{0.9557±0.0128} & 0.7662±0.0244 & 0.9422±0.0038 & 0.6218±0.1724 \\
TinyImageNet & \textbf{0.8692±0.0212} & 0.4630±0.0520 & 0.7875±0.0169 & 0.4815±0.1091 \\ \bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:vpt} further illustrates IMPROVE’s effectiveness by comparing it to Visual Prompt Tuning (VPT)~\citep{jia2022vpt}. IMPROVE not only surpasses VPT but also outperforms the full-parameter fine-tuning baselines reported in the same study. On both datasets, IMPROVE again shows clear improvements over zero-shot prompting, regardless of whether GPT-4o or o1 is used.

\begin{table}[ht]
\centering
\small
\caption{Average classification accuracy for IMPROVE-generated models and zeroshot prompting LLMs on two VTAB Datasets: SVHN and dSprites Orientation. The table also includes the results from full-parameter fine-tuning (FFT) and Visual Prompt Tuning (VPT) from ~\cite{jia2022vpt} for comparison. The best accuracy on each dataset is bolded.}
\label{tab:vpt}
\begin{tabular}{@{}cccccc@{}}
\toprule
Model                   & Dataset              & IMPROVE                & Zeroshot  & FFT & VPT \\ \midrule
\multirow{2}{*}{o1}     & SVHN                 & \textbf{0.9779±0.0015} & 0.9513±0.0027 & 0.8740           & 0.7810               \\
                        & dSprites & \textbf{0.9667±0.0005} & 0.5997±0.1407 & 0.4670           & 0.4790               \\ \midrule
\multirow{2}{*}{GPT-4o} & SVHN                 & \textbf{0.9695±0.0046} & 0.9250±0.0307 & 0.8740           & 0.7810               \\
                        & dSprites & \textbf{0.9523±0.0074} & 0.6515±0.2199 & 0.4670           & 0.4790               \\ \bottomrule
\end{tabular}
\end{table}

In addition to standard datasets, IMPROVE’s performance on Kaggle competition datasets is presented in Table~\ref{tab:kaggle}. These datasets vary in size, image quality, and domain, offering a more realistic assessment of IMPROVE’s adaptability in real-world scenarios. Across all four datasets, IMPROVE consistently outperforms zero-shot prompting, though its top-1 accuracy remains slightly below the highest-ranking Kaggle leaderboard results. Nevertheless, IMPROVE demonstrates its ability to achieve competitive performance comparable to human ML experts, ranking particularly well on smaller and less complex datasets.

These results were achieved with computational efficiency comparable to that of human experts. Each IMPROVE run was limited to 20 iterations, with more than half of these typically encountering code issues such as undefined variables, wrong package usage, or tensor shape mismatches that cause execution failures. However, these error-containing iterations always terminated quickly, typically within a minute, allowing IMPROVE to proceed to the next iteration with minimal impact on overall efficiency. As a result, fewer than 10 valid iterations are executed and analyzed per run. 

This number of valid attempts is comparable to the number of submission attempts made by human practitioners in Kaggle competitions, as shown in Table~\ref{tab:kaggle}. However, it is important to note that human-submitted Kaggle code is typically tested extensively in local environments on a validation dataset before submission to ensure the code contains no errors and can achieve at least a decent performance. As a result, human practitioners likely conduct far more optimization iterations beyond those reflected in the reported submission counts. Despite these constraints, IMPROVE demonstrates performance close to that of top human ML practitioners, which again highlights its potential as a powerful tool for automated model development in real-world conditions.

\begin{table*}[ht]
\centering
\caption{Average classification accuracy and leaderboard metrics for IMPROVE-generated models and zeroshot prompting LLMs on four Kaggle Datasets. The table also includes the leaderboard rank of the IMPROVE result, the highest accuracy among all Kaggle submissions, and the average submission attempts for the top 5 leaderboard positions.}
\label{tab:kaggle}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Dataset} & \multirow{2}{*}{IMPROVE} & \multirow{2}{*}{Zero-shot} & \multicolumn{3}{c}{Kaggle Statistics} \\
                        &                      &                 &                 & Rank      & Top Acc. & Top Attempts \\ \midrule
\multirow{4}{*}{o1}     & Cassava Leaf Disease & 0.8931±0.0023   & 0.8093±0.0163   & 1591/3900 & 0.9152   & 98           \\
                        & 4 Animals            & 0.9982±0.0015   & 0.9506±0.0323   & 1/221     & 0.9991   & 12           \\
                        & Arabic Letters       & 0.9510±0.0161  & 0.9162±0.0051   & 6/177     & 0.9680   & 10           \\
                        & Kitchenware          & 0.9763±0.0048   & 0.9044±0.0074  & 31/115    & 0.9958   & 10           \\ \midrule
\multirow{4}{*}{GPT-4o} & Cassava Leaf Disease & 0.8574±0.0275   & 0.7748±0.0552   & 2892/3900 & 0.9152   & 98           \\
                        & 4 Animals            & 0.9518±0.0330   & 0.9196±0.0600   & 184/221   & 0.9991   & 12           \\
                        & Arabic Letters       & 0.8403±0.0824   & 0.5946±0.3264   & 85/177    & 0.9680   & 10           \\
                        & Kitchenware          & 0.9793±0.0022   & 0.8581±0.0956   & 25/115    & 0.9958   & 10           \\ \bottomrule
\end{tabular}
\end{table*}

In Table~\ref{tab:ir}, we evaluate the effectiveness of our core technique: Iterative Refinement. To provide a comparison, we developed an alternate version of IMPROVE in which all pipeline components were allowed to be modified simultaneously by their respective agents in every iteration, rather than focusing on improving one component at a time. 

The results indicate that this alternative approach led to less coordinated improvements and significantly higher variability in the outcomes. Interestingly, while some individual runs achieved higher performance than IMPROVE, many others performed worse. Observing the trend in Figure~\ref{fig:graph}, we hypothesize that this variability stems from the increased flexibility of modifying multiple components simultaneously. As shown, the approach not using Iterative Refinement expands the search space, offering greater potential to discover an optimal configuration, resulting in fast improvements in the earlier iterations. However, it also introduces a higher risk of failing to converge on a well-performing configuration within the given time constraints. It can be seen that in the later iterations, the version of IMPROVE without Iterative Refinement struggles to find ways to further improve the system, making blind attempts with similar accuracies, whereas IMPROVE with Iterative Refinement can steadily improve its accuracy over time.

\begin{table}[ht]
\small 
\centering
\caption{Average classification accuracy for IMPROVE-generated models, with and without Iterative Refinement (IR), and zeroshot prompting LLMs on CIFAR-10 and TinyImageNet. The best accuracy on each dataset is bolded.}
\label{tab:ir}
\begin{tabular}{@{}ccccc@{}}
\toprule
Model                   & Dataset      & IMPROVE                & (NO IR) &  Zero-shot \\ \midrule
\multirow{2}{*}{o1}     & CIFAR-10     & \textbf{0.9825±0.0018} & 0.9579±0.0364   & 0.7940±0.0287 \\
                        & TinyImageNet & \textbf{0.8692±0.0212} & 0.8339±0.0105   & 0.4630±0.0520 \\ \midrule
\multirow{2}{*}{GPT-4o} & CIFAR-10     & \textbf{0.9626±0.0311}          & 0.9610±0.0223   & 0.9290±0.0331 \\
                        & TinyImageNet & \textbf{0.7875±0.0169} & 0.6202±0.1908   & 0.4815±0.1091 \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht] 
    \centering
    \includegraphics[width=0.47\textwidth]{images/graph.png} %
    \caption{Average accuracy and standard deviation per iteration for a batch of IMPROVE runs on TinyImageNet using o1. The plot compares the performance trajectory of IMPROVE with Iterative Refinement against a variant where Iterative Refinement is removed. Shaded regions around each line indicate the standard deviation. Iterations that resulted in errors, which do not impact accuracy, are omitted.}
    \label{fig:graph}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablation}
\paragraph{Unified Pipeline Initialization.} 
As described earlier, IMPROVE employs an initialization strategy known as unified pipeline initialization (UPI). In this approach, the Project Architect generates a complete training pipeline in a single step using zero-shot prompting, which is then broken down into individual components. UPI leverages the LLM's ability to generate coherent code and recall previously learned configurations, ensuring that the techniques and parameters used across components are well-aligned. As a result, the generated code is more cohesive, leading to a stronger initialization configuration for the model. In contrast, generating each component sequentially - first creating data augmentation code, passing it to the Model Engineer, and then sharing the data and model code with the Training Engineer - often results in less coherent code and reduced model performance. In Table~\ref{tab:init}, we can see that even though IMPROVE without UPI can still outperform the baseline LLM-generated model, the final accuracy is notably lower than that achieved using UPI. This highlights the importance of a strong initial configuration, which enables IMPROVE to converge to a high-performing model more efficiently.

\begin{table}[ht]
\small 
\centering
\caption{Average classification accuracy for IMPROVE-generated models,  with and without unified pipeline initialization (UPI), and zeroshot prompting LLMs on CIFAR-10 and TinyImageNet. The best accuracy on each dataset is bolded.}
\label{tab:init}
\begin{tabular}{@{}ccccc@{}}
\toprule
Model                   & Dataset      & IMPROVE                & (NO UPI) & Zero-shot \\ \midrule
\multirow{2}{*}{o1}     & CIFAR-10     & \textbf{0.9825±0.0018} & 0.9528±0.0267    & 0.7940±0.0287 \\
                        & TinyImageNet & \textbf{0.8692±0.0212} & 0.7974±0.0162    & 0.4630±0.0520 \\ \midrule
\multirow{2}{*}{GPT-4o} & CIFAR-10     & \textbf{0.9626±0.0311} & 0.9476±0.0259    & 0.9290±0.0331 \\
                        & TinyImageNet & \textbf{0.7875±0.0169} & 0.6646±0.0981    & 0.4815±0.1091 \\ \bottomrule
\end{tabular}
\end{table}

\textit{Smaller LLMs.}
In our experiments, we primarily used two high-performing LLMs, GPT-4o and o1, to build our LLM agents. To test IMPROVE's robustness with smaller LLMs, we also evaluated its performance using GPT-4o-mini. GPT-4o-mini is presented by OpenAI as the official successor to GPT-3.5, offering improvements in cost, speed, and computational efficiency compared to GPT-3.5 while maintaining a strong performance.

\begin{table}[]
\centering
\small 
\caption{Average classification accuracy for IMPROVE-generated models and zeroshot prompting LLMs on CIFAR-10 and TinyImageNet using GPT-4o mini, with results from o1 and GPT-4o for comparison.}
\label{tab:mini}
\begin{tabular}{@{}cccc@{}}
\toprule
Model                        & Dataset      & IMPROVE       & Zero-shot \\ \midrule
\multirow{2}{*}{o1}          & CIFAR-10     & 0.9825±0.0018 & 0.7940±0.0287 \\
                             & TinyImageNet & 0.8692±0.0212 & 0.4630±0.0520 \\ \midrule
\multirow{2}{*}{GPT-4o}      & CIFAR-10     & 0.9626±0.0311 & 0.9290±0.0331 \\
                             & TinyImageNet & 0.7875±0.0169 & 0.4815±0.1091 \\ \midrule
\multirow{2}{*}{GPT-4o-mini} & CIFAR-10     & 0.9590±0.0221 & 0.8364±0.0493 \\
                             & TinyImageNet & 0.6847±0.1226 & 0.5781±0.0673 \\ \bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:mini}, IMPROVE continues to deliver strong results even when using GPT-4o-mini. Despite its smaller size, the model achieves classification accuracies that are significantly higher than those obtained by zero-shot prompting LLMs. This demonstrates IMPROVE's ability to perform well even with limited budget.

\paragraph{Dataset Information Utilization}
\label{sec:dataset}
We also explored how providing additional dataset-specific information influences the design choices and strategies employed by the LLM agents. Specifically, we hypothesize that having access to dataset details would be particularly beneficial for selecting appropriate data augmentation strategies. Our experimental results support this hypothesis, as IMPROVE consistently selected augmentations well-suited to the characteristics of each dataset.

We examined the augmentation strategies IMPROVE employed for each dataset and the reasoning behind them. When training on SVHN, a dataset composed of real-world images of house numbers, IMPROVE selected the ColorJitter augmentation, justifying its choice by stating: "Color jitter (brightness, contrast, saturation, hue) can help in robustifying the model against variations in lighting conditions." This decision is indeed appropriate in this case, as real-world images often contain lighting inconsistencies that can significantly impact model performance.

Conversely, not all augmentations contribute positively to performance. For instance, in the dSprites Orientation dataset, where classification is based on object orientation, applying RandomHorizontalFlip negatively impacts performance by altering class labels and introducing label noise. Although RandomHorizontalFlip was sometimes included in the initial configuration, IMPROVE’s Performance Analyst identified this issue and suggested: "RandomHorizontalFlip may not be useful for orientation classification tasks," and "Orientation-based tasks will not benefit from horizontal flipping; it could even confuse the model."

These findings suggest that IMPROVE is capable of dataset-aware decision-making. Unlike traditional automated approaches that filter through augmentation policies through trial and error, IMPROVE can dynamically adapt its choices based on dataset characteristics and task requirements. By leveraging dataset-specific insights, IMPROVE not only optimizes the augmentation pipeline more effectively but also accelerates model convergence by avoiding ineffective or detrimental transformations.