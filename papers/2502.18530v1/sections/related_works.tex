\label{sec:related_works}
\subsection{Large Language Models and LLM Agents}
Large language models (LLMs) are pre-trained on massive text corpora, often with millions to trillions of parameters. Some of the most well-known LLMs today include GPT-3.5 and GPT-4~\citep{openai2024gpt4} from OpenAI, Claude 3.5 from Anthropic, Mixtral from Mistral, and Llama 3 from Meta. While LLMs can be used directly after pre-training, they often undergo additional training stages, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)~\citep{long2022rlhf}. These stages aim to align LLMs with specific behaviors and objectives, resulting in models capable of performing tasks such as general-purpose chatbots, code generation, and information retrieval.

A subset of LLMs specializes in code generation. Codex~\citep{chen2021codex}, for example, is a GPT model fine-tuned on publicly available GitHub code with a focus on Python programming. Following Codex, Code Llama was introduced, extending coding capabilities to various programming languages such as Python, C++, Java, PHP, and TypeScript. More recently, larger natural language focused LLMs, including GPT-3.5 and Llama 3-8B, have also demonstrated significant coding proficiency across multiple languages, without needing to be explicitly fine-tuned on code.

LLMs have also been used to create human-like agents capable of executing complex instructions autonomously. Examples of such LLM-based agents include ReAct~\citep{yao2023react}, Reflexion~\citep{shinn2023reflexion}, and SwiftSage~\cite{lin2023swiftsage}, which have demonstrated effectiveness in complex reasoning and decision-making tasks. Expanding on this, works like AutoGPT~\citep{autogpt} enabled LLM agents to move beyond reasoning and autonomously perform actions, such as executing code and receiving feedback from outputs. Another line of research explores the collaboration of multiple LLM agents, each with specialized roles~\citep{hong2024metagpt, du2024team, park2023simulacra}. These multi-agent frameworks have shown that LLMs can effectively assume distinct roles~\citep{tseng2024roleplay}, and role specialization enhances their ability to retrieve relevant knowledge, outperforming single-agent systems on tasks requiring reasoning and strategic planning~\citep{sreedhar2024singlemulti}.

\subsection{Automated Machine Learning}
Automated Machine Learning (AutoML) is a field of research seeking to automate various stages of the ML pipeline, making it more efficient to develop ML models. Most works focus on automating specific parts of the process, such as data preparation, model architecture selection, and hyperparameter optimization~\citep{he2021automl}.

For instance, works like AutoAugment~\citep{cubuk2019autoaugment}, Faster AutoAugment~\citep{hataya2019faster}, DADA~\citep{li2020dada}, and TrivialAugment~\citep{muller2021trivial} concentrate on automating the data augmentation component of image classification by applying search strategies over predefined search spaces. Another major focus in AutoML is neural architecture search (NAS), which aims to find the most optimal model architecture from a given search space. A prominent example is \citet{elsken2017nas}, where a hill-climbing algorithm is used to identify the best convolutional neural network (CNN) architecture. More advanced approaches, such as those by \citet{kandasamy2018bo}, leverage Bayesian optimization and optimal transport to refine this search.

In addition to NAS, hyperparameter optimization (HPO) is a closely related area that seeks to identify the best set of hyperparameters for a fixed architecture. Random search~\citep{bergstra2012hpo} is a simple yet efficient method that outperforms traditional grid search~\citep{bergstra2011hpo} by exploring the search space more effectively. To further enhance the search process, many works incorporate Bayesian optimization in HPO~\citep{snoek2012hpo, springenberg2016hpo, falkner2018hpo}, enabling better use of past information to guide future decisions.

\subsection{AutoML with LLMs}
With the recent advancement of LLMs, many researchers have started exploring using LLMs to tackle problems in AutoML, as LLMs offer much greater flexibility in the search space over traditional methods. One of the earliest works in this direction is~\cite{yu2023gptnas}, which utilizes a fine-tuned Generative Pre-Trained Transformer (GPT) model to create new architectures using crossover, mutation, and selection strategies. Later, many works took advantage of the pre-training of LLMs, which grants them an immense amount of implicit knowledge on neural architectures. GENIUS~\citep{zheng2023genius} is one such work, where NAS is performed by simply prompting GPT-4 to generate different configurations, which are evaluated iteratively for a pre-determined number of iterations. In a different direction, \cite{hollmann2024caafe} proposed the CAAFE method that uses LLMs to automate feature engineering by generating semantically meaningful features for tabular datasets. \cite{zhang2023automlgpt} worked on a more comprehensive pipeline, attempting to simultaneously tackle many tasks including object detection, object classification and question answering with LLM-based AutoML, by optimizing the pipeline with LLM-predicted training log. Another work, AutoMMLab~\citep{yang2024autommlab}, focuses on automating computer vision model training, guiding LLMs to handle data selection, model selection, and hyperparameter tuning sequentially, based on natural language requests.


Recently, researchers have advanced the integration of LLM agents into AutoML, evolving their use from simple code generation tools to comprehensive systems that can automate a wider range of components in the machine learning pipeline. For instance, Agent K~\citep{grosnit2024kaggleagent} proposes an end-to-end pipeline, where the agent scrapes data from a Kaggle URL, trains a model, and submits results, leveraging LLM reasoning and Bayesian optimization in the process to optimize the pipeline. Similarly, AutoML-Agent~\citep{trirat2024automlagent} employs a multi-agent framework that automates model generation in a multi-stage pipeline, such as data preprocessing, model design, and hyperparameter optimization, and includes multi-stage verification to ensure correctness. Data Interpreter~\citep{hong2024datainterpreter} approaches AutoML through dynamic task decomposition, constructing, adjusting, and executing task and action graphs where each node represents a specific sub-task. Other frameworks focus specifically on data science. For example, AutoKaggle~\citep{li2024autokaggle} targets tabular data with a similar multi-agent setup, but includes a library of machine learning functions like missing value imputation and one-hot encoding for the agents to use. DS-Agent\citep{guo2024dsagent} develops models for text, time series, and tabular data by utilizing case-based reasoning with insights from Kaggle technical reports and code. Although many works in this direction aim to maximize automation, some others focus on human-AI collaboration. For instance, LAMBDA~\citep{sun2024lambda} is a human-in-the-loop data science agent that employs multi-agent collaboration for code generation, verification, and execution, similar to other methods, but is able to assist users interactively through an intuitive interface.
