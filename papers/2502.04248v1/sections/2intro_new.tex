%A large body of work has shown that image classification systems can easily be fooled by small perturbations \cite{szegedy2013intriguing,goodfellow2015,XiaoZ0HLS18,xiao2018generating,kurakin2018adversarial,kaufmann2019testing}. These perturbed inputs, known as adversarial examples, are typically bounded by small $\ell_p$ perturbations and imperceptible to the human eye~\citep{goodfellow2015}. The most promising line of defense against such attacks is known as adversarial training, which incorporates adversarial examples into the training process~\citep{madry2017towards, zhang2019theoretically, croce2020robustbench, gowal2020uncovering, cohen2019certified,  gowal2021improving, sehwag2021robust}. 

\begin{figure}[th]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/summary2.pdf}
        \vspace{-20pt}
    \caption{\textbf{Impact of our proposed regularization term (ALR) in both training and fine-tuning on CIFAR-10.}  Adversarial $\ell_2$ regularization (ALR) significantly improves generalization to the unforeseen StAdv attack when performing adversarial training for $\ell_2$ robustness.  Using ALR when subsequently fine-tuning with only StAdv attack also decreases the drop in $\ell_2$ robustness.}
    \label{fig:summary}
    \vspace{-20pt}
\end{figure}

For safety critical applications, it is important to defend machine learning (ML) models against test-time attacks.  However, many existing defenses \citep{madry2017towards, zhang2019theoretically, croce2020robustbench} assume that the adversary is restricted to a narrow threat model such as an $\ell_p$ ball of fixed radius around the input.
When this assumption is violated, the robustness of adversarially trained models can significantly degrade~\citep{dai2023multirobustbench, kaufmann2019testing}. Additionally, due to rapid development of new types of attacks \citep{XiaoZ0HLS18, LaidlawF19, laidlaw2020perceptual, kaufmann2019testing}, it is difficult to anticipate all types of attacks in advance.  This raises the question: \emph{how can we defend models as new attacks emerge?}%Several variants of adversarial training have been developed to train models robust to sets of attacks -- a quality we refer to as \textit{multi-robustness}~\citep{MainiWK20, TB19, madaan2020learning, Croce020,croce2022adversarial}. However, given the rapid development of new types of attacks, even a multi-robust model could still be fooled by an adversary not accounted for during training. This raises the question: \emph{how can we defend models as new adversaries emerge?}
%Prior work in the field of multi-robust training provides possible starting points. One approach would be train a model using a method that ensures robustness against all types of attacks, including those not known at training time~\cite{laidlaw2020perceptual,dai2022formulating}. However, these approaches inevitably make assumptions about the kinds of attacks that will be seen in the future. For example, \citet{laidlaw2020perceptual} provide a method to defend against a wide spectrum of imperceptible attacks. In practice, though, attacks against image classifiers need not be imperceptible, so long as they do not change the semantic meaning of an image~\cite{kaufmann2019testing}. This limitation makes it difficult for these techniques to provide strong guarantees against adaptive adversaries. Another approach would be to train a new multi-robust model from scratch on a set of known attacks each time a new attack is introduced~\cite{MainiWK20,TB19, madaan2020learning, Croce020, croce2022adversarial}. However, these techniques are computationally intensive, and do not make use of the information that was learned in previous iterations of model training. An ideal approach would be able to rapidly learn defenses against new attacks while not forgetting information already learned about previous attacks.

For long-term robustness, models must quickly adapt to new attacks without sacrificing robustness to previous ones, a goal known as continual adaptive robustness (CAR) \citep{dai2024position} (\S \ref{sec: setup}). A natural approach is to apply adversarial training on known attacks and fine-tune when new ones emerge, a process we call continual robust training (CRT). However, adversarial training provides poor generalization to unseen attacks, leading to suboptimal starting points for fine-tuning, and fine-tuning itself can degrade robustness against past attacks (\cref{fig:summary}).

We theoretically show that the robustness gap between attacks is linked to logit-space distances between perturbed and clean inputs and regularizing these distances can improve generalization to new attacks and reduce drops in robustness on previous attacks. Extensive experiments confirm these findings. Our key contributions are as follows:

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/overview_figure_3.pdf}
	\caption{\textbf{An overview of the problem of adapting to new adversaries (continual adaptive robustness) and our solution framework (Regularized Continual Robust Training).} The defender learns about the existence of new attacks sequentially, and at time $t$ aims to achieve robustness against $K(t)$, the set of attacks known at times $\le t$.  A model $h_0$ is deployed at time $0$ to be robust against an initial set of known attacks, and new attacks are introduced at times $t_1$, $t_2$, and $t_3$. We propose performing regularized initial robust training \edit{on the initially known attack(s)} and then using regularized fine-tuning to adapt the model against future attacks within time $\Delta t$, leading to a sequence of models $h_0, h_{t_1 + \Delta t}, h_{t_3 + \Delta t}, h_{t_3 + \Delta t}$.}
	\label{fig:overview}
	\vspace{-10pt}
\end{figure*}

\noindent \textbf{Regularized Continual Robust Training for Adapting to New Adversaries} (\cref{sec: theory_methods}) . To enhance CRT, we analyze the difference in robust losses between attacks and show it is upper bounded by the sum of the maximal $\ell_2$ distance between clean and perturbed logits for both attacks. Minimizing this bound can thus improve generalization to new attacks and preserve robustness against previous ones. This motivates our proposed \textit{adversarial $\ell_2$ regularization (ALR)}, which penalizes the $\ell_2$ distance between adversarial and benign logits.
%To improve the performance of CRT, we analyze the difference in robust losses for a pair of attacks.  We prove this value is upper bounded by the maximal $\ell_2$ distance between the logits of the clean and a perturbed versions of a data point. Reducing this upper bound can thus improve generalization to unforeseen attacks and reduce degradation in robustness on previous attacks when used during fine-tuning.  This leads us to introduce a regularization term which we call \textit{adversarial $\ell_2$ regularization} (ALR). ALR induces a penalty to the loss proportional to the $\ell_2$ distance between adversarial and benign logits.

\noindent \textbf{Empirical Validation on Sequentially Introduced Attacks} (\cref{sec:car_reg}).
We conduct experiments on 2 sequences of 4 attacks across 3 datasets (CIFAR-10, CIFAR-100, and Imagenette). Our results show that ALR improves robustness in CRT with a 5.48\% gain in Union accuracy (worst-case across all attacks) across $\ell_2$, StAdv \citep{XiaoZ0HLS18}, and Recolor attacks \citep{LaidlawF19} over its unregularized counterpart. \cref{fig:summary} visualizes improvements brought through ALR for a sequence of 2 attacks.  %We find that when applied on a sequence of $\ell_2$, StAdv \citep{XiaoZ0HLS18}, and Recolor attacks \citep{LaidlawF19} on CIFAR-10, regularized CRT achieves 5.48\% higher average robustness compared to its unregularized counterpart.

\noindent \textbf{Impact of ALR and Efficient Approximations in Training and Fine-Tuning} (\cref{sec:init_train_impact},\cref{sec:fine-tuning_impact}). We conduct ablations using over $100$ attack combinations (12 attack types, with 9 non-$\ell_p$) to study ALRâ€™s role in different stages of CRT. We also explore random noise-based regularization as a more efficient alternative.  We find that while noise-based regularization improves generalization in initial training, ALR is essential for maintaining robust performance during fine-tuning and improves Union accuracy by up to 7.85\%.

%Our paper is organized as follows: in \S \ref{sec: setup}, we introduce continual adaptive robustness (CAR).  In \S \ref{sec: theory_methods}, we introduce continual robust training (CRT), provide theoretical analysis and motivate the use of regularization in CRT.  In \S \ref{sec: results}, we provide rigorous experimental results to demonstrate improvements through regularization.

%\noindent %\textbf{Regularization Methods (\cref{subsec: methods}):} Our theoretical results suggest a canonical regularization term, which we call \textit{adversarial $\ell_2$ regularization} (ALR). This regularization term induces a penalty to the loss proportional to the Euclidean distance between adversarial and benign logits. As points of comparison, we also consider two potential variants to improve efficiency and robustness: i) \emph{Random Noise Regularization:} the perturbations used are generated using \emph{random noise} instead of worst-case adversarial perturbations; ii) \emph{Variation Regularization \citep{dai2022formulating}:} the Euclidean distance between logits resulting from different instantiations of the adversarial perturbation is minimized.


%\noindent %\textbf{Empirical Findings (\cref{sec: results}):} We conduct experiments on hundreds of attack combinations on 3 vision datasets to rigorously determine the multi-robustness of models trained using our technique~\footnote{Our anonymized code is available at \url{https://anonymous.4open.science/r/MultiRobustness-5B72/README.md}.}. We compare with the `training-from-scratch' and fine-tuning without regularization baselines.
%We find that all methods of regularization can help a pretrained model generalize to new attacks. For instance, when incorporating ALR into $\ell_2$ adversarial training on CIFAR-10 we observe robust accuracy on a spatial attack (StAdv \citep{XiaoZ0HLS18}) to increase from 2.08\% to 48.23\%. This effect holds for all of the regularization terms that we test. When learning to be robust against new attacks, however, more care is necessary. We find that regularization techniques based on adversarial perturbation (ALR and variation regularization) are effective, while terms based on random perturbation can lead to forgetting of prior attacks.
%For example, when fine-tuning a model initially robust to $\ell_2$ attacks, using ALR can improve robust accuracy on the StAdv attack from 54.3\% to 65.1\%, while reducing forgetting by 1.5 percentage points. Further, when considering a sequence of 4 attacks, regularized fine-tuning can cut training time by half while matching the 'training-from-scratch' baseline.
% Interestingly, we observe that when used in initial training, all variants of regularization generally improve robustness on new attacks. For example, using regularization based on uniform noise, we can improve robust accuracy on a spatial attack (StAdv \citep{XiaoZ0HLS18}) from 0.79\% to 26.22\% while training only with $\ell_2$ attacks on CIFAR-10. We further observe that variants of regularization using adversarial perturbations (adversarial $\ell_2$ and variation regularization) generally improve robustness when used during fine-tuning. For example, when fine-tuning a model initially robust to $\ell_2$ attacks, using adversarial $\ell_2$ regularization can improve robustness on the snow attack~\citep{kaufmann2019testing} by 7.85\% on ImageNette \citep{imagenette} (from 27.64\% to 35.49\%).

%\noindent \textbf{Looking ahead (\cref{sec: discussion}):} Our methods can help practitioners rapidly adapt their models to remain robust against changing real-world threats and allows researchers to rapidly prototype their defenses for multi-robustness. However, our methods are largely agnostic to the specific attacks being defended against. Future work is necessary to better understand how interactions between different adversarial constraints impact the joint learnability of attacks. We believe our methods could be adapted to ensure other desirable properties, such as compliance with changing standards for fairness or privacy. 