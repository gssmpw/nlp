
\textbf{Adversarial Attacks and Defenses:} ML models are vulnerable to input-space perturbations known as adversarial examples \citep{szegedy2013intriguing}.  These attacks come in different formulations including $\ell_p$-norm bounded attacks \citep{madry2017towards, carlini2017towards}, spatial transformations \citep{XiaoZ0HLS18}, color shifts \citep{LaidlawF19}, JPEG compression and weather changes \citep{kaufmann2019testing}, bounded Wasserstein distance \citep{wasserstein_attacks, wu2020stronger} as well as attacks based on distances that are more aligned with human perception such as SSIM \citep{GRAGNANIELLO2021142} and LPIPS distances \citep{laidlaw2020perceptual, ghazanfari2023r}.

Despite the wide variety of attacks that have been introduced, defenses against adversarial examples focus mainly on $\ell_{\infty}$ or $\ell_2$-norm bounded perturbations \citep{cohen2019certified, zhang2020towards, madry2017towards, zhang2019theoretically, croce2020robustbench}.  Of existing defenses, adversarial training \citep{madry2017towards}, an approach that uses adversarial examples generated by the attack of interest during training, can most easily be adjusted to different attacks.  In our work, we build off of adversarial training in order to adapt to new adversaries.

\textbf{Training Techniques for Multi-Robustness:}A few prior works have studied the problem of achieving robustness against multiple attacks, under the assumption that all attacks are known a priori.  These include training based approaches \citep{MainiWK20, TB19, madaan2020learning} which incorporate adversarial examples from the threat models of interest (usually the combination of $\ell_1$, $\ell_2$, and $\ell_\infty$ norm bounded attacks) during training.  \citet{Croce020} provides a robustness certificate of all $\ell_p$ norms given certified robustness against $\ell_\infty$ and $\ell_1$ attacks.

Another line of works has looked at defending against attacks that are not known by the defender, which is a problem known as unforeseen robustness.  These techniques are all training-based and include \citet{laidlaw2020perceptual} which proposes training based on LPIPS \citep{zhang2018lpips}, a metric more aligned with human perception than $\ell_p$ distances, and \citet{dai2022formulating, jin2020manifold} which use regularization during training in order to obtain better generalization to unforeseen attacks.  \citet{dai2023multirobustbench} provides a comprehensive leaderboard for the performance of existing defenses against a large variety of attacks at different attack strengths.

Our work differs from these lines of works since we assume that while the defender may not know all attacks a priori, they are allowed to adjust their defense when they become aware of new attacks.  The work most similar to ours is \citet{croce2022adversarial}, which proposes fine-tuning a model robust against one $\ell_p$ attack to be robust against the union of $\ell_p$ attacks.  Specifically, they demonstrate that we can achieve simultaneous multiattack robustness for the union of $\ell_p$ attacks by obtaining robustness against $\ell_1$ and $\ell_{\infty}$ attacks, and thus propose fine-tuning with $\ell_1$ and $\ell_{\infty}$ attacks to achieve this efficiently.  Our work differs from this work since we explore adapting to attacks outside of $\ell_p$ attacks, investigate ways of improving the initial state of the model prior to fine-tuning, and consider adapting to sequences of attacks.

\textbf{Continual Learning:} A similar direction of research is continual learning (CL) in which a set of tasks are learned sequentially with the goal of performing as well as if they were learned simultaneously \citep{wang2023comprehensive}.  Few works have studied CL in conjunction with adversarial ML.  Of these, most works focus on evaluating or improving the robustness of models trained in the CL framework \citep{bai2023towards,9892970, khan2022susceptibility}. The most similar to our work is \citet{wang2023continual} which treats different attacks as tasks and uses approaches in CL in order to sequentially adapt a model against new attacks.  The attacks they consider follow the same threat model (ie. $\ell_{\infty}$ attacks using different optimization procedures to find the adversarial example).  In our work, we investigate adapting to new threat models.

%\textbf{Representation Similarity } \sophie{depending on how large of a portion CKA results make up the experimental section and how much space we have, can move this section into the appendix}
%\citep{cianfarani2022understanding}
