In this section, we introduce the problem of continual adaptive robustness (CAR)~\cite{dai2024position}, which aims to achieve robustness against new attacks as they are sequentially discovered. We survey existing approaches to this problem, with additional related work included in \cref{appsec: add_rel_work}.  CAR is visualized in Figure \ref{fig:overview}.

\subsection{A Motivating Example}
Consider an entity that wants to deploy a robust ML system. The entity uses recent techniques (\emph{e.g.} adversarial training) to defend their model  against existing attack types (such as $\ell_p$ perturbations) and deploys their model at time $t=0$. At a later time $t_1$, a research group publishes a paper about a new attack type (\emph{e.g.} spatial perturbations \citep{XiaoZ0HLS18}) against which the entity's model is not robust.  Since the ML system has been deployed, the entity would want to \textit{quickly} modify the model to be robust against the new attack while maintaining robustness against previous attacks.  Having a quick update procedure would minimize the time that an attacker can exploit this vulnerability.  Quick adaptation to new attacks is the foundation of continual adaptive robustness (CAR), a problem setting first introduced in a position paper \citep{dai2024position}.  We propose and analyze the first dedicated defense for CAR in this paper.%, and any adversary can take advantage of the new attack type to break the defended model. Thus, the entity needs to develop a new defense.
%Consider an entity which has deployed an ML system involving an image classifier and wants their classifier to predict robustly even in the presence of an adversary. This entity follows the adversarial robustness literature and uses a technique such as adversarial training to secure their model against $\ell_p$ perturbations and deploys their model at time $t=0$.  At some later time $t_1$, a research group publishes a paper about a new attack type (for example, spatial perturbations \citep{XiaoZ0HLS18}) that the entity's image classifier is not robust against.  In this case, what should the entity do?

%The commonly used definition of adversarial robustness is unable to capture these goals since it does not consider the dimension of time in its formulation. \citet{dai2024position} \edit{is a position paper which} introduces a problem setting called continual adaptive robustness (CAR) which models a dynamic setting of robustness where new attack types are \emph{revealed} to the defender sequentially and the defender has the ability to update their model with information of the new attack.  We now provide the problem formulation for CAR and demonstrate how it models the example described in this section.  \edit{In later sections, we will introduce a novel defense framework for this problem which we call continual robust training (CRT) and theoretically and empirically demonstrate how regularized CRT can help improve performance in CAR.}%. Other possible models are discussed in \cref{sec: discussion}.

\subsection{Problem Formulation}
\noindent
\textbf{Notation:} $\mathcal{D} = X \times Y$ denotes a data distribution where $X$ and $Y$ are the support of inputs and labels, respectively.  $\mathcal{H}$ denotes the hypothesis class.  We use $C:X \to \tilde{X}$ to define an adversarial constraint where $\tilde{X}$ is the space of adversarial examples.  $\ell: Y \times Y \to \mathbb{R}$ denotes the loss function.

\noindent
\textbf{Attack sequences:} In CAR~\cite{dai2024position}, different test-time attacks are introduced sequentially (Figure \ref{fig:overview}). Each attack (perturbed input) can be formulated as the maximizer of the loss $P_C(x, y, h) = \argmax_{x' \in C(x)} \ell(h(x), y)$ (within the constraint $C$) and has a corresponding time $T(P_C)$ at which it is discovered by the defender.  We call the set of attacks known by the defender at a given time $t$ the \textit{knowledge set} at time $t$: $K(t) = \{P\ | T(P) \le t\}$. The expansion of $K$ over time models settings such as research groups or a security team discovering new attack types. 

\noindent
\textbf{Goals in CAR:}  A defender in CAR uses a defense algorithm $\mathcal{A}_{\text{CAR}}$ to deploy a model $h_t = \mathcal{A}_{\text{CAR}}(\mathcal{D},K(t),\mathcal{H})$ at each time step $t$. Performance at time $t$ is measured by Union robust loss 
% \anote{Say that this is the Union loss quantity we care about in general, good robustness on all known attack} a
across the knowledge set:
$\mathcal{L}(h, t) = \mathbb{E}_{(x,y)\sim \mathcal{D}} \max_{P \in K(t)} [\ell(P(x,y, h), y)]$.%Overall, there are 3 goals of the defender: (1) achieve good robustness on the set of known attacks, (2) achieve some robustness on previously unseen attacks, and (3) recover quickly from recently introduced attacks.  We now provide a more formal description of what it means for a defense to achieve CAR: \anote{The 3 goals are defined again below the definition, also are these and the definition taken from the position paper? If yes, then clarify}

\begin{definition}[Continual Adaptive Robustness \citep{dai2024position}] Given loss tolerances $\delta_{\text{known}}$ and $\delta_{\text{unknown}}$ with $0 < \delta_{\text{known}} < \delta_{\text{unknown}}$ and grace period $\Delta t$ for recovering from a new attack, a defense algorithm $\mathcal{A}_{\text{CAR}}$ achieves CAR if for all $t > 0$:
\begin{itemize}
    \item When $t - T(P) < \Delta t$ for any attack $P$ and $T(P) < t$, $h_t$ satisfies $\mathcal{L}(h_t, t) \le \delta_{\text{unknown}}$
    \item Otherwise, $\mathcal{L}(h_t, t) \le \delta_{\text{known}}$.
\end{itemize}
\end{definition}

These criteria capture 3 distinct goals for the defender:  (1) The model at time $t$ must \textit{achieve good robustness if no attacks have been introduced recently} (within $\Delta t$ time). This is due to the $\delta_{\text{known}}$ threshold on the robust loss in the second criterion;  (2) If a new attack has occurred within $\Delta t$ period before the current time $t$, the model at time $t$ must \textit{achieve some robustness against the new attack}.  This is modeled by the $\delta_{\text{unknown}}$ threshold in the first criterion.  Since $0 < \delta_{\text{known}} < \delta_{\text{unknown}}$, CAR tolerates a degradation in robustness between the 2 cases; (3) The defense is expected to \textit{recover robustness quickly after new attacks}.  This is modeled by the $\Delta t$ time window; $\Delta t$ time after the introduction of a new attack, the loss threshold changes from $\delta_{\text{unknown}}$ to $\delta_{\text{known}}$.
%\begin{enumerate}
%    \item \textit{Achieve good robustness when all attacks are known}: without the introduction of new attacks, the defender has a lower error tolerance $\delta_{\text{known}}$ for $\mathcal{L}(h_t, K(t))$.  We also note that $\mathcal{L}$ measured is the worst-case over all attacks which ensures that the model must perform well over all attacks.
%    \item \textit{Recover robustness quickly after the introduction of a new attack}: After a new attack is introduced, the defender must recover robustness within $\Delta t$ time since after the $\Delta t$ grace period, the defender must achieve $\mathcal{L}(h_t, K(t)) \le \delta_{\text{known}}$ again.  
%    \item \textit{Residual robustness when a new attack is introduced}: Within the $\Delta t$ grace period after a new attack is introduced, the defender should have $\mathcal{L}(h_t, K(t)) \le \delta_{\text{unknown}}$.  While $\delta_{\text{unknown}}$ is larger than $\delta_{\text{known}}$, meaning we do not expect as much robustness as on the set of known attacks (used by the defense), we still have an upper bound on the allowed error so we hope to have some robustness against the new attack as well.
%\end{enumerate}

%The goal of the defender is to ensure robustness against the set of attacks known by the defender at any time $t\ge 0$ (where $t = 0$ denotes the time at which the initial model is deployed): $K(t) = \{P\ | T(P) \le t\}$.  We call $K(t)$ the defender's knowledge set, and the expansion of $K$ over time models settings such as research groups or a security team discovering new attack types.

%\noindent
%\textbf{Multi-robustness:} Let $$\mathcal{L}(h, K(t)) = \mathbb{E}_{(x,y)\sim \mathcal{D}} \max_{P \in K(t)} [h(P(x)) \ne y]$$ be the adversarial 0-1 loss
%of model $h \in \mathcal{H}$ to the union of attacks in $K(t)$.  In the original formulation of adversarial robustness, the goal of the defender is to choose an algorithm $\mathcal{A}_{\text{AR}}$ which chooses a model $h$ such that $\forall t \ge 0$, $\mathcal{L}(h, K(t)) \le \delta$, where $\delta > 0$ is a small value representing the amount of error that can be tolerated by the application of interest; which would then lead to the defender failing this meet this objective for the first new attack $P$ for which $T(P) > 0$.

%\noindent \textbf{Robust model sequence:} To resolve this, firstly, a defender in CAR chooses an algorithm that not only gives a single model but rather gives a sequence of models $h_t$ for each time step.  Additionally, rather having a single error threshold $\delta$, the CAR problem has two separate error tolerances representing the tolerance when all attacks are known to the defender $\delta_{\text{known}}$ and when a new attack is introduced $\delta_{\text{unknown}}$ with $0 < \delta_{\text{known}} < \delta_{\text{unknown}}$.  $\delta_{\text{unknown}}$ is in effect for a small time window $\Delta t$ representing the grace period for recovering after a new attack is introduced.  More formally,
%\begin{itemize}
%    \item When $t - T(P) < \Delta t$ for any attack $P$ and $T(P) < t$, the defender's algorithm $\mathcal{A}_{\text{CAR}}$ aims to give a $h_t$ such that $\mathcal{L}(h_t, K(t)) \le \delta_{\text{unknown}}$
%    \item Otherwise, $\mathcal{L}(h_t, K(t)) \le \delta_{\text{known}}$.
%\end{itemize}



%Then, the goal of the defender is choose an algorithm $\mathcal{A}$ which $\forall t \ge 0$ outputs a $h_t$ such that, $\mathcal{L}(h_t, K(t)) < \delta$, where $\delta > 0$ is a small value representing the amount of error that can be tolerated by the application of interest. Unlike existing defenses that output a single robust model, $\mathcal{A}$ outputs a model at each time step and is able to use information from $K(t)$ and previous models $\{h_{i}\}_{i < t}$ in order to generate $h_{t + \Delta t}$, where $\Delta t$ is the the time taken to update the model.

%\begin{enumerate}
%    \item Achieve robustness on the new attack while maintaining robustness on previous attacks - given that the objective of the defender is to minimize $\mathcal{L}(h_t, K(t)) = \mathbb{E}_{(x,y)\sim \mathcal{D}} \max_{P \in K(t)} [h_t(P(x)) \ne y]$, the defender needs to optimize for a model that has good performance across the new attack and previous attacks.
%    \item Quickly update vulnerable models - 
%    \item Have robustness to unforeseen attacks - 
%\end{enumerate}

% Unlike existing defense algorithms that focus on outputting a single defended model, a defense algorithm for the CAR problem outputs a model $h_t \in \mathcal{H}$ for each time step.  To choose $h_t$, the defender's algorithm $\mathcal{A}$ can only 

%\textbf{Why is CAR important? } For domains such as vision, it is hard to define the space of all possible attacks, due to which new attack types are periodically discovered \citep{kaufmann2019testing, XiaoZ0HLS18, LaidlawF19}, bypassing previous defenses. It is thus difficult to ensure that a single defended model is robust to all possible future attacks. CAR can be thought of as a framework with the goal of resilience: even if a new attack is introduced, we should be able to quickly update the model to recover from this new attack. A defense for CAR can also be applied with multiple known attacks for efficient training of multi-robust models. %\arjun{Also important even when we know attacks already, we can train one-by-one instead of all at once for efficiency, which could be useful}

\subsection{Baseline approaches to CAR}
\noindent
\textbf{CAR through multiattack robustness (MAR). }Prior works for multiattack robustness often involve training with multiple attacks simultaneously \citep{TB19, MainiWK20}, which can be computationally expensive. A trivial (but expensive) defense algorithm for CAR is to use these training-based techniques and retrain a model from scratch on $K(t)$ every time it changes.  However, this would require us to tolerate larger values of $\Delta t$. % For CAR, inefficiency in updating a model with each new attack is harmful since the adversary can continue to exploit the new attack while the model is being updated.% \arjun{This paragraph should say that sMAR is the framework that all previous work has basically operated in}

\noindent
\textbf{CAR through unforeseen attack robustness (UAR). } Defenses for unforeseen attack robustness (UAR) aim to attacks outside of those used in the design of the defense. Another trivial defense algorithm for CAR is to use a UAR defense \citep{laidlaw2020perceptual,dai2022formulating} to get a model $h$ and use $h$ for all time steps.  This approach is efficient as no time is spent updating the model but would require much higher values of $\delta_{\text{known}}$ as these methods do not obtain high robustness across attacks \citep{dai2023multirobustbench}.%  Prior work  demonstrates that current techniques for UAR performs poorly when evaluated on a wide variety of attacks with best performing approaches achieving only 3\% when considering the worst-case attack of the set. 
% \arjun{Worried about the phrasing here, we might get asked why we don't compare to UAR based defenses}
%\arjun{There needs to be more motivation either here or in the next section about why we ask the theoretical question we do ask. }