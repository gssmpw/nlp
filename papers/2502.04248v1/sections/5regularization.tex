\label{sec:regularization}
Theorem \ref{thm:robustness} suggests that reducing the sensitivity of logits to \textit{either} attack has the potential to reduce the performance gap between attacks (see \cref{fig:loss_gap} in the Appendix for an empirical validation of this effect). To this end, we propose incorporating regularization into both training stages.  Specifically, we adopt modified training objective $\mathcal{L}_{\text{reg}}(h,t) = \mathcal{L}(h,t) + \lambda R(h, K(t))$, 
% andx\anote{I think this is now $\mathcal{L}(h,0)$, and respectively for the fine-tuning loss} $L_{\text{fine-tune-reg}}(h, t) = L_{\text{fine-tune}}(h) + \lambda R(h, K(t))$, 
where $\lambda$ is the regularization strength and $R(h)$ is the regularization term used. We will now discuss several forms of regularization.


%We adopt a modified training objective $L(h) = L_{\text{init}}(h) + \lambda R(h)$,
%where $\lambda$ is the regularization strength and $R(h)$ is the regularization term used. 
%\edit{\st{Following the notation of our theoretical results, we define $h$ to be a composition of a feature extractor and a final linear layer (i.e. $h(x) = Wg(x)$).}}
% Notably, we introduce regularization terms in both pretraining and fine-tuning, which is supported both by our theoretical results and prior work showing that regularization can improve model performance on unseen attacks \cite{dai2022formulating}. 
%As regularization with respect to \textit{either} attack has the potential to directly reduce the upper bound in \ref{thm:robustness}, we would expect the modified objective to be beneficial in both the pre-training stage (when only the initial attack is known) and the fine-tuning stage (when both attacks are known).


\noindent
\textbf{Adversarial $\ell_2$ regularization. (ALR)}
Driven by our theoretical results, we first introduce adversarial $\ell_2$ regularization: $R_{\text{ALR}}(h, K(t)) =  \frac{1}{m} \sum_{i=1}^m \max_{x' \in C(x_i)} \|h(x') - h(x_i)\|_2$ where $C = C_{\text{init}}$ in initial training and corresponds to attack $P_C = F(K(t), (x_i, y_i))$ chosen by the fine-tuning strategy.  $\ell_2$ regularization penalizes the maximum distance between a sample's \edit{logits} and the furthest adversarially perturbed \edit{logits} within that sample's neighborhood. Using this regularization term would directly minimize the upper bounds in Theorem~\ref{thm:robustness} and Corollary~\ref{thm:corollary}.
We note that while ALR is similar in form to  TRADES~\cite{zhang2019theoretically}, it uses a Euclidean distance instead of the KL-divergence. Our paper is the first to show that this form of regularization is beneficial for CAR.

\noindent
\textbf{Efficiently approximating ALR. } Computing ALR uses multi-step optimization which can be costly to compute in practice. To improve efficiency in experiments, we consider (1) using single step optimization for ALR and (2) using randomly sampled, unoptimized perturbations can help with CAR. For (2), we consider Gaussian noise regularization (GR) and Uniform noise regularization (UR), specifically:
$R_{\text{GR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m \|\edit{h}(x') - \edit{h}(x_i)\|_2$ where $x' \sim \mathcal{N}(0, \sigma^2)$ and 
$R_{\text{UR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m \|\edit{h}(x') - \edit{h}(x_i)\|_2$ where $x' \sim \mathcal{U}(-\sigma, \sigma)$.

\noindent
\textbf{Other Baselines.} We compare to variation regularization (VR), which has been shown to improve generalization to unforeseen attacks \citep{dai2022formulating}. VR is defined as: $R_{\text{VR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m \max_{x', x'' \in C(x_i)} \|\edit{h}(x') - \edit{h}(x'')\|_2$ where $C = C_{\text{init}}$ in initial training.  We also consider VR in finetuning with $C$ corresponding to attack $P_C = F(K(t), (x_i, y_i))$.
The link between VR and ALR is discussed in \cref{app:alr_vr}.
% \sophie{add link to Appendix, mention that the terms are connected.}
%\edit{We note that since VR maximizes over 2 perturbations and $x_i \in C(x_i)$, we can lower bound VR with ALR:} $R_\text{ALR}(h) \leq R_\text{VR}(h)$.  \edit{Additionally, by adding and subtracting $h(x_i)$ from within the distance computation and applying triangle inequality, we have that }$R_\text{VR}(h) \le 2R_\text{ALR}(h)$.
%Since during the initial training phase, the CAR problem reduces to an unforeseen robustness problem, variation regularization may provide a good starting point in terms of robustness.
%Algorithmically, \citet{dai2022formulating} estimate the maximization within $R_{\text{VR}}(h)$ by optimizing both $x'$ and $x''$ simultaneously with PGD.



