
This appendix is organized as follows:
\begin{enumerate}
    \item Additional related work (\cref{appsec: add_rel_work})
    \item Future directions (\cref{appsec:future_directions})
        \item Proofs (\cref{sec:proof})
    \item Connection to variation regularization (\cref{app:alr_vr})

        \item Experimental verification of theoretical results (\cref{appsec:experimental_verification_of_theory})
    \item Additional experimental setup details (training and attack parameters, model selection, regularization setup) (\cref{app:exp_setup})
    \item Additional experiments
    \begin{itemize}
        \item Longer attack sequences and different datasets (CIFAR-100 and ImageNette) (\cref{app:exp_seq})
        \item Ablations on initial training (attack choice, regularization parameters) (\cref{app:init_train})
        \item Ablations on fine-tuning (attack choice, regularization parameters) ((\cref{app:fine-tuning}))
    \end{itemize}
\end{enumerate}


\section{Additional Related Work}
\label{appsec: add_rel_work}
\input{sections/3related_works}

\section{Future Directions}
\label{appsec:future_directions}
We now discuss a few directions for future work in depth.

\noindent\edit{\textbf{Choice of initial attacks and attack similarities. } In this work, we looked at $\ell_2$ and $\ell_\infty$ attacks as the initial attack in the CAR problem.  However, in practice, we would like to choose an initial attack that is the most representative of the attacks we want to be robust against, in order to generalize to downstream new attacks.  Further research on understanding and improving the initial attack can improve the accuracies achieved through training with CRT.  Additionally, having ways of measuring attack similarity between the known attacks and new attacks can help allow us to decide whether using CRT is sufficient for achieving good robustness or whether we need to train from scratch or combine the model with other defenses tailored towards the new attack.}

\noindent\edit{\textbf{Attack Monitoring. }}One assumption of CAR is that the defender is able to discover when a new attack exists.  While this is clear in cases such as a research group publishing a paper with a new attack or a company's security team finding a vulnerabilities, in practice, we would also be interested in recovering after an adversary discovers a new, unknown attack and successfully attacks the model. \edit{In this case, we would need a good monitoring system for detecting and synthesizing these new attacks for use with CRT.}

\noindent\edit{\textbf{Reducing catastrophic forgetting. } In CAR, since attacks are introduced sequentially, catastrophic forgetting is an important problem.  In our work, we utilized replay via \citet{croce2022adversarial}'s fine-tuning approach and also found that ALR reduces catastrophic forgetting to some extent.  Future work on reducing catastrophic forgetting can help improve the effectiveness of updating the model with CRT.}

\noindent\edit{\textbf{Training and fine-tuning efficiency.}} \edit{In our experiments,} we combine regularization with \citet{croce2022adversarial}'s fine-tuning approach due to the effectiveness and efficiency of that approach.  Further research on developing better and more efficient fine-tuning techniques for achieving robustness to new attacks (while maintaining robustness against previous attacks) can improve our CRT framework.

\noindent\edit{\textbf{Model capacity.} Current works in adversarial robustness literature show that adversarially robust models need higher model capacity~\citep{madry2017towards, gowal2020uncovering,cianfarani2022understanding}. As we increase the space of attacks to defend against, we may need to increase the capacity of the model in order to achieve multi-robustness~\citep{dai2024characterizing}. An interesting future direction is looking at the connection between model capacity and CAR and seeing if adding more parameters to the network during fine-tuning (such as using adapters~\citep{rebuffirevisiting}) can be used to address the issue of model capacity.}

\noindent\edit{\textbf{Theory.}}  We believe further work is necessary to extend the theory of CAR. Our results focus on the relationship between robust loss and \edit{logit} distance between attacks for a \emph{single model}. However, we do not extend them to comparisons between loss under different attacks for \emph{different} models, such as the initial robust model and the one at the end of fine-tuning. Additionally, the CAR framework could be extended to the multi-task setting, as is the case in multi-task representation learning \cite{watkinsadversarially,tripuraneni2020theory}. These prior works connect the ability of a class of models to learn a set of tasks to the complexity of that class (measured using Gaussian or Rademacher complexity, for example). Similar methods may also be useful for proving a model's ability to defend against multiple adversaries.

\section{Proofs}
\label{sec:proof}
\noindent\subsection{Proof of Theorem~\ref{thm:robustness}}
The proof of Theorem~\ref{thm:robustness} adapts that of Theorem I from \citet{nern2023transfer} by considering multiple attacks compared to the single one considered there.
\begin{proof}
    Define independent random variables $D_1,\ldots,D_n$ as
    \[D_i = \max_{x_i' \in C_1(x_i)}\ell(h(x_i'),y_i) - \max_{x_i'' \in C_2(x_i)}\ell(h(x_i''),y_i),\]
    based on independently drawn data points with probability distribution $\mathcal{P}(X)$. Using Hoeffding’s inequality, we get
    \begin{align*}
    &\mathbb{P}\left(\left| \sum_{i=1}^n D_i - n\mathbb{E}[D] \right| \geq t\right) \leq 2\cdot\exp\left( \frac{-2t^2}{nM_2^2} \right) \\ 
    \implies &\mathbb{P}\left(\left| \frac{1}{n}\sum_{i=1}^n D_i - \mathbb{E}[D] \right| \leq M_2\sqrt{\frac{\log(\rho/2)}{-2n}}\right) \geq 1 - \rho. \end{align*}
    Thus, with probability at least $1-\rho$ it holds that
    \begin{align}
    \nonumber\mathbb{E}[D] &= \left| \mathcal{L}_1(h) - \mathcal{L}_2(h) \right| \\
    \nonumber&= \left| \mathbb{E}_{(x,y)}\left[\max_{x' \in C_1(x)}\ell(h(x'),y) - \max_{x'' \in C_2(x)}\ell(h(x''),y)\right]\right|\\ 
    &\leq \left| \frac{1}{n}\sum_{i=1}^n\max_{x' \in C_1(x)}\ell(h(x'),y_i) - \max_{x'' \in C_2(x)}\ell(h(x''),y_i)\right| + M_2\sqrt{\frac{\log(\rho/2)}{-2n}}.\label{eq1}\end{align}
    We can further bound the first term on the right hand side, since the loss function $\ell(r,y)$ is $M_1$-Lipschitz in $\|\cdot\|_2$ for $r \in h(X)$:
    \begin{align}
    \nonumber&\left|\frac{1}{n}\sum_{i=1}^{n}\max_{x' \in C_1(x)}\ell(h(x'),y_i) - \max_{x'' \in C_2(x)}\ell(h(x''),y_i)\right|\\
    \nonumber&\leq \left|\frac{1}{n}\sum_{i=1}^{n}|\ell(h(x_i'),y_i) - \ell(h(x_i''),y_i)|\right| \\
    &\leq M_1 \frac{1}{n} \sum_{i=1}^{n}\|h(x_i') - h(x_i'')\|_2, \label{eq2}
    \end{align}
    where $x_1',\ldots,x_n'$ with $x_i' \in C_1(x_i)$ and $x_1'',\ldots,x_n''$ with $x_i'' \in C_2(x_i)$ are chosen to maximize $\ell(h(\cdot),y_i)$ for each $i$. 
    The perturbed samples represented in this inequality might not maximize the distance between the logits, but that distance can be bounded by the maximally distant perturbations within each neighborhood. Making use of the triangle inequality, we obtain:
    \begin{align}
    \nonumber&\sum_{i=1}^n\|h(x_i') - h(x_i'')\|_2\\
    \nonumber&=\sum_{i=1}^n\|(h(x_i') - h(x_i)) - (h(x_i'') - h(x_i))\|_2\\
    %&=\sum_{i=1}^n\|(g(x_i + \delta_{\psi,i}) - g(x_i)) - (g (x) - g(x_i + \delta_{\omega,o}))\|_2\\
    \nonumber&\leq\sum_{i=1}^n\|h(x_i') - h(x_i)\|_2 + \|h(x_i'') - h(x_i)\|_2\\
    &\leq\sum_{i=1}^n\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 + \max_{x'' \in C_2(x_i)}\|h(x'') - h(x_i)\|_2.
    \end{align}
    We then achieve our final result, recalling the assumption that $\mathcal{L}_1(h) \geq \mathcal{L}_2(h)$:
    \begin{align}
        \nonumber\mathcal{L}_1(h) - \mathcal{L}_2(h) &= \left| \mathcal{L}_1(h) - \mathcal{L}_2(h)\right|  \\
        &\leq M_1\frac{1}{n}\sum_{i=1}^n\Bigl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 +  \max_{x'' \in C_2(x_i)}\|h(x'') - h(x_i)\|_2\Bigr) + D,
    \end{align}
    where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$.
\end{proof}

\noindent\subsection{Proof of Corollary~\ref{thm:corollary}}
\begin{proof}
    Define independent random variables $D_1,\ldots,D_n$ as
    \[D_i = \max_{x_i' \in C_1(x_i) \cup C_2(x_i)}\ell(h(x_i'),y_i) - \ell(h(x_i),y_i),\]
    based on independently drawn data points with probability distribution $\mathcal{P}(\mathcal{X})$. Using Hoeffding’s inequality, we get
    \begin{align*}
    &\mathbb{P}\left(\left| \sum_{i=1}^n D_i - n\mathbb{E}[D] \right| \geq t\right) \leq 2\cdot\exp\left( \frac{-2t^2}{nM_2^2} \right) \\ 
    \implies &\mathbb{P}\left(\left| \frac{1}{n}\sum_{i=1}^n D_i - \mathbb{E}[D] \right| \leq M_2\sqrt{\frac{\log(\rho/2)}{-2n}}\right) \geq 1 - \rho. \end{align*}
    Thus, with probability at least $1-\rho$ it holds that
    \begin{align}
    \nonumber\mathbb{E}[D] &= \left| \mathcal{L}_{1,2}(h) - \mathcal{L}(h) \right| \\
    \nonumber&= \left| \mathbb{E}_{(x,y)}\left[\max_{x' \in C_1(x) \cup C_2(x)}\ell(h(x'),y) - \ell(h(x),y)\right]\right|\\ 
    &\leq \left| \frac{1}{n}\sum_{i=1}^n\max_{x' \in C_1(x_i) \cup C_2(x_i)}\ell(h(x'),y_i) - \ell(h(x_i),y_i)\right| + M_2\sqrt{\frac{\log(\rho/2)}{-2n}}.\label{eq1}\end{align}
    We can further bound the first term on the right hand side, since the loss function $\ell(r,y)$ is $M_1$-Lipschitz in $\|\cdot\|_2$ for $r \in h(\mathcal{X})$:
    \begin{align}
    \nonumber&\left|\frac{1}{n}\sum_{i=1}^{n}\max_{x' \in C_1(x_i) \cup C_2(x_i)}\ell(h(x'),y_i) - \ell(h(x_i),y_i)\right|\\
    \nonumber&= \left|\frac{1}{n}\sum_{i=1}^{n}|\ell(h(x_i'),y_i) - \ell(h(x_i),y_i)|\right| \\
    &\leq M_1 \frac{1}{n} \sum_{i=1}^{n}\|h(x_i') - h(x_i)\|_2, \label{eq2}
    \end{align}
    where $x_1',\ldots,x_n'$ with $x_i' \in C_1(x_i) \cup C_2(x_i)$ are chosen to maximize $\ell(h(\cdot),y_i)$ for each $i$. 
    The perturbed samples represented in this inequality might not maximize the distance between the logits, but that distance can be bounded by the maximally distant perturbations within each neighborhood. 
    \begin{align}
    \nonumber&\sum_{i=1}^n\|h(x_i') - h(x_i)\|_2\\
    \nonumber&\leq\sum_{i=1}^n \max_{x'\in C_1(x_i) \cup C_2(x_i)} \|h(x') - h(x_i)\|_2\\
    \nonumber&=\sum_{i=1}^n \max_{C \in \{C_1,C_2\}}\max_{x'\in C(x_i)} \|h(x') - h(x_i)\|_2\\
    \end{align}
    We then achieve our final result :
    \begin{align}
        \nonumber\mathcal{L}_{1,2}(h) - \mathcal{L}(h) &= \left| \mathcal{L}_{1,2}(h) - \mathcal{L}(h)\right|\\
        &\leq M_1\frac{1}{n}\sum_{i=1}^n\Bigl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 +  \max_{x'' \in C_2(x_i)}\|h(x'') - h(x_i)\|_2\Bigr) + D,
    \end{align}
    where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$.
\end{proof}
%\begin{proof}
%    Since $C_1(x) \subset C_1(x) \cup C_2(x)$, therefore $\max_{x' \in C_1(x) \cup C_2(x)} \ell(h(x'),y) \geq \max_{x' \in C_1(x)} \ell(h(x'),y)$ for all $(x,y) \in \mathcal{D}$, so $\mathcal{L}_{1,2}(h) \geq \mathcal{L}_1(h)$. Similarly, since $x \in C_2(x)$, we have that $\mathcal{L}_2(h) \geq \mathcal{L}(h)$. It follows that $\mathcal{L}_{1,2}(h) - \mathcal{L}(h) \leq \mathcal{L}_1(h) - \mathcal{L}_2(h)$, so 
%    \begin{align*}
%        \mathcal{L}_{1,2}(h) - \mathcal{L}(h) \leq M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 + \max_{x' \in C_2(x_i)}\|h(x') - h(x_i)\|_2\biggl) + D.
%    \end{align*}
%\end{proof}

\noindent\subsection{Relating the Loss Gap to Internal Representations}

While our results bound the robust loss gap in terms of the distance between logits of samples perturbed with different attacks, similar results hold for the distance between internal activations. To show how our results can apply to common transfer learning settings (such as that of \citet{nern2023transfer}), we prove the following corollary:
\begin{corollary} \label{corollary: deeper_reps}
    Let $h: \mathbb{R}^d \rightarrow \mathbb{R}^c$ be a $c$ class neural network classification model with a final linear layer (i.e. $h(c) = Wg(x)$, where $g: \mathbb{R}^d \rightarrow \mathbb{R}^r$, and $W \in \mathbb{R}^{c \times r}$). 
    Assume that loss $\ell(\hat{y},y)$ is $M_1$-Lipschitz in $\|\cdot\|_\alpha$ for $\alpha \in \{1,2,\infty\}$, for $\hat{y} \in h(X)$ with $M_1 > 0$ and bounded by $M_2 > 0$, i.e. $0 \leq \ell(\hat{y},y) \leq M_2$ $\forall \hat{y} \in h(X)$. Then, for a subset $\mathbb{X} = \{x_i\}_{i=1}^n$ independently drawn from $\mathcal{D}$, the following holds with probability at least $1-\rho$:
    \begin{align*}
        \mathcal{L}_1(h) - \mathcal{L}_2(h) \leq L_\alpha(W)M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|g(x') - g(x_i)\|_2 + \max_{x' \in C_2(x_i)}\|g(x') - g(x_i)\|_2\biggl) + D,
    \end{align*}

    where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$ and
    \[
    L_\alpha(W) := \begin{cases}
    \|W\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_2, \\
    \sum_i\|W_i\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_1, \\
    \max_i\|W_i\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_\infty. 
    \end{cases}
    \]
\end{corollary}

\begin{proof}
    From (\ref{eq1}), (\ref{eq2}), and the definition of $h$, we have that 
    \begin{align*}
        \left|\mathcal{L}_1 - \mathcal{L}_2\right| \leq M_1 \frac{1}{n}\sum_{i=1}^n \|Wg(x_i') - h(x_i'')\|_2 + M_2\sqrt{\frac{\log{\rho/2}}{-2n}}.
    \end{align*}
    We then apply Lemma 2 from \citet{nern2023transfer} and the definition of $L_\alpha$:
    \begin{align*}
    M_1 \frac{1}{n} &\sum_{i=1}^{n}\|Wg(x_i') - Wg(x_i'')\|_\alpha \\
    &\leq L_\alpha(W)M_1 \frac{1}{n} \sum_{i=1}^{n}\|g(x_i') - g(x_i'')\|_2.
    \end{align*}
    As in the proof for Theorem~\ref{thm:robustness}, the perturbed samples represented in this inequality might not maximize the distance between the representations, but that distance can be bounded by the maximally distant perturbations within each neighborhood. Making use of the triangle inequality, we obtain:
    \begin{align*}
    &\sum_{i=1}^n\|g(x_i') - g(x_i'')\|_2\\
    &=\sum_{i=1}^n\|(g(x_i') - g(x_i)) - (g(x_i'') - g(x_i))\|_2\\
    %&=\sum_{i=1}^n\|(g(x_i + \delta_{\psi,i}) - g(x_i)) - (g (x) - g(x_i + \delta_{\omega,o}))\|_2\\
    &\leq\sum_{i=1}^n\|g(x_i') - g(x_i)\|_2 + \|g(x_i'') - g(x_i)\|_2\\
    &\leq\sum_{i=1}^n\max_{x' \in C_1(x_i)}\|g(x') - g(x_i)\|_2 + \max_{x'' \in C_2(x_i)}\|g(x'') - g(x_i)\|_2.
    \end{align*}
    We then achieve our final result:
    \begin{align*}
        \mathcal{L}_1(h) - \mathcal{L}_2(h) &= \left| \mathcal{L}_1(h) - \mathcal{L}_2(h)\right|\\
        &\leq L_\alpha(W)M_1 \frac{1}{n}\sum_{i=1}^n\Bigl(\max_{x' \in C_1(x_i)}\|g(x') - g(x_i)\|_2 +  \max_{x'' \in C_2(x_i)}\|g(x'') - g(x_i)\|_2\Bigr) + D,
    \end{align*}
    where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$.
\end{proof}

\section{Connection Between Adversarial $\ell_2$ Regularization and Variation Regularization} \label{app:alr_vr}
In this section, we will show the relationship between adversarial $\ell_2$ regularization (ALR) and variation regularization (VR) \citep{dai2022formulating}.  To begin, we first revisit the definitions of ALR and VR:
$$R_{\text{ALR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m \max_{x' \in C(x_i)} \|\edit{h}(x') - \edit{h}(x_i)\|_2$$
$$R_{\text{VR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m  \max_{x', x'' \in C(x_i)} \|\edit{h}(x') - \edit{h}(x'')\|_2$$
Since VR optimizes over 2 perturbations $x'$ and $x''$ for each example while ALR optimizes only for $x'$, it is clear that $R_{\text{ALR}} \le R_{\text{VR}}$.  Additionally, we note that:
$$R_{\text{VR}}(h, K(t)) = \frac{1}{m} \sum_{i=1}^m \max_{x', x'' \in C(x_i)} \|\edit{h}(x') -h(x) + h(x) - \edit{h}(x'')\|_2$$
$$\le  \frac{1}{m} \sum_{i=1}^m \max_{x', x'' \in C(x_i)} \|h(x') -h(x)\|_2 + \|h(x) - h(x'')\|_2$$
$$= \frac{2}{m}\sum_{i=1}^m \max_{x'\in C(x_i)}\|h(x') -h(x)\|_2$$
$$= 2 R_\text{ALR}$$
Thus, ALR and VR are related in the sense that $R_{\text{ALR}} \le R_{\text{VR}} \le 2R_{\text{ALR}}$.

\section{Experimental Verification of Theoretical Results} 
\label{appsec:experimental_verification_of_theory}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/union_regularization_logit_distance.png}
    \vspace{-15pt}
    \caption{Adversarial loss gap ($\mathcal{L}_{1,2}(h) - \mathcal{L}(h)$) and average $\ell_2$ distance between \edit{logits} of $\ell_2$ ($\epsilon = 0.5$, representing $P_{C_1}$) and StAdv ($\epsilon = 0.05$, representing $P_{C_2}$) attacked samples over 25 epochs of fine-tuning using \cite{croce2022adversarial}'s fine-tuning method, both with and without regularization. Each model is fine-tuned starting from a model that is adversarially trained against an $\ell_2$ adversary, as described in Section~\ref{sec:exp_setup}. In all training scenarios, there is a visible correlation between the loss gap and the \edit{logit} distance, aligning with the theoretical result in Corollary~\ref{thm:corollary}. }
    \label{fig:loss_gap}
    \vspace{-10pt}
\end{figure*}

We now briefly demonstrate that our chosen regularization terms align with our theoretical results. In Figure~\ref{fig:loss_gap}, we start with WRN-28-10 models that were adversarially trained to be robust against $\ell_2$-bounded attacks, and fine-tune them to increase their robustness against StAdv attacks using either no regularization, uniform regularization, or adversarial $\ell_2$ regularization. We observe a number of trends:

\noindent
\textbf{Sensitivity correlates with loss gap.} Whether or not regularization is used, there is a clear correlation between total adversarial sensitivity across both attacks (i.e. $\max_{x'\in C_1(x)}\|\edit{h}(x')-\edit{h}(x)\| + \max_{x'\in C_2(x)}\|\edit{h}(x')-\edit{h}(x)\|$) and the loss gap between the union robust loss and the benign loss (i.e. $\mathcal{L}_{1,2}(h) - \mathcal{L}(h)$). 

\noindent
\textbf{Regularization reduces sensitivity and loss gap.} Both metrics are significantly lower throughout fine-tuning when regularization is used, indicating that regularization is successfully targeting our theoretical bounds.

\noindent
\textbf{Loss gap increases over time.} Across all three models there is an increase in both loss gap and adversarial sensitivity over the course of fine-tuning. While this may seem like a failure of regularization, the benefit is more apparent when further analyzing what is causing the loss gap to increase. In the regularized fine-tuning runs, both benign and robust losses are decreasing, with benign loss decreasing more quickly. This is likely influenced by an initial increase in benign loss at the very beginning of fine-tuning which is not captured in Figure~\ref{fig:loss_gap}. However, without regularization, benign loss decreases while union robust loss increases. This shows us that despite theoretically targeting the gap between union robust loss and benign loss, the use of regularization still aids in individually reducing both losses in absolute terms.


\section{Additional Experimental Setup Details}
\label{app:exp_setup}
\textbf{Additional training details. }For initial training, we start with a learning rate of 0.1 and then use the multistep learning rate scheduling proposed by \citet{gowal2020uncovering}; specifically, we scale the learning rate down by a factor of 10 halfway and 3/4 of the way through initial training or fine-tuning.  For fine-tuning, we maintain a learning rate of 0.001.  We train with SGD with momentum of 0.9 and weight decay of 0.0005.

\textbf{Additional Attack parameters in training. } Following other works on adversarial robustness, we use a step size of 0.075 for $\ell_2$ attacks on CIFAR-10, 0.15 for $\ell_2$ attacks on ImageNette, and $\frac{2}{255}$ for $\ell_\infty$ attacks.  For other attacks, we use $\frac{\epsilon}{8}$ where $\epsilon$ is the attack strength as the step size during training.

\textbf{Model selection. }In the main paper, we stated that we perform evaluation using the epoch at which the model has the best performance measured across known attack types.  Specifically, after each epoch of training, we evaluate the performance of each model against the attacks used during training (with the same attack parameters as used during training).  For training with AVG, we use the best performing model with respect to the AVG objective (which is the model with the best performance measured as an average over individual attack accuracies).  Meanwhile for MAX and FT MAX, we use the best performing model with respect to the MAX objective (which is the best performing model across the union of all attacks).  For procedures that only use a single attack per batch during training (Random, FT Single, FT Croce, and our procedure), we use the best performing model measured by sampling attacks per batch randomly.

\textbf{Regularization setup. }\edit{We note that all attacks used in this paper use a gradient based optimization scheme for finding the attack.  In order to compute regularization for non-$\ell_p$ threat models, we follow the same optimization scheme used by the attack \citep{XiaoZ0HLS18, LaidlawF19, kaufmann2019testing} but replace the classification loss portion of the optimization objective to be the $\ell_2$ distance between features/logits between the perturbed and unperturbed input. For fine-tuning with regularization, }since \citet{croce2022adversarial}'s fine-tuning approach only uses a single attack per batch, we structure the regularization to mimic \citet{croce2022adversarial}'s fine-tuning procedure.  Specifically, for each batch, the regularization is for a single attack type (the same one which is selected to use with adversarial training by \citet{croce2022adversarial}'s fine-tuning approach).  This helps to reduce the overhead from regularization.


\section{Additional Experimental Results for CAR}
\label{app:exp_seq}
We present additional results for CAR on CIFAR-10 in Tables \ref{tab:main_results_cifar_epochs} and  \ref{app:main_results_cifar_epochs_2}, results for CAR on Imagenette in Table \ref{app:main_results_imagenette_epochs_1} and \ref{app:main_results_imagenette_epochs_2} and results for CIFAR-100 in Tables \ref{app:main_results_cifar100_full} and \ref{app:main_results_cifar100_full_2}.  We also compare different fine-tuning approaches in the absense of regularization.

%\subsection{Continual Robust Training for \probname}
%\label{sec:crt_vs_from_scratch}

\noindent\textbf{Training time and robust performance.} We find that fine-tuning with MAX objective (FT MAX) or \citet{croce2022adversarial} (FT Croce) can generally achieve robustness across previous attacks and the new attack in the sequence comparable to training from scratch.  For example, in Table \ref{app:main_results_cifar_epochs_2}, when fine-tuning to gain robustness against StAdv attack starting from a model initially trained with adversarial training on $\ell_\infty$ attacks on CIFAR-10, we find that FT MAX achieves 50.75\% average robustness across the two attacks and 41.57\% union robustness across the two attacks, and FT Croce achieves 49.48\% average robustness and 29.69\% union robustness.  These values lie within (or even above) the range obtained through training from scratch (42.23\%-49.61\% average 
robustness and 28.03\%-40.8\% union robustness).  We find that this trend generally holds as well across time steps when new attacks are introduced, when using a different sequence ordering (Table \ref{tab:main_results_cifar_epochs}).

Of these two techniques, we find that FT MAX generally achieves higher average and union accuracies across the set of known attacks, but is less efficient when used in fine-tuning.  For example, In Table \ref{app:main_results_cifar_epochs_2}, FT MAX takes 3.99 hours for 10 epochs of fine-tuning from an $\ell_{\infty}$ robust model while FT Croce takes 2.31 hours.  The time complexity of FT MAX also scales as the number of attacks increases, leading to 7.9 hours of fine-tuning for 10 epochs when there are 4 known attacks while FT Croce maintains approximately the same training time.

In comparison to naively training from scratch, we also find that these fine-tuning techniques can be much more efficient.  For example, a model robust to a sequence of 4 attacks in Table~\ref{tab:main_results_cifar_epochs} can be found in roughly 17 hours using CRT, but training from scratch each time would require 44 hours cumulatively.

\noindent\textbf{Importance of replay. }We find that replay of previous attacks is important for achieving good robustness across the set of known attacks when training with CRT. Fine-tuning with only the new attack (FT Single) usually leads to rapid forgetting of the previous attack.  For example, in Table \ref{app:main_results_cifar_epochs_2} we observe that the accuracy of robustness on the initial attack ($\ell_\infty$) drops to 31.14\% robust accuracy at time step 1 (from the initial accuracy of 51.49\% at time step 0) and then further drops to 25.27\% at time step 2 when the third attack (Recolor) is introduced.  This forgetting is independent from tradeoffs between attacks as we find that training from scratch and FT MAX and FT Croce techniques can all achieve at least 40\% $\ell_{\infty}$ accuracy at time step 1 and at least 35\% $\ell_{\infty}$ accuracy at time step 2.  The forgetting of previous attacks is also analogous to catastrophic forgetting of previous tasks in continual learning \citep{wang2023continual, MCCLOSKEY1989109}.  We note however that forgetting is less of a limitation in CAR than in continual learning since the defender's knowledge set only grows over time; they do not forget the formulation of previous attacks and can thus can always use methods such as replay.


\textbf{ALR applied on logits vs features. }In Table \ref{tab:main_results_cifar_epochs}, we also provide results for using regularization based on distances in the feature space (before the final linear layer), which are labelled with ``+ ALR feature".  Overall we observe that using regularization in the feature space can also help improve performance on average and union robustness across known attacks as well as improve unforeseen robustness over baselines.  However, we observe that feature space regularization leads to larger tradeoffs in clean accuracy than regularization on the logits (``+ ALR" rows) while robust performance is comparable to regularization applied on the logits.

\textbf{Training durations. } Across all tables we also provide experiments for fine-tuning with 25 epochs (as opposed to 10 epochs reported in the main body).  We find that increasing the number of fine-tuning epochs can help methods such as FT Croce achieve robustness closer to that of training from scratch, but at the cost of increased time for updating the model.

\textbf{Performance on other datasets. }We find that the gain in performance through using ALR varies across datasets.  For Imagenette the gain in performance is generally much smaller than on CIFAR-10 (ALR closes the gap between fine-tuning based updates and training from scratch rather than surpassing training from scratch as in CIFAR-10.  On CIFAR-100 ALR generally does not improve performance over fine-tuning.  We believe that this is because achieving robustness on multiple attacks is quite hard on CIFAR-10; clean accuracy is between 60-70\% and robust accuracies are even lower with StAdv and $\ell_{\infty}$ robustness only achieving up to 32\% robust accuracy and 25\% robust accuracy respectively.


\begin{table*}[ht]
\centering
\scalebox{0.68}{
\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Time\\ Step \end{tabular}} &Procedure & Threat Models & \multicolumn{1}{c|}{Clean} & \multicolumn{1}{c}{$\ell_2$} & \multicolumn{1}{c}{StAdv} & \multicolumn{1}{c}{$\ell_\infty$} & \multicolumn{1}{c|}{Recolor} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Time\\ (hrs)\end{tabular}} \\ \hline
\multirow{ 3}{*}{0} & AT & $\ell_2$ & \textbf{91.17} & \cellcolor[HTML]{B7E1CD}69.7 & 2.08 & 28.41 & 44.94 & 69.7 & 69.7 & 36.28 & 1.24 & 8.35 \\ %16.69
& AT + ALR ($\lambda=1$) & $\ell_2$ & 89.43 & \cellcolor[HTML]{B7E1CD}\textbf{69.84} & \textbf{48.23} & \textbf{34.00} & \textbf{65.46} & \textbf{69.84} & \textbf{69.84} & \textbf{54.38} & \textbf{31.27} & 11.15\\ %22.29
%& AT + ALR feature ($\lambda = 1$) & $\ell_2$ & 84.53 & \cellcolor[HTML]{B7E1CD}63.22 & 7.78 & 25.74 & 51.76 & 63.22 & 63.22 & 37.12 & 6.07\\
%& AT + ALR feature ($\lambda=2$) & $\ell_2$ & 84.53 & \cellcolor[HTML]{B7E1CD}64.22 & 14.64 & 30.55 &  57.16 & 64.22 & 64.22 & 41.64 & 11.4\\
& AT + ALR feature ($\lambda=5$) & $\ell_2$ & 83.7 & \cellcolor[HTML]{B7E1CD}63.1 & 26.57 & 31.6 & 62.53  & 63.1 & 63.1 & 45.95 & 20.16 & 11.13\\

\hline
\multirow{ 17}{*}{1} & AVG & $\ell_2$, StAdv & 87.74 & \cellcolor[HTML]{B7E1CD} 62.17 & \cellcolor[HTML]{B7E1CD}50.92 & 17.17 & 45.47 & 56.55 & 47.55 & 43.93 & 15.92 &23.72 \\ %20.26
& MAX & $\ell_2$, StAdv & 86.18 & \cellcolor[HTML]{B7E1CD}58.65 & \cellcolor[HTML]{B7E1CD}57.21 & 11.21 & 43.07 & 57.93 & 51.72 & 42.54 & 11.03 & 23.69 \\ %20.23
& Random & $\ell_2$, StAdv & 84.91 & \cellcolor[HTML]{B7E1CD}57.77 & \cellcolor[HTML]{B7E1CD}59.74 & 14.05 & 44.88 & 58.76 & 52.15 & 44.11 & 13.68 & 10.92 \\ %9.36
\cdashline{2-13}
 & FT MAX (10 ep) &  $\ell_2$, StAdv & 83.73 & \cellcolor[HTML]{B7E1CD}57.07 & \cellcolor[HTML]{B7E1CD}58.67 & 12.51 & 49.03 & 57.87 & 51.32 & 44.32 & 12.36 & 4 \\
 & FT MAX (25 ep) & $\ell_2$, StAdv & 84.85 & \cellcolor[HTML]{B7E1CD}56.44 & \cellcolor[HTML]{B7E1CD} 61.34 & 10.35 & 48.08 & 58.89 & 52.52 & 44.05 & 10.24 & 10 \\ %
 & FT Croce (10 ep) & $\ell_2$, StAdv & 84.7 & \cellcolor[HTML]{B7E1CD}57.88 & \cellcolor[HTML]{B7E1CD}54.27 & 14.38 & 51.08 & 56.07 & 48.13 & 44.4 & 13.8 & 2.4\\
  & FT Croce (25 ep) & $\ell_2$, StAdv & 86.24 & \cellcolor[HTML]{B7E1CD}58.94 & \cellcolor[HTML]{B7E1CD}57.37 & 13.26 & 50.36 & 58.16 & 50.89 & 44.98 & 13 & 5.98 \\ %
   & FT Single (10 ep) & $\ell_2$, StAdv &  80.89
& \cellcolor[HTML]{B7E1CD}45.45& \cellcolor[HTML]{B7E1CD}54.5
& 6.09 & 41.98 & 49.98 & 41.05 & 37 & 5.87 & 2.78 \\
 & FT Single (25 ep)  & $\ell_2$, StAdv & 81.21 & \cellcolor[HTML]{B7E1CD}44.17 & \cellcolor[HTML]{B7E1CD}54.6 & 5.56 & 40.95 & 49.38 & 39.76 & 36.32 & 5.36 & 6.92 \\ %
    & FT Single + ALR (10 ep) & $\ell_2$, StAdv & 87.24 & \cellcolor[HTML]{B7E1CD}62.22& \cellcolor[HTML]{B7E1CD}61.5
& 21.4 & \textbf{\underline{70.87}} & 61.86 & 55.04 & 54 & 21.14 & 4.24 \\
 & FT Single + ALR (25 ep) & $\ell_2$, StAdv & 87.54 & \cellcolor[HTML]{B7E1CD}61.21 & \cellcolor[HTML]{B7E1CD}60.38 & 20.81 & 69.49 & 60.8 & 54.22 & 52.97 & 20.48 & 8.77 \\ %
& FT Single + ALR feature ($\lambda=2$, 10 ep) &  $\ell_2$, StAdv & 81.79 & \cellcolor[HTML]{B7E1CD}56.98 & \cellcolor[HTML]{B7E1CD}60.28 & 20.59 & 63.64 & 58.63 & 51.65 & 50.37 & 20.21 & 3.52 \\
  & FT Single + ALR feature ($\lambda=5$, 10 ep) &  $\ell_2$, StAdv & 81.26 & \cellcolor[HTML]{B7E1CD}60.43 & \cellcolor[HTML]{B7E1CD}57.61 & \textbf{\underline{28.17}} & 67.95 & 59.02 & 51.99 & 53.54 & \textbf{\underline{27.24}} & 3.53 \\
   & FT Croce + ALR (10 ep) & $\ell_2$, StAdv & 86.03 & \cellcolor[HTML]{B7E1CD}59.18 &\cellcolor[HTML]{B7E1CD}\textbf{\underline{65.14}} & 15.36  & 63.31 & \textbf{\underline{62.16}} & \textbf{\underline{55.83}} & 50.75 & 15.29 & 3.47 \\
 & FT Croce + ALR (25 ep) & $\ell_2$, StAdv & \underline{\textbf{88.5}} & \cellcolor[HTML]{B7E1CD}\textbf{\underline{64.88}} & \cellcolor[HTML]{B7E1CD}58.98 & 23.9 & 70.79 & 61.93 & 55.03 & \underline{\textbf{54.64}} & 23.33 & 7.96 \\ %
 & FT Croce + ALR feature ($\lambda=2$, 10 ep) & $\ell_2$, StAdv &  83.19 & \cellcolor[HTML]{B7E1CD}61.28 &\cellcolor[HTML]{B7E1CD} 59.04 & 23.98 & 62.69 & 60.16 & 53.25 & 51.75 & 23.2 & 2.97
\\
 & FT Croce + ALR feature ($\lambda=5$, 10 ep) & $\ell_2$, StAdv & 83.51 & \cellcolor[HTML]{B7E1CD}61.69 &\cellcolor[HTML]{B7E1CD} 61.76 & 23.33 & 62.48 & 61.73 & 55.25 & 52.31 & 22.77 & 3.13\\

\hline

\multirow{ 17}{*}{2}&AVG & $\ell_2$, StAdv, $\ell_\infty$ & 85.98 & \cellcolor[HTML]{B7E1CD}67.60 & \cellcolor[HTML]{B7E1CD}45.81 & \cellcolor[HTML]{B7E1CD}42.39 & 62.43 & 51.93 & 34.05 & 54.56 & 33.39 & 33.12 \\
&MAX & $\ell_2$, StAdv, $\ell_\infty$ & 84.54 & \cellcolor[HTML]{B7E1CD}54.87 & \cellcolor[HTML]{B7E1CD}52.33 & \cellcolor[HTML]{B7E1CD}38.23 & 55.90 & 48.48 & 35.25 & 50.33 & 34.08 & 79.04 \\
&Random & $\ell_2$, StAdv, $\ell_\infty$ & 39.52 & \cellcolor[HTML]{B7E1CD}67.46 & \cellcolor[HTML]{B7E1CD}47.35 & \cellcolor[HTML]{B7E1CD}42.12 & 63.61 & 52.31 & 35.46 & 55.13 & 34.79 & 10.92 \\ \cdashline{2-13}
 & FT MAX (10 ep) & $\ell_2$, StAdv, $\ell_\infty$ & 83.16 &  \cellcolor[HTML]{B7E1CD}65.63 &  \cellcolor[HTML]{B7E1CD}56.68 &  \cellcolor[HTML]{B7E1CD}36.9 & 65.69 & 53.07 & 35.18 & 56.23 & 34.83 & 5.62 \\
 & FT MAX (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 83.99 & \cellcolor[HTML]{B7E1CD}65.69 & \cellcolor[HTML]{B7E1CD} 58.16 & \cellcolor[HTML]{B7E1CD}37.21 & 65.52 & 53.69 & 35.76 & 56.65 & 35.31 & 12.88 \\
 & FT Croce (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 85.05 &  \cellcolor[HTML]{B7E1CD}67.3 &  \cellcolor[HTML]{B7E1CD}48.07 &  \cellcolor[HTML]{B7E1CD}33.38 & 62.52 & 49.58 & 28.96 & 52.82 & 28.63 & 2.27 \\
 & FT Croce (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 86.14 & \cellcolor[HTML]{B7E1CD}67.3 & \cellcolor[HTML]{B7E1CD}52.47 & \cellcolor[HTML]{B7E1CD}35.86 & 63.43 & 51.88 & 32.54 & 54.77 & 32.08 & 5.01 \\
  & FT Single (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 87.99 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{70.53}} & \cellcolor[HTML]{B7E1CD}11.17
 & \cellcolor[HTML]{B7E1CD}41.63 & 63.46 &41.11 & 7.95 & 46.7 & 7.74  & 1.57 \\
 & FT Single (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 88.67 & \cellcolor[HTML]{B7E1CD} 70.23 & \cellcolor[HTML]{B7E1CD}8.79 & \cellcolor[HTML]{B7E1CD} 43.4 & 63.03 & 40.81 & 6.19 & 46.36 & 6.05 & 3.91 \\
 & FT Single + ALR (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & \textbf{\underline{88.74}} & \cellcolor[HTML]{B7E1CD}69.15 & \cellcolor[HTML]{B7E1CD}47.33 & \cellcolor[HTML]{B7E1CD}42.08 & 68.62 & 52.85 & 36.66 & 56.8 & 36.62 & 2.26 \\
 & FT Single + ALR (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 88.14 & \cellcolor[HTML]{B7E1CD}68.26 & \cellcolor[HTML]{B7E1CD}49.1 & \cellcolor[HTML]{B7E1CD}41.48 & 66.73 & 52.95 & \textbf{\underline{37.55}} & 56.39 & 37.5 & 5.4 \\
 & FT Single + ALR feature ($\lambda=2$, 10 ep) &  $\ell_2$, StAdv, $\ell_{\infty}$ & 85.69 & \cellcolor[HTML]{B7E1CD}67.62 & \cellcolor[HTML]{B7E1CD}29.42 & \cellcolor[HTML]{B7E1CD}43.68 & 68.75 & 46.91 & 24.44 & 52.37 & 24.38 & 2.16 \\
 & FT Single + ALR feature ($\lambda=5$, 10 ep) &  $\ell_2$, StAdv, $\ell_{\infty}$ & 84.03 & \cellcolor[HTML]{B7E1CD}67.64 & \cellcolor[HTML]{B7E1CD}42.03 & \cellcolor[HTML]{B7E1CD} \textbf{\underline{44.36}} & 71.36 & 51.34 & 32.54 & 56.35 & 32.48 & 2.29 \\
 & FT Croce + ALR (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 86.57 & \cellcolor[HTML]{B7E1CD}67.99 & \cellcolor[HTML]{B7E1CD} \textbf{\underline{61.55}} & \cellcolor[HTML]{B7E1CD}36.59 & 72.16 & \textbf{\underline{55.38}} & 35.68 & 59.57 & 35.52 & 2.87 \\
 & FT Croce + ALR (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$ & 86.96 & \cellcolor[HTML]{B7E1CD}68.91 & \cellcolor[HTML]{B7E1CD}57.21 & \cellcolor[HTML]{B7E1CD}39.65 & \textbf{\underline{72.22}} & 55.26 & 37.25 & \textbf{\underline{59.5}} & 37.14 & 6.87 \\
  & FT Croce + ALR feature ($\lambda=2$, 10 ep) &  $\ell_2$, StAdv, $\ell_{\infty}$ & 83.13 & \cellcolor[HTML]{B7E1CD}66.91 & \cellcolor[HTML]{B7E1CD}56.76 & \cellcolor[HTML]{B7E1CD}38.66 & 68.57 & 54.11 & 35.95 & 57.73 & 35.76 & 2.82 \\
  & FT Croce + ALR feature ($\lambda=5$, 10 ep) &  $\ell_2$, StAdv, $\ell_{\infty}$ & 84.25 & \cellcolor[HTML]{B7E1CD}68.14 & \cellcolor[HTML]{B7E1CD}57.7 & \cellcolor[HTML]{B7E1CD}39.8 & 70.29 & 55.21& 37.4 & 58.98 & \underline{\textbf{37.21}} & 2.79
  \\
  \hline

\multirow{ 17}{*}{3} & AVG & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 87.77 & \cellcolor[HTML]{B7E1CD} 68.55 & \cellcolor[HTML]{B7E1CD}39.55 & \cellcolor[HTML]{B7E1CD}\textbf{41.97} & \cellcolor[HTML]{B7E1CD}67.93 & 54.5 & 30.39 & 54.5 & 30.39 & 50.54 \\
& MAX & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 84.3 & \cellcolor[HTML]{B7E1CD}57.62 & \cellcolor[HTML]{B7E1CD}52.3 & \cellcolor[HTML]{B7E1CD}41.69 & \cellcolor[HTML]{B7E1CD}65.1 & 54.18 & \textbf{37.44} & 54.18 & \textbf{37.44} & 55.54 \\
& Random & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 86.32 & \cellcolor[HTML]{B7E1CD}65.87 & \cellcolor[HTML]{B7E1CD}47.82 & \cellcolor[HTML]{B7E1CD}35.04 & \cellcolor[HTML]{B7E1CD}68.35 & 54.27 & 30.76 & 54.27 & 30.76 & 12.41 \\ \cdashline{2-13}
& FT MAX (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 83.64 & \cellcolor[HTML]{B7E1CD}66.21 & \cellcolor[HTML]{B7E1CD}57.53 & \cellcolor[HTML]{B7E1CD}37.77 & \cellcolor[HTML]{B7E1CD}69.32 & 57.71 & 36.02 & 57.71 & 36.02 & 8.45\\
& FT MAX (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 83.9 & \cellcolor[HTML]{B7E1CD}65.72 & \cellcolor[HTML]{B7E1CD}57.84 & \cellcolor[HTML]{B7E1CD}38.37 & \cellcolor[HTML]{B7E1CD}68.84 & 57.69 & 36.87 & 57.69 & 36.87 & 21.44 \\
& FT Croce (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 86.64 & \cellcolor[HTML]{B7E1CD} \textbf{\underline{68.76}} & \cellcolor[HTML]{B7E1CD}44.81 & \cellcolor[HTML]{B7E1CD}36.02 & \cellcolor[HTML]{B7E1CD}68.05 & 54.41 & 29.44 & 54.41 & 29.44 & 2.34 \\
 & FT Croce (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 87.11 & \cellcolor[HTML]{B7E1CD}67.89 & \cellcolor[HTML]{B7E1CD}49.57 & \cellcolor[HTML]{B7E1CD}35.58 & \cellcolor[HTML]{B7E1CD}67.05 & 55.02 & 31.21 & 55.02 & 31.21 & 5.9 \\
   & FT Single (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor &  90.41& \cellcolor[HTML]{B7E1CD}66.47 & \cellcolor[HTML]{B7E1CD}3.93 & \cellcolor[HTML]{B7E1CD}29.6& \cellcolor[HTML]{B7E1CD}69.03 & 42.26 & 2.49 & 42.26 & 2.49 & 3.11 \\
 & FT Single (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & \textbf{\underline{90.89}} & \cellcolor[HTML]{B7E1CD}65.14 & \cellcolor[HTML]{B7E1CD}3.02 & \cellcolor[HTML]{B7E1CD}30.32 & \cellcolor[HTML]{B7E1CD}68.54 & 41.75 & 1.92 & 41.75 & 1.92 & 7.41 \\
 & FT Single + ALR (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 90.45 & \cellcolor[HTML]{B7E1CD}61.58 & \cellcolor[HTML]{B7E1CD}25.77 & \cellcolor[HTML]{B7E1CD}27.43 & \cellcolor[HTML]{B7E1CD}69.26 & 46.01 & 19.2 & 46.01 & 19.2 & 4.24 \\
 & FT Single + ALR (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 90.4 & \cellcolor[HTML]{B7E1CD}57.07 & \cellcolor[HTML]{B7E1CD}24.91 & \cellcolor[HTML]{B7E1CD}22.91 & \cellcolor[HTML]{B7E1CD}67.39 & 43.07 & 17.21 & 43.07 & 17.21 & 9.79 \\
 & FT Single + ALR feature ($\lambda=2$, 10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 90.15 & \cellcolor[HTML]{B7E1CD}57.89 & \cellcolor[HTML]{B7E1CD}8.75 & \cellcolor[HTML]{B7E1CD}22.86 & \cellcolor[HTML]{B7E1CD}72.27 & 40.44 & 6.61 & 40.44 & 6.61 & 3.94\\
 & FT Single + ALR feature ($\lambda=5$, 10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 88.44 & \cellcolor[HTML]{B7E1CD}66.03 & \cellcolor[HTML]{B7E1CD}18.88 & \cellcolor[HTML]{B7E1CD}34.17 & \cellcolor[HTML]{B7E1CD}69.35 & 47.11 & 16.1 & 47.11 & 16.1 & 3.76 \\
  & FT Croce + ALR (10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor &87.62 & \cellcolor[HTML]{B7E1CD}68.14 & \cellcolor[HTML]{B7E1CD}58.5 & \cellcolor[HTML]{B7E1CD}36.39 & \cellcolor[HTML]{B7E1CD}72.35 & 58.85 & 34.92 & 58.85 & 34.92 & 3.35 \\
 & FT Croce + ALR (25 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 87.05 & \cellcolor[HTML]{B7E1CD} 68.05 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{59.26}} & \cellcolor[HTML]{B7E1CD} 38.38 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{73.42}} & \textbf{\underline{59.78}} & 36.83 & \underline{\textbf{59.78}} & 36.83 & 7.78\\
 & FT Croce + ALR feature ($\lambda=2$, 10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor & 84.78 & \cellcolor[HTML]{B7E1CD}67.67 & \cellcolor[HTML]{B7E1CD}53.13 & \cellcolor[HTML]{B7E1CD} \underline{40.25} & \cellcolor[HTML]{B7E1CD}69.99 & 57.76 & 36.3 & 57.76 & 36.3 & 3.04\\
  & FT Croce + ALR feature ($\lambda=5$, 10 ep) & $\ell_2$, StAdv, $\ell_{\infty}$, Recolor &83.94 & \cellcolor[HTML]{B7E1CD}67.28 & \cellcolor[HTML]{B7E1CD}59.21 & \cellcolor[HTML]{B7E1CD}39.38 & \cellcolor[HTML]{B7E1CD}71.67 & 59.38 & \underline{37.15} & 59.38 & \underline{37.15} & 2.91\\
 \hline
\end{tabular}}

\caption{\textbf{Continual Robust Training on CIFAR-10 (Sequence of 4 attacks starting with $\ell_2$).} The learner initially has knowledge of $\ell_2$ attacks and over time, we are sequentially introduced to StAdv, $\ell_\infty$, and ReColor attacks. We report clean accuracy, accuracy on different attack types, and average and union accuracies.  The threat models column represents the set of attacks known to the defender and accuracies on known attacks are highlighted with in green cells.  ``FT" procedures are fine-tuning approaches starting from adversarially trained to $\ell_2$ model (AT) and then sequentially fine-tuning with new attacks for 25 epochs. AVG, MAX, and Random strategies train models from scratch with all attacks for 100 epochs. The ``Avg (known)" and ``Union (known)" columns represent average and union accuracies on attacks known to the defender while ``Avg (all)" and ``Union (all)" columns represent average and union accuracies on all four attacks. Additionally, we report training times for the procedure (non-cumulative) in the ``Time" column.  Best performance out of both training from scratch and fine-tuning are bolded, while best performance when only comparing fine-tuning approaches is underlined.}
\label{tab:main_results_cifar_epochs}
\end{table*}

\begin{table*}[ht]
\centering
\scalebox{0.75}{
\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
 \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Time\\ Step \end{tabular}} &Procedure & Threat Models & Clean & $\ell_\infty$ & StAdv & Recolor & $\ell_2$ & \begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular} & Time \\ \hline
\multirow{ 2}{*}{0} & AT & $\ell_\infty$ & \textbf{85.93} & \cellcolor[HTML]{B7E1CD}51.44 & 14.87 & 62.48 & \textbf{59.48} & 51.44 & 51.44 & 47.07 & 11.9 & 7.52 \\
& AT + ALR & $\ell_\infty$ & 83.18 & \cellcolor[HTML]{B7E1CD}\textbf{51.49} & \textbf{34.78} & \textbf{58.15} & 58.15 & \textbf{51.49} & \textbf{51.49} & \textbf{53.27} & \textbf{29.87} & 11.12 \\ \hline

\multirow{ 13}{*}{1}& AVG & $\ell_\infty$, StAdv & \textbf{86.44} & \cellcolor[HTML]{B7E1CD}30.05 & \cellcolor[HTML]{B7E1CD}54.4 & 46.71 & 52.1 & 42.23 & 28.03 & 45.81 & 26.75 & 23.68 \\
& MAX & $\ell_\infty$, StAdv & 82.62 & \cellcolor[HTML]{B7E1CD}44.96 & \cellcolor[HTML]{B7E1CD}53.68 & 64.24 & \textbf{60.85} & 49.32 & 40.8 & 55.93 & 39.81 & 23.68 \\
& Random & $\ell_\infty$, StAdv & 83.15 & \cellcolor[HTML]{B7E1CD}40.86 & \cellcolor[HTML]{B7E1CD}58.37 & 60.53 & 58.17 & 49.61 & 38.95 & 54.48 & 37.64 & 11.70 \\ \cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{\infty}$, StAdv & 81.63 &\cellcolor[HTML]{B7E1CD}44.13 & \cellcolor[HTML]{B7E1CD}57.38 & 66.66 & 60.27 & 50.75 & 41.57 & 57.11 & 40.96 & 3.99 \\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv & 81.99 & \cellcolor[HTML]{B7E1CD}44.32 & \cellcolor[HTML]{B7E1CD}57.8 & 66.25 & 60.29 & 51.06 & 41.98 & 57.16 & 41.25 & 9.93 \\
 & FT Croce (10 ep) & $\ell_{\infty}$, StAdv & 82.66 & \cellcolor[HTML]{B7E1CD}44.75 & \cellcolor[HTML]{B7E1CD}54.2 & 65.99 & 60.27 & 49.48 & 39.69 & 56.3 & 39.01 & 2.31 \\

 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv & \underline{83.55} & \cellcolor[HTML]{B7E1CD}45.12 & \cellcolor[HTML]{B7E1CD}53.25 & 66.44 & \underline{60.65} & 49.19 & 39.43 & 56.36 & 38.74 & 5.44 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv & 80.39 & \cellcolor[HTML]{B7E1CD}31.14 & \cellcolor[HTML]{B7E1CD}55.88 & 59.13 & 51.58 & 43.51 & 29.01 & 49.43 & 28.67 & 2.77\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv & 79.85 & \cellcolor[HTML]{B7E1CD}31.34 & \cellcolor[HTML]{B7E1CD}54.86 & 58.69 & 51.43 & 43.1 & 29.01 & 49.08 & 28.66 & 6.6 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv & 82.77 & \cellcolor[HTML]{B7E1CD}35.67 & \cellcolor[HTML]{B7E1CD}57.92 & 68.38 & 54.91 & 46.8 & 33.69 & 54.22 & 33.65 & 3.51 \\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv & 81.81 & \cellcolor[HTML]{B7E1CD}35.4 & \cellcolor[HTML]{B7E1CD}59.47 & 68.63 & 54.34 & 47.44 & 33.72 & 54.46 & 33.66 & 8.73 \\
  & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv & 82.94
& \cellcolor[HTML]{B7E1CD}\textbf{\underline{46.39}} & \cellcolor[HTML]{B7E1CD}\textbf{\underline{64.13}} & \textbf{\underline{73.58}} & 59.41 & \textbf{\underline{55.26}} & \textbf{\underline{44.47}} & \underline{\textbf{60.88}} & \underline{\textbf{44.03}}&2.99

 \\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv & 82.3 & \cellcolor[HTML]{B7E1CD}45.89 & \cellcolor[HTML]{B7E1CD}63.76 & 72.8 & 59.56 & 54.82 & 44 & 60.5 & 43.54 & 7.5 \\ \hline

\multirow{ 13}{*}{2} & AVG & $\ell_\infty$, StAdv, Recolor & 88.67 & \cellcolor[HTML]{B7E1CD}39.46 & \cellcolor[HTML]{B7E1CD}47.1 & \cellcolor[HTML]{B7E1CD}66.87 & 57.16 & 51.14 & 32.61 & 52.65 & 32.55 & 39.72 \\
& MAX & $\ell_\infty$, StAdv, Recolor & 83.42 & \cellcolor[HTML]{B7E1CD}44.54 & \cellcolor[HTML]{B7E1CD}53.06 & \cellcolor[HTML]{B7E1CD}67.56 & 60.71 & 55.05 & 40.23 & 56.47 & 40.17 & 47.21 \\
& Random & $\ell_\infty$, StAdv, Recolor & 83.23 & \cellcolor[HTML]{B7E1CD}35.01 & \cellcolor[HTML]{B7E1CD}54.7 & \cellcolor[HTML]{B7E1CD}68.68 & \textbf{62.92} & 52.8 & 32.83 & 55.33 & 32.83 & 13.81 \\ \cdashline{2-13}
& FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 81.97 & \cellcolor[HTML]{B7E1CD}44.1 & \cellcolor[HTML]{B7E1CD}57.36 & \cellcolor[HTML]{B7E1CD}68.68 & 60.37 & 56.71 & 41.21 & 57.63 & 41.2 & 6.72\\
& FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 82.24 & \cellcolor[HTML]{B7E1CD}44.36 & \cellcolor[HTML]{B7E1CD}58.52 & \cellcolor[HTML]{B7E1CD}68.87 & 60.23 & 57.25 & \textbf{\underline{41.73}} & 57.99 & \textbf{\underline{41.67}} & 16.69 \\
& FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.98
&\cellcolor[HTML]{B7E1CD}43.32 & \cellcolor[HTML]{B7E1CD}52.45 & \cellcolor[HTML]{B7E1CD}69.46 & 61.04 & 55.08 & 37.05 & 56.57 & 37.01 & 2.53 \\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.89 & \cellcolor[HTML]{B7E1CD}44.66 & \cellcolor[HTML]{B7E1CD}51.6 & \cellcolor[HTML]{B7E1CD}68.86 & 61.59 & 55.04 & 38.02 & 56.68 & 37.96 & 6.3 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 90.55 & \cellcolor[HTML]{B7E1CD}25.27 & \cellcolor[HTML]{B7E1CD}12.77 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{74.01}} & 48.99 & 37.35 & 10.85 & 40.26 & 10.85 & 4.35\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor & \textbf{90.24} & \cellcolor[HTML]{B7E1CD}33.94 & \cellcolor[HTML]{B7E1CD}13.43 & \cellcolor[HTML]{B7E1CD}73.23 & 53.51 & 40.2 & 10.67 & 43.53 & 10.64 & 7.83 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 88.38 & \cellcolor[HTML]{B7E1CD}38.62 & \cellcolor[HTML]{B7E1CD}24.87 & \cellcolor[HTML]{B7E1CD}72.69 & 56.66 & 45.39 & 19.2 & 48.21 & 19.19 & 3.41 \\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 89.38 & \cellcolor[HTML]{B7E1CD}33.64 & \cellcolor[HTML]{B7E1CD}20.91 & \cellcolor[HTML]{B7E1CD}73.52 & 53.36 & 42.69 & 17.39 & 45.36 & 17.38 & 9.87 \\
  & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor& 84.3 & \cellcolor[HTML]{B7E1CD}44.39 & \cellcolor[HTML]{B7E1CD}58.86 & \cellcolor[HTML]{B7E1CD}71.67 & 60.42 & 58.31 & 40.82 & 58.84 & 40.69 & 3.52 \\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.69 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{44.96}} & \cellcolor[HTML]{B7E1CD}\textbf{\underline{59.53}} & \cellcolor[HTML]{B7E1CD}73.54 & \underline{61.73} & \underline{\textbf{59.34}} & 41.39 & \textbf{\underline{59.94}} & 41.22 & 8.21 \\ \hline

\multirow{ 13}{*}{3}& AVG & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 87.77 & \cellcolor[HTML]{B7E1CD}41.97 & \cellcolor[HTML]{B7E1CD}39.55 & \cellcolor[HTML]{B7E1CD}67.93 & \cellcolor[HTML]{B7E1CD}68.55 & 54.5 & 30.39 & 54.5 & 30.39 & 50.54 \\
& MAX & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 84.3 & \cellcolor[HTML]{B7E1CD}41.69 & \cellcolor[HTML]{B7E1CD}52.3 & \cellcolor[HTML]{B7E1CD}65.1 & \cellcolor[HTML]{B7E1CD}57.62 & 54.18 & 37.44 & 54.18 & 37.44 & 55.54 \\
& Random & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 86.32 & \cellcolor[HTML]{B7E1CD}35.04 & \cellcolor[HTML]{B7E1CD}47.82 & \cellcolor[HTML]{B7E1CD}68.35 & \cellcolor[HTML]{B7E1CD}65.87 & 54.27 & 30.76 & 54.27 & 30.76 & 12.41 \\ \cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 82.27
 & \cellcolor[HTML]{B7E1CD}44.21 & \cellcolor[HTML]{B7E1CD}58.13 & \cellcolor[HTML]{B7E1CD}69.08 & \cellcolor[HTML]{B7E1CD}60.7 & 58.03 & \textbf{\underline{41.48}} & 58.03 & \textbf{\underline{41.48}} & 7.9\\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 82.6 & \cellcolor[HTML]{B7E1CD}43.84 & \cellcolor[HTML]{B7E1CD}57.75 & \cellcolor[HTML]{B7E1CD}68.84 & \cellcolor[HTML]{B7E1CD}60.23 & 57.66 & 41.19 & 57.66 & 41.19 & 19.74 \\
   & FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 85.11 & \cellcolor[HTML]{B7E1CD}44.71 & \cellcolor[HTML]{B7E1CD}50.32 & \cellcolor[HTML]{B7E1CD}68.39 & \cellcolor[HTML]{B7E1CD}63.29 & 56.68 & 37.23 & 56.68 & 37.23 & 2.37 \\


 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 85.33 & \cellcolor[HTML]{B7E1CD}43.8 & \cellcolor[HTML]{B7E1CD}50.28 & \cellcolor[HTML]{B7E1CD}68.77 & \cellcolor[HTML]{B7E1CD}63.17 & 56.51 & 36.77 & 56.51 & 36.77 & 5.95 \\ 
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ &88.49 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{44.93}} & \cellcolor[HTML]{B7E1CD}18.06 & \cellcolor[HTML]{B7E1CD}65.96 & \cellcolor[HTML]{B7E1CD}67.56 & 49.13 & 15.78 & 49.13 & 15.78 & 1.63 \\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & \textbf{\underline{89.3}} & \cellcolor[HTML]{B7E1CD}42.72 & \cellcolor[HTML]{B7E1CD}11.85 & \cellcolor[HTML]{B7E1CD}60.27 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{69.12}} & 45.99 & 10.71 & 45.99 & 10.71 & 4.07 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 88.14 & \cellcolor[HTML]{B7E1CD}41.52 & \cellcolor[HTML]{B7E1CD}26.06 & \cellcolor[HTML]{B7E1CD}61.97 & \cellcolor[HTML]{B7E1CD}68.77 & 49.58 & 24.19 & 49.58 & 24.19 & 2.52\\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 87.8 & \cellcolor[HTML]{B7E1CD}40.78 & \cellcolor[HTML]{B7E1CD}28.34 & \cellcolor[HTML]{B7E1CD}59.47 & \cellcolor[HTML]{B7E1CD}68.32 & 49.23 & 25.92 & 49.23 & 25.92 & 5.89 \\
  & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ &  84.56
 & \cellcolor[HTML]{B7E1CD}42.19 & \cellcolor[HTML]{B7E1CD}55.55 & \cellcolor[HTML]{B7E1CD}69.95 & \cellcolor[HTML]{B7E1CD}60.69 & 57.1 & 38.24 & 57.1 & 38.24 & 3.4 \\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 84.1 & \cellcolor[HTML]{B7E1CD}43.32 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{58.2}} & \cellcolor[HTML]{B7E1CD}\textbf{\underline{72.09}} & \cellcolor[HTML]{B7E1CD}61.96 & \textbf{\underline{58.89}} & 39.97 & \textbf{\underline{58.89}} & 39.97 & 8.28 \\ \hline
\end{tabular}}
\caption{\textbf{Continual Robust Training on CIFAR-10 (Sequence of 4 attacks starting with $\ell_\infty$).} The learner initially has knowledge of $\ell_\infty$ attacks and over time, we are sequentially introduced to StAdv, ReColor, and $\ell_2$ attacks. We report clean accuracy, accuracy on different attack types, and average and union accuracies.  The threat models column represents the set of attacks known to the defender and accuracies on known attacks are highlighted with in green cells.  ``FT" procedures are fine-tuning approaches starting from adversarially trained to $\ell_\infty$ model (AT) and then sequentially fine-tuning with new attacks for 25 epochs. AVG, MAX, and Random strategies train models from scratch with all attacks for 100 epochs. The ``Avg (known)" and ``Union (known)" columns represent average and union accuracies on attacks known to the defender while ``Avg (all)" and ``Union (all)" columns represent average and union accuracies on all four attacks. Additionally, we report training times for the procedure (non-cumulative) in the ``Time" column.  Best performance out of both training from scratch and fine-tuning are bolded, while best performance when only comparing fine-tuning approaches is underlined.}
\label{app:main_results_cifar_epochs_2}
\end{table*}


\begin{table*}[]
\scalebox{0.75}{
\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
\begin{tabular}[c]{@{}c@{}}Time\\ Step\end{tabular} & Procedure & Threat Models & Clean & $\ell_2$ & StAdv & $\ell_\infty$ & Recolor & \begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\ (hrs)\end{tabular} \\ \hline
\multirow{2}{*}{0} & AT & $\ell_2$ & \textbf{90.22} & \cellcolor[HTML]{B7E1CD}83.95 & 10.65 & 7.67 & 49.22 & 83.95 & 83.95 & 37.87 & 3.16 & 1.71 \\
 & AT + ALR & $\ell_2$ & 89.76 & \cellcolor[HTML]{B7E1CD}\textbf{84.41} & \textbf{28.23} & \textbf{25.22} & \textbf{54.70} & \textbf{84.41} & \textbf{84.41} & \textbf{48.14} & \textbf{18.01} & 2.15 \\ \hline
 \multirow{13}{*}{1}& AVG & $\ell_2$, StAdv & 84.56 & \cellcolor[HTML]{B7E1CD}77.68 & \cellcolor[HTML]{B7E1CD}74.32 & 7.57 & 31.33 & 76 & 73.68 & 47.73 & 7.44 & 3.58 \\
 & MAX & $\ell_2$, StAdv & 85.22 & \cellcolor[HTML]{B7E1CD}76.87 & \cellcolor[HTML]{B7E1CD}\textbf{77.63} & 4.94 & 27.61 & \textbf{77.25} & \textbf{75.57} & 46.76 & 4.76 & 3.52 \\
 & Random & $\ell_2$, StAdv & 85.71 & \cellcolor[HTML]{B7E1CD}77.55 & \cellcolor[HTML]{B7E1CD}74.32 & 5.78 & 29.61 & 75.94 & 73.55 & 46.82 & 5.53 & 2.58 \\\cdashline{2-13}
  & FT MAX (10 ep) & $\ell_{2}$, StAdv & 83.92 & \cellcolor[HTML]{B7E1CD}77.5 & \cellcolor[HTML]{B7E1CD}69.02 & 10.78 & 35.77 & 73.26 & 68.89 & 48.27 & 10.45 & 0.61 \\
  & FT MAX (25 ep) & $\ell_{2}$, StAdv & 84.56 &\cellcolor[HTML]{B7E1CD}77.73 & \cellcolor[HTML]{B7E1CD}69.35 & 9.76 & 36.15 & 73.54 & 69.1 & 48.25 & 9.43 & 1.44 \\
  & FT Croce (10 ep) & $\ell_{2}$, StAdv & 85.07 & \cellcolor[HTML]{B7E1CD}78.62 & \cellcolor[HTML]{B7E1CD}67.52 & 10.57 & 38.34 & 73.07 & 67.31 & 48.76 & 10.29 & 0.4  \\
 & FT Croce (25 ep) & $\ell_{2}$, StAdv & \textbf{\underline{86.37}} & \cellcolor[HTML]{B7E1CD}\underline{\textbf{79.67}} & \cellcolor[HTML]{B7E1CD}69.32 & 9.81 & 38.27 & 74.5 & 69.17 & 49.27 & 9.63 & 0.98 \\
 & FT Single (10 ep) & $\ell_2$, StAdv & 84.08 & \cellcolor[HTML]{B7E1CD}77.86 & \cellcolor[HTML]{B7E1CD}68.31 & 10.83 & 36.97 & 73.08 & 68.13 & 48.49 & 10.45 & 0.51 \\
 & FT Single (25 ep) & $\ell_{2}$, StAdv & 85.63 & \cellcolor[HTML]{B7E1CD}78.39 & \cellcolor[HTML]{B7E1CD}\underline{72.31} & 7.57 & 35.31 & \underline{75.35} & \underline{72.08} & 48.39 & 7.36 & 1.15 \\
 & FT Single + ALR (10 ep) & $\ell_{2}$, StAdv & 83.8 & \cellcolor[HTML]{B7E1CD}77.94 & \cellcolor[HTML]{B7E1CD}71.62 & \underline{\textbf{20.71}} & 43.13 & 74.78 & 71.34 & \underline{\textbf{53.35}} & \underline{\textbf{20.13}} & 0.58 \\
 & FT Single + ALR (25 ep) & $\ell_{2}$, StAdv & 83.9 & \cellcolor[HTML]{B7E1CD}77.78 & \cellcolor[HTML]{B7E1CD}71.97 & 17.35 & 38.39 & 74.88 & 71.59 & 51.38 & 16.76 & 1.44  \\
 & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv & 85.04 & \cellcolor[HTML]{B7E1CD}79.54 &\cellcolor[HTML]{B7E1CD}69.99 & 18.68 & 42.93 & 74.76 & 69.89 & 52.78 & 18.09 & 0.51 \\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv & 85.07 & \cellcolor[HTML]{B7E1CD}79.39 & \cellcolor[HTML]{B7E1CD}68 & 19.57 & \underline{\textbf{43.67}} & 73.69 & 67.97 & 52.66 & 19.16 & 1.24 \\ \hline
\multirow{13}{*}{2} & AVG & $\ell_2$, StAdv, $\ell_\infty$ & \textbf{86.62} & \cellcolor[HTML]{B7E1CD}\textbf{84.92} & \cellcolor[HTML]{B7E1CD}68.89 & \cellcolor[HTML]{B7E1CD}50.57 & 66.98 & \textbf{68.13} & 49.17 & \textbf{67.84} & 47.82 & 10.51 \\
 & MAX & $\ell_2$, StAdv, $\ell_\infty$ & 80.36 & \cellcolor[HTML]{B7E1CD}78.09 & \cellcolor[HTML]{B7E1CD}68.38 & \cellcolor[HTML]{B7E1CD}\textbf{52.61} & 67.29 & 66.36 & \textbf{51.77} & 66.59 & \textbf{50.37} & 11.96 \\
 & Random & $\ell_2$, StAdv, $\ell_\infty$ & 84.92 & \cellcolor[HTML]{B7E1CD}83.06 & \cellcolor[HTML]{B7E1CD}68.76 & \cellcolor[HTML]{B7E1CD}49.50 & 66.11 & 67.11 & 48.15 & 66.86 & 46.60 & 4.29 \\\cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 81.76 & \cellcolor[HTML]{B7E1CD}76.69 & \cellcolor[HTML]{B7E1CD}71.03 & \cellcolor[HTML]{B7E1CD}28.31 & 54.32 & 58.68 & 28.31 & 57.59 & 27.69 & 0.67\\
 & FT MAX (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 82.04 & \cellcolor[HTML]{B7E1CD}77.86 & \cellcolor[HTML]{B7E1CD}69.02 & \cellcolor[HTML]{B7E1CD}42.83 & 66.9 & 63.24 & 42.37 & 64.15 & 41.86 & 1.71 \\
  & FT Croce (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 83.59 & \cellcolor[HTML]{B7E1CD}78.8 & \cellcolor[HTML]{B7E1CD}69.53 & \cellcolor[HTML]{B7E1CD}34.17 & 61.5 & 60.83 & 34.06 & 61 & 33.61 & 0.3\\
 & FT Croce (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & \underline{85.22} & \cellcolor[HTML]{B7E1CD}\underline{81.02} & \cellcolor[HTML]{B7E1CD}69.58 & \cellcolor[HTML]{B7E1CD}39.92 & 64.79 & 63.51 & 39.59 & 63.83 & 39.03 & 0.73 \\
 & FT Single (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 82.06 & \cellcolor[HTML]{B7E1CD}77.25 & \cellcolor[HTML]{B7E1CD}73.1 & \cellcolor[HTML]{B7E1CD}27.21 & 57.4 & 59.18 & 27.21 & 58.74 & 26.9 & 0.22\\
 & FT Single (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 82.04 & \cellcolor[HTML]{B7E1CD}77.96 & \cellcolor[HTML]{B7E1CD}70.42 & \cellcolor[HTML]{B7E1CD}41.15 & 66.09 & 63.18 & 40.92 & 63.9 & 40.46 & 0.54 \\
  & FT Single + ALR (10 ep)& $\ell_{2}$, StAdv, $\ell_{\infty}$ & 81.38 & \cellcolor[HTML]{B7E1CD}77.89 & \cellcolor[HTML]{B7E1CD}71.8 & \cellcolor[HTML]{B7E1CD}46.68 & \underline{\textbf{72.13}} & \underline{65.45} & 46.5 & \underline{67.12} & 46.14 & 0.31 \\
 & FT Single + ALR (25 ep)& $\ell_{2}$, StAdv, $\ell_{\infty}$ & 80.92 & \cellcolor[HTML]{B7E1CD}77.43 & \cellcolor[HTML]{B7E1CD}70.78 & \cellcolor[HTML]{B7E1CD}\underline{47.16} & 70.6 & 65.12 & \underline{46.96} & 66.49 & \underline{46.62} & 0.79 \\
 & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 83.95  & \cellcolor[HTML]{B7E1CD}79.57 &  \cellcolor[HTML]{B7E1CD}69.22 & \cellcolor[HTML]{B7E1CD}37.96 & 59.77 & 62.25 & 37.86 &  61.63 & 36.99 & 0.40\\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 83.11 & \cellcolor[HTML]{B7E1CD}79.24 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{72.38}} & \cellcolor[HTML]{B7E1CD}36.61 & 60.18 & 62.74 & 36.59 & 62.1 & 36.15 & 1.01 \\
  \hline
\multirow{13}{*}{3} & AVG & $\ell_2$, StAdv, $\ell_\infty$, Recolor & \textbf{87.67} & \cellcolor[HTML]{B7E1CD}\textbf{85.66} & \cellcolor[HTML]{B7E1CD}66.06 & \cellcolor[HTML]{B7E1CD}50.42 & \cellcolor[HTML]{B7E1CD}75.90 & 69.51 & 47.90 & 69.51 & 47.90 & 13.79 \\
 & MAX & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 83.26 & \cellcolor[HTML]{B7E1CD}81.22 & \cellcolor[HTML]{B7E1CD}70.70 & \cellcolor[HTML]{B7E1CD}\textbf{56.94} & \cellcolor[HTML]{B7E1CD}74.80 & \textbf{70.92} & \textbf{55.31} & \textbf{70.92} & \textbf{55.31} & 14.60 \\
 & Random & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 86.55 & \cellcolor[HTML]{B7E1CD}84.64 & \cellcolor[HTML]{B7E1CD}66.52 & \cellcolor[HTML]{B7E1CD}47.29 & \cellcolor[HTML]{B7E1CD}74.93 & 68.34 & 45.71 & 68.34 & 45.71 & 9.61 \\\cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 81.99 & \cellcolor[HTML]{B7E1CD}77.78 & \cellcolor[HTML]{B7E1CD}68.28 & \cellcolor[HTML]{B7E1CD}41.83 & \cellcolor[HTML]{B7E1CD}69.91 & 64.45 & 41.4 & 64.45 & 41.4 & 1.31
 \\
  & FT MAX (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 82.78 & \cellcolor[HTML]{B7E1CD}79.21 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{70.83}} & \cellcolor[HTML]{B7E1CD}45.15 & \cellcolor[HTML]{B7E1CD}71.39 & 66.64 & \underline{44.76} & 66.64 & \underline{44.76} & 3.6 \\
   & FT Croce (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 84.87 & \cellcolor[HTML]{B7E1CD}80.38 & \cellcolor[HTML]{B7E1CD}66.68 & \cellcolor[HTML]{B7E1CD}36.82 &\cellcolor[HTML]{B7E1CD}68.61 & 63.12 & 36.31 & 63.12 & 36.31 & 0.45\\ 
 & FT Croce (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 86.32 & \cellcolor[HTML]{B7E1CD}82.11 & \cellcolor[HTML]{B7E1CD}68.79 & \cellcolor[HTML]{B7E1CD}41.27 & \cellcolor[HTML]{B7E1CD}72.41 & 66.15 & 40.69 & 66.15 & 40.69 & 1.2 \\
 & FT Single (10 ep)& $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 86.27 & \cellcolor[HTML]{B7E1CD}81.35 & \cellcolor[HTML]{B7E1CD}54.73 & \cellcolor[HTML]{B7E1CD}23.59 & \cellcolor[HTML]{B7E1CD}70.17 & 57.46 & 22.55 & 57.46 & 22.55 & 0.71\\
 & FT Single (25 ep)& $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 85.1 & \cellcolor[HTML]{B7E1CD}80.48 & \cellcolor[HTML]{B7E1CD}58.17 & \cellcolor[HTML]{B7E1CD}36.38 & \cellcolor[HTML]{B7E1CD}70.62 & 61.41 & 34.45 & 61.41 & 34.45 & 2.03 \\
 & FT Single + ALR (10 ep) &  $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 85.3 & \cellcolor[HTML]{B7E1CD}81.04 & \cellcolor[HTML]{B7E1CD}49.35 & \cellcolor[HTML]{B7E1CD}40.48 & \cellcolor[HTML]{B7E1CD}74.8 & 61.42 & 35.62 & 61.42 & 35.62 & 0.85\\
 & FT Single + ALR (25 ep)& $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & \underline{86.78} & \cellcolor[HTML]{B7E1CD}\underline{82.8} & \cellcolor[HTML]{B7E1CD}47.82 & \cellcolor[HTML]{B7E1CD}33.12 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{77.58}} & 60.33 & 29.17 & 60.33 & 29.17 & 2.38 \\
 & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 85.3 & \cellcolor[HTML]{B7E1CD}81.3 & \cellcolor[HTML]{B7E1CD}69.35 & \cellcolor[HTML]{B7E1CD}43.13 & \cellcolor[HTML]{B7E1CD}70.85 & 66.16 & 42.62 & 66.16 & 42.62 & 0.53
\\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 85.81 & \cellcolor[HTML]{B7E1CD}81.76 & \cellcolor[HTML]{B7E1CD}67.13 & \cellcolor[HTML]{B7E1CD}\underline{45.38} & \cellcolor[HTML]{B7E1CD}73.02 & \underline{66.82} & 44.56 & \underline{66.82} & 44.56 & 1.36
 \\ \hline
\end{tabular}
}\caption{\textbf{Continual Robust Training on ImageNette (Sequence of 4 attacks starting with $\ell_2$).}}
\label{app:main_results_imagenette_epochs_1}
\end{table*}

\begin{table*}[]
\scalebox{0.75}{
\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
\begin{tabular}[c]{@{}c@{}}Time\\ Step\end{tabular} & Procedure & Threat Models & Clean & $\ell_\infty$ & StAdv & Recolor & $\ell_2$ & \begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\ (hrs)\end{tabular} \\ \hline
 & AT & $\ell_\infty$ & \textbf{82.52} & \cellcolor[HTML]{B7E1CD}56.94 & \textbf{61.32} & 71.62 & \textbf{78.39} & 56.94 & 56.94 & \textbf{67.07} & 50.32 & 1.70 \\
\multirow{-2}{*}{0} & AT + ALR & $\ell_\infty$ & 81.52 & \cellcolor[HTML]{B7E1CD}\textbf{59.62} & 60.51 & \textbf{73.50} & 72.69 & \textbf{59.62} & \textbf{59.62} & 66.58 & \textbf{52.92} & 2.67 \\ \hline
 \multirow{13}{*}{1} & AVG & $\ell_\infty$, StAdv & \textbf{85.78} & \cellcolor[HTML]{B7E1CD}53.30 & \cellcolor[HTML]{B7E1CD}\textbf{75.69} & 67.69 & \textbf{81.96} & 64.5 & 53.02 & 69.66 & 51.11 & 5.87 \\
 & MAX & $\ell_\infty$, StAdv & 83.77 & \cellcolor[HTML]{B7E1CD}\textbf{58.04 }& \cellcolor[HTML]{B7E1CD}70.04 & \textbf{72.76} & 80.38 & 64.04 & \textbf{56.54} & \textbf{70.31} & 55.26 & 6.11 \\
 & Random & $\ell_\infty$, StAdv & 83.34 & \cellcolor[HTML]{B7E1CD}52.23 & \cellcolor[HTML]{B7E1CD}73.76 & 67.85 & 79.87 & 62.99 & 51.77 & 68.43 & 50.39 & 2.44 \\\cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{\infty}$, StAdv & 82.27 & \cellcolor[HTML]{B7E1CD}55.03 & \cellcolor[HTML]{B7E1CD}70.52 & 69.78 & 78.27 & 62.78 & 54.17 & 68.4 & 52.66 & 0.62 \\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv & 82.57 & \cellcolor[HTML]{B7E1CD}55.46 & \cellcolor[HTML]{B7E1CD}71.75 & 69.94 & 78.73 & 63.61 & 54.85 & 68.97 & 53.22 & 1.48 \\
 & FT Croce (10 ep)& $\ell_{\infty}$, StAdv & 82.29 & \cellcolor[HTML]{B7E1CD}54.62 & \cellcolor[HTML]{B7E1CD}69.2 & 68.87 & 78.32 & 61.91 & 53.35 & 67.75 & 51.9 & 0.37\\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv & 83.67 & \cellcolor[HTML]{B7E1CD}54.27 & \cellcolor[HTML]{B7E1CD}71.57 & 69.07 & \underline{79.54} & 62.92 & 53.58 & 68.61 & 52.2 & 0.86 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv & 83.06 & \cellcolor[HTML]{B7E1CD}50.52 & \cellcolor[HTML]{B7E1CD}71.52 & 65.43 & 78.78 & 61.02 & 49.96 & 66.56 & 48.18 & 0.51 \\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv & \underline{84.00} & \cellcolor[HTML]{B7E1CD}43.59 & \cellcolor[HTML]{B7E1CD}\underline{73.68} & 58.14 & 78.85 & 58.64 & 43.46 & 63.57 & 41.27 & 1.16 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv & 82.19 & \cellcolor[HTML]{B7E1CD}42.7 & \cellcolor[HTML]{B7E1CD}73.17 & 60.97 & 77.55 & 57.94 & 42.6 & 63.6 & 41.27 & 0.58\\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv & 81.4 & \cellcolor[HTML]{B7E1CD}51.8 & \cellcolor[HTML]{B7E1CD}69.35 & 66.32 & 76.54 & 60.57 & 51.08 & 66 & 50.04 & 1.47 \\
 & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv & 82.62 & \cellcolor[HTML]{B7E1CD}\underline{57.71} & \cellcolor[HTML]{B7E1CD}70.11 & \underline{72.41} & 77.89 & 63.91 & \underline{\textbf{56.54}} & 69.53 & 55.62 & 0.49\\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv & 82.37 & \cellcolor[HTML]{B7E1CD}57.55 & \cellcolor[HTML]{B7E1CD}71.64 & 70.93 & 78.14 & \underline{\textbf{64.6}} & \underline{\textbf{56.54}} & \underline{69.57} & \underline{\textbf{55.64}} & 1.11 \\ \hline
 \multirow{13}{*}{2}& AVG & $\ell_\infty$, StAdv, Recolor & \textbf{86.39} & \cellcolor[HTML]{B7E1CD}51.80 & \cellcolor[HTML]{B7E1CD}\textbf{73.81} & \cellcolor[HTML]{B7E1CD}\textbf{77.99} & \textbf{83.13} & \textbf{67.86} & 51.31 & \textbf{71.68} & 51.31 & 11.57 \\
 & MAX & $\ell_\infty$, StAdv, Recolor & 81.20 & \cellcolor[HTML]{B7E1CD}54.55& \cellcolor[HTML]{B7E1CD}68.64 & \cellcolor[HTML]{B7E1CD}72.82 & 78.01 & 65.33 & 53.12 & 68.5 & 53.12 & 13.55 \\
 & Random & $\ell_\infty$, StAdv, Recolor & 86.29 & \cellcolor[HTML]{B7E1CD}50.96 & \cellcolor[HTML]{B7E1CD}72.28 & \cellcolor[HTML]{B7E1CD}76.59 & 82.96 & 66.61 & 50.27 & 70.69 & 50.27 & 4.90 \\\cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 82.34 & \cellcolor[HTML]{B7E1CD}55.34 & \cellcolor[HTML]{B7E1CD}71.34 & \cellcolor[HTML]{B7E1CD}72.87 & 78.22 & 66.51 & 54.04 & 69.44 & 54.04 &1.21 \\
& FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 83.75 & \cellcolor[HTML]{B7E1CD}55.29 & \cellcolor[HTML]{B7E1CD}\underline{72.56} & \cellcolor[HTML]{B7E1CD}74.83 & 79.97 & \underline{67.56} & 54.27 & \underline{70.66} & 54.27 & 3.03 \\
& FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.28 & \cellcolor[HTML]{B7E1CD}54.01 & \cellcolor[HTML]{B7E1CD}69.96 & \cellcolor[HTML]{B7E1CD}72.56 & 79.72 & 65.51 & 52.13 & 69.06 & 52.13 & 0.5\\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.05 & \cellcolor[HTML]{B7E1CD}52.99 & \cellcolor[HTML]{B7E1CD}70.47 & \cellcolor[HTML]{B7E1CD}73.07 & 80.33 & 65.51 & 51.92 & 69.22 & 51.92 & 1.23 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 83.77 & \cellcolor[HTML]{B7E1CD}53.61 & \cellcolor[HTML]{B7E1CD}65.38 & \cellcolor[HTML]{B7E1CD}73.35 & 79.36 & 64.11 & 50.7 & 67.92 & 50.7 & 0.75\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor & \underline{85.07} & \cellcolor[HTML]{B7E1CD}48.2 & \cellcolor[HTML]{B7E1CD}65.86 & \cellcolor[HTML]{B7E1CD}\underline{75.41} & \underline{80.41} & 63.16 & 46.34 & 67.47 & 46.34 & 1.88 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 84.94	& \cellcolor[HTML]{B7E1CD}50.8 & \cellcolor[HTML]{B7E1CD}65.71 & \cellcolor[HTML]{B7E1CD}76.33 & 80.15 & 64.28 & 48.31 & 68.25 & 48.31 & 0.88\\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 82.09 & \cellcolor[HTML]{B7E1CD}55.46 & \cellcolor[HTML]{B7E1CD}66.27 & \cellcolor[HTML]{B7E1CD}73.63 & 77.76 & 65.12 & 52.89 & 68.28 & 52.89 & 2.14 \\
 & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 82.42 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{55.75}} & \cellcolor[HTML]{B7E1CD}65.83 & \cellcolor[HTML]{B7E1CD}73.3 & 78.06 & 64.96 & 52.94 & 68.24 & 52.94& 0.61 \\
   & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 83.9 & \cellcolor[HTML]{B7E1CD}55.52 & \cellcolor[HTML]{B7E1CD}71.21 & \cellcolor[HTML]{B7E1CD}75.29 & 79.82 & 67.34 & \underline{\textbf{54.32}} & 70.46 & \underline{\textbf{54.32}} & 1.49 \\ \hline
 \multirow{13}{*}{3}& AVG & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & \textbf{87.67} & \cellcolor[HTML]{B7E1CD}50.42 & \cellcolor[HTML]{B7E1CD}66.06 & \cellcolor[HTML]{B7E1CD}\textbf{75.90} & \cellcolor[HTML]{B7E1CD}\textbf{85.66} & 69.51 & 47.90 & 69.51 & 47.90 & 13.79 \\
 & MAX & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 83.26 & \cellcolor[HTML]{B7E1CD}56.94 & \cellcolor[HTML]{B7E1CD}\textbf{70.70} & \cellcolor[HTML]{B7E1CD}74.80 & \cellcolor[HTML]{B7E1CD}81.22 & \textbf{70.92} & \textbf{55.31} & \textbf{70.92} & \textbf{55.31} & 14.60 \\
 & Random & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 86.55 & \cellcolor[HTML]{B7E1CD}47.29 & \cellcolor[HTML]{B7E1CD}66.52 & \cellcolor[HTML]{B7E1CD}74.93 & \cellcolor[HTML]{B7E1CD}84.64 & 68.34 & 45.71 & 68.34 & 45.71 & 4.58 \\\cdashline{2-13}
 & FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 82.73 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{55.08}} & \cellcolor[HTML]{B7E1CD}71.36 & \cellcolor[HTML]{B7E1CD}73.5 & \cellcolor[HTML]{B7E1CD}78.98 & 69.73 & \underline{54.19} & 69.73 & \underline{54.19} & 1.3\\
  & FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 83.72 & \cellcolor[HTML]{B7E1CD}54.93 & \cellcolor[HTML]{B7E1CD}\underline{72.18} & \cellcolor[HTML]{B7E1CD}74.42 & \cellcolor[HTML]{B7E1CD}79.9 & \underline{70.36} & 53.94 & \underline{70.36} & 53.94 & 3.26 \\
  & FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 84.33 & \cellcolor[HTML]{B7E1CD}52.18 & \cellcolor[HTML]{B7E1CD}69.5 & \cellcolor[HTML]{B7E1CD}72.74 & \cellcolor[HTML]{B7E1CD}79.97 & 68.6 & 50.55 & 68.6 & 50.55 & 0.44 \\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 84.84 & \cellcolor[HTML]{B7E1CD}52.59 & \cellcolor[HTML]{B7E1CD}68.74 & \cellcolor[HTML]{B7E1CD}73.27 & \cellcolor[HTML]{B7E1CD}\underline{81.45} & 69.01 & 50.96 & 69.01 & 50.96 & 1.1 \\
  & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 84.79 & \cellcolor[HTML]{B7E1CD}53.02 & \cellcolor[HTML]{B7E1CD}65.43 & \cellcolor[HTML]{B7E1CD}72.82 & \cellcolor[HTML]{B7E1CD}80.08 & 67.83 & 50.29 & 67.83 & 50.29 & 0.26\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & \underline{85.5} & \cellcolor[HTML]{B7E1CD}49.12 & \cellcolor[HTML]{B7E1CD}64.79 & \cellcolor[HTML]{B7E1CD}74.27 & \cellcolor[HTML]{B7E1CD}80.76 & 67.24 & 47.21 & 67.24 & 47.21 & 0.63 \\
 & FT Single + ALR (10 ep)& $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 85.1 & \cellcolor[HTML]{B7E1CD}45.4 & \cellcolor[HTML]{B7E1CD}63.44 & \cellcolor[HTML]{B7E1CD}67.06 & \cellcolor[HTML]{B7E1CD}80.59 & 64.12 & 42.96 & 64.12 & 42.96 & 0.35\\
 & FT Single + ALR (25 ep)& $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 83.64 & \cellcolor[HTML]{B7E1CD}54.37 & \cellcolor[HTML]{B7E1CD}64.48 & \cellcolor[HTML]{B7E1CD}72.41 & \cellcolor[HTML]{B7E1CD}79.69 & 67.74 & 50.96 & 67.74 & 50.96 & 0.92 \\
 & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 83.03 & \cellcolor[HTML]{B7E1CD}53.96 & \cellcolor[HTML]{B7E1CD}67.9 & \cellcolor[HTML]{B7E1CD}72.38 & \cellcolor[HTML]{B7E1CD}79.08 & 68.33 & 51.95 & 68.33 & 51.95 & 0.55 \\
  & FT Croce + ALR (25 ep)& $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 84.84 & \cellcolor[HTML]{B7E1CD}53.94 & \cellcolor[HTML]{B7E1CD}68.51 & \cellcolor[HTML]{B7E1CD}\underline{75.11} & \cellcolor[HTML]{B7E1CD}81.32 & 69.72 & 52.23 & 69.72 & 52.23 & 1.36 \\ \hline
\end{tabular}
}\caption{\textbf{Continual Robust Training on ImageNette (Sequence of 4 attacks starting with $\ell_\infty$).}}
\label{app:main_results_imagenette_epochs_2}
\end{table*}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table*}[]
\scalebox{0.75}{\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
 \multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Time\\ Step \end{tabular}} & \multicolumn{1}{|c|}{Procedure} & \multicolumn{1}{c|}{Threat Models} & Clean & $\ell_2$ & StAdv & $\ell_\infty$ & Recolor & \begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\ (hrs)\end{tabular} \\ \hline
\multirow{ 2}{*}{0}& AT & $\ell_2$ & \textbf{67.75} & \cellcolor[HTML]{B7E1CD}41.65 & 4.21 & 14.28 & 22.46 & 41.65 & 41.65 & 20.65 & 2.34 & 14.85 \\
& Ours & $\ell_2$ & 63.53 & \cellcolor[HTML]{B7E1CD}\textbf{42.88} & \textbf{6.16} & \textbf{19.8} & \textbf{23.36} & \textbf{42.88} & \textbf{42.88} & \textbf{23.05} & \textbf{4.37} & 21.98 \\ \hline
\multirow{13}{*}{1}&AVG & $\ell_2$, StAdv & \textbf{64.48} & \cellcolor[HTML]{B7E1CD}\textbf{35.72} & \cellcolor[HTML]{B7E1CD}28.57 & 9.18 & 19.51 & 32.15 & 25.25 & \textbf{23.24} & 7.53 & 47.24 \\
&MAX & $\ell_2$, StAdv & 61.80 & \cellcolor[HTML]{B7E1CD}33.82 & \cellcolor[HTML]{B7E1CD}31.77 & 7.58 & 17.76 & \textbf{32.79} & \textbf{27.68} & 22.73 & 6.51 & 47.24 \\
&Random & $\ell_2$, StAdv & 62.7 & \cellcolor[HTML]{B7E1CD}31.75 & \cellcolor[HTML]{B7E1CD}\textbf{32.25} & 6.50 & 17.62 & 32.00 & 26.28 & 22.03 & 5.79 & 23.56 \\ \cdashline{2-13}
& FT MAX (10 ep) & $\ell_{2}$, StAdv & 60.3 & \cellcolor[HTML]{B7E1CD}30.9 & \cellcolor[HTML]{B7E1CD}29.32 & 5.96 & 17.04 & 30.11 & 23.97 & 20.8 & 5.15 & 4\\
 & FT MAX (25 ep) & $\ell_{2}$, StAdv & 61.14 & \cellcolor[HTML]{B7E1CD}31.69 & \cellcolor[HTML]{B7E1CD}29.93 & 6.21 & 17.84 & \underline{30.81} & \underline{24.84} & 21.42 & 5.3 & 10.71 \\
 & FT Croce (10 ep) & $\ell_{2}$, StAdv & 62.68 & \cellcolor[HTML]{B7E1CD}35.2 & \cellcolor[HTML]{B7E1CD}23.7 & 8.9 & \textbf{\underline{19.93}} & 29.45 & 21.02 & 21.93 & 6.77 & 2.6\\
 & FT Croce (25 ep) & $\ell_{2}$, StAdv & \underline{63.39} & \cellcolor[HTML]{B7E1CD}31.38 & \cellcolor[HTML]{B7E1CD}28.95 & 5.7 & 17.98 & 30.16 & 23.87 & 21 & 5.04 & 5.96 \\
  & FT Single (10 ep) & $\ell_{2}$, StAdv & 60.48 & \cellcolor[HTML]{B7E1CD}18.29 & \cellcolor[HTML]{B7E1CD}30.83 & 2.24 & 13.28 & 24.56 & 16.19 & 16.16 & 1.84 & 2.77\\
 & FT Single (25 ep) & $\ell_{2}$, StAdv & 60.78 & \cellcolor[HTML]{B7E1CD}18.22 & \cellcolor[HTML]{B7E1CD}\underline{31.79} & 1.99 & 13.32 & 25 & 16.34 & 16.33 & 1.6 & 6.91 \\
 & FT Single + ALR (10 ep) & $\ell_{2}$, StAdv & 53 & \cellcolor[HTML]{B7E1CD}29.23 & \cellcolor[HTML]{B7E1CD}24 & 8.45 & 17.18 & 26.61 & 20.12 & 19.71 & 6.71 & 2.77\\
 & FT Single + ALR (25 ep) & $\ell_{2}$, StAdv & 54.44 & \cellcolor[HTML]{B7E1CD}22.03 & \cellcolor[HTML]{B7E1CD}29.1 & 4.46 & 13.6 & 25.56 & 19.21 & 17.3 & 3.78 & 8.73 \\
 & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv & 59.09 & \cellcolor[HTML]{B7E1CD}34.09 & \cellcolor[HTML]{B7E1CD}26.59 & 9.84 & 19.2 & 30.34 & 23.62 & 22.43 & 8.28 & 3.11 \\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv & 58.59 & \cellcolor[HTML]{B7E1CD}\underline{34.65} & \cellcolor[HTML]{B7E1CD}26.45 & \textbf{\underline{10.55}} & 19.65 & 30.55 & 23.67 & \underline{22.82} & \underline{\textbf{8.64}} & 7.67 \\ \hline
\multirow{13}{*}{2}& AVG & $\ell_2$, StAdv, $\ell_\infty$ & 62.09 & \cellcolor[HTML]{B7E1CD}40.34 & \cellcolor[HTML]{B7E1CD}25.13 & \cellcolor[HTML]{B7E1CD}20.88 & 28.84 & \textbf{28.78} & 16.18 & 28.80 & 14.71 & 69.49 \\
& MAX & $\ell_2$, StAdv, $\ell_\infty$ & 56.94 & \cellcolor[HTML]{B7E1CD}34.74 & \cellcolor[HTML]{B7E1CD}\textbf{28.46} & \cellcolor[HTML]{B7E1CD}22.72 & \textbf{30.35} & 28.64 & \textbf{19.66} & \textbf{29.04} & \textbf{17.55} & 69.36 \\
& Random & $\ell_2$, StAdv, $\ell_\infty$ & 60.98 & \cellcolor[HTML]{B7E1CD}38.01 & \cellcolor[HTML]{B7E1CD}25.21 & \cellcolor[HTML]{B7E1CD}17.25 & 25.76 & 26.82 & 14.26 & 26.56 & 12.97 & 19.58 \\\cdashline{2-13} 
& FT MAX (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 59.9 & \cellcolor[HTML]{B7E1CD}38.94 & \cellcolor[HTML]{B7E1CD}26.55 & \cellcolor[HTML]{B7E1CD}16.95 & 25.72 & 27.48 & 14.78 & 27.04 & 13.07 & 5.2\\
& FT MAX (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 61.01 & \cellcolor[HTML]{B7E1CD}38.56 & \cellcolor[HTML]{B7E1CD}\underline{27.54} & \cellcolor[HTML]{B7E1CD}17.05 & 25.91 & \underline{27.72} & \underline{15.34} & \underline{27.27} & \underline{13.66} & 12.25 \\
 & FT Croce (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 62.78 & \cellcolor[HTML]{B7E1CD}39.87 & \cellcolor[HTML]{B7E1CD}20.17 & \cellcolor[HTML]{B7E1CD}14.59 & 23.7 & 24.88 & 10.46 & 24.58 & 9.2 & 2.29 \\
 & FT Croce (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 65.85 & \cellcolor[HTML]{B7E1CD}42.51 & \cellcolor[HTML]{B7E1CD}11.05 & \cellcolor[HTML]{B7E1CD}19.67 & 26.55 & 24.41 & 8.06 & 24.95 & 7.3 & 5.06 \\
 & FT Single (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 65.62 & \cellcolor[HTML]{B7E1CD}42.95 & \cellcolor[HTML]{B7E1CD}7.55 & \cellcolor[HTML]{B7E1CD}22.03 & 28.06 & 24.18 & 5.57 & 25.15 & 4.98 & 1.49\\
 & FT Single (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & \underline{\textbf{65.93}} & \cellcolor[HTML]{B7E1CD}42.82 & \cellcolor[HTML]{B7E1CD}7.62 & \cellcolor[HTML]{B7E1CD}21.83 & \underline{28.2} & 24.09 & 5.65 & 25.12 & 5.12 & 3.71 \\
  & FT Single + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 62.35 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{43.56}} & \cellcolor[HTML]{B7E1CD}8.76 & \cellcolor[HTML]{B7E1CD}23.72 & 27.77 & 25.35 & 6.98 & 25.95 & 6.29 & 2.16
\\
 & FT Single + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 62.56 & \cellcolor[HTML]{B7E1CD}42.33 & \cellcolor[HTML]{B7E1CD}7.7 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{25.06}} & 26.57 & 25.03 & 6.67 & 25.41 & 6.02 & 5.39 \\
 & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 60.67 & \cellcolor[HTML]{B7E1CD}42.06 & \cellcolor[HTML]{B7E1CD}16.66 & \cellcolor[HTML]{B7E1CD}21.18 & 25.89 & 26.63 & 12.59 & 26.45 & 11.21 & 3.05\\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$ & 63.43 & \cellcolor[HTML]{B7E1CD}42.92 & \cellcolor[HTML]{B7E1CD}10.14 & \cellcolor[HTML]{B7E1CD}23.16 & 26.37 & 25.41 & 8.29 & 25.65 & 7.64 & 6.7 \\
  \hline
\multirow{ 13}{*}{3}&AVG & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 65.61 & \cellcolor[HTML]{B7E1CD}40.86 & \cellcolor[HTML]{B7E1CD}22.4 & \cellcolor[HTML]{B7E1CD}20.45 & \cellcolor[HTML]{B7E1CD}37.27 & 30.25 & 14.09 & 30.25 & 14.09 & 101.43 \\
& MAX & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 59.12 & \cellcolor[HTML]{B7E1CD}33.89 & \cellcolor[HTML]{B7E1CD}\textbf{28.02} & \cellcolor[HTML]{B7E1CD}\textbf{22.20} & \cellcolor[HTML]{B7E1CD}35.00 & 29.78 & \textbf{18.74} & 29.78 & \textbf{18.74} & 101.43 \\
& Random & $\ell_2$, StAdv, $\ell_\infty$, Recolor & 63.1 & \cellcolor[HTML]{B7E1CD}39.47 & \cellcolor[HTML]{B7E1CD}24.79 & \cellcolor[HTML]{B7E1CD}19.04 & \cellcolor[HTML]{B7E1CD}38.15 & \textbf{30.36} & 14.57 & \textbf{30.36} & 14.57 & 22.87 \\\cdashline{2-13}
& FT MAX (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 61.5 & \cellcolor[HTML]{B7E1CD}39.34 & \cellcolor[HTML]{B7E1CD}26.97 & \cellcolor[HTML]{B7E1CD}17.25 & \cellcolor[HTML]{B7E1CD}33.56 & \underline{29.28} & 14.55 & \underline{29.28} & 14.55 & 8.61\\
& FT MAX (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 62.14 & \cellcolor[HTML]{B7E1CD}38.68 & \cellcolor[HTML]{B7E1CD}\underline{27.51} & \cellcolor[HTML]{B7E1CD}17.13 & \cellcolor[HTML]{B7E1CD}33.06 & 29.09 & \underline{14.84} & 29.09 & \underline{14.84} & 19.67 \\
& FT Croce (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 64.82 & \cellcolor[HTML]{B7E1CD}41.09 & \cellcolor[HTML]{B7E1CD}19.78 & \cellcolor[HTML]{B7E1CD}16.55 & \cellcolor[HTML]{B7E1CD}32.26 & 27.42 & 10.57 & 27.42 & 10.57 & 2.42\\
 & FT Croce (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 66.31 & \cellcolor[HTML]{B7E1CD}41.02 & \cellcolor[HTML]{B7E1CD}13.42 & \cellcolor[HTML]{B7E1CD}17.34 & \cellcolor[HTML]{B7E1CD}31.02 & 25.7 & 8.4 & 25.7 & 8.4 & 6.03 \\
 & FT Single (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & \underline{\textbf{69.82}} & \cellcolor[HTML]{B7E1CD}32.63 & \cellcolor[HTML]{B7E1CD}4.07 & \cellcolor[HTML]{B7E1CD}9.38 & \cellcolor[HTML]{B7E1CD}40.07 & 21.54 & 1.42 & 21.54 & 1.42 & 3.06\\
 & FT Single (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 68.63 & \cellcolor[HTML]{B7E1CD}37.06 & \cellcolor[HTML]{B7E1CD}5.57 & \cellcolor[HTML]{B7E1CD}13.28 & \cellcolor[HTML]{B7E1CD}37.66 & 23.39 & 2.76 & 23.39 & 2.76 & 7.78 \\
 & FT Single + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 66.58 & \cellcolor[HTML]{B7E1CD}37.98 & \cellcolor[HTML]{B7E1CD}6.65 & \cellcolor[HTML]{B7E1CD}16.37 & \cellcolor[HTML]{B7E1CD}39.23 & 25.06 & 3.83 & 25.06 & 3.83 & 3.91\\
 & FT Single + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 68.15 & \cellcolor[HTML]{B7E1CD}32.05 & \cellcolor[HTML]{B7E1CD}5.08 & \cellcolor[HTML]{B7E1CD}11.82 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{41.5}} & 22.61 & 2.46 & 22.61 & 2.46 & 9.72 \\
  & FT Croce + ALR (10 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 64.11 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{42.52}} & \cellcolor[HTML]{B7E1CD}10.89 & \cellcolor[HTML]{B7E1CD}\underline{21.36} & \cellcolor[HTML]{B7E1CD}34.05 & 27.21 & 8.11 & 27.21 & 8.11 & 3.42\\
 & FT Croce + ALR (25 ep) & $\ell_{2}$, StAdv, $\ell_{\infty}$, Recolor & 65.33 & \cellcolor[HTML]{B7E1CD}39.4 & \cellcolor[HTML]{B7E1CD}11.41 & \cellcolor[HTML]{B7E1CD}16.84 & \cellcolor[HTML]{B7E1CD}34.15 & 25.45 & 7.35 & 25.45 & 7.35 & 7.41
 \\ \hline
\end{tabular}}
\caption{\textbf{Continual Robust Training on CIFAR-100 (Sequence of 4 attacks starting with $\ell_2$).}}
\label{app:main_results_cifar100_full}
\end{table*}

\begin{table*}[]
\scalebox{0.75}{\begin{tabular}{|c|l|l|c|cccc|cc|cc|c|}
\hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Time\\ Step \end{tabular}}&\multicolumn{1}{|c|}{Procedure} & Threat Models & Clean & $\ell_\infty$ & StAdv & Recolor & $\ell_2$ & \begin{tabular}[c]{@{}c@{}}Avg\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (known)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Union\\ (all)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\ (hrs)\end{tabular} \\ \hline
\multirow{ 2}{*}{0}&AT & $\ell_\infty$ & \textbf{60.95} & \cellcolor[HTML]{B7E1CD}27.61 & 9.92 & \textbf{33.2} & \textbf{35.6} & 27.61 & 27.61 & \textbf{26.58} & 7.45 & 16.33 \\
& AT + ALR & $\ell_{\infty}$ & 55.36 & \cellcolor[HTML]{B7E1CD}\textbf{28.01} & \textbf{11.25} & 31.01 & 33.95 & \textbf{28.01} & \textbf{28.01} & 26.05 & 8.62 & 23.75 \\ \hline
\multirow{ 13}{*}{1}& AVG & $\ell_\infty$, StAdv & \textbf{66.09} & \cellcolor[HTML]{B7E1CD}8.65 & \cellcolor[HTML]{B7E1CD}33.19 & 18.42 & 21.58 & 20.92 & 8.18 & 20.46 & 7.47 & 47.59 \\
&MAX & $\ell_\infty$, StAdv & 57.01 & \cellcolor[HTML]{B7E1CD}22.8 & \cellcolor[HTML]{B7E1CD}29.23 & 30.54 & \textbf{33.58} & \textbf{26.02} & \textbf{20.28} & \textbf{29.04} & \textbf{17.96} & 46.97 \\
&Random & $\ell_\infty$, StAdv & 47.14 & \cellcolor[HTML]{B7E1CD}16.62 & \cellcolor[HTML]{B7E1CD}25.61 & 25.35 & 27.25 & 21.12 & 14.63 & 23.71 & 13.37 & 23.92 \\\cdashline{2-13}
& FT MAX (10 ep) & $\ell_{\infty}$, StAdv & 58.05 & \cellcolor[HTML]{B7E1CD}22.48 & \cellcolor[HTML]{B7E1CD}28.66 & 30.43 & 33.15 & 25.57 & 18.95 & 28.68 & 17.05 & 4.03\\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv & 58.56 & \cellcolor[HTML]{B7E1CD}22.36 & \cellcolor[HTML]{B7E1CD}29.38 & \textbf{\underline{30.97}} & 33.16 & \underline{25.87} & \underline{19.38} & \underline{28.97} & \underline{17.43} & 10.02 \\
  & FT Croce (10 ep) & $\ell_{\infty}$, StAdv & 60.17 & \cellcolor[HTML]{B7E1CD}22.75 & \cellcolor[HTML]{B7E1CD}26.81 & 31.22 & 33.68 & 24.78 & 17.54 & 28.62 & 15.95 & 2.49\\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv & \underline{60.27} & \cellcolor[HTML]{B7E1CD}22.18 & \cellcolor[HTML]{B7E1CD}28.17 & 30.38 & \underline{33.25} & 25.18 & 17.81 & 28.5 & 16.21 & 5.69 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv & 55.87 & \cellcolor[HTML]{B7E1CD}15.43 & \cellcolor[HTML]{B7E1CD}24.29 & 24.34 & 27.54 & 19.86 & 11.9 & 22.9 & 10.95 & 2.77\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv & 56.11 & \cellcolor[HTML]{B7E1CD}16.09 & \cellcolor[HTML]{B7E1CD}24.46 & 24.84 & 28.65 & 20.27 & 12.5 & 23.51 & 11.33 & 6.95 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv & 56.03 & \cellcolor[HTML]{B7E1CD}3.81 & \cellcolor[HTML]{B7E1CD}35.21 & 18.52 & 17.31 & 19.51 & 3.77 & 18.71 & 3.51 & 75.49 \\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv & 59.65 & \cellcolor[HTML]{B7E1CD}2.6 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{37.96}} & 18.51 & 13.52 & 20.28 & 2.58 & 18.15 & 2.41 & 8.34 \\
  & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv & 54.86 & \cellcolor[HTML]{B7E1CD}\textbf{\underline{23.33}} & \cellcolor[HTML]{B7E1CD}27.19 & 30.15 & 31.54 & 25.26 & 18.75 & 28.05 & 16.86 & 2.97\\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv & 54.27 & \cellcolor[HTML]{B7E1CD}23.27 & \cellcolor[HTML]{B7E1CD}26.27 & 29.4 & 31.79 & 24.77 & 18.25 & 27.68 & 16.06 & 7.4 \\
  \hline
\multirow{ 13}{*}{2}&AVG & $\ell_\infty$, StAdv, Recolor & \textbf{68.19 }& \cellcolor[HTML]{B7E1CD}16.88 & \cellcolor[HTML]{B7E1CD}\textbf{29.53} & \cellcolor[HTML]{B7E1CD}38.12 & 30.75 & 28.18 & 14.14 & 28.82 & 14.11 & 79.47 \\
& MAX & $\ell_\infty$, StAdv, Recolor & 57.96 & \cellcolor[HTML]{B7E1CD}22.38 & \cellcolor[HTML]{B7E1CD}28.92 & \cellcolor[HTML]{B7E1CD}35.24 & 33.9 & 28.85 & \textbf{19.27} & 30.11 & \textbf{19.17} & 79.5 \\
& Random & $\ell_\infty$, StAdv, Recolor & 47.14 & \cellcolor[HTML]{B7E1CD}16.62 & \cellcolor[HTML]{B7E1CD}25.61 & \cellcolor[HTML]{B7E1CD}25.35 & 27.25 & 21.12 & 14.63 & 23.71 & 13.37 & 23.92 \\\cdashline{2-13}
& FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 58.96 & \cellcolor[HTML]{B7E1CD}22.04 & \cellcolor[HTML]{B7E1CD}29 & \cellcolor[HTML]{B7E1CD}36.35 & 34.55 & 29.13 & 18.37 & 30.48 & 18.31 & 6.75\\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 59.41 & \cellcolor[HTML]{B7E1CD}21.7 & \cellcolor[HTML]{B7E1CD}\underline{29.2} & \cellcolor[HTML]{B7E1CD}35.57 & 33.24 & 28.82 & \underline{18.21} & 29.93 & \underline{18.09} & 16.75 \\
  & FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 62.27 & \cellcolor[HTML]{B7E1CD}22.42 & \cellcolor[HTML]{B7E1CD}26.16 & \cellcolor[HTML]{B7E1CD}37.25 & 34.5 & 28.61 & 16.34 & 30.08 & 16.24 & 2.77 \\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 62.09 & \cellcolor[HTML]{B7E1CD}\underline{23.24} & \cellcolor[HTML]{B7E1CD}25.6 & \cellcolor[HTML]{B7E1CD}36.91 & \underline{35.89} & 28.58 & 16.63 & \underline{30.41} & 16.52 & 6.46 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 64.02 & \cellcolor[HTML]{B7E1CD}20.72 & \cellcolor[HTML]{B7E1CD}14.72 & \cellcolor[HTML]{B7E1CD}41.7 & 32.89 & 25.71 & 9.61 & 27.51 & 9.57 & 3.01\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor & \underline{67.39} & \cellcolor[HTML]{B7E1CD}13.25 & \cellcolor[HTML]{B7E1CD}6.94 & \cellcolor[HTML]{B7E1CD}45.1 & 27.71 & 21.76 & 3.59 & 23.25 & 3.57 & 7.86 \\
 & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 64.86 & \cellcolor[HTML]{B7E1CD}8.76 & \cellcolor[HTML]{B7E1CD}9.34 & \cellcolor[HTML]{B7E1CD}46.4 & 25.62 & 21.5 & 3.91 & 22.53 & 3.91 & 3.8 \\
 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 66.87 & \cellcolor[HTML]{B7E1CD}3.74 & \cellcolor[HTML]{B7E1CD}8.44 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{50.81}} & 19.81 & 21 & 2.03 & 20.7 & 2.03 & 9.89 \\
 & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor & 56.41 & \cellcolor[HTML]{B7E1CD}24.28 & \cellcolor[HTML]{B7E1CD}25.62 & \cellcolor[HTML]{B7E1CD}36.21 & 34.43 & 28.7 & 17.6 & 30.14 & 17.41& 3.6\\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor & 56.83 & \cellcolor[HTML]{B7E1CD}22.43 & \cellcolor[HTML]{B7E1CD}25.9 & \cellcolor[HTML]{B7E1CD}38.27 & 33.5 & \textbf{\underline{28.87}} & 16.98 & 30.03 & 16.76 & 8.28 \\
  \hline
\multirow{ 13}{*}{3}&AVG & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 64.8 & \cellcolor[HTML]{B7E1CD}20.9 & \cellcolor[HTML]{B7E1CD}22.46 & \cellcolor[HTML]{B7E1CD}37.27 & \cellcolor[HTML]{B7E1CD}41.05 & 30.42 & 14.56 & 30.42 & 14.56 & 101.39 \\
&MAX & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 57.9 & \cellcolor[HTML]{B7E1CD}22.39 & \cellcolor[HTML]{B7E1CD}28.72 & \cellcolor[HTML]{B7E1CD}35.96 & \cellcolor[HTML]{B7E1CD}35.65 & \textbf{30.68} & \textbf{19.15} & \textbf{30.68} & \textbf{19.15} & 101.25 \\
&Random & $\ell_\infty$, StAdv, Recolor, $\ell_2$ & 63.23 & \cellcolor[HTML]{B7E1CD}19.52 & \cellcolor[HTML]{B7E1CD}21.29 & \cellcolor[HTML]{B7E1CD}\textbf{39.71} & \cellcolor[HTML]{B7E1CD}39.95 & 30.12 & 13.6 & 30.12 & 13.6 & 24.88 \\\cdashline{2-13}
& FT MAX (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 59.61 & \cellcolor[HTML]{B7E1CD}22.14 & \cellcolor[HTML]{B7E1CD}29.13 & \cellcolor[HTML]{B7E1CD}36.17 & \cellcolor[HTML]{B7E1CD}34.35 & 30.45 & 18.61 & 30.45 & 18.61 & 8.58\\
 & FT MAX (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 59.42 & \cellcolor[HTML]{B7E1CD}22.02 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{29.28}} & \cellcolor[HTML]{B7E1CD}35.96 & \cellcolor[HTML]{B7E1CD}34.45 & \underline{30.43} & \underline{18.64} & \underline{30.43} & \underline{18.64} & 21.36 \\
 & FT Croce (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 62.44 & \cellcolor[HTML]{B7E1CD}20.96 & \cellcolor[HTML]{B7E1CD}26.06 & \cellcolor[HTML]{B7E1CD}35.91 & \cellcolor[HTML]{B7E1CD}36.77 & 29.93 & 15.83 & 29.93 & 15.83 & 2.38 \\
 & FT Croce (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 62.17 & \cellcolor[HTML]{B7E1CD}21.84 & \cellcolor[HTML]{B7E1CD}26.14 & \cellcolor[HTML]{B7E1CD}\underline{36.92} & \cellcolor[HTML]{B7E1CD}36.69 & 30.4 & 16.08 & 30.4 & 16.08 & 5.81 \\
 & FT Single (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 63.94 & \cellcolor[HTML]{B7E1CD}23.86 & \cellcolor[HTML]{B7E1CD}13.73 & \cellcolor[HTML]{B7E1CD}37.22 & \cellcolor[HTML]{B7E1CD}41.47 & 29.07 & 9.92 & 29.07 & 9.92 & 1.61\\
 & FT Single (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & \underline{\textbf{66.44}} & \cellcolor[HTML]{B7E1CD}21.17 & \cellcolor[HTML]{B7E1CD}7.72 & \cellcolor[HTML]{B7E1CD}31.83 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{42.5}} & 25.8 & 5.67 & 25.8 & 5.67 & 4.07 \\
  & FT Single + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 60.76 & \cellcolor[HTML]{B7E1CD}22.36 & \cellcolor[HTML]{B7E1CD}10.35 & \cellcolor[HTML]{B7E1CD}31.33 & \cellcolor[HTML]{B7E1CD}41.91 & 26.49 & 7.99 & 26.49 & 7.99 & 2.35\\

 & FT Single + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 62.25 & \cellcolor[HTML]{B7E1CD}20.56 & \cellcolor[HTML]{B7E1CD}7.92 & \cellcolor[HTML]{B7E1CD}30.69 & \cellcolor[HTML]{B7E1CD}41.42 & 25.15 & 6.25 & 25.15 & 6.25 & 6.35 \\
 & FT Croce + ALR (10 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 57.56 & \cellcolor[HTML]{B7E1CD}24.64 & \cellcolor[HTML]{B7E1CD}22.52 & \cellcolor[HTML]{B7E1CD}35.77 & \cellcolor[HTML]{B7E1CD}37.55 & 30.12 & 15.96 & 30.12 & 15.96 & 3.36\\
 & FT Croce + ALR (25 ep) & $\ell_{\infty}$, StAdv, Recolor, $\ell_{2}$ & 58.14 & \cellcolor[HTML]{B7E1CD}\underline{\textbf{24.85}} & \cellcolor[HTML]{B7E1CD}18.69 & \cellcolor[HTML]{B7E1CD}36.75 & \cellcolor[HTML]{B7E1CD}39.16 & 29.86 & 13.87 & 29.86 & 13.87 & 7.64\\
 \hline
\end{tabular}}
\caption{\textbf{Continual Robust Training on CIFAR-100 (Sequence of 4 attacks starting with $\ell_\infty$).}}
\label{app:main_results_cifar100_full_2}
\end{table*}


\section{Initial Training Ablations}
\label{app:init_train}
In this section, we present some ablations across regularization strength of each regularization method on the initial training portion of our approach pipeline.  We present ablation results for CIFAR-10 and ImageNette.
\subsection{Performance across different threat models} \label{app:init_train_different_attack_types}
In this section, we perform initial training with models using different initial attacks including attacks in the UAR benchmark \citep{kaufmann2019testing} and evaluate the performance across attack types for training with single-step variation regularization, single-step adversarial $\ell_2$ regularization, uniform regularization, and gaussian regularization.

We present ablation results for CIFAR-10 (Table \ref{app:IT_ablation_VR_CIF} for variation regularization, Table \ref{app:IT_ablation_L2_CIF} for adversarial $\ell_2$ regularization, Table \ref{app:IT_ablation_UR_CIF} for uniform regularization, and Table \ref{app:IT_ablation_GR_CIF} for Gaussian regularization) and ImageNette (Table \ref{app:IT_ablation_VR_IM} for variation regularization, Table \ref{app:IT_ablation_L2_IM} for adversarial $\ell_2$ regularization, Table \ref{app:IT_ablation_UR_IM} for uniform regularization, and Table \ref{app:IT_ablation_GR_IM} for Gaussian regularization).  Overall, we find that across different starting attacks and unseen test attacks, regularization generally improves performance on unseen attacks, leading to increases in average and union accuracy across all attacks with regularization.  We find that in many cases (especially using random noise types) using regularization trades off clean accuracy.  Additionally, some threat models such as Snow are generally more difficult to gain improvement on via regularization; for many starting models, using regularization decreases accuracy on Snow attack.
\input{sections/appendix_pretrain_abl}

%\subsubsection{Impact of number of iterations for computing variation regularization and adversarial $\ell_2$ regularization}
%\label{app:num_iterations_abl}

\subsection{Impact of random noise parameter $\sigma$}
\label{app:random_noise_var_abl}
To investigate the impact of the noise parameter $\sigma$, we perform initial training on CIFAR-10 with uniform and gaussian regularization at different values of $\sigma$.  We maintain a value of regularization strength $\lambda=5$ to isolate the impact of the noise variance from regularization strength.  We report results in Table \ref{tab:rand_noise_sigma}.  Overall, we find that $\sigma$ has an effect similar to the effect of increasing $\lambda$ where higher values of $\sigma$ leads to higher average and union robust accuracies at the cost of lower clean accuracy and accuracy on the initial attack.
\begin{table*}[]
\centering
\begin{tabular}{|l|c|c|cccc|cc|}
\hline
Noise type & $\sigma$ & Clean & $\ell_2$ & $\ell_\infty$ & StAdv & Recolor & Avg & Union \\ \hline
Uniform & 0.5 & 90.40 & 70.21 & 31.91 & 1.31 & 39.83 & 35.81 & 0.86 \\
Uniform & 1 & 90.76 & 69.89 & 32.58 & 1.49 & 40.14 & 36.02 & 1.15 \\
Uniform & 2 & 85.28 & 63.65 & 50.73 & 10.62 & 60.10 & 46.28 & 8.40 \\ \hline
Gaussian & 0.05 & 90.04 & 69.62 & 31.61 & 7.25 & 43.84 & 38.08 & 6.64 \\
Gaussian & 0.1 & 88.53 & 68.54 & 32.04 & 14.5 & 51.73 & 41.70 & 12.63 \\
Gaussian & 0.2 & 87.00 & 64.88 & 27.46 & 31.82 & 63.59 & 46.94 & 18.7 \\ \hline
\end{tabular}
\caption{\textbf{Impact of $\sigma$ on regularization based on random noise in initial training.} We maintain regularization strength $\lambda = 5$ and perform initial training on CIFAR-10 with $\ell_2$ attacks. We report the clean accuracy, accuracy on $\ell_2$, $\ell_\infty$, StAdv, and Recolor attacks, and the average and union accuracies on the set.}
\label{tab:rand_noise_sigma}
\end{table*}

\subsection{Impact of starting and new attack pairs}
\label{app:fine-tuning_pairs_init_train}
In order to see how much our results depend on attack choice, we experiment with starting with a model initially trained with a starting attack and then fine-tuned for robustness to a new attack for different starting and new attack pairs on Imagenette.  In this section, we ask the question: does regularization in initial training generally lead to better starting points for fine-tuning?  In the following experiments, we use adversarial training as the base initial training procedure and \citet{croce2022adversarial}'s fine-tuning approach as the base fine-tuning procedure.  We consider these approaches with and without regularization.

We compare models initially trained with regularization (and fine-tuned without regularization) to models initially trained without regularization (and fine-tuned without regularization).  We present the differences in average accuracy across the 2 attacks, union accuracy across the 2 attacks, accuracy on the starting attack, accuracy on the new attack, and clean accuracy between the 2 settings for adversarial $\ell_2$ regularization (with $\lambda=0.5$) in \cref{fig:imagenette_finetune_abl_froml2}, for variation regularization in \cref{fig:imagenette_finetune_abl_fromvarreg} (with $\lambda=0.2$), for uniform regularization (with $\sigma=2$ and $\lambda=1$) in Figure \ref{fig:imagenette_finetune_abl_fromuniform}, and for gaussian regularization (with $\sigma=0.2$ and $\lambda=0.5$) in Figure \ref{fig:imagenette_finetune_abl_fromgauss}. In these figures, we highlight gains in accuracy larger than 1\% in green and drops in accuracy larger than 1\% in red.

\textbf{Regularization in initial training generally improves performance. }Across all figures, we can see that for most pairs of attacks, regularization leads to improvements on average accuracy, union accuracy, accuracy on the initial attack, accuracy on the new attack.  We find that this improvement is more consistent across attack types when using adversarial versions of regularization such as adversarial $\ell_2$ regularization or variation regularization in comparison to random noise based regularizations.  This improvement in performance may be due to the fact that regularization improves unforeseen robustness, causing the initial accuracy on the new attack to generally be higher, and thus a better starting point for fine-tuning the model for robustness against new attacks.

\textbf{Uniform regularization in initial training can improve clean accuracy for certain starting attack types. }From Figure \ref{fig:imagenette_finetune_abl_fromuniform}e, we observe that using uniform regularization in initial training can lead to increases in clean accuracy after fine-tuning for several initial attack types: StAdv, ReColor, Pixel, Elastic, Wood, and Kaleidoscope attacks.  In comparison, Figure \ref{fig:imagenette_finetune_abl_froml2}e, demonstrates that using adversarial $\ell_2$ regularization does not improve clean accuracy for as many threat models as uniform regularization; for adversarial $\ell_2$ regularization, the most improvements in clean accuracy are when the initial attack is Elastic attack or when the new attack is $\ell_\infty$ attack. Adversarial $\ell_2$ regularization generally maintains clean accuracy for most attacks, but leads a drop in clean accuracy when the starting attack type is StAdv attack.  We find that similarly, variation regularization also maintains clean accuracy.  Gaussian regularization on the other hand either maintains or exhibits a tradeoff with clean accuracy.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_froml2_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_froml2_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_froml2_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_froml2_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_froml2_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with models initally trained with adversarial $\ell_2$ regularization different initial attack and new attack pairs.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization in initial training.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red. % See Appendix \ref{app:fine-tuning_pairs} for experimental setup details.
    }
    \label{fig:imagenette_finetune_abl_froml2}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromvarreg_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromvarreg_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromvarreg_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromvarreg_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromvarreg_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with models initally trained with variation regularization different initial attack and new attack pairs.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization in initial training.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red. % See Appendix \ref{app:fine-tuning_pairs} for experimental setup details.
    }
    \label{fig:imagenette_finetune_abl_fromvarreg}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromuniform_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromuniform_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromuniform_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromuniform_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromuniform_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with models initally trained with uniform regularization different initial attack and new attack pairs.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization in initial training.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red. % See Appendix \ref{app:fine-tuning_pairs} for experimental setup details.
    }
    \label{fig:imagenette_finetune_abl_fromuniform}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromgaussian_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromgaussian_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromgaussian_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromgaussian_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_fromgaussian_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with models initally trained with Gaussian regularization different initial attack and new attack pairs.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization in initial training.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red. % See Appendix \ref{app:fine-tuning_pairs} for experimental setup details.
    }
    \label{fig:imagenette_finetune_abl_fromgauss}
\end{figure*}

\section{Fine-tuning Ablations}
\label{app:fine-tuning}
\subsection{Impact of starting and new attack pairs}
\label{app:fine-tuning_pairs}
Similar to \cref{app:fine-tuning_pairs_init_train}, we ablate over starting and new attack pairs in finetuning.  In this section, we address the question: does regularization in fine-tuning generally lead to more robust models?  We follow the same setup as in \cref{app:fine-tuning_pairs_init_train} but  we compare models fine-tuned with regularization (with no regularization in pretraining) to models fine-tuned without regularization (with no regularization in pretraining).  We present the differences in average accuracy across the 2 attacks, union accuracy across the 2 attacks, accuracy on the starting attack, accuracy on the new attack, and clean accuracy between the 2 settings for adversarial $\ell_2$ regularization (with $\lambda=0.5$) in Figure \ref{fig:imagenette_finetune_abl} and for uniform regularization (with $\sigma=2$ and $\lambda=1$) in Figure \ref{fig:imagenette_finetune_abl_uniform}.  In these figures, we highlight gains in accuracy larger than 1\% in green and drops in accuracy larger than 1\% in red.

\textbf{Adversarial $\ell_2$ regularization in fine-tuning generally improves performance but trades off clean accuracy. }From Figure \ref{fig:imagenette_finetune_abl}, we can see that for many pairs of initial and new attack, regularization leads to improvements in union accuracy, average accuracy, and new attack accuracy.  However, this comes at a clear tradeoff with clean accuracy.  For accuracy on the initial attack, it is difficult to see clear trends; depending on threat models there can be gains in robustness or drops in robustness.  For example, when the new attack is $\ell_\infty$, we find that the initial attack accuracy generally drops.  We find that variation regularization can also lead to gains in performance, but these gains are much less consistent than compared to adversarial $\ell_2$ regularization.

\textbf{Random noise regularization in fine-tuning hurts overall robustness.}  Unlike adversarial $\ell_2$ regularization which can improve performance when used in both initial training and regularization, we find that uniform and Gaussian regularization generally hurts average, union, initial attack, and new attack accuracies when incorporated in fine-tuning.  This suggests that while random noise based regularization may help with initial training (and unforeseen robustness), they do not necessarily help with continual adaptive robustness through fine-tuning.




\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_l2_reg_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_l2_reg_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_l2_reg_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_l2_reg_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_l2_reg_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with adversarial $\ell_2$ regularization.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red.}
    \label{fig:imagenette_finetune_abl}
\end{figure*}


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_varreg_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_varreg_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_varreg_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_varreg_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_varreg_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with variation regularization.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red.}
    \label{fig:imagenette_finetune_abl_varreg}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_uniform_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_uniform_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_uniform_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_uniform_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_uniform_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with uniform regularization.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red.}
    \label{fig:imagenette_finetune_abl_uniform}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_gaussian_avg.pdf}
        \vspace{-10pt}
        \caption{Difference in Avg Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_gaussian_union.pdf}
        \vspace{-10pt}
        \caption{Difference in Union Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_gaussian_source_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Initial Attack Acc}
    \end{subfigure}%
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_gaussian_target_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in New Attack Acc}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/imagenette_finetune_gaussian_clean_acc.pdf}
        \vspace{-10pt}
        \caption{Difference in Clean Acc}
    \end{subfigure}
    \caption{\textbf{Change in robust accuracy after fine-tuning with Gaussian regularization.}  We fine-tune models on Imagenette across 144 pairs of initial attack and new attack.  The initial attack corresponds to the row of each grid and new attack corresponds to each column.  Values represent differences between the accuracy measured on a model fine-tuned with and without regularization.  Gains in accuracy of at least 1\% are highlighted in green, while drops in accuracy of at least 1\% are highlighted in red.}
    \label{fig:imagenette_finetune_abl_uniform}
\end{figure*}

%\subsubsection{Impact of fine-tuning time}

%\subsubsection{Impact of learning rate}

%\clearpage

%\section*{Previous Reviews and Our Responses (Paper 22)} \label{appsec: reviews}

%\setcounter{subsubsection}{0}

%\subsubsection{Reviewer 1}


%\noindent \textbf{Review:}

%\textit{Summary}:
%This paper explores the use of regularization techniques within Continual Robust Training (CRT) to enhance multi-robustness in machine learning models. The study demonstrates that employing regularization during both initial training and fine-tuning stages can improve robustness against a variety of attacks on multiple datasets, including CIFAR-10, CIFAR-100, and ImageNette. Empirical results indicate that regularized continual robust training is able to a good balance between performance and efficiency.

%\textit{Metrics}:
%Soundness: 2: fair
%Presentation: 2: fair
%Contribution: 2: fair


%\textit{Strengths}:
% -Theoretically demonstrate that the difference in robust losses for a pair of attacks is upper bounded by the sensitivity of a model’s internal representation function to the perturbations introduced by each attack.

%- The findings have practical implications for enhancing model robustness in real-world applications, where models may face a range of adversarial attacks over time.

%- Extensive experiments on multiple datasets and attack types validate the proposed methods, demonstrating significant improvements in robustness.

%\textit{Weaknesses}:
%- The paper does not propose any new method but just uses regularization for extensive experiments, which is a lack of novelty.

%- The experiments, particularly those involving multiple attacks and fine-tuning, are computationally intensive. This may limit the practicality of the approach for organizations with limited resources.

%- The paper does not consider the potential problem of forgetting, which should be the core issue in continual learning, simply using regularization to maintain the performance of the previous methods.

%- Does the sequence of the attacks matter? Especially for the initial threat model using StAdv/Recolor.

%\textit{Questions}:
%See the weaknesses above.

%\textit{Limitations}:
%N/A

%\textit{Flag For Ethics Review}: No ethics review needed.

%\textit{Rating}: 3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and/or incompletely addressed ethical considerations.

%\textit{Confidence}: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

%\textit{Code Of Conduct}: Yes

%\noindent \textbf{Response:}\\
%Like other reviewers, Reviewer 1 brings up issues with the novelty of our work, along with its relationship to the literature on forgetting in continual learning. In response to the concerns about novelty, we added additional discussion as to how our technique differs from prior approaches to the Introduction of this submission. The key point here is that it is \emph{a priori} unclear what regularization would work best to reduce forgetting and enhance overall robustness, for which we provide a theoretically grounded solution. Our paper also provides considerable experimental novelty as we are among the first to investigate the training of multi-robust models beyond $\ell_p$ threat models. In particular, we are the first to show that a fine-tuning based approach is feasible outside of $\ell_p$ threat models.

%With regards to the relationship to continual learning, we added an explanation of how the forgetting of earlier attacks we observed in our experiments relates to the concept of “catastrophic forgetting” in Section IV, under the paragraph heading “Importance of replay”.

%In response to the concerns regarding the computationally intensive nature of our approach, we pointed to the information included in what is now Table I in our current submission. In short, using a fine-tuning approach requires significantly less training time than robust training from scratch when new attacks are introduced, especially as the set of known attacks is increasing in size. In our current submission, we include extra discussion on the computational efficiency of fine-tuning over training from scratch in Section IV, under the “Training time and robust performance” paragraph heading.

%The reviewer also asked how the sequencing of attacks impacted the performance of our training method. We pointed the reviewer to the results presented in Figure III, which presents the robustness of models trained when different combinations of attacks are used for pre-training and fine-tuning while using our regularization term. In our current submission, we have also moved Table II into the main body of our paper, which reports the same information as Table I but using a different sequence of 4 attacks.




%\subsubsection{Reviewer 2}

%\noindent \textbf{Review:}

%\textit{Summary}:
%the paper introduces Continual Robust Training (CRT) in which concepts from Continual Learning are adapted to multi-attack adversarial training. The main idea is to fine-tune a hypothesis that is robust to a set of attacks in a such a way so that the hypothesis become robust to some newly discovered attack in an efficient manner.

%\textit{Metrics}:
%Soundness: 2: fair
%Presentation: 2: fair
%Contribution: 2: fair


%\textit{Strengths}:
%- Originality:
%The paper is original in introducing Continual Learning to a new setting.

%- Quality:
%The paper tries to be formal in its description of the proposal.

%- Clarity:
%The paper attempts to describe the reasons behind empirical observations and matching them theoretical analysis.

%- Significance:
%The paper is attempting to improve an important shortcoming of adversarial training.

%\textit{Weaknesses}:
%All in all, I believe that the paper is not ready to be published. I have compiled a list of things that I believe should be improved:

%Introduction:
%-- Certifiable robustness methods should be introduced as alternatives to CRT. The paper builds on some of the papers from Croce \& Hein and I believe that the authors are already familiar with the literature.

%-- The paper's structure does not help with the readability of the paper. Specifically, I think it would be better if the paper's structure is described in a paragraph. A good practice is to match the structure of the paper with papers that are accepted in previous iterations of Neurips.

%Setup:
%-- The maximization program in line 88 is performed over $x’$ but it is not used in the objective of the program.

%-- The constraints $C: X \rightarrow 2^X$  is an odd notation and it is hard to figure out what it is that the paper is describing as a constraint.

%-- 0-1 loss function is defined for binary classification problems as far as I know. The paper should not change the meaning and the definition of established terms.

%-- The importance of CAR should have been described in the introduction IMO.

%-- The sections on the relation between sMAR and UAR are not describing a relation between CAR and these methods. I believe the paper is comparing these methods.

%Theory and Methods:
%-- Theorem 3.1 is not complete. For example, it is not clear what is $y$ in $0 \leq \ell(\hat{y},y) \leq M_2$.
%. The subset $\mathbb{X}$ is not used. $\mathcal{L}$ marginalizes out $(x,y)$ so it is not clear what does it mean for the expression to be true with some probability $1 - \rho$.


%-- Regularization is already a prominent method for improving robustness, specifically, regularization of the norm of the gradient of the hypothesis.

%-- What is the difference between ALR and TRADES?

%-- The provided analysis in section 3.3 is purely speculative. Even the correlation between theory and practice are described pictorially.

%Experimental Results:
%-- What is surprising about the effect of random regularization on the improvement in the initial step?

%-- While the trade-off between accuracy and robustness is alluded to, the paper does not report how CAR affects clean accuracy.

%-- The paper does not deal with the observation that training from scratch is worse than CAR in a satisfiable manner. This does not align with observations made in Continual Learning if I'm not mistaken.

%EDITED ---------------

%Some of my concerns are alleviated by the authors response.

%\textit{Questions}:
%Please refer to the weaknesses section

%\textit{Limitations}:
%The limitations of Continual Learning in general should be added to the paper as far as I understand it.

%\textit{Flag For Ethics Review}: No ethics review needed.

%\textit{Rating}: 4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

%\textit{Confidence}: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

%\textit{Code Of Conduct}: Yes

%\noindent \textbf{Response:}\\
%Reviewer 2’s main concerns with the paper were regarding some of the notation used in the paper, the novelty of using regularization to improve performance as well as our experimental observations. 

%We have addressed all concerns regarding notation in the current version of the paper. In particular, we clarified what the probability $\rho$ was defined over and explicitly stated how the subset $\mathbb{X}$ is sampled from the distributions $\mathcal{D}$. We also updated the notation in the proof of Theorem III.1 in the Appendix to be more consistent with the notation of the theorem statement in Section III in the SaTML submission.

%In terms of novelty, we agree that regularization has been used in previous works on adversarial robustness but argue that we have other contributions including being the first to study the problem of CAR and theoretical results which motivates the design of the regularization term. In addition, it is \emph{a priori} unclear what regularization would work best to reduce forgetting and enhance overall robustness, for which we provide a theoretically grounded solution. Our paper also provides considerable experimental novelty as we are among the first to investigate the training of multi-robust models beyond $\ell_p$ threat models. In particular, we are the first to show that a fine-tuning based approach is feasible outside of $\ell_p$ threat models. We have clarified our contributions in the Introduction in the submitted paper.

%In the experimental section, we clearly report in each table the impact of different training strategies on the clean accuracy, with our ALR method providing the best, or close to the best, clean accuracies, in addition to being robust against the union of existing attacks. 

%In terms of connections with continual learning, we added an explanation of how the forgetting of earlier attacks we observed in our experiments relates to the concept of ``catastrophic forgetting'' in Section IV-B, under the paragraph heading ``Importance of replay''.  Additionally, we discuss in Section IV-C in paragraph ``Regularized CRT can outperform training from scratch for
%multiattack robustness'' that our results indicate that existing methods for training from scratch with multiple attacks may perform suboptimally.

%We also note that certifiable methods are not alternatives to CRT since certifications are threat model specific, while CRT looks at a threat model that changes over time.  To the best of our knowledge no paper has looked at certifications for unions of threat models including perturbation types outside of $\ell_p$ attacks.


%\subsubsection{Reviewer 3}

%\noindent \textbf{Review:}

%\textit{Summary}:
%This paper proposes to improve model robustness under multiple, continuous attacks. A new training framework called continual robust training is proposed, in which a model is continually trained to be robust against new attacks. The paper provides theoretical analysis and empirical evidence to show the effectiveness of the proposed method.

%\textit{Metrics}:
%Soundness: 3: good
%Presentation: 3: good
%Contribution: 2: fair


%\textit{Strengths}:
%- The paper studies a novel problem of making deep models robust to new attacks. A continue robust training framework is proposed.

%- The paper provide theoretical results to analyze the robust losses of different attacks and motivate empirical methods.

%- The paper is well-written and solid.

%\textit{Weaknesses}:
%- The paper only considers digital-space attacks in experiments - although they are different attacks, they all apply to image pixels. How about the performance when considering more difficult attacks, such as patch-based attacks in the physical world.

%- Although the paper show improvements over the baseline, the authors are encouraged to compare with method to achieve general robustness under diverse attacks (such as [1,2]).

%Reference: [1] Nie et al., Diffusion Models for Adversarial Purification. ICML 2022. [2] Chen et al., Robust Classification via a Single Diffusion Model. ICML 2024.

%\textit{Questions}:
%How about the computation complexity of the proposed method? Calculating the regularization term may need more computations than traditional adversarial training.

%\textit{Limitations}:
%N/A

%\textit{Flag For Ethics Review}: No ethics review needed.

%\textit{Rating}:  5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.

%\textit{Confidence}: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

%\textit{Code Of Conduct}: Yes

%\noindent \textbf{Response:}\\
%Reviewer 3’s main concerns were surrounding our choices of threat model and baselines. As they point out, we mainly consider norm-bounded pixel space attacks, rather than unbounded attacks like adversarial patches. We noted to the reviewer that due to the substantial differences between the two modalities of attacks, it is unlikely that defenses against pixel-based attacks like ours are unlikely to be successful against patches. This in and of itself would be a significant research progress to attempt to address. However, existing certified adversarial patch defenses which are not based on adversarial training can still be used in conjunction with models that are trained and fine-tuned under our CAR framework. 

%The reviewer also cited recent work on robust diffusion classifiers as a potential baseline for our experiments. While these works do report impressive results, we noted that due to the stochastic nature of gradients in diffusion models, it is difficult to assess the robustness of defenses based on diffusion. We also pointed out the existence of adaptive attacks for diffusion-based defenses in image classification, making it unclear whether these are reasonable baselines.

%The reviewer also brought up specific questions related to the computational complexity of our regularization term, pointing out that it would increase the computational overhead needed for adversarial training. We pointed to the training times reported in our evaluation section to show that the increases in training time we observed were modest. Furthermore, we explained that we were able to speed up the computation of ALR and variation regularization by using single step optimization. We include a more thorough discussion of the trade-offs of our regularization approach associated with training time in Section IV of our submission, under the paragraph heading ``Regularization balances performance and efficiency''.



%\subsubsection{Reviewer 4}

%\noindent \textbf{Review:}

%\textit{Summary}:
%The study addresses a scenario of Continual Adaptive Robustness(CAR), where the model adaptively learns in the dynamic presence of unforeseen attacks. The difference in robust loss between attack pairs is upper bounded by the sensitivity of a model’s internal representation to perturbations from each attack. Based on this point, the study proposes Regularized Continual Robust Training (RCRT), which minimizes the l2 distance between representations for attack pairs through regularization. It demonstrates a balance between performance and efficiency across three datasets: CIFAR-10, CIFAR-100, and Imagenette.

%\textit{Metrics}:
%Soundness: 2: fair
%Presentation: 2: fair
%Contribution: 2: fair


%\textit{Strengths}:
%- The method for reducing robust loss between attack pairs in the scenario of continual adaptive robustness is clearly stated and easy to understand.

%- It has been proven that there is an upper bound on the robustness loss for attack pairs based on the model's representation sensitivity to input perturbations. Accordingly, it has been demonstrated why regularization can be effectively applied to reduce model representation distance in the presence of multiple attacks, thus enhancing multiple robustness learning.

%\textit{Weaknesses}:
%1. In the new CAR scenario, aside from showing that adding regularization to the robust loss during initial training and fine-tuning improves performance, there are no novel elements presented.

%    2. There are several elements to consider to verify if the Continual Adaptive Robustness (CAR) setting is realistic:

%i) There is no process for detecting the type of sequentially incoming attacks. Continual adversarial training without an attack detection process seems unrealistic. Continuously fine-tuning with a pre-prepared dataset, knowing in advance which attacks will be sequentially introduced, is bound to perform better compared to a scenario where all possible prior attacks are learned simultaneously.

%ii) Fine-tuning after an attack has already occurred is akin to "mending the barn after the horse is stolen." According to Table 2, the additional training time seems substantial, often close to or exceeding 10 hours. A preemptive simultaneous multi attack robustness(sMAR) scenario with the application of regularization methods seems to be a more realistic approach.

%\textit{Questions}:
%Refer to Weaknesses

%\textit{Limitations}:
%Refer to Weaknesses

%\textit{Flag For Ethics Review}: No ethics review needed.

%\textit{Rating}: 3: Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility and/or incompletely addressed ethical considerations.

%\textit{Confidence}: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

%\textit{Code Of Conduct}: Yes

%\noindent \textbf{Response:}\\
%Reviewer 4’s comments were mainly related to novelty and the motivation of the problem setting studied in this work.  In our submission to SaTML, this comment on novelty is addressed in the Introduction where we bold our different contributions, particularly noting our extensive experimental evaluation, and note that we are the first to propose an algorithm for CAR. In terms of the regularization term proposed, the key point here is that it is \emph{a priori} unclear what regularization would work best to reduce forgetting and enhance overall robustness, for which we provide a theoretically grounded solution. Our paper also provides considerable experimental novelty as we are among the first to investigate the training of multi-robust models beyond $\ell_p$ threat models. In particular, we are the first to show that a fine-tuning based approach is feasible outside of $\ell_p$ threat models.

%To address the concern about the realism of the setting, we edited the text to make the motivation of studying CAR more clear.  In our submission to SaTML, we updated the Introduction to make the main question of ``how can we defend models as new adversaries emerge?'' more visible and we made the motivation more clear by expanding Section II on the problem setup.  We added Section II-A which gave a motivating example for why we should care about quick recovery from new attacks.  Additionally, we made the formulation of the CAR problem more clear by enumerating the goals of the defender in Section II-B, which includes ``Recover robustness quickly after the introduction of a new attack''. Using sMAR methods is infeasible as the accuracy against unknown attacks increaeses substantially only when training on them. This edit addresses the reviewer’s second comment on the realism of the CAR problem. 

%To address the reviewer’s first comment on the realism of CAR and correctness of evaluation, we made the mathematical formulation of CAR more clear by introducing multiple thresholds for performance on known and unknown attacks and a time threshold representing the grace period.  This rewrite of the problem formulation makes the connection to the metrics used to measure CAR performance in evaluation (robust accuracy on only known attacks, robust accuracy on all attacks, and time taken for fine-tuning) more clear.  Additionally, we add a discussion in the limitations section to address the reviewer’s comment about an attack detection process, which is outside of the scope of this paper. In our work, we assume that the defender is able to learn about the existence of new attacks (via published research papers or a security team discovering these new attacks).
