Research on designing attacks against image classification has led to a wide variety of attack types\citep{madry2017towards, XiaoZ0HLS18, LaidlawF19, laidlaw2020perceptual, wasserstein_attacks, wu2020stronger, brown2017adversarial}, such as $\ell_p$-bounded attacks \citep{madry2017towards}, spatial transformation based attacks \citep{XiaoZ0HLS18}, and color shift based attacks \citep{LaidlawF19}.  However, the scope of research on defending against multiple attacks simultaneously is limited in comparison. Existing work on defending against adversarial examples has focused primarily on achieving robustness against a single attack type (primarily $\ell_p$ bounded attacks) \citep{madry2017towards, zhang2019theoretically, croce2020robustbench, gowal2020uncovering, cohen2019certified,  gowal2021improving, sehwag2021robust}.  While a few works have looked at achieving robustness against multiple attacks \citep{MainiWK20, TB19, madaan2020learning, Croce020,croce2022adversarial}, a property we refer to as \emph{multi-robustness} for short, these works all assume that every attack we would like to be robust against is known \textit{a priori} by the defender.  This is unrealistic as it is difficult to model the space of perturbations we would like to be robust to, and over time, new perturbation types may be introduced.  Thus, it is important to consider a temporal dimension to the problem of multi-robustness; specifically, new attacks are introduced \textit{sequentially} and the defender can continuously update the deployed model to be robust against the new attack, while maintaining robustness on the previous attacks.  This dynamic setting of robustness is known as \textit{continual adaptive robustness} (CAR) \citep{dai2024position}.

\textbf{How can continual adaptive robustness be obtained? (\cref{sec: setup})} In CAR, it is important that models can be adapted quickly to new attacks when they are introduced.  While this setting was introduced by \citet{dai2024position}, to the best of our knowledge, no prior works have analyzed and proposed defenses in this setting.  We propose \textit{continual robust training} (CRT) as a natural solution for CAR.  CRT (Figure \ref{fig:overview}) consists of two main parts: (1) initial adversarial training, where we perform adversarial training to obtain robustness against the set of attacks known before deployment, and (2) iterative robust fine-tuning where each time a new attack is introduced post-deployment, we fine-tune the model to gain robustness against the new attack.

%Robustness to adversarial examples has emerged as one of the key requirements for the safe deployment of machine learning (ML) models. Most work on creating robust models has focused on adversarial examples generated via a single attack. Recently, there has been a push to make ML models robust to adversarial examples generated by multiple attacks \citep{MainiWK20, TB19, madaan2020learning, Croce020,croce2022adversarial}, which we refer to as \emph{multi-robustness} for short. Multi-robustness is critical for the practical deployment of robust models since practitioners will be unwilling to deploy an expensively trained model only for it to be vulnerable to the next attack that emerges. \christian{Minor point but I think plenty of practitioners are willing to deploy non-robust models.}

%\textbf{Paragraph 1}: Why should we care about multi-robustness - this is the only way robustness can be made practical. It is also intellectually challenging as it is harder than achieving singly robust models and can inform their design. prior work has however focused on the setting where all the attacks to be defended against are known simultaneoulsy

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\textwidth]{figures/overview_figure_2.pdf}
	\caption{An overview of the problem of adapting to new adversaries (Continual Adaptive Robustness) and our solution framework (Regularized Continual Robust Training).  In this problem, the defender learns about the existence of new attacks sequentially, and at time $t$ aims to achieve robustness against all attacks seen at times $\le t$.  The model is deployed at time $0$ to be robust against an initial set of attacks, new attacks are introduced at times $t_1$, $t_2$, and $t_3$. We propose to performing initial robust training when the first attack (or set of attacks) is available and then use fine-tuning to adapt the model against future attacks within time $\Delta t$.}
	\label{fig:overview}
	\vspace{-20pt}
\end{figure*}

%\arjun{Paragraph below needs a rewrite. Start with multi-robustness being important, and then its relation to the problem of CAR and why we should care about it. Then, propose CRT as a natural solution. But, then ask, how is it best done? That's where our theory and RCRT comes in.}

%Not only does multi-robustness introduce extra computational considerations, it is often unclear whether joint robustness is even possible for attacks with very different constraint sets. Most prior work has focused on training models that are jointly robust against different $\ell_p$ adversaries \citep{MainiWK20, TB19, madaan2020learning}. The critical assumption here is that all the attacks that need to be defended against are known \emph{a priori}. In this paper, we consider the scenario where models are trained to be multi-robust one attack at a time, a paradigm known as Continual Adaptive Robustness \citep{dai2024position}. This is of interest for two reasons: i) new attacks are likely be revealed over time requiring model trainers to have a training strategy that accounts for them; ii) continual training improves efficiency by obviating the need to train from scratch with all attacks for each new attack, and can leverage the benefits of residual robustness, if any. Thus, the key question we ask in this paper is:

%\begin{center}
%\emph{How can a model trainer effectively and efficiently adapt to new attacks?}
%\end{center}

%\textbf{Paragraph 2}: Perhaps a more practical scenario is where different attacks reveal themselves over time. even if you know all attacks, you can benefit from residual robustness and reduce overall training time, and also not hit capacity by trying to train in too many attacks at the same time. Given this, we consider the paradigm of CAR, where the defender trains a model to be robust one attack at a time. The key question we ask is: how can a defender efficiently and effectively adapt to new attacks?

\noindent \textbf{Theoretical improvements via regularization (\cref{subsec: theory}):} A natural question to ask is: what changes to the training procedure can we make to best perform CRT?  The key challenges here are to obtain robust representations before each new attack is introduced that provide as much robustness to unseen attacks as possible, and to fine-tune with minimal degradation in robustness against previous attacks. Our theoretical analysis shows that the difference in robust losses with respect to a pair of attacks is upper bounded by a function of the \textit{adversarial sensitivity} of a model's representation function: the maximal $\ell_2$ distance between the representations of a data point and a perturbed version of that data point, given the two adversarial constraints.
% \sophie{unclear what adversarial sensitivity means, might help to mention L2 to make the connection to next paragraph more clear} 
As a corollary, we show the difference between the robust loss over the union of two attacks and the benign loss has the same upper bound. These results motivate regularizing robust representations to avoid loss degradation against previous attacks and to have acceptable performance on benign data.

\textbf{Regularization Methods (\cref{subsec: methods}):} Our theoretical results motivate the use of regularization in CRT, and thus, we propose Regularized CRT (RCRT) using \emph{adversarial $\ell_2$ regularization} during both robust pre-training and fine-tuning. This regularization term, motivated by our theoretical results, promotes closeness in terms of Euclidean distance between adversarial and benign representations. We also consider two potential variants to improve efficiency and robustness: i) \emph{Random Noise Regularization:} the representations used are generated using \emph{random noise} instead of worst-case adversarial perturbations; ii) \emph{Variation Regularization \citep{dai2022formulating}:} the Euclidean distance between representations resulting from different instantiations of the adversarial perturbation is minimized.

\noindent \textbf{Empirical Findings (\cref{sec: results}):} We conduct experiments on hundreds of attack combinations on 3 vision datasets to rigorously determine the multi-robustness of models trained using RCRT.  Interestingly, we observe that when used in initial training, all variants of regularization generally improve robustness on new attacks.  For example, using regularization based on uniform noise, we can improve robust accuracy on a spatial attack (StAdv \citep{XiaoZ0HLS18}) from 0.79\% to 26.22\% while training only with $\ell_2$ attacks on CIFAR-10.  We further observe that variants of regularization using adversarial perturbations (adversarial $\ell_2$ and variation regularization) generally improve robustness when used during fine-tuning.  For example, when fine-tuning a model initially robust to $\ell_2$ attacks, using adversarial $\ell_2$ regularization can improve robustness on the snow attack~\citep{kaufmann2019testing} by 7.85\% on ImageNette \citep{imagenette} (from 27.64\% to 35.49\%).%We compare these models to those trained using 3 baselines for continual robust training and 3 for simultaneous multi-robust training from prior work. While no single method dominates in all settings, we find that RCRT provides a good balance between performance and efficiency compared to existing methods for CRT. In addition, we find that CRT methods in general approach or even surpass simultaneous multi-robust training, while being much more efficient. \arjun{Maybe add a couple of illustrative numbers here Sophie?}

\noindent \textbf{Looking ahead (\cref{sec: discussion}):} We show how CAR can be effectively achieved using iterative fine-tuning and regularization techniques. However, future work is necessary to better understand how interactions between sets of adversarial constraints impact the joint learnability of different attacks. We hope that our RCRT method will help practitioners ensure that their models remain robust against changing real-world threats, and may be adapted to ensure other desirable properties, such as compliance with changing standards for fairness or privacy. 
% \sophie{privacy and fairness definitions don't change over time as it does with adversarial examples}


%\textbf{Paragraph 3:} The simple answer to this question is to just pre-train with a specific attack and then fine-tune . However, the robust loss on different attacks for the same classifier, even if jointly trained on both attacks, can be quite different. To control how large this term is, we first prove that the difference in the robust loss for different attacks is bounded by the distance between of the learned robust features. 

%\arjun{The direct way to use the theory is to just consider the classifier trained jointly on both type of adversaries, and then the motivation for regularization is clear. But what would be nicer is to say that if you regularize in pre-training, then you reduce the difference in loss for any subsequent classifier}

%\arjun{Why do continual over joint in practice should be the only argument we make. We should not explicitly say that we are trying to beat joint, just get in the ballpark.}



%In this work, we study a new direction in adversarial robustness called \probname ~(\probabbrv) \sophie{add citation once position paper is up on arxiv}.  In \probabbrv, attacks are introduced in a sequential manner after the defender's model is deployed.  The goal of the defender is to quickly adapt the deployed model to be robust against the new attack while retaining robustness against all previous attacks in the sequence.

%To address this problem, we propose using an initial training and fine-tuning framework which involves first performing adversarial training with the initial attack and then fine-tuning for a few epochs on examples using new attacks once they are introduced (Figure \ref{fig:overview}).  We demonstrate that regularization is crucial for this problem \sophie{add more details here}

% Old text
%While existing machine learning (ML) models can achieve high accuracy, they are vulnerable to imperceptible perturbations in the input space called adversarial examples \citep{szegedy2013intriguing}.  These perturbations can be exploited by an attacker in order to cause misclassification, which may be harmful in safety critical areas.

%Prior works in adversarial robustness consider designing defenses for achieving robustness against a single attack type.  For example, many works propose defenses for achieving robustness against an attacker using a bounded $\ell_\infty$ or $\ell_2$ attack \citep{cohen2019certified, madry2017towards, zhang2019theoretically}.  However, these attacks can only capture a small portion of the space of possible perturbations; many works in attack literature have proposed other forms of attacks that are not captured by the $\ell_p$ threat model \citep{XiaoZ0HLS18, LaidlawF19, laidlaw2020perceptual, kaufmann2019testing}.  This motivates the design of a defense that can be efficiently adapted over time to new attacks.