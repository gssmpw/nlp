% In order to achieve a balance between efficiency and robust performance, we propose using an initial robust training and iterative robust fine-tuning pipeline which we call Continual Robust Training (CRT).  We study the impact of using regularization with CRT (RCRT), diagrammed in Figure \ref{fig:overview}.  In this pipeline, we perform (regularized) adversarial training in order to gain robustness against the initial space of known attacks and then utilize fine-tuning every time a new attack is introduced in order to achieve robustness against the new attack. Our approach of RCRT is most similar to that of \citet{croce2022adversarial}, which uses finetuning (without regularization) to gain robustness to unions of $\ell_p$ norms. However, we demonstrate that \textit{the use of regularization in both the initial adversarial training and fine-tuning steps can improve robust performance} and we expand the scope of our experimentation beyond pairs of $\ell_p$ bounded attacks.

% Finetuning (without regularization) for robustness to unions of $\ell_p$ norms has been utilized by \citet{croce2022adversarial}, and the conjunction of adversarial training on the initial set of attacks and \citet{croce2022adversarial}'s finetuning approach can be thought of as an instantiation of CRT.  We distinguish ourselves from this work by demonstrating that \textit{the use of regularization in both the initial adversarial training and finetuning steps can improve robust performance}.  In addition, we study the effectiveness of adapting to attacks outside of $\ell_p$ bounded attacks and look at longer sequences of attacks (\citet{croce2022adversarial} look at finetuning for sequences of 2 attacks).

% We will now show that there is theoretical justification for such an approach. In recent work, \citet{nern2023transfer} present theoretical results supporting the use of regularization in robust learning with a single adversary, showing that the maximum increase in loss incurred by adversarial perturbations is proportional to the adversarial sensitivity of a model's internal representations. Following Nern et al., adversarial sensitivity refers to the adversarial robustness of the representation function (i.e. $\max_{x' \in C(x)}\|g(x') - g(x)\|_2$).
% Building on this framework, we show that these results extend to the setting of multi-robustness, justifying RCRT as valid approach for continual adaptive robustness.

%To approach CAR, we propose Continual Robust Training (CRT), which involves initial robust training against a single attack followed by iterative robust fine-tuning against new attacks. 
% \arjun{Do we want to say something like: `For this to hold, the difference in robust loss with respect to different attacks must be small. We theoretically show...'}
% This dynamic would have the effect of reducing the gap between the adversarial losses of the previously seen attacks and the adversarial loss of the new attack at each iteration.
% We theoretically show that these gaps can be bounded above by a function of the adversarial sensitivity of a model's internal representation function. This suggests a form of regularization which directly minimizes adversarial sensitivity, thereby shrinking the adversarial loss gaps. 
%  Our theoretical results motivate a regularization term which we incorporate into CRT.
%We then propose regularized CRT (RCRT), incorporating a regularization term in both the initial training and fine-tuning losses to minimize the model's sensitivity to each attack, as an enhanced approach for achieving CAR.

% \arjun{These two paragraphs can be shrunk a fair bit: start by saying we approach CAR by proposing CRT. But then we ask, how to improve it? We want to bound the change in loss between different constraints to prevent forgetting etc. We show this change is bounded by representation sensitivity to adversarial perturbations, building on Nern et al, so we ultimately we propose RCRT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% We now present justification for our use of regularization techniques to achieve continual adaptive robustness. 

%It is not immediately clear whether it is possible to train a model that is suitably robust to both types of attack. We provide a theoretical intuition for why joint robustness of this form is achievable.
In this section, we introduce continual robust training (CRT) and provide theoretical results to demonstrate that adding a regularization term bounding adversarial logit distances can help balance performance across a set of adversaries.
\subsection{Continual Robust Training}
We now introduce continual robust training (CRT).  CRT consists of 2 parts, \textit{initial training} and \textit{iterative fine-tuning} (Figure \ref{fig:overview}).  The output of initial training is deployed at $t=0$ while fine-tuning is used as new attacks are introduced.

% \anote{Okay but why can CRT meet the defender's goals? The fine-tuning reduces $\Delta t$, and can satisfy the $\delta_{\text{known}}$ condition given enough time.}

At time $t=0$, the goal of the defender is to minimize the initial training objective: $\mathcal{L}(h,0) = \frac{1}{m}\sum_{i=1}^m\ell(h(P_{C_\text{init}}(x_i, y_i, h)), y_i)$
where $\{(x_i, y_i)\}_{i=1}^m$ is the training dataset and $P_{C_\text{init}}$ is the initial attack. 
Notably, using standard training in this stage yields a high $\delta_\text{unknown}$.
% The goal of the defender is to minimize $L_{\text{init}}$ in initial training.

At $t>0$, as new attacks are introduced, we use a fine-tuning strategy $F$ to select the attack from $K(t)$ to use for each example.  Specifically, we formulate this as:
$    \mathcal{L}(h, t) = \frac{1}{m}\sum_{i=1}^m \ell(h(P_{C}(x_i, y_i, h)), y_i)$ where $P_{C}= F(K(t), (x_i, y_i))$.
Fine-tuning strategies include picking the attack that maximizes $\ell(x_i, y_i)$, randomly sampling from $K(t)$, and using the newest attack.  A good fine-tuning strategy would be able to quickly adapt the model to new attacks, allowing it to satisfy a small $\Delta t$ threshold.
However, naive fine-tuning does not guarantee good performance across all attacks and may require large values of $\delta_\text{known}$. 
% \anote{Cast this in terms of the requirements: too much degradation will cause an unacceptable $\delta_{\text{known}}$} 
As illustrated in \cref{fig:summary}, a model may lose robustness to the initial attack after the fine-tuning stage. We now discuss how such degradation can be addressed through regularization.
% \sophie{point to \cref{fig:summary}, naively doing CRT doesn't guarantee good robust performance for CAR - poor generalization to unforeseen attacks, forgetting of initial attack. Then connect with next section.}

\subsection{Bounding the difference in adversarial losses}\label{subsec: theory}
% \anote{Motivate the terms we consider: we want L1-L2 to be small for two reasons: some unforeseen robustness, which helps meet the $\delta_{\text{unknown}}$ requirement; and prevent degradation on known attacks, which helps meet the $\delta_{\text{known}}$ requirement}

In order for CRT to be a practical solution for CAR, it is important that CRT enhances robustness to new attacks without losing robustness against attacks we have already learned. 
We now relate the gap in robust loss between two attacks to how far each attack can perturb the logits for a given example,
which suggests that regularization in CRT may improve decrease the drop in robustness across attacks. 
%Such a bound can be useful in designing new defenses, as it could correspond to an upper bound on the increase in adversarial loss on a previously learned attack after a new attack is introduced.
%Ideally, we would want to ensure not only that the gap between the losses is minimal, but also that the individual losses are small in absolute terms~\cite{yin2019rademacher}. While our results do not directly ensure low individual losses, they hold for any model that follows our assumptions, including those that perform well against both attacks. Moreover, they suggest that a model whose \edit{logits} are highly sensitive to perturbations will not perform well against both attacks.

% Let $h:\mathbb{R}^d \rightarrow \mathbb{R}^c$ be a $c$ class neural network classification model with a final linear layer (i.e. $h(x) = Wg(x)$, where $g: \mathbb{R}^d \rightarrow \mathbb{R}^r$ is a representation function and $W \in \mathbb{R}^{c \times r}$). 
\edit{Let $h:\mathbb{R}^d \rightarrow \mathbb{R}^k$ be a $k$ class neural network classification model.}
To simplify the problem, we focus on the state of the model when attacks $P_{C_1}$ and $P_{C_2}$ (with corresponding adversarial constraints $C_1$ and $C_2$) are known to the defender. %, 
%although the bounds we derive hold for any model of the above form.
Consider the following two adversarial loss functions:
$\mathcal{L}_1(h) := \mathbb{E}_{\mathcal{D}}\left[\ell(h(P_{C_1}(x,y)),y)\right]$
and
$\mathcal{L}_2(h) := \mathbb{E}_{\mathcal{D}}\left[\ell(h(P_{C_2}(x,y)),y)\right].$
Without loss of generality, assume that $\mathcal{L}_1(h)
\geq \mathcal{L}_2(h)$. We can then bound the difference between $\mathcal{L}_1(h)$ and $\mathcal{L}_2(h)$, \edit{adapting a result from}~\citet{nern2023transfer}, as follows\footnote{\edit{As stated, these results hold for loss functions that are Lipschitz with respect to the $\ell_2$ norm. We note that similar bounds can be derived for other norms by applying a constant scaling factor to the first term of the bound (i.e. for losses Lipschitz with respect to the $\ell_1$ norm, the scaling factor would be $\sqrt{c}$).}}:
\vspace{-3pt}
%We arrive at the following theorem, building on Nern \textit{et. al.}~\cite{nern2023transfer}, bounding the difference between $\mathcal{L}_1(h)$ and $\mathcal{L}_2(h)$:

%In order for CRT to be effective, it must be possible for a model to achieve acceptable loss on multiple attacks. Therefore, we demonstrate theoretically that the loss of a model on additional attacks is bounded by an objective that can be directly minimized. 

%For ease of notation, we will refer to $P_1$ as $\psi$ and $P_2$ as $\omega$.

% , which mirrors Theorem 1 from \cite{nern2023transfer} \sophie{do we need citation here?}:
\begin{theorem}
    \label{thm:robustness}
    Assume that loss $\ell(\hat{y},y)$ is $M_1$-Lipschitz in $\|\cdot\|_2$, for $\hat{y} \in h(X)$ with $M_1 > 0$ and bounded by $M_2 > 0$ \footnote{We note that surrogate losses such as the cross-entropy used during training are not bounded, but the $0-1$ loss which is often the key quantity of interest \emph{is bounded}.}, i.e. $0 \leq \ell(\hat{y},y) \leq M_2$ $\forall \hat{y} \in h(X)$. Then, for a subset $\mathbb{X} = \{x_i\}_{i=1}^n$ independently drawn from $\mathcal{D}$, the following holds with probability at least $1-\rho$:
    \begin{align*}
        \mathcal{L}_1(h) - \mathcal{L}_2(h) \leq \;&M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 \\
        &+ \max_{x' \in C_2(x_i)}\|h(x') - h(x_i)\|_2\biggl) + D,
    \end{align*}
    where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$. 
\end{theorem}
This result suggests that regularization with respect to a single attack (say, in pre-training) will give the model greater resiliency against unforeseen attacks and help meet the $\delta_\text{unknown}$ threshold. Using regularization when fine-tuning on a new attack could also prevent degradations in robustness against previously seen attacks, helping to meet the $\delta_\text{known}$ threshold.
Using similar reasoning, we can also bound the gap between Union and clean loss:
\begin{corollary}
\label{thm:corollary}
Let $\mathcal{L}_{1,2}(h) := \mathbb{E}_{\mathcal{D}}\left[\max{(\ell(h(P_{C_1}(x,y,h)),y),\ell(h(P_{C_2}(x,y,h)),y)})\right]$. Then, with probability at least $1 - \rho$,
\begin{align*}
        \mathcal{L}_{1,2}&(h) - \mathcal{L}(h) 
        \leq M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2 \\
        &+ \max_{x' \in C_2(x_i)}\|h(x') - h(x_i)\|_2\biggl) + D.
    \end{align*}
\end{corollary}
This corollary helps characterize the trade-off between clean and robust loss in our setting. Proofs of Theorem \ref{thm:robustness} and Corollary \ref{thm:corollary} are present in Appendix \ref{sec:proof}.
% \begin{theorem}
%     \label{thm:robustness}
%     Assume that loss $\ell(\hat{y},y)$ is $M_1$-Lipschitz in $\|\cdot\|_\alpha$ for $\alpha \in \{1,2,\infty\}$, for $\hat{y} \in h(X)$ with $M_1 > 0$ and bounded by $M_2 > 0$, i.e. $0 \leq \ell(\hat{y},y) \leq M_2$ $\forall \hat{y} \in h(X)$. Then, for a subset $\mathbb{X} = \{x_i\}_{i=1}^n$ independently drawn from $\mathcal{D}$, the following holds with probability at least $1-\rho$:
%     \begin{align*}
%         \mathcal{L}_1&(h) - \mathcal{L}_2(h) \\
%         &\leq L_\alpha(W)M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|g(x') - g(x_i)\|_2 \\
%         &+ \max_{x' \in C_2(x_i)}\|g(x') - g(x_i)\|_2\biggl) + D,
%     \end{align*}
%     % \begin{align*}
%     %     \mathcal{L}_\psi(f_{W,\theta}) &- \mathcal{L}_\omega(f_{W,\theta}) \\
%     %     &\leq L_\alpha(W)C_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{\|\delta_\psi\|_\psi \leq \epsilon_\psi}\|g_\theta(x_i + \delta_{\psi}) - g_\theta(x_i)\|_2 \\
%     %     &+ \max_{\|\delta_\omega\|_\omega \leq \epsilon_\omega}\|g_\theta(x_i + \delta_{\omega}) - g_\theta(x_i)\|_2\biggl) + C_2\sqrt{\frac{\log(\rho/2)}{-2n}},
%     % \end{align*}
%     where $D = M_2\sqrt{\frac{\log(\rho/2)}{-2n}}$ and
%     \[
%     L_\alpha(W) := \begin{cases}
%     \|W\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_2, \\
%     \sum_i\|W_i\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_1, \\
%     \max_i\|W_i\|_2 &, \text{if } \|\cdot\|_\alpha = \|\cdot\|_\infty. 
%     \end{cases}
%     \]
% \end{theorem}

%As a corollary, we can bound the difference in the adversarial loss over the union of two attacks and the benign loss:

%From this result, we can derive a similar bound for the difference in loss on the union of two attacks and loss on unperturbed data:
% \begin{corollary}
% \label{thm:corollary}
% Let $\mathcal{L}_{1,2}(h) := \mathbb{E}_{\mathcal{D}}\left[\max{(\ell(h(P_{C_1}(x,y,h)),y),\ell(h(P_{C_2}(x,y,h)),y)})\right]$. Then, with probability at least $1 - \rho$,
% \begin{align*}
%         \mathcal{L}_{1,2}&(h) - \mathcal{L}(h) \\
%         &\leq L_\alpha(W)M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|g(x') - g(x_i)\|_2 \\
%         &+ \max_{x' \in C_2(x_i)}\|g(x') - g(x_i)\|_2\biggl) + D.
%     \end{align*}
% \end{corollary}

%\begin{corollary}
%\label{thm:corollary}
%Let $\mathcal{L}_{1,2}(h) := \mathbb{E}_{\mathcal{D}}\left[\max{(\ell(h(P_{C_1}(x,y,h)),y),\ell(h(P_{C_2}(x,y,h)),y)})\right]$. Then, with probability at least $1 - \rho$,
%\begin{align*}
%        \mathcal{L}_{1,2}(h) - \mathcal{L}(h) \leq \;&\edit{M_1 \frac{1}{n}\sum_{i=1}^n\biggl(\max_{x' \in C_1(x_i)}\|h(x') - h(x_i)\|_2} \\
%        &\edit{+ \max_{x' \in C_2(x_i)}\|h(x') - h(x_i)\|_2\biggl)} + D.
%    \end{align*}
%\end{corollary}

%\edit{An additional \cref{corollary: deeper_reps} extends these results to bounds in terms of the difference of penultimate layer representations (see Appendix~\ref{sec:proof})}. Put simply, Theorem~\ref{thm:robustness} shows that the gap between $\mathcal{L}_1(h)$ and $\mathcal{L}_2(h)$ is upper bounded by the \textit{adversarial sensitivity} ($\ell_2$ distance between benign and perturbed \edit{logits}) of the model to perturbations satisfying the constraint sets $C_1$ and $C_2$ 
%Corollary~\ref{thm:corollary} shows that the gap between the union robust loss ($\mathcal{L}_{1,2}$) and the benign loss is bounded above by the same term. 

% Our results point towards regularization as a promising approach for achieving robustness to multiple perturbations. 
%In principle, regularization can reduce the distances between learned robust representations for different attack types, directly minimizing the upper bound in the theorem statement. We further discuss regularization in the next section. 

\noindent \textbf{Comparison to \citet{dai2022formulating}: } We note that \citet[Theorem 4.2]{dai2022formulating} derive a related bound on the adversarial loss gap between two attacks in the context of UAR. However, their formulation assumes that the constraint set of the target attack is a strict superset of that of the source attack, whereas we make no assumptions about the relationship between the two constraint sets.
%\edit{
%\begin{remark}
   
%\end{remark}
%}
% Theorem~\ref{thm:robustness} therefore suggests that regularization will decrease the upper bound on the pairwise difference in performance on multiple attacks. Corollary~\ref{thm:corollary} implies this will also hold for the model's performance on the union of two attacks and its performance on benign examples.


% \christian{Maybe remove rest of this paragraph}
% In removing this restriction, our bounds more directly imply that regularization will reduce the robust loss gap both during pretraining (i.e. regularizing just with respect to attack 1) and during fine-tuning to a previously unknown attack (i.e. simultaneously regularizing with respect to attacks 1 and 2). 
% From Theorem~\ref{thm:robustness}, we see that when using a $M$-Lipschitz loss function 
% (which includes common loss functions like softmax cross-entropy loss), the pairwise 
% difference between adversarial losses is bounded above by a function 
% of $g$'s robustness to each of the perturbations, as
% well as the norm of the final linear layer $W$. 
%Beyond providing bounds on the gap in adversarial robustness, this finding suggests a framework for understanding how close existing training methods are to achieving optimal joint robustness.
% This theoretical result points towards regularization as a promising approach for achieving robustness to multiple perturbations. \sophie{needs more intuition here}

% \begin{table}[]
% \centering
% \begin{tabular}{@{}lllll@{}}
% \toprule
%                   & $P_1$ acc & $P_2$ acc & $\ell_2$ distance & CKA   \\ \midrule
% $\ell_2$ (Single) & 0.726        & 0.018     & 439,479.13         & 0.445 \\ \midrule
% MAX (Joint)       & 0.637        & 0.544     & 103,696.29         & 0.958 \\
% AVG (Joint)       & 0.649        & 0.497     & 116,405.75         & 0.930 \\
% RAND (Joint)      & 0.645        & 0.504     & 117,666.86         & 0.947 \\ \bottomrule
% \end{tabular}
% \label{tab:similarity}
% \caption{\christian{Preliminary results} Robust accuracy, $\ell_2$ distance between robust representations of $P_1$ and $P_2$, and CKA similarity between robust representations of $P_1$ and $P_2$ for multi-robust training methods. Here, the size of the adaptive knowledge set is 2, $P_1$ represents an $\ell_2$ attack with $\epsilon = 0.5$ and $P_2$ represents a spatial attack with $\epsilon = 0.05$.}
% \end{table}

% Furthermore, this distance appears roughly correlated with the CKA similarity between the two representations, supporting the assumption that multi-robust training is capable of enforcing similarity between representations of different perturbation types in the same way that normal adversarial training is capable of enforcing similarity between adversarial and benign representations. 
% Lastly, the lowest $\ell_2$ distance and highest CKA similarity is seen in the model that has the smallest gap between $P_1$ and $P_2$ accuracy, providing reassurance that the relevant metrics from our theoretical results and prior work on adversarial robustness correlate with meaningful downstream metrics in the \probabbrv\ setting.

% \sophie{should also discuss a comparison to theoretical results in \citet{dai2022formulating}.}