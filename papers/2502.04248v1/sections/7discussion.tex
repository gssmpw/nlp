This work makes early progress towards deployable defenses that mitigate model obsolescence in the face of evolving adversaries. Such approaches could promote the adoption of robust models, as they allow model trainers to `patch' against vulnerabilities without training from scratch. 


% towards creating defenses which can be adapted to new attack types. We propose a defense framework called continual robust training (CRT) and a theoretically-motivated regularization term for improving performance against an evolving set of attacks. Our method achieves good performance across known attacks while being more efficient than training from scratch.% The effectiveness of regularization is motivated by our theoretical results relating the change in robust loss for two attacks to the distances between their corresponding adversarially perturbed logits.



\noindent\textbf{Related Work:} Prior works investigate multiattack robustness (MAR) \citep{MainiWK20,TB19,madaan2020learning,Croce020} and unforeseen attack robustness \citep{laidlaw2020perceptual,zhang2018lpips,dai2022formulating,jin2020manifold,dai2023multirobustbench}. Unlike these methods, we assume that the defender may not know all attacks \emph{a priori} but adjusts their model as new attacks emerge. \citet{croce2022adversarial} propose a fine-tuning method for MAR on unions of $\ell_p$ attacks. Our work differs by exploring additional attack types (\emph{e.g.} spatial attacks \citep{XiaoZ0HLS18} and color shifts \citep{LaidlawF19}) and improvements to the initial training stage prior to fine-tuning. Additional related work is discussed in \cref{appsec: add_rel_work}.

\noindent\textbf{Limitations:} 
% \sophie{make limitations more clear, not always the best performing out of prior work, understanding connections between threat models.} In this work, we experimented with using existing robust training and fine-tuning techniques in CRT.  Further research on robust training methods can improve the performance of our framework. Our thoughts on how to address these limitations are in Appendix \ref{appsec:future_directions}.
More work is needed to improve the performance of RCRT, as our approach does not outperform existing baselines in all settings. It also remains unclear whether training from scratch with all attacks or fine-tuning on new attacks is optimal from both a theoretical and empirical perspective. Future theoretical work could characterize the convergence rates of each approach, as well as the gap in robustness between models at different stages in CRT. Further limitations and future directions are discussed in Appendix~\ref{appsec:future_directions}.

%Experimentally, due to the limited study of sMAR, we compared to AVG, MAX, and random baselines in prior work \citep{TB19, madaan2020learning}.  Future work may develop better multi-robust training methods which would require us to revisit our baselines.


%\noindent\textbf{Broader Impact:} The defense framework proposed can be useful for safety in practical, high-risk applications of supervised machine learning such as autonomous vehicles \citep{tencent2019experimental, 272270, 291108}, content moderation \citep{ye2023noisyhate,schaffner2024community}, and face authentication \citep{komkov2021advhat, wei2022adversarial} and provides first steps towards training and updating models in order to maintain robustness over time.  However, there are cases in which adversarial examples are used for good (\textit{e.g.} defending against website fingerprinting \citep{rahman2020mockingbird,shan2021patch}) which may be adversely affected by models robust to adversarial examples, including our proposed approach.