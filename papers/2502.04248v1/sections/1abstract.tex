% New abstract
%Adversarial training typically only provides protection against adversarial examples defined with respect to a given threat model. Attacks generated using different constraints can easily evade adversary-specific defenses. Several techniques have been developed to train models that are robust to multiple types of adversarial attacks, a quality we call multi-robustness for short. However, they offer little guidance on how to adapt models in the face of new adversaries.
%Adversarial training typically provides robustness only against adversarial examples defined by a specific attack type, such as an $\ell_p$ attack with a fixed budget. However, over time, defenders may encounter new attack types that were not accounted for during training. 
Robust training methods typically defend against specific attack types, such as $\ell_p$ attacks with fixed budgets, and rarely account for the fact that defenders may encounter new attacks over time.  A natural solution is to adapt the defended model to new adversaries as they arise via fine-tuning, a method which we call continual robust training (CRT).  However, when implemented naively, fine-tuning on new attacks degrades robustness on previous attacks.  This raises the question: \textit{how can we improve the initial training and fine-tuning of the model to simultaneously achieve robustness against previous and new attacks?} We present theoretical results which show that the gap in a model's robustness against different attacks is bounded by how far each attack perturbs a sample in the model's logit space, suggesting that regularizing with respect to this logit space distance can help maintain robustness against previous attacks.
Extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and over 100 attack combinations demonstrate that the proposed regularization improves robust accuracy with little overhead in training time. Our findings and open-source code\footnote{Our code is available at: \url{https://github.com/inspire-group/continual_robust_training/}} lay the groundwork for the deployment of models robust to evolving attacks.
%We present a new multi-robust training framework, called regularized continual robust training, which uses appropriately regularized fine-tuning to update models to be robust to new attacks. When naively implemented, fine-tuning can result in the forgetting of previous attacks. To counter this, we present theoretical results which show that the gap in a model's robustness against different attacks is related to how far each attack can perturb a sample in the model's \edit{logit} space. This finding informs the regularization term we adopt to maintain robustness to multiple attacks. We demonstrate the benefits of using regularization through extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and hundreds of attack combinations. We find that regularized continual robust training achieves a good balance between performance and efficiency, although neither it nor several baselines from prior work we compare to dominate in all settings. Our findings and open-source code lay the groundwork for the deployment of models robust to evolving attacks.

% Old abstract
% Since many types of adversarial perturbations exist (\textit{e.g.} $\ell_p$ perturbations, spatial perturbations and so on), it is essential for models to be robust against multiple attacks, a property we call \textit{multi-robustness}.  Prior work on multi-robustness assumes that all attacks are initially known; however, in practice, new attacks emerge over time. Thus, it is critical for defenses to consider a dynamic setting for multi-robustness, known as \textit{continual adaptive robustness} (CAR), in which a model can be continually updated to be robust against emerging test-time attacks.
% %Multi-robustness, the property that models should be robust to multiple types of adversarial examples simultaneously, is key to the widespread adoption of robust models given the wide array of possible attacks. Since new attacks can be introduced over time, the dynamic setting for multi-robustness, known as continual adaptive robustness (CAR) is critical to consider. In this setting, a model can be continually updated to be robust against emerging test-time attacks. % In applications such as image classification, there is ongoing research on modeling different types of adversarial perturbations (ie. $\ell_p$ attacks, adversarial spatial transformations).  
% %Multi-robustness, the property that models should be robust to multiple types of adversarial examples (ie. L2 and Linf) simultaneously, is key to the widespread adoption of robust models especially as attacks evolve over time. While prior works generally study the setting where the model is fixed after deployment, this paper focuses on the dynamic setting, known as continual adaptive robustness (CAR), in which a model can be updated after deployment for robustness against emerging test-time attacks in a continual fashion. \sophie{first 2 sentences can be a bit confusing, lead with emerging attack types sentence first to motivate CAR, don't need to first introduce multi-robustness and situate along this line of works}
% To achieve CAR, we adopt a training framework that we call continual robust training: start with a model robust to a single attack and iteratively fine-tune it to be robust to new attacks. We theoretically demonstrate that the difference in adversarial losses for a pair of attacks is upper bounded by the sensitivity of a model's internal representation function to the perturbations introduced by each attack. Consequently, we propose the use of regularization during both initial robust training and iterative robust fine-tuning to minimize the $\ell_2$ distance between representations from different attacks for robust models. %Our comprehensive experiments compare standard multi-robustness metrics for our method and 6 previous approaches across 6 attack sequence combinations on 3 computer vision datasets.
% We demonstrate the benefits of using regularization through extensive experiments on 3 datasets (CIFAR-10, CIFAR-100, and ImageNette) and hundreds of attack combinations. We find that regularized continual robust training achieves a good balance between performance and efficiency, although neither it nor several baselines from prior work we compare to dominate in all settings. Our findings lay the groundwork for the deployment of models robust to evolving attacks.

% %\st{We carry out a detailed investigation of how a model can be sequentially adapted to be robust against new attacks by starting with a model robust to a single attack and following up with continual robust fine-tuning, a framework we call continual robust training (CRT). }