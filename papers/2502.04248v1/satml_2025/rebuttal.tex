% \documentclass{article}
\documentclass[conference]{IEEEtran}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage{amsfonts}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\usepackage{enumitem}
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}
\renewcommand\labelitemi{-}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
% \usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\title{Rebuttal for \#22: Towards Multi-robust Models via Regularized Continual Robust Training}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}

\maketitle

\section*{Global Response}
We thank all reviewers for their insightful feedback on our paper.  We are encouraged that the reviewers find the problem of maintaining and updating robustness as new attacks emerge important (A, C) and well-motivated (D), find our proposed regularization theoretically grounded with solid mathematical support (A), and appreciate the large scope of experiments (B, C).  We have updated the paper to incorporate suggestions from reviewers.  In the updated paper we have made the following edits (all edits are highlighted in \textcolor{red}{red} in the updated paper):
\begin{itemize}[leftmargin=*]
\item We have moved Fig. 3 sub-figures for Variation Regularization and Gaussian regularization to the appendix to make room for more discussions
\item Expanded the discussion section to discuss the how more research on understanding relationships between different attack types and reducing catastrophic forgetting can help improve the performance of CAR (A) and discuss additional steps for bridging the gap with practical application (C)
\item Added a paragraph discussion “Impact of dataset” to Section IV-C (B)
\item Updated theorem statement to be based on distances in the logit space to be consistent with experimental results and moved theoretical results for the feature space to the Appendix. (B)
\item Updated Figure 2 to show distance between logits, rather than distance between internal representations
\item Clarified the definition of evaluation metrics in section IV-A (D)
\item Clarified the distinction between this work and \citet{dai2024position} in Section II-A (D)
\end{itemize}

\section*{Response to Reviewer A}
\noindent\textbf{Choice of initial attack: }For the initial attack in the sequence, we chose to start with $L_2$ or $L_\infty$ attacks because many defenses in adversarial ML focus primarily on defending against these attacks \citep{croce2020robustbench}.  Since these attacks are commonly considered, we thought it would be fitting for these attacks to be the initially known attacks, while other attacks such as StAdv and ReColor to be “new” attacks introduced after model deployment.  We agree that the choice of the initial attack is an interesting problem in CAR and understanding how the choice of initial attack impacts CRT is an interesting future direction.  We note that our main theorem and corollaries are agnostic to which attack is considered the initial attack and which is considered the new attack, so it does not give guidance on how to choose the initial attack.  We have added discussion of the choice of initial attack to the “Limitations and future directions” paragraph of Section V.

\noindent\textbf{Application to non-$L_p$ norm based attacks:
}Yes, ALR is applicable to non-$L_p$ norm based attacks; note that the regularization is applied to the representation space and not the input space.  This regularization term can be thought of as finding a perturbation (subject to the attack constraint) which maximizes the $\ell_2$ distance between the \emph{representation of the perturbed image} and the \emph{representation of the clean image}. The attack constraint itself does not need to be an $L_p$ constraint. For StAdv \citep{XiaoZ0HLS18} and ReColor\citep{LaidlawF19} (and attacks from the UAR benchmark~\citep{kaufmann2019testing}), attack examples are generated by using gradient-based optimization to maximize cross-entropy loss; we just replace the cross-entropy part of the objective with the $\ell_2$ distance between representations and use the same gradient-based optimization approach to maximize this updated objective.  We have added these details about computing regularization for non-$L_p$ attacks into Appendix C.  We note that in Figure 3, we show results for fine-tuning from models initially trained with regularization for a wide variety of attack types.

\noindent\textbf{Catastrophic forgetting: }We agree that additional research on reducing catastrophic forgetting can help with this setting. In some initial experiments for this project, we tried incorporating EWC \citep{kirkpatrick2017overcoming} into the training setup, but we found that EWC did not help for this setting and can hurt the robust accuracy of the final model. Interestingly, in our experiments, we find that ALR can reduce catastrophic forgetting to some extent (See paragraph on “Regularization can reduce forgetting” in Section IV-C).  In our experiments, we also attempt to reduce forgetting by combining ALR with the fine-tuning approach from \citet{croce2022adversarial} which uses replay of previously seen attacks.  We have added discussion of improving finetuning methods and regularization to reduce catastrophic forgetting into the “Limitations and future directions” paragraph of Section V. 

\section*{Response to Reviewer B}
\noindent\textbf{Discrepancy between theory and proposed method:} We are glad to have the opportunity to discuss and rectify the inconsistency between our theory and proposed empirical method. We saw value in aligning our notation and results with \citet{nern2023transfer}, as it both shows the similarity between singly-robust and multi-robust learning, and shows how our results can be useful in settings where the transfer learning paradigm is employed. However, we appreciate that this has introduced a discrepancy between our theoretical results and our proposed regularization term. To clarify this discrepancy, we note that regularizing with respect to the logit layer is still consistent with our theoretical results. The result for the logit layer is implied by an intermediate step in the proof of (the older) Theorem III.1. We can show that 
\begin{align*}
\mathcal{L}_1&(h) - \mathcal{L}_2(h) \\
&\leq M_1\frac{1}{n}\sum_{i=1}^{n} \| h(x_i') - h(x_i'')\|_2 + M_2 \sqrt{\frac{\log(\rho/2)}{-2n}},
\end{align*}
where $h(\cdot)$ is the output of the logit layer.
Using a similar invocation of the triangle inequality as used in Appendix B, we can arrive at
\begin{align*}\mathcal{L}_1&(h) - \mathcal{L}_2(h)\\ 
&\leq M_1\frac{1}{n}\sum_{i=1}^{n} \left(\max_{x' \in C_1(x)}\| h(x') - h(x)\|_2 \right.\\&+ \left.\max_{x'' \in C_2(x)}\| h(x'') - h(x)\|_2\right) + M_2 \sqrt{\frac{\log(\rho/2)}{-2n}}.\end{align*}

It is not immediately clear whether this bound is tighter than the one we initially provided, but it leads to the same conclusion about the efficacy of regularizing with respect to the internal representation layer as it does with respect to the logit layer. We note this formulation now assumes that the loss function is Lipschitz with respect to the $\ell_2$ norm, rather than any of the $\ell_1$, $\ell_2$, or $\ell_\infty$ norms. This is largely a notational convenience, as this bound can be adapted to losses that are Lipschitz with respect to other norms by scaling the first term of the bound by a constant factor (i.e. $\sqrt{c}$ for losses Lipschitz w.r.t. the $\ell_1$ norm).

We have updated the theorem and corollary statements along with the accompanying proofs in the Appendix to reference logit distances rather than internal representation distances. We have moved the original theorem statement referencing representation distances and its accompanying proof to Appendix B as Corollary A.1. for those interested in the applicability of our results to the transfer learning setting.

\noindent\textbf{VR vs ALR:} The relevant term derived within the proof of Theorem 3.1 is subtly different from the term in the VR definition. Within the proof, the term is of the form:
$$\|h(x') - h(x'')\|_2.$$
Here, $x'$ represents an input perturbed by attack 1 (i.e. $x' \in C_1(x)$) and $x''$ represents an input perturbed by attack 2 (i.e. $x'' \in C_2(x)$). In the definition of VR, we instead have a term of the form: 
$$\max_{x',x'' \in C(x)}\|h(x') - h(x'')\|_2.$$
Here, both $x'$ and $x''$ are perturbed using the \emph{same attack}. This highlights an important implication of Theorem III.1, which is that regularizing with respect to each attack independently can reduce the upper bound on the adversarial loss gap. This is significant in our setting, where we assume that the set of known attacks can change over time.

%\textcolor{red}{Discuss the relationship between VAR and ALR, and how ALR is a lower bound for VAR (not the other way around).}
In terms of the relationship between VR and ALR, we note that ALR is a \textit{lower bound} for VR.  This is because by definition, VAR optimizes for 2 perturbations to maximize the distance between the features/logits ($\max_{x',x'' \in C(x)}\|g(x') - g(x'')\|_2$), while ALR maximizes the distance in features/logit between a perturbed input and the clean image ($\max_{x' \in C(x)}\|g(x') - g(x)\|_2$).  Since the clean image lies in the perturbation space $x \in C(x)$, $\text{VAR} \ge \text{ALR}$.

\noindent\textbf{Relationship to Nern et al.:} We agree that our theoretical results are adapted from Nern et al., and we have updated our terminology to make that more clear in Section III.

\noindent\textbf{Loss function assumptions:} 
Thank you for pointing out that the bounded loss assumption does not hold for the cross-entropy loss typically used for training. We have added this clarification in Section III. The boundedness assumption allows the of use concentration inequalities to bound the population loss with a finite sample sum, as commonly done to provide generalization bounds on adversarial risk~\cite{yin2019rademacher}. We further note if $\ell(\cdot)$ is considered to be the $0-1$ loss, which is bounded, the theorem holds. While the $0-1$ loss is not typically used for training due to well-known hardness results~\cite{shalev2014understanding}, we do report our final results in terms of it (since accuracy is $1- \ell_{0-1}(\cdot)$). Thus, it makes intuitive sense to use ALR during training as it bounds the quantity of final interest, even though a surrogate loss is used during training. 

\noindent\textbf{Results on other datasets: }We agree with the reviewer that results on other datasets are important, but it is difficult to move the tables for other datasets in the appendix into the main body due to space limits.  Currently, we mention the difference in performance on other datasets in Section IV-C at the end of the paragraph on “Regularized CRT can outperform training from scratch for multiattack robustness.”  To further highlight results on other datasets and discuss the differences in performance for known attacks, we have added a paragraph “Impact of dataset” to Section IV-C.

\noindent\textbf{Full ImageNet results: }
We note that most works in adversarial training \citep{croce2020robustbench} (especially those targeting multirobustness \citep{dai2023multirobustbench}) do not present results on full ImageNet since it is computationally expensive to train.  In our paper, we present results for ImageNette and CIFAR-100 along with CIFAR-10 so we can ablate over the impact of higher image resolution (ImageNette) and more classes (CIFAR-100).

\noindent\textbf{Runtime fluctuations: }
We find runtime trends quite consistent (FT+ALR is always slower than the base FT approach due to the incorporation of ALR regularization) and are unsure what the reviewer means by runtime fluctuations.  If the reviewer is referring to the difference in measured times between time steps in the table, this is natural since the set of attacks used during fine-tuning changes between time steps and some attacks are slower to compute than others.

\noindent\textbf{Fine-tuning details: }
For fine-tuning, we maintain a learning rate of 0.001.  Details about learning rates for training/fine-tuning, learning rate scheduler for training, and details about attack generation and early stopping are in Appendix C.

\section*{Response to Reviewer C}
\noindent\textbf{Longer term fine-tuning: }
We thank the reviewer for this suggestion as longer term fine-tuning is important for practical applications.  One aspect that we did not explore in this work is the connection between model capacity and CAR; current works in adversarial robustness literature demonstrate that adversarially robust models need higher model capacity \citep{madry2017towards, gowal2020uncovering}. As we increase the space of attacks we want to defend against, we may need to increase the capacity of the model in order to achieve robustness against the expanding space \citep{dai2024characterizing}.  An interesting future direction is looking at the connection between model capacity and CAR and seeing if adding more parameters to the network during fine-tuning (such as using adapters \citep{rebuffirevisiting}) can be used to address the issue of model capacity allowing for longer term robustness.

\noindent\textbf{Discussion of steps towards practical application: 
}We thank the reviewer for this suggestion and discuss several directions that should be explored further to bridge the gap with practical application:

\noindent\textit{Monitoring for new attacks:}  One limitation of this work is that we focus mainly on designing an algorithm which allows us to adapt an existing model to new attack types given that we have a monitoring system in place for identifying new attack types that we want to be robust to.  Thus, for practical application, it is also important to design a robust monitoring system for detecting when new attack types arise.

\noindent\textit{Understanding attack similarities:} In practical applications, it may be the case that some new attacks may be very different from previous attacks seen in the sequence.  For example, perhaps in training all known attacks are small imperceptible perturbations, but then we want to adapt the model to also be robust against patch attacks \citep{brown2017adversarial}, which allow for unbounded perturbations in a small region of the image.  In this case, the approach may not lead to good performance since likely the features that we learned by the model have no robustness to patch attacks, so fine-tuning would not be sufficient for achieving good robustness against the new attack.  Thus, when attacks are too different from each other, we may need to use joint training from scratch with the new attack or combine the approach with defenses tailored towards the new attack.  For practical applications, it may also be useful to have a way of classifying what attacks are considered ``similar” enough to the set of known attacks (perhaps using the representation similiarity approach from~\cite{cianfarani2022understanding}) and can be useful for deciding when it’s necessary to train from scratch.

\noindent\textit{Expanding model capacity:} Larger capacity models may be necessary as the space of attacks increases.  Please refer to the previous discussion on “Longer term fine-tuning” for more details.

We have added these discussions into the “Limitations and future directions” paragraph of Section V in the updated paper.

\section*{Response to Reviewer D}
\noindent\textbf{Clarifying Evaluation Metrics: }The union accuracy presented is a measure of accuracy across multiple attacks that reports the overall accuracy if for each test example, we chose the worst case attack from the set of attacks.  We present 2 values in the table for this worst-case accuracy, “Union (known)” and “Union (all)”, where “Union (known)” is the accuracy on the set of attacks that are known to the defender and has been in either training/finetuning the model (highlighted in green in Tables I and II) while “Union (all)” represents the \emph{worst-case accuracy across all attacks in the table}.  We have updated the paper Section IV-A paragraph on “Evaluation” to specify that union accuracy is the accuracy if the worst-case attack from the set is chosen per test example.

\noindent\textbf{Distortion Budgets: }We thank the reviewer for raising this interesting point about distortion budgets. Setting distortion budgets for multiple attacks is currently an open problem \citep{cianfarani2022understanding, dai2024characterizing, dai2023multirobustbench, kaufmann2019testing}.  While for $L_{\infty}$ attacks it is possible to quantify distortion by a percentage because of the range of pixel values, for other attacks used in the paper such as StAdv which spatially warps the image, it is unclear how to measure distortion through a percentage. In our work, since we evaluate models on existing attacks (StAdv \citep{XiaoZ0HLS18}, ReColor \citep{LaidlawF19}, UAR attacks \citep{kaufmann2019testing}), we chose to follow the same bounds used in the papers proposing these attacks.  In practice, these bounds would likely depend on the application and how much trade-off with clean accuracy the defender can tolerate.

\noindent\textbf{Baseline Comparisons: }
To clarify, the results for fine-tuning in the CRT framework without regularization (FT MAX, FT Croce, and FT Single) all represent continuing training from the previous adversarially trained checkpoint.  Our method, regularized CRT, also falls into this framework of continuing training from the previous adversarially trained checkpoint. FT MAX continues training and chooses the worst attack per example for training, FT Croce \citep{croce2022adversarial} samples an attack per batch with probability proportional to the robust loss measured across each attack, and FT Single continues training with only the new attack. In our results, we compare the impact of FT Croce + ALR and FT Single + ALR to these techniques in order to understand the benefits of our proposed regularization.

\noindent\textbf{Continual Robust Training vs Continual Adaptive Robustness:} Continual adaptive robustness (CAR) is the name of the problem setting we consider, first introduced in \citet{dai2024position}.  \citet{dai2024position} is a position paper which motivates the problem of CAR and outlines open directions for research but does not propose an approach for addressing this problem.  Continual robust training (CRT) is the approach introduced in our paper for addressing the problem of CAR, and the results in Section III (theory) and Section IV (experiments) are all new in this paper.  We have updated the paper to clarify this distinction when referring to \citet{dai2024position} in Section II.

\bibliography{multi_robust}

\end{document}