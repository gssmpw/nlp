\vspace{-0.3cm}
\section{Evaluation} \label{6}
\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.1cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/setup2.png}
        % \vspace{-0.6cm}
    \caption{Experimental setup and scenarios of mmE-Loc. \textnormal{(a) A laboratory scenario with motion capture system for ground-truth collection. (b) An outdoor scenario with RTK system for ground-truth collection.  (c)-(f) Different drone with different size (DJI MAVIC 2 and DJI MINI 3 Pro), and different illumination of the laboratory (normal and weak), respectively.}}
    \label{setup}
    \vspace{-0.2cm}
\end{figure*} 

% \vspace{-0.3cm}

\begin{figure*}
\setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.2cm}
\setlength{\subfigcapskip}{-0.3cm}
    % \begin{minipage}[b]{0.9\columnwidth}
    \centering
        \subfigure[Indoor Accuracy Comparison]{
            \centering
            \includegraphics[width=0.66\columnwidth]{evaFigs/indoor_cdf.png}
            %\caption{fig1}
            \label{fig:indoor_cdf}
        }%
        \subfigure[Outdoor Accuracy Comparison]{
            \centering
            \includegraphics[width=0.66\columnwidth]{evaFigs/oudoor_cdf_I.png}
            \label{fig:outdoor_cdf}
        }%
        % \subfigure[\textbf{Variation of errors}]{
        %     \centering
        %     \includegraphics[width=0.5\columnwidth]{evaFigs/indoor_var.png}
        %     %\caption{fig1}
        %     \label{fig:indoor_var}
        % }%
        \subfigure[Latency Comparison]{
            \centering
            \includegraphics[width=0.66\columnwidth]{evaFigs/indoor_runtime.png}
            \label{fig:indoor_runtime}
        }%
    \caption{Overall performance comparison of mmE-Loc and four related works.}
    \label{fig:Overallperf}
    \vspace{-0.35cm}
\end{figure*}

% \begin{figure}
% \setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
% \setlength{\belowcaptionskip}{-0.2cm}
% \setlength{\subfigcapskip}{-0.3cm}
%     % \begin{minipage}[b]{0.9\columnwidth}
%     \centering
%         \subfigure[Localization error distribution along the x, y, and z dimensions]{
%             \centering
%             \includegraphics[width=0.45\columnwidth]{evaFigs/xyz.png}
%             \label{fig:xyz}
%         }%
%         \subfigure[Localization errors in the near-distance setting]{
%             \centering
%             \includegraphics[width=0.45\columnwidth]{evaFigs/near_cdf.png}
%             \label{fig:near_cdf}
%         }%
%     \caption{Overall performance comparison of mmE-Loc and four related works.}
%     \label{fig:Overallperf}
%     \vspace{-0.3cm}
% \end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{-0.15cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.2cm}
\setlength{\subfigcapskip}{-0.25cm}
  \begin{minipage}[t]{0.48\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/xyz.png}
    \caption{Error distribution along the x, y, and z.}
    \label{fig:xyz}
  \end{minipage}
  \begin{minipage}[t]{0.48\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/near_cdf.png}
    \caption{Error in the near-distance setting}
    \label{fig:near_cdf}
  \end{minipage}
  \hfill
  \vspace{-0.5cm}
\end{figure}



% \begin{figure*}
% \setlength{\abovecaptionskip}{-0.15cm} % height above Figure X caption
% \setlength{\belowcaptionskip}{-0.2cm}
% \setlength{\subfigcapskip}{-0.25cm}
%   \begin{minipage}[t]{0.692\columnwidth}
%     \centering
%     \includegraphics[width=1\columnwidth]{evaFigs/indoor_cdf.png}
%     \caption{Indoor Accuracy Comparison}
%     \label{fig:indoor_cdf}
%   \end{minipage}
%   \begin{minipage}[t]{0.692\columnwidth}
%     \centering
%     \includegraphics[width=1\columnwidth]{evaFigs/oudoor_cdf_I.png}
%     \caption{Outdoor Accuracy Comparison}
%     \label{fig:outdoor_cdf}
%   \end{minipage}
%   \begin{minipage}[t]{0.692\columnwidth}
%     \centering
%     \includegraphics[width=1\columnwidth]{evaFigs/indoor_runtime.png}
%     \caption{Latency Comparison}
%     \label{fig:indoor_runtime}
%   \end{minipage}
%   \hfill
%   \vspace{-0.6cm}
% \end{figure*}

% Our evaluation of mmE-Loc is five-pronged and entirely based on real-world experiments.
% Following a description of the implementation we use and of the experimental setting in Sec. \ref{5.1}, we report in Sec. \ref{5.2} on the crucial performance metrics we target: localization accuracy and processing latency.
% We proceed by investigating the influence of key system parameters in Sec. \ref{5.3}. 
% Further, we study in Sec. \ref{5.4} the impact on mmE-Loc of external factors, such as flight range and velocity of drone and background noise.

% The results we collect across a total of \notice{xx} flight hours lead us to \notice{xx} key conclusions:


% Our evaluation of mmE-Loc is comprehensive, structured around five key aspects and grounded entirely in real-world experimentation. 
% We commence by detailing the implementation and experimental setting in Sec. \ref{5.1}. 
% Subsequently, in Sec. \ref{5.2}, we focus on pivotal performance metrics we target: localization accuracy and processing latency. 
% Sec. \ref{5.3} explores external influences on mmE-Loc, such as drone type, velocity, and background noise and environmental illumination.
% Sec. \ref{5.4} explores the improvements brought by multi-sensor fusion, as well as the roles and performance of different modules in mmE-Loc. 
% Meanwhile, in Sec. \ref{5.5}, we test the load of system, including system latency, CPU load, and memory load.

Our evaluation of mmE-Loc is comprehensive and grounded entirely in real-world experimentation.  
% structured around five key aspects, 
We begin with experimental settings in §\ref{5.1}. 
Then, in §\ref{5.2}, we focus on key performance metrics: localization accuracy and latency. 
§\ref{5.3} delves into external factors impacting mmE-Loc, such as drone characteristics and environmental conditions. 
§\ref{5.4} assesses benefits of sensor fusion and the performance of various modules. 
Finally, in §\ref{5.5}, we evaluate system load, including latency, CPU usage, and memory consumption.

% Drawing from observations gathered over a cumulative flight duration of 30 hours, our analysis yields five insights: 
% \scalebox{1.2}{\ding{182}} mmE-Loc successfully enhances event cameras with mm-Wave radar in 3D localization;
% \scalebox{1.2}{\ding{183}} mmE-Loc attains \textit{cm-level} accuracy as the drone landing to the platform;
% \scalebox{1.2}{\ding{184}} mmE-Loc updates the location of drone with an \textit{ms-level} average latency, compatible with use in flight control loops and helps the drone quickly adjust its location. 
% \scalebox{1.2}{\ding{185}} mmE-Loc is compatible with \textit{various types of drones} and requires \textit{no modifications} to drones.
% \scalebox{1.2}{\ding{186}} mmE-Loc is \textit{marginally affected} by factors it cannot control, such as drone speed, occlusion, background.\\
% Rest of the section provides experimental evidence.


\vspace{-0.3cm}
\subsection{Experimental Methodology} \label{5.1}
% \noindent \textbf{Implementation.}
% As illustrated in \fig \ref{setup}, we implement our sensing system with a handheld prototype equipped with multiple sensors including
% $(i)$ a Prophesee EVK4 HD evaluation kit to gather event data, which features the IMX636ES event-based vision sensor, offering HD resolution (1280 $\times$ 720 pixels).
% % , paired with the Soyo SFA0820-5M lens.
% $(ii)$ a Texas Instruments (TI) IWR1843 board for transmitting and receiving mmWave signals within the frequency range of 76 GHz to 81 GHz. The TI IWR1843 integrates three transmitting antennas labeled TX1 to TX3, and four receiving antennas labeled RX1 to RX4. These antennas are arranged in two linear configurations on the horizontal plane.
% % $(iii)$ an Intel D435i Depth camera for both RGB and depth image capture, which is also utilized to implement baseline methods.
% All the sensors are synchronized through Robot Operating System (ROS).
% The mmE-Loc operates on a Desktop PC equipped with Ubuntu 20.04, an Intel i7-12900K processor, and 32GB of RAM.
In this section, we will outline the experimental methodology.

% \noindent 
\textbf{Experiment setting.} 
% We use a deployment setup and drone equipment that closely mimic real-world applications.
% \fig \ref{setup} illustrates the experimental scenarios in an \textit{indoor} laboratory and an secluded \textit{outdoor} flight test site. The experimental setup, depicted in \fig \ref{setup}a, shows the event camera and mmWave radar mounted on a bracket at the front of the experimental area.
% We evaluate mmE-Loc on various target drones, including $(i)$ a commercial DJI Mini 3 Pro drone, measured 2.5$dm$ $\times$ 3.6 $dm$ $\times$ 0.70$dm$ (length, width, height) with unfolded propellers \tocite; $(ii)$ a commercial DJI MAVIC 2 drone, measured 3.2$dm$ $\times$ 2.4$dm$ $\times$ 0.8 $dm$ with unfolded propellers \tocite; $(iii)$ \notice{a DJI M30T}, measured \notice{4.9$dm$ $\times$ 6.1 $dm$ $\times$ 2.2$dm$} with unfolded propellers \tocite, and $(iv)$ a custom drone developed by a delivery company exploring the feasibility of instant deliveries with drones. 
% The custom drone is equipped with six propellers and is managed by a PX4 flight controller running at 400 Hz. 
% We conduct extensive experiments for around 30 hours and collect more than 400GB event camera and mmWave raw samples.
We use a deployment setup and drone equipment that closely mimic real-world applications. 
\fig \ref{setup} shows the experimental scenarios in both an \textit{indoor} laboratory and an \textit{outdoor} flight test site. 
The setup in \fig \ref{setup}a includes the event camera and mmWave radar mounted together \textit{at the ground of the experimental area}.
We evaluate mmE-Loc on various target drones:
$(i)$ DJI Mini 3 Pro: 0.25$m$ $\times$ 0.36$m$ $\times$ 0.07$m$ with unfolded propellers.
$(ii)$ DJI MAVIC 2: 0.32$m$ $\times$ 0.24$m$ $\times$ 0.08$m$ with unfolded propellers.
$(iii)$ DJI M30T: 0.49$m$ $\times$ 0.61$m$ $\times$ 0.22$m$ with unfolded propellers.
We conducted extensive experiments over 30 hours, collecting more than 400GB of raw data.

% The main differences between the two drones are their sizes: the DJI Mini 3 Pro measures 251$mm$ in length, 362 $mm$ in width, and 70$mm$ in height, while the DJI Mavic 2 measures 322$mm$ in length, 242$mm$ in width, and 84 $mm$ in height (unfolded with propellers).

% \noindent 
\textbf{Ground truth.}
% In the indoor scenario, we deployed an motion capture system containing fourteen NOKOV \tocite cameras with 180fps to cover an area of 8m $\times$ 8m $\times$ 8m, which could provide <1mm localization ground truth. 
% In the indoor scenario, we deployed a motion capture system consisting of fourteen NOKOV \tocite cameras, each operating at 180 fps. This system covers an 8m $\times$ 8m $\times$ 8m area, providing highly accurate localization ground truth with an accuracy of less than 1 mm. This precise tracking enables us to thoroughly evaluate the performance of our system under controlled conditions.
% In the outdoor scenario, we cannot deploy motion capture system.
% Since we conduct the outdoor experiments in an secluded flight test site, the reception of GPS signals is of very high quality, we deploy an RTK base station close to the experimental site, which keep rebroadcasting the phase of the GPS signal it observes.
% In such a setting, the RTK processing on the drone works at high fidelity as well.
% Therefore, we use the localization results of RTK as ground truth.
In the indoor scenario, we used a motion capture system with fourteen cameras. 
This system covers an \textit{8m $\times$ 8m $\times$ 8m} area, providing localization accuracy within \textit{1 mm}, allowing precise performance evaluation under controlled conditions.
In the outdoor scenario,  we conducted experiments at a secluded site with excellent GPS signal reception. We set up a Real-Time Kinematic Positioning (RTK) base station to rebroadcast the GPS signal phase, ensuring high-fidelity RTK processing. The RTK localization results served as the ground truth for our outdoor tests.
% 
% In the indoor scenario, we deployed a motion capture system consisting of fourteen NOKOV \tocite cameras, each operating at 300 fps. 
% This system covers an 8m $\times$ 8m $\times$ 8m area, providing highly accurate localization ground truth with an accuracy of less than \textit{1 mm}.
% This precise tracking allows us to thoroughly evaluate our system's performance under controlled conditions.
% In the outdoor scenario, deploying a motion capture system is not feasible. Instead, we conduct the experiments at a secluded flight test site with excellent GPS signal reception. 
% We set up an RTK base station near the experimental site to continuously rebroadcast the observed GPS signal phase, ensuring high-fidelity RTK processing on the drone.
% Subsequently, we use the RTK localization results as the ground truth for our outdoor tests.


% \noindent 
\textbf{Comparative Methods.}
% We compare localization accuracy and latency of mmE-Loc under various conditions with three related systems:
We compare mmE-Loc under various conditions with three related systems:
% deep learning-based
$(i)$ \textit{Baseline-I} \cite{zhao20213d}: a SOTA point cloud-based 
drone localization system using single-chip mmWave radar, which is a ground localization system.
$(ii)$ \textit{Baseline-II} \cite{falanga2020dynamic}: a SOTA single event camera-based drone ground localization system applies to drones with known geometries.
The original method, designed for onboard obstacle localization, lacks publicly available code. 
We implement its monocular 3D localization module which uses drone physical characteristics, adapting it for \textit{ground-based} drone localization.
For a fair comparison, we exclude the original method’s ego-motion compensation used for noise reduction due to drone movement.
% We then implement the original method’s monocular 3D localization module incorporating drone physical characteristics, adapting it for \textit{ground-based} drone localization.
$(iii)$ \textit{Baseline-III} \cite{falanga2020dynamic}: a SOTA dual event camera-based drone localization system. 
The original method utilizes onboard stereo event cameras for obstacle localization, but its source code is not available. Our implementation excludes the ego-motion compensation component from the original method, implementing its stereo 3D location estimation module to enable \textit{ground-based} drone localization.
$(iii)$ \textit{Baseline-IV} \cite{shuai2021millieye}:
% a SOTA deep learning based object localization system with monocular images and mmWave radar point cloud as input, leveraging complementary information of the radar and the camera. we train neural networks in advance and utilize a Kalman Filter to adapt it in \textit{ground-based} drone 3D localization. 
a SOTA deep learning-based object localization system utilizes monocular images and mmWave point clouds as input, harnessing the complementary information from both the radar and the camera. 
We pre-train the neural networks and apply a Kalman Filter to adapt the original method for \textit{ground-based} 3D drone localization.
% The neural networks are pre-trained, and a Kalman Filter is applied to adapt the system for ground-based 3D drone localization.


% 是一种基于机载双目事件相机的Moving Object Detection and Tracking方法，用于无人机感知并躲避障碍物。但是，原方法中并不包含对障碍物的3D 位置估计。
% We modified it to localize drones in 3D by employing the method from \cite{xu2023taming}.


% We compare the localization accuracy in various conditions and latency of mmE-Loc with three related systems including
% $(i)$ Baseline-I \cite{zhao20213d}: the state-of-the-art deep learning-based drone location estimation system, which takes single-chip mmWave radar samples as input;
% $(ii)$ Baseline-II \cite{falanga2020dynamic}: the state-of-the-art event camera-based drone tracking and localization system uses asynchronous events as input. However, it can only localize drones in 3D with known geometries;
% $(iii)$ Baseline-III \cite{mitrokhin2018event}: the state-of-the-art dual event cameras-based moving object detection and tracking system, which takes asynchronous events from dual event cameras as input. 
% We modified it to localize drones in 3D using the method introduced in \cite{xu2023taming}, leveraging the tracking results from the system.

% \noindent 
\revise{
\textbf{Evaluation Metrics.}
The mmE-Loc continuously reports the drone's location estimation with low latency, ensuring accurate and responsive localization. 
Similar to related works, we measure location estimation error in meters and processing latency in milliseconds, allowing for a direct comparison with existing methods in terms of both accuracy and computational efficiency.
}

% \noindent
\textbf{Robustness Experiments.}
During the drone landing, localization may be performed under varying lighting conditions.
% , with potential obstacles, and for different drone models. 
To assess the adaptability of mmE-Loc in different scenarios, we conduct experiments across a range of conditions, including different environments, drone models, lighting intensities (\fig \ref{setup}a-f), and background dynamics (controlled by the presence of other moving objects, \eg, balls). 
We also evaluate the impact of varying distances, occlusions (by partially obstructing the drone within the field of view of the event camera), and velocities to highlight the robustness.
% In this part, we compare mmE-Loc with Baseline-I, -II and -III, due to the high-latency of Baseline-IV
% In this part, we compare mmE-Loc with Baselines-I, -II, and -III, excluding Baseline-IV due to its high latency.
% In this part, Baseline-IV is excluded from comparison due to its high latency caused by exposure time of the frame camera.
In this part, Baseline-IV is excluded from comparison due to its high latency stemming from frame camera's exposure time.


 
\vspace{-0.4cm}
\subsection{Overall Performance} \label{5.2}
% \noindent 
In this section, we demonstrate overall performance of mmE-Loc.

\revise{

\textbf{Drone localization.} 
\fig \ref{fig:Overallperf}a illustrates the localization performance of mmE-Loc compared to four baselines in an indoor environment, using a DJI Mini 3 Pro drone.
mmE-Loc's average end-to-end localization error is 0.083$m$, which outperforms the other baselines with errors of 0.261$m$, 0.345$m$, and 0.209$m$, 0.160$m$, making it suitable for aiding in landing.
\fig \ref{fig:Overallperf}b shows mmE-Loc's localization performance in an outdoor setting using a DJI M30T drone. 
mmE-Loc achieves the lowest average error of 0.135$m$ compared to the other baselines, outperforming them by 39.2\%, 56.0\%, 43.8\%, 31.8\%. 
\fig \ref{fig:xyz} shows error distribution along the x, y, and z dimensions during the typical landing process, and \fig \ref{fig:near_cdf} shows errors in the near-distance setting.
% Moreover, as shown in \fig \ref{fig:Overallperf}c, which depicts the localization error over time, mmE-Loc consistently maintains a low error level. 
% This is because radar measurements, due to phase center offset, cause point cloud data to deviate from the drone’s geometric center, introducing measurement errors. 
% The dual event camera system in Baseline-III uses stereo vision for depth estimation, while Baseline-II, using a single event camera, estimates depth based on known drone geometries. 
% Both are prone to errors when bounding box detection is affected by dynamic objects in outdoor environments, leading to significant depth estimation inaccuracies, particularly for Baseline-III.
% In contrast, mmE-Loc combines the complementary strengths of radar and event cameras, using radar for depth and the event camera for geometric centering. Additionally, mmE-Loc introduces the \textit{CCT} and \textit{GAJO} modules for joint filtering and optimization, enhancing localization accuracy.
% Baseline-I introduces point cloud errors due to phase center offset, which causes point cloud data to deviate from the drone’s geometric center. 
% Meanwhile, in the indoor environment, the radar is prone to be affected by specular reflections, diffraction, and multi-path effects, introducing measurement errors.
Baseline-I introduces point cloud errors due to phase center offset, causing the point cloud data to deviate from the drone’s geometric center. Additionally, in indoor environments, radar measurements are susceptible to specular reflections, diffraction, and multi-path effects, further contributing to measurement errors.
Baseline-II uses a single event camera with known drone geometries, while Baseline-III, with dual event cameras, estimates depth through stereo vision.
Both are prone to errors when drone detection is affected by outdoor environmental noise (\eg, birds), leading to depth estimation inaccuracies.
Baseline IV incorporates deep learning, which can lead to increased errors when used in environments not present in the training set.
% Above theoretical and empirical results demonstrate mmE-Loc achieves remarkable performance gains based on combining the complementary strengths of radar and event cameras.
% Meanwhile, mmE-Loc does not require the complex pre-train procedure, ensuring the ubiquitous.
The results demonstrate that mmE-Loc achieves significant improvements by combining complementary strengths of radar and event camera.
mmE-Loc does not require a pre-training procedure, ensuring its applicability.
% mmE-Loc also introduces the \textit{CCT} and \textit{GAJO} modules for joint filtering and optimization, enhancing localization accuracy.
}

\begin{figure*}
\setlength{\abovecaptionskip}{-0.15cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.2cm}
\setlength{\subfigcapskip}{-0.25cm}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/drone_type.png}
    \caption{Impact of Drone Type}
    \label{fig:drone_type}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/illumination.png}
    \caption{Impact of Env. \& Illu.}
    \label{fig:illumination}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/background.png}
    \caption{Impact of Background}
    \label{fig:background}
  \end{minipage}
  \hfill
  \vspace{-0.61cm}
\end{figure*}

\begin{figure*}
\setlength{\abovecaptionskip}{-0.15cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.2cm}
\setlength{\subfigcapskip}{-0.25cm}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/distance.png}
    \caption{Impact of Distance}
    \label{fig:distance}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/occlusion.png}
    \caption{Impact of Occlusion}
    \label{fig:occlusion}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/velocity.png}
    \caption{Impact of Velocity}
    \label{fig:velocity}
  \end{minipage}
  \hfill
  \vspace{-0.7cm}
\end{figure*}


% Radar measurements inherently include a phase center offset, causing the point cloud data to deviate from the drone’s geometric center and introducing measurement errors. 
% 而基于dual event camera 的 Baseline-III利用双目视觉来估算 drone 的深度，基于 single event camera的 Baseline-II 利用 drone 的known geometries来估算深度，其 bounding box 检测容易受到室外环境中出现的动态物体的影响。一旦bounding box检测出现误差，Baseline-II和-III 的深度估计就会出现显著错误，尤其是具有 dual event cameras 的 baseline-III。
% Unlike the other methods that rely solely on radar or event cameras, mmE-Loc leverages the complementary strengths of both，在利用 radar 提供的深度的同时，利用 event camera 提供geometric center，and introduces the \textit{CCT} and \textit{GAJO} modules for joint filtering and optimization, enhancing localization accuracy.
% \fig \ref{fig:Overallperf}a illustrates the localization performance of mmE-Loc compared to three other comparative systems in an indoor environment, using a DJI Mini 3 Pro drone. The average end-to-end localization error of mmE-Loc is xx, outperforming the other three baselines, whose localization errors are xx, xx, and xx, respectively. Due to the phase center offset inherent in mmWave radar measurements, the radar-detected point cloud data does not perfectly correspond to the geometric center of the drone, resulting in intrinsic radar measurement errors. Given that the drone used in the experiment measures 2.5$dm$ $\times$ 3.6$dm$ $\times$ 0.7$dm$, the localization error of mmE-Loc will not exceed the length of the drone body, making it suitable for assisting the drone in landing.
% \fig \ref{fig:Overallperf}b depicts the localization performance of mmE-Loc compared to three other comparative systems in an outdoor environment, using a DJI M30T drone. mmE-Loc achieves the lowest average localization error compared to the other three baselines, outperforming all baselines by xx. Moreover, as shown in \fig \ref{fig:Overallperf}c, which illustrates the localization error of different methods over time, mmE-Loc consistently maintains a low level of localization error. Unlike the three related works that depend solely on mmWave or event cameras, mmE-Loc leverages the advantages of both and introduces the CCT module and \textit{GAJO} module for joint filtering and optimization, enhancing the system's localization accuracy.

\vspace{-0.3cm}
\textbf{End-to-end latency.}
% We evaluate the end-to-end latency, including the \textit{CCT} and \textit{GAJO} phases. 
% As shown in \fig \ref{fig:Overallperf}d, in an indoor environment, mmE-Loc's latency is 5.15$ms$, outperforming the other four baselines, which are 10.19$ms$, 15.12$ms$, 15.52$ms$, and xx$ms$, respectively.
% In an outdoor environment, due to increased environmental complexity, the latencies of Baselines increase. 
% mmE-Loc maintains the lowest latency, outperforming all baselines by 62.5\%, 69.6\%, 72.7\%, and 72.7\%.
% Typically, related works encounter increased latency due to environmental noise and the rise in parameters to be optimized. 
% mmE-Loc tightly couples the event camera and mmWave radar, introducing the \textit{CCT} module during detection, leveraging consistent information. 
% The \textit{GAJO} module and an adaptive optimization method are used to fully exploit complementary information, further reducing end-to-end latency.
We evaluate end-to-end latency, including the \textit{CCT} and \textit{GAJO} phases.
As shown in \fig \ref{fig:Overallperf}c, mmE-Loc achieves 5.15$ms$ latency indoors, outperforming the four baselines at 10.19$ms$, 15.12$ms$, 15.52$ms$, and 31.2 $ms$, respectively. 
In outdoor scenarios with higher complexity, baseline latencies increase, and mmE-Loc maintains the lowest latency, outperforming them by 62.5\%, 69.6\%, 72.7\%, and 83.3\%. 
Baselines face increased latency due to environmental noise and more optimization parameters.
The mmE-Loc tightly couples the event camera and mmWave radar, introducing the \textit{CCT} module to leverage temporal consistency and periodic micro motion for drone detection.
The \textit{GAJO} module and the adaptive optimization method are then used to exploit spatial complementarity, further reducing latency.

% Moreover, mmE-Loc can track the drone's location without complex pre-modeling or training procedures, ensuring the ubiquity of drone-based applications.

\vspace{-0.6cm}
\subsection{System Robustness Evaluation} \label{5.3}
% To demonstrate the versatility of mmE-Loc, we conduct experiments under various conditions, including different types of drones, illumination intensities, and background motions. 
% We also evaluate the system's performance with varying drone-platform distances, drone occlusions, and drone velocities to showcase robustness of mmE-Loc.
To demonstrate the versatility and robustness of mmE-Loc, we conduct experiments under various conditions.

% \noindent
\textbf{Impact of Drone Type.} 
We evaluate the impact of different drone types under controlled indoor conditions, using a DJI Mini 3 Pro (\fig \ref{setup}c) and a DJI Mavic 2 (\fig \ref{setup}d), the results are presented in \fig \ref{fig:drone_type}. 
The results of average errors show that mmE-Loc outperforms all baselines with the DJI Mini 3 Pro.
% are 1.01$dm$, 2.61$dm$, 3.45$dm$, and 3.12$dm$, respectively. 
With the DJI Mavic 2, Baseline-II and -III show larger localization errors due to changes in drone geometry affecting depth estimation. 
mmE-Loc's localization error of 0.135$m$ remains within an acceptable range, which outperforms all baselines with 0.222$m$, 0.639$m$, and 0.403$m$, demonstrating its effectiveness across different drone types.
% We evaluate the impact of different types of drones under controlled indoor conditions, including a commercial DJI Mini 3 Pro and a DJI Mavic 2. The results are shown in \fig \ref{fig:drone_type}.
% As shown in \fig \ref{fig:drone_type}, the average errors for mmE-Loc, Baseline-I, Baseline-II, and Baseline-III with DJI Mini 3 Pro are 1.01$dm$, 2.61$dm$, 3.45$dm$, and 3.12$dm$, respectively. 
% When we switched to the DJI Mavic 2, Baseline-II and -III experienced larger localization errors. Further analysis indicates that the change in drone geometry caused errors in depth estimation for Baseline-II and -III, likely distorting the drone's localization.
% Despite this, the localization error of mmE-Loc remained within an acceptable range, proving it to be a viable solution for different drone types.

% We evaluate the impact of different types of drones under controlled indoor conditions, including a commercial DJI Mini 3 Pro and a DJI Mavic 2. The results are shown in \fig \ref{fig:drone_type}. 
% The DJI Mini 3 Pro measures 251$mm$ \times 362$mm$ \times 70$mm$, while the DJI Mavic 2 measures 322$mm$ \times 242$mm$ \times 84$mm$ (both unfolded with propellers).
% As seen, for the DJI Mini 3 Pro, the average location errors for mmE-Loc, Baseline-I, Baseline-II, and Baseline-III are 1.01$dm$, 2.61$dm$, 3.45$dm$, and 3.12$dm$, respectively. When we switched to the DJI Mavic 2, Baseline-II and Baseline-III showed larger localization errors due to geometry-induced depth estimation errors.
% However, mmE-Loc's localization error remained within an acceptable range, proving it to be a viable solution for different drone types.
% 的平均误差分别是 1.23dm，2.22dm，6.39dm，以及 5.94dm。

% \noindent
\textbf{Impact of Environment \& Illumination.}
The performance of mmE-Loc in different environments with varying lighting conditions (\fig \ref{setup}e and \fig \ref{setup}f) is shown in \fig \ref{fig:illumination}. 
Although event cameras have a high dynamic range, drone-generated events are still affected by illumination. 
Baseline-II and -III experience increased errors as illumination decreases due to deteriorating depth estimation. 
% In contrast, mmE-Loc maintains an average error of around 0.1$m$. 
% Even under weak illumination, mmE-Loc records an average error of 0.103$m$ and a maximum error of 0.27$m$.
In comparison, mmE-Loc sustains a relatively low average error even under low illumination, recording an average error of 0.103$m$ and a maximum error of 0.27$m$.
% In contrast, mmE-Loc maintains a relatively low average error, even under weak illumination, achieving an average error of 0.103m and a maximum error of 0.27m.
mmE-Loc's consistent performance across different lighting conditions, without requiring pre-training or prior knowledge, makes it a versatile solution.
% for drone localization.
% Although event cameras have a high dynamic range, the events generated by drones are still affected by illumination. 
% Specifically, the depth estimation results of Baseline-II and Baseline-III deteriorate as illumination decreases, leading to increased localization errors. 
% However, as the environmental illumination changes, mmE-Loc's average location error remains around 1 $dm$, showing consistent performance across different lighting conditions. Under weak illumination, mmE-Loc maintains an average localization error of 1.03$dm$.
% Unlike other baselines, mmE-Loc does not require pre-training or prior knowledge about different environments, making it a more versatile and ubiquitous solution for drone localization.

% The performance of mmE-Loc in different environments with different lighting conditions is shown in \ref{fig:illumination}.
% 虽然事件相机具有较大的动态范围，但因无人机而产生的事件依然会受到光照的影响。具体而言，Baseline-II 和-III的 深度估计结果因为illumination的减弱而变差，进而导致了定位误差的增大。
% 但是，随着环境illumination从 weak 到 light，mmE-Loc 的 average location error 一直保持在 1$dm$附近，which is almost the same in different environments illumination intensity.
% As for the system robustness under weak illumination intensity, mmE-Loc maintains the average localization error bias with 1.03$dm$.
% Compared with other baselines, mmE-Loc does not require pre-training and prior knowledge in different environments, making FollowUpAR more ubiquitous.

% \noindent 
\textbf{Impact of Background Motion.}
We test mmE-Loc with dynamic background motion, as shown in \fig \ref{fig:background}. 
The intensity of dynamic background motion is controlled by deploying other moving objects in the scene (\eg, balls).
All baselines show higher localization errors with increasing background motion due to misidentification of the radar and camera.
Despite this, mmE-Loc maintains an average error of 0.129$m$ in the most challenging scenarios.
% In real-world delivery, where drones are larger, misidentification will be reduced. 
% This is due to the consistency-instructed measurements filter of mmE-Loc which utilizes periodic micro motion of drone (propeller rotation) to extract drone from other objects.
This is achieved through mmE-Loc’s consistency-instructed measurement filter, which uses the drone’s periodic micro-motions of propeller rotation to distinguish it from other objects.
% Additionally, mmE-Loc complements existing solutions for more reliable and accurate service.
% We conducted experiments with dynamic background motion, and the results are shown in \fig \ref{fig:background}. 
% As background motion increases, Baseline-I, -II, and -III exhibit higher localization errors due to misidentification by both mmWave radar and event cameras. 
% Despite the decrease in tracking performance with increasing background dynamics, mmE-Loc still maintains an average localization accuracy of 1.29$dm$ in the most challenging scenarios. 
% In real-world drone delivery tasks, where drones are typically larger, misidentification will be significantly reduced. 
% Additionally, our method will complement existing localization solutions and work alongside them for a more reliable and accurate localization service.
% We conduct experiments with dynamic background motion, and the evaluation results are presented in \fig \ref{fig:background}.
% As seen, as the dynamic background motion increases, the localization errors for Baseline-I, -II, and -III also increase. This is because both mmWave radar and event cameras misidentify moving objects in the environment, leading to localization errors. 
% Despite the decrease in tracking performance with increasing background dynamics, mmE-Loc still maintains an average localization accuracy of 1.29$dm$ in the most challenging scenarios. 
% In real-world drone delivery tasks, where drones are typically larger, the likelihood of misidentification will be significantly reduced.
% Moreover, as mentioned above, our method is a complement to existing localization solutions, not a replacement. In real-world applications, mmE-Loc will work in conjunction with other methods to provide a more reliable and accurate localization service.

% We conduct experiments with dynamic background motion. The evaluation result is presented in \fig \ref{fig:background}.
% As seen, 随着dynamic background motion的增加，Baseline-I，-II 以及-III对无人机的定位误差都有所上升，这是因为 mmwave 和事件相机都会对环境中的动态物体产生错误识别，进而导致定位误差。
% Although the tracking performance decreases with the increasing of background dynamics, mmE-Loc still maintains an average localization accuracy of 1.29 $dm$ in most challenging scene.
% 在真实的无人机配送任务中，无人机的体积会更大，we believe the accuracy meets the needs of 这种配送无人机。Moreover，as mentioned above，我们的方法is a complement, not a replacement, to the existing localization solutions. 在真实任务中，mmE-Loc will cooperate with RTK to provide a more reliable and accurate localization service.

% \noindent 
\textbf{Impact of Distance.}
We investigate the impact of drone-to-platform distance indoors with a DJI Mini 3 Pro (0.25$m$ $\times$ 0.36$m$ $\times$ 0.07$m$). 
\fig \ref{fig:distance} shows the results, categorized into Near (< 3$m$), Normal (3$m$ $\sim$ 6$m$), and Far (> 6$m$) distances. 
As distance increases, all methods show higher errors. 
mmE-Loc achieves an average error of 0.102$m$ for far distances, outperforming other baselines. 
% Longer distances result in fewer events and mmWave samples, challenging drone localization.
% mmE-Loc leverages mmWave radar for depth information and an event camera for high-resolution 2D imaging, overcoming low spatial resolution of radar and scale uncertainty of event camera.
% Acknowledging the importance of precise landings, mmE-Loc complements existing solutions. 
mmE-Loc leverages mmWave radar for depth information and an event camera for high-resolution 2D imaging, effectively overcoming the low spatial resolution of mmWave radar and the scale uncertainty of the event camera.
Acknowledging the importance of precise landings, mmE-Loc complements existing solutions.
Working alongside RTK and visual markers, it enhances the reliability and accuracy of localization in real-world scenarios.
% Working alongside RTK and visual markers, it enhances reliability of localization in real-world scenarios.

\begin{figure*}
\setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.25cm}
\setlength{\subfigcapskip}{-0.2cm}
    % \begin{minipage}[b]{0.9\columnwidth}
    \centering
        \subfigure[Effectiveness of Sensor Fusion]{
            \centering
            \includegraphics[width=0.5\columnwidth]{evaFigs/fusion.png}
            %\caption{fig1}
            \label{fig:fusion}
        }%
        \subfigure[Impact of Different Module]{
            \centering
            \includegraphics[width=0.5\columnwidth]{evaFigs/module.png}
            \label{fig:module}
        }%
        \subfigure[Performance of \textit{CCT}]{
            \centering
            \includegraphics[width=0.5\columnwidth]{evaFigs/CCT.png}
            %\caption{fig1}
            \label{fig:CCT}
        }%
        \subfigure[Performance of \textit{GAJO}]{
            \centering
            \includegraphics[width=0.5\columnwidth]{evaFigs/GAJO.png}
            \label{fig:GAJO}
        }%
    \caption{Ablation Study.}
    \label{fig:ablation}
    \vspace{-0.3cm}
\end{figure*}

\begin{figure*}
\setlength{\abovecaptionskip}{-0.2cm} % height above Figure X caption
\setlength{\belowcaptionskip}{-0.15cm}
\setlength{\subfigcapskip}{-1cm}
  \begin{minipage}[t]{0.695\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/latency.png}
    \caption{System Latency}
    \label{fig:latency}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/cpu.png}
    \caption{CPU Workload}
    \label{fig:cpu}
  \end{minipage}
  \begin{minipage}[t]{0.692\columnwidth}
    \centering
    \includegraphics[width=1\columnwidth]{evaFigs/memory.png}
    \caption{Memory Usage}
    \label{fig:memory}
  \end{minipage}
  \hfill
  \vspace{-0.8cm}
\end{figure*}
% , especially for smaller drones. 
% Recognizing the importance of precise landings, mmE-Loc acts as a complementary system rather than a replacement for existing solutions. 
% By working alongside RTK and visual markers, mmE-Loc enhances the reliability and accuracy of localization services in real-world scenarios.
% Given importance of precise drone landings, mmE-Loc serves as a complementary system rather than a replacement for existing localization solutions, which means mmE-Loc functions in conjunction with RTK and visual markers, ensuring a more reliable and accurate localization service in real-world scenarios.
% mmE-Loc works at landing phase of the drone, and at a higher altitude, it can work in conjunction with RTK or visual markers to ensure precise landing performance

% mmE-Loc is a complement, not a replacement, to the existing localization solutions. 
% To assist drones in precise landing, mmE-Loc will not work alone but will cooperate with RTK and the visual marker to provide a more reliable and accurate localization service in real-world setting.
% Larger drones and powerful radar in real-world settings can alleviate this issue, allowing for greater localization distances.
% We investigate the impact of drone-to-platform distance indoors. 
% For this experiment, we use a DJI Mini 3 Pro drone measuring 2.5$dm$ $\times$ 3.6$dm$ $\times$ 0.7$dm$.
% The results are depicted in \fig \ref{fig:distance}.
% We categorize the distances into three groups: Near refers to distances below 2$m$, Normal indicates distances between 2$m$ and 3$m$, and Far denotes distances above 3$m$.
% With increasing distance, all methods exhibit increased location error. mmE-Loc achieves average errors of 0.98 dm, 1.08 dm, and 1.12 dm for near, normal, and far distances, respectively, outperforming other baselines.
% Longer distances lead to insufficient events and mmWave samples, posing a greater challenge to localization.
% Additionally, the small size of the drones exacerbates this issue. However, larger drone volumes and reflective surfaces in real-world deployments may alleviate this challenge to some extent, allowing for increased localization distances.

% Additionally, the relatively small size of the drones used in our experiments exacerbates this issue. However, in real-world deployments, larger drone volumes and reflective surfaces may mitigate this challenge to some extent, allowing for increased localization distances.
% We examine the impact of the distance between the drone and sensor platform indoor. In this experiment, we utilize DJI Mini 3 Pro drone which is measured 2.5$dm$ $\times$ 3.6$dm$ $\times$ 0.7$dm$. The results are shown in \fig \ref{fig:distance}.
% We divide the distance into three categories. Near 意味着drone and sensor platform的距离在 2m 以下，Normal意味着drone and sensor platform的距离大于等于 2m 且小于 3m，Far意味着drone and sensor platform的距离大于等于3m。
% As seen, 随着距离的增大，不同方法在 location error 上都有一定程度的提升。
% mmE-Loc 在距离 near，normal 和 far 时，average location error 分别是 0.98dm，1.08dm和 1.12dm，超过了其他的 baseline。
% Generally, a longer distance fails to generate sufficient events and mmWave samples, making the localization more challenging.
% 不仅如此，我们在实验过程中使用的无人机体积相对较小，更是加重了这一情况。真实部署时的无人机体积与反射面较大，这种情况会有一定程度的减轻，定位距离也有有所增加。

% \noindent 
\textbf{Impact of Occlusion.}
We validate mmE-Loc's robustness with partially occluded drones. 
The occlusion is controlled by partially obstructing the drone within FoV of the event camera.
\fig \ref{fig:occlusion} shows that with 25\% occlusions, mmE-Loc maintains high performance with an average error of 0.094$m$. 
At 50\% occlusion, the average error of mmE-Loc increases to 0.12$m$. 
Baseline-II and -III show larger errors due to incorrect depth estimation caused by occlusion. 
% Advantage of mmE-Loc over other single modality-based methods is its ability to accurately track the drone's location despite partial occlusion.
% mmE-Loc’s advantage over single-modality methods is its accurate tracking of the drone’s location even under partial occlusion.
% mmE-Loc leverages advantages of both modalities to accurate tracking of the drone’s location even under partial occlusion.
mmE-Loc harnesses the strengths of both modalities in accurately tracking the drone’s location, even under partial occlusion.
% We validate the system's robustness in scenarios where the drone is partially occluded. As depicted in \fig \ref{fig:occlusion}, even with 25\% occlusion, mmE-Loc maintains high performance with an average location error of 0.78$dm$. With a 50\% occlusion rate, the average location error increases to 1.02$dm$. 
% In contrast, Baseline-II and -III exhibit larger location errors due to incorrect depth estimation caused by occlusion.
% Compared to other model-based and learning-based methods, mmE-Loc's superiority lies in its ability to track the drone's location normally under partial occlusion.
% We further verify the robustness of the system when the drone are partially occluded.
% As shown in \fig \ref{fig:occlusion}, when 25\% of the drone is occluded, mmE-Loc still able to maintain high performance with an average 0.78dm location error. When the rate rises to 50\%, the average location error and increase to 1.02dm.而Baseline-II 和 -III 则因为occlusion 而对物体深度估计错误，进而导致较大的 locaiton error。
% Compared with the other model-based and learning-based methods, it is our superiority that mmE-Loc can track the location of drone normally with partial occlusion.

% \noindent 
\textbf{Impact of Drone Velocity.}
We further evaluate mmE-Loc's robustness under different drone velocities in \fig \ref{fig:velocity}. 
Velocities are categorized as slow ($v < 0.5m/s$), medium ($0.5m/s \leq v < 1m/s$), and rapid ($1m/s \leq v < 1.5m/s$), corresponding to various drone landing stages. 
% 1.02$dm$, 1.03$dm$, and
%mmE-Loc achieved an average error of 0.108$m$ in rapid speed , benefiting from the low latency of event cameras and mmWave radar. Despite minor error increases for other methods, mmE-Loc consistently outperformed them by 44\%, 68.5\% and 52.4\%.
Although error of all methods increased with speed, mmE-Loc maintained an average error of 0.11$m$ even in rapid speed scenarios, outperforming baselines by 44\%, 68.5\%, and 52.4\%.
% , respectively. 
% Additionally, mmE-Loc's performance at medium velocities meets the requirements for drone landing.
% We further evaluate mmE-Loc's robustness under different drone velocities, as illustrated in \fig \ref{fig:velocity}. 
% Velocities are categorized into three types: slow ($v < 0.5m/s$), medium ($0.5m/s \leq v < 1m/s$), and rapid ($1m/s \leq v < 1.5m/s$), corresponding to various drone landing stages. 
% mmE-Loc achieves location errors of 1.02$dm$, 1.03$dm$, and 1.08$dm$, respectively, benefitting from the high sampling rates of event cameras and mmWave radar. Despite minor increases in location error for other methods, mmE-Loc consistently outperforms them. 
% Furthermore, mmE-Loc's performance at medium velocities meets the requirements for drone operations during the landing phase.
% We further evaluate the robustness of mmE-Loc on a different drone moving velocities.
% The results of location error are illustrated in \fig \ref{fig:velocity}. We divide the velocities into three categories, corresponding to the demands of different 阶段 of drone landing. 
% The velocity type 1, 2, and 3 represent slow, medium, and rapid speed with $v < 0.5m/s$, $0.5m/s \leq v < 1m/s$, $1m/s \leq v < 1.5m/s$, respectively.
% And the corresponding location error of mmE-Loc is 1.02dm, 1.03dm, and 1.08dm.这得益于事件相机和 mmwave radar较高的采样率。虽然不同的方法在 location error 上也只有轻微的增加，但 mmE-Loc 依然outperforms other baselines。
% 同时，因为 mmE-Loc 面向的是无人机在接驳阶段的定位，mmE-Loc 在 medium 时的定位性能meets the needs of the drone。

\vspace{-0.1cm}
\subsection{Ablation Study} \label{5.4}
We experimentally analyze core components of mmE-Loc, focusing on enhancements contributed by each component to overall system.

% \noindent 
\textbf{Effectiveness of Multi-modal Fusion.}
% In this part, we demonstrate that fusing radar and event camera is superior to individual of them.
% As illustrated in \fig \ref{fig:ablation}a, the performance of fusion-based approach far exceeds that of radar-only and event-camera-only in terms of location error.
% Specifically, the average location error under our proposed mmE-Loc, event-camera-only-based, and radar-only-based ranging method is 1.28dm, 2.29dm, and 1.78dm, respectively. Our method outperforms the event-camera-only-based by 28\%, and exceeds the radar-only-based solution by 44\%.
We demonstrate the superiority of fusing radar and event cameras over using each sensor individually. 
\fig \ref{fig:ablation}a shows that the fusion-based approach significantly outperforms both radar-only and event camera-only methods in terms of location error. 
mmE-Loc outperforms event-only approach by 63.6\% and exceeds radar-only method by 52.9\%.
% In this section, we demonstrate the superiority of fusing radar and event camera data over using each sensor individually. As shown in \fig \ref{fig:ablation}a, the fusion-based approach significantly outperforms both radar-only and event-camera-only methods in terms of location error. 
% Specifically, our method outperforms the event-camera-only approach by 28\% and exceeds the radar-only method by 44\%.

% \noindent 
\textbf{Contributions of each module.}
% We investigate the contributions of CCT and GAJO to mmE-Loc by gradually integrating them with event camera into the baseline system (\ie, the radar-only-based method) and assessing localization accuracy and end-to-end latency. 
% As depicted in \fig \ref{fig:ablation}b, without these modules, the baseline method achieves a localization error of 2.29$dm$ and a latency of 10.19$ms$. Integrating event camera with the CCT module reduces the localization error to 1.78$dm$ and decreases latency to 7.27$ms$. 
% We then integrate event camera with the GAJO into baseline method and the localization error reduces to 1.39$dm$, although the delay increases due to the absence of an efficient filtering mechanism. 
% Finally, integrating both CCT and GAJO minimizes both the localization error and latency.
We investigate the contributions of \textit{CCT} and \textit{GAJO} to mmE-Loc by gradually integrating them with the event camera into the baseline system (\ie, the radar-only-based method) and assessing localization accuracy and end-to-end latency. 
\fig \ref{fig:ablation}b illustrates that without these modules, the baseline method achieves a localization error of 0.229$m$ and latency of 10.19$ms$.
Integrating the event camera with the \textit{CCT} module reduces the localization error to 0.178$m$ and decreases latency to 7.27$ms$. 
Integrating the event camera with \textit{GAJO} further reduces the error to 0.139$m$, although the delay increases due to the absence of an efficient detection mechanism. 
Finally, integrating both \textit{CCT} and \textit{GAJO} minimizes both the error and latency.
% We examine how \textit{CCT} and GAJO contribute to mmE-Loc. We gradually embed CCJ and GAJO with event camera into the baseline system (\ie radar-only-based method) and repeatedly examine the localization accuracy and end-to-end latency.
% As shown in \fig \ref{fig:ablation}b, without applying these two modules, baseline method achieves a localization error and latency of 2.29dm and 10.19ms. 
% As we integrate \textit{CCT} module into baseline method, we observe the localization error declines to 1.78dm and the
% latency drops to 7.27ms.
% We then integrate GAJO into baseline method and observe that the localization error drops remarkably
% to 1.39dm. However, the delay grows due to the lack of an efficient filtering mechanism.
% Finally, we integrate both \textit{CCT} and GAJO into baseline method. As expected, both the localization error and latency reach the minimum


% \noindent 
\textbf{Performance of \textit{CCT}.}
% We tested CCT's filtering performance on mmWave and event data in both indoor and outdoor scenarios. 
% In \fig \ref{fig:ablation}c, higher recall means more drone-triggered events are preserved, while higher precision indicates more background events are removed. In indoor conditions, CCT achieves recalls over 86\% for mmWave and 89\% for event data, with precision above 85\% and 80\%, respectively. In outdoor conditions, recall and precision remain above 80\% for both data types.
% These results demonstrate CCT's effective event filtering.
We tested \textit{CCT}'s filtering performance on mmWave and event data in both indoor and outdoor scenarios. 
In \fig \ref{fig:ablation}c, higher recall signifies more drone-triggered events preserved, while higher precision indicates more background events removed. 
In indoor conditions, \textit{CCT} achieves recalls over 86\% for mmWave and 89\% for event, with precision above 85\% and 80\%, respectively. 
In outdoor conditions, recall and precision remain above 80\% for both types, demonstrating \textit{CCT}'s effectiveness.
% In both indoor and outdoor scenarios, we specifically tested the filtering performance of CCT on mmWave and event data. We denote the filtering performance of CCT on mmWave data in indoor and outdoor environments as "mmWave Indoor" and "mmWave Outdoor", respectively. Similarly, we denote the filtering performance on event data as "Event Indoor" and "Event Outdoor".
% As shown in Figure \ref{fig:ablation}c, a higher recall indicates that more drone-triggered events are preserved, while a higher precision means more background events are removed. In indoor conditions, CCT achieves recalls of over 86\% for mmWave and 89\% for event data, with precisions exceeding 85\% and 80\%, respectively. Although recall and precision slightly decrease in outdoor conditions, they remain above 80\% for both mmWave and event data.
% These results demonstrate the efficacy of CCT in event filtering.
% 在室内和室外场景中，我们针对性地测试了 CCT 对 mmWave 以及 event 的滤波性能。我们将 CCT 对毫米波在室内和室外滤波的性能分别表示为 mmWave indoor 和 mmWave Ourdoor，respectively。Likewise, 我们将 CCT 对event在室内和室外滤波的性能分别表示为 event indoor 和 event Ourdoor。In \fig \ref{fig:ablation}c, a higher recall means more drone-triggered events are preserved while a higher precision indicates more background events are removed.
% As seen, 在室内条件下，CCT 对 mmWave 和 event 滤波的 recall 分别大于 86\% 和 89\%, 对二者的precision分别大于 85\%和 80\%.在室外条件下，虽然 CCT对 mmWave 和 event 滤波的recall 和precision都有一定程度的下降，但均超过了 80\%.
% This result demonstrates the efficacy of CCT in event filtering.


% \noindent 
\textbf{Performance of \textit{GAJO}.}
% We also compare the localization performance of different multi-modal data fusion strategies. Specifically, we evaluate our novel factor graph-based fusion framework against two widely used approaches: the extended Kalman filter (EKF) and Graph Optimization (GO). 
% As shown in \fig \ref{fig:GAJO}, mmE-Loc improves localization performance by over 38\% compared to EKF and 20\% compared to GO. This outstanding performance is due to the factor graph's tightly coupled multi-modal data fusion and our adaptive optimization method.
We also compare the performance of different multi-modal fusion strategies. 
Specifically, we evaluate mmE-Loc against two widely used approaches: the extended Kalman filter (EKF) and Graph Optimization (GO). 
As shown in \fig \ref{fig:ablation}d, mmE-Loc enhances localization performance by over 57.9\% compared to EKF and 47.1\% compared to GO, due to its tightly coupled multi-modal fusion and adaptive optimization method.
% \fig \ref{fig:GAJO} illustrates that mmE-Loc improves localization performance by over 38\% compared to EKF and 20\% compared to GO, attributing to the mmE-Loc's tightly coupled multi-modal fusion and the adaptive optimization method.
% We also compare the localization performance with different multi-modal data fusion strategies.
% In this part of the experiment, we compare our novel factor graph based fusion framework with another two fusion approaches, extended Kalman filter (EKF) and Graph Optimization (GO), both widely used in previous works.
% As \fig \ref{fig:GAJO} illustrated, mmE-Loc enhanced the localization performance for more than 38\% and 20\% compared to EKF and GO, respectively.
% These outstanding performance, on the one hand, is due to the leverage of the factor graph as a tightly coupled manner to fuse multi-modal data; on the other hand, lies in our adaptive optimization method.


% \vspace{-0.3cm}

% As a reminder, mmE-Loc stands out from existing model-based and learning-based methods due to its extremely low latency and minimal resource overhead. \fig \ref{fig:latency} illustrates the end-to-end latency (including delays from the CCT and GAJO modules) throughout the localization process. 
% The average end-to-end latency of mmE-Loc is around 3.58$ms$, with the CCT module contributing an average delay of 0.09 $ms$ and the GAJO module adding 3.49$ms$. 
% During drone localization, latency of mmE-Loc may fluctuate due to the adaptive optimization method, which jointly optimizes different sets of locations at different times.
% Nonetheless, mmE-Loc's latency remains suitable for use in flight control loops.
% Additionally, \fig \ref{fig:cpu} and \fig \ref{fig:memory} show that CPU usage does not exceed 20\%, with average memory usage under 120 $MB$ during localization process.
\vspace{-0.3cm}
\subsection{System Efficiency Study} \label{5.5}
\revise{

The mmE-Loc distinguishes itself from existing models and learning-based methods due to its low latency and minimal resource overhead. 
\fig \ref{fig:latency} illustrates the end-to-end latency (including delays from \textit{CCT} and \textit{GAJO} modules) throughout the localization process. 
The average end-to-end latency of mmE-Loc is around 3.58$ms$, with the \textit{CCT} module contributing an average delay of 0.09$ms$ and the \textit{GAJO} module adding 3.49$ms$. 
During drone localization, mmE-Loc's latency may fluctuate due to the adaptive optimization method, which optimizes different sets of locations at different times. 
Nonetheless, mmE-Loc's latency remains suitable for use in flight control loops. 
\fig \ref{fig:cpu} and \fig \ref{fig:memory} indicate that CPU usage doesn't exceed 20\%, with memory usage under 120$MB$.
Meanwhile, memory usage increases as the location set size grows.
% Memory usage fluctuates due to adaptive optimization method, which dynamically selects location sets for optimization and, under certain conditions, optimizes all locations, as described in Sec. 4.1. 
% The varying sizes of location sets result in different memory consumption.
}
% The memory usage experience a fluctuate due to the adaptive optimization method, which will adaptively select collection of locations for optimization and optimize all locations in certain conditions as described in Sec 4.1.
% Different size of collection leads to different memory usage.

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-1cm}
    \centering
        \includegraphics[width=1.95\columnwidth]{Figs/casestudy.png}
        \vspace{-0.15cm}
    \caption{Case Study: Delivery Drone Airport. }
    % We incorporate the event camera with mmWave radar for delivery drone localization.
    \label{airport}
    \vspace{-0.15cm}
\end{figure*} 


% As a reminder, compared to existing model-based and learning-based methods, it is our superiority that mmE-Loc can run with extremely low latency and does not require considerable resource overhead.
% \fig \ref{fig:latency} illustrates the end-to-end latency (including CCT module delay and GAJO module delay) through the whole localization process.
% As seen, the average end-to-end latency of mmE-Loc is around 3.58ms,其中，CCT 模块的平均延迟为 0.09ms，GAJO 模块的平均延迟在 3.49ms。
% 在对 drone 进行 localization 的过程中，mmE-Loc 的定位延迟会有一定程度的波动，这是因为 adaptive optimization method 会在不同时刻对不同的位置集合进行联合优化。
% 尽管如此，mmE-Loc 的定位延迟依然is compatible with use in flight control loops;
% \fig \ref{fig:cpu} and \fig \ref{fig:memory} further depict that the CPU usage on the PC does not exceed 20\% with an average memory usage under 120MB in the localization process.


\vspace{-0.2cm}
\section{Case Study: Drone Airport}
As shown in \fig \ref{airport}a, to verify the system's usability, we conduct an experiment using a custom drone equipped with six propellers and managed by a PX4 flight controller. 
This drone is developed by a world-class delivery company exploring the feasibility of instant deliveries.
The experiment takes place at a real-world delivery drone airport.
To enable drone localization over a larger area, we employ an ARS548 mmWave radar, as depicted in \fig \ref{airport}b. 
% \fig \ref{airport}c, \fig \ref{airport}d, and \fig \ref{airport}e illustrate the localization results as the drone flying a squared spiral trajectory at an altitude of 30 $m$. 
\fig \ref{airport}c, \fig \ref{airport}d, and \fig \ref{airport}e illustrate the localization results as the drone follows a square spiral trajectory at an altitude of 30 $m$.
% The result demonstrates that, under real-world scenarios, apart from cm-level localization precision at discrete points, our mmE-Loc was also able to produce smooth trajectories, which fit closely to the RTK ones, proving that our solution has great potential for repalcing RTK for aiding drone landings in some challenging sceens, such as the city conyons, where the RTK may fail due to block or multipath effects.
% The results demonstrate that, in real-world scenarios, mmE-Loc not only achieves high localization precision with maximum error below 0.5$m$ at discrete points but also produced smooth trajectories closely matching the RTK ones. 
% This indicates that our solution has great potential to work as a complementary system to RTK in aiding drone landings in challenging environments, such as city canyons, where RTK may fail due to blockage or multipath effects.
The results show that in real-world scenarios, mmE-Loc achieves high localization precision, maintaining a maximum absolute location error error below 0.5$m$ and relative location error error below 0.1$m$, and producing smooth trajectories that closely align with those of RTK. 
mmE-Loc has significant potential as a complementary system to RTK, aiding drone in challenging environments (\eg, urban canyons, where RTK may be compromised by signal blockage).
% or multipath effects.
% The plots demonstrate that mmE-Loc successfully localized the drone throughout the entire flight compare with RTK.