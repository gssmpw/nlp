% \vspace{-0.3cm}
\revise{\section{Introduction}}
% Drones have been one of the most disruptive inventions in recent years, giving rise to numerous innovative applications such as last-mile delivery, sky networking, and industrial inspection. 
% For example, in recent years, the roll-out of 5G networks in cities has made it possible for drones to perform beyond-line-of-sight operations and achieve instant asset delivery.
% Specifically, a drone can now fly autonomously to deliver goods over long distances without a human pilot on-site, while being monitored in real-time via an Internet-connected back-end system.
% The advantages of using drones for delivery are clear: drones can bypass complex urban traffic and deliver packages much faster, which is ideal for time-sensitive deliveries such as medical supplies. 
% Many companies are exploring the commercial feasibility of instant deliveries with drones, including Amazon, Alphabet's Project Wing, Walmart, JD.com, Domino’s, UPS, Ele.me, and Meituan.
% Drones have become one of the most disruptive inventions in recent years, leading to applications like last-mile delivery and industrial inspection. 
% For example, the roll-out of 5G networks in cities has enabled beyond-line-of-sight operations, allowing drones to deliver goods autonomously over long distances while being monitored in real-time. 
% These drones can bypass urban traffic, making them ideal for time-sensitive deliveries such as medical supplies. 
% Companies like Amazon, Alphabet's Project Wing, Walmart, JD.com, Domino’s, UPS, Ele.me, and Meituan are exploring the commercial potential of instant drone deliveries.
Drones have become one of the most disruptive technologies, with applications ranging from last-mile delivery \cite{floreano2015science} to industrial inspection \cite{bartolini2020multi}. 
The roll-out of 5G networks in cities enhances their capabilities, allowing drones to autonomously deliver goods over long distances while being monitored in real-time \cite{bertizzolo2021streaming}. 
Bypassing urban traffic, drones are especially effective for time-sensitive deliveries, such as medical supplies \cite{MedicalDelivery}. Various businesses, including Amazon \cite{Amazondrone}, Alphabet Wing \cite{Wingdrone}, and Meituan \cite{Meituandrone}, are exploring the viability of drones for instant deliveries, as shown in \fig \ref{intro}a.
% Drones have emerged as one of the most disruptive technologies in recent years \cite{floreano2015science}, with applications ranging from last-mile delivery\cite{xiang2021reusing} to industrial inspection \cite{bartolini2020multi}. 
% The roll-out of 5G networks in cities further enhances their capabilities, allowing drones to autonomously deliver goods over long distances while being monitored in real-time \cite{bertizzolo2021streaming}. 
% Bypassing urban traffic, drones are especially effective for time-sensitive deliveries (\eg medical supplies \cite{MedicalDelivery}).
% A variety of businesses are examining the viability of drones for instant deliveries, including Amazon\cite{Amazondrone}, Alphabet Wing\cite{Wingdrone}, Meituan\cite{Meituandrone}, as illustrated in \fig \ref{intro}a.

% Takeoff and landing are critical steps in the operation of a drone because during these stages, the drone is most vulnerable \cite{wang2022micnest, xu2023taming, zhao2023smoothlander}. 
% Specifically, the risk of damaging the drone or its surroundings is highest during these stages, which lead to financial losses and pose a threat to human safety, creating a significant barrier to the widespread adoption of drones\cite{Russiandrone}.
% Accurate and low-latency localization is crucial for the effective takeoff and landing of drones \cite{sun2023indoor}, as illustrated in \fig \ref{intro}b.
% Higher localization accuracy increases the likelihood of successful takeoff and landing on the designated platform, while lower localization latency provides more time for the drone to react to unexpected situations \cite{famili2022pilot, he2023acoustic, chi2022wi}. 

Takeoff and landing are critical phases in drone operation, requiring accurate, low-latency localization due to the heightened vulnerability of the drone during these stages , as shown in \fig \ref{intro}b \cite{wang2022micnest, zhao2023smoothlander, sun2023indoor}. 
During these phases, the risk of damage to the drone or its surroundings is elevated, potentially resulting in financial losses and posing safety risks, which hinder the widespread adoption of drones \cite{Russiandrone}. 
Enhanced localization accuracy improves the likelihood of successful takeoff and landing on the intended platform, while reduced localization latency allows the drone to respond more effectively to unexpected situations \cite{famili2022pilot, he2023acoustic, chi2022wi}. 

\begin{figure}[t]
    \setlength{\abovecaptionskip}{-0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.24cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=1\columnwidth]{Figs/intro.png}
        \vspace{-0.2cm}
    \caption{Illustration of Delivery Drone.}
    \label{intro}
    \vspace{-0.4cm}
\end{figure} 



Unfortunately, current methods are not able to offer feasible solutions for accurate and low-latency localization of drone, which can be divided into two categories: \\
$\bullet$ \textbf{Extra infrastructure based solutions.} 
These solutions rely on external devices (\eg, GPS\cite{nirjon2014coin}, RTK\cite{wang2022micnest}, WiFi\cite{hu2023wisdom, chen2015drunkwalk}, microphone\cite{guo2021infrastructure, weiguo2020symphony}) to provide reference signals for localization, which require LoS connection between localization infrastructure and drones.
However, in densely built urban environments, the accuracy and environmental compatibility of these methods significantly degrades as altitude decreases, rendering ineffective for low-altitude operations \cite{li2021infocom_itoloc}. \\
$\bullet$ \textbf{Intra on-board sensor based solutions.}  
These solutions leverage onboard sensors (\eg, cameras\cite{xu2019ivr}, IMUs\cite{lu2020milliego}, lidar\cite{he2021vi} or radar\cite{dai2023interpersonal}), combined with simultaneous localization and mapping (SLAM) techniques\cite{campos2021orb} or visual markers\cite{krogius2019iros}, to achieve high-precision location estimation. 
% However, due to sensor performance limitations, these methods have significant localization latency, which is insufficient for the rapid localization requirements during drone landing \cite{xu2021followupar}.\\
However, the low spatio-temporal sampling resolution of these onboard sensors poses a challenge for drones to achieve timely and accurate localization during landing.\\
\textbf{Remark.} The absence of the efficient algorithms and sensors make it challenging for existing methods to achieve timely and accurate localization for drone, which requires $(i)$ $cm-level$ localization accuracy and $(ii)$ $ms-level$ localization latency to feed the control loop for effective takeoff and landing. \\
% Event cameras are innovative bio-inspired sensors that asynchronously report pixel-wise intensity changes. With microsecond resolution, they capture high-speed motions without blurring, making them ideal for tasks like high-speed feature and motion tracking.
% However, like RGB cameras, event cameras face scale uncertainty and cannot determine object depth, hindering accurate drone localization. 
% Current methods address this by integrating depth cameras or using dual event cameras. Yet, depth cameras' lower update frequency diminishes event cameras' advantages, and the significant noise in event cameras leads to lower-quality data and higher pose estimation errors.
% \begin{figure*}[t]
%     \setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
%     \setlength{\belowcaptionskip}{-0.3cm}
%     \setlength{\subfigcapskip}{-0.25cm}
%     \centering
%         \includegraphics[width=2.1\columnwidth]{evaFigs/related.png}
%         \vspace{-0.2cm}
%     \caption{Principle of event camera and performance of existing solutions at different settings.}
%     \label{relatedwork}
%     \vspace{-0.2cm}
% \end{figure*} 

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{evaFigs/relatedall_2.png}
        % \vspace{-0.2cm}
    \caption{Benchmark study on drone localization and performance of existing solutions at different settings.}
    \label{relatedwork}
    % \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Localization with event camera and mmWave radar.}
% Event cameras are bio-inspired sensors that asynchronously report pixel-wise intensity changes. With microsecond resolution, they capture high-speed motions without blurring, ideal for tasks like high-speed feature and motion tracking. However, like RGB cameras, they face scale uncertainty and cannot determine object depth, hindering accurate drone localization. Integrating depth cameras or using dual event cameras helps, but depth cameras' lower update frequency and event cameras' noise reduce data quality and increase pose estimation errors.
% We find that millimeter-wave (mmWave) radar, which can transmit and receive frequency-modulated continuous waves (FMCW) with ms-level latency, measuring the relative angle and distance of moving objects. 
% Both mmWave radar and event camera share ms-level time resolution, inspiring us to fuse them for high-accuracy, low-latency drone localization. 
% Event cameras are bio-inspired sensors that asynchronously report pixel-wise intensity changes with ms-level latency, capturing high-speed motions without blurring, ideal for tasks like feature and motion tracking. 
% However, similar to frame cameras, they struggle with scale uncertainty, hindering utilization in accurate drone localization. 
% Integrating depth cameras or using dual event cameras may help, but lower update frequency of depth cameras and noise of event cameras increase localization error and latency.
% Millimeter-wave (mmWave) radar, which transmits and receives frequency-modulated continuous waves (FMCW) with ms-level latency, measures the relative angle and distance of moving objects. 
% Both event camera and mmWave radar share ms-level time resolution, inspiring us to fuse them for high-accuracy, low-latency drone localization.
Event camera is bio-inspired sensor that report pixel-wise intensity changes with $ms-level$ latency \cite{xu2023taming}, capturing high-speed motions without blurring \cite{he2024microsaccade}, ideal for tasks like feature and motion tracking \cite{alzugaray2018asynchronous}.
% as shown in \fig \ref{relatedwork}a. 
However, they struggle with scale uncertainty, hindering accurate drone localization \cite{falanga2020dynamic}. 
Integrating depth cameras or using dual event cameras may help, but the lower update frequency of depth cameras and noise from event cameras increase localization latency and error \cite{he2021fast}. 
Millimeter-wave (mmWave) radar, utilizing frequency-modulated continuous waves (FMCW) with $ms-level$ latency, measures the relative angle and distance of moving objects \cite{woodford2023metasight, zheng2023neuroradar}. 
Both event camera and mmWave radar share $ms-level$ time resolution, inspiring us to fuse them for accurate, low-latency drone localization.
However, according to our benchmark study conducted with setup in \fig \ref{relatedwork}a, translating this intuition into a practical system is non-trivial and faces significant challenges:\\
% \noindent $\bullet$ \textbf{C1: Noisy sensing results impairs drone detection and tracking.}
% Environmental unexpected changes often introduce irrelevant information in the sensing results from the event camera and mmWave radar, hindering the system's ability to identify signals changes caused by the drones to be tracked, as shown in \fig \ref{relatedwork}b. 
% However, traditional noise filtering algorithms\cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} achieve a low event and mmWave point cloud filtering rate (recall and precision < 65\% in \fig \ref{relatedwork}c) due to their rule-based pipelines, resulting in detection precision bottlenecks.
% Additionally, modern flight control loops operate at frequencies exceeding 400 Hz, requiring the system to filter the noise and track the drone rapidly. \\
\noindent $\bullet$ \textbf{C1: Noise distribution characteristics of both modalities differ, hindering drone detection.}
% Both sensor modalities yield not only heterogeneous information about the drone but also generate significantly heterogeneous noise. 
These two sensor modalities not only provide different types of information but also generate significantly heterogeneous noise. 
Event cameras produce noise due to unexpected changes in brightness conditions, whereas mmWave radar struggles with noise caused by signal multipath effects.
These noises differ greatly in both dimension and pattern, which can also be asynchronous, especially under high temporal resolution (\fig \ref{relatedwork}b).
This spatial and temporal heterogeneity complicates noise filtering, causing detection bottlenecks \cite{xu2023taming}.
Unfortunately, existing traditional noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} typically target a single modality, resulting in low noise event and point cloud filtering rates (recall and precision < 65\% in \fig \ref{relatedwork}c), limiting their effectiveness in our scenario.\\
% \noindent $\bullet$ \textbf{C2: Heterogeneous data fusion delays drone localization.}
% Once the drone is detected, accurate 3D spatial localization of it is essential, which is more time-consuming than detection and tracking due to additional processing (\eg, feature matching). 
% As depicted in \fig \ref{relatedwork}d, although the localization accuracy is boosted, existing methods Baseline-I, -II and -III introduces significant delays, mismatching with flight control loops.
% Moreover, event cameras provide asynchronous event streams, while mmWave radar generates sparse point clouds with relatively low spatial resolution. 
% Previous fusion methods (\eg, extended Kalman filters and graph optimization in \fig \ref{relatedwork}d) often suffer from significant cumulative drift error and lengthy processing times, rendering them inadequate for accurate and low-latency localization.
\noindent $\bullet$ \textbf{C2: Ultra-large data volume burden the heterogeneous data fusion, delaying drone localization.}
Accurately estimating 3D location of the drone after detection involves time-consuming processing steps, such as sensor fusion and optimization. 
% Once the drone is detected, we proceed to perform 3D localization on it.
% Accurately estimating 3D spatial location of drone involves several time-consuming processing steps, including the sensor fusion and optimization.
The ultra-large amount of data due to the high frequency further exacerbates the processing time, causing significant delays \cite{xu2021followupar}.
Meanwhile, the asynchronous event streams and sparse point clouds are heterogeneous in terms of precision, scale, and density, add complexity the sensor fusion.
Existing methods (\eg, Extended kalman filter, particle filter, and graph optimization) suffer from cumulative drift error, heterogeneity issues and lengthy processing latency, rendering them inadequate for accurate and low-latency localization as shown in \fig \ref{relatedwork}d \cite{zhao20213d, falanga2020dynamic, mitrokhin2018event, grisetti2010tutorial}.

% \noindent \textbf{Our work.}
% In this work, we delve into the sensing principles of event camera and mmWave radar and propose HFPL, a system that provides $cm-level$ accurate drone localization with $ms-level$ latency. 
% HFPL features three key designs to fully harness the potential of events and mmWave point cloud for drone localization and is implemented with adaptively acceleration algorithms to further improve accuracy and reduce latency, as elaborated below: \\
% \noindent $\bullet$ \textit{On system architecture front.}
% By incorporating event camera and mmWave radar, sharing $ms-level$ time resolution, we improve drone localization accuracy and reduce latency at the data source.
% HFPL features a carefully designed system architecture that tightly couples event camera and mmWave radar. 
% This integration spans from early-stage filtering to later-stage fusion and optimization, fully extracting and leveraging the unique advantages of both sensors (§\ref{3.2}). \\
% \noindent $\bullet$ \textit{On system algorithm front.}
% We first introduce the Consi-stency-Instructed Collaborative Tracking (\textit{CCT}) algorithm to utilize the disparities in sensing principles of both modalities to filter out unexpected environment-triggered events and point clouds from a large volume of data with a low false positive rate, enhancing the tracking performance (§\ref{4.1}). 
% We then present the Graph-Informed Adaptive Joint Optimization (\textit{GAJO}) algorithm to fully fuse drone-related information from both modalities, reducing latency of the optimization in localizing the drone (§\ref{4.2}). \\
% \noindent $\bullet$ \textit{On system implementation front.}
% We further analyze the sources of latency and propose an Adaptive Optimization method for boosting the \textit{GAJO}. This method dynamically optimizes the set of locations rather than relying on a fixed sliding window, further enhancing the accuracy of localization and reducing latency (§\ref{5.1}).

% \noindent \textbf{Evaluation and Result.} 
% We fully implement HFPL with COTS mmWave radar and event camera and deploy it on drone landing platforms in both indoor and outdoor environments. 
% Extensive experiments are conducted with different flight conditions. 
% We compared the end-to-end drone localization accuracy and latency of HFPL with three SOTA methods.
% % $(i)$ a radar-based localization system \cite{zhao20213d}, $(ii)$ an event camera-based localization system \cite{falanga2020dynamic}, and $(iii)$ a dual camera-based localization system .
% % Over 30 hours of real-world experiments and 400GB of data collection, we show that HFPL meets the required standards, achieving a drone localization accuracy of \notice{xx} dm, outperforming the baselines by over \notice{xx\%}, a average latency of location update of \notice{xx} ms, suitable for flight control loops.
% Through over 30 hours of real-world experiments and 400GB of data collection, we demonstrate that HFPL meets the required standards. HFPL achieves a localization accuracy of 1.01 $dm$, surpassing all baselines by >50\%. HFPL further achieves localization latency of 5.15 $ms$, outperforming baselines by 50\% in average, suitable for flight control loops.
% Additionally, HFPL is shown to be marginally affected by factors such as drone type, velocity, and envir. conditions.\\
% \textbf{Real-world deployment.}
% We have deployed the sensing platform and HFPL at a real-world drone delivery airport as shown in \fig \ref{intro} to demonstrate usability of the system.
% 10 hours study shows that HFPL meets drone landing demands within the constraints of available resources.

% \begin{figure*}[t]
%     \setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
%     \setlength{\belowcaptionskip}{-0.3cm}
%     \setlength{\subfigcapskip}{-0.25cm}
%     \centering
%         \includegraphics[width=2.1\columnwidth]{Figs/overview.png}
%         \vspace{-0.2cm}
%     \caption{System architecture of HFPL.}
%     \label{overview}
%     \vspace{-0.2cm}
% \end{figure*} 
\noindent \textbf{Our work.}
% We explore the sensing principles of the event camera and mmWave radar and propose EventLoc, an low latency-oriented event camera enhancement system that provides cm-level accurate 3D object localization with millisecond level latency to enable application of event camera in various 3D vision tasks.
We delve into the sensing principles of event cameras and mmWave radar, introducing EventLoc. 
This system enhances event camera functionality with a focus on low-latency drone localization, providing cm-level accuracy with millisecond latency in average. 
% As a result, EventLoc broadens event camera application in diverse 3D vision tasks.
In detail, EventLoc features three key designs to fully unleash the potential of event camera and mmWave radar for drone localization: \\
% and is implemented with adaptively acceleration algorithms to further improve accuracy and reduce latency, 
\noindent $\bullet$ \textbf{On system architecture front.}
By incorporating mmWave radar with millisecond latency, we enhance the performance of event camera and improve 3D localization performance at the data source.
EventLoc features a carefully designed system architecture that tightly couples event camera and mmWave radar. 
This integration spans from early-stage filtering to later-stage fusion and optimization, fully leveraging the unique advantages of both sensors (§\ref{3.2}). \\
\noindent $\bullet$ \textbf{On system algorithm front.}
We first introduce the Consi-stency-Instructed Collaborative Tracking (\textit{CCT}) algorithm to extract \textit{consistent information} in sensing data from both modalities to filter out environment-triggered noise with a low false positive rate, enhancing the detection performance with a low-latency (§\ref{4.1}). 
We then present the Graph-Informed Adaptive Joint Optimization (\textit{GAJO}) algorithm to fully fuse \textit{complementary information} from both modalities, accelerating the optimization in localizing the object (§\ref{4.2}). \\
\noindent $\bullet$ \textbf{On system implementation front.}
We further analyze the sources of latency and propose an Adaptive Optimization method for boosting the \textit{GAJO}. 
This method dynamically optimizes the set of locations rather than relying on a fixed sliding window, further enhancing the accuracy of localization and reducing latency (§\ref{5.1}).

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.4cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/overview2.png}
        \vspace{-0.2cm}
    \caption{System architecture of EventLoc.}
    \label{overview}
    % \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Evaluation and Result.} 
We fully implement EventLoc with COTS event camera and mmWave radar.
Extensive experiments in indoor/outdoor environments are conducted with different drone flight conditions to comprehensively evaluate performance of EventLoc.
We compare the end-to-end drone localization accuracy and latency of EventLoc with three SOTA methods.
% Through over 30 hours of real-world experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving a localization accuracy of 1.01 $dm$, surpassing all baselines by >50\%. EventLoc further achieves localization latency of 5.15 $ms$, outperforming baselines by >50\% in average.
Through over 30 hours experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving a average localization accuracy of 0.101 $m$ and latency of 5.15 $ms$, surpassing all baselines by >50\% in average.
Additionally, EventLoc is marginally affected by factors such as drone type and envir. conditions.\\
\textbf{Real-world deployment.}
We have deployed the sensor platform with EventLoc at a real-world drone delivery airport as shown in \fig \ref{relatedwork}a to demonstrate practicability of the system.
10 hours study shows that EventLoc meets drone landing demands within the constraints of available resources.


% \noindent \textbf{Contributions.}
% \scalebox{1.2}{\ding{182}} We propose HFPL, a novel framework that tightly fuses asynchronous events and sparse point cloud to localize landing drone. HFPL can be integrated with existing GPS/RTK and visual marker to facilitate safe drone landing. 
% \scalebox{1.2}{\ding{183}} We propose the $CCT$, a light-weight cross-modal filter to push the limit of detection and tracking accuracy of drone by leveraging the differing trigger principles of the event camera and mmWave radar. 
% \scalebox{1.2}{\ding{184}} We design the $GAJO$, a factor graph-based optimization framework that fuses heterogeneous data to enhance localization accuracy and reduce latency by fully harnessing the potential of both modalities.
% \scalebox{1.2}{\ding{185}} We implement and extensively evaluate HFPL by comparing it with three SOTA methods. Evaluation results show the effectiveness of HFPL. We also deploy HFPL in a real-world drone delivery airport, demonstrating feasibility of HFPL.
\noindent \textbf{Contributions.} This paper makes following contributions.

\noindent $(1)$ We propose EventLoc, a novel low latency-oriented drone localization system. It tightly integrates asynchronous events and mmWave radar sparse point clouds, achieving accurate drone localization with millisecond latency.\\
\noindent $(2)$ We propose the $CCT$, a light-weight cross-modal noise filter to push the limit of detection accuracy by leveraging the \textit{consistent information} from both modalities. \\
\noindent $(3)$  We propose the $GAJO$, a factor graph-based optimization framework that fully harnessing \textit{complementary information} from both modalities to enhance localization performance.\\
% accuracy and latency
\noindent $(4)$ We implement and extensively evaluate EventLoc by comparing it with three SOTA methods, showing its effectiveness. We also deploy EventLoc in a real-world drone delivery airport, demonstrating feasibility of EventLoc.