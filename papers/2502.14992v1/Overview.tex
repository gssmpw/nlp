% \vspace{-0.2cm}
\section{System Overview}
The mmE-Loc enhances the mmWave radar with an event camera to achieve accurate and low-latency drone ground localization, allowing the drone to rapidly adjust its location state and perform a precise landing.
% Given the paramount importance of safety in commercial drone operations, mmE-Loc can collaborate with RTK or visual markers to ensure precise landings.
Given the critical importance of safety in commercial drone operations, mmE-Loc can work in conjunction with RTK or visual markers to ensure precise landing performance.
In this section, we mathematically introduce the problem that mmE-Loc tries to address and provide an overview of the system design.

% mmE-Loc leverages the integration of mmWave radar and event camera data to achieve accurate, low-latency drone ground localization, allowing the drone to quickly adjust its position and perform precise landings. Given the critical importance of safety in commercial drone operations, mmE-Loc can also work in conjunction with RTK or visual markers to ensure precise landing performance.

% enabling a reliable and accurate localization service for drones (\fig \ref{overview}).

\vspace{-0.3cm}
\subsection{Problem Formulation}
In this section, we illustrate key variables in mmE-Loc and introduce the system's inputs and outputs.

% \begin{figure}[t]
%     \setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
%     \setlength{\belowcaptionskip}{-0.3cm}
%     \setlength{\subfigcapskip}{-0.25cm}
%     \centering
%         \includegraphics[width=1\columnwidth]{Figs/reference.png}
%         \vspace{-0.2cm}
%     \caption{Illustration of reference systems and essential variables in mmE-Loc.}
%     \label{reference}
%     % \vspace{-0.6cm}
% \end{figure} 

\textbf{Reference systems.} \label{3.2}
% As shown in \fig \ref{reference}, t
There are four reference (\aka, coordinate) systems in mmE-Loc: 
$(i)$ the Event camera reference system $\mathtt{E}$; 
$(ii)$ the Radar reference system $\mathtt{R}$; 
$(iii)$ the Object reference system $\mathtt{O}$;
$(iv)$ the Drone reference system $\mathtt{D}$.
Note that a drone can be considered as an object.
For clarity, before an object is identified as a drone, we utilize $\mathtt{O}$. 
Once confirmed as a drone, we use $\mathtt{D}$ for the drone and continue using $\mathtt{O}$ for other objects.
Throughout the operation of system, $\mathtt{E}$ and $\mathtt{R}$ remain stationary and are rigidly attached together, while $\mathtt{O}$ and $\mathtt{D}$ undergo changes in accordance with movement of the object and the drone, respectively. 
The transformation from $\mathtt{R}$ to $\mathtt{E}$ can be readily obtained from calibration \cite{wang2023vital}. 

\textbf{Goal of mmE-Loc.}
The goal of mmE-Loc is to determine 3D location of the drone, defined as $t_{\mathtt{ED}}$, the translation from coordinate system $\mathtt{D}$ to $\mathtt{E}$.
Specifically, mmE-Loc optimizes and reports 3D location of drone $(l_x, l_y, l_z)$ at each timestamp $i$ with input from event stream and radar sample.
% where $t^i_{\mathtt{ED}}$ represents the translation vector from $\mathtt{D}$ to $\mathtt{E}^3$. 
$t_{\mathtt{ED}}$ and ($l_x$, $l_y$, $l_z$) are equivalent representations of the drone’s location and can be inter-converted with Rodrigues’ formula \cite{min2021joint}. 
The former representation is adopted in the paper, as it is commonly used in drone flight control systems.

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.3cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.2cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=1.85\columnwidth]{Figs/trackingmodel.png}
        \vspace{-0.3cm}
    \caption{Illustration of tracking models in Consistency-instructed Collaborative Tracking algorithm.}
    \label{CCT}
    \vspace{-0.2cm}
\end{figure*} 

% We first briefly describe and illustrate some essential variables in mmE-Loc and the problem of localizing the landing drone，which is the 3D location ($l_x$, $l_y$, $l_z$).
% \todo{As shown in \fig )}, there are three reference (\aka, coordinate) systems in mmE-Loc: $(i)$ the Event camera reference system \mathtt{E}; $(ii)$ the Radar reference system \mathtt{R}; $(iii)$ the Drone reference system \mathtt{D}.


% \subsection{mmE-Loc: System goals}
% \noindent $\bullet$ \textbf{How to detect and track the drones from noisy sensing results at a high frequency?}
% Environmental changes often introduce irrelevant information in the sensing results from the event camera and mmWave radar, hindering the system's ability to identify signals changes caused by the drones to be tracked. 
% Additionally, modern flight control loops operate at frequencies exceeding 400 Hz, requiring the system to detect and track drones rapidly. 
% However, traditional noise filtering, object detection and tracking algorithms have high time complexities, resulting in precision and tracking frequency bottlenecks. 
% \todo{Explanation in figures or tables.}

% \noindent $\bullet$ \textbf{How to fuse two types of heterogeneous data to precisely localize drones at a high frequency?} 
% Once the drone is detected, accurate 3D spatial localization of it is essential, which is more time-consuming than detection and tracking due to additional processing. 
% Moreover, event cameras provide asynchronous event streams, while mmWave radar generates sparse point clouds with relatively low spatial resolution. 
% Previous fusion methods (\eg extended Kalman filters and particle filters) often suffer from significant cumulative drift error and lengthy processing times, rendering them inadequate for high-frequency and high-precision tracking.
% \todo{Explanation in figures or tables.}

\vspace{-0.3cm}
\subsection{Overview}
% mmE-Loc is a localization system designed for high-frequency and precise localization of drones, enabling them to rapidly adjust their location state and achieve accurate landing. 
As illustrated in \fig \ref{overview}, mmE-Loc comprises two key modules: 
% $(i)$ \textit{CCT} (\textbf{C}onsistency-instructed \textbf{C}ollaborative \textbf{T}racking) for noise filtering and drone detection and 
% $(ii)$ \textit{GAO} (\textbf{G}raph-informed \textbf{A}daptive \textbf{O}ptimization) for localization of drones from the integrated event stream and mmWave 3D point cloud. \todo{Fix the figure.}

\noindent $\bullet$ 
The \textit{CCT} (\textbf{C}onsistency-instructed \textbf{C}ollaborative \textbf{T}racking) for noise filtering, drone detection, and preliminary localization of the drone.
This module utilizes time-synchronized event streams and mmWave radar measurements as inputs. 
Subsequently, the \textit{Radar Tracking Model} processes radar measurements to generate a sparse 3D point cloud. 
Meanwhile, the \textit{Event Tracking Model} takes into the stream of asynchronous events for event filtering, drone detection, and tracking. 
% Finally, \textit{Consistency-instructed Measurements Filter} aligns the results of the both tracking models leverage the consistency between both modalities, and then utilizes periodic micro motion of drone to extract drone-related measurements,  eliminate noise and error detections, and roughly localize the drone.
Finally, \textit{Consistency-instructed Measurements Filter} aligns the outputs of both tracking models by leveraging \textit{temporal-consistency} between the two modalities. 
It then utilizes the drone's periodic micro-motion to extract drone-specific measurements and achieve drone preliminary localization.

\noindent $\bullet$
The \textit{GAJO} (\textbf{G}raph-informed \textbf{A}daptive \textbf{J}oint \textbf{O}ptimization) for fine localization and trajectory optimization of the drone.
% GAJO initially derives preliminary estimates of drone motion and location, using radar measurements and event camera tracking results, respectively. 
% GAJO includes a carefully designed \textit{factor graph-based optimization} method, which 通过深入挖掘事件相机与radar的潜力， jointly fuses and refines the outputs from both the \textit{Event Tracking Model} and \textit{Radar Tracking Model} to accurately determine the location of drone. 
Based on the operational principles of two sensors and their respective noise distributions, \textit{GAJO} incorporates a meticulously designed \textit{factor graph-based optimization} method. 
This module employs the \textit{spatial-complementarity} from both modalities to unleash the potential of event camera and mmWave radar in drone ground localization.
Specifically, \textit{GAJO} jointly fuses the preliminary location estimation from the \textit{Event Tracking Model} and the \textit{Radar Tracking Model} and adaptively refines them, determining the fine location of drone with $ms$-level processing time.
% Finally, to expedite the process of the \textit{factor graph-based location optimization} method, we transform factor graph into a Bayes tree through elimination and optimize the drone's location adaptively via a \textit{Incremental Optimization method}. 