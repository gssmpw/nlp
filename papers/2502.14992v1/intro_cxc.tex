\vspace{-0.5cm} 
\section{Introduction}

Event cameras are innovative bio-inspired sensors that asynchronously report pixel intensity changes as events with \textit{millisecond latency} (\fig \ref{intro}a), instead of fixed interval frames \cite{he2024microsaccade, gehrig2024low}.  
With high temporal resolution and a high dynamic range, event cameras excel at capturing high-speed motions without blurring and perform well under varied illumination conditions \cite{falanga2020dynamic, xu2023taming}.
Thus, event cameras are envisioned as an ideal solution for 2D vision tasks (\eg, low latency and accurate object detection in \fig \ref{intro}b \cite{gallego2020event}).

Although event cameras excel in 2D vision tasks, they face fundamental challenges in 3D vision, preventing their full potential from being realized.
Specifically, 3D object localization, identifying the location of objects within the camera's field of view in three dimensions, is a fundamental function for various vital 3D vision tasks (\eg, drone localization \cite{wang2022micnest}, AR/MR \cite{xu2021followupar}).
However, event cameras, capturing only 2D images without depth details, can't directly gauge object distance, causing scale uncertainty. 

This limitation restricts event cameras in 3D object localization (\fig \ref{intro}c), hindering the exploitation of their low-latency advantage in 3D vision tasks.
To address this, two types of solutions are proposed:


\noindent $\bullet$ \textbf{Events only-based solutions.}
These methods solely utilize event data for object depth estimation and fall into two types. 
$(i)$ Incorporating known geometric information with observations to deduce depth relies heavily on prior knowledge, leading to poor performance in new scenes or with new objects \cite{falanga2020dynamic}.
$(ii)$ These methods involves machine learning algorithms, which either stick events within a time window (\eg, $1ms$) into an image for DNN-based estimation \cite{guo2022low}, or devise event-oriented networks (\eg, SNN \cite{zhou2023computational}) for object localization \cite{barchid2023spiking}. 
However, they often entail significant delays (\eg, tens to hundreds of milliseconds) for network inference \cite{diehl2015unsupervised, guo2021toward}, negating low-latency benefits of event cameras.

\noindent $\bullet$ \textbf{Fusion-based solutions.}
These methods enhance event cameras for 3D object localization through sensor fusion, categorized into two types.
$(i)$ Several practices involve dual event cameras \cite{zhou2021event, xu2023taming}. 
% for 3D object localization
However, they often require meticulous calibration and feature matching between cameras, which are time-consuming and sensitive to environmental noise \cite{falanga2020dynamic}.
$(ii)$ Introducing dedicated depth estimation sensors (\eg, depth cameras \cite{he2021fast}, LiDAR \cite{cui2022dense}) provides event cameras with depth of objects \cite{li2022motion}. 
However, these sensors typically operate with sensing latency of 30$ms$ $\sim$ 100$ms$ \cite{li2023leovr}, requiring downsampling event cameras to synchronize, which nullify the low-latency benefits of event cameras.

\noindent \textbf{Remark.}
Inappropriate sensor choice for fusion and the absence of suitable algorithms negate the low-latency advantages of event cameras, posing challenges in fully leveraging their potential for low-latency 3D object localization.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0.25cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=0.98\columnwidth]{Figs/intro_new.png}
        \vspace{-0.2cm}
    \caption{Illustration of events generation and applications of event cameras.}
    \label{intro}
    \vspace{-0.3cm}
\end{figure} 

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{evaFigs/relatedall_2.png}
        \vspace{-0.2cm}
    \caption{Benchmark study on drone localization and performance of existing solutions at different settings.}
    \label{relatedwork}
    \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Enhance event camera with mmWave radar.}
mmWave signals, operating at high frequencies (30 $\sim$ 300 GHz) with wide bandwidth, offer high sensing sensitivity and precision \cite{fiandrino2019scaling, zhang2023survey}.
Endowed with fine-grained, directional sensing capability, and resistance to weather and illumination conditions, mmWave sensing has great advantages in object depth estimation \cite{sie2023batmobility, iizuka2023millisign, lu2020see, lu2020milliego}.
More importantly, both event cameras and mmWave radar share \textit{millisecond latency} \cite{mmWaveUser}. These factors make mmWave a promising enhancement for event cameras in low-latency 3D object localization.

To explore the potential of fusing the event camera and mmWave radar for low-latency and accurate 3D object localization, we conduct a benchmark study on drone localization at landing phase at a real-world drone delivery airport (\fig \ref{relatedwork}a). 
Low-latency and accurate localization is crucial for effective landing of the drone \cite{wang2022micnest, sun2023indoor}, as in this phase the drone is most vulnerable \cite{xu2023taming, floreano2015science}, posing financial risks and safety threats \cite{Russiandrone}. 
Enhanced accuracy improves landing success on designated platforms, while reduced latency provides more reaction time for unexpected situations \cite{famili2022pilot, he2023acoustic, chi2022wi}.
Our benchmark study reveals that this fusion faces fundamental challenges in 3D object localization, as elaborated below:
 
\noindent $\bullet$ \textbf{C1: Differing noise distribution characteristics of both modalities hinder drone detection.}
Both sensor modalities yield not only heterogeneous information about the drone but also generate significantly heterogeneous noise. 
Event cameras produce noise due to unexpected changes in brightness conditions, while mmWave radar struggles with noise caused by signal multipath effects.
This noise differs greatly in dimensions and patterns, and it may lacks temporal synchronization, particularly under millisecond latency (\fig \ref{relatedwork}b). 
These factors make noise filtering challenging, causing detection bottlenecks and reducing localization efficiency and accuracy \cite{xu2023taming}.
Traditional noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} target a specific modality, resulting in low noise event and point cloud filtering rates (recall and precision < 65\% in \fig \ref{relatedwork}c), limiting their effectiveness in our scenario.


\noindent $\bullet$ \textbf{C2: Ultra-large amount data burden the heterogeneous data fusion, delaying drone localization.}
Once the drone is detected, accurate 3D spatial location estimation of it is essential, which is more time-consuming than detection due to additional processing (\eg, sensor fusion and optimization).
The ultra-large amount of data generated by the millisecond latency further burdens the time consumption \cite{xu2021followupar}. 
Although the localization accuracy is boosted, existing methods \cite{zhao20213d, falanga2020dynamic, mitrokhin2018event} introduces significant delays (\fig \ref{relatedwork}d).
Moreover, asynchronous event streams and sparse point clouds from mmWave radar are heterogeneous in terms of precision, scale, and density. 
Previous fusion methods (\eg, Extended kalman filter, particle filter, and graph optimization \cite{grisetti2010tutorial} in \fig \ref{relatedwork}d) suffer from severe cumulative drift error and lengthy processing latency, rendering them inadequate for accurate and low-latency localization.

\noindent \textbf{Our work.}
We delve into the sensing principles of event cameras and mmWave radar, introducing EventLoc. This system, focused on low-latency, enhances event camera functionality, providing cm-level 3D object localization with millisecond latency in average, broadening its application in diverse 3D vision tasks.
EventLoc features three key designs to fully unleash the potential of event camera and mmWave radar for 3D object localization, as elaborated below: \\
\noindent $\bullet$ \textbf{On system architecture front.}
By incorporating mmWave radar with millisecond latency, we enhance the performance of event camera and improve 3D localization performance at the data source.
EventLoc features a carefully designed system architecture that tightly couples event camera and mmWave radar. 
This integration spans from early-stage filtering to later-stage fusion and optimization, fully leveraging the unique advantages of both sensors (ยง\ref{3.2}). \\
\noindent $\bullet$ \textbf{On system algorithm front.}
We first introduce the Consi-stency-Instructed Collaborative Tracking (\textit{CCT}) algorithm to extract \textit{consistent information} in sensing data from both modalities to filter out environment-triggered noise with a low false positive rate, enhancing the detection performance with a low-latency (ยง\ref{4.1}). 
We then present the Graph-Informed Adaptive Joint Optimization (\textit{GAJO}) algorithm to fully fuse \textit{complementary information} from both modalities, accelerating the optimization in localizing the object (ยง\ref{4.2}). \\
\noindent $\bullet$ \textbf{On system implementation front.}
We further analyze the sources of latency and propose an Adaptive Optimization method for boosting the \textit{GAJO}. 
This method dynamically optimizes the set of locations rather than relying on a fixed sliding window, further enhancing the accuracy of localization and reducing latency (ยง\ref{5.1}).

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.4cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/overview2.png}
        \vspace{-0.2cm}
    \caption{System architecture of EventLoc.}
    \label{overview}
    % \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Evaluation and Result.} 
We fully implement EventLoc with COTS event camera and mmWave radar.
Extensive experiments in indoor/outdoor environments are conducted with different drone flight conditions to comprehensively evaluate performance of EventLoc.
We compare the end-to-end drone localization accuracy and latency of EventLoc with three SOTA methods.
Through over 30 hours experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving a average localization accuracy of 0.101 $m$ and latency of 5.15 $ms$, surpassing all baselines by >50\% in average.
Additionally, EventLoc is marginally affected by factors such as drone type and envir. conditions.\\
\textbf{Real-world deployment.}
We have deployed the sensor platform with EventLoc at a real-world drone delivery airport as shown in \fig \ref{relatedwork}a to demonstrate practicability of the system.
10 hours study shows that EventLoc meets drone landing demands within the constraints of available resources.

\noindent \textbf{Contributions.} This paper makes following contributions.

\noindent $(1)$ We propose EventLoc, a novel low latency-oriented event camera enhancement system. It tightly integrates asynchronous events and mmWave radar sparse point clouds, achieving accurate drone localization with millisecond latency.\\
\noindent $(2)$ We propose the $CCT$, a light-weight cross-modal noise filter to push the limit of detection accuracy by leveraging the \textit{consistent information} from both modalities. \\
\noindent $(3)$  We propose the $GAJO$, a factor graph-based optimization framework that fully harnessing \textit{complementary information} from both modalities to enhance localization performance.\\
\noindent $(4)$ We implement and extensively evaluate EventLoc by comparing it with three SOTA methods, showing its effectiveness. We also deploy EventLoc in a real-world drone delivery airport, demonstrating feasibility of EventLoc.

