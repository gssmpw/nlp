\vspace{-0.5cm} 
\section{Introduction}
% Event cameras are innovative bio-inspired sensors.
% Unlike traditional frame cameras, Event cameras do not operate at a fixed rate but asynchronously report pixel-wise intensity changes, known as events (\fig \ref{relatedwork}a). 
% With microsecond level resolution and an asynchronous, differential operating principle, event cameras excel at capturing high-speed motions that cause severe motion blur in frame cameras. 
% Additionally, Event cameras have a very high dynamic range (HDR) of 140dB compared to 60dB in frame cameras, performing well under varied illumination conditions. 
% Consequently, event cameras are considered an important sensing modality and are increasingly used for tasks like motion tracking and Simultaneous Localization and Mapping (SLAM).


% Event cameras are innovative bio-inspired sensors that report pixel-wise intensity changes as events asynchronously with microsecond sensing latency (\fig \ref{intro}a), rather than fixed interval frames \tocite. sensing latency is the time from a visual stimulus appearing to its sensor readout.
% Event cameras are innovative bio-inspired sensors that asynchronously report pixel brightness changes as events with \textit{millisecond latency} (\fig \ref{intro}a), instead of fixed interval frames \cite{he2024microsaccade, gehrig2024low}.  
% % sensing latency is the time from visual stimulus to sensor readout \cite{gehrig2024low}.
% With high temporal resolution and a high dynamic range, event cameras excel at capturing high-speed motions without blurring and perform well under varied illumination conditions \cite{falanga2020dynamic, xu2023taming}.
% Thus, event cameras are envisioned as an ideal solution for challenging 2D vision tasks, such as low latency and accurate object detection in \fig \ref{intro}b \cite{gallego2020event}.

Event cameras are innovative bio-inspired sensors that report changes in pixel brightness asynchronously as events with \textit{millisecond latency} (\fig \ref{intro}a), rather than at fixed-time
intervals \cite{he2024microsaccade, gehrig2024low}.  
% sensing latency is the time from visual stimulus to sensor readout \cite{gehrig2024low}.
With high temporal resolution and a high dynamic range, event cameras excel at capturing high-speed motions without blurring and perform well under varied illumination conditions \cite{falanga2020dynamic, xu2023taming}.
Thus, event cameras are envisioned as an ideal solution for 2D vision tasks, such as low latency and accurate object detection as shown in \fig \ref{intro}b \cite{gallego2020event}.

% Similar to frame cameras, event cameras encounter scale uncertainty (\aka, they struggle to accurately estimate object depth) \tocite.
% This challenge hinders event cameras from fully realizing their potential in 3D object localization \tocite. 
% Similar to frame cameras, event cameras encounter scale uncertainty (\aka, they struggle to accurately estimate object depth) \tocite.
% This challenge hinders event cameras from fully realizing their potential in 3D object localization \tocite. 
% 尽管事件相机在上述 2D vision tasks取得了不错的表现，
% However, event cameras struggle to fully realize their potential in low-latency 3D object localization, which has various potential applications (\eg, drone localization，入侵物体定位等）
% because they encounter scale uncertainty (\aka, they struggle to accurately estimate depth) \cite{zhang2022mobidepth}. 
% Although event cameras perform well in the aforementioned 2D vision tasks, they struggle to fully realize their potential in low-latency 3D object localization, which 对事件相机视野中出现的物体进行三维定位， has various potential applications (\eg, drone localization, intruding object detection, AR/MR) due to scale uncertainty (\aka, difficulty in accurately estimating depth) \cite{zhang2022mobidepth}.
% Although event cameras excel in the 2D vision tasks, they struggle to fully realize their potential in 3D vision tasks 以 low-latency 3D object localization为代表, which involves localizing the object within the camera's field of view in three dimensions (\fig \ref{intro}c). 
% Although event cameras excel in aforementioned 2D vision tasks, they struggle to fully realize their potential in 3D vision tasks, particularly in low-latency 3D object localization, which involves localizing objects within the camera's field of view in three dimensions (\fig \ref{intro}c) \cite{qin2019monogrnet}.
% The latency measures the time elapsed from visual stimulus to resulting localization output.
% This limitation, due to scale uncertainty (\ie, difficulty in accurately estimating depth) \cite{zhang2022mobidepth}, affects various potential vital applications of event cameras (\eg, drone localization \cite{wang2022micnest}, intruding object detection\cite{han2015twins}).

% Although event cameras excel in 2D vision tasks, they face fundamental challenges in 3D vision, preventing their full potential from being realized.
% Specifically, 3D object localization, identifying the location of objects within the camera's field of view in three dimensions, is a fundamental function for various vital 3D vision tasks (\eg, drone localization \cite{wang2022micnest}, AR/MR \cite{xu2021followupar}).
% However, event cameras, capturing per-pixel brightness changes in 2D without depth details, can't directly gauge object distance, causing scale uncertainty. 
% This limitation restricts event cameras in 3D object localization , hindering the exploitation of their low-latency advantage in 3D vision tasks.
% % To address this, 
% Two types of solutions are proposed:

However, event cameras face significant challenges when applied to more complex 3D vision tasks.
% which prevents their full potential from being realized. 
For instance, 3D object localization, which identifies the location of objects within the camera's field of view in three dimensions, is a fundamental block for various vital 3D vision tasks (\eg, drone navigation \cite{wang2022micnest}, augmented/mixed reality \cite{xu2021followupar}).
Event cameras only capture per-pixel brightness changes in 2D devoid of depth details, resulting in scale uncertainty that hinders their effectiveness in 3D object localization (\fig \ref{intro}c).
This limitation further restricts the exploitation of their potential in various 3D vision tasks.
To address this, two primary types of solutions are proposed to enhance event cameras:

% However, event cameras can only capture 2D images and lack depth information, making it impossible to directly measure the actual distance of the object. 
% This leads to scale uncertainty, preventing event cameras from performing 3D object localization (\fig \ref{intro}c).
% This prevents 3D object localization from leveraging the low-latency advantage of event cameras and hinders their use in various vital 3D vision applications.
% Two type solutions are proposed to augment event cameras:

% However, similar to frame cameras, event cameras face scale uncertainty (\aka, they cannot accurately estimate the depth of objects) \tocite.
% This is a fundamental challenge that prevents event cameras from fully realizing their potential in 3D object localization and tracking \tocite. 
% There are mainly two types of solutions proposed to address this issue, supplementing event cameras with depth information of objects:

\noindent $\bullet$ \textbf{Events only-based solutions.}
These methods rely solely on event data for object depth estimation and fall into two types. 
$(i)$ Incorporating known geometric information with observations to deduce depth. 
These methods rely heavily on prior knowledge, leading to poor performance in new scenes or with new objects \cite{falanga2020dynamic}.
$(ii)$ Employing machine learning algorithms that either stick events within a time window (\eg, $1ms$) into an image for DNN-based estimation \cite{guo2022low}, or devise event-oriented networks (\eg, SNN) for object localization \cite{zhou2023computational, barchid2023spiking}. 
These methods are computationally intensive during network inference \cite{diehl2015unsupervised, guo2021toward}, potentially entailing significant latency (\eg, tens to hundreds of milliseconds) in practice.

% However, they often entail significant delays (\eg, tens to hundreds of milliseconds) for network inference \cite{diehl2015unsupervised, guo2021toward}, negating low-latency benefits of event cameras.

% CNNs struggle to process event data directly due to its asynchronous nature \tocite. 
% Current practices

% These methods use only event data for object depth estimation, which can be categorized into two types.
% One type is machine learning algorithms. 
% Convolutional neural networks (CNNs) cannot directly process event data due to its asynchronous nature \tocite. 
% Current methods either stick events within a short time window (\eg, $<1ms$) into an image for CNN-based depth estimation \tocite or design event-oriented networks (\eg, spiking neural networks) for object localization \tocite. 
% However, these methods often require significant delays (\eg, tens to hundreds of milliseconds) for inference \cite{diehl2015unsupervised, guo2021toward}, negating the low-latency benefits of event cameras\tocite.
% The other type of methods incorporates known geometric information of the target object combined with observational data to infer depth, which heavily rely on prior knowledge, resulting in poor performance in new scenes and with unfamiliar objects.

\noindent $\bullet$ \textbf{Fusion-based solutions.}
These methods enhance event cameras for 3D object localization through sensor fusion, categorized into two types.
$(i)$ Involving dual event cameras \cite{zhou2021event, xu2023taming}. 
These methods often require meticulous calibration and feature matching between event cameras, which are time-consuming and sensitive to environmental noise \cite{falanga2020dynamic}.
$(ii)$ Introducing dedicated depth estimation sensors (\eg, depth cameras \cite{he2021fast}, LiDAR \cite{cui2022dense}) to provide event cameras with depth information \cite{li2022motion}. 
% However, these sensors typically operate at 10$Hz$ $\sim$ 30$Hz$ \tocite, requiring downsampling event cameras to synchronize, which nullify the low-latency benefits of event cameras \tocite.
However, these sensors typically operate with latencies ranging from 30$ms$ to 100$ms$ \cite{li2023leovr}, necessitating the downsampling of event data in the temporal domain for synchronization.
% requiring downsampling event cameras to synchronize, which nullify the low-latency benefits of event cameras.

\noindent \textbf{Remark.}
% In summary, current methods entail lengthy processing times or necessitate downsampling event cameras for synchronization with other sensors, presenting significant challenges in fully harnessing the potential of event cameras for low-latency 3D object localization.
% Inappropriate sensor choice for fusion and the absence of suitable algorithms negate the low-latency advantages of event cameras, posing challenges in fully leveraging their potential for low-latency 3D object localization.
In summary, the absence of efficient algorithms and the sensor with matched frequencies for depth estimation introduces substantial delays in 3D object localization. 
This limitation prevents the complete exploitation of the low-latency benefits of event cameras.

% "In summary, the lack of efficient algorithms and appropriately synchronized sensors for depth estimation causes significant delays in 3D object localization. This hurdle hinders the complete exploitation of the low-latency advantages offered by event cameras."
% event cameras’ potential.

% \noindent $\bullet$ \textbf{Dedicated depth sensors-based solution.}
% By introducing dedicated depth estimation sensors (\eg, depth cameras and LiDAR), these solutions provide event cameras with depth information of objects. 
% Specifically, these sensors emit light in a specific spectrum and calculate depth based on reflection time.
% However, they typically operate at frequencies of 10Hz $\sim$ 30Hz, much lower than the sampling frequency of event cameras, degrading localization performance.

% \noindent $\bullet$ \textbf{Learning-based solution.}
% Machine learning algorithms, such as convolutional neural networks (CNNs), cannot directly process event camera data because it consists of asynchronous events, not fixed-rate frames. 
% Current practices either $(i)$ stick all events within a short time window (\eg, $<1ms$) into an image for CNN-based depth estimation, or $(ii)$ design event-oriented networks (\eg, spiking neural networks) for object localization. 
% They rely heavily on labeled training data, leading to poor performance with new scenes and objects. Also, they introduce significant delays, negating the low-latency benefits of event cameras.

% \noindent $\bullet$ \textbf{Motion-based solution.}
% These methods combine information from inertial measurement units (IMUs) and use visual-inertial odometry to estimate 3D location of the target object. 
% Although they do not rely on dedicated sensors or training data, they require camera movement while the object remains stationary, which severely limits usage scenarios. 
% Additionally, current practices involve using dual event cameras with known pose relationships for 3D object localization. 
% However, these methods often require meticulous calibration and feature matching between cameras, which are highly sensitive to unexpected noise in the environment.

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0.25cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=0.98\columnwidth]{Figs/intro_new.png}
        \vspace{-0.2cm}
    \caption{Illustration of events generation and applications of event cameras.}
    \label{intro}
    \vspace{-0.3cm}
\end{figure} 

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{evaFigs/relatedall_2.png}
        \vspace{-0.2cm}
    \caption{Benchmark study on drone localization and performance of existing solutions at different settings.}
    \label{relatedwork}
    \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Enhance event camera with mmWave radar.}
% MmWave radar, utilizing frequency-modulated continuous waves (FM-CW) with microsecond level latency, measures relative angle and distance of moving objects, generating sparse point cloud \cite{woodford2023metasight, zheng2023neuroradar}. 
% with microsecond level latency
% MmWave radar, utilizing frequency-modulated continuous waves (FM-CW), has been widely employed to measure the relative angle and distance of moving objects, resulting in a sparse point cloud \cite{woodford2023metasight, zheng2023neuroradar}.
% The mmWave radar, utilizing frequency-modulated continuous waves (FM-CW), has been widely employed in detection and tracking of moving objects, resulting in a sparse point cloud \cite{woodford2023metasight, zheng2023neuroradar}.
% Inspired by achievements of mmWave-based sensing techniques, we observe that both event camera and mmWave radar share microsecond time resolution, making mmWave radar a promising modality to enhance the event camera in 3D object localization.
% This presents a significant opportunity for event-based accurate and low-latency localization.
mmWave signals, operating at high frequencies (30 $\sim$ 300 GHz) with wide bandwidth, offer high sensing sensitivity and precision \cite{fiandrino2019scaling, zhang2023survey}.
Endowed with fine-grained, directional sensing capability, and resistance to weather and illumination conditions, mmWave sensing has great advantages in object depth estimation \cite{sie2023batmobility, iizuka2023millisign, lu2020see, lu2020milliego}.
More importantly, both event cameras and mmWave radar feature \textit{millisecond latency} \cite{mmWaveUser}. These factors make mmWave a promising enhancement for event cameras in low-latency 3D object localization.
% Meanwhile, this fusion also holds potential in solving the issues of limited spatial resolution and scatter center drift faced by mmWave radar.

% mmWave signals, operating at high frequencies (30-300 GHz) with wide bandwidth, offer high sensing sensitivity. With fine-grained, directional sensing capability, mmWave sensing excels in object depth estimation. Both event cameras and mmWave radar share millisecond latency, making mmWave a promising enhancement for event cameras in low-latency 3D object localization. Additionally, this fusion can address the issues of limited spatial resolution and scatter center drift faced by mmWave radar.

% and resistance to weather and illumination conditions, 
% More importantly,尽管 mmWave 面临limited angular resolution和scatter center drift问题， both event cameras and mmWave radar share \textit{millisecond latency} \cite{mmWaveUser}, making mmWave a promising enhancement for event cameras in low-latency 3D object localization.
% Despite the issues of limited spatial resolution and scatter center drift faced by mmWave, both event cameras and mmWave radar share \textit{millisecond latency} \cite{mmWaveUser}. 
% This makes mmWave a promising enhancement for event cameras in low-latency 3D object localization, while the event camera also holds potential in solving mmWave radar issues.

% To better understand the potential of fusing the event camera and mmWave radar for low-latency and accurate localization, we conduct a benchmark study on landing drone localization at a real-world drone delivery airport (\fig \ref{relatedwork}a), as accurate and low-latency localization is essential for effective drone landing \cite{sun2023indoor}. 
% This is because landing is a critical phase where drones are most vulnerable \cite{wang2022micnest, xu2023taming, floreano2015science}, posing financial risks and safety threats \cite{Russiandrone}. 
% Higher accuracy improves landing success on designated platforms, while lower latency allows more reaction time to unexpected situations \cite{famili2022pilot, he2023acoustic, chi2022wi}.

To explore the potential of fusing the event camera and mmWave radar for improved 3D object localization, we conduct a benchmark study on drone localization during landing phase at a real-world drone delivery airport (\fig \ref{relatedwork}a). 
Accurate and low-latency localization is crucial for effective landing of the drone \cite{wang2022micnest, sun2023indoor}, as in this phase the drone is most vulnerable, posing financial risks and safety threats \cite{floreano2015science, Russiandrone}. 
Enhanced accuracy ensures successful landing on designated platforms, while reduced latency provides more reaction time for unexpected situations \cite{famili2022pilot, he2023acoustic, chi2022wi}.
Our benchmark study reveals that existing methods face fundamental challenges in 3D object localization, as elaborated below:

% \noindent $\bullet$ \textbf{C1: Millisecond sensing latency amplifies sensing noise, impairing drone detection.}
% \noindent $\bullet$ \textbf{C1: Differing noise distribution characteristics of both modalities hinder drone detection.}
% Unexpected environmental changes introduce irrelevant information as noise in sensing results \cite{xu2023taming}.
% Although both sensors have matched sensing latency, their noise distribution characteristics differ significantly due to their different mechanisms, hindering system's ability to identify signals changes caused by drone in both modalities \cite{zuo2024cross}, especially in millisecond latency (\fig \ref{relatedwork}b).
% However, traditional single modality-oriented noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} achieve a low event and point cloud filtering rate (recall and precision < 65\% in \fig \ref{relatedwork}c) due to their rule-based pipelines struggle to distinguish drone-induced signal changes from scene dynamics.
% This results in detection precision bottlenecks, significantly diminishing the efficiency and accuracy of localization.

% 事件相机容易由于什么产生噪声，雷达容易由于什么产生噪声。对于同一个目标，这两种不同的传感器不仅产生异构的target-trigger的信息，也产生了空间上不同分布（dimentions，patterns）的噪声，而且这些噪声时间上也可能不同步，特别是在高时间分辨率的情况下。不幸的是，传统的方法要不就是针对单模态的滤波，要不就是对两个相似的信号进行滤波，不能应用到我们这个异构的高频场景下。
% Unexpected environmental changes introduce irrelevant information as noise in sensing results \cite{xu2023taming}.
% \noindent $\bullet$ \textbf{C1: Differing noise distribution characteristics of both modalities hinder drone detection.}
% Both sensor modalities yield not only heterogeneous information about the drone but also generate significantly heterogeneous noise. 
% Event cameras produce noise due to unexpected changes in brightness conditions, while mmWave radar struggles with noise caused by signal multipath effects.
% This noise differs greatly in dimensions and patterns, and it may lacks temporal synchronization, particularly under millisecond latency (\fig \ref{relatedwork}b). 
% These factors make noise filtering challenging, causing detection bottlenecks and reducing localization efficiency and accuracy \cite{xu2023taming}.
% Traditional noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} target a specific modality, resulting in low noise event and point cloud filtering rates (recall and precision < 65\% in \fig \ref{relatedwork}c), limiting their effectiveness in our scenario.

% 一句背景，一句现象，一句结果，一句实验数据。
\noindent $\bullet$ \textbf{C1: Noise distribution characteristics of both modalities differ, hindering drone detection.}
% Both sensor modalities yield not only heterogeneous information about the drone but also generate significantly heterogeneous noise. 
These two sensor modalities not only provide different types of information but also generate significantly heterogeneous noise. 
Event cameras produce noise due to unexpected changes in brightness conditions, whereas mmWave radar struggles with noise caused by signal multipath effects.
These noises differ greatly in both dimension and pattern, which can also be asynchronous, especially under high temporal resolution (\fig \ref{relatedwork}b).
This spatial and temporal heterogeneity complicates noise filtering, causing detection bottlenecks \cite{xu2023taming}.
Unfortunately, existing traditional noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} typically target a single modality, resulting in low noise event and point cloud filtering rates (recall and precision < 65\% in \fig \ref{relatedwork}c), limiting their effectiveness in our scenario.


% However, traditional noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} are solely targeted at a specific modality and fail to exploit the consistency among different modalities, achieving a low event and point cloud filtering rate (recall and precision < 65\% in \fig \ref{relatedwork}c), which cannot be utilized in effective noise filtering in our scenario.
% This results in detection precision bottlenecks, significantly diminishing the efficiency and accuracy of localization.
% Although both sensors have matched sensing latency, their noise distribution characteristics differ significantly due to their different mechanisms, hindering system's ability to identify signals changes caused by drone in both modalities \cite{zuo2024cross}, especially in millisecond latency (\fig \ref{relatedwork}b).
% However, traditional single modality-oriented noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} achieve a low event and point cloud filtering rate (recall and precision < 65\% in \fig \ref{relatedwork}c) due to their rule-based pipelines struggle to distinguish drone-induced signal changes from scene dynamics.

% \noindent $\bullet$ \textbf{C2: Ultra-large amount data burden the heterogeneous data fusion, delaying drone localization.}
% Once the drone is detected, accurate 3D spatial location estimation of it is essential, which is more time-consuming than detection due to additional processing (\eg, sensor fusion and optimization).
% The ultra-large amount of data generated by the millisecond latency further burdens the time consumption . 
% Although the localization accuracy is boosted, existing methods \cite{zhao20213d, falanga2020dynamic, mitrokhin2018event} introduces significant delays (\fig \ref{relatedwork}d).
% Moreover, asynchronous event streams and sparse point clouds from mmWave radar are heterogeneous in terms of precision, scale, and density. 
% Previous fusion methods (\eg, Extended kalman filter, particle filter, and graph optimization \cite{grisetti2010tutorial} in \fig \ref{relatedwork}d) suffer from severe cumulative drift error and lengthy processing latency, rendering them inadequate for accurate and low-latency localization.

\noindent $\bullet$ \textbf{C2: Ultra-large data volume burdens the heterogeneous data fusion, delaying drone localization.}
Accurately estimating 3D location of the drone after detection involves time-consuming processing steps, such as sensor fusion and optimization. 
% Once the drone is detected, we proceed to perform 3D localization on it.
% Accurately estimating 3D spatial location of drone involves several time-consuming processing steps, including the sensor fusion and optimization.
The ultra-large amount of data due to the high frequency further exacerbates the processing time, causing significant delays \cite{xu2021followupar}.
Meanwhile, the asynchronous event streams and sparse point clouds are heterogeneous in terms of precision, scale, and density, adding complexity to the sensor fusion.
Existing methods (\eg, Extended Kalman filter, particle filter, and graph optimization) suffer from cumulative drift error, heterogeneity issues, and lengthy processing latency, rendering them inadequate for accurate and low-latency localization as shown in \fig \ref{relatedwork}d \cite{zhao20213d, falanga2020dynamic, mitrokhin2018event, grisetti2010tutorial}.


\noindent \textbf{Our work.}
% We explore the sensing principles of the event camera and mmWave radar and propose EventLoc, an low latency-oriented event camera enhancement system that provides cm-level accurate 3D object localization with millisecond level latency to enable application of event camera in various 3D vision tasks.
We delve into the sensing principles of event cameras and mmWave radar, introducing EventLoc. 
This system enhances event camera functionality with a focus on low-latency 3D object localization, providing cm-level accuracy with millisecond latency on average. 
As a result, EventLoc broadens event camera application in diverse 3D vision tasks.
In detail, EventLoc features three key designs to fully unleash the potential of event camera and mmWave radar for 3D object localization, as elaborated below: \\
% and is implemented with adaptively acceleration algorithms to further improve accuracy and reduce latency, 
\noindent $\bullet$ \textbf{On system architecture front.}
By incorporating mmWave radar with millisecond latency, we enhance the performance of event camera and improve 3D localization performance at the data source.
EventLoc features a carefully designed system architecture that tightly couples event camera and mmWave radar. 
This integration spans from early-stage filtering to later-stage fusion and optimization, fully leveraging the unique advantages of both sensors (§\ref{3.2}). \\
\noindent $\bullet$ \textbf{On system algorithm front.}
We first introduce the Consi-stency-Instructed Collaborative Tracking (\textit{CCT}) algorithm to extract \textit{consistent information} in sensing data from both modalities to filter out environment-triggered noise with a low false positive rate, enhancing the detection performance with a low-latency (§\ref{4.1}). 
We then present the Graph-Informed Adaptive Joint Optimization (\textit{GAJO}) algorithm to fully fuse \textit{complementary information} from both modalities, accelerating the optimization in localizing the object (§\ref{4.2}). \\
\noindent $\bullet$ \textbf{On system implementation front.}
We further analyze the sources of latency and propose an Adaptive Optimization method for boosting the \textit{GAJO}. 
This method dynamically optimizes the set of locations rather than relying on a fixed sliding window, further enhancing the accuracy of localization and reducing latency (§\ref{5.1}).

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.4cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/overview2.png}
        \vspace{-0.2cm}
    \caption{System architecture of EventLoc.}
    \label{overview}
    % \vspace{-0.2cm}
\end{figure*} 

\noindent \textbf{Evaluation and Result.} 
We fully implement EventLoc with COTS event camera and mmWave radar.
Extensive experiments in indoor/outdoor environments are conducted with different drone flight conditions to comprehensively evaluate performance of EventLoc.
We compare the end-to-end drone localization accuracy and latency of EventLoc with three SOTA methods.
% Through over 30 hours of real-world experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving a localization accuracy of 1.01 $dm$, surpassing all baselines by >50\%. EventLoc further achieves localization latency of 5.15 $ms$, outperforming baselines by >50\% in average.
Through over 30 hours of experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving an average localization accuracy of 0.101 $m$ and latency of 5.15 $ms$, surpassing all baselines by >50\% on average.
Additionally, EventLoc is marginally affected by factors such as drone type and envir. conditions.\\
\textbf{Real-world deployment.}
We have deployed the sensor platform with EventLoc at a real-world drone delivery airport as shown in \fig \ref{relatedwork}a to demonstrate practicability of the system.
10 hours study shows that EventLoc meets drone landing demands within the constraints of available resources.

\noindent \textbf{Contributions.} This paper makes following contributions.

\noindent $(1)$ We propose EventLoc, a novel low latency-oriented event camera enhancement system. It tightly integrates asynchronous events and mmWave radar sparse point clouds, achieving accurate drone localization with millisecond latency.\\
\noindent $(2)$ We propose the $CCT$, a light-weight cross-modal noise filter to push the limit of detection accuracy by leveraging the \textit{consistent information} from both modalities. \\
\noindent $(3)$  We propose the $GAJO$, a factor graph-based optimization framework that fully harnessing \textit{complementary information} from both modalities to enhance localization performance.\\
% accuracy and latency
\noindent $(4)$ We implement and extensively evaluate EventLoc by comparing it with three SOTA methods, showing its effectiveness. We also deploy EventLoc in a real-world drone delivery airport, demonstrating feasibility of EventLoc.
% The remainder of the paper is organized as follows:
% §2 provides an overview of EventLoc, with detailed descriptions of the Consistency-Instructed Collaborative Tracking algorithm in §3.1 and the Graph-Informed Adaptive Joint Optimization algorithm in §3.2. §4 showcases the adaptive acceleration algorithms and implementation. §5 details our extensive indoor and outdoor experiments with EventLoc. §6 presents our experiments conducted at a real-world delivery airport. §7 discusses related work. §8 concludes the paper.
