% \vspace{-0.3cm}
\section{Introduction}
% Drones have become one of the most disruptive technologies, with applications ranging from last-mile delivery \cite{floreano2015science} to industrial inspection \cite{bartolini2020multi}. 
% The roll-out of 5G networks in cities enhances their capabilities, allowing drones to autonomously deliver goods over long distances while being monitored in real-time \cite{bertizzolo2021streaming}. 
% Bypassing urban traffic, drones are especially effective for time-sensitive deliveries, such as medical supplies \cite{MedicalDelivery}. Various businesses, including Amazon \cite{Amazondrone}, Alphabet Wing \cite{Wingdrone}, and Meituan \cite{Meituandrone}, are exploring the viability of drones for instant deliveries, as shown in \fig \ref{intro}a.
% Drones have emerged as a transformative technology in applications of last-mile delivery \cite{floreano2015science}. 
% Enabled by 5G, they can autonomously deliver goods over long distances, bypassing urban traffic—ideal for urgent needs like medical supplies \cite{MedicalDelivery}. 


Drones have become a transformative technology for last-mile delivery, with 5G enabling autonomous transport—ideal for urgent deliveries like medical supplies \cite{floreano2015science, MedicalDelivery}. 
A key phase in drone operation is landing, requiring active, accurate and low-latency localization from the ground due to the drone's increased vulnerability (\fig \ref{intro}a) \cite{wang2022micnest, zhao2023smoothlander, sun2023indoor}. 
Improved accuracy increases the chances of successful landing on target platforms, while lower latency enables faster responses to unforeseen situations \cite{famili2022pilot, he2023acoustic, chi2022wi} (\fig \ref{intro}b).
Mainstream ground-based systems leverage radar (\eg, mmWave radar) to achieve this goal, as it performs reliably across various lighting and noise conditions and offers a wide detection range \tocite.
% Currently ground-based practices primarily utilize radar (\eg, mmWave radar) to achieve active, accurate, low-latency localization for landing drone




% Currently ground-based practices primarily utilize radar (\eg, mmWave radar) to achieve active, accurate, low-latency localization for landing drone, which includes: 
Existing active, ground-based drone localization solutions utilizing mmWave radar fall into three categories:
% $(i)$ Introducing visual sensors, such as cameras, enables ground-based drone localization through multi-modal fusion. However, these methods increase system latency, as visual sensors require long frame exposure time and excessive image processing delay. Meanwhile, the motion blurring shown in each frame would confuse the localization algorithm, exacerbating localization errors.
% $(ii)$ \textit{conventional signal intensity-based methods}, however, challenge to accurately tracking drone's center due to its large size (\eg, 80 cm) and non-uniform blob radar readout. These methods also yield unstable results with frequent outliers, as noise or multipath scattering overpower the main signal.
$(i)$ \textit{conventional signal intensity-based methods}, however, struggle to accurately track the drone’s center due to its large size (\eg, 80$cm$ across), which cause the drone appears as a non-uniform blob in radar returns. These methods also yield unstable results with frequent outliers, as radar’s low spatial resolution and multipath scattering can overpower the main signal \tocite;
% $(ii)$ \textit{deep learning-based methods}, however, require extensive pre-modeling of the drone and prior neural network training. Meanwhile, These works can not track different models of drone.
$(iii)$ \textit{deep learning-based methods}, however, necessitate extensive pre-modeling of the drone and prior neural network training. These methods struggle to track different drone models and perform poorly in environments not represented in the training dataset \tocite. 
$(iii)$ \textit{Fusion-based methods} introduces visual sensors (\eg, cameras) to aid radar. However, these methods add system latency due to long exposure times (around 20$ms$) and additional image processing delays (another 10-20$ms$). Also, motion blur in frames can confuse the algorithm, increasing errors \tocite;\\
\textbf{Remark.} None of the previous solutions can achieve accurate, low-latency localization in diverse scenarios due to inefficient algorithms, environmental and drone-related uncertainties, and unsuitable sensor choices.

% $\bullet$ \textbf{Extra infrastructure based solutions.} 
% These solutions rely on external devices (\eg, GPS\cite{nirjon2014coin}, RTK\cite{wang2022micnest}, WiFi\cite{hu2023wisdom, chen2015drunkwalk}, microphone\cite{guo2021infrastructure, weiguo2020symphony}) to provide reference signals for localization, which require LoS connection between localization infrastructure and drones.
% However, in densely built urban environments, the accuracy and environmental compatibility of these methods significantly degrades as altitude decreases, rendering ineffective for low-altitude operations \cite{li2021infocom_itoloc}. \\
% $\bullet$ \textbf{Intra on-board sensor based solutions.}  
% These solutions leverage onboard sensors (\eg, cameras\cite{xu2019ivr}, IMUs\cite{lu2020milliego}, lidar\cite{he2021vi} or radar\cite{dai2023interpersonal}), combined with simultaneous localization and mapping (SLAM) techniques\cite{campos2021orb} or visual markers\cite{krogius2019iros}, to achieve high-precision location estimation. 
% However, the low spatio-temporal sampling resolution of these onboard sensors poses a challenge for drones to achieve timely and accurate localization during landing.\\
% \textbf{Remark.} The absence of the efficient algorithms and sensors make it challenging for existing methods to achieve timely and accurate localization for drone, which requires $(i)$ $cm-level$ localization accuracy and $(ii)$ $ms-level$ localization latency to feed the control loop for effective takeoff and landing. \\


\noindent \textbf{Enhance mmWave radars with event cameras.}
Event cameras are bio-inspired sensors that report pixel-wise intensity changes with $ms-level$ latency (\fig \ref{event}) \cite{xu2023taming}, capturing high-speed motions without blurring \cite{he2024microsaccade}, ideal for feature tracking tasks \cite{alzugaray2018asynchronous}.
% Event cameras share $ms-level$ time resolution with mmWave radar，and the 2D sensing capability of event cameras and the depth sensing capability of radar complement each other, inspiring us to enhance mmWave radar with event cameras for accurate, low-latency drone localization.
Event cameras also offer $ms-level$ time resolution consistent to radar, and their 2D sensing capability complements the limited spatial resolution of radar.
The both \textit{consistent} and \textit{complementary} information across both modalities inspiring us to enhance mmWave radar with event cameras for accurate, low-latency drone localization.

To explore the potential of this idea, we conduct a benchmark study on landing drone localization at a real-world drone delivery airport (\fig \ref{relatedwork}a). 
Our benchmark study reveals that translating this intuition into a practical localization system is non-trivial and faces significant challenges:\\
\noindent $\bullet$ \textit{C1: Noise distribution characteristics of both modalities differ, hindering drone detection.}
% Both sensor modalities yield not only heterogeneous information about the drone but also generate significantly heterogeneous noise. 
These two sensor modalities not only provide different types of information but also generate significantly heterogeneous noise. 
Event cameras produce noise due to unexpected changes in brightness conditions, whereas mmWave radar struggles with noise caused by signal multi-path effects.
These noises differ greatly in both dimension and pattern, which can also be asynchronous under high temporal resolution (\fig \ref{relatedwork}b).
This spatial and temporal heterogeneity complicates noise filtering, causing detection bottlenecks.
Existing noise filtering algorithms \cite{cao2024virteach, liu2024pmtrack, wang2021asynchronous, alzugaray2018asynchronous} \notice{chanage ref} typically target a single modality, resulting in low noise event and point cloud filtering rates (recall and precision < 65\% in \fig \ref{relatedwork}c), limiting their effectiveness.\\
\noindent $\bullet$ \textit{C2: Ultra-large data volume burden the heterogeneous data fusion, delaying drone localization.}
Accurately estimating 3D location of the drone after detection involves time-consuming sensor fusion and optimization. 
The ultra-large amount of data due to the high frequency further exacerbates the processing time, causing significant delays \cite{xu2021followupar}.
Meanwhile, the asynchronous event streams and radar readout are heterogeneous in terms of precision, scale, and density, add complexity to the sensor fusion.
Existing methods (\eg, Extended kalman filter and particle filter) suffer from cumulative drift error, heterogeneity issues and lengthy processing latency, rendering them inadequate for accurate and low-latency localization as shown in \fig \ref{relatedwork}d \cite{zhao20213d, falanga2020dynamic, mitrokhin2018event}. \notice{change ref in fig}

\begin{figure}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=1\columnwidth]{Figs/event.png}
        \vspace{-0.5cm}
    \caption{Illustration of synchronous frames and asynchronous events. \textnormal{Frame cameras use a global shutter to output image frames at fixed intervals. In contrast, each pixel in an event camera responds independently and asynchronously, generating positive or negative events when intensity changes exceed a threshold.}}
    \label{event}
    \vspace{-0.4cm}
\end{figure} 

\noindent \textbf{Our work.}
We propose \textbf{EventLoc}, a robust, high-precision and low-latency landing drone localization system through enhancing mmWave radars functionality with event cameras.
% We delve into the sensing principles of event cameras and mmWave radar, introducing EventLoc. 
% This system enhances event camera functionality with a focus on low-latency drone localization, providing cm-level accuracy with millisecond latency in average. 
% As a result, EventLoc broadens event camera application in diverse 3D vision tasks.
EventLoc features three key designs to unleash the potential of mmWave radar and event camera for drone localization: \\
\noindent $\bullet$ \textbf{On system architecture front.}
% By incorporating event camera with $ms-level$ latency, we enhance the performance of mmWave radar and improve localization performance at the data source.
Incorporating an event camera with $ms-level$ latency enhances mmWave radar and improves localization at the data source.
% EventLoc features a carefully designed system architecture that tightly couples mmWave radar and event camera. 
% This integration spans from early-stage filtering to later-stage fusion and optimization, fully leveraging the unique advantages of both sensors (§\ref{3.2}). \\
The system architecture of EventLoc tightly integrates mmWave radar and the event camera, from early-stage filtering to later-stage fusion and optimization, fully leveraging the unique advantages of both sensors (§\ref{3.2}).\\
\noindent $\bullet$ \textbf{On system algorithm front.}
We first introduce a Consi-stency-Instructed Collaborative Tracking (\textit{CCT}) algorithm to extract \textit{consistent information} from both modalities to filter out environment-triggered noise with a low false positive rate, enhancing the detection performance with a low latency (§\ref{4.1}). 
We then present a Graph-Informed Adaptive Joint Optimization (\textit{GAJO}) algorithm to fully fuse \textit{complementary information}, accelerating the optimization (§\ref{4.2}). \\
\noindent $\bullet$ \textbf{On system implementation front.}
We further analyze the sources of latency and propose an Adaptive Optimization method for boosting the \textit{GAJO}. 
This method dynamically optimizes the set of locations, further enhancing the accuracy of localization and reducing latency (§\ref{5.1}).
\begin{figure}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=1\columnwidth]{Figs/relatedall_3.png}
        \vspace{-0.5cm}
    \caption{Benchmark study on drone localization and performance of existing solutions at different settings.}
    \label{relatedwork}
    \vspace{-0.4cm}
\end{figure} 


\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.4cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.5cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/overview2.png}
        \vspace{-0.2cm}
    \caption{System architecture of EventLoc.}
    \label{overview}
    % \vspace{-0.2cm}
\end{figure*} 




% \noindent \textbf{Evaluation and Result.} 
% We fully implement EventLoc with COTS event camera and mmWave radar.
% Extensive experiments in indoor/outdoor environments are conducted with different drone flight conditions to comprehensively evaluate performance of EventLoc.
% We compare the end-to-end drone localization accuracy and latency of EventLoc with three SOTA methods.
% Through over 30 hours experiments, we demonstrate that EventLoc enhances event camera with mmWave radar by achieving a average localization accuracy of 0.101 $m$ and latency of 5.15 $ms$, surpassing all baselines by >50\% in average.
% Additionally, EventLoc is marginally affected by factors such as drone type and envir. conditions.\\
% \textbf{Real-world deployment.}
% We have deployed the sensor platform with EventLoc at a real-world drone delivery airport as shown in \fig \ref{relatedwork}a to demonstrate practicability of the system.
% 10 hours study shows that EventLoc meets drone landing demands within the constraints of available resources.
\noindent \textbf{Evaluation and Results.}  
We fully implement EventLoc using COTS event cameras and mmWave radar. 
Comprehensive indoor and outdoor experiments under various drone flight conditions assess its localization accuracy and end-to-end latency performance against three SOTA methods.
Over 30+ hours of testing, EventLoc achieves an average localization accuracy of 0.101 m and latency of 5.15 ms, surpassing baselines by over 50\% and showing minimal sensitivity to drone type and environment.
% \textbf{Real-World Deployment.}  
We also deploy EventLoc at a real-world drone delivery airport (\fig \ref{relatedwork}a), demonstrating its practicality. A 10-hour study confirms EventLoc meets drone landing requirements within resource limits.
 
In summary, this paper makes following contributions.\\
\noindent $(1)$ We propose EventLoc, a active, contactless, ground-based, low latency-oriented landing drone localization system. 
It enhance the mmWave radar with asynchronous events, achieving accurate drone localization with millisecond latency. \\
% It tightly integrates asynchronous events and mmWave radar sparse point clouds, achieving accurate drone localization with millisecond latency.\\
\noindent $(2)$ We propose the $CCT$, a light-weight cross-modal noise filter to push the limit of detection accuracy by leveraging the \textit{consistent information} from both modalities. \\
\noindent $(3)$  We propose the $GAJO$, a factor graph-based optimization framework that fully harnessing \textit{complementary information} from both modalities to enhance localization performance.\\
% accuracy and latency
\noindent $(4)$ We implement and extensively evaluate EventLoc by comparing it with three SOTA methods, showing its effectiveness. We also deploy EventLoc in a real-world drone delivery airport, demonstrating feasibility of EventLoc.