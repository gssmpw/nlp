\vspace{-0.3cm}
\section{System Design} \label{4}
In this section, we introduce \textit{CCT}: Consistency-instructed Collaborative Tracking for noise filtering, detection, and preliminary localization of drone (§ \ref{4.1}). 
Subsequently, we delve into \textit{GAJO}: Graph-informed Adaptive Joint Optimization for fine localization and trajectory optimization of drone (§ \ref{4.2}).

\vspace{-0.2cm}
\subsection{\textit{CCT}: Consistency-instructed \\ Collaborative Tracking} \label{4.1}

% \noindent \textbf{Challenge.} 
% Events and mmWave samples contain noise. 
% Event camera asynchronously measures per-pixel brightness changes, often triggered by non-drone factors like shadows. 
% The mmWave radar suffers from signal multipath effects, causing erroneous point clouds. 
% The event camera captures per-pixel brightness changes asynchronously, which are frequently influenced by non-drone factors such as shadows. 
The mmWave radar is prone to signal multipath effects, leading to inaccurate point cloud data.
Meanwhile, the event camera captures per-pixel brightness changes asynchronously, which are frequently influenced by non-drone factors such as shadows.  
% However, the lack of inherent drone semantic information and great difference in dimension and pattern of these two modalities pose challenges to noise filtering, leading to detection bottlenecks and further diminishing the efficiency and precision of localization.
However, the absence of intrinsic drone semantic information, combined with significant differences in dimension and patterns between these two modalities, presents challenges for noise filtering. 
This results in drone detection bottlenecks, which further reduce the efficiency and accuracy of localization.
Therefore, in this part, we focus on enhancing noise filtering and drone detection, while providing preliminary localization of the drone.
% Millisecond latency amplify this phenomenon.
% Millisecond latency adds complexity.
% Previous methods filter noise separately but fall short in achieving low-latency, accurate noise reduction for both sensors. 
% These elements pose challenges to noise filtering, leading to detection bottlenecks and diminishing the efficiency and precision of localization.


% \textit{How to accurately extract drone-related measurements} given the immense noisy output of event cameras and mmWave radars, which also lack inherent drone semantic information and differ greatly in dimension and pattern?
% Both modalities are sensitive to environmental variations (\eg, changes in lighting conditions).

To address this challenge, we explore the operational principles of both sensors. 
Our design is based on observations: \textit{(i) Event camera and mmWave radar demonstrate temporal consistency and distinct response mechanisms.}
% Event camera and mmWave radar are consistent in $ms-level$ latency.
% Meanwhile, event camera are not affected by multipath effects, while mmWave radars are immune to changes in brightness. 
Event camera and mmWave radar maintain $ms$-level latency.
Additionally, event cameras are unaffected by multipath effects, whereas mmWave radar remains impervious to changes in brightness.
\textit{(ii) Drone exhibits periodic micro motion features (\eg, propeller rotation),} which can serve as stable and distinctive features of drone.
% These enable efficient cross-modal noise filtering and drone detection by aligning event camera and mmWave radar measurements to extract drone-specific data.
These facilitate efficient cross-modal noise filtering by aligning measurements from the event camera and mmWave radar and enable drone detection by extracting drone measurements through periodic micro-motions.
% These enable efficient cross-modal noise filtering and detection by aligning event camera and mmWave radar measurements and extracting drone-specific data with periodic micro motion.

% These enable efficient cross-modal noise filtering by aligning event camera and mmWave radar measurements, and detection by extracting drone-specific data with periodic micro motion.
% These enable a light-weight cross-modal noise filtering and drone detection by aligning event camera and mmWave radar measurements, and extract drone-related measurements.

% This enables a light-weight map synchronization by avoiding transferring massive map-point data and the bulky geographical descriptors such as their spatial locations, features, observation relationships with keyframes

% To address this challenge, we explore the operational principles of both sensors. 
% Our design is based on the observation: \textit{both the event camera and mmWave radar respond to dynamic objects, albeit through different mechanisms.}
% The events are triggered by brightness changes, while radar generates point clouds from frequency differences. 
% Event cameras are not affected by multipath effects, while mmWave radars are immune to changes in brightness. 
% Moreover, moving objects cause both brightness variations and frequency differences, eliciting responses from both modalities.
% This allows cross-modal noise filtering by employing \textit{consistent information} of both modalities.

% Compared to current methods, our design leverages the advantages of both sensors, which achieves efficient noise filtering, enabling detection and rough localization of the drone.
\begin{figure}[t]
    \setlength{\abovecaptionskip}{0.2cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.4cm}
    \centering
        \includegraphics[width=0.85\columnwidth]{Figs/event.png}
        % \vspace{-0.18cm}
    \caption{Illustration of synchronous frames and asynchronous events. \textnormal{Frame cameras use a global shutter to capture images at fixed intervals, while each pixel in an event camera responds independently, generating events asynchronously when intensity changes exceed a threshold.}}
    \label{event}
    \vspace{-0.4cm}
\end{figure} 

\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.4cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.34cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=2\columnwidth]{Figs/performance.png}
        \vspace{-0.28cm}
    \caption{Step-by-step filtering performance. \textnormal{The \textit{CCT} module in mmE-Loc eliminate noise events, mmWave point cloud and erroneous detection by employing \textit{temporal-consistency} of both modalities.}}
    \label{performance}
    \vspace{-0.2cm}
\end{figure*}

% \vspace{-0.38cm}
To realize this idea, we design \textit{CCT}, a lightweight cross-modal drone detector and tracker.
% optimized for efficient noise filtering, drone detection, and preliminary ground localization of drones. 
\textit{CCT} includes several components:
$(i)$ a Radar Tracking Model (§\ref{4.1.1}) providing sparse point cloud indicating distance and direction information of objects;
$(ii)$ an Event Tracking Model (§\ref{4.1.2}) for event filtering, detection, and tracking of objects;
% $(iii)$ a Consistency-Instructed Measurements Filter (§\ref{4.1.3}) leverages periodic micro motion feature of the drone and consistency of both modalities to efficiently eliminate erroneous detections and point cloud, enabling rough localization of the drone.
$(iii)$ a Consistency-instructed Measurements Filter (§\ref{4.1.3}) utilizes temporal consistency between both modalities and drone's periodic micro-motion features to extract detection and point cloud of drone, facilitating the preliminary localization.

\subsubsection{\textbf{Radar Tracking Model}} \label{4.1.1}
In this part, we calculate the distance $D$ and direction vector $\vec{v}$ between the radar and objects, along with a preliminary estimation of the object's location, as depicted in \fig \ref{CCT}a and \fig \ref{CCT}b.

% \noindent 
\textbf{Distance calculation.} 
% The difference in frequency between the transmitted signal (TX signal) and received signal (RX signal) reflects the signal propagation time, providing insight into the distance between object and radar.
As shown in \fig \ref{CCT}a, the frequency difference between the transmitted (TX) and received (RX) signals indicates the signal propagation time, revealing the distance between the object and the radar.
Denoting $D^i$ as the distance at time $i$, TX and RX signals as:
\begin{equation}
\begin{aligned}
S_{TX}^i\!=\!\exp \left[j\left(2 \pi f_c i+\pi K i^2\right)\right], 
S_{RX}^i\!=\!\alpha S_{TX}\left[i-2D^i/{c}\right],
\end{aligned}
\end{equation}
where $\alpha$ denotes the attenuation rate, $f_c$ is the initial frequency, $K$ represents the chirp slope of the FMCW signal, and $c$ stands for speed of light.
The TX and RX signals undergo mixing and low-pass filter (LPF) to extract intermediate frequency signal (IF signal) $s(t)$: 
% given as:
\begin{equation}
S_{IF}^i=LPF(S_{TX}^{i*} S_{RX}^{i}) \approx \alpha \exp \left[j 2 \pi\left(2KD^i/c\right)i\right].
\end{equation}
The frequency value $f_{IF}$ within $S_{IF}^i$ encapsulates distance information. 
After the Range-FFT operation $S_{IF}^i$, $f_{IF}$ is extracted, facilitating distance calculation $D^i=c f_{IF} / 2K$.


% \noindent
\textbf{Direction calculation.}
% With a fixed antenna array, mmWave radar determines the object's direction using two orthogonal linear antenna arrays. 
% As shown in \fig \ref{CCT}c, each linear array captures an Angle of Arrival (AoA), calculated from the phase difference between adjacent antennas spaced apart by $d$ as $cos \theta = \Delta \phi \lambda/2 \pi d$, where $\theta$ is AoA, $\lambda$ is the wavelength, and $\Delta \phi$ is the phase difference. 
% Having two orthogonal arrays allows the radar to obtain two AoAs, $\theta_x$ and $\theta_y$. The unit vector indicating the object’s direction at time $i$ is then given by:
Using a fixed antenna array, the mmWave radar determines the object's direction by employing two orthogonal linear arrays. 
As depicted in \fig \ref{CCT}b, each linear array captures an Angle of Arrival (AoA), calculated from the phase difference between adjacent antennas spaced apart by $d$ as $cos \theta = \Delta \phi \lambda/2 \pi d$, where $\theta$ represents AoA, $\lambda$ denotes the wavelength and $\Delta \phi$ indicates the phase difference. 
With two orthogonal arrays, the radar obtains two AoAs, $\theta_x$ and $\theta_y$. The unit vector indicating the object's direction at time $i$ is given by
$\vec{v}^i=[\cos \theta_x \cos \theta_y \sqrt{1-\cos ^2 \theta_x-\cos ^2 \theta_y}]^{\mathrm{T}}$.

% \noindent \textbf{Preliminary estimation.}
Using the distance and angle information obtained above, along with the spatial relationship between radar and event camera, we can determine the preliminary 3D location estimation of the object in $\mathtt{E}$ as $P_E = D\vec{v}+t_{ER}$.
% , as depicted in \fig \ref{performance}d.
% We t the 3D location of the object at each timestamp for object tracking and compute the translation $t_{\mathtt{EO}}$ of the object from $\mathtt{O}$ to $\mathtt{E}$ at each timestamp as:
We then leverage the mmWave radar for object 3D location tracking, estimating the translation $t_{\mathtt{EO}}$ of the object from $\mathtt{O}$ to $\mathtt{E}$ at time $i$:
\begin{equation}
\begin{aligned}
\vspace{-0.2cm}
t_{\mathtt{EO}}^i & =t_{\mathtt{EO}}^{i-1}+U_{\mathtt{E}}^{i}+w^i + w^{i-1} \\
& =t_{\mathtt{EO}}^{i-1}+\left(P_{\mathtt{E}}^i-P_{\mathtt{E}}^{i-1}\right)+w^i + w^{i-1}.
\vspace{-0.2cm}
\end{aligned}
\end{equation}
$U_{\mathtt{E}}^{i}$ is discrepancy between two radar calculation results at times $i$ and ${i-1}$ in $\mathtt{E}$. $w_i$ and $w_{i-1}$ signify the measurement noise.

% Althourgh the mmWave radars have the  capability to excel in accurately estimating the depth of objects along the radial direction, they struggle to precisely capture motion in the tangential direction, which encompasses horizontal and vertical movements. 
% 为了解决该问题，我们引入event cameras，with similar latency to the mmWave radars，but uses a completely different sensing principle，以高空间分辨率 detect 无人机,弥补 mmWave radar 在tangential direction方向上的不足。
\revise{
While mmWave radars excel at estimating object depth along the radial direction, they struggle to accurately capture horizontal and vertical (tangential) motion \cite{qian20203d, zhang2023push}.
To address this issue, we introduce the event camera, which has similar latency but a different sensing principle. 
With high spatial resolution, the event camera detects objects and compensates for mmWave radars' limitations in the tangential direction.
}

% \vspace{-0.4cm}
\subsubsection{\textbf{Event Tracking Model}} \label{4.1.2}
% Compare with frame cameras which use a global shutter to capture images at fixed intervals, event cameras report pixel-wise intensity changes with $ms$-level resolution and $ms$-level sampling latency, capturing high-speed motions without blurring (\fig \ref{event}).
In this part, we demonstrate the process of noise filtering from a stream of asynchronous events, and how to detect and track objects with the filtered events, as depicted in \fig \ref{CCT}c.
Compared to frame cameras that use a global shutter to capture images at fixed intervals, event cameras record pixel-wise intensity changes with $ms$-level resolution and sampling latency, enabling high-speed motion capture without blurring but adding complexity to noise filtering and object detection (Fig. \ref{event}).

% enabling the capture of high-speed motion without blurring, but add complexity to noise filtering, detect and track objects (\fig \ref{event}).
% estimate objects' states based on the detection and tracking results, 

% \noindent 
\textbf{Similarity-informed event filtering.}
Event cameras are prone to noise from transistor circuits and other non-idealities, requiring pre-processing filtering. 
For the $i^{th}$ event $e^i_{(x, y)}$ with the timestamp $t^i_{(x, y)}$, we assess the timestamp ($t^i_{n(x, y)}$) of the most recent neighboring event in all directions. 
Events with a time difference less than the threshold $T_n$ are retained, indicating object activity, while those exceeding it are discarded as noise (\fig \ref{performance}b, \fig \ref{performance}c).
\revise{
We utilize the Surface of Active Events (SAE) \cite{lin2020efficient} to manage events, mapping coordinates $(x, y)$ to timestamps $(t_l, t_r)$.
Upon a new event's arrival, $t_l$ updates accordingly, and $t_r$ updates only if the previous event at the same location occurred outside the time window $T_k$ or had a different polarity. 
Events that update value of $t_r$ are retained.
The event stream, segregated by polarity, is processed with distinct SAEs. 
This method ensures precise spatial-temporal representation, reducing events and conserving computational resources.
}

% Event cameras are prone to noise due to transistor circuit noise, and non-idealities, \etc. Implementing a pre-processing filtering block is essential to mitigate these effects.  
% Initially, for the $i^{th}$ event $e^i_{(x, y)}$ at coordinates $(x, y)$ with timestamp $t^i_{(x, y)}$, we assess the timestamp ($t^i_{n(x, y)}$) of the most recent neighboring event $e^i_{n(x, y)}$ in all directions (horizontal, vertical, and diagonal). 
% Events with a time difference less than the threshold $T_n$ are retained, indicating a connection to neighborhood activity and likely to the object. 
% Events exceeding this threshold are likely noise and discarded.
% To manage events efficiently, we employ a Surface of Active Events (SAE), summarizing the event stream. 
% The SAE $\mathcal{S}$ maps coordinates $(x, y)$ to timestamps $(t_r, t_l)$. 
% The event stream is segregated by polarity, processed separately with distinct SAEs. 
% Upon a new event arrival at time $t$, $t_l$ updates accordingly, while $t_r$ updates only if the previous event at the same location occurred outside the time window $\mathtt{k}$ or had a different polarity. 
% This method ensures precise spatial and temporal representation while drastically reducing the event stream by eliminating redundancies, thus conserving computational resources.

% Event cameras, like other vision sensors, are prone to noise due to shot noise in photons, transistor circuit noise, and non-idealities. Implementing a pre-processing filtering block is essential to mitigate these effects. 
% Initially, for the $i^{th}$ event $e^i_{(x, y)}$ at coordinates $(x, y)$ with timestamp $t^i_{(x, y)}$, we assess the timestamp ($t^i_{n(x, y)}$) of the most recent neighboring event $e^i_{n(x, y)}$ in all directions (horizontal, vertical, and diagonal). 
% Events with a time difference less than the threshold $T_n$ are retained, indicating a connection to neighborhood activity and likely to the drone. 
% Events exceeding this threshold are likely noise and discarded.

% We further analyze previous events to determine if $e^i_{(x, y)}$ is linked to the drone.
% To efficiently manage past events, we utilize a Surface of Active Events (SAE) \tocite, summarizing the event stream. 
% The SAE $\mathcal{S}$ is defined as $\mathcal{S}: (x, y) \in \mathbb{R}^2 \mapsto (t_r, t_l) \in \mathbb{R}^2$, where $t_l$ is the timestamp of the latest event, and $t_r$ is the reference time.
% We segregate the event stream by polarity, processing each set separately with distinct SAEs. 
% Upon arrival of a new event at time $t$, $t_l$ updates to $t$, while $t_r$ updates only if the previous event at the same location occurred outside the time window $\mathtt{k}$ or had a different polarity. 
% Events that update the value of $t_r$ are considered in the subsequent algorithm.
% This method ensures precise spatial and temporal representation while drastically reducing the event stream by eliminating redundancies, thus conserving computational resources.

% Event cameras, akin to other vision sensors, are susceptible to noise stemming from inherent shot noise in photons, transistor circuit noise, and non-idealities. Employing a pre-processing filtering block is crucial to mitigate these effects.
% Initially, for the $i^{th}$ event $e^i_{(x, y)}$ occurring at coordinates $(x, y)$ with timestamp $t^i_{(x, y)}$, we examine the timestamp ($t^i_{n(x, y)}$) of the most recent neighboring event $e^i_{n(x, y)}$ in all directions (horizontal, vertical, and diagonal). If the time difference between $t^i_{(x, y)}$ and $t^i_{n(x, y)}$ is less than the threshold $T_n$, the event is retained.
% Filtered events signify their association with neighborhood activity, implying a probable connection to the drone. 
% Events surpassing the threshold are likely noise and thus discarded.

% We further analyze previously triggered events in the stream to determine if $e^i_{(x, y)}$ is connected to the drone. To handle the large volume of past events efficiently, we utilize a Surface of Active Events (SAE) \tocite to summarize the event stream at any given moment. The SAE $\mathcal{S}$ is defined as $\mathcal{S}: (x, y) \in \mathbb{R}^2 \mapsto (t_r, t_l) \in \mathbb{R}^2$, where $t_l$ represents the timestamp of the latest event triggered at pixel location $(x, y)$, and $t_r$ is the reference time.
% We partition the event stream based on polarity and process each set independently using separate SAEs. 
% When a new event arrives at time $t$, the value of $t_l$ at that location is always updated ($t_l \leftarrow t$) in $\mathcal{S}$. However, the reference time $t_r$ is only updated if the previous event at the same location occurred outside the time window $\mathtt{k}$, indicated by $t_r \leftarrow t$ if $t > t_l + \mathtt{k}$, or if the polarity of the latest event differs from that of the incoming one. Events that update the value of $t_r$ in $\mathcal{S}$ are considered in the subsequent algorithm.
% This approach ensures more accurate spatial and temporal representation of high contrast regions while significantly reducing the event stream by eliminating redundant events, thus saving computational resources.


% \noindent

\textbf{Filter-based detection and tracking.} 
We employ a grid-based method to cluster events to facilitate object detection. 
The camera's field of view is partitioned into elementary cells sized $c_w \times c_h$. 
For each cell, we compare the event count within a specified time interval ($c_{\Delta t}$) to an activation threshold $c_{thres}$. 
Cells surpassing $c_{thres}$ are marked as active and connected to form clusters, serving as object detection results, including those generated by the drone.
\revise{
For tracking, we deploy Kalman filter-based trackers with a constant velocity motion model, as the Kalman filter provides low-latency estimates with minimal computational cost.
% For tracking, we deploy the Kalman filter-based trackers built on a constant velocity motion model due to their simplicity and efficiency, as Kalman filter providing low-latency estimates with minimal computational cost.
A tracker predicts the state of the current object and associates it with the input cluster that has the largest Intersection Over the Union area. 
The input cluster corrects tracker state, generating bounding boxes, and effectively tracking moving objects, including the drone.
}

% We cluster events with a grid-based method to facilitate object detection. 
% Initially, we partition the camera's field of view (FOV) into elementary cells with a regular grid, each cell sized $c_w \times c_h$. 
% For each cell, we compare the event count within it over a specified time interval ($c_{\Delta t}$) to an activation threshold $c_{thres}$. Cells surpassing $c_{thres}$ are marked as active. 
% Active cells are then connected to form clusters, serving as detection results, including those generated by the drone.
% For object tracking, we deploy trackers based on a constant velocity motion model and Kalman filter. These trackers generate bounding box proposals from input clusters. 
% Initially, a tracker predicts the current object state using the constant velocity motion model. Upon receiving a cluster input, we associate it with the tracker exhibiting the largest Intersection Over Union (IOU) area compared to the cluster. 
% Finally, we use the input cluster as an observation state to update the current tracker state, and the trackers then generate bounding box proposals. 
% This method facilitates tracking of moving objects, including drones.
% In cases where no IOU association is feasible but the distance between the tracker and the cluster is less than $d_{iou}$, the cluster is linked to the nearest tracker. 

% \noindent \textbf{Preliminary estimation.}
\revise{
Using bounding box proposals and the pinhole camera model with projection function $\pi$, we estimate the preliminary 3D locations of objects.
Specifically, the projection function $\pi$ transforms a 3D point $\textbf{X}_\mathtt{E}$ in $\mathtt{E}$ into a 2D pixel $x$ in the image plane as: 
% $x\!=\!\pi\left(\textbf{X}_\mathtt{E}\right)\!=\![f_x X_\mathtt{E} / Z_\mathtt{E}+c_x,
% f_y Y_\mathtt{E} / Z_\mathtt{E}+c_y]^T, 
% \textbf{X}_\mathtt{E}\!=\![X_\mathtt{E}, Y_\mathtt{E}, Z_\mathtt{E} ]^T,$
\begin{equation}
x\!=\!\pi\left(\textbf{X}_\mathtt{E}\right)\!=\![f_x X_\mathtt{E} / Z_\mathtt{E}+c_x,
f_y Y_\mathtt{E} / Z_\mathtt{E}+c_y]^T, 
\textbf{X}_\mathtt{E}\!=\![X_\mathtt{E}, Y_\mathtt{E}, Z_\mathtt{E} ]^T,
\end{equation}
where $[f_x, f_y]^T$ is the focal length of the event camera, and $[c_x, c_y]^T$ denotes the principal point, both being intrinsic camera parameters. 
% The $Z_\mathtt{E}$ is measured by mmWave radar ($t_{EO}^i)$.
Then, the object's preliminary location at time $i$ is estimated using the center point of bounding box proposal $x^i$ as: 
% $x^i =\pi(\textbf{X}_\mathtt{E}^i)+v^i =\pi(\textbf{X}_\mathtt{O}^i+t_{\mathtt{EO}}^i)+v^i, $
\begin{equation}
x^i =\pi(\textbf{X}_\mathtt{E}^i)+v^i =\pi(\textbf{X}_\mathtt{O}^i+t_{\mathtt{EO}}^i)+v^i,
\end{equation}
% \begin{equation}
% \begin{aligned}
% x^i & =\pi\left(\textbf{X}_\mathtt{E}^i\right)+v^i =\pi\left(\textbf{X}_\mathtt{O}
% ^i+t_{\mathtt{EO}}^i\right)+v^i,
% \end{aligned}
% \end{equation}
% $t_{\mathtt{EO}}^i$ is mmWave radar measurement,
where $\textbf{X}_\mathtt{O}^i$ represents the corresponding 3D point of center point $x^i$ in the object reference $\mathtt{O}$, $v^i$ denotes the random noise.
% Eq. (3) and Eq. (5)
% of center point.
When extracting center points from bounding box proposals, we first undistort their coordinates. 
}

% However, the event camera encounter a scale uncertainty issue as it cannot determine the object's depth $Z_\mathtt{E}$ from a single central point of detection result. 
% The mmWave radar, which operates with similar latency to the event camera but uses a completely different sensing principle, generating sparse point clouds with depth information.

% 同时，mmWave radar 没有语义信息，无法直接从 objects measurements中分辨出 drone 相关的 measurements。


% Utilizing bounding box proposals and a conventional pinhole camera model with a projection function $\pi$, we can determine the preliminary 3D location estimation of the object. 
% $\pi$ transforms a 3D point $X_\mathtt{E}$ in $\mathtt{E}$, into a 2D pixel $x$ in image plane as:
% \begin{equation}
% \pi\left(X_\mathtt{E}\right)=\left[\begin{array}{l}
% f_x \frac{X_\mathtt{E}}{Z_\mathtt{E}}+c_x \\
% f_y \frac{Y_\mathtt{E}}{Z_\mathtt{E}}+c_y
% \end{array}\right], \quad X_\mathtt{E}=\left[X_\mathtt{E} \space Y_\mathtt{E} \space Z_\mathtt{E} \right]^T,
% \end{equation}
% where $\left[f_x, f_y\right]^T$ is the focal length of the event camera and $\left[c_x, c_y\right]^T$ is the principle point. They are the camera intrinsic parameters.
% When extracting the center points from bounding box proposals, we first undistort their coordinates. 
% Subsequently, we can obtain the object's location estimation under $\mathtt{E}$ at time $i$ with the center point of bounding box proposal $x^i$ as:
% \begin{equation}
% \begin{aligned}
% x^i & =\pi\left(X_\mathtt{E}^i\right)+v^i =\pi\left(X_\mathtt{O}
% ^i+t_{\mathtt{EO}}^i\right)+v^i,
% \end{aligned}
% \end{equation}
% where $X_\mathtt{O}^i$ is the corresponding 3D point of $x^i$ in object reference, and $v^i$ is the random noise of the feature point.
% It's worth noting that we encounter a scale issue because we only have one central point.

% We perform event clustering with a grid-based method to aid in objects detection.
% Firstly, we divide the camera's field of view (FOV) into elementary cells using a regular grid, with each cell sized according to $c_w \times c_h$.
% Then, for each cell, we compare the number of events within it over a specified time interval ($c_{\Delta t}$) against an activation threshold $c_{thres}$.
% If this count exceeds $c_{thres}$, the cell is marked as active.
% Finally, we connect active cells to form clusters as detection results, containing clusters generated by drones.
% We employ trackers built upon a constant velocity motion model and Kalman filter to track objects. 
% These trackers generate bounding box proposals based on input clusters. 
% Specifically, a tracker initially employs the constant velocity motion model to predict the current state of the object based on its previous state.
% Subsequently, upon receiving a cluster input, we associate it with the tracker exhibiting the largest Intersection Over Union (IOU) area compared to the cluster.
% In cases where no IOU association is feasible but the distance between the tracker and the cluster is less than $d_{iou}$, the cluster is linked to the nearest tracker. 
% Finally, we utilize the input cluster as an observation state to correct the current state of the tracker. 
% This approach enables the tracking of moving objects, including drones.




\vspace{-0.8cm}
\subsubsection{\textbf{Consistency-instructed Measurements Filter}} \label{4.1.3}
% The output of the \textit{Event Tracking Model} includes detection results for drones as well as other objects in the environment that cause changes in light intensity, such as indicator lights near drone landing sites or shadows cast by drones and objects passing near the event camera. 
% The system needs to separate the detection results for the landing drone from the detections of various objects.
% Similarly, the 3D point cloud output by the \textit{Radar Tracking Model} contains points for the landing drone as well as noise points generated by multipath effects. 
% It is necessary to extract the points relevant to the landing drone from this noisy point cloud.

% The \textit{Event Tracking Model} detects drones and other objects causing light intensity changes, such as indicator lights or shadows near the event camera. The system must distinguish the detection results for the landing drone from other objects. 
% Similarly, the \textit{Radar Tracking Model} outputs a 3D point cloud with points for the drone and noise from multipath effects. It is necessary to extract the relevant points for the landing drone from this noisy cloud.

The \textit{Event Tracking Model} detects drones and other objects causing light changes, such as indicator lights or shadows. The system must distinguish the landing drone from these objects. Similarly, the \textit{Radar Tracking Model} outputs a 3D point cloud containing both the drone and noise from multipath effects, requiring extraction of the drone’s relevant points.

% \noindent 
\textbf{Consistency-instructed alignment.} 
Utilizing the \textit{temporal-consistency} from the event camera and radar, and their distinct mechanisms respond to dynamic objects, we filter event camera results affected by lighting variations on stationary objects and vice versa for radar points influenced by multipath effects.
Specifically, we align synchronized radar points (\textit{Radar Tracking Model}) to each event bounding box (\textit{Event Tracking Model}) (\fig \ref{performance}d). 
Using event camera's projection, we determine that object's location lies along the ray from the camera's optical center through bounding box center. 
The system then identifies the nearest radar points along this ray to isolate the object-associated points.
If no radar point is detected, the bounding box is treated as noise and disregarded.

% Leveraging the \textit{consistent information} of event camera and mmWave radar respond to dynamic objects with different mechanisms, we filter out event camera tracking results caused by light variations of stationary objects using radar tracking results and, conversely, filter out radar points generated by multipath effects using event camera tracking results. 
% Specifically, for synchronized bounding boxes from the \textit{Event Tracking Model} and point clouds from the \textit{Radar Tracking Model}, we match radar points to each event camera tracking result, as shown in \fig \ref{performance}e. 
% Referring to projection function of event camera, we establish that the object's location lies along the ray originating from the event camera optical center and passing through the center point of the bounding box on the image plane. 
% Our system then identifies the radar points closest to this ray, enabling us to isolate the radar point associated with the object while disregarding other measurements.
% If no radar point is detected, we consider the bounding box to be the result of unexpected noise and disregard this.\\
% \noindent 

\textbf{Periodic micro motion-aid measurements extraction.} 
% Each platform supports one drone landing at a time, we need to find a feature of drone， and utilize it to extract landing drone-specific measurements from the aligned tracking results. 
% This feature must effectively distinguish drones from noise.
Since each platform supports one drone landing at a time, we need to identify a distinguishing feature of the landing drone, which effectively differentiates the drone from noise, and use it to extract landing drone-specific measurements from the aligned tracking results. 
% This feature must effectively differentiate drones from noise.
Our finding is that drones exhibit periodic micro-motions (\eg, propeller rotation), which can serve as stable and distinctive features of the drone. 
We transform the spatio-temporal distribution of events into a heatmap and apply statistical metrics to isolate drone measurements leveraging this feature. 
% Specifically, events within a time window $[i, i + \delta i]$ are binned into a 2D histogram, with each bin representing a spatial region (\eg, $5\times 5$ pixels). 
% Bins with periodic micro-motions contain more events due to rapid light changes. 
% Meanwhile, periodic micro-motions produces bipolar events in a bin, whereas background motion and noise tends to be unipolar (\eg, flying birds)
% We 根据event counts and the proportion of positive events in each bin select  bins with periodic micro-motions which exhibit higher event counts and a more balanced proportion of event polarities.
% 随后，我们选择包含 bins with periodic micro-motions最多的event tracking results and corresponding point clouds ($t_{EO}$) as the drone tracking result ($t_{ED}$) to roughly localize the drone.
Specifically, within a time window $[i, i + \delta i]$, events are binned into a 2D histogram where each bin corresponds to a spatial region (\eg., $5\times 5$ pixels). 
Bins containing propeller rotation tend to accumulate more events due to rapid light intensity changes. 
Meanwhile, these propeller rotations generate bipolar events within a bin, while background motion and noise typically result in unipolar events (\eg, from flying birds).
Therefore, we select bins with propeller rotation based on event counts and the proportion of positive events, favoring those with higher counts and a more balanced ratio. 
Finally, we identify event tracking results with the most bins indicative of propeller rotation and corresponding point clouds ($t_{EO}$), designating them as drone tracking results ($t_{ED}$) for preliminary localization from two models as shown in \fig \ref{performance}e.
% The drone near to the platform
% This approach reduces tracking latency by leveraging both sensors, enhancing accuracy and reliability.
When multiple drones are scheduled to land, they descend and land sequentially. 
This method accurately identifies the landing drone and extracts relevant measurements.
% This method accurately determines the drone nearest to the platform, identifying the landing drone.
% This method accurately determines the landing drone, extract measurements related to it.
% Assuming the number of landing drones is provided and no other moving objects are present, we select the tracking results with the largest bounding box and associated point clouds ($t_{EO}$) as the drones tracking result ($t_{ED}$), which are used to roughly localize the drone. 
% \notice{select the tracking results with the rotation part. Only one drone landing at one time}
% % This method effectively reduces tracking latency by combining the strengths of both sensors.
% This innovative approach significantly minimizes tracking latency by leveraging the combined strengths of both sensors, resulting in enhanced accuracy and reliability.

\vspace{-0.2cm}
\subsection{\textit{GAJO}: Graph-informed Adaptive \\ Joint Optimization} \label{4.2}

% \noindent \textbf{Challenge.} 
% So far, we attain a preliminary estimation results of drone location. 
% However, results of both event camera tracking model and radar tracking model suffer from severe location tracking bias. 
% The event camera estimations are hampered by scale uncertainty and restricted resolutions, while radar estimations grapple with the challenges of low spatial resolution and accumulating drift. 
% Additionally, data from both sensors is heterogeneous in nature.
% In addition to this, accurate 3D localization proves to be more time-consuming than detection and tracking due to additional processing.
% Thus, in mmE-Loc, we focus on the accurate localization of drone and latency minimization.
The preliminary drone location estimations from the event and radar tracking models suffer from biases. 
Specifically, event camera estimations face scale uncertainty, while radar estimations struggle with limited spatial resolution, scatter center drift, and accumulating drift. 
% Meanwhile, measurements from different tracking models are heterogeneous in precision, scale, and density, which adds complexity to fusion and optimization. 
Additionally, estimations from different models are heterogeneous in precision, scale, and density, complicating the fusion and optimization.
Therefore, in this part, we prioritize accurate drone ground localization and trajectory tracking.


% \noindent \textbf{Observation.} 
% Our design is based on an observation that \textit{the event camera tracking and radar tracking models leverage two distinct modalities and features, respectively.} 
% Consequently, the two models benefit from their individual yet complementary advantages, and thus a joint optimization would enhance the overall performance, yielding a trajectory that exhibits both low bias and low cumulative drift.

% (1) Supplementing the design section with more design details, providing insight into how the spatial complementarity features of the radar and event camera are fused;
\revise{

Our design is founded on the insight that \textit{the Event Tracking Model and Radar Tracking Model provide distinct features that are spatial-complementarity to each other.} 
As a result, the 2D imaging capability of event cameras and the depth sensing capability of mmWave radar mutually enhance each other when combined, as demonstrated in \fig \ref{relationship}. 
Since both the event stream and mmWave samples are drone-related, fully leveraging the \textit{spatial- complementarity} of these two modalities through joint optimization offers the potential to significantly improve performance. This leads to a trajectory with reduced bias and minimized cumulative drift.
% Our design is founded on the insight that \textit{the Event Tracking Model and Radar Tracking Model provide distinct features that are spatial-complementarity to each other.}
% As a result, the 2D imaging capability of event cameras and depth sensing capability of mmWave radar are mutually beneficial when combined, as shown in \fig \ref{relationship}.
% Since the event stream and mmWave sample are all drone-related, fully harnessing the \textit{spatial-complementarity} of both modalities through a joint optimization holds promise for comprehensively enhancing performance, resulting in a trajectory characterized by reduced bias and decreased cumulative drift.
}
% 充分挖掘两个model的潜力，同时Integrating these models through a joint optimization process is promising to enhance performance comprehensively, resulting in a trajectory characterized by reduced bias and a decrease in cumulative drift. 

To realize this idea and push the limit of localization accuracy while minimizing latency, we introduce a \textit{GAJO}, a factor graph-based location optimization framework designed for low-latency and accurate drone 3D localization (§\ref{4.2.1}).
\textit{GAJO} includes two parallel tightly coupled modules: $(i)$ short-term (inter-SAE tracking) and $(ii)$ long-term (local location optimization) optimizations, collectively enhancing location tracking precision (§\ref{4.2.3}).
Beyond the capabilities of \textit{Event Tracking model} and \textit{Radar Tracking model}, \textit{GAJO} assimilates prior knowledge of drone's flight dynamics to refine the trajectory for enhanced smoothness and accuracy (§\ref{4.2.2}).


\subsubsection{\textbf{Factor graph-based optimization}}\label{4.2.1}

% ChatGPT
% A factor graph consists of two types of nodes: $(i)$ the variable nodes which indicate the states to be optimized (\eg, $t_{ED}^i$); $(ii)$ the factor nodes which represent the probability of certain states given a measurement result.
% In mmE-Loc, these measurements come from the ET (abbr. for Event Tracking) model ($x^i$) and RT (abbr. for Radar Tracking) model ($D^i$, $\vec{v}^i$ and $U_E^{i}$).
% In order to estimate the values of a certain set of variable nodes $\boldsymbol{\mathcal{X}} = \{t_{ED}^i | i \in \mathcal{T}\}$ given measurements $\boldsymbol{\mathcal{Z}} = \{x^i, D^i, \vec{v}^i, U_E^{i} | i \in \mathcal{T}\}$, \textit{GAJO} optimizes all the factor nodes connected with them based on maximum a posteriori estimation:
A factor graph comprises variable nodes, indicating the states to be optimized (\eg, $t_{ED}^i$), and factor nodes, representing the probability of certain states given a measurement result. 
In mmE-Loc, measurements are derived from the Event Tracking (ET) model ($x^i$) and Radar Tracking (RT) model ($D^i$, $\vec{v}^i$, and $U_E^{i}$).
To estimate the values of a set of variable nodes $\boldsymbol{\mathcal{X}} = \{t_{ED}^i | i \in \mathcal{T}\}$ given measurements $\boldsymbol{\mathcal{Z}} = \{x^i, D^i, \vec{v}^i, U_E^{i} | i \in \mathcal{T}\}$, \textit{GAJO} optimizes all connected factor nodes based on maximum a posteriori estimation:
\begin{align}
\begin{split}
\hat{\boldsymbol{\mathcal{X}}} & =\underset{\boldsymbol{\mathcal{X}}}{\arg \max } \ p(\boldsymbol{\mathcal{X}} \mid \boldsymbol{\mathcal{Z}}) = \underset{\boldsymbol{\mathcal{X}}} {\arg \max } \  p(\boldsymbol{\mathcal{X}}) \ p(\boldsymbol{\mathcal{Z}} \mid \boldsymbol{\mathcal{X}}) \\
& =\underset{\boldsymbol{\mathcal{X}}}{\arg \max } \ 
p(\boldsymbol{\mathcal{X}}) \prod_{i \in \mathcal{T}} \ p\left(x^i \mid t_{ED}^i\right) p\left(D^i, \vec{v}^i, U_E^{i} \mid t_{ED}^i\right),
\end{split}
\label{factor_graph}
\end{align}
which follows the Bayes theorem.
% , and all measurements are independent.
$p(\boldsymbol{\mathcal{X}})$ is the prior information over $\boldsymbol{\mathcal{X}} = \{t_{ED}^i | i \in \mathcal{T}\}$, which is inferred from drone flight characteristics.
The $p\left(x^i \mid t_{ED}^i\right)$ is the likelihood of the ET model measurements. The $p\left(D^i \mid t_{ED}^i\right)$, $p\left(\vec{v}^i \mid t_{ED}^i\right)$ and $p\left(U_E^{i} \mid t_{ED}^i\right)$ are likelihood of the RT model measurements.

\begin{figure}[t]
    % \setlength{\abovecaptionskip}{-0.1cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.2cm}
    \setlength{\subfigcapskip}{-0.6cm}
    \centering
        \includegraphics[width=0.95\columnwidth]{Figs/relationship.png}
        \vspace{-0.4cm}
    \caption{Illustration of relationship between \textit{GAJO} and \textit{CCT}. \textnormal{The \textit{GAJO} module harness the \textit{spatial-complementarity} of both modalities through a join optimization.}}
    \label{relationship}
    \vspace{-0.5cm}
\end{figure} 


\begin{figure*}[t]
    \setlength{\abovecaptionskip}{0.05cm} % height above Figure X caption
    \setlength{\belowcaptionskip}{-0.3cm}
    \setlength{\subfigcapskip}{-0.25cm}
    \centering
        \includegraphics[width=1.92\columnwidth]{Figs/factorgraph.png}
        % \vspace{-0.1cm}
    \caption{Long-short term optimization based on the factor graph.}
    \label{factorgraph}
    \vspace{-0.25cm}
\end{figure*} 


\subsubsection{\textbf{Probabilistic Representation}} \label{4.2.2}
Inferring the drone's location requires prior term and likelihood term in \eqn \eqref{factor_graph}.

% \noindent 
\textbf{Prior term.} 
% The prior term $p(t_{ED}^i)$ indicates the probability distribution of the drone’s location time $i$ without knowing any measurement result. 
% Based on the kinetic characteristics of the drone, the constant velocity model, which has been widely used in both flight control and SLAM system, is adopted to derive the prior term. Specifically, the drone is assumed to move at an approximately constant speed during a short period of time. On this basis, the prior location can be inferred from 
% The prior term regarding the drone's position probability distribution  at time $i$, not influenced by current measurements, is expressed with $p(t_{ED}^i)$. 
% This prior is derived from the constant velocity model used in modern flight control system, revealing that the drone likely maintains steady speed over short intervals. 
% This assumption allows us to predict the prior location through the relation
% The prior term, $p(t_{ED}^i)$, represents the probability distribution of the drone's position at time $i$ unaffected by current measurements. It is derived from a constant velocity model commonly used in modern flight control systems, indicating that the drone likely maintains a steady speed over short intervals. This assumption enables us to predict the prior location:
The prior term, $p(t_{ED}^i)$, represents the drone's location probability distribution at time $i$ unaffected by current measurements. Derived from a constant velocity model, it suggests the drone maintains steady speed over short intervals, allowing us to predict the prior location using:
\begin{equation}
\vspace{-0.1cm}
\bar{t}_{\mathrm{ED}}^i-t_{\mathrm{ED}}^{i-1}=t_{\mathrm{ED}}^{i-1}-t_{\mathrm{ED}}^{i-2}.
% \vspace{-0.1cm}
\end{equation}

% \noindent 
\textbf{ET model likelihood.} 
% The likelihood of the ET model $p(x^i|t_{ED}^i)$ indicates the distribution of the center point at a given drone location.
% Most existing vision-based systems treat random noise of center point $v^i$ as Gaussian distribution (\ie normal distribution). The assumption has been proved to be effective in many tracking systems.
% Therefore, the ET model measurement likelihood can be presented as follows:
The likelihood $p(x^i|t_{ED}^i)$ from ET model represents the center point distribution at a given drone location. 
In many tracking systems \cite{campos2021orb}, center point noise $v^i$ is assumed Gaussian, proving effective. Thus, likelihood of ET model is:
% $p(x^i|t_{ED}^i) \sim \mathcal{N}(\pi(\textbf{X}_E^i), \sigma_{ET})$,
% \vspace{-0.4cm}
\begin{equation}
p(x^i|t_{ED}^i) \sim \mathcal{N}(\pi(\textbf{X}_E^i), \sigma_{ET}),
% \vspace{-0.4cm}
\end{equation}
where $\sigma_{ET}$ is the center point standard deviation.
% where the $\sigma_{ET}$ is the standard deviation of center point measurement.

% \noindent 
\textbf{RT model likelihood.}
The likelihood of the RT model $p(D^i \mid t_{ED}^i)$, $p(\vec{v}^i \mid t_{ED}^i)$ and $p(U_E^{i} \mid t_{ED}^i)$ indicates the distribution of the measured distance, angle, and motion at a given drone location.
The distance, angle, and motion from RT model likelihood are:
% \vspace{-0.2cm}
\begin{equation}
\begin{aligned}
p(D^i \mid t_{ED}^i&) \sim  \mathcal{N}(||t_{ED}^i||, \sigma_{D}), \quad p(\vec{v}^i \mid t_{ED}^i) \sim \mathcal{N}(\vec{v}_{t_{ED}^i}, \sigma_{\vec{v}}), \\
& p(U_E^{i} \mid t_{ED}^i) \sim \mathcal{N}(t_{ED}^i - t_{ED}^{i - 1}, \sigma_{U_E}),
\vspace{-0.4cm}
\end{aligned}
\end{equation}
where $\sigma_{D}$, $\sigma_{\vec{v}}$ and $\sigma_{U_E^{i}}$ are the standard deviation of distance, angle, and motion measurements respectively.

\subsubsection{\textbf{Fusion-based Tracking}} \label{4.2.3}
% As illustrated in \fig \todo{Figure}, two types of fusion schemes are adopted in mmE-Loc.
% Specifically, the inter-frame tracking infers the drone’s location in real-time. In contrast, the local pose tracking focuses on the overall accuracy of the flight trajectory over a period of time.
% In mmE-Loc, two fusion schemes are utilized, as shown in \fig \ref{factorgraph}. 
% The first scheme, inter-frame tracking, aims to estimate the real-time location of the drone. 
% On the other hand, the second scheme, local pose tracking, focuses on ensuring the overall accuracy of the flight trajectory over a certain time period.
\revise{
In mmE-Loc, two fusion schemes are employed for sensor fusion and optimization, as depicted in \fig \ref{factorgraph}. 
The first, inter-SAE tracking, aims for instant drone location estimation by minimizing errors across different tracking models simultaneously.  
The second, local location optimization, enhances overall trajectory accuracy through the joint optimization of a selected set of locations.
}
% The first, inter-SAE tracking, aims for instant drone location estimation by minimizing the error of different tracking model at the same time.
% The second, local location optimization, ensures overall trajectory accuracy through joint optimization of selected location set.

% \noindent 
\textbf{Inter-SAE tracking.}
% (\aka projection error term)
Once the measurements of ET model and RT model $(x^i, D^i, \vec{v}^i, U_E^i)$ received, the prior factor, ET factor and the RT factor are formulated as follows:
% \vspace{-0.5cm}
\begin{equation}
\begin{aligned}
E^i_{\text {Prior }} & =-\log p\left(t_{ED}^i\right) \propto \left\|t_{ED}^i-\bar{t}_{ED}^i\right\|_{\sigma_{t_{ED}}}^2, \\
E^i_{\mathrm{ET}} & =-\log p\left(x^i \mid t_{ED}^i\right) \propto \rho(\left\| x^i - \pi(\textbf{X}_E^i) \right\|^2_{\Sigma_E}), \\
E^i_{\mathrm{RT}} & =-\log p\left(D^i, \vec{v}^i, U_E^i \mid t_{ED}^i\right) \\
\propto & \left\| ||t_{ED}^i|| \!-\! D^i \right\|^2_{\sigma_D} \!+\!  \left\| \vec{v}_{t_{ED}^i} \!-\! \vec{v}^i \right\|^2_{\sigma_{\vec{v}}} \!+\! \left\| (t_{ED}^i - t_{ED}^{i-1}) \!-\! U_E^i \right\|^2_{\sigma_{U_E}},
\end{aligned}
\end{equation}
% where $\left\|e \right\|^2_\Omega=e^T\Omega e$. The symbol $\Omega_E$ represents the information matrix, which is the inverse of the covariance matrix associated with the event camera measurements.
where $\left\|e \right\|^2_{\Sigma_E}=e^T\Sigma^{-1} e$.
% is defined as the squared Mahalanobis distance with covariance matrix $\Sigma_E$.  
The symbol $\Sigma_E$ represents the covariance matrix associated with the event camera measurements.

On this basis, the inter-SAE tracking in \fig \ref{factorgraph}a is performed to give an instant location result based on \eqn \eqref{factor_graph} as follows:
% \vspace{-0.5cm}
\begin{equation}
\begin{aligned}
& \hat{t}_{ED}^i \!=\! \underset{\boldsymbol{t_{ED}^i}}{\arg \max } \ p ( t_{ED}^i \!\mid\! t_{ED}^{i-1}, t_{ED}^{i-2} ) p(x^i \!\mid\! t_{ED}^i) \ p(D^i, \vec{v}^i, U_E^{i} \!\mid\! t_{ED}^i) \\
\vspace{1ex}
& = \underset{\boldsymbol{t_{ED}^i}}{\arg \min } \!-\!\log \!\left(p ( t_{ED}^i \!\mid\! t_{ED}^{i-1}, t_{ED}^{i-2} ) p(x^i \!\mid\! t_{ED}^i) p(D^i\!,\! \vec{v}^i\!,\! U_E^{i} \!\mid\! t_{ED}^i)\right) \\
\vspace{1ex}
& = \underset{\boldsymbol{t_{ED}^i}}{\arg \min } \left( E^i_{\text {prior }} + E^i_{\mathrm{ET}} + E^i_{\mathrm{RT}}\right).
\end{aligned}
\label{inter_frame}
\end{equation}
% Inter-SAE tracking

% \noindent 
\textbf{Local location optimization.}
% For every few frames, the local location tracking is triggered to correct the cumulative drift. 
% Local location tracking takes several frames and jointly optimizes their locations:
% Denote the set of frames as $\mathcal{T}$, the optimization problem can be formulated as follows:
% To address cumulative drift, periodic local location tracking is performed, which corrects the estimated locations based on several consecutive frames. 
% This optimization process involves jointly optimizing the locations of a set of frames denoted as $\mathcal{X}=\underset{i \in \mathcal{T}}{\bigcup}\left\{t_{ED}^i\right\}$. 
% The formulation of the optimization problem is as follows:
To mitigate cumulative drift, periodic local location optimization is conducted, correcting estimated locations based on multiple consecutive SAEs. 
This optimization entails jointly optimizing the locations of a SAE set denoted as $\mathcal{X}=\underset{i \in \mathcal{T}}{\bigcup}\left\{t_{ED}^i\right\}$, as shown in \fig \ref{factorgraph}b, where $W=|\mathcal{T}|$.
The optimization problem formulation is as follows:
\begin{equation}
\begin{aligned}
\hat{\boldsymbol{\mathcal{X}}} & =\underset{\boldsymbol{\mathcal{X}}}{\arg \max } \ p(\boldsymbol{\mathcal{X}}) \prod_{i \in \mathcal{T}} \ p\left(x^i \mid t_{ED}^i\right) p\left(D^i, \vec{v}^i, U_E^{i} \mid t_{ED}^i\right), \\
& = \underset{\boldsymbol{\mathcal{X}}}{\arg \min } \sum_{i \in \mathcal{T}}\left(E_i^{\mathrm{prior}}+E_i^{\mathrm{ET}}+E_i^{\mathrm{RT}}\right) .
\end{aligned}
\label{local_location}
\end{equation}
It is worth noting that $(i)$ when the local location optimization is triggered, $(ii)$ what is the size of $\mathcal{T}$ ($W$ = $|\mathcal{T}|$), and $(iii)$ how to solve the inter-SAE tracking and local location optimization problems affect the latency and accuracy of localization.
Hence, we enhance the efficiency of \textit{GAJO} through an adaptive optimization method.

% \subsubsection{\textbf{Adaptively Optimization method}}
% Now we express the estimation problem \eqn \ref{inter_frame} and \eqn \ref{local_location} using a graphical model. 
% When solving both nonlinear least-squares problems, we linearize the observation model and solve the least squares formulation as follows:
% \begin{equation}
% \hat{\boldsymbol{\mathcal{X}}}=\arg \min _{\boldsymbol{\mathcal{X}}}\|A \boldsymbol{\mathcal{X}}-\mathbf{b}\|^2,
% \end{equation}
% where the matrix $A \in \mathbb{R}^{m \times n}$ is a measurement Jacobian and $\mathbf{b} \in \mathbb{R}^m$ is the right-hand side vector \tocite.
% The QR matrix factorization $A = Q[R, 0]^T$ is then utilized, and the least squares problem $R \hat{\boldsymbol{\mathcal{X}}}=\mathbf{d}$ is solved through backsubstitution to get optimized locations $\hat{\boldsymbol{\mathcal{X}}}$, where $R \in \mathbb{R}^{n \times n}$ is the upper triangular square root information matrix, $Q \in \mathbb{R}^{m \times m}$ is an orthogonal matrix and $\textbf{d} \in \mathbb{R}^n$. 
% More detail can refer to \tocite.
% % A batch solution solves the complete problem at every step, including all previous measurements, which performs unnecessary calculations.
% % A  exploits incrementally updating the square root information matrix R with new measurements.
% Although re-linearization and re-generate $R$ as new measurements comes can reduce system error, for the problem \eqn \ref{local_location}, which requires joint optimization of multiple locations, this process can be computationally expensive \tocite.

% To address this problem, we propose the Adaptively Optimization method, based on the observation that \textit{new measurements often have a localized impact, leaving remote parts of the graph unaffected}, which enable us to incrementally update $R$ \tocite. 
% When solving local location tracking at each step, this method adaptively combines incrementally updated $R$ and re-generated $R$, reducing latency and improving accuracy.
% \alg \ref{algorithm} shows how Adaptively Optimization method solves local location tracking problem.
% Line 1-3 represents local location tracking with incrementally updated $R$ \tocite.
% Line 4-16 show local location tracking with re-generated $R$.
% Specifically, when receive new measurements, function $\mathtt{AddFactorToGraph}$ updates factor graph, and function $\mathtt{IncrementalUpdate}$ incrementally update $R$ with new measurements \tocite. 
% We then solve local location tracking with this incrementally updated $R$.
% When one of two conditions is met, we solve local location tracking with re-generated $R$: 
% $(i)$ we tracks locations that have changed significantly in a set $L = \{t_{ED}^i: \hat{t^i_{ED}} - t^i_{ED} \geq \delta\}$. If enough locations have undergone significant changes (\ie $|L| \geq L_T$), we solve local location tracking with re-generated $R$ output by function $\mathtt{FullUpdate}$ \tocite;
% $(ii)$ if the norm of the total locations changes becomes too large (\ie $||\hat{\mathcal{X}} - \mathcal{X} || \geq \Delta$), we solve local location tracking with re-generated $R$; 
% Since the local location tracking involves repeatedly solving linear equations, this condition keeps the current solution from diverging too far from the optimal solution.


% \begin{algorithm}[t]
% \caption{Adaptively Optimization method}
% \label{algorithm}
% \KwData{Original factor graph $G$; New measurements $D, \vec{v}, U^i_E$; square root information matrix $R$}
% \KwResult{Updated locations $\hat{\mathcal{X}}$}
% $G \leftarrow \mathtt{AddFactorToGraph}(G, D, \vec{v}, U^i_E)$\;
% $R \leftarrow \mathtt{\textbf{IncrementalUpdate}}(G)$\;
% $\hat{\mathcal{X}} \leftarrow \mathtt{Backsubstitution}(R)$\;
% $L \leftarrow \emptyset$;  $\quad\quad \triangleright \textit{Set of nodes need to be linearized}$\;
% \For{all $t^i_{ED} \in \mathcal{X}$ and all $\hat{t^i_{ED}} \in \hat{\mathcal{X}}$}
% {
% \If{$\hat{t^i_{ED}} - t^i_{ED} \geq \delta$}
% {$L \leftarrow L \cup t^i_{ED}$\;}
% }
% \If{$|L| \geq L_T$ or $||\hat{\mathcal{X}} - \mathcal{X} || \geq \Delta$}
% {
% \For{all $t^i_{ED} \in \mathcal{X}$}
% {
% $\mathtt{UpdateLinearizationPoint}(t^i_{ED})$\;
% }
% $R \leftarrow \mathtt{\textbf{FullUpdate}}(G)$\;
% $\hat{\mathcal{X}} \leftarrow \mathtt{Backsubstitution}(R)$\;
% }
% \end{algorithm}