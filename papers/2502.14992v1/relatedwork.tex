\section{Related work}
% We briefly review the related works in the following.

% refer to https://zhangjia44.github.io/files/PDF/mmHawkeye_tosn.pdf
% \noindent 
\revise{
\textbf{Drone ground localization.}
Several types of work have been proposed to assist drones in localization.
$(i)$ \textit{Satellite-based systems}.
The Global Positioning System (GPS) provides $m$-level accuracy outdoors \cite{wang2023global, dong2023gpsmirror}, while Real-Time Kinematic (RTK) achieves $cm$-level precision but is costly. 
However, these satellite-based systems struggle in urban canyons \cite{wang2022micnest, xu2020edge, wang2024transformloc, wang2022h}. 
$(ii)$ \textit{Optics-based systems}.
These systems, such as motion capture, offer $cm$-level accuracy indoors but require precise calibration, making them impractical for outdoor use \cite{xue2022m4esh}. 
$(iii)$ \textit{Sensor-based systems}.
To address these issues, various sensor-based techniques are proposed, including camera \cite{he2022automatch, ben2022edge, tasneem2020adaptive}, radar \cite{sie2023batmobility, iizuka2023millisign}, LiDAR \cite{cui2024vilam, jian2024lvcp} and acoustic \cite{wang2022micnest} often combined with SLAM (Simultaneous Localization and Mapping) \cite{chen2020h, chen2017design, chen2023adaptslam, chen2015drunkwalk} or deep learning algorithms \cite{zhao2024foes, zhao2023smoothlander, hu2024seesys, zhao2024understanding, yimiao2023bifrost}, aim to improve drone localization. 
However, limited spatio-temporal resolution in these sensors affects precise, low-latency landing drone localization. 
% For example, mmWave radars excel at estimating depth along the radial axis but struggle with tangential motion \cite{qian20203d, zhao20213d}.
For example, cameras and LiDARs, with low frame rates (< 50$Hz$), may miss rapid movements during frames, reducing localization accuracy \cite{jian2023path}.
Acoustic signal-based systems are highly susceptible to environmental noise, with even nearby humans impacting their stability.
Visual marker-based systems work in conjunction with downward-facing cameras mounted on drones. 
However, while these systems assist drones in obtaining their location, they do not help ground platforms track drones. 
They also are sensitive to lighting conditions due to limited dynamic range of frame cameras.

Compared to previous methods, mmE-Loc leverages a novel sensor configuration combining event camera with mmWave radar, harmonizing ultra-high sampling frequencies, to achieve superior drone ground localization accuracy with low latency.
Meanwhile, mmE-Loc is resistant to lighting variations due to its sensor configuration, with event cameras offering a high dynamic range. 
It is important to note that mmE-Loc complements, rather than replaces, existing localization solutions.
To ensure precise landing, mmE-Loc will work in conjunction with RTK and visual markers, providing a more reliable and accurate localization service.
% Meanwhile, mmE-Loc is resist to noise affection and lighting conditions due to event cameras and mmWave signal-based sensor configuration, where event cameras have high dynamic range.
% It's worth to note that mmE-Loc is a complement, not a replacement, to the existing localization solutions. 
% The safety of commercial drones can not be overemphasized. 
% To assist drones in precise landing, mmE-Loc will not work alone but will cooperate with RTK and the visual marker to provide a more reliable and accurate localization service.
}
% offering millisecond-level latency.


% \noindent
\textbf{mmWave for localization and tracking.}
Millimeter-wave is highly sensitive and more accurate due to its mm-level wavelength.
% Several mmWave radar-based drone ground localization solutions 结合 signal intensity-based methods, however, struggle to accurately track the drone’s center due to its large size (\eg, 80$cm$ across), which cause the drone appears as a non-uniform blob in radar returns. These methods also yield unstable results with frequent outliers, as radar’s low spatial resolution and multipath scattering can overpower the main signal \tocite;
% 还有一些practices 利用 deep learning-based methods, however, necessitate extensive pre-modeling of the drone and prior neural network training. These methods struggle to track different drone models and perform poorly in environments not represented in the training dataset \tocite. 
% 除了软件方法，还有一些 practices introduces visual sensors (\eg, cameras) to aid radar. However, these methods add system latency due to long exposure times (around 20$ms$) and additional image processing delays (another 10-20$ms$). Also, motion blur in frames can confuse the algorithm, increasing errors \tocite;
mmWave radar offers high sensitivity and precision due to its sub-millimeter wavelength \cite{Harlow_2024,s23218901,lu2020smokerobustindoormapping}. 
Several mmWave radar-based solutions for drone ground localization combine signal intensity methods, but face challenges in accurately tracking the drone's center \cite{qian20203d, asi6040068, 10.1145/3678549}. 
This difficulty arises from the drone's large size (\eg, 80 $cm$ across), causing it to appear as a non-uniform blob in radar returns. 
Additionally, these methods often produce unstable results with frequent outliers, as multipath scattering can obscure the main signal and low spatial resolution of radar \cite{Li2024AHH}.
Other approaches leverage deep learning-based methods but require extensive pre-modeling and neural network training for each drone model \cite{lu2020see, zheng2023neuroradar, ALLQUBAYDHI2024100614}. 
These methods tend to struggle with tracking different drone models and perform poorly in environments not represented in training dataset. 
% Beyond software-based methods, 
Several solutions integrate visual sensors to assist radar \cite{shuai2021millieye,chadwick2019distant, cho2014multi}. 
% However, these approaches introduce latency due to long exposure times of cameras (around 20 $ms$) and additional image processing delays (10-20 $ms$). 
However, these approaches introduce latency due to exposure times and image processing delays.
% However, these approaches introduce latency due to exposure times of cameras and additional image processing delays. 
% Moreover, motion blur in camera frames can also confuse algorithms, increasing errors. 

% To overcome the accuracy and latency bottlenecks, we introduce event cameras to enhance mmWave radar performance. 
To overcome the accuracy and latency bottlenecks, we upgrade frame cameras with event cameras to pair with mmWave radar, boosting system performance. 
By exploiting temporal consistency and drone's periodic micro-motion with \textit{CCT} module, mmE-Loc achieves impressive tracking accuracy without the need for prior knowledge, (\eg, training data or 3D models). 
The integration of the \textit{GAJO} module employing spatial complementarity and an adaptive optimization method further enables accurate drone ground localization and reduces latency to the $ms$-level.

\revise{
\textbf{Sensor Fusion Techniques.}
Sensor fusion techniques are widely used in localization \cite{li2022motion, li2023riscan, wang2023meta, xue2023towards}. 
\textit{(i) Traditional pipeline fusion methods.}  
These approaches typically integrate dense point clouds from sensors (\eg, LiDAR) to provide depth information, alongside pixel data from frame cameras \cite{he2023vi, shi2024soar}. 
However, these methods are generally limited to fusing two types of dense measurements and cannot directly accommodate the sparse data output from event cameras and mmWave radar. 
\textit{(ii) Deep learning-based fusion methods.}  
Recent advancements have explored learning-based methods for fusing mmWave radar, frame cameras, and IMUs for localization \cite{xu2021followupar, safa2023fusing}. 
While these methods offer promising results, they often require extensive labeled training data and experience performance degradation in dynamic environments \cite{ shuai2021millieye, lu2020see, ding2023push, li2023egocentric, muller2023aircraft}. 
In contrast, mmE-Loc adopts a tightly coupled fusion framework, leveraging factor graphs and adaptive optimization to jointly optimize radar and event tracking models. 
This approach provides a clearer probabilistic interpretation compared to deep learning-based methods.
% Sensor fusion techniques are often used in localization and mapping \cite{li2022motion, li2023riscan, wang2023meta, xue2023towards}. 
% \textit{(i) Traditional pipeline fusion methods.}
% These methods typically integrate dense point clouds from sensors (\eg LiDAR) to provide depth information alongside pixel data from frame cameras \cite{he2023vi, shi2024soar}, which are typically limited to fusing two types of dense measurements. 
% However, the output of event cameras and mmWave radar is sparse, these methodes can't be directly used.
% \textit{(i) Deep learning-based fusion methods.}
% Recent advancements explore learning-based methods to fuse mmWave radar, frame camera and IMU for localization, which often demand extensive labeled training data and suffer from performance degradation when the environment changes \cite{xu2021followupar, shuai2021millieye, lu2020see, ding2023push, li2023egocentric}.
% Inspired by existing SLAM systems, mmE-Loc adopts a tightly coupled fusion framework based on factor graphs and an adaptive optimization method to jointly optimize the radar and event tracking models, which has a clear interpretation in probability compared with deep learning-based methods.

}




% \noindent \textbf{Event-based systems for localization.}
% Event cameras provide several advantages over traditional frame-based cameras, such as high temporal resolution\cite{cannici2019asynchronous}, low latency \cite{gallego2020event}, and a wide dynamic range \cite{he2021fast, he2024microsaccade}. 
% They have been utilized in SLAM\cite{zhou2021event} and object tracking\cite{zuo2024cross}.
% \cite{falanga2020dynamic} presents an obstacle avoidance solution for drones using event cameras. The monocular version estimates depth using known obstacle information, while the binocular version uses computational geometry. 
% However, this method and others like \cite{mitrokhin2018event} focus primarily on obstacle detection.
% % Additionally, ESVO\cite{zhou2021event} uses binocular event cameras for SLAM, and BioDrone \cite{xu2023taming} employs them for obstacle localization during high-speed flight, also relying on computational geometry for depth estimation, which are limited by camera resolution.
% Additionally, ESVO \cite{zhou2021event} uses binocular event cameras for SLAM, and BioDrone \cite{xu2023taming} employs them for obstacle localization during high-speed flight, both relying on computational geometry for depth estimation, which are limited by camera resolution.
% In this work, we enhance the accuracy and efficiency of event-based systems with mmWave radar, operating at $ms$-level, and introduce a novel signal processing pipeline featuring two new modules, \textit{CCT} and \textit{GAJO}, benefiting low-latency localization.
% Event cameras offer numerous potential advantages over conventional frame-based cameras, including high temporal resolution, low latency, and high dynamic range. Recent systems use them for scene reconstruction, SLAM, object tracking, and HDR image reconstruction.
% 举例而言，\cite{falanga2020dynamic} presents a notable obstacle avoidance solution for drones.该方法包含单目事件相机和双目事件相机两个版本。在使用单目版本时，该方法利用已知的障碍物信息来估计深度；而在双目版本中，该方法利用计算几何的方式来进行深度估计。
% However, this method，包括\cite{mitrokhin2018event}，is primarily oriented towards obstacle detection tasks。
% 除此以外，ESVO 利用双目事件相机来进行SLAM。\cite{xu2023taming}利用双目事件相机，在高速飞行时对障碍物进行定位。这些方法都是利用计算几何的方式来进行障碍物深度的估计。
% In this work, we combine事件相机 与 相同采样频率level的传感器，mmWave radar，and introduce a novel event camera-mmWave radar signal processing pipeline, featuring two new modules, CCT and GAJO.
% Our design aims to enhance the accuracy and efficiency of event-based systems, with particular benefits for low latency localization of drone.


% \noindent \textbf{mmWave for Localization and Tracking.}

% \noindent \textbf{Sensor Fusion Techniques.}
% Sensor fusion techniques are commonly used in autonomous driving systems for localization and mapping, primarily relying on dense point clouds from sensors including LiDAR to provide depth information for each pixel from an RGB camera. 
% However, these techniques are limited to fusing two types of dense measurements.
% Recent approaches have utilized learning-based method including CNN fuse mmWave radar and IMU signals for end-to-end localization.
% % These methods, however, require large amounts of labeled training data.
% Sensor fusion techniques are often used in localization and mapping \cite{li2022motion, li2023riscan, wang2023meta, xue2023towards}. 
% They typically integrate dense point clouds from sensors (\eg LiDAR) to provide depth information alongside pixel data from RGB cameras \cite{he2023vi, shi2024soar}, which are typically limited to fusing only two types of dense measurements. 
% Recent advancements explore learning-based methods to fuse mmWave radar and IMU signals for localization, which often demand extensive labeled training data \cite{lu2020see, ding2023push, li2023egocentric}.
% FollowUpAR \cite{xu2021followupar} combines RGB cameras and mmWave radar for 6-DoF tracking of the object, which has a latency of over 60$ms$ due to the camera.
% Inspired by existing systems, mmE-Loc adopts a tightly coupled fusion framework based on factor graphs and an adaptive optimization method to jointly optimize the radar and event tracking models, exhibiting superior efficiency.


% Existing literature can be categorized into two types: $(i)$ extra infrastructure-based solutions.
% GPS is the most popular technique, providing m-level accuracy outdoors \cite{wang2023global, dong2023gpsmirror}. 
% RTK, an improved but more expensive version of GPS, achieves cm-level accuracy. 
% However, these satellite-based systems become inaccurate in urban canyons due to the need for line-of-sight connections \cite{wang2022micnest, xu2020edge, wang2024transformloc}. 
% Motion capture systems, can also localize drones with cm-level accuracy but are typically used indoors and require precise calibration, making them unsuitable for precise landing \cite{xue2022m4esh}.
% $(ii)$ intra on-board sensor-based solutions, which leverage onboard sensors like cameras\cite{he2022automatch}, IMUs\cite{lu2020milliego}, LiDAR\cite{cui2024vilam}, radar\cite{sie2023batmobility, iizuka2023millisign} and microphone \cite{wang2022micnest, sun2022aim, weiguo2020symphony}, along with SLAM \cite{xu2022swarmmap}, deep learning techniques \cite{zhao2024understanding, yimiao2023bifrost} and visual markers\cite{krogius2019iros}, are proposed to achieve high-precision localization. 
% However, these methods suffer from high update latency and envir. compatibility.
% % , failing to meet the demands of drone landing.
% In summary, no existing solutions can simultaneously fulfill the requirements of high precision and low latency for drone landing.
% Compared with previous works, mmE-Loc achieves delightful tracking accuracy in extremely low latency, without any prior knowledge (\eg, training data and geometries knowledge).


% (\eg, training data and geometries knowledge).