% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User defined packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Algorithm}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins, theorems}
% 
\usepackage{adjustbox}
% \usepackage{mdframed}
\usepackage[framemethod=tikz]{mdframed}
% \newenvironment{maintheorem}[1][]{%
%   \begin{mdframed}[backgroundcolor=blue!5, linecolor=blue!75!black, linewidth=0pt, roundcorner=0pt, skipabove=5pt]%
%   \begin{theorem}[#1]%
% }{%
%   \end{theorem}%
%   \end{mdframed}%
%   \vspace{5pt}
% }
\newenvironment{maintheorem}[1][]{%
  \begin{tcolorbox}[
    colback = blue!5,
    colframe = white,
    fonttitle = \bfseries,
    breakable = true]
  \begin{theorem}[#1]%
}{%
  \end{theorem}%
  \end{tcolorbox}
  \vspace{5pt}
}
\newenvironment{maintheoreminformal}[1][]{%
  \begin{mdframed}[backgroundcolor=blue!5, linecolor=blue!75!black, linewidth=2pt, roundcorner=5pt, skipabove=5pt]%
  \begin{theorem*}%
}{%
  \end{theorem*}%
  \end{mdframed}%
  \vspace{5pt}
}
% \newenvironment{mainlemma}[1][]{%
%   \begin{mdframed}[backgroundcolor=gray!5, linecolor=gray!75!gray, linewidth=0pt, roundcorner=5pt, skipabove=5pt]%
%   \begin{lemma}[#1]%
% }{%
%   \end{lemma}%
%   \end{mdframed}%
%   \vspace{5pt}
% }
\newenvironment{mainlemma}[1][]{%
  \begin{tcolorbox}[
    colback = gray!5,
    colframe = white,
    fonttitle = \bfseries,
    breakable = true]
  \begin{lemma}[#1]%
}{%
  \end{lemma}%
  \end{tcolorbox}
  \vspace{5pt}
}

% \newenvironment{mainobservation}[1][]{%
%   \begin{mdframed}[backgroundcolor=green!5, linecolor=green!75!green, linewidth=0pt, roundcorner=5pt, skipabove=5pt]%
%   \begin{observation}[#1]%
% }{%
%   \end{observation}%
%   \end{mdframed}%
%   \vspace{5pt}
% }
\newenvironment{mainobservation}[1][]{%
  \begin{tcolorbox}[
    colback = green!5,
    colframe = white,
    fonttitle = \bfseries,
    breakable = true]
  \begin{observation}[#1]%
}{%
  \end{observation}%
  \end{tcolorbox}
  \vspace{5pt}
}
% \newenvironment{maincoro}[1][]{%
%   \begin{mdframed}[backgroundcolor=red!5, linecolor=red!75!red, linewidth=0pt, roundcorner=5pt, skipabove=5pt]%
%   \begin{corollary}[#1]%
% }{%
%   \end{corollary}%
%   \end{mdframed}%
%   \vspace{5pt}
% }
\newenvironment{maincoro}[1][]{%
  \begin{tcolorbox}[
    colback = red!5,
    colframe = white,
    fonttitle = \bfseries,
    breakable = true]
  \begin{corollary}[#1]%
}{%
  \end{corollary}%
  \end{tcolorbox}
  \vspace{5pt}
}
\newenvironment{mainassumption}[1][]{%
  \begin{tcolorbox}[
    colback = yellow!5,
    colframe = white,
    fonttitle = \bfseries,
    breakable = true]
  \begin{assumption}[#1]%
}{%
  \end{assumption}%
  \end{tcolorbox}
  \vspace{5pt}
}
% \newenvironment{mainassumption}[1][]{%
%   \begin{mdframed}[backgroundcolor=yellow!5, linecolor=red!75!red, linewidth=0pt, roundcorner=5pt, skipabove=5pt]%
%   \begin{assumption}[#1]%
% }{%
%   \end{assumption}%
%   \end{mdframed}%
%   \vspace{5pt}
% }
\usepackage{framed}

\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\newcommand{\BOS}{\textsc{BOS}}
\newcommand{\EOS}{\textsc{EOS}}
\newcommand{\vocab}{\mathcal{V}}
 \newcommand{\Umax}{{U}_\mathrm{{max}}}
\newcommand{\Phuman}{P_\mathrm{{human}}}
\newcommand{\Pmodel}{P_\mathrm{{model}}} % In math
% \newcommand{\human}{h^{\mathrm{human}}}
% \newcommand{\hmonte}{h^{\mathrm{mc}}}
% \newcommand{\hmodel}{h^{\mathrm{model}}}
\newcommand{\human}{y^*}
\newcommand{\hmodel}{y^{\mathrm{m}}}
\newcommand{\hmonte}{\hat{y}^m}
\newcommand{\errh}{{\mathrm{err}}({h^{\mathrm{human}}})}
\newcommand{\errmc}{\mathrm{err}({h^{\mathrm{mc}}})}
\newcommand{\haterrh}{{\widehat{\mathrm{err}}}({h^{\mathrm{human}}})}
\newcommand{\haterrmc}{\widehat{\mathrm{err}}({h^{\mathrm{mc}}})}
\newcommand{\diffmodel}{\mathrm{diff}({P_\mathrm{{model}}})}
\newcommand{\Href}{\mathcal{H_{\mathrm{ref}}}}
\let\P\relax
\DeclareMathOperator*{\argmax}{arg\,max}
% {\argmin}{arg\,min}
% \newcommand{\dE}{\mathbb{E}}
\newcommand{\ba}{\boldsymbol{\alpha}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ichihara}[1]{\textcolor{red}{\textbf{[ichihara: }#1\textbf{]}}}

\newcommand{\hm}{\hat{y}^m_{\mathrm{MAP}}} 
\newcommand{\maphuman}{y^*_{\mathrm{MAP}}}
\newcommand{\mapmodel}{y^m_{\mathrm{MAP}}}
\newcommand{\1}{\mbox{1}\hspace{-0.25em}\mbox{l}}

\newcommand{\Yn}{\mathcal{Y}^\mathrm{n}_{\mathrm{ref}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\theoremstyle{theorem*}
\newtheorem*{theorem*}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{assumption}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\theoremstyle{observation}
\newtheorem{observation}{Observation}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Theoretical Guarantees for Minimum Bayes Risk Decoding}
\author{
Yuki Ichihara$^{1}$\qquad Yuu Jinnai$^{2}$\qquad Kaito Ariu$^{2}$\qquad Tetsuro Morimura$^{2}$\qquad Eiji Uchibe$^{3}$\\
$^1$Nara Institute of Science and Technology\qquad $^2$CyberAgent\\\qquad$^3$Advanced Telecommunications Research Institute International\\
\texttt{ichihara.yuki.iu1@is.naist.jp}\\ \texttt{\{jinnai\_yu,kaito\_ariu,morimura\_tetsuro\}@cyberagent.co.jp}\\ \texttt{uchibe@atr.jp}
}

\begin{document}
\maketitle
\begin{abstract}
Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing the expected utility value of an underlying human distribution.
While prior work has shown the effectiveness of MBR decoding through empirical evaluation, few studies have analytically investigated why the method is effective.
As a result of our analysis, we show that, given the size $n$ of the reference hypothesis set used in computation, MBR decoding approaches the optimal solution with high probability at a rate of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$, under certain assumptions, even though the language space $\mathcal{Y}$ is significantly larger $\mathcal{Y}\gg n$.  
This result helps to theoretically explain the strong performance observed in several prior empirical studies on MBR decoding.
In addition, we provide the performance gap for maximum-a-posteriori (MAP) decoding and compare it to MBR decoding. The result of this paper indicates that MBR decoding tends to converge to the optimal solution faster than MAP decoding in several cases.
\end{abstract}
\section{Introduction}
Minimum Bayes Risk (MBR) decoding \cite{kumar-byrne-2002-minimum,kumar-byrne-2004-minimum} is a decision rule used to generate sequences from autoregressive probability models (e.g., LLMs). MBR decoding has been shown to produce high-quality texts in various directed text generation tasks, such as machine translation \cite{tromble-etal-2008-lattice, de-gispert-etal-2009-minimum, stahlberg-etal-2017-neural}, text summarization \cite{rush-etal-2015-neural, narayan-etal-2018-dont}, text simplification \cite{heineman-etal-2024-improving}, image captioning \cite{borgeaud-emerson-2020-leveraging}, and instruction-following \cite{wu2025better}.  Numerous experiments have reported the advantages of MBR decoding over maximum-a-posteriori (MAP) decoding (e.g., beam search) \cite{ehling-etal-2007-minimum,  eikema-aziz-2020-map, muller-sennrich-2021-understanding, eikema-aziz-2022-sampling, bertsch-etal-2023-mbr}.

Experimental results confirm that the larger the number of candidates and hypothesis sets collected, the better performance \cite{eikema-aziz-2022-sampling,freitag-etal-2022-high}.
However, there is no theoretical explanation for the convergence rate of approaching optimal output.
The answers to this question are the number of elements in the candidate and the hypothesis set in this paper.
Our results show the following theorem.
\begin{maintheoreminformal}(Convergence Rate of MBR Decoding; Informal)
    Under certain assumptions, MBR decoding approaches the optimal solution with high probability at a rate of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$ for the size $n$ of the reference hypothesis set.
\end{maintheoreminformal}
This theoretical result is consistent with the empirical results of previous studies \cite{eikema-aziz-2022-sampling,freitag-etal-2022-high}.
We also confirm that if the human distribution is similar to the model distribution, the performance of MBR decoding can be improved, as indicated by \citet{ohashi-etal-2024-true}. 
In addition, we derive the convergence rate of the optimal output for MAP decoding and compare it to MBR decoding. Our results show that MBR decoding tends to converge faster than MAP decoding in several cases.

Specifically, our main contributions are that we provide high probability and expected regret's upper bounds by MBR decoding in several cases (Theorem~\ref{theorem:bound3}, Theorem~\ref{theorem:bound}, and Corollary ~\ref{propotion:mbr}) and we compare the performance gap and convergence rate of MBR decoding and MAP decoding within the same framework of the upper bound we derived in Section~\ref{section:map_mbr}.

In summary, there are few theoretical analyses of MBR decoding, and thus a comprehensive theoretical framework has yet to be fully established. 
Through these contributions, we believe that this study offers new perspectives that advance the understanding of MBR decoding.

\section{Background and Notations}
Text generation involves producing an output sequence based on an input sequence, the set of input sequences is defined by $\mathcal{X}$. Probabilistic text generators define a probability distribution over the output space of hypotheses $\mathcal{Y}$. The set of complete hypotheses $\mathcal{Y}$ is:
\begin{equation*}
    \mathcal{Y} := \{\BOS \circ \bv \circ \EOS | \bv \in \mathcal{V}^*\}.
\end{equation*}
where $\circ$ is a string concatenation and $\mathcal{V}^*$ is the Kleene closure of a set of vocabulary $\mathcal{V}$. 
The goal of decoding is to find the best hypothesis for a given input. 
For simplicity 
we write $\mathcal{M}^{\mathcal{X}}_{\mathcal{Y}}$ to denote a set of conditional probability distributions over a finite set $\mathcal{Y}$, given $\mathcal{X}$ as context sets.
and $\mathcal{O}(n)$ is Big O notation.

\subsection{MBR Decoding}
Let $\mathcal{X}$ denote the input space and $\mathcal{Y}$ the output space. Given an input $x \in \mathcal{X}$, a probabilistic model defines a distribution $p\in\mathcal{M}^{\mathcal{X}}_{\mathcal{Y}}$ over possible outputs $y' \in \mathcal{Y}$. The goal of Bayes Risk minimization in structured prediction and sequence generation tasks is to select an output that minimizes the expected loss relative to the true distribution \cite{bach2024learning}.

For a loss function $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$, the Bayes Risk is defined as:
\begin{equation*}
\mathcal{R}(y \mid x) = \E_p\left[\ell(y', y) \right].
\end{equation*}
\begin{equation*}
y^* = \arg\min_{y \in \mathcal{Y}} \mathcal{R}(y \mid x)
\end{equation*}
If the goal is to maximize some utility function $u$ rather than to minimize a loss, it can also be interpreted as a performance metric $\ell = -u $.

The objective of MBR decoding is similar to the Bayes Risk, finding the output that maximizes the expected utility, thereby effectively minimizing risk \cite{kumar-byrne-2002-minimum,kumar-byrne-2004-minimum}.

The procedure consists of two key components: the human distribution $\Phuman \in \mathcal{M}^{\mathcal{X}}_\mathcal{Y}$ given input $x\in \mathcal{X}$ and a utility function. For simplicity, let $P(y \mid x) = P(y)$, since $x$ is fixed in this paper.
The utility function evaluates the quality of a candidate output $\mathcal{H} \subseteq \mathcal{Y}$ with respect to a reference output $\mathcal{Y}$.  
In this paper, we assume that the candidate hypotheses $\mathcal{H}$ are identical to the reference outputs $\mathcal{Y}$.  
Ideally, MBR decoding selects the optimal hypothesis by maximizing its expected utility over the distribution of human references:
\begin{align}
    u_h(y) &= \sum_{y' \in \mathcal{Y}} u(y, y')\cdot\Phuman(y').\\
    \human&= \argmax_{y \in \mathcal{Y}} u_h(y).  \label{eq:true_MBR decoding}
\end{align}
where utility function $u$: $\mathcal{Y} \times \mathcal{Y} \rightarrow [0,\Umax]$, $\Umax \in [0,1]$ denotes the maximum utility value.

Since $\Phuman$ is unknown, MBR decoding instead uses $\Pmodel\in \mathcal{M}_\mathcal{Y}^{\mathcal{X}}$ to approximate $\Phuman$.
\begin{align}
    u_m(y) &= \sum_{y' \in \mathcal{Y}} u(y, y')\cdot\Pmodel(y').\\
     \hmodel &= \argmax_{y \in \mathcal{Y}} u_m(y).  
\label{eq:model-MBR decoding}
\end{align}
However, summation over $\mathcal{Y}$ is computationally intractable, so Eq.~\eqref{eq:model-MBR decoding} is approximated by a Monte Carlo estimation \cite{eikema-aziz-2022-sampling,farinhas-etal-2023-empirical} using a collection of reference hypotheses $\Yn$ sampled from the model $\Pmodel$.
\begin{align}
    \widehat{u}_m(y) &= \frac{1}{|\Yn|} \sum_{y' \in \Yn} u(y, y').\\
    \hmonte &= \argmax_{y \in \mathcal{\Yn}} \widehat{u}_m(y). \label{eq:monte_MBR decoding}
\end{align}
We denote the number of samples used for the Monte Carlo estimate of the MBR decoding as $n \coloneqq \left|\Yn\right|$.

Therefore, to derive a practical application equation (Eq.~\ref{eq:monte_MBR decoding}), two approximation operations are performed from the objective equation for true MBR decoding (Eq.~\ref{eq:true_MBR decoding}).
\subsection{MAP Decoding}
The most intuitive decoding method is MAP decoding, which selects a mode based on the human distribution $\Phuman$. MAP decoding is also a special case in which the utility function of MBR decoding is used as the indicator function.
The objective function of MAP decoding is defined by:
\begin{equation}\label{eq:map_human}
    \maphuman = \argmax_{y \in \mathcal{Y}} \Phuman(y).
\end{equation}
The objective equation using the model probability $\Pmodel$ is similar to the MBR decoding:
\begin{equation}\label{eq:map_model}
    \mapmodel = \argmax_{y \in \mathcal{Y}} \Pmodel(y).
\end{equation}
In addition, the objective equation for the Monte Carlo estimation of the MAP decoding is defined as:
\begin{equation}\label{eq:empirical_P}
\widehat{P}(y)=\frac{\sum_{{y}^{\prime} \in \Yn} \mathbb{I}\left({y}={y}^{\prime}\right)}{\left|\Yn\right|} .
\end{equation}
where  $\mathcal{Y}^\mathrm{n}_{\mathrm{ref}}$ collected n samples from $\Pmodel$. 

We reformulate the practical objective function of MAP decoding using Eq.~\eqref{eq:empirical_P}:
\begin{equation}\label{eq:map_monte}
    \hm = \argmax_{y \in \Yn} \widehat{P}(y).
\end{equation}
Eq.~\eqref{eq:map_monte} shows the computationally feasible approximation of the MAP decoding. 
While beam search is the most common sampling strategy to approximate MAP decoding. 

We focus on the analysis of MAP decoding and MBR decoding with random sampling in this paper, the case of considering temperature sampling for MBR decoding in Appendix~\ref{appendix:coro_tempre}.
The goal of the study is to investigate the statistics of the MBR and MAP objectives.

\section{Analysis of MBR decoding}
In this section, we evaluate the performance of MBR decoding (Eq.~\ref{eq:monte_MBR decoding}) compared to the ideal MBR solution (Eq.~\ref{eq:true_MBR decoding}) under various assumptions.

\subsection{Problem Setting}
The optimal MBR decoding output is $\human$ (Eq.~\ref{eq:true_MBR decoding}). However, as mentioned earlier, due to practical limitations, only a Monte Carlo solution $\hmonte$ (Eq.~\ref{eq:monte_MBR decoding}) can be obtained in practice. On the other hand, since this $\hmonte$ is ultimately evaluated under the human distribution $\Phuman$, the following performance difference arises:
\begin{equation}
\mathrm{Regret}_{n}\coloneqq u_h(\human) - u_h(\hmonte).
\end{equation}
We refer to this quantity $\mathrm{Regret}_{n}$ as regret.
The goal of this study is to obtain an upper bound of regret theoretically. Suppose we can show this upper bound on the order of the number of elements in candidate and hypothesis sets. In that case, we can provide a theoretical guarantee for the performance of MBR decoding using the Monte Carlo method.

\subsection{The Analysis of $\mathrm{Regret}_{n}$}
We define the following notation:
\begin{equation}\label{eq:delta}
    \Delta(u_p,u_q,y) \coloneqq u_p(y)- u_q(y).
\end{equation}
This expresses the residual of the utility of $y$ assuming $q$ as the probability distribution when the target probability distribution is $p$. 
Using Eq.~\eqref{eq:delta}, we divide the regret $\mathrm{Regret}_{n}$ into four terms:
\begin{align}
%  \mathrm{Regret}_{n} &= u_h(\human) - u_m(\human)+u_m(\human) - u_m(\hmonte)\\
% % &+u_m(\human) - u_m(\hmonte) 
% &+ u_m(\hmonte) - u_h(\hmonte)\\
% &\leq  \Delta(u_h,u_m,\human) + \Delta(u_m,\widehat{u}_m,\human)\\&+\Delta(\widehat{u}_m,u_m,\hmonte)
% + \Delta(u_m,u_h,\hmonte).
 \mathrm{Regret}_{n} &\leq  \Delta(u_h,u_m,\human) + \Delta(u_m,\widehat{u}_m,\human)\nonumber\\
 &+\Delta(\widehat{u}_m,u_m,\hmonte)+ \Delta(u_m,u_h,\hmonte).\label{equation:regret_n}
% \underbrace{u_m(\human)-\widehat{u}_m(\human)}_{\haterrh}+ \underbrace{\widehat{u}_m(\hmonte) - u_m(\hmonte)}_{\haterrmc}
\end{align}

In the following analysis, we first derive an upper bound for each $\Delta$ in Eq.~\eqref{equation:regret_n}. Then, using the upper bounds derived for each $\Delta$, we derive an upper bound for $\mathrm{Regret}_{n}$.
% \subsection{On the Effect of the Size of Samples}
\paragraph{Analysis of $u_m$ and $\widehat{u}_m$.} 
First, we derive an upper bound for the terms involving $u_m, \widehat{u}_m$.
We consider the following assumption about the utility function.
\begin{mainassumption}[Inner Product Representation of the Utility Function]\label{assumption:utility}
% Based on the setting of this study, 
Let $\ba (y)\in\mathbb{R}^d$ be an embedding for each $y \in \mathcal{Y}$.
We assume that the utility function $u(y, y')$ is given by the inner product of the embeddings, i.e., 
\begin{equation*}
    u(y, y') = \ba (y)^{\top} \ba(y').
\end{equation*}
\end{mainassumption}
There are examples of utility functions that satisfy these properties such as the $F_1$ measure of the BERTScore and the inner product of the embedding functions \cite{bert-score,reimers-gurevych-2019-sentence,SFR-embedding-2}. Note that several state-of-the-art utility functions for machine translation do not satisfy this assumption (e.g., COMET and Metric-X; \citealt{rei-etal-2020-comet, guerreiro-etal-2024-xcomet, juraska-etal-2024-metricx}), since they are trained to do the computation of the utility and the quality estimation at the same time.

By applying Hoeffding’s Inequality (Lemma~\ref{theorem:hoeffding}) and Uniform Concentration Inequality (Lemma~\ref{theorem:uci}), see Appendix~\ref{appendix:lemmas}, along with Assumption~\ref{assumption:utility}, the following lemma about the terms involving $u_m, \widehat{u}_m$ is established.
\begin{mainlemma}[Upper Bound for the terms involving $u_m, \widehat{u}_m$]\label{lemma:heart}
    Under Assumption~\ref{assumption:utility} and assuming $d \geq 4$, the following bound holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
% \begin{align*}
%         &\Delta(u_m,\widehat{u}_m,\human)+\Delta(\widehat{u}_m,u_m,\hmonte) \\
%         &\leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{4}{\delta}} \\
%         &+ \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right).
%     \end{align*}
\begin{align*}
        &\Delta(u_m,\widehat{u}_m,\human)+\Delta(\widehat{u}_m,u_m,\hmonte) \\
        % &\leq 3\sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\left(\sqrt{d \log(\sqrt{d})}\right).
        &\leq 3\sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\sqrt{d \log d}.
    \end{align*}
\end{mainlemma}

The proof can be found in Appendix~\ref{apendix:heart}. Since the dimensions of the embedding models are usually larger than $4$, we assume them in this study and proceed with our analysis under this assumption.\footnote{For readers interested in the case $d < 4$, see Appendix~\ref{apendix:heart}.}
Lemma~\ref{lemma:heart} shows that the upper bound of regret with $u_m, \widehat{u}_m$ terms depends only on the number of samples $n$ and decreases at a rate of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$.  
Notably, this result can also be interpreted as a regret bound, specifically $\mathrm{Regret}_{n}$, under the condition that $\Phuman$ and $\Pmodel$ are identical (Appendix~\ref{appendix:human_equal_model}).

\paragraph{Analysis of $u_h$ and $u_m$.} 
Next, we analyze the $u_h,u_m$ terms involved, but before doing so, we consider the following assumptions.
\begin{mainassumption}[Utility Function Smoothness]\label{assumption:lip}
    For all $y,y^\prime,y^{\prime \prime} \in \mathcal{Y}$, we assume the utility function satisfies the following inequality:
\begin{equation*}
|u(y,y') - u(y,y'')| \leq C(y',y'')
\end{equation*}
where $C \in \mathbb{R}^{\mathcal{Y}\times \mathcal{Y}}$ is a cost function.
\end{mainassumption}
The assumption is not a restrictive assumption. For any utility functions bounded by $[0, \Umax]$, $C(y', y'') = \Umax$ satisfies the assumption. Note also that Assumption~\ref{assumption:utility} entails Assumption~\ref{assumption:lip}.
The assumption is known as the Lipschitz condition \cite{jeffreys1988lipschitz}. 
It claims that the value of the utility function is smooth under the cost function $C$: the difference in the utility between an output $y$ and other outputs $y'$ and $y''$ can be bounded by some ``distance'' $C$ between the $y'$ and $y''$.
Intuitively, if $y'$ and $y''$ are similar, then $C$ wants to be small, and otherwise large. Many of the utility functions are designed to be so by minimizing the prediction error from the human evaluation (e.g., MQM score) \cite{rei-etal-2020-comet,juraska-etal-2024-metricx}.
Under the Assumption~\ref{assumption:lip}, the following lemma holds: %for the analysis of terms containing $u_h,u_m$ terms.
\begin{mainlemma}[Upper Bound for the terms involving $u_h, u_m$]\label{lemma:wd} Under Assumption~\ref{assumption:lip}, the following bound can be derived:
\begin{align*}
        &\Delta(u_h,u_m,\human) +\Delta(u_m,u_h,\hmonte) \\
        &\leq 2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}),
    \end{align*}
    % where $\mathrm{WD}$ is Wasseratein distance.
    where $\mathrm{WD}$ is Wasseratein distance with $C$ being the cost function.
    \end{mainlemma}
The definition of Wasserstein distance \cite{wang2012coupling} is described in Appendix~\ref{appendix:lemmas}. The proof can be found in Appendix~\ref{appendix:wd}.
Lemma~\ref{lemma:wd} implies that minimizing the terms involving  $u_h, u_m$ requires choosing $\Pmodel$ that closely approximates $\Phuman$.

\paragraph{Upper bound of $\mathrm{Regret}_{n}$.}
Using Lemma~\ref{lemma:heart} and Lemma~\ref{lemma:wd}, we can derive an upper bound for $\mathrm{Regret}_{n}$.
\begin{maintheorem}[Regret Bound for MBR decoding]\label{theorem:bound3}
    Under Assumption~\ref{assumption:utility}, Assumption~\ref{assumption:lip}, and assuming $d\geq 4$, the regret upper bound of the MBR decoding holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
 \begin{align*}
    &\mathrm{Regret}_{n} \leq  3 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\sqrt{d \log d}\\&+2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}).
\end{align*}
\end{maintheorem}
Theorem~\ref{theorem:bound3} can be interpreted as follows. 
First, the upper bound decreases with a larger number of samples from $\Pmodel$ with the convergence speed of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$.
This implies that you can reduce the upper bound by $30\%$ by doubling the number of samples $2n$, you are probably to need at least four times more samples $4n$ to reduce the initial error by $50\%$.
The other insight is that the error is inherently limited by the Wasserstein distance between $P_{\mathrm{human}}$ and $P_{\mathrm{model}}$, which means that, as expected, the accuracy of $P_{\mathrm{model}}$ is desirable.
This observation is consistent with the finding that \citet{ohashi-etal-2024-true} indicates that MBR decoding performance is improved when $\Phuman$ and $\Pmodel$ are similar.
\subsection{On the Effect of the Training Dataset Size}
In practice, we cannot compute the exact value of the Wasserstein distance as it requires enumeration over all possible sentences.  
To derive a more digestible bound, we consider the simplest example where $\Pmodel$ is an empirical distribution of $\Phuman$.
Formally, we consider the following assumption:
\begin{mainassumption}[$\Pmodel$ as an Empirical Distribution Sampled the Size of Training Dataset $|D|$ from $\Phuman$.]\label{assumption:human}
    Let $\Pmodel$ be the empirical distribution of $|D|$ samples obtained from $\Phuman$.
% \end{assumption}
$\Pmodel$ has the following expression:
\begin{align*}
    \Pmodel(y) &=  \frac{1}{|D|}\sum_{y^\prime \in D}\mathbb{I}(y= y^\prime). \\
    & D \sim \Phuman(\cdot) 
\end{align*}
where $\mathbb{I}$ is an indicator function.
\end{mainassumption}
Assumption~\ref{assumption:human} is not intended to be an assumption that is directly applicable to real-world scenarios. 
In a real-world scenario, $\Pmodel$ is almost always represented by function approximation models (e.g., neural networks) for text generation tasks. 
They are often pretrained by unsupervised learning using a language model objective, then fine-tuned by supervised learning and preference learning \cite{gpt1,stiennon2020learning,ouyang2022training}.

Given the diversity and complexity of models used in practice, we instead analyze a simple model where it has no function approximation, pertaining, or post-training processes.
Such a simple model is likely to be worse than models used in practice.
Therefore, the bounds we derive from this simple model serve as informal worst-case bounds for the state-of-the-art models.

The size of the training dataset is usually much larger than the number of samples for MBR decoding: $|D| \gg n $. 

\paragraph{Analysis of $u_m$ and $\widehat{u}_m$ with the training dataset size $|D|$.} 
 Under Assumption~\ref{assumption:human}, we derive the analysis on the terms in $u_h,u_m$, using the Hoeffding’s Inequality (Lemma~\ref{theorem:hoeffding}, see Appendix~\ref{appendix:lemmas}), we can get the following the upper bound.
 \begin{mainlemma}[Upper Bound for the terms involving $u_h, u_m$ with the Size of Training Dataset $|D|$]\label{lemma:black}
    Under Assumption ~\ref{assumption:human}, the following bound holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
    % \begin{align*}
    %     &\Delta(u_h,u_m,\human)+ \Delta(u_m,u_h,\hmonte)\\
    %     &\leq 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{4}{\delta}}.
    % \end{align*}
    \begin{align*}
        &\Delta(u_h,u_m,\human)+ \Delta(u_m,u_h,\hmonte)\\
        &\leq 3\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
    \end{align*}
\end{mainlemma}
The proof can be found in Appendix~\ref{appendix:black}.
This shows that the upper bounds for the $u_h,u_m$ terms vary only with the size of the training dataset $|D|$ and that the upper bound decays at a rate of $\mathcal{O}\left(|D|^{-\frac{1}{2}}\right)$ with its size.

Under Assumption~\ref{assumption:human}, regret depends on both samples $n$ and $|D|$. For clarity, we define a regret under Assumption~\ref{assumption:human} as follows:
\begin{equation}
    \mathrm{Regret}_{n,D}\coloneqq u_h(\human) - u_h(\hmonte).
\end{equation}
\paragraph{Upper bound of $\mathrm{Regret}_{n,D}$.}
We can immediately derive the upper bound for $\mathrm{Regret}_{n,D}$ from Lemma~\ref{lemma:heart} and Lemma~\ref{lemma:black} as follows:
\begin{maintheorem}[Regret Bound for MBR decoding with the Size of Training Dataset $|D|$]\label{theorem:bound}
    Under Assumption~\ref{assumption:utility}, Assumption~\ref{assumption:human}, and assuming $d\geq 4$, the regret upper bound of the MBR decoding holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
    % \begin{align*}
    %    &\mathrm{Regret}_{n,D}  \leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} \\
    %     &+ \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) \\
    %     &+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}.
    % \end{align*}
    \begin{align*}
       \mathrm{Regret}_{n,D}  &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} \\
        &+ \frac{36 }{n}\sqrt{d \log  d}. 
        % &+ 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
    \end{align*}
\end{maintheorem}
Theorem~\ref{theorem:bound} shows that MBR decoding approaches the optimal output with a convergence rate related to the size of the reference hypothesis set $n$ and the size of the training dataset $|D|$, suggesting why MBR decoding has good experimental performance. This implies that sample and dataset sizes are significant for MBR decoding.

\subsection{Extended Analysis of MBR Decoding}
\paragraph{Expected regret bounds.}
So far, we have found that we can obtain upper bounds that occur with high probability, and from these upper bounds, we can immediately determine the expected regret upper bound for Theorem~\ref{theorem:bound3} and Theorem~\ref{theorem:bound}.
\begin{maincoro}[Expected Regret Upper Bound of MBR decoding]\label{propotion:mbr}
% The expected regret $R \in\{\mathrm{Regret}_{n}, \mathrm{Regret}_{n,D}\}$ is bounded as follows:
The expected regret upper bounds $\mathrm{Regret}_{n},\mathrm{Regret}_{n,D}$ are bounded as follows for any $\delta\in(0,1)$ under assuming $d\geq4$:
% \begin{align*}
%     \E\left[\mathrm{Regret}_{n}  \right] &= (1-\delta)\epsilon +\delta \Umax \\
%     &\leq \epsilon + \delta \Umax.
% \end{align*}
\begin{align*}
    \E\left[\mathrm{Regret}_{n}  \right] &\leq 3 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\sqrt{d \log d}\\&+2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}) + \delta 
    % \E\left[\mathrm{Regret}_{n,D}  \right] &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\frac{36 }{n}\sqrt{d \log  d} \\
    %     &+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}  + \delta 
\end{align*}
\vspace{-0.5cm}
\begin{align*}
    \E\left[\mathrm{Regret}_{n,D}  \right] &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\frac{36}{n}\sqrt{d \log  d} \\
        &+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}  + \delta 
\end{align*}
% \begin{align*}
%     \E\left[\mathrm{Regret}_{n,D}  \right] &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} \\
%         &+ \frac{36 }{n}\sqrt{d \log  d} + \delta 
% \end{align*}
\end{maincoro}
The proof is in Appendix~\ref{appendix:reg_exp}. Corollary~\ref{propotion:mbr} can be used to estimate how large the regret will be, on average.

\paragraph{On the effect of errors in the utility function.}
In the real world, we cannot always have access to the true utility function $u$, instead, we assume that the proxy utility function is only available $u'$, and we explain the bound of the difference in the utility function. We focus exclusively on the conditions outlined in Theorem~\ref{theorem:bound}.

We define the following expression:
\begin{align*}
    u'(y) &= \frac{1}{|\Yn|} \sum_{y' \in \Yn} u'(y, y').\\
    y' &= \argmax_{y \in \mathcal{\Yn}} u'(y). 
\end{align*}
We want to find the upper bound of $u_h(\human) - u_h(y')$.  
Our new objective function in this paragraph is defined as:
\begin{equation}
    \mathrm{Regret}^{u}_{n,D} \coloneqq u_h(\human) - u_h(y').
\end{equation}
Under Assumption~\ref{assumption:utility} and Assumption~\ref{assumption:human}, an upper bound of $\mathrm{Regret}^{u}_{n,D}$ is derived using the Hoeffding's inequality (Lemma~\ref{theorem:hoeffding}). Let $\alpha_{err} \coloneqq \max_{y, y' \in \Yn}|| \ba(y) - \ba'(y')||$.

\begin{maincoro}[Regret Bound for MBR decoding with utility function error]\label{coro:utility}
    Under Assumption~\ref{assumption:utility} and Assumption~\ref{assumption:human}, the regret upper bound of the MBR decoding with utility function error holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
%     \begin{align*}
%     &\mathrm{Regret}^{u}_{n,D} \leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} + 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}\\ &+ \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
% \end{align*}
\begin{align*}
    \mathrm{Regret}^{u}_{n,D} &\leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} + 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}\\ &+ 2d  \alpha_{err}.
\end{align*}
\end{maincoro}
The proof can be found in Appendix~\ref{appendix:utility}.
In Corollary~\ref{coro:utility}, the upper bound decreases as the number of samples $n$ and the size of training dataset $|D|$ increases.  
However, it does not ultimately converge to zero, as the term $\alpha_{err}$ remains.

\section{Analysis of MAP Decoding}
In this section,  we derive the regret of MAP decoding between the optimal value and the Monte Carlo estimated value, expressed as $\Phuman(\maphuman) - \Phuman(\hm)$. 

We define the regret of the MAP decoding as follows:
% $\mathrm{Regret}^\mathrm{MAP}_n = \Phuman(\maphuman) - \Phuman(\hm)$.
\begin{equation}
\mathrm{Regret}^\mathrm{MAP}_n = \Phuman(\maphuman) - \Phuman(\hm). 
\end{equation}
We refer to $\mathrm{Regret}^\mathrm{MAP}_n$ as MAP regret.
Under the conditions of Theorem~\ref{theorem:bound3}, the upper bound of $\mathrm{Regret}^\mathrm{MAP}_n$ is given by the following result using Dvoretzky–Kiefer–Wolfowitz inequality (Lemma~\ref{theorem:DKW}, see in Appendix~\ref{appendix:lemmas}).
\begin{maintheorem}[Regret Bound for MAP decoding]\label{theorem:map_bound2}
% Under the conditions of Theorem~\ref{theorem:bound3}, the map regret upper bound of the MAP decoding holds with probability at least $1 - \frac{\delta}{2}$:
% \begin{align*}
%     \mathrm{Regret}^\mathrm{MAP}_n &\leq 4\sqrt{\frac{1}{2n}\log\frac{8}{\delta}} \\
%     &\quad + 2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}).
% \end{align*}
% \end{maintheorem}
% \ichihara{$\delta$}
Under the conditions of Theorem~\ref{theorem:bound3}, the MAP regret upper bound of the MAP decoding holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
% \begin{align*}
%     \mathrm{Regret}^\mathrm{MAP}_n &\leq 4\sqrt{\frac{1}{2n}\log\frac{4}{\delta}} \\
%     &\quad + 2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}).
% \end{align*}
% \begin{align*}
%     \mathrm{Regret}^\mathrm{MAP}_n &\leq 6\sqrt{\frac{1}{n}\log\frac{1}{\delta}} \\
%     & + 2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}).
% \end{align*}
\begin{align*}
    \mathrm{Regret}^{\mathrm{MAP}}_n &\leq 6\sqrt{\frac{1}{n}\log\frac{1}{\delta}} \\
    & + 2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}).
\end{align*}
\end{maintheorem}

Furthermore, under the conditions of Theorem~\ref{theorem:bound}, MAP regret depends on the number of samples $n$ and the size of the training dataset $|D|$.

Our new objective formulation is defined as:
\begin{equation}
\mathrm{Regret}^\mathrm{MAP}_{n,D} = \Phuman(\maphuman) - \Phuman(\hm)
\end{equation}
The upper bound of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$ is also immediately obtained by Lemma~\ref{theorem:DKW} as follows.
\begin{maintheorem}[Regret Bound for MAP decoding with the Size of Training Dataset $|D|$]\label{theorem:map_bound3}
Under the conditions of Theorem~\ref{theorem:bound}, the MAP regret upper bound of the MAP decoding holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
% \begin{align*}
%     \mathrm{Regret}^\mathrm{MAP}_{n,D} &\leq 4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right).
% \end{align*}
% \begin{align*}
%     \mathrm{Regret}^\mathrm{MAP}_{n,D} &\leq 4\sqrt{\frac{1}{2n}\log\frac{8}{\delta}}+4\sqrt{\frac{1}{2|D|}\log\frac{8}{\delta}}.
% \end{align*}
\begin{align*}
    \mathrm{Regret}^\mathrm{MAP}_{n,D} &\leq 8\sqrt{\frac{1}{n}\log\frac{1}{\delta}}+8\sqrt{\frac{1}{|D|}\log\frac{1}{\delta}}.
\end{align*}
\end{maintheorem}
The proof is in Appendix~\ref{appendix:map_true}. Note that Theorem~\ref{theorem:map_bound2} and Theorem~\ref{theorem:map_bound3} decrease in the same order Theorem~\ref{theorem:bound3} and Theorem~\ref{theorem:bound} respectively.
In other words, if we compare the difference between these bounds more clearly, we focus on the constant term.

\section{Performance Comparison}\label{section:map_mbr}
So far, we have analyzed MBR decoding and MAP decoding independently.  
In this section, we compare the performance of MBR decoding and MAP decoding within the same framework, in terms of the upper bound and we focus exclusively on the conditions outlined in Theorem~\ref{theorem:bound}, Theorem~\ref{theorem:map_bound3}.

\paragraph{Difference between MBR and MAP Decoding target values.}
First, we aim to analyze the difference $u_h(\human) - u_h(\hm) $, where $\human$ is the optimal output and $\hm$ is a suboptimal output. We can analyze this error bound to see how the optimal solution in MAP decoding behaves in an ideal MBR decoding environment.
\begin{mainobservation}[Error between $\human$ and $\hm$ with $u_h$]\label{obs:1}
    Error bound between $\human$ and $\hm$ with $u_h$ satisfies for any $\delta\in\left(0,\frac{2}{5}\right)$, with probability at least $1-\frac{5}{2}\delta$:
% \begin{align*}
%      &u_h(\human) -u_h(\hm) \leq  u_m(\hmonte) -u_m(\hm) \\
%      &+ \mathcal{O}\left(\max \left(\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{D}}\right)\right).
% \end{align*}
\begin{align*}
     &u_h(\human) -u_h(\hm) \leq  u_m(\hmonte) -u_m(\hm) \\
     &+ \mathcal{O}\left(\max \left(n^{-\frac{1}{2}}, |D|^{-\frac{1}{2}}\right)\right).
\end{align*}
\end{mainobservation}
The detail is in Appendix~\ref{appendix:observation1}.
This observation confirms that MAP decoding and MBR decoding theoretically have different objectives under certain conditions. 
We provide the analysis of the MAP regret $\mathrm{Regret}^\mathrm{MAP}_{n,D}$ of the two decoding algorithms in Appendix~\ref{appendix:observation1}.

\paragraph{Convergence speed of upper bound of $\mathrm{Regret}_{n,D}$ and $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.}
Next, we compare the upper bounds of the convergence rate between MBR decoding and MAP decoding presented in this study.
\begin{mainobservation}[Comparison of the Convergence Speed]\label{obs:2}
We compare the upper bounds of $\mathrm{Regret}_{n,D}$ and $\mathrm{Regret}^\mathrm{MAP}_{n,D}$ under three different scenarios:
\paragraph{1. $n \to \infty$ and $|D|$ is finite.}
The upper bound of $\mathrm{Regret}_{n,D}$ is strictly smaller than the upper bound of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.

\paragraph{2. $D \to \infty$ and $n$ is finite.}
For number of samples $n$ and utility $d$ such that the following inequality holds, the upper bound of $\mathrm{Regret}_{n,D}$ is smaller than the upper bound of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.
 % \begin{align*}
 %        \sqrt{\frac{n}{72}\log{\frac{8}{\delta}}} \geq \sqrt{d \log (2 \sqrt{d})} + 2 \sqrt{d}.
 %    \end{align*}
 \begin{align*}
        \frac{1}{9}\sqrt{n\log{\frac{1}{\delta}}} \geq \sqrt{d \log d}.
    \end{align*}
    
\paragraph{3. Both $D$ and $n$ are finite.}
For the number of samples $n$, utility $d$, and the size of training dataset $|D|$ such that the following inequality holds, the upper bound of $\mathrm{Regret}_{n,D}$ is smaller than the upper bound of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.
\begin{align*}
    % n\sqrt{\frac{1}{72}\log\frac{8}{\delta}}\left(\frac{1}{\sqrt{n}}+ \frac{1}{\sqrt{D}}\right)&\geq \sqrt{d \log (2 \sqrt{d})} \\
    % & + 2 \sqrt{d}.
    \frac{n}{9}\left(\sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}\right) &\geq \sqrt{d \log d}.
\end{align*}
\end{mainobservation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/samples_n_D.pdf}
    \caption{Conceptual visualization of Observation~\ref{obs:2}. The convergence rates of the upper bound of $\mathrm{Regret}_{n, D}$ and $\mathrm{Regret}_{n, D}^{\mathrm{MAP}}$ with the number of samples $n$ and training dataset size $|D|$ are compared. For $n$ and $|D|$ on the right side of this line plot, it means that the upper bound of $\mathrm{Regret}_{n, D}$ is smaller.}
    \label{fig:compare}
\end{figure}
% \vspace{-4mm}
The proofs are given in Appendix~\ref{appendix:observation2} and the result of numerical experiments comparing the rate of convergence of these upper bounds depending on the number of samples $n$ is shown in Appendix~\ref{appendix:observation_exp}. As can be seen by comparing $\mathrm{Regret}_{n, D}$ and $\mathrm{Regret}_{n, D}^{\mathrm{MAP}}$ in cases 2 and 3 of Observation~\ref{obs:2}, as the number of samples $n$ increases, the upper bound on $\mathrm{Regret}_{n, D}$ converges more faster (Fig.~\ref{fig:compare}). 

The analysis shows that, for any model, there exists a large enough $n$ such that the upper bound of the regret of MBR decoding is smaller than that of MAP decoding. 
This observation may help explain why the empirical performance of MBR decoding can exceed that of MAP decoding.
% This may be the reason why MBR decoding is better than MAP decoding.

\section{Numerical Simulation}\label{sec:experiments}
% In this section, we empirically evaluate MBR decoding's upper bounds.
In this section, we computationally evaluate the upper bounds of $\mathrm{Regret}_{n}$ and $\mathrm{Regret}_{n,D}$.  
It is important to emphasize that the experiments conducted in this paper are not intended to show the tightness of the results in the practical NLP tasks. Rather, they are intended to provide a visual representation of the theoretical results presented in this paper.

For the performance of MBR decoding in real-world NLP tasks, we refer to previous work \cite{freitag-etal-2023-epsilon,bertsch-etal-2023-mbr,heineman-etal-2024-improving,wu2025better}.

We consider $\mathcal{Y}=10{,}000$ hypotheses $y_i$ ($i=1,\dots,\mathcal{H}$) with the dataset size of $|D|$ and the number of samples $n$ model samples to study regret and bound behavior. We test $\delta\in\{0.01,0.1\}$, and set $d=4$. 
The details of the experimental setup are given in Appendix~\ref{appendix:exp}.
\subsection{Results}
 \begin{figure}
             \centering
             \includegraphics[width=\linewidth]{img/wasserstein_distance.pdf}
             % \vspace{-4mm}
             \caption{The upper bound ($\delta=\{\color{red}0.01,\color{blue}0.1\color{black}\}$) of the $\mathrm{Regret}_{n}$ derived by Theorem~\ref{theorem:bound3} and the value of \color{teal} $\mathrm{Regret}_{n}$\color{black} in the simulation. The number of samples $n$ is fixed to $500$ and the training dataset size is $|D|=[0,1000]$}
             \label{fig:wasserstein_distance}
         \end{figure}
\begin{figure}[t]
     % \centering
         % \centering
         \includegraphics[width=0.48\textwidth]{img/regret_bound_fixed_D.pdf}
         % \vspace{-4mm}
         \caption{The upper bound ($\delta=\{\color{red}0.01,\color{blue}0.1\color{black}\}$) of the $\mathrm{Regret}_{n,D}$ derived by Theorem~\ref{theorem:bound} and the value of \color{teal} $\mathrm{Regret}_{n,D}$\color{black} in the simulation. The training dataset size $|D|$ is fixed to $5000$ and the number of samples for MBR decoding is $n=[0,500]$.}
         \label{fig:fix_d}
         \end{figure}
         
         \begin{figure}
             % \centering
             \includegraphics[width=\linewidth]{img/regret_bound_fixed_n.pdf}
             % \vspace{-4mm}
             \caption{The upper bound ($\delta=\{\color{red}0.01,\color{blue}0.1\color{black}\}$) of the $\mathrm{Regret}_{n,D}$ derived by Theorem~\ref{theorem:bound} and the value of \color{teal} $\mathrm{Regret}_{n,D}$ \color{black} in simulation. The number of samples $n$ is fixed to $500$ and the training dataset size is $|D|=[0,10000]$}
             \label{fig:fix_n}
         \end{figure}


% Fig.~\ref{fig:wasserstein_distance} shows that the upper bound of $\mathrm{Regret}_{n}$ is a tight bound compared to the actual $\mathrm{Regret}_{n}$. 
Fig.~\ref{fig:wasserstein_distance} clearly demonstrates that our theoretical upper bound on $\mathrm{Regret}_{n}$ is tight when compared to the actual regret observed. This close correspondence indicates that the assumptions and inequalities used in deriving the bound are well-justified, providing evidence in the numerical experiment. 

The results of Fig.~\ref{fig:fix_d} and Fig.~\ref{fig:fix_n} show that the obtained upper bound converges to $\mathrm{Regret}_{n,D}$ as the number of samples increases. 
% The results of Fig.~\ref{fig:fix_d} and Fig.~\ref{fig:fix_n} show that the obtained upper bound converges to $\mathrm{Regret}_{n,D}$the true performance difference as the number of samples increases. 
This behavior suggests the theoretical validity of the bound indicating that the looseness of the upper bound is gradually eliminated as the number of samples increases, improving the ability to accurately capture the true performance difference accurately. 

\section{Conclusions}
% This paper presents a theoretical analysis of MBR decoding and shows that, under reasonable assumptions, it converges with high probability to the optimal solution at a rate of $\mathcal{O}\left(n^{-1 / 2}\right)$.
% In addition, we compare MBR and MAP decoding in terms of performance difference and convergence speed to the optimal solution. 
% As a result, we confirm that MAP decoding and MBR decoding have theoretically different objectives, and MBR decoding is more efficient than MAP decoding in approaching the optimal output under certain conditions. 

% In this work, we have provided a theoretical analysis of MBR decoding. Our analysis demonstrates that, under a set of reasonable assumptions, MBR decoding converges to the optimal solution with high probability at a rate of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$ relative to the size $n$ of the reference hypothesis set even when the overall language space $\mathcal{Y}$ is exceedingly large. This convergence rate  offers a formal justification for the impressive empirical performance observed in several prior studies,.

% Furthermore, our comparative evaluation with MAP decoding reveals that the performance gap of MAP decoding decreases at a slower rate under certain situation. This finding suggests that, in scenarios where the reference set is limited, MBR decoding is more efficient in approximating the optimal solution. 

% Looking ahead, future work may focus on practical experiment and analyses of the theory utility functions, and sampling methods.
% This work presents a theoretical analysis of MBR decoding, showing that under reasonable assumptions it converges to the optimal solution with high probability at a rate of $\mathcal{O}\left(n^{-1 / 2}\right)$ relative to the size $n$ of the reference hypothesis set even when the overall language space $\mathcal{Y}$ is vast. This convergence rate underpins the strong empirical performance reported in previous studies. Additionally, our comparison with MAP decoding indicates that its performance gap narrows more slowly in some cases, suggesting that MBR decoding is more efficient when the reference set is limited. 
This paper presents a theoretical analysis of MBR decoding and shows that, under reasonable assumptions, it converges with high probability to the optimal solution at a rate of $\mathcal{O}\left(n^{-\frac{1}{2}}\right)$, even when the total language space $\mathcal{Y}$ is large. 
% This convergence rate supports the strong empirical performance reported in previous studies. 
% In addition, Our comparison shows MBR decoding is more efficient than MAP decoding under the certain conditions. 
In addition, we compare MBR and MAP decoding about the performance difference and the convergence speed to the optimal solution. 
As a result, we confirm MAP decoding and MBR decoding theoretically have different objectives, and from the upper bound, MBR decoding is more efficient than MAP decoding in approaching the optimal output under certain conditions. 
% Future research may focus on confirming the theorem in practical experiments and further analyses of theoretical utility functions and sampling methods.



\clearpage

\section{Limitations}
\label{sec:limitations}

This study provides the first theoretical bounds on MBR decoding. As it is one of the first analyses on MBR decoding, it has several limitations, particularly regarding its alignment with practical implementations.

\paragraph{Assumptions.}
Our analysis assumes that the set of candidate hypotheses $\mathcal{H}$ is identical to the set of reference outputs $\mathcal{Y}$. However, in practice, using a small number of high-quality but biased candidates alongside a larger set of unbiased references has been found to be more effective.

We have considered three assumptions for the analysis. The assumptions do not cover all the situations of text generation applications. For example, the state-of-the-art utility functions for machine translation (COMET and Metric-X; \citealt{rei-etal-2020-comet,guerreiro-etal-2024-xcomet,juraska-etal-2024-metricx}) are not linear function (Assumption~\ref{assumption:utility}).

In practice, the models are represented by a neural network, and they are often pretrained using unsupervised learning before supervised fine-tuning. This point is not considered in Assumption~\ref{assumption:human}.

\paragraph{Aspects not considered.}
The analysis does not account for the role of neural networks. In particular, it is known that solutions corresponding to flat minima tend to generalize better than those with sharp minima \cite{dinh2017sharp}. Understanding the role of neural networks for MBR decoding is future work.

We analyze a model that predicts sequences, but practical implementations typically use autoregressive language models \cite{lin-etal-2021-limitations}. Incorporating the autoregressive assumption may lead to improved theoretical bounds.

The study considers only random sampling and temperature sampling. However, other strategies, such as epsilon sampling \cite{hewitt-etal-2022-truncation} and beam search (which is commonly used for MAP decoding), are not analyzed.

Our analysis does not frame the problem as an NLP task. Incorporating domain-specific characteristics could lead to tighter bounds.
This study is purely theoretical and does not include empirical experiments to validate the results on real-world NLP tasks. Instead, we rely on prior experimental findings \cite{freitag-etal-2023-epsilon,bertsch-etal-2023-mbr,suzgun-etal-2023-follow} for providing empirical support for our theoretical conclusions.

Another key limitation is that the bounds derived in this study are not proven to be tight, leaving room for refinement. Furthermore, to measure how tight the upper bounds is, we also need to derive the lower bound in MBR decoding.

Lastly, while our study focuses on sample complexity, practical implementations of MBR decoding are often constrained by computational complexity \cite{cheng-vlachos-2023-faster,vamvas2024lineartime}. Combining our sampling complexity result with the existing computational complexity bounds \cite{jinnai2024hyperparameterfree} is future work.

\paragraph{Summary.}
This work provides fundamental theoretical bounds for MBR decoding. However, there remain avenues for improvement, including empirical validation, refinement of theoretical bounds, and comparative analysis with alternative decoding algorithms.


\section{Ethics Statements}
\label{sec:ethics}
We do not foresee any ethical concerns regarding the analysis of the paper.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{anthology,ms}
\bibliography{anthology,ms}
\newpage
\appendix
\onecolumn

% \section{Related Works}
% % \section{Related Work}
% % \paragraph{Alignment Theory.}
% % \citet{beirami2024theoretical} conducted an analysis comparing the policies selected by Best-of-Sampling with the base policies used for sample generation.
% % \cite{u-maji-2006-computational} demonstrates that finding the optimal translation in statistical machine translation (IBM model) is NP-hard when using general probabilistic models (some IBM models) and grammars, revealing significant inherent computational challenges.
% \paragraph{model distribution.}
% % \citet{smith-eisner-2006-minimum} proposed annealed minimum risk training (MRT), which minimizes expected loss by defining a probability distribution over hypotheses and gradually sharpening it to focus on the 1-best hypothesis. Their experiments showed significant improvements in BLEU for machine translation and accuracy in dependency parsing, highlighting the advantages of risk-based optimization over traditional methods. They also stated "We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder."

% \citet{jinnai2023modelbased} proposed model-based MBR (MBMBR), which uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. Analytical and empirical results show that MBMBR outperforms traditional MBR in various tasks, using both encoder-decoder architectures and language models.

% % In recent work, \citet{yan-etal-2024-dc} proposed Distributional Cooling MBR (DC-MBR), which adjusts the entropy of output distributions by reducing the softmax temperature, address inconsistencies between token-level and sequence-level distributions that lead to so-called autoregressive over-smoothing. This approach bridges the gap between label smoothing and MBR decoding, with theoretical support and extensive experimental validation demonstrating its effectiveness on various NMT benchmarks.


% \paragraph{Experimental findings in MBR decoding.}
% % The performance of MBR decoding increases with a larger number of pseudo-references.
% % However, there is no analytical explanation for why it scales so \cite{}.


% Prior works \cite{freitag-etal-2022-high,fernandes-etal-2022-quality} show that the performance of the MBR decoding depends on the selection of the utility function. 
% Experiments combining MBR decoding with neural reference-based metrics, such as BLEURT, demonstrate significant improvements in human evaluations. 
% % Notably, these translations differ from traditional beam search outputs, exhibiting lower model likelihoods but higher quality as measured by advanced metrics.

% \paragraph{Analysis of MBR decoding.}
% By focusing on the theoretical and empirical underpinnings of MBR decoding, recent work sheds new light on its foundational principles and performance determinants.
% \citet{kamigaito2024theoretical} conduct on the intricate relationship between bias and diversity in MBR decoding. Their bias-diversity decomposition framework theoretically explains the trade-offs observed in empirical studies and highlights the importance of enhancing diversity to boost performance.
\section{Related Works}
% \citet{smith-eisner-2006-minimum} proposed annealed minimum risk training (MRT), which minimizes expected loss by defining a probability distribution over hypotheses and gradually sharpening it to focus on the 1-best hypothesis. Their experiments showed significant improvements in BLEU for machine translation and accuracy in dependency parsing, highlighting the advantages of risk-based optimization over traditional methods. They also stated "We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder."

% In recent work, \citet{yan-etal-2024-dc} proposed Distributional Cooling MBR (DC-MBR), which adjusts the entropy of output distributions by reducing the softmax temperature, address inconsistencies between token-level and sequence-level distributions that lead to so-called autoregressive over-smoothing. This approach bridges the gap between label smoothing and MBR decoding, with theoretical support and extensive experimental validation demonstrating its effectiveness on various NMT benchmarks.


\paragraph{Experimental findings in MBR decoding.}
Many studies have reported that the performance of MBR decoding increases with a larger number of samples \cite{eikema-aziz-2022-sampling,freitag-etal-2022-high}.
% However, there is no analytical explanation for why it scales so \cite{}.
Prior works \cite{freitag-etal-2022-high,fernandes-etal-2022-quality} show that the performance of the MBR decoding depends on the selection of the utility function. 
Experiments combining MBR decoding with neural reference-based metrics, such as BLEURT, demonstrate significant improvements in human evaluations. 
In recent work, \citet{yan-etal-2024-dc} propose Distributional Cooling MBR, this approach bridges the gap between label smoothing and MBR decoding, with extensive experimental validation demonstrating its effectiveness on various NMT benchmarks and \citet{wu2025better} shows that leveraging reference-based LLM judges with MBR decoding improves the output quality of instruction-following LLMs compared to greedy decoding, best-of-N approaches.
% Notably, these translations differ from traditional beam search outputs, exhibiting lower model likelihoods but higher quality as measured by advanced metrics.

\paragraph{Analysis of MBR decoding.}
% By focusing on the theoretical and empirical underpinnings of MBR decoding, recent work sheds new light on its foundational principles and performance determinants.
\citet{kamigaito2024theoretical} conduct on the intricate relationship between bias and diversity in MBR decoding. Their bias-diversity decomposition framework theoretically explains the trade-offs observed in empirical studies.
\section{Useful Lemmas and Definition}
\label{appendix:lemmas}
In this section, we present the concentration inequality used in the paper.
The following inequalities represent a uniform concentration inequality.

\begin{lemma}[Hoeffding's inequality; Corollary 1.1 in \citealt{bach2024learning}]\label{theorem:hoeffding}
$\{X_i\}_{i=1}^n \in [0,b]$ being i.i.d. samples drawn from same distribution.
    \begin{equation*}
    \operatorname{Pr}\left(\left|\E\left[X\right] - \frac{1}{n}\sum_{i=1}^n X_i\right| \geq \epsilon\right) 
    \leq 2\exp\left(-\frac{2 n\epsilon^2}{b^2}\right).
\end{equation*}
\end{lemma}
The following inequalities represent a uniform concentration inequality.
\begin{lemma}[Uniform Concentration Inequality; Theorem 4.10 in \citealt{wainwright2019high}]\label{theorem:uci}
$\mathcal{F}$ is a class of functions $f: \mathcal{X} \rightarrow [0,b]$.
\begin{align*}
    \operatorname{Pr}\left(\left\|\mathbb{P}_n - \mathbb{P}\right\|_{\mathcal{F}} \geq 2 \mathcal{R}_n(\mathcal{F}) + \epsilon\right) 
    &\leq  \exp\left(-\frac{2n \epsilon^2}{b^2}\right).
\end{align*}
\end{lemma}

where $\left\|\mathbb{P}_n - \mathbb{P}\right\|_{\mathcal{F}} = \sup_{f \in \mathcal{F}} \left|\mathbb{P}_n f - \mathbb{P} f\right|$, $\mathbb{P}_{n} f = \frac{1}{n} \sum_{i=1}^n f(X_i)$ and $\mathbb{P} f = \E[f(X)]$, with $X$ and $\{X_i\}_{i=1}^n$ being i.i.d. samples drawn from $\mathbb{P}$, $\mathcal{R}_n:\left(\mathcal{F},\left\{X_i\right\}_{i=1}^n\right) \rightarrow \mathbb{R}$.


$\mathcal{R}_n(\mathcal{F})$ represents the Rademacher complexity of the function class $\mathcal{F}$ (Definition 3.1 \citep{mohri2018foundations}). Rademacher complexity is a measure of model complexity, indicating how well a function class can fit random noise. It provides a uniform bound on the deviation between the empirical and true expectations across all functions in $\mathcal{F}$, serving as a key tool for analyzing generalization error in statistical learning theory.

\begin{lemma}[Dvoretzky–Kiefer–Wolfowitz inequality; \citealt{massart1990tight}]\label{theorem:DKW}
\begin{equation*}
\operatorname{Pr}\left(\sup _{x \in \mathbb{R}}\left|F_n(x)-F(x)\right|>\varepsilon\right) \leq 2 \exp(-2 n \varepsilon^2 ).
\end{equation*}
\end{lemma}

Given a natural number $n$, let $X_1, X_2, \cdots, X_n$ be real-valued independent and identically distributed random variables with cumulative distribution function $F(\cdot)$. Let $F_n$ denote the associated empirical distribution function defined by $
F_n(x)=\frac{1}{n} \sum_{i=1}^n \mathbf{1}_{\left\{X_i \leq x\right\}}$

\begin{definition}[Wassertstein Distance]
    
The Wassertstein Distance ($\mathrm{WD}$) \citep{wang2012coupling} is defined as:
\begin{equation}\label{eq:wd}
\mathrm{WD}(\nu , \mu) = \inf_{\gamma \in \Gamma(\nu, \mu)} \sum_{(i, j) \in N \times N} \gamma_{ij} \, C_{ij},
\end{equation}
\end{definition}
where $N$: the total number of samples, consisting of the set $\{ y_1, y_2, \dots, y_N \}$, $\nu, \mu \in \Delta_N$: probability measure on the aforementioned sets ($\nu_i, \mu_i$ refer to the probability value $\nu(y_i), \mu(y_i)$), $C$: $N\times N \rightarrow \mathbb{R}$ a cost function measuring the distance between two outputs (e.g. $C_{ij}$ refers to the amount to
be transported from place $y_i$ to palace $y_j$), and $\Gamma(\nu, \mu)$ denotes the set of all joint distributions $\gamma$ whose marginals are $\nu$ and $\mu$. The constraints on $\gamma$ are given by:
\begin{equation*}
\begin{aligned}
\sum_{j \in n} \gamma_{ij} &= \nu_i, \quad \forall i \in n, \\
\sum_{i \in n} \gamma_{ij} &= \mu_j, \quad \forall j \in n, \\
\gamma_{ij} &\geq 0, \quad \forall i,j \in n.
\end{aligned}
\end{equation*}
The $\mathrm{WD}$, also known as the Earth Mover's Distance (EMD), is a metric used to quantify the dissimilarity between two probability distributions.
Unfortunately, computing WD between two probability distributions over $\mathcal{Y}$ exactly is generally intractable, as it requires an enumeration over $\mathcal{Y}$.
Still, WD can be approximated by using empirical distributions with a finite number of samples with the convergence rate of $\mathcal{O}(n^{-\frac{1}{d}})$ \cite{peyre2020computational}.
%, which is computationally infeasible for language models.
% \section{Proof of Lemma~\ref{lemma:heart}}
% \label{apendix:heart}
% We start by analyzing the $u_m(\human) - u_m(\hmonte)$. We decompose the $\diffmodel$ as follows.
%       \begin{align*}
%         &u_m(\human)-\widehat{u}_m(\human) + \widehat{u}_m(\hmonte) - u_m(\hmonte) +\underbrace{\widehat{u}_m(\human) - \widehat{u}_m(\hmonte)}_{\leq 0}\\
%         &\leq \Delta(u_m,\widehat{u}_m,\human)+\Delta(\widehat{u}_m,u_m,\hmonte)
%     \end{align*}


% We can express $ \Delta(u_m,\widehat{u}_m,\human)$ using the following formulation derived from Lemma~\ref{theorem:hoeffding}.

% \begin{equation*}
%     \operatorname{Pr}\left(\left| \Delta(u_m,\widehat{u}_m,\human)\right| \leq \epsilon\right) 
%     \leq 1- 2\exp\left(-\frac{2 n\epsilon^2}{\Umax^2}\right) = {1- \frac{\delta}{4}}.
% \end{equation*}
% $ \Delta(u_m,\widehat{u}_m,\human)$ holds the following value with probability at least $1-\frac{\delta}{4}$.
% \begin{align*}
%     \Delta(u_m,\widehat{u}_m,\human)  \leq \Umax\sqrt{\frac{1}{2n}\log \left(\frac{8}{\delta}\right)} .
% \end{align*}

% Next, we analyze $\Delta(\widehat{u}_m,u_m,\hmonte)$. However, Lemma~\ref{theorem:hoeffding} cannot be directly applied since $\Delta(\widehat{u}_m,u_m,\hmonte)$ depends on $\widehat{u}_m$.  
% To address this dependency, we instead utilize Lemma~\ref{theorem:uci}.

% We can express $\Delta(\widehat{u}_m,u_m,\hmonte)$ from Lemma~\ref{theorem:uci}.

% \begin{align*}
%     \operatorname{Pr}\left(\max_y\left|\widehat{u}_m(y) - u_m(y)\right| \leq 2 \mathcal{R}_n(\mathcal{F}) + \epsilon\right) 
%     &\leq 1- \exp\left(-\frac{2n \epsilon^2}{\Umax^2}\right) = 1-\frac{\delta}{4}.
% \end{align*}
% We can express $\haterrmc$ with probability at least $1- \frac{\delta}{4}$.
% \begin{align*}
%     \widehat{u}_m(\hmonte) - u_m(\hmonte)\leq \max_y\left|\widehat{u}_m(y) - u_m(y)\right| \leq2 \mathcal{R}_n(\mathcal{F}) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)}.
% \end{align*}


% From Section 27.2 \cite{shalev2014understanding}, the following upper bound on the Rademacher complexity $\mathcal{R}_n(\mathcal{F})$ is obtained under Assumption~\ref{assumption:utility}:
% \begin{align*}
%     2 \mathcal{R}_n(\mathcal{F}) \leq \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right)
% \end{align*}

% \begin{align*}
%     \widehat{u}_m(\hmonte) - u_m(\hmonte)&\leq \max_y\left|\widehat{u}_m(y) - u_m(y)\right| \leq\frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)}.\\
%     &\leq \max_y\left|\widehat{u}_m(y) - u_m(y)\right| \leq\frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{8}{\delta}\right)}.\\
% \end{align*}
\section{Proof of Lemma~\ref{lemma:heart}}
\label{apendix:heart}
We start by analyzing the $u_m(\human) - u_m(\hmonte)$. 

We decompose it as follows:
      \begin{align*}
        &u_m(\human)-\widehat{u}_m(\human) + \widehat{u}_m(\hmonte) - u_m(\hmonte) +\underbrace{\widehat{u}_m(\human) - \widehat{u}_m(\hmonte)}_{\leq 0}\\
        &\leq \Delta(u_m,\widehat{u}_m,\human)+\Delta(\widehat{u}_m,u_m,\hmonte)
    \end{align*}


We can express $ \Delta(u_m,\widehat{u}_m,\human)$ using the following formulation, derived from Lemma~\ref{theorem:hoeffding}.

\begin{equation*}
    \operatorname{Pr}\left(\left| \Delta(u_m,\widehat{u}_m,\human)\right| \leq \epsilon\right) 
    \leq 1- 2\exp\left(-\frac{2 n\epsilon^2}{\Umax^2}\right) = {1- \frac{\delta}{2}}.
\end{equation*}
$ \Delta(u_m,\widehat{u}_m,\human)$ holds the following inequality with probability at least $1-\frac{\delta}{2}$.
\begin{align*}
    \Delta(u_m,\widehat{u}_m,\human)  \leq \Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)} .
\end{align*}

Next, we analyze $\Delta(\widehat{u}_m,u_m,\hmonte)$, however, Lemma~\ref{theorem:hoeffding} cannot be directly applied because $\hmonte$ depends on $\widehat{u}_m$. To address this dependency, we instead utilize Lemma~\ref{theorem:uci}, and we can get the following formulation.
% We can express $\Delta(\widehat{u}_m,u_m,\hmonte)$ from Lemma~\ref{theorem:uci}.
\begin{align*}
    \operatorname{Pr}\left(\max_y\left|\Delta(\widehat{u}_m,u_m,y)\right| \leq 2 \mathcal{R}_n(\mathcal{F}) + \epsilon\right) 
    &\leq 1- \exp\left(-\frac{2n \epsilon^2}{\Umax^2}\right) = 1-\frac{\delta}{2}.
\end{align*}
We can thus express $\Delta(\widehat{u}_m,u_m,\hmonte)$ with probability at least $1- \frac{\delta}{2}$.
\begin{align*}
    \Delta(\widehat{u}_m,u_m,\hmonte)\leq \max_y\left|\Delta(\widehat{u}_m,u_m,y)\right| \leq2 \mathcal{R}_n(\mathcal{F}) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{2}{\delta}\right)}.
\end{align*}


From Section 27.2 \cite{shalev2014understanding}, the following upper bound on the Rademacher complexity $\mathcal{R}_n(\mathcal{F})$ is obtained under Assumption~\ref{assumption:utility}:
\begin{align*}
    2 \mathcal{R}_n(\mathcal{F}) \leq \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right)
\end{align*}
From above inequality, we can get the following the bound:
\begin{align*}
    \Delta(\widehat{u}_m,u_m,\hmonte)&\leq \max_y\left|\Delta(\widehat{u}_m,u_m,y)\right| \\
    &\leq\frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{2}{\delta}\right)}\\
    &\leq\frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)}.\\
\end{align*}
If $d \geq 4$, 
\begin{align*}
    \Delta(\widehat{u}_m,u_m,\hmonte)& \leq\frac{36 \Umax}{n}\sqrt{d \log(\sqrt{d})} +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)}.\\
    &\leq 3\sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\sqrt{d \log d}.
\end{align*}
Otherwise, if  $d < 4$,
\begin{align*}
    \Delta(\widehat{u}_m,u_m,\hmonte)& \leq\frac{36 \Umax}{n}\sqrt{d \log(\sqrt{d})} +\Umax\sqrt{\frac{1}{2n}\log \left(\frac{4}{\delta}\right)}.\\
    &\leq 3\sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{72\sqrt{d}}{n}.
\end{align*}
\section{The Case of $\Phuman$ and $\Pmodel$ are identical.}\label{appendix:human_equal_model}
% \begin{maincoro}\label{theorem:bound2}
If $\Phuman$ and $\Pmodel$ are equal, the upper bound of $\mathrm{Regret}_{n}$ corresponds to Lemma~\ref{lemma:heart}:
    \begin{align*}
    \mathrm{Regret}_{n} &\leq  2\Umax \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} + \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right).\\
    &\leq 3\sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\left(\sqrt{d \log d}\right).
\end{align*}
% \end{maincoro}
In most studies, the primary goal of MBR decoding studies is to derive $\hmodel$, given that $\Phuman$ is inaccessible. These studies implicitly assume $\Pmodel=\Phuman$, highlighting the significance of the results. This finding is for understanding and improving the practical application of MBR decoding methods.

\section{Proof of Lemma~\ref{lemma:wd}}\label{appendix:wd}
% We define $\Delta u(h) = u_h (h) - u_m(h)$
We can  derive the following inequality under Assumption~\ref{assumption:lip} for any $y \in \mathcal{Y}$:
% \begin{align*}
%    \Delta(u_h,u_m,y) &\leq  |u_h(y) - u_m(y)|\\
%     &= \left|\sum_{y\in \mathcal{Y}} P_{\mathrm{human}}(y) u(h,y) - \sum_{y^\prime \in \mathcal{Y}} P_{\mathrm{model}}(y^\prime) u(h,y^\prime)\right|\\
%     &= \left| \sum_{y,y^\prime} (u(h,y) - u(h,y^\prime)) \gamma(y,y^\prime) \right| \\
%     & \text{where} \quad \sum_y \gamma(y,y^\prime)= P_{\mathrm{model}}(y^\prime), \quad  \sum_{y^\prime} \gamma(y,y^\prime) = P_{\mathrm{human}}(y), \quad \gamma(y,y^\prime) \geq 0 \\
%     &\leq \min_\gamma \sum_{y,y^\prime} |u(h,y) - u(h,y^\prime)| \gamma(y,y^\prime)  \\
%     &\leq \min_\gamma \sum_{y,y^\prime} C(y,y^\prime) \gamma(y,y^\prime)  \\
%     &= \mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}})
% \end{align*}
\begin{align*}
   \Delta(u_h,u_m,y) &\leq  | \Delta(u_h,u_m,y)|\\
    &= \left|\sum_{y'\in \mathcal{Y}} P_{\mathrm{human}}(y') u(y,y') - \sum_{y'' \in \mathcal{Y}} P_{\mathrm{model}}(y'') u(y,y^{\prime\prime})\right|\\
    &= \left| \sum_{y',y^{\prime\prime}} (u(y,y') - u(y,y^{\prime\prime})) \gamma(y',y^{\prime\prime}) \right| \\
    & \text{where} \quad \sum_{y'} \gamma(y',y^{\prime\prime})= P_{\mathrm{model}}(y^{\prime\prime}), \quad  \sum_{y^{\prime\prime}} \gamma(y,y^{\prime\prime}) = P_{\mathrm{human}}(y'), \quad \gamma(y',y^{\prime\prime}) \geq 0 \\
    &\leq \min_\gamma \sum_{y',y^{\prime\prime}} |u(y,y') - u(y,y^{\prime\prime})| \gamma(y',y^{\prime\prime})  \\
    &\leq \min_\gamma \sum_{y',y^{\prime\prime}} C(y',y^{\prime\prime}) \gamma(y',y^{\prime\prime})  \\
    &= \mathrm{WD}(\Phuman,\Pmodel)
\end{align*}

% \ichihara{\begin{equation}
% \left|u\left(y, y^{\prime}\right)-u\left(y, y^{\prime \prime}\right)\right| \leq\|\boldsymbol{\alpha}(y)\| \cdot\left\|\boldsymbol{v}\left(y^{\prime}\right)-\boldsymbol{v}\left(y^{\prime \prime}\right)\right\| .
% \end{equation}
% If we can consider as $\|\boldsymbol{\alpha}(y)\| \cdot\left\|\boldsymbol{v}\left(y^{\prime}\right)-\boldsymbol{v}\left(y^{\prime \prime}\right)\right\| = C(y',y'')$, it doesn't need assumption 2}
% \section{Proof of Lemma~\ref{lemma:black}}
% \label{appendix:black}

% Under the Assumption ~\ref{assumption:human}, using the Lemma ~\ref{theorem:hoeffding}, the $\errh$ term is expressed as follows:
% \begin{equation*}
%     \operatorname{Pr}\left(\left|\Delta(u_h,u_m,\human) \right| \leq \epsilon\right) 
%     \leq 1- 2\exp\left(-\frac{2 |D|\epsilon^2}{\Umax^2}\right) = {1- \frac{\delta}{4}}.
% \end{equation*}
% % where $\delta \in \mathbb{R}^{+}$
% Now, let us rearrange $\epsilon$, which can be expressed as follows:

% \begin{align*}
%     \epsilon &= \Umax\sqrt{\frac{1}{2|D|}\log \left(\frac{8}{\delta}\right)}.
% \end{align*}  

% In other words
%     The upper bound of $\errh$ has a probability of at least $1-\frac{\delta}{4}$.
% \begin{align*}
%     \Delta(u_h,u_m,\human) \leq \Umax\sqrt{\frac{1}{2|D|}\log \left(\frac{8}{\delta}\right)}.
% \end{align*}
% For the $\Delta(u_h,u_m,\hmonte)$ term, the upper bound can be obtained by the same operation, and the complement Lemma~\ref{lemma:black} is proved. 

\section{Proof of Lemma~\ref{lemma:black}}
\label{appendix:black}

Under the Assumption ~\ref{assumption:human}, with using the Lemma ~\ref{theorem:hoeffding}, the $\Delta(u_h,u_m,\human)$ term is expressed as follows:
\begin{equation*}
    \operatorname{Pr}\left(\left|\Delta(u_h,u_m,\human) \right| \leq \epsilon\right) 
    \leq 1- 2\exp\left(-\frac{2 |D|\epsilon^2}{\Umax^2}\right) = {1- \frac{\delta}{2}}.
\end{equation*}
% where $\delta \in \mathbb{R}^{+}$
We rearrange $\epsilon$ as follows:

\begin{align*}
    \epsilon &= \Umax\sqrt{\frac{1}{2|D|}\log \left(\frac{4}{\delta}\right)}.
\end{align*}  

In other words, the upper bound of $\Delta(u_h,u_m,\human)$ has a probability of at least $1-\frac{\delta}{2}$.
\begin{align*}
    \Delta(u_h,u_m,\human) \leq \Umax\sqrt{\frac{1}{2|D|}\log \left(\frac{4}{\delta}\right)}.
\end{align*}
For the $\Delta(u_h,u_m,\hmonte)$ term, the upper bound can be obtained by the same operation, and the complement Lemma~\ref{lemma:black} is proved. 

\begin{align*}
        &\Delta(u_h,u_m,\human)+ \Delta(u_m,u_h,\hmonte)\leq 3\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
    \end{align*}

% \section{Proof of Theorem~\ref{theorem:minimax}}\label{appendix:minimax}
% % We define $u_\mathcal{P}(h_P^*) - u_\mathcal{P}(h) = \E_{p}\left[ \left(u(y^*_P,y) - u(h,y)\right)\right]$.
% % where $u^*_h$ is an optimal probability in $P$, $h$ is an arbitrary probability (e.g. $\hmonte$).
% % We now consider the following formulation. Using the definition, we can see $u_\mathcal{P}(y^*_P) - u(h) = \left|u_\mathcal{P}(y^*_P) - u(h)\right|$




% % \begin{align}
% %     \max_{P \in \mathcal{P}}[u_\mathcal{P}(y^*_P) - u_\mathcal{P}(h)] &=\left|u_\mathcal{P}(y^*_P) - u_\mathcal{P}(h)\right| \\
% %     &= \max_{P \in \mathcal{P}}\E_{P}\left[\left|\left(u(y^*_P,y) - u(h,y)\right)\right| \right]\\
% %     &\geq \min_{f} \max_{P \in \mathcal{P}} \E_{Y^n \sim P}\underbrace{\left[\left|\left(u(y^*_P,y) - u(f_{Y^n}(y),y)\right) \right|\right]}_{\rho(y^*(P)), f_{Y^n})}\\
% %     &= \min_f \max_P \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right]\\
% %     &= \mathcal{R} (\mathcal{P}, f_{Y^n}).
% % \end{align}
% % where $h^{*}_P$ is the learner required by the learning policies of MBR decoding. $y^*(P)$ output optimal $h$ in $P$, $f_{Y^n}$ is an estimator that takes the n samples $Y^n$ generated from $P$ and estimates output $h$ emitted from $y^*(P)$. 

% % We define a risk with an arbitrary $f$:
% % \begin{equation}
% %    \mathcal{R}^f (\mathcal{P}, f_{Y^n}) =  \max_P \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right].
% % \end{equation}

% % \begin{align}
% % \mathcal{R}^f (\mathcal{P}, f_{Y^n}) & =\max_{P \in \mathcal{P}}  \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right] \\
% % & \geq \max_{P \in \mathcal{P}}  \E_{Y^n \sim P}\left[\beta \cdot \1\left[\rho(y^*(P), f_{Y^n}) \geq \beta\right]\right] \\
% % & =\max_{P \in \mathcal{P}} \beta \cdot P\left[\rho(y^*(P), f_{Y^n}) \geq \beta\right] \\
% % &\geq \beta \cdot \max_P \mathbb{P}\left[\rho(y^*(P), f_{Y^n})\geq \beta\right].
% % \end{align}
% % where $\beta\in \mathbb{R}^{+}$

% % % In the above, we chose the worst case among all possible $P$, but here, we take some representatives from them and set their index as $V$, and the number of representatives is $\mathcal{V}$ in total.
% % In the previous analysis, we considered the worst-case scenario across all possible distributions $P$.  
% % Here, however, we select a subset of representative distributions, indexing them by $V$, where the total number of representatives is denoted by $\mathcal{V}$.

% % \begin{equation}
% % \Psi\left(Y^n\right):=\operatorname{argmin}_{v \in \mathcal{V}}  \rho \left(y^*(P_{v}), f_{Y^n}\right) .
% % \end{equation}

% % We can see if $\Psi(Y^n) \neq v$, $\rho(y^*(P), f_{Y^n})\geq \beta$. 

% % \begin{align}
% %     \max_P \mathbb{P}\left[\rho(y^*(P), f_{Y^n})\geq \beta\right] &\geq \frac{1}{\mathcal{V}} \sum_{v \in \mathcal{V}} \mathbb{P}\left[\rho(y^*(P_v), f_{Y^n})\geq \beta | \mathrm{V} = v  \right]\\
% %     &\geq \frac{1}{\mathcal{V}}\sum_{v \in \mathcal{V}} \mathbb{P}\left[\Psi(Y^n) \neq v |\mathrm{V} = v \right]\\
% %     &= \mathbb{P}(\Psi(Y^n) \neq \mathrm{V}).
% % \end{align}

% % Finally, we can see the following inequality:

% % \begin{align}
% %     \mathcal{R}^f (\mathcal{P}, f_{Y^n}) &\geq \beta \cdot \mathbb{P}(\Psi(Y^n) \neq \mathrm{V})\\
% %     \mathcal{R} (\mathcal{P}, f_{Y^n}) &\geq \beta \cdot \min_{\Psi} \mathbb{P}(\Psi(Y^n) \neq \mathrm{V})
% % \end{align}

% % We can apply Fano's inequality to analyze the equation in our setting.



% % \begin{equation}
% % \mathcal{R} (\mathcal{P}, f_{Y^n}) \geq \beta\left[1-\frac{I\left(V ; Y^n\right)+\log (2)}{\log |\mathcal{V}|}\right].
% % \end{equation}
% % We assume the following inequality:
% % \begin{equation}
% %     \rho\left( y^*(P_v), \human(P_{v^\prime}) \right) \geq 2\beta.
% % \end{equation}

% % The definition of $I(V ; Y^n) $ can be bound by KL-divergence, so we firstly derive the bound of KL-divergence.

% % \begin{equation}
% %     \mathrm{KL}(P_v \| P_{v^\prime}) \leq \epsilon,    
% % \end{equation}

% % We have $P_v^{(n)}=\prod_{i=1}^n P_v\left(Y_i\right)$ and $D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)=n D_{\mathrm{KL}}\left(P_v \| P_{v^{\prime}}\right)$.

% % \begin{equation}
% % I\left(V ; Y^n\right) \leq \frac{1}{|\mathcal{V}|^2} \sum_{v , v^{\prime}} D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)
% % \end{equation}

% % Finally we can get as follow:
% % \begin{equation}
% %     I(V ; Y^n)  \leq n\epsilon
% % \end{equation}

% % \begin{equation}
% % \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \beta\left(1-\frac{n \epsilon+\log 2}{\log |\mathcal{V}|}\right).
% % \end{equation}

% % Putting $\log |\mathcal{V}|=2 n \epsilon$,

% % \begin{equation}
% % \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \frac{\beta}{2}\left(1-\frac{\log 2}{ n \epsilon}\right).
% % \end{equation}
% We define $u_\mathcal{P}(y_P^*) - u_\mathcal{P}(y) = \max_P \E_{p}\left[ \left(u(y^*_P,y') - u(y,y')\right)\right]$.
% where $y$ is an arbitrary probability (e.g. $\hmonte$).
% We now consider the following formulation. Using the definition, we can see $u_\mathcal{P}(y^*_P) - u(y) = \left|u_\mathcal{P}(y^*_P) - u(y)\right|$




% \begin{align*}
%     \max_{P \in \mathcal{P}}[u_\mathcal{P}(y^*_P) - u_\mathcal{P}(y)] &=\left|u_\mathcal{P}(y^*_P) - u_\mathcal{P}(y)\right| \\
%     &= \max_{P \in \mathcal{P}}\E_{P}\left[\left|\left(u(y^*_P,y') - u(y,y')\right)\right| \right]\\
%     &\geq \min_{f} \max_{P \in \mathcal{P}} \E_{Y^n \sim P}\underbrace{\left[\left|\left(u(y^*_P,y') - u(f_{Y^n}(y),y')\right) \right|\right]}_{\rho(y^*(P)), f_{Y^n})}\\
%     &= \min_f \max_P \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right]\\
%     &= \mathcal{R} (\mathcal{P}, f_{Y^n}).
% \end{align*}
% where $h^{*}_P$ is the learner required by the learning policies of MBR decoding. $y^*(P)$ output optimal $y$ in $P$, $f_{Y^n}$ is an estimator that takes the n samples $Y^n$ generated from $P$ and estimates output $y$ emitted from $y^*(P)$. $f$ does not have to be a decoding technique for the MBR. 

% We define a risk $\mathcal{R}$ with an arbitrary $f$:
% \begin{equation*}
%    \mathcal{R}^f (\mathcal{P}, f_{Y^n}) \coloneqq  \max_P \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right].
% \end{equation*}

% \begin{align*}
% \mathcal{R}^f (\mathcal{P}, f_{Y^n}) & =\max_{P \in \mathcal{P}}  \E_{Y^n \sim P}\left[\rho(y^*(P), f_{Y^n})\right] \\
% & \geq \max_{P \in \mathcal{P}}  \E_{Y^n \sim P}\left[\beta \cdot \1\left[\rho(y^*(P), f_{Y^n}) \geq \beta\right]\right] \\
% & =\max_{P \in \mathcal{P}} \beta \cdot P\left[\rho(y^*(P), f_{Y^n}) \geq \beta\right] \\
% &\geq \beta \cdot \max_{P \in \mathcal{P}} \mathbb{P}\left[\rho(y^*(P), f_{Y^n})\geq \beta\right].
% \end{align*}
% where $\beta\in \mathbb{R}^{+}$

% % In the above, we chose the worst case among all possible $P$, but here, we take some representatives from them and set their index as $V$, and the number of representatives is $\mathcal{V}$ in total.
% In the previous analysis, we considered the worst-case scenario across all possible distributions $P$.  
% Here, however, we select a subset of representative distributions, indexing them by $v$, where the total number of representatives is denoted by $\mathcal{V}$.

% \begin{equation*}
% \Psi\left(Y^n\right):=\operatorname{argmin}_{v \in \mathcal{V}}  \rho \left(y^*(P_{v}), f_{Y^n}\right) .
% \end{equation*}

% We can see if $\Psi(Y^n) \neq v$, $\rho(y^*(P), f_{Y^n})\geq \beta$. 

% \begin{align*}
%     \max_P \mathbb{P}\left[\rho(y^*(P), f_{Y^n})\geq \beta\right] &\geq \frac{1}{\mathcal{V}} \sum_{v \in \mathcal{V}} \mathbb{P}\left[\rho(y^*(P_v), f_{Y^n})\geq \beta | \mathrm{V} = v  \right]\\
%     &\geq \frac{1}{\mathcal{V}}\sum_{v \in \mathcal{V}} \mathbb{P}\left[\Psi(Y^n) \neq v |\mathrm{V} = v \right]\\
%     &= \mathbb{P}(\Psi(Y^n) \neq \mathrm{V}).
% \end{align*}

% Finally, we can see the following inequality:

% \begin{align*}
%     \mathcal{R}^f (\mathcal{P}, f_{Y^n}) &\geq \beta \cdot \mathbb{P}(\Psi(Y^n) \neq \mathrm{V})\\
%     \mathcal{R} (\mathcal{P}, f_{Y^n}) &\geq \beta \cdot \min_{\Psi} \mathbb{P}(\Psi(Y^n) \neq \mathrm{V})
% \end{align*}

% We can apply Fano's inequality \cite{cover1999elements,pmlr-v202-majumdar23a} to analyze the equation in our setting.
% \begin{equation*}
% \mathcal{R} (\mathcal{P}, f_{Y^n}) \geq \beta\left[1-\frac{I\left(V ; Y^n\right)+\log (2)}{\log |\mathcal{V}|}\right].
% \end{equation*}

% We assume the following inequality:
% % \begin{equation*}
% %     \rho\left( y^*(P_v), \human(P_{v^\prime}) \right) \geq 2\beta.
% % \end{equation*}
% \begin{equation*}
%     \rho\left( y^*(P_v), \human(P_{v^\prime}) \right) \geq \beta.
% \end{equation*}

% The definition of $I(V ; Y^n) $ can be bound by KL-divergence:
% \begin{equation*}
%    D_{\mathrm{KL}}(P_v \| P_{v^\prime}) \leq \epsilon,    
% \end{equation*}

% We have $P_v^{(n)}=\prod_{i=1}^n P_v\left(Y_i\right)$ and $D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)=n D_{\mathrm{KL}}\left(P_v \| P_{v^{\prime}}\right)$.

% \begin{equation*}
% I\left(V ; Y^n\right) \leq \frac{1}{|\mathcal{V}|^2} \sum_{v , v^{\prime}} D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)
% \end{equation*}

% Finally, we can get the following inequality:
% \begin{equation*}
%     I(V ; Y^n)  \leq n\epsilon
% \end{equation*}

% \begin{equation*}
% \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \beta\left(1-\frac{n \epsilon+\log 2}{\log |\mathcal{V}|}\right).
% \end{equation*}

% Putting $\log |\mathcal{V}|=2 n \epsilon$,

% \begin{equation*}
% \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \frac{\beta}{2}\left(1-\frac{\log 2}{ n \epsilon}\right).
% \end{equation*}


% \section{Proof of Corollary~\ref{coro:utility}}\label{appendix:utility}
% Before the proof, we derive the upper bound of the utility function difference.

% The expectation difference is:
% \begin{align*}
%     &\mathbb{E}\left[u(y, y')\right] 
%     \;-\; \mathbb{E}\left[u'(y, y')\right]\\
%     &= \mathbb{E}\left[\ba(y)^{\top}\bv(y')\right]
%        - \mathbb{E}\left[\ba'(y)^{\top}\bv(y')\right]\\
%     &= \mathbb{E}\left[(\,\ba(y)
%       - \ba'(y)\,)^{\top} \bv(y')\right].
% \end{align*}
% By applying the Cauchy–Schwarz inequality, we obtain:
% \begin{align*}
%     &\E[u(\human,y')] - \E[u^\prime(\human,y')] \\
%     &\leq \|\ba(\human)-\ba^{\prime}(\human)\|\cdot \|\E[\bv(y')]\|\\
%     &\leq \|\ba(\human)-\ba^{\prime}(\human)\|.
% \end{align*}

% Next, we prove the corollary~\ref{coro:utility}.
% % We define the following expression:
% % \begin{align*}
% %     u'(y) &= \frac{1}{|\Yn|} \sum_{y' \in \Yn} u'(y, y').\\
% %     y' &= \argmax_{y \in \mathcal{\Href}} u'(y). 
% % \end{align*}
% % The proxy utility function $u'$ under Assumption~\ref{assumption:utility} is defined as:
% % \begin{equation*}
% %     u^\prime(y, y') = \mathbf{\ba^\prime}(y)^{\top} \bv(y').
% % \end{equation*}

% % \begin{align*}
% %     \mathrm{Regret}^{u}_{n,D} \coloneqq u_h(\human) - u_h(y').
% % \end{align*}
% \begin{align*}
%     u_h(\human) - u_h(y') = \Delta(u_h,u_m,\human)+ u_m(\human) - u_m(y') + \Delta(u_m,u_h,y').
% \end{align*}
% From Appendix~\ref{appendix:black}, we can get the following bound with probability at least $1-\frac{\delta}{2}$:
% \begin{align*}
%     \Delta(u_h,u_m,\human) + \Delta(u_m,u_h,y')\leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
% \end{align*}

% The next step is to prove the remaining conditions.
% % 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}
% % \begin{align*}
% %     u_m(\human) - u_m(y')&=u_m(\human) - u'(\human)  - u_m(y') + u'(y') - u'(y') + u'(\human)\\
% %     &\leq u_m(\human) - u'(\human)  - u_m(y') + u'(y')\\
% %     &= u_m(\human) - \widehat{u}_m(\human) - u'(\human) + \widehat{u}_m(\human) - u_m(y') + \widehat{u}_m(y') + u'(y') - \widehat{u}_m(y')\\
% %     &\leq  4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
% % \end{align*}
% \begin{align*}
%     u_m(\human) - u_m(y')&=\Delta(u_m,u',\human) +\Delta(u',u_m,y') - u'(y') + u'(\human)\\
%     &\leq \Delta(u_m,u',\human) +\Delta(u',u_m,y')\\
%     &= \Delta(u_m,\widehat{u}_m,\human) +\Delta(\widehat{u}_m,u',\human) +\Delta(\widehat{u}_m,u_m,y')+\Delta(u',\widehat{u}_m,y')\\
%     &\leq  4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
% \end{align*}


% Finally, we can get the following the bound under Assumption~\ref{assumption:utility} and Assumption~\ref{assumption:human} with probability at least $1-\delta$:
% \begin{align*}
%     \mathrm{Regret}^{u}_{n,D} \leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} + 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
% \end{align*}

% The definition of $I(V ; Y^n) $ can be bound by KL-divergence:
% \begin{equation*}
%    D_{\mathrm{KL}}(P_v \| P_{v^\prime}) \leq \epsilon,    
% \end{equation*}

% We have $P_v^{(n)}=\prod_{i=1}^n P_v\left(Y_i\right)$ and $D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)=n D_{\mathrm{KL}}\left(P_v \| P_{v^{\prime}}\right)$.

% \begin{align*}
% I\left(V ; Y^n\right) &\leq \frac{1}{|\mathcal{V}|^2} \sum_{v , v^{\prime}} D_{\mathrm{KL}}\left(P_v^{(n)} \| P_{v^{\prime}}^{(n)}\right)\\
% &\leq c^2n\beta^2
% \end{align*}
% where c is some quantity.

% % Finally, we can get the following inequality:
% % \begin{equation*}
% %     I(V ; Y^n)  \leq n\epsilon
% % \end{equation*}
% By decreasing $\beta$ sufficiently, we may thereby ensure that
% \begin{equation*}
% \frac{I\left(V ; Y^n\right)+\log 2}{\log |\mathcal{V}|} \leq \frac{1}{2}
% \end{equation*}

% \begin{equation*}
% \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \beta\left(1-\frac{c^2n\beta^2+\log 2}{\log |\mathcal{V}|}\right).
% \end{equation*}

% From $\log |\mathcal{V}|\geq 2 (c^2n\beta^2+\log 2)$,

% \begin{equation*}
% \mathcal{R}\left(\mathcal{P}, f_{Y^n}\right) \geq \frac{\beta}{2}.
% \end{equation*}
% \section{Proof of Corollary~\ref{coro:bound}}\label{appendix:coro_tempre}
\section{Regret Bound for MBR decoding with temperature sampling}\label{appendix:coro_tempre}

We have been considering random sampling so far, but we also analyze what the bounds would be if we did temperature sampling, considering practical aspects.
\begin{equation*}
    \Pmodel^t(y) = \frac{\exp\left(t^{-1}\Pmodel(y)\right)}{\sum_{y^\prime \in \mathcal{Y}} \exp\left(t^{-1}\Pmodel(y^\prime)\right)}
\end{equation*}
where $t\in\mathbb{R}^{+}$. 
% The objective equation for MBR decoding using model distribution can be redefined as:
% \begin{align}
%     u_m(y) &= \sum_{y' \in \mathcal{Y}} u(y, y')\cdot\Pmodel(y').\\
%      \hmodel &= \argmax_{y \in \mathcal{H}} u_m(y).  
% \end{align}
The objective equation for MBR decoding of the Monte Carlo estimates using a collection of reference hypotheses $\Yn$ sampled from the model $\Pmodel^t$ is as follows:
\begin{align*}
    \widehat{u}_m^t(y) &= \frac{1}{|\Yn|} \sum_{y' \in \Yn} u(y, y').\\
\hat{y}^m_t &= \argmax_{y \in \mathcal{\Yn}} \widehat{u}_m^t(y).
\end{align*}
Our new objective formulation is defined as:
% \begin{equation}
%     \mathrm{Regret}(\human, h_t^{\mathrm{mc}})
%  \end{equation}
\begin{equation}
\mathrm{Regret}_{n,D}^t \coloneqq u_h(\human) - u_h(\hat{y}^m_t).
 \end{equation}
 % From the results of the previous analysis, 
 % $\mathrm{Regret}_{n,D}^t$ is decomposed in the same way as Eq.~\eqref{equation:regret_n},
 % we apply to Lemma~\ref{theorem:uci} with all terms involving $u_m, \widehat{u}_m^t$, we also use Lemma~\ref{theorem:hoeffding} with all terms involving $u_h, u_m$, and we can derive the upper bound as follows under the condition Theorem~\ref{theorem:bound} for any $t\in \mathbb{R}^+$.
 % $\mathrm{Regret}_{n,D}^t$ is decomposed in the same way as Eq.~\eqref{equation:regret_n},
 % we apply to Lemma~\ref{theorem:uci} with all terms involving $u_m, \widehat{u}_m^t$, we also use Lemma~\ref{theorem:hoeffding} with all terms involving $u_h, u_m$, and we can derive the upper bound as follows under the condition Theorem~\ref{theorem:bound} for any $t\in \mathbb{R}^+$.
We can derive the upper bound of $\mathrm{Regret}_{n,D}^t$ as follows under the condition Theorem~\ref{theorem:bound} for any $t\in \mathbb{R}^+$.
\begin{maincoro}[Regret Bound for MBR decoding with temperature sampling]\label{coro:bound}
    Under Assumption~\ref{assumption:utility}, Assumption~\ref{assumption:lip}, Assumption~\ref{assumption:human} and assuming $d \geq 4$, the regret upper bound of the MBR decoding holds for any $\delta\in(0,1)$, with probability at least $1 - \delta$:
    \begin{align*}
        % u_h(\human) - u_h(\hmonte) &\leq 2 \Umax\left(\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{|D|}}\right)\left(\sqrt{\frac{1}{2} \log \frac{6}{\delta}}+2\right).
       % &\mathrm{Regret}_{n,D}^t \leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} \\
       %  &+ \frac{24 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) \\
       %  &+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}.
       % &\mathrm{Regret}_{n,D}^t \leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{4}{\delta}} \\
       %  &+ \frac{24 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) \\
       %  &+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}.
       % \mathrm{Regret}_{n,D}^t &\leq 3 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+ 4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} \\
       %  &+ \frac{72}{n}\sqrt{d \log d}. 
       &\mathrm{Regret}_{n,D}^t \leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+ 4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} \\
        &+ \frac{36}{n}\sqrt{d \log d} + \mathrm{WD}(\Pmodel,\Pmodel^t). 
        % &+ 4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
    \end{align*}
\end{maincoro}
% where $\delta\in(0,1)$, $d \geq 4$.
% The proof is in Appendix~\ref{appendix:coro_tempre}. 
The case of the little samples $n$ might lead to better performance with using the temperature sampling rather than using $\Pmodel$ if $t$ is large. However, the above bound has an extra term $\mathrm{WD}$ when performing temperature sampling compared to Theorem~\ref{theorem:bound}, so the upper bound of $\mathrm{Regret}_{n,D}^t $ might be improved tighter than Corollary~\ref{coro:bound}'s derived from. We discuss this later in this section.
% Corollary~\ref{coro:bound} indicates that if we consider temperature sampling, the upper bound is the same convergence rate without considering the constant factors.
\begin{proof}
    
% \end{proof}
From the definition of $\mathrm{Regret}_{n,D}^t$:
\begin{equation*}
    \mathrm{Regret}_{n,D}^t \coloneqq u_h(\human) - u_h(\hat{y}^m_t).
 \end{equation*}
 The objective equation for MBR decoding using temperature model distribution can be redefined as:
\begin{align*}
    u^t_m(y) &= \sum_{y' \in \mathcal{Y}} u(y, y')\cdot\Pmodel^t(y').\\
     \hmodel_t &= \argmax_{y \in \mathcal{H}} u_m(y). 
\end{align*}
 We can decompose the following terms:
% \begin{align*}
%      \mathrm{Regret}_{n,D}^t \leq  \Delta(u_h,u_m,\human) + \Delta(u_m,\widehat{u}^t_m,\human)\nonumber
%  +\Delta(\widehat{u}^t_m,u_m,\hmonte_t)+ \Delta(u_m,u_h,\hmonte_t).
% \end{align*}
\begin{align*}
     \mathrm{Regret}_{n,D}^t &\leq  \Delta(u_h,u_m,\human)+\Delta(u_m,u_h,\hmonte_t) + u_m(\human) - u^t_m(\human)+u_m^t(\human) - \widehat{u}^t_m(\human)\\
     &+ \widehat{u}^t_m(\hmonte_t) - u_m^t(\hmonte_t) + u_m^t(\hmonte_t) - u_m(\hmonte_t)\\
     &= \Delta(u_h,u_m,\human)+\Delta(u_m,u_h,\hmonte_t) + \Delta(u_m,u_m^t,\human) + \Delta(u_m^t,\widehat{u}^t_m,\human) \\
     &+ \Delta(\widehat{u}^t_m,u_m^t,\hmonte_t) + \Delta(u_m^t,u_m,\hmonte_t)
\end{align*}

First, the involving the terms $u_h,u_m$ is immediately bounded by Lemma~\ref{lemma:black} with probability at least $1-\frac{\delta}{2}$ under Assumption~\ref{assumption:human}.
\begin{align*}
    \Delta(u_h,u_m,\human) + \Delta(u_m,u_h,\hmonte_t) &\leq 2\Umax\sqrt{\frac{1}{2|D|}\log \left(\frac{8}{\delta}\right)}\\
    &\leq4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
\end{align*}

Next we derive $\Delta(u_m,u_m^t,\human) + \Delta(u_m^t,\widehat{u}^t_m,\human)$'s upper bound.
Under Assummption~\ref{assumption:lip}, we can get the following the bound.
\begin{align*}
    \Delta(u_m,u_m^t,\human) +\Delta(u_m^t,u_m,\hmonte_t) \leq 2\mathrm{WD}(\Pmodel,\Pmodel^t)
\end{align*}
% \begin{align*}
%     \Delta(u_m,\widehat{u}^t_m,\human) = \sum_{y' \in \mathcal{Y}} u(\human, y')\cdot\Pmodel(y') - \frac{1}{|\Yn|} \sum_{y' \in \Yn} u(\human, y')
% \end{align*}
% $\Yn$ sampled from the model $\Pmodel^t$ not $\Pmodel$.

Finally, we derive the upper bound for the terms involving $u_m$ and $\widehat{u}^t_m$.  
We conduct the sample operation he proof of Lemma~\ref{lemma:heart}.
% However, when incorporating temperature sampling, there exists a possibility that the sample $ \human$ is not obtained for a certain temperature parameter $t$.  
We can get the following bound with probability at least $1-\frac{\delta}{2}$ under Assumption~\ref{assumption:utility} and assuming $d\geq 4$:
\begin{align*}
\Delta(u_m^t,\widehat{u}^t_m,\human)+\Delta(\widehat{u}^t_m,u_m^t,\hmonte_t)  
&\leq 2\Umax\sqrt{\frac{1}{2n}\log \left(\frac{8}{\delta}\right)} + \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right)\\
&\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}
        + \frac{36 }{n}\sqrt{d \log  d}. \\
\end{align*}
\end{proof}

\paragraph{The upper bound of $\mathrm{Regret}_{n,D}^t $ might be improved}
We focus on $\Delta(u_m,u_m^t,\human)$. In this paper, we can derive the upper bound with Wasserstein Distance.
However, if $\Pmodel$ capture $\Phuman$, $u_m(\human)< u_h(\human)$, but we consider $\Pmodel^t$, it is possible to be $y_m^t = \human$ with little samples, so $\Delta(u_m,u_m^t,\human)$ can be negative value.
In summary, rather than simply deriving an upper bound on the Wasserstein Distance, this bound could be improved by taking into account a more detailed analysis of the temperature sampling.
% Under Assumption~\ref{assumption:utility}, Assumption~\ref{assumption:human} and assuming $d \geq 4$, the regret upper bound of the MBR decoding holds with probability at least $1 - \delta$ for any $\delta\in(0,1)$:
%     \begin{align*}
%         % u_h(\human) - u_h(\hmonte) &\leq 2 \Umax\left(\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{|D|}}\right)\left(\sqrt{\frac{1}{2} \log \frac{6}{\delta}}+2\right).
%        % &\mathrm{Regret}_{n,D}^t \leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} \\
%        %  &+ \frac{24 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) \\
%        %  &+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}.
%        % &\mathrm{Regret}_{n,D}^t \leq 2\Umax \sqrt{\frac{1}{2n} \log \frac{4}{\delta}} \\
%        %  &+ \frac{24 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) \\
%        %  &+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}.
%        \mathrm{Regret}_{n,D}^t &\leq 3 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+ 4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} \\
%         &+ \frac{72}{n}\sqrt{d \log d}. 
%         % &+ 4 \sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
%     \end{align*}
\section{Proof of the Corollary~\ref{propotion:mbr}}\label{appendix:reg_exp}
We drive the expected upper bound from high probability upper bound. If we have the regret value $R$ with probability at least $1-\delta$, we can get the expected upper bound the following the equation with worst-case value $U$ (e.g. when considering the MBR decoding in this paper, worst-case value can be $1$.)
\begin{align*}
    \textbf{Expected Upper Bound for Regret} = (1-\delta)\cdot R + \delta \cdot U  
\end{align*}
By applying the above equation to Theorem~\ref{theorem:bound3} and Theorem~\ref{theorem:bound}, the following upper bound is derived.
\begin{align*}
    \E\left[\mathrm{Regret}_{n}  \right] &\leq 3 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} +\frac{36}{n}\sqrt{d \log d}+2\mathrm{WD}(P_{\mathrm{human}}, P_{\mathrm{model}}) + \delta 
    % \E\left[\mathrm{Regret}_{n,D}  \right] &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\frac{36 }{n}\sqrt{d \log  d} \\
    %     &+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}  + \delta 
\end{align*}
\begin{align*}
    \E\left[\mathrm{Regret}_{n,D}  \right] &\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\frac{36}{n}\sqrt{d \log  d}
        +4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}  + \delta 
\end{align*}


\section{Proof of Corollary~\ref{coro:utility}}\label{appendix:utility}
Before the proof, we derive the upper bound of the utility function difference.

The expectation difference is:
\begin{align*}
    &\mathbb{E}\left[u(y, y')\right] 
    \;-\; \mathbb{E}\left[u'(y, y')\right]\\
    &= \mathbb{E}\left[\ba(y)^{\top}\bv(y')\right]
       - \mathbb{E}\left[\ba'(y)^{\top}\bv(y')\right]\\
    &= \mathbb{E}\left[(\,\ba(y)
      - \ba'(y)\,)^{\top} \bv(y')\right].
\end{align*}
By applying the Cauchy–Schwarz inequality, we obtain:
\begin{align*}
    &\E[u(\human,y')] - \E[u^\prime(\human,y')] \\
    &\leq \|\ba(\human)-\ba^{\prime}(\human)\|\cdot \|\E[\bv(y')]\|\\
    &\leq \|\ba(\human)-\ba^{\prime}(\human)\|.
\end{align*}

Next, we prove the corollary~\ref{coro:utility}.
% We define the following expression:
% \begin{align*}
%     u'(y) &= \frac{1}{|\Yn|} \sum_{y' \in \Yn} u'(y, y').\\
%     y' &= \argmax_{y \in \mathcal{\Href}} u'(y). 
% \end{align*}
% The proxy utility function $u'$ under Assumption~\ref{assumption:utility} is defined as:
% \begin{equation*}
%     u^\prime(y, y') = \mathbf{\ba^\prime}(y)^{\top} \bv(y').
% \end{equation*}
% \begin{align*}
%     \mathrm{Regret}^{u}_{n,D} \coloneqq u_h(\human) - u_h(y').
% \end{align*}
\begin{align*}
    u_h(\human) - u_h(y') = \Delta(u_h,u_m,\human)+ u_m(\human) - u_m(y') + \Delta(u_m,u_h,y').
\end{align*}
From Appendix~\ref{appendix:black}, we can get the following bound with probability at least $1-\frac{\delta}{2}$:
\begin{align*}
    \Delta(u_h,u_m,\human) + \Delta(u_m,u_h,y')\leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}.
\end{align*}

The next step is to prove the remaining conditions.
% 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}
% \begin{align*}
%     u_m(\human) - u_m(y')&=u_m(\human) - u'(\human)  - u_m(y') + u'(y') - u'(y') + u'(\human)\\
%     &\leq u_m(\human) - u'(\human)  - u_m(y') + u'(y')\\
%     &= u_m(\human) - \widehat{u}_m(\human) - u'(\human) + \widehat{u}_m(\human) - u_m(y') + \widehat{u}_m(y') + u'(y') - \widehat{u}_m(y')\\
%     &\leq  4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
% \end{align*}
\begin{align*}
    u_m(\human) - u_m(y')&=\Delta(u_m,u',\human) +\Delta(u',u_m,y') - u'(y') + u'(\human)\\
    &\leq \Delta(u_m,u',\human) +\Delta(u',u_m,y')\\
    &= \Delta(u_m,\widehat{u}_m,\human) +\Delta(\widehat{u}_m,u',\human) +\Delta(\widehat{u}_m,u_m,y')+\Delta(u',\widehat{u}_m,y')\\
    &\leq  4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
\end{align*}


Finally, we can get the following the bound under Assumption~\ref{assumption:utility} and Assumption~\ref{assumption:human} with probability at least $1-\delta$:
\begin{align*}
    \mathrm{Regret}^{u}_{n,D} \leq 4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} + 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}} + \|\ba(\human)-\ba^{\prime}(\human)\| + \|\ba(y')-\ba^{\prime}(y')\|.
\end{align*}

\section{MAP Decoding Upper Bound} \label{appendix:map_true}

In MAP decoding, our objective is to analyze the difference $\Phuman(\maphuman) - \Phuman(\hm)$.  
To obtain an upper bound, we decompose $\Phuman(\maphuman) - \Phuman(\hm)$ as follows.
\begin{align*}
\Phuman(\maphuman) - \Phuman(\hm) &= \Phuman(\maphuman) - \Pmodel(\maphuman)
+\Pmodel(\maphuman) - \Pmodel(\hm)\\
&+ \Pmodel(\hm) - \Phuman(\hm).
\end{align*}

We solve the upper bound with Lemma~\ref{theorem:DKW}, so we bound the difference of distributions with the difference of empirical distributions.
We also denote $y^{-}$ as the value immediately before $y$. 

The following equation holds for all $y$ with probability at least $1-\frac{2}{\delta}$:
\begin{align*}
|\widehat{P}(y)-\Pmodel(y)|&=\left|\left(\widehat{F}(y)-\widehat{F}\left(y^{-}\right)\right)-\left(F_{\mathrm{model}}(y)-F_{\mathrm{model}}\left(y^{-}\right)\right)\right| .\\
&\leq \left(|\widehat{F}(y)-F_{\mathrm{model}}(y)|+\left|\widehat{F}\left(y^{-}\right)-F_{\mathrm{model}}\left(y^{-}\right)\right|\right) .\\
\max _y|\widehat{P}(y)-\Pmodel(y)|&\leq \max _y\left(|\widehat{F}(y)-F_{\mathrm{model}}(y)|+\left|\widehat{F}\left(y^{-}\right)-F_{\mathrm{model}}\left(y^{-}\right)\right|\right)\label{eq:F_P} \leq 2\epsilon_1 .\\
\end{align*}

% \subsection{Proof of Lemma\ref{theorem:map_heart}}\label{appendix:map_heart}

We apply Lemma~\ref{theorem:DKW} to the above formulation:
    \begin{equation*}
\operatorname{Pr}\left( \max_{y \in \mathcal{Y}} \left|\widehat{F}(y)-F_{\mathrm{model}}(y)\right|>\epsilon_1\right) \leq 2 \exp \left(-2 n\epsilon_1^2\right).
\end{equation*}



Finally, we get the bound with probability at least $1-\frac{\delta}{2}$:
\begin{align*}
    \max _{y \in \mathcal{Y}}|\widehat{P}(y)-\Pmodel(y)|&\leq 2 \sqrt{\frac{1}{2n}\log \frac{8}{\delta}}.
\end{align*}

The following inequality holds for $\hm$:
\begin{equation*}
    \Pmodel(\hm) \geq \widehat{P}(\hm)-2\epsilon_1.
\end{equation*}

This also applies to $\maphuman$:
\begin{equation*}
    \Pmodel\left(\maphuman\right) \leq \widehat{P}\left(\maphuman\right)+2\epsilon_1.
\end{equation*}

From the definition, it is clear that $\widehat{P}(\hm) \geq\widehat{P}\left(\maphuman\right)$.
\begin{align*}
    \Pmodel(\hm) &\geq \widehat{P}(\hm)-2\epsilon_1 \\
    &\geq \widehat{P}\left(\maphuman\right)-2\epsilon_1 \\
    &\geq \Pmodel\left(\maphuman\right)-4 \epsilon_1 .
\end{align*}

Therefore, we can get the upper bound at least $1-\frac{\delta}{2}$:
\begin{align*}
    \Pmodel\left(\maphuman\right)-\Pmodel(\hm) &\leq 4\sqrt{\frac{1}{2n}\log \frac{8}{\delta}}.
\end{align*}
% \subsection{Proof of Lemma\ref{theorem:map_black}}\label{appendix:map_black}
\begin{equation*}
\operatorname{Pr}\left( \max_{y \in \mathcal{Y}} \left|F_{\mathrm{model}}(h)-F_{\mathrm{human}}(h)\right|>\epsilon_2\right) \leq 2 \exp \left(-2 |D|\epsilon_2^2\right).
\end{equation*}

We also use Lemma~\ref{theorem:DKW}. It satisfies with probability at least $1-\frac{\delta}{2}$:
\begin{align*}
        \Phuman(\maphuman) - \Pmodel(\maphuman) +  \Pmodel(\hm) - \Phuman(\hm)&\leq 4\sqrt{\frac{1}{2|D|}\log \frac{8}{\delta}}.
    \end{align*}

Finally,  we get the following upper bound with probability at least $1-\delta$:

\begin{align*}
   \mathrm{Regret}^\mathrm{MAP}_{n,D}&\leq 4\sqrt{\frac{1}{2n}\log \frac{8}{\delta}} + 4\sqrt{\frac{1}{2|D|}\log \frac{8}{\delta}}\\
    % &= 4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right)
    &\leq 8\sqrt{\frac{1}{n}\log\frac{1}{\delta}}+8\sqrt{\frac{1}{|D|}\log\frac{1}{\delta}}.
\end{align*}


% \section{Proof of Theorem~\ref{theorem:mbr_map}}\label{appendix:map_mbr}
\section{Observation}\label{appendix:map_mbr}
% \subsection{Observation 1}\label{appendix:observation1}
We describe the derivations of the Observation~\ref{obs:1} and \ref{obs:2}.

% Assuming the MBR decoding goal is the true value, we want to know $\Phuman(h^*) - \Phuman(\hmonte)$, where $\hmonte$ is the optimal probability based on the empirical distribution of $\Pmodel$, $u_h(h) = \sum \Phuman(y)u(h,y)$.
% Remind of $u_h(\human) - u_h(\hmonte)\leq 2 \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} + \frac{12 }{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) 
%         + 2 \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}} = \sigma_1$
% \begin{align}
%     \Phuman(\hm)u_h(\human) - \Phuman(\hm)u_h(\hmonte) &\leq \Phuman(\hm) \cdot \sigma_1.
% \end{align}

% Remind of $\Phuman(h^*) - \Phuman(\hm)\leq 4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right)$.
% \begin{align}
%     \Phuman(h^*)u_h(\hm) - \Phuman(\hm)u_h(\hm)&\leq \underbrace{4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right)}_{\sigma_2}.
% \end{align}


% Combined above formulation:

% \begin{align}
%     &\Phuman(\hm)u_h(\human) - \Phuman(\hm)u_h(\hmonte) \\
%     &+ \Phuman(h^*)u_h(\hm) - \Phuman(\hm)u_h(\hm)\leq  \Phuman(\hm) \sigma_1 +\sigma_2\\
%     &\Phuman(\hm)u_h(\human)- \Phuman(\hm)u_h(\hm) \\
%     &\leq \Phuman(\hm)u_h(\hmonte)-\Phuman(h^*)u_h(\hm)+ \Phuman(\hm) \sigma_1 +\sigma_2\\
%     & u_h(\human) -u_h(\hm) \leq u_h(\hmonte) - u_h(\hm) +\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)} \\
%     & u_h(\human) -u_h(\hm) \leq u_h(\hmonte)-u_m(\hmonte) + u_m(\hmonte) -u_m(\hm) + u_m(\hm) - u_h(\hm)  \\
%     &+\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)} \\
%     & u_h(\human) -u_h(\hm) \leq u_m(\hmonte) -u_m(\hm)+ 2\Umax \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}  +\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)}. \\
%     &  u_h(\human) -u_h(\hm) \leq u_m(\hmonte) -u_m(\hm)+ O\left(\max \left(\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{D}}\right)\right)
% \end{align}

% -------
% assume map true value
% We can get the bound.
% \begin{align}
%     \Phuman(\hmonte)u_h(\human) - \Phuman(\hmonte)u_h(\hmonte) &\leq \Phuman(\hmonte) \cdot \sigma_1.
% \end{align}

% \begin{align}
%     \Phuman(h^*)u_h(\hmonte) - \Phuman(\hm)u_h(\hmonte)&\leq u_h(\hmonte)\cdot \sigma_2. 
% \end{align}
% Combined above formulation:
% \begin{align*}
%     \Phuman(h^*)u_h(\hmonte) - \Phuman(\hmonte)u_h(\hmonte)&\leq  \Phuman(\hmonte) \sigma_1 + u_h(\hmonte)\sigma_2 + \Phuman(\hmc)u_h(\hmonte) \\
%     &- \Phuman(\hmonte)u_h(\human)\\
%     \Phuman(h^*) - \Phuman(\hmonte) &\leq \Phuman(\hmc)- \Phuman(\hmonte) \frac{u_h(\human)}{u_h(\hmonte)} + \frac{\Phuman(\hmonte)}{u_h(\hmonte)}\sigma_1 + \sigma_2\\
%     \Phuman(h^*) - \Phuman(\hmonte) &\leq \Phuman(\hmc) -\Pmodel(\hm)+\Pmodel(\hm) -\Pmodel(\hmonte) \\
%     &+ \Pmodel(\hmonte) - \Phuman(\hmonte)  + \frac{\sigma_1}{u_h(\hmonte)} + \sigma_2\\
%     \Phuman(h^*) - \Phuman(\hmonte) &\leq 4\sqrt{\frac{1}{2|D|}\log \frac{8}{\delta}} + \frac{\sigma_1}{u_h(\hmonte)} + \sigma_2 + \Pmodel(\hm) -\Pmodel(\hmonte)\\
%     \Phuman(h^*) - \Phuman(\hmonte) &\leq O\left(\max \left(\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{D}}\right)\right)+ \Pmodel(\hm) -\Pmodel(\hmonte)\\
% \end{align*}
\subsection{Proof of Observation 1}\label{appendix:observation1}


Assuming the MBR decoding goal is the true value, we aim to know $\Phuman(h^*) - \Phuman(\hmonte)$, where $\hmonte$ is the optimal probability based on the empirical distribution of $\Pmodel$, $u_h(h) = \sum \Phuman(y)u(h,y)$.
% Remind of $u_h(\human) - u_h(\hmonte)\leq 2 \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} + \frac{12 }{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right) 
%         + 2 \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}} = \sigma_1$
Remind of $u_h(\human) - u_h(\hmonte)\leq 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} 
        + \frac{36 }{n}\sqrt{d \log  d} = \sigma_1$
\begin{align*}
    \Phuman(\hm)u_h(\human) - \Phuman(\hm)u_h(\hmonte) &\leq \Phuman(\hm) \cdot \sigma_1.
\end{align*}

Remind of $\Phuman(h^*) - \Phuman(\hm)\leq 4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right)$.
% \begin{align*}
%     \Phuman(h^*)u_h(\hm) - \Phuman(\hm)u_h(\hm)&\leq \underbrace{4\left(\sqrt{\frac{1}{n}} + \sqrt{\frac{1}{|D|}}\right)\left(\sqrt{\frac{1}{2}\log\frac{8}{\delta}} \right)}_{\sigma_2}.
% \end{align*}
\begin{align*}
    \Phuman(h^*)u_h(\hm) - \Phuman(\hm)u_h(\hm)&\leq \underbrace{8\sqrt{\frac{1}{n}\log\frac{1}{\delta}}+8\sqrt{\frac{1}{|D|}\log\frac{1}{\delta}}}_{\sigma_2}.
\end{align*}


Combined above formulation:

\begin{align*}
    \Phuman(\hm)u_h(\human) &- \Phuman(\hm)u_h(\hmonte) + \Phuman(h^*)u_h(\hm) - \Phuman(\hm)u_h(\hm)\\&\leq  \Phuman(\hm) \sigma_1 +\sigma_2,\\
    &\Phuman(\hm)u_h(\human)- \Phuman(\hm)u_h(\hm) \\
    &\leq \Phuman(\hm)u_h(\hmonte)-\Phuman(h^*)u_h(\hm)+ \Phuman(\hm) \sigma_1 +\sigma_2,
    \end{align*}
    \begin{align*}
    & u_h(\human) -u_h(\hm) \leq u_h(\hmonte) - u_h(\hm) +\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)} ,\\
    & \leq u_h(\hmonte)-u_m(\hmonte) + u_m(\hmonte) -u_m(\hm) + u_m(\hm) - u_h(\hm)  \\
    &+\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)} ,\\
    &  \leq u_m(\hmonte) -u_m(\hm)+ 2\sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}  +\sigma_1 +  \frac{\sigma_2}{\Phuman(\hm)}, \\
    &  \leq u_m(\hmonte) -u_m(\hm)+ O\left(\max \left(\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{D}}\right)\right).
\end{align*}

\paragraph{From this, we assume that the MAP decoding target is the true value.}


% We can get the bound.
\begin{align*}
    \Phuman(\hmonte)u_h(\human) - \Phuman(\hmonte)u_h(\hmonte) &\leq \Phuman(\hmonte) \cdot \sigma_1.
\end{align*}
\begin{align*}
    \Phuman(\maphuman)u_h(\hmonte) - \Phuman(\hm)u_h(\hmonte)&\leq u_h(\hmonte)\cdot \sigma_2. 
\end{align*}
Combined above formulation:
\begin{align*}
    \Phuman(\maphuman)u_h(\hmonte) - \Phuman(\hmonte)u_h(\hmonte)&\leq  \Phuman(\hmonte) \sigma_1 + u_h(\hmonte)\sigma_2 + \Phuman(\hm)u_h(\hmonte) \\
    &- \Phuman(\hmonte)u_h(\human)
    \end{align*}
    \begin{align*}
    \Phuman(\maphuman) - \Phuman(\hmonte) &\leq \Phuman(\hm)- \Phuman(\hmonte) \frac{u_h(\human)}{u_h(\hmonte)} + \frac{\Phuman(\hmonte)}{u_h(\hmonte)}\sigma_1 + \sigma_2\\
     &\leq \Phuman(\hm) -\Pmodel(\hm)+\Pmodel(\hm) -\Pmodel(\hmonte) \\
    &+ \Pmodel(\hmonte) - \Phuman(\hmonte)  + \frac{\sigma_1}{u_h(\hmonte)} + \sigma_2\\
 &\leq 4\sqrt{\frac{1}{2|D|}\log \frac{8}{\delta}} + \frac{\sigma_1}{u_h(\hmonte)} + \sigma_2 + \Pmodel(\hm) -\Pmodel(\hmonte)\\
   &\leq O\left(\max \left(\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{D}}\right)\right)+ \Pmodel(\hm) -\Pmodel(\hmonte).\\
\end{align*}
\subsection{Proof of Observation 2}\label{appendix:observation2}
% We assume $\Umax = 1$
Remid of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$ and $\operatorname{Regret}_{n,D}$ 's upper bound:
% \begin{align*}
%    \mathrm{Regret}^\mathrm{MAP}_{n,D}&\leq \underbrace{4\sqrt{\frac{1}{2n}\log \frac{8}{\delta}} + 4\sqrt{\frac{1}{2|D|}\log \frac{8}{\delta}}}_{\phi_1}
%    \end{align*}
\begin{align*}
   \mathrm{Regret}^\mathrm{MAP}_{n,D}&\leq \underbrace{ 8\sqrt{\frac{1}{n}\log\frac{1}{\delta}}+8\sqrt{\frac{1}{|D|}\log\frac{1}{\delta}}}_{\phi_1}.
   \end{align*}
% \begin{align*}
% & \operatorname{Regret}_{n,D}  \leq  \underbrace{2\sqrt{\frac{1}{2 n} \log \frac{8}{\delta}} +\frac{12}{n}(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}) 
%  +2  \sqrt{\frac{1}{2|D|} \log \frac{8}{\delta}}}_{\phi_2}
% \end{align*}

\begin{align*}
& \operatorname{Regret}_{n,D}  \leq  \underbrace{4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} 
        + \frac{36 }{n}\sqrt{d \log  d}}_{\phi_2}.
\end{align*}


\begin{align*}
    &\phi_1- \phi_2 = 4 \sqrt{\frac{1}{n} \log \frac{1}{\delta}}+4\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}} 
        - \frac{36 }{n}\sqrt{d \log  d}.
\end{align*}

From the above inequality, we can derive this observation.
\begin{itemize}
    \item $n\rightarrow \infty$ and $D$ is finite, $\mathrm{Regret}_{n,D}$ < $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.
    \item $D$ is infinite, $n$ is finite, $\mathrm{Regret}_{n,D}$ < $\mathrm{Regret}^\mathrm{MAP}_{n,D}$.
    \begin{align*}
    \frac{1}{9}\sqrt{n\log{\frac{1}{\delta}}} \geq \sqrt{d \log d}.
\end{align*}
\item $D$ is finite and $n$ is finite, 
the upper bound of $\mathrm{Regret}_{n,D}$ remains less than or equal to the upper bound of $\mathrm{Regret}^\mathrm{MAP}_{n,D}$, provided the following condition holds:
\begin{align*}
    % n\sqrt{\frac{1}{72}\log\frac{8}{\delta}}\left(\frac{1}{\sqrt{n}}+ \frac{1}{\sqrt{D}}\right)&\geq \sqrt{d \log (2 \sqrt{d})} +
    % 2 \sqrt{d}.
    \frac{n}{9}\left(\sqrt{\frac{1}{n} \log \frac{1}{\delta}}+\sqrt{\frac{1}{|D|} \log \frac{1}{\delta}}\right) &\geq \sqrt{d \log d}.
\end{align*}
\end{itemize}

% \section{Difference Utility function}\label{appendix:utilit_diff}
%  The difference in expectation is:
% \begin{align*}
%     &\mathbb{E}\left[u(y, y')\right] 
%     \;-\; \mathbb{E}\left[u'(y, y')\right]\\
%     &= \mathbb{E}\left[\ba(y)^{\top}\bv(y')\right]
%        - \mathbb{E}\left[\ba'(y)^{\top}\bv(y')\right]\\
%     &= \mathbb{E}\left[(\,\ba(y)
%       - \ba'(y)\,)^{\top} \bv(y')\right].
% \end{align*}
% By applying the Cauchy–Schwarz inequality, we obtain:
% \begin{align*}
%     &\E[u(\human,y')] - \E[u^\prime(\human,y')] \\
%     &\leq \|\ba(\human)-\ba^{\prime}(\human)\|\cdot \|\E[\bv(y')]\|\\
%     &\leq \|\ba(\human)-\ba^{\prime}(\human)\|
% \end{align*}
% \newpage
\section{Observation~\ref{obs:2} Experiment}\label{appendix:observation_exp}
This result shows that the upper bound of MBR decoding is less than the upper bound of MAP decoding at the number of samples described in Observation~\ref{obs:2}'s 3).
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{img/Compare.pdf}
    \caption{Numerical Experiment for Observation~\ref{obs:2}'s 3}
    \label{fig:convergence-rate}
\end{figure}
% \section{$\Phuman = \Pmodel$}\label{appendix:human_equal_model}
% % \begin{maincoro}\label{theorem:bound2}
% If $\Phuman$ and $\Pmodel$ are equal, the upper bound of $\mathrm{Regret}_{n}$ corresponds to Lemma~\ref{lemma:heart}:
%     \begin{align*}
%     &\mathrm{Regret}_{n} \leq  2\Umax \sqrt{\frac{1}{2n} \log \frac{8}{\delta}} + \frac{12 \Umax}{n}\left(\sqrt{d \log (2 \sqrt{d})}+2 \sqrt{d}\right).
% \end{align*}
% % \end{maincoro}
% In most studies, the primary goal of MBR decoding studies is to derive $\hmodel$, given that $\Phuman$ is inaccessible. These studies implicitly assume $\Pmodel=\Phuman$, highlighting the significance of the results. This finding is for understanding and improving the practical application of MBR decoding methods.

\section{Experimental Details of Numerical Simulation (Section~\ref{sec:experiments})}\label{appendix:exp}
$\Phuman$ is non-uniform distribution (reflecting real-world biases), for each seed, we generate $\Phuman$ via i.i.d. sampling, then form the empirical model distribution $\Pmodel$ by drawing $D$ times from $\Phuman$ (i.e, $\widehat{P}$ represents hypothesis frequencies).
In the experiment setting of Fig.~\ref{fig:wasserstein_distance}, we applied the utility function according to Assumption~\ref{assumption:utility}, and in  Fig.~\ref{fig:fix_d} and Fig.~\ref{fig:fix_n}, we assume a symmetric utility matrix $u\in\mathbb{R}^{\mathcal{Y}\times\mathcal{Y}}$ with $u(i,i)=1$ and $u(i,j)\in[0,1]$ for $i\neq j$, assigning slightly higher utilities to outcomes with higher $\Phuman$ probabilities.

% \begin{figure*}[htb]
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{img/regret_bound_heart.pdf}
%          \caption{}
%          \label{fig:heart}
%      \end{subfigure}
%      \hspace{12pt}
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{img/regret_bound_clubs.pdf}
%          \caption{)}
%          \label{fig:club}
%      \end{subfigure}
%      \vspace{-4mm}
%     \caption{}
%     \label{}
% \end{figure*}
\end{document}