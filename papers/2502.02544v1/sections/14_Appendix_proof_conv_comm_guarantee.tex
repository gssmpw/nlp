\section{Proof of~\texorpdfstring{\cref{thm:conv}}{Theorem Reference} and Convergence-communication Guarantees for IW-ERM with VRLS\label{app:conv}}

We now establish convergence rates for IW-ERM with VRLS and show our proposed importance weighting achieves {\it the same rates} with the data-dependent {\it constant terms} increase linearly with $\max_{y \in \mathcal{Y}}\sup_f r_f(y)=r_{\max}$ under negligible communication overhead over the baseline  ERM-solvers without importance weighting. In~\cref{app:conv}, we establish tight convergence rates and communication guarantees for IW-ERM with VRLS in a broad range of importance optimization settings including convex optimization, second-order differentiability, composite optimization with proximal operator, optimization with adaptive step-sizes, and nonconvex optimization, along the lines of~e.g.,~\citep{woodworth2020local,haddadpour2021federated,glasgow2022sharp,liu2023high,Prox,AdaptiveFL,liu2023high}. 

By estimating the ratios locally and absorbing into local losses, we note that the properties of the modified local loss w.r.t. the neural network parameters $\boldsymbol{w}$, e.g., convexity and smoothness, do not change. The data-dependent parameters such as Lipschitz and smoothness constants for $\ell\circ h_{\boldsymbol{w}}$ w.r.t. $\boldsymbol{w}$ are scaled linearly by $r_{\max}$. Our method of density ratio estimation trains the pre-defined predictor {\it exclusively using local training data}, which implies IW-ERM with VRLS achieves the same privacy guarantees as the baseline  ERM-solvers without importance weighting. For ratio estimation, the communication between clients involves only the estimated marginal label distribution, instead of data, ensuring  negligible communication overhead. 
Given the size of variables to represent marginal distributions, which is by orders of magnitude smaller than the number of parameters of the underlying neural networks for training and the fact that ratio estimation involves only one round of communication, the overall communication overhead for ratio estimation is masked by the communication costs of model training. The communication costs for IW-ERM with VRLS over the course of optimization are exactly the same as those of the baseline  ERM-solvers without importance weighting. All in all, importance weighting does not negatively impact  communication guarantees throughout the course of optimization, which proves~\cref{thm:conv}.

In the following,  we establish tight convergence rates and communication guarantees for IW-ERM with VRLS in a broad range of importance optimization settings including convex optimization, second-order differentiability, composite optimization with proximal operator, optimization with adaptive step-sizes, and nonconvex optimization. 


For convex and second-order Differentiable optimization, we establish a lower bound on the  convergence rates for IW-ERM in with VRLS and local updating along the lines of~e.g.,~\citep[Theorem 3.1]{glasgow2022sharp}.


\begin{assumption}[PL with Compression]
\label{assumption:PL} 1) The $\ell(h_{\boldsymbol{w}}(\boldsymbol{x}),y)$ is $\beta$-smoothness and convex w.r.t. $\boldsymbol{w}$ for any $(\boldsymbol{x},y)$ and satisfies Polyak-{\L}ojasiewicz (PL) condition (there exists $\alpha_{\ell} >0$ such that, for all $\boldsymbol{w}\in\mathcal{W}$, we have 
$\ell(h_{\boldsymbol{w}})\le  {\| \nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}}) \|_2^2}/{(2\alpha_{\ell})}$; 2) The compression scheme $\mathcal{Q}$ is unbiased with bounded variance, i.e., $\E[\mathcal{Q}(\boldsymbol{x})]=\boldsymbol{x}$ and $\E[\|\mathcal{Q}(\boldsymbol{x})-\boldsymbol{x}\|_2^2\leq q\|\boldsymbol{x}\|_2^2]$; 3) The stochastic gradient $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased, i.e., $\E[\boldsymbol{g}(\boldsymbol{w})]=\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ for any $\boldsymbol{w}\in\mathcal{W}$ with bounded variance  $\E[\|\boldsymbol{g}(\boldsymbol{w})-\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})\|_2^2]$.
\end{assumption}


For nonconvex optimization with PL condition and communication compression, we establish convergence and communication guarantees  for IW-ERM with VRLS, compression,  and local updating along the lines of~e.g.,~\citep[Theorem 5.1]{haddadpour2021federated}.

\begin{theorem}
[Convergence and Communication Bounds for Nonconvex Optimization with PL]\label{app:PL} Let $\kappa$ denote the condition number, $\tau$ denote the number of local steps,  $R$ denote the number of communication rounds, and $\max_{y\in\mathcal{Y}}\sup_f r_f(y)=r_{\max}$. Under~\cref{assumption:PL}, suppose~\cref{alg:IWERM_detail} with $\tau$ local updates and communication compression~\citep[Algorithm 1]{haddadpour2021federated} is run for $T=\tau R$ total stochastic gradients per node with fixed step-sizes $\eta=1/(2r_{\max}\beta\gamma\tau(q/K+1))$ and $\gamma\geq K$. Then we have $\E[\ell(h_{\boldsymbol{w}_T})-\ell(h_{\boldsymbol{w}^\star})]\leq\epsilon$ by setting 
\begin{align}
R\lesssim \Big(\frac{q}{K}+1\Big)\kappa\log\Big(\frac{1}{\epsilon}\Big) \quad\text{and} \quad \tau\lesssim\Big(\frac{q+1}{K(q/K+1)\epsilon}\Big). 
\end{align}
\end{theorem}

\begin{assumption}[Nonconvex Optimization with Adaptive Step-sizes]
\label{assumption:adaptive} 1) The $\ell\circ h_{\boldsymbol{w}}$ is $\beta$-smoothness with bounded gradients; 2) The stochastic gradients $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased with bounded variance $\E[\|\boldsymbol{g}(\boldsymbol{w})-\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})\|_2^2]$; 3) Adaptive matrices $A_t$ constructed as in~\citep[Algorithm 2]{AdaptiveFL} are diagonal and the minimum eigenvalues satisfy $\lambda_{\min}(A_t) \geq \rho >0$ for some $\rho\in\mathbb{R}_+$.%
\end{assumption}

For nonconvex optimization with adaptive step-sizes, we establish convergence and communication guarantees for IW-ERM with VRLS and local updating along the lines of~e.g.,~\citep[Theorem 2]{AdaptiveFL}.


\begin{theorem}[Convergence and Communication Guarantees for  Nonconvex Optimization with Adaptive Step-sizes]\label{app:adaptive} Let $\tau$ denote the number of local steps,  $R$ denote the number of communication rounds, and $\max_{y\in\mathcal{Y}}\sup_f r_f(y)=r_{\max}$. Under~\cref{assumption:adaptive}, suppose~\cref{alg:IWERM_detail} with $\tau$ local updates is run for $T=\tau R$ total stochastic gradients per node with an adaptive step-size similar to~\citep[Algorithm 2]{AdaptiveFL}. Then we $\mathbb{E}[\|\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}_T})\|_2] \leq \epsilon$ by setting: 
\begin{align}
T\lesssim  \frac{r_{\max}}{K\epsilon^3}\quad \text{and} \quad R\lesssim\frac{r_{\max}}{\epsilon^2}. 
\end{align}
\end{theorem}


\begin{assumption}[Composite Optimization with Proximal Operator]\label{assumption:proxy} 1) The $\ell\circ h_{\boldsymbol{w}}$ is smooth and strongly convex with condition number $\kappa$; 2) The stochastic gradients $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased.
\end{assumption}

For composite optimization with strongly convex and smooth functions and proximal operator, we establish an upper bound on oracle complexity to achieve $\epsilon$ error on the  Lyapunov function defined as in~\citep[Section 4]{Prox}  for Gradient Flow-type transformation of IW-ERM with VRLS in the limit of infinitesimal step-size.

\begin{theorem}[Oracle Complexity of Proximal Operator for Composite Optimization]\label{app:proxy} Let $\kappa$ denote the condition number.  Under~\cref{assumption:proxy}, suppose Gradient Flow-type transformation of ~\cref{alg:IWERM_detail} with VRLS and Proximal Operator evolves in the limit of infinitesimal step-size ~\citep[Algorithm 3]{Prox}.
Then it achieves $\mathcal{O}\big(r_{\max}\sqrt{\kappa}\log(1/\epsilon)\big)$ Proximal Operator Complexity.
\end{theorem}
