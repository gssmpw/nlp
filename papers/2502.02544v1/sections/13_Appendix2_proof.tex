\section{Proof of~\texorpdfstring{\cref{thm:est}}{Theorem Reference}\label{app:thm:est}}
\begin{proof}
    Let $H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) = -\log(f(\boldsymbol{x}, \boldsymbol{\theta})^\top \boldsymbol{r})$. From the strong convexity in \cref{lem:popcvx}, we have that
    \begin{align}
    \label{eq:wbound}
    \| \hat{\boldsymbol{r}}_{{n}^{\text{te}}} - \boldsymbol{r}_{f^\star} \|_2^2 \leq \frac{2}{\mu p_{\min}}\left( \mathcal{L}_{\boldsymbol{\theta}^\star}(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}) - \mathcal{L}_{\boldsymbol{\theta}^\star}(\boldsymbol{r}_{f^\star}) \right)
    \end{align}

    Now focusing on the term on the right-hand side, we find by invoking \cref{lem:lipschitz} that
    \begin{align}
    &\mathcal{L}_{\boldsymbol{\theta}^\star}(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}) - \mathcal{L}_{\boldsymbol{\theta}^\star}(\boldsymbol{r}_{f^\star}) \nonumber \\
    &\leq \E\bigg[ H(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}) \bigg] 
    - \E\bigg[ H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}) \bigg]
    + 2L \E\bigg[\|\hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}} - \boldsymbol{\theta}^\star\|_2 \bigg] \nonumber\\ 
    &= \E\bigg[H(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, x) \bigg] 
    - \frac{1}{{n}^{\text{te}}}\sum_{j=1}^{{n}^{\text{te}}}H(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}, \hat{\boldsymbol{\theta}}_{{{n}^{\text{tr}}}}, \boldsymbol{x}_j)
    + \frac{1}{{n}^{\text{te}}}\sum_{j=1}^{{n}^{\text{te}}}H(\hat{\boldsymbol{r}}_{n}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}_j) \nonumber\\ 
     &\quad\quad\quad\quad\quad\quad\quad\quad\quad -\E\bigg[ H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}) \bigg]
    + 2L \E\bigg[\|\hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}} - \boldsymbol{\theta}^\star\|_2 \bigg] \nonumber\\
    &\leq \E\bigg[H(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x})\bigg] 
    - \frac{1}{{n}^{\text{te}}}\sum_{j=1}^{{n}^{\text{te}}}H(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}_j) 
    + \frac{1}{{n}^{\text{te}}}\sum_{j=1}^{{n}^{\text{te}}}H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}_j) \nonumber\\ 
    &\quad\quad\quad\quad\quad\quad\quad\quad\quad -\E\bigg[ H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}, \boldsymbol{x}) \bigg]
    + 2L \E\bigg[\|\hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}} - \boldsymbol{\theta}^\star\|_2 \bigg], \nonumber\\
    \end{align}
    where in the last inequality we used the fact that $\hat{\boldsymbol{r}}_{n}$ is a minimizer of $\boldsymbol{r} \mapsto \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}_j)$. Finally by using \cref{lem:rad1} and \cref{lem:rad2} with $\delta/2$ each, we have that with probability $1-\delta$,
    \begin{equation}
    \begin{aligned}
    \mathcal{L}_{\boldsymbol{\theta}^\star}(\hat{\boldsymbol{r}}_{{n}^{\text{te}}}) - \mathcal{L}_{\boldsymbol{\theta}^\star}(\boldsymbol{r}_{f^\star}) \leq 
    &\frac{4}{\sqrt{{n}^{\text{te}}}} \text{Rad}(\mathcal{F}) +2L \E\bigg[\|\hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}} - \boldsymbol{\theta}^\star\|_2 \bigg] + 4B\sqrt{\frac{\log(4/\delta)}{{n}^{\text{te}}}}
    \end{aligned}
    \end{equation}
    Plugging this back into \cref{eq:wbound}, we have that
    \begin{equation}
    \begin{aligned}
    \|\hat{\boldsymbol{r}}_{{n}^{\text{te}}} - \boldsymbol{r}_{f^\star}\|_2^2 &\leq \frac{2}{\mu p_{\min}}\left( \frac{4}{\sqrt{{n}^{\text{te}}}} \text{Rad}(\mathcal{F}) + 4B\sqrt{\frac{\log(4/\delta)}{{n}^{\text{te}}}} \right) + \frac{4L}{\mu p_{\min}} \mathbb{E}\left[\|\hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}} - \boldsymbol{\theta}^\star\|_2\right].
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}
    \label{lem:upperbound}
    For any $\boldsymbol{r} \in \mathbb{R}_{+}^m,\; \boldsymbol{\theta} \in \Theta,\; \boldsymbol{x} \in \mathcal{X}$, we have that
    \[
    \boldsymbol{r}^\top  f(\boldsymbol{x}, \boldsymbol{\theta}) \leq \frac{1}{p_{min}}.
    \]
\end{lemma}
\begin{proof}
    Applying H\"{o}lder's inequality we have that
    \[
    \boldsymbol{r}^\top  f(\boldsymbol{x}, \boldsymbol{\theta}) \leq  \|\boldsymbol{r}\|_{\infty} \|f(\boldsymbol{x}, \boldsymbol{\theta})\|_1 = \|\boldsymbol{r}\|_{\infty}.
    \]
    Moreover, since $\boldsymbol{r} \in \mathbb{R}_{+}^m$, we have that
    \(
    \sum_y r_y p_{tr}(y) = 1
    \)
    This implies that $\|\boldsymbol{r}\|_{\infty} \leq \frac{1}{p_{\min}}$, which yields the result.
\end{proof}



\begin{lemma}[Implication of Assumption \cref{assumption:bounded}]
\label{lem:termbound}
    Under \cref{assumption:bounded}, there exists $B>0$ such that for any $\boldsymbol{r} \in \mathbb{R}_{+}^m,\; \boldsymbol{\theta} \in \Theta,\; \boldsymbol{x} \in \mathcal{X}$,
    \[
        |\log(\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}))| \leq B.
    \]
\end{lemma}
\begin{proof}
    Since $\boldsymbol{r} \in \mathbb{R}_{+}^m$, it has at least one non-zero coordinate and $f(\boldsymbol{x}, \boldsymbol{\theta})$ is the output of a softmax layer so all of its coordinates are non-zero. Consequently,
    \[
    \boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}) > 0
    \]
    So by \cref{assumption:bounded}, the function $(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) \mapsto \log(\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}))$ is defined and continuous over a compact set, so there exists a constant $B$ giving us the result. 
\end{proof}

\begin{lemma}[Population Strong Convexity]
\label{lem:popcvx_con} 
Let $H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) = -\log(\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}))$. 
Under Assumption \cref{assumption:calibration}, the function 
\[
\mathcal{L}_{\boldsymbol{\theta}^\star}: \boldsymbol{r} \mapsto \mathbb{E}\bigg[H(\boldsymbol{r}, \boldsymbol{\theta}^\star, \boldsymbol{x})\bigg]
\]
is $\mu p_{\min}$-strongly convex.
\end{lemma}
\begin{proof}
    We first compute the Hessian of $\mathcal{L}$ to find that
    \[
    \nabla^2 \mathcal{L}(\boldsymbol{r}) = \mathbb{E}\bigg[\frac{1}{(\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}^\star))^2}f(\boldsymbol{x}, \boldsymbol{\theta}^\star) f(\boldsymbol{x}, \boldsymbol{\theta}^\star)^\top \bigg].
    \]
    Since by \cref{lem:upperbound}, we have that $\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}^\star) \leq p_{\min}^{-1}$, we conclude that
    \[
    \nabla^2 \mathcal{L}(\boldsymbol{r}) \succeq p_{\min}  \mathbb{E}\bigg[f(\boldsymbol{x}, \boldsymbol{\theta}^\star) f(\boldsymbol{x}, \boldsymbol{\theta}^\star)^\top \bigg] \succeq \mu p_{\min} \mathbf{I}_m.
    \]
\end{proof}

\begin{lemma}[Lipschitz Parametrization]
\label{lem:lipschitz}
    Let $H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) = -\log(f(\boldsymbol{x}, \boldsymbol{\theta})^\top \boldsymbol{r})$. There exists $L > 0$ such that for any $\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \Theta$, and $\boldsymbol{r} \in \mathbb{R}_{+}^m$, we have that
    \[
    |H(\boldsymbol{r}, \boldsymbol{\theta}_1, \boldsymbol{x}) -  H(\boldsymbol{r}, \boldsymbol{\theta}_2, \boldsymbol{x})| \leq L \|\boldsymbol{\theta}_1 - \boldsymbol{\theta}_2\|_2.
    \]
\end{lemma}
\begin{proof}
    The gradient of $H$ with respect to $\boldsymbol{\theta}$ is given by
    \[
    \nabla_{\boldsymbol{\theta}} H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) = -\frac{1}{f(\boldsymbol{x}, \boldsymbol{\theta})^\top \boldsymbol{r}} \nabla_{\boldsymbol{\theta}}f(\boldsymbol{x}, \boldsymbol{\theta})
    \]
    Reasoning like in \cref{lem:upperbound}, we know that $\frac{1}{f(\boldsymbol{x}, \boldsymbol{\theta})^\top \boldsymbol{r}}$ is defined and continuous over the compact set of its parameters, we also know that $f$ is a neural network parametrized by $\boldsymbol{\theta}$, hence $\nabla_{\boldsymbol{\theta}}f(\boldsymbol{x}, \boldsymbol{\theta})$ is bounded when $\boldsymbol{\theta}$ and $\boldsymbol{x}$ are bounded. Consequently, under \cref{assumption:bounded}, there exists a constant $L > 0$ such that
    \[
    \|\nabla_{\boldsymbol{\theta}} H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x})\|_2 \leq L.
    \]
\end{proof}


\begin{lemma}[Uniform Bound 1]
\label{lem:rad1}
    Let $\delta \in (0,1)$, with probability $1-\delta$, we have that
    \begin{equation}
    \begin{aligned}
    &\mathbb{E}\bigg[H(\hat{\boldsymbol{r}}_{n}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}) \bigg] - \frac{1}{n}\sum_{j=1}^{n}H(\hat{\boldsymbol{r}}_{n}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}_j) \\
    & \leq \frac{2}{\sqrt{n}} \text{Rad}(\mathcal{F}) + 2B\sqrt{\frac{\log(4/\delta)}{n}}.
    \end{aligned}
    \end{equation}
\end{lemma}

\begin{proof}
    Let $\delta \in (0,1)$. Since $\hat{\boldsymbol{r}}_{n}$ is learned from the samples $\boldsymbol{x}_j$, we do not have independence, which would have allowed us to apply a concentration inequality. Hence, we derive a uniform bound as follows. We begin by observing that:
    \[
    \begin{aligned}
        &\mathbb{E}\bigg[H(\hat{\boldsymbol{r}}_{n}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x})\bigg] - \frac{1}{n}\sum_{j=1}^{n}H(\hat{\boldsymbol{r}}_{n}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}_j) \\
        &\leq \sup_{\boldsymbol{r}, \boldsymbol{\theta}} \left(\mathbb{E}\bigg[H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x})\bigg] - \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}_j)\right)
    \end{aligned}
    \]
    Now since \cref{lem:termbound} holds, we can apply McDiarmid's Inequality to get that with probability $1-\delta$, we have:
    \begin{align*}
    &\sup_{\boldsymbol{r}, \boldsymbol{\theta}} \left(\mathbb{E}\bigg[ H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) \bigg] - \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}_j)\right) \\
    &\leq \mathbb{E}\bigg[\sup_{\boldsymbol{r}, \boldsymbol{\theta}} \left( \mathbb{E}\big[H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) \big] - \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}_j) \right)\bigg] + 2B\sqrt{\frac{\log(2/\delta)}{n}}
    \end{align*}
    The expectation of the supremum on the right-hand side can be bounded by the Rademacher complexity of  $\mathcal{F} := \{ \boldsymbol{x} \mapsto \boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}), \; (\boldsymbol{r}, \boldsymbol{\theta}) \in\mathbb{R}_{+}^m\times\Theta\}$, and we obtain:
    \begin{equation}
    \begin{aligned}
    &\sup_{\boldsymbol{r}, \boldsymbol{\theta}} \left(\mathbb{E}\big[H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}) \big] - \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}, \boldsymbol{\theta}, \boldsymbol{x}_j)\right) \\
    &\leq \frac{2}{\sqrt{n}} \text{Rad}(\mathcal{F}) + 2B\sqrt{\frac{\log(2/\delta)}{n}}.
    \end{aligned}
    \end{equation}
\end{proof}

\begin{lemma}[Uniform Bound 2]
\label{lem:rad2}
    Let $\delta \in (0,1)$, with probability $1-\delta$, we have that
    \begin{equation}
    \begin{aligned}
    &\mathbb{E}\bigg[H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}) \bigg] - \frac{1}{n}\sum_{j=1}^{n}H(\boldsymbol{r}_{f^\star}, \hat{\boldsymbol{\theta}}_t, \boldsymbol{x}_j) \\
    & \leq \frac{2}{\sqrt{n}} \text{Rad}(\mathcal{F}) + 2B\sqrt{\frac{\log(2/\delta)}{n}}.
    \end{aligned}
    \end{equation}
\end{lemma}

\begin{proof}
    The proof is identical to that of \cref{lem:rad1}.
\end{proof}

\begin{lemma}[Strong Convexity of Population Loss]
\label{lem:popcvx}
    Let $\mathcal{L}(\boldsymbol{r}, \boldsymbol{\theta})$ be the population loss as defined in \cref{lem:popcvx}. We establish that $\mathcal{L}(\boldsymbol{r}, \boldsymbol{\theta})$ is $\mu p_{\min}$-strongly convex under the assumptions of calibration (\cref{assumption:calibration}).
\end{lemma}

\begin{proof}
    We compute the Hessian of the population loss $\mathcal{L}$ as in \cref{lem:popcvx}, obtaining that:
    \[
    \nabla^2 \mathcal{L}(\boldsymbol{r}) = \mathbb{E}\bigg[\frac{1}{(\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}))^2} f(\boldsymbol{x}, \boldsymbol{\theta}) f(\boldsymbol{x}, \boldsymbol{\theta})^\top\bigg].
    \]
    From \cref{lem:upperbound}, we have that $\boldsymbol{r}^\top f(\boldsymbol{x}, \boldsymbol{\theta}) \leq p_{\min}^{-1}$. Therefore, we conclude:
    \[
    \nabla^2 \mathcal{L}(\boldsymbol{r}) \succeq p_{\min} \mathbb{E}\bigg[f(\boldsymbol{x}, \boldsymbol{\theta}) f(\boldsymbol{x}, \boldsymbol{\theta})^\top\bigg] \succeq \mu p_{\min} \mathbf{I}_m.
    \]
\end{proof}

\begin{lemma}[Bound on Empirical Loss]
\label{lem:empcvx}
    Under \cref{assumption:bounded}, the empirical loss $\mathcal{L}_{{n}^{\text{te}}}(\boldsymbol{r}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}})$ satisfies the following concentration bound:
    \[
    \mathbb{P}\left( \sup_{\boldsymbol{r} \in \mathbb{R}_{+}^m} \left| \mathcal{L}_{{n}^{\text{te}}}(\boldsymbol{r}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}) - \mathcal{L}(\boldsymbol{r}, \hat{\boldsymbol{\theta}}_{{n}^{\text{tr}}}) \right| > \epsilon \right) \leq 2\exp\left(-c {n}^{\text{te}} \epsilon^2\right).
    \]
\end{lemma}

\begin{proof}
    This result follows from standard concentration inequalities, such as McDiarmid's inequality, together with the Lipschitz continuity of the loss function $\mathcal{L}$ with respect to the samples.
\end{proof}

