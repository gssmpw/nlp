\section{Introduction}\label{sec:intro}

The classical learning theory relies on the assumption that data samples, during training and testing, are {\it independently and identically distributed (i.i.d.)} drawn from an unknown distribution. However, this {\it i.i.d.} assumption is often overly idealistic in real-world settings, where the distributions of training and testing samples can differ significantly and change dynamically as the operational environment evolves. 
In distributed learning~\citep{pmlr-v162-kim22a, wen2023survey, ye2023heterogeneous, 10203330}, where nodes retain their own data without sharing, these discrepancies across nodes become more pronounced, further intensifying the learning challenge~\citep{rahman2023federated, wang2023flexifed}.

{\it Label shifts}~\citep{bbse, garg2022OSLS, mani2022unsupervised, zhou2023domain} represent a form of distributional discrepancy that arises when the marginal distribution of labels in the training set differs from that in the test set, i.e., $p^{\text{te}}(\boldsymbol{y}) \neq p^{\text{tr}}(\boldsymbol{y})$, while the conditional distribution of features given labels, $p(\boldsymbol{x}|\boldsymbol{y})$, remains largely stable across both datasets.
Label shifts commonly manifest both \textit{inter-node} and \textit{intra-node}, complicating the learning process in real-world distributed learning scenarios. However, a commonly used learning principle in this distributed setting, empirical risk minimization (ERM)~\citep{10.5555/3666122.3667754}, operates under the assumption that the training and test distributions are identical on each node and across nodes. This overlooks these shifts, failing to account for the statistical heterogeneity across decentralized data sources.
While the current literature~\citep{yin2024optimization} addresses statistical heterogeneity across nodes, it often neglects distribution shifts at test or operation time, which has been a significant challenge in the entire data science over decades.

The primary technical challenge in addressing label shifts lies in the efficient and accurate estimation of the test-to-train density ratios, $p^{\text{te}}(\boldsymbol{y}) / p^{\text{tr}}(\boldsymbol{y}) $, across all labels.
A widely popular solution is Maximum Likelihood Label Shift Estimation (MLLS)~\citep{mlls}, which frames this estimation as a convex optimization problem, akin to the Expectation-Maximization (EM) algorithm~\citep{bbse_2002}. Model calibration refers to the process of ensuring that predicted probabilities reflect the true likelihood of correctness, which is crucial for improving the accuracy of density ratio estimation~\citep{calibration_modern, mlls}.
Bias-Corrected Calibration (BCT)~\citep{AlexandariEM} serves as an efficient calibration method that enhances the EM algorithm within MLLS. 

While BCT and other post-hoc calibration techniques~\citep{on_cali, 10.5555/3454287.3455390, NEURIPS2021_61f3a6db, sun2024minimum} contribute to improved calibration and may potentially improve model performance, their primary focus remains on refining classification outcomes rather than on accurately approximating the true conditional distribution $p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x})$. 
The ``predictor'' in these literature captures the relationship between the input features \( \boldsymbol{x} \) and the corresponding output probabilities across the labels in the discrete label space \( \mathcal{Y} \), with \( |\mathcal{Y}| = m \), which should approximate the true distribution of $p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x})$. 
Despite this goal, training with conventional cross-entropy loss often leads to models that produce predictions that are either highly over-confident or under-confident, resulting in poorly calibrated outputs~\citep{calibration_modern}.
Consequently, the predictor fails to capture the underlying uncertainty inherent in $p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x})$, which limits its effectiveness in estimating density ratios~\citep{AlexandariEM, mlls, guo2020ltf, modern_calib, pereyra2017regularizing, FedAvg}.

To address this limitation, we propose a novel Versatile Robust Label Shift (VRLS) method, specifically designed to improve density ratio estimation for tackling the label shift problem. 
A key idea of our VRLS method is to approximate $p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x})$ in a way that accounts for the inherent uncertainty over the label space $\mathcal{Y}$ for each input $\boldsymbol{x}$. Accordingly, we propose a new objective function incorporating regularization to penalize predictions that lack proper uncertainty calibration. 
We show that training the predictor in this manner significantly reduces estimation error under various label shift conditions.

Building upon our VRLS method, we extend its application to multi-node settings by proposing an Importance Weighted-ERM (IW-ERM) framework. Within the multi-node distributed environment, our IW-ERM aims to find an unbiased estimate of the overall true risk minimizer across multiple nodes with varying label distributions. By effectively addressing both intra-node and inter-node label shifts with generalization guarantees, our framework handles the statistical heterogeneity inherent in decentralized data sources.
Our extensive experiments demonstrate that the IW-ERM framework, which trains predictors exclusively on local node data, significantly improves overall test error. Moreover, it maintains convergence rates and privacy levels comparable to standard ERM methods while achieving minimal communication and computational overhead compared to existing baselines.
Our main contributions are as follows:

\begin{itemize}[leftmargin=0pt]
\item We propose VRLS, which enhances the approximation of the probability distribution \( p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x}) \) by incorporating a novel regularization term based on Shannon entropy~\citep{neo2024maxent}. This regularization leads to more accurate estimation of the test-to-train label density ratio, resulting in improved predictive performance under various label shift conditions.

\item By integrating our VRLS ratio estimation into multi-node distributed learning environment, we achieve performance close to an upper bound that uses true ratios on Fashion MNIST and CIFAR-10 datasets with 5, 100, and 200 nodes. Our IW-ERM framework effectively manages both inter-node and intra-node label shifts while remaining data confined within each node, resulting in up to 20\% improvements in average test error over current baselines.
\item We establish high-probability estimation error bounds for VRLS, as well as high-probability convergence bounds for IW-ERM with VRLS in nonconvex optimization settings (\cref{sec:theory_guarantee}, Appendices \ref{app:thm:est}, \ref{app:conv}). Additionally, we demonstrate that incorporating importance weighting does not negatively impact convergence rates or communication guarantees across various optimization settings.
\end{itemize}
