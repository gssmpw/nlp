
\newpage
\section{BBSE and MLLS family}\label{app:bbse_mlls_family}
\label{IRforNOLS}

In this section, we summarize the contributions of BBSE \citep{bbse} and MLLS \citep{mlls}. Our objective is to estimate the ratio ${p^{\text{te}}(y)}/{p^{\text{tr}}(y)}$. We consider a scenario with $m$ possible label classes, where $y = c$ for $c \in [m]$. Let $\boldsymbol{r}^{\star} = [r^{\star}_{1}, \ldots, r^{\star}_{m}]^{\top}$ represent the true ratios, with each $r^{\star}_{c}$ defined as $r^{\star}_{c} = \frac{p^{\text{te}}(y = c)}{p^{\text{tr}}(y = c)}$ \citep{mlls}. We then define a family of distributions over $\mathcal{Z}$, parameterized by $\boldsymbol{r} = [r_1, \ldots, r_m]^{\top} \in \mathbb{R}^m$, where $r_c$ is the $c$-th element of the ratio vector.
\begin{align}
\begin{split}
p_{\boldsymbol{r}}(\boldsymbol{z}) := \sum_{c=1}^{m} {p^{\text{te}}(\boldsymbol{z}|y=c)} \cdot p^{\text{tr}}(y=c) \cdot r_c \label{p_w_z}
\end{split}
\end{align}
Here, ${r_{c} \geq 0}$ for $c \in [m]$ and $ \sum_{c=1}^{m} r_c \cdot p^{\text{tr}}(y=c) = \sum_{c=1}^{m} p^{\text{te}}(y=c)= 1 $ as constraints. When $\boldsymbol{r} = \boldsymbol{r}^{\star}$, e.g., $r_c = r^{\star}_c$ for $c \in [m]$, we have $p_{\boldsymbol{r}}(\boldsymbol{z}) = p_{\boldsymbol{r}^{\star}}(\boldsymbol{z}) = p^{\text{te}}(\boldsymbol{z})$ \citep{mlls}. So our task is to find $\boldsymbol{r}$ such that
\begin{align}\label{IWpwz_estimate_w*}
\begin{split}
&\sum_{c=1}^{m} {p^{\text{te}}(\boldsymbol{z}|y=c)} \cdot p^{\text{tr}}(y=c) \cdot r_c \boldsymbol{x}\\
&= 
\sum_{c=1}^{m} {p^{\text{tr}}(\boldsymbol{z}, y=c)}\cdot r_c =
p^{\text{te}}(\boldsymbol{z})
\end{split}
\end{align}

\citet{bbse} introduced Black Box Shift Estimation (BBSE) to address this issue. With a pre-trained classifier \( f \) for the classification task, BBSE assumes that the latent space \(\mathcal{Z}\) is discrete and defines \( p(\boldsymbol{z}|\boldsymbol{x}) = \delta_{\argmax f(\boldsymbol{x})} \), where the output of \( f(\boldsymbol{x}) \) is a probability vector (or a simplex) over \( m \) classes. BBSE estimates \( p^{\text{te}}(\boldsymbol{z}|y) \) as a confusion matrix, using both the training and validation data. It calculates \( p^{\text{tr}}(y = c) \) from the training set and \( p^{\text{te}}(\boldsymbol{z}) \) from the test data. The problem then reduces to solving the following equation:

\begin{align}\label{IWpwz_general_equation}
\begin{split}
\boldsymbol{A} \boldsymbol{w} = \boldsymbol{B}
\end{split}
\end{align}
where $\lvert \mathcal{Z} \rvert = m$, ${\boldsymbol{A}} \in \mathbb{R}^{m\times m}$ with ${A}_{jc} = {p^{\text{te}}(z=j|y=c)} \cdot p^{\text{tr}}(y=c)$, and $\boldsymbol{B} \in \mathbb{R}^{m}$ with $B_{j} = p^{\text{te}}(z=j)$ for $c, j \in [m]$.  

The estimation of the confusion matrix in terms of $p^{\text{te}}(\boldsymbol{z}|y)$ leads to the loss of calibration information \citep{mlls}. Furthermore, when defining $\mathcal{Z}$ as a continuous latent space, the confusion matrix becomes intractable since $\boldsymbol{z}$ has infinitely many values. Therefore, MLLS directly minimizes the divergence between $p^{\text{te}}(\boldsymbol{z})$ and $p_{\boldsymbol{r}}(\boldsymbol{z})$, instead of solving the linear system in \cref{IWpwz_general_equation}.

Within the $f$-divergence family, MLLS seeks to find a weight vector $\boldsymbol{r}$ by minimizing the KL-divergence $\KL\left(p^{\text{te}}(\boldsymbol{z}), p_{\boldsymbol{r}}(\boldsymbol{z})\right)=\mathbb{E}_{\text{te}}\left[\log p^{\text{te}}(\boldsymbol{z}) / p_{\boldsymbol{r}}(\boldsymbol{z})\right]$, for $p_{\boldsymbol{r}}(\boldsymbol{z})$ defined in \cref{p_w_z}. Leveraging on the properties of the logarithm, this is equivalent to maximizing the $\log$-likelihood: $\boldsymbol{r}:=\argmax _{\boldsymbol{r} \in \R} \E_{\text{te}}\left[\log p_{\boldsymbol{r}}(\boldsymbol{z})\right]$. Expanding $p_{\boldsymbol{r}}(\boldsymbol{z})$, we have 
\begin{align}
\begin{split}
\mathbb{E}_{\text{te}}\left[\log p_{\boldsymbol{r}}(\boldsymbol{z})\right] &= \mathbb{E}_{\text{te}}\left[\log (\sum_{c=1}^m p^{\text{tr}}(\boldsymbol{z}, y=c) r_c)\right] \\
&= \mathbb{E}_{\text{te}}\left[\log (\sum_{c=1}^m p^{\text{tr}}(y=c \mid \boldsymbol{z}) r_c) + \log p^{\text{tr}}(\boldsymbol{z})\right]. 
\end{split}
\end{align}

Therefore the unified form of MLLS can be formulated as:
\begin{align}
\begin{split}
\boldsymbol{r}:=\underset{\boldsymbol{r} \in \R}{\argmax}~ \E_{\text{te}}\left[\log (\sum_{c=1}^m p^{\text{tr}}(y=c \mid \boldsymbol{z}) r_c)\right] .
\end{split}
\end{align}

This is a convex optimization problem and can be solved efficiently using methods such as EM, an analytic approach, and also iterative optimization methods like gradient descent with labeled training data and unlabeled test data. MLLS defines the $p(\boldsymbol{z}|\boldsymbol{x})$ as $\delta_{\boldsymbol{x}}$, plugs in the pre-defined $f$ to approximate $p^{\text{tr}}(y|\boldsymbol{x})$ and optimizes the following objective: 

\begin{align}
\begin{split}
\boldsymbol{r}_f:=\underset{\boldsymbol{r} \in \R}{\argmax } ~ \ell(\boldsymbol{r}, f):=\underset{\boldsymbol{r} \in \R}{\argmax }~ \E_{\text{te}}\left[\log (f(\boldsymbol{x})^T \boldsymbol{r})\right] .
\end{split}
\label{eq:wf_expanded}
\end{align}

With the Bias-Corrected Calibration (BCT) \citep{bct} strategy, they adjust the logits $\hat{f}(\boldsymbol{x})$ of $f(\boldsymbol{x})$ element-wise for each class, and the objective becomes:

\begin{align}
\begin{split}
\boldsymbol{r}_f := \underset{\boldsymbol{r} \in \R}{\argmax } ~ \ell(\boldsymbol{r}, f) := \underset{\boldsymbol{r} \in \R}{\argmax}~ \E_{\text{te}}\left[\log (g\circ \hat{f}(\boldsymbol{x}))^T \boldsymbol{r})\right],
\end{split}
\end{align}
where $g$ is a calibration function.
