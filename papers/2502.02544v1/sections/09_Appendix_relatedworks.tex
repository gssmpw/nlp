\newpage
\section{Related work}\label{app:relatedwork}
In the context of distributed learning with label shifts, importance ratio estimation is tackled either by solving a linear system as in \citep{bbse, rlls} or by minimizing distribution divergence as in \citep{mlls}. In this section, we overview complete related work. 

\paragraph{Federated learning (FL).} Much of the current research in FL predominantly centers around the minimization of empirical risk, operating under the assumption that each node maintains the same training/test data distribution~\citep{FL}.~Prominent methods in FL include  FedAvg~\citep{FedAvg}, FedBN~\citep{fedbn}, FedProx~\citep{fedprox} and SCAFFOLD~\citep{SCAFFOLD}. FedAvg and its variants such as ~\citep{huang2021personalized, pmlr-v119-karimireddy20a} 
have been the subject of thorough investigation in optimization literature, exploring facets such as communication efficiency, node participation, and privacy assurance \citep{ALQ,NUQSGD,QGenX,ali2023federated}.~Subsequent work, such as the study by \citet{de2022mitigating}, explores Federated Domain Generalization and introduces data augmentation to the training. This model aims to generalize to both in-domain datasets from participating nodes and an out-of-domain dataset from a non-participating node. Additionally, \citet{gupta2022fl} introduces FL Games, a game-theoretic framework designed to learn causal features that remain invariant across nodes. This is achieved by employing ensembles over nodes' historical actions and enhancing local computation, under the assumption of consistent training/test data distribution across nodes. The existing strategies to address statistical heterogeneity across nodes during training primarily rely on heuristic-based personalization methods, which currently lack theoretical backing in statistical learning~\citep{FLMultiTask,Khodak,li2021ditto}.  In contrast, we aim to minimize overall test error amid both intra-node and inter-node distribution shifts, a situation frequently observed in real-world scenarios. Techniques ensuring communication efficiency, robustness, and secure aggregations serve as complementary.

\paragraph{Importance ratio estimation} 
Classical Empirical Risk Minimization (ERM) seeks to minimize the expected loss over the training distribution using finite samples. When faced with distribution shifts, the goal shifts to minimizing the expected loss over the target distribution, leading to the development of Importance-Weighted Empirical Risk Minimization (IW-ERM)\citep{shimodaira2000improving, sugiyama2006importance, byrd2019effect, fang2020rethinking}. \citet{shimodaira2000improving} established that the IW-ERM estimator is asymptotically unbiased. Moreover, \citet{ali2023federated} introduced FTW-ERM, which integrates density ratio estimation.


\paragraph{Label shift and MLLS family}
For theoretical analysis, the conditional distribution \( p(\boldsymbol{x}|\boldsymbol{y}) \) is held strictly constant across all distributions \citep{bbse, mlls, bbse_2002}. Both BBSE \citep{bbse} and RLLS \citep{rlls} designate a discrete latent space \( \boldsymbol{z} \) and introduce a confusion matrix-based estimation method to compute the ratio \( \boldsymbol{w} \) by solving a linear system \citep{bbse_2002, bbse}. This approach is straightforward and has been proven consistent, even when the predictor is not calibrated. However, its subpar performance is attributed to the information loss inherent in the confusion matrix \citep{mlls}.

Consequently, MLLS \citep{mlls} introduces a continuous latent space, resulting in a significant enhancement in estimation performance, especially when combined with a post-hoc calibration method \citep{bct}. It also provides a consistency guarantee with a canonically calibrated predictor. This EM-based MLLS method is both concave and can be solved efficiently.


\paragraph{Discrepancy Measure}
In information theory and statistics, discrepancy measures play a critical role in quantifying the differences between probability distributions. One such measure is the Bregman Divergence \citep{bregman}, defined as 
\[D_\phi(\boldsymbol{x} \| \boldsymbol{y}) = \phi(\boldsymbol{x}) - \phi(\boldsymbol{y}) - \langle \nabla \phi(\boldsymbol{y}), \boldsymbol{x} - \boldsymbol{y} \rangle,\] 
which encapsulates the difference between the value of a convex function \(\phi\) at two points and the value of the linear approximation of \(\phi\) at one point, leveraging the gradient at another point.

Discrepancy measures are generally categorized into two main families: Integral Probability Metrics (IPMs) and \(f\)-divergences. IPMs, including Maximum Mean Discrepancy \citep{MMD} and Wasserstein distance \citep{wasserstein}, focus on distribution differences \(P - Q\). In contrast, \(f\)-divergences, such as KL-divergence \citep{KL} and Total Variation distance, operate on ratios \({P}/{Q}\) and do not satisfy the triangular inequality. Interconnections and variations between these families are explored in studies like \((f, \Gamma)\)-Divergences \citep{article_f}, which interpolate between \(f\)-divergences and IPMs, and research outlining optimal bounds between them \citep{article_f2}. 

MLLS \citep{mlls} employs \( f \)-divergence, notably the KL divergence, which is not a metric as it doesn't satisfy the triangular inequality, and requires distribution \( P \) to be absolutely continuous with respect to \( Q \). Concerning IPMs, while MMD is reliant on a kernel function, it can suffer from the curse of dimensionality when faced with high-dimensional data. On the other hand, the Wasserstein distance can be reformulated using Kantorovich-Rubinstein duality \citep{rubinstein, wgan} as a maximization problem subject to a Lipschitz constrained function \( f: \mathbb{R}^d \rightarrow \mathbb{R} \). 
