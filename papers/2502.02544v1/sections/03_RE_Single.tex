\section{ Versatile Robust Label Shift: Regularized Ratio Estimation}\label{sec:ratio}

In this section, we introduce the Versatile Robust Label Shift (VRLS) method for density ratio estimation in a single-node setting, which forms the basis of the IW-ERM framework. 
To solve the optimization problem of IW-ERM, each node \( k \) requires an accurate estimate of the ratio:
\begin{equation}
r_k(\boldsymbol{y}) = \frac{\sum_{j=1}^K p_j^{\text{te}}(\boldsymbol{y})}{p_k^{\text{tr}}(\boldsymbol{y})},    
\label{eq:densityratio}
\end{equation}
where \( p_j^{\text{te}}(\boldsymbol{y}) \) and \( p_k^{\text{tr}}(\boldsymbol{y}) \) represent the test and training label densities, respectively.
To improve clarity and avoid over-complicating notations, we first consider the scenario where we have only one node under label shifts and then extend to multiple nodes.
\cref{app:tab:scenario} presents various scenarios.
In a single-node label shift scenario, the goal is to estimate the ratio \( r(\boldsymbol{y}) = p^{\text{te}}(\boldsymbol{y}) / p^{\text{tr}}(\boldsymbol{y}) \). 
Following the seminal work of~\citet{mlls}, we formulate density ratio estimation as a Maximum Likelihood Estimation (MLE) problem by constructing an optimization problem based on Kullback-Leibler (KL) divergence to directly estimate \( r(\boldsymbol{y}) \). 
We train a predictor \( f_{\boldsymbol{\theta}} \) to approximate \( p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x}) \), where \( \boldsymbol{\theta} \) denotes the parameters of a neural network.
After training, we apply the predictor  $f_{\boldsymbol{\theta}^\star}$ to a finite set of unlabeled samples drawn from the test distribution to obtain predicted label probabilities. These predictions are then used to estimate the ratio $\boldsymbol{r}_{f^\star}$.
Further details are provided in~\Cref{alg:pred}.

One of the novelties of VRLS is its ability to better calibrate the predictor, enabling it to better approximate the true conditional distribution \( p^{\text{tr}}(\boldsymbol{y} | \boldsymbol{x}) \). 
This approximation faces two main challenges, as highlighted in Theorem 3 of~\citep{mlls}: finite-sample error and miscalibration error.
Entropy-based regularization can directly tackle miscalibration, which occurs when predicted probabilities systematically deviate from true likelihoods.
Building on these insights, we introduce an \textit{explicit} entropy regularizer into the training objective, which is based on Shannon's entropy~\citep{pereyra2017regularizing, neo2024maxent}.
The regularization term $\Omega(f_{{\boldsymbol{\theta}}})$ is defined as:
\begin{flalign}
\Omega(f_{{\boldsymbol{\theta}}}) = \sum_{c=1}^{m} \phi\big(f_{{\boldsymbol{\theta}}}(\boldsymbol{x})\big)_c \log \bigg( \phi\big(f_{{\boldsymbol{\theta}}}(\boldsymbol{x})\big)_c \bigg) ,
\end{flalign}
where \( \phi \) denotes the softmax function, and \( c \)\ represents the $c^{\text{th}}$ element of the softmax output vector.
\begin{algorithm}[t!] 
\caption{VRLS Density Ratio Estimation Algorithm}
\label{alg:pred}
\footnotesize
\begin{algorithmic}[1]
    \Require Labeled training data $\{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^{n^{\text{tr}}}$.
    \Require Unlabeled test data $\{\boldsymbol{x}_j\}_{j=1}^{n^{\text{te}}}$.
    \Require Initial predictor \( f_{\boldsymbol{\theta}} \).
    \Ensure Optimized predictor \( f_{\boldsymbol{\theta}^{*}}
    \) and estimated density ratio \( {\boldsymbol{r}_{{f}^{*}}} \).

    \State \textbf{Training}:
        \State \hspace{\algorithmicindent} Optimize \( f_{\boldsymbol{\theta}} \) using~\cref{eq:f_g} via SGD.
        \State \hspace{\algorithmicindent} Continue until the training loss drops below a threshold or the maximum epochs are reached.
        \State \hspace{\algorithmicindent}  Obtain the optimized predictor \( f_{\boldsymbol{\theta}^{*}} \).
    \State \textbf{Density Ratio Estimation}:
        \State  \hspace{\algorithmicindent} With the optimized predictor \( f_{\boldsymbol{\theta}^{*}} \), estimate the density ratio \( {\boldsymbol{r}_{{f}^{*}}} \) using equation~\cref{eq:f_wo_g}.
\end{algorithmic}
\end{algorithm}
With this regularization to the softmax outputs, VRLS encourages smoother and more reliable predictions that account for inherent uncertainty in the data, leading to more accurate density ratio estimates and improving the SotA in practice. 
These improvements are empirically demonstrated in~\cref{sec:experiment}.
Our proposed VRLS objective is formulated as follows:

\begin{equation}\label{eq:f_wo_g}
{\boldsymbol{r}_{f^\star}} = \underset{\boldsymbol{r} \in \mathbb{R}_+^{m}}{\argmax}~ \E_{\text{te}}\left[\log (f_{\boldsymbol{\theta}^\star}(\boldsymbol{x})^\top \boldsymbol{r})\right],
\end{equation} where 
\begin{equation}\label{eq:f_g}
{\boldsymbol{\theta}^\star} = \underset{{\boldsymbol{\theta}}}{\argmin}~\E_{\text{tr}}\Bigl[\ell_{CE}\big(f_{{\boldsymbol{\theta}}}(\boldsymbol{x}),\boldsymbol{y}\big) + \zeta\Omega(f_{{\boldsymbol{\theta}}})\Bigr].
\end{equation}


The vector \( \boldsymbol{r} \) in~\Cref{eq:f_wo_g}, representing the density ratios for all \( m \) classes, belongs to the non-negative real space \( \mathbb{R}_{+}^m \). This constraint set is defined similarly to MLLS~\citep{garg2022OSLS}, and we use the expected value  \( \mathbb{E}_{\text{te}} \) for estimation, denoting the optimal density ratio as \( {\boldsymbol{r}_{f^\star}} \).
To train the predictor $\boldsymbol{\theta}$, we minimize the cross-entropy loss \( \ell_{CE} \) together with a scaled regularization term $\zeta\Omega(f_{{\boldsymbol{\theta}}})$, where $\zeta>0$ is a coefficient controlling the regularization strength.
Incorporating the regularizer $\Omega(f_{{\boldsymbol{\theta}}})$ improves the model calibration under the influence of \( \ell_{CE} \)  loss.
