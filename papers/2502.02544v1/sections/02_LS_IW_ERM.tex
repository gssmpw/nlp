\section{Density ratio estimation and Importance Weighted-ERM \label{sec:prelim}}

\paragraph{Density ratio estimation} Density ratio estimation for label shifts has been addressed by methods such as solving linear systems \citep{bbse, rlls} and minimizing distribution divergences \citep{mlls}, primarily in the context of a single node.~\citet{bbse, rlls, mlls} assumed the conditional distribution $p(\boldsymbol{x}|\boldsymbol{y})$ remains fixed between the training and test datasets, while the label distribution $p(\boldsymbol{y})$ changes. 
Black Box Shift Estimation (BBSE) \citep{bbse, ts1} and Regularized Learning under Label Shift (RLLS) \citep{rlls} are confusion matrix-based methods for estimating density ratios in label shift problems. While BBSE has been shown consistent even when the predictor is not calibrated, its subpar performance is attributed to information loss inherent in using confusion matrices \citep{mlls}. To overcome this, \citet{mlls} has introduced the MLLS, resulting in significant improvements in estimation performance, especially when combined with post-hoc calibration methods like BCT \citep{bct}. This EM algorithm based MLLS method \citep{bbse_2002,mlls} is concave and can be solved efficiently.

\begin{table*}
\footnotesize
\centering
\bgroup
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.5}
\def\arraystretch{1.3}
 \caption{Details of the label shift scenarios. Their IW-ERM formulas are presented in~\cref{app:IWERM}.}
 \label{app:tab:scenario} 
  \begin{tabular}{l c c c}
    \hline
  \textbf{Scenario} & \textbf{\#Nodes} & \textbf{Assumptions on Distributions} & \textbf{Ratio Node i Needs} \\
  \hline
    	{\tt No-LS} in~\Cref{IWERM:nocovar}  &  2 & $p_1^{\text{tr}}(\boldsymbol{y})=p_1^{\text{te}}(\boldsymbol{y})$, $p_1^{\text{tr}}(\boldsymbol{y})\neq p_2^{\text{tr}}(\boldsymbol{y})$  & ${p_1^{\text{tr}}(\boldsymbol{y})}/{p_2^{\text{tr}}(\boldsymbol{y})}$  \\
  	 {\tt LS on single} in~\Cref{IWERM:covar1}   & 2 &  $p_1^{\text{tr}}(\boldsymbol{y})\neq p_1^{\text{te}}(\boldsymbol{y})$, $p_2^{\text{tr}}(\boldsymbol{y})= p_2^{\text{te}}(\boldsymbol{y})$ & ${p_1^{\text{te}}(\boldsymbol{y})}/{p_1^{\text{tr}}(\boldsymbol{y})}$, ${p_1^{\text{te}}(\boldsymbol{y})}/{p_2^{\text{tr}}(\boldsymbol{y})}$\\
 {\tt LS on both} in~\Cref{IWERM:covar1}      & 2 & $p_1^{\text{tr}}(\boldsymbol{y})\neq $ $p_1^{\text{te}}(\boldsymbol{y})$, $p_2^{\text{tr}}(\boldsymbol{y})\neq p_2^{\text{te}}(\boldsymbol{y})$ & ${p_1^{\text{te}}(\boldsymbol{y})}/{p_1^{\text{tr}}(\boldsymbol{y})}$, ${p_1^{\text{te}}(\boldsymbol{y})}/{p_2^{\text{tr}}(\boldsymbol{y})}$ \\     
   {\tt LS on multi} in~\Cref{IWERM:gen}  & $K$  &  $p_k^{\text{tr}}(\boldsymbol{y})\neq p_1^{\text{te}}(\boldsymbol{y})$ for all $k$ & ${p_1^{\text{te}}(\boldsymbol{y})}/{p_k^{\text{tr}}(\boldsymbol{y})}$ for all $k$ \\  
 	  \hline
 \end{tabular}
  \egroup
\end{table*}

\paragraph{Importance Weighted-ERM} Classical ERM seeks to minimize the expected loss over the training distribution using finite samples. However, when there is a distribution shift between the training and test data, the objective of ERM is not to minimize the expected loss over the test distribution, regardless of the number of training samples. To address this, IW-ERM is developed \citep{shimodaira2000improving, sugiyama2006importance, byrd2019effect, fang2020rethinking}, which adjusts the training loss by weighting samples according to the density ratio, i.e., the ratio of the test density to the training density.~\citet{shimodaira2000improving} has shown that the IW-ERM estimator is asymptotically unbiased under certain conditions. Building on this, \citet{ali2023federated} have recently introduced Federated IW-ERM, which incorporates density ratio estimation to handle covariate shifts in distributed learning. However, this approach has limitations, as it does not address label shifts and the density ratio estimation method poses potential privacy risks.


In this work, we focus on label shifts and propose an IW-ERM framework enhanced by our VRLS method. We show that our IW-ERM with VRLS performs comparably to an upper bound that utilizes true density ratios, all while preserving data privacy across distributed data sources. This approach 
effectively addresses both intra-node and inter-node label shifts while ensuring convergence in probability to the overall true risk minimizer.
