\section{VRLS for Multi-Node Environment}


We now extend VRLS to the multi-node environment, taking into account the privacy and communication requirements. 
This extension naturally aligns with the concept of IW-ERM, effectively integrating these considerations into the multi-node learning paradigm.
We consider multiple nodes where each node has distinct training and test distributions. The goal here is to train a global model that utilizes local data and addresses overall test error. In this setup, each node uses its local data to estimate the required density ratios, as outlined in~\cref{sec:ratio}, and shares only low-dimensional ratio information, without the need to share any local data.

The process begins with each node training a global model on its local data, independently estimating its density ratios. These locally computed ratios are then shared amongst the nodes, allowing for the aggregated ratio required for IW-ERM to be computed centrally. This aggregated ratio is then used to further refine the global model in a second round of global training. This approach ensures minimal communication overhead and preserves node data privacy, as detailed in \cref{sec:theory_guarantee}. Our experimental results in~\cref{sec:experiment} demonstrate that the IW-ERM framework significantly improves test error performance while minimizing communication and computation overhead compared to baseline ERM. The density ratio estimation and IW-ERM are described in~\cref{alg:IWERM_detail}.

\begin{algorithm}[t!]
\caption{IW-ERM with VRLS in Distributed Learning}
\label{alg:IWERM_detail}
\footnotesize
\begin{algorithmic}[1]
    \Require Labeled training data $\{(\boldsymbol{x}_{k,i}^{\text{tr}}, \boldsymbol{y}_{k,i}^{\text{tr}})\}_{i=1}^{n_k^{\text{tr}}}$ at each node $k$, for $k = [K]$.
    \Require Unlabeled test data $\{\boldsymbol{x}_{k,j}^{\text{te}}\}_{j=1}^{n_k^{\text{te}}}$ at each node $k$, for $k = [K]$.
    \Require Initial global model $h_{\boldsymbol{w}}$.
    \Ensure Trained global model $h_{\boldsymbol{w}}$ optimized with IW-ERM.
    \State \textbf{Phase 1: Density Ratio Estimation with VRLS}
    \For{\textbf{each node} $k = 1$ to $K$ \textbf{in parallel}}
        \State Train local predictor $f_{k, {\theta}}$ on local training data $\{(\boldsymbol{x}_{k,i}^{\text{tr}}, \boldsymbol{y}_{k,i}^{\text{tr}})\}$.
        \State Use $f_{k, {\theta}^{*}}$ to estimate the density ratio \( {\boldsymbol{r}}_{k, f^{*}} \) on unlabeled test data $\{\boldsymbol{x}_{k}^{\text{te}}\}$ at node $k$.
    \EndFor
    \State \textbf{Phase 2: Density Ratio Aggregation}
    \For{\textbf{each node} $k = 1$ to $K$}
            \State Aggregate density ratio  using~\Cref{eq:densityratio}.
    \EndFor
    \State \textbf{Phase 3: Global Model Training with IW-ERM}
    \State Train global model $h_{\boldsymbol{w}}$ using~\Cref{IWERM:gen;R} with the aggregated density ratios.
\end{algorithmic}
\end{algorithm}

To provide a more comprehensive understanding of the multi-node environment, the following discussion delves into its details.
Let $\mathcal{X}\subseteq \mathbb{R}^{d_0}$ be a compact metric space for input features, $\mathcal{Y}$ be a discrete label space with $|\mathcal{Y}|=m$, and $K$ be the number of nodes in an multi-node setting.\footnote{Sets and scalars are represented by calligraphic and standard fonts, respectively. We use $[m]$ to denote $\{1,\ldots,m\}$ for an integer $m$. We use $\lesssim$ to ignore terms up to constants and logarithmic factors. We use $\E[\cdot]$ to denote the expectation and $\|\cdot\|$ to represent the Euclidean norm of a vector. We use lower-case bold font to denote vectors.} 
Let $\mathcal{S}_k=\{(\boldsymbol{x}_{k,i}^{\text{tr}},{\boldsymbol{y}}_{k,i}^{\text{tr}})\}_{i=1}^{n_k^{\text{tr}}}$ denote the training set of node $k$ with $n_k^{\text{tr}}$ samples drawn i.i.d. from a probability distribution $p_k^{\text{tr}}$ on $\mathcal{X} \times \mathcal{Y}$.
The test data of node $k$ is drawn from another probability distribution $p_k^{\text{te}}$ on $\mathcal{X} \times \mathcal{Y}$. We assume that the class-conditional distribution $p_k^{\text{te}}(\boldsymbol{x}|\boldsymbol{y})=p_k^{\text{tr}}(\boldsymbol{x}|\boldsymbol{y}) := p(\boldsymbol{x}|\boldsymbol{y})$ remains the same for all nodes $k$. This is a common assumption and holds when label shifts primarily affect labels' prior distribution of the labels $p(\boldsymbol{y})$ rather than the underlying feature distribution given the labels, e.g., when features that are generated given a label remains constant~\citep{zadrozny2004learning,huang2006correcting,sugiyama2007covariate}. 
Note that $p_k^{\text{tr}}(\boldsymbol{y})$ and $p_k^{\text{te}}(\boldsymbol{y})$ can be arbitrarily different, which gives rise to intra- and inter-node \emph{label shifts}~\citep{zadrozny2004learning,huang2006correcting,sugiyama2007covariate,rls}.


In this multi-node environment, the aim is to find an unbiased estimate of the overall \emph{true risk} minimizer across multiple nodes under both intra-node and inter-node \emph{label shifts}. Specifically, we aim to find a hypothesis $h_{\boldsymbol{w}}\in\mathcal{H}: \mathcal{X} \rightarrow \mathcal{Y}$, represented by a neural network parameterized by ${\boldsymbol{w}}$,  such that $h_{\boldsymbol{w}}(\boldsymbol{x})$  provides a good approximation of the label $\boldsymbol{y} \in \mathcal{Y}$ corresponding to a new sample $\boldsymbol{x} \in \mathcal{X}$ drawn from the aggregated \emph{test} data.
Let $\ell:\mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}_+$ denote a loss function. Node $k$ aims to learn a hypothesis $h_{\boldsymbol{w}}$ that minimizes its true (expected) risk:
\begin{equation}\tag{Local Risk}
    R_k(h_{\boldsymbol{w}}) = \mathbb{E}_{(\boldsymbol{x},\boldsymbol{y})\sim p_k^{\text{te}}(\boldsymbol{x},\boldsymbol{y})}[\ell(h_{\boldsymbol{w}}(\boldsymbol{x}),\boldsymbol{y})].
\end{equation}
We now modify the classical ERM and formulate IW-ERM to find a predictor that minimizes the overall true risk over all nodes under label shifts:
\begin{equation}\label{IWERM:gen;R}\tag{IW-ERM}
\min_{h_{\boldsymbol{w}} \in \mathcal{H}} \sum_{k=1}^K \frac{1}{n_k^{\text{tr}}}\sum_{i=1}^{n_k^{\text{tr}}} \frac{\sum_{j=1}^K p_j^{\text{te}}(\boldsymbol{y}_{k,i}^{\text{tr}})}{p_k^{\text{tr}}(\boldsymbol{y}_{k, i}^{\text{tr}})}\ell(h_{\boldsymbol{w}}(\boldsymbol{x}_{k,i}^{\text{tr}}),{\boldsymbol{y}}_{k,i}^{\text{tr}}),
\end{equation}
where $n_k^{\text{tr}}$ is the number of training samples at node $k$.

To incorporate our VRLS density ratio estimation method into the IW-ERM framework, we replace the ratio term $\frac{\sum_{j=1}^K p_j^{\text{te}}(\boldsymbol{y}_{k,i}^{\text{tr}})}{p_k^{\text{tr}}(\boldsymbol{y}_{k,i}^{\text{tr}})}$ with our estimated density ratios. 
This modification aims to align the empirical risk minimization with the true risk minimization over all nodes. We formalize the convergence of this approach in~\Cref{Prop:IW-ERM}.
\begin{proposition}\label{Prop:IW-ERM} 
Under the label shift setting described in \cref{sec:intro},~\eqref{IWERM:gen;R} is consistent and the learned function $h_{\boldsymbol{w}}$ converges in probability towards the optimal function that minimizes the overall \emph{true risk} across nodes, $\sum_{k=1}^K R_k$.
\end{proposition}
\begin{proof}
Due to space limitations, the proof is provided in~\cref{app:IWERM}. Convergence in probability is established by applying the law of large numbers following ~\citep{shimodaira2000improving}[Section 3] and ~\citep{sugiyama2007covariate}[Section 2.2].
\end{proof}
