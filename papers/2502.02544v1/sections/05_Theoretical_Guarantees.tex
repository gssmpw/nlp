\section{Ratio Estimation Bounds and Convergence Rates }\label{sec:theory_guarantee}


In this section, we present bounds on ratio estimation and convergence rates for the finite sample errors incurred during the estimation, as further discussed in Appendices \ref{app:thm:est}, \ref{app:conv}. In practice, we only have access to a finite number of labeled training samples, $\{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^{{n}^{\text{tr}}}$, and a finite number of unlabeled test samples, $\{\boldsymbol{x}_j\}_{j=1}^{{n}^{\text{te}}}$. These samples serve to compute the following estimates:\\ 
\[{\hat{\boldsymbol{\theta}}}_{{n}^{\text{tr}}} = \underset{\boldsymbol{\theta} \in \Theta}{\argmin} \frac{1}{{n}^{\text{tr}}}\sum_{i=1}^{{n}^{\text{tr}}}\bigg(\ell_{CE}(f_{\boldsymbol{\theta}}(\boldsymbol{x}_i), \boldsymbol{y}_i) + \zeta\Omega(f_{\boldsymbol{\theta}})\bigg),\] 
\[\text{and  } {\hat{\boldsymbol{r}}}_{{n}^{\text{te}}} = \underset{\boldsymbol{r} \in \mathbb{R}_{+}^m}{\argmax} \frac{1}{{n}^{\text{te}}}\sum_{j=1}^{{n}^{\text{te}}}\log (f_{{\hat{\boldsymbol{\theta}}}_{{n}^{\text{tr}}}}(\boldsymbol{x}_j)^\top \boldsymbol{r}).\]

We will show that the errors of these estimates can be controlled. The following assumptions are necessary to establish our results.

\begin{assumption}[Boundedness]\label{assumption:bounded}
The data and the parameter space $\Theta$ are bounded, i.e, there exists $b_\mathcal{X}, b_\Theta > 0$ such that
\[
\forall \boldsymbol{x} \in \mathcal{X},\; \|\boldsymbol{x}\|_2 \leq b_\mathcal{X} \quad \quad \text{and} \quad \quad \forall \boldsymbol{\theta} \in \Theta,\; \|\boldsymbol{\theta}\|_2 \leq b_\Theta.
\]
\end{assumption}
\begin{assumption}[Calibration]\label{assumption:calibration}
    Let $\boldsymbol{\theta}^\star$ be as defined in \Cref{eq:f_g}. There exists $\mu > 0$ such that
    \[
    \mathbb{E}\left[{f_{\boldsymbol{\theta}^\star}(\boldsymbol{x})f_{\boldsymbol{\theta}^\star}(\boldsymbol{x})^\top}\right] \succeq \mu \boldsymbol{I}_m.
    \]
\end{assumption}

The calibration~\cref{assumption:calibration} first appears in \citep{mlls}. 
It is necessary for the ratio estimation procedure to be consistent and we refer the reader to Section 4.3 of \citet{mlls} for more details. We further need~\cref{assumption:bounded} because, unlike \citep{mlls}, the empirical estimator $\hat{\boldsymbol{r}}_{{n}^{\text{te}}}$ is estimated using another estimator ${\hat{\boldsymbol{\theta}}}_{{n}^{\text{tr}}}$.
Uniform bounds are therefore needed to control finite sample error as we cannot directly apply concentration inequalities, as is done in the proof of \citep[Lemma 3]{mlls}, since we do not have independence of the terms appearing in the empirical sums. We nonetheless prove a similar result in the following theorem.

\begin{theorem}[Ratio Estimation Error Bound]
\label{thm:est}
    Let $\delta \in (0,1)$ and $\mathcal{F} := \{ \boldsymbol{x} \mapsto \boldsymbol{r}^\top f_{\boldsymbol{\theta}}(\boldsymbol{x}), \; (\boldsymbol{r}, \boldsymbol{\theta}) \in\mathcal{R}\times\Theta\}$. Under  Assumptions \ref{assumption:bounded}-\ref{assumption:calibration}, there exist constants $L>0, B>0$ such that with probability at least $1-\delta$:
    \begin{align}
    \| \hat{\boldsymbol{r}}_{{n}^{\text{te}}} - \boldsymbol{r}_{f^\star} \|_2 \leq \frac{2}{\mu p_{\min}}\Big( \frac{4}{\sqrt{{n}^{\text{te}}}} \text{Rad}(\mathcal{F}) + 4B\sqrt{\frac{\log(4/\delta)}{{n}^{\text{te}}}} \Big)+ \frac{4L}{\mu p_{\min}} \mathbb{E} \left[ {\|\boldsymbol{\theta} - \boldsymbol{\theta}^\star\|_2} \right].
    \end{align}
    Here, $p_{\min} = \min_{y} p(y)$ and
    \begin{align}
    \text{Rad}(\mathcal{F}) = \frac{1}{\sqrt{{n}^{\text{tr}}}} \mathbb{E}_{\sigma_1, \dots, \sigma}\left[ \sup_{(\boldsymbol{r}, \boldsymbol{\theta})\in\mathcal{R}\times\Theta} \left|\sum_{i=1}^{{n}^{\text{tr}}}\sigma_i \boldsymbol{r}^\top f_{{\boldsymbol{\theta}}}(\boldsymbol{x}_i)\right|\right],
    \end{align}
where $\sigma_1, \dots, \sigma$ are Rademacher variables uniformly chosen from $\{-1,1\}$.
\end{theorem}

\begin{proof}
The proof of~\cref{thm:est} is provided in~\cref{app:thm:est}. The Rademacher complexity appearing in the bound will depend on the function class chosen for $f$. Moreover as regularization often encourages lower complexity functions, this complexity can be reduced because of the presence of the regularization term in the estimation of $\boldsymbol{\theta}$ in our setting.
\end{proof}


By estimating the ratios locally and incorporating them into local losses, the properties of the modified loss with respect to neural network parameters $\boldsymbol{w}$ remain unchanged, with data-dependent parameters like Lipschitz constants scaled linearly by $r_{\max}$. Our approach trains the predictor using only local data, ensuring IW-ERM with VRLS retains the same privacy guarantees as baseline ERM-solvers. Communication involves only the marginal label distribution, adding negligible overhead, as it is far smaller than model parameters and requires just one round of communication. Overall, importance weighting does not impact communication guarantees during optimization.

\begin{theorem}
[Convergence-communication]\label{thm:conv} Let $\max_{\boldsymbol{y}\in\mathcal{Y}}\sup_f r_f(\boldsymbol{y})=r_{\max}$.  Suppose \cref{alg:IWERM_detail}, e.g., IW-ERM with VRLS for multi-node environment, is run for $T$ iterations. Then \cref{alg:IWERM_detail} achieves a convergence rate of $\mathcal{O}(r_{\max} h(T))$, where $\mathcal{O}(h(T))$ denotes the rate of ERM-solver baseline without importance weighting. Throughout the course of optimization,~\cref{alg:IWERM_detail} has the same overall communication guarantees as the baseline.
\end{theorem}

In the following, we establish tight convergence rates and communication guarantees for IW-ERM with VRLS in a broad range of importance optimization settings including upper- and lower-bounds for convex optimization (Theorems \ref{app:convexsmooth}- \ref{app:second}), second-order differentiability, composite optimization with proximal operator (\cref{app:proxy}), optimization with adaptive step-sizes, and nonconvex optimization (Theorems \ref{app:PL}- \ref{app:adaptive}), along the lines of~e.g.,~\citep{woodworth2020local,haddadpour2021federated,glasgow2022sharp,liu2023high,Prox,AdaptiveFL,liu2023high}. 

\begin{assumption}[Convex and Smooth]
\label{assumption:convexsmooth} 1) A minimizer $\boldsymbol{w}^\star$ exists with bounded $\|\boldsymbol{w}^\star\|_2$; 2) The $\ell\circ h_{\boldsymbol{w}}$ is $\beta$-smoothness and convex w.r.t. $\boldsymbol{w}$; 3) The stochastic gradient $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased, i.e., $\mathbb{E}[\boldsymbol{g}(\boldsymbol{w})]=\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ for any $\boldsymbol{w}\in\mathcal{W}$ with bounded variance  $\mathbb{E}[\|\boldsymbol{g}(\boldsymbol{w})-\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})\|_2^2]$.
\end{assumption}


For convex and smooth optimization, we establish convergence rates for IW-ERM with VRLS and local updating along the lines of~e.g.,~\citep[Theorem 2]{woodworth2020local}. 


\begin{theorem}
[Upper Bound for Convex and Smooth]\label{app:convexsmooth} Let $D=\|\boldsymbol{w}_0-\boldsymbol{w}^\star\|$, $\tau$ denote the number of local steps (number of stochastic gradients per round of communication per node),  $R$ denote the number of communication rounds, and $\max_{\boldsymbol{y}\in\mathcal{Y}}\sup_f r_f(\boldsymbol{y})=r_{\max}$. Under~\cref{assumption:convexsmooth}, suppose~\cref{alg:IWERM_detail} with $\tau$ local updates is run for $T=\tau R$ total stochastic gradients per node with an optimally tuned and constant step-size. Then we have the following upper bound: 
\begin{align}
\E[\ell(h_{\boldsymbol{w}_T})-\ell(h_{\boldsymbol{w}^\star})]  \lesssim \frac{{r}_{\max} \beta D^2}{\tau R}+\frac{(r_{\max} \beta D^4)^{1/3}}{(\sqrt{\tau}R)^{2/3}} +\frac{D}{\sqrt{K\tau R}}.    
\end{align}
\end{theorem}

\begin{assumption}[Convex and Second-order Differentiable]
\label{assumption:second} 1) The $\ell(h_{\boldsymbol{w}}(\boldsymbol{x}),\boldsymbol{y})$ is $\beta$-smoothness and convex w.r.t. $\boldsymbol{w}$ for any $(\boldsymbol{x},y)$; 2) The stochastic gradient $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased, i.e., $\E[\boldsymbol{g}(\boldsymbol{w})]=\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ for any $\boldsymbol{w}\in\mathcal{W}$ with bounded variance  $\E[\|\boldsymbol{g}(\boldsymbol{w})-\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})\|_2^2]$.
\end{assumption}

\begin{theorem}
[Lower Bound for Convex and Second-order Differentiable]\label{app:second} Let $D=\|\boldsymbol{w}_0-\boldsymbol{w}^\star\|$, $\tau$ denote the number of local steps,  $R$ denote the number of communication rounds, and $\max_{\boldsymbol{y}\in\mathcal{Y}}\sup_f r_f(\boldsymbol{y})=r_{\max}$. Under~\cref{assumption:second}, suppose~\cref{alg:IWERM_detail} with $\tau$ local updates is run for $T=\tau R$ total stochastic gradients per node with a tuned and constant step-size. Then we have the following lower bound: 
\begin{align}
\E[\ell(h_{\boldsymbol{w}_T})-\ell(h_{\boldsymbol{w}^\star})]
\gtrsim \frac{r_{\max}\beta D^2}{\tau R}+\frac{(r_{\max}\beta D^4)^{1/3}}{(\sqrt{\tau}R)^{2/3}} +\frac{D}{\sqrt{K\tau R}}.   
\end{align}
\end{theorem}

We finally establish high-probability convergence bounds for IW-ERM with VRLS along the lines of~e.g.,~\citep[Theorem 4.1]{liu2023high}. To show the impact of importance weighting on convergence rate decoupled from the impact of number of nodes and obtain the current SotA {\it high-probability} bounds for nonconvex optimization, we focus on IW-ERM with $K=1$. 
\begin{assumption}[Sub-Gaussian Noise]
\label{assumption:noise} 1) A minimizer $\boldsymbol{w}^\star$ exists; 2) The stochastic gradients $\boldsymbol{g}(\boldsymbol{w})=\widetilde\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ is unbiased, i.e., $\E[\boldsymbol{g}(\boldsymbol{w})]=\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})$ for any $\boldsymbol{w}\in\mathcal{W}$; 3) The noise $\|\boldsymbol{g}(\boldsymbol{w})-\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}})\|_2$ is $\sigma$-sub-Gaussian ~\citep{vershynin2018high}.

\end{assumption}

\begin{theorem}[High-probability Bound for Nonconvex Optimization]\label{app:convprob} Let $\delta \in (0,1)$ and $T\in\mathbb{Z}_+$. Let $K=1$ and $\max_{\boldsymbol{y}\in\mathcal{Y}}\sup_f r_f(\boldsymbol{y})=r_{\max}$.
Under~\cref{assumption:noise} and $\beta$-smoothness of {\it nonconvex} $\ell\circ h_{\boldsymbol{w}}$, suppose IW-ERM is run for $T$ iterations with a step-size $\min\Big\{\frac{1}{r_{\max}\beta},\sqrt{\frac{1}{\sigma^2r_{\max}\beta T}}\Big\}$. Then with probability $1-\delta$, gradient norm squareds satisfy: 
\begin{align}
\frac{1}{T}\sum_{t=1}^T\|\nabla_{\boldsymbol{w}}\ell(h_{\boldsymbol{w}_t})\|_2^2=O\Big(\sigma\sqrt{\frac{r_{\max}\beta}{T}}+\frac{\sigma^2\log(1/\delta)}{T}\Big).  
\end{align}
\end{theorem}

\begin{proof}
We note that density ratios do not depend on the model parameters $\boldsymbol{w}$ and the Lipschitz and smoothness constants for $\ell\circ h_{\boldsymbol{w}}$ w.r.t. $\boldsymbol{w}$ are scaled by $r_{\max}$. The rest of the proof follows the arguments of~\citep[Theorem 4.1]{liu2023high}.
\end{proof} 


\cref{app:convprob} shows that when the stochastic gradients are too noisy $\sigma=\Omega(\sqrt{r_{\max}\beta}/\log(1/\delta))$ such that the second term in the rate dominates, then importance weighting does not have any negative impact on the convergence rate.
