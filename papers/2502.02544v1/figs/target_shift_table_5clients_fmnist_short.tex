\begin{minipage}[!t]{0.5\linewidth}
    \centering
    \begin{tabular}{ll}
    \toprule
    Method & \text{Avg.accuracy} \\
    \midrule
    {\bf Our IW-ERM } & $\boldsymbol{0.7520}$ $\pm$ $\boldsymbol{0.0209}$ \\
    \midrule
     \text{Our IW-ERM (small)} & ${0.7376}$ $\pm$ ${0.0099}$\\
    \midrule
    FedAvg& 0.5472  $\pm$ 0.0297 \\
    \midrule
    FedBN & 0.5359 $\pm$ 0.0306 \\
    \midrule
    FedProx & 0.5606 $\pm$ 0.0070\\
    \midrule
    SCAFFOLD & 0.5774$ \pm$ 0.0036 \\
    \midrule \midrule
    Upper Bound & 0.8273 $\pm$ 0.0041\\
    \bottomrule
    \end{tabular}
\end{minipage}
\begin{minipage}[h]{0.5\linewidth}
    \captionof{table}{We utilize LeNet on Fashion MNIST to address label shifts across 5 nodes. For the baseline methods—FedAvg, FedBN, FedProx, and SCAFFOLD—we run 15,000 iterations, while both the Upper Bound (IW-ERM with true ratios) and our IW-ERM with VRLS are limited to 5,000 iterations. Notably, we employ a simple MLP with dropout for training the predictor. The model labeled {\it Our IW-ERM (small)} refers to our approach where the black-box predictor is trained using only 10\% of the available training data, balancing computational efficiency with competitive performance. }
    \label{fig:target-shift-fmnist-5}
\end{minipage}
