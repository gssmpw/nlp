\section{literature review}
\subsection{Reinforcement Learning for Autonomous  Parking}
In the field of automatic parking, traditional parking algorithms often rely on predefined rules or manually designed controllers, making it difficult to handle complex and dynamic real-world scenarios. 
To address these challenges, reinforcement learning (RL) has gradually emerged as an innovative approach in autonomous parking. 
Through continuous interaction and self-learning, RL algorithms can optimize decision-making strategies in dynamic environments, enabling more intelligent and flexible parking maneuvers.

In the early stages of artificial intelligence development, Maravall et al. **Maravall, D., et al., "Autonomous Parking using Artificial Neural Networks"** proposed a theoretical framework for autonomous parking using artificial neural networks (ANN), designing both model-free and model-based RL parking networks. 
In recent years, many studies have also explored the use of RL algorithms for trajectory planning in parking scenarios. 
Thunyapoo et al. **Thunyapoo, S., et al., "Autonomous Parking using Proximal Policy Optimization"** proposed an autonomous parking approach using Proximal Policy Optimization (PPO) in a simulation environment. 
Takehara et al. **Takehara, T., et al., "Visual Deep Reinforcement Learning for Autonomous Parking"** combined sensor data and introduced a purely visual deep reinforcement learning (DRL) parking solution based on the Unity simulator. 
Zhang et al. **Zhang, J., et al., "Reward Functions for Data Generation in RL"** extended RL to the data generation phase by designing reward functions to filter the generated data, optimizing model performance, and reducing dependence on human data. Additionally, Zhang et al. **Zhang, J., et al., "End-to-End RL with Real Sensor Data and Parking Space Tracking"** integrated real sensor data and proposed an end-to-end RL algorithm that uses parking space tracking for perception, which was deployed on an experimental vehicle. This approach improved performance in real-world deployments through a multi-stage training strategy.

Some studies have also combined rule-based non-learning trajectory planners with learning-based planners, proposing hybrid RL parking algorithms. 
For instance, Shi et al. **Shi, X., et al., "Hybrid Parking Algorithm with Model Predictive Control"** introduced a hybrid parking algorithm that uses a rule-based Model Predictive Control (MPC) planner for parking space searching and a RL algorithm for parking maneuvers. 
Jiang et al. **Jiang, J., et al., "Hybrid Policy Path Planner (HOPE)"** proposed Hybrid Policy Path Planner (HOPE), which integrates RL algorithms with RS trajectory planners during the parking phase, enhancing parking capability and training efficiency.

\subsection{Simulation-to-Reality Transfer in Reinforcement Learning}

Sim-to-Real (Simulation-to-Reality) refers to the process of applying reinforcement learning strategies or models, trained in simulation environments, to real-world scenarios **Tobin, J., et al., "Domain Randomization for Robust Real-World Performance"**. 
In reinforcement learning, simulation environments offer abundant, low-cost, and limitless training data, facilitating rapid iterations and strategy optimization. 
However, due to discrepancies of sensor inputs, physics simulation, and sensor and control delays **Bousmalis, K., et al., "Domain Adaptation for Real-World Deployment"** between simulation and reality, strategies trained in simulations often experience performance degradation when deployed in the real world, a phenomenon known as the â€œSim-to-Real Gap".

To address this issue, Sim-to-Real methods aim to bridge the gap between simulation and reality through various techniques, ensuring robust performance in real-world applications. 
Some approaches reduce the sim-to-real gap by minimizing the differences between the simulator and reality. 
Tobin et al. **Tobin, J., et al., "Domain Randomization for Robust Real-World Performance"** proposed Domain Randomization, which enhances robustness in real environments by randomizing simulator parameters such as colors, textures, and dynamics. 
Bousmalis et al. **Bousmalis, K., et al., "Domain Adaptation for Real-World Deployment"** introduced Domain Adaptation, aligning the feature spaces of the source and target domains to reduce differences and improve the model's adaptability in real environments.

Inspired by the hybrid policy proposed by Jiang et al. **Jiang, J., et al., "Hybrid Policy Path Planner (HOPE)"**, we propose an innovative hybrid reinforcement learning parking algorithm that uses LiDAR Occupancy Grid Maps (OGM) as input. This algorithm improves upon the original hybrid strategy combining the Reeds-Shepp (RS) planner and reinforcement learning planner by leveraging LiDAR OGM to unify complex environmental information into a consistent input format, effectively reducing the Sim-to-Real Gap between simulation and reality.



\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/workflow.png}
	\caption{
		The figure shows the overall workflow. The proposed integrated hybrid reinforcement learning system for autonomous parking leverages OGM representations derived from LiDAR and IMU data. 
  The global OGM aids in creating diverse simulation scenarios, while the local OGM is used for inference in the hybrid reinforcement learning network. 
  The hybrid reinforcement learning planner combines a rule-based Reeds-Shepp (RS) planner with a learning-based RL planner, taking OGMs and target positions as inputs to generate actions and trajectories for precise parking maneuvers.
		}
	\label{fig:framework}
       \vspace{-0.5cm}
\end{figure*}