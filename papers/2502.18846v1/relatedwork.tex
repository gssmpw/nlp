\section{literature review}
\subsection{Reinforcement Learning for Autonomous  Parking}
In the field of automatic parking, traditional parking algorithms often rely on predefined rules or manually designed controllers, making it difficult to handle complex and dynamic real-world scenarios. 
To address these challenges, reinforcement learning (RL) has gradually emerged as an innovative approach in autonomous parking. 
Through continuous interaction and self-learning, RL algorithms can optimize decision-making strategies in dynamic environments, enabling more intelligent and flexible parking maneuvers.

In the early stages of artificial intelligence development, Maravall et al.\cite{maravall2003automatic} proposed a theoretical framework for autonomous parking using artificial neural networks (ANN), designing both model-free and model-based RL parking networks. 
In recent years, many studies have also explored the use of RL algorithms for trajectory planning in parking scenarios. 
Thunyapoo et al.\cite{thunyapoo2020self} proposed an autonomous parking approach using Proximal Policy Optimization (PPO) in a simulation environment. 
Takehara et al.\cite{takehara2021autonomous} combined sensor data and introduced a purely visual deep reinforcement learning (DRL) parking solution based on the Unity simulator. 
Zhang et al.\cite{zhang2020data} extended RL to the data generation phase by designing reward functions to filter the generated data, optimizing model performance, and reducing dependence on human data. Additionally, Zhang et al.\cite{zhang2019e2e} integrated real sensor data and proposed an end-to-end RL algorithm that uses parking space tracking for perception, which was deployed on an experimental vehicle. This approach improved performance in real-world deployments through a multi-stage training strategy.

Some studies have also combined rule-based non-learning trajectory planners with learning-based planners, proposing hybrid RL parking algorithms. 
For instance, Shi et al.\cite{shi2023model} introduced a hybrid parking algorithm that uses a rule-based Model Predictive Control (MPC) planner for parking space searching and a RL algorithm for parking maneuvers. 
Jiang et al. \cite{jiang2024hope} proposed Hybrid Policy Path Planner (HOPE), which integrates RL algorithms with RS trajectory planners during the parking phase, enhancing parking capability and training efficiency.

\subsection{Simulation-to-Reality Transfer in Reinforcement Learning}

Sim-to-Real (Simulation-to-Reality) refers to the process of applying reinforcement learning strategies or models, trained in simulation environments, to real-world scenarios\cite{zhao2020sim}. 
In reinforcement learning, simulation environments offer abundant, low-cost, and limitless training data, facilitating rapid iterations and strategy optimization. 
However, due to discrepancies of sensor inputs, physics simulation, and sensor and control delays\cite{robotics_sim2real_yu2024} between simulation and reality, strategies trained in simulations often experience performance degradation when deployed in the real world, a phenomenon known as the â€œSim-to-Real Gap".

To address this issue, Sim-to-Real methods aim to bridge the gap between simulation and reality through various techniques, ensuring robust performance in real-world applications. 
Some approaches reduce the sim-to-real gap by minimizing the differences between the simulator and reality. 
Tobin et al.\cite{tobin2017domain} proposed Domain Randomization, which enhances robustness in real environments by randomizing simulator parameters such as colors, textures, and dynamics. 
Bousmalis et al.\cite{bousmalis2018using} introduced Domain Adaptation, aligning the feature spaces of the source and target domains to reduce differences and improve the model's adaptability in real environments.

% For task-specific challenges, Rajeswaran et al.\cite{rajeswaran2017learning} proposed Imitation Learning, where expert demonstrations are used to learn complex task reward functions, providing robust rewards for RL strategies. 
% In terms of model performance, Arndt et al.\cite{arndt2020meta} proposed Meta-Learning, training across multiple tasks to enable strategies to quickly adapt to new tasks, thereby improving generalization across different tasks. 
% Rusu et al.\cite{rusu2015policy} introduced Knowledge Distillation, where knowledge is extracted from a large teacher network to train a smaller student network, maintaining the effectiveness and efficiency of the strategy.

Inspired by the hybrid policy proposed by Jiang et al.\cite{jiang2024hope}, we propose an innovative hybrid reinforcement learning parking algorithm that uses LiDAR Occupancy Grid Maps (OGM) as input. This algorithm improves upon the original hybrid strategy combining the Reeds-Shepp (RS) planner and reinforcement learning planner by leveraging LiDAR OGM to unify complex environmental information into a consistent input format, effectively reducing the Sim-to-Real Gap between simulation and reality.



\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/workflow.png}
	\caption{
		The figure shows the overall workflow. The proposed integrated hybrid reinforcement learning system for autonomous parking leverages OGM representations derived from LiDAR and IMU data. 
  The global OGM aids in creating diverse simulation scenarios, while the local OGM is used for inference in the hybrid reinforcement learning network. 
  The hybrid reinforcement learning planner combines a rule-based Reeds-Shepp (RS) planner with a learning-based RL planner, taking OGMs and target positions as inputs to generate actions and trajectories for precise parking maneuvers.
		}
	\label{fig:framework}
       \vspace{-0.5cm}
\end{figure*}