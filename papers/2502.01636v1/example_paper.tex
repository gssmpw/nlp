%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{makecell} % For multi-line text in cells
\usepackage{array}    % For better table alignment
\usepackage{caption}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Lifelong Sequential Knowledge Editing without Model Degradation}

\begin{document}

\twocolumn[
\icmltitle{Lifelong Sequential Knowledge Editing without Model Degradation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Akshat Gupta}{yyy}
\icmlauthor{Phudish Prateepamornkul}{yyy,datax}
\icmlauthor{Maochuan Lu}{yyy}
\\
\icmlauthor{Ahmed Alaa}{yyy}
\icmlauthor{Thomas Hartvigsen}{comp}
\icmlauthor{Gopala Anumanchipalli}{yyy}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{University of California, Berkeley}
\icmlaffiliation{comp}{University of Virginia}
\icmlaffiliation{datax}{
SCB DataX}
\icmlcorrespondingauthor{Akshat Gupta}{akshat.gupta@berkeley.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Prior work in parameter-modifying knowledge editing has shown that large-scale sequential editing leads to significant model degradation. In this paper, we study the reasons behind this and scale sequential knowledge editing to 10,000 sequential edits, while maintaining the downstream performance of the original model. We first show that locate-then-edit knowledge editing methods lead to overfitting on the edited facts. We also show that continuous knowledge editing using these methods leads to disproportionate growth in the norm of the edited matrix. We then provide a crucial insight into the inner workings of locate-then-edit methods. We show that norm-growth is a hidden trick employed by these methods that gives larger importance to the output activations produced from the edited layers. With this ``importance hacking", the edited layers provide a much larger contributions to the model's output. To mitigate these issues, we present \textbf{ENCORE} - \textbf{\underline{E}}arly stopping and \textbf{\underline{N}}orm-\textbf{\underline{Co}}nstrained \textbf{\underline{R}}obust knowledge \textbf{\underline{E}}diting. ENCORE controls for overfitting and the disproportionate norm-growth to enable long-term sequential editing, where we are able to perform up to 10,000 sequential edits without loss of downstream performance. ENCORE is also 61\% faster than MEMIT and 64\% faster than AlphaEdit on Llama3-8B.
%To mitigate overfitting on edited facts, we propose a variant of early stopping called most-probable early stopping (MPES) which prevents over-optmization of the model on edited facts. We control the growing norm of edited matrix by adding a norm constraint to the optimization objective of MEMIT, increasing the robustness of editing performance over large number of edits. 
%Finally, we combine these two strategies to present ENCORE - Early stopping and Norm-COnstrained based Regularized knoweldge Editing. ENCORE enable long-term sequential editing, where we are able to edit models for upto 10,000 sequential edits without loss of downstream performance. ENCORE 61\% faster than MEMIT and 64\% faster than AlphaEdit while achieving higher editing performance without loss of downstream performance. 
\end{abstract}


\section{Introduction}

\begin{figure}[h]
    \centering

    \subfigure[MEMIT]{
        \includegraphics[width=0.45\linewidth]{figures/main-paper-figures/memit-llama3-weight-norm.png} % Replace with your image
        \label{fig:norm-growth-subset-alphaedit}
    }
    \subfigure[AlphaEdit]{
        \includegraphics[width=0.45\linewidth]{figures/main-paper-figures/alphaedit-llama3-weight-norm.png} % Replace with your image
        \label{fig:norm-growth-subset-MEMIT}
    }
    \vskip -0.1in
    \caption{The continuous growth of norm of edited MLP matrices in LLama3-8B during sequential knowledge editing, as a function of number edits.}
    \label{fig:norm-growth-subset}
    \vskip -0.1in
\end{figure}


Knowledge editing is the task of editing specific facts that a model learns during pretraining in a data and computing efficient manner \cite{metamodel, editing-survey, grace}. In this paper, we focus on a category of parameter-modifying knowledge editing methods called ``locate-then-edit" methods, which modify only a small subset of the model parameters to add or update knowledge. These methods have been the focus of many recent works \cite{hurt, akshat-rebuilding, akshat-unified, ma2024perturbation, composable-interventions, alphaedit}. While prior work showed knowledge editing methods like ROME \cite{ROME} and MEMIT \cite{MEMIT} lead to a catastophic loss of downstream performance within a few hundred sequential edits \cite{hurt, akshat-catastrophic}, a recently introduced method called AlphaEdit \cite{alphaedit} has been able to push this to 3000 edits. In this paper, we aim to enable large-scale sequential knowledge editing without causing model degradation. Towards this goal, we push sequential knowledge editing to what we propose as the next frontier - \textit{performing 10,000 sequential knowledge edits \textbf{without} loss of downstream performance}.


%While prior work showed knowledge editing methods like ROME \cite{ROME} and MEMIT \cite{MEMIT} lead to a catastophic loss of downstream performance within a few hundred sequential edits \cite{hurt, akshat-catastrophic}, a recently introduced method called AlphaEdit \cite{alphaedit} has been able to push this to 3000 edits. 

%say that recent works show norm or condition number is the issue. But we show that editing causes an increase in singular value in all singular values, resulting in increase in norm and also condition number since larger singular values increase at a larger rate (explaining both previous observations). It causes a preference towards information contained in a narrow subspace of the input, causing the model to do not incorporate the entire input vector. solution - > edit model such that the edited matrix still looks at the entire reresentation space rather than just a subset corresponding to high singular values. 




Our work is based on two main observations. Firstly, we show that existing locate-then-edit knowledge editing methods are prone to overfitting on the edited fact, where the output probability of an edited fact is unusually higher when compared to the confidence with which an unedited model predicts facts. Secondly, we show that sequential knowledge updates made to a model consistently lead to an increase in the norm of the weight matrix being edited, as shown in Figure \ref{fig:norm-growth-subset}. We show that this norm-growth is a secret trick used by locate-then-edit methods, which we call ``importance hacking". The increasing norm of the edited matrix also increases the norm of the activation vectors produced by the edited layers on average. This allows the outputs produced from the edited layers to have a larger influence on the final output of the model. With this, the edited layers are able to override the information coming from other parts of the model, leading to successful knowledge edits but inadvertently causing a loss of general ability, which might require information coming from other parts of the model. 

%While the increasing norm may not be concerning in general, it is especially detrimental for performing localized updates as done in locate-then-edit knowledge editing methods. This is because disproportionate and continuous growth in the norm of one or few intermediate layers of a model, while the rest of the model remains frozen, will compromise the balance and stability of the entire system, eventually leading to a breaking point. This disproportionate growth of intermediate layers is shown in Figure. (include this after experiments) We also study to consequence of norm growth and give insights into the mechanisms of knowledge editing. 

Based on these observations, we present targeted interventions to overcome the limitations of overfitting on edited facts and disproportionate norm growth. We first formalize locate-then-edit methods as a two-step finetuning process, where the first step uses gradient descent for optimization and the second step directly optimizes the loss using a least-square objective. This way of understanding knowledge editing enable us to apply appropriate interventions depending on the optimization scenario. To mitigate overfitting during the gradient descent process, we propose \textbf{M}ost-\textbf{P}robable \textbf{E}arly \textbf{S}topping (MPES) - where we halt gradient descent when edited facts become most probable across all the different contexts used to calculate the loss. MPES reduces overfitting on edited facts and consequently improves editing performance while reducing loss of downstream performance. To counteract the growth of the norm of the edited matrix during sequential knowledge editing, we add a frobenius-norm constraint within the knowledge editing objective of locate-then-edit methods \citep{MEMIT}. This constraint still allows for a closed-form solution and gives us the benefits of computationally efficient knowledge edits while allowing us to control the increasing norm of edited weight matrices, thus enabling longer and more robust sequential knowledge editing. 
 

%While both these interventions improve knowledge editing and downstream performance, they alone are unable to achieve our goal of performing 10,000 sequential edits without model degradation. 

Towards large-scale sequential knowledge editing, we present \textbf{ENCORE} - \textbf{\underline{E}}arly stopping and \textbf{\underline{N}}orm-\textbf{\underline{Co}}nstrained \textbf{\underline{R}}obust knowledge \textbf{\underline{E}}diting. ENCORE combines MPES with Frobenius-norm constrained objective and enables performing 10,000 sequential knowledge edits while maintaining the downstream performance of the original unedited model. ENCORE significantly outperforms prior locate-then-edit methods like ROME, MEMIT, and AlphaEdit, as verified on GPT2-XL, Llama-2-7B, and Llama-3-8B, and is significantly faster than its predecessors. Specifically, ENCORE is 61\% faster than MEMIT and 64\% faster than AlphaEdit when editing Llama3-8B.


%This also shows improvement. Combination leads to 10k edits without model degradation. 

%In the next paragraph present your method as a new approch to model editing. 

%We then present locate-then-edit knowledge editing methods as a two-step finetuning process. This way of understanding knowledge editing enables us to see how current knowledge editing methods lead to overfitting on the edited facts. Secondly, we show that 



%While some recent works try to point out the reasons for such model degradation \cite{akshat-catastrophic, akshat-rebuilding, hurt, ma2024perturbation}, an exhaustive explanation of the reasons behind this degradation has remained elusive. In this paper, we show that knowledge editing using locate-then-edit methods causes an increase in \textbf{all} singular values of the edited matrix, with larger singular values increasing at a higher rate. This also explains the increase in the frobenius norm \cite{akshat-catastrophic} and the condition number \cite{ma2024perturbation} of the edited matrix, which have been two proposed reasons for model degradation in prior works. Since the rate of increase of larger singular values is much larger than the smaller singular values, the model begins to prefer the information contained in a subspace defined by a small set dominant singular values, causing the information contained outside this subspace to not be passed through the model. This is what causes the model to slowly degrade and eventually collapse. 

%In this paper, we first revisit classic locate-then-edit knowledge editing methods and analyze the growing singular values during continuous sequential editing. We then show that prior knowledge editing methods were prone to overfitting on the edited fact, where the final probability of the edited facts was unusually higher compared to the confidence with which a model knows unedited facts. This overfitting leads to worse editing performance as well as larger forgetting of previously edited facts. We then present different families of regularization methods for preventing overfitting on edited facts and growth of singular values of the edited matrix, resulting in long term sequential knowledge editing with significantly minimal model degradation on downstream tasks. 


%As a summary, we make the following contributions:
%\begin{itemize}
%    \item We present locate-then-edit knowledge editing methods as a two-step finetuning process. This way of understanding knowledge editing enables us to see how current knowledge editing methods lead to overfitting to the edited facts.
%    \item We show that the loss of downstream performance and editability of the models are accompanied by increase in all singular values of the edited matrix. 
%    \item We present various regularization methods to reduce overfitting on the edited fact as well mitigate increase in singular values of the edited matrix. We are able to improve long term sequential knowledge editing by x number of edits without loss of downstream performance. 
%    \item Finally, we also show that single layer sequential knowledge editing performs comparabily to multi-layer knowledge editing, with fewer forgetting of previously edited facts.
%\end{itemize}



\section{Background and Related Work}\label{sec:background}
In this section, we provide a brief introduction to locate-then-edit knowledge editing methods and present them as a two-step fine-tuning process. For a more detailed introduction to these methods, we refer the reader to prior works \cite{ROME, MEMIT, akshat-unified}. 

Locate-then-edit family of methods like ROME \cite{ROME} , MEMIT \cite{MEMIT} and AlphaEdit \cite{alphaedit} are used to update facts that can be represented in the form of triplets of the form (subject, relation, object) or $(s,r,o)$. Instead of updating all the weights of a model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall \cite{ROME}. The location of an edit within a model is described by a two-dimensional address - (i) an intermediate layer to be edited and (ii) a token from the list of input tokens used to create the target representation. The exact layer to be edited is found using causal tracing \cite{ROME} or an empirical sweep over all decoder layers of the model \cite{localization-inform-editing, akshat-llama3}. Additionally, updating the second MLP layer in the FFN module of decoder layers has shown optimal knowledge editing performance \citep{key-value-memories, ROME}. This provides the first part of the editing address. \citet{ROME} also showed that using the output representation of the subject token of a sentence produces the best editing results. This provides the second part of the editing address, where the edit is made using the position index of the last token of the subject. We explain this with an example below.


\begin{figure}[t]
    \centering
    % Subfigure 1
    \subfigure[Gradient descent step which finds the target activations for the MLP matrix.]{
        \includegraphics[width=0.45\linewidth]{figures/ft-step1.png} % Replace with your image
        \label{fig:editing-process-gd}
    }
    \hfill
    % Subfigure 2
    \subfigure[Target activations are used to update the second MLP matrix (in red).]{
        \includegraphics[width=0.45\linewidth]{figures/ft-step2.png} % Replace with your image
        \label{fig:editing-process-closed-form}
    }
    \caption{Presenting locate-then-edit knowledge editing methods as a two-step fine-tuning process.}
    \label{fig:editing-process}
    \vskip -0.1in
\end{figure}


Given a fact to be edited, for example - \textit{``The capital of Malaysia is Singapore"}, the query phrase for the editing process is \textit{``The capital of Malaysia is"} and the target phrase is \textit{``Singapore"}. The first part of the editing address, the exact layer whose second MLP matrix gets edited, is decided before the editing begins. The second part of the editing address is the token index of the last subject token, which in this case would be the last subword-token in \textit{``Malasiya"}. The intermediate hidden representation of this last subject token is used to make the edit.

Once the editing address has been decided, instead of updating the chosen MLP weight matrix directly using gradient descent, the process of locate-then-edit knowledge editing proceeds in two steps -  

\begin{enumerate}
    \item In the first step (Figure \ref{fig:editing-process-gd}), gradient descent is used to find the appropriate activation vector that acts as a target for the weight matrix to be edited. This target activation is found such that the edited fact is generated by the model. In the example, this activation will cause the model to generate ``Singapore" in response to the question. Note that in this step, no weights are updated and just an intermediate activation vector is found. This new activation vector now serves as the target for the MLP weight matrix chosen for editing.

    \item  The weight update happens in the second step of editing (Figure \ref{fig:editing-process-closed-form}), where the MLP matrix is updated with the target activation vector found in the previous step, using a least squares loss function. This loss function tries to preserve the outputs of the MLP matrix for unrelated contexts while generating the target activation when the input corresponds to the query phrase. 
\end{enumerate}

% \begin{table*}[t]
% \caption{Comparison between prediction probabilities of facts that have been edited into a model compared to facts that a model knows through pretraining. }
% \label{tab:overfitting}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Method & Model  & \multirow{2}{*}{\makecell{Unedited \\ 
%  Fact Prob.}} & \multirow{2}{*}{\makecell{Edited \\ Fact Prob.}} \\
% & & & \\
% \midrule
% EMMET    & GPT2-XL& & 0.98&  \\
%  & Llama2-7B& & 0.92 &\\
%     & Llama3-8B& &  0.99& \\
% \midrule
% MEMIT    & GPT2-XL& & 0.65 & \\
%  & Llama2-7B& & \\
%     & Llama3-8B& &  \\
% \midrule
% AlphaEdit    & GPT2-XL& &  0.98 \\
%   & Llama2-7B& & \\
%      & Llama3-8B& &  \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}



Specifically, let $W_0$ be the initial weights of the second MLP matrix, which is being edited to $\hat{W}$. $k_0$ is used to indicate an input to the MLP matrix representing activation vectors for information we want to preserve from the original model, and $k_e$ is input activation vectors representing facts we want to insert into the model. Let $v_e$ be the desired target activation vector for the edited MLP matrix found in step 1 of editing using gradient descent. Then the loss function used to update the MLP weight matrix is formulated using least-squares in the form of a preservation-memorization objective \cite{akshat-unified}:
\vskip -0.5cm
\begin{equation}\label{eq:memit_objective}
\begin{aligned}
     \underset{\hat{W}}{\operatorname{argmin}} \hspace{5pt} L(\hat{W}) \hspace{10pt} \text{where}& \hspace{50pt}\\ 
     L(\hat{W}) = \hspace{4pt} \underbrace{\lambda \sum^{P}_{i=1} \left\| \hat{W} k^i_0 - W_0 k^i_0 \right\|^2_2}_{\text{preservation}}  +&
     \underbrace{\sum^{B}_{j=1} \left\|\hat{W} k^j_e - v^j_e\right\|^2_2}_{\text{memorization}}
\end{aligned}
\end{equation}




%A closed form solution for the above objective exists, and is shown below:

%\begin{equation}\label{eq:memit}
%\begin{aligned}
%    \hat{W} &= W_0 + \Delta \hspace{10pt} \text{where} \hspace{10pt}  
%    \\ \Delta &= \big(V_E - W_0K_E \big) K_E^T \big( \lambda C_0 + K_EK_E^T \big)^{-1}
%\end{aligned}
%\end{equation}

Since the above objective is linear in the argument, we do not need to use gradient descent for optimization. Thus, locate-then-edit methods can be seen as a unique type of fine-tuning method. Instead of updating the MLP matrix directly using gradient descent on the desired data, the weight update happens in two steps using two different types of objective functions for each step. The first step uses gradient descent whereas the second step uses a closed-form solution. %Closed-form solutions are rare in deep learning models, which is why knowledge editing methods might seem different from fine-tuning, but if we define fine-tuning as optimizing for a loss function to produce a desired output for weight updates, then knowledge editing is indeed fine-tuning. 

%\citet{gangadhar2024model} empirically support this by showing that knowledge editing using standard fine-tuning produces similar results to locate-then-edit knowledge editing methods. If we consider knowledge editing methods as fine-tuning, observing signs of overfitting and catastrophic fogetting \cite{akshat-catastrophic} are natural outcomes. 

%Also write about how it is closely related to continually learning methods like replay.




\section{Methods, Models, Datasets and Evaluation}
In this paper, we focus on three ``locate-then-edit'' methods - ROME \cite{ROME}, MEMIT \cite{MEMIT} and AlphaEdit \cite{alphaedit}. As discussed in section \ref{sec:background}, each of these algorithms use gradient descent to find intermediate target representations followed by a second optimization step which is linear in the argument. ROME uses an equality-constraint to enforce memorization in place of the least square constraint as shown in equation \ref{eq:memit_objective}, whereas equation \ref{eq:memit_objective} represents the objective function used in MEMIT. AlphaEdit adds a null-space projection term in the MEMIT objective. 



% \begin{table}[t]
% \caption{Comparison between prediction probabilities of facts that have been edited into a model compared to facts that a model knows through pretraining. }
% \label{tab:overfitting}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccc}
% \toprule
% Method & Model &  \makecell{Edited \\ Fact Prob.} & \makecell{Post MPES} \\
% \midrule
% EMMET    & GPT2-XL    & 0.98 & 0.47 \\
%          & Llama2-7B  & 0.92 & 0.59 \\
%          & Llama3-8B  & 0.99 & 0.61 \\
% \midrule
% MEMIT    & GPT2-XL    & 0.65 & 0.14 \\
%          & Llama2-7B  & 0.75 & 0.41 \\
%          & Llama3-8B  & 0.77 & 0.41 \\
% \midrule
% AlphaEdit & GPT2-XL   & 0.98 & 0.39 \\
%           & Llama2-7B & 0.84 & 0.37 \\
%           & Llama3-8B & 0.75 & 0.33 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}


Prior work has shown model degradation during sequential editing with ROME and MEMIT \cite{hurt, akshat-catastrophic}, whereas AlphaEdit is a recent method that is able to perform sequential editing for up to 3,000 facts. In this paper, we adopt the experimental setting of AlphaEdit where they perform sequential edits in batches of 100 facts. This means that 100 facts are edited into the model with each weight update, and multiple such updates are performed sequentially. Since ROME only allows for one edit to be made at a time to the model, we used the batched generalization of ROME, called EMMET \cite{akshat-unified}, which uses the same equality-constraint objective as ROME but generalizes it to the batched editing case. 

We evaluate all algorithms on three representative models - GPT2-XL \cite{gpt-2}, Llama2-7B \cite{llama2} and Llama3-8B \cite{akshat-llama3}. All experiments are performed on the CounterFact \cite{ROME} and zsRE \cite{MEND} datasets, which are standard knowledge editing datasets. We present the results for Llama2-7B and Llama3-8B on CounterFact dataset in the main paper and present the remaining results in the appendix due to space constraints.


%while the results for GPT2-XL and for all models on zsRE dataset are presented in the appendix due to space constraints.

In this paper, we evaluate the editing algorithms along two dimensions - editing performance and downstream performance. The editing performance evaluates the success of the knowledge editing algorithm in making successful edits, while downstream performance evaluates the extent of model degradation following prior work \cite{alphaedit, akshat-catastrophic, hurt}. 



\begin{table*}[t]
\caption{Comparison between prediction probabilities of edited facts versus facts that a model knows through pretraining. MPES results in more natural prediction probabilities while being 39\% - 76\% faster than other methods.}
\label{tab:overfitting}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc|ccc|cc}
\toprule
Method & Model & \makecell{Original Fact \\ Prob} & \makecell{Edited Fact \\Prob w/o MPES} & \makecell{Edited Fact \\ Prob w/ MPES} & \makecell{Time Per Edit\\ w/o MPES (s)} & \makecell{Time Per Edit\\ w/ MPES (s)} \\
\midrule
EMMET    & GPT2-XL    & 0.39 & 0.98 & 0.47 & 1.26 & 0.63 ($\downarrow$ 50\%)\\
         & Llama2-7B  & 0.52 & 0.92 & 0.59 & 6.61 & 1.86 ($\downarrow$ 71\%)\\
         & Llama3-8B  & 0.49 & 0.99 & 0.61 & 7.65 & 1.86 ($\downarrow$ 76\%)\\
\midrule
MEMIT    & GPT2-XL    & 0.39 & 0.65 & 0.14 & 1.60 & 0.97 ($\downarrow$ 39\%)\\
         & Llama2-7B  & 0.52& 0.75 & 0.41 & 4.84 & 2.79 ($\downarrow$ 42\%)\\
         & Llama3-8B  & 0.49 & 0.77 & 0.41 & 8.71 & 3.31 ($\downarrow$ 61\%)\\
\midrule
AlphaEdit & GPT2-XL   & 0.39 & 0.98 & 0.39 & 1.49 & 0.83 ($\downarrow$ 44\%)\\
          & Llama2-7B & 0.52 & 0.84 & 0.37 & 5.89 & 2.69 ($\downarrow$ 54\%)\\
          & Llama3-8B & 0.49 & 0.75 & 0.33 & 9.44 & 3.43 ($\downarrow$ 63\%)\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\paragraph{Knowledge Editing Metrics:} To evaluate editing performance, we use five standard knowledge editing metrics \cite{ROME}. (i) Efficacy Score (ES), which measures if an edit was successfully made, (ii) Paraphrase Score (PS), which measures if the model is able to recall edited facts in different scenarios, (iii) Neighborhood Score (NS), which measures if edited facts disturbs unrelated knowledge, (iv) Overall Score (S), which is the harmonic mean of ES, PS and NS, and (v) Generation Entropy (GE), which measures the fluency of a model. A detailed explanation of these metrics is given in Appendix \ref{appendix:knowledge-editing-metrics}.

%A lower GE indiciates repetitive text generation, a common failure mode \cite{ROME}.

The editing metrics for each model are calculated after making all 10,000 sequential edits \cite{alphaedit}. This approach ensures that the metrics capture both the success of the edits of the latest batch of facts as well as facts that were edited in the past. 



%We use the following editing metrics to evaluate editing performance of the algorithm:

% \begin{enumerate}
%     \item \textbf{Efficacy Score (ES)}: assesses whether an edit has been successful. It is calculated as the percentage of edits where $P(\text{new fact}) > P(\text{old fact})$ when evaluated on paraphrases of the query prompt.   
%     \item \textbf{Paraphrase Score (PS)}: measures the model's ability to generalize after an edit. Specifically, it is the percentage of edits where $P(\text{new fact}) > P(\text{old fact})$ for paraphrased versions of the query prompt.
%     \item \textbf{Neighborhood Score (NS)}: evaluates the locality of a model edit by determining whether editing one fact affects other facts stored in the model. It is the percentage of unaffected facts in the neighborhood of the edited fact.
%     \item \textbf{Generation Entropy (GE)}: measures the fluency of the model's text generation post-edit. GE is computed as the weighted average of bi-gram and tri-gram entropies in the text generated by the edited model. A lower GE indicates repetitive text generation, a common failure mode \cite{ROME, akshat-catastrophic}. 
%     \item \textbf{Score (S)}: introduced by \cite{ROME}, this composite metric combines edit success, generalization, and locality into a single score. It is calculated as the harmonic mean of the Efficacy Score (ES), Paraphrase Score (PS), and Neighborhood Score (NS).
% \end{enumerate}


% \begin{table*}[t]
% \caption{Comparison between prediction probabilities of facts that have been edited into a model compared to facts that a model knows through pretraining. }
% \label{tab:overfitting}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lc|ccc|cc}
% \toprule
% Method & Model & \makecell{Unedited Fact \\ Prob} & \makecell{Edited Fact \\Prob w/o MPES} & \makecell{MPES Edited \\ Fact Prob} & \makecell{Time Per Edit\\ w/o MPES (s)} & \makecell{MPES Time\\ Per Edit (s)} \\
% \midrule
% EMMET    & GPT2-XL    & 0.39 & 0.98 & 0.47 & 1.26 & 0.63 ($\downarrow$ 50\%)\\
%          & Llama2-7B  & 0.52 & 0.92 & 0.59 & 6.61 & 1.86 ($\downarrow$ 71\%)\\
%          & Llama3-8B  & 0.49 & 0.99 & 0.61 & 7.65 & 1.86 ($\downarrow$ 76\%)\\
% \midrule
% MEMIT    & GPT2-XL    & 0.39 & 0.65 & 0.14 & 1.60 & 0.97 ($\downarrow$ 39\%)\\
%          & Llama2-7B  & 0.52& 0.75 & 0.41 & 4.84 & 2.79 ($\downarrow$ 42\%)\\
%          & Llama3-8B  & 0.49 & 0.77 & 0.41 & 8.71 & 3.31 ($\downarrow$ 61\%)\\
% \midrule
% AlphaEdit & GPT2-XL   & 0.39 & 0.98 & 0.39 & 1.49 & 0.83 ($\downarrow$ 44\%)\\
%           & Llama2-7B & 0.52 & 0.84 & 0.37 & 5.89 & 2.69 ($\downarrow$ 54\%)\\
%           & Llama3-8B & 0.49 & 0.75 & 0.33 & 9.44 & 3.43 ($\downarrow$ 63\%)\\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}



\paragraph{Downstream Performance Metrics:} Following prior work \cite{alphaedit, akshat-catastrophic}, we measure downstream performance during knowledge editing using 6 tasks - massive multitask language understanding (MMLU) \cite{MMLU}, natural language inference (NLI, RTE) \cite{nli1, nli2, nli3}, sentiment analysis (SST2) \cite{sst2}, paraphrase detection (MRPC) \cite{mrpc}, and linguistic acceptability classification (CoLA) \cite{cola}. We measure the downstream performance of models every 1000 edits, following the experimental settings of \citet{alphaedit}. More details about experimental details can be found in Appendix \ref{appendix:downstream}.



\section{Overfitting during Knowledge Editing}
%Our work begins with a simple but powerful observation that models edited using current knowledge editing methods overfit on the edited facts. The overfitting can be observed by comparing the probability with which a model predicts an edited fact to an unedited fact. This was also observed in \citet{overfitting-modelediting}. We hypothesize that this overfitting on edited facts is one of the reasons why edited models lose their ability to perform downstream tasks, and when edited sequentially also forget facts that were edited in the past \cite{akshat-catastrophic}. 


As discussed in section \ref{sec:background}, locate-then-edit methods can be seen as a two-step fine tuning process. The first step uses gradient descent to create intermediate target representations. The gradient descent step minimizes average cross-entropy loss for predicting the target fact over the query phrase augmented by `$N$' random prefixes. The random prefixes are supposed to represent different contexts in which the edited fact can be recalled, thus aiming to create a more general query representation. The average cross-entropy loss over all contexts is optimized as shown:

\begin{equation}
   L (\theta) = \frac{1}{N} \sum^{j = N}_{j = 1} -log \mathcal{P}_\theta [ o^*| x_j + p ]
\end{equation}

Here $p$ represents the query phrase input to the model, $o^*$ represents the target fact to be edited into the model and $x_j$ represent random prefixes added to the query phrase.


\begin{figure*}[h]
    \centering
    
    \subfigure[MEMIT]{
        \includegraphics[width=0.31\linewidth]{figures/downstream-combo-plots/MEMIT_downstream_f1_combined_mpes.png} % Replace with your image
        \label{fig:downstream-mpes-memit}
    }
    \hfill
    \subfigure[AlphaEdit]{
        \includegraphics[width=0.31\linewidth]{figures/downstream-combo-plots/AlphaEdit_downstream_f1_combined_mpes.png} % Replace with your image
        \label{fig:downstream-mpes-alphaedit}
    }
    \hfill
    \subfigure[EMMET]{
        \includegraphics[width=0.31\linewidth]{figures/downstream-combo-plots/EMMET_downstream_f1_combined_mpes.png} % Replace with your image
        \label{fig:downstream-mpes-emmet}
    }
    
    
    \caption{Average downstream performance measured over 6 tasks (sec \ref{sec:background}) for MEMIT, AlphaEdit and EMMET. We see that MPES is able to delay loss of downstream performance for both Llama3-8B and Llama2-7B with additional gains in efficiencey.}\label{fig:downstream-baseline-main}
\end{figure*}



\begin{table*}[t]
\caption{Sequential knowledge editing performance after 10,000 edits for different algorithms. We see that using MPES improves editing metrics across all algorithms and models.}
\label{tab:editing-performance-overfitting}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Method & Model  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} & \multirow{2}{*}{\makecell{Overall \\ Score}} & \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
& & & \\
\midrule
MEMIT & Llama2-7B&   81.04 & 64.67 & 60.95 & 67.859 & 442.59 \\
    & Llama3-8B&   49.68 & 49.29 & 51.31 & 50.078 & 373.48  \\
MEMIT   & Llama2-7B& \textbf{88.43} & \textbf{70.83} & \textbf{65.86} & \textbf{73.873} & \textbf{542.1} \\
+ MPES & Llama3-8B&  \textbf{65.78} & \textbf{57.58} & \textbf{50.25} & 
 \textbf{57.176} & \textbf{560.78}  \\
\midrule
AlphaEdit  & Llama2-7B&  61.1 & 55.86 & 53.75 &  56.74 & 540.92 \\
     & Llama3-8B&  72.67 & 63.44 & 52.9 & 61.948 & 465.81  \\
AlphaEdit  & Llama2-7B&  \textbf{84.15} & \textbf{74.94} & \textbf{62.87} & \textbf{72.933} & \textbf{583.4} \\
+ MPES & Llama3-8B&   \textbf{88.43} & \textbf{82.08} & \textbf{56.5} & \textbf{72.832} & \textbf{565.36}  \\
\midrule
EMMET  & Llama2-7B &  94.98 & 84.05 & 55.76 & 74.331 & 569.24 \\
    & Llama3-8B&  85.03 & 75.06 & 49.08 & 65.995 & \textbf{567.34}\\
EMMET & Llama2-7B&  \textbf{96.8} &\textbf{88.4} & \textbf{58.41} & \textbf{77.393} & \textbf{584.28} \\ 
+ MPES & Llama3-8B&  \textbf{92.96} &\textbf{86.12} & \textbf{55.74} & \textbf{74.424} & 564.82  \\  
 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}





% \begin{figure}[h]
%     \centering
    
%     \subfigure[EMMET - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/EMMET_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama2}
%     }
%     \subfigure[EMMET - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/EMMET_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama3}
%     }

%     \vspace{1em} % Space between rows

%     \subfigure[MEMIT - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/MEMIT_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-memit-llama2}
%     }
%     \subfigure[MEMIT - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/MEMIT_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-memit-llama3}
%     }

%     \vspace{1em} % Space between rows

%     \subfigure[AlphaEdit - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/AlphaEdit_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-alphaedit-llama2}
%     }
%     \subfigure[AlphaEdit - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-original/AlphaEdit_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-alphaedit-llama3}
%     }

    
%     \caption{Performance of edited models on downstream tasks for 10,000 sequential edits. The downstream evaluation is done after every 1000 edits.}\label{fig:downstream-baseline}
% \end{figure}




% \begin{figure}[h]
%     \centering
    
%     \subfigure[EMMET - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/EMMET_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama2}
%     }
%     \subfigure[EMMET - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/EMMET_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama3}
%     }

%     \vspace{1em} % Space between rows

%     \subfigure[MEMIT - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/MEMIT_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-memit-llama2}
%     }
%     \subfigure[MEMIT - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/MEMIT_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-memit-llama3}
%     }

%     \vspace{1em} % Space between rows

%     \subfigure[AlphaEdit - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/AlphaEdit_glue_f1-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-alphaedit-llama2}
%     }
%     \subfigure[AlphaEdit - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-mpes/AlphaEdit_glue_f1-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-alphaedit-llama3}
%     }

    
%     \caption{Post MPES (PUT IN THE CORRECT PLOTS!!)}\label{fig:downstream-mpes}
% \end{figure}



We observe that \textbf{when the gradient descent process stops, the intermediate target representations are over-optimized on the edited facts and make the probability of the target fact unusually high}. This can be seen in Table \ref{tab:overfitting}, where the edited facts are predicted with a probability much closer to 1 compared to the probability with which an unedited model recalls a fact. This phenomenon was also observed by \citet{overfitting-modelediting}. More details on the experimental settings can be found in Appendix \ref{appendix:overfitting}. 


%This hyper-optimization over a small subset of edited facts leads to a model behavior where these points are recalled with unnaturally high certainty, which is not typical for broader, more general predictions. 

As seen in Table \ref{tab:overfitting}, the probability with which the unedited GPT2-XL, Llama2-7B, and Llama3-8B models recall a fact are 39\%, 52\% and 49\% respectively. However, facts that get edited into the model with algorithms like Alphaedit or EMMET are predicted with average probabilities of 98-99\%. The vocabulary size of these models is in the tens of thousands and placing such a high probability mass over every edited fact is a result of clear overfitting. We hypothesize that this overfitting of edited facts is one of the reasons why edited models lose their ability to perform downstream tasks when continuously edited using knowledge editing methods. 

\subsection{MPES and Knowledge Editing}
To overcome this overfitting over a small subset of edited facts, we propose a variant of early stopping called ``most-probable early stopping" (MPES). Conventionally, early stopping is used during training while monitoring validation loss, where training is halted when the validation loss stops improving. In MPES, we stop the gradient descent process in knowledge editing when the target fact becomes the most probable token for all `$q$' query phrases used for optimization. While the gradient descent loss continues to go down and the target probability continues to increase, the rank of the predicted word does not change further. With MPES, the rank of the predicted word performs the same function as validation loss in conventional early stopping.  %This contrasts with the current stopping criteria in knowledge editing, where gradient descent is halted after either 20 iterations or upon reaching a loss threshold. These criteria leave open the possibility of over-optimization (and hence overfitting, as observed empirically in Table \ref{tab:editing-performance-overfitting}), or underfitting.

Apart from preventing the model from becoming overly optimized towards a specific target fact, using MPES for halting gradient descent also has two other advantages. Firstly, it simplifies monitoring of the gradient descent process and provides a principled approach to stopping gradient descent which is directly tied to the knowledge editing objective, that is, accurately recalling edited facts in a variety of scenarios without overfitting. Secondly, by optimally stopping the gradient descent process, we also improve the efficiency of locate-then-edit algorithms. MPES leads to a 39-76\% reduction in editing time per edit depending on the editing algorithm and model used (Table \ref{tab:overfitting}). 



%The downstream performance of baseline EMMET, MEMIT and AlphaEdit for Llama2-7B and Llama3-8B for 10,000 edits is shown in Figure \ref{fig:downstream-baseline}. We see clear model degradation when performing sequential edits, especially for MEMIT on Llama3-8B. The downstream performance of these methods when using MPES is seen in Figure \ref{fig:downstream-mpes}. 

The average downstream performance of AlphaEdit, MEMIT, and EMMET for Llama2-7B and Llama3-8B over 10,000 sequential edits is shown in Figure \ref{fig:downstream-baseline-main}\footnote{For task-wise downstream performance, please refer to Appendix \ref{appendix:downstream-performance}}. Comparing baseline methods with MPES, we see that MPES shows significant improvements in downstream performance for both AlphaEdit and MEMIT. For EMMET, MPES performs on par with the baseline but is much faster, as shown in Table \ref{tab:overfitting}. This shows that by halting gradient descent at a critical point where the edited fact is recognized but not overly dominant, MPES helps maintain the model's general abilities for much longer during sequential editing.

The editing performance using MPES can be seen in Table \ref{tab:editing-performance-overfitting}. We see that MPES not only maintains the model's general abilities for longer but also improves editing performance for all algorithms. This is because as the model over-optimizes for edited facts, it not only loses its general abilities but also the ability to recall facts edited in previous iterations, resulting in lower editing scores. This also verifies our hypothesis that overfitting over edited facts is one of the reasons behind the loss of model's general ability.  

%thus showing that by halting gradient descent at the critical point where the edited fact is recognized but not overly dominant, MPES helps maintain the model's general abilities for much longer during sequential editing.


%Since the numbers in Table \ref{tab:editing-performance-overfitting} are calculated after finishing editing of all 10k facts, the performance also depends on if a model is able to recall facts that were edited in the very beginning. Overfitting on a current batch of edits as done in baseline methods also interfers with previously edited facts, which is mitigated when using MPES. 

To summarize, \textbf{with MPES we present a principled way of stopping the gradient descent step during knowledge editing} which results in improved editing performance, delays the loss of downstream performance, and makes current knowledge editing methods much faster. %MPES is a stark departure from prior work that have solely focused on improving knowledge editing through the optimization objective. To the best of our knowledge, this is the first work that improves editing performance by improving the gradient descent step in knowledge editing. Viewing 








%First motivate overfitting through data. Start from saying that overfitting can only happen in the gradient descent step and prior work has focussed solely on the objective. 







\begin{figure}[h]
    \centering

    \subfigure[Alphaedit (Llama3-8B)]{
        \includegraphics[width=0.45\linewidth]{figures/norm-growth/llama-3-8b_alphaedit.png} % Replace with your image
        \label{fig:norm-growth-alphaedit}
    }
    \subfigure[MEMIT (Llama3-8B)]{
        \includegraphics[width=0.45\linewidth]{figures/norm-growth/llama-3-8b_memit.png} % Replace with your image
        \label{fig:norm-growth-MEMIT}
    }

    \caption{Comparison between norm of edited MLP matrices and norm of unedited matrices after 5,000 and 10,000 sequential edits.}
    \label{fig:norm-growth}
\end{figure}






\section{Norm Growth during Sequential Knowledge Editing}
Past work has shown that sequential knowledge editing can lead to an increase in the norm of the edited matrix \cite{akshat-catastrophic}. This disproportionate growth of norm can be seen in Figure \ref{fig:norm-growth-subset} and \ref{fig:norm-growth} when using AlphaEdit and MEMIT on Llama3-8B. Both AlphaEdit and MEMIT modify the second MLP matrix of layers 4 to 8. To put the norm growth in perspective with the rest of the model, we plot the norms of the edited matrices after 5k and 10k edits along with the norm of other MLP matrices of the model. We clearly see the disproportionate growth in the norm for the edited layers in Figure \ref{fig:norm-growth}, where the norm grows to more than 10 times its original value when using MEMIT, and twice the original norm when using AlphaEdit, while the norm of the matrices before and after the edited layers is not changed. The growth of the norm of the edited matrix is a continuous process during knowledge editing. As shown in Figure \ref{fig:norm-growth-subset}, for not even one edit does the norm of the edited matrix remain constant or decrease during editing. The norm always increases!








But is this growing norm harmful to the model? We answer this question by analyzing the residual stream and reveal a very important property about the inner workings of knowledge editing methods. 

\subsection{The Secret Mechanics of Knowledge Editing}
Due to residual connections in an LLM, the final output of a model can be written as a summation of the outputs of the individual sub-modules. Let $h^l$ represent the intermediate hidden state vectors between each decoder layer. Then, the computations within a layer of a general decoder-only LLMs proceed as follows:
\begin{align}
    f^l &= \texttt{LN1}(h^{l-1}) \label{eq:ln1} \\
    a^l &= \texttt{Att}(f^l)\\
    g^l &= \texttt{LN2}(h^{l-1} + a^l) \label{eq:ln2}\\
    m^l &= W^l_{proj} \sigma(W^l_{fc}g^{l}  + b^l_{fc}) + b_{proj}\label{eq:MLP-math}\\ 
    h^l &= h^{l-1} + a^l + m^l\label{eq:resiudal-recursive}
\end{align}

The intermediate hidden vector between each layer, $h^l$, is also sometimes referred to as the \textit{residual stream}. \texttt{LN1} represents the first LayerNorm (or equivalently RMSNorm for Llama models) that acts just before the attention module and \texttt{LN2} represents the second LayerNorm just before the MLP module. \texttt{Att} represents the self-attention module in an LLM whereas the action of a traditional MLP module is written in terms of the individual weight matrices ($W_{fc}, W_{proj}$). As the vectors computed in the attention and MLP modules get added back to the residual stream at each layer, the residual stream represents a summation of an increasing number of vectors as we go deeper into the model. A non-recursive formula for the output of the transformer just before unembedding is shown below:

\begin{equation}\label{eq:resiudal-summation}
    h^L = h^{0} + \sum^{i = L}_{i = 1} a^i + \sum^{i = L}_{i = 1} m^i
\end{equation}





Here, $L$ represents the total number of layers in a model and $h^L$ represents the residual stream after the final layer. Note that $h^L$ is the activation just before the application of the final layernorm and unembedding matrices. Thus, the output vector at the final layer is a summation of the outputs of individual self-attention and MLP sub-modules. 

Now, if the norm of the $W_{proj}$ matrix grows as disproportionately as shown in Figures  \ref{fig:norm-growth-subset} and \ref{fig:norm-growth}, the norm of the vectors that are produced from those edited MLP sub-modules will also grow. As the norm of a few vectors in the summation in equation \ref{eq:resiudal-summation} grows, these vectors will begin to dominate the sum. Proof for this is shown in Appendix \ref{appendix:activation-norm-growth-proof}, where we show that if the norm of a vector in a summation grows, the overall sum effectively tends towards that vector.



\begin{figure*}[h]
    \centering
    \subfigure[Average Norm Proportion For Unedited Llama3-8B]{
        \includegraphics[width=0.45\linewidth]{figures/activation-norm-growth/average_norm_ratio_baseline_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-baseline}
    }
    \hfill
    \subfigure[Average Norm Proportion for Llama3-8B using Alphaedit]{
        \includegraphics[width=0.45\linewidth]{figures/activation-norm-growth/average_norm_ratio_alphaedit_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-alphaedit}
    }
    
    \subfigure[Average Norm Proportion for Llama3-8B using MEMIT]{
        \includegraphics[width=0.45\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-memit}
    }
    \hfill
    \subfigure[Average Norm Proportion for Llama3-8B using ENCORE]{
        \includegraphics[width=0.45\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama3-8b_combination.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-encore}
    }

    \caption{The figure shows the proportion of contribution of activations vectors produced from each sub-module to the residual stream. The edited layers are shown in red. We see that the influence of the output of the edited layers grows dramatically after contiuous editing.}
    \label{fig:activation-norm-growth}
\end{figure*}



We also show this effect empirically. The growing norm of activation vectors produced by edited layers before and after editing for Llama3-8B can be seen in Figure \ref{fig:activation-norm-growth}. Figure \ref{fig:activation-norm-growth-llama3-baseline} shows the percentage norm for activation vectors produced from each sub-module of the unedited Llama3-8B model, in comparison to the sum of their norms\footnote{This is averaged over 30,000 generated tokens (Appendix \ref{appendix:activation-norm-growth}).}. After editing using AlphaEdit, which edits layers 4-8 in the Llama model for 10,000 sequential edits (edited layers are shown in red color on x-axis), we can see a drastic increase in the percentage norms for vectors produced by the edited layers (Figure \ref{fig:activation-norm-growth-llama3-alphaedit}). For example, the norm of vectors produced by layer 8, which accounted for only 1\% of the total norm, after editing using AlphaEdit accounts for 7\% of the norm. This effect is even more drastic for MEMIT as seen in Figure \ref{fig:activation-norm-growth-llama3-memit}, where the activation vectors produced by layer 8 account for almost 40\% of the total norm, and the vectors produced by all edited layers account for 85\% of the total. 

%Thus, since the norm of the edited matrix continues to grow throughout the process of sequential editing (Figure \ref{fig:norm-growth}), the norm of the produced activation vectors also grow (Figure \ref{fig:activation-norm-growth}). 





This gives us a crucial insight into the inner workings of locate-then-edit methods. Apart from optimizing for the preservation-memorization objective in equation \ref{eq:memit_objective}, \textbf{these methods also \underline{implicitly} increase the norms of the edited matrices, and as a consequence, the norm of the activation vectors produced from those matrices also increases. This makes the representation after the final layer ($h^L$) be dominated by the output of edited layers,} thus giving the edited layers a larger authority over the final representations. This allows edited layers to override the information produced from other parts of the model, consequently leading to the success of these knowledge editing methods. 

This finding is central to explaining the inner workings of locate-then-edit methods, and it also explains their successes and failures. This kind of ``importance hacking'' makes the output of the edited layers more dominant in the final output and is a shortcut taken by these methods to achieve edit success. However, as the norm of the edited matrix continues to grow when we edit it sequentially, suddenly the final representations begin to largely be composed of the outputs of only the edited layers. This makes it impossible for the model to account for the information processed from other sub-modules which might be important to perform other downstream or unrelated tasks. We hypothesize that this is another cause of the loss of downstream performance. 


%Thus ideally, knowledge editing should not  

%Since the edited layers contain the edited fact and other information who's outputs are preserved in those matrices, 



%While this kind of ``importance hacking'' results in successful knowledge edits, paying too much attention on the activations of the edited matrix results in reduced importance given to output activations of other sub-modules. Being able to do downstream tasks may require representations extracted from modules and layers other than ones that were edited. Thus, as the model pays a lot more attention to the outputs generated from the edited layers, it slowly loses its general abilities. We hypothesize that this is the cause of loss of downstream performance.  %This effect is further exaggerated when models are continuously edited and the norm of the edited matrices continues to grow. 



%Thus blowing up of norm of the edited matrix results in a drastic increase in the norm of the vectors that are output from that sub-module. These vectors then dominate the residual stream which forms the output representation. Thus, the output representation in edited models have an anomalously large contributions from the edited layers, dominating the output representations. This also highlights the underlying mechanism behind these knowledge editing methods. \textbf{Apart from optimizing for the editing objectives, knowledge editing happens by increasing the norm of the edited matrix and consequently its output activations. Thus, the importance of the activations created from the edited matrix in the final residual summation is increased due to this norm increase.} While this kind of importance hacking results in successful knowledge edits, paying too much attention on the activations of the edited matrix results in reduced importance given to output activations of other sub-modules. We hypothesize that this is the cause of loss of downstream performance which might require representations extracted from modules other than the edited modules. 



\begin{table*}[t]
\caption{Editing performance of ENCORE when compared to baseline MEMIT and modifications using MPES and Norm Constraint. }
\label{tab:editing-performance-encore-main}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Method & Model  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} & \multirow{2}{*}{\makecell{Overall \\ Score}} & \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
& & & \\
\midrule
MEMIT & Llama2-7B&  81.04 & 64.67 & 60.95 &  67.859 & 442.59\\
    & Llama3-8B&  49.68 & 49.29 & 51.31 & 50.078 &  373.48\\
\midrule
MEMIT   & Llama2-7B&   88.43 & 70.83 & 65.86 & 73.873 & 542.1 \\
+ MPES & Llama3-8B&  65.78 & 57.58 & 50.25 & 57.17 & \textbf{560.78}\\

\midrule
MEMIT  & Llama2-7B& 90.94 & 81.31 & 59.73 & 74.931 & 539.58 \\
+ Norm-Const  & Llama3-8B&   85.72 & 77.08 & 58.45 & 71.86 & 367.46 \\
\midrule
ENCORE     & Llama2-7B&  \textbf{92.57} & \textbf{82.64} & \textbf{60.43} & \textbf{76.043} & \textbf{560.16}  \\
     & Llama3-8B& \textbf{88.77} & \textbf{78.19} & \textbf{60.07} & \textbf{73.707} &  523.61  \\
    
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}







\subsection{Introducing ENCORE}
%correct previous writing
%add all captions
%explain why norm constraint is only added to memit objective

In the above discussion, we show how growing norm of edited matrices is detrimental to the functioning of edited models and hypothesize that it is the cause of loss of downstream performance. To test our hypothesis, we propose to add an additional term to the preservation-memorization objective to control this norm growth. Thus, we augment the MEMIT objective with a norm-constraint:

\begin{equation}\label{eq:encore_objective}
\begin{aligned}
     L(\hat{W}) = \hspace{4pt} &\underbrace{\lambda_p \sum^{P}_{i=1} \left\| \hat{W} k^i_0 - W_0 k^i_0 \right\|^2_2}_{\text{preservation}}  +
     \underbrace{\sum^{B}_{j=1} \left\|\hat{W} k^j_e - v^j_e\right\|^2_2}_{\text{memorization}} \\
     & \hspace{50pt} + \underbrace{\lambda_n \left\|\hat{W}- W_0\right\|^2_F}_{\text{norm-constraint}}
\end{aligned}
\end{equation}


The original weight matrix is represented by $W_0$ and $\hat{W}$ represents the edited matrix. The above objective has a closed-form solution as shown below (proof, Appendix \ref{appendix:encore-proof}):
\begin{equation}
\begin{aligned}
     \hat{W} &= W_0 + \Delta \hspace{10pt}\text{where} \\
     \Delta = (V_{1} - W_{0}K_{1})&K_{1}^{T} (\lambda_p K_{0}K_{0}^{T} + K_{1}K_{1}^{T} + \lambda_n I)^{-1}
\end{aligned}
\end{equation}

Editing performance for 10,000 sequential edits using norm-constraint can be seen in Table \ref{tab:editing-performance-encore-main}. Since we add the norm-constraint to the MEMIT objective \footnote{While adding norm-constraint works well with the MEMIT objective, using norm constraint with EMMET and AlphaEdit does not lead to such extensive improvements. We discuss this further in Appendix \ref{appendix:norm-constraint-emmet}.}, we compare the performance of MEMIT and MEMIT+MPES. We see a significant improvement in all editing metrics for both Llama2-7B and Llama3-8B. The downstream performance can be seen in Figure \ref{fig:downstream-norm-constraint}, where norm-constraint sustains downstream performance for much longer compared to MEMIT and MPES. 



%This also verifies our hypothesis that rapid increase in norm of edited matrices is another cause of general model degradation. 

\begin{figure}[t]
    \centering
    
    \subfigure[Llama2-7B]{
        \includegraphics[width=0.45\linewidth]{figures/downstream-combo-plots/MEMIT_downstream_f1_combined_llama2.png} % Replace with your image
        \label{fig:downstream-encore-main-llama2}
    }
    \hfill
    \subfigure[Llama3-8B]{
        \includegraphics[width=0.45\linewidth]{figures/downstream-combo-plots/MEMIT_downstream_f1_combined_llama3.png} % Replace with your image
        \label{fig:downstream-encore-main-llama3}
    }
    
    \caption{Average downstream performance for during sequential editing with ENCORE compared to baseline of MEMIT and addition of MPES and Norm-Constraint (NC).}\label{fig:downstream-norm-constraint}
\end{figure}

\begin{figure}[t]
    \centering
    
    \subfigure[Comparison with unedited layers.]{
        \includegraphics[width=0.4\linewidth]{figures/norm-growth/llama-3-8b_regmemit_encore.png} % Replace with your image
        \label{fig:}
    }
    \hfill
    \subfigure[Norm growth as function of edits.]{
        \includegraphics[width=0.4\linewidth]{figures/norm-growth-subset/new_weights_norm_encore.png} % Replace with your image
        \label{fig:}
    }
    
    \caption{Growth in norm of edited matrices for ENCORE for Llama3-8B.}\label{fig:encore-norm-growth}
    \vskip -0.3cm
\end{figure}


To achieve our goal of performing 10,000 sequential edits without loss of downstream performance, we combine MPES with the norm-constraint objective in equation \ref{eq:encore_objective} and present \textbf{ENCORE} - \textbf{\underline{E}}arly stopping and \textbf{\underline{N}}orm-\textbf{\underline{Co}}nstrained \textbf{\underline{R}}obust knowledge \textbf{\underline{E}}diting. With ENCORE, we bring back the classical knowledge editing objective of MEMIT in a stronger, faster, and more robust way. The editing performance for ENCORE can be seen in Table \ref{tab:editing-performance-encore-main} and Figure \ref{fig:downstream-norm-constraint}. For Llama2-7B, we see that maximum improvement in downstream performance (Figure \ref{fig:downstream-encore-main-llama2}) comes from the norm constraint and ENCORE performs equally well. But ENCORE is not only much faster, but also has a better overall editing performance (Table \ref{tab:editing-performance-encore-main}). For Llama3-8B (Figure \ref{fig:downstream-encore-main-llama3}), we see that the combination of MPES and norm-constraint is crucial for sustained downstream performance. 

ENCORE also controls the growing norm of edited matrices, as shown in Figure \ref{fig:encore-norm-growth}. As a consequence, the activation norms for ENCORE have a very similar distribution to the original model, which is shown in Figure \ref{fig:activation-norm-growth-llama3-encore}. Thus, ENCORE is able to perform large-scale knowledge editing without taking the shortcut of importance hacking, which also prevents the loss of downstream performance. With ENCORE, we combine the insights presented in this paper about overfitting and implicit importance hacking in locate-then-edit methods and enable long-term sequential knowledge editing. ENCORE is able to consistently sustain downstream evaluation metrics of the original model while going through 10,000 sequential edits. ENCORE is also 61\% faster than MEMIT and 64\% faster than AlphaEdit for Llama3-8B\footnote{ENCORE takes 1.01 seconds per edit for GPT2-XL, 3.33 seconds for Llama2-7B and 3.35 seconds for Llama3-8B.}. 





%The downstream performance for Llama2-7B is significantly more stable than MEMIT and Llama3-8B does not collapse and experiences a more gradual decay (refer to Figure \ref{fig:downstream-baseline} for baseline MEMIT plots). 





% \begin{figure}[h]
%     \centering
    
%     \subfigure[Norm-Constraint - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-encore/norm-constraint-llama2.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama2}
%     }
%     \subfigure[Norm-Constraint - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-encore/norm-constraint-llama3.png} % Replace with your image
%         \label{fig:downstream-baseline-emmet-llama3}
%     }
    
%     \caption{Norm Constraint}\label{fig:downstream-norm-constraint}
% \end{figure}




% \begin{figure}[h]
%     \centering
    
%     \subfigure[ENCORE - Llama2 (7B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-encore/encore-llama2.png} % Replace with your image
%         \label{fig:}
%     }
%     \subfigure[ENCORE - Llama3 (8B)]{
%         \includegraphics[width=0.4\linewidth]{figures/downstream-encore/encore-llama3.png} % Replace with your image
%         \label{fig:}
%     }
    
%     \caption{Encore}\label{fig:downstream-encore}
% \end{figure}










% \begin{figure}[h]
%     \centering
    
%     \subfigure[Average Norm Proportion for LLama3-8B using ENCORE]{
%         \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama3-8b_combination.png} % Replace with your image
%         \label{fig:}
%     }

%     \caption{Encore}\label{fig:encore-activation-norm}
% \end{figure}


\section{Conclusion}
In this paper we present two major drawbacks of existing knowledge editing methods - (i) overfitting on edited facts and (ii) continuous and disproportionate norm-growth of edited matrix. We also present insights into the inner workings of locate-then-edit methods and show that they achieve edit success using the short-cut of norm-increase, which increases the importance of the output of edited matrices. We then present ENCORE, a faster and stronger knowledge editing method compared to its predecessors. ENCORE is able to perform 10,000 sequential knowledge edits without noticeable loss of downstream performance in about 40\% of the time taken by other algorithms for Llama3-8B. While many recent works have shown limitations of knowledge editing methods at scale, we show that with a better understanding and appropriate regularizations, these methods can indeed be scaled. We hope this brings further research and excitement into the field of knowledge editing. 







\section*{Impact Statement}
This paper advances the field of knowledge editing in LLMs by enabling scalable, updates to a model's stored knowledge without significant model degradation. Such techniques could be beneficial for multiple practical and societal uses, including quick updating of time-sensitive facts such as changing a model's knowledge following natural disasters or newly published research. 

However, as with any technology, there are potential risks with this method. The ability to efficiently overwrite and retain specific facts raises concerns about malicious editing or the injection of wrong information. Further research on robust governance and the use of these frameworks is still needed.  Overall, we believe this research can lead to more responsive, up-to-date, and ethically aligned LLMs provided that careful attention is paid to responsible use.

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

% \begin{table*}[t]
% \caption{Knowledge editing performance for GPT2-XL on the CounterFact dataset for different algorithms in combination with MPES.}
% \label{tab:editing-performance-overfitting-gpt2xl}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccccccr}
% \toprule
% Model & Method  & \multirow{2}{*}{\makecell{Overall \\ Score}} & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} & \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
% & & & \\
% \midrule
% GPT2-XL &  EMMET    & 64.48& 79.27 & 67.45 & 52.39 & 570.24  \\
% & EMMET+MPES   & 73.43 & 95.08 & 79.47 & 56.33 & 555.39  \\
% & AlphaEdit    & 69.2 & 88.58 & 70.33 & 56.04 & 580.27  \\
% & AlphaEdit + MPES    & 76.32 & 95.52 & 82.08 & 60.03 & 565.44  \\
% & MEMIT    & 74.22 & 94.04 & 79.91 & 57.9 & 517.37  \\
% & MEMIT + MPES   & 73.68 & 91.43 & 73.68 & 61.71 & 532.47  \\
% & Norm-Constraint  &  74.53 & 93.89 & 80.9 & 58.0 & 504.68\\
% & ENCORE     &   74.58 & 93.21 & 78.04 & 59.95 & 524.34 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\begin{table*}[t]
\caption{Knowledge editing performance for GPT2-XL on the CounterFact dataset for different algorithms in combination with MPES.}
\label{tab:editing-performance-overfitting-gpt2xl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Model & Method  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} & \multirow{2}{*}{\makecell{Overall \\ Score}} & \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
& & & & & & \\
\midrule
GPT2-XL &  EMMET    & 79.27 & 67.45 & 52.39 & 64.48 & 570.24  \\
& EMMET+MPES   & 95.08 & 79.47 & 56.33 & 73.43 & 555.39  \\
& AlphaEdit    & 88.58 & 70.33 & 56.04 & 69.2 & 580.27  \\
& AlphaEdit + MPES    & 95.52 & 82.08 & 60.03 & 76.32 & 565.44  \\
& MEMIT    & 94.04 & 79.91 & 57.9 & 74.22 & 517.37  \\
& MEMIT + MPES   & 91.43 & 73.68 & 61.71 & 73.68 & 532.47  \\
& Norm-Constraint  &  93.89 & 80.9 & 58.0 & 74.53 & 504.68\\
& ENCORE     &   93.21 & 78.04 & 59.95 & 74.58 & 524.34 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\section{Knowledge Editing Metrics}\label{appendix:knowledge-editing-metrics}
A more detailed explanation of the knowledge editing metrics used in this paper is below:

\begin{enumerate}
    \item \textbf{Efficacy Score (ES)}: assesses whether an edit has been successful. It is calculated as the percentage of edits where $P(\text{new fact}) > P(\text{old fact})$ when evaluated on paraphrases of the query prompt.   
    \item \textbf{Paraphrase Score (PS)}: measures the model's ability to generalize after an edit. Specifically, it is the percentage of edits where $P(\text{new fact}) > P(\text{old fact})$ for paraphrased versions of the query prompt.
    \item \textbf{Neighborhood Score (NS)}: evaluates the locality of a model edit by determining whether editing one fact affects other facts stored in the model. It is the percentage of unaffected facts in the neighborhood of the edited fact.
    \item \textbf{Generation Entropy (GE)}: measures the fluency of the model's text generation post-edit. GE is computed as the weighted average of bi-gram and tri-gram entropies in the text generated by the edited model. A lower GE indicates repetitive text generation, a common failure mode \cite{ROME, akshat-catastrophic}. 
    \item \textbf{Score (S)}: introduced by \cite{ROME}, this composite metric combines edit success, generalization, and locality into a single score. It is calculated as the harmonic mean of the Efficacy Score (ES), Paraphrase Score (PS), and Neighborhood Score (NS).
\end{enumerate}






%Phudish
\section{Experimental Detail on Overfitting During Knowledge Editing}\label{appendix:overfitting}

% \begin{table*}[t]
% \caption{Comparison between prediction probabilities of facts that have been edited into a model compared to facts that a model knows through pretraining. }  
% \label{tab:overfitting-appendix}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccccc}
% \toprule
% Method & Model & \makecell{Next \\ Token} & \makecell{Unedited \\ Fact Prob.} & \makecell{Edited \\ Fact Prob.} & \makecell{Post MPES} \\
% \midrule
% EMMET    & GPT2-XL    & 0.26 & 0.04 & 0.98 & 0.47 \\
%          & Llama2-7B  & 0.38 & 0.19 & 0.92 & 0.59 \\
%          & Llama3-8B  & 0.36 & 0.12 & 0.99 & 0.61 \\
% \midrule
% MEMIT    & GPT2-XL    & 0.26 & 0.04 & 0.65 & 0.14 \\
%          & Llama2-7B  & 0.38& 0.19 & 0.75 & 0.41 \\
%          & Llama3-8B  & 0.36 & 0.12 & 0.77 & 0.41 \\
% \midrule
% AlphaEdit & GPT2-XL   & 0.26 & 0.04 & 0.98 & 0.39 \\
%           & Llama2-7B & 0.38 & 0.19 & 0.84 & 0.37 \\
%           & Llama3-8B &0.36 & 0.12 & 0.75 & 0.33 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}
%To evaluate the impact of probabilities associated with facts that have been edited into a model, as compared to those acquired through pre-training, as well as our MPES method. 
For each method and model, we conducted three different experiments: 

\begin{enumerate}

    \item Unedited fact recall probability - In this case, we calculate the average probability with which a fact is recalled by the unedited/original model. These are the facts that the model learnt through its pre-training. The model is asked questions from the CounterFact dataset, and we average the probability with which the model predicts the fact correctly. 
    
    \item Edited fact probability without MPES - In this case, we evaluate the probability with which a model recalls a fact that is edited into the model. This is the standard baseline case without MPES.
    
    \item Edited fact probability WITH MPES - Here we also evaluate the average probability by which a model recalls an edited fact. In this case, MPES is used during editing the fact.
\end{enumerate}


In each of the experiments we performed, we passed average numbers over 1000 edited facts with a batch size of 1. We use the CounterFact dataset \cite{ROME} for all of these experiments. We show the result in table \ref{tab:overfitting}. As we can see from the table, the unedited fact token probability is pretty low but once we run the edited fact the probability increases to almost 1 for some cases. MPES brings the probability of fact recall for edited facts down to a more natural value, which prevents the overfitting problem that we present in this paper.

%Phudish
\section{Experimental Details on Activation Norm Growth}\label{appendix:activation-norm-growth}
To assess the impact of the activation vectors generated by the edited layers before and after editing, we conducted an experiment where we edited our model on a total of 10,000 facts using a batch size of 100. Once the model was edited, we evaluated the norm of the activations produced by each layer by passing it through a Wikipedia dataset containing 30,000 examples. For each example, the model performed a one-word prediction task given an input context, and we measured the norm of the activation vectors produced from each layer in the model. We repeated the same process for the unedited model to compare the differences. 

For each of the 30,000 examples, we calculated the proportion of the activation norm at each layer. We then plotted the mean and standard deviation of these proportions for both the edited and unedited models in figures  \ref{fig:activation-norm-growth-GPT2-XL} - \ref{fig:activation-norm-growth-Llama3-8B-norm}. As shown in our results, the proportion of activation norms for the layers that were edited is significantly higher than their neighboring layers. In fact, some of the edited layers show the highest proportions overall. The edited layers are highlighted with \textit{red color} on the x-axis.
%Describe all the steps taken in the experiments 
%Add all the figures in the appendix 
%write a discussion on these figures
\subsection{Theoretical Analaysis of Growth of Vector Norm in a Summation}\label{appendix:activation-norm-growth-proof}
We want to understand the effect of excessive increase in the norm of a vector in a sum of vectors. First, let's start with an easy example where we have a summation of two vectors, $\bold{s} = \bold{a} + \bold{b}$ and then there is excessive increase in the norm of the first vector, that is $\bold{s} = k\bold{a} + \bold{b}$ where $k$ is some positive scalar. To evaluate the effect of this increase, we analyses the tendencies of the sum $\bold{s}$ as k increases. 

We first want to understand the norm of $||\bold{s}||$. We have the following :

\begin{align*}
    ||\bold{s}||^{2} = ||k\bold{a} + \bold{b}||^{2} = k^{2} ||\bold{a}||^{2} + 2k \bold{a} \cdot \bold{b} + ||\bold{b}||^{2}
\end{align*}

From this we can clearly see that as $k$ increases the first term quadratic in $k$ will dominate. This means that as $k \to \infty$, $||\bold{s}||^{2} \to k^{2} ||\bold{a}||^{2}$, or $||\bold{s}|| \to k ||\bold{a}||$, which is the norm of the new vector. Thus, as the norm of one of the vectors in the summation increases, the norm of the summation tends to the norm of that vector with increasing norm.

Next, we look at the tendencies of the orientation of the summation as the norm of one vector increases. Let $\theta$ be the angle between $\bold{s}$ and $k\bold{a}$. Then, the cosine of the angle between the summation and the new vector $k\bold{a}$ is as follows (note that angle between $\bold{s}$ and $k\bold{a}$ is same as the angle between $\bold{s}$ and $\bold{a}$):

\begin{align*}
    \cos{\theta} = \frac{\bold{s} \cdot \bold{a}}{||\bold{s}|| ||\bold{a}||} = \frac{(k\bold{a}+\bold{b})\cdot \bold{a}}{||k\bold{a}+\bold{b}|| ||\bold{a}||}
\end{align*}

In the limit of $k \to \infty$, $||\bold{s}|| \to k ||\bold{a}||$ as shown above. Thus, the cosine expression in the limit can be written as:

\begin{align*}
    \underset{k \to \infty}{\cos{\theta}} = \frac{(k\bold{a}+\bold{b})\cdot \bold{a}}{k||\bold{a}||^2} = \frac{k\bold{a}\cdot \bold{a}}{k||\bold{a}||^2} + \frac{\bold{b}\cdot \bold{a}}{k||\bold{a}||^2} \\
    = 1 +  \frac{\bold{b}\cdot \bold{a}}{k||\bold{a}||^2}
\end{align*}


Thus, as $k \to \infty$, the cosine of angle between the sum and the vector tends to 1, or the angle between the summation and the vector tends to zero. This shows that as the norm of a vector in the summation continues to increase, the both the norm and the orientation of the summation tends towards the vector with increasing norm.

Finally, the same proof can be generalied to a summation of multiple vectors, where $\bold{b}$ represents the sum of the remaining vectors.


\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For Unedited GPT2-XL]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_base_gpt2-xl.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2-xl-baseline}
    }
    \subfigure[Average Norm Proportion for GPT2-XL using Alphaedit]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_alphaedit_gpt2-xl.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2=xl-alphaedit}
    }
    \subfigure[Average Norm Proportion for GPT2-XL using EMMET]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_emmet_gpt2-xl.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2-xl-emmet}
    }
    \subfigure[Average Norm Proportion for GPT2-XL using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_gpt2-xl.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2-xl-memit}
    }
    \caption{Activation norm growth for GPT2-XL.}
    \label{fig:activation-norm-growth-GPT2-XL}
\end{figure}





\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For Unedited Llama2-7B]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_baseline_llama2-7b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama2-baseline}
    }
    \subfigure[Average Norm Proportion for Llama2-7B using Alphaedit]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_alphaedit_llama2-7b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama2-alphaedit}
    }
    \subfigure[Average Norm Proportion for Llama2-7B using EMMET]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_emmet_llama2-7b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama2-emmet}
    }
    \subfigure[Average Norm Proportion for Llama2-7B using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_llama2-7b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama2-memit}
    }
    \caption{Activation norm growth for Llama2-7B.}
    \label{fig:activation-norm-growth-llama2-7b}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For Unedited Llama3-8B]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_baseline_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-baseline-appendix}
    }
    \subfigure[Average Norm Proportion for Llama3-8B using Alphaedit]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_alphaedit_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-alphaedit-appendix}
    }
    \subfigure[Average Norm Proportion for Llama3-8B using EMMET]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_emmet_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-emmet}
    }
    \subfigure[Average Norm Proportion for Llama3-8B using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-memit-appendix}
    }
    \caption{Activation norm growth for Llama3-8B.}
    \label{fig:activation-norm-growth-llama3-8b}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For GPT2-XL using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_gpt2-xl.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2xl-memit-appendix}
    }
    \subfigure[Average Norm Proportion for GPT2-XL using Norm Constraint]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_gpt2-xl_norm.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2-xl-norm-appendix}
    }
    \subfigure[Average Norm Proportion for GPT2-XL using ENCORE]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_gpt2-xl_combination.png} % Replace with your image
        \label{fig:activation-norm-growth-gpt2-xl-combination-apendix}
    }
    \caption{Activation norm growth for GPT2-XL using Norm Constraint and ENCORE.}
    \label{fig:activation-norm-growth-gpt2-xl-norm}
\end{figure}



\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For Llama2-7B using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_llama2-7b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama2-7b-baseline-appendix}
    }
    \subfigure[Average Norm Proportion for Llama2-7B using Norm Constraint]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama2-7b_norm.png} % Replace with your image
        \label{fig:activation-norm-growth-Llama2-7B-norm-appendix}
    }
    \subfigure[Average Norm Proportion for Llama2-7B using ENCORE]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama2-7b_combination.png} % Replace with your image
        \label{fig:activation-norm-growth-Llama2-7B-combination-apendix}
    }
    \caption{Activation norm growth for Llama2-7B using Norm Constraint and ENCORE.}
    \label{fig:activation-norm-growth-Llama2-7B-norm}
\end{figure}


\begin{figure}[h]
    \centering
    \subfigure[Average Norm Proportion For Llama3-8B using MEMIT]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_memit_llama3-8b.png} % Replace with your image
        \label{fig:activation-norm-growth-llama3-8b-baseline-appendix}
    }
    \subfigure[Average Norm Proportion for Llama3-8B using Norm Constraint]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama3-8b_norm.png} % Replace with your image
        \label{fig:activation-norm-growth-Llama3-8B-norm-appendix}
    }
    \subfigure[Average Norm Proportion for Llama3-8B using ENCORE]{
        \includegraphics[width=\linewidth]{figures/activation-norm-growth/average_norm_ratio_regmemit_llama3-8b_combination.png} % Replace with your image
        \label{fig:activation-norm-growth-Llama3-8B-combination-apendix}
    }
    \caption{Activation norm growth for Llama3-8B using Norm Constraint and ENCORE.}
    \label{fig:activation-norm-growth-Llama3-8B-norm}
\end{figure}

\clearpage

%Now if we increase $k$ then we have that the numerator tend to $k ||\bold{a}||^{2}$ and the denominator will also tend to $k ||\bold{a}||^{2}$ as $k$ is dominating which mean that $\cos{\theta}$ is tending to 1 which mean that $\theta$ tend toward 0 which mean that $\bold{s}$ becomes more aligned with $\bold{a}$.




%Phudish % first part is P second part is B
\section{Proof for ENCORE Objective}\label{appendix:encore-proof}
First we have that we can write the equation \ref{eq:encore_objective} in term of matrix form where we can stack the $k_{0}^{i}$. Specifically, we define $K_{0} = [k_{0}^{1} \; | k_{0}^{2} \; | \cdots \; | k_{0}^{P}]$, 
$K_{1} = [k_{e}^{1} \; | k_{e}^{2} \; | \cdots \; | k_{e}^{B}]$ and $V_{1} = [v_{e}^{1} \; | v_{e}^{2} \; | \cdots \; | v_{e}^{B}]$


column wise and instead the L2 norm will become the frobenius norm and we have that 
\begin{equation*}
    \lambda_p \left\|\hat{W}K_{0}- W_0K_{0}\right\|^2_F + \left\|\hat{W}K_{1}- V_{1}\right\|^2_F + \lambda_n \left\|\hat{W}- W_0\right\|^2_F   
\end{equation*}
We can differentiate this expression with respect to $\hat{W}$ and let it equal to 0, we get the following 
\begin{align*}
    \lambda_p \hat{W}K_{0}K_{0}^{T} - \lambda_p W_{0}K_{0}K_{0}^{T} + \hat{W}K_{1}K_{1}^{T} 
    \\
    - V_{1}K_{1}^{T} + \lambda_n \hat{W} - \lambda_n W_{0} = 0
\end{align*}
\begin{align*}
    \lambda_p \hat{W}K_{0}K_{0}^{T} +  \hat{W}K_{1}K_{1}^{T} +  \lambda_n \hat{W} \\
    = \lambda_p W_{0}K_{0}K_{0}^{T} + V_{1}K_{1}^{T} + \lambda_n W_{0}
\end{align*}
Since $\hat{W} = W_{0} + \Delta$ we have the following
\begin{align*}
    \lambda_p (W_{0} + \Delta)K_{0}K_{0}^{T} +  (W_{0} + \Delta)K_{1}K_{1}^{T} +  \lambda_n (W_{0} + \Delta) \\
    =\lambda_p W_{0}K_{0}K_{0}^{T} + V_{1}K_{1}^{T} + \lambda_n W_{0}
\end{align*}
\begin{align*}
    \lambda_p W_{0} K_{0}K_{0}^{T} + \lambda_p \Delta K_{0}K_{0}^{T} +  W_{0}K_{1}K_{1}^{T} + \Delta K_{1}K_{1}^{T} \\
    +  \lambda_n (W_{0} + \Delta) 
    =\lambda_p W_{0}K_{0}K_{0}^{T} + V_{1}K_{1}^{T} + \lambda_n W_{0}
\end{align*}
\begin{align*}
    &\Delta(\lambda_p K_{0}K_{0}^{T} + K_{1}K_{1}^{T} + \lambda_n I) = (V_{1} - W_{0}K_{1})K_{1}^{T}
    \\
    &\Delta = (V_{1} - W_{0}K_{1})K_{1}^{T} (\lambda_p K_{0}K_{0}^{T} + K_{1}K_{1}^{T} + \lambda_n I)^{-1}
\end{align*}




% \begin{table*}[t]
% \caption{Editing performance for EMMET with Llama2-7B with norm constraint.}
% \label{tab:editing-performance-emmet}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccccccr}
% \toprule
% Method & Model  & \multirow{2}{*}{\makecell{Overall \\ Score}} & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} &  \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
% & & & \\
% \midrule
% EMMET baseline & Llama2-7B& 76.281 &  93.85 & 87.32 & 58.07  & 579.79\\
% EMMET best-editing  & Llama2-7B&  74.12 & 94.24 & 87.2 & 54.36 & 566.08\\
% EMMET best-downstream  & Llama2-7B&   72.57 & 90.35 & 85.97 & 53.65 & 541.16 \\
    
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\begin{table*}[t]
\caption{Editing performance for EMMET with Llama2-7B with norm constraint.}
\label{tab:editing-performance-emmet}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Method & Model  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} & \multirow{2}{*}{\makecell{Overall \\ Score}} &  \multirow{2}{*}{\makecell{Generation \\ Entropy}} \\
& & & & & & \\
\midrule
EMMET baseline & Llama2-7B & 93.85 & 87.32 & 58.07 & 76.281 & 579.79\\
EMMET best-editing  & Llama2-7B & 94.24 & 87.2 & 54.36 & 74.12 & 566.08\\
EMMET best-downstream  & Llama2-7B & 90.35 & 85.97 & 53.65 & 72.57 & 541.16 \\
    
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}





\section{Norm-Constraint with Other Objectives}\label{appendix:norm-constraint-emmet}


While theoretically norm-constraint can be added to both EMMET and AlphaEdit objectives, it doesn't work well with these objectives. AlphaEdit already has a norm constraint in its objective which is still not able to alleviate norm growth as shown in the main paper. Moreover, when norm-constraint is used with the EMMET objective, we see no appreciable improvements in downstream performance but incur a penalty in editing scores. 

We believe that this is the nature of how norm-constraint works. When the norm-constraint is added, it penalizes change in weights from the original weight values. One way to prevent loss of downstream performance is to set the hyperparameter for norm-constraint very high, in which case we will never lose downstream performance. But in this scenario, we will also not be able to edit the model. In the other case, if we don't restrict norm growth, the editor becomes free to increase the norm of edited matrices, which causes anomalous behaviors as discussed in the main paper. Thus, the norm constraint always comes at the trade-off of editability. 

For the MEMIT objective, the norm-constraint helps both editing and downstream performance and the algorithm is best able to make use of this constraint. However, this is not the case for other algorithms. The theoretical reasons why this happens are beyond the scope of this work. But we provide evidence for this trade-off empirically for EMMET below. 

Table \ref{tab:editing-performance-emmet} shows the editing performance of the EMMET baseline without any norm-constraint compared against two cases where norm-constraint is added to its objective. In one case, we present the best editing score that can be achieved when a non-zero norm-constrain hyperparameter is added to the EMMET objective ($\lambda_n = 20$). In this case, we clearly see that even the best editing score with norm constraint is worse than the baseline EMMET. Figure \ref{fig:emmet-nc-best-edit} shows the downstream performance for this case, which is even worse than the baseline (Figure \ref{fig:emmet-nc-baseline}). 

Secondly, in Table \ref{tab:editing-performance-emmet}  we also show the editing score for the hyperparameter that achieves the best downstream performance with non-zero norm-constrain hyperparameter in the EMMET objective ($\lambda_n = 100$). We clearly see a significant drop from baseline editing performance across all metrics. The downstream performance of this case is shown in Figure \ref{fig:emmet-nc-best-downstream}, which is not much different from the baseline (Figure \ref{fig:emmet-nc-baseline}). Thus, we show that the norm-constraint does not work well with the EMMET objective.

\begin{figure*}[h]
    \centering
    
    \subfigure[EMMET baseline]{
        \includegraphics[width=0.31\linewidth]{figures/emmet-norm-constraint/emmet-llama2-nc-baseline.png} % Replace with your image
        \label{fig:emmet-nc-baseline}
    }
    \hfill
    \subfigure[Downstream performance for best editing score with norm constraint.]{
        \includegraphics[width=0.31\linewidth]{figures/emmet-norm-constraint/emmet-llama2-nc-best-editing.png} % Replace with your image
        \label{fig:emmet-nc-best-edit}
    }
    \hfill
    \subfigure[Downstream peformance for best the scenario where we get best downstream performance with norm constraint.]{
        \includegraphics[width=0.31\linewidth]{figures/emmet-norm-constraint/emmet-llama2-nc-best-downstream.png} % Replace with your image
        \label{fig:emmet-nc-best-downstream}
    }
    
    
    \caption{Comparison between the effect of adding norm-constrain the EMMET objective. We see no appreciable improvement in downstream performace but incur loss in editing performance.}\label{fig:downstream-baseline}
\end{figure*}







\clearpage
\section{Editing Hyperparameters and Hardware Details}
Here we present the hyperparameters that we find and used for the MPES, Norm constraint, and ENCORE. Tables \ref{tab:hparams-mpes-mcf-appendix}, \ref{tab:hparams-norm-mcf-appendix} and \ref{tab:hparams-encore-mcf-appendix} show the hyperparameters for CounterFact dataset and tables \ref{tab:hparams-mpes-zsre}, \ref{tab:hparams-norm-zsre-appendix} and \ref{tab:hparams-encore-zsre-appendix} show the hyperparameters for the zsRE dataset.
Additionally, there is one more hyperparameter that requires tuning, as stopping immediately when the target token becomes the most probable may not always be optimal. We define this hyperparameter as the probability cutoff, which determines how many additional steps we take before stopping. Specifically, a cutoff of $+n$ means that we continue for $n$ more steps after the target token first becomes the most probable.

All experiments in this paper are done on NVIDIA A6000, including experiments where editing speeds of different methods are timed. 

\begin{table*}[t]
\caption{Hyperparameters for different algorithms with MPES on CouterFact dataset }
\label{tab:hparams-mpes-mcf-appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Method & Model & \multirow{2}{*}{\makecell{$\lambda_{p}$}} & \multirow{2}{*}{\makecell{Probability \\ Cut Off}}  \\
& & & \\
\midrule
EMMET + MPES & GPT2-XL & 10,000 & +1 &  \\
    & Llama2-7B & 15,000 & +0 & \\
    & Llama3-8B & 15,000 & +0 & \\
\midrule
AlphaEdit + MPEs & GPT2-XL & 20,000 & +1  \\
    & Llama2-7B & 15,000 & +0  \\
    & Llama3-8B & 15,000 & +0 \\

\midrule
MEMIT + MPEs & GPT2-XL & 20,000 & +2  \\
    & Llama2-7B & 15,000 & +1  \\
    & Llama3-8B & 15,000 & +2 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\begin{table*}[t]
\caption{Hyperparameters for Norm Constraint on CouterFact dataset }
\label{tab:hparams-norm-mcf-appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Method & Model & \makecell{$\lambda_{p}$} & \makecell{$\lambda_{n}$}\\
\midrule
Norm Constraint  &  GPT2-XL   & 20,000 &  10 \\
         & Llama2-7B  & 15,000 & 10 \\
         & Llama3-8B  & 15,000  & 20  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Hyperparameters for ENCORE on CouterFact dataset }
\label{tab:hparams-encore-mcf-appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Method & Model & \makecell{$\lambda_{p}$} & \makecell{$\lambda_{n}$} & \makecell{Probability \\ Cut Off}\\
\midrule
ENCORE  &  GPT2-XL   & 20,000 & 10 & +3  \\
         & Llama2-7B  & 15,000 & 10 & +2  \\
         & Llama3-8B  & 15,000 & 20 & +1  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Hyperparameters for different algorithms with MPES on zsRE dataset }
\label{tab:hparams-mpes-zsre}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Method & Model & \multirow{2}{*}{\makecell{$\lambda_{p}$}} & \multirow{2}{*}{\makecell{Probability \\ Cut Off}}  \\
& & & \\
\midrule
EMMET + MPES & GPT2-XL & 10,000 & +1 &  \\
    & Llama2-7B & 15,000 & +1 & \\
    & Llama3-8B & 15,000 & +2 & \\
\midrule
AlphaEdit + MPEs & GPT2-XL & 20,000 & +1  \\
    & Llama2-7B & 15,000 & +1  \\
    & Llama3-8B & 15,000 & +3 \\

\midrule
MEMIT + MPEs & GPT2-XL & 20,000 & +5  \\
    & Llama2-7B & 15,000 & +1  \\
    & Llama3-8B & 15,000 & +4 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\begin{table*}[t]
\caption{Hyperparameters for Norm Constraint on zsRE dataset }
\label{tab:hparams-norm-zsre-appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Method & Model & \makecell{$\lambda_{p}$} & \makecell{$\lambda_{n}$}\\
\midrule
Norm Constraint  &  GPT2-XL   & 20,000 &  40 \\
         & Llama2-7B  & 15,000 & 10 \\
         & Llama3-8B  & 15,000  & 20  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Hyperparameters for ENCORE on zsRE dataset }
\label{tab:hparams-encore-zsre-appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Method & Model & \makecell{$\lambda_{p}$} & \makecell{$\lambda_{n}$} & \makecell{Probability \\ Cut Off}\\
\midrule
ENCORE  &  GPT2-XL   & 20,000 & 40 & +4  \\
         & Llama2-7B  & 15,000 & 10 & +2  \\
         & Llama3-8B  & 15,000 & 10 & +3  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

% We also have that the probability cut-off controls the early stopping criterion in the optimization process based on the predicted probability in the target token. Specifically, if probability cut-off $>$ 0 then the optimization stop as soon as the mean of the predicted probability of the target token across prompts exceeds the specified threshold probability cut-off. If probability cut-off $<$ 0 then the optimization continues for |probability cut-off| additional steps after the target token has become the most probable.
\clearpage

\section{Norm growth Result}
In this section we present the observation of norm growth during editing for different methods (including Norm Constraint and ENCORE) with different models. Figures \ref{fig:norm-growth-gpt2-xl} and \ref{fig:norm-growth-gpt2-xl-encore} show the result for different methods for GPT2-XL. Figures \ref{fig:norm-growth-llama2-7b} and \ref{fig:norm-growth-llama2-7b-encore} show the result for Llama2-7B. Lastly figures \ref{fig:norm-growth-llama3-8b} and \ref{fig:norm-growth-llama3-8b-encore} show the result for Llama3-8B.

\begin{figure*}[h]
    \centering

    \subfigure[Norm growth of GPT2-XL using Alphaedit]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/gpt2-xl_alphaedit.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-alphaedit}
    }
    \subfigure[Norm growth of GPT2-XL using EMMET
    ]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/gpt2-xl_emmet.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-emmet}
    }
    \subfigure[Norm growth of GPT2-XL using MEMIT]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/gpt2-xl_memit.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-memit}
    }
    \caption{Norm growth of GPT2-XL across different methods} 
    \label{fig:norm-growth-gpt2-xl}
\end{figure*}

\begin{figure*}[h]
    \centering

    \subfigure[Norm growth of GPT2-XL using MEMIT]{
        \includegraphics[width=0.29\linewidth]{figures/norm-growth/gpt2-xl_memit.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-memit}
    }
    \subfigure[Norm growth of GPT2-XL using Norm Constraint]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/gpt2-xl_regmemit_norm.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-norm}
    }
    \subfigure[Norm growth of GPT2-XL using ENCORE]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/gpt2-xl_regmemit_encore.png} % Replace with your image
        \label{fig:norm-growth-gpt2-xl-encore}
    }
    \caption{Norm growth of GPT2-XL using Norm Constraint and ENCORE}
    \label{fig:norm-growth-gpt2-xl-encore}
\end{figure*}

\begin{figure*}[h]
    \centering

    \subfigure[Norm growth of Llama2-7B using Alphaedit]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-2-7b_alphaedit.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-alphaedit}
    }
    \subfigure[Norm growth of Llama2-7B using EMMET
    ]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-2-7b_emmet.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-emmet}
    }
    \subfigure[Norm growth of Llama2-7B using MEMIT]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-2-7b_memit.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-memit}
    }
    \caption{Norm growth of Llama2-7B across different methods }
    \label{fig:norm-growth-llama2-7b}
\end{figure*}



\begin{figure*}[h]
    \centering

    \subfigure[Norm growth of Llama2-7B using MEMIT]{
        \includegraphics[width=0.29\linewidth]{figures/norm-growth/llama-2-7b_memit.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-memit}
    }
    \subfigure[Norm growth of Llama2-7B using Norm Constraint]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-2-7b_regmemit_norm.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-norm}
    }
    \subfigure[Norm growth of Llama2-7B using ENCORE]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-2-7b_regmemit_encore.png} % Replace with your image
        \label{fig:norm-growth-llama2-7b-encore}
    }
    \caption{Norm growth of Llama2-7B using Norm Constraint and ENCORE}
    \label{fig:norm-growth-llama2-7b-encore}
\end{figure*}

\begin{figure*}[h]
    \centering

    \subfigure[Norm growth of Llama3-8B using Alphaedit]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-3-8b_alphaedit.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-alphaedit}
    }
    \subfigure[Norm growth of Llama3-8B using EMMET
    ]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-3-8b_emmet.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-emmet}
    }
    \subfigure[Norm growth of Llama3-8B using MEMIT]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-3-8b_memit.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-MEMIT}
    }
    \caption{Norm growth of Llama3-8B across different methods}
    \label{fig:norm-growth-llama3-8b}
\end{figure*}






\begin{figure*}[h]
    \centering
    \subfigure[Norm growth of Llama3-8B using MEMIT]{
        \includegraphics[width=0.29\linewidth]{figures/norm-growth/llama-3-8b_memit.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-MEMIT}
    }
    \subfigure[Norm growth of Llama3-8B using Norm Constraint]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-3-8b_regmemit_norm.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-norm}
    }
    \subfigure[Norm growth of Llama3-8B using ENCORE]{
        \includegraphics[width=0.3\linewidth]{figures/norm-growth/llama-3-8b_regmemit_encore.png} % Replace with your image
        \label{fig:norm-growth-llama3-8b-encore}
    }
    \caption{Norm growth of Llama3-8B using Norm Constraint and ENCORE}
    \label{fig:norm-growth-llama3-8b-encore}
\end{figure*}

\clearpage

\section{Downstream Performance}\label{appendix:downstream-performance}
In this section, we show the downstream performance for each method and also for different algorithms. Note that the reason why x -axis shows the number of edits to be from 0 to 100 is because it shows a number of gradient updates (or batch index) instead of a number of facts edited. Since editing is done in batches of 100, to achieve 10,000 edits we perform a total of 100 batched updates to the models sequentially. Thus, to get the total number of edits finished until a certain point, the x-axis needs to be multiplied by 100, which is the batch size.

Figures \ref{fig:downstream-gpt2-xl-alphaedit} - \ref{fig:downstream-gpt2-xl-memit-zsre} show the result for the downstream performance for GPT2-XL on both CounterFact and zsRE datasets. Figures \ref{fig:downstream-Llama2-7B-alphaedit} - \ref{fig:downstream-Llama2-7B-memit-zsre} show the result for Llama2-7B on both datasets.
Lastly, figures \ref{fig:downstream-Llama3-8B-alphaedit-mcf} - \ref{fig:downstream-Llama3-8B-memit-zsre} show the result for Llama3-8B on both datasets.

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/gpt2_xl_alphaedit.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-alphaedit}
    }
    \subfigure[Downstream Performance for GPT2-XL using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/gpt2_xl_alphaedit_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-alphaedit-mpes}
    }
    \caption{Downstream Performance for GPT2-XL using AlphaEdit and MPES with CounterFact dataset}
    \label{fig:downstream-gpt2-xl-alphaedit}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/gpt2_xl_emmet.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-emmet}
    }
    \subfigure[Downstream Performance for GPT2-XL using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/gpt2_xl_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-emmet-mpes}
    }
    \caption{Downstream Performance for GPT2-XL using EMMET and MPES with CounterFact dataset}
    \label{fig:downstream-gpt2-xl-emmet}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/gpt2_xl_memit.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/gpt2_xl_memit_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-mpes}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/gpt2_xl_memit_norm.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-mpes}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/gpt2_xl_memit_encore.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-encore}
    }
    \caption{Downstream Performance for GPT2-XL using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset}
    \label{fig:downstream-gpt2-xl-memit}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/gpt2-xl_alphaedit.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-alphaedit-zsre}
    }
    \subfigure[Downstream Performance for GPT2-XL using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/gpt2-xl_alphaedit_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-alphaedit-mpes-zsre}
    }
    \caption{Downstream Performance for GPT2-XL using AlphaEdit and MPES with zsRE dataset}
    \label{fig:downstream-gpt2-xl-alphaedit-zsre}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/gpt2-xl_emmet.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-emmet-zsre}
    }
    \subfigure[Downstream Performance for GPT2-XL using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/gpt2-xl_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-emmet-mpes-zsre}
    }
    \caption{Downstream Performance for GPT2-XL using EMMET and MPES with zsRE dataset}
    \label{fig:downstream-gpt2-xl-alphaedit-zsre}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for GPT2-XL using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/gpt2-xl-memit.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-zsre}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/gpt2-xl-memit_mpes.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/gpt2-xl-memit_norm.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for GPT2-XL using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/gpt2-xl-memit_encore.png} % Replace with your image
        \label{fig:downstream-gpt2-xl-memit-encore-zsre}
    }
    \caption{Downstream Performance for GPT2-XL using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset}
    \label{fig:downstream-gpt2-xl-memit-zsre}
\end{figure*}



\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama2_7b_alphaedit.png} % Replace with your image
        \label{fig:downstream-llama2-7b-alphaedit}
    }
    \subfigure[Downstream Performance for Llama2-7B using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama2_7b_alphedit_mpes.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-alphaedit-mpes}
    }
    \caption{Downstream Performance for Llama2-7B using AlphaEdit and MPES with CounterFact dataset}
    \label{fig:downstream-Llama2-7B-alphaedit}
\end{figure*}




\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama2-7b_emmet.png} % Replace with your image
        \label{fig:downstream-llama2-7b-emmet}
    }
    \subfigure[Downstream Performance for Llama2-7B using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama2_7b_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-emmet-mpes}
    }
    \caption{Downstream Performance for Llama2-7B using EMMET and MPES with CounterFact dataset}
    \label{fig:downstream-Llama2-7B-emmet}
\end{figure*}



\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama2_7b_memit.png} % Replace with your image
        \label{fig:downstream-llama2-7b-memit}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama2_7b_memit_mpes.png} % Replace with your image
        \label{fig:downstream-llam2-7b-memit-mpes}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama2_7b_memit_norm.png} % Replace with your image
        \label{fig:downstream-llama2-7b-memit-mpes}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama2_7b_memit_encore.png} % Replace with your image
        \label{fig:downstream-llama2-7b-memit-encore}
    }
    \caption{Downstream Performance for Llama2-7B using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset}
    \label{fig:downstream-Llama2-7B-memit}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama2_alphaedit.png} % Replace with your image
        \label{fig:downstream-llama2-7b-alphaedit-zsre}
    }
    \subfigure[Downstream Performance for Llama2-7B using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama2_alphaedit_mpes.png} % Replace with your image
        \label{fig:downstream-llama2-7b-alphaedit-mpes-zsre}
    }
    \caption{Downstream Performance for Llama2-7B using AlphaEdit and MPES with zsRE dataset}
    \label{fig:downstream-llam2-7b-alphaedit-zsre}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama2_emmet.png} % Replace with your image
        \label{fig:downstream-llama2-7b-emmet-zsre}
    }
    \subfigure[Downstream Performance for Llama2-7B using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama2_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-llama2-7b-emmet-mpes-zsre}
    }
    \caption{Downstream Performance for Llama2-7B using EMMET and MPES with zsRE dataset}
    \label{fig:downstream-llama2-7b-emmet-zsre}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama2-7B using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama2_memit.png} % Replace with your image
        \label{fig:downstream-llama2-memit-zsre}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama2_memit_mpes.png} % Replace with your image
        \label{fig:downstream-llama2-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama2_memit_norm.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for Llama2-7B using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama2_memit_encore.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-memit-encore-zsre}
    }
    \caption{Downstream Performance for Llama2-7B using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset}
    \label{fig:downstream-Llama2-7B-memit-zsre}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama3_8b_alphaedit.png} % Replace with your image
        \label{fig:downstream-llama3-8b-alphaedit}
    }
    \subfigure[Downstream Performance for Llama3-8B using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama3_8b_alphaedit_mpes.png} % Replace with your image
        \label{fig:downstream-Llama3-8B-alphaedit-mpes}
    }
    \caption{Downstream Performance for Llama3-8B using AlphaEdit and MPES with CounterFact dataset}
    \label{fig:downstream-Llama3-8B-alphaedit-mcf}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama3-8b_emmet.png} % Replace with your image
        \label{fig:downstream-llama3-8b-emmet}
    }
    \subfigure[Downstream Performance for Llama3-8B using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream_mcf/llama3-8b_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-Llama3-8B-emmet-mpes}
    }
    \caption{Downstream Performance for Llama3-8B using EMMET and MPES with CounterFact dataset}
    \label{fig:downstream-Llama3-8B-emmet}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama3-8b_memit.png} % Replace with your image
        \label{fig:downstream-llama3-8b-memit}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama3-8b_memit_mpes.png} % Replace with your image
        \label{fig:downstream-llam3-8b-memit-mpes}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama3_8b_norm.png} % Replace with your image
        \label{fig:downstream-llama3-8b-memit-mpes}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream_mcf/llama3-8b_memit_encore.png} % Replace with your image
        \label{fig:downstream-llama3-8b-memit-encore}
    }
    \caption{Downstream Performance for Llama3-8B using MEMIT, MPES, Norm Constraint and ENCORE with CounterFact dataset}
    \label{fig:downstream-llama3-8B-memit}
\end{figure*}








\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using AlphaEdit]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama3_alphaedit.png} % Replace with your image
        \label{fig:downstream-llama3-8b-alphaedit-zsre}
    }
    \subfigure[Downstream Performance for Llama3-8B using AlphaEdit with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama3_alphaedit_mpes.png} % Replace with your image
        \label{fig:downstream-llama3-8b-alphaedit-mpes-zsre}
    }
    \caption{Downstream Performance for Llama3-8B using AlphaEdit and MPES with zsRE dataset}
    \label{fig:downstream-llama3-8b-alphaedit-zsre}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using EMMET]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama3_emmet.png} % Replace with your image
        \label{fig:downstream-llama3-8b-emmet-zsre}
    }
    \subfigure[Downstream Performance for Llama3-8B using EMMET with MPES]{
        \includegraphics[width=0.3\linewidth]{figures/downstream-zsre/llama3_emmet_mpes.png} % Replace with your image
        \label{fig:downstream-llama3-8b-emmet-mpes-zsre}
    }
    \caption{Downstream Performance for Llama3-8B using EMMET and MPES with zsRE dataset}
    \label{fig:downstream-llama3-8b-emmet-zsre}
\end{figure*}

\begin{figure*}[h]
    \centering
    \subfigure[Downstream Performance for Llama3-8B using MEMIT]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama3_memit.png} % Replace with your image
        \label{fig:downstream-llama3-memit-zsre}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with MPES]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama3_memit_mpes.png} % Replace with your image
        \label{fig:downstream-llama3-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with Norm Constraint]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/llama3_memit_norm.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-memit-mpes-zsre}
    }
    \subfigure[Downstream Performance for Llama3-8B using MEMIT with ENCORE]{
        \includegraphics[width=0.2\linewidth]{figures/downstream-zsre/new_llama3_encore_zsre.png} % Replace with your image
        \label{fig:downstream-Llama2-7B-memit-encore-zsre}
    }
    \caption{Downstream Performance for Llama3-8B using MEMIT, MPES, Norm Constraint and ENCORE with zsRE dataset}
    \label{fig:downstream-Llama3-8B-memit-zsre}
\end{figure*}

\clearpage


\section{Editing Performance on zsRE dataset}
Tables \ref{tab:editing-performance-zsre-gpt2xl} - \ref{tab:editing-performance-zsre-llama3} show the editing scores for the sequential editing experiments on zsRE.





\begin{table*}[t]
\caption{Editing performance for GPT2-XL on zsre dataset}
\label{tab:editing-performance-zsre-gpt2xl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Model & Method  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} \\
& & & \\
\midrule
GPT2-XL &  EMMET    & 39.71 & 27.17 & 9.78  \\
& EMMET+MPES   & 53.55 & 39.42 & 16.81 \\
& AlphaEdit    & 42.1 & 33.61 & 14.61  \\
& AlphaEdit + MPES    & 54.99 & 43.18 & 18.4 \\
& MEMIT    & 74.6 & 61.77 & 22.4  \\
& MEMIT + MPES   & 75.09 & 61.58 & 23.37  \\
& Norm-Constraint  &  74.51 & 61.9 & 23.39 \\
& ENCORE     &   74.46 & 61.79 & 23.41  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[t]
\caption{Editing performance for Llama2-7B on zsre dataset}
\label{tab:editing-performance-zsre-llama2}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Model & Method  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} \\
& & & \\
\midrule
Llama2-7B &  EMMET & 75.42 & 69.69 & 33.89  \\
& EMMET+MPES   & 84.07 & 76.9 & 41.88 \\
& AlphaEdit    & 83.77 & 77.12 & 41.96  \\
& AlphaEdit + MPES    & 83.8 & 77.64 & 41.97 \\
& MEMIT    & 79.49 & 74.29 & 41.8  \\
& MEMIT + MPES   & 83.01 & 77.45 & 44.64  \\
& Norm-Constraint  &  88.73 & 84.05 & 47.98 \\
& ENCORE     &   89.1 & 84.28 & 48.51  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Editing performance for Llama3-8B on zsre dataset}
\label{tab:editing-performance-zsre-llama3}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccccr}
\toprule
Model & Method  & \multirow{2}{*}{\makecell{Edit \\ Score}} & \multirow{2}{*}{\makecell{Paraphrase \\ Score}} & \multirow{2}{*}{\makecell{Neighborhood \\ Score}} \\
& & & \\
\midrule
Llama3-8B &  EMMET & 96.97 & 90.96 & 45.26  \\
& EMMET+MPES   & 96.75 & 91.31 & 46.44 \\
& AlphaEdit    & 89.27 & 82.19 & 45.23  \\
& AlphaEdit + MPES    & 93.54 & 85.93 & 47.32 \\
& MEMIT    & 96.45 & 90.3 & 48.91  \\
& MEMIT + MPES   & 96.85 & 90.76 & 47.34  \\
& Norm-Constraint  & 90.4  & 84.58 & 49.09 \\
& ENCORE     & 93.15   & 86.19 & 49.81  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\clearpage

\section{Evaluation of Downstream Performance}\label{appendix:downstream}


In this paper, we assess model degradation by measuring downstream performance at regular intervals of edits. Our evaluation suite is wide-ranging and consists of the following 6 tasks – sentiment analysis (SST2) \cite{sst2}, paraphrase detection (MRPC) \cite{mrpc}, natural language inference (NLI, RTE) \cite{nli1, nli2, nli3, nli4}, linguistic acceptability classification (CoLA) \cite{cola}, and massive multitask language understanding (MMLU) \cite{MMLU}.

For each task, we created a subset of 100 examples balanced across all multiple-choice options. The models were evaluated on the tasks above, and the accuracy score was measured every 1000 edits. In order to improve the models' initial performance and achieve meaningful signals, we provided few-shot examples. The few-shot prompt templates used for each task are shown in Figures \ref{fig:sst-prompt}-\ref{fig:nli-prompt}.

%%Phudish


\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            Review : an exhilarating futuristic thriller-noir , minority report twists the best of technology around a gripping story , delivering a riveting , pulse intensifying escapist adventure of the first order \newline
            Sentiment : positive \newline \newline
            Review : try as i may , i ca n't think of a single good reason to see this movie , even though everyone in my group extemporaneously shouted , ` thank you ! ' \newline
            Sentiment : negative \newline \newline
            Review : the film 's performances are thrilling .  \newline
            Sentiment : positive \newline \newline
            Review : vera 's technical prowess ends up selling his film short ; he smoothes over hard truths even as he uncovers them . \newline
            Sentiment : negative \newline \newline
            Review : [input] \newline
            Sentiment :
        }
    }
    \caption{Few shot prompt template used for SST-2 }
    \label{fig:sst-prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            Question: Which expression is equivalent to 4 x 9? \newline
            (A) (4x 4) + (4x5) \newline
            (B) (4+4) x (4+5) \newline
            (C) (4+4)+(4+5) \newline
            (D) (4x 4) x (4x5) \newline
            Answer: A\newline\newline
            Question: A marketing researcher is conducting a survey in a large selling area by contacting a small group of people that is representative of all people in that area. The small, representative group is known as the \newline
            (A) population\newline
            (B) sample\newline
            (C) stratification\newline
            (D) universe\newline
            Answer: B\newline\newline
            Question: A research participant eats half a bowl of M\&M candies, and then stops eating. How would a motivation researcher using drive reduction theory explain this participant's behavior?\newline
            (A) Humans are instinctively driven to eat sugar and fat when presented to them.\newline
            (B) The Yerkes-Dodson law explains that people will eat food when presented to them, but usually in moderate amounts in order to avoid being perceived as selfish.\newline
            (C) The primary drive of hunger motivated the person to eat, and then stop when she/he regained homeostasis.\newline
            (D) The research participant was satisfying the second step on the hierarchy of needs: Food needs.\newline
            Answer: C\newline\newline
            Question: In a deductively valid argument\newline
            (A) If all the premises are true, the conclusion must be true\newline
            (B) The conclusion has already been stated in its premises\newline
            (C) If all the premises are true, the conclusion may still be false\newline
            (D) Both A and B\newline
            Answer: D\newline\newline
            Question: [input] \newline
            Answer:
        }
    }
    \caption{Few shot prompt template used for MMLU}
    \label{fig:mmlu-prompt}
\end{figure*}


\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            Are the sentences paraphrases of each other. \newline
            Sentence 1: Federal regulators have turned from sour to sweet on a proposed \$2.8 billion merger of ice cream giants Nestle Holdings Inc. and Dreyer 's Grand Ice Cream Inc .\newline
            Sentence 2: Federal regulators have changed their minds on a proposed \$2.8 billion merger of ice cream giants Nestle Holdings and Dreyer 's Grand Ice Cream .\newline
            Answer: Yes\newline\newline
            Are the sentences paraphrases of each other.\newline
            Sentence 1: In the year-ago quarter , the steelmaker recorded a profit of \$16.2 million , or 15 cents per share , on sales of \$1.14 billion .\newline
            Sentence 2: In the second quarter last year , AK Steel reported a profit of \$16.2 million , or 15 cents a share .\newline
            Answer: No\newline\newline
            Are the sentences paraphrases of each other.\newline
            Sentence 1: He added : ``I 've never heard of more reprehensiblebehaviour by a doctor .\newline
            Sentence 2: The Harrisons ’ lawyer Paul LiCalsi said : “ I ’ ve never heard of more reprehensible behaviour by a doctor .\newline
            Answer: Yes\newline\newline
            Are the sentences paraphrases of each other.\newline
            Sentence 1: While dioxin levels in the environment were up last year , they have dropped by 75 percent since the 1970s , said Caswell .\newline
            Sentence 2: The Institute said dioxin levels in the environment have fallen by as much as 76 percent since the 1970s .\newline
            Answer: No\newline\newline
            Are the sentences paraphrases of each other.\newline
            Sentence 1: [input 1]\newline
            Sentence 2: [input 2]\newline
            Answer:
        }
    }
    \caption{Few shot prompt template used for MRPC}
    \label{fig:mrpc-prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            Is this sentence linguistically acceptable?\newline
            Sentence: The evidence assembled by the prosecution convinced the jury.\newline
            Answer: Yes\newline\newline
            Is this sentence linguistically acceptable?\newline
            Sentence: I live at the place where Route 150 crosses the Hudson River and my dad lives at it too.\newline
            Answer: No\newline\newline
            Is this sentence linguistically acceptable?\newline
            Sentence: The government's imposition of a fine.\newline
            Answer: Yes\newline\newline
            Is this sentence linguistically acceptable?\newline
            Sentence: Sam gave the ball out of the basket.\newline
            Answer: No\newline\newline
            Is this sentence linguistically acceptable?\newline
            Sentence: [input] \newline
            Answer: 
        }
    }
    \caption{Few shot prompt template used for RTE}
    \label{fig:rte-prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            The town is also home to the Dalai Lama and to more than 10,000 Tibetans living in exile. \newline
            Question: The Dalai Lama has been living in exile since 10,000. True or False? \newline
            Answer: True \newline\newline
            P. Prayong, who like Kevala belongs to the Theravada sect of Buddhism, chose India over other Buddhist majority nations as it is the birthplace of Gautama Buddha. \newline
            Question: P. Prayong is a member of Theravada. True or False? \newline
            Answer: False \newline\newline
            The medical student accused of murdering an erotic masseuse he met on Craigslist is drowning in more than \$100,000 in student loan debt and is so broke he can't afford to pay an attorney, according to court papers. Philip Markoff, a 23-year-old suspended Boston University medical school student, owes \$130,000 in student loans and does not get money from his parents, leaving him to lean on a taxpayer-funded attorney for his defense, according to a court document in Boston Municipal Court that labels him indigent. Markoff graduated from the State University of New York-Albany and was a second-year medical student at BU.\newline
            Question: The medical student Philip Markoff was engaged. True or False?\newline
            Answer: True\newline\newline
            Traditionally, the Brahui of the Raisani tribe are in charge of the law and order situation through the Pass area. This tribe is still living in present day Balochistan in Pakistan. \newline
            Question: The Raisani tribe resides in Pakistan. True or False? \newline
            Answer: False \newline\newline
            The latest attacks targeted the U-S embassy and a top prosecutor's office in the Uzbek capital.\newline
            Question: [input]. True or False?\newline
            Answer: 
        }
    }
    \caption{Few shot prompt template used for CoLA}
    \label{fig:cola-prompt}
\end{figure*}

\begin{figure*}
    \centering
    \fbox{
        \parbox{0.8\textwidth}{
            Turkey is unlikely to become involved in, or allow U.S. forces to use Turkish territory in a Middle East war that does not threaten her territory directly. entails the U.S. to use Turkish military bases. \newline True or False? \newline Answer: False \newline \newline
            Brooklyn Borough Hall featured a Who's Who in New York's literary community during the second annual Brooklyn Book Festival. According to Brooklyn Borough President Marty Markowitz, the borough's zip code 11215 boasts more authors than anywhere else in the country. It appeared to be the case on Sunday. More than 100 authors were featured at the day-long event, including The Basketball Diaries writer Jim Carroll, former M*A*S*H star Mike Farrell, author and illustrator Mo Willems, Jack Kerouac's sometime lover and National Book Critics Circle Award recipient Joyce Johnson and PEN American Center President Francine Prose. entails the The Brooklyn Book Festival is held in Brooklyn Borough every year. \newline True or False? \newline Answer: True \newline\newline
            NASA's Saturn exploration spacecraft, Cassini , has discovered an atmosphere about the moon Enceladus . This is the first such discovery by Cassini, other than Titan , of the presence of an atmosphere around a Saturn moon. entails the Titan is the fifteenth of Saturn's known satellites.\newline True or False? \newline Answer: False \newline\newline
            Dr. Eric Goosby, a pioneer in the fight against AIDS, is President Obama's choice to run the American effort to combat the disease globally, the White House announced Monday. The President's Emergency Plan For AIDS Relief, known as Pepfar, was championed by President George W. Bush. It is expected to spend \$48 billion over the next five years and is credited with markedly reducing the disease's death rate. Its prevention policy has been controversial because of its emphasis on socially conservative methods. With a new administration and a Democratic majority in the House, organizations seeking prevention choices beyond abstinence and fidelity — including a renewed commitment to distributing condoms — are eager to try to rewrite the guidelines. entails the Pepfar is committed to fighting AIDS. \newline True or False? \newline Answer: True\newline\newline
            [input] \newline True or False? \newline Answer:
        }
    }
    \caption{Few shot prompt template used for NLI}
    \label{fig:nli-prompt}
\end{figure*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
