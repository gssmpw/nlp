% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{localization-inform-editing,
  title={Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ma2024perturbation,
  title={Perturbation-Restrained Sequential Model Editing},
  author={Ma, Jun-Yu and Wang, Hong and Xu, Hao-Xiang and Ling, Zhen-Hua and Gu, Jia-Chen},
  journal={arXiv preprint arXiv:2405.16821},
  year={2024}
}

@article{wilke,
  title={Wilke: Wise-layer knowledge editor for lifelong knowledge editing},
  author={Hu, Chenhui and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2402.10987},
  year={2024}
}


@article{mamba-editing,
  title={Locating and editing factual associations in mamba},
  author={Sharma, Arnab Sen and Atkinson, David and Bau, David},
  journal={arXiv preprint arXiv:2404.03646},
  year={2024}
}

@article{comeba,
  title={Consecutive Model Editing with Batch alongside HooK Layers},
  author={Li, Shuaiyi and Deng, Yang and Cai, Deng and Lu, Hongyuan and Chen, Liang and Lam, Wai},
  journal={arXiv preprint arXiv:2403.05330},
  year={2024}
}

@article{grace,
  title={Aging with grace: Lifelong model editing with discrete key-value adaptors},
  author={Hartvigsen, Tom and Sankaranarayanan, Swami and Palangi, Hamid and Kim, Yoon and Ghassemi, Marzyeh},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{composable-interventions,
  title={Composable interventions for language models},
  author={Kolbeinsson, Arinbjorn and O'Brien, Kyle and Huang, Tianjin and Gao, Shanghua and Liu, Shiwei and Schwarz, Jonathan Richard and Vaidya, Anurag and Mahmood, Faisal and Zitnik, Marinka and Chen, Tianlong and others},
  journal={arXiv preprint arXiv:2407.06483},
  year={2024}
}

@article{finetuning-modelediting,
  title={Model Editing by Pure Fine-Tuning},
  author={Gangadhar, Govind and Stratos, Karl},
  journal={arXiv preprint arXiv:2402.11078},
  year={2024}
}

@inproceedings{nli1,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  pages={177--190},
  year={2005},
  organization={Springer}
}

@inproceedings{nli2,
  title={The second pascal recognising textual entailment challenge},
  author={Haim, R Bar and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
  booktitle={Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment},
  volume={7},
  pages={785--794},
  year={2006}
}

@inproceedings{nli3,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}

@article{nli4,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  journal={TAC},
  volume={7},
  pages={8},
  year={2009},
  publisher={Citeseer}
}

@article{cola,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{quantifying_memorization,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{gpt-2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models, 2023},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={URL https://arxiv. org/abs/2307.09288},
  year={2023}
}


@misc{llama3,
  title = {Introducing Meta Llama 3: The most capable openly available LLM to date},
  author= {Meta},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}},
  year={2024}
}

@article{hypernetwork,
  title={A Brief Review of Hypernetworks in Deep Learning},
  author={Chauhan, Vinod Kumar and Zhou, Jiandong and Lu, Ping and Molaei, Soheila and Clifton, David A},
  journal={arXiv preprint arXiv:2306.06955},
  year={2023}
}

@article{glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{rag,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{icl,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@inproceedings{sst2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}

@article{catastrophic,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@article{catastophic2,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}

@article{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{fewshot1,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}

@article{linearmemory1,
  title={Correlation matrix memories},
  author={Kohonen, Teuvo},
  journal={IEEE transactions on computers},
  volume={100},
  number={4},
  pages={353--359},
  year={1972},
  publisher={IEEE}
}

@article{linearmemory2,
  title={A simple neural network generating an interactive memory},
  author={Anderson, James A},
  journal={Mathematical biosciences},
  volume={14},
  number={3-4},
  pages={197--220},
  year={1972},
  publisher={Elsevier}
}

@inproceedings{integrated-gradients,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={International conference on machine learning},
  pages={3319--3328},
  year={2017},
  organization={PMLR}
}


%%%%KNOWLEDGE EDITING

@article{key-value-memories,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  journal={arXiv preprint arXiv:2012.14913},
  year={2020}
}

@article{easyedit,
  title={Easyedit: An easy-to-use knowledge editing framework for large language models},
  author={Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and others},
  journal={arXiv preprint arXiv:2308.07269},
  year={2023}
}

@article{metamodel,
  title={Editing factual knowledge in language models},
  author={De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  journal={arXiv preprint arXiv:2104.08164},
  year={2021}
}

@article{knowledgeneurons,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}

@article{MEND,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@inproceedings{SERAC,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{ROME,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{MEMIT,
  title={Mass-editing memory in a transformer},
  author={Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2210.07229},
  year={2022}
}

@article{PMET,
  title={Pmet: Precise model editing in a transformer},
  author={Li, Xiaopeng and Li, Shasha and Song, Shezheng and Yang, Jing and Ma, Jun and Yu, Jie},
  journal={arXiv preprint arXiv:2308.08742},
  year={2023}
}

@article{ripple-effects,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={arXiv preprint arXiv:2307.12976},
  year={2023}
}

@article{editing-survey,
  title={Editing Large Language Models: Problems, Methods, and Opportunities},
  author={Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2305.13172},
  year={2023}
}

@article{pitfalls,
  title={Unveiling the pitfalls of knowledge editing for large language models},
  author={Li, Zhoubo and Zhang, Ningyu and Yao, Yunzhi and Wang, Mengru and Chen, Xi and Chen, Huajun},
  journal={arXiv preprint arXiv:2310.02129},
  year={2023}
}

@article{mquake,
  title={MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
  author={Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D and Potts, Christopher and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14795},
  year={2023}
}


@article{hurt,
  title={Model Editing Can Hurt General Abilities of Large Language Models},
  author={Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:2401.04700},
  year={2024}
}

@article{causal,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}


@article{zsre,
  title={Zero-shot relation extraction via reading comprehension},
  author={Levy, Omer and Seo, Minjoon and Choi, Eunsol and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1706.04115},
  year={2017}
}



@article{disabling-wilke,
  title={WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing},
  author={Hu, Chenhui and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2402.10987},
  year={2024}
}

@article{disabling-butterfly,
  title={The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse},
  author={Yang, Wanli and Sun, Fei and Ma, Xinyu and Liu, Xun and Yin, Dawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2402.09656},
  year={2024}
}


@article{MALMEN,
  title={Massive editing for large language models via meta learning},
  author={Tan, Chenmien and Zhang, Ge and Fu, Jie},
  journal={arXiv preprint arXiv:2311.04661},
  year={2023}
}

@article{crosslingual-analysis,
  title={Cross-lingual knowledge editing in large language models},
  author={Wang, Jiaan and Liang, Yunlong and Sun, Zengkui and Cao, Yuxuan and Xu, Jiarong},
  journal={arXiv preprint arXiv:2309.08952},
  year={2023}
}

@article{crosslingual-retrieval,
  title={Retrieval-augmented Multilingual Knowledge Editing},
  author={Wang, Weixuan and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:2312.13040},
  year={2023}
}

@article{lora-learns-less,
  title={Lora learns less and forgets less},
  author={Biderman, Dan and Portes, Jacob and Ortiz, Jose Javier Gonzalez and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}

@article{MMLU,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{survey-comprehensive,
  title={A comprehensive study of knowledge editing for large language models},
  author={Zhang, Ningyu and Yao, Yunzhi and Tian, Bozhong and Wang, Peng and Deng, Shumin and Wang, Mengru and Xi, Zekun and Mao, Shengyu and Zhang, Jintian and Ni, Yuansheng and others},
  journal={arXiv preprint arXiv:2401.01286},
  year={2024}
}


@article{akshat-catastrophic,
  title={Model Editing at Scale leads to Gradual and Catastrophic Forgetting},
  author={Gupta, Akshat and Rao, Anurag and Anumanchipalli, Gopala},
  journal={arXiv preprint arXiv:2401.07453},
  year={2024}
}

@article{hernandez2023inspecting,
  title={Inspecting and editing knowledge representations in language models},
  author={Hernandez, Evan and Li, Belinda Z and Andreas, Jacob},
  journal={arXiv preprint arXiv:2304.00740},
  year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{rlhf-ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{dpo-rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{unlearningsurvey-hase,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@inproceedings{wei2024magicoder,
  title={Magicoder: Empowering code generation with oss-instruct},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{norm-growth,
  title={Effects of parameter norm growth during transformer training: Inductive bias from gradient descent},
  author={Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah},
  journal={arXiv preprint arXiv:2010.09697},
  year={2020}
}

@article{akshat-rebuilding,
  title={Rebuilding rome: Resolving model collapse during sequential model editing},
  author={Gupta, Akshat and Baskaran, Sidharth and Anumanchipalli, Gopala},
  journal={arXiv preprint arXiv:2403.07175},
  year={2024}
}

@article{akshat-unified,
  title={A unified framework for model editing},
  author={Gupta, Akshat and Sajnani, Dev and Anumanchipalli, Gopala},
  journal={arXiv preprint arXiv:2403.14236},
  year={2024}
}

@article{akshat-llama3,
  title={Is Bigger Edit Batch Size Always Better?--An Empirical Study on Model Editing with Llama-3},
  author={Yoon, Junsang and Gupta, Akshat and Anumanchipalli, Gopala},
  journal={arXiv preprint arXiv:2405.00664},
  year={2024}
}

@inproceedings{hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{mutual,
    title = "MuTual: A Dataset for Multi-Turn Dialogue Reasoning",
    author = "Cui, Leyang  and Wu, Yu and Liu, Shujie and Zhang, Yue and Zhou, Ming" ,
    booktitle = "Proceedings of the 58th Conference of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@article{alphaedit,
  title={Alphaedit: Null-space constrained knowledge editing for language models},
  author={Fang, Junfeng and Jiang, Houcheng and Wang, Kun and Ma, Yunshan and Wang, Xiang and He, Xiangnan and Chua, Tat-seng},
  journal={arXiv preprint arXiv:2410.02355},
  year={2024}
}

@article{gangadhar2024model,
  title={Model Editing by Pure Fine-Tuning},
  author={Gangadhar, Govind and Stratos, Karl},
  journal={arXiv preprint arXiv:2402.11078},
  year={2024}
}

@article{overfitting-modelediting,
  title={Uncovering overfitting in large language model editing},
  author={Zhang, Mengqi and Ye, Xiaotian and Liu, Qiang and Ren, Pengjie and Wu, Shu and Chen, Zhumin},
  journal={arXiv preprint arXiv:2410.07819},
  year={2024}
}