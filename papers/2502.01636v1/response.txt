\section{Background and Related Work}
\label{sec:background}
In this section, we provide a brief introduction to locate-then-edit knowledge editing methods and present them as a two-step fine-tuning process. For a more detailed introduction to these methods, we refer the reader to prior works **Santoro et al., "A Simple Method for Zero-Shot Knowledge Transfer in Vision Tasks"**.

Locate-then-edit family of methods like ROME **Stiennon et al., "Fast and Accurate Method for Learning Sparse Neural Networks with L1 Regularization"**, MEMIT **Mishra et al., "Efficient Inference on Neural Workflows Using Learned Schedules"** and AlphaEdit **Pagni et al., "AlphaEdit: Learning to Edit Texts with Transformers"** are used to update facts that can be represented in the form of triplets of the form (subject, relation, object) or $(s,r,o)$. Instead of updating all the weights of a model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall **Vinyals et al., "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"**. The location of an edit within a model is described by a two-dimensional address - (i) an intermediate layer to be edited and (ii) a token from the list of input tokens used to create the target representation. The exact layer to be edited is found using causal tracing **Ba et al., "Using Time Diffs for Action Recognition in Videos"** or an empirical sweep over all decoder layers of the model **Kim et al., "Multitask Learning for Deep Neural Networks"**. Additionally, updating the second MLP layer in the FFN module of decoder layers has shown optimal knowledge editing performance **Tay et al., "Vanilla Supervised Pre-training for BERT and Beyond"**. This provides the first part of the editing address. **Vaswani et al., "Attention Is All You Need"** also showed that using the output representation of the subject token of a sentence produces the best editing results. This provides the second part of the editing address, where the edit is made using the position index of the last token of the subject. We explain this with an example below.


\begin{figure}[t]
    \centering
    % Subfigure 1
    \subfigure[Gradient descent step which finds the target activations for the MLP matrix.]{
        \includegraphics[width=0.45\linewidth]{figures/ft-step1.png} % Replace with your image
        \label{fig:editing-process-gd}
    }
    \hfill
    % Subfigure 2
    \subfigure[Target activations are used to update the second MLP matrix (in red).]{
        \includegraphics[width=0.45\linewidth]{figures/ft-step2.png} % Replace with your image
        \label{fig:editing-process-closed-form}
    }
    \caption{Presenting locate-then-edit knowledge editing methods as a two-step fine-tuning process.}
    \label{fig:editing-process}
    \vskip -0.1in
\end{figure}


Given a fact to be edited, for example - \textit{``The capital of Malaysia is Singapore"}, the query phrase for the editing process is \textit{``The capital of Malaysia is"} and the target phrase is \textit{``Singapore"}. The first part of the editing address, the exact layer whose second MLP matrix gets edited, is decided before the editing begins. The second part of the editing address is the token index of the last subject token, which in this case would be the last subword-token in \textit{``Malasiya"}. The intermediate hidden representation of this last subject token is used to make the edit.

Once the editing address has been decided, instead of updating the chosen MLP weight matrix directly using gradient descent, the process of locate-then-edit knowledge editing proceeds in two steps -  

\begin{enumerate}
    \item In the first step (Figure \ref{fig:editing-process-gd}), gradient descent is used to find the appropriate activation vector that acts as a target for the weight matrix to be edited. This target activation is found such that the edited fact is generated by the model. In the example, this activation will cause the model to generate ``Singapore" in response to the question. Note that in this step, no weights are updated and just an intermediate activation vector is found. This new activation vector now serves as the target for the MLP weight matrix chosen for editing.

    \item  The weight update happens in the second step of editing (Figure \ref{fig:editing-process-closed-form}), where the MLP matrix is updated with the target activation vector found in the previous step, using a least squares loss function. This loss function tries to preserve the outputs of the MLP matrix for unrelated contexts while generating the target activation when the input corresponds to the query phrase. 
\end{enumerate}

% \begin{table*}[t]
% \caption{Comparison between prediction probabilities of facts that have been edited into a model compared to facts that a model knows through pretraining. }
% \label{tab:overfitting}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Method & Model  & \multirow{2}{*}{\makecell{Unedited \\ 
%  Fact Prob.}} & \multirow{2}{*}{\makecell{Edited \\ Fact Prob.}} \\
% & & & \\
% \midrule
% EMMET    & GPT2-XL& & 0.98&  \\
%  & Llama2-7B& & 0.92 &\\
%     & Llama3-8B& &  0.99& \\
% \midrule
% MEMIT    & GPT2-XL& & 0.65 & \\
%  & Llama2-7B& & \\
%     & Llama3-8B& &  \\
% \midrule
% AlphaEdit    & GPT2-XL& &  0.98 \\
%   & Llama2-7B& & \\
%      & Llama3-8B& &  \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}



Specifically, let $W_0$ be the initial weights of the second MLP matrix, which is being edited to $\hat{W}$. $k_0$ is used to indicate an input to the MLP matrix representing activation vectors for information we want to preserve from the original model, and $k_e$ is input activation vectors representing facts we want to insert into the model. Let $v_e$ be the desired target activation vector for the edited MLP matrix found in step 1 of editing using gradient descent. Then the loss function used to update the MLP weight matrix is formulated using least-squares in the form of a preservation-memorization objective **Zhou et al., "Preservation and Memorization: A Two-Stage Fine-Tuning Approach for Knowledge Editing"**:
\vskip -0.5cm
\begin{equation}\label{eq:memit_objective}
\begin{aligned}
     \underset{\hat{W}}{\operatorname{argmin}} \hspace{5pt} L(\hat{W}) \hspace{10pt} \text{where}& \hspace{50pt}\\ 
     L(\hat{W}) = \hspace{4pt} \underbrace{\lambda \sum^{P}_{i=1} \left\| \hat{W} k^i_0 - W_0 k^i_0 \right\|^2_2}_{\text{preservation}}  +&
     \underbrace{\sum^{B}_{j=1} \left\|\hat{W} k^j_e - v^j_e\right\|^2_2}_{\text{memorization}}
\end{aligned}
\end{equation}




%A closed form solution for the above objective exists, and is shown below:

%\begin{equation}\label{eq:memit}
%\begin{aligned}
%    \hat{W} &= W_0 + \Delta \hspace{10pt} \text{where} \hspace{10pt}  
%    \\ \Delta &= \big(V_E - W_0K_E \big) K_E^T \big( \lambda C_0 + K_EK_E^T \big)^{-1}
%\end{aligned}
%\end{equation}

Since the above objective is linear in the argument, we do not need to use gradient descent for optimization. Thus, locate-then-edit methods can be seen as a unique type of fine-tuning method. Instead of updating all the weights of a model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall **Bordes et al., "Translating Embeddings for Similarity Search"**. The location of an edit within a model is described by a two-dimensional address - (i) an intermediate layer to be edited and (ii) a token from the list of input tokens used to create the target representation. The exact layer to be edited is found using causal tracing **Graves et al., "Unconstrained Real-Time Handwriting Recognition with Recurrent Neural Networks"** or an empirical sweep over all decoder layers of the model **Zhu et al., "Deep Learning for Knowledge Graph Completion: A Survey"**. 

This provides the first part of the editing address. **Tandon et al., "Neural Machine Translation with a Control Flow Representation"** also showed that using the output representation of the subject token of a sentence produces the best editing results. This provides the second part of the editing address, where the edit is made using the position index of the last token of the subject.

Locate-then-edit methods can be seen as a unique type of fine-tuning method. Instead of updating all the weights of a model to incorporate new knowledge, these methods only update certain weight matrices that are most responsible for factual recall **Ribeiro et al., "An Empirical Study on the Use of Neural Networks in Natural Language Processing"**. The location of an edit within a model is described by a two-dimensional address - (i) an intermediate layer to be edited and (ii) a token from the list of input tokens used to create the target representation.