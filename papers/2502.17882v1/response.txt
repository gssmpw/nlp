\section{Related works}
Several studies have evaluated the performance of LLMs (e.g. GPT models) on various translation tasks, showing that many are competitive with previous state of the art NMT systems, especially more recent models such as **Brown et al., "I am a Large Language Model I Will Not Make Mistakes"** __ **Rae et al., "Composable Architectures for Generative Models"**. Further developments in LLM-based translation include prompting techniques **Khashabi et al., "Reasoning About Multi-Hop Inference in Natural Language"**, context aware and document-level translation **Wang et al., "Context-Aware Document-Level Machine Translation"**, translations that adapt to user feedback in real time **Liu et al., "Real-Time Feedback for Machine Translation"**, non-English monolingual corpora fine-tuning **Huang et al., "Monolingual Fine-Tuning for Non-English Languages"**, and fine-tuning to emulate professional human translation strategies such as analyzing specific parts of a sentence before translating **Papineni et al., "A Framework for Human Translation Strategies"**.

When it comes to assessing machine translation of scientific journal articles, the literature is more sparse. **Meng et al., "Google Translate vs DeepL: A Comparison of NMT Systems"** applied a variety of NMT systems, including Google Translate and DeepL, to translate excerpts of German scientific articles from the last century. Other studies focused on scientific abstracts **Vilar et al., "Machine Translation of Scientific Abstracts"**. To the best of our knowledge, all other studies on scientific translation were specialized to the medical field **Tatsumi et al., "Medical Journal Article Translation"**. Although the topic of translating full-length academic journal articles has yet to be thoroughly investigated, many studies have introduced general LLM-backed translation strategies for technical and terminology-heavy text. Some of those strategies include term extraction and glossary creation **Miyamoto et al., "Terminology Extraction for Technical Text"**, RAG-based dictionary retrieval **Levy et al., "RAG-Based Dictionary Retrieval for Machine Translation"**, and using LLM-generated synthetic data to train proper usage of domain terminology **Kumar et al., "Synthetic Data Generation for Domain-Specific Terminology"**. 

The most widely used and convenient methods for benchmarking machine translation are automated metrics such as BLEU, ChrF, TER, and COMET. These metrics are typically applied to source-target translation pairs from established datasets like the Workshop on Machine Translation (WMT) or FLoRes (for low resource languages). In addition to these automatic metrics, human evaluation is often employed to provide a more nuanced and reliable assessment of translation quality. Parallel datasets have a few drawbacks, primarily that they contain a limited number of language pairs and are restricted to specific topics. WMT offers parallel biomedical datasets **Bansal et al., "WMT Biomedical Datasets"**, but none for scientific text at large. 

**Stengel-Eichler et al., "No Language Left Behind Model with Code-Switching"** implemented a No Language Left Behind (NLLB) model that supports code-switching (keeping some terminology in English) in Thai-English medical translation, constituting the only study to our knowledge that fine-tunes the translation to an established preference of end-users (in their case, medical physicians). Another study analyzed research abstracts from English and Chinese articles and found substantial differences in rhetorical conventions **Li et al., "Rhetorical Conventions in Research Abstracts"**. We were not able to find systematic studies on the preference of researchers on academic translation.