\section{Related works}
Several studies have evaluated the performance of LLMs (e.g. GPT models) on various translation tasks, showing that many are competitive with previous state of the art NMT systems, especially more recent models such as GPT-4~\citep{hendy2023good}~\citep{jiao2023chatgpt}. Further developments in LLM-based translation include prompting techniques~\citep{vilar2022prompting}~\citep{zhang2023prompting}, context aware and document-level translation~\citep{wang2023document}, translations that adapt to user feedback in real time~\citep{moslem2023adaptive}, non-English monolingual corpora fine-tuning~\citep{xu2023paradigm}, and fine-tuning to emulate professional human translation strategies such as analyzing specific parts of a sentence before translating~\citep{he2024exploring}.

When it comes to assessing machine translation of scientific journal articles, the literature is more sparse. \citet{zulfiqar2018machine} applied a variety of NMT systems, including Google Translate and DeepL, to translate excerpts of German scientific articles from the last century. Other studies focused on scientific abstracts~\citep{tongpoon2020google}~\citep{wei2017machine}. To the best of our knowledge, all other studies on scientific translation were specialized to the medical field~\citep{soto2019neural}~\citep{daniele2019performance}~\citep{sebo2024performance}. Although the topic of translating full-length academic journal articles has yet to be thoroughly investigated, many studies have introduced general LLM-backed translation strategies for technical and terminology-heavy text. Some of those strategies include term extraction and glossary creation~\citep{kim2024efficient}, RAG-based dictionary retrieval~\citep{zheng2024fine}, and using LLM-generated synthetic data to train proper usage of domain terminology~\citep{moslem-etal-2023-domain}. 

The most widely used and convenient methods for benchmarking machine translation are automated metrics such as BLEU, ChrF, TER, and COMET. These metrics are typically applied to source-target translation pairs from established datasets like the Workshop on Machine Translation (WMT) or FLoRes (for low resource languages). In addition to these automatic metrics, human evaluation is often employed to provide a more nuanced and reliable assessment of translation quality. Parallel datasets have a few drawbacks, primarily that they contain a limited number of language pairs and are restricted to specific topics. WMT offers parallel biomedical datasets~\citep{neves-EtAl:2022:WMT}, but none for scientific text at large. 

\citet{pengpun2024creating} implemented a No Language Left Behind (NLLB) model that supports code-switching (keeping some terminology in English) in Thai-English medical translation, constituting the only study to our knowledge that fine-tunes the translation to an established preference of end-users (in their case, medical physicians). Another study analyzed research abstracts from English and Chinese articles and found substantial differences in rhetorical conventions~\citep{li2020mediating}. We were not able to find systematic studies on the preference of researchers on academic translation.