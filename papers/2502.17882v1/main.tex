
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{kotex}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Hannah Calzi Kleidermacher \email kleid@stanford.edu \\
      \addr Department of Electrical Engineering\\
      Stanford University
      \AND
      \name James Zou \email jamesz@stanford.edu \\
      \addr Department of Biomedical Data Science\\
      Stanford University}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9\%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms "overtranslated," expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at \hyperlink{https://hankleid.github.io/ProjectMundo/}{hankleid.github.io/ProjectMundo}.
\end{abstract}

\section{Introduction}
Around 98\% of all peer-reviewed scientific articles are published in English, but only around 7\% of the world's population speaks English as a first language~\citep{liu2017changing}. While having a common language among academic journals facilitates international scientific discourse, it also creates a significant barrier to access scientific knowledge for non-native English speakers. For instance, a large-scale survey found that 96\% of respondents agree or strongly agree that English as the dominant academic language disproportionately advantages native speakers, among other similar studies~\citep{ferguson2011english}~\citep{tardy2004role}~\citep{flowerdew1999writing}. This linguistic dominance introduces challenges across multiple aspects of science, from biases in peer review against non-native English writers to global implications for science-informed policy~\citep{steigerwald2022overcoming}. At the heart of this issue is language accessibility in existing scientific literature. Academic journals, especially widely-read and open-access journals, cater to a global audience~\citep{natureindex}. The availability of scientific literature in a person's native language could play a crucial role in shaping their decision to pursue a career in science.

 Many studies have explored potential solutions and paradigm shifts to overcome the language barrier in academic publishing. Given the systemic role that academic journals play in disseminating scientific knowledge, the most direct and impactful solution is for journals themselves to support translations of their articles to other languages. Several challenges hinder the adoption of multilingual translations for journal articles, including cost, logistical complexity, and the question of how best to translate scientific text. The search for a feasible, accurate, and easily adoptable method for translating scientific articles remains elusive.

Machine translation offers a cost-effective and scalable solution for translating text. With the rapid development of neural-based approaches and deep learning, machine translation improved enormously, and neural machine translation (NMT) systems like Google Translate and DeepL have been the gold standard for both general and professional translation tasks ~\citep{kalchbrenner2013recurrent}~\citep{stahlberg2019neural}. Recently, with the rise of transformer-based large language models (LLMs), the landscape is shifting. Recent studies show that LLMs match and often surpass NMT systems in performance across a wide variety of translation tasks, including scientific text~\citep{hendy2023good}~\citep{jiao2023chatgpt}~\citep{mohsen2024artificial}. What truly sets LLMs apart, however, is their ability to process complex instructions and generate context-aware, customized outputs. By leveraging simple in-context learning techniques alone, LLMs can be trained to produce translations that are specifically tailored to the requirements of the scientific community, accommodating factors like formatting preferences and domain-specific vocabulary. This flexibility opens the door to incorporating a wide range of potential feedback from non-native English-speaking researchers, enabling more specialized and effective translations.

In this article, we develop LLM-backed automated translation solutions to support lowering the language barrier in the scientific community. We introduce a method for generating publisher-ready full-length article translations, propose a novel QA benchmarking strategy to evaluate translation quality, and demonstrate how LLM few-shot prompting can be used to integrate feedback from actual authors of research papers into the translation process. We assess the strengths and weaknesses of LLM translation through both automated evaluations and user studies. 

\subsection{Related works}
Several studies have evaluated the performance of LLMs (e.g. GPT models) on various translation tasks, showing that many are competitive with previous state of the art NMT systems, especially more recent models such as GPT-4~\citep{hendy2023good}~\citep{jiao2023chatgpt}. Further developments in LLM-based translation include prompting techniques~\citep{vilar2022prompting}~\citep{zhang2023prompting}, context aware and document-level translation~\citep{wang2023document}, translations that adapt to user feedback in real time~\citep{moslem2023adaptive}, non-English monolingual corpora fine-tuning~\citep{xu2023paradigm}, and fine-tuning to emulate professional human translation strategies such as analyzing specific parts of a sentence before translating~\citep{he2024exploring}.

When it comes to assessing machine translation of scientific journal articles, the literature is more sparse. \citet{zulfiqar2018machine} applied a variety of NMT systems, including Google Translate and DeepL, to translate excerpts of German scientific articles from the last century. Other studies focused on scientific abstracts~\citep{tongpoon2020google}~\citep{wei2017machine}. To the best of our knowledge, all other studies on scientific translation were specialized to the medical field~\citep{soto2019neural}~\citep{daniele2019performance}~\citep{sebo2024performance}. Although the topic of translating full-length academic journal articles has yet to be thoroughly investigated, many studies have introduced general LLM-backed translation strategies for technical and terminology-heavy text. Some of those strategies include term extraction and glossary creation~\citep{kim2024efficient}, RAG-based dictionary retrieval~\citep{zheng2024fine}, and using LLM-generated synthetic data to train proper usage of domain terminology~\citep{moslem-etal-2023-domain}. 

The most widely used and convenient methods for benchmarking machine translation are automated metrics such as BLEU, ChrF, TER, and COMET. These metrics are typically applied to source-target translation pairs from established datasets like the Workshop on Machine Translation (WMT) or FLoRes (for low resource languages). In addition to these automatic metrics, human evaluation is often employed to provide a more nuanced and reliable assessment of translation quality. Parallel datasets have a few drawbacks, primarily that they contain a limited number of language pairs and are restricted to specific topics. WMT offers parallel biomedical datasets~\citep{neves-EtAl:2022:WMT}, but none for scientific text at large. 

\citet{pengpun2024creating} implemented a No Language Left Behind (NLLB) model that supports code-switching (keeping some terminology in English) in Thai-English medical translation, constituting the only study to our knowledge that fine-tunes the translation to an established preference of end-users (in their case, medical physicians). Another study analyzed research abstracts from English and Chinese articles and found substantial differences in rhetorical conventions~\citep{li2020mediating}. We were not able to find systematic studies on the preference of researchers on academic translation.

\subsection{Our contributions} 

\paragraph{Journal-compatible translation.} To the best of our knowledge, we develop the first pipeline using LLMs to translate scientific articles while preserving standard publishing formats (JATS XML). 

\paragraph{Automated QA benchmarking.} We have developed an automated benchmarking method specifically tailored for scientific documents (Section \ref{qna}). Our approach requires only the original and translated documents to assess translation quality, making it language agnostic, article specific, and independent of parallel translation datasets.

\paragraph{Translation preferences for scientific text.} We have gathered feedback on machine translations in a variety of languages directly from authors of research papers across multiple scientific disciplines. Subsequently, similar to \citet{pengpun2024creating}, we have also create code-switched translations; instead of masking, we implemented few-shot prompting using a scientist-curated example translation. 


\section{Journal-compatible translation}
\label{jats}

Journals have the power to change language barrier norms, as they serve as the primary forum for scientific knowledge. However, for multilingual translation to be widely adopted in scientific publishing, the process must be practical for journals to implement. In this section, we demonstrate how LLMs can preserve the formatting of journal articles during translation, offering an approach that is adaptable and easy to integrate.

In 2002, the NIH introduced the Journal Article Tag Suite (JATS), an XML protocol for structuring scientific journal articles. Since then, JATS has become part of the National Information Standards Organization (NISO) and is the global standard for academic publishing. Despite the distinct "look and feel" of articles across different publishers, they all share the same underlying JATS XML structure. For instance, academic journal articles universally include <front>, <body>, and <back> sections that contain the main text; <article-title> and <abstract> sections; a <contrib-group> section that stores author information, and many more~\citep{needleman2012niso}.

We employ an LLM (GPT-4o) to translate journal articles in their native JATS form, ensuring that the XML structure remains intact while translating the content. Figure \ref{xml_trans} illustrates the core principal of the approach. When tasked with translating this section ("Sec5"), GPT-4o successfully translates the text while preserving the surrounding <sec>, <title>, <p>, and <xref> tags. A full article is much more complex, however, consisting of multiple, heavily nested elements that include figures, tables, equations, and more. We translate each full article in a series of API calls to GPT-4o, processing memory-manageable chunks (typically around 5 paragraphs) at a time. To increase context awareness, we prepend the prompt with the contents of the full original document.

Even for complex elements, we find that GPT-4o reliably maintains XML formatting without introducing errors. However, occasional issues arise with nesting, such as paragraph text incorrectly appearing inside a figure caption. To ensure structural accuracy, we translate tables and figures—typically the most complex elements in a JATS article—independently before appending them to their respective sections. Additionally, we observe that special characters like '<' and '>' can sometimes cause truncation of the article text, even though the XML structure remains intact. To address these issues, we modify the prompt to include \textit{"Do not cut sentences short and include all symbols,"} after which the model produces fully translated articles with no other structural issues. In the 348 translations we generated over the course of this study, we identified no truncation errors and only one nesting error, resulting in a 99.7\% accuracy in preserving the original JATS structures.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1\textwidth]{figs/xml_trans.pdf}
\end{center}
\caption{Example of a JATS-formatted article snippet~\citep{lepczyk2023global}, translated with our method into three of the 28 languages included in our study (Spanish, Hindi, and Chinese). The XML tags (black) are preserved while the article text (blue) is translated. Below each translated JATS XML snippet is the resulting section of the HTML-displayed article.}
\label{xml_trans}
\end{figure}

Using our method, we successfully translate full articles into 28 different languages while fully preserving the JATS formatting. Because of its compatibility with native article formatting, this translation step can be applied at the final stage of publication or to articles that have already been published. This ensures maximum compatibility with the publication framework and paves the way for widespread translation of articles across different journals. Moreover, the translation is applicable to an arbitrary number of languages. While JATS is the ubiquitous standard, this approach is adaptable to other XML protocols as long as the tag suite is properly documented, ensuring broad compatibility across scientific publishing.  A database of all translated articles, totaling to 348 translations, is available on our webpage: \hyperlink{https://hankleid.github.io/ProjectMundo/}{hankleid.github.io/ProjectMundo}.

\section{QA-style automated benchmarking}
\label{qna}

In this section, we evaluate the translation quality of our approach. Traditional machine translation evaluation relies on automated benchmarking metrics such as BLEU, which compare translations against parallel reference data. However, to our knowledge, no dataset exists that provides parallel, document-level scientific translations across the diverse range of languages and disciplines we have included here. Instead, we introduce a novel question-and-answer (QA) style benchmarking method. In this approach, an LLM generates a "quiz" with multiple-choice questions designed to capture key details from the original scientific article. The LLM then "reads" the translated article and attempts to answer these questions based solely on the translated content. The higher the accuracy, the better the translation conveys the scientific details of the original text.

A key advantage of this benchmarking method is its automation and adaptability. Unlike traditional evaluation techniques, it does not require parallel translation data and is therefore applicable to any article, in any format, and in any language. This flexibility is particularly valuable for evaluating translations in "low-resource" languages, where high-quality parallel datasets are scarce.

\subsection{Benchmarking procedure}
\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1\textwidth]{figs/QA_flow.pdf}
\end{center}
\caption{QA benchmark process. \textbf{a.} Flowchart schematic of the benchmark process. \textbf{b.} An example of two quiz questions generated from a scientific article. In this case, the model reading the translated article answered both questions correctly (inset), indicating that the scientific details covered by those questions were accurately translated.}
\label{QA_flow}
\end{figure}

The benchmarking procedure is illustrated in Figure \ref{QA_flow}. First, we prepare the quiz by providing GPT-4o with the original English text and prompting it to create 50 multiple-choice questions that encapsulate key details of the paper, along with a corresponding answer key. The exact prompt is as follows:

\textit{"Please read the following scientific journal article. Generate 50 detailed and specific questions to test a reader's understanding of the findings of the article. Each question should be unique. The questions should labeled 1-50. The questions should be multiple choice with 6 possible answers: 5 are labeled A-E, and the 6th option should say 'I don't know'. There should only be one correct answer from the options. The questions should cover the unique results, figures, and tables of the article as much as possible."}

To execute the benchmark, we then prompt the model to read the translated article and answer the quiz questions. In this scenario, the model simulates a real person reading the translated text; if the translated article effectively conveys the core details and central findings, the model should perform well on the evaluation. The model’s quiz accuracy, graded against the answer key, constitutes the benchmark result. For instance, if the LLM correctly answers 48 out of 50 questions for a given article in a particular language, the benchmark score for that translation would be 96\%. To ensure that the quiz-taking LLM relies only on the translated article rather than its prior knowledge, we implement two safeguards. First, we exclude the quiz-generation exchange from the model’s memory before administering the quiz. Second, we prevent pre-training contamination by filtering for articles that score 0\% on the benchmark when the article is not provided. Full details on prompts and model parameters are provided in the Appendix.

\subsection{Results}

For this study, we apply the QA benchmark to six articles spanning a wide range of disciplines, from medicine to archaeology to quantum optics. Each article is evaluated across translations in 28 different languages, which were selected based on countries in Nature Index's Research Leaders list and further supplemented to include languages from more regions of the world. We also include an English baseline, which we perform by conducting the quiz on the original article. Figure \ref{benchmark} presents the results. The overall average performance across all 29 languages and all six articles is 95.9\%, with the lowest average score at 91.7\% (Tamil) and the highest average score at 98.0\% (Swedish). Notably, no individual quiz score falls below 84\%, and translations in 23 languages score 100\% on at least one article. These high QA results indicate that our translation approach effectively conveys the key findings and essential details of scientific articles across diverse disciplines.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\textwidth]{figs/benchmark.pdf}
\end{center}
\caption{QA benchmarking results for six articles translated into 28 different languages, plotted by highest average score. The dashed line indicates the overall average performance (95.6\%) across all languages and articles.}
\label{benchmark}
\end{figure}

The English baseline score (97.3\%) is higher than the overall average, but not a perfect 100\%. We attribute this to two potential factors: (1) the quiz-taking LLM, like a human reader, may exhibit slight imperfections in reading comprehension, and (2) quiz questions or answer choices may be occasionally ambiguous (see Section \ref{qstats_section} for further analysis). While refining the quiz questions could further improve the benchmark, we believe the current methodology already provides a strong evaluation framework. Additionally, our results reveal that "low-resource" languages such as Urdu, Telugu, and Tamil perform slightly below high-resource languages, aligning with prior findings in both machine translation and multilingual LLM research~\citep{nicholas2023lost}~\citep{jiao2023chatgpt}. However, since even the lowest-performing languages achieve an average accuracy above 91\%, this effect is minor, demonstrating that our benchmarking technique is applicable across a wide range of languages.

Comparisons with the same articles translated as plain text (without JATS formatting) by GPT-4o and Google Translate further support our finding that highly structured text is translated just as effectively as unstructured text. We observe no degradation in translation quality due to the additional task of preserving structured content. Specifically, the average benchmark score for GPT-4o’s XML-based translations (95.9\%) closely matches that of GPT-4o’s plain text translations (96.0\%) and Google Translate (95.8\%) (Section \ref{benchmark_appendix}). While this study focuses on benchmarking JATS-formatted translations as a form of customization, our QA-based evaluation method is broadly applicable for evaluating translations across various formats, even when other types of translation customizations are applied.

\section{Feedback from authors}
\label{human}

In this section we complement the QA benchmarking results with evaluations from 15 human scientists across various languages and disciplines, including theoretical and experimental quantum optics, nanophotonics, biostatistics, materials science, magneto-electronics, machine learning, and more. In this study, each participant is provided with a translation of their own scientific paper in their native language, generated using our method. We then gather feedback on translation quality using the following questions:

\begin{enumerate}
    \item How effectively does the translation \textbf{convey the original information} of the article?
    \item How well do you think another speaker of this language would be able to \textbf{understand the key ideas} of this paper just from this translation?
    \item How satisfied are you with the translation of \textbf{technical terms} in the article?
    \item How well does the translation \textbf{flow and maintain cohesion} throughout the text?
    \item How well does the translation maintain the original \textbf{tone and style} of the article?
\end{enumerate}

For each question, the three possible options are \textit{few or no issues}, \textit{some issues}, \textit{many issues}, and \textit{other}. Participants also have the opportunity to provide free-form comments with their observations and opinions. Questions 1 and 2 target the accuracy and main details, similar to the QA benchmark, while questions 3, 4, and 5 probe stylistic and subjective aspects of translation quality. Through this questionnaire we aim to gain deeper insights into the academic community's perspective on what defines effective scientific translation.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\textwidth]{figs/survey.pdf}
\end{center}
\caption{Feedback from scientists. \textbf{a.} Survey responses from 15 participants after reading their translated paper, with questions sorted by average score. \textbf{b.} Languages represented by the participants.}
\label{survey}
\end{figure}

As expected, nearly all researchers in our study (93.3\%) report that the translation of their paper contains \textit{few or no issues} in conveying the original information, reinforcing the findings from our QA benchmarking. Participants also generally agree (86.7\%) that other scientists reading their translated paper would understand the key ideas with \textit{few or no issues} (Fig. \ref{survey}a). Based on some participant comments, the most commonly cited issues in this area include minor misinterpretations and inconsistencies in vocabulary (e.g., a specific word being translated differently throughout the text).

Key insights arise from the more subjective questions. As one might anticipate from machine translation, authors rate lower scores in the categories of tone and style, flow and cohesion, and technical terms. In particular, many participants (86.7\%) describe an unnatural quality to the translation or dissatisfaction attributed to the handling of technical and domain-specific vocabulary. With regard to technical vocabulary, participants reported two kinds of issues:

\begin{enumerate}[label=(\roman*)]
    \item \textbf{Mistranslation}: This technical term exists in their native language, but the model translated it awkwardly or incorrectly.
    \begin{enumerate}
        \item \textbf{Example 1}: The model translated \textit{edge coupling} into French as \textit{couplage par bord}, but the more commonly-used phrase is \textit{couplage par la tranche}.
        \item \textbf{Example 2}: The model translated \textit{switching} (e.g. magnetic switching) into Chinese as \textit{切換}, but \textit{轉換} is a better fit.
    \end{enumerate}
    \item \textbf{Overtranslation}: This technical term does not exist in their native language, or is rarely used in practice, and the original English word is preferred.
    \begin{enumerate}
        \item \textbf{Example 1}: The model provided a literal translation of \textit{rigorous coupled-wave analysis} into Korean (\textit{엄밀 결합 파동 해석}), but using the English term is preferred.
        \item \textbf{Example 2}: The model translated \textit{gap} (e.g. Hamiltonian/energetic gap) into Spanish as \textit{brecha} (breach). A better translation might be \textit{salto}, as in \textit{salto de energía} (energy jump), but many scientists would simply use the English \textit{gap}.
    \end{enumerate}
\end{enumerate}

Whether certain terms might be more appropriately left untranslated is not a typical factor in traditional machine translation. However, the feedback from scientists highlights the importance of this consideration in scientific translation. The frequency of overtranslation comments in our survey responses (33.3\%) suggests the  need for nuanced translation approaches that align with how technical terms are used in practice.

\section{Feedback-adaptive translation}
\label{technicalterms}

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\textwidth]{figs/oneshot.pdf}
\end{center}
\caption{In-context learning to customize scientific translation. \textbf{a.} One-shot prompt construction. In the one-shot example, the word "transition metal dichalgocenides" (in bold) is kept in English, while the rest of the excerpt is translated. Full prompts are available in the Appendix. \textbf{b.} An example sentence translated three different ways, per feedback from scientists: 1) direct translation, 2) preserving the technical term in the original English, 3) direct translation plus original English word in brackets. }
\label{oneshot}
\end{figure}

In this section, we leverage LLM output customization to incorporate the feedback from scientists. In particular, since so many scientists expressed a preference for retaining some technical terms in English, we apply a targeted prompting technique to preserve some English vocabulary during translation.

To translate text while maintaining the appropriate English terms, a key challenge is the inherently subjective nature of deciding which terms to keep and which to translate. To navigate this, we employ few-shot prompting, an in-context learning technique where GPT-4o is provided with verified examples to improve responses in scenarios where data is scarce~\citep{brown2020language}. Specifically, we construct a one-shot prompt using a translated paragraph from a scientific article in which the author of the article has reviewed the translation and identified terms that should remain in English. This curated example then serves as a guide for translating other texts (Fig. \ref{oneshot}a).

Using this prompting method, we generate new translations and seek feedback from five authors who previously expressed concerns about technical term translations. Each participant reviews two versions of an excerpt from their paper: one direct translation and one generated with the one-shot prompt that retains some English terms. They are then asked to indicate their preference between the two versions.

The results of this follow-up survey reveal a diverse range of preferences. As anticipated from the initial survey, three of five participants find that retaining some English terms produces a more natural and readable scientific text. Conversely, the other two participants are more inclined toward the complete translation, citing a preference for better-translated terms over English terms. From the responses, one interesting observation is that speakers of languages with a higher prevalence of English loanwords, such as Korean, tend to favor English technical terms compared to those from languages with fewer English loanwords, such as French, a phenomenon which might be influenced by historical linguistic reasons\footnote{For instance, many scientific terms in English originally derive from French~\citep{faure2018accouchement}.}~\citep{blackwood2013french}~\citep{tyson1993english}. Additionally, one participant proposes a balanced approach: to present the original English term in brackets alongside the translated word, rather than strictly choosing one over the other (Fig. \ref{oneshot}b). The strength of LLM-based translation lies in its ability to integrate diverse customization and feedback, enabling tailored and therefore more effective translations. While this study focuses on the overtranslation phenomenon, the prompting technique we utilize in this section can be applied further to other vocabulary or stylistic preferences by incorporating additional examples.

\section{Discussion}
\label{others}

In this study, we utilized LLM-powered translation to go beyond traditional plain-text translation, resulting in scientific translations that are tailored with both publishers and authors in mind. Our method successfully translates scientific articles in JATS XML while maintaining the complex structure, opening a new avenue for academic journals to include translations for their articles. Through a novel automated QA benchmarking approach, we quantitatively evaluated full article translations across 28 languages and many scientific disciplines, finding that key scientific details are reliably conveyed even in low-resource languages. Further, our human evaluation study revealed valuable insights into the qualitative aspects of scientific translation. While the survey results confirmed the high overall translation accuracy, they also highlighted areas for improvement, such as the handling of technical vocabulary. By leveraging the few-shot prompting technique, we incorporated feedback and generate customized translations that align with the linguistic preferences of researchers across different fields and languages. 

Ultimately, our findings emphasize that the flexibility of LLMs allows for nearly limitless degrees of customizability, making it possible to improve translations based on domain-specific requirements and preferences. This adaptability presents a significant step toward breaking language barriers in academic publishing, fostering broader accessibility and collaboration in global research.

\subsection{Limitations and future directions}
Two participants in our user study reported inconsistencies in the translation of certain terms throughout the article. This likely stems from our approach of translating articles in separate sections to mitigate memory and XML nesting issues (Section \ref{jats}), leading to potential variations in the model's vocabulary choices between different API calls. One possible solution is to track all translations of the same term and standardize them at the end by replacing inconsistent terms with the most common translation. Furthermore, while our method includes the full original article in the prompt to provide context, further research could explore ways to enhance context-awareness in scientific translation, which may also help reduce vocabulary inconsistencies. Additionally, we informally evaluated Llama 3.1-405B for our translation approach and found it to be more prone to XML nesting issues compared to GPT-4o, although further testing with other LLMs could provide valuable insights into their suitability for the translation methods introduced in this study.

Future work could also expand on understanding researchers' preferences in scientific translation. A larger-scale survey may reveal additional patterns in translation preferences, particularly across different languages (as discussed in Section \ref{technicalterms}). Moreover, if our method—or a similar approach—is adopted for large-scale scientific translation, the resulting corpora could be used to fine-tune models, further improving consistency and overall translation quality.

%\subsubsection*{Author Contributions}
%(TBD)

%\subsubsection*{Acknowledgments}
%(TBD) 

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}

\subsection{QA benchmark}
\label{benchmark_appendix}
The journal articles translated for this study are:
\begin{itemize}
    \item Article snippet in Figure \ref{xml_trans}: \citet{lepczyk2023global}
    \item Article 1: \citet{heo2024combined}
    \item Article 2: \citet{fernandez2023large}
    \item Article 3: \citet{rus2023treatment}
    \item Article 4: \citet{sephton2023quantum}
    \item Article 5: \citet{yang2023biofilm}
    \item Article 6: \citet{peng2023one}
\end{itemize}

Model information for generating and executing the QA benchmark:
\begin{itemize}
    \item \textbf{Model}: gpt-4o-2024-08-06
    \item \textbf{Temperature}: 1
    
    \item \textbf{Quiz generation prompt:} "Please read the following scientific journal article. Generate 50 detailed and specific questions to test a reader's understanding of the findings of the article. Each question should be unique. The questions should labeled 1-50. The questions should be multiple choice with 6 possible answers: 5 are labeled A-E, and the 6th option should say 'I don't know'. There should only be one correct answer from the options. The questions should cover the unique results, figures, and tables of the article as much as possible. If you are able to answer any of the questions without having read the article, please generate a better question. Please format your response as a JSON object with the question, possible answers, and correct answers. The JSON key to each question should be its number. Here is the article: [\textit{original article}]"
    
    \item \textbf{Quiz execution prompt}: "Please read the following scientific journal article, which has been translated into [\textit{lang}]. Then answer the questions based on your understanding. Report your answers as a JSON where the keys are the question numbers and the values are your letter answers. Here is the article to read: [\textit{translated article}] and here are the questions: [\textit{questions}]. If you do not know the answer, select 'I don't know' as your answer. Do not make guesses."
\end{itemize}

\subsubsection{Translating quiz questions}
In generating the data in Figure \ref{benchmark} of the main text, we translate the quiz questions into the target language before executing the quiz benchmark. We translate the questions with the following prompt (temperature = 0):
\begin{itemize}
    \item "The following JSON comprises a list of questions about an academic journal article. Please translate the questions and options into [\textit{lang}]. Do not translate the keys of the JSON. Please return the translated JSON. Here is the JSON to translate: [\textit{questions}]"
\end{itemize}

Here, we perform the quiz benchmark without translating the quiz questions (keeping them in English) and find an increase in overall accuracy from 95.6\% to 97.2\% (Fig. \ref{benchmark_eng}), suggesting that the language of the quiz questions may play a role in the model's general performance.

\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\textwidth]{figs/benchmark_engq.pdf}
\end{center}
\caption{QA benchmarking results, where quiz questions are kept in English, plotted by highest average score. The dashed line indicates the overall average performance (97.2\%) across all languages and articles.}
\label{benchmark_eng}
\end{figure}


\subsubsection{Comparisons with plain text translations}
\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\textwidth]{figs/compare.pdf}
\end{center}
\caption{Our XML-based translation approach benchmarked against plain text translation by GPT-4o and Google Translate (GNMT). Solid lines represent the average benchmark scores: 95.9\% for our XML-based approach, 96.0\% for GPT-4o plain text translations, and 95.8\% for Google Translate.}
\label{compare}
\end{figure}
We translate the six articles using the same model (GPT-4o-2024-08-06) but processed the article as plain text, not JATS-formatted, as well as with Google Translate (GNMT) and compare the benchmarking results. The performance among all three translation methods were very similar (Fig. \ref{compare}), indicating that our JATS-formatted translation method sees no degradation as a result of the LLM parsing XML at the same time as translating. 

\subsubsection{Quiz questions with high error rates}
\label{qstats_section}
\begin{figure}[t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=1.0\textwidth]{figs/qstats.pdf}
\end{center}
\caption{Number of incorrect responses on the QA benchmark for each quiz question (1-50).}
\label{qstats}
\end{figure}
In Figure \ref{qstats} we analyze the number of incorrect responses for specific quiz questions across all six articles and 29 languages. In particular, we note that question \# 8 on Article 3 was incorrect for all languages, including English. The quiz question is as follows: "What percentage of patients reported dissociative symptoms that disappeared after SSRI treatment?" with possible answers "15\%," "25\%," "35\%," "45\%," "55\%," and "I don't know." Upon investigation of Article 3, the study reports that all patients who experienced dissociative symptoms before SSRI treatment had those symptoms alleviated with SSRIs. The quiz question is therefore malformed, and further research could be useful to determine methods for generating more robust quiz questions.

\subsection{One-shot prompt for technical terms}
Here we provide the full prompt used for translating excerpts while preserving some English terminology. First, we describe the one-shot example prompt:

\paragraph{Q:} "Here is an excerpt of a scientific article: [\textit{original text}]. Please take note of any highly domain specific words in this excerpt. Then, please translate the excerpt into Korean. But do not translate those highly domain specific words that you identified. For those words, keep the original English words in your translation instead. Everything else in the excerpt should be translated into Korean."

\paragraph{A:} [\textit{translated text with some technical terms preserved}]

We use the following excerpt from \citet{ahn2017strain} as our author-curated one-shot example:

\paragraph{Original excerpt:} \textit{Two-dimensional (2D) transition metal dichalcogenides (TMDCs) have been the subject of focused research owing to their potential applications in optoelectronics and sub 10nm transistors. The primary attraction of TMDCs such as MoS2 and WSe2 for both applications is their naturally terminated surface, which allows them to be scaled down to the atomic limit without the concern of surface dangling bonds. Furthermore, in many 2D materials, a number of desirable properties emerge at the monolayer limit, the most notable of which being the presence of a direct bandgap. Many studies based on mechanical bending of exfoliated 2D TMDCs have been conducted on flexible substrates, and they have shown that the application of strain can tune the properties of this new class of materials. For example, it has been demonstrated that in multilayer WSe2, particularly in nominally indirect-gap bilayer WSe2, application of tensile strain can result in a transition from an indirect-to-direct bandgap. Growth on epitaxial substrates with a controlled lattice constant mismatch has typically been utilized to establish built-in strain in three-dimensional semiconductors. However, due to the relatively weak interaction between 2D materials and substrates, this established method of strain engineering is likely not applicable for the strain-engineered growth of TMDCs. In this work, we demonstrate strain engineering of 2D materials directly via chemical vapor deposition (CVD) growth while simultaneously maintaining high material quality, by utilizing the thermal coefficient of expansion (TCE) mismatch between the TMDC and the growth substrate.}

\paragraph{Translation:} \textit{이차원(2D) 전이 \textbf{transition metal dichalcogenides} (TMDCs)은 광전자공학 및 10nm 이하 트랜지스터에 대한 잠재적 응용 가능성으로 인해 집중적인 연구의 대상이 되어 왔습니다. MoS2 와 WSe2 와 같은 TMDCs의 주요 매력은 자연적으로 종결된 표면으로, 표면 매달린 결합에 대한 걱정 없이 원자적 한계까지 축소할 수 있다는 점입니다. 게다가, 많은 2D 물질에서 단층 한계에서 \textbf{direct-bandgap}의 존재와 같은 여러 바람직한 특성이 나타납니다. \textbf{exfoliated 2D TMDCs}의 기계적 굽힘을 기반으로 한 많은 연구가 유연한 기판에서 수행되었으며, 변형의 적용이 이 새로운 물질 클래스의 특성을 조정할 수 있음을 보여주었습니다. 예를 들어, 다층 WSe2, 특히 명목상 \textbf{indirect bandgap} 을 가진 이중층 WSe2 에서 인장 \textbf{tensile strain}의 적용이 \textbf{indirect-to-direct bandgap}전환을 초래할 수 있음이 입증되었습니다. 제어된 \textbf{lattice constant}불일치를 가진 \textbf{epitaxial}기판에서의 성장은 일반적으로 3차원 반도체에서 내장된 변형을 확립하는 데 사용되었습니다. 그러나 2D 물질과 기판 간의 상대적으로 약한 상호작용으로 인해, 이 확립된 변형 공학 방법은 TMDCs의 변형 공학적 성장을 위해 적용될 가능성이 낮습니다. 이 연구에서는 TMDC와 성장 기판 간의 열팽창 계수(TCE) 불일치를 활용하여 CVD 성장을 통해 2D 물질의 변형 공학을 직접적으로 시연하면서 동시에 높은 물질 품질을 유지합니다.}

To generate this example, the excerpt was translated by GPT-4o as before, then we re-inserted specific English terms (in bold) at the paper author's discretion. We feed this to the model as an explicit example (one-shot example). Using the same prompt as above, but replacing 'Korean' with the target language, we prompt the model to translate excerpts from other articles into other languages, resulting in translations with occasional English technical terminology.

\end{document}
