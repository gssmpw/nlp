\section{Prior Work}
\label{subsec:priorwork}
\textbf{Disentanglement} Despite the vast amount of literature in disentanglement particularly in the context of causal representation learning \cite{}, there are only a few that are close to the exact setting we assume in this work. \cite{10.5555/3540261.3541519} also studied causally disambiguating domain intrinsic exogenous variables ("style") and an effect intrinsic variables ("context"), but they assume the dataset for either domain to be sampled using the same context distribution, whereas we allow for different observed context distributions in the observational data (cf. Figure \ref{fig:jointCausalGraph}). \cite{pmlr-v206-morioka23a, daunhawer2023identifiability} extended the above work and theoretically analyzed the above setting, known as "multimodal" SCMs. However, they all seem to assume access to paired data across domains that share the same context, which we don't. Additionally, our kernel-based losses critically differ from their contrastive loss based approaches. Lastly, our setting for unpaired observed data is closest to the Out-Of-Variable generalization setting proposed in \cite{guo2024outofvariable}, but we specifically solve (counterfactual) generative tasks in this setting, whereas they restrict themselves to discriminative tasks.

\textbf{Domain Shifts} Informally known by various names in the literature such as Domain Adaptation (DA) (\cite{10.1007/978-3-030-71704-9_65, article}), Translation (\cite{8100115,9010872}), etc. The underlying idea is to train a model using a samples from one distribution, i.e. the source, and be able to predict objects belonging to another distribution, termed as the target distribution. Whilst a plethora of such works exist in this area, to the best of our knowledge, it appears that a causal angle to this problem has largely been overlooked. \cite{NEURIPS2018_39e98420} proposes a causal DAG modeling domain adaptation and incorporates a context variable, as we also do. \cite{10.5555/3524938.3525815} on the other hand delves into this approach deeper and attempts to model the DA problem via structural equation models, but relies on the assumption of having the data generative mechanism to be invariant across domains. Our methodology however, admits different priors as well as mechanisms across domains. 
Lastly, \cite{Zhang_Gong_Schoelkopf_2015} also examine the DA problem in a simple two-variable setting and demonstrate conditions for transfer of the relevant context across domains by considering the appropriate prior and posterior distributions of the target domain. However, they constrain the causal mechanism to linear structural equations. Furthermore, none of the above works involve answering counterfactual queries in the target domain, nor deal with counterfactual generation in its rigorous form.