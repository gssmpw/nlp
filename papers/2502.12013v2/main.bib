@inproceedings{10.5555/3042817.3043028,
author = {Zhang, Kun and Sch\"{o}lkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
title = {Domain adaptation under target and conditional shift},
year = {2013},
publisher = {JMLR.org},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–819–III–827},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inbook{10.1145/3501714.3501743,
author = {Bareinboim, Elias and Correa, Juan D. and Ibeling, Duligur and Icard, Thomas},
title = {On Pearl’s Hierarchy and the Foundations of Causal Inference},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3501714.3501743},
booktitle = {Probabilistic and Causal Inference: The Works of Judea Pearl},
pages = {507–556},
numpages = {50}
}

@inproceedings{
xia2023neural,
title={Neural Causal Models for Counterfactual Identification and Estimation},
author={Kevin Muyuan Xia and Yushu Pan and Elias Bareinboim},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=vouQcZS8KfW}
}

@inproceedings{
xia2021the,
title={The Causal-Neural Connection: Expressiveness, Learnability, and Inference},
author={Kevin Muyuan Xia and Kai-Zhan Lee and Yoshua Bengio and Elias Bareinboim},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=hGmrNwR8qQP}
}

@INPROCEEDINGS{8237506,
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2242-2251},
  keywords={Training;Painting;Training data;Semantics;Extraterrestrial measurements;Graphics},
  doi={10.1109/ICCV.2017.244}}

@INPROCEEDINGS{10204821,
author = {S. Xie and Y. Xu and M. Gong and K. Zhang},
booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Unpaired Image-to-Image Translation with Shortest Path Regularization},
year = {2023},
volume = {},
issn = {},
pages = {10177-10187},
abstract = {Unpaired image-to-image translation aims to learn proper mappings that can map images from one domain to another domain while preserving the content of the input image. However, with large enough capacities, the network can learn to map the inputs to any random permutation of images in another domain. Existing methods treat two domains as discrete and propose different assumptions to address this problem. In this paper, we start from a different perspective and consider the paths connecting the two domains. We assume that the optimal path length between the input and output image should be the shortest among all possible paths. Based on this assumption, we propose a new method to allow generating images along the path and present a simple way to encourage the network to find the shortest path without pair information. Extensive experiments on various tasks demonstrate the superiority of our approach. The code is available at https://github.com/Mid-Push/santa.},
keywords = {computer vision;codes;pattern recognition;task analysis},
doi = {10.1109/CVPR52729.2023.00981},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00981},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{
korotin2023neural,
title={Neural Optimal Transport},
author={Alexander Korotin and Daniil Selikhanovych and Evgeny Burnaev},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=d8CBRlWNkqH}
}

@INPROCEEDINGS{9577868,
  author={Li, Xinyang and Zhang, Shengchuan and Hu, Jie and Cao, Liujuan and Hong, Xiaopeng and Mao, Xudong and Huang, Feiyue and Wu, Yongjian and Ji, Rongrong},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image-to-image Translation via Hierarchical Style Disentanglement}, 
  year={2021},
  volume={},
  number={},
  pages={8635-8644},
  keywords={Training;Computer vision;Codes;Scalability;Process control;Semisupervised learning;Pattern recognition},
  doi={10.1109/CVPR46437.2021.00853}}

@INPROCEEDINGS{9010872,
  author={Lin, Yu-Jing and Wu, Po-Wei and Chang, Che-Han and Chang, Edward and Liao, Shih-Wei},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes}, 
  year={2019},
  volume={},
  number={},
  pages={5913-5921},
  keywords={Interpolation;Generators;Gallium nitride;Hair;Image color analysis;Image reconstruction;Training},
  doi={10.1109/ICCV.2019.00601}}

@misc{yan2016attribute2imageconditionalimagegeneration,
      title={Attribute2Image: Conditional Image Generation from Visual Attributes}, 
      author={Xinchen Yan and Jimei Yang and Kihyuk Sohn and Honglak Lee},
      year={2016},
      eprint={1512.00570},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1512.00570}, 
}


@InProceedings{pmlr-v238-manupriya24a,
  title = 	 {Consistent Optimal Transport with Empirical Conditional Measures},
  author =       {Manupriya, Piyushi and Das, Rachit K. and Biswas, Sayantan and N Jagarlapudi, SakethaNath},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3646--3654},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/manupriya24a/manupriya24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/manupriya24a.html},
  abstract = 	 {Given samples from two joint distributions, we consider the problem of Optimal Transportation (OT) between them when conditioned on a common variable. We focus on the general setting where the conditioned variable may be continuous, and the marginals of this variable in the two joint distributions may not be the same. In such settings, standard OT variants cannot be employed, and novel estimation techniques are necessary. Since the main challenge is that the conditional distributions are not explicitly available, the key idea in our OT formulation is to employ kernelized-least-squares terms computed over the joint samples, which implicitly match the transport plan’s marginals with the empirical conditionals. Under mild conditions, we prove that our estimated transport plans, as a function of the conditioned variable, are asymptotically optimal. For finite samples, we show that the deviation in terms of our regularized objective is bounded by $O(m^{-1/4})$, where $m$ is the number of samples. We also discuss how the conditional transport plan could be modelled using explicit probabilistic models as well as using implicit generative ones. We empirically verify the consistency of our estimator on synthetic datasets, where the optimal plan is analytically known. When employed in applications like prompt learning for few-shot classification and conditional-generation in the context of predicting cell responses to treatment, our methodology improves upon state-of-the-art methods.}
}

@inproceedings{kantorovich1942transfer,
  title={On the transfer of masses (in Russian)},
  author={Kantorovich, L},
  booktitle={Doklady Akademii Nauk},
  volume={37},
  pages={227},
  year={1942}
}

@inproceedings{NIPS2015_a9eb8122,
 author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning with a Wasserstein Loss},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/a9eb812238f753132652ae09963a05e9-Paper.pdf},
 volume = {28},
 year = {2015}
}

@InProceedings{pmlr-v70-arjovsky17a,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/arjovsky17a.html},
  abstract = 	 {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.}
}

@InProceedings{fatras2021jumbot,
author    = {Fatras, Kilian and S\'ejourn\'e, Thibault and Courty, Nicolas and Flamary, R\'emi},
title     = {Unbalanced minibatch Optimal Transport; applications to Domain Adaptation},
booktitle = {Proceedings of the 38th International Conference on Machine Learning},
year      = {2021}
}

@InProceedings{pmlr-v162-nguyen22d,
  title = 	 {On Transportation of Mini-batches: A Hierarchical Approach},
  author =       {Nguyen, Khai and Nguyen, Dang and Nguyen, Quoc Dinh and Pham, Tung and Bui, Hung and Phung, Dinh and Le, Trung and Ho, Nhat},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16622--16655},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nguyen22d.html},
  abstract = 	 {Mini-batch optimal transport (m-OT) has been successfully used in practical applications that involve probability measures with a very high number of supports. The m-OT solves several smaller optimal transport problems and then returns the average of their costs and transportation plans. Despite its scalability advantage, the m-OT does not consider the relationship between mini-batches which leads to undesirable estimation. Moreover, the m-OT does not approximate a proper metric between probability measures since the identity property is not satisfied. To address these problems, we propose a novel mini-batch scheme for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT), that finds the optimal coupling between mini-batches and it can be seen as an approximation to a well-defined distance on the space of probability measures. Furthermore, we show that the m-OT is a limit of the entropic regularized version of the BoMb-OT when the regularized parameter goes to infinity. Finally, we carry out experiments on various applications including deep generative models, deep domain adaptation, approximate Bayesian computation, color transfer, and gradient flow to show that the BoMb-OT can be widely applied and performs well in various applications.}
}

@inproceedings{10.1007/978-3-030-01225-0_28,
author = {Damodaran, Bharath Bhushan and Kellenberger, Benjamin and Flamary, R\'{e}mi and Tuia, Devis and Courty, Nicolas},
title = {DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation},
year = {2018},
isbn = {978-3-030-01224-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-01225-0_28},
doi = {10.1007/978-3-030-01225-0_28},
abstract = {In computer vision, one is often confronted with problems of domain shifts, which occur when one applies a classifier trained on a source dataset to target data sharing similar characteristics (e.g. same classes), but also different latent data structures (e.g. different acquisition conditions). In such a situation, the model will perform poorly on the new data, since the classifier is specialized to recognize visual cues specific to the source domain. In this work we explore a solution, named DeepJDOT, to tackle this problem: through a measure of discrepancy on joint deep representations/labels based on optimal transport, we not only learn new data representations aligned between the source and target domain, but also simultaneously preserve the discriminative information used by the classifier. We applied DeepJDOT to a series of visual recognition tasks, where it compares favorably against state-of-the-art deep domain adaptation methods.},
booktitle = {Computer Vision – ECCV 2018: 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IV},
pages = {467–483},
numpages = {17},
keywords = {Optimal transport, Deep domain adaptation},
location = {Munich, Germany}
}


@InProceedings{pmlr-v202-nasr-esfahany23a,
  title = 	 {Counterfactual Identifiability of Bijective Causal Models},
  author =       {Nasr-Esfahany, Arash and Alizadeh, Mohammad and Shah, Devavrat},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25733--25754},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/nasr-esfahany23a/nasr-esfahany23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/nasr-esfahany23a.html},
  abstract = 	 {We study counterfactual identifiability in causal models with bijective generation mechanisms (BGM), a class that generalizes several widely-used causal models in the literature. We establish their counterfactual identifiability for three common causal structures with unobserved confounding, and propose a practical learning method that casts learning a BGM as structured generative modeling. Learned BGMs enable efficient counterfactual estimation and can be obtained using a variety of deep conditional generative models. We evaluate our techniques in a visual task and demonstrate its application in a real-world video streaming simulation task.}
}

@inproceedings{ijcai2024p907,
  title     = {Learning Structural Causal Models through Deep Generative Models: Methods, Guarantees, and Challenges},
  author    = {Poinsot, Audrey and Leite, Alessandro and Chesneau, Nicolas and Sebag, Michele and Schoenauer, Marc},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {8207--8215},
  year      = {2024},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2024/907},
  url       = {https://doi.org/10.24963/ijcai.2024/907},
}

@misc{nasresfahany2023counterfactualnonidentifiabilitylearnedstructural,
      title={Counterfactual (Non-)identifiability of Learned Structural Causal Models}, 
      author={Arash Nasr-Esfahany and Emre Kiciman},
      year={2023},
      eprint={2301.09031},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2301.09031}, 
}

@book{10.5555/1642718,
author = {Pearl, Judea},
title = {Causality: Models, Reasoning and Inference},
year = {2009},
isbn = {052189560X},
publisher = {Cambridge University Press},
address = {USA},
edition = {2nd},
abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{simonyan2015deepconvolutionalnetworkslargescale,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}

@INPROCEEDINGS{9710580,
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
  year={2021},
  volume={},
  number={},
  pages={9992-10002},
  keywords={Image segmentation;Computer vision;Visualization;Computational modeling;Semantics;Object detection;Computer architecture;Representation learning;Detection and localization in 2D and 3D;Recognition and classification;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.00986}}

@inproceedings{zhang2018perceptual,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@article{Vlontzos2021EstimatingCC,
  title={Estimating categorical counterfactuals via deep twin networks},
  author={Athanasios Vlontzos and Bernhard Kainz and Ciar{\'a}n M. Gilligan-Lee},
  journal={Nature Machine Intelligence},
  year={2021},
  volume={5},
  pages={159-168},
  url={https://api.semanticscholar.org/CorpusID:248986851}
}

@ARTICLE{9336268,
author={Ye, Mang and Shen, Jianbing and Lin, Gaojie and Xiang, Tao and Shao, Ling and Hoi, Steven C. H.},
journal={ IEEE Transactions on Pattern Analysis \& Machine Intelligence },
title={{ Deep Learning for Person Re-Identification: A Survey and Outlook }},
year={2022},
volume={44},
number={06},
ISSN={1939-3539},
pages={2872-2893},
abstract={ Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for four different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed. },
keywords={Annotations;Cameras;Training;Training data;Feature extraction;Data models;Deep learning},
doi={10.1109/TPAMI.2021.3054775},
url = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3054775},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jun}

@article{a1ba6a14f6cc486880a9789690c52025,
title = "Structural counterfactuals: A brief introduction",
abstract = "Recent advances in causal reasoning have given rise to a computational model that emulates the process by which humans generate, evaluate, and distinguish counterfactual sentences. Contrasted with the {\"}possible worlds{\"} account of counterfactuals, this {\"}structural{\"} model enjoys the advantages of representational economy, algorithmic simplicity, and conceptual clarity. This introduction traces the emergence of the structural model and gives a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences.",
keywords = "Causal reasoning, Counterfactuals, Structural models",
author = "Judea Pearl",
year = "2013",
month = aug,
doi = "10.1111/cogs.12065",
language = "אנגלית",
volume = "37",
pages = "977--985",
number = "6",
}

@article{10.1007/s10618-021-00818-9,
author = {Marchezini, Guilherme F. and Lacerda, Anisio M. and Pappa, Gisele L. and Meira, Wagner and Miranda, Debora and Romano-Silva, Marco A. and Costa, Danielle S. and Diniz, Leandro Malloy},
title = {Counterfactual inference with latent variable and its application in mental health care},
year = {2022},
issue_date = {Mar 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {36},
number = {2},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00818-9},
doi = {10.1007/s10618-021-00818-9},
abstract = {This paper deals with the problem of modeling counterfactual reasoning in scenarios where, apart from the observed endogenous variables, we have a latent variable that affects the outcomes and, consequently, the results of counterfactuals queries. This is a common setup in healthcare problems, including mental health. We propose a new framework where the aforementioned problem is modeled as a multivariate regression and the counterfactual model accounts for both observed and a latent variable, where the latter represents what we call the patient individuality factor (φ). In mental health, focusing on individuals is paramount, as past experiences can change how people see or deal with situations, but individuality cannot be directly measured. To the best of our knowledge, this is the first counterfactual approach that considers both observational and latent variables to provide deterministic answers to counterfactual queries, such as: what if I change the social support of a patient, to what extent can I change his/her anxiety? The framework combines concepts from deep representation learning and causal inference to infer the value of φ and capture both non-linear and multiplicative effects of causal variables. Experiments are performed with both synthetic and real-world datasets, where we predict how changes in people’s actions may lead to different outcomes in terms of symptoms of mental illness and quality of life. Results show the model learns the individually factor with errors lower than 0.05 and answers counterfactual queries that are supported by the medical literature. The model has the potential to recommend small changes in people’s lives that may completely change their relationship with mental illness.},
journal = {Data Min. Knowl. Discov.},
month = mar,
pages = {811–840},
numpages = {30},
keywords = {Mental health, Multivariate regression, Counterfactual inference}
}

@inproceedings{
jin2023cladder,
title={{CL}adder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models},
author={Zhijing Jin and Yuen Chen and Felix Leeb and Luigi Gresele and Ojasv Kamal and Zhiheng LYU and Kevin Blin and Fernando Gonzalez Adauto and Max Kleiman-Weiner and Mrinmaya Sachan and Bernhard Sch{\"o}lkopf},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=e2wtjx0Yqu}
}

@inproceedings{
wu2023interpretability,
title={Interpretability at Scale: Identifying Causal Mechanisms in Alpaca},
author={Zhengxuan Wu and Atticus Geiger and Thomas Icard and Christopher Potts and Noah Goodman},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=nRfClnMhVX}
}

@inproceedings{
hu2021a,
title={A Causal Lens for Controllable Text Generation},
author={Zhiting Hu and Li Erran Li},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=kAm9By0R5ME}
}

@inproceedings{
pan2024counterfactual,
title={Counterfactual Image Editing},
author={Yushu Pan and Elias Bareinboim},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=OXzkw7vFIO}
}

@article{Yang2020CausalVAEDR,
  title={CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models},
  author={Mengyue Yang and Furui Liu and Zhitang Chen and Xinwei Shen and Jianye Hao and Jun Wang},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={9588-9597},
  url={https://api.semanticscholar.org/CorpusID:220280826}
}

@InProceedings{pmlr-v202-de-sousa-ribeiro23a,
  title = 	 {High Fidelity Image Counterfactuals with Probabilistic Causal Models},
  author =       {De Sousa Ribeiro, Fabio and Xia, Tian and Monteiro, Miguel and Pawlowski, Nick and Glocker, Ben},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {7390--7425},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/de-sousa-ribeiro23a/de-sousa-ribeiro23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/de-sousa-ribeiro23a.html},
  abstract = 	 {We present a general causal generative modelling framework for accurate estimation of high fidelity image counterfactuals with deep structural causal models. Estimation of interventional and counterfactual queries for high-dimensional structured variables, such as images, remains a challenging task. We leverage ideas from causal mediation analysis and advances in generative modelling to design new deep causal mechanisms for structured variables in causal models. Our experiments demonstrate that our proposed mechanisms are capable of accurate abduction and estimation of direct, indirect and total effects as measured by axiomatic soundness of counterfactuals.}
}

@INPROCEEDINGS{9706958,
author = { Dash, Saloni and Balasubramanian, Vineeth N and Sharma, Amit },
booktitle = { 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) },
title = {{ Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals }},
year = {2022},
volume = {},
ISSN = {},
pages = {3879-3888},
abstract = { Counterfactual examples for an input—perturbations that change specific features but not others—have been shown to be useful for evaluating bias of machine learning models, e.g., against specific demographic groups. However, generating counterfactual examples for images is nontrivial due to the underlying causal structure on the various features of an image. To be meaningful, generated perturbations need to satisfy constraints implied by the causal model. We present a method for generating counterfactuals by incorporating a structural causal model (SCM) in an improved variant of Adversarially Learned Inference (ALI), that generates counterfactuals in accordance with the causal relationships between attributes of an image. Based on the generated counterfactuals, we show how to explain a pre-trained machine learning classifier, evaluate its bias, and mitigate the bias using a counterfactual regularizer. On the Morpho-MNIST dataset, our method generates counterfactuals comparable in quality to prior work on SCM-based counterfactuals (DeepSCM), while on the more complex CelebA dataset our method outperforms DeepSCM in generating high-quality valid counterfactuals. Moreover, generated counterfactuals are indistinguishable from reconstructed images in a human evaluation experiment and we subsequently use them to evaluate the fairness of a standard classifier trained on CelebA data. We show that the classifier is biased w.r.t. skin and hair color, and how counterfactual regularization can remove those biases. },
keywords = {Hair;Computer vision;Image color analysis;Perturbation methods;Computational modeling;Prototypes;Machine learning},
doi = {10.1109/WACV51458.2022.00393},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV51458.2022.00393},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jan}

@article{Feuerriegel_2024,
   title={Causal machine learning for predicting treatment outcomes},
   volume={30},
   ISSN={1546-170X},
   url={http://dx.doi.org/10.1038/s41591-024-02902-1},
   DOI={10.1038/s41591-024-02902-1},
   number={4},
   journal={Nature Medicine},
   publisher={Springer Science and Business Media LLC},
   author={Feuerriegel, Stefan and Frauen, Dennis and Melnychuk, Valentyn and Schweisthal, Jonas and Hess, Konstantin and Curth, Alicia and Bauer, Stefan and Kilbertus, Niki and Kohane, Isaac S. and van der Schaar, Mihaela},
   year={2024},
   month=apr, pages={958–968} }

@inproceedings{ijcai2024p721,
  title     = {Beyond What If: Advancing Counterfactual Text Generation with Structural Causal Modeling},
  author    = {Wang, Ziao and Zhang, Xiaofeng and Du, Hongwei},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {6522--6530},
  year      = {2024},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2024/721},
  url       = {https://doi.org/10.24963/ijcai.2024/721},
}

@article{QIU2020265,
title = {Inferring Causal Gene Regulatory Networks from Coupled Single-Cell Expression Dynamics Using Scribe},
journal = {Cell Systems},
volume = {10},
number = {3},
pages = {265-274.e11},
year = {2020},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2020.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405471220300363},
author = {Xiaojie Qiu and Arman Rahimzamani and Li Wang and Bingcheng Ren and Qi Mao and Timothy Durham and José L. McFaline-Figueroa and Lauren Saunders and Cole Trapnell and Sreeram Kannan},
keywords = {single-cell RNA-seq, gene regulatory network inference, pseudotime, single-cell trajectories, Scribe, RNA velocity, causal network inference, real time, coupled dynamics, slam-seq},
abstract = {Summary
Here, we present Scribe (https://github.com/aristoteleo/Scribe-py), a toolkit for detecting and visualizing causal regulatory interactions between genes and explore the potential for single-cell experiments to power network reconstruction. Scribe employs restricted directed information to determine causality by estimating the strength of information transferred from a potential regulator to its downstream target. We apply Scribe and other leading approaches for causal network reconstruction to several types of single-cell measurements and show that there is a dramatic drop in performance for “pseudotime”-ordered single-cell data compared with true time-series data. We demonstrate that performing causal inference requires temporal coupling between measurements. We show that methods such as “RNA velocity” restore some degree of coupling through an analysis of chromaffin cell fate commitment. These analyses highlight a shortcoming in experimental and computational methods for analyzing gene regulation at single-cell resolution and suggest ways of overcoming it.}
}

@inproceedings{jeanneret2024text,
  title={Text-to-image models for counterfactual explanations: a black-box approach},
  author={Jeanneret, Guillaume and Simon, Lo{\"\i}c and Jurie, Fr{\'e}d{\'e}ric},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4757--4767},
  year={2024}
}

@article{52906,title	= {Explaining counterfactual images},author	= {Oran Lang and Ilana Traynis and Yun Liu},year	= {2023},URL	= {https://rdcu.be/dwVKK},journal	= {Nature Biomedical Engineering}}

@article{JMLR:v12:sriperumbudur11a,
  author  = {Bharath K. Sriperumbudur and Kenji Fukumizu and Gert R.G. Lanckriet},
  title   = {Universality, Characteristic Kernels and RKHS Embedding of Measures},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {70},
  pages   = {2389--2410},
  url     = {http://jmlr.org/papers/v12/sriperumbudur11a.html}
}

@INPROCEEDINGS{7410480,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
  year={2015},
  volume={},
  number={},
  pages={1026-1034},
  keywords={Training;Computational modeling;Adaptation models;Testing;Gaussian distribution;Biological neural networks},
  doi={10.1109/ICCV.2015.123}}

@InProceedings{10.1007/978-3-030-71704-9_65,
author="Farahani, Abolfazl
and Voghoei, Sahar
and Rasheed, Khaled
and Arabnia, Hamid R.",
editor="Stahlbock, Robert
and Weiss, Gary M.
and Abou-Nasr, Mahmoud
and Yang, Cheng-Ying
and Arabnia, Hamid R.
and Deligiannidis, Leonidas",
title="A Brief Review of Domain Adaptation",
booktitle="Advances in Data Science and Information Engineering",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="877--894",
abstract="Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, this assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources or having an outdated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a subfield within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, it presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.",
isbn="978-3-030-71704-9"
}

@INPROCEEDINGS{8100115,
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image-to-Image Translation with Conditional Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={5967-5976},
  keywords={Gallium nitride;Generators;Training;Image edge detection;Force;Image resolution},
  doi={10.1109/CVPR.2017.632}}

@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}

@article{article,
author = {Liu, Xiaofeng and Yoo, Chaehwa and Xing, Fangxu and Oh, Hyejin and Fakhri, Georges and Kang, Je-Won and Woo, Jonghye},
year = {2022},
month = {05},
pages = {},
title = {Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives},
journal = {APSIPA Transactions on Signal and Information Processing},
doi = {10.1561/116.00000192}
}
@inproceedings{NEURIPS2018_39e98420,
 author = {Magliacane, Sara and van Ommen, Thijs and Claassen, Tom and Bongers, Stephan and Versteeg, Philip and Mooij, Joris M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{10.5555/3524938.3525815,
author = {Teshima, Takeshi and Sato, Issei and Sugiyama, Masashi},
title = {Few-shot domain adaptation by causal mechanism transfer},
year = {2020},
publisher = {JMLR.org},
abstract = {We study few-shot supervised domain adaptation (DA) for regression problems, where only a few labeled target domain data and many labeled source domain data are available. Many of the current DA methods base their transfer assumptions on either parametrized distribution shift or apparent distribution similarities, e.g., identical conditionals or small distributional discrepancies. However, these assumptions may preclude the possibility of adaptation from intricately shifted and apparently very different distributions. To overcome this problem, we propose mechanism transfer, a meta-distributional scenario in which a data generating mechanism is invariant across domains. This transfer assumption can accommodate nonparametric shifts resulting in apparently different distributions while providing a solid statistical basis for DA. We take the structural equations in causal modeling as an example and propose a novel DA method, which is shown to be useful both theoretically and experimentally. Our method can be seen as the first attempt to fully leverage the structural causal models for DA.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {877},
numpages = {12},
series = {ICML'20}
}

@article{Zhang_Gong_Schoelkopf_2015, title={Multi-Source Domain Adaptation: A Causal View}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9542}, DOI={10.1609/aaai.v29i1.9542}, abstractNote={ &lt;p&gt; This paper is concerned with the problem of domain adaptation with multiple sources from a causal point of view. In particular, we use causal models to represent the relationship between the features X and class label Y , and consider possible situations where different modules of the causal model change with the domain. In each situation, we investigate what knowledge is appropriate to transfer and find the optimal target-domain hypothesis. This gives an intuitive interpretation of the assumptions underlying certain previous methods and motivates new ones. We finally focus on the case where Y is the cause for X with changing PY and PX|Y , that is, PY and PX|Y change independently across domains. Under appropriate assumptions, the availability of multiple source domains allows a natural way to reconstruct the conditional distribution on the target domain; we propose to model PX|Y (the process to generate effect X from cause Y ) on the target domain as a linear mixture of those on source domains, and estimate all involved parameters by matching the target-domain feature distribution. Experimental results on both synthetic and real-world data verify our theoretical results. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhang, Kun and Gong, Mingming and Schoelkopf, Bernhard}, year={2015}, month={Feb.} }

@article{Xia_Bareinboim_2024, title={Neural Causal Abstractions}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30044}, DOI={10.1609/aaai.v38i18.30044}, abstractNote={The ability of humans to understand the world in terms of cause and effect relationships, as well as their ability to compress information into abstract concepts, are two hallmark features of human intelligence. These two topics have been studied in tandem under the theory of causal abstractions, but it is an open problem how to best leverage abstraction theory in real-world causal inference tasks, where the true model is not known, and limited data is available in most practical settings. In this paper, we focus on a family of causal abstractions constructed by clustering variables and their domains, redefining abstractions to be amenable to individual causal distributions. We show that such abstractions can be learned in practice using Neural Causal Models, allowing us to utilize the deep learning toolkit to solve causal tasks (identification, estimation, sampling) at different levels of abstraction granularity. Finally, we show how representation learning can be used to learn abstractions, which we apply in our experiments to scale causal inferences to high dimensional settings such as with image data.}, number={18}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Xia, Kevin and Bareinboim, Elias}, year={2024}, month={Mar.}, pages={20585-20595} }

@article{Muandet_2017,
   title={Kernel Mean Embedding of Distributions: A Review and Beyond},
   volume={10},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000060},
   DOI={10.1561/2200000060},
   number={1–2},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
   year={2017},
   pages={1–141} }

@book{brockwell1991time,
  title={Time series: Theory and methods},
  author={Brockwell, Peter J},
  year={1991},
  publisher={Springer-Verlag}
}

@inproceedings{10.5555/3540261.3541519,
author = {von K\"{u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch\"{o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
title = {Self-supervised learning with data augmentations provably isolates content from style},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1258},
numpages = {17},
series = {NIPS '21}
}

@InProceedings{pmlr-v206-morioka23a,
  title = 	 {Connectivity-contrastive learning: Combining causal discovery and representation learning for multimodal data},
  author =       {Morioka, Hiroshi and Hyvarinen, Aapo},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3399--3426},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/morioka23a/morioka23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/morioka23a.html},
  abstract = 	 {Causal discovery methods typically extract causal relations between multiple nodes (variables) based on univariate observations of each node. However, one frequently encounters situations where each node is multivariate, i.e. has multiple observational modalities. Furthermore, the observed modalities may be generated through an unknown mixing process, so that some original latent variables are entangled inside the nodes. In such a multimodal case, the existing frameworks cannot be applied. To analyze such data, we propose a new causal representation learning framework called connectivity-contrastive learning (CCL). CCL disentangles the observational mixing and extracts a set of mutually independent latent components, each having a separate causal structure between the nodes. The actual learning proceeds by a novel self-supervised learning method in which the pretext task is to predict the label of a pair of nodes from the observations of the node pairs. We present theorems which show that CCL can indeed identify both the latent components and the multimodal causal structure under weak technical assumptions, up to some indeterminacy. Finally, we experimentally show its superior causal discovery performance compared to state-of-the-art baselines, in particular demonstrating robustness against latent confounders.}
}

@inproceedings{
daunhawer2023identifiability,
title={Identifiability Results for Multimodal Contrastive Learning},
author={Imant Daunhawer and Alice Bizeul and Emanuele Palumbo and Alexander Marx and Julia E Vogt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=U_2kuqoTcB}
}

@inproceedings{
guo2024outofvariable,
title={Out-of-Variable Generalisation for Discriminative Models},
author={Siyuan Guo and Jonas Bernhard Wildberger and Bernhard Sch{\"o}lkopf},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=zwMfg9PfPs}
}

@article{schrab2021mmd,
  author  = {Antonin Schrab and Ilmun Kim and M{\'e}lisande Albert and B{\'e}atrice Laurent and Benjamin Guedj and Arthur Gretton},
  title   = {{MMD} Aggregated Two-Sample Test},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {194},
  pages   = {1--81},
  url     = {http://jmlr.org/papers/v24/21-1289.html}
}