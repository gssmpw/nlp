@InProceedings{10.1007/978-3-030-71704-9_65,
author="Farahani, Abolfazl
and Voghoei, Sahar
and Rasheed, Khaled
and Arabnia, Hamid R.",
editor="Stahlbock, Robert
and Weiss, Gary M.
and Abou-Nasr, Mahmoud
and Yang, Cheng-Ying
and Arabnia, Hamid R.
and Deligiannidis, Leonidas",
title="A Brief Review of Domain Adaptation",
booktitle="Advances in Data Science and Information Engineering",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="877--894",
abstract="Classical machine learning assumes that the training and test sets come from the same distributions. Therefore, a model learned from the labeled training data is expected to perform well on the test data. However, this assumption may not always hold in real-world applications where the training and the test data fall from different distributions, due to many factors, e.g., collecting the training and test sets from different sources or having an outdated training set due to the change of data over time. In this case, there would be a discrepancy across domain distributions, and naively applying the trained model on the new dataset may cause degradation in the performance. Domain adaptation is a subfield within machine learning that aims to cope with these types of problems by aligning the disparity between domains such that the trained model can be generalized into the domain of interest. This paper focuses on unsupervised domain adaptation, where the labels are only available in the source domain. It addresses the categorization of domain adaptation from different viewpoints. Besides, it presents some successful shallow and deep domain adaptation approaches that aim to deal with domain adaptation problems.",
isbn="978-3-030-71704-9"
}

@inproceedings{10.5555/3524938.3525815,
author = {Teshima, Takeshi and Sato, Issei and Sugiyama, Masashi},
title = {Few-shot domain adaptation by causal mechanism transfer},
year = {2020},
publisher = {JMLR.org},
abstract = {We study few-shot supervised domain adaptation (DA) for regression problems, where only a few labeled target domain data and many labeled source domain data are available. Many of the current DA methods base their transfer assumptions on either parametrized distribution shift or apparent distribution similarities, e.g., identical conditionals or small distributional discrepancies. However, these assumptions may preclude the possibility of adaptation from intricately shifted and apparently very different distributions. To overcome this problem, we propose mechanism transfer, a meta-distributional scenario in which a data generating mechanism is invariant across domains. This transfer assumption can accommodate nonparametric shifts resulting in apparently different distributions while providing a solid statistical basis for DA. We take the structural equations in causal modeling as an example and propose a novel DA method, which is shown to be useful both theoretically and experimentally. Our method can be seen as the first attempt to fully leverage the structural causal models for DA.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {877},
numpages = {12},
series = {ICML'20}
}

@inproceedings{10.5555/3540261.3541519,
author = {von K\"{u}gelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Sch\"{o}lkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
title = {Self-supervised learning with data augmentations provably isolates content from style},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1258},
numpages = {17},
series = {NIPS '21}
}

@INPROCEEDINGS{8100115,
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image-to-Image Translation with Conditional Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={5967-5976},
  keywords={Gallium nitride;Generators;Training;Image edge detection;Force;Image resolution},
  doi={10.1109/CVPR.2017.632}}

@INPROCEEDINGS{9010872,
  author={Lin, Yu-Jing and Wu, Po-Wei and Chang, Che-Han and Chang, Edward and Liao, Shih-Wei},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes}, 
  year={2019},
  volume={},
  number={},
  pages={5913-5921},
  keywords={Interpolation;Generators;Gallium nitride;Hair;Image color analysis;Image reconstruction;Training},
  doi={10.1109/ICCV.2019.00601}}

@inproceedings{NEURIPS2018_39e98420,
 author = {Magliacane, Sara and van Ommen, Thijs and Claassen, Tom and Bongers, Stephan and Versteeg, Philip and Mooij, Joris M},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/39e98420b5e98bfbdc8a619bef7b8f61-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{Zhang_Gong_Schoelkopf_2015, title={Multi-Source Domain Adaptation: A Causal View}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9542}, DOI={10.1609/aaai.v29i1.9542}, abstractNote={ &lt;p&gt; This paper is concerned with the problem of domain adaptation with multiple sources from a causal point of view. In particular, we use causal models to represent the relationship between the features X and class label Y , and consider possible situations where different modules of the causal model change with the domain. In each situation, we investigate what knowledge is appropriate to transfer and find the optimal target-domain hypothesis. This gives an intuitive interpretation of the assumptions underlying certain previous methods and motivates new ones. We finally focus on the case where Y is the cause for X with changing PY and PX|Y , that is, PY and PX|Y change independently across domains. Under appropriate assumptions, the availability of multiple source domains allows a natural way to reconstruct the conditional distribution on the target domain; we propose to model PX|Y (the process to generate effect X from cause Y ) on the target domain as a linear mixture of those on source domains, and estimate all involved parameters by matching the target-domain feature distribution. Experimental results on both synthetic and real-world data verify our theoretical results. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Zhang, Kun and Gong, Mingming and Schoelkopf, Bernhard}, year={2015}, month={Feb.} }

@article{article,
author = {Liu, Xiaofeng and Yoo, Chaehwa and Xing, Fangxu and Oh, Hyejin and Fakhri, Georges and Kang, Je-Won and Woo, Jonghye},
year = {2022},
month = {05},
pages = {},
title = {Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives},
journal = {APSIPA Transactions on Signal and Information Processing},
doi = {10.1561/116.00000192}
}

@InProceedings{pmlr-v206-morioka23a,
  title = 	 {Connectivity-contrastive learning: Combining causal discovery and representation learning for multimodal data},
  author =       {Morioka, Hiroshi and Hyvarinen, Aapo},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3399--3426},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/morioka23a/morioka23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/morioka23a.html},
  abstract = 	 {Causal discovery methods typically extract causal relations between multiple nodes (variables) based on univariate observations of each node. However, one frequently encounters situations where each node is multivariate, i.e. has multiple observational modalities. Furthermore, the observed modalities may be generated through an unknown mixing process, so that some original latent variables are entangled inside the nodes. In such a multimodal case, the existing frameworks cannot be applied. To analyze such data, we propose a new causal representation learning framework called connectivity-contrastive learning (CCL). CCL disentangles the observational mixing and extracts a set of mutually independent latent components, each having a separate causal structure between the nodes. The actual learning proceeds by a novel self-supervised learning method in which the pretext task is to predict the label of a pair of nodes from the observations of the node pairs. We present theorems which show that CCL can indeed identify both the latent components and the multimodal causal structure under weak technical assumptions, up to some indeterminacy. Finally, we experimentally show its superior causal discovery performance compared to state-of-the-art baselines, in particular demonstrating robustness against latent confounders.}
}

