\subsection{Problem Formulation}

Consider a robot manipulating objects in a bounded workspace $\mathcal W$.
Denote the state space of $\mathcal{W}$ by $S$, each state $s\in S$ includes the robot configuration and the poses of all movable objects in $\mathcal W$.
We define robot action  $ a \in \mathcal A\subseteq SO(3)\times\{-1,0,1\}$,, which includes an end-effector pose in $SO(3)$ and a relative signal of the suction cup with $-1/0/1$ for deactivation/idling/activation respectively. 

Given a long-horizon manipulation task and its final goal, it can be decomposed into modular sub-tasks, which are manipulation primitives. Each manipulation primitive  comes with a human demonstration dataset $D_i:=\{\tau_i^k\}_{k=1}^{n}$,  where  $\tau_i^k \in D_i$ is a sequence of state-action pairs $\{(s_i^k, a_i^k)\}_{k=1}^{|\tau_i|}$. Additionally the system transition  model $\mathcal{T}$ is demonstrated by  \( s_i^{(k+1)} = \mathcal T(s_i^k, a_i^k)\). 

A  primitive manipulation policy \(\pi_i (a_i|s_i)\) is  learned from its corresponding demonstration dataset \(D_i\), with its initiation set defining its states \(\cup s_i^o\)in which the policy can be invoked, and its termination set \( \cup s_i^g\)specifying  the states in which the policy is terminated.

Our assumption is that   a  set of primitive manipulation policies $\Pi :=\{\pi_i\}_{i=1}^{N}$  is is both expandable and sufficient to compose a long-horizon manipulation task and achieve its final goal.

To  effectively address the complexity of actions in the long-horizon manipulation tasks, we  use policy coordination strategy that includes action selection and feedback loops. Action selection determines  next manipulation primitive policy  based on the current state and task goal, while feedback loops ensure continuous adjustments to the policies using  real-time information.

Action selection 