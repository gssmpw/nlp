As robotics technology advances, the deployment of robots in everyday tasks is rapidly transitioning from a conceptual idea to a practical reality. Unlike traditional industrial robots, which are typically designed for repetitive, single-purpose tasks, the next generation of robots is expected to perform complex, dexterous manipulation tasks that require the seamless integration of multiple skills and the execution of diverse sub-tasks.

Recently, advances in policy learning~\cite{chi2023diffusion, zhao2023learning, team2024octo} have shown great promise by effectively learning from human demonstrations to address dexterous manipulation tasks that were notoriously difficult to design and program manually. These methods leverage the richness and variety of human demonstrations, capable of capturing multi-modal data, and many also utilize foundation models to improve generalization and learning efficiency~\cite{open_x_embodiment_rt_x_2023, brohan2023rt1roboticstransformerrealworld, brohan2023rt2visionlanguageactionmodelstransfer, ze20243ddiffusionpolicygeneralizable, yang2024equibotsim3equivariantdiffusionpolicy}. However, despite these advances, most systems are designed and tested to handle specific, single tasks and often fall short when faced with long-horizon tasks that require combining and sequencing multiple skills over extended periods~\cite{mandlekar2021learninggeneralizelonghorizontasks, duan2017oneshotimitationlearning}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/prob.pdf}
   \includegraphics[width=0.5\textwidth]{figures/Intro.pdf}
    \caption{[Top] An example of long-horizon dexterous manipulation. The robot executes four skills to manipulate an object from the boundary of the picking tote to the corner of the packing tote. [Bottom] Our proposed imitation learning model \ours with N-skill and skill selector \progss.}
    \label{fig:intro}
\end{figure}

A key hypothesis we have to improve reliability of policy learning for long-horizon task is that many such tasks can be decomposed into multiple heterogeneous sub-tasks, where each sub-task requires a distinct skill, and these skills are highly reusable. This is particularly evident in warehouse robotics, where tasks such as picking and packing involve dinstinct skills like flipping, grasping, and pushing. For example, Fig.~\ref{fig:intro}[Top] shows a robot flipping an object from the boundary of a picking tote and compactly packing it in the corner of a packing tote. %In this paper, we refer to these skill modules interchangeably as sub-tasks or skills.

Decades of research have explored the problem of chaining multiple skills to achieve long-horizon tasks. Task and Motion Planning (TAMP) methods integrate high-level symbolic reasoning with low-level motion planning, enabling robots to sequence skills while ensuring physical feasibility~\cite{Dantam16, Srivastava14, Wolfe10}. Similarly, reinforcement learning (RL), particularly hierarchical RL, decomposes tasks into sub-skills, facilitating more efficient exploration in complex environments~\cite{kulkarni2016hierarchicaldeepreinforcementlearning, eysenbach2018diversityneedlearningskills, vezhnevets2017feudalnetworkshierarchicalreinforcement, bacon2016optioncriticarchitecture}. However, these methods still face challenges. For instance, RL methods struggle with exploration and scalability, as the search space for multi-skill tasks grows exponentially with complexity~\cite{nasiriany2022augmenting}. %Additionally, ensuring smooth transitions between skills is difficult, requiring precise coordination of states.

Instead, we propose a novel approach, \ours (Multi-Head Skill Transformer), designed to enhance the reliability of policy learning by primarily building upon the policy's existing structure, while introducing minimal additional complexity. \ours operates by decomposing long-horizon tasks into sequence of reusable skills. At each timestamp, the appropriate skill is selected based on the current observation and state, enabled by a robust progress estimator for each skill and a skill selector. 

Specifically, \ours extends the policy learning model Octo~\cite{team2024octo} by introducing multiple heads, with each head responsible for a specific skill, along with a progress estimator that tracks the progress of skill execution. A skill selector function, named \progss, maps the progress across all skill heads to determine the appropriate skill to execute at any given state. Both the skill heads and progress estimators are trained simultaneously using the same pre-trained Octo transformer backbone. With the multi-head structure, \ours allows for the training of multiple skills either synchronously or asynchronously, facilitating the integration of a large skill set and the addition of new skills as needed.

The main advantage of \ours is that it provides a clear understanding of each individual skill and the overall task progress through continuous progress estimation. It also offers flexibility in determining when to terminate a skill by setting a termination threshold. Additionally, \ours enhances reliability under disturbance, as the model continuously reasons when to skip or redo certain skills to ensure task completion.

Comprehensive experiments in both simulated and real-world environments demonstrate that \ours effectively addresses challenging long-horizon dexterous manipulation tasks, showing significant improvements over the Octo baseline models. In tasks involving flipping, picking, packing, and pushing, \ours increased the overall task completion rate from 32.5\% with the baseline Octo single policy to 90\% with \ours.

