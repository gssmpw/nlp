As robotics technology advances, the deployment of robots in everyday tasks is quickly evolving from a conceptual idea to a practical reality.
Unlike traditional industrial robots that are typically designed for repetitive, single-purpose tasks, the next generation of robots will be expected to perform complex, dexterous manipulation tasks, often requiring the seamless execution of multiple heterogeneous sub-tasks. 
Fig.~\ref{fig:intro}[Top] shows an example of long horizon dexterous manipulation where a robot is tasked to flip an object from the boundary of the picking tote and compactly pack it at the corner of the packing tote.
The intricate nature of these tasks makes it increasingly difficult to train a single policy capable of handling the entire spectrum of required actions.

\todo{Kai: Two contributions of MuST in policy learning: reducing action complexity, provide details during execution (executing which skill and the progress)}

To deal with action complexity, previous efforts in reinforcement learning\cite{nasiriany2022augmenting} and imitation learning\cite{belkhale2023hydra} decompose the manipulation tasks into sub-tasks, learn a policy for each sub-task, and use a classifier-based skill selector to determine which policy to execute at the current state.
\todo {Jane:  above paragraph leads you to focus on one challenge in Dexterous Long Horizon manipulation, so that next three paragraphs went to a solution - with un-necessary details in introduction section - are there other challenges MuST aims to address ? }
 
In contrast, we propose \progss, a ``progress'' guided skill selector for imitation learning. 
Specifically, we estimate the current execution progress of each skill. 
By comparing the current progress values with those in demonstration episodes, the robot selects the proper skill to execute at the current state.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/prob.pdf}
   \includegraphics[width=0.5\textwidth]{figures/Intro.pdf}
    \caption{[Top] An example of long-horizon dexterous manipulation. The robot executes four skills to manipulate an object from the boundary of the picking tote to the corner of the packing tote. [Bottom] Our proposed imitation learning model \ours with N-skill and skill selector \progss.}
    \label{fig:intro}
\end{figure}
\todo{Jane: Introduction vs. Method.B ProSS section - decide what level of discussion here or later}
\progss includes the following benefits:
First, by continuously estimating skill progress values and comparing the values with those in human demonstrations, \progss allows the robot to not only choose the task sequence to follow when multiple task sequences are demonstrated, but also skip and redo skills after unexpected disturbance.
Second, in the case of closed-loop control, progress values change continuously throughout the execution while classifier-based skill selectors have a sudden phase transition as the executing skill switches. 
Third, \progss enables flexible design choices on termination progress thresholds of sub-tasks, which allows users to balance completion precision and execution time.
Finally, in the same spirit of the ``state value function'' in reinforcement learning, \progss estimates fine-grained completion progress of both the whole manipulation task and individual sub-tasks during execution. 
Therefore, \progss can be used for higher level task planners to evaluate the cost of following a task sequence.% or serve as a critic network in a reinforcement learning framework. 


\todo{Jane: is it important to go into training details here or svae for later?  }
To learn \progss from demonstration, we also propose an imitation learning framework \ours({\bf Mu}lti-Head {\bf S}kill {\bf T}ransformer), where a set of skills and \progss is trained sharing the same pre-trained Octo transformer backbone\cite{team2024octo}. 
The multi-head design enables us to efficiently train multiple skills synchronously or asynchronously, and deploy a large set of skills. Comprehensive experiments in both simulated and real-world environment demonstrate that \ours effectively solves challenging long-horizon dexterous manipulation tasks and offers a substantial improvement compared with Octo baseline models.
% \todo{Jame: do we need briefly introduce how MuST uses Multi-head attention feature in the transformer to deliver better performance  vs  "  a single policy for all N skills "}
In summary, we deliver the following contributions in this paper:
\begin{enumerate}
        \item We propose an imitation learning framework \ours, a $N+1$-head transformer architecture for a $N-$skill set. 
        \ours is capable of improving an existing model or introducing a new skill without degrading the performance of the current skills.
        \item Introduce the concept of object-centric skill progress, and propose \progss as +1 additional attention head. \progss is a skill selector guided by progress values for imitation learning in long horizon dexterous manipulation.  
        % To efficiently annotate skills and the associated progress values, we also propose a two-round annotation process.
        \item \ours with \progss enables us to customize skill execution based on goal-state indicators, such as images and language prompts, while also achieving strong generalization across different sequences of sub-tasks.
        \item A novel action representation featuring dilated relative suction to control suction activation. This design effectively addresses the ambiguity associated with suction  operations in pick-n-pack scenarios.
    \end{enumerate}

%\textbf{Paper Organization} The rest of the paper is organized as follows. We first provide an overview of the related literature in Sec~\ref{sec:related}. 
%After that, in Sec.~\ref{sec:prob}, we formulate the studied problem of long horizon dexterous manipulation. 
%In Sec.~\ref{sec:method}, we provide technical details of \ours and \progss. 
%Then, in Sec.~\ref{sec:experiments}, we show experimental results of the proposed methods. 
%We conclude in Sec.\ref{sec:conclusion}.