\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/must_overview.pdf}
    \caption{Overview of \ours(Multi-Head Skill Transformer). 
    The model consists of a pre-trained Octo transformer backbone\cite{team2024octo} and $N+1$ heads for an $N-$skill set. 
    Each of the skill head computes an action sequence of its skill.The progress head \progss, the skill selector, estimates the progress of the entire skill set.}
    \label{fig:overview}
\end{figure}
\subsection{Multi-Head Skill Transformer (\ours)}

\ours (Multi-Head Skill Transformer) reduces action complexity by decomposing long-horizon tasks into a set of skills. \ours learns individual skills and then combines them sequentially to complete the overall task.
\ours has $N+1$ action heads, with one action head for each skill in $\mathcal{S}$ and one \emph{progress head} to estimate progress of the skill execution. 
%Noting that the skills are executed by the same robot embodiment in the same environment,
The $N+1$ heads share the same Octo transformer backbone~\cite{team2024octo}, which is pre-trained with 800k robot manipulation episodes.

Fig.~\ref{fig:overview} is \ours architecture overview with three key components: input tokenizers, a transformer backbone, and output decoding heads for N skills, along with a progress head, all extending from the readout tokens.
%\ours first tokenizes multi-modal inputs and convert them into language embedding tokens\cite{raffel2020exploring} and flattened patches\cite{alexey2020image}.
%The input tokens are arranged into a sequence with position embeddings.
%The transformer backbone uses a block-wise masked attention pattern, where the observation tokens causally attend task tokens and observations up until the same timestamp.
%The model outputs a readout token attending task and observation tokens before it in the sequence.
The input tokenizers and the transformer backbone are the same as in Octo, while the readout token serves as a fused representation of the multi-modal observations.
Taking the readout token as input, \ours extends skill heads and progress heads where each head is an L1 action head which consists of a multi-head attention pooling block followed by a dense layer. Each skill head computes an action for the corresponding skill $P_t^{(i)}, \text{ for all } i = 1, 2, \dots, N$, 
 and the progress head learns a numerical vector $\{\rho_t^{(1)}, \rho_t^{(2)}, \dots, \rho_t^{(N)}\}$ to estimate the current progress of each skill. 
%The details of \progss is explained in Sec.~\ref{sec:progss}.

\subsection{\progss: Progress Guided Skill Selector}\label{sec:progss}
%In \ours, we estimate a progress value for each skill at the current state and use \progss, a progress guided skill selector, to determine which skill to execute. 

Given the progress vector from the progress head, we design a progress-guided skill selector, \progss, to determine the optimal skill to execute at the current state. Progress values range from 0 to 1, where lower values indicate pending or ongoing execution, and higher values suggest completion. These values help identify the current phase of execution in long-horizon manipulation.


\subsubsection{Object-Centric Progress Annotation}
%In our framework, when an episode of manipulation execution is decomposed into sub-tasks, 
We define each skill's trajectory segment to consist of three parts: the pre-skill transit trajectory, the skill execution trajectory, and the post-skill transit trajectory.
% In pre-skill and post-skill transit trajectories, the robot approaches the manipulating object before execution and retreats from the object after execution respectively.

% Pre-skill and post-skill trajectories involve approaching and retreating from the object, while 
Skill execution period is defined as the time between the robot's first and last contact with the object. We increase progress value only during this phase, making the progress value object-centric and less sensitive to the robot pose.
% , as it increases only when the object is manipulated, making the progress annotation less sensitive to the robot's pose.

\begin{wrapfigure}{r}{0.25\textwidth}  % 'r' means the figure is on the right
    \centering
    \includegraphics[width=0.25\textwidth]{figures/progress_annotation.pdf}
    \vspace{-7mm}
    \caption{Annotation of skill progress in a skill-related episode segment.}
    \label{fig:annotation}
\end{wrapfigure}

%We define the skill execution to be between the first and last time step when the robot touches the object in the trajectory segment. 
%For annotation in human demonstrations, the progress value only increases during the skill execution, which makes the progress value object-centric.
%In other words, the progress value only increases when the object is manipulated, and such annotation design makes it less sensitive to the robot pose.
Fig.~\ref{fig:annotation} shows the skills progress annotation of a skill-related trajectory segmentation, for pre- and post-skill transit trajectories, we set the progress value to be $\alpha$ and $1$ respectively. $\alpha:=1-t/M$, where $t$ is the duration of skill execution in the current episode and $M$ is the maximum duration of skill execution among the demonstration episodes of the skill.
% Note that in  Fig.~\ref{fig:annotation}, 
The progress value increases linearly with the time steps in demonstration episodes. %which makes it a measure of the needed execution time for this skill. 
% However, the mapping between demonstration episode and the progress values can be customized depending on the desired cost function. 
% For example, it can increase linearly with the traveling distance of the robot end-effector.




\subsubsection{Skill Selection with Single Skill Sequence}
\label{sec:sequential_progss}
%Let \emph{skill sequence} be the sequence of skills in human demonstrations.
When only one skill sequence $\tau=\{s_i\}_{i=1}^{|\tau|}$ of the manipulation task is demonstrated, \progss selects the first skill $s_i$ in the sequence with the current value $\rho^{(i)}_t $ below its termination threshold $\theta_i$. 
Fig.~\ref{fig:exp-skill-selector} is an example of three skills in one sequence $s_1\rightarrow s_2\rightarrow s_3$. 
When \progss with 4 skills infers a vector of $(1, 0.2, 0, 1)$, $s_2$ is selected for execution next for $\theta_i = 0.9, \forall 1\leq i \leq 4$. A side note that $s_4$ is not in the skill sequence, thus its progress value is always $1$.
\vspace{-1mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/exp_sequential_skill_selector.pdf}
    \vspace{-7mm}
    \caption{An example of \progss with a single skill sequence.}
    \label{fig:exp-skill-selector}
\end{figure}
% 
% In the case when a skill $s_i$ has multiple segments in a demonstration, $\alpha$ is determined by the total execution duration of all $s_i$ segments. 
% The progress value $\rho^{(i)}_t$ increases to $1$ after the last skill execution. 
% When selecting skills, we choose the first skill in the sequence whose progress value $\rho^{(i)}_t<min(\theta_i, \alpha^i_j)$, where $\alpha^i_j$ is the progress upper bound of $s_i$ in the current segment.

When a skill \( s_i \) is divided into segments (\( k > 1 \)), the progress value \( \rho^{(i)}_t \) increments in steps as the robot completes each segment. Instead of increasing continuously from \( \alpha \) to 1, the progress moves between stages, each representing a segment of the skill. For each segment \( j \), progress \( \rho^{(i)}_t \) increases from \( \alpha^i_j \) (start of the segment) to \( \alpha^i_{j+1} \) (end of the segment). The upper bound progress for segment \( j \) is calculated as:
\[
\alpha^i_j = \alpha^i_{j-1} + \frac{(1 - \alpha) \cdot T_j^i}{\sum_{j=1}^k T_j^i}
\]
In this formula, \( \alpha^i_0 = \alpha \) is the initial value. \( T_j^i \) is the duration of segment \( j \) of skill \( s_i \), and \( \sum_{j=1}^k T_j^i \) is the total duration of all segments for skill \( s_i \). \( (1 - \alpha) \) represents the available progress range (from \( \alpha \) to 1). Thus, the progress value increases based on the relative duration of each segment.

At time \( t \), \progss selects the first segment \( j \) of skill \( s_i \) where the progress \( \rho^{(i)}_t \) is below both the termination threshold \( \theta_i \) and the upper bound \( \alpha^i_j \), i.e. 
\(
\rho^{(i)}_t < \min(\theta_i, \alpha^i_j)
\).

Here, \( \theta_i \) is the termination threshold for skill \( s_i \), and \( \alpha^i_j \) is the progress upper bound for segment \( j \). This ensures that the robot progresses through the skill in an incremental manner, selecting the next segment that is not yet completed.



\subsubsection{Skill Selection with Multiple Skill Sequences}
When multiple skill sequences $\{\tau_i\}$ are demonstrated for any combination of skills, Alg.~\ref{alg:arbitrary_progss} is used to select next skill.  
We compute the progress trajectories of demonstrated skill sequences, resulting in a map, $M$, in the skill progress space (line 1).   
%Each skill sequence in demonstration forms a trajectory in the progress space, indicating the progress change from $(0,0,...)$ to $(1,1,...)$ in demonstrations.
%\progss first computes the progress trajectories of demonstrated skill sequences, which forms a map $M$ in progress space (Line 1). 
\progss searches for the nearest progress trajectory of $\rho_t$ in $M$ and outputs the corresponding skill sequence $\tau$ (Line 2).
We choose the first skill $s_i\in \tau$ with progress value below its threshold, i.e., $\rho^{(i)}_t<\theta_i$ (Line 3-4).


\begin{algorithm}
\begin{small}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \SetKwComment{Comment}{\% }{}
    \caption{Arbitrary-Sequence-Skill-Selector}
		\label{alg:arbitrary_progss}
    \SetAlgoLined
		\vspace{0.5mm}
    \Input{$T$: Demonstrated skill orderings;\\ 
    $\rho_t$: Estimated progress values;\\
    $\Theta$: Progress termination thresholds.
    }
    \Output{$s$: next skill to execute.}
\vspace{0.5mm}
$M\leftarrow$ Progress-Trajectory-Computation($T$)\\
$\tau\leftarrow$ Nearest-Demonstrations($\rho_t,T,M$).\\
\For{$s_i \in \tau$}{\lIf{$\rho^{(i)}_t<\theta_i$}{\Return $s_i$}}
\end{small}
\end{algorithm}

% \subsubsection{Benefits of Progress Values}
% Comparing with classification-based skill selectors, the skill selector of \ours includes the following benefits:
% First, it provides fine-grind progress during execution. The progress values measures the distance to the completion of sub-tasks and the whole long-horizon task. These values can be used for higher level task planners to evaluate the cost of following a task sequence or serve as the ``critic'' in the reinforcement learning framework. 
% Second, progress values during the transition of skills are constant. 
% In closed-loop control, classification-based skill selectors has a sudden phase transition during the transition of the skills, while the skill progress value stays constant in those periods and is continuous and monotonously increasing throughout the episode. 
% Finally, the decision threshold allows us to customize the size of the goal region during inference. Each skill makes progress to the ``goal region'' of the corresponding sub-task in the observation space conditioned by task descriptions. By adjusting the decision threshold $1-\delta$, we can customize goal region size. For example, for a sub-task of pushing an object to the corner, a low decision threshold allows the robot to stop pushing earlier as the object is close to the corner.



\input{04-method-skill-training}
