In this section, we evaluate the performance of \ours in both simulated and real-robot environments, with a total of six experiments, under the various conditions:
\begin{itemize}
    \item Performance gain of MuST in comparison with the single-head Octo model 
    
   \item  Performance of MuST, conditioned on task goals specified by images or language instructions
    
    \item Performance of MuST across a diverse set of objects
    
    \item Ability for MuST to react to unexpected environment disturbance, based on the skill progress values
\end{itemize}

The performance is measured by two  metrics:
\begin{enumerate}
    \item {\bf Skill completion and task completion:} we report the completion of a single skill, and the task is completed only if all skills in the task have been successful executed. Any failed skill will result in failure of all subsequent skills.
    \item {\bf Execution time:} The execution time is defined as spent time for manipulation until the object consistently stays at the goal pose $g$ for 100 time steps. 
\end{enumerate}

We compare \ours with the single-head Octo model, which learn all skills without progress estimation.
Both models finetune Octo-Base (93M params) checkpoint and use L1 action heads as decoding heads for skills and progress. 
For closed-loop control scenarios, both models carry out inference every 50 time steps and compute the action sequence for the next 50 time steps.
We train the models with an Nvidia V100 16GB GPU.


\subsection{Simulation Experimental Results and Comparison with Octo Baseline  }

Our goal-state conditioned Pick-n-Pack manipulation task Fig.~\ref{fig:intro}[Top] consists of four skills:
%In simulation,  we first test \ours on goal-state conditioned pick-n-pack to evaluate the overall performance of \ours and compare it with Octo.
%Shown in Fig.~\ref{fig:intro}[Top], this manipulation task requires four skills:
First, the robot flips down the object from the tote boundary to enable picking. 
Next, it picks the object from the picking tote.
Based on the goal-state indicator $I_g$ or $l_g$, the robot packs the object near the desired corner of the packing tote.
Finally, the robot pushs the object, both  rotating and translating it, to fit it tightly the desired corner.

%In simulation, we collect x human demonstrations of this task is collected by controlling robot states with keyboard.The models are trained with only 15 human demonstrations for each of the four corners as goal states.

The goal is given by either a language prompt or a goal image Fig.~\ref{fig:end_state_indicator} . 
For the goal images, instead of specific images of objects, we use a blue patch to suggest the goal state which makes it independent of object appearance, enhancing the model's generalization capability across different object types.
%The human demonstrations of this task is collected by controlling robot states with keyboard.
%In experiments, we present the task completion rate for each skill. 
%Test cases fail in earlier skills are labeled as failures in subsequent skills as well.
%To further evaluate the execution efficiency, we also present the average execution time of successful test cases.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/end_state_indicator.pdf}
    \caption{We use either language prompts or images as goal state indicators to customize packing poses.}
    \label{fig:end_state_indicator}
\end{figure}

Tab.\ref{tab:single_language_results} reports the performance metrics for both  \ours  and the Single Head Octo. The task is language conditioned Pick-n-Pack with the ``long box''(Fig.~\ref{fig:object_set}), and each task is repeated 10 times. While Octo model succeeded in  $32.5\%$ tasks, \ours maintains $80\%-90\%$ success rate.
In addition, \ours is $23.7\%-38.4\%$ faster than Octo in execution time for the finished tasks.
The results suggest that \ours is more robust than Octo baseline in long horizon manipulation tasks. 
%The models are trained with only 15 human demonstrations for each of the four corners as goal states.
%In this task, each camera has good visibility in some skills but is occluded in others, which makes it hard to learn a shared encoder for all skills.


% \begin{table*}[ht]
% \centering
% \caption{Language Conditioned Pick-n-Pack (Long Box)}
% \label{tab:single_language_results}
% \begin{tabular}{@{\centering\arraybackslash}m{1.7cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>
% {\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}@{}}
% \toprule
%  & \multicolumn{10}{c}{\textbf{Task Completion}} \\ \cmidrule(lr){2-11} 
%  & \multicolumn{2}{c}{\textbf{Flip}} & \multicolumn{2}{c}{\textbf{Pick}} & \multicolumn{2}{c}{\textbf{Pack}} & \multicolumn{2}{c}{\textbf{Push (Orientation)}} & \multicolumn{2}{c}{\textbf{Push (Position)}} & \multicolumn{2}{c}{\textbf{Execution Time}}\\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
%  \textbf{End State} & \textbf{\ours} & \textbf{Octo} & \textbf{\ours} & \textbf{Octo} & \textbf{\ours} & \textbf{Octo} & \textbf{\ours} & \textbf{Octo}& \textbf{\ours} & \textbf{Octo}& \textbf{\ours} & \textbf{Octo} \\ \midrule
% % \textbf{Task} & \textbf{Method 1 (Success)} & \textbf{Method 1 (Time)} & \textbf{Method 2 (Success)} & \textbf{Method 2 (Time)} & \textbf{Method 3 (Success)} & \textbf{Method 3 (Time)} & \textbf{Method 4 (Success)} & \textbf{Method 4 (Time)} & \textbf{Method 5 (Success)} & \textbf{Method 5 (Time)} \\ \midrule
% Top Left & \multicolumn{1}{|c}{10/10} & 10/10 & \multicolumn{1}{|c}{10/10} & 8/10 & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{10/10} & 4/10 & \multicolumn{1}{|c}{10/10} & 4/10 & \multicolumn{1}{|c}{1324.2} & 1734.3 \\
% Top Right & \multicolumn{1}{|c}{10/10} & 10/10 & \multicolumn{1}{|c}{10/10} & 10/10 & \multicolumn{1}{|c}{9/10} & 9/10 & \multicolumn{1}{|c}{8/10} & 7/10 & \multicolumn{1}{|c}{8/10} & 4/10 & \multicolumn{1}{|c}{1530.0} & 2639.7 \\
% Bottom Left & \multicolumn{1}{|c}{10/10} & 10/10 & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{9/10} & 5/10 & \multicolumn{1}{|c}{9/10} & 3/10 & \multicolumn{1}{|c}{1538.9} & 2337.3\\
% Bottom Right & \multicolumn{1}{|c}{10/10} & 10/10 & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{9/10} & 6/10 & \multicolumn{1}{|c}{9/10} & 4/10 & \multicolumn{1}{|c}{9/10} & 2/10 & \multicolumn{1}{|c}{1571.7} & 2550.5 \\ \bottomrule
% \end{tabular}
% \end{table*}

\scriptsize
\begin{table}[ht]
\centering
\caption{Language Conditioned Pick-n-Pack (Long Box)}
\label{tab:single_language_results}
\begin{tabular}{@{\centering\arraybackslash}m{1.4cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>
{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}>{\centering\arraybackslash}m{0.8cm}@{}}
\toprule
 & \multicolumn{6}{c}{\textbf{Task Completion}} \\ \cmidrule(lr){2-7} 
 & \multicolumn{2}{c}{\scriptsize{\textbf{Flip$\rightarrow$Pack}}} & \multicolumn{2}{c}{\textbf{{\scriptsize Push (Orientation)}}} & \multicolumn{2}{c}{{\scriptsize \textbf{Push \newline (Position)}}} & \multicolumn{2}{c}{{\scriptsize \textbf{Execution \newline Time}}}\\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 \textbf{End State} & \textbf{\ours} & \textbf{Octo} & \textbf{\ours} & \textbf{Octo}& \textbf{\ours} & \textbf{Octo}& \textbf{\ours} & \textbf{Octo} \\ \midrule
% \textbf{Task} & \textbf{Method 1 (Success)} & \textbf{Method 1 (Time)} & \textbf{Method 2 (Success)} & \textbf{Method 2 (Time)} & \textbf{Method 3 (Success)} & \textbf{Method 3 (Time)} & \textbf{Method 4 (Success)} & \textbf{Method 4 (Time)} & \textbf{Method 5 (Success)} & \textbf{Method 5 (Time)} \\ \midrule
{\scriptsize Top Left} & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{10/10} & 4/10 & \multicolumn{1}{|c}{10/10} & 4/10 & \multicolumn{1}{|c}{1324} & 1734 \\
{\scriptsize Top Right} & \multicolumn{1}{|c}{9/10} & 9/10 & \multicolumn{1}{|c}{8/10} & 7/10 & \multicolumn{1}{|c}{8/10} & 4/10 & \multicolumn{1}{|c}{1530} & 2639 \\
{\scriptsize Bottom Left} & \multicolumn{1}{|c}{10/10} & 7/10 & \multicolumn{1}{|c}{9/10} & 5/10 & \multicolumn{1}{|c}{9/10} & 3/10 & \multicolumn{1}{|c}{1538} & 2337\\
{\scriptsize Bottom Right} & \multicolumn{1}{|c}{9/10} & 6/10 & \multicolumn{1}{|c}{9/10} & 4/10 & \multicolumn{1}{|c}{9/10} & 2/10 & \multicolumn{1}{|c}{1571} & 2550 \\ \bottomrule
\end{tabular}
\end{table}
\normalsize


\begin{figure}
    \centering
    \includegraphics[width=0.37\textwidth]{figures/object_set.pdf}
    \caption{Training object set and test object set in simulation. The 3D model used are open-source models sampled from the YCB Object and Model Set~\cite{YCB}, NVIDIA SimReady assets~\cite{nvidia_simready}, open-source models from SketchFab~\cite{sketchfab_open_source}, and the Google Scanned Objects dataset~\cite{2022googlescannedobjectshighquality}.}
    \label{fig:object_set}
\end{figure}

\subsection{Additional Evaluation of \ours and \progss in Simulation}
In this section, we further evaluate the performance of \ours, conditioned with goal images, on a diverse object set, and on progress estimation.

Tab.~\ref{tab:single_image_results} summarizes  the performance of \ours on image-conditioned Pick-n-Pack task on the same Long Box object. \ours maintains $80\%-90\%$ success rate for 40 evaluation trials.



%In Tab.~\ref{tab:four_obj_language_results}, we test \ours on a diverse object set.
Furthermore, we evaluate \ours on a diverse object set. In this experiment, we train \ours on four different boxes(Fig.~\ref{fig:object_set}). The training dataset include 15 demonstrations for each of training objects at each of the four goal poses.
We test \ours on all objects including four training objects and two novel objects, and results are reported in Tab.~\ref{tab:four_obj_language_results} for 20 trials on each object.
For the first three skills, \ours maintains over $90\%$ completion rate on training object set and that drops to $70\%-75\%$ on objects in the new category.
For the last push skill, \ours solves $80\%$ test cases on the cracker box, $65\%-70\%$ on other training objects.
\ours only finishes around $40\%$ pushes on novel objects. 
In the training dataset with cuboid objects, the push demonstrations use box corners for rotation.  Our hypothesis is that the same push behavior does not generalize well to the tested novel objects with irregular shapes.


\begin{table}[H]
\centering
\caption{Image Conditioned Pick-n-Pack (Long Box)}
\label{tab:single_image_results}
\begin{tabular}{@{\centering\arraybackslash}m{1.7cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1cm}@{}}
\toprule
 & \multicolumn{5}{c}{\textbf{Task Completion}} \\ \cmidrule(lr){2-6} 
\textbf{Packing \newline Corner} & \textbf{Flip} & \textbf{Pick} & \textbf{Pack} & \textbf{Push (Orientation)} & \textbf{Push (Position)} & \textbf{Execution Time}\\ \midrule
% \textbf{Task} & \textbf{Method 1 (Success)} & \textbf{Method 1 (Time)} & \textbf{Method 2 (Success)} & \textbf{Method 2 (Time)} & \textbf{Method 3 (Success)} & \textbf{Method 3 (Time)} & \textbf{Method 4 (Success)} & \textbf{Method 4 (Time)} & \textbf{Method 5 (Success)} & \textbf{Method 5 (Time)} \\ \midrule
Top Left & 10/10 & 10/10 & 10/10 & 9/10 & 8/10 & 1570 \\
Top Right & 9/10 & 9/10 & 9/10 & 8/10 & 8/10 & 1765 \\
Bottom Left & 10/10 & 10/10 & 10/10 & 9/10 & 9/10 & 1225 \\
Bottom Right & 10/10 & 10/10 & 10/10 & 8/10 & 9/10 & 1158 \\ \bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\centering
\caption{Language-Conditioned Pick-n-Pack with Diverse Object Set}
\label{tab:four_obj_language_results}
\begin{tabular}{@{\centering\arraybackslash}m{1.7cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1.2cm}>{\centering\arraybackslash}m{1cm}@{}}
\toprule
 & \multicolumn{5}{c}{\textbf{Task Completion}} \\ \cmidrule(lr){2-6} 
\textbf{Test Object} & \textbf{Flip} & \textbf{Pick} & \textbf{Pack} & \textbf{Push (Orientation)} & \textbf{Push (Position)} & \textbf{Execution Time}\\ \midrule
Cracker Box & 20/20 & 20/20 & 19/20 & 16/20 & 16/20 & 2073 \\
Liquid Box & 20/20 & 20/20 & 19/20 & 14/20 & 13/20 & 1769 \\
Long Box & 20/20 & 20/20 & 18/20 & 14/20 & 13/20 & 1330 \\
Oil Tin & 20/20 & 20/20 & 19/20 & 14/20 & 13/20 & 1875 \\ \midrule
Bottle1 (OOD) & 20/20 & 20/20 & 15/20 & 12/20 & 9/20 & 1608\\ 
Bottle2 (OOD) & 20/20 & 20/20 & 14/20 & 9/20 & 7/20 & 2009\\ \bottomrule
\end{tabular}
\end{table}

Additionally, we demonstrate that \ours can react to unexpected environment disturbance based on the skill progress values. As an example, when user resets an object on the tote edge, \ours would select the Flip skill repeatedly. Similarly, \ours would skip the first skill, and start with the second Pick skill when the object is in the pick state. We include these demonstrations with the skill progress value graphs in the accompanying video.

\subsection{Handling Multiple Sequences in Simulation}\label{sec:sim_task2}

We designed the task of multi-sequence pick-n-pack to evaluate the performance of \ours when multiple skill sequences are demonstrated.
As shown in Fig.~\ref{fig:multi_sequence}, the robot is tasked to flip down a box and place it at the packing tote. 
Specifically, when the object is located in the central area of the tote, the robot can choose to flip the object before or after pick-n-place.
However, when the object is located at the boundary of the picking tote, the robot cannot execute pick-n-place before a successful flip.
For each of the three cases of skill sequences in Fig.~\ref{fig:multi_sequence}, we made 50 demonstrations. 
% We test \ours on 80 trials for each of the two initial state categories. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/multi_sequence.pdf}
    \vspace{-3mm}
    \caption{Multi-sequence pick-n-pack. When the object is sampled at the central area of the tote, there are two possible skill orderings; when the object is sampled at the edge, the robot cannot directly pick it up before flipping.}
    \label{fig:multi_sequence}
\end{figure}

Tab.~\ref{tab:multi_sequence_results} shows sequence selection distribution and task completion rate.
When the object is sampled at the central area of the picking tote, out of the 80 trials, \ours chooses ``pick first'' and ``flip first'' sequences in $37.5\%$ and $62.5\%$ trials respectively with around $80\%$ success rate on both skill sequences.
When the object is sampled at the edge of the picking tote, picking directly is impossible. 
\ours chooses the ``flip first'' sequence in $96.2\%$ test cases.
The results suggest that \ours effectively handles multiple skill sequences and avoids ``modality collapses'' in long horizon manipulation tasks.



\begin{table}[H]
\centering
\caption{Multi-Sequence Pick-n-Pack}
\label{tab:multi_sequence_results}
\begin{tabular}{@{\centering\arraybackslash}m{4cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{2cm}@{}}
\toprule
 % & \multicolumn{2}{c}{\textbf{Task Completion}} \\ \cmidrule(lr){2-3} 
\textbf{Test Cases} & \textbf{Pick First} & \textbf{Flip First}\\ \midrule
Manipulate from central area & 24/30 & 39/50\\
Manipulate from edge & 0/3 & 73/77\\ \bottomrule
\end{tabular}
\end{table}




