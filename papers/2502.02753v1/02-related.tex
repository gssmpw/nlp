{\bf Policy Learning} 
Dexterous manipulation is a crucial capability for robots, enabling them to perform complex tasks in the human environment\cite{shah2021rrl,padalkar2023open, ahn2024autort,yang2023moma,zhao2023learning}.
Extensive research has focused on learning dexterous skills through policy learning, both in reinforcement learning and imitation learning, with various input modalities, including vision sensory inputs\cite{andrychowicz2020learning,yang2023moma}, robot states\cite{shah2021rrl,chi2023diffusion,zhao2023learning,team2024octo,mandlekar2021matters,liu2024moka} and language prompts\cite{team2024octo,jiang2022vima}. 
To achieve more flexible goal-oriented tasks, prior works have also explored skill learning conditioned on goal images\cite{team2024octo,jiang2022vima,du2024learning}. 
For example, Du et. al.\cite{du2024learning} generate key image frames to guide robot execution.

Recent advances have involved fine-tuning foundation models pre-trained on large image datasets\cite{shah2021rrl,yang2023moma,chi2023diffusion,xu2023joint} or trajectories from various robot embodiments\cite{padalkar2023open,ahn2024autort,team2024octo}. 
Specifically, Octo model team\cite{team2024octo} pre-trains a transformer-based generalist robot policy with over 800k robot episodes from Open X-Embodiment dataset\cite{open_x_embodiment_rt_x_2023}.
In our work, we explore multiple input modalities, including goal state indicators in natural language and goal images. We fine-tune the Octo transformer to learn a diverse set of skills concurrently.


{\bf Long-horizon manipulation planning} 
Long-horizon manipulation tasks often involve a sequence of sub-tasks, requiring both the selection of the correct motion primitives and the precise execution of each action at every time step. 
% These are the two primary challenges in long-horizon manipulation planning. 
Extensive research has focused on sequencing prehensile actions (e.g., pick and place), addressing the combinatorial challenges\cite{garrett2020pddlstream,gao2023minimizing,wang2021uniform} and the need for efficient state sampling\cite{garrett2020pddlstream,chang2023lgmcts,gao2022fast}. However, when non-prehensile actions (e.g., pushing or poking) are introduced, the increased uncertainty in the resulting states after actions complicates long-horizon planning. 
Researchers have tackled this challenge using Monte Carlo tree search\cite{chang2023lgmcts,vieira2023effective}, probabilistic models\cite{mishra2023generative}, and deep reinforcement learning models\cite{nasiriany2022augmenting,agia2023stap}. 
Additionally, previous works have generated sub-goals in the form of robot poses\cite{liu2024moka, belkhale2023hydra} or future camera observations\cite{du2024learning,mandlekar2020learning} to guide robot execution. 
When a manipulation task requires a heterogeneous skill set, a common approach is to leverage a high-level classifier based skill selector to determine which skill to execute. 
Nasiriany et. al.\cite{nasiriany2022augmenting} train a task policy to determine the motion primative to apply. 
Belkhale et. al. \cite{belkhale2023hydra} train a mode head to choose whether moving directly to a distant waypoint or following a dense action sequence.
However, the one-hot classification result provides limited information of the progress in execution and is ambiguously annotated during the transition of skills.
In this work, we propose \progss, a ``skill progress'' guided skill selector to determine the appropriate skill to execute in long-horizon dexterous manipulation tasks. 
Instead of training a skill selector from scratch independent to the skill set, our skill selector shares the transformer backbone with the skills to enhance efficiency in training and inference.
