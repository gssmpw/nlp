Let the long-horizon task be represented as a sequence of \emph{skills}, denoted by a finite set \( \mathcal{S} = \{s_1, s_2, \dots, s_N\} \), where \( N \) is the total number of skills. The task execution is governed by a policy \( \pi \), which, at each timestep \( t \), takes as input the observation \( o_t \in \mathcal{O} \) and the current robot state \( x_t \in \mathcal{X} \), and outputs the action \( P_t \in \mathcal{A} \), which will be further detailed below for our problem setting.

\subsection{Single Policy Learning}

The observation \( o_t \) is a representation of the environment at time \( t \), which could include sensor readings or environmental context. The robot state \( x_t \) includes the joint configurations, velocities, and other internal variables defining the robot's configuration.

The policy \( \pi \) learns to map:
\[
\pi : (o_t, x_t) \to P_t
\]
where \( P_t \in \mathcal{A} \) is the predicted action. This action consists of two components: the robot’s 6D pose \( p_t = (p_t^x, p_t^y, p_t^z, \theta_t^x, \theta_t^y, \theta_t^z) \in \mathbb{R}^6 \) in Cartesian space and orientation, and a discrete value \( u_t \in \{-1, 0 , 1\} \) representing whether the suction is turned on (\( u_t = 1 \)), turned off (\( u_t = -1 \)), or remains in its current state (\( u_t = 0 \)).

Thus, the action \( P_t \) predicted by the policy can be expressed as:
$P_t = (p_t, u_t)$, 
where \( p_t \) is the 6D pose and \( u_t \) is the suction indicator.

\subsection{\ours: Decomposition of Skills and Progress}

Unlike learning the long-horizon task in one policy, our method, \ours, decomposes the task into skill-specific predictions. For each skill \( s_i \in \mathcal{S} \), we predict not only the robot’s action \( P_t^{(i)} \in \mathcal{A} \), but also a progress value \( \rho_t^{(i)} \in [0, 1] \) that indicates how much of the skill \( s_i \) has been completed at time \( t \). The progress value evolves over time and reaches \( \rho_t^{(i)} = \theta_i \), where \( \theta_i \) is the termination threshold when a skill is considered fully executed.

Thus, for each skill \( s_i \), MuST outputs a tuple:
$(P_t^{(i)}, \rho_t^{(i)})$
at each timestep \( t \), where \( P_t^{(i)} = (p_t^{(i)}, u_t^{(i)}) \) is the predicted action for skill \( s_i \), and \( \rho_t^{(i)} \) is the progress indicator.

\subsection{Skill Selection and Execution (\progss)}

At each timestep \( t \), a skill selector function \( \sigma \) determines the next skill to execute based on the progress values \( \rho_t^{(i)} \) for all skills:
\[
\sigma: \{\rho_t^{(1)}, \rho_t^{(2)}, \dots, \rho_t^{(N)}\} \to s_j
\]
where \( s_j \in \mathcal{S} \) is the selected skill to be executed at time \( t \).

The robot then executes the action \( P_t^{(j)} \) predicted by MuST for the selected skill \( s_j \):
$a_t = P_t^{(j)}$,
where \( a_t \in \mathcal{A} \) represents the robot’s pose and suction status to execute at timestep \( t \).

\subsection{Goal-Conditioned Policy Learning}

In both the single policy learning and MuST approaches, an alternative model variant can be used where the input also encodes a goal condition. This goal condition can be represented as either a goal image \( I_g \in \mathcal{I} \) or a goal language instruction \( l_g \in \mathcal{L} \), where \( \mathcal{I} \) is the set of possible goal images and \( \mathcal{L} \) is the set of possible language instructions.

The policy can then be learned as a mapping that includes the goal condition:
\[
\pi : (o_t, x_t, I_g \text{ or } l_g) \to P_t
\]
where the goal condition, whether in the form of a goal image \( I_g \) or a language instruction \( l_g \), provides additional information to the policy for determining the action \( P_t \).

In this variant, the policy outputs \( P_t = (p_t, u_t) \), as described earlier, but the decision process is now conditioned on the provided goal.


