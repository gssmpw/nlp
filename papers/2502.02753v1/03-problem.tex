\subsection{Problem Formulation: Goal-State Conditioned Manipulation}
%\todo{Jane: I think we need to start with Imitation learning as in Hydra paper, given demo dataset of observation and actions }
\todo{Jane will work on this section - will propose to use notations at three levels per slack message}
Consider a robot manipulating objects in a bounded workspace $\mathcal W$.
Denote the state space of $\mathcal{W}$ by $S$, each state $s\in S$ includes the robot configuration and the poses of all movable objects in $\mathcal W$.
We define our action space as $\mathcal A\subseteq SO(3)\times\{-1,0,1\}$, which includes an end-effector pose in $SO(3)$ and a relative signal of the suction cup with $-1/0/1$ for deactivation/idling/activation respectively. 
For the manipulation task, the robot is given an expert demonstration dataset $D:=\{\tau_i\}_{i=1}^{n}$. 
Each $\tau_i \in D$ is a sequence of state-action pairs $\{(s_j, a_j)\}_{j=1}^{|\tau_i|}$.
Given the termination conditions of the manipulation task, we denote the goal states of the manipulation task as $S_G\subset S$. 
Correspondingly, $D^{S_G} \subseteq D$ is a subset of demonstrations, in which the demonstration sequences terminate in $S_G$, i.e., $\tau[-1]\in S_G \times \mathcal A, \ \forall \tau \in D^{S_G}$.

Inside the workspace, there is skill set $\Sigma:=\{\sigma_i\}_{i=1}^{N}$.
Each skill $\sigma_i:=\{a^i_{j}\}\subset \mathcal A$ is an action set representing a manipulation primitive in $\mathcal W$. 
Let $D^{S_G}_i$ be the demonstration dataset of $\sigma_i$ in $D^{S_G}$.
Each $\tau_i\in D^{S_G}_i$ is a segment of a demonstration in $D^{S_G}$ and all the actions of $\tau_i$ are in $\sigma_i$.

We present the current state $s$ and the goal states $S_G$ as observations $\mathcal O(s)$ and $\mathcal I(S_G)$, where $\mathcal O$ and $\mathcal I$ map $s$ and $S_G$ to multi-modal data, such as RGBD images, proprioceptive data, and/or natural language.
The robot learns a set of policies $\Pi :=\{\pi_i\}_{i=1}^{N}$ for the skill set $\Sigma$.
Each policy $\pi_i:\mathcal O \times \mathcal I \rightarrow \mathcal A$ learns $\sigma_i$ with dataset $D^{S_G}_i$ minimizing an supervised loss 
$$\mathcal L:=\mathbb{E}_{(s,a)\sim p_{D^{S_G}_i}} d(a,\pi_i(\mathcal O(s), \mathcal I(S_G)))$$ 
where $d$ is a distance metric in $\mathcal A$.

During roll-outs of $\pi_i$, at each time step $t$, the next state is obtained by $s_{t+1}=\mathcal T(s_t, a_t)$, where $\mathcal{T}$ is the state transition model in $\mathcal W$ and $a_t=\pi_i(\mathcal O(s_t), \mathcal I(S_G))$ is the computed action of $\pi_i$.

To complete the manipulation task, a skill selector $\Phi:\mathcal{O}\times \mathcal{I} \rightarrow \Pi$ chooses a policy to execute until $S_G$ is reached.

\subsection{Action Space with Dilated Relative Suction}
\todo{Jane: Should we include more than Suction?}
In the action space $\mathcal A$, we use relative representation of suction activity, since it works better in pick and pack scenarios, where the suction cup only activates in the picking tote and deactivates in the packing tote. 
In contrast, the absolute representation has mixed signals in both totes, which results in 
occasional deactivation at the picking tote.
The biggest weakness of the relative representation is the sparsity in each episode\cite{team2024octo}. 
To address this issue, we extend the non-zero entries by applying k-neighborhood dilation in demonstration episodes, which sets the k-neighborhood elements non-zero as well.

