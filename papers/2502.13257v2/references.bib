 @article{Kendall1938, title={A New Measure of Rank Correlation}, volume={30}, ISSN={0006-3444}, DOI={10.2307/2332226}, number={1/2}, journal={Biometrika}, publisher={[Oxford University Press, Biometrika Trust]}, author={Kendall, M. G.}, year={1938}, pages={81–93} }


@article{Narayan2021assessing, title={Assessing single-cell transcriptomic variability through density-preserving data visualization}, volume={39}, rights={2021 The Author(s), under exclusive licence to Springer Nature America, Inc.}, ISSN={1546-1696}, DOI={10.1038/s41587-020-00801-7}, abstractNote={Nonlinear data visualization methods, such as t-distributed stochastic neighbor embedding (t-SNE) and uniform manifold approximation and projection (UMAP), summarize the complex transcriptomic landscape of single cells in two dimensions or three dimensions, but they neglect the local density of data points in the original space, often resulting in misleading visualizations where densely populated subsets of cells are given more visual space than warranted by their transcriptional diversity in the dataset. Here we present den-SNE and densMAP, which are density-preserving visualization tools based on t-SNE and UMAP, respectively, and demonstrate their ability to accurately incorporate information about transcriptomic variability into the visual interpretation of single-cell RNA sequencing data. Applied to recently published datasets, our methods reveal significant changes in transcriptomic variability in a range of biological processes, including heterogeneity in transcriptomic variability of immune cells in blood and tumor, human immune cell specialization and the developmental trajectory of Caenorhabditis elegans. Our methods are readily applicable to visualizing high-dimensional data in other scientific domains.}, number={6}, journal={Nature Biotechnology}, publisher={Nature Publishing Group}, author={Narayan, Ashwin and Berger, Bonnie and Cho, Hyunghoon}, year={2021}, month=jun, pages={765–774}, language={en} }


@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year={1947},
  publisher={JSTOR}
}

@article{Zilionis2019single, title={Single-Cell Transcriptomics of Human and Mouse Lung Cancers Reveals Conserved Myeloid Populations across Individuals and Species}, volume={50}, ISSN={1074-7613}, DOI={10.1016/j.immuni.2019.03.009}, abstractNote={Tumor-infiltrating myeloid cells (TIMs) comprise monocytes, macrophages, dendritic cells, and neutrophils, and have emerged as key regulators of cancer growth. These cells can diversify into a spectrum of states, which might promote or limit tumor outgrowth but remain poorly understood. Here, we used single-cell RNA sequencing (scRNA-seq) to map TIMs in non-small-cell lung cancer patients. We uncovered 25 TIM states, most of which were reproducibly found across patients. To facilitate translational research of these populations, we also profiled TIMs in mice. In comparing TIMs across species, we identified a near-complete congruence of population structures among dendritic cells and monocytes; conserved neutrophil subsets; and species differences among macrophages. By contrast, myeloid cell population structures in patients’ blood showed limited overlap with those of TIMs. This study determines the lung TIM landscape and sets the stage for future investigations into the potential of TIMs as immunotherapy targets.}, number={5}, journal={Immunity}, author={Zilionis, Rapolas and Engblom, Camilla and Pfirschke, Christina and Savova, Virginia and Zemmour, David and Saatcioglu, Hatice D. and Krishnan, Indira and Maroni, Giorgia and Meyerovitz, Claire V. and Kerwin, Clara M. and Choi, Sun and Richards, William G. and De Rienzo, Assunta and Tenen, Daniel G. and Bueno, Raphael and Levantini, Elena and Pittet, Mikael J. and Klein, Allon M.}, year={2019}, month=may, pages={1317-1334.e10} }

@article{gray2022tissue,
  title={Tissue-resident immune cells in humans},
  author={Gray, Joshua I and Farber, Donna L},
  journal={Annual review of immunology},
  volume={40},
  number={1},
  pages={195--220},
  year={2022},
  publisher={Annual Reviews}
}

@article{yoder2011phylogenetic,
  title={The phylogenetic origins of natural killer receptors and recognition: relationships, possibilities, and realities},
  author={Yoder, Jeffrey A and Litman, Gary W},
  journal={Immunogenetics},
  volume={63},
  pages={123--141},
  year={2011},
  publisher={Springer}
}

@article{MendozaQuispe2016extreme, title={Extreme learning machine for out-of-sample extension in Laplacian eigenmaps}, volume={74}, ISSN={0167-8655}, DOI={10.1016/j.patrec.2016.01.024}, abstractNote={Manifold learning techniques have shown a great potential for computer vision problems; however, they do not extend easily to points different from the ones on which they were trained (out-of-sample). On the other hand, extreme learning machine (ELM) is a powerful method that allows to perform nonlinear, multivariate regression. This paper discusses the effectiveness of ELM for the out-of-sample problem and compares it to the state-of-the-art solution : the Nyström extension. Both methods are evaluated through the reconstruction of the manifold learnt using Laplacian eigenmaps, via experiments on a wide range of publicly available image datasets. We show that when reducing the data dimension to its intrinsic dimension, the ELM offers a better approximation of the embedded coordinates, also with reduced computational costs during testing.}, journal={Pattern Recognition Letters}, author={Mendoza Quispe, Arturo and Petitjean, Caroline and Heutte, Laurent}, year={2016}, month=apr, pages={68–73} }


@inproceedings{Rudi2015less, title={Less is More: Nyström Computational Regularization}, volume={28}, url={https://proceedings.neurips.cc/paper/2015/hash/03e0704b5690a2dee1861dc3ad3316c9-Abstract.html}, abstractNote={We study Nyström type subsampling approaches  to large   scale  kernel methods, and  prove   learning bounds in the  statistical learning setting,  where random sampling and high probability estimates are considered.   In particular, we prove that these approaches  can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple  incremental variant of Nyström kernel ridge regression, where the subsampling level  controls at the same time  regularization and computations.  Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo}, year={2015} }


@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrzebski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}

@article{zhang2016understanding,
  author       = {Chiyuan Zhang and
                  Samy Bengio and
                  Moritz Hardt and
                  Benjamin Recht and
                  Oriol Vinyals},
  title        = {Understanding deep learning requires rethinking generalization},
  journal      = {CoRR},
  volume       = {abs/1611.03530},
  year         = {2016},
  url          = {http://arxiv.org/abs/1611.03530},
  eprinttype    = {arXiv},
  eprint       = {1611.03530},
  timestamp    = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ZhangBHRV16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kullback1951information,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@Article{Breiman2001randomforests,
    author={L. Breiman},
    title={Random Forests},
    journal={Mach. Learn.},
    year={2001},
    month={Oct.},
    day={01},
    volume={45},
    number={1},
    pages={5-32},
    abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
    issn={1573-0565},
    doi={10.1023/A:1010933404324},
    url={http://doi.org/10.1023/A:1010933404324}
}

% TODO:
@misc{BreimanRandomForest:Online,
    author = {Breiman, L. and Cutler, A.},
    title = {Random Forests},
    howpublished = {\url{https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prox}},
    note = {(Accessed on 4/10/2023)}
    }

@inproceedings{gerasimiuk2021mural,
  title={MURAL: An Unsupervised Random Forest-Based Embedding for Electronic Health Record Data},
  author={Gerasimiuk, Michal and Shung, Dennis and Tong, Alexander and Stanley, Adrian and Schultz, Michael and Ngu, Jeffrey and Laine, Loren and Wolf, Guy and Krishnaswamy, Smita},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)},
  pages={4694--4704},
  year={2021},
  organization={IEEE}
}

@article{lin2006adaptiveNN,
 ISSN = {01621459},
 abstract = {In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.},
 author = {Yi Lin and Yongho Jeon},
 journal = {J. Am. Stat. Assoc.},
 number = {474},
 pages = {578--590},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Random Forests and Adaptive Nearest Neighbors},
 volume = {101},
 year = {2006},
 url = {https://doi.org/10.1198/016214505000001230}
}

@article{Zhou2010GeneSU,
  title={Gene Selection Using Random Forest and Proximity Differences Criterion on DNA Microarray Data},
  author={Q. Zhou and W. Hong and L. Luo and F. Yang},
  journal={J. Convergence Inf. Technol.},
  year={2010},
  volume={5},
  pages={161-170},
  url = {http://doi.org/10.4156/JCIT.VOL5.ISSUE6.17}
}

@InProceedings{gray2011dementia,
    author="Gray, K. R.
    and Aljabar, P.
    and Heckemann, R. A.
    and Hammers, A.
    and Rueckert, D.",
    editor="Suzuki, Kenji
    and Wang, Fei
    and Shen, D.
    and Yan, P.",
    title="Random Forest-Based Manifold Learning for Classification of Imaging Data in Dementia",
    booktitle="Machine Learning in Medical Imaging",
    year="2011",
    publisher="Springer",
    address="Berlin, Heidelberg",
    url={https://doi.org/10.1007/978-3-642-24319-6_20},
    pages="159--166",
    abstract="Neurodegenerative disorders are characterized by changes in multiple biomarkers, which may provide complementary information for diagnosis and prognosis. We present a framework in which proximities derived from random forests are used to learn a low-dimensional manifold from labelled training data and then to infer the clinical labels of test data mapped to this space. The proposed method facilitates the combination of embeddings from multiple datasets, resulting in the generation of a joint embedding that simultaneously encodes information about all the available features. It is possible to combine different types of data without additional processing, and we demonstrate this key feature by application to voxel-based FDG-PET and region-based MR imaging data from the ADNI study. Classification based on the joint embedding coordinates out-performs classification based on either modality alone. Results are impressive compared with other state-of-the-art machine learning techniques applied to multi-modality imaging data.",
    isbn="978-3-642-24319-6"
}

% TODO:
@inproceedings{seoane2014geneannotation,
  title = {A Random Forest proximity matrix as a new measure for gene annotation},
  author = {José A. Seoane and Ian N. M. Day and Juan P. Casas and Colin Campbell and Tom R. Gaunt},
  year = {2014},
  researchr = {https://researchr.org/publication/SeoaneDCCG14},
  booktitle = {22th European Symposium on Artificial Neural Networks, ESANN 2014, Bruges, Belgium, April 23-25, 2014},
}


﻿@Article{Zhao2016propensity,
    author={Zhao, P.
    and Su, X.
    and Ge, T.
    and Fan, J.},
    title={Propensity score and proximity matching using random forest},
    journal={Contemp. Clin. Trials},
    year={2016},
    month={Mar},
    edition={2015/12/17},
    volume={47},
    pages={85-92},
    keywords={Matching; Observational study; Propensity score; Proximity; Random forest; Adult; Aged; Body Mass Index; Case-Control Studies; Data Interpretation, Statistical; Databases, Factual; Female; Humans; Male; Middle Aged; Nutrition Surveys; Obesity/*epidemiology; *Propensity Score; Smoking/*epidemiology; Statistics as Topic; United States/epidemiology},
    abstract={In order to derive unbiased inference from observational data, matching methods are often applied to produce balanced treatment and control groups in terms of all background variables. Propensity score has been a key component in this research area. However, propensity score based matching methods in the literature have several limitations, such as model mis-specifications, categorical variables with more than two levels, difficulties in handling missing data, and nonlinear relationships. Random forest, averaging outcomes from many decision trees, is nonparametric in nature, straightforward to use, and capable of solving these issues. More importantly, the precision afforded by random forest (Caruana et al., 2008) may provide us with a more accurate and less model dependent estimate of the propensity score. In addition, the proximity matrix, a by-product of the random forest, may naturally serve as a distance measure between observations that can be used in matching. The proposed random forest based matching methods are applied to data from the National Health and Nutrition Examination Survey (NHANES). Our results show that the proposed methods can produce well balanced treatment and control groups. An illustration is also provided that the methods can effectively deal with missing data in covariates.},
    issn={1559-2030},
    doi={10.1016/j.cct.2015.12.012},
    language={eng},
    url = {http://doi.org/10.1016/j.cct.2015.12.012}
}

@misc{whitmore2018explicating,
      title={Explicating feature contribution using Random Forest proximity distances}, 
      author={L. S. Whitmore and A. George and C. M. Hudson},
      year={2018},
      eprint={1807.06572},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url = {https://arxiv.org/abs/1807.06572}
}

@article{ishwaran2007vimp,
    author = "Ishwaran, H.",
    doi = "10.1214/07-EJS039",
    fjournal = "Electronic Journal of Statistics",
    journal = "Electron. J. Stat.",
    pages = "519--537",
    publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
    title = "Variable importance in binary regression trees and forests",
    volume = "1",
    year = "2007"
}

﻿@Article{Strobl2008conditionalVIMP,
    author={Strobl, C.
    and Boulesteix, A.
    and Kneib, T.
    and Augustin, T.
    and Zeileis, A.},
    title={Conditional variable importance for random forests},
    journal={BMC Bioinf.},
    year={2008},
    month={Jul},
    day={11},
    volume={9},
    number={1},
    pages={307},
    abstract={Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.},
    issn={1471-2105},
    doi={10.1186/1471-2105-9-307}
}

﻿@Article{Nicodemus2010permImpBehaviour,
    author={Nicodemus, K. K.
    and Malley, J. D.
    and Strobl, C.
    and Ziegler, A.},
    title={The behaviour of random forest permutation-based variable importance measures under predictor correlation},
    journal={BMC Bioinf.},
    year={2010},
    month={Feb},
    day={27},
    volume={11},
    number={1},
    pages={110},
    abstract={Random forests (RF) have been increasingly used in applications such as genome-wide association and microarray studies where predictor correlation is frequently observed. Recent works on permutation-based variable importance measures (VIMs) used in RF have come to apparently contradictory conclusions. We present an extended simulation study to synthesize results.},
    issn={1471-2105},
    doi={10.1186/1471-2105-11-110}
}

@article{Touw2013jungle,
  title={Data mining in the Life Sciences with Random Forest: a walk in the park or lost in the jungle?},
  author={W. G. Touw and others},
  journal={Briefings Bioinf.},
  year={2013},
  volume={14},
  pages={315 - 326}
}

@Article{Shi2005tumor,
    author={Shi, T.
    and Seligson, D.
    and Belldegrun, A. S.
    and Palotie, A.
    and Horvath, S.},
    title={Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma},
    journal={Mod. Pathol.},
    year={2005},
    month={Apr},
    day={01},
    volume={18},
    number={4},
    pages={547-557},
    abstract={We describe a novel strategy (random forest clustering) for tumor profiling based on tissue microarray data. Random forest clustering is attractive for tissue microarray and other immunohistochemistry data since it handles highly skewed tumor marker expressions well and weighs the contribution of each marker according to its relatedness with other tumor markers. This is the first tumor class discovery analysis of renal cell carcinoma patients based on protein expression profiles. The tissue array data contained at least three tumor samples from each of 366 renal cell carcinoma patients. The eight tumor markers explore tumor proliferation, cell cycle abnormalities, cell mobility, and the hypoxia pathway. Since the procedure is unsupervised, no clinicopathological data or traditional classifications are used a priori. To explore whether the tissue microarray data can be used to identify fundamental subtypes of renal cell carcinoma patients, we first carried out random forest clustering of all 366 patients. By analyzing the tumor markers simultaneously, the procedure automatically detected classes that correspond to clear- vs non-clear cell tumors (demonstration of proof-of-principle). The resulting molecular grouping provides better prediction of survival (logrank P=0.000090) than this classical pathological grouping (logrank P=0.023). We then sought to extend the class discovery by searching for finer subclasses of clear cell patients. The procedure automatically discovered: (a) two classes corresponding to low- and high-grade patients (demonstration of proof-of-principle); (b) a subgroup of long-surviving clear cell patients with a distinct molecular profile and (c) two novel tumor subclasses in low-grade clear cell patients that could not be explained by any clinicopathological variables (demonstration of discovery).},
    issn={1530-0285},
    doi={10.1038/modpathol.3800322},
    url={https://doi.org/10.1038/modpathol.3800322}
}

@article{horvath2006unsupervised,
    author = {T. Shi and S. Horvath},
    title = {Unsupervised Learning With Random Forest Predictors},
    journal = {J. Comput. Graphical Stat.},
    volume = {15},
    number = {1},
    pages = {118-138},
    year  = {2006},
    publisher = {Taylor & Francis},
    doi = {10.1198/106186006X94072},
    
    eprint = { 
            https://doi.org/10.1198/106186006X94072
        
    },
    url = {https://doi.org/10.1198/106186006X94072}
}

@article{finehout2007cerebrospinal,
	title = {Cerebrospinal fluid proteomic biomarkers for {Alzheimer}'s disease},
	volume = {61},
	issn = {03645134, 15318249},
	doi = {10.1002/ana.21038},
	language = {en},
	number = {2},
	urldate = {2021-01-15},
	journal = {Ann. Neurol.},
	author = {Finehout, E. J. and Franck, Z. and Choe, L. H. and Relkin, N. and Lee, K. H.},
	month = feb,
	year = {2007},
	pages = {120--129},
    url = {https://doi.org/10.1002/ana.21038}
}

@article{pang2006pathways,
    author = {Pang, H. and Lin, A. and Holford, M. and Enerson, B. E. and others},
    title = "{Pathway analysis using random forests classification and regression}",
    journal = {Bioinformatics},
    volume = {22},
    number = {16},
    pages = {2028-2036},
    year = {2006},
    month = {06},
    abstract = "{Motivation: Although numerous methods have been developed to better capture biological information from microarray data, commonly used single gene-based methods neglect interactions among genes and leave room for other novel approaches. For example, most classification and regression methods for microarray data are based on the whole set of genes and have not made use of pathway information. Pathway-based analysis in microarray studies may lead to more informative and relevant knowledge for biological researchers.Results: In this paper, we describe a pathway-based classification and regression method using Random Forests to analyze gene expression data. The proposed methods allow researchers to rank important pathways from externally available databases, discover important genes, find pathway-based outlying cases and make full use of a continuous outcome variable in the regression setting. We also compared Random Forests with other machine learning methods using several datasets and found that Random Forests classification error rates were either the lowest or the second-lowest. By combining pathway information and novel statistical methods, this procedure represents a promising computational strategy in dissecting pathways and can provide biological insight into the study of microarray data.Availability: Source code written in R is available from Contact:hongyu.zhao@yale.eduSupplementary Information: Supplementary Data are available at }",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btl344},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/22/16/2028/548216/btl344.pdf},
    url = {http://doi.org/10.1093/bioinformatics/btl344}
}


@incollection{Cutler2012randomforests,
author="Cutler, Adele
and Cutler, D. Richard
and Stevens, John R.",
editor="Zhang, Cha
and Ma, Yunqian",
title="Random Forests",
bookTitle="Ensemble Machine Learning: Methods and Applications",
year="2012",
publisher="Springer US",
address="Boston, MA",
pages="157--175",
abstract="Random Forests were introduced by Leo Breiman [6] who was inspired by earlier work by Amit and Geman [2]. Although not obvious from the description in [6], Random Forests are an extension of Breiman's bagging idea [5] and were developed as a competitor to boosting. Random Forests can be used for either a categorical response variable, referred to in [6] as ``classification,'' or a continuous response, referred to as ``regression.'' Similarly, the predictor variables can be either categorical or continuous.",
isbn="978-1-4419-9326-7",
doi="10.1007/978-1-4419-9326-7_5",
url="https://doi.org/10.1007/978-1-4419-9326-7_5"
}




@Manual{R2021,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019}
}

@Article{randomForest,
    title = {Classification and Regression by random{F}orest},
    author = {A. Liaw and M. Wiener},
    journal = {R News},
    year = {2002},
    volume = {2},
    number = {3},
    pages = {18-22}
}



@article{ishwaran2007vimp,
    author = "Ishwaran, H.",
    doi = "10.1214/07-EJS039",
    fjournal = "Electronic Journal of Statistics",
    journal = "Electron. J. Stat.",
    pages = "519--537",
    publisher = "The Institute of Mathematical Statistics and the Bernoulli Society",
    title = "Variable importance in binary regression trees and forests",
    volume = "1",
    year = "2007"
}

﻿@Article{Strobl2008conditionalVIMP,
    author={Strobl, C.
    and Boulesteix, A.
    and Kneib, T.
    and Augustin, T.
    and Zeileis, A.},
    title={Conditional variable importance for random forests},
    journal={BMC Bioinf.},
    year={2008},
    month={Jul},
    day={11},
    volume={9},
    number={1},
    pages={307},
    abstract={Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.},
    issn={1471-2105},
    doi={10.1186/1471-2105-9-307}
}

﻿@Article{Nicodemus2010permImpBehaviour,
    author={Nicodemus, K. K.
    and Malley, J. D.
    and Strobl, C.
    and Ziegler, A.},
    title={The behaviour of random forest permutation-based variable importance measures under predictor correlation},
    journal={BMC Bioinf.},
    year={2010},
    month={Feb},
    day={27},
    volume={11},
    pages={110},
    abstract={Random forests (RF) have been increasingly used in applications such as genome-wide association and microarray studies where predictor correlation is frequently observed. Recent works on permutation-based variable importance measures (VIMs) used in RF have come to apparently contradictory conclusions. We present an extended simulation study to synthesize results.},
    issn={1471-2105},
    doi={10.1186/1471-2105-11-110}
}

@article{pouyan2016distancelearning,
  title={Metric learning using random forest for cytometry data},
  author={Maziyar Baran Pouyan and Javad Birjandtalab and Mehrdad Nourani},
  journal={2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year={2016},
  pages={2590-2590},
  url = {http://doi.org/10.1109/EMBC.2016.7591260}
}
  
@ARTICLE{zhang2008Network,
  author={J. {Zhang} and M. {Zulkernine} and A. {Haque}},
  journal={IEEE Trans. Syst. Man Cybern., Part C (Applications and Reviews)}, 
  title={Random-Forests-Based Network Intrusion Detection Systems}, 
  year={2008},
  volume={38},
  number={5},
  pages={649-659},
  doi={10.1109/TSMCC.2008.923876}}

  
@incollection{hastie2017elements,
author="Hastie, Trevor
and Tibshirani, Robert
and Friedman, Jerome",
title="Random Forests",
bookTitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
year="2009",
publisher="Springer New York",
address="New York, NY",
pages="587--604",
isbn="978-0-387-84858-7",
doi="10.1007/978-0-387-84858-7_15",
url="https://doi.org/10.1007/978-0-387-84858-7_15",
edition={2}
}
  
@article{gislason2006landcover,
    title = "Random Forests for land cover classification",
    journal = "Pattern Recognit. Lett.",
    volume = "27",
    number = "4",
    pages = "294 - 300",
    year = "2006",
    note = "Pattern Recognition in Remote Sensing (PRRS 2004)",
    issn = "0167-8655",
    doi = "https://doi.org/10.1016/j.patrec.2005.08.011",
    author = "P. O. Gislason and J. A. Benediktsson and J. R. Sveinsson",
    keywords = "Random Forests, Classification, Decision trees, Multisource remote sensing data",
    abstract = "Random Forests are considered for classification of multisource remote sensing and geographic data. Various ensemble classification methods have been proposed in recent years. These methods have been proven to improve classification accuracy considerably. The most widely used ensemble methods are boosting and bagging. Boosting is based on sample re-weighting but bagging uses bootstrapping. The Random Forest classifier uses bagging, or bootstrap aggregating, to form an ensemble of classification and regression tree (CART)-like classifiers. In addition, it searches only a random subset of the variables for a split at each CART node, in order to minimize the correlation between the classifiers in the ensemble. This method is not sensitive to noise or overtraining, as the resampling is not based on weighting. Furthermore, it is computationally much lighter than methods based on boosting and somewhat lighter than simple bagging. In the paper, the use of the Random Forest classifier for land cover classification is explored. We compare the accuracy of the Random Forest classifier to other better-known ensemble methods on multisource remote sensing and geographic data."
    }
    
@article{Kokla2019metabolomicsimputation,
    author={Kokla, M.
    and Virtanen, J.
    and Kolehmainen, M.
    and Paananen, J.
    and Hanhineva, K.},
    title={Random forest-based imputation outperforms other methods for imputing {LC}-{MS} metabolomics data: a comparative study},
    journal={BMC Bioinf.},
    year={2019},
    month={Oct},
    day={11},
    volume={20},
    number={1},
    pages={492},
    abstract={LC-MS technology makes it possible to measure the relative abundance of numerous molecular features of a sample in single analysis. However, especially non-targeted metabolite profiling approaches generate vast arrays of data that are prone to aberrations such as missing values. No matter the reason for the missing values in the data, coherent and complete data matrix is always a pre-requisite for accurate and reliable statistical analysis. Therefore, there is a need for proper imputation strategies that account for the missingness and reduce the bias in the statistical analysis.},
    issn={1471-2105},
    doi={10.1186/s12859-019-3110-0},
    url = {http://doi.org/10.1186/s12859-019-3110-0}
}

@article{Hong2020imputationaccuracy,
    author={Hong, S.
    and Lynn, H. S.},
    title={Accuracy of random-forest-based imputation of missing data in the presence of non-normality, non-linearity, and interaction},
    journal={BMC Med. Res. Method.},
    year={2020},
    month={Jul},
    day={25},
    volume={20},
    number={1},
    pages={199},
    abstract={Missing data are common in statistical analyses, and imputation methods based on random forests (RF) are becoming popular for handling missing data especially in biomedical research. Unlike standard imputation approaches, RF-based imputation methods do not assume normality or require specification of parametric models. However, it is still inconclusive how they perform for non-normally distributed data or when there are non-linear relationships or interactions.},
    issn={1471-2288},
    doi={10.1186/s12874-020-01080-1},
    url = {http://doi.org/10.1186/s12874-020-01080-1}
}

@article{englund2012novelaproach,
    author = {Englund, C. and Verikas, A.},
    title = {A Novel Approach to Estimate Proximity in a Random Forest: An Exploratory Study},
    year = {2012},
    issue_date = {December, 2012},
    publisher = {Pergamon Press, Inc.},
    address = {USA},
    volume = {39},
    number = {17},
    issn = {0957-4174},
    doi = {10.1016/j.eswa.2012.05.094},
    url = {http://doi.org/10.1016/j.eswa.2012.05.094},
    abstract = {A data proximity matrix is an important information source in random forests (RF) based data mining, including data clustering, visualization, outlier detection, substitution of missing values, and finding mislabeled data samples. A novel approach to estimate proximity is proposed in this work. The approach is based on measuring distance between two terminal nodes in a decision tree. To assess the consistency (quality) of data proximity estimate, we suggest using the proximity matrix as a kernel matrix in a support vector machine (SVM), under the assumption that a matrix of higher quality leads to higher classification accuracy. It is experimentally shown that the proposed approach improves the proximity estimate, especially when RF is made of a small number of trees. It is also demonstrated that, for some tasks, an SVM exploiting the suggested proximity matrix based kernel, outperforms an SVM based on a standard radial basis function kernel and the standard proximity matrix based kernel.},
    journal = {Expert Syst. Appl.},
    month = dec,
    pages = {13046–13050},
    numpages = {5},
    keywords = {Kernel matrix, Proximity matrix, Random forest, Data mining, Support vector machine}
}

@misc{davies2014randomforestkernel,
      title={The Random Forest Kernel and other kernels for big data from random partitions}, 
      author={A. Davies and Z. Ghahramani},
      year={2014},
      eprint={1402.4293},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url = {https://arxiv.org/abs/1402.4293}
}

@article{cao2019multi-viewradiomics,
    title = "Random forest dissimilarity based multi-view learning for Radiomics application",
    journal = "Pattern Recognit.",
    volume = "88",
    pages = "185 - 197",
    year = "2019",
    issn = "0031-3203",
    doi = "https://doi.org/10.1016/j.patcog.2018.11.011",
    author = "H. Cao and S. Bernard and R. Sabourin and L. Heutte",
    keywords = "Radiomics, Dissimilarity space, Random forest, Machine learning, Feature selection, Multi-view learning, High dimension, Low sample size",
    abstract = "Radiomics is a medical imaging technique that aims at extracting a large amount of features from one or several modalities of medical images, in order to help diagnose and treat diseases like cancers. Many recent studies have shown that Radiomics features can offer a lot of useful information that physicians cannot extract from these images, and can be efficiently associated with other information like gene or protein data. However, most of the classification studies in Radiomics report the use of feature selection methods without identifying the underlying machine learning challenges. In this paper, we first show that the Radiomics classification problem should be viewed as a high dimensional, low sample size, multi-view learning problem. Then, we propose a dissimilarity-based method for merging the information from the different views, based on Random Forest classifiers. The proposed approach is compared to different state-of-the-art Radiomics and multi-view solutions, on different public multi-view datasets as well as on Radiomics datasets. In particular, our experiments show that the proposed approach works better than the state-of-the-art methods from the Radiomics, as well as from the multi-view learning literature.",
    url={https://doi.org/10.1016/j.patcog.2018.11.011}
}


@article{Cao2020novelRFMV,
  title={A Novel Random Forest Dissimilarity Measure for Multi-View Learning},
  author={Hongliu Cao and Simon Bernard and Robert Sabourin and Laurent Heutte},
  journal={2020 25th Int. Conf. Pattern Recognit. (ICPR)},
  year={2021},
  pages={1344-1351},
  url = {http://doi.org/DOI:10.1109/ICPR48806.2021.9412961}
}

@misc{UCI2019,
author = "Dua, D. and Graff, C.",
year = "2017",
title = "{UCI} Machine Learning Repository",
institution = "University of California, Irvine, School of Information and Computer Sciences",
url = {http://archive.ics.uci.edu/ml},
note={(Accessed on 05/21/2020)}
}

@article{benali2019solar,
title = {Solar radiation forecasting using artificial neural network and random forest methods: Application to normal beam, horizontal diffuse and global components},
journal = {Renewable Energy},
volume = {132},
pages = {871-884},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.08.044},
author = {L. Benali and G. Notton and A. Fouilloy and C. Voyant and R. Dizene},
keywords = {Solar irradiation forecasting, ANN, Random forest, Beam solar radiation, Diffuse solar radiation, Global solar radiation},
abstract = {Three methods, smart persistence, artificial neural network and random forest, are compared to forecast the three components of solar irradiation (global horizontal, beam normal and diffuse horizontal) measured on the site of Odeillo, France, characterized by a high meteorological variability. The objective is to predict hourly solar irradiations for time horizons from h+1 to h+6. The random forest (RF) method is the most efficient and forecasts the three components with a nRMSE from 19.65\% for h+1–27.78\% for h+6 for the global horizontal irradiation (GHI), a nRMSE from 34.11\% for h+1–49.08\% for h+6 for the beam normal irradiation (BNI); a nRMSE from 35.08\% for h+1–49.14\% for h+6 for diffuse horizontal irradiation (DHI). The improvement brought by the use of RF compared to Artificial Neural Network (ANN) and smart persistence (SP) increases with the forecasting horizon. A seasonal study is realized and shows that the forecasting of solar irradiation during spring and autumn is less reliable than during winter and summer because during these periods the meteorological variability is more important.},
url={doi = {https://doi.org/10.1016/j.renene.2018.08.044}
}}

@article{Han2018ComparisonOR,
  title={Comparison of random forest, artificial neural networks and support vector machine for intelligent diagnosis of rotating machinery},
  author={T. Han and D. Jiang and Q. Zhao and L. Wang and Kai Yin},
  journal={Trans. Inst. Meas. Control},
  year={2018},
  volume={40},
  pages={2681 - 2693},
  url={https://doi.org/10.1177/0142331217708242}
}

@ARTICLE{iwendi2020covid,
AUTHOR={Iwendi, Celestine and Bashir, Ali Kashif and Peshkar, Atharva and Sujatha, R. and Chatterjee, Jyotir Moy and Pasupuleti, Swetha and Mishra, Rishita and Pillai, Sofia and Jo, Ohyun},   
TITLE={{COVID}-19 Patient Health Prediction Using Boosted Random Forest Algorithm},      
JOURNAL={Front. Public Health},      
VOLUME={8},      
PAGES={357},     
YEAR={2020},      
DOI={10.3389/fpubh.2020.00357},      
ISSN={2296-2565},   
ABSTRACT={Integration of artificial intelligence (AI) techniques in wireless infrastructure, real-time collection, and processing of end-user devices is now in high demand. It is now superlative to use AI to detect and predict pandemics of a colossal nature. The Coronavirus disease 2019 (COVID-19) pandemic, which originated in Wuhan China, has had disastrous effects on the global community and has overburdened advanced healthcare systems throughout the world. Globally; over 4,063,525 confirmed cases and 282,244 deaths have been recorded as of 11th May 2020, according to the European Centre for Disease Prevention and Control agency. However, the current rapid and exponential rise in the number of patients has necessitated efficient and quick prediction of the possible outcome of an infected patient for appropriate treatment using AI techniques. This paper proposes a fine-tuned Random Forest model boosted by the AdaBoost algorithm. The model uses the COVID-19 patient's geographical, travel, health, and demographic data to predict the severity of the case and the possible outcome, recovery, or death. The model has an accuracy of 94\% and a F1 Score of 0.86 on the dataset used. The data analysis reveals a positive correlation between patients' gender and deaths, and also indicates that the majority of patients are aged between 20 and 70 years.},
url={https://doi.org/10.3389/fpubh.2020.00357}
}

@article{YESILKANAT2020covidSpac,
title = {Spatio-temporal estimation of the daily cases of {COVID}-19 in worldwide using random forest machine learning algorithm},
journal = {Chaos, Solitons Fractals},
volume = {140},
pages = {110210},
year = {2020},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2020.110210},
url = {https://doi.org/10.1016/j.chaos.2020.110210},
author = {C. M. Yeşilkanat},
keywords = {COVID-19, Random forest, Machine learning, Estimating, Mapping},
abstract = {Novel Coronavirus pandemic, which negatively affected public health in social, psychological and economical terms, spread to the whole world in a short period of 6 months. However, the rate of increase in cases was not equal for every country. The measures implemented by the countries changed the daily spreading speed of the disease. This was determined by changes in the number of daily cases. In this study, the performance of the Random Forest (RF) machine learning algorithm was investigated in estimating the near future case numbers for 190 countries in the world and it is mapped in comparison with actual confirmed cases results. The number of confirmed cases between 23/01/2020 - 17/06/2020 were divided into 3 main sub-datasets: training sub-data, testing sub-data (interpolation data) and estimating sub-data (extrapolation data) for the random forest model. At the end of the study, it has been found that R2 values for testing sub-data of RF model estimates range between 0.843 and 0.995 (average R2= 0.959), and RMSE values between 141.76 and 526.18 (mean RMSE = 259.38); and that R2 values for estimating sub-data range between 0.690 and 0.968 (mean R2 = 0.914), and RMSE values between 549.73 and 2500.79 (mean RMSE = 909.37). These results show that the random forest machine learning algorithm performs well in estimating the number of cases for the near future in case of an epidemic like Novel Coronavirus, which outbreaks suddenly and spreads rapidly.}
}


@Article{nhu2020landslide,
AUTHOR = {Nhu, V. and Shirzadi, A. and Shahabi, H. and Chen, W. and Clague, J. J. and Geertsema and others},
TITLE = {Shallow Landslide Susceptibility Mapping by Random Forest Base Classifier and Its Ensembles in a Semi-Arid Region of {I}ran},
JOURNAL = {Forests},
VOLUME = {11},
YEAR = {2020},
NUMBER = {4},
ARTICLE-NUMBER = {421},
ISSN = {1999-4907},
ABSTRACT = {We generated high-quality shallow landslide susceptibility maps for Bijar County, Kurdistan Province, Iran, using Random Forest (RAF), an ensemble computational intelligence method and three meta classifiers&mdash;Bagging (BA, BA-RAF), Random Subspace (RS, RS-RAF), and Rotation Forest (RF, RF-RAF). Modeling and validation were done on 111 shallow landslide locations using 20 conditioning factors tested by the Information Gain Ratio (IGR) technique. We assessed model performance with statistically based indexes, including sensitivity, specificity, accuracy, kappa, root mean square error (RMSE), and area under the receiver operatic characteristic curve (AUC). All four machine learning models that we tested yielded excellent goodness-of-fit and prediction accuracy, but the RF-RAF ensemble model (AUC = 0.936) outperformed the BA-RAF, RS-RAF (AUC = 0.907), and RAF (AUC = 0.812) models. The results also show that the Random Forest model significantly improved the predictive capability of the RAF-based classifier and, therefore, can be considered as a useful and an effective tool in regional shallow landslide susceptibility mapping.},
DOI = {10.3390/f11040421},
url = {https://doi.org/10.3390/f11040421}
}

﻿@Article{Fang2020diar,
author={Fang, X.
and Liu, We.
and Ai, J.
and He, M.
and Wu, Y.},
title={Forecasting incidence of infectious diarrhea using random forest in {J}iangsu Province, {C}hina},
journal={BMC Infect. Dis.},
year={2020},
month={Mar},
day={14},
volume={20},
number={1},
pages={222},
abstract={Infectious diarrhea can lead to a considerable global disease burden. Thus, the accurate prediction of an infectious diarrhea epidemic is crucial for public health authorities. This study was aimed at developing an optimal random forest (RF) model, considering meteorological factors used to predict an incidence of infectious diarrhea in Jiangsu Province, China.},
issn={1471-2334},
doi={10.1186/s12879-020-4930-2},
url = {https://doi.org/10.1186/s12879-020-4930-2}
}

﻿@Article{Yang2020cardio,
author={Yang, L.
and Wu, H.
and Jin, X.
and Zheng, P.
and Hu, S.
and Xu, X.
and Yu, W.
and Yan, J.},
title={Study of cardiovascular disease prediction model based on random forest in eastern {C}hina},
journal={Sci. Rep.},
year={2020},
month={Mar},
day={23},
volume={10},
number={1},
pages={5245},
abstract={Cardiovascular disease (CVD) is the leading cause of death worldwide and a major public health concern. CVD prediction is one of the most effective measures for CVD control. In this study, 29930 subjects with high-risk of CVD were selected from 101056 people in 2014, regular follow-up was conducted using electronic health record system. Logistic regression analysis showed that nearly 30 indicators were related to CVD, including male, old age, family income, smoking, drinking, obesity, excessive waist circumference, abnormal cholesterol, abnormal low-density lipoprotein, abnormal fasting blood glucose and else. Several methods were used to build prediction model including multivariate regression model, classification and regression tree (CART), Na{\"i}ve Bayes, Bagged trees, Ada Boost and Random Forest. We used the multivariate regression model as a benchmark for performance evaluation (Area under the curve, AUC{\thinspace}={\thinspace}0.7143). The results showed that the Random Forest was superior to other methods with an AUC of 0.787 and achieved a significant improvement over the benchmark. We provided a CVD prediction model for 3-year risk assessment of CVD. It was based on a large population with high risk of CVD in eastern China using Random Forest algorithm, which would provide reference for the work of CVD prediction and treatment in China.},
issn={2045-2322},
doi={10.1038/s41598-020-62133-5},
url={https://doi.org/10.1038/s41598-020-62133-5}
}


@article{saha2020deforest,
title = {Predicting the deforestation probability using the binary logistic regression, random forest, ensemble rotational forest, {REPT}ree: A case study at the {G}umani {R}iver {B}asin, {I}ndia},
journal = {Sci. Total Environ.},
volume = {730},
pages = {139197},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.139197},
url = {https://doi.org/10.1016/j.scitotenv.2020.139197},
author = {S. Saha and M. Saha and K. Mukherjee and A. Arabameri and P. T. T. Ngo and G. C. Paul},
keywords = {Forest canopy density, Deforestation, Machine learning algorithms, Probabilistic model, Ensemble model},
abstract = {Rapid population growth and its corresponding effects like the expansion of human settlement, increasing agricultural land, and industry lead to the loss of forest area in most parts of the world especially in such highly populated nations like India. Forest canopy density (FCD) is a useful measure to assess the forest cover change in its own as numerous works of forest change have been done using only FCD with the help of remote sensing and GIS. The coupling of binary logistic regression (BLR), random forest (RF), ensemble of rotational forest and reduced error pruning trees (RTF-REPTree) with FCD makes it more convenient to find out the deforestation probability. Advanced vegetation index (AVI), bare soil index (BSI), shadow index (SI), and scaled vegetation density (VD) derived from Landsat imageries are the main input parameters to identify the FCD. After preparing the FCDs of 1990, 2000, 2010 and 2017 the deforestation map of the study area was prepared and considered as dependent parameter for deforestation probability modelling. On the other hand, twelve deforestation determining factors were used to delineate the deforestation probability with the help of BLR, RF and RTF-REPTree models. These deforestation probability models were validated through area under curve (AUC), receiver operating characteristics (ROC), efficiency, true skill statistics (TSS) and Kappa co-efficient. The validation result shows that all the models like BLR (AUC = 0.874), RF (AUC = 0.886) and RTF-REPTree (AUC = 0.919) have good capability of assessing the deforestation probability but among them, RTF-REPTree has the highest accuracy level. The result also shows that low canopy density area i.e. not under the dense forest cover has increased by 9.26\% from 1990 to 2017. Besides, nearly 30\% of the forested land is under high to very high deforestation probable zone, which needs to be protected with immediate measures.}
}

@article{GHOLIZADEH2020nanofluid,
title = {Prediction of nanofluids viscosity using random forest ({RF}) approach},
journal = {Chemom. Intell. Lab. Syst.},
volume = {201},
pages = {104010},
year = {2020},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2020.104010},
url = {https://doi.org/10.1016/j.chemolab.2020.104010},
author = {M. Gholizadeh and M. Jamei and I. Ahmadianfar and R. Pourrajab},
keywords = {Relative viscosity, Nanofluid, Nanoparticle, RF, SVR, MLP},
abstract = {Accurate estimation of viscosity, one of the most important thermo-physical properties of nanofluids, is essential in heat transfer fluid applications in many industries. In this paper, for the first time, the random forest (RF), a robust artificial intelligence method is utilized to accurately estimate the viscosity of Newtonian nanofluids. To develop the model five input parameters were used, namely the temperature, solid volume fraction, viscosity of the base fluid, nanoparticle size, and density of nanoparticle. Further, 2890 datasets were collected from 50 references representing a wide range of experimental settings. The model’s predictive performance was assessed against those of a multilayer perceptron (MLP) model, a support vector regression (SVR) and various classical and empirical models. The models’ performance were analyzed using various statistical performance indicators and graphical plots. Performance criteria assessment, using the testing dataset, showed that the RF model provided the best prediction of the viscosity of nanofluids (R ​= ​0.989, RMSE ​= ​0.139, MAPE ​= ​4.758\%) in comparison to those of the MLP (R ​= ​0.915, RMSE ​= ​0.377, MPE ​= ​16.194\%) and the SVR (R ​= ​0.941, RMSE ​= ​0.315, MAPE ​= ​7.895\%). Moreover, a sensitivity analysis demonstrated that the volume fraction and density of nanoparticles were the most and second most significant factors affecting the viscosity of nanofluid, respectively.}
}

@article{rao2020credit,
title = {2-stage modified random forest model for credit risk assessment of P2P network lending to “Three Rurals” borrowers},
journal = {Appl. Soft Comput.},
volume = {95},
pages = {106570},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106570},
url = {https://doi.org/10.1016/j.asoc.2020.106570},
author = {C. Rao and M. Liu and M. Goh and J. Wen},
keywords = {P2P network lending, ”Three Rurals” borrowers, Credit risk evaluation, Random forest},
abstract = {With the rapid growth of the P2P online loan industry in the “Three Rurals” (agriculture, rural areas, and farmers) sector, it is imperative to manage the borrowing risk of borrowers in the rural areas. A credit risk assessment model is proposed to classify the credit worthiness of the “Three Rurals” borrowers. We select the loan data of the Pterosaur Loan platform as the research sample, and establish a 2-stage Syncretic Cost-sensitive Random Forest (SCSRF) model to evaluate the credit risk of the borrowers. From the random forest, we construct a cost relationship from the actual distribution of the data categories, introduce a weighted Mahalanobis distance using the entropy weight method in the cost function, and adopt a weighted voting for the cost-sensitive decision tree base classifier. The parameters of the SCSRF model are optimized via a grid search. We validate the SCSRF classification model against several established credit evaluation models.}
}

﻿@Article{Lv2020rna,
author={Lv, Z.
and Zhang, J.
and Ding, H.
and Zou, Q.},
title={{RF}-{P}se{U}: A Random Forest Predictor for RNA Pseudouridine Sites},
journal={Front. Bioeng. Biotechnol.},
year={2020},
month={Feb},
day={26},
publisher={Frontiers Media S.A.},
volume={8},
pages={134-134},
keywords={RNA; light gradient boosting; machine learning; pseudouridine sites; random forest},
abstract={One of the ubiquitous chemical modifications in RNA, pseudouridine modification is crucial for various cellular biological and physiological processes. To gain more insight into the functional mechanisms involved, it is of fundamental importance to precisely identify pseudouridine sites in RNA. Several useful machine learning approaches have become available recently, with the increasing progress of next-generation sequencing technology; however, existing methods cannot predict sites with high accuracy. Thus, a more accurate predictor is required. In this study, a random forest-based predictor named RF-PseU is proposed for prediction of pseudouridylation sites. To optimize feature representation and obtain a better model, the light gradient boosting machine algorithm and incremental feature selection strategy were used to select the optimum feature space vector for training the random forest model RF-PseU. Compared with previous state-of-the-art predictors, the results on the same benchmark data sets of three species demonstrate that RF-PseU performs better overall. The integrated average leave-one-out cross-validation and independent testing accuracy scores were 71.4{\%} and 74.7{\%}, respectively, representing increments of 3.63{\%} and 4.77{\%} versus the best existing predictor. Moreover, the final RF-PseU model for prediction was built on leave-one-out cross-validation and provides a reliable and robust tool for identifying pseudouridine sites. A web server with a user-friendly interface is accessible at http://148.70.81.170:10228/rfpseu.},
issn={2296-4185},
doi={10.3389/fbioe.2020.00134},
language={eng},
url={https://doi.org/10.3389/fbioe.2020.00134}
}

@article{tahir2020sensor,
  title={Wearable Sensors for Activity Analysis using {SMO}-based Random Forest over Smart home and Sports Datasets},
  author={Sheikh Badar ud din Tahir and Ahmad Jalal and Mouazma Batool},
  journal={2020 3rd International Conference on Advancements in Computational Sciences (ICACS)},
  year={2020},
  pages={1-6},
  url={http://doi.org/10.1109/ICACS47775.2020.9055944}
}


@article{tan2020soil,
title = {Estimation of the spatial distribution of heavy metal in agricultural soils using airborne hyperspectral imaging and random forest},
journal = {J. Hazard. Mater.},
volume = {382},
pages = {120987},
year = {2020},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2019.120987},
url = {https://doi.org/10.1016/j.jhazmat.2019.120987},
author = {K. Tan and H. Wang and L. Chen and Q. Du and P. Du and C. Pan},
keywords = {Airborne hyperspectral remote sensing, Random forest, Soil heavy metal},
abstract = {Hyperspectral imaging, with the hundreds of bands and high spectral resolution, offers a promising approach for estimation of heavy metal concentration in agricultural soils. Using airborne imagery over a large-scale area for fast retrieval is of great importance for environmental monitoring and further decision support. However, few studies have focused on the estimation of soil heavy metal concentration by airborne hyperspectral imaging. In this study, we utilized the airborne hyperspectral data in LiuXin Mine of China obtained from HySpex VNIR-1600 and HySpex SWIR-384 sensor to establish the spectral-analysis-based model for retrieval of heavy metals concentration. Firstly, sixty soil samples were collected in situ, and their heavy metal concentrations (Cr, Cu, Pb) were determined by inductively coupled plasma-mass spectrometry analysis. Due to mixed pixels widespread in airborne hyperspectral images, spectral unmixing was conducted to obtain purer spectra of the soil and to improve the estimation accuracy. Ten of estimated models, including four different random forest models (RF)—standard random forest (SRF), regularized random forest (RRF), guided random forest (GRF), and guided regularized random forest (GRRF)—were introduced for hyperspectral estimated model in this paper. Compared with the estimation results, the best accuracy for Cr, Cu, and Pb is obtained by RF. It shows that RF can predict the three heavy metals better than other models in this area. For Cr, Cu, Pb, the best model of RF yields Rp2 values of 0.75,0.68 and 0.74 respectively, and the values of RMSEp are 5.62, 8.24, and 2.81 (mg/kg), respectively. The experiments show the average estimated values are close to the truth condition and the high estimated values concentrated near several industries, valifating the effectiveness of the presented method.}
}

@article{nesa2018iot,
  title={Outlier detection in sensed data using statistical learning models for {I}o{T}},
  author={Nashreen Nesa and Tania Ghosh and Indrajit Banerjee},
  journal={2018 IEEE Wireless Communications and Networking Conference (WCNC)},
  year={2018},
  pages={1-6},
  doi={10.1109/WCNC.2018.8376988},
  url = {http://doi.org/10.1109/WCNC.2018.8376988}
}
  
@article{liu2018species,
    author = {Liu, C. and White, M. and Newell, G.},
    title = {Detecting outliers in species distribution data},
    journal = {J. Biogeogr.},
    volume = {45},
    number = {1},
    pages = {164-176},
    keywords = {outlier, outlier detection, random forest, species distribution, species distribution modelling, support vector machine, virtual species},
    url = {https://doi.org/10.1111/jbi.13122},
    doi = {https://doi.org/10.1111/jbi.13122},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jbi.13122},
    abstract = {Abstract Aim Species distribution data play a pivotal role in the study of ecology, evolution, biogeography and biodiversity conservation. Although large amounts of location data are available and accessible from public databases, data quality remains problematic. Of the potential sources of error, positional errors are critical for spatial applications, particularly where these errors place observations beyond the environmental or geographical range of species. These outliers need to be identified, checked and removed to improve data quality and minimize the impact on subsequent analyses. Manually checking all species records within large multispecies datasets is prohibitively costly. This work investigates algorithms that may assist in the efficient vetting of outliers in such large datasets. Location We used real, spatially explicit environmental data derived from the western part of Victoria, Australia, and simulated species distributions within this same region. Methods By adapting species distribution modelling (SDM), we developed a pseudo-SDM approach for detecting outliers in species distribution data, which was implemented with random forest (RF) and support vector machine (SVM) resulting in two new methods: RF\_pdSDM and SVM\_pdSDM. Using virtual species, we compared eight existing multivariate outlier detection methods with these two new methods under various conditions. Results The two new methods based on the pseudo-SDM approach had higher true skill statistic (TSS) values than other approaches, with TSS values always exceeding 0. More than 70\% of the true outliers in datasets for species with a low and intermediate prevalence can be identified by checking 10\% of the data points with the highest outlier scores. Main conclusions Pseudo-SDM-based methods were more effective than other outlier detection methods. However, this outlier detection procedure can only be considered as a screening tool, and putative outliers must be examined by experts to determine whether they are actual errors or important records within an inherently biased set of data.},
    year = {2018}
}

@article{santana2019foodadult,
title = {Random forest as one-class classifier and infrared spectroscopy for food adulteration detection},
journal = {Food Chem.},
volume = {293},
pages = {323-332},
year = {2019},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2019.04.073},
url = {https://doi.org/10.1016/j.foodchem.2019.04.073},
author = {F. B. {de Santana} and W. {Borges Neto} and R. J. Poppi},
keywords = {One-class, Random forest, Artificial outliers, Food adulteration, Infrared spectroscopy},
abstract = {This paper proposes the use of random forest for adulteration detection purposes, combining the random forest algorithm with the artificial generation of outliers from the authentic samples. This proposal was applied in two food adulteration studies: evening primrose oils using ATR-FTIR spectroscopy and ground nutmeg using NIR diffuse reflectance spectroscopy. The primrose oil was adulterated with soybean, corn and sunflower oils, and the model was validated using these adulterated oils and other different oils, such as rosehip and andiroba, in pure and adulterated forms. The ground nutmeg was adulterated with cumin, commercial monosodium glutamate, soil, roasted coffee husks and wood sawdust. For the primrose oil, the proposed method presented superior performance than PLS-DA and similar performance to SIMCA and for the ground nutmeg, the random forest was superior to PLS-DA and SIMCA. Also, in both applications using the random forest, no sample was excluded from the external validation set.}
}

@article{baron2016galaxy,
    author = {Baron, D. and Poznanski, D.},
    title = "{The weirdest SDSS galaxies: results from an outlier detection algorithm}",
    journal = {Mon. Not. R. Astron. Soc.},
    volume = {465},
    number = {4},
    pages = {4530-4555},
    year = {2016},
    month = {11},
    abstract = "{How can we discover objects we did not know existed within the large data sets that now abound in astronomy? We present an outlier detection algorithm that we developed, based on an unsupervised Random Forest. We test the algorithm on more than two million galaxy spectra from the Sloan Digital Sky Survey and examine the 400 galaxies with the highest outlier score. We find objects which have extreme emission line ratios and abnormally strong absorption lines, objects with unusual continua, including extremely reddened galaxies. We find galaxy–galaxy gravitational lenses, double-peaked emission line galaxies and close galaxy pairs. We find galaxies with high ionization lines, galaxies that host supernovae and galaxies with unusual gas kinematics. Only a fraction of the outliers we find were reported by previous studies that used specific and tailored algorithms to find a single class of unusual objects. Our algorithm is general and detects all of these classes, and many more, regardless of what makes them peculiar. It can be executed on imaging, time series and other spectroscopic data, operates well with thousands of features, is not sensitive to missing values and is easily parallelizable.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stw3021},
    url = {http://doi.org/10.1093/mnras/stw3021},
    eprint = {https://academic.oup.com/mnras/article-pdf/465/4/4530/10254645/stw3021.pdf},
}

@article{vejendla2020network,
title = {Accurate identification and detection of outliers in networks using group random forest methodoly},
journal = {J. Crit. Rev.},
year = {2020},
volume = {7},
number = {6},
pages = {381-384},
author = {Narayana, V.L. and Gopi, A.P. and Khadherbhi, S.R. and Pavani, V.},
url = {http://doi.org/10.31838/jcr.07.06.67}
}

@InProceedings{pantanowitz2009missingdata,
    author="Pantanowitz, A.
    and Marwala, T.",
    editor="Yu, W.
    and Sanchez, E. N.",
    title="Missing Data Imputation Through the Use of the Random Forest Algorithm",
    booktitle= "Advances in Computational Intelligence",
    year="2009",
    publisher="Springer",
    address="Berlin, Heidelberg",
    pages="53--62",
    abstract="This paper presents a comparison of different paradigms used for missing data imputation. The data set used is HIV seroprevalence data from an antenatal clinic study survey performed in 2001. Data imputation is performed through five methods: Random Forests; auto-associative neural networks with genetic algorithms; auto-associative neuro-fuzzy configurations; and two random forest and neural network based hybrids. Results indicate that Random Forests are superior in imputing missing data for the given data set in terms of accuracy and in terms of computation time, with accuracy increases of up to 32 {\%} on average for certain variables when compared with auto-associative networks. While the concept of hybrid systems has promise, the presented systems appear to be hindered by their auto-associative neural network components.",
    isbn="978-3-642-03156-4",
    url = {http://doi.org/10.1007/978-3-642-03156-4_6}
}

﻿@Article{Shah2014caliber,
author={Shah, A. D.
and Bartlett, J. W.
and Carpenter, J.
and Nicholas, O.
and Hemingway, H.},
title={Comparison of random forest and parametric imputation models for imputing missing data using {MICE}: a CALIBER study},
journal={Am. J. Epidemiol.},
year={2014},
month={Mar},
day={15},
edition={2014/01/12},
publisher={Oxford University Press},
volume={179},
number={6},
pages={764-774},
keywords={angina, stable; imputation; missing data; missingness at random; regression trees; simulation; survival; Age Factors; Angina, Stable/epidemiology; *Artificial Intelligence; Bias; *Computer Simulation; Confidence Intervals; *Epidemiologic Methods; Health Behavior; Health Status; Humans; Proportional Hazards Models; Random Allocation; Sex Factors},
abstract={Multivariate imputation by chained equations (MICE) is commonly used for imputing missing data in epidemiologic research. The "true" imputation model may contain nonlinearities which are not included in default imputation models. Random forest imputation is a machine learning technique which can accommodate nonlinearities and interactions and does not require a particular regression model to be specified. We compared parametric MICE with a random forest-based MICE algorithm in 2 simulation studies. The first study used 1,000 random samples of 2,000 persons drawn from the 10,128 stable angina patients in the CALIBER database (Cardiovascular Disease Research using Linked Bespoke Studies and Electronic Records; 2001-2010) with complete data on all covariates. Variables were artificially made "missing at random," and the bias and efficiency of parameter estimates obtained using different imputation methods were compared. Both MICE methods produced unbiased estimates of (log) hazard ratios, but random forest was more efficient and produced narrower confidence intervals. The second study used simulated data in which the partially observed variable depended on the fully observed variables in a nonlinear way. Parameter estimates were less biased using random forest MICE, and confidence interval coverage was better. This suggests that random forest imputation may be useful for imputing complex epidemiologic data sets in which some patients have missing data.},
issn={1476-6256},
doi={10.1093/aje/kwt312},
language={eng},
url = {http://doi.org/10.1093/aje/kwt312}
}

﻿@Article{Ramosaj2019predictmiss,
author={Ramosaj, B.
and Pauly, M.},
title={Predicting missing values: a comparative study on non-parametric approaches for imputation},
journal={Comput. Stat.},
year={2019},
month={Dec},
day={01},
volume={34},
number={4},
pages={1741-1764},
abstract={Missing data is an expected issue when large amounts of data is collected, and several imputation techniques have been proposed to tackle this problem. Beneath classical approaches such as MICE, the application of Machine Learning techniques is tempting. Here, the recently proposed missForest imputation method has shown high imputation accuracy under the Missing (Completely) at Random scheme with various missing rates. In its core, it is based on a random forest for classification and regression, respectively. In this paper we study whether this approach can even be enhanced by other methods such as the stochastic gradient tree boosting method, the C5.0 algorithm, BART or modified random forest procedures. In particular, other resampling strategies within the random forest protocol are suggested. In an extensive simulation study, we analyze their performances for continuous, categorical as well as mixed-type data. An empirical analysis focusing on credit information and Facebook data complements our investigations.},
issn={1613-9658},
doi={10.1007/s00180-019-00900-3}
}

@article{narayana2020outlierdet,
  title={Accurate identification and detection of outliers in networks using group random forest methodoly},
  author={Narayana, V. L. and Gopi, A. P. and Khadherbhi, S. R. and Pavani, V},
  journal={J. Crit. Rev.},
  volume={7},
  number={6},
  pages={381--384},
  year={2020},
  publisher={Innovare Academics Sciences}
}


@Article{Moon2019phate,
    author={Moon, Kevin R.
    and van Dijk, David
    and Wang, Zheng
    and Gigante, Scott
    and Burkhardt, Daniel B.
    and Chen, William S.
    and Yim, Kristina
    and Elzen, Antonia van den
    and Hirn, Matthew J.
    and Coifman, Ronald R.
    and Ivanova, Natalia B.
    and Wolf, Guy
    and Krishnaswamy, Smita},
    title={Visualizing structure and transitions in high-dimensional biological data},
    journal={Nat. Biotechnol.},
    year={2019},
    month={Dec},
    day={01},
    volume={37},
    number={12},
    pages={1482-1492},
    url={https://doi.org/10.1038/s41587-019-0336-3}
}


@article{vanDerMaaten2008tsne,
  author = {van der Maaten, L. and Hinton, G.},
  journal = {J. Mach. Learn. Res.},
  keywords = {dimensionality_reduction tSNE visualization},
  pages = {2579--2605},
  timestamp = {2015-08-19T15:19:11.000+0200},
  title = {Visualizing Data using {t-SNE} },
  volume = 9,
  year = 2008,
  url  = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}



@book{KruskalWish1978mds,
  title={Multidimensional Scaling},
  author={Kruskal, J.B. and Wish, M.},
  number={11},
  isbn={9780803909403},
  lccn={77093286},
  series={Multidimensional Scaling},
  year={1978},
  publisher={Sage Publications},
  address={Newbury Park, California}
}




@article{lel2018umap,
    title={{UMAP: U}niform Manifold Approximation and Projection for Dimension Reduction},
    author={L. McInnes and J. Healy and J. Melville},
    year={2018},
    volume={abs/1802.03426},
    journal={arXiv},
    url = {https://arxiv.org/abs/1802.03426}
}

@article{McInnes2018umap, doi = {10.21105/joss.00861}, url = {https://doi.org/10.21105/joss.00861}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {29}, pages = {861}, author = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Großberger}, title = {UMAP: Uniform Manifold Approximation and Projection}, journal = {Journal of Open Source Software} }


@article{duque2020grae,
  title={Extendable and invertible manifold learning with geometry regularized autoencoders},
  author={Andr'es F. Duque and Sacha Morin and Guy Wolf and Kevin R. Moon},
  journal={2020 IEEE International Conference on Big Data (Big Data)},
  year={2020},
  pages={5027-5036},
  url = {http://doi.org/10.1109/BigData50022.2020.9378049}
}
  
 @article {Tenenbaum2000isomap,
	author = {Tenenbaum, J. B. and Silva, V. and Langford, J. C.},
	title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
	volume = {290},
	number = {5500},
	pages = {2319--2323},
	year = {2000},
	doi = {},
	journal = {Science},
    url = {https://doi.org/10.1126/science.290.5500.2319}
}


﻿@Article{Cortes1995svm,
author={Cortes, C.
and Vapnik, V.},
title={Support-vector networks},
journal={Mach. Learn.},
year={1995},
month={Sep},
day={01},
volume={20},
number={3},
pages={273-297},
abstract={The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
issn={1573-0565},
doi={10.1007/BF00994018}
}


@book{Scholkopf99kernelprincipal,
editor = {Sch\"{o}lkopf, Bernhard and Burges, Christopher J. C. and Smola, Alexander J.},
title = {Advances in Kernel Methods: Support Vector Learning},
year = {1999},
isbn = {0262194163},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

@article{ng2001spectral,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, A. and Jordan, M. and Weiss, Y.},
  journal={Adv. Neural. Inf. Process. Syst.},
  volume={14},
  pages={849--856},
  year={2001}
}


﻿@Article{rhodes2021rfphate,
author={Rhodes, Jake S.
and Cutler, Adele
and Wolf, Guy
and Moon, Kevin R.},
title={Random Forest-Based Diffusion Information Geometry for Supervised Visualization and Data Exploration},
journal={2021 IEEE Statistical Signal Processing Workshop (SSP)},
year={2021},
publisher={IEEE},
pages={331-335},
keywords={data reduction; data visualisation; image classification; learning (artificial intelligence); pattern classification; principal component analysis; class labels; increased classification accuracy; improved data visualization; visualization technique; random forest proximities; diffusion-based information geometry; supervised task; reliable visualizations; data exploration; random forest-based diffusion information geometry; supervised visualization; dimensionality reduction techniques; important patterns; supervised dimensionality reduction methods; account auxiliary annotations; Dimensionality reduction; Information geometry; Data visualization; Signal processing algorithms; Signal processing; Reliability; Task analysis},
abstract={Most dimensionality reduction techniques to date are unsupervised; they do not take class labels into account (e.g., PCA, MDS, t-SNE, Isomap). Such methods require large amounts of data and are often sensitive to noise that may obfuscate important patterns in the data. Various attempts at supervised dimensionality reduction methods that take into account auxiliary annotations (e.g., class labels) have been successfully implemented with goals of increased classification accuracy or improved data visualization. In this paper, we describe a novel supervised visualization technique based on random forest proximities and a diffusion-based information geometry. We show, both qualitatively and quantitatively, the advantages of our approach in retaining local and global structure in data, while demonstrating the spatial relevance of features important for the supervised task. Importantly, our approach is robust to noise and parameter tuning, thus making it simple to use while producing reliable visualizations for data exploration.},
doi={10.1109/SSP49050.2021.9513749},
url={https://doi.org/10.1109/SSP49050.2021.9513749}
}


@article{vanburren2011mice,
   author = {S. van Buuren and K. Groothuis-Oudshoorn},
   title = {mice: Multivariate Imputation by Chained Equations in {R}},
   journal = {J. Stat. Softw.},
   volume = {45},
   number = {3},
   year = {2011},
   keywords = {},
   abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
   issn = {1548-7660},
   pages = {1--67},
   doi = {10.18637/jss.v045.i03},
   url = {http://doi.org/10.18637/jss.v045.i03}
}

  @Article{wright2017ranger,
    title = {{ranger}: A Fast Implementation of Random Forests for High
      Dimensional Data in {C++} and {R}},
    author = {M. N. Wright and A. Ziegler},
    journal = {J. Stat. Softw.},
    year = {2017},
    volume = {77},
    number = {1},
    pages = {1--17},
    doi = {10.18637/jss.v077.i01},
  }
  
  @article{deng2012mnist,
      title={The mnist database of handwritten digit images for machine learning research},
      author={Deng, Li},
      journal={IEEE Signal Process. Mag.},
      volume={29},
      number={6},
      pages={141--142},
      year={2012},
      publisher={IEEE}
}

@ARTICLE{hearst1998svm,
  author={Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal={IEEE Intell. Syst.}, 
  title={Support vector machines}, 
  year={1998},
  volume={13},
  number={4},
  pages={18-28},
  doi={10.1109/5254.708428},
  url={http://doi.org/10.1109/5254.708428}}
  
  

@inproceedings{arjovsky2017wasserstein,
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L\'{e}on},
title = {Wasserstein Generative Adversarial Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
booktitle = {Proceedings of the 34th Int. Conf. on Mach. Learn. - Volume 70},
pages = {214–223},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}



@InProceedings{Ribeiro2008esiso,
author="Ribeiro, Bernardete
and Vieira, Armando
and Carvalho das Neves, Jo{\~a}o",
editor="Ruiz-Shulcloper, Jos{\'e}
and Kropatsch, Walter G.",
title="Supervised Isomap with Dissimilarity Measures in Embedding Learning",
booktitle="Progress in Pattern Recognition, Image Analysis and Applications",
year="2008",
publisher="Springer",
address="Berlin, Heidelberg",
pages="389--396",
abstract="In this paper we propose a supervised version of the Isomap algorithm by incorporating class label information into a dissimilarity matrix in a financial analysis setting. On the credible assumption that corporates financial status lie on a low dimensional manifold, nonlinear dimensionality reduction based on manifold learning techniques has strong potential for bankruptcy analysis in financial applications. We apply the method to a real data set of distressed and healthy companies for proper geometric tunning of similarity cases. We show that the accuracy of the proposed approach is comparable to the state-of-the-art Support Vector Machines (SVM) and Relevance Vector Machines (RVM) despite the fewer dimensions used resulting from embedding learning.",
isbn="978-3-540-85920-8",
url = {https://doi.org/10.1007/978-3-540-85920-8_48}
}



@article{ZHANG2009eslle,
    title = "Enhanced supervised locally linear embedding",
    journal = "Pattern Recognit. Lett",
    volume = "30",
    number = "13",
    pages = "1208 - 1218",
    year = "2009",
    doi = "",
    url = "https://doi.org/10.1016/j.patrec.2009.05.011",
    author = "S. Zhang",
    keywords = "Dimensionality reduction, Supervised locally linear embedding, Face recognition",
    abstract = "In this paper, a new nonlinear dimensionality reduction algorithm, called enhanced supervised locally linear embedding (ESLLE), is proposed. The ESLLE method attempts to make the interclass dissimilarity definitely larger than the intraclass dissimilarity in an effort to strengthen the discriminating power and generalization ability of embedded data representation. Simulation studies on artificial manifold data demonstrate that ESLLE can give better embedding results in dimensionality reduction and is more robust to noise in comparison with the original supervised LLE (SLLE). Experimental results on extended Yale face database B and CMU PIE face databases demonstrate that ESLLE obtains better performance on face recognition compared with other famous methods such as principal component analysis (PCA), locally linear embedding (LLE) as well as SLLE."
}

@ARTICLE{Jia2019ssnmf,
  author={{Jia}, Y. and {Kwong}, S. and {Hou}, J. and others},
  journal={IEEE Trans. Neural. Netw. Learn. Syst.}, 
  title={Semi-Supervised Non-Negative Matrix Factorization With Dissimilarity and Similarity Regularization}, 
  year={2019},
  volume={},
  number={},
  pages={1-12},
  url={http://doi.org/10.1109/TNNLS.2019.2933223}
}


@inproceedings{goldberber2004nca,
author = {Goldberger, J. and Roweis, S. and Hinton, G. and Salakhutdinov, R.},
title = {Neighbourhood Components Analysis},
year = {2004},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction.},
booktitle = {Adv. Neural. Inf. Process. Systs.},
pages = {513–520},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'04}
}


@book{breiman1984cart,
  added-at = {2020-05-07T22:53:11.000+0200},
  address = {Monterey, CA},
  author = {Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  interhash = {61f3e6d61ba17bb493014bd1c6dfa670},
  intrahash = {7f293aa2bdfd10960ef36928f2795f1d},
  keywords = {ma treelearning},
  publisher = {Wadsworth and Brooks},
  serial = {bre84a},
  timestamp = {2020-05-07T22:53:11.000+0200},
  title = {Classification and Regression Trees},
  year = 1984,
  url = {https://doi.org/10.1201/9781315139470}
}



@inproceedings{sklearn_api,
  author    = {L. Buitinck and G. Louppe and M. Blondel and others},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@Article{Chang2013rnaseq,
author={Chang, K.
and Creighton, C. J.
and Davis, C.
and others},
title={The Cancer Genome Atlas Pan-Cancer analysis project},
journal={Nat. Genet.},
year={2013},
month={Oct},
day={01},
volume={45},
number={10},
pages={1113-1120},
abstract={Current clinical practice is organized according to tissue or organ of origin of tumors. Now, The Cancer Genome Atlas (TCGA) Research Network has started to identify genomic and other molecular commonalities among a dozen different types of cancer. Emerging similarities and contrasts will form the basis for targeted therapies of the future and for repurposing existing therapies by molecular rather than histological similarities of the diseases.},
issn={1546-1718},
doi={10.1038/ng.2764},
url={https://doi.org/10.1038/ng.2764}
}


@TECHREPORT{vangumalli2019kmedoids,
title = {Clustering, Forecasting and Cluster Forecasting: using k-medoids, k-{NN}s and random forests for cluster selection},
author = {Vangumalli, D. R. and Nikolopoulos, K. and Litsiou, K.},
year = {2019},
institution = {Bangor Business School, Prifysgol Bangor University (Cymru / Wales)},
type = {Working Papers},
number = {19016},
abstract = {Data analysts when facing a forecasting task involving a large number of time series, they regularly employ one of the following two methodological approaches: either select a single forecasting method for the entire dataset (aggregate selection), or use the best forecasting method for each time series (individual selection). There is evidence in the predictive analytics literature that the former is more robust than the latter, as in individual selection you tend to overfit models to the data. A third approach is to firstly identify homogeneous clusters within the dataset, and then select a single forecasting method for each cluster (cluster selection). This research examines the performance of three well-celebrated machine learning clustering methods: k-medoids, k-NN and random forests. We then forecast every cluster with the best possible method, and the performance is compared to that of aggregate selection. The aforementioned methods are very often used for classification tasks, but since in our case there is no set of predefined classes, the methods are used for pure clustering. The evaluation is performed in the 645 yearly series of the M3 competition. The empirical evidence suggests that: a) random forests provide the best clusters for the sequential forecasting task, and b) cluster selection has the potential to outperform aggregate selection.},
keywords = {Clustering; k-medoids; Nearest Neighbors; Random Forests; Forecasting;},
url = {https://doi.org/10.1080/01969722.2021.1902049}
}


@Article{Alhusain2017RFclue,
author={Alhusain, L.
and Hafez, A. M.},
title={Cluster ensemble based on Random Forests for genetic data},
journal={BioData Min.},
year={2017},
month={Dec},
day={15},
volume={10},
number={1},
pages={37},
abstract={Clustering plays a crucial role in several application domains, such as bioinformatics. In bioinformatics, clustering has been extensively used as an approach for detecting interesting patterns in genetic data. One application is population structure analysis, which aims to group individuals into subpopulations based on shared genetic variations, such as single nucleotide polymorphisms. Advances in DNA sequencing technology have facilitated the obtainment of genetic datasets with exceptional sizes. Genetic data usually contain hundreds of thousands of genetic markers genotyped for thousands of individuals, making an efficient means for handling such data desirable.},
issn={1756-0381},
doi={10.1186/s13040-017-0156-2},
url={https://doi.org/10.1186/s13040-017-0156-2}
}

@article{ALDRICH2010Fault,
title = {Fault detection and diagnosis with random forest feature extraction and variable importance methods},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {9},
pages = {79-86},
year = {2010},
note = {13th IFAC Symposium on Automation in Mining, Mineral and Metal Processing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100802-3-ZA-2014.00020},
url = {https://doi.org/10.3182/20100802-3-ZA-2014.00020},
author = {C. Aldrich and L. Auret},
keywords = {Random forest models, feature extraction, variable importance, fault diagnosis, multivariate statistical process control},
abstract = {The ever-present drive to safer, more cost-effective and cleaner processes motivates the exploration of a variety of process monitoring methods. In the domain of data-driven approaches, random forest models present a nonlinear framework. Random forest models consist of ensembles of classification and regression trees in which the model response is determined by voting committees of independent binary decision trees. Data-driven approaches to fault diagnosis often involve summarizing potentially large numbers of process variables in lower dimensional diagnostic sequences. Random forest feature extraction allows for the monitoring of process in feature and residual spaces, while random forest variable importance measures can potentially be used to identify process variables contribution to fault conditions. In this study, a framework for diagnosing steady state faults with random forests is proposed and demonstrated with a simple nonlinear system and the benchmark Tennessee Eastman process.}
}


@article{slot2022turbine,
author = {M. S. Nielsen and V. Rohde},
title = {A surrogate model for estimating extreme tower loads on wind turbines based on random forest proximities},
journal = {J. Appl. Stat.},
volume = {49},
number = {2},
pages = {485-497},
year  = {2022},
publisher = {Taylor & Francis},
doi = {10.1080/02664763.2020.1815675},

URL = { 
        https://doi.org/10.1080/02664763.2020.1815675
    
},
eprint = { 
        https://doi.org/10.1080/02664763.2020.1815675
    
}
}


@article{resende2019detsystems,
author = {Resende, P. A. A. and Drummond, A. C.},
title = {A Survey of Random Forest Based Methods for Intrusion Detection Systems},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3178582},
doi = {10.1145/3178582},
abstract = {Over the past decades, researchers have been proposing different Intrusion Detection approaches to deal with the increasing number and complexity of threats for computer systems. In this context, Random Forest models have been providing a notable performance on their applications in the realm of the behaviour-based Intrusion Detection Systems. Specificities of the Random Forest model are used to provide classification, feature selection, and proximity metrics. This work provides a comprehensive review of the general basic concepts related to Intrusion Detection Systems, including taxonomies, attacks, data collection, modelling, evaluation metrics, and commonly used methods. It also provides a survey of Random Forest based methods applied in this context, considering the particularities involved in these models. Finally, some open questions and challenges are posed combined with possible directions to deal with them, which may guide future works on the area.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {48},
numpages = {36},
keywords = {Random Forest methods, Machine Learning, behavioural methods, anomaly detection, Intrusion Detection Systems}
}

@article{Feng2020survival,
  title={Random Forest (RF) Kernel for Regression, Classification and Survival},
  author={D. Feng and R. Baumgartner},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.00089},
  url = {https://arxiv.org/abs/2009.00089}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={G. E. Hinton and O. Vinyals and J. Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531},
  url = {https://arxiv.org/abs/1503.02531}
}

@Article{Amir2013visne,
author={Amir, E. D.
and Davis, K. L.
and Tadmor, M. D.
and Simonds, E. F.
and others},
title={viSNE enables visualization of high dimensional single-cell data and reveals phenotypic heterogeneity of leukemia},
journal={Nat. Biotechnol.},
year={2013},
month={Jun},
day={01},
volume={31},
number={6},
pages={545-552},
abstract={A new tool to visualize high-dimensional single-cell data, when integrated with mass cytometry, reveals phenotypic heterogeneity of human leukemia.},
issn={1546-1696},
doi={10.1038/nbt.2594},
url={https://doi.org/10.1038/nbt.2594}
}

@inproceedings{belkin2001laplacian,
author = {Belkin, M. and Niyogi, P.},
title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {585–591},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

 @article{Chao2019supdimred, title={Recent Advances in Supervised Dimension Reduction: A Survey}, volume={1}, ISSN={2504-4990}, url={http://dx.doi.org/10.3390/make1010020}, DOI={10.3390/make1010020}, number={1}, journal={Mach. Learn. Knowl. Extr.}, publisher={MDPI AG}, author={Chao, G. and Luo, Y. and Ding, W.}, year={2019}, month={Jan}, pages={341–358}
}

@ARTICLE{Horton1996ecoli,
  title    = "A probabilistic classification system for predicting the cellular
              localization sites of proteins",
  author   = "Horton, P and Nakai, K",
  abstract = "We have defined a simple model of classification which combines
              human provided expert knowledge with probabilistic reasoning. We
              have developed software to implement this model and have applied
              it to the problem of classifying proteins into their various
              cellular localization sites based on their amino acid sequences.
              Since our system requires no hand tuning to learn training data,
              we can now evaluate the prediction accuracy of protein
              localization sites by a more objective cross-validation method
              than earlier studies using production rule type expert systems.
              336 E. coli proteins were classified into 8 classes with an
              accuracy of 81\% while 1484 yeast proteins were classified into
              10 classes with an accuracy of 55\%. Additionally we report
              empirical results using three different strategies for handling
              continuously valued variables in our probabilistic reasoning
              system.",
  journal  = "Proc. Int. Conf. Intell. Syst. Mol. Biol.",
  volume   =  4,
  pages    = "109--115",
  year     =  1996,
  address  = "United States",
  language = "en"
}

@article{coifman2006dm,
title = {Diffusion maps},
journal = {Appl. Comput. Harmon. Anal.},
volume = {21},
number = {1},
pages = {5-30},
year = {2006},
note = {Special Issue: Diffusion Maps and Wavelets},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2006.04.006},
url = {https://doi.org/10.1016/j.acha.2006.04.006},
author = {R. R. Coifman and S. Lafon},
keywords = {Diffusion processes, Diffusion metric, Manifold learning, Dimensionality reduction, Eigenmaps, Graph Laplacian},
abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.}
}

@inproceedings{huang2022learning,
  title={Learning shared neural manifolds from multi-subject FMRI data},
  author={Huang, Jessie and Busch, Erica and Wallenstein, Tom and Gerasimiuk, Michal and Benz, Andrew and Lajoie, Guillaume and Wolf, Guy and Turk-Browne, Nicholas and Krishnaswamy, Smita},
  booktitle={2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)},
  pages={01--06},
  year={2022},
  organization={IEEE}
}
@article{bogaert2023mother,
  title={Mother-to-infant microbiota transmission and infant microbiota development across multiple body sites},
  author={Bogaert, Debby and Van Beveren, Gina J and de Koff, Emma M and Parga, Paula Lusarreta and Lopez, Carlos E Balcazar and Koppensteiner, Lilian and Clerc, Melanie and Hasrat, Raiza and Arp, Kayleigh and Chu, Mei Ling JN and others},
  journal={Cell Host \& Microbe},
  volume={31},
  number={3},
  pages={447--460},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{horoi2022exploring,
  title={Exploring the geometry and topology of neural network loss landscapes},
  author={Horoi, Stefan and Huang, Jessie and Rieck, Bastian and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita},
  booktitle={International Symposium on Intelligent Data Analysis},
  pages={171--184},
  year={2022},
  organization={Springer}
}

@article{rieck2020uncovering,
  title={Uncovering the topology of time-varying fmri data using cubical persistence},
  author={Rieck, Bastian and Yates, Tristan and Bock, Christian and Borgwardt, Karsten and Wolf, Guy and Turk-Browne, Nicholas and Krishnaswamy, Smita},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6900--6912},
  year={2020}
}

@article{combes2021global,
  title={Global absence and targeting of protective immune states in severe COVID-19},
  author={Combes, Alexis J and Courau, Tristan and Kuhn, Nicholas F and Hu, Kenneth H and Ray, Arja and Chen, William S and Chew, Nayvin W and Cleary, Simon J and Kushnoor, Divyashree and Reeder, Gabriella C and others},
  journal={Nature},
  volume={591},
  number={7848},
  pages={124--130},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{guan2022chemical,
  title={Chemical reprogramming of human somatic cells to pluripotent stem cells},
  author={Guan, Jingyang and Wang, Guan and Wang, Jinlin and Zhang, Zhengyuan and Fu, Yao and Cheng, Lin and Meng, Gaofan and Lyu, Yulin and Zhu, Jialiang and Li, Yanqin and others},
  journal={Nature},
  volume={605},
  number={7909},
  pages={325--331},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{yu2021blastocyst,
  title={Blastocyst-like structures generated from human pluripotent stem cells},
  author={Yu, Leqian and Wei, Yulei and Duan, Jialei and Schmitz, Daniel A and Sakurai, Masahiro and Wang, Lei and Wang, Kunhua and Zhao, Shuhua and Hon, Gary C and Wu, Jun},
  journal={Nature},
  volume={591},
  number={7851},
  pages={620--626},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{yazar2022single,
  title={Single-cell eQTL mapping identifies cell type--specific genetic control of autoimmune disease},
  author={Yazar, Seyhan and Alquicira-Hernandez, Jose and Wing, Kristof and Senabouth, Anne and Gordon, M Grace and Andersen, Stacey and Lu, Qinyi and Rowson, Antonia and Taylor, Thomas RP and Clarke, Linda and others},
  journal={Science},
  volume={376},
  number={6589},
  pages={eabf3041},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


@article{bair2006spc,
author = {E. Bair and T. Hastie and D. Paul and R. Tibshirani},
title = {Prediction by Supervised Principal Components},
journal = {J. Am. Stat. Assoc.},
volume = {101},
number = {473},
pages = {119-137},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/016214505000000628},

URL = { 
        https://doi.org/10.1198/016214505000000628
    
},
eprint = { 
        https://doi.org/10.1198/016214505000000628
    
}}

@inproceedings{yu2006spca,
author = {Yu, S. and Yu, K. and Tresp, V. and Kriegel, H. and Wu, M.},
title = {Supervised Probabilistic Principal Component Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150454},
doi = {10.1145/1150402.1150454},
abstract = {Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.},
booktitle = {Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.},
pages = {464–473},
numpages = {10},
keywords = {dimensionality reduction, supervised projection, principal component analysis, semi-supervised projection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}




@Article{Lee1999,
    author={Lee, D.D.
    and Seung, H. S.},
    title={Learning the parts of objects by non-negative matrix factorization},
    journal={Nature},
    year={1999},
    month={Oct},
    day={01},
    volume={401},
    number={6755},
    pages={788-791},
    url = {https://doi.org/10.1038/44565}
}

@article{BARSHAN2011spcav,
title = {Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds},
journal = {Pattern Recognit.},
volume = {44},
number = {7},
pages = {1357-1371},
year = {2011},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2010.12.015},
url = {https://doi.org/10.1016/j.patcog.2010.12.015},
author = {Elnaz Barshan and Ali Ghodsi and Zohreh Azimifar and Mansoor {Zolghadri Jahromi}},
keywords = {Dimensionality reduction, Principal component analysis (PCA), Kernel methods, Supervised learning, Visualization, Classification, Regression},
abstract = {We propose “supervised principal component analysis (supervised PCA)”, a generalization of PCA that is uniquely effective for regression and classification problems with high-dimensional input data. It works by estimating a sequence of principal components that have maximal dependence on the response variable. The proposed supervised PCA is solvable in closed-form, and has a dual formulation that significantly reduces the computational complexity of problems in which the number of predictors greatly exceeds the number of observations (such as DNA microarray experiments). Furthermore, we show how the algorithm can be kernelized, which makes it applicable to non-linear dimensionality reduction tasks. Experimental results on various visualization, classification and regression problems show significant improvement over other supervised approaches both in accuracy and computational efficiency.}
}


 @article{Tsuge2001,
  title={Dimensionality reduction using non-negative matrix factorization for information retrieval},
  author={Satoru Tsuge and Masami Shishibori and Shingo Kuroiwa and Kenji Kita},
  journal={2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)},
  year={2001},
  volume={2},
  pages={960-965 vol.2},
  url = {http://doi.org/10.1109/ICSMC.2001.973042}
}

 @ARTICLE{Liu2012,
  author={ {Liu}, H. and  {Zhaohui}, W. and  {Xuelong}, L. and others},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.}, 
  title={Constrained Nonnegative Matrix Factorization for Image Representation}, 
  year={2012},
  volume={34},
  number={7},
  pages={1299-1311},
  url = {https://doi.org/10.1007/s11277-018-5325-1}
}

@ARTICLE{Li2018,
  author={Z. {Li} and J. {Tang} and X. {He}},
  journal={IEEE Trans. Neural Netw. Learn. Syst}, 
  title={Robust Structured Nonnegative Matrix Factorization for Image Representation}, 
  year={2018},
  volume={29},
  number={5},
  pages={1947-1960},
  url = {http://doi.org/10.1109/TNNLS.2017.2691725}
}

@ARTICLE{Zhang2016,
  author={X. {Zhang} and L. {Zong} and X. {Liu} and J. {Luo}},
  journal={IEEE Trans. Neural. Netw. Learn. Syst.}, 
  title={Constrained Clustering With Nonnegative Matrix Factorization}, 
  year={2016},
  volume={27},
  number={7},
  pages={1514-1526},
}

@article{Liu2008,
title = "Reducing microarray data via nonnegative matrix factorization for visualization and clustering analysis",
journal = "J. Biomed. Inform.",
volume = "41",
number = "4",
pages = "602 - 606",
year = "2008",
doi = "https://doi.org/10.1016/j.jbi.2007.12.003",
url = "https://doi.org/10.1016/j.jbi.2007.12.003",
author = "W. Liu and K. Yuan and D. Ye",
keywords = "Microarray data, Nonnegative Matrix Factorization, Principal component analysis, Visualization, Clustering analysis",
abstract = "In microarray data analysis, each gene expression sample has thousands of genes and reducing such high dimensionality is useful for both visualization and further clustering of samples. Traditional principal component analysis (PCA) is a commonly used method which has problems. Nonnegative Matrix Factorization (NMF) is a new dimension reduction method. In this paper we compare NMF and PCA for dimension reduction. The reduced data is used for visualization, and clustering analysis via k-means on 11 real gene expression datasets. Before the clustering analysis, we apply NMF and PCA for reduction in visualization. The results on one leukemia dataset show that NMF can discover natural clusters and clearly detect one mislabeled sample while PCA cannot. For clustering analysis via k-means, NMF most typically outperforms PCA. Our results demonstrate the superiority of NMF over PCA in reducing microarray data."
}

@article{tapson2005,
title = "PLANT DATA VISUALIZATION USING NON-NEGATIVE MATRIX FACTORIZATION",
journal = "IFAC",
volume = "38",
number = "1",
pages = "73 - 78",
year = "2005",
issn = "1474-6670",
doi = "https://doi.org/10.3182/20050703-6-CZ-1902.01814",
url = "https://doi.org/10.3182/20050703-6-CZ-1902.01814",
author = "J. Tapson and J.R. Greene"
}

@article{Babaee2016ImmersiveVO,
  title={Immersive visualization of visual data using nonnegative matrix factorization},
  author={M. Babaee and S. Tsoukalas and G. Rigoll and M. Datcu},
  journal={Neurocomputing},
  year={2016},
  volume={173},
  pages={245-255},
  url = {https://doi.org/10.1016/j.neucom.2015.03.121}
}

  
  @article{Cai2008,
  title={Non-negative Matrix Factorization on Manifold},
  author={Deng Cai and Xiaofei He and Xiaoyun Wu and Jiawei Han},
  journal={2008 Eighth IEEE International Conference on Data Mining},
  year={2008},
  pages={63-72},
  url = {https://doi.org/10.1007/s11063-019-10111-y}
}
  
 @inproceedings{vlachos2002,
author = {Vlachos, M. and Domeniconi, C. and Gunopulos, D. and Kollios, G. and others},
title = {Non-Linear Dimensionality Reduction Techniques for Classification and Visualization},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775143},
doi = {10.1145/775047.775143},
abstract = {In this paper we address the issue of using local embeddings for data visualization in two and three dimensions, and for classification. We advocate their use on the basis that they provide an efficient mapping procedure from the original dimension of the data, to a lower intrinsic dimension. We depict how they can accurately capture the user's perception of similarity in high-dimensional data for visualization purposes. Moreover, we exploit the low-dimensional mapping provided by these embeddings, to develop new classification techniques, and we show experimentally that the classification accuracy is comparable (albeit using fewer dimensions) to a number of other classification procedures.},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {645–651},
numpages = {7},
location = {Edmonton, Alberta, Canada},
series = {KDD '02}
}


@InProceedings{ridder2003,
author="de Ridder, Dick
and Kouropteva, Olga
and Okun, Oleg
and Pietik{\"a}inen, Matti
and Duin, Robert P. W.",
editor="Kaynak, Okyay
and Alpaydin, Ethem
and Oja, Erkki
and Xu, Lei",
title="Supervised Locally Linear Embedding",
booktitle="Artificial Neural Networks and Neural Information Processing --- ICANN/ICONIP 2003",
year="2003",
publisher="Springer",
address="Berlin, Heidelberg",
pages="333--341",
abstract="Locally linear embedding (LLE) is a recently proposed method for unsupervised nonlinear dimensionality reduction. It has a number of attractive features: it does not require an iterative algorithm, and just a few parameters need to be set. Two extensions of LLE to supervised feature extraction were independently proposed by the authors of this paper. Here, both methods are unified in a common framework and applied to a number of benchmark data sets. Results show that they perform very well on high-dimensional data which exhibits a manifold structure.",
isbn="978-3-540-44989-8",
url = {https://doi.org/10.1007/3-540-44989-2_40}
}




@incollection{polito2002,
    title = {Grouping and dimensionality reduction by locally linear embedding},
    author = {M. Polito and Perona, P.},
    booktitle = {NeurIPS},
    editor = {T. G. Dietterich and S. Becker and Z. Ghahramani},
    pages = {1255--1262},
    year = {2002},
    publisher = {MIT Press},
    url = {}
}


@article{anderson1936iris,
 ISSN = {00266493},
 URL = {https://doi.org/10.2307/2394164},
 author = {Edgar Anderson},
 journal = {Annals of the Missouri Botanical Garden},
 number = {3},
 pages = {457--509},
 publisher = {Missouri Botanical Garden Press},
 title = {The Species Problem in Iris},
 urldate = {2022-07-29},
 volume = {23},
 year = {1936}
}




@article{ZHANG2020221labelfree,
title = {Label-free discrimination and quantitative analysis of oxidative stress induced cytotoxicity and potential protection of antioxidants using Raman micro-spectroscopy and machine learning},
journal = {Anal. Chim. Acta},
volume = {1128},
pages = {221-230},
year = {2020},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2020.06.074},
url = {https://doi.org/10.1016/j.aca.2020.06.074},
author = {Wei Zhang and Jake S. Rhodes and Ankit Garg and Jon Y. Takemoto and Xiaojun Qi and Sitaram Harihar and Cheng-Wei {Tom Chang} and Kevin R. Moon and Anhong Zhou},
keywords = {Raman spectroscopy, Machine learning, PHATE, Mutual information, Antioxidant},
abstract = {Diesel exhaust particles (DEPs) are major constituents of air pollution and associated with numerous oxidative stress-induced human diseases. In vitro toxicity studies are useful for developing a better understanding of species-specific in vivo conditions. Conventional in vitro assessments based on oxidative biomarkers are destructive and inefficient. In this study, Raman spectroscopy, as a non-invasive imaging tool, was used to capture the molecular fingerprints of overall cellular component responses (nucleic acid, lipids, proteins, carbohydrates) to DEP damage and antioxidant protection. We apply a novel data visualization algorithm called PHATE, which preserves both global and local structure, to display the progression of cell damage over DEP exposure time. Meanwhile, a mutual information (MI) estimator was used to identify the most informative Raman peaks associated with cytotoxicity. A health index was defined to quantitatively assess the protective effects of two antioxidants (resveratrol and mesobiliverdin IXα) against DEP induced cytotoxicity. In addition, a number of machine learning classifiers were applied to successfully discriminate different treatment groups with high accuracy. Correlations between Raman spectra and immunomodulatory cytokine and chemokine levels were evaluated. In conclusion, the combination of label-free, non-disruptive Raman micro-spectroscopy and machine learning analysis is demonstrated as a useful tool in quantitative analysis of oxidative stress induced cytotoxicity and for effectively assessing various antioxidant treatments, suggesting that this framework can serve as a high throughput platform for screening various potential antioxidants based on their effectiveness at battling the effects of air pollution on human health.}
}


  @article{moon2017mi,
  title={Ensemble estimation of mutual information},
  author={Kevin R. Moon and Kumar Sricharan and Alfred O. Hero},
  journal={2017 IEEE International Symposium on Information Theory (ISIT)},
  year={2017},
  pages={3030-3034},
  doi={10.1109/ISIT.2017.8007086}
  
}
  
 @article{ALIKHAN2022salinity,
title = {Application of random forest for modelling of surface water salinity},
journal = {Ain Shams Engineering Journal},
volume = {13},
number = {4},
pages = {101635},
year = {2022},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2021.11.004},
url = {https://doi.org/10.1016/j.asej.2021.11.004},
author = {M. {Ali Khan} and M. {Izhar Shah} and M. {Faisal Javed} and M. {Ijaz Khan} and others},
keywords = {Surface water salinity, Electrical conductivity (EC), Total dissolved solids (TDS), Inputs optimization, Random Forest modeling},
abstract = {Modeling surface water quality using artificial intelligence-based models is essential in projecting suitable mitigation measures. However, it remains a challenge and requires further research to enhance the modeling accuracy. For this aim, this article presents a methodology to optimize the modeling inputs and reduce the associated complexity. The proposed approach employs Random Forest for modeling surface water salinity in terms of electrical conductivity (EC) and total dissolved solids (TDS) in the upper Indus River basin, one of the major rivers in Asia. Various water quality parameters measured monthly over a historical 30-year period were utilized in the modeling process. Various statistical indicators were used to evaluate the model performance. Random Forest process is suitable technique to simulate the salinity of surface water bodies, and effective tool in minimizing the modeling complexity and elaborating proper management and mitigation measures.}
}

@article{ZHANG2021clay,
title = {Prediction of undrained shear strength using extreme gradient boosting and random forest based on {B}ayesian optimization},
journal = {Geosci. Front.},
volume = {12},
number = {1},
pages = {469-477},
year = {2021},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2020.03.0E07},
url = {https://doi.org/10.1016/j.gsf.2020.03.007},
author = {W. Zhang and C. Wu and H. Zhong and Y. Li and L. Wang},
keywords = {Undrained shear strength, Extreme gradient boosting, Random forest, Bayesian optimization, k-fold CV},
abstract = {Accurate assessment of undrained shear strength (USS) for soft sensitive clays is a great concern in geotechnical engineering practice. This study applies novel data-driven extreme gradient boosting (XGBoost) and random forest (RF) ensemble learning methods for capturing the relationships between the USS and various basic soil parameters. Based on the soil data sets from TC304 database, a general approach is developed to predict the USS of soft clays using the two machine learning methods above, where five feature variables including the preconsolidation stress (PS), vertical effective stress (VES), liquid limit (LL), plastic limit (PL) and natural water content (W) are adopted. To reduce the dependence on the rule of thumb and inefficient brute-force search, the Bayesian optimization method is applied to determine the appropriate model hyper-parameters of both XGBoost and RF. The developed models are comprehensively compared with three comparison machine learning methods and two transformation models with respect to predictive accuracy and robustness under 5-fold cross-validation (CV). It is shown that XGBoost-based and RF-based methods outperform these approaches. Besides, the XGBoost-based model provides feature importance ranks, which makes it a promising tool in the prediction of geotechnical parameters and enhances the interpretability of model.}
}

@article{LIN2021co2,
title = {Analyzing the impact of three-dimensional building structure on {CO}$_2$ emissions based on random forest regression},
journal = {Energy},
volume = {236},
pages = {121502},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121502},
url = {https://doi.org/10.1016/j.energy.2021.121502},
author = {J. Lin and S. Lu and X. He and F. Wang},
keywords = {CO emission, Building structure, Energy planning, Urban design, Random forest},
abstract = {Carbon dioxide (CO2) is the primary greenhouse gas that increasingly threatens environmental conditions and public health. In addition to conventional socio-economic mitigation measures, a healthy urban design can substantially contribute to the reduction of CO2 emissions. Nevertheless, previous attempts only concentrated on the impacts of horizontal landscape pattern and spatial structure on CO2 emissions. The relationship between three-dimensional building structure and CO2 emissions remains to be explored. To fill this knowledge gap, our study analyzed which building indicators matter most to CO2 emissions in high-density areas. First, we discovered the linear relationships between CO2 emissions and various potential spatial drivers based on Pearson correlation test. Second, we examined whether the additional consideration of different building-related indicators can better explain the variation in CO2 emissions using random forest regression. These experiments indicated that building coverage ratio, mean building number, spatial congestion degree, and floor area ratio can exert substantial impacts on CO2 emissions in the study area. Building structure is a key factor affecting CO2 emission volumes. For example, our improved model yields a lower root relative squared error (32.53\%) than the benchmark model (34.68\%). This methodological framework, which can be easily applied to any other regions, is expected to provide valuable information for the reduction of CO2 emissions from the perspective of vertical urban planning. Policy-makers should carefully consider the impact of building structure on CO2 emissions at an earlier stage of the healthy urban design.}
}

@article{wang2020lgae, title={Discriminative Auto-Encoder With Local and Global Graph Embedding}, volume={8}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2020.2972132}, abstractNote={In order to exploit the potential intrinsic low-dimensional structure of the high-dimensional data from the manifold learning perspective, we propose a global graph embedding with globality-preserving property, which requires that samples should be mapped close to their low-dimensional class representation data distribution centers in the embedding space. Then we propose a novel local and global graph embedding auto-encoder(LGAE) to capture the geometric structure of data, its cost function have three terms, a reconstruction loss to reproduce the input data based on the learned representation, a local graph embedding regularization to enforce mapping the neighboring samples close together in the embedding space, a global embedding regularization to enforce mapping samples close to their low-dimensional class representation distribution centers. Thus in the learning process, our LGAE can map samples from same class close together in the embedding space, as well as reduce the scatter within-class and increase the margin between-class, it will also detect the local and global intrinsic geometric structure of data and discover the latent discriminant information in the embedding space. We build stacked LGAE for classification tasks and conduct comprehensive experiments on several benchmark datasets, the results confirm that our proposed framework can learn discriminative representation, speed up the network convergence process, and significantly improve the classification performance.}, journal={IEEE Access}, author={Li, Rui and Wang, Xiaodan and Lai, Jie and Song, Yafei and Lei, Lei}, year={2020}, pages={28614–28623} }

@article{WANG2021water,
title = {Spatial heterogeneity modeling of water quality based on random forest regression and model interpretation},
journal = {Environ. Res.},
volume = {202},
pages = {111660},
year = {2021},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2021.111660},
url = {https://doi.org/10.1016/j.envres.2021.111660},
author = {F. Wang and Y. Wang and K. Zhang and M. Hu and others},
keywords = {Water quality assessment, Machine learning, Random forest regression, Driving force analysis, Shapley additive explanations},
abstract = {A systematic understanding of the spatial distribution of water quality is critical for successful watershed management; however, the limited number of physical monitoring stations has restricted the evaluation of spatial water quality distribution and the identification of features impacting the water quality. To fill this gap, we developed a modeling process that employed the random forest regression (RFR) to model the water quality distribution for the Taihu Lake basin in Zhejiang Province, China, and adopted the Shapley Additive exPlanations (SHAP) method to interpret the underlying driving forces. We first used RFR to model three water quality parameters: permanganate index (CODMn), total phosphorus (TP), and total nitrogen (TN), based on 16 watershed features. We then applied the built models to generate water quality distribution maps for the basin, with the CODMn ranging from 1.39 to 6.40 mg/L, TP from 0.02 to 0.23 mg/L, and TN from 1.43 to 4.27 mg/L. These maps showed generally consistent patterns among the CODMn, TN, and TP with minor differences in the spatial distribution. The SHAP analysis showed that the TN was mainly affected by agricultural non-point sources, while the CODMn and TP were affected by agricultural and domestic sources. Due to differences in sewage collection and treatment between urban and rural areas, the water quality in highly populated urban areas was better than that in rural areas, which led to an unexpected positive relationship between water quality and population density. Overall, with the RFR models and SHAP interpretation, we obtained a continuous distribution pattern of the water quality and identified its driving forces in the basin. These findings provided important information to assist water quality restoration projects.}
}

@ARTICLE{rhodes2023geometry,
  author={Rhodes, Jake S. and Cutler, Adele and Moon, Kevin R.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Geometry- and Accuracy-Preserving Random Forest Proximities}, 
  year={2023},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/TPAMI.2023.3263774}}

@article{GORMAN198875sonar,
title = {Analysis of hidden units in a layered network trained to classify sonar targets},
journal = {Neural Netw},
volume = {1},
number = {1},
pages = {75-89},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90023-8},
url = {https://doi.org/10.1016/0893-6080(88)90023-8},
author = {R.Paul Gorman and Terrence J. Sejnowski},
keywords = {Learning algorithms, Hidden units, Multilayered neural network, Sonar, Signal processing},
abstract = {A neural network learning procedure has been applied to the classification of sonar returns from two undersea targets, a metal cylinder and a similarly shaped rock. Networks with an intermediate layer of hidden processing units achieved a classification accuracy as high as 100\% on a training set of 104 returns. These networks correctly classified up to 90.4\% of 104 test returns not contained in the training set. This performance was better than that of a nearest neighbor classifier, which was 82.7\%, and was close to that of an optimal Bayes classifier. Specific signal features extracted by hidden units in a trained network were identified and related to coding schemes in the pattern of connection strengths between the input and the hidden units. Network performance and classification strategy was comparable to that of trained human listeners.}
}

@techreport{brin1998pagerank,
          number = {1999-66},
           month = {November},
          author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
            note = {Previous number = SIDL-WP-1999-0120},
           title = {The PageRank Citation Ranking: Bringing Order to the Web.},
            type = {Technical Report},
       publisher = {Stanford InfoLab},
            year = {1999},
     institution = {Stanford InfoLab},
             url = {http://ilpubs.stanford.edu:8090/422/},
        abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}

  @Article{ishwaran2007randomForestSRC,
    title = {Random Survival Forests for R},
    author = {H. Ishwaran and U.B. Kogalur},
    journal = {R News},
    year = {2007},
    volume = {7},
    number = {2},
    pages = {25--31},
    month = {October},
    url = {https://CRAN.R-project.org/doc/Rnews/},
    pdf = {https://CRAN.R-project.org/doc/Rnews/Rnews_2007-2.pdf},
  }

  @Article{hothorn2006party,
    title = {Survival Ensembles},
    author = {Torsten Hothorn and Peter Buehlmann and Sandrine Dudoit
      and Annette Molinaro and Mark {Van Der Laan}},
    journal = {Biostatistics},
    year = {2006},
    volume = {7},
    number = {3},
    pages = {355--373},
    url = {https://doi.org/10.1093/biostatistics/kxj011}
  }
  
  
@Book{ggplot2,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer},
    year = {2016},
    address="New York, NY",
    isbn = {978-3-319-24277-4},
    url = {https://doi.org/10.1007/978-0-387-98141-3},
}






@article{pearson1901pca,
    author = {Pearson, Karl},
    title = {LIII. On lines and planes of closest fit to systems of points in space},
    journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
    volume = {2},
    number = {11},
    pages = {559-572},
    year  = {1901},
    publisher = {Taylor & Francis},
    doi = {10.1080/14786440109462720},
    
    URL = { 
            https://doi.org/10.1080/14786440109462720
        
    },
    eprint = { 
            https://doi.org/10.1080/14786440109462720
    }
}

@article{Chawla2002smote,
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
title = {{SMOTE}: Synthetic Minority over-Sampling Technique},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.},
journal = {J. Artif. Int. Res.},
month = {jun},
pages = {321–357},
numpages = {37}
}


@ARTICLE{ramanchadram2017multimodal,
  author={Ramachandram, Dhanesh and Taylor, Graham W.},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Multimodal Learning: A Survey on Recent Advances and Trends}, 
  year={2017},
  volume={34},
  number={6},
  pages={96-108},
  doi={10.1109/MSP.2017.2738401},
  url = {http://doi.org/10.1109/MSP.2017.2738401}}
  
  
  
@Manual{rockel2020missMethods,
title = {missMethods: Methods for Missing Data},
author = {Tobias Rockel},
year = {2020},
note = {R package version 0.2.0},
url = {https://CRAN.R-project.org/package=missMethods},
}

@inproceedings{hinton1994autoencoders,
 author = {Hinton, Geoffrey E and Zemel, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
 url = {https://proceedings.neurips.cc/paper/1993/file/9e3cfc48eccf81a0d57663e129aef3cb-Paper.pdf},
 volume = {6},
 year = {1993}
}

@ARTICLE{cheng2015stsne,  author={Cheng, Jian and Liu, Haijun and Wang, Feng and Li,             Hongsheng and Zhu, Ce},  
    journal={IEEE Transactions on Image Processing},   
    title={Silhouette Analysis for Human Action Recognition Based on Supervised Temporal {t-SNE} and Incremental Learning},   
    year={2015},  
    volume={24},  
    number={10},  
    pages={3203-3217},  
    doi={10.1109/TIP.2015.2441634},
    url = {http://doi.org/10.1109/TIP.2015.2441634}
    }
    
    
@inproceedings{hajderanj2019stsne,
    author = {Hajderanj, Laureta and Weheliye, Isakh and Chen, Daqing},
    title = {A New Supervised {T-SNE} with Dissimilarity Measure for Effective Data Visualization and Classification},
    year = {2019},
    isbn = {9781450361057},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3328833.3328853},
    doi = {10.1145/3328833.3328853},
    abstract = {In this paper, a new version of the Supervised t- Stochastic Neighbor Embedding (S-tSNE) algorithm is proposed which introduces the use of a dissimilarity measure related to class information. The proposed S-tSNE can be applied in any high dimensional dataset for visualization or as a feature extraction for classification problems. In this study, the S-tSNE is applied to three datasets MNIST, Chest x-ray, and SEER Breast Cancer. The two-dimensional data generated by the S-tSNE showed better visualization and an improvement in terms of classification accuracy in comparison to the original t- Stochastic Neighbor Embedding(t-SNE) method. The results from k-nearest neighbors (k-NN) classification model which used the lower dimension space generated by the new S-tSNE method showed more than 20\% improvement on average in accuracy in all the three datasets compared with the t-SNE method. In addition, the classification accuracy using the S-tSNE for feature extraction was even higher than classification accuracy obtained from the original high dimensional data.},
    booktitle = {Proceedings of the 2019 8th International Conference on Software and Information Engineering},
    pages = {232–236},
    numpages = {5},
    keywords = {supervised dimensionality reduction, k-NN classification, dissimilarity measure, High dimensional data, supervised t-SNE},
    location = {Cairo, Egypt},
    series = {ICSIE '19}
    }
    
@article{sainburg2021ssumap,
    author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
    title = "{Parametric UMAP embeddings for representation and semisupervised learning}",
    journal = {Neural Computation},
    volume = {33},
    number = {11},
    pages = {2881-2907},
    year = {2021},
    month = {10},
    abstract = "{UMAP is a nonparametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) computing a graphical representation of a data set (fuzzy simplicial complex) and (2) through stochastic gradient descent, optimizing a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that parametric UMAP performs comparably to its nonparametric counterpart while conferring the benefit of a learned parametric mapping (e.g., fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semisupervised learning by capturing structure in unlabeled data.1}",
    issn = {0899-7667},
    doi = {10.1162/neco\_a\_01434},
    url = {https://doi.org/10.1162/neco\_a\_01434},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/11/2881/1966656/neco\_a\_01434.pdf},
}


@article{chun2006seisomap,
  title={Supervised Isomap with Explicit Mapping},
  author={Chun-Guang Li and Jun Guo},
  journal={First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)},
  year={2006},
  volume={3},
  pages={345-348},
  doi={10.1109/ICICIC.2006.530},
  url = {http://doi.org/10.1109/ICICIC.2006.530}
}


@article{fisher1936lda,
    author = {Fisher, R. A.},
    title = {THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS},
    journal = {Annals of Eugenics},
    volume = {7},
    number = {2},
    pages = {179-188},
    doi = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
    url = {https://doi.org/10.1111/j.1469-1809.1936.tb02137.x},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
    abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
    year = {1936}
}

@article {amouzgar2022hsslda,
	author = {Amouzgar, Meelad and Glass, David R. and Baskar, Reema and Averbukh, Inna and Kimmey, Samuel C. and Tsai, Albert G. and Hartmann, Felix J. and Bendall, Sean C.},
	title = {Supervised dimensionality reduction for exploration of single-cell data by Hybrid Subset Selection - Linear Discriminant Analysis},
	elocation-id = {2022.01.06.475279},
	year = {2022},
	doi = {10.1101/2022.01.06.475279},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Single-cell technologies generate large, high-dimensional datasets encompassing a diversity of omics. Dimensionality reduction enables visualization of data by representing cells in two-dimensional plots that capture the structure and heterogeneity of the original dataset. Visualizations contribute to human understanding of data and are useful for guiding both quantitative and qualitative analysis of cellular relationships. Existing algorithms are typically unsupervised, utilizing only measured features to generate manifolds, disregarding known biological labels such as cell type or experimental timepoint. Here, we repurpose the classification algorithm, linear discriminant analysis (LDA), for supervised dimensionality reduction of single-cell data. LDA identifies linear combinations of predictors that optimally separate a priori classes, enabling users to tailor visualizations to separate specific aspects of cellular heterogeneity. We implement feature selection by hybrid subset selection (HSS) and demonstrate that this flexible, computationally-efficient approach generates non-stochastic, interpretable axes amenable to diverse biological processes, such as differentiation over time and cell cycle. We benchmark HSS-LDA against several popular dimensionality reduction algorithms and illustrate its utility and versatility for exploration of single-cell mass cytometry, transcriptomics and chromatin accessibility data.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/01/06/2022.01.06.475279},
	eprint = {https://www.biorxiv.org/content/early/2022/01/06/2022.01.06.475279.full.pdf},
	journal = {bioRxiv}
}

@article{wold2001pls,
title = {PLS-regression: a basic tool of chemometrics},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {58},
number = {2},
pages = {109-130},
year = {2001},
note = {PLS Methods},
issn = {0169-7439},
doi = {https://doi.org/10.1016/S0169-7439(01)00155-1},
url = {https://doi.org/10.1016/S0169-7439(01)00155-1},
author = {Svante Wold and Michael Sjöström and Lennart Eriksson},
keywords = {PLS, PLSR, Two-block predictive PLS, Latent variables, Multivariate analysis},
abstract = {PLS-regression (PLSR) is the PLS approach in its simplest, and in chemistry and technology, most used form (two-block predictive PLS). PLSR is a method for relating two data matrices, X and Y, by a linear multivariate model, but goes beyond traditional regression in that it models also the structure of X and Y. PLSR derives its usefulness from its ability to analyze data with many, noisy, collinear, and even incomplete variables in both X and Y. PLSR has the desirable property that the precision of the model parameters improves with the increasing number of relevant variables and observations. This article reviews PLSR as it has developed to become a standard tool in chemometrics and used in chemistry and engineering. The underlying model and its assumptions are discussed, and commonly used diagnostics are reviewed together with the interpretation of resulting parameters. Two examples are used as illustrations: First, a Quantitative Structure–Activity Relationship (QSAR)/Quantitative Structure–Property Relationship (QSPR) data set of peptides is used to outline how to develop, interpret and refine a PLSR model. Second, a data set from the manufacturing of recycled paper is analyzed to illustrate time series modelling of process data by means of PLSR and time-lagged X-variables.}
}


@article{mantel1967mantel,
    author = {Mantel, Nathan},
    title = "{The detection of disease clustering and a generalized regression approach}",
    journal = {Cancer Research},
    volume = {27},
    number = {2\_Part\_1},
    pages = {209-220},
    year = {1967},
    month = {02},
    abstract = "{The problem of identifying subtle time-space clustering of disease, as may be occurring in leukemia, is described and reviewed. Published approaches, generally associated with studies of leukemia, not dependent on knowledge of the underlying population for their validity, are directed towards identifying clustering by establishing a relationship between the temporal and the spatial separations for the n(n - 1)/2 possible pairs which can be formed from the n observed cases of disease. Here it is proposed that statistical power can be improved by applying a reciprocal transform to these separations. While a permutational approach can give valid probability levels for any observed association, for reasons of practicability, it is suggested that the observed association be tested relative to its permutational variance. Formulas and computational procedures for doing so are given.While the distance measures between points represent symmetric relationships subject to mathematical and geometric regularities, the variance formula developed is appropriate for arbitrary relationships. Simplified procedures are given for the case of symmetric and skew-symmetric relationships. The general procedure is indicated as being potentially useful in other situations as, for example, the study of interpersonal relationships. Viewing the procedure as a regression approach, the possibility for extending it to nonlinear and multivariate situations is suggested.Other aspects of the problem and of the procedure developed are discussed.Similarly, pure temporal clustering can be identified by a study of incidence rates in periods of widespread epidemics. In point of fact, many epidemics of communicable diseases are somewhat local in nature and so these do actually constitute temporal-spatial clusters. For leukemia and similar diseases in which cases seem to arise substantially at random rather than as clear-cut epidemics, it is necessary to devise sensitive and efficient procedures for detecting any nonrandom component of disease occurrence.Various ingenious procedures which statisticians have developed for the detection of disease clustering are reviewed here. These procedures can be generalized so as to increase their statistical validity and efficiency. The technic to be given below for imparting statistical validity to the procedures already in vogue can be viewed as a generalized form of regression with possible useful application to problems arising in quite different contexts.}",
    issn = {0008-5472},
    eprint = {https://aacrjournals.org/cancerres/article-pdf/27/2\_Part\_1/209/2382183/cr0272p10209.pdf},
}


@article{jiang2009slapeig,
  title={Supervised Laplacian Eigenmaps for Machinery Fault Classification},
  author={Quansheng Jiang and Minping Jia},
  journal={2009 WRI World Congress on Computer Science and Information Engineering},
  year={2009},
  volume={7},
  pages={116-120},
  doi={10.1109/CSIE.2009.765},
  url = {http://doi.org/10.1109/CSIE.2009.765}
}


@InProceedings{ridder2003slle,
author="de Ridder, Dick
and Kouropteva, Olga
and Okun, Oleg
and Pietik{\"a}inen, Matti
and Duin, Robert P. W.",
editor="Kaynak, Okyay
and Alpaydin, Ethem
and Oja, Erkki
and Xu, Lei",
title="Supervised Locally Linear Embedding",
booktitle="Artificial Neural Networks and Neural Information Processing --- ICANN/ICONIP 2003",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="333--341",
abstract="Locally linear embedding (LLE) is a recently proposed method for unsupervised nonlinear dimensionality reduction. It has a number of attractive features: it does not require an iterative algorithm, and just a few parameters need to be set. Two extensions of LLE to supervised feature extraction were independently proposed by the authors of this paper. Here, both methods are unified in a common framework and applied to a number of benchmark data sets. Results show that they perform very well on high-dimensional data which exhibits a manifold structure.",
isbn="978-3-540-44989-8"
}

@ARTICLE{hajderanj2021impactsupman, 
    author={Hajderanj, Laureta and Chen, Daqing and Weheliye, Isakh}, 
    journal={IEEE Access},  
    title={The Impact of Supervised Manifold Learning on Structure Preserving and Classification Error: A Theoretical Study},   
    year={2021},  
    volume={9},  
    number={},  
    pages={43909-43922},  
    doi={10.1109/ACCESS.2021.3066259},
    url = {10.1109/ACCESS.2021.3066259}
}

﻿@Article{Balcan2008simtheory,
    author={Balcan, Maria-Florina
    and Blum, Avrim
    and Srebro, Nathan},
    title={A theory of learning with similarity functions},
    journal={Mach. Learn.},
    year={2008},
    month={Aug},
    day={01},
    volume={72},
    number={1},
    pages={89-112},
    abstract={Kernel functions have become an extremely popular tool in machine learning, with an attractive theory as well. This theory views a kernel as implicitly mapping data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. However, while quite elegant, this theory does not necessarily correspond to the intuition of a good kernel as a good measure of similarity, and the underlying margin in the implicit space usually is not apparent in ``natural'' representations of the data. Therefore, it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand. Moreover, the requirement of positive semi-definiteness may rule out the most natural pairwise similarity functions for the given problem domain.},
    issn={1573-0565},
    doi={10.1007/s10994-008-5059-5},
    url={https://doi.org/10.1007/s10994-008-5059-5}
}


@INPROCEEDINGS{duque2019visualizing,
  author={Duque, Andrés F. and Wolf, Guy and Moon, Kevin R.},
  booktitle={2019 IEEE 29th International Workshop on Machine Learning for Signal Processing}, 
  title={Visualizing High Dimensional Dynamical Processes}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  address={Pittsburgh, PA},
  doi={10.1109/MLSP.2019.8918875},
  url = {http://doi.org/10.1109/MLSP.2019.8918875}}

  
@article{haghverdi2016diffusion,
  title={Diffusion pseudotime robustly reconstructs lineage branching},
  author={Haghverdi, Laleh and Buettner, Maren and Wolf, F Alexander and Buettner, Florian and Theis, Fabian J},
  journal={Nature Methods},
  volume={13},
  number={10},
  pages={845},
  year={2016},
  publisher={Nature Publishing Group},
  url = {http://doi.org/10.1038/nmeth.3971}
}

@INPROCEEDINGS{bohanec1988cars,
    author = {Marko Bohanec and Vladislav Rajkovič},
    title = {V.: Knowledge Acquisition and Explanation for Multi-Attribute Decision},
    booktitle = {Making, 8 th International Workshop “Expert Systems and Their Applications},
    year = {1988}
}


@article{nadler2006diffusion,
title = {Diffusion maps, spectral clustering and reaction coordinates of dynamical systems},
journal = {Applied and Computational Harmonic Analysis},
volume = {21},
number = {1},
pages = {113-127},
year = {2006},
note = {Special Issue: Diffusion Maps and Wavelets},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2005.07.004},
url = {https://doi.org/10.1016/j.acha.2005.07.004},
author = {Boaz Nadler and Stéphane Lafon and Ronald R. Coifman and Ioannis G. Kevrekidis},
abstract = {A central problem in data analysis is the low dimensional representation of high dimensional data and the concise description of its underlying geometry and density. In the analysis of large scale simulations of complex dynamical systems, where the notion of time evolution comes into play, important problems are the identification of slow variables and dynamically meaningful reaction coordinates that capture the long time evolution of the system. In this paper we provide a unifying view of these apparently different tasks, by considering a family of diffusion maps, defined as the embedding of complex (high dimensional) data onto a low dimensional Euclidean space, via the eigenvectors of suitably defined random walks defined on the given datasets. Assuming that the data is randomly sampled from an underlying general probability distribution p(x)=e−U(x), we show that as the number of samples goes to infinity, the eigenvectors of each diffusion map converge to the eigenfunctions of a corresponding differential operator defined on the support of the probability distribution. Different normalizations of the Markov chain on the graph lead to different limiting differential operators. Specifically, the normalized graph Laplacian leads to a backward Fokker–Planck operator with an underlying potential of 2U(x), best suited for spectral clustering. A different anisotropic normalization of the random walk leads to the backward Fokker–Planck operator with the potential U(x), best suited for the analysis of the long time asymptotics of high dimensional stochastic systems governed by a stochastic differential equation with the same potential U(x). Finally, yet another normalization leads to the eigenfunctions of the Laplace–Beltrami (heat) operator on the manifold in which the data resides, best suited for the analysis of the geometry of the dataset regardless of its possibly non-uniform density.}
}


﻿@Article{Kobak2021tsneglobal,
    author={Kobak, Dmitry
    and Linderman, George C.},
    title={Initialization is critical for preserving global data structure in both {t-SNE} and {UMAP}},
    journal={Nature Biotechnology},
    year={2021},
    month={Feb},
    day={01},
    volume={39},
    number={2},
    pages={156-157},
    issn={1546-1696},
    doi={10.1038/s41587-020-00809-z},
    url={https://doi.org/10.1038/s41587-020-00809-z}
}


@ARTICLE{geng2005sisomap,
  author={Xin Geng and De-Chuan Zhan and Zhi-Hua Zhou},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
  title={Supervised nonlinear dimensionality reduction for visualization and classification}, 
  year={2005},
  volume={35},
  number={6},
  pages={1098-1107},
  doi={10.1109/TSMCB.2005.850151}}


@article{he2021groundwater,
title = {Predictive modeling of groundwater nitrate pollution and evaluating its main impact factors using random forest},
journal = {Chemosphere},
volume = {290},
pages = {133388},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133388},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038625},
author = {Song He and Jianhua Wu and Dan Wang and Xiaodong He},
keywords = {Groundwater nitrate, Environmental factors, Machine learning, Random forest, Predictor variable importance},
abstract = {Groundwater quality in plains and basins of arid and semi-arid regions with increased agriculture and urbanization development faces severe nitrate pollution, which is affected by both climate and anthropogenic activities. Here, shallow groundwater nitrate concentrations in the Yinchuan Region in central Yinchuan Plain were modeled during 2000, 2005, 2010, and 2015 using random forest. Multiple spatial environment factors were taken as predictor variables. The relative importance of these factors was also calculated using the constructed model. Remote sensing and GIS methods were used to compile various environmental factors to generate training and test sets for training and validation of the random forest model. Mean absolute error (MAE), root mean square error (RMSE), and coefficient of determination (R2) between the observed and predicted groundwater nitrate concentrations were used to measure the model performance. As indicated by these metrics, the random forest model for groundwater nitrate prediction was performed well. The relative importance of the predictor variables computed by the model indicated groundwater nitrate was mainly affected by the distance to the Yellow River, meteorological elements (precipitation, evaporation, and mean air temperature), and water level elevation. Additionally, urban and arable land were the two land use/land cover types that mainly influenced groundwater nitrate concentration in the Yinchuan Region, of which urban land was more influential than arable land as a result of intense expansion of urban land from 2000 to 2015. Overall, the current study provides an approach to integrate multiple environmental factors for groundwater quality study and is also significant for sustainable groundwater management in the Yinchuan Region.}
}


﻿@Article{Simon2023ecomodels,
author={Simon, Sophia M.
and Glaum, Paul
and Valdovinos, Fernanda S.},
title={Interpreting random forest analysis of ecological models to move from prediction to explanation},
journal={Scientific Reports},
year={2023},
month={Mar},
day={08},
volume={13},
number={1},
pages={3881},
abstract={As modeling tools and approaches become more advanced, ecological models are becoming more complex. Traditional sensitivity analyses can struggle to identify the nonlinearities and interactions emergent from such complexity, especially across broad swaths of parameter space. This limits understanding of the ecological mechanisms underlying model behavior. Machine learning approaches are a potential answer to this issue, given their predictive ability when applied to complex large datasets. While perceptions that machine learning is a ``black box'' linger, we seek to illuminate its interpretive potential in ecological modeling. To do so, we detail our process of applying random forests to complex model dynamics to produce both high predictive accuracy and elucidate the ecological mechanisms driving our predictions. Specifically, we employ an empirically rooted ontogenetically stage-structured consumer-resource simulation model. Using simulation parameters as feature inputs and simulation output as dependent variables in our random forests, we extended feature analyses into a simple graphical analysis from which we reduced model behavior to three core ecological mechanisms. These ecological mechanisms reveal the complex interactions between internal plant demography and trophic allocation driving community dynamics while preserving the predictive accuracy achieved by our random forests.},
issn={2045-2322},
doi={10.1038/s41598-023-30313-8},
url={https://doi.org/10.1038/s41598-023-30313-8}
}

﻿@Article{Kong2018rfdnn,
author={Kong, Yunchuan
and Yu, Tianwei},
title={A Deep Neural Network Model using Random Forest to Extract Feature Representation for Gene Expression Data Classification},
journal={Scientific Reports},
year={2018},
month={Nov},
day={07},
volume={8},
number={1},
pages={16477},
abstract={In predictive model development, gene expression data is associated with the unique challenge that the number of samples (n) is much smaller than the amount of features (p). This ``n{\thinspace}≪{\thinspace}p'' property has prevented classification of gene expression data from deep learning techniques, which have been proved powerful under ``n{\thinspace}>{\thinspace}p'' scenarios in other application fields, such as image classification. Further, the sparsity of effective features with unknown correlation structures in gene expression profiles brings more challenges for classification tasks. To tackle these problems, we propose a newly developed classifier named Forest Deep Neural Network (fDNN), to integrate the deep neural network architecture with a supervised forest feature detector. Using this built-in feature detector, the method is able to learn sparse feature representations and feed the representations into a neural network to mitigate the overfitting problem. Simulation experiments and real data analyses using two RNA-seq expression datasets are conducted to evaluate fDNN's capability. The method is demonstrated a useful addition to current predictive models with better classification performance and more meaningful selected features compared to ordinary random forests and deep neural networks.},
issn={2045-2322},
doi={10.1038/s41598-018-34833-6},
url={https://doi.org/10.1038/s41598-018-34833-6}
}

﻿@Article{Loef2022longitudinal,
author={Loef, Bette
and Wong, Albert
and Janssen, Nicole A. H.
and Strak, Maciek
and Hoekstra, Jurriaan
and Picavet, H. Susan J.
and Boshuizen, H. C. Hendriek
and Verschuren, W. M. Monique
and Herber, Gerrie-Cor M.},
title={Using random forest to identify longitudinal predictors of health in a 30-year cohort study},
journal={Scientific Reports},
year={2022},
month={Jun},
day={20},
volume={12},
number={1},
pages={10372},
abstract={Due to the wealth of exposome data from longitudinal cohort studies that is currently available, the need for methods to adequately analyze these data is growing. We propose an approach in which machine learning is used to identify longitudinal exposome-related predictors of health, and illustrate its potential through an application. Our application involves studying the relation between exposome and self-perceived health based on the 30-year running Doetinchem Cohort Study. Random Forest (RF) was used to identify the strongest predictors due to its favorable prediction performance in prior research. The relation between predictors and outcome was visualized with partial dependence and accumulated local effects plots. To facilitate interpretation, exposures were summarized by expressing them as the average exposure and average trend over time. The RF model's ability to discriminate poor from good self-perceived health was acceptable (Area-Under-the-Curve=0.707). Nine exposures from different exposome-related domains were largely responsible for the model's performance, while 87 exposures seemed to contribute little to the performance. Our approach demonstrates that ML can be interpreted more than widely believed, and can be applied to identify important longitudinal predictors of health over the life course in studies with repeated measures of exposure. The approach is context-independent and broadly applicable.},
issn={2045-2322},
doi={10.1038/s41598-022-14632-w},
url={https://doi.org/10.1038/s41598-022-14632-w}
}

﻿@Article{Stephan2015geneticeffects,
author={Stephan, Johannes
and Stegle, Oliver
and Beyer, Andreas},
title={A random forest approach to capture genetic effects in the presence of population structure},
journal={Nature Communications},
year={2015},
month={Jun},
day={25},
volume={6},
number={1},
pages={7432},
abstract={The accurate mapping of causal variants in genome-wide association studies requires the consideration of both, confounding factors (for example, population structure) and nonlinear interactions between individual genetic variants. Here, we propose a method termed `mixed random forest' that simultaneously accounts for population structure and captures nonlinear genetic effects. We test the model in simulation experiments and show that the mixed random forest approach improves detection power compared with established approaches. In an application to data from an outbred mouse population, we find that mixed random forest identifies associations that are more consistent with prior knowledge than competing methods. Further, our approach allows predicting phenotypes from genotypes with greater accuracy than any of the other methods that we tested. Our results show that approaches that simultaneously account for both, confounding due to population structure and epistatic interactions, are important to fully explain the heritable component of complex quantitative traits.},
issn={2041-1723},
doi={10.1038/ncomms8432},
url={https://doi.org/10.1038/ncomms8432}
}

﻿@Article{Seifert2020rfsers,
author={Seifert, Stephan},
title={Application of random forest based approaches to surface-enhanced Raman scattering data},
journal={Scientific Reports},
year={2020},
month={Mar},
day={25},
volume={10},
number={1},
pages={5436},
abstract={Surface-enhanced Raman scattering (SERS) is a valuable analytical technique for the analysis of biological samples. However, due to the nature of SERS it is often challenging to exploit the generated data to obtain the desired information when no reporter or label molecules are used. Here, the suitability of random forest based approaches is evaluated using SERS data generated by a simulation framework that is also presented. More specifically, it is demonstrated that important SERS signals can be identified, the relevance of predefined spectral groups can be evaluated, and the relations of different SERS signals can be analyzed. It is shown that for the selection of important SERS signals Boruta and surrogate minimal depth (SMD) and for the analysis of spectral groups the competing method Learner of Functional Enrichment (LeFE) should be applied. In general, this investigation demonstrates that the combination of random forest approaches and SERS data is very promising for sophisticated analysis of complex biological samples.},
issn={2045-2322},
doi={10.1038/s41598-020-62338-8},
url={https://doi.org/10.1038/s41598-020-62338-8}
}

﻿@Article{Pellegrino2021oncosomatic,
author={Pellegrino, Eric
and Jacques, Coralie
and Beaufils, Nathalie
and Nanni, Isabelle
and Carlioz, Antoine
and Metellus, Philippe
and Ouafik, L'Houcine},
title={Machine learning random forest for predicting oncosomatic variant NGS analysis},
journal={Scientific Reports},
year={2021},
month={Nov},
day={08},
volume={11},
number={1},
pages={21820},
abstract={Since 2017, we have used IonTorrent NGS platform in our hospital to diagnose and treat cancer. Analyzing variants at each run requires considerable time, and we are still struggling with some variants that appear correct on the metrics at first, but are found to be negative upon further investigation. Can any machine learning algorithm (ML) help us classify NGS variants? This has led us to investigate which ML can fit our NGS data and to develop a tool that can be routinely implemented to help biologists. Currently, one of the greatest challenges in medicine is processing a significant quantity of data. This is particularly true in molecular biology with the advantage of next-generation sequencing (NGS) for profiling and identifying molecular tumors and their treatment. In addition to bioinformatics pipelines, artificial intelligence (AI) can be valuable in helping to analyze mutation variants. Generating sequencing data from patient DNA samples has become easy to perform in clinical trials. However, analyzing the massive quantities of genomic or transcriptomic data and extracting the key biomarkers associated with a clinical response to a specific therapy requires a formidable combination of scientific expertise, biomolecular skills and a panel of bioinformatic and biostatistic tools, in which artificial intelligence is now successful in developing future routine diagnostics. However, cancer genome complexity and technical artifacts make identifying real variants challenging. We present a machine learning method for classifying pathogenic single nucleotide variants (SNVs), single nucleotide polymorphisms (SNPs), multiple nucleotide variants (MNVs), insertions, and deletions detected by NGS from different types of tumor specimens, such as: colorectal, melanoma, lung and glioma cancer. We compared our NGS data to different machine learning algorithms using the k-fold cross-validation method and to neural networks (deep learning) to measure the performance of the different ML algorithms and determine which one is a valid model for confirming NGS variant calls in cancer diagnosis. We trained our machine learning with 70{\%} of our data samples, extracted from our local database (our data structure had 7 parameters: chromosome, position, exon, variant allele frequency, minor allele frequency, coverage and protein description) and validated it with the 30{\%} remaining data. The model offering the best accuracy was chosen and implemented in the NGS analysis routine. Artificial intelligence was developed with the R script language version 3.6.0. We trained our model on 70{\%} of 102,011 variants. Our best error rate (0.22{\%}) was found with random forest machine learning (ntree = 500 and mtry = 4), with an AUC of 0.99. Neural networks achieved some good scores. The final trained model with the neural network achieved an accuracy of 98{\%} and an ROC-AUC of 0.99 with validation data. We tested our RF model to interpret more than 2000 variants from our NGS database: 20 variants were misclassified (error rate < 1{\%}). The errors were nomenclature problems and false positives. After adding false positives to our training database and implementing our RF model routinely, our error rate was always < 0.5{\%}. The RF model shows excellent results for oncosomatic NGS interpretation and can easily be implemented in other molecular biology laboratories. AI is becoming increasingly important in molecular biomedical analysis and can be very helpful in processing medical data. Neural networks show a good capacity in variant classification, and in the future, they may be useful in predicting more complex variants.},
issn={2045-2322},
doi={10.1038/s41598-021-01253-y},
url={https://doi.org/10.1038/s41598-021-01253-y}
}

﻿@Article{Kapsiani2021ageing,
author={Kapsiani, Sofia
and Howlin, Brendan J.},
title={Random forest classification for predicting lifespan-extending chemical compounds},
journal={Scientific Reports},
year={2021},
month={Jul},
day={05},
volume={11},
number={1},
pages={13812},
abstract={Ageing is a major risk factor for many conditions including cancer, cardiovascular and neurodegenerative diseases. Pharmaceutical interventions that slow down ageing and delay the onset of age-related diseases are a growing research area. The aim of this study was to build a machine learning model based on the data of the DrugAge database to predict whether a chemical compound will extend the lifespan of Caenorhabditis elegans. Five predictive models were built using the random forest algorithm with molecular fingerprints and/or molecular descriptors as features. The best performing classifier, built using molecular descriptors, achieved an area under the curve score (AUC) of 0.815 for classifying the compounds in the test set. The features of the model were ranked using the Gini importance measure of the random forest algorithm. The top 30 features included descriptors related to atom and bond counts, topological and partial charge properties. The model was applied to predict the class of compounds in an external database, consisting of 1738 small-molecules. The chemical compounds of the screening database with a predictive probability of{\thinspace}≥{\thinspace}0.80 for increasing the lifespan of Caenorhabditis elegans were broadly separated into (1) flavonoids, (2) fatty acids and conjugates, and (3) organooxygen compounds.},
issn={2045-2322},
doi={10.1038/s41598-021-93070-6},
url={https://doi.org/10.1038/s41598-021-93070-6}
}

﻿@Article{Wang2023rfheartdisease,
author={Wang, Jing
and Rao, Congjun
and Goh, Mark
and Xiao, Xinping},
title={Risk assessment of coronary heart disease based on cloud-random forest},
journal={Artificial Intelligence Review},
year={2023},
month={Jan},
day={01},
volume={56},
number={1},
pages={203-232},
abstract={Coronary heart disease (CHD) is a major public health problem affecting a nation's economic and social development. Risk assessing CHD in a timely manner helps to stop, reverse, and reduce the spread of many chronic diseases and health hazards. This paper proposes a cloud-random forest (C-RF) model combining cloud model and random forest to assess the risk of CHD. In this model, based on the traditional classification and regression trees (CART), a weight determining algorithm based on the cloud model and decision-making trial and evaluation laboratory is applied to obtain the weights of the evaluation attributes. The attribute weight and the gain value of the smallest Gini coefficient corresponding to the same attribute are weighted and summed. The weighted sum is then used to replace the original gain value. This value rule is used as a new CART node split criterion to construct a new decision tree, thus forming a new random forest, namely, the C-RF. The Framingham dataset of the Kaggle platform is the research sample for the empirical analysis. Comparing the C-RF model with CART, support vector machine (SVM), convolutional neural network (CNN), and random forest (RF) using standard performance evaluation indexes such as accuracy, error rates, ROC curve and AUC value. The result shows that the classification accuracy of the C-RF model is 85{\%}, which is improved by 8, 9, 4 and 3{\%} respectively compared with CART, SVM, CNN and RF. The error rate of the first type is 13.99{\%}, which is 6.99, 7.44, 4.47 and 3.02{\%} lower than CART, SVM, CNN and RF respectively. The AUC value is 0.85, which is also higher than other comparison models. Thus, the C-RF model is more superior on classification performance and classification effect in the risk assessment of CHD.},
issn={1573-7462},
doi={10.1007/s10462-022-10170-z},
url={https://doi.org/10.1007/s10462-022-10170-z}
}

@InProceedings{palimkar2022rfdiabetes,
author="Palimkar, Prajyot
and Shaw, Rabindra Nath
and Ghosh, Ankush",
editor="Bianchini, Monica
and Piuri, Vincenzo
and Das, Sanjoy
and Shaw, Rabindra Nath",
title="Machine Learning Technique to Prognosis Diabetes Disease: Random Forest Classifier Approach",
booktitle="Advanced Computing and Intelligent Technologies",
year="2022",
publisher="Springer Singapore",
address="Singapore",
pages="219--244",
abstract="Diabetes is one among many chronic diseases. It is the most common disease and lots of peoples are affected by this. There are many things that are liable for diabetes, mainly age, obesity, weakness, sudden weight loss, and many more. Diabetes patients have high risk of diseases like cardiopathy, renal disorder, stroke, nerve damage, eye damage, etc. Detection of the disease isn't very easy and prediction is additionally costlier. In today's situation, hospitals are extremely busy due to COVID-19 pandemic, and it might be revolutionary if one could know if they're at risk of being diabetic without visiting a doctor. But the rise in Artificial Intelligence techniques can be used for disease prognosis. The objective of this study is to develop a model with significant accuracy to diagnose diabetes in patients. Moreover, this paper also presents an effective diabetes prediction model for better classification of diabetes and to enhance the accuracy in diabetes prediction using several machine learning algorithms. Different machine learning algorithms are utilized for early stage diabetes prediction, namely, Logistic Regression, Random Forest Classifier, Support Vector Machine, Decision Trees, K-Nearest Neighbors, Gaussian Process Classifier, AdaBoost Classifier, and Gaussian Na{\"i}ve Bayes. The performances of these models are measured on respective criteria like Accuracy, Precision, Recall, F-Measure, and Error. For this research work, latest available dataset dated 22nd July, 2020, is being utilized. Latest updated dataset will show comparatively better result.",
isbn="978-981-16-2164-2"
}

@incollection{Hastie2009RandomForest,
    author="Hastie, T.
    and Tibshirani, R.
    and Friedman, J.",
    title="Random Forests",
    booktitle="The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
    year="2009",
    edition="2nd ed.",
    publisher="Springer New York",
    address="New York, NY",
    pages="587--604",
    isbn="978-0-387-84858-7",
    doi="10.1007/978-0-387-84858-7_15"
}

@ARTICLE{zhao2022tif1,
  
AUTHOR={Zhao, Lijuan and Xie, Shuoshan and Zhou, Bin and Shen, Chuyu and Li, Liya and Pi, Weiwei and Gong, Zhen and Zhao, Jing and Peng, Qi and Zhou, Junyu and Peng, Jiaqi and Zhou, Yan and Zou, Lingxiao and Song, Liang and Zhu, Honglin and Luo, Hui},   
	 
TITLE={Machine Learning Algorithms Identify Clinical Subtypes and Cancer in Anti-TIF1$\gamma$+ Myositis: A Longitudinal Study of 87 Patients},      
	
JOURNAL={Frontiers in Immunology},      
	
VOLUME={13},           
	
YEAR={2022},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fimmu.2022.802499},       
	
DOI={10.3389/fimmu.2022.802499},      
	
ISSN={1664-3224},   
   
}

@article{dexter2018stress,
author = {Dexter, Eric and Rollwagen-Bollens, Gretchen and Bollens, Stephen M.},
title = {The trouble with stress: A flexible method for the evaluation of nonmetric multidimensional scaling},
journal = {Limnology and Oceanography: Methods},
volume = {16},
number = {7},
pages = {434-443},
doi = {https://doi.org/10.1002/lom3.10257},
url = {https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.1002/lom3.10257},
eprint = {https://aslopubs.onlinelibrary.wiley.com/doi/pdf/10.1002/lom3.10257},
abstract = {Abstract Nonmetric multidimensional scaling (NMDS) is a powerful statistical tool which enables complex multivariate data sets to be visualized in a reduced number of dimensions. Users typically evaluate the fit of an NMDS ordination via ordination “stress” (i.e., data distortion) against a commonly accepted set of heuristic guidelines. However, these guidelines do not account for the mathematical relationship which links ordination stress to sample size. Consequently, researchers working with large data sets may unnecessarily present ordinations in an intractable number of dimensions, subdivide their data, or forego the use of NMDS entirely and lose the benefits of this highly flexible and useful technique. In order to overcome the limitations of these practices, we advocate for an alternative approach to the evaluation of NMDS ordination fit via the usage of permutation-based ecological null models. We present the rationale for this approach from a theoretical basis, supported by a brief literature review, and an example usage of the methodology. Our literature review shows that NMDS analyses often far exceed the number of observations under which the original stress guidelines were formulated—with a significant increasing trend in recent decades. Adoption of a permutation-based approach will consequently provide a more flexible and quantitative evaluation of NMDS fit and allow for the continued application of NMDS in an era of increasingly large datasets.},
year = {2018}
}

@ARTICLE{Zhang2010baseline,
  title    = "Baseline correction using adaptive iteratively reweighted
              penalized least squares",
  author   = "Zhang, Zhi-Min and Chen, Shan and Liang, Yi-Zeng",
  abstract = "Baseline drift always blurs or even swamps signals and
              deteriorates analytical results, particularly in multivariate
              analysis. It is necessary to correct baseline drift to perform
              further data analysis. Simple or modified polynomial fitting has
              been found to be effective to some extent. However, this method
              requires user intervention and is prone to variability especially
              in low signal-to-noise ratio environments. A novel algorithm
              named adaptive iteratively reweighted Penalized Least Squares
              (airPLS) that does not require any user intervention and prior
              information, such as peak detection etc., is proposed in this
              work. The method works by iteratively changing weights of sum
              squares errors (SSE) between the fitted baseline and original
              signals, and the weights of the SSE are obtained adaptively using
              the difference between the previously fitted baseline and the
              original signals. The baseline estimator is fast and flexible.
              Theory, implementation, and applications in simulated and real
              datasets are presented. The algorithm is implemented in R
              language and MATLAB, which is available as open source software
              (http://code.google.com/p/airpls).",
  journal  = "Analyst",
  volume   =  135,
  number   =  5,
  pages    = "1138--1146",
  month    =  feb,
  year     =  2010,
  address  = "England",
  language = "en"
}

@article{cathignol2024magnetoencephalography,
  title={Magnetoencephalography dimensionality reduction informed by dynamic brain states},
  author={Cathignol, Annie Esther and Kusch, Lionel and Angiolelli, Marianna and Troisi Lopez, Emahnuel and Polverino, Arianna and Romano, Antonella and Sorrentino, Giuseppe and Jirsa, Viktor and Rabuffo, Giovanni and Sorrentino, Pierpaolo},
  journal={bioRxiv},
  pages={2024--08},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}

@Article{smith2016raman,
author ="Smith, Rachael and Wright, Karen L. and Ashton, Lorna",
title  ="Raman spectroscopy: an evolving technique for live cell studies",
journal  ="Analyst",
year  ="2016",
volume  ="141",
issue  ="12",
pages  ="3590-3600",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/C6AN00152A",
url  ="http://dx.doi.org/10.1039/C6AN00152A",
abstract  ="One of the most exciting developments in Raman spectroscopy in the last decade has been its application to cells and tissues for diagnostic and pharmaceutical applications{,} and in particular its use in the analysis of cellular dynamics. Raman spectroscopy is rapidly advancing as a cell imaging method that overcomes many of the limitations of current techniques and is earning its place as a routine tool in cell biology. In this review we focus on important developments in Raman spectroscopy that have evolved into the exciting technique of live-cell Raman microscopy and highlight some of the most recent and significant applications to cell biology."}



@ARTICLE{Zhang2019res,
	author = {Zhang, Wei and Li, Qifei and Tang, Mingjie and Zhang, Han and Sun, Xiaoping and Zou, Sige and Jensen, Judy L. and Liou, Theodore G. and Zhou, Anhong},
	title = {A multi-scale approach to study biochemical and biophysical aspects of resveratrol on diesel exhaust particle-human primary lung cell interaction},
	year = {2019},
	journal = {Scientific Reports},
	volume = {9},
	number = {1},
	doi = {10.1038/s41598-019-54552-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076033623&doi=10.1038%2fs41598-019-54552-w&partnerID=40&md5=d3026dc241b2b8a10b5f324f2895b7ff},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access, Green Open Access}
}


@ARTICLE{huang2013cytotoxin,
	author = {Huang, Hao and Shi, Hong and Feng, Shangyuan and Chen, Weiwei and Yu, Yun and Lin, Duo and Chen, Rong},
	title = {Confocal Raman spectroscopic analysis of the cytotoxic response to cisplatin in nasopharyngeal carcinoma cells},
	year = {2013},
	journal = {Analytical Methods},
	volume = {5},
	number = {1},
	pages = {260 – 266},
	doi = {10.1039/c2ay25684c},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870936974&doi=10.1039%2fc2ay25684c&partnerID=40&md5=d94ca5753fcf4a7d3d0e87629f729097},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 39}
}

@ARTICLE{Atalay2019cbd,
  title    = "Antioxidative and {Anti-Inflammatory} Properties of Cannabidiol",
  author   = "Atalay, Sinemyiz and Jarocka-Karpowicz, Iwona and Skrzydlewska,
              Elzbieta",
  abstract = "Cannabidiol (CBD) is one of the main pharmacologically active
              phytocannabinoids of Cannabis sativa L. CBD is non-psychoactive
              but exerts a number of beneficial pharmacological effects,
              including anti-inflammatory and antioxidant properties. The
              chemistry and pharmacology of CBD, as well as various molecular
              targets, including cannabinoid receptors and other components of
              the endocannabinoid system with which it interacts, have been
              extensively studied. In addition, preclinical and clinical
              studies have contributed to our understanding of the therapeutic
              potential of CBD for many diseases, including diseases associated
              with oxidative stress. Here, we review the main biological
              effects of CBD, and its synthetic derivatives, focusing on the
              cellular, antioxidant, and anti-inflammatory properties of CBD.",
  journal  = "Antioxidants (Basel)",
  volume   =  9,
  number   =  1,
  month    =  dec,
  year     =  2019,
  address  = "Switzerland",
  keywords = "cannabidiol; cannabidiol synthetic derivatives; endocannabinoids;
              inflammation; lipid peroxidation; membrane receptors; oxidative
              stress",
  language = "en"
}

@ARTICLE{Renaud1992res-protect,
  title    = "Wine, alcohol, platelets, and the French paradox for coronary
              heart disease",
  author   = "Renaud, S and de Lorgeril, M",
  abstract = "In most countries, high intake of saturated fat is positively
              related to high mortality from coronary heart disease (CHD).
              However, the situation in France is paradoxical in that there is
              high intake of saturated fat but low mortality from CHD. This
              paradox may be attributable in part to high wine consumption.
              Epidemiological studies indicate that consumption of alcohol at
              the level of intake in France (20-30 g per day) can reduce risk
              of CHD by at least 40\%. Alcohol is believed to protect from CHD
              by preventing atherosclerosis through the action of
              high-density-lipoprotein cholesterol, but serum concentrations of
              this factor are no higher in France than in other countries.
              Re-examination of previous results suggests that, in the main,
              moderate alcohol intake does not prevent CHD through an effect on
              atherosclerosis, but rather through a haemostatic mechanism. Data
              from Caerphilly, Wales, show that platelet aggregation, which is
              related to CHD, is inhibited significantly by alcohol at levels
              of intake associated with reduced risk of CHD. Inhibition of
              platelet reactivity by wine (alcohol) may be one explanation for
              protection from CHD in France, since pilot studies have shown
              that platelet reactivity is lower in France than in Scotland.",
  journal  = "Lancet",
  volume   =  339,
  number   =  8808,
  pages    = "1523--1526",
  month    =  jun,
  year     =  1992,
  address  = "England",
  language = "en"
}

﻿@Article{Kuchroo2022multiscalePHATE,
author={Kuchroo, Manik
and Huang, Jessie
and Wong, Patrick
and Grenier, Jean-Christophe
and Shung, Dennis
and Tong, Alexander
and Lucas, Carolina
and Klein, Jon
and Burkhardt, Daniel B.
and Gigante, Scott
and Godavarthi, Abhinav
and Rieck, Bastian
and Israelow, Benjamin
and Simonov, Michael
and Mao, Tianyang
and Oh, Ji Eun
and Silva, Julio
and Takahashi, Takehiro
and Odio, Camila D.
and Casanovas-Massana, Arnau
and Fournier, John
and Obaid, Abeer
and Moore, Adam
and Lu-Culligan, Alice
and Nelson, Allison
and Brito, Anderson
and Nunez, Angela
and Martin, Anjelica
and Wyllie, Anne L.
and Watkins, Annie
and Park, Annsea
and Venkataraman, Arvind
and Geng, Bertie
and Kalinich, Chaney
and Vogels, Chantal B. F.
and Harden, Christina
and Todeasa, Codruta
and Jensen, Cole
and Kim, Daniel
and McDonald, David
and Shepard, Denise
and Courchaine, Edward
and White, Elizabeth B.
and Song, Eric
and Silva, Erin
and Kudo, Eriko
and DeIuliis, Giuseppe
and Wang, Haowei
and Rahming, Harold
and Park, Hong-Jai
and Matos, Irene
and Ott, Isabel M.
and Nouws, Jessica
and Valdez, Jordan
and Fauver, Joseph
and Lim, Joseph
and Rose, Kadi-Ann
and Anastasio, Kelly
and Brower, Kristina
and Glick, Laura
and Sharma, Lokesh
and Sewanan, Lorenzo
and Knaggs, Lynda
and Minasyan, Maksym
and Batsu, Maria
and Tokuyama, Maria
and Muenker, M. Cate
and Petrone, Mary
and Kuang, Maxine
and Nakahata, Maura
and Campbell, Melissa
and Linehan, Melissa
and Askenase, Michael H.
and Smolgovsky, Mikhail
and Grubaugh, Nathan D.
and Sonnert, Nicole
and Naushad, Nida
and Vijayakumar, Pavithra
and Lu, Peiwen
and Earnest, Rebecca
and Martinello, Rick
and Herbst, Roy
and Datta, Rupak
and Handoko, Ryan
and Bermejo, Santos
and Lapidus, Sarah
and Prophet, Sarah
and Bickerton, Sean
and Velazquez, Sofia
and Mohanty, Subhasis
and Alpert, Tara
and Rice, Tyler
and Schulz, Wade
and Khoury-Hanold, William
and Peng, Xiaohua
and Yang, Yexin
and Cao, Yiyun
and Strong, Yvette
and Farhadian, Shelli
and Dela Cruz, Charles S.
and Ko, Albert I.
and Hirn, Matthew J.
and Wilson, F. Perry
and Hussin, Julie G.
and Wolf, Guy
and Iwasaki, Akiko
and Krishnaswamy, Smita
and Team, Yale IMPACT},
title={Multiscale PHATE identifies multimodal signatures of COVID-19},
journal={Nature Biotechnology},
year={2022},
month={May},
day={01},
volume={40},
number={5},
pages={681-691},
abstract={As the biomedical community produces datasets that are increasingly complex and high dimensional, there is a need for more sophisticated computational tools to extract biological insights. We present Multiscale PHATE, a method that sweeps through all levels of data granularity to learn abstracted biological features directly predictive of disease outcome. Built on a coarse-graining process called diffusion condensation, Multiscale PHATE learns a data topology that can be analyzed at coarse resolutions for high-level summarizations of data and at fine resolutions for detailed representations of subsets. We apply Multiscale PHATE to a coronavirus disease 2019 (COVID-19) dataset with 54 million cells from 168 hospitalized patients and find that patients who die show CD16hiCD66blo neutrophil and IFN-$\gamma$+ granzyme B+ Th17 cell responses. We also show that population groupings from Multiscale PHATE directly fed into a classifier predict disease outcome more accurately than naive featurizations of the data. Multiscale PHATE is broadly generalizable to different data types, including flow cytometry, single-cell RNA sequencing (scRNA-seq), single-cell sequencing assay for transposase-accessible chromatin (scATAC-seq), and clinical variables.},
issn={1546-1696},
doi={10.1038/s41587-021-01186-x},
url={https://doi.org/10.1038/s41587-021-01186-x}
}

@Article{Goldberg2009localprocrustes,
author={Goldberg, Yair
and Ritov, Ya'acov},
title={Local procrustes for manifold embedding: a measure of embedding quality and embedding algorithms},
journal={Machine Learning},
year={2009},
month={Oct},
day={01},
volume={77},
number={1},
pages={1-25},
abstract={We present the Procrustes measure, a novel measure based on Procrustes rotation that enables quantitative comparison of the output of manifold-based embedding algorithms such as LLE (Roweis and Saul, Science 290(5500), 2323--2326, 2000) and Isomap (Tenenbaum et al., Science 290(5500), 2319--2323, 2000). The measure also serves as a natural tool when choosing dimension-reduction parameters. We also present two novel dimension-reduction techniques that attempt to minimize the suggested measure, and compare the results of these techniques to the results of existing algorithms. Finally, we suggest a simple iterative method that can be used to improve the output of existing algorithms.},
issn={1573-0565},
doi={10.1007/s10994-009-5107-9},
url={https://doi.org/10.1007/s10994-009-5107-9}
}

@inproceedings{
    rhodes2023supervised,
    title={Supervised Manifold Learning via Random Forest Geometry-Preserving Proximities},
    author={Jake Slater Rhodes},
    booktitle={Fourteenth International Conference on Sampling Theory and Applications},
    year={2023},
    url={https://openreview.net/forum?id=t6E4dZjp-e}
}

@article{brunet2021integrated,
  title={Integrated immunovirological profiling validates plasma SARS-CoV-2 RNA as an early predictor of COVID-19 mortality},
  author={Brunet-Ratnasingham, Elsa and Anand, Sai Priya and Gantner, Pierre and Dyachenko, Alina and Moquin-Beaudry, Ga{\"e}l and Brassard, Nathalie and Beaudoin-Bussi{\`e}res, Guillaume and Pagliuzza, Am{\'e}lie and Gasser, Romain and Benlarbi, Mehdi and others},
  journal={Science Advances},
  volume={7},
  number={48},
  pages={eabj5629},
  year={2021},
  publisher={American Association for the Advancement of Science}
}

@article{brunet2023sustained,
  title={Sustained IFN signaling is associated with delayed development of SARS-CoV-2-specific immunity},
  author={Brunet-Ratnasingham, Elsa and Morin, Sacha and Randolph, Haley and Labrecque, Marjorie and Belair, Justin and Lima-Barbosa, Raphael and Pagliuzza, Amelie and Marchitto, Lorie and Hultstrom, Michael and Niessl, Julia and others},
  journal={medRxiv},
  pages={2023--06},
  year={2023},
  publisher={Cold Spring Harbor Laboratory Press}
}

@article{fajnzylber2020CoV2vRNA,
  title={SARS-CoV-2 viral load is associated with increased disease severity and mortality.},
  author={Fajnzylber, J and Regan, J and Coxen, K and Corry, H and Wong, C and Rosenthal, A and Worrall, D and Giguel, F and Piechocka-Trocha, A and Atyeo, C and others},
  journal={Nat Commun},
  pages={5493},
  year={2020},
  publisher={Nature Portfolio}
}

@article{bqc19,
  title={The Biobanque québécoise de la COVID-19 (BQC19)-A cohort to prospectively study the clinical and biological determinants of COVID-19 clinical trajectories.},
  author={Tremblay, K and Rousseau, S and Zawati,  MH and Auld, D and Chassé, M and Coderre, D and Falcone, EL and Gauthier, N and Grandvaux, N and Gros-Louis, F and others},
  journal={PLoS One},
  pages={16(5):e0245031},
  year={2021},
  publisher={PLOS}
}

@article{prevostCoV2,
  title={Cross-Sectional Evaluation of Humoral Responses against SARS-CoV-2 Spike.},
  author={Prévost,  J and Gasser,  R and  Beaudoin-Bussières,  G and Richard, J abd Duerr, R and Laumaea, A and Anand,  SP and Goyette, G and Benlarbi,  M and Ding,  S and Medjahed and others},
  journal={Cell Rep Med},
  pages={1(7):100126},
  year={2020},
  publisher={Cell Press}
}


@misc{ghojogh2022kspca,
      title={Unsupervised and Supervised Principal Component Analysis: Tutorial}, 
      author={Benyamin Ghojogh and Mark Crowley},
      year={2022},
      eprint={1906.03148},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

﻿@Article{Weigert2018content-aware,
    author={Weigert, Martin
    and Schmidt, Uwe
    and Boothe, Tobias
    and M{\"u}ller, Andreas
    and Dibrov, Alexandr
    and Jain, Akanksha
    and Wilhelm, Benjamin
    and Schmidt, Deborah
    and Broaddus, Coleman
    and Culley, Si{\^a}n
    and Rocha-Martins, Mauricio
    and Segovia-Miranda, Fabi{\'a}n
    and Norden, Caren
    and Henriques, Ricardo
    and Zerial, Marino
    and Solimena, Michele
    and Rink, Jochen
    and Tomancak, Pavel
    and Royer, Loic
    and Jug, Florian
    and Myers, Eugene W.},
    title={Content-aware image restoration: pushing the limits of fluorescence microscopy},
    journal={Nature Methods},
    year={2018},
    month={Dec},
    day={01},
    volume={15},
    number={12},
    pages={1090-1097},
    abstract={Fluorescence microscopy is a key driver of discoveries in the life sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how content-aware image restoration based on deep learning extends the range of biological phenomena observable by microscopy. We demonstrate on eight concrete examples how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how near isotropic resolution can be achieved with up to tenfold under-sampling along the axial direction, and how tubular and granular structures smaller than the diffraction limit can be resolved at 20-times-higher frame rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software in Python, FIJI, and KNIME.},
    issn={1548-7105},
    doi={10.1038/s41592-018-0216-7},
    url={https://doi.org/10.1038/s41592-018-0216-7}
}


﻿@Article{Qiao2023rdl,
    author={Qiao, Chang
    and Li, Di
    and Liu, Yong
    and Zhang, Siwei
    and Liu, Kan
    and Liu, Chong
    and Guo, Yuting
    and Jiang, Tao
    and Fang, Chuyu
    and Li, Nan
    and Zeng, Yunmin
    and He, Kangmin
    and Zhu, Xueliang
    and Lippincott-Schwartz, Jennifer
    and Dai, Qionghai
    and Li, Dong},
    title={Rationalized deep learning super-resolution microscopy for sustained live imaging of rapid subcellular processes},
    journal={Nature Biotechnology},
    year={2023},
    month={Mar},
    day={01},
    volume={41},
    number={3},
    pages={367-377},
    abstract={The goal when imaging bioprocesses with optical microscopy is to acquire the most spatiotemporal information with the least invasiveness. Deep neural networks have substantially improved optical microscopy, including image super-resolution and restoration, but still have substantial potential for artifacts. In this study, we developed rationalized deep learning (rDL) for structured illumination microscopy and lattice light sheet microscopy (LLSM) by incorporating prior knowledge of illumination patterns and, thereby, rationally guiding the network to denoise raw images. Here we demonstrate that rDL structured illumination microscopy eliminates spectral bias-induced resolution degradation and reduces model uncertainty by five-fold, improving the super-resolution information by more than ten-fold over other computational approaches. Moreover, rDL applied to LLSM enables self-supervised training by using the spatial or temporal continuity of noisy data itself, yielding results similar to those of supervised methods. We demonstrate the utility of rDL by imaging the rapid kinetics of motile cilia, nucleolar protein condensation during light-sensitive mitosis and long-term interactions between membranous and membrane-less organelles.},
    issn={1546-1696},
    doi={10.1038/s41587-022-01471-3},
    url={https://doi.org/10.1038/s41587-022-01471-3}
}

﻿@Article{Kim2024deepcas9,
    author={Kim, Nahye
    and Choi, Sungchul
    and Kim, Sungjae
    and Song, Myungjae
    and Seo, Jung Hwa
    and Min, Seonwoo
    and Park, Jinman
    and Cho, Sung-Rae
    and Kim, Hyongbum Henry},
    title={Deep learning models to predict the editing efficiencies and outcomes of diverse base editors},
    journal={Nature Biotechnology},
    year={2024},
    month={Mar},
    day={01},
    volume={42},
    number={3},
    pages={484-497},
    abstract={Applications of base editing are frequently restricted by the requirement for a protospacer adjacent motif (PAM), and selecting the optimal base editor (BE) and single-guide RNA pair (sgRNA) for a given target can be difficult. To select for BEs and sgRNAs without extensive experimental work, we systematically compared the editing windows, outcomes and preferred motifs for seven BEs, including two cytosine BEs, two adenine BEs and three C{\textbullet}G to G{\textbullet}C BEs at thousands of target sequences. We also evaluated nine Cas9 variants that recognize different PAM sequences and developed a deep learning model, DeepCas9variants, for predicting which variants function most efficiently at sites with a given target sequence. We then develop a computational model, DeepBE, that predicts editing efficiencies and outcomes of 63 BEs that were generated by incorporating nine Cas9 variants as nickase domains into the seven BE variants. The predicted median efficiencies of BEs with DeepBE-based design were 2.9- to 20-fold higher than those of rationally designed SpCas9-containing BEs.},
    issn={1546-1696},
    doi={10.1038/s41587-023-01792-x},
    url={https://doi.org/10.1038/s41587-023-01792-x}
}


﻿@Article{Mathis2023prime,
    author={Mathis, Nicolas
    and Allam, Ahmed
    and Kissling, Lucas
    and Marquart, Kim Fabiano
    and Schmidheini, Lukas
    and Solari, Cristina
    and Bal{\'a}zs, Zsolt
    and Krauthammer, Michael
    and Schwank, Gerald},
    title={Predicting prime editing efficiency and product purity by deep learning},
    journal={Nature Biotechnology},
    year={2023},
    month={Aug},
    day={01},
    volume={41},
    number={8},
    pages={1151-1159},
    abstract={Prime editing is a versatile genome editing tool but requires experimental optimization of the prime editing guide RNA (pegRNA) to achieve high editing efficiency. Here we conducted a high-throughput screen to analyze prime editing outcomes of 92,423 pegRNAs on a highly diverse set of 13,349 human pathogenic mutations that include base substitutions, insertions and deletions. Based on this dataset, we identified sequence context features that influence prime editing and trained PRIDICT (prime editing guide prediction), an attention-based bidirectional recurrent neural network. PRIDICT reliably predicts editing rates for all small-sized genetic changes with a Spearman's R of 0.85 and 0.78 for intended and unintended edits, respectively. We validated PRIDICT on endogenous editing sites as well as an external dataset and showed that pegRNAs with high (>70) versus low (<70) PRIDICT scores showed substantially increased prime editing efficiencies in different cell types in vitro (12-fold) and in hepatocytes in vivo (tenfold), highlighting the value of PRIDICT for basic and for translational research applications.},
    issn={1546-1696},
    doi={10.1038/s41587-022-01613-7},
    url={https://doi.org/10.1038/s41587-022-01613-7}
}


@ARTICLE{Brugnone2020condensation,
    title    = "Coarse Graining of Data via Inhomogeneous Diffusion Condensation",
    author   = "Brugnone, Nathan and Gonopolskiy, Alex and Moyle, Mark W and
              Kuchroo, Manik and van Dijk, David and Moon, Kevin R and
              Colon-Ramos, Daniel and Wolf, Guy and Hirn, Matthew J and
              Krishnaswamy, Smita",
    abstract = "Big data often has emergent structure that exists at multiple
              levels of abstraction, which are useful for characterizing
              complex interactions and dynamics of the observations. Here, we
              consider multiple levels of abstraction via a multiresolution
              geometry of data points at different granularities. To construct
              this geometry we define a time-inhomogemeous diffusion process
              that effectively condenses data points together to uncover nested
              groupings at larger and larger granularities. This inhomogeneous
              process creates a deep cascade of intrinsic low pass filters on
              the data affinity graph that are applied in sequence to gradually
              eliminate local variability while adjusting the learned data
              geometry to increasingly coarser resolutions. We provide
              visualizations to exhibit our method as a
              ``continuously-hierarchical'' clustering with directions of
              eliminated variation highlighted at each step. The utility of our
              algorithm is demonstrated via neuronal data condensation, where
              the constructed multiresolution data geometry uncovers the
              organization, grouping, and connectivity between neurons.",
    journal  = "Proc IEEE Int Conf Big Data",
    volume   =  2019,
    pages    = "2624--2633",
    month    =  feb,
    year     =  2020,
    address  = "United States",
    keywords = "diffusion; graph signal processing; hierarchical clustering;
              manifold learning",
    language = "en"
}


@ARTICLE{Levine2015phenograph,
    title    = "{Data-Driven} Phenotypic Dissection of {AML} Reveals
              Progenitor-like Cells that Correlate with Prognosis",
    author   = "Levine, Jacob H and Simonds, Erin F and Bendall, Sean C and
              Davis, Kara L and Amir, El-Ad D and Tadmor, Michelle D and
              Litvin, Oren and Fienberg, Harris G and Jager, Astraea and
              Zunder, Eli R and Finck, Rachel and Gedman, Amanda L and Radtke,
              Ina and Downing, James R and Pe'er, Dana and Nolan, Garry P",
    abstract = "Acute myeloid leukemia (AML) manifests as phenotypically and
              functionally diverse cells, often within the same patient.
              Intratumor phenotypic and functional heterogeneity have been
              linked primarily by physical sorting experiments, which assume
              that functionally distinct subpopulations can be prospectively
              isolated by surface phenotypes. This assumption has proven
              problematic, and we therefore developed a data-driven approach.
              Using mass cytometry, we profiled surface and intracellular
              signaling proteins simultaneously in millions of healthy and
              leukemic cells. We developed PhenoGraph, which algorithmically
              defines phenotypes in high-dimensional single-cell data.
              PhenoGraph revealed that the surface phenotypes of leukemic
              blasts do not necessarily reflect their intracellular state.
              Using hematopoietic progenitors, we defined a signaling-based
              measure of cellular phenotype, which led to isolation of a gene
              expression signature that was predictive of survival in
              independent cohorts. This study presents new methods for
              large-scale analysis of single-cell heterogeneity and
              demonstrates their utility, yielding insights into AML
              pathophysiology.",
    journal  = "Cell",
    volume   =  162,
    number   =  1,
    pages    = "184--197",
    month    =  jun,
    year     =  2015,
    address  = "United States",
    language = "en"
}



@article{blondel2008fast,
    title={Fast unfolding of communities in large networks},
    author={Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
    journal={Journal of statistical mechanics: theory and experiment},
    volume={2008},
    number={10},
    pages={P10008},
    year={2008},
    publisher={IOP Publishing}
}

﻿@Article{Nissen2021vamb,
    author={Nissen, Jakob Nybo
    and Johansen, Joachim
    and Alles{\o}e, Rosa Lundbye
    and S{\o}nderby, Casper Kaae
    and Armenteros, Jose Juan Almagro
    and Gr{\o}nbech, Christopher Heje
    and Jensen, Lars Juhl
    and Nielsen, Henrik Bj{\o}rn
    and Petersen, Thomas Nordahl
    and Winther, Ole
    and Rasmussen, Simon},
    title={Improved metagenome binning and assembly using deep variational autoencoders},
    journal={Nature Biotechnology},
    year={2021},
    month={May},
    day={01},
    volume={39},
    number={5},
    pages={555-560},
    abstract={Despite recent advances in metagenomic binning, reconstruction of microbial species from metagenomics data remains challenging. Here we develop variational autoencoders for metagenomic binning (VAMB), a program that uses deep variational autoencoders to encode sequence coabundance and k-mer distribution information before clustering. We show that a variational autoencoder is able to integrate these two distinct data types without any previous knowledge of the datasets. VAMB outperforms existing state-of-the-art binners, reconstructing 29--98{\%} and 45{\%} more near-complete (NC) genomes on simulated and real data, respectively. Furthermore, VAMB is able to separate closely related strains up to 99.5{\%} average nucleotide identity (ANI), and reconstructed 255 and 91 NC Bacteroides vulgatus and Bacteroides dorei sample-specific genomes as two distinct clusters from a dataset of 1,000{\thinspace}human gut microbiome samples. We use 2,606{\thinspace}NC bins from this dataset to show that species of the human gut microbiome have different geographical distribution patterns. VAMB can be run on standard hardware and is freely available at https://github.com/RasmussenLab/vamb.},
    issn={1546-1696},
    doi={10.1038/s41587-020-00777-4},
    url={https://doi.org/10.1038/s41587-020-00777-4}
}

@article{lindenbaum2020multi-view-dm,
    title = {Multi-view diffusion maps},
    journal = {Information Fusion},
    volume = {55},
    pages = {127-149},
    year = {2020},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2019.08.005},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253518303877},
    author = {Ofir Lindenbaum and Arie Yeredor and Moshe Salhov and Amir Averbuch},
    keywords = {Dimensionality reduction, Manifold learning, Diffusion maps, Multi-view},
    abstract = {In this paper, we address the challenging task of achieving multi-view dimensionality reduction. The goal is to effectively use the availability of multiple views for extracting a coherent low-dimensional representation of the data. The proposed method exploits the intrinsic relation within each view, as well as the mutual relations between views. The multi-view dimensionality reduction is achieved by defining a cross-view model in which an implied random walk process is restrained to hop between objects in the different views. The method is robust to scaling and insensitive to small structural changes in the data. We define new diffusion distances and analyze the spectra of the proposed kernel. We show that the proposed framework is useful for various machine learning applications such as clustering, classification, and manifold learning. Finally, by fusing multi-sensor seismic data we present a method for automatic identification of seismic events.}
}

@inproceedings{delaporte2008introdm,
    title={An introduction to diffusion maps},
    author={Jacqueline Delaporte and B. M. Herbst and Willy A. Hereman and Van der Walt St{\'e}fan},
    year={2008},
    url={https://api.semanticscholar.org/CorpusID:18021541}
}

@article{haghverdi2015dm-singlecell,
    author = {Haghverdi, Laleh and Buettner, Florian and Theis, Fabian J.},
    title = "{Diffusion maps for high-dimensional single-cell analysis of differentiation data}",
    journal = {Bioinformatics},
    volume = {31},
    number = {18},
    pages = {2989-2998},
    year = {2015},
    month = {05},
    abstract = "{Motivation: Single-cell technologies have recently gained popularity in cellular differentiation studies regarding their ability to resolve potential heterogeneities in cell populations. Analyzing such high-dimensional single-cell data has its own statistical and computational challenges. Popular multivariate approaches are based on data normalization, followed by dimension reduction and clustering to identify subgroups. However, in the case of cellular differentiation, we would not expect clear clusters to be present but instead expect the cells to follow continuous branching lineages.Results: Here, we propose the use of diffusion maps to deal with the problem of defining differentiation trajectories. We adapt this method to single-cell data by adequate choice of kernel width and inclusion of uncertainties or missing measurement values, which enables the establishment of a pseudotemporal ordering of single cells in a high-dimensional gene expression space. We expect this output to reflect cell differentiation trajectories, where the data originates from intrinsic diffusion-like dynamics. Starting from a pluripotent stage, cells move smoothly within the transcriptional landscape towards more differentiated states with some stochasticity along their path. We demonstrate the robustness of our method with respect to extrinsic noise (e.g. measurement noise) and sampling density heterogeneities on simulated toy data as well as two single-cell quantitative polymerase chain reaction datasets (i.e. mouse haematopoietic stem cells and mouse embryonic stem cells) and an RNA-Seq data of human pre-implantation embryos. We show that diffusion maps perform considerably better than Principal Component Analysis and are advantageous over other techniques for non-linear dimension reduction such as t-distributed Stochastic Neighbour Embedding for preserving the global structures and pseudotemporal ordering of cells.Availability and implementation: The Matlab implementation of diffusion maps for single-cell data is available at https://www.helmholtz-muenchen.de/icb/single-cell-diffusion-map.Contact:  fbuettner.phys@gmail.com, fabian.theis@helmholtz-muenchen.deSupplementary information:  Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btv325},
    url = {https://doi.org/10.1093/bioinformatics/btv325},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/31/18/2989/49035225/bioinformatics\_31\_18\_2989.pdf},
}


@article{angerer2015destiny,
    author = {Angerer, Philipp and Haghverdi, Laleh and Büttner, Maren and Theis, Fabian J. and Marr, Carsten and Buettner, Florian},
    title = "{destiny: diffusion maps for large-scale single-cell data in R}",
    journal = {Bioinformatics},
    volume = {32},
    number = {8},
    pages = {1241-1243},
    year = {2015},
    month = {12},
    abstract = "{Summary: Diffusion maps are a spectral method for non-linear dimension reduction and have recently been adapted for the visualization of single-cell expression data. Here we present destiny, an efficient R implementation of the diffusion map algorithm. Our package includes a single-cell specific noise model allowing for missing and censored values. In contrast to previous implementations, we further present an efficient nearest-neighbour approximation that allows for the processing of hundreds of thousands of cells and a functionality for projecting new data on existing diffusion maps. We exemplarily apply destiny to a recent time-resolved mass cytometry dataset of cellular reprogramming.Availability and implementation: destiny is an open-source  R/Bioconductor package “bioconductor.org/packages/destiny” also available at www.helmholtz-muenchen.de/icb/destiny. A detailed vignette describing functions and workflows is provided with the package.Contact:  carsten.marr@helmholtz-muenchen.de or f.buettner@helmholtz-muenchen.deSupplementary information:  Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btv715},
    url = {https://doi.org/10.1093/bioinformatics/btv715},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/32/8/1241/49018680/bioinformatics\_32\_8\_1241.pdf},
}


﻿@Article{moignard2015decoding-reg-network,
    author={Moignard, Victoria
    and Woodhouse, Steven
    and Haghverdi, Laleh
    and Lilly, Andrew J.
    and Tanaka, Yosuke
    and Wilkinson, Adam C.
    and Buettner, Florian
    and Macaulay, Iain C.
    and Jawaid, Wajid
    and Diamanti, Evangelia
    and Nishikawa, Shin-Ichi
    and Piterman, Nir
    and Kouskoff, Valerie
    and Theis, Fabian J.
    and Fisher, Jasmin
    and G{\"o}ttgens, Berthold},
    title={Decoding the regulatory network of early blood development from single-cell gene expression measurements},
    journal={Nature Biotechnology},
    year={2015},
    month={Mar},
    day={01},
    volume={33},
    number={3},
    pages={269-276},
    abstract={An early stage in mouse blood development is reconstructed from gene expression data on thousands of single cells.},
    issn={1546-1696},
    doi={10.1038/nbt.3154},
    url={https://doi.org/10.1038/nbt.3154}
}


@INPROCEEDINGS{mishne2014multiscale-anomaly,
    author={Mishne, Gal and Cohen, Israel},
    booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
    title={Multiscale anomaly detection using diffusion maps and saliency score}, 
    year={2014},
    volume={},
    number={},
    pages={2823-2827},
    keywords={Sonar detection;Noise;Approximation methods;Noise measurement;Laplace equations;Image resolution;anomaly detection;diffusion maps;dimensionality reduction;multiscale representation;automated mine detection},
    doi={10.1109/ICASSP.2014.6854115}
}

﻿@Article{Schneider2023cebra,
    author={Schneider, Steffen
    and Lee, Jin Hwa
    and Mathis, Mackenzie Weygandt},
    title={Learnable latent embeddings for joint behavioural and neural analysis},
    journal={Nature},
    year={2023},
    month={May},
    day={01},
    volume={617},
    number={7960},
    pages={360-368},
    abstract={Mapping behavioural actions to neural activity is a fundamental goal of neuroscience. As our ability to record large neural and behavioural data increases, there is growing interest in modelling neural dynamics during adaptive behaviours to probe neural representations1--3. In particular, although neural latent embeddings can reveal underlying correlates of behaviour, we lack nonlinear techniques that can explicitly and flexibly leverage joint behaviour and neural data to uncover neural dynamics3--5. Here, we fill this gap with a new encoding method, CEBRA, that jointly uses behavioural and neural data in a (supervised) hypothesis- or (self-supervised) discovery-driven manner to produce both consistent and high-performance latent spaces. We show that consistency can be used as a metric for uncovering meaningful differences, and the inferred latents can be used for decoding. We validate its accuracy and demonstrate our tool's utility for both calcium and electrophysiology datasets, across sensory and motor tasks and in simple or complex behaviours across species. It allows leverage of single- and multi-session datasets for hypothesis testing or can be used label free. Lastly, we show that CEBRA can be used for the mapping of space, uncovering complex kinematic features, for the production of consistent latent spaces across two-photon and Neuropixels data, and can provide rapid, high-accuracy decoding of natural videos from visual cortex.},
    issn={1476-4687},
    doi={10.1038/s41586-023-06031-6},
    url={https://doi.org/10.1038/s41586-023-06031-6}
}

@article{wang2023stability,
    title={Stability of random forests and coverage of random-forest prediction intervals},
    author={Wang, Yan and Wu, Huaiqing and Nettleton, Dan},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    pages={31558--31569},
    year={2023}
}




Dimensionality reduction techniques can be used for either visualization or summarization of the underlying data topology. On the basis of other studies, PCA can be used for data summarization and t-SNE, UMAP and PHATE for more flexible visualization of scRNA-seq data5,48.

@article{heumos2023best,
  title={Best practices for single-cell analysis across modalities},
  author={Heumos, Lukas and Schaar, Anna C and Lance, Christopher and Litinetskaya, Anastasia and Drost, Felix and Zappia, Luke and L{\"u}cken, Malte D and Strobl, Daniel C and Henao, Juan and Curion, Fabiola and others},
  journal={Nature Reviews Genetics},
  volume={24},
  number={8},
  pages={550--572},
  year={2023},
  publisher={Nature Publishing Group UK London}
}



@article{acosta2022multimodal,
  title={Multimodal biomedical AI},
  author={Acosta, Juli{\'a}n N and Falcone, Guido J and Rajpurkar, Pranav and Topol, Eric J},
  journal={Nature Medicine},
  volume={28},
  number={9},
  pages={1773--1784},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


@article{greener2022guide,
  title={A guide to machine learning for biologists},
  author={Greener, Joe G and Kandathil, Shaun M and Moffat, Lewis and Jones, David T},
  journal={Nature reviews Molecular cell biology},
  volume={23},
  number={1},
  pages={40--55},
  year={2022},
  publisher={Nature Publishing Group UK London}
}









@article{baccin2020combined,
  title={Combined single-cell and spatial transcriptomics reveal the molecular, cellular and spatial bone marrow niche organization},
  author={Baccin, Chiara and Al-Sabah, Jude and Velten, Lars and Helbling, Patrick M and Gr{\"u}nschl{\"a}ger, Florian and Hern{\'a}ndez-Malmierca, Pablo and Nombela-Arrieta, C{\'e}sar and Steinmetz, Lars M and Trumpp, Andreas and Haas, Simon},
  journal={Nature cell biology},
  volume={22},
  number={1},
  pages={38--48},
  year={2020},
  publisher={Nature Publishing Group UK London}
}










@InProceedings{maaten2009parametric_tsne,
  title = 	 {Learning a Parametric Embedding by Preserving Local Structure},
  author = 	 {van der Maaten, Laurens},
  booktitle = 	 {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {384--391},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/maaten09a/maaten09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/maaten09a.html},
  abstract = 	 {The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space.  We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.}
}


@article{sainburg2021parametric_umap,
    author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
    title = {Parametric UMAP Embeddings for Representation and Semisupervised Learning},
    journal = {Neural Computation},
    volume = {33},
    number = {11},
    pages = {2881-2907},
    year = {2021},
    month = {10},
    abstract = {UMAP is a nonparametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) computing a graphical representation of a data set (fuzzy simplicial complex) and (2) through stochastic gradient descent, optimizing a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that parametric UMAP performs comparably to its nonparametric counterpart while conferring the benefit of a learned parametric mapping (e.g., fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semisupervised learning by capturing structure in unlabeled data.1},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01434},
    url = {https://doi.org/10.1162/neco\_a\_01434},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/11/2881/1966656/neco\_a\_01434.pdf},
}




@InProceedings{pontil2013multitask,
  title = 	 {Excess risk bounds for multitask learning with trace norm regularization},
  author = 	 {Pontil, Massimiliano and Maurer, Andreas},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {55--76},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v30/Pontil13.pdf},
  url = 	 {https://proceedings.mlr.press/v30/Pontil13.html},
  abstract = 	 {Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be infinite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments.}
}


@article{Maurer2006multitask, title={Bounds for Linear Multi-Task Learning}, volume={7}, number={5}, journal={Journal of Machine Learning Research}, author={Maurer, Andreas}, year={2006}, pages={117–139} }


@article{baxter2000inductive_bias,
author = {Baxter, Jonathan},
title = {A model of inductive bias learning},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {149–198},
numpages = {50}
}

@inproceedings{baxter1995learning,
author = {Baxter, Jonathan},
title = {Learning internal representations},
year = {1995},
isbn = {0897917235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/225298.225336},
doi = {10.1145/225298.225336},
booktitle = {Proceedings of the Eighth Annual Conference on Computational Learning Theory},
pages = {311–320},
numpages = {10},
location = {Santa Cruz, California, USA},
series = {COLT '95}
} 

@article{Caruana1997multitask, title={Multitask Learning}, volume={28}, ISSN={1573-0565}, DOI={10.1023/A:1007379606734}, abstractNote={Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.}, number={1}, journal={Machine Learning}, author={Caruana, Rich}, year={1997}, month=jul, pages={41–75}, language={en} }

@inproceedings{Wager2013dropout, title={Dropout Training as Adaptive Regularization}, volume={26}, url={https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Wager, Stefan and Wang, Sida and Liang, Percy S}, editor={Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.}, year={2013} }

 @inproceedings{bourlard1989early, title={Generalization and Parameter Estimation in Feedforward Nets: Some Experiments}, volume={2}, url={https://proceedings.neurips.cc/paper_files/paper/1989/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Morgan-Kaufmann}, author={Morgan, N. and Bourlard, H.}, editor={Touretzky, D.}, year={1989} }

@inproceedings{Bengio2006pretraining, title={Greedy Layer-Wise Training of Deep Networks}, volume={19}, url={https://proceedings.neurips.cc/paper_files/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={MIT Press}, author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo}, editor={Schölkopf, B. and Platt, J. and Hoffman, T.}, year={2006} }

@inproceedings{Le2018supervised_autoencoders, title={Supervised autoencoders: Improving generalization performance with unsupervised regularizers}, volume={31}, url={https://proceedings.neurips.cc/paper_files/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Le, Lei and Patterson, Andrew and White, Martha}, editor={Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.}, year={2018} }




@INPROCEEDINGS{ni2024enhancing,
  author={Ni, Shuang and Aumon, Adrien and Wolf, Guy and Moon, Kevin R. and Rhodes, Jake S.},
  booktitle={2024 IEEE 34th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={Enhancing Supervised Visualization Through Autoencoder and Random Forest Proximities for Out-of-Sample Extension}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Dimensionality reduction;Data visualization;Prototypes;Training data;Forestry;Signal processing;Reconstruction algorithms;Robustness;Random forests;Dimensionality reduction;Autoencoders;Random Forest Proximities},
  doi={10.1109/MLSP58920.2024.10734764}}

@article{bengio2003out,
  title={Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering},
  author={Bengio, Y. and Paiement, J. and others},
  journal={NeurIPS},
  volume={16},
  year={2003}
}


@article{coifman2006geometric,
title = {Geometric harmonics: A novel tool for multiscale out-of-sample extension of empirical functions},
journal = {Appl. Comput. Harmon. Anal.},
volume = {21},
number = {1},
pages = {31-52},
year = {2006},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2005.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1063520306000522},
author = {R. R. Coifman and S. Lafon},
keywords = {Nyström method, Intrinsic and extrinsic geometries, Subsampling, Prolate functions},
abstract = {We describe a simple scheme, based on the Nyström method, for extending empirical functions f defined on a set X to a larger set X¯. The extension process that we describe involves the construction of a specific family of functions that we term geometric harmonics. These functions constitute a generalization of the prolate spheroidal wave functions of Slepian in the sense that they are optimally concentrated on X. We study the case when X is a submanifold of Rn in greater detail. In this situation, any empirical function f on X can be characterized by its decomposition over the intrinsic Fourier modes, i.e., the eigenfunctions of the Laplace–Beltrami operator, and we show that this intrinsic frequency spectrum determines the largest domain of extension of f to the entire space Rn. Our analysis relates the complexity of the function on the training set to the scale of extension off this set. This approach allows us to present a novel multiscale extension scheme for empirical functions.}
}

@article{duque2022geometry,
  title={Geometry regularized autoencoders},
  author={Duque, A.F. and Morin, S. and Wolf, G. and Moon, K.R.},
  journal={IEEE PAMI},
  volume = {45},
  number = {6},
  pages = {7381-7394},
  year={2022},
  publisher={IEEE}
}

@inproceedings{theis2017lossy,
  title={Lossy image compression with compressive autoencoders},
  author={Theis, L. and Shi, W. and others},
  booktitle={ICLR},
  year={2022}
}

@article {rhodes2024gaining,
    author = {J. S. Rhodes and A. Aumon and others},
    title = {Gaining Biological Insights through Supervised Data Visualization},
    elocation-id = {2023.11.22.568384},
    year = {2024},
    doi = {10.1101/2023.11.22.568384},
    publisher = {Cold Spring Harbor Laboratory},
    abstract = {Dimensionality reduction-based data visualization is pivotal in comprehending complex biological data. The most common methods, such as PHATE, t-SNE, and UMAP, are unsupervised and therefore reflect the dominant structure in the data, which may be independent of expert-provided labels. Here we introduce a supervised data visualization method called RF-PHATE, which integrates expert knowledge for further exploration of the data. RF-PHATE leverages random forests to capture intricate featurelabel relationships. Extracting information from the forest, RF-PHATE generates low-dimensional visualizations that highlight relevant data relationships while disregarding extraneous features. This approach scales to large datasets and applies to classification and regression. We illustrate RF-PHATE's prowess through three case studies. In a multiple sclerosis study using longitudinal clinical and imaging data, RF-PHATE unveils a sub-group of patients with non-benign relapsingremitting Multiple Sclerosis, demonstrating its aptitude for time-series data. In the context of Raman spectral data, RF-PHATE effectively showcases the impact of antioxidants on diesel exhaust-exposed lung cells, highlighting its proficiency in noisy environments. Furthermore, RF-PHATE aligns established geometric structures with COVID-19 patient outcomes, enriching interpretability in a hierarchical manner. RF-PHATE bridges expert insights and visualizations, promising knowledge generation. Its adaptability, scalability, and noise tolerance underscore its potential for widespread adoption. Competing Interest Statement The authors have declared no competing interest.},
    URL = {https://www.biorxiv.org/content/early/2024/01/21/2023.11.22.568384},
    eprint = {https://www.biorxiv.org/content/early/2024/01/21/2023.11.22.568384.full.pdf},
    journal = {bioRxiv}
}

 @inproceedings{LEE2021nrae, title={Neighborhood Reconstructing Autoencoders}, volume={34}, url={https://proceedings.neurips.cc/paper/2021/hash/05311655a15b75fab86956663e1819cd-Abstract.html}, abstractNote={Vanilla autoencoders often produce manifolds that overfit to noisy training data, or have the wrong local connectivity and geometry. Autoencoder regularization techniques, e.g., the denoising autoencoder, have had some success in reducing overfitting, whereas recent graph-based methods that exploit local connectivity information provided by neighborhood graphs have had some success in mitigating local connectivity errors. Neither of these two approaches satisfactorily reduce both overfitting and connectivity errors; moreover, graph-based methods typically involve considerable preprocessing and tuning. To simultaneously address the two issues of overfitting and local connectivity, we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder (NRAE). Unlike existing graph-based methods that attempt to encode the training data to some prescribed latent space distribution -- one consequence being that only the encoder is the object of the regularization -- NRAE merges local connectivity information contained in the neighborhood graphs with local quadratic approximations of the decoder function to formulate a new neighborhood reconstruction loss. Compared to existing graph-based methods, our new loss function is simple and easy to implement, and the resulting algorithm is scalable and computationally efficient; the only required preprocessing step is the construction of the neighborhood graph. Extensive experiments with standard datasets demonstrate that, compared to existing methods, NRAE improves both overfitting and local connectivity in the learned manifold, in some cases by significant margins. Code for NRAE is available at https://github.com/Gabe-YHLee/NRAE-public.}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={LEE, Yonghyeon and Kwon, Hyeokjun and Park, Frank}, year={2021}, pages={536–546} }

 @article{Nazari2023geometric, title={Geometric Autoencoders -- What You See is What You Decode}, url={http://arxiv.org/abs/2306.17638}, DOI={10.48550/arXiv.2306.17638}, abstractNote={Visualization is a crucial step in exploratory data analysis. One possible approach is to train an autoencoder with low-dimensional latent space. Large network depth and width can help unfolding the data. However, such expressive networks can achieve low reconstruction error even when the latent representation is distorted. To avoid such misleading visualizations, we propose first a differential geometric perspective on the decoder, leading to insightful diagnostics for an embedding’s distortion, and second a new regularizer mitigating such distortion. Our ``Geometric Autoencoder’’ avoids stretching the embedding spuriously, so that the visualization captures the data structure more faithfully. It also flags areas where little distortion could not be achieved, thus guarding against misinterpretation.}, note={arXiv:2306.17638 [cs]}, number={arXiv:2306.17638}, publisher={arXiv}, author={Nazari, Philipp and Damrich, Sebastian and Hamprecht, Fred A.}, year={2023}, month=jun }

 @inproceedings{Wang2016sdne, address={New York, NY, USA}, series={KDD ’16}, title={Structural Deep Network Embedding}, ISBN={978-1-4503-4232-2}, url={https://dl.acm.org/doi/10.1145/2939672.2939753}, DOI={10.1145/2939672.2939753}, abstractNote={Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.}, booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, publisher={Association for Computing Machinery}, author={Wang, Daixin and Cui, Peng and Zhu, Wenwu}, year={2016}, month=aug, pages={1225–1234}, collection={KDD ’16} }

 @inproceedings{Wang2014gae, title={Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction}, url={https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Wang_Generalized_Autoencoder_A_2014_CVPR_paper.html}, author={Wang, Wei and Huang, Yan and Wang, Yizhou and Wang, Liang}, year={2014}, pages={490–497} }

@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}

 @article{Ghosh2022centroid, title={Supervised Dimensionality Reduction and Visualization using Centroid-Encoder}, volume={23}, ISSN={1533-7928}, abstractNote={We propose a new tool for visualizing complex, and potentially large and high-dimensional, data sets called Centroid-Encoder (CE). The architecture of the Centroid-Encoder is similar to the autoencoder neural network but it has a modified target, i.e., the class centroid in the ambient space. As such, CE incorporates label information and performs a supervised data visualization. The training of CE is done in the usual way with a training set whose parameters are tuned using a validation set. The evaluation of the resulting CE visualization is performed on a sequestered test set where the generalization of the model is assessed both visually and quantitatively. We present a detailed comparative analysis of the method using a wide variety of data sets and techniques, both supervised and unsupervised, including NCA, non-linear NCA, t-distributed NCA, t-distributed MCML, supervised UMAP, supervised PCA, Colored Maximum Variance Unfolding, supervised Isomap, Parametric Embedding, supervised Neighbor Retrieval Visualizer, and Multiple Relational Embedding. An analysis of variance using PCA demonstrates that a non-linear preprocessing by the CE transformation of the data captures more variance than PCA by dimension.}, number={20}, journal={Journal of Machine Learning Research}, author={Ghosh, Tomojit and Kirby, Michael}, year={2022}, pages={1–34} }


@article{kaneko2022cvpfi,
author = {Kaneko, Hiromasa},
title = {Cross-validated permutation feature importance considering correlation between features},
journal = {Analytical Science Advances},
volume = {3},
number = {9-10},
pages = {278-287},
keywords = {correlation, cross-validation, feature importance, model interpretation, permutation importance},
doi = {https://doi.org/10.1002/ansa.202200018},
url = {https://chemistry-europe.onlinelibrary.wiley.com/doi/abs/10.1002/ansa.202200018},
eprint = {https://chemistry-europe.onlinelibrary.wiley.com/doi/pdf/10.1002/ansa.202200018},
abstract = {Abstract In molecular design, material design, process design, and process control, it is important not only to construct a model with high predictive ability between explanatory features x and objective features y using a dataset but also to interpret the constructed model. An index of feature importance in x is permutation feature importance (PFI), which can be combined with any regressors and classifiers. However, the PFI becomes unstable when the number of samples is low because it is necessary to divide a dataset into training and validation data when calculating it. Additionally, when there are strongly correlated features in x, the PFI of these features is estimated to be low. Hence, a cross-validated PFI (CVPFI) method is proposed. CVPFI can be calculated stably, even with a small number of samples, because model construction and feature evaluation are repeated based on cross-validation. Furthermore, by considering the absolute correlation coefficients between the features, the feature importance can be evaluated appropriately even when there are strongly correlated features in x. Case studies using numerical simulation data and actual compound data showed that the feature importance can be evaluated appropriately using CVPFI compared to PFI. This is possible when the number of samples is low, when linear and nonlinear relationships are mixed between x and y when there are strong correlations between features in x, and when quantised and biased features exist in x. Python codes for CVPFI are available at https://github.com/hkaneko1985/dcekit.},
year = {2022}
}

@Article{Vogelstein2021lol,
author={Vogelstein, J. T.
and Bridgeford, E. W.
and Tang, M.
and Zheng, D.
and others},
title={Supervised dimensionality reduction for big data},
journal={Nat. Commun.},
year={2021},
month={May},
day={17},
volume={12},
number={1},
pages={2872},
abstract={To solve key biomedical problems, experimentalists now routinely measure millions or billions of features (dimensions) per sample, with the hope that data science techniques will be able to build accurate data-driven inferences. Because sample sizes are typically orders of magnitude smaller than the dimensionality of these data, valid inferences require finding a low-dimensional representation that preserves the discriminating information (e.g., whether the individual suffers from a particular disease). There is a lack of interpretable supervised dimensionality reduction methods that scale to millions of dimensions with strong statistical theoretical guarantees. We introduce an approach to extending principal components analysis by incorporating class-conditional moment estimates into the low-dimensional projection. The simplest version, Linear Optimal Low-rank projection, incorporates the class-conditional means. We prove, and substantiate with both synthetic and real data benchmarks, that Linear Optimal Low-Rank Projection and its generalizations lead to improved data representations for subsequent classification, while maintaining computational efficiency and scalability. Using multiple brain imaging datasets consisting of more than 150 million features, and several genomics datasets with more than 500,000 features, Linear Optimal Low-Rank Projection outperforms other scalable linear dimensionality reduction techniques in terms of accuracy, while only requiring a few minutes on a standard desktop computer.},
issn={2041-1723},
doi={10.1038/s41467-021-23102-2},
url={https://doi.org/10.1038/s41467-021-23102-2}
}


 @article{Szubert2019ivis, title={Structure-preserving visualisation of high dimensional single-cell datasets}, volume={9}, rights={2019 The Author(s)}, ISSN={2045-2322}, DOI={10.1038/s41598-019-45301-0}, abstractNote={Single-cell technologies offer an unprecedented opportunity to effectively characterize cellular heterogeneity in health and disease. Nevertheless, visualisation and interpretation of these multi-dimensional datasets remains a challenge. We present a novel framework, ivis, for dimensionality reduction of single-cell expression data. ivis utilizes a siamese neural network architecture that is trained using a novel triplet loss function. Results on simulated and real datasets demonstrate that ivis preserves global data structures in a low-dimensional space, adds new data points to existing embeddings using a parametric mapping function, and scales linearly to hundreds of thousands of cells. ivis is made publicly available through Python and R interfaces on https://github.com/beringresearch/ivis.}, number={1}, journal={Scientific Reports}, publisher={Nature Publishing Group}, author={Szubert, Benjamin and Cole, Jennifer E. and Monaco, Claudia and Drozdov, Ignat}, year={2019}, month=jun, pages={8914}, language={en} }


 @article{graving2020vae-sne, title={VAE-SNE: a deep generative model for simultaneous dimensionality reduction and clustering}, url={https://www.biorxiv.org/content/early/2020/07/17/2020.07.17.207993}, DOI={10.1101/2020.07.17.207993}, abstractNote={Scientific datasets are growing rapidly in scale and complexity. Consequently, the task of understanding these data to answer scientific questions increasingly requires the use of compression algorithms that reduce dimensionality by combining correlated features and cluster similar observations to summarize large datasets. Here we introduce a method for both dimension reduction and clustering called VAE-SNE (variational autoencoder stochastic neighbor embedding). Our model combines elements from deep learning, probabilistic inference, and manifold learning to produce interpretable compressed representations while also readily scaling to tens-of-millions of observations. Unlike existing methods, VAE-SNE simultaneously compresses high-dimensional data and automatically learns a distribution of clusters within the data — without the need to manually select the number of clusters. This naturally creates a multi-scale representation, which makes it straightforward to generate coarse-grained descriptions for large subsets of related observations and select specific regions of interest for further analysis. VAE-SNE can also quickly and easily embed new samples, detect outliers, and can be optimized with small batches of data, which makes it possible to compress datasets that are otherwise too large to fit into memory. We evaluate VAE-SNE as a general purpose method for dimensionality reduction by applying it to multiple real-world datasets and by comparing its performance with existing methods for dimensionality reduction. We find that VAE-SNE produces high-quality compressed representations with results that are on par with existing nonlinear dimensionality reduction algorithms. As a practical example, we demonstrate how the cluster distribution learned by VAE-SNE can be used for unsupervised action recognition to detect and classify repeated motifs of stereotyped behavior in high-dimensional timeseries data. Finally, we also introduce variants of VAE-SNE for embedding data in polar (spherical) coordinates and for embedding image data from raw pixels. VAE-SNE is a robust, feature-rich, and scalable method with broad applicability to a range of datasets in the life sciences and beyond.Competing Interest StatementThe authors have declared no competing interest.}, journal={bioRxiv}, publisher={Cold Spring Harbor Laboratory}, author={Graving, Jacob M. and Couzin, Iain D.}, year={2020} }


@inproceedings{espadoto2021ssnp,
title = "Self-supervised Dimensionality Reduction with Neural Networks and Pseudo-labeling",
abstract = "Dimensionality reduction (DR) is used to explore high-dimensional data in many applications. Deep learning techniques such as autoencoders have been used to provide fast, simple to use, and high-quality DR. However, such methods yield worse visual cluster separation than popular methods such as t-SNE and UMAP. We propose a deep learning DR method called Self-Supervised Network Projection (SSNP) which does DR based on pseudo-labels obtained from clustering. We show that SSNP produces better cluster separation than autoencoders, has out-of-sample, inverse mapping, and clustering capabilities, and is very fast and easy to use.",
keywords = "Autoencoders, Dimensionality Reduction, Machine Learning, Neural Networks",
author = "Mateus Espadoto and Hirata, {Nina S.T.} and Telea, {Alexandru C.}",
note = "Funding Information: This study was financed in part by FAPESP (2015/22308-2 and 2017/25835-9) and the Coordenac¸{\~a}o de Aperfeic¸oamento de Pessoal de N{\'i}vel Superior - Brasil (CAPES) - Finance Code 001. Publisher Copyright: Copyright {\textcopyright} 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved; 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021 ; Conference date: 08-02-2021 Through 10-02-2021",
year = "2021",
doi = "10.5220/0010184800270037",
language = "English",
series = "VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",
publisher = "SciTePress",
pages = "27--37",
editor = "Christophe Hurter and Helen Purchase and Jose Braz and Kadi Bouatouch",
booktitle = "IVAPP",

}

@inproceedings{damrich2023from,
  title={From $t$-{SNE} to {UMAP} with contrastive learning},
  author={Damrich, Sebastian and B{\"o}hm, Jan Niklas  and Hamprecht, Fred A and Kobak, Dmitry},
  booktitle={International Conference on Learning Representations},
  year={2023},
}


@InProceedings{moor2020topo,
  title = 	 {Topological Autoencoders},
  author =       {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {7045--7054},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/moor20a/moor20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/moor20a.html},
  abstract = 	 {We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term. Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information. We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.}
}

 @article{Kobak2019tsne, title={The art of using t-SNE for single-cell transcriptomics}, volume={10}, rights={2019 The Author(s)}, ISSN={2041-1723}, DOI={10.1038/s41467-019-13056-x}, abstractNote={Single-cell transcriptomics yields ever growing data sets containing RNA expression levels for thousands of genes from up to millions of cells. Common data analysis pipelines include a dimensionality reduction step for visualising the data in two dimensions, most frequently performed using t-distributed stochastic neighbour embedding (t-SNE). It excels at revealing local structure in high-dimensional data, but naive applications often suffer from severe shortcomings, e.g. the global structure of the data is not represented accurately. Here we describe how to circumvent such pitfalls, and develop a protocol for creating more faithful t-SNE visualisations. It includes PCA initialisation, a high learning rate, and multi-scale similarity kernels; for very large data sets, we additionally use exaggeration and downsampling-based initialisation. We use published single-cell RNA-seq data sets to demonstrate that this protocol yields superior results compared to the naive application of t-SNE.}, number={1}, journal={Nature Communications}, publisher={Nature Publishing Group}, author={Kobak, Dmitry and Berens, Philipp}, year={2019}, month=nov, pages={5416}, language={en} }

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@INPROCEEDINGS{gigante2019compressed,
  author={Gigante, Scott and Stanley, Jay S. and Vu, Ngan and Dijk, David van and Moon, Kevin R. and Wolf, Guy and Krishnaswamy, Smita},
  booktitle={2019 13th International conference on Sampling Theory and Applications (SampTA)}, 
  title={Compressed Diffusion}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  keywords={Kernel;Manifolds;Geometry;Data analysis;Eigenvalues and eigenfunctions;Coherence;Diffusion processes},
  doi={10.1109/SampTA45681.2019.9030994}}


@inproceedings{Gisbrecht2012oos,
  title={Out-of-sample kernel extensions for nonparametric dimensionality reduction},
  author={Gisbrecht, Andrej and Lueks, Wouter and Mokbel, Bassam and Hammer, Barbara and others},
  booktitle={ESANN},
  volume={2012},
  pages={531--536},
  year={2012}
}

 @article{Gisbrecht2015kernel-tsne, series={Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012)}, title={Parametric nonlinear dimensionality reduction using kernel t-SNE}, volume={147}, ISSN={0925-2312}, DOI={10.1016/j.neucom.2013.11.045}, abstractNote={Novel non-parametric dimensionality reduction techniques such as t-distributed stochastic neighbor embedding (t-SNE) lead to a powerful and flexible visualization of high-dimensional data. One drawback of non-parametric techniques is their lack of an explicit out-of-sample extension. In this contribution, we propose an efficient extension of t-SNE to a parametric framework, kernel t-SNE, which preserves the flexibility of basic t-SNE, but enables explicit out-of-sample extensions. We test the ability of kernel t-SNE in comparison to standard t-SNE for benchmark data sets, in particular addressing the generalization ability of the mapping for novel data. In the context of large data sets, this procedure enables us to train a mapping for a fixed size subset only, mapping all data afterwards in linear time. We demonstrate that this technique yields satisfactory results also for large data sets provided missing information due to the small size of the subset is accounted for by auxiliary information such as class labels, which can be integrated into kernel t-SNE based on the Fisher information.}, journal={Neurocomputing}, author={Gisbrecht, Andrej and Schulz, Alexander and Hammer, Barbara}, year={2015}, month=jan, pages={71–82}, collection={Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012)} }

 @article{Ghojogh2020lle, title={Locally Linear Embedding and its Variants: Tutorial and Survey}, url={http://arxiv.org/abs/2011.10925}, DOI={10.48550/arXiv.2011.10925}, abstractNote={This is a tutorial and survey paper for Locally Linear Embedding (LLE) and its variants. The idea of LLE is fitting the local structure of manifold in the embedding space. In this paper, we first cover LLE, kernel LLE, inverse LLE, and feature fusion with LLE. Then, we cover out-of-sample embedding using linear reconstruction, eigenfunctions, and kernel mapping. Incremental LLE is explained for embedding streaming data. Landmark LLE methods using the Nystrom approximation and locally linear landmarks are explained for big data embedding. We introduce the methods for parameter selection of number of neighbors using residual variance, Procrustes statistics, preservation neighborhood error, and local neighborhood selection. Afterwards, Supervised LLE (SLLE), enhanced SLLE, SLLE projection, probabilistic SLLE, supervised guided LLE (using Hilbert-Schmidt independence criterion), and semi-supervised LLE are explained for supervised and semi-supervised embedding. Robust LLE methods using least squares problem and penalty functions are also introduced for embedding in the presence of outliers and noise. Then, we introduce fusion of LLE with other manifold learning methods including Isomap (i.e., ISOLLE), principal component analysis, Fisher discriminant analysis, discriminant LLE, and Isotop. Finally, we explain weighted LLE in which the distances, reconstruction weights, or the embeddings are adjusted for better embedding; we cover weighted LLE for deformed distributed data, weighted LLE using probability of occurrence, SLLE by adjusting weights, modified LLE, and iterative LLE.}, note={arXiv:2011.10925 [stat]}, number={arXiv:2011.10925}, publisher={arXiv}, author={Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark}, year={2020}, month=nov }


 @book{Ran2024kumap, title={Kumap: Kernel Uniform Manifold Approximation and Projection for Out-of-sample Extensions Problem}, DOI={10.21203/rs.3.rs-3872850/v1}, abstractNote={Uniform Manifold Approximation and Projection (UMAP) is a popular dimensionality reduction and visualization algorithm recently proposed and widely used in several fields. However, UMAP encounters difficulties in mapping new samples into low-dimensional embeddings with what has been learnt from the learning process, which often referred to as the out-of-sample problem. In this paper, a kernel UMAP (KUMAP) method is proposed to address this problem, which is a kernel-based expansion technique. It uses the Laplacian kernel function to map the original samples to the low-dimensional space. In addition, to make full use of the label information in the sample data, a supervised kernel UMAP (SKUMAP) is also proposed. The KUMAP and SKUMAP methods are evaluated on different scale datasets in terms of the preservation of structure in small neighborhood data, silhouette coefficients, and classification accuracy. Compared with UMAP and other representative method, the KUMAP and SKUMAP methods have better embedding quality, higher classification accuracy, and better visualization.}, author={Ran, Ruisheng and Li, Benchao and Zou, Yun}, year={2024}, month=jan }


 @article{Chen2013sparse, title={Sparse projections of medical images onto manifolds}, volume={23}, ISSN={1011-2499}, DOI={10.1007/978-3-642-38868-2_25}, abstractNote={Manifold learning has been successfully applied to a variety of medical imaging problems. Its use in real-time applications requires fast projection onto the low-dimensional space. To this end, out-of-sample extensions are applied by constructing an interpolation function that maps from the input space to the low-dimensional manifold. Commonly used approaches such as the Nyström extension and kernel ridge regression require using all training points. We propose an interpolation function that only depends on a small subset of the input training data. Consequently, in the testing phase each new point only needs to be compared against a small number of input training data in order to project the point onto the low-dimensional space. We interpret our method as an out-of-sample extension that approximates kernel ridge regression. Our method involves solving a simple convex optimization problem and has the attractive property of guaranteeing an upper bound on the approximation error, which is crucial for medical applications. Tuning this error bound controls the sparsity of the resulting interpolation function. We illustrate our method in two clinical applications that require fast mapping of input images onto a low-dimensional space.}, journal={Information Processing in Medical Imaging: Proceedings of the ... Conference}, author={Chen, George H. and Wachinger, Christian and Golland, Polina}, year={2013}, pages={292–303}, language={eng} }



@article{
roweis2000lle,
author = {Sam T. Roweis  and Lawrence K. Saul },
title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
journal = {Science},
volume = {290},
number = {5500},
pages = {2323-2326},
year = {2000},
doi = {10.1126/science.290.5500.2323},
URL = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.}}


 @inproceedings{Arias2007connecting, title={Connecting the Out-of-Sample and Pre-Image Problems in Kernel Methods}, ISSN={1063-6919}, url={https://ieeexplore.ieee.org/document/4270063}, DOI={10.1109/CVPR.2007.383038}, abstractNote={Kernel methods have been widely studied in the field of pattern recognition. These methods implicitly map, “the kernel trick,” the data into a space which is more appropriate for analysis. Many manifold learning and dimensionality reduction techniques are simply kernel methods for which the mapping is explicitly computed. In such cases, two problems related with the mapping arise: The out-of-sample extension and the pre-image computation. In this paper we propose a new pre-image method based on the Nystrom formulation for the out-of-sample extension, showing the connections between both problems. We also address the importance of normalization in the feature space, which has been ignored by standard pre-image algorithms. As an example, we apply these ideas to the Gaussian kernel, and relate our approach to other popular pre-image methods. Finally, we show the application of these techniques in the study of dynamic shapes.}, booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition}, author={Arias, Pablo and Randall, Gregory and Sapiro, Guillermo}, year={2007}, month=jun, pages={1–8} }


 @article{saul2003think, title={Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds}, volume={4}, ISSN={ISSN 1533-7928}, abstractNote={The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm’s performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.}, number={Jun}, journal={Journal of Machine Learning Research}, author={Saul, Lawrence K. and Roweis, Sam T.}, year={2003}, pages={119–155} }



@misc{GTZAN,
author = {Andrada},
booktitle = {kaggle},
mendeley-groups = {Mila},
pages = {1},
title = {{GTZAN Dataset - Music Genre Classification}},
url = {https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification},
urldate = {2025-01-31},
year = {2019}
}
@misc{FashionMNIST,
author = {{Zalando Research} and Crawford, Chris},
mendeley-groups = {Mila},
title = {{Fashion MNIST}},
year= {2017},
url = {https://www.kaggle.com/datasets/zalando-research/fashionmnist},
urldate = {2025-01-31}
}

@misc{SignMNIST,
abstract = {The original MNIST image dataset of handwritten digits is a popular benchmark for image-based machine learning methods but researchers have renewed efforts to update it and develop drop-in replacements that are more challenging for computer vision and original for real-world applications. As noted in one recent replacement called the Fashion-MNIST dataset, the Zalando researchers quoted the startling claim that "Most pairs of MNIST digits (784 total pixels per sample) can be distinguished pretty well by just one pixel". To stimulate the community to develop more drop-in replacements, the Sign Language MNIST is presented here and follows the same CSV format with labels and pixel values in single rows. The American Sign Language letter database of hand gestures represent a multi-class problem with 24 classes of letters (excluding J and Z which require motion).},
author = {Tecperson},
booktitle = {Kaggle},
mendeley-groups = {Mila},
pages = {1},
title = {{Sign Language MNIST}},
url = {https://www.kaggle.com/datasets/datamunge/sign-language-mnist https://www.kaggle.com/datamunge/sign-language-mnist},
urldate = {2025-01-31},
year = {2017}
}

@misc{MNIST,
author = {Khodabakhsh, Hojjat},
booktitle = {Kaggle},
mendeley-groups = {Mila},
title = {{MNIST Dataset}},
url = {https://www.kaggle.com/datasets/hojjatk/mnist-dataset},
urldate = {2025-01-31},
year = {2019}
}



