@inproceedings{Arias2007connecting, title={Connecting the Out-of-Sample and Pre-Image Problems in Kernel Methods}, ISSN={1063-6919}, url={https://ieeexplore.ieee.org/document/4270063}, DOI={10.1109/CVPR.2007.383038}, abstractNote={Kernel methods have been widely studied in the field of pattern recognition. These methods implicitly map, “the kernel trick,” the data into a space which is more appropriate for analysis. Many manifold learning and dimensionality reduction techniques are simply kernel methods for which the mapping is explicitly computed. In such cases, two problems related with the mapping arise: The out-of-sample extension and the pre-image computation. In this paper we propose a new pre-image method based on the Nystrom formulation for the out-of-sample extension, showing the connections between both problems. We also address the importance of normalization in the feature space, which has been ignored by standard pre-image algorithms. As an example, we apply these ideas to the Gaussian kernel, and relate our approach to other popular pre-image methods. Finally, we show the application of these techniques in the study of dynamic shapes.}, booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition}, author={Arias, Pablo and Randall, Gregory and Sapiro, Guillermo}, year={2007}, month=jun, pages={1–8} }

@inproceedings{Bengio2006pretraining, title={Greedy Layer-Wise Training of Deep Networks}, volume={19}, url={https://proceedings.neurips.cc/paper_files/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={MIT Press}, author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo}, editor={Schölkopf, B. and Platt, J. and Hoffman, T.}, year={2006} }

@article{Caruana1997multitask, title={Multitask Learning}, volume={28}, ISSN={1573-0565}, DOI={10.1023/A:1007379606734}, abstractNote={Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.}, number={1}, journal={Machine Learning}, author={Caruana, Rich}, year={1997}, month=jul, pages={41–75}, language={en} }

@article{Chen2013sparse, title={Sparse projections of medical images onto manifolds}, volume={23}, ISSN={1011-2499}, DOI={10.1007/978-3-642-38868-2_25}, abstractNote={Manifold learning has been successfully applied to a variety of medical imaging problems. Its use in real-time applications requires fast projection onto the low-dimensional space. To this end, out-of-sample extensions are applied by constructing an interpolation function that maps from the input space to the low-dimensional manifold. Commonly used approaches such as the Nyström extension and kernel ridge regression require using all training points. We propose an interpolation function that only depends on a small subset of the input training data. Consequently, in the testing phase each new point only needs to be compared against a small number of input training data in order to project the point onto the low-dimensional space. We interpret our method as an out-of-sample extension that approximates kernel ridge regression. Our method involves solving a simple convex optimization problem and has the attractive property of guaranteeing an upper bound on the approximation error, which is crucial for medical applications. Tuning this error bound controls the sparsity of the resulting interpolation function. We illustrate our method in two clinical applications that require fast mapping of input images onto a low-dimensional space.}, journal={Information Processing in Medical Imaging: Proceedings of the ... Conference}, author={Chen, George H. and Wachinger, Christian and Golland, Polina}, year={2013}, pages={292–303}, language={eng} }

@article{Ghosh2022centroid, title={Supervised Dimensionality Reduction and Visualization using Centroid-Encoder}, volume={23}, ISSN={1533-7928}, abstractNote={We propose a new tool for visualizing complex, and potentially large and high-dimensional, data sets called Centroid-Encoder (CE). The architecture of the Centroid-Encoder is similar to the autoencoder neural network but it has a modified target, i.e., the class centroid in the ambient space. As such, CE incorporates label information and performs a supervised data visualization. The training of CE is done in the usual way with a training set whose parameters are tuned using a validation set. The evaluation of the resulting CE visualization is performed on a sequestered test set where the generalization of the model is assessed both visually and quantitatively. We present a detailed comparative analysis of the method using a wide variety of data sets and techniques, both supervised and unsupervised, including NCA, non-linear NCA, t-distributed NCA, t-distributed MCML, supervised UMAP, supervised PCA, Colored Maximum Variance Unfolding, supervised Isomap, Parametric Embedding, supervised Neighbor Retrieval Visualizer, and Multiple Relational Embedding. An analysis of variance using PCA demonstrates that a non-linear preprocessing by the CE transformation of the data captures more variance than PCA by dimension.}, number={20}, journal={Journal of Machine Learning Research}, author={Ghosh, Tomojit and Kirby, Michael}, year={2022}, pages={1–34} }

@inproceedings{Gisbrecht2012oos,
  title={Out-of-sample kernel extensions for nonparametric dimensionality reduction},
  author={Gisbrecht, Andrej and Lueks, Wouter and Mokbel, Bassam and Hammer, Barbara and others},
  booktitle={ESANN},
  volume={2012},
  pages={531--536},
  year={2012}
}

@article{Gisbrecht2015kernel-tsne, series={Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012)}, title={Parametric nonlinear dimensionality reduction using kernel t-SNE}, volume={147}, ISSN={0925-2312}, DOI={10.1016/j.neucom.2013.11.045}, abstractNote={Novel non-parametric dimensionality reduction techniques such as t-distributed stochastic neighbor embedding (t-SNE) lead to a powerful and flexible visualization of high-dimensional data. One drawback of non-parametric techniques is their lack of an explicit out-of-sample extension. In this contribution, we propose an efficient extension of t-SNE to a parametric framework, kernel t-SNE, which preserves the flexibility of basic t-SNE, but enables explicit out-of-sample extensions. We test the ability of kernel t-SNE in comparison to standard t-SNE for benchmark data sets, in particular addressing the generalization ability of the mapping for novel data. In the context of large data sets, this procedure enables us to train a mapping for a fixed size subset only, mapping all data afterwards in linear time. We demonstrate that this technique yields satisfactory results also for large data sets provided missing information due to the small size of the subset is accounted for by auxiliary information such as class labels, which can be integrated into kernel t-SNE based on the Fisher information.}, journal={Neurocomputing}, author={Gisbrecht, Andrej and Schulz, Alexander and Hammer, Barbara}, year={2015}, month=jan, pages={71–82}, collection={Advances in Self-Organizing Maps Subtitle of the special issue: Selected Papers from the Workshop on Self-Organizing Maps 2012 (WSOM 2012)} }

@inproceedings{LEE2021nrae, title={Neighborhood Reconstructing Autoencoders}, volume={34}, url={https://proceedings.neurips.cc/paper/2021/hash/05311655a15b75fab86956663e1819cd-Abstract.html}, abstractNote={Vanilla autoencoders often produce manifolds that overfit to noisy training data, or have the wrong local connectivity and geometry. Autoencoder regularization techniques, e.g., the denoising autoencoder, have had some success in reducing overfitting, whereas recent graph-based methods that exploit local connectivity information provided by neighborhood graphs have had some success in mitigating local connectivity errors. Neither of these two approaches satisfactorily reduce both overfitting and connectivity errors; moreover, graph-based methods typically involve considerable preprocessing and tuning. To simultaneously address the two issues of overfitting and local connectivity, we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder (NRAE). Unlike existing graph-based methods that attempt to encode the training data to some prescribed latent space distribution -- one consequence being that only the encoder is the object of the regularization -- NRAE merges local connectivity information contained in the neighborhood graphs with local quadratic approximations of the decoder function to formulate a new neighborhood reconstruction loss. Compared to existing graph-based methods, our new loss function is simple and easy to implement, and the resulting algorithm is scalable and computationally efficient; the only required preprocessing step is the construction of the neighborhood graph. Extensive experiments with standard datasets demonstrate that, compared to existing methods, NRAE improves both overfitting and local connectivity in the learned manifold, in some cases by significant margins. Code for NRAE is available at https://github.com/Gabe-YHLee/NRAE-public.}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={LEE, Yonghyeon and Kwon, Hyeokjun and Park, Frank}, year={2021}, pages={536–546} }

@article{Maurer2006multitask, title={Bounds for Linear Multi-Task Learning}, volume={7}, number={5}, journal={Journal of Machine Learning Research}, author={Maurer, Andreas}, year={2006}, pages={117–139} }

@Article{Moon2019phate,
    author={Moon, Kevin R.
    and van Dijk, David
    and Wang, Zheng
    and Gigante, Scott
    and Burkhardt, Daniel B.
    and Chen, William S.
    and Yim, Kristina
    and Elzen, Antonia van den
    and Hirn, Matthew J.
    and Coifman, Ronald R.
    and Ivanova, Natalia B.
    and Wolf, Guy
    and Krishnaswamy, Smita},
    title={Visualizing structure and transitions in high-dimensional biological data},
    journal={Nat. Biotechnol.},
    year={2019},
    month={Dec},
    day={01},
    volume={37},
    number={12},
    pages={1482-1492},
    url={https://doi.org/10.1038/s41587-019-0336-3}
}

@article{Nazari2023geometric, title={Geometric Autoencoders -- What You See is What You Decode}, url={http://arxiv.org/abs/2306.17638}, DOI={10.48550/arXiv.2306.17638}, abstractNote={Visualization is a crucial step in exploratory data analysis. One possible approach is to train an autoencoder with low-dimensional latent space. Large network depth and width can help unfolding the data. However, such expressive networks can achieve low reconstruction error even when the latent representation is distorted. To avoid such misleading visualizations, we propose first a differential geometric perspective on the decoder, leading to insightful diagnostics for an embedding’s distortion, and second a new regularizer mitigating such distortion. Our ``Geometric Autoencoder’’ avoids stretching the embedding spuriously, so that the visualization captures the data structure more faithfully. It also flags areas where little distortion could not be achieved, thus guarding against misinterpretation.}, note={arXiv:2306.17638 [cs]}, number={arXiv:2306.17638}, publisher={arXiv}, author={Nazari, Philipp and Damrich, Sebastian and Hamprecht, Fred A.}, year={2023}, month=jun }

@book{Ran2024kumap, title={Kumap: Kernel Uniform Manifold Approximation and Projection for Out-of-sample Extensions Problem}, DOI={10.21203/rs.3.rs-3872850/v1}, abstractNote={Uniform Manifold Approximation and Projection (UMAP) is a popular dimensionality reduction and visualization algorithm recently proposed and widely used in several fields. However, UMAP encounters difficulties in mapping new samples into low-dimensional embeddings with what has been learnt from the learning process, which often referred to as the out-of-sample problem. In this paper, a kernel UMAP (KUMAP) method is proposed to address this problem, which is a kernel-based expansion technique. It uses the Laplacian kernel function to map the original samples to the low-dimensional space. In addition, to make full use of the label information in the sample data, a supervised kernel UMAP (SKUMAP) is also proposed. The KUMAP and SKUMAP methods are evaluated on different scale datasets in terms of the preservation of structure in small neighborhood data, silhouette coefficients, and classification accuracy. Compared with UMAP and other representative method, the KUMAP and SKUMAP methods have better embedding quality, higher classification accuracy, and better visualization.}, author={Ran, Ruisheng and Li, Benchao and Zou, Yun}, year={2024}, month=jan }

@inproceedings{Wager2013dropout, title={Dropout Training as Adaptive Regularization}, volume={26}, url={https://proceedings.neurips.cc/paper_files/paper/2013/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Wager, Stefan and Wang, Sida and Liang, Percy S}, editor={Burges, C. J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.}, year={2013} }

@inproceedings{Wang2014gae, title={Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction}, url={https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Wang_Generalized_Autoencoder_A_2014_CVPR_paper.html}, author={Wang, Wei and Huang, Yan and Wang, Yizhou and Wang, Liang}, year={2014}, pages={490–497} }

@inproceedings{Wang2016sdne, address={New York, NY, USA}, series={KDD ’16}, title={Structural Deep Network Embedding}, ISBN={978-1-4503-4232-2}, url={https://dl.acm.org/doi/10.1145/2939672.2939753}, DOI={10.1145/2939672.2939753}, abstractNote={Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.}, booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, publisher={Association for Computing Machinery}, author={Wang, Daixin and Cui, Peng and Zhu, Wenwu}, year={2016}, month=aug, pages={1225–1234}, collection={KDD ’16} }

@inproceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrzebski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International conference on machine learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}

@inproceedings{baxter1995learning,
author = {Baxter, Jonathan},
title = {Learning internal representations},
year = {1995},
isbn = {0897917235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/225298.225336},
doi = {10.1145/225298.225336},
booktitle = {Proceedings of the Eighth Annual Conference on Computational Learning Theory},
pages = {311–320},
numpages = {10},
location = {Santa Cruz, California, USA},
series = {COLT '95}
}

@article{baxter2000inductive_bias,
author = {Baxter, Jonathan},
title = {A model of inductive bias learning},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {149–198},
numpages = {50}
}

@inproceedings{belkin2001laplacian,
author = {Belkin, M. and Niyogi, P.},
title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
year = {2001},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami operator on a manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered.},
booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
pages = {585–591},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'01}
}

@article{bengio2003out,
  title={Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering},
  author={Bengio, Y. and Paiement, J. and others},
  journal={NeurIPS},
  volume={16},
  year={2003}
}

@inproceedings{bourlard1989early, title={Generalization and Parameter Estimation in Feedforward Nets: Some Experiments}, volume={2}, url={https://proceedings.neurips.cc/paper_files/paper/1989/file/63923f49e5241343aa7acb6a06a751e7-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Morgan-Kaufmann}, author={Morgan, N. and Bourlard, H.}, editor={Touretzky, D.}, year={1989} }

@article{duque2022geometry,
  title={Geometry regularized autoencoders},
  author={Duque, A.F. and Morin, S. and Wolf, G. and Moon, K.R.},
  journal={IEEE PAMI},
  volume = {45},
  number = {6},
  pages = {7381-7394},
  year={2022},
  publisher={IEEE}
}

@inproceedings{espadoto2021ssnp,
title = "Self-supervised Dimensionality Reduction with Neural Networks and Pseudo-labeling",
abstract = "Dimensionality reduction (DR) is used to explore high-dimensional data in many applications. Deep learning techniques such as autoencoders have been used to provide fast, simple to use, and high-quality DR. However, such methods yield worse visual cluster separation than popular methods such as t-SNE and UMAP. We propose a deep learning DR method called Self-Supervised Network Projection (SSNP) which does DR based on pseudo-labels obtained from clustering. We show that SSNP produces better cluster separation than autoencoders, has out-of-sample, inverse mapping, and clustering capabilities, and is very fast and easy to use.",
keywords = "Autoencoders, Dimensionality Reduction, Machine Learning, Neural Networks",
author = "Mateus Espadoto and Hirata, {Nina S.T.} and Telea, {Alexandru C.}",
note = "Funding Information: This study was financed in part by FAPESP (2015/22308-2 and 2017/25835-9) and the Coordenac¸{\~a}o de Aperfeic¸oamento de Pessoal de N{\'i}vel Superior - Brasil (CAPES) - Finance Code 001. Publisher Copyright: Copyright {\textcopyright} 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved; 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2021 ; Conference date: 08-02-2021 Through 10-02-2021",
year = "2021",
doi = "10.5220/0010184800270037",
language = "English",
series = "VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",
publisher = "SciTePress",
pages = "27--37",
editor = "Christophe Hurter and Helen Purchase and Jose Braz and Kadi Bouatouch",
booktitle = "IVAPP",

}

@INPROCEEDINGS{gigante2019compressed,
  author={Gigante, Scott and Stanley, Jay S. and Vu, Ngan and Dijk, David van and Moon, Kevin R. and Wolf, Guy and Krishnaswamy, Smita},
  booktitle={2019 13th International conference on Sampling Theory and Applications (SampTA)}, 
  title={Compressed Diffusion}, 
  year={2019},
  volume={},
  number={},
  pages={1-4},
  keywords={Kernel;Manifolds;Geometry;Data analysis;Eigenvalues and eigenfunctions;Coherence;Diffusion processes},
  doi={10.1109/SampTA45681.2019.9030994}}

@article{graving2020vae-sne, title={VAE-SNE: a deep generative model for simultaneous dimensionality reduction and clustering}, url={https://www.biorxiv.org/content/early/2020/07/17/2020.07.17.207993}, DOI={10.1101/2020.07.17.207993}, abstractNote={Scientific datasets are growing rapidly in scale and complexity. Consequently, the task of understanding these data to answer scientific questions increasingly requires the use of compression algorithms that reduce dimensionality by combining correlated features and cluster similar observations to summarize large datasets. Here we introduce a method for both dimension reduction and clustering called VAE-SNE (variational autoencoder stochastic neighbor embedding). Our model combines elements from deep learning, probabilistic inference, and manifold learning to produce interpretable compressed representations while also readily scaling to tens-of-millions of observations. Unlike existing methods, VAE-SNE simultaneously compresses high-dimensional data and automatically learns a distribution of clusters within the data — without the need to manually select the number of clusters. This naturally creates a multi-scale representation, which makes it straightforward to generate coarse-grained descriptions for large subsets of related observations and select specific regions of interest for further analysis. VAE-SNE can also quickly and easily embed new samples, detect outliers, and can be optimized with small batches of data, which makes it possible to compress datasets that are otherwise too large to fit into memory. We evaluate VAE-SNE as a general purpose method for dimensionality reduction by applying it to multiple real-world datasets and by comparing its performance with existing methods for dimensionality reduction. We find that VAE-SNE produces high-quality compressed representations with results that are on par with existing nonlinear dimensionality reduction algorithms. As a practical example, we demonstrate how the cluster distribution learned by VAE-SNE can be used for unsupervised action recognition to detect and classify repeated motifs of stereotyped behavior in high-dimensional timeseries data. Finally, we also introduce variants of VAE-SNE for embedding data in polar (spherical) coordinates and for embedding image data from raw pixels. VAE-SNE is a robust, feature-rich, and scalable method with broad applicability to a range of datasets in the life sciences and beyond.Competing Interest StatementThe authors have declared no competing interest.}, journal={bioRxiv}, publisher={Cold Spring Harbor Laboratory}, author={Graving, Jacob M. and Couzin, Iain D.}, year={2020} }

@InProceedings{maaten2009parametric_tsne,
  title = 	 {Learning a Parametric Embedding by Preserving Local Structure},
  author = 	 {van der Maaten, Laurens},
  booktitle = 	 {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {384--391},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/maaten09a/maaten09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/maaten09a.html},
  abstract = 	 {The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space.  We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.}
}

@InProceedings{pontil2013multitask,
  title = 	 {Excess risk bounds for multitask learning with trace norm regularization},
  author = 	 {Pontil, Massimiliano and Maurer, Andreas},
  booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
  pages = 	 {55--76},
  year = 	 {2013},
  editor = 	 {Shalev-Shwartz, Shai and Steinwart, Ingo},
  volume = 	 {30},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Princeton, NJ, USA},
  month = 	 {12--14 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v30/Pontil13.pdf},
  url = 	 {https://proceedings.mlr.press/v30/Pontil13.html},
  abstract = 	 {Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be infinite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments.}
}

@article{sainburg2021parametric_umap,
    author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
    title = {Parametric UMAP Embeddings for Representation and Semisupervised Learning},
    journal = {Neural Computation},
    volume = {33},
    number = {11},
    pages = {2881-2907},
    year = {2021},
    month = {10},
    abstract = {UMAP is a nonparametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) computing a graphical representation of a data set (fuzzy simplicial complex) and (2) through stochastic gradient descent, optimizing a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that parametric UMAP performs comparably to its nonparametric counterpart while conferring the benefit of a learned parametric mapping (e.g., fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semisupervised learning by capturing structure in unlabeled data.1},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01434},
    url = {https://doi.org/10.1162/neco\_a\_01434},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/11/2881/1966656/neco\_a\_01434.pdf},
}

@article{saul2003think, title={Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds}, volume={4}, ISSN={ISSN 1533-7928}, abstractNote={The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm’s performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.}, number={Jun}, journal={Journal of Machine Learning Research}, author={Saul, Lawrence K. and Roweis, Sam T.}, year={2003}, pages={119–155} }

@inproceedings{theis2017lossy,
  title={Lossy image compression with compressive autoencoders},
  author={Theis, L. and Shi, W. and others},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{vincent2008extracting,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1096--1103},
  year={2008}
}

@article{wang2020lgae, title={Discriminative Auto-Encoder With Local and Global Graph Embedding}, volume={8}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2020.2972132}, abstractNote={In order to exploit the potential intrinsic low-dimensional structure of the high-dimensional data from the manifold learning perspective, we propose a global graph embedding with globality-preserving property, which requires that samples should be mapped close to their low-dimensional class representation data distribution centers in the embedding space. Then we propose a novel local and global graph embedding auto-encoder(LGAE) to capture the geometric structure of data, its cost function have three terms, a reconstruction loss to reproduce the input data based on the learned representation, a local graph embedding regularization to enforce mapping the neighboring samples close together in the embedding space, a global embedding regularization to enforce mapping samples close to their low-dimensional class representation distribution centers. Thus in the learning process, our LGAE can map samples from same class close together in the embedding space, as well as reduce the scatter within-class and increase the margin between-class, it will also detect the local and global intrinsic geometric structure of data and discover the latent discriminant information in the embedding space. We build stacked LGAE for classification tasks and conduct comprehensive experiments on several benchmark datasets, the results confirm that our proposed framework can learn discriminative representation, speed up the network convergence process, and significantly improve the classification performance.}, journal={IEEE Access}, author={Li, Rui and Wang, Xiaodan and Lai, Jie and Song, Yafei and Lei, Lei}, year={2020}, pages={28614–28623} }

@article{zhang2016understanding,
  author       = {Chiyuan Zhang and
                  Samy Bengio and
                  Moritz Hardt and
                  Benjamin Recht and
                  Oriol Vinyals},
  title        = {Understanding deep learning requires rethinking generalization},
  journal      = {CoRR},
  volume       = {abs/1611.03530},
  year         = {2016},
  url          = {http://arxiv.org/abs/1611.03530},
  eprinttype    = {arXiv},
  eprint       = {1611.03530},
  timestamp    = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/ZhangBHRV16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

