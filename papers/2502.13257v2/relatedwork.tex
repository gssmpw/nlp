\section{Related work}
\subsection{Regularization with multi-task autoencoders}\label{subsec:multitask_autoencoders}
Given a high-dimensional training dataset \( X = \{\mathbf{x}_i \in \mathbb{R}^D \mid i = 1, \hdots, N\} \)—where \( X \) can represent tabular data, images, or other modalities—a manifold learning algorithm can be extended to test data by training a neural network to regress onto the precomputed non-parametric embeddings $\mathbf{z}_i^G$, or by means of a cost function underlying the manifold learning algorithm, as in parametric  $t$-SNE \cite{maaten2009parametric_tsne} and parametric UMAP \cite{sainburg2021parametric_umap}. However, solely training a neural network for this supervised task often leads to an under-constrained problem, resulting in solutions that memorize the data but fail to capture meaningful patterns or generalize effectively \cite{zhang2016understanding, arpit2017closer}. Beyond implicit regularization techniques such as early stopping \cite{bourlard1989early}, dropout \cite{Wager2013dropout}, or layer-wise pre-training \cite{Bengio2006pretraining}, multi-task learning \cite{Caruana1997multitask} has been shown to improve generalization. Early studies demonstrated that jointly learning tasks reduces the number of required samples \cite{baxter1995learning, baxter2000inductive_bias}, while later work introduced trace-norm regularization on the weights of a linear, single hidden-layer neural network for a set of tasks \cite{Maurer2006multitask, pontil2013multitask}. Motivated by Le et al. \yrcite{Le2018supervised_autoencoders}, who empirically showed that training neural networks to predict both target embeddings and inputs (reconstruction) improves generalization compared to encoder-only architectures, we focus on multi-task learning-based regularization in the context of regularized autoencoders.

AE networks serve two primary purposes: first, they learn an encoder function \( f(\mathbf{x}_i) = \mathbf{z}_i \in \mathbb{R}^d \) (\( d \ll D \)), which compresses the input data into a latent representation via a bottleneck layer \cite{theis2017lossy}. Second, they learn a decoder function \( g(\mathbf{z}_i) = \hat{\mathbf{x}}_i \), which maps the low-dimensional embedding back to the original input space. In essence, AEs aim to optimize the encoding and decoding functions such that \( \hat{\mathbf{x}}_i \approx \mathbf{x}_i \) by minimizing the reconstruction loss
\begin{align*}
    L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
\end{align*}
% \begin{align*}
%     L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, g(f(\mathbf{x}_i))) 
%     \\&= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
% \end{align*}
where \( L_{\text{recon}}(\cdot, \cdot) \) is typically defined as the squared Euclidean distance. This loss quantifies the ability of the AE to reconstruct the input. Through optimization, AEs learn compact data representations while ensuring that these representations remain meaningfully related to the input data. However, standard AEs often fail to capture the intrinsic geometry of the data and do not produce embeddings aligned with human-interpretable visualizations \cite{duque2022geometry}. To address this, regularization techniques have been proposed to guide the optimization process toward better local minima in the latent space.

Structural Deep Network Embedding (SDNE) \cite{Wang2016sdne} preserves both first- and second-order graph neighborhoods for graph-structured data by combining adjacency vector reconstruction with Laplacian Eigenmaps \cite{belkin2001laplacian} regularization. Local and Global Graph Embedding Autoencoders \cite{wang2020lgae} enforce two constraints on the embedding layer: a local constraint to cluster \( k \)-nearest neighbors and a global constraint to align data points with their class centers. VAE-SNE \cite{graving2020vae-sne} integrates parametric $t$-SNE with variational AEs, enhancing global structure preservation while retaining $t$-SNE's strength in preserving local structure. GRAE \cite{duque2022geometry} explicitly impose geometric consistency between the latent space and precomputed manifold embeddings.

Other approaches focus on regularizing the decoder. Inspired by Denoising Autoencoders \cite{vincent2008extracting}, Generalized Autoencoders \cite{Wang2014gae} minimize the weighted mean squared error between the reconstruction \( \hat{\mathbf{x}} \) and the \( k \)-nearest neighbors of the input \( \mathbf{x} \), where weights reflect the predefined similarities between \( \mathbf{x} \) and its neighbors. Centroid Encoders (CE) \cite{Ghosh2022centroid} minimize within-class reconstruction variance to ensure that samples within the same class are mapped close to their respective centroids. Self-Supervised Network Projection (SSNP) \cite{espadoto2021ssnp} incorporates neighborhood information by jointly optimizing reconstruction and classification at the output layer, using pseudo-labels generated through clustering. Neighborhood Reconstructing Autoencoders \cite{LEE2021nrae} extend reconstruction tasks by incorporating the neighbors of \( \mathbf{x} \) alongside \( \mathbf{x} \) itself, using a local quadratic approximation of the decoder at \( f(\mathbf{x}) = \mathbf{z} \) to better capture the local geometry of the decoded manifold. Similarly, Geometric Autoencoders \cite{Nazari2023geometric} introduce a regularization term in the reconstruction loss, leveraging the generalized Jacobian determinant computed at \( f(\mathbf{x}) = \mathbf{z} \) to mitigate local contractions and distortions in latent representations.

\subsection{Kernel methods for out-of-sample extension}\label{subsec:kernel_methods}
Let $k(\cdot, \cdot)$ be a data-dependent symmetric positive definite kernel function $(\mathbf{x}, \mathbf{x}')\mapsto k(\mathbf{x}, \mathbf{x}')\geq 0$. For simplicity, we consider normalized kernel functions that satisfy the sum-to-one property $\sum_{i=1}^N k(\mathbf{x}, \mathbf{x}_i)=1$. Kernel methods for out-of-sample extensions seek an embedding function $\mathbf{k}\mapsto f(\mathbf{k})=\mathbf{z}\in \mathbb{R}^d$ where the input
$
    \mathbf{k} =  \mathbf{k}_{\mathbf{x}} = \begin{bmatrix}
     k(\mathbf{x}, \mathbf{x}_1)  & \hdots &     
     k(\mathbf{x}, \mathbf{x}_N)
\end{bmatrix}
$
is an $N$-dimensional similarity vector representing pairwise proximities between any instance $\mathbf{x}$ and all the points in the training set $X$. Under the linear assumption \(f(\mathbf{k}) = \mathbf{k}\mathbf{W}\), where \(\mathbf{W} \in \mathbb{R}^{N \times d}\) is a projection matrix to be determined, we directly define \(\mathbf{W}\) in the context of a regression task \cite{Gisbrecht2012oos, Gisbrecht2015kernel-tsne} by minimizing the least-squares error
\begin{equation}\label{eq:least-squares}
    \sum_{i=1}^N \|\mathbf{z}_i^G - f(\mathbf{k}_i)\|_2^2,
\end{equation}
which yields the explicit solution
$\mathbf{W} = \mathbf{K}^{-1}\mathbf{Y}$,
where \(\mathbf{K}^{-1}\) refers to the pseudo-inverse of the training Gram matrix $\mathbf{K}=[k(\mathbf{x}_i, \mathbf{x}_j)]_{1\leq i,j \leq N}$, and \(\mathbf{Y} = \begin{bmatrix}
    \mathbf{z}_1^G & \cdots & \mathbf{z}_N^G
\end{bmatrix}^T\) contains the precomputed training manifold embeddings. In particular, for manifold learning algorithms that directly assign the low-dimensional coordinates from the eigenvectors of $\mathbf{K}$, e.g., Laplacian Eigenmaps \cite{belkin2001laplacian}, we have the well-known Nyström formula \(\mathbf{W} = \mathbf{U}\mathbf{\Lambda}^{-1}\) \cite{bengio2003out, Arias2007connecting, Chen2013sparse}, where \(\mathbf{\Lambda}^{-1} = \text{diag}\left(  \lambda_1^{-1}, \hdots, \lambda_d^{-1} \right)\). Here, \(\lambda_i\) are the $d$ largest (or smallest, depending on the method) eigenvalues of $\mathbf{K}$, and \(\mathbf{U}\) is the matrix whose columns are the corresponding eigenvectors. In Locally Linear Embedding \cite{saul2003think} and PHATE \cite{Moon2019phate}, the authors suggested a default out-of-sample extension through linear reconstruction $\mathbf{W}=\begin{bmatrix}
    \mathbf{z}_1^G & \cdots & \mathbf{z}_N^G
\end{bmatrix}^T$. In diffusion-based methods, this provides an alternative means to compress the diffusion process through the training points---the \textit{landmarks}---and has been shown to outperform a direct application of the Nyström extension to diffusion maps~\cite{gigante2019compressed}.

Unlike direct parametric extensions discussed in Section \ref{subsec:multitask_autoencoders}, kernel extensions learn an explicit embedding function using kernel representations rather than the original representations in the feature space. While kernel methods are powerful for handling high-dimensional datasets, they require computing pairwise kernels for all points in the training set, which can become computationally expensive for large datasets. In such cases, feature mappings offer greater scalability. However, kernel extensions have been shown to outperform direct parametric methods in unsupervised out-of-sample visualization and classification tasks \cite{Gisbrecht2015kernel-tsne, Ran2024kumap}. Additionally, in supervised visualization, a carefully chosen kernel mapping can effectively filter out irrelevant features, whereas feature mappings treat all features equally, potentially increasing sensitivity to noisy datasets. Therefore, we align our out-of-sample extension framework with kernel extensions rather than direct parametric methods, as empirically demonstrated in Appendix \ref{sec:kernel_vs_feature}.


While traditional kernel extensions have demonstrated computational benefits, they are limited to linear kernel mappings and are primarily designed for unsupervised data visualization or classification using either unsupervised or class-conditional kernel functions \cite{Gisbrecht2015kernel-tsne, Ran2024kumap}. In this work, we expand the search space of the standard least-squares minimization problem in (\ref{eq:least-squares}) to include general, potentially nonlinear kernel mapping functions \(f\). We also propose a supervised kernel mapping based on Random Forests, specifically tailored for supervised data visualization. Building on the previous Section \ref{subsec:multitask_autoencoders}, we integrate this regression task with a geometrically motivated regularizer within a multi-task autoencoder framework.