%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Random Forest Autoencoders for Guided Representation Learning}

\begin{document}

\twocolumn[
\icmltitle{Random Forest Autoencoders for Guided Representation Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Adrien Aumon}{dms-udem,mila}
\icmlauthor{Shuang Ni}{mila,diro-udem}
\icmlauthor{Myriam Lizotte}{dms-udem,mila}
\icmlauthor{Guy Wolf}{dms-udem,mila}
\icmlauthor{Kevin R. Moon}{dms-utah}
\icmlauthor{Jake S. Rhodes}{stat-brig}
\end{icmlauthorlist}

\icmlaffiliation{dms-udem}{Department of Mathematics and Statistics, Université de Montréal, Montreal, Canada}
\icmlaffiliation{mila}{Mila - Quebec AI Institute, Montreal, Canada}
\icmlaffiliation{diro-udem}{Department of Computer Science and Operations Research, Université de Montréal, Montreal, Canada}
\icmlaffiliation{dms-utah}{Department of Mathematics and Statistics, Utah State University, Logan, Utah, USA}
\icmlaffiliation{stat-brig}{Department of Statistics, Brigham Young University, Provo, Utah, USA}

% \icmlcorrespondingauthor{Kevin R. Moon}{kevin.moon@usu.edu}
\icmlcorrespondingauthor{Jake S. Rhodes}{rhodes@stat.byu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Manifold learning, Random Forest Proximities, Regularized Autoencoders, Semi-supervised Visualization, Out-of-sample Extension}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}

Decades of research have produced robust methods for unsupervised data visualization, yet supervised visualization—where expert labels guide representations—remains underexplored, as most supervised approaches prioritize classification over visualization. Recently, RF-PHATE, a diffusion-based manifold learning method leveraging random forests and information geometry, marked significant progress in supervised visualization. However, its lack of an explicit mapping function limits scalability and prevents application to unseen data, posing challenges for large datasets and label-scarce scenarios. To overcome these limitations, we introduce Random Forest Autoencoders (RF-AE), a neural network-based framework for out-of-sample kernel extension that combines the flexibility of autoencoders with the supervised learning strengths of random forests and the geometry captured by RF-PHATE. RF-AE enables efficient out-of-sample supervised visualization and outperforms existing methods, including RF-PHATE's standard kernel extension, in both accuracy and interpretability. Additionally, RF-AE is robust to the choice of hyper-parameters and generalizes to any kernel-based dimensionality reduction method.

\end{abstract}





\section{Introduction}

Manifold learning methods, such as $t$-SNE \cite{vanDerMaaten2008tsne}, UMAP \cite{McInnes2018umap}, and PHATE \cite{Moon2019phate}, are essential for exploring high-dimensional data by revealing patterns, clusters, and outliers through low-dimensional embeddings. While traditional methods excel at uncovering dominant data structures, they often fail to capture task-specific insights when auxiliary labels or metadata are available. Supervised approaches like RF-PHATE \cite{rhodes2024gaining} bridge this gap by integrating label information into the kernel function through Random Forest-derived proximities \cite{rhodes2023geometry}, generating representations that align with domain-specific objectives without introducing the exaggerated separations or distortions seen in class-conditional methods \cite{hajderanj2021impactsupman}. Specifically, RF-PHATE has provided critical insights in biology, such as identifying multiple sclerosis subtypes, demonstrating antioxidant effects in lung cancer cells, and aligning COVID-19 antibody profiles with patient outcomes \cite{rhodes2024gaining}.

Still, most supervised and unsupervised manifold learning algorithms generate fixed coordinates within a latent space but lack a mechanism to accommodate new observations. In other words, to embed previously unseen data, the algorithm must rerun with the new data as part of the training set. One well-known solution to this lack of out-of-sample support is the Nyström extension~\cite{bengio2003out} and its variants, such as linear reconstruction~\cite{roweis2000lle} or geometric harmonics~\cite{coifman2006geometric}. These methods derive an empirical mapping function for new points defined as the linear combination of precomputed training embeddings, weighted with a kernel function expressing the similarities between the new point and the points in the training set. However, these methods are restricted to linear kernel mappings and are based on an unconstrained least-squares minimization problem \cite{bengio2003out}, which makes them sensitive to the quality of the training set and prone to failing in accurately recovering the true manifold structure \cite{Rudi2015less, MendozaQuispe2016extreme}. As an alternative extension method, recent neural network-based approaches, such as Geometry-Regularized Autoencoders (GRAE)~\cite{duque2022geometry}, offer a promising solution for extending embeddings to out-of-sample data points.

In this study, we present Random Forest Autoencoders (RF-AE), an autoencoder (AE) architecture that addresses out-of-sample extension, while taking inspiration from the principles of GRAE~\cite{duque2022geometry}, which uses a manifold embedding to regularize the bottleneck layer. Instead of reconstructing the original input vectors, RF-AE incorporates supervised information by reconstructing Random Forest- Geometry- and Accuracy-Preserving (RF-GAP) proximities \cite{rhodes2023geometry}, enabling us to learn an embedding function extendable to new data in a supervised context. Furthermore, we introduced a proximity-based landmark selection approach to reduce time and space complexities during training and inference. Notably, our approach extends out-of-sample examples without relying on label information for new points, which is helpful in situations where label information is expensive or scarce.

We empirically showed that RF-AE outperforms existing approaches in embedding new data while preserving the local and global structure of the important features for the underlying classification task. RF-AE improves the adaptability and scalability of the manifold learning process, allowing for seamless integration of new data points while maintaining the desirable traits of established embedding methods.


\section{Related work}




\subsection{Regularization with multi-task autoencoders}\label{subsec:multitask_autoencoders}
Given a high-dimensional training dataset \( X = \{\mathbf{x}_i \in \mathbb{R}^D \mid i = 1, \hdots, N\} \)—where \( X \) can represent tabular data, images, or other modalities—a manifold learning algorithm can be extended to test data by training a neural network to regress onto the precomputed non-parametric embeddings $\mathbf{z}_i^G$, or by means of a cost function underlying the manifold learning algorithm, as in parametric  $t$-SNE \cite{maaten2009parametric_tsne} and parametric UMAP \cite{sainburg2021parametric_umap}. However, solely training a neural network for this supervised task often leads to an under-constrained problem, resulting in solutions that memorize the data but fail to capture meaningful patterns or generalize effectively \cite{zhang2016understanding, arpit2017closer}. Beyond implicit regularization techniques such as early stopping \cite{bourlard1989early}, dropout \cite{Wager2013dropout}, or layer-wise pre-training \cite{Bengio2006pretraining}, multi-task learning \cite{Caruana1997multitask} has been shown to improve generalization. Early studies demonstrated that jointly learning tasks reduces the number of required samples \cite{baxter1995learning, baxter2000inductive_bias}, while later work introduced trace-norm regularization on the weights of a linear, single hidden-layer neural network for a set of tasks \cite{Maurer2006multitask, pontil2013multitask}. Motivated by Le et al. \yrcite{Le2018supervised_autoencoders}, who empirically showed that training neural networks to predict both target embeddings and inputs (reconstruction) improves generalization compared to encoder-only architectures, we focus on multi-task learning-based regularization in the context of regularized autoencoders.

AE networks serve two primary purposes: first, they learn an encoder function \( f(\mathbf{x}_i) = \mathbf{z}_i \in \mathbb{R}^d \) (\( d \ll D \)), which compresses the input data into a latent representation via a bottleneck layer \cite{theis2017lossy}. Second, they learn a decoder function \( g(\mathbf{z}_i) = \hat{\mathbf{x}}_i \), which maps the low-dimensional embedding back to the original input space. In essence, AEs aim to optimize the encoding and decoding functions such that \( \hat{\mathbf{x}}_i \approx \mathbf{x}_i \) by minimizing the reconstruction loss
\begin{align*}
    L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
\end{align*}
% \begin{align*}
%     L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, g(f(\mathbf{x}_i))) 
%     \\&= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
% \end{align*}
where \( L_{\text{recon}}(\cdot, \cdot) \) is typically defined as the squared Euclidean distance. This loss quantifies the ability of the AE to reconstruct the input. Through optimization, AEs learn compact data representations while ensuring that these representations remain meaningfully related to the input data. However, standard AEs often fail to capture the intrinsic geometry of the data and do not produce embeddings aligned with human-interpretable visualizations \cite{duque2022geometry}. To address this, regularization techniques have been proposed to guide the optimization process toward better local minima in the latent space.

Structural Deep Network Embedding (SDNE) \cite{Wang2016sdne} preserves both first- and second-order graph neighborhoods for graph-structured data by combining adjacency vector reconstruction with Laplacian Eigenmaps \cite{belkin2001laplacian} regularization. Local and Global Graph Embedding Autoencoders \cite{wang2020lgae} enforce two constraints on the embedding layer: a local constraint to cluster \( k \)-nearest neighbors and a global constraint to align data points with their class centers. VAE-SNE \cite{graving2020vae-sne} integrates parametric $t$-SNE with variational AEs, enhancing global structure preservation while retaining $t$-SNE's strength in preserving local structure. GRAE \cite{duque2022geometry} explicitly impose geometric consistency between the latent space and precomputed manifold embeddings.

Other approaches focus on regularizing the decoder. Inspired by Denoising Autoencoders \cite{vincent2008extracting}, Generalized Autoencoders \cite{Wang2014gae} minimize the weighted mean squared error between the reconstruction \( \hat{\mathbf{x}} \) and the \( k \)-nearest neighbors of the input \( \mathbf{x} \), where weights reflect the predefined similarities between \( \mathbf{x} \) and its neighbors. Centroid Encoders (CE) \cite{Ghosh2022centroid} minimize within-class reconstruction variance to ensure that samples within the same class are mapped close to their respective centroids. Self-Supervised Network Projection (SSNP) \cite{espadoto2021ssnp} incorporates neighborhood information by jointly optimizing reconstruction and classification at the output layer, using pseudo-labels generated through clustering. Neighborhood Reconstructing Autoencoders \cite{LEE2021nrae} extend reconstruction tasks by incorporating the neighbors of \( \mathbf{x} \) alongside \( \mathbf{x} \) itself, using a local quadratic approximation of the decoder at \( f(\mathbf{x}) = \mathbf{z} \) to better capture the local geometry of the decoded manifold. Similarly, Geometric Autoencoders \cite{Nazari2023geometric} introduce a regularization term in the reconstruction loss, leveraging the generalized Jacobian determinant computed at \( f(\mathbf{x}) = \mathbf{z} \) to mitigate local contractions and distortions in latent representations.

\subsection{Kernel methods for out-of-sample extension}\label{subsec:kernel_methods}
Let $k(\cdot, \cdot)$ be a data-dependent symmetric positive definite kernel function $(\mathbf{x}, \mathbf{x}')\mapsto k(\mathbf{x}, \mathbf{x}')\geq 0$. For simplicity, we consider normalized kernel functions that satisfy the sum-to-one property $\sum_{i=1}^N k(\mathbf{x}, \mathbf{x}_i)=1$. Kernel methods for out-of-sample extensions seek an embedding function $\mathbf{k}\mapsto f(\mathbf{k})=\mathbf{z}\in \mathbb{R}^d$ where the input
$
    \mathbf{k} =  \mathbf{k}_{\mathbf{x}} = \begin{bmatrix}
     k(\mathbf{x}, \mathbf{x}_1)  & \hdots &     
     k(\mathbf{x}, \mathbf{x}_N)
\end{bmatrix}
$
is an $N$-dimensional similarity vector representing pairwise proximities between any instance $\mathbf{x}$ and all the points in the training set $X$. Under the linear assumption \(f(\mathbf{k}) = \mathbf{k}\mathbf{W}\), where \(\mathbf{W} \in \mathbb{R}^{N \times d}\) is a projection matrix to be determined, we directly define \(\mathbf{W}\) in the context of a regression task \cite{Gisbrecht2012oos, Gisbrecht2015kernel-tsne} by minimizing the least-squares error
\begin{equation}\label{eq:least-squares}
    \sum_{i=1}^N \|\mathbf{z}_i^G - f(\mathbf{k}_i)\|_2^2,
\end{equation}
which yields the explicit solution
$\mathbf{W} = \mathbf{K}^{-1}\mathbf{Y}$,
where \(\mathbf{K}^{-1}\) refers to the pseudo-inverse of the training Gram matrix $\mathbf{K}=[k(\mathbf{x}_i, \mathbf{x}_j)]_{1\leq i,j \leq N}$, and \(\mathbf{Y} = \begin{bmatrix}
    \mathbf{z}_1^G & \cdots & \mathbf{z}_N^G
\end{bmatrix}^T\) contains the precomputed training manifold embeddings. In particular, for manifold learning algorithms that directly assign the low-dimensional coordinates from the eigenvectors of $\mathbf{K}$, e.g., Laplacian Eigenmaps \cite{belkin2001laplacian}, we have the well-known Nyström formula \(\mathbf{W} = \mathbf{U}\mathbf{\Lambda}^{-1}\) \cite{bengio2003out, Arias2007connecting, Chen2013sparse}, where \(\mathbf{\Lambda}^{-1} = \text{diag}\left(  \lambda_1^{-1}, \hdots, \lambda_d^{-1} \right)\). Here, \(\lambda_i\) are the $d$ largest (or smallest, depending on the method) eigenvalues of $\mathbf{K}$, and \(\mathbf{U}\) is the matrix whose columns are the corresponding eigenvectors. In Locally Linear Embedding \cite{saul2003think} and PHATE \cite{Moon2019phate}, the authors suggested a default out-of-sample extension through linear reconstruction $\mathbf{W}=\begin{bmatrix}
    \mathbf{z}_1^G & \cdots & \mathbf{z}_N^G
\end{bmatrix}^T$. In diffusion-based methods, this provides an alternative means to compress the diffusion process through the training points---the \textit{landmarks}---and has been shown to outperform a direct application of the Nyström extension to diffusion maps~\cite{gigante2019compressed}.

Unlike direct parametric extensions discussed in Section \ref{subsec:multitask_autoencoders}, kernel extensions learn an explicit embedding function using kernel representations rather than the original representations in the feature space. While kernel methods are powerful for handling high-dimensional datasets, they require computing pairwise kernels for all points in the training set, which can become computationally expensive for large datasets. In such cases, feature mappings offer greater scalability. However, kernel extensions have been shown to outperform direct parametric methods in unsupervised out-of-sample visualization and classification tasks \cite{Gisbrecht2015kernel-tsne, Ran2024kumap}. Additionally, in supervised visualization, a carefully chosen kernel mapping can effectively filter out irrelevant features, whereas feature mappings treat all features equally, potentially increasing sensitivity to noisy datasets. Therefore, we align our out-of-sample extension framework with kernel extensions rather than direct parametric methods, as empirically demonstrated in Appendix \ref{sec:kernel_vs_feature}.


While traditional kernel extensions have demonstrated computational benefits, they are limited to linear kernel mappings and are primarily designed for unsupervised data visualization or classification using either unsupervised or class-conditional kernel functions \cite{Gisbrecht2015kernel-tsne, Ran2024kumap}. In this work, we expand the search space of the standard least-squares minimization problem in (\ref{eq:least-squares}) to include general, potentially nonlinear kernel mapping functions \(f\). We also propose a supervised kernel mapping based on Random Forests, specifically tailored for supervised data visualization. Building on the previous Section \ref{subsec:multitask_autoencoders}, we integrate this regression task with a geometrically motivated regularizer within a multi-task autoencoder framework.

\section{Methods}


We elaborate on the methodology related to our RF-AE framework to extend any kernel-based dimensionality reduction method with Random Forests and autoencoders for supervised data visualization. Specifically, we explain how we combined RF-GAP proximities and the visualization strengths of RF-PHATE with the flexibility of autoencoders to develop a new parametric supervised embedding method, Random Forest Autoencoders (RF-AE). Additionally, we define our evaluation metrics and provide a detailed description of our experimental setup, including the models and datasets used in our experiments and our hyper-parameter selection.



\subsection{RF-GAP kernel function}\label{subsec:oosRFGAP}

The RF-GAP proximity \cite{rhodes2023geometry} between observations $\mathbf{x}_i$ and $\mathbf{x}_j$ is defined as
\[p_{GAP}(\mathbf{x}_i, \mathbf{x}_j)=\dfrac{1}{\left|S_{i}\right|} \sum_{t \in S_{i}}  \frac{c_j(t) \cdot I\left(j \in J_{i}(t)\right)}{\left|M_{i}(t)\right|},\]
where $S_i$ denotes the out-of-bag trees for observation $\mathbf{x}_i$, $c_j(t)$ is the number of in-bag repetitions for observation $\mathbf{x}_j$ in tree $t$, $I(.)$ is the indicator function, $J_i(t)$ is the set of in-bag points residing in the terminal node of observation $\mathbf{x}_i$ in tree $t$, and $M_i(t)$ is the multiset of in-bag observation indices, including repetitions, co-occurring in a terminal node with $\mathbf{x}_i$ in tree $t$.

We describe how RF-GAP proximities can be extended to a new test set of out-of-sample points, which will be needed during inference. Each out-of-sample observation can be considered OOB. Thus, for an out-of-sample point $\mathbf{x}_o$, we calculate:
\[p_{GAP}(\mathbf{x}_o, \mathbf{x}_j)=\dfrac{1}{|T|} \sum_{t \in T}  \frac{c_j(t) \cdot I\left(j \in J_{o}(t)\right)}{\left|M_{o}(t)\right|},\]
where $T$ is the set of all trees in the forest of size $|T|$.


The RF-GAP definition requires that self-similarity be zero, that is, $p_{GAP}(\mathbf{x}_i, \mathbf{x}_i) = 0$. However, this is not suitable as a similarity measure in some applications. Due to the scale of the proximities (the rows sum to one, so the proximity values are all near zero for larger datasets), it is not practical to re-assign self-similarities to one. Otherwise, self-similarity would carry equal weight to the combined significance of all other similarities. Instead, we assign values by, in essence, passing down an identical OOB point to all trees where the given observation is in-bag. That is:

\[p_{GAP}(\mathbf{x}_i, \mathbf{x}_i)=\dfrac{1}{\left|\bar{S}_{i}\right|} \sum_{t \in \bar{S}_{i}}  \frac{c_i(t)}{\left|M_{i}(t)\right|},\]
where $\left|\bar{S}_{i}\right|$ is the set of trees for which $\mathbf{x}_i$ is in-bag. This formulation guarantees $p_{GAP}(\mathbf{x}_i, \mathbf{x}_i) \ge p_{GAP}(\mathbf{x}_i, \mathbf{x}_j)$ for all $j \ne i$. Under this formulation, $p_{GAP}(\mathbf{x}_i, \mathbf{x}_i)$ is on a scale more similar to other proximity values. Additionally, we symmetrize proximities by averaging:
$p_{GAP}(\mathbf{x}_i, \mathbf{x}_j) \mapsto \frac{1}{2}(p_{GAP}(\mathbf{x}_i, \mathbf{x}_j) + p_{GAP}(\mathbf{x}_j, \mathbf{x}_i))$. From now on, we will consider this symmetrized version and refer to it as $p_{GAP}$ for simplicity.


\subsection{RF-AE architecture}\label{subsec:rfae_arch}
\begin{figure}[ht]
    \centering
    \includegraphics[width = 0.4\textwidth]{figures/RF-AE-architecture.png}
    \caption{RF-AE architecture with landmark selection and geometric regularization. First, the original feature vectors $\mathbf{x}_i$ are transformed into one-step transition probability vectors $\mathbf{p}_i$ derived from RF-GAP proximities (Section \ref{subsec:oosRFGAP}). The input dimensionality is then reduced from $N$ to $N_{\text{land}}<<N$, where $\mathbf{p}^{\text{land}}_i$ represents one-step RF-GAP transition probabilities between instance $i$ and each of the $N_{\text{land}}$ cluster obtained by a variation of spectral clustering (Section \ref{subsec:landmark_selection}). Then, manifold embeddings $G=\{\mathbf{z}_i^G\mid i=1,\hdots,N\}$ are generated using RF-PHATE from the whole training set of kernel vectors $\mathbf{p}^{\text{land}}_1, \hdots, \mathbf{p}^{\text{land}}_N$ (Section \ref{subsec:mapping_oos}). Finally, $\mathbf{p}^{\text{land}}_i$ and $\mathbf{z}_i^G$ serve as input to the RF-AE network within the dashed box, training an encoder $f$ and a decoder $g$ by simultaneously minimizing the reconstruction loss $L_{recon}$ and the geometric loss $L_{geo}$ defined in Section \ref{subsec:choice_loss}.}
    \label{fig:rfae_arch}
\end{figure}

To leverage the knowledge gained from an RF model, we modify the traditional AE architecture to incorporate the RF's learning. The forest-generated proximity measures~\cite{rhodes2023geometry}, which indicate similarities between data points relative to the supervised task, serve as a foundation for extending the embedding while integrating the insights acquired through the RF's learning process. In our RF-AE, the original input vectors $\mathbf{x}_i\in \mathbb{R}^D$ used in the vanilla AE are now replaced with the row-normalized RF-GAP proximity vector between training instance $\mathbf{x}_i$ and all the other training instances, including itself. That is, each input $\mathbf{x}_i$ used for training is now represented as an $N$-dimensional vector $\mathbf{p}_i$ encoding local-to-global supervised neighbourhood information around $\mathbf{x}_i$, defined as
\begin{align*}
    \mathbf{p}_i &= \begin{bmatrix}
    \mathbf{p}_{i}[1] & \hdots & \mathbf{p}_{i}[N]
\end{bmatrix}
\in [0,1]^N,\\
\mathbf{p}_{i}[j] &= \frac{
     p_{GAP}(\mathbf{x}_i, \mathbf{x}_j)
}{\sum_{j=1}^N p_{GAP}(\mathbf{x}_i, \mathbf{x}_j)},  
\end{align*}
where $p_{GAP}(\cdot,\cdot)$ denotes the kernel function defined by the symmetrized RF-GAP proximities with self-similarity (Section \ref{subsec:oosRFGAP}). Thus, elements of $\mathbf{p}_i$ sum to one so that $\mathbf{p}_i$ contains one-step transition probabilities from observation with index $i$ to its supervised neighbors indexed $j=1,\hdots,N$ derived from RF-GAP proximities. Note that this construction is also performed in RF-PHATE  to capture global information by powering the resulting diffusion operator. For computational efficiency, we further reduced the input dimensionality of $\mathbf{p}_i$ from to $N$ to $N_{\text{land}}<<N$ using a variation of spectral clustering, as described in Section \ref{subsec:landmark_selection}. %Similarly to GRAE \cite{duque2022geometry}, we added a geometric loss to ensure that the network embeddings $\mathbf{z}_i$ agree with the local-to-global supervised geometry learned by known RF-PHATE embeddings $\mathbf{z}_i^G$. 
Thus, the reconstruction loss for the encoder $f$ and decoder $g$ of the RF-AE network are  obtained through mini-batch gradient descent by minimizing
\begin{align*}
    L(f, g) &= \frac{1}{N}\sum_{i=1}^N L_{recon}(\mathbf{p}_i, \hat{\mathbf{p}}_i).
\end{align*}
% \begin{align*}
%     L(f, g) &= \frac{1}{N}\sum_{i=1}^N L_{recon}(\mathbf{p}_i, g(f(\mathbf{p}_i)))\\
%     &=\frac{1}{N}\sum_{i=1}^N L_{recon}(\mathbf{p}_i, \hat{\mathbf{p}}_i)
% \end{align*}
The definition and justification of $L_{recon}$  can be found in Section \ref{subsec:choice_loss}. For out-of-sample points, we construct transition probability vectors derived from RF-GAP proximities (Section \ref{subsec:oosRFGAP}), which are then input to the learned encoder to produce supervised out-of-sample embeddings. 


Given a learned set of low-dimensional manifold embeddings $G=\{\mathbf{z}^G_i\in \mathbb{R}^d\mid i=1,\hdots, N\}$, we additionally force the RF-AE to learn a latent representation $f(\mathbf{p}_i)=\mathbf{z}_i$ similar to its precomputed counterpart $\mathbf{z}_i^G$ via an explicit geometric constraint to the bottleneck layer, similar to GRAE~\cite{duque2022geometry}. This translates into an added term in the loss formulation, which now takes the form:
$$
L(f, g) = \frac{1}{N} \sum_{i=1}^N \Big[ 
    \lambda L_{recon}(\mathbf{p}_i, \hat{\mathbf{p}}_i) 
    + (1 - \lambda) L_{geo}(\mathbf{z}_i, \mathbf{z}_i^G) 
\Big].
$$
The parameter $\lambda \in [0,1]$ controls the degree to which the precomputed embedding is used in encoding $\mathbf{x}_i$: $\lambda = 1$ is equivalent to our vanilla RF-AE model, while $\lambda=0$ reproduces $\mathbf{z}_i^G$ as in the standard kernel mapping formulation. This multi-task approach not only estimates a manifold-learning function that can extend the manifold to new data points, but it also learns a function to reconstruct the underlying kernel function from the embedding space. In this work, we set $d=2$ to ensure visual interpretability and use RF-PHATE as the geometric constraint due to its effectiveness in supervised data visualization \cite{rhodes2021rfphate, rhodes2024gaining}, although any dimensionality reduction method can be extended this way. Refer to Fig.~\ref{fig:rfae_arch} for a comprehensive illustration of our RF-AE architecture.


\subsection{Choice of reconstruction and geometric losses}\label{subsec:choice_loss}

We use the standard Euclidean distance for the geometric loss to match the standard least-squares formulation in Equation \ref{eq:least-squares}. Naively, we could also define the reconstruction loss as the squared Euclidean distance between input RF proximity vectors $\mathbf{p}_i$ and their reconstruction. However, since most entries are near zero in larger datasets, the learning process overemphasizes reconstructing zero-valued entries over positive ones. Unlike positive entries, which quantify links between point $i$ and its supervised neighbors, near-zero values do not necessarily encode repulsive forces. Consequently, the standard loss formulation risks converging to a local minimum where the latent representation fails to capture the supervised neighborhoods learned by RF-GAP proximities. One solution is to impose a higher penalty on reconstructing nonzero elements \cite{Wang2016sdne}, but this introduces an additional hyper-parameter and increases model complexity.

Here, we use the fact that each training input $\mathbf{p}_i$ and its reconstruction $\hat{\mathbf{p}}_i= g(f(\mathbf{p}_i))$ form probability distributions. We thus leverage statistical distances instead of Euclidean distances. That is, we define
\begin{align*}
L_{recon}(\mathbf{p}_i, \hat{\mathbf{p}}_i) &= D_{\mathrm{KL}}(\mathbf{p}_i || \hat{\mathbf{p}}_i)
=\sum_{j=1}^N \mathbf{p}_i[j] \log\left(\frac{\mathbf{p}_i[j]}{\hat{\mathbf{p}}_i[j]}\right),\\
L_{geo}(\mathbf{z}_i,\mathbf{z}_i^G) &= \| \mathbf{z}_i - \mathbf{z}_i^G \|_2^2,
\end{align*}

where $D_{\mathrm{KL}}(p || q)$ denotes the Kullback–Leibler (KL) divergence \cite{kullback1951information} of an actual distribution $p$ from a predicted distribution $q$. This definition of $L_{recon}$ naturally guides the learning process toward a latent representation that reflects RF-GAP supervised neighbourhoods. Indeed, the asymmetry of KL loss function penalizes pairs of nearby points in the RF-GAP space that are poorly reconstructed more heavily than faraway points
(since the corresponding $\mathbf{p}_i[j] \gg \hat{\mathbf{p}}_i[j] \approx 0$). Thus, the optimization of RF-AE favours latent representations that capture local-to-global supervised neighbourhoods, without requiring the specification of a strict cut-off. Moreover, since RF-PHATE efficiently captures both local and global supervised information, the geometric term $L_{geo}$ is potentially helpful to reach a better local minimum for $L_{recon}$, ensuring a latent representation of supervised neighbourhoods that is geometrically insightful while maintaining global consistency.


\subsection{Input dimensionality reduction with landmark selection}\label{subsec:landmark_selection}

The input dimensionality of our RF-AE architecture scales with the training size $N$, which may cause memory issues during GPU-optimized training when dealing with voluminous training sets. To improve scalability, we applied a variation of spectral clustering \cite{Moon2019phate} on the RF-GAP diffusion operator $\mathbf{P}=\begin{bmatrix}
    \mathbf{p}_1, \hdots, \mathbf{p}_N
\end{bmatrix}^T\in \mathbb{R}^{N\times N}$. First, we applied PCA on $\mathbf{P}$ and used $k$-means clustering on the principal components to partition the data into $k=N_{\text{land}} \ll N$ clusters $C_1, \hdots, C_{N_{\text{land}}}$. Then, for any point $i$, instead of using its RF-GAP transition probabilities to every training points $j$ as before, we form the RF-GAP transition probability to each cluster $C_j$ as $\mathbf{p}^{\text{land}}_i[j]=\sum_{\xi\in C_j}\mathbf{P}[i,\xi]$.

\subsection{Mapping out-of-sample instances}\label{subsec:mapping_oos}


Thanks to the extended definition of RF-GAP proximities (Section \ref{subsec:oosRFGAP}), we can embed an out-of-sample (OOS) sample $\mathbf{x}_o$ by computing its normalized RF-GAP proximities from every point in the training set to form a probability vector $\mathbf{p}_o\in [0,1]^N$. Then, we compute $\mathbf{p}_o^{\text{land}}=\begin{bmatrix}
    \mathbf{p}_o^{\text{land}}[1] \hdots \mathbf{p}_o^{\text{land}}[N_{\text{land}}]
\end{bmatrix}\in [0,1]^{N_{\text{land}}}$ representing one-step transition probabilities to the $N_{\text{land}}$ clusters obtained during the training phase as described in  Section \ref{subsec:landmark_selection}. Finally, $\mathbf{p}_o^{\text{land}}$ is fed into the encoder of the trained network to produce the OOS embedding $\mathbf{z}_o$. Thus, both new and existing data can be effectively incorporated into the learned embedding space while maintaining the integrity of the underlying manifold structure and without retraining either the forest or rerunning the full RF-PHATE embedding algorithm. 

\subsection{Quantifying supervised out-of-sample embedding fit}\label{subsec:quantify_oos_embedding}
In unsupervised data visualization, we measure the performance of a model in producing low-dimensional embeddings \( Z \subset \mathbb{R}^d \) by quantifying the preservation of structural properties from the original data \( X \subset \mathbb{R}^D \) using a structure preservation scoring function \( s: (X, Z) \mapsto s(X, Z) \in \mathbb{R} \). For instance, the degree to which an embedding preserves the local neighborhood around each observation quantifies local structure preservation \cite{sainburg2021parametric_umap}, while global structure preservation is assessed by comparing pairwise distances between the embedding and input spaces \cite{Kobak2019tsne}.

In supervised data visualization, we are not interested in naively preserving the original structure, as we assume that some features are more important than others. Instead, we aim to measure the ability of an embedding method to emphasize structure preservation for the most important features. To address this, we adopt a similar evaluation scheme to Rhodes et al. \yrcite{rhodes2024gaining}. Given a training/test split \((X, Y) = (X_{\text{train}} \cup X_{\text{test}}, Y_{\text{train}} \cup Y_{\text{test}})\), we first generate pseudo-ground truth permutation feature importances \(\mathcal{C} = \{\mathcal{C}_1, \hdots, \mathcal{C}_D\}\) relative to the classification task using a baseline classifier, typically \(k\)-NN. Since we consider relatively larger datasets, both in terms of the number of features and sample sizes, we replace the standard data perturbation approach with a correlation-aware random sampling strategy \cite{kaneko2022cvpfi} (Appendix \ref{sec:sampling}).

To assess the extent to which the feature hierarchy in \(\mathcal{C}\) is preserved in the test embeddings \( f(X_{\text{test}}) = Z_{\text{test}} \), generated by a parametric model trained on \((X_{\text{train}}, Y_{\text{train}})\), we compute another set of feature importances \(\mathcal{S} = \{\mathcal{S}_1, \hdots, \mathcal{S}_D\}\), derived from a structure preservation scoring function \(s\). Specifically, \(\mathcal{S}_i = s(\Tilde{X}_{\text{test}}^i, Z_{\text{test}})\), where \(\Tilde{X}_{\text{test}}^i\) is the perturbed input test set for feature \(i\), obtained via Algorithm \ref{alg:sampling}. The idea is that if the OOS structure preservation score is relatively low after perturbing feature \(i\) and its correlated features in the original space, then the OOS embedding structure is dominated by feature \(i\).

Finally, an effective embedding method should ensure that the structure of its generated OOS embeddings is primarily influenced by the most important features for the underlying classification task. To quantify this, we define an importance preservation score as
$\rho_s = \rho(\mathcal{C}, \mathcal{S}),$
where \(\rho\) represents a rank-based correlation. In practice, \(\rho_s\) is computed using various scoring functions \(s\) to evaluate the method's ability to preserve the important structures across local-to-global scales.


\section{Results}

\subsection{Quantitative comparison}\label{subsec:quant_comp}

We assessed the supervised structure preservation capabilities of RF-AE in comparison with several baseline methods on 19 datasets spanning diverse domains. Each dataset contained a minimum of 1,000 samples and at least 10 features. Training and OOS embeddings were generated using an 80/20 stratified train/test split, except for Crowdsourced Mapping, Isolet, Landsat Satellite, and Optical Digits, where predefined splits were used. Detailed descriptions of the datasets are provided in Appendices \ref{sec:artificial_tree} and \ref{sec:data}.

Supervised structure preservation was quantified locally using \( s = QNX \), which measures the proportion of $k$-nearest neighbors in the embedding space that are also $k$-nearest neighbors in the original space \cite{sainburg2021parametric_umap}. To evaluate supervised structure preservation from a global perspective, we also considered \( s = {Spear} \), which computes the Spearman correlation between pairwise distances in the embedding and original spaces \cite{Kobak2019tsne}.  

The quantitative experiments were repeated 10 times, ranking each model per dataset based on $\rho_{QNX}$ and $\rho_{Spear}$. The final local and global rankings were obtained by averaging these ranks across all datasets for each model. To provide a comprehensive performance assessment across local-to-global scales, we also computed a composite score as the average of the final local and global ranks. The results are presented in Table \ref{tab:quantitative_comp}.

We compared RF-AE with \( \lambda = 10^{-3} \) to the default RF-PHATE kernel out-of-sample extension by linear reconstruction \cite{Moon2019phate} (Section \ref{subsec:kernel_methods}) and the kernel extension through an encoder-only neural network ($\lambda = 0$), denoted as RF-PHATE (Ker.) and RF-PHATE (Enc.), respectively. We also included RF-AE without geometric regularization ($\lambda=1$), denoted simply as RF-AE. Additionally, we evaluated eleven baseline methods, including vanilla AE, Principal Component Analysis (PCA), parametric \( t \)-SNE (P-TSNE) \cite{maaten2009parametric_tsne}, CE \cite{Ghosh2022centroid}, CEBRA \cite{Schneider2023cebra}, SSNP \cite{espadoto2021ssnp} (supervised version), Linear Optimal Low Rank Projection (LOL) \cite{Vogelstein2021lol}, Neighborhood Component Analysis (NCA) \cite{goldberber2004nca}, partial least squares discriminant analysis (PLS-DA), supervised PCA, and parametric supervised UMAP (P-SUMAP) \cite{sainburg2021parametric_umap}. Experimental setting details can be found in Appendix \ref{sec:rfae_exp_setting}.


\begin{table}[ht]
\caption{Average local and global ranks of 15 embedding methods across 19 datasets. Methods are sorted according to their overall rank as shown in the last column $\langle \textsc{Rank}\rangle$. Supervised methods are marked by an asterisk. The top two ranks are in bold, while the top ranks are also underlined. Unsupervised methods like parametric $t$-SNE, PCA, and vanilla autoencoders performed poorly, lacking supervised guidance, while RF-AE struggled without regularization due to difficulties in achieving good local minima. Introducing a small geometric regularization term ($\lambda=10^{-3}$) in RF-AE significantly improved performance, making it the top method overall, especially for preserving supervised global structure, unlike RF-PHATE's standard kernel extensions, which struggled to generalize effectively.}
\label{tab:quantitative_comp}
\vskip 0.1in
\centering
\small
\begin{sc}
\begin{tabular}{lccc}
\toprule
 & Local & Global & $\langle$Rank$\rangle$ \\
\midrule
RF-AE* ($\lambda=10^{-3}$) & \phantom{0}\underline{\textbf{4.25}} & \phantom{0}\underline{\textbf{4.50}} & \phantom{0}\underline{\textbf{4.38}} \\
NCA* & \phantom{0}6.25 & \phantom{0}\textbf{5.00} & \phantom{0}5.63 \\
RF-PHATE (Ker.)* & \phantom{0}\textbf{5.30} & \phantom{0}7.00 & \phantom{0}6.15 \\
RF-PHATE (Enc.)* & \phantom{0}5.60 & \phantom{0}6.70 & \phantom{0}6.15 \\
CE* & \phantom{0}6.25 & \phantom{0}8.95 & \phantom{0}7.60 \\
CEBRA* & \phantom{0}8.30 & \phantom{0}7.15 & \phantom{0}7.73 \\
RF-AE* & \phantom{0}5.60 & 10.50 & \phantom{0}8.05 \\
SSNP* & \phantom{0}6.45 & \phantom{0}9.95 & \phantom{0}8.20 \\
SPCA* & 10.60 & \phantom{0}6.75 & \phantom{0}8.68 \\
LOL* & \phantom{0}9.60 & \phantom{0}8.15 & \phantom{0}8.88 \\
PLS-DA* & \phantom{0}9.60 & \phantom{0}8.60 & \phantom{0}9.10 \\
P-TSNE & \phantom{0}9.85 & \phantom{0}8.45 & \phantom{0}9.15 \\
PCA & 11.80 & \phantom{0}7.55 & \phantom{0}9.68 \\
P-SUMAP* & \phantom{0}9.35 & 10.20 & \phantom{0}9.78 \\
AE & 10.55 & \phantom{0}9.85 & 10.20 \\
\bottomrule
\end{tabular}
\end{sc}
\vskip -0.1in
\end{table}

Unsurprisingly, unsupervised methods such as parametric $t$-SNE, PCA, and vanilla autoencoders performed poorly, as they lack supervised knowledge. RF-AE without regularization also struggled to preserve feature importances, likely due to the well-known difficulty of achieving good local minima without regularization. Despite its relatively strong performance in preserving local importance, it struggled to capture global structures effectively. This outcome aligns with our reconstruction loss, which is designed to reconstruct RF-GAP neighborhoods. The kernel extension of RF-PHATE with a neural network encoder did not demonstrate a clear advantage over the default kernel extension, underscoring the need for a more advanced architecture to improve generalization. However, introducing a small geometric regularization term ($\lambda=10^{-3}$) in RF-AE significantly boosted its performance, establishing it as the top-performing approach overall. Compared to both RF-PHATE kernel extensions, the primary benefits of our regularized RF-AE lie in better preserving supervised global structure. This observation is consistent with previous studies \cite{graving2020vae-sne}, which showed that incorporating a reconstruction term into the cost function of local methods such as $t$-SNE enhances global structure consistency without compromising local structure preservation. Compared to the results in \cite{rhodes2024gaining}, where RF-PHATE was quantitatively the best method overall on training data across local-to-global scales, RF-PHATE here struggles to consistently outperform other methods through its standard kernel extensions. This further justifies the effectiveness of our multi-task kernel extension via autoencoders for enhanced generalization performance. We also demonstrated that the superiority of RF-AE is robust the choice of $\lambda$ and the number of selected landmarks $N_{\text{land}}$, as shown in the ablation studies in Appendix \ref{sec:ablation_supplemental}.




\subsection{Out-of-sample visualization}\label{subsec:oos_viz}
We qualitatively assessed the capability of four methods to embed OOS instances on Sign MNIST (A--K) and Zilionis single-cell data (subset). Each model was trained on the training subset, and we subsequently mapped the test set with the learned encoder. Fig. \ref{fig:viz_sign_mnist_zilionis} depicts the resulting visualizations.

\begin{figure*}[ht]
    \centering
    \includegraphics[width = 0.95\textwidth]{figures/sign_mnist_Zilionis.png}
    \caption{Out-of-sample visualization using four different dimensionality reduction methods.
    \textbf{a.} Sign MNIST (A--K) dataset (Table \ref{tab:rfae_data}): training points are colored by labels, and test points are depicted with original images. RF-AE with RF-PHATE regularization ($\lambda=10^{-3}$) reveals local and global supervised structures, preserving class-specific variations in shadowing and hand orientations as well as similarities between classes. The default RF-PHATE's kernel extension compresses clusters excessively. Parametric $t$-SNE and supervised UMAP demonstrate sensitivity to irrelevant features.
    \textbf{b.} Zilionis single-cell dataset: training points (circles) and test points (triangles) are colored by major cell type. RF-AE with RF-PHATE regularization ($\lambda=10^{-3}$) effectively preserves relationships between cell types, maintaining the similarity of related immune populations while retaining distinctions that reflect their functional and developmental differences. Other methods either over-compress related subsets or over-separate clusters, distorting biological insights. }
    \label{fig:viz_sign_mnist_zilionis}
\vskip -0.1in
\end{figure*}


% From the Sign MNIST plot in Fig. \ref{fig:viz_sign_mnist_zilionis}a, RF-AE with \(\lambda=10^{-3}\) effectively inherits the global structure of the RF-PHATE embeddings while providing greater detail within class clusters. In contrast, RF-PHATE (Ker.) tends to compress representations within each cluster, which are associated with individual classes. Although OOS embeddings are mostly assigned to their correct ground truth labels, the local arrangement of these samples on the sub-manifold is not easily visualized in RF-PHATE (Ker.). RF-AE ($\lambda=10^{-3}$), however, expands the class clusters, revealing within-class patterns that are obscured in the RF-PHATE (Ker.) plot. For example, the top-right cluster in the RF-AE ($\lambda=10^{-3}$) plot illustrates different ways to represent the letter ``C'', showing a logical transition between variations based on hand shadowing and orientation. Such nuanced differences are more challenging to detect in RF-PHATE (Ker.), which compresses these representations into an overly restrictive branch structure. This limitation of RF-PHATE may stem from excessive reliance on the diffusion operator, which overemphasizes global smoothing. Since RF-GAP already captures local-to-global supervised neighborhoods effectively, the additional diffusion applied by RF-PHATE likely diminishes fine-grained local details. Thus, we have demonstrated that RF-AE offers a superior balance for visualizing the local-to-global supervised structure compared to the basic RF-PHATE kernel extension. P-TSNE is effective at identifying clusters of similar samples but often splits points from the same class into distinct, distant clusters. This appears to result from variations such as background shadowing, which obstruct the important part of the image. Thus, ``G'' and ``H'' instances are closer than expected due to similar shadowing. In contrast, regularized RF-AE correctly assigns ``G'' and ``H'' instances to their own clusters while dissociating between same-class points with different shadowing, effectively reflecting within-class variations. This demonstrates that P-TSNE is overly sensitive to irrelevant factors, such as background differences, which are unrelated to the underlying labels.  Similarly, P-SUMAP exhibits this sensitivity but produces a sparser representation. Despite being a supervised method, P-SUMAP incorporates class labels in a way that artificially clusters same-class points, potentially oversimplifying their intrinsic relationships. This behavior highlights the limitations of such methods in preserving both local and global class structure meaningfully.


From the Sign MNIST (A--K) plot in Fig. \ref{fig:viz_sign_mnist_zilionis}a, RF-AE ($\lambda=10^{-3}$) retains the overall shape of the RF-PHATE embedding while providing finer within-class detail. In contrast, RF-PHATE's default kernel extension compresses class clusters, making local relationships harder to discern. RF-AE expands these clusters, revealing within-class patterns, such as the logical transition between variations in shadowing and hand orientation to represent the letter ``C'' (top-right cluster). This detail is lost in RF-PHATE's default kernel extension, which over-relies on diffusion and smooths out local differences. P-TSNE identifies clusters well but often splits same-class samples due to irrelevant variations in background shadowing, grouping ``G'' and ``H'' too closely. RF-AE ($\lambda=10^{-3}$) avoids this issue, distinguishing within-class variations while preserving the class-specific clusters. P-SUMAP is also sensitive to irrelevant features and tends to artificially cluster neighboring points of the same class, leading to a sparse and fragmented embedding.

From the Zilionis single-cell dataset plot in Fig. \ref{fig:viz_sign_mnist_zilionis}b, RF-AE with \(\lambda=10^{-3}\) effectively retains biologically meaningful local and global relationships among major cell types. It preserves the hierarchical organization of blood-derived (b) and tissue-resident (t) cells, positioning them closely while maintaining clear separation. For example, bNeutrophils and tNeutrophils cluster together, reflecting their shared lineage while remaining distinct. Similarly, bB cells and tB cells, as well as bT cells and tT cells, form structured clusters that capture their biological relationships, where tissue-resident cells exhibit distinct gene expression and activation states \cite{gray2022tissue}. At the global scale, neutrophils are distinctly separated from B and T cells, highlighting their functional and molecular differences. Additionally, NK cells cluster closer to T cells than B cells, aligning with their role in cell-mediated immunity \cite{yoder2011phylogenetic}. RF-PHATE's default kernel extension and P-TSNE overly compresses clusters, leading to a loss of fine-grained distinctions between closely related cell types. This over-smoothing leads to biologically distinct subpopulations, such as bB and tB cells, becoming indistinguishable, making it difficult to capture meaningful transcriptional and functional variations. P-SUMAP over-separates clusters, with test points deviating from their corresponding training distributions. This over separation disrupts intrinsic cellular relationships and makes developmental transitions harder to interpret.


This qualitative analysis of the Sign MNIST and Zilionis single-cell datasets underscores the importance of regularization and methodological choices in creating meaningful embeddings for supervised tasks. Our RF-AE architecture, enhanced with RF-PHATE regularization, effectively preserves both local and global structures, outperforming existing methods.
% In the Sign MNIST case, RF-AE better reveals class clusters and within-class variations, while in the Zilionis dataset, it provides a more structured and interpretable mapping of immune cell populations, distinguishing closely related cell types and preserving broader immunological relationships. These capabilities make RF-AE valuable for tasks like classification and single-cell analyses, where capturing both fine-grained and broad patterns is critical. 
The visualizations and analyses of the other models, along with their quantitative comparisons, are provided in Appendix \ref{sec:oos_viz_supplemental}. 

\section{Discussion}
The significance of supervised dimensionality reduction lies in its ability to reveal meaningful relationships between features and label information. As shown in \cite{rhodes2024gaining}, RF-PHATE stands out as a strong solution in the domain of supervised data visualization. However, this method lacks an embedding function for out-of-sample extension. To overcome this limitation, we designed Random Forest Autoencoders (RF-AE), an AE-based architecture that reconstructs RF-GAP neighborhoods while preserving the latent supervised geometry learned by precomputed RF-PHATE embeddings. Our experimental results confirmed the utility of this autoencoder extension, illustrating its capacity to extend embeddings to new data points while preserving the intrinsic supervised manifold structure. We quantitatively demonstrated that RF-AE with RF-PHATE regularization outperforms baseline kernel extensions and other existing parametric embedding models in generating out-of-sample embeddings that retain feature importances essential for the underlying classification task. We also showed that RF-AE's superiority is robust to the choice of the hyper-parameters $\lambda$ and $N_{\text{land}}$ underlying the reconstruction strength and the input dimensionality reduction, respectively. Visually, we found that RF-AE regularized with RF-PHATE inherits the denoised local-to-global supervised structure learned by RF-PHATE, while increasing the embedding resolution for enhanced within-class structure visualization. This translates into a better local-global preservation tradeoff than baseline RF-PHATE kernel extensions. Other methods either over-emphasize class separability or fail to incorporate enough supervised knowledge, resulting in noisier visualizations. Furthermore, the ability to project unseen points without relying on labels makes our approach a promising solution for semi-supervised tasks with extensive datasets. 

Future work will focus on adaptive landmark selection and efficiency improvements to further alleviate the computational cost associated with RF-GAP calculations on large-scale datasets. Moreover, since the RF-GAP kernel function is compatible with any kernel-based dimensionality reduction method, we will focus on extending additional methods within our RF-AE framework.


% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}
This paper advances guided data representation learning by integrating expert-derived data annotations and enabling out-of-sample extension, allowing generalization beyond the training set. The proposed method assists decision-makers with interpretable visualizations while improving scalability and applicability in semi-supervised tasks. It has potential societal impacts in biomedical research, as well as data-driven insights in healthcare, finance, and multimedia.


% This paper presents work whose goal is to advance the field of guided data visualization using expert-derived data annotations. Potential societal consequences 
% of our work include the identification of complex patterns underlying domain-specific contexts, such as mechanism-based biomarker discovery.

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\twocolumn

% Add the prefix 'S' for Figures
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0} % Reset figure counter

% Add the prefix 'S' for Tables
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{0} % Reset table counter

% Add the prefix 'S' for Algorithms
\renewcommand{\thealgorithm}{S\arabic{algorithm}}
\setcounter{algorithm}{0} % Reset algorithm counter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Advantages of kernel mapping over feature mapping in supervised settings}\label{sec:kernel_vs_feature}

We designed our RF-AE framework under the premise that encoders operating on (supervised) kernel representations are better suited for supervised settings than those utilizing raw input features. This assumption stems from the ability of well-chosen kernel functions to effectively filter out irrelevant features, thereby enhancing the encoder’s robustness to highly noisy datasets. To empirically validate this, we conducted a toy experiment using the artificial tree dataset described in Appendices \ref{sec:artificial_tree} and \ref{sec:data}. To simulate a noisy input space, we augmented the dataset with 940 additional features sampled from a uniform distribution $U(0,1)$. A random subset of 1000 samples was selected to train both approaches to extend RF-PHATE, using the same neural network-based encoder and hyper-parameters as described in Appendix \ref{sec:rfae_exp_setting}. This setup produced a training set of 1000 samples, each with 1000 features, ensuring that the learning process of both approaches differed solely in their input representations. For the kernel extension, input vectors were computed using the RF-GAP kernel function (Section \ref{subsec:oosRFGAP}) and normalized to sum to one (Section \ref{subsec:rfae_arch}).

We evaluated the trained encoders on the remaining 440 test points and visualized their two-dimensional embeddings alongside the ground truth tree structure in Fig. \ref{fig:kernel_vs_feature}. Both approaches accurately captured the training data, confirming their ability to learn a suitable mapping function. However, notable differences were observed in the test embeddings. The kernel-based extension effectively recovered the tree structure, while the encoder using direct parametric extensions struggled to generalize, resulting in noisy embeddings with overlapping branches that significantly diverged from the ground truth tree structure. The abundance of irrelevant features disrupted the optimization process, causing it to overfit the noise and learn spurious relationships between the noisy input and the target embedding. As a result, the embeddings of unseen test points, dominated by noise, failed to align with the true underlying structure.


\begin{figure*}[ht]
    \centering
    \includegraphics[width = 0.9\textwidth]{figures/kernel_vs_feature.png}
    \caption{Comparison of training and test embeddings produced by feature-based (first row) and RF-GAP kernel-based (second row) encoders on the artificial tree dataset (Appendix \ref{sec:artificial_tree}), augmented with 940 noise features to simulate a high-noise setting. Both encoders accurately captured the training data (first column). However, for the test data (second column), the feature-based encoder failed to generalize, producing noisy embeddings with overlapping branches that diverged significantly from the ground truth (right). In contrast, the kernel-based encoder effectively recovered the true tree structure, demonstrating its robustness to noisy datasets. Branches are color-coded according to the ground truth structure.}
    \label{fig:kernel_vs_feature}
\end{figure*}

\section{Feature correlation-aware data perturbation}\label{sec:sampling}

In this section, we detail the procedure for generating perturbed datasets using a correlation-aware random sampling strategy \cite{kaneko2022cvpfi}. This approach is employed to compute feature importances as part of our evaluation scheme (Section \ref{subsec:quantify_oos_embedding}). For each feature \(i\), instead of permuting feature \(i\)'s column values---as in the standard permutation approach---, we reassign them by randomly sampling values from the feature space. Additionally, all other feature column values are randomly replaced with a probability proportional to their absolute correlation with \(i\). In other words, column values for features highly correlated with \(i\) are also replaced by random sampling, while column values for features not correlated with \(i\) remain unchanged. This prevents the determination of fallacious feature importances where all correlated features are assigned zero importance. Refer to Algorithm \ref{alg:sampling} for a step-by-step description of this feature-wise data perturbation procedure.

\begin{algorithm}[tb]
\caption{Feature-wise data perturbation with random sampling}
\label{alg:sampling}
\begin{algorithmic}
   \STATE {\bfseries Input:} Input data $X$, feature correlation matrix $\mathbf{C}$.
   \STATE Initialize the list of perturbed data sets $\Tilde{\mathbf{X}}$ associated with each feature perturbation.
   \STATE Generate $\Tilde{X}$ from $X$ by randomly sampling column values without replacement.
   \FOR{each feature $i$}
   \STATE Generate a mask matrix $\mathbf{M}$ with elements  $\mathbf{M}[i,j]\in\{0,1\}$ sampled from $ \text{Bernoulli}(|\mathbf{C}[i,j]|)$.
   \STATE Build perturbed dataset $\Tilde{X}^i=\mathbf{M}\odot \Tilde{X} + (I-\mathbf{M})\odot X$ associated with perturbed feature $i$.
   \STATE Store $\Tilde{\mathbf{X}}[i] = \Tilde{X}^i$.
   \ENDFOR
   \STATE {\bfseries Return:}  $\Tilde{\mathbf{X}}$

\end{algorithmic}
\end{algorithm}

\section{Artificial tree construction}\label{sec:artificial_tree}

We constructed the artificial tree data used in Section \ref{subsec:quant_comp} and Appendix \ref{sec:kernel_vs_feature} following the method described in the original PHATE paper \cite{Moon2019phate}. The first branch of the tree consists of 100 linearly spaced points spanning four dimensions, with all other dimensions set to zero. The second branch starts at the endpoint of the first branch, with its 100 points remaining constant in the first four dimensions while progressing linearly in the next four dimensions, leaving all others at zero. Similarly, the third branch progresses linearly in dimensions 9–12, with subsequent branches following the same pattern but differing in length. Each branch endpoint and branching point includes an additional 40 points, and zero-mean Gaussian noise (standard deviation 7) is added to simulate gene expression advancement along the branches. To further increase dimensionality, additional noise dimensions are added, bringing the total to 60. Before visualization, all features are normalized to the range [0, 1]. 


\section{Description of the datasets}\label{sec:data}

We provide additional details on the datasets used for the quantitative and qualitative comparisons between RF-AE and other methods. All datasets are publicly available from the \href{https://archive.ics.uci.edu/} {UCI Machine Learning Repository}, except for Sign MNIST (A–K) \cite{SignMNIST}, MNIST (test subset) \cite{MNIST}, Fashion MNIST (test subset) \cite{FashionMNIST}, and GTZAN (3-second version) \cite{GTZAN}, which were obtained from Kaggle; the artificial tree dataset, described in Appendix \ref{sec:artificial_tree}; and the Zilionis single-cell dataset \cite{Zilionis2019single}. The Sign MNIST (A–K) dataset is a subset of the original, containing the first 10 letters (excluding J, which requires motion). The Zilionis dataset was used in its preprocessed form (48,969 cells, 306 principal components) from \url{https://cb.csail.mit.edu/densvis/datasets/} \cite{Narayan2021assessing}. For faster computations, we used random subsets of 20\% of the original data for each repetition, resulting in 9793 cells before train/test splits.


\begin{table*}[ht]
\small
\caption{Description of the data sets used in our experiments.}
\label{tab:rfae_data}
\vskip 0.15in
\centering
\begin{tabular}{lcccc}
\toprule
\textsc{Dataset} & \textsc{Size} & \textsc{Test/Train ratio} & \textsc{Dimensions} & \textsc{Number of Classes} \\
\midrule
\textsc{Artificial Tree} & 1440 & 0.20 & 60 & 10 \\
\textsc{UCI} & & & \\
\quad\textsc{Cardiotocography} & 2126 & 0.20 & 21 & 10 \\
\quad\textsc{Chess} & 3196 & 0.20 & 36 & 2 \\
\quad\textsc{Crowdsourced Mapping} & 10845 & 0.03 & 28 & 6 \\
\quad\textsc{Diabetic Retinopathy Debrecen} & 1151 & 0.20 & 19 & 2 \\
\quad\textsc{Isolet} & 7797 & 0.20 & 617 & 26 \\
\quad\textsc{Landsat Satellite} & 6435 & 0.31 & 36 & 6 \\
\quad\textsc{Obesity} & 2111 & 0.20 & 16 & 7 \\
\quad\textsc{Optical Burst Switching Network} & 1060 & 0.20 & 21 & 4 \\
\quad\textsc{Optical Digits} & 5620 & 0.32 & 64 & 10 \\
\quad\textsc{QSAR Biodegradation} & 1055 & 0.20 & 41 & 2 \\
\quad\textsc{Spambase} & 4601 & 0.20 & 57 & 2 \\
\quad\textsc{Sports Articles} & 1000 & 0.20 & 59 & 2 \\
\quad\textsc{Waveform} & 5000 & 0.20 & 40 & 3 \\
\quad\textsc{White Wine Quality} & 4898 & 0.20 & 11 & 7 \\
\textsc{Image} & & & \\
\quad\textsc{MNIST (test)} & 10000 & 0.20 & 784 & 10 \\
\quad\textsc{Fashion MNIST (test)} & 10000 & 0.20 & 784 & 10 \\
\quad\textsc{Sign MNIST (A--K)} & 14482 & 0.20 & 784 & 26 \\
\textsc{Audio} & & & \\
\quad\textsc{GTZAN (3-sec)} & 9990 & 0.20 & 57 & 10 \\
\textsc{Single-cell} & & & \\
\quad\textsc{Zilionis (subset)} & 9793 & 0.20 & 306 & 20 \\
\bottomrule
\end{tabular}
\end{table*}




\section{Experimental setting}\label{sec:rfae_exp_setting}
RF-AE was implemented by us in PyTorch. The encoder \( f \) comprised three consecutive hidden layers with sizes 800, 400, and 100, respectively. For data visualization tasks, we set the bottleneck layer dimension to two. The decoder \( g \) was constructed symmetrically, with three consecutive hidden layers of sizes 100, 400, and 800, followed by an output layer matching the original input dimensions. We applied ELU activations in all layers except for the bottleneck and output layers, where identity and softmax activations were used, respectively. The models were trained using the AdamW optimizer \cite{loshchilov2018decoupled} with a batch size of 64, a learning rate of \( 10^{-3} \), and a weight decay of \( 10^{-5} \). Training was conducted over 200 epochs without early stopping. We also implemented SSNP, CE, and vanilla AE using the same AE architecture and hyper-parameters as our RF-AE models.

We used existing implementations for other methods. Parametric \( t \)-SNE was implemented following \cite{damrich2023from}, available at \url{https://github.com/sdamrich/cl-tsne-umap}. CEBRA was obtained from \url{https://github.com/AdaptiveMotorControlLab/CEBRA}, LOL from \url{https://github.com/neurodata/lollipop}, SPCA from \url{https://github.com/bghojogh/Principal-Component-Analysis}, and parametric supervised UMAP from \url{https://github.com/lmcinnes/umap}. For PCA, NCA, and PLS-DA, we used the built-in \texttt{scikit-learn} \cite{scikit-learn} implementation. We used the default parameters for each of those models, except for the number of training epochs in CEBRA which was set to $200$.

For the general comparison, we considered the landmark version of the baseline RF-PHATE kernel extensions and our RF-AE extension, using the same default landmark selection for a fair comparison ($N_{\text{land}}=2000$). The parameter $k$ for our baseline $k$-NN classifier was set to $k=10$. The $QNX$ scoring function was averaged over $k=10, 20, \hdots, 200$, as proposed by Moor et al. \yrcite{moor2020topo}.

We used Kendall's rank correlation coefficient \cite{Kendall1938} for the importance preservation scores $\rho_s$ to better account for ties in the feature importance sets. We minimally preprocessed the data sets with min-max normalization. For the Zilionis dataset, we directly used the 306 principal components from the preprocessed data available at \url{https://cb.csail.mit.edu/densvis/datasets/}. Since SSNP requires input features in the range $[0,1]$, we min-max normalized this dataset specifically for that model.




\section{Ablation experiments}\label{sec:ablation_supplemental}
We conducted ablation experiments on the two main RF-AE hyper-parameters: the loss balancing parameter $\lambda$ and the number of selected landmarks, $N_{\text{land}}$.

\subsection{Impact of the loss balancing parameter $\lambda$}\label{subsec:ablation_lambda}


Table \ref{tab:robustness_lambda} summarizes the ranks of each method under the same experimental setup as in Section \ref{subsec:quant_comp}, including regularized RF-AE variants with $\lambda \in \{ 10^{-4}, 10^{-3}, 10^{-1}\}$. RF-AE consistently ranked highest overall, regardless of $\lambda$, highlighting its robustness and strong out-of-sample performance. RF-AE with $\lambda = 10^{-2}$ and $\lambda = 10^{-3}$ exhibited similar overall rankings, with $\lambda = 10^{-2}$ slightly outperforming in terms of supervised global structure preservation, and $\lambda = 10^{-3}$ showing a slight edge in supervised local structure preservation. Interestingly, while RF-AE with $\lambda = 10^{-4}$ and $\lambda = 10^{-1}$ also ranked first overall, they both performed worse than $\lambda \in \{ 10^{-3}, 10^{-2}\}$ in supervised global structure preservation, though their supervised local structure preservation remained stable. This suggests that RF-AE with $\lambda \in \{ 10^{-3}, 10^{-2}\}$ offers the best tradeoff between local and global supervised structure preservation.

Fig. \ref{fig:robustness_lambda} visually demonstrates the impact of varying $\lambda \in \{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\}$ on Sign MNIST (A--K) (Fig. \ref{fig:robustness_lambda}a) and Zilionis (Fig. \ref{fig:robustness_lambda}b). The RF-PHATE kernel encoder ($\lambda = 0$) reproduces RF-PHATE without added generalization benefits. Adding minimal reconstruction strength ($\lambda = 10^{-4}$) led to significant qualitative improvements, while excessive reconstruction ($\lambda > 10^{-1}$) distorted the RF-PHATE geometry, losing its denoised manifold structure. The range $\lambda = 10^{-4}$ to $\lambda = 10^{-1}$ offers consistent structure, balancing condensed, denoised branches at smaller $\lambda$ and expanded branches at larger $\lambda$. Exploring this range is recommended for practical applications.



% \begin{table}
% \caption{Average $\rho_{QNX}$, $\rho_{Spear}$, and overall ranks for each embedding method across 19 datasets, including RF-AE with varying $\lambda\in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. Methods are sorted according to their overall average rank as shown in the last column $\langle \textsc{Rank}\rangle$. Supervised methods are marked by an asterisk. The second-best scores are in bold, while the top scores are also underlined.}
% \label{tab:robustness_lambda}
% \vskip 0.15in
% \centering
% \begin{tabular}{lccc}
% \toprule
%  & $\rho_{QNX}$ & $\rho_{Spear}$ & $\langle \textsc{Rank}\rangle$ \\
% \midrule
% RF-AE & & & \\
% \qquad $\lambda=10^{-2}$ & \phantom{0}\textbf{5.95} & \phantom{0}\underline{\textbf{5.32}} & \phantom{0}\underline{\textbf{5.63}} \\
% \qquad $\lambda=10^{-3}$ & \phantom{0}6.00 & \phantom{0}\textbf{5.89} & \phantom{0}\textbf{5.95} \\
% \qquad $\lambda=10^{-1}$ & \phantom{0}\underline{\textbf{5.26}} & \phantom{0}6.74 & \phantom{0}6.00 \\
% \qquad $\lambda=10^{-4}$ & \phantom{0}6.53 & \phantom{0}7.42 & \phantom{0}6.97 \\
% NCA & \phantom{0}8.37 & \phantom{0}6.37 & \phantom{0}7.37 \\
% RF-PHATE (Ker.) & \phantom{0}6.84 & \phantom{0}9.42 & \phantom{0}8.13 \\
% RF-PHATE (Enc.) & \phantom{0}7.63 & \phantom{0}9.16 & \phantom{0}8.39 \\
% CE & \phantom{0}8.00 & 11.11 & \phantom{0}9.55 \\
% CEBRA & 10.42 & \phantom{0}9.47 & \phantom{0}9.95 \\
% SSNP & \phantom{0}7.95 & 12.11 & 10.03 \\
% RF-AE & \phantom{0}7.68 & 13.42 & 10.55 \\
% SPCA & 13.21 & \phantom{0}8.11 & 10.66 \\
% PLS-DA & 11.95 & 10.42 & 11.18 \\
% LOL & 12.16 & 10.53 & 11.34 \\
% P-TSNE & 12.68 & 10.84 & 11.76 \\
% PCA & 14.53 & \phantom{0}9.05 & 11.79 \\
% P-SUMAP & 12.16 & 12.79 & 12.47 \\
% AE & 13.42 & 12.16 & 12.79 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table*}[htb]
\caption{Average local and global ranks of 15 embedding methods across 19 datasets, including RF-AE with varying $\lambda\in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$. The $\langle\textsc{Rank}\rangle$ column represents the average of the aggregated local and global ranks. Supervised methods are marked by an asterisk. The second-best scores are in bold, while the top scores are also underlined.}
\label{tab:robustness_lambda}
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.2}
\begin{sc}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccc|ccc|ccc|ccc}
& \multicolumn{3}{c|}{$\lambda=10^{-4}$} & \multicolumn{3}{c|}{$\lambda=10^{-3}$} & \multicolumn{3}{c|}{$\lambda=10^{-2}$} & \multicolumn{3}{c}{$\lambda=10^{-1}$} \\
 & Local & Global & $\langle$Rank$\rangle$ & Local & Global & $\langle$Rank$\rangle$ & Local & Global & $\langle$Rank$\rangle$ & Local & Global & $\langle$Rank$\rangle$ \\
\midrule
RF-AE* ($0<\lambda<1$) & \phantom{0}\underline{\textbf{4.40}}& \phantom{0}\textbf{5.30} & \phantom{0}\underline{\textbf{4.85}} & \phantom{0}\underline{\textbf{4.25}} & \phantom{0}\underline{\textbf{4.50}} & \phantom{0}\underline{\textbf{4.38}}& \phantom{0}\underline{\textbf{4.35}} & \phantom{0}\underline{\textbf{4.25}} & \phantom{0}\underline{\textbf{4.30}} & \phantom{0}\underline{\textbf{4.05}} & \phantom{0}\textbf{5.45} & \phantom{0}\underline{\textbf{4.75}} \\
NCA* & \phantom{0}6.15 & \phantom{0}\underline{\textbf{5.00}} & \phantom{0}\textbf{5.58} & \phantom{0}6.25 & \phantom{0}\textbf{5.00} & \phantom{0}\textbf{5.62} & \phantom{0}6.30 & \phantom{0}\textbf{5.05} & \phantom{0}\textbf{5.67} & \phantom{0}6.30 & \phantom{0}\underline{\textbf{5.00}} & \phantom{0}\underline{\textbf{5.65}} \\
RF-PHATE (Enc.)* & \phantom{0}5.60 & \phantom{0}6.70 & \phantom{0}6.15 & \phantom{0}5.60 & \phantom{0}6.70 & \phantom{0}6.15 & \phantom{0}5.45 & \phantom{0}6.70 & \phantom{0}6.08 & \phantom{0}5.55 & \phantom{0}6.70 & \phantom{0}6.12 \\
RF-PHATE (Ker.)* & \phantom{0}\textbf{5.40} & \phantom{0}7.05 & \phantom{0}6.22 & \phantom{0}\textbf{5.30} & \phantom{0}7.00 & \phantom{0}6.15 & \phantom{0}\textbf{5.30} & \phantom{0}7.00 & \phantom{0}6.15 & \phantom{0}\textbf{5.40} & \phantom{0}6.95 & \phantom{0}6.18 \\
CE* & \phantom{0}6.25 & \phantom{0}8.90 & \phantom{0}7.58 & \phantom{0}6.25 & \phantom{0}8.95 & \phantom{0}7.60 & \phantom{0}6.20 & \phantom{0}9.05 & \phantom{0}7.62 & \phantom{0}6.25 & \phantom{0}8.95 & \phantom{0}7.60 \\
CEBRA* & \phantom{0}8.30 & \phantom{0}7.00 & \phantom{0}7.65 & \phantom{0}8.30 & \phantom{0}7.15 & \phantom{0}7.73 & \phantom{0}8.20 & \phantom{0}7.15 & \phantom{0}7.67 & \phantom{0}8.20 & \phantom{0}7.10 & \phantom{0}7.65 \\
RF-AE* & \phantom{0}5.55 & 10.50 & \phantom{0}8.03 & \phantom{0}5.60 & 10.50 & \phantom{0}8.05 & \phantom{0}5.50 & 10.55 & \phantom{0}8.03 & \phantom{0}5.55 & 10.45 & \phantom{0}8.00 \\
SSNP* & \phantom{0}6.45 & \phantom{0}9.90 & \phantom{0}8.18 & \phantom{0}6.45 & \phantom{0}9.95 & \phantom{0}8.20 & \phantom{0}6.45 & 10.00 & \phantom{0}8.22 & \phantom{0}6.50 & \phantom{0}9.85 & \phantom{0}8.18 \\
SPCA & 10.60 & \phantom{0}6.65 & \phantom{0}8.62 & 10.60 & \phantom{0}6.75 & \phantom{0}8.68 & 10.65 & \phantom{0}6.75 & \phantom{0}8.70 & 10.60 & \phantom{0}6.65 & \phantom{0}8.62 \\
LOL* & \phantom{0}9.55 & \phantom{0}8.10 & \phantom{0}8.82 & \phantom{0}9.60 & \phantom{0}8.15 & \phantom{0}8.88 & \phantom{0}9.60 & \phantom{0}8.10 & \phantom{0}8.85 & \phantom{0}9.60 & \phantom{0}7.95 & \phantom{0}8.78 \\
PLS-DA* & \phantom{0}9.60 & \phantom{0}8.45 & \phantom{0}9.02 & \phantom{0}9.60 & \phantom{0}8.60 & \phantom{0}9.10 & \phantom{0}9.60 & \phantom{0}8.60 & \phantom{0}9.10 & \phantom{0}9.70 & \phantom{0}8.50 & \phantom{0}9.10 \\
P-TSNE & \phantom{0}9.80 & \phantom{0}8.30 & \phantom{0}9.05 & \phantom{0}9.85 & \phantom{0}8.45 & \phantom{0}9.15 & \phantom{0}9.85 & \phantom{0}8.60 & \phantom{0}9.22 & \phantom{0}9.85 & \phantom{0}8.50 & \phantom{0}9.18 \\
PCA & 11.80 & \phantom{0}7.50 & \phantom{0}9.65 & 11.80 & \phantom{0}7.55 & \phantom{0}9.68 & 11.85 & \phantom{0}7.55 & \phantom{0}9.70 & 11.85 & \phantom{0}7.50 & \phantom{0}9.68 \\
P-SUMAP* & \phantom{0}9.30 & 10.05 & \phantom{0}9.68 & \phantom{0}9.35 & 10.20 & \phantom{0}9.77 & \phantom{0}9.35 & 10.30 & \phantom{0}9.82 & \phantom{0}9.30 & 10.10 & \phantom{0}9.70 \\
AE & 10.45 & \phantom{0}9.85 & 10.15 & 10.55 & \phantom{0}9.85 & 10.20 & 10.55 & \phantom{0}9.95 & 10.25 & 10.55 & \phantom{0}9.95 & 10.25 \\
\bottomrule
\end{tabular}}
\end{sc}
\label{tab:rankings}
\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.99\textwidth]{figures/RFAE_ablation_lam.png}
    \caption{Visualization of RF-AE with $\lambda$ varying from 0 (RF-PHATE kernel encoder) to 1 (vanilla RF-AE).  
    \textbf{a.} Sign MNIST (A–K) dataset: training points are shown as color-coded circles based on their labels, while test points are displayed with their original images, tinted to match their labels. Small reconstruction strengths ($\lambda = 10^{-4}$) preserve RF-PHATE's denoised manifold structure, while larger values ($\lambda = 10^{-1}$) expand branches and deviate from the geometry. The range $\lambda = 10^{-4}$ to $10^{-1}$ offers consistent structure, balancing denoising and branch expansion. 
    \textbf{b.} Zilionis single-cell dataset: training points (circles) and test points (triangles) are colored by major cell type. Small reconstruction strengths ($\lambda = 10^{-4}$) preserve global structures, maintaining the overall organization of immune cell populations. However, some closely related subpopulations are not well separated. As $\lambda$ increases, local separation improves, with $\lambda = 10^{-3}$, achieving a balance between global coherence and local distinctions. At higher values ($\lambda = 10^{-2}$ and $\lambda = 10^{-1}$), the cell types remain well-clustered, but the global relationship between cell types are lost. }
    \label{fig:robustness_lambda}
\end{figure*}



\subsection{Robustness to the number of landmarks $N_{\text{land}}$}\label{sec:ablation_landmarks}
In all our previous experiments, we set the number of selected landmarks to $N_{\text{land}}=2000$ to match the landmarks used in the underlying RF-PHATE algorithm. However, depending on the size of the training dataset, it may be more convenient to define $N_{\text{land}}$ as a percentage of the training set size.  

In Table \ref{tab:robustness_pct_land}, we show the variation in out-of-sample local ($\rho_{QNX}$) and global ($\rho_{Spear}$) scores when setting $N_{\text{land}}$ to 5\%, 10\%, 25\%, and 100\% of the training set, compared to our default $N_{\text{land}}=2000$. The first row presents the average scores across all datasets, while the second row shows the averages over 10 repetitions on Zilionis only.  

We performed a two-sided Mann-Whitney U test \cite{mann1947test} with a significance level of $\alpha = 0.05$ to detect differences in local and global scores between the default RF-AE ($N_{\text{land}}=2000$) and each percentage-based selection of landmarks. The results indicate that the local and global scores for different landmark percentages failed to reject the null hypothesis, as indicated by the asterisks.  

Thus, we conclude that our model is highly robust to the choice of $N_{\text{land}}$, and that the percentage of selected landmarks can be set as low as 5\% of the training set without compromising supervised structure preservation. We also validated this finding visually on Zilionis, as shown in Fig. \ref{fig:robustness_landmark_Zilionis}.

\begin{table*}[ht]
\caption{Average $\rho_{QNX}$ and $\rho_{Spear}$ scores across all 19 datasets (first row) and over 10 repetitions on Zilionis only (second row) for our RF-AE model, using the default number of training landmarks $N_{\text{land}}=2000$ and varying values of $N_{\text{land}} \in \{5\%, 10\%, 25\%, 100\%\}$ (representing the fraction of training points used as landmarks). The loss balancing parameter $\lambda$ was set to our default $10^{-3}$ values in all cases. A two-sided Mann-Whitney U test \cite{mann1947test} with a significance level of $\alpha=0.05$ was applied to determine whether varying the percentage of landmarks resulted in different local and global scores compared to the default model ($N_{\text{land}}=2000$, first and second columns). An asterisk indicates that the varying landmark percentages did not significantly differ from the reference RF-AE ($N_{\text{land}}=2000$) scores, as the null hypothesis could not be rejected.}
\label{tab:robustness_pct_land}
\vskip 0.15in
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{\textwidth}{!}{ % Ensure the resize is correctly wrapping the entire table
\begin{tabular}{c|cc||cc|cc|cc|cc}
& \multicolumn{2}{c||}{$N_{\text{land}}=2000$} & \multicolumn{2}{c|}{$N_{\text{land}}=5\%$} & \multicolumn{2}{c|}{$N_{\text{land}}=10\%$} & \multicolumn{2}{c|}{$N_{\text{land}}=25\%$} & \multicolumn{2}{c}{$N_{\text{land}}=100\%$} \\
& $\rho_{QNX}$ & $\rho_{Spear}$ & $\rho_{QNX}$ & $\rho_{Spear}$ & $\rho_{QNX}$ & $\rho_{Spear}$ & $\rho_{QNX}$ & $\rho_{Spear}$ & $\rho_{QNX}$ & $\rho_{Spear}$ \\
\midrule
19 Datasets & 0.817 & 0.731 & 0.814* & 0.719* & 0.816* & 0.721* & 0.817* & 0.725* & 0.815* & 0.726* \\
Zilionis & 0.703 & 0.570 & 0.703* & 0.572* & 0.702* & 0.571* & 0.704* & 0.571* & 0.704* & 0.569* \\
\end{tabular}}
\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.99\textwidth]{figures/Zilionis_RFAE_ablation_pct_landmark.png}
    \caption{Robustness of RF-AE’s out-of-sample visualization on Zilionis with respect to the number of selected landmarks. Training points (circles) and test points (triangles) are colored by major cell type. The two-dimensional embeddings of the default RF-AE model ($N_{\text{land}}=2000$, far-left plot) are compared with embeddings using varying landmark percentages ($N_{\text{land}} = 5\%, 10\%, 25\%, 100\%$ of the training set size). We set our loss balancing hyper-parameter to the default value $\lambda=10^{-3}$ in all cases. Overall, the output is robust to the choice of $N_{\text{land}}$, with similar qualitative interpretations across all plots.}
    \label{fig:robustness_landmark_Zilionis}
\end{figure*}



\section{Extended plots and qualitative analysis}\label{sec:oos_viz_supplemental}
We present out-of-sample visualization plots and quantitative comparison for all models on Sign MNIST (Fig. \ref{fig:sign_mnist_cropped_full_999}, Table \ref{tab:sign_scores}) and Zilionis (Fig. \ref{fig:Zilionis}, Table \ref{tab:Zilionis_scores}) to support our analysis in Section \ref{subsec:oos_viz}.

Fig. \ref{fig:sign_mnist_cropped_full_999} presents visualizations of all models for the Sign MNIST (A–K) dataset. RF-AE with \(\lambda=10^{-3}\) effectively inherits the global structure of the RF-PHATE embeddings while providing greater detail within class clusters. In contrast, RF-PHATE (Ker.) tends to compress representations within each cluster, which are associated with individual classes. Although OOS embeddings are mostly assigned to their correct ground truth labels, the local arrangement of these samples on the sub-manifold is not easily visualized in RF-PHATE (Ker.). RF-AE ($\lambda=10^{-3}$), however, expands the class clusters, revealing within-class patterns that are obscured in the RF-PHATE (Ker.) plot. For example, the top-right cluster in the RF-AE ($\lambda=10^{-3}$) plot illustrates different ways to represent the letter ``C'', showing a logical transition between variations based on hand shadowing and orientation. Such nuanced differences are more challenging to detect in RF-PHATE (Ker.), which compresses these representations into an overly restrictive branch structure. This limitation of RF-PHATE may stem from excessive reliance on the diffusion operator, which overemphasizes global smoothing. Since RF-GAP already captures local-to-global supervised neighborhoods effectively, the additional diffusion applied by RF-PHATE likely diminishes fine-grained local details. Thus, we have demonstrated that RF-AE offers a superior balance for visualizing the local-to-global supervised structure compared to the basic RF-PHATE kernel extension. P-TSNE is effective at identifying clusters of similar samples but often splits points from the same class into distinct, distant clusters. This appears to result from variations such as background shadowing, which obstruct the important part of the image. Thus, ``G'' and ``H'' instances are closer than expected due to similar shadowing. In contrast, regularized RF-AE correctly assigns ``G'' and ``H'' instances to their own clusters while dissociating between same-class points with different shadowing, effectively reflecting within-class variations. This demonstrates that P-TSNE is overly sensitive to irrelevant factors, such as background differences, which are unrelated to the underlying labels.  Similarly, P-SUMAP exhibits this sensitivity but produces a sparser representation. Despite being a supervised method, P-SUMAP incorporates class labels in a way that artificially clusters same-class points, potentially oversimplifying their intrinsic relationships.

RF-AE without regularization effectively captures local structure and partitions the space according to class labels, correctly embedding test points within their respective clusters. However, the resulting embedding suffers from local distortions, and the global structure is poorly represented, appearing as an amorphous ``blob''. This highlights the importance of proper regularization in RF-AE to achieve interpretable visualizations, as previously noted in works such as \cite{duque2022geometry, Nazari2023geometric}. CEBRA yields a circular pattern that offers limited utility for qualitative interpretation. CE and SSNP over-separates the classes without connections between them, further hindering the visualization of inter-class relationships. NCA retains decent local and global relationships, but within-class variations and transitions between classes are visually less evident than in regularized RF-AE. Other methods produced noisy embeddings.

For the Zilionis single-cell dataset, all models are visualized in Fig. \ref{fig:Zilionis}. As analyzed in Section \ref{subsec:oos_viz}, RF-AE with RF-PHATE regularization ($\lambda=10^{-3}$) effectively preserves biologically meaningful relationships between cell types. In contrast, RF-PHATE kernel extension and P-TSNE over-compress related subsets, and P-SUMAP over-separate clusters, distorting biological insights. RF-AE without regularization separates the cell types well but fails to capture the global structure, leading to discontinuities in immune lineage organization. The characteristic circular pattern produced by CEBRA offers limited utility for qualitative interpretation. LOL and PLS-DA maintain some separation of major cell types but fail to preserve finer intra-cluster structures, leading to biologically uninformative embeddings. CE, AE, NCA, and SPCA exhibit some degree of clustering but lack clear local and global organization, making it difficult to infer meaningful cell-type relationships. PCA and SSNP fail to capture relevant biological patterns, producing embeddings that do not effectively separate immune cell populations or maintain meaningful transitions. Overall, these comparisons highlight the advantages of RF-AE with RF-PHATE regularization in generating biologically interpretable visualizations of high-dimensional single-cell data. 

\begin{table}[!htb]
\caption{Average $\rho_{QNX}$ and $\rho_{Spear}$ scores, and overall ranks of 15 embedding methods on Sign MNIST (A--K) dataset. Supervised methods are marked by an asterisk. The third-best and second-best scores are in bold, while the best scores are also underlined.}
\label{tab:sign_scores}
\vskip 0.1in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccc}
\toprule
& \multicolumn{3}{l}{Sign MNIST (A–K)} \\
\midrule
&  $\rho_{QNX}$ & $\rho_{Spear}$ & $\langle$Rank$\rangle$ \\
\midrule
RF-AE* ($\lambda=10^{-3}$) & \underline{\textbf{0.909} $\boldsymbol{\pm}$ \textbf{0.00}} & \textbf{0.610} $\boldsymbol{\pm}$ \textbf{0.11} & \phantom{0}\underline{\textbf{2.0}} \\
SSNP* & 0.883 $\pm$ 0.02 & \textbf{0.628} $\boldsymbol{\pm}$ \textbf{0.14} & \phantom{0}\textbf{3.5} \\
RF-AE* & \textbf{0.907} $\boldsymbol{\pm}$ \textbf{0.00} & 0.503 $\pm$ 0.22 & \phantom{0}\textbf{4.5} \\
PLS-DA* & 0.734 $\pm$ 0.01 & \underline{\textbf{0.685} $\boldsymbol{\pm}$ \textbf{0.01}} & \phantom{0}5.5 \\
CEBRA* & 0.837 $\pm$ 0.06 & 0.525 $\pm$ 0.11 & \phantom{0}6.5 \\
NCA* & 0.879 $\pm$ 0.01 & 0.470 $\pm$ 0.06 & \phantom{0}7.0 \\
RF-PHATE (Enc.)* & \textbf{0.902} $\boldsymbol{\pm}$ \textbf{0.01} & 0.430 $\pm$ 0.11 & \phantom{0}7.5 \\
RF-PHATE (Ker.)* & \textbf{0.902} $\boldsymbol{\pm}$ \textbf{0.01} & 0.430 $\pm$ 0.11 & \phantom{0}7.5 \\
SPCA* & 0.682 $\pm$ 0.01 & 0.527 $\pm$ 0.01 & \phantom{0}8.5 \\
LOL* & 0.794 $\pm$ 0.01 & 0.446 $\pm$ 0.02 & \phantom{0}9.0 \\
PCA & 0.643 $\pm$ 0.01 & 0.574 $\pm$ 0.02 & \phantom{0}9.0 \\
P-SUMAP* & 0.741 $\pm$ 0.01 & 0.428 $\pm$ 0.05 & 11.0 \\
CE* & 0.514 $\pm$ 0.41 & 0.450 $\pm$ 0.42 & 12.0 \\
AE & 0.685 $\pm$ 0.05 & 0.366 $\pm$ 0.13 & 12.5 \\
P-TSNE & 0.674 $\pm$ 0.02 & 0.248 $\pm$ 0.06 & 14.0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\end{table}


\begin{table}[!htb]
\caption{Average $\rho_{QNX}$ and $\rho_{Spear}$ scores, and overall ranks of 15 embedding methods on Zilionis Single-cell dataset. Supervised methods are marked by an asterisk.  The third-best and second-best scores are in bold, while the best scores are also underlined.}
\label{tab:Zilionis_scores}
\vskip 0.1in
\begin{center}
\begin{scriptsize}
\begin{sc}
\begin{tabular}{lccc}
\toprule
& \multicolumn{3}{l}{Zilionis (subset)} \\
\midrule
&  $\rho_{QNX}$ & $\rho_{Spear}$ & $\langle$Rank$\rangle$ \\
\midrule
RF-AE* ($\lambda=10^{-3}$) & \underline{\textbf{0.703} $\boldsymbol{\pm}$ \textbf{0.02}} & 0.570 $\pm$ 0.02 & \phantom{0}\underline{\textbf{2.5}} \\
P-TSNE & 0.696 $\pm$ 0.02 & \textbf{0.580} $\boldsymbol{\pm}$ \textbf{0.02} & \phantom{0}\textbf{4.5} \\
P-SUMAP* & \textbf{0.701} $\boldsymbol{\pm}$ \textbf{0.02} & 0.562 $\pm$ 0.03 & \phantom{0}\textbf{5.0} \\
LOL* & 0.693 $\pm$ 0.02 & \textbf{0.587} $\boldsymbol{\pm}$ \textbf{0.02} & \phantom{0}5.5 \\
CEBRA* & 0.690 $\pm$ 0.03 & \underline{\textbf{0.597} $\boldsymbol{\pm}$ \textbf{0.03}} & \phantom{0}6.0 \\
NCA* & 0.698 $\pm$ 0.01 & 0.551 $\pm$ 0.02 & \phantom{0}6.0 \\
RF-AE* & \textbf{0.702} $\boldsymbol{\pm}$ \textbf{0.01} & 0.508 $\pm$ 0.07 & \phantom{0}6.0 \\
RF-PHATE (Enc.)* & 0.695 $\pm$ 0.01 & 0.566 $\pm$ 0.02 & \phantom{0}6.5 \\
RF-PHATE (Ker.)* & 0.691 $\pm$ 0.02 & 0.570 $\pm$ 0.02 & \phantom{0}7.5 \\
CE* & 0.697 $\pm$ 0.03 & 0.506 $\pm$ 0.05 & \phantom{0}8.0 \\
AE & 0.694 $\pm$ 0.02 & 0.538 $\pm$ 0.03 & \phantom{0}8.5 \\
SPCA* & 0.666 $\pm$ 0.02 & 0.488 $\pm$ 0.02 & 12.5 \\
PLS-DA* & 0.689 $\pm$ 0.02 & 0.355 $\pm$ 0.02 & 13.0 \\
PCA & 0.627 $\pm$ 0.02 & 0.418 $\pm$ 0.02 & 13.5 \\
SSNP* & 0.181 $\pm$ 0.14 & 0.049 $\pm$ 0.04 & 15.0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{scriptsize}
\end{center}
\end{table}

Additionally, we provide a qualitative analysis of the MNIST test subset, accompanied by visualizations in Fig. \ref{fig:mnist_test_full_3}. The embedding generated by the default kernel extension of RF-PHATE performs even worse than in the Sign MNIST (A--K) case (Section \ref{subsec:oos_viz}). While the digits 0, 1, and 7 are positioned along three separate branches pointing in different directions, the remaining digits are crowded together in the center of the plot. This central clustering makes it nearly impossible to discern meaningful local or global relationships for these other classes.

In contrast, RF-AE with RF-PHATE regularization (\(\lambda=10^{-3}\)) preserves the global structure seen in RF-PHATE—digits 0, 1, and 7 are located in distinct, opposing regions—while expanding the intermediate regions to reveal within-class and between-class relationships for the other digits. This creates a more interpretable visualization compared to the overly compressed RF-PHATE embedding.

Vanilla RF-AE visualization suffers from the same distortions as described earlier, with poor representation of global structure. Parametric $t$-SNE effectively separates clusters and reveals hidden patterns but introduces more noise and lacks the smooth transition structure between clusters seen in RF-AE (\(\lambda=10^{-3}\)) and RF-PHATE extensions. On the other hand, P-SUMAP, SSNP and CE artificially assign isolated regions to same-class points, which obliterates much of the data's intrinsic structure. CEBRA once again produces its characteristic circular pattern, offering limited utility for qualitative analysis. Other methods provide noisier and poorly defined visualizations, making them less effective for capturing nuanced relationships within this dataset.

\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.8\textwidth]{figures/sign_mnist_cropped_full_999.png}
    \caption{Visualization of the Sign MNIST (A--K) dataset (Table \ref{tab:rfae_data}) using 15 dimensionality reduction methods. Training points are shown as color-coded circles based on their labels, while test points are displayed with their original images, tinted to match their labels. Refer to Section \ref{subsec:oos_viz} and Appendix \ref{sec:oos_viz_supplemental} for a full qualitative analysis.}
\label{fig:sign_mnist_cropped_full_999}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.8\textwidth]{figures/Zilionis_all_models_444.png}
    \caption{Visualization of the Zilionis (subset) (Table \ref{tab:rfae_data}) using 15 dimensionality reduction methods. Training points are shown as color-coded circles based on their labels, while test points are triangle. RF-AE with RF-PHATE regularization preserves both local and global immune cell structures, while other methods either over-compress, over-separate, or fail to maintain biologically meaningful relationships. Refer to Section \ref{subsec:oos_viz} and Appendix \ref{sec:oos_viz_supplemental} for a full qualitative analysis.}
    \label{fig:Zilionis}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.8\textwidth]{figures/mnist_test_full_3.png}
    \caption{Visualization of the MNIST dataset (Table \ref{tab:rfae_data}) using 15 dimensionality reduction methods. Training points are shown as color-coded circles based on their labels, while test points are displayed with their original images, tinted to match their labels. For clarity, we randomly plotted 50\% of the test images. Refer to Appendix \ref{sec:oos_viz_supplemental} for a full qualitative analysis.}
    % The kernel extension of RF-PHATE struggles with interpretability, clustering most digits centrally while separating only 0, 1, and 7 into distinct branches. RF-AE with RF-PHATE regularization ($\lambda=10^{-2}$) retains this global structure but expands intermediate regions to better reveal within- and between-class relationships. P-TSNE separates clusters well but introduces some noise and class overlap. CE and P-SUMAP over-separates the classes, losing intrinsic structure, while CEBRA’s circular layout and other methods’ noisy embeddings remain less effective.
    
    \label{fig:mnist_test_full_3}
\end{figure*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
