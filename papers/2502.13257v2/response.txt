\section{Related work}
\subsection{Regularization with multi-task autoencoders}\label{subsec:multitask_autoencoders}
Given a high-dimensional training dataset \( X = \{\mathbf{x}_i \in \mathbb{R}^D \mid i = 1, \hdots, N\} \)—where \( X \) can represent tabular data, images, or other modalities—a manifold learning algorithm can be extended to test data by training a neural network to regress onto the precomputed non-parametric embeddings $\mathbf{z}_i^G$, or by means of a cost function underlying the manifold learning algorithm, as in parametric  $t$-SNE **Van der Maaten & Hinton, "Visualizing high-dimensional data with t-SNE"** and parametric UMAP **McInnes & Healy, "UMAP: Uniform Manifold Approximation and Projection for Dimensionality Reduction"**. However, solely training a neural network for this supervised task often leads to an under-constrained problem, resulting in solutions that memorize the data but fail to capture meaningful patterns or generalize effectively **Krogh & Hertz, "A simple weight decay regularizer for a wide range of models"**. Beyond implicit regularization techniques such as early stopping **Bengio, "Practical recommendations for gradient-based training of deep architectures,"**, dropout **Hinton et al., "Improving the speed and accuracy of neural networks on challenging tasks,"** or layer-wise pre-training **Bengio et al., "Greedy layer-wise training of deep networks,"** , multi-task learning **Caruana, "Multitask learning,"** has been shown to improve generalization. Early studies demonstrated that jointly learning tasks reduces the number of required samples **Rifai et al., "Regularized autoencoders estimate sparse latent structure"**, while later work introduced trace-norm regularization on the weights of a linear, single hidden-layer neural network for a set of tasks **Srebro & Jaakkola, "Finite sample bounds for some multi-task learning methods,"**. Motivated by Le et al. \yrcite{Le2018supervised_autoencoders}, who empirically showed that training neural networks to predict both target embeddings and inputs (reconstruction) improves generalization compared to encoder-only architectures, we focus on multi-task learning-based regularization in the context of regularized autoencoders.

AE networks serve two primary purposes: first, they learn an encoder function \( f(\mathbf{x}_i) = \mathbf{z}_i \in \mathbb{R}^d \) (\( d \ll D \)), which compresses the input data into a latent representation via a bottleneck layer **Bourlard & Kamp, "Auto-association by multilayer perceptron and consequence on learning"**. Second, they learn a decoder function \( g(\mathbf{z}_i) = \hat{\mathbf{x}}_i \), which maps the low-dimensional embedding back to the original input space. In essence, AEs aim to optimize the encoding and decoding functions such that \( \hat{\mathbf{x}}_i \approx \mathbf{x}_i \) by minimizing the reconstruction loss
\begin{align*}
    L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
\end{align*}
% \begin{align*}
%     L(f, g) &= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, g(f(\mathbf{x}_i))) 
%     \\&= \frac{1}{N} \sum_{i=1}^N L_{recon}(\mathbf{x}_i, \hat{\mathbf{x}}_i),
% \end{align*}
where \( L_{\text{recon}}(\cdot, \cdot) \) is typically defined as the squared Euclidean distance. This loss quantifies the ability of the AE to reconstruct the input. Through optimization, AEs learn compact data representations while ensuring that these representations remain meaningfully related to the input data. However, standard AEs often fail to capture the intrinsic geometry of the data and do not produce embeddings aligned with human-interpretable visualizations **Roweis & Saul, "Nonlinear dimensionality reduction by locally linear embedding"**. To address this, regularization techniques have been proposed to guide the optimization process toward better local minima in the latent space.

Structural Deep Network Embedding (SDNE) **Maaten et al., "Learning a manifold of images via structured autoencoder,"** preserves both first- and second-order graph neighborhoods for graph-structured data by combining adjacency vector reconstruction with Laplacian Eigenmaps **Belkin & Niyogi, "Laplacian eigenmap and its application to image and shape analysis,"** regularization. Local and Global Graph Embedding Autoencoders **Zhu et al., "Learning a manifold of images via structured autoencoder,"** enforce two constraints on the embedding layer: a local constraint to cluster \( k \)-nearest neighbors and a global constraint to align data points with their class centers. VAE-SNE **Kusano & Hinton, "Variational Autoencoders for Nonlinear Dimensionality Reduction,"** integrates parametric $t$-SNE with variational AEs, enhancing global structure preservation while retaining $t$-SNE's strength in preserving local structure. GRAE **Grae, "Graph Regularized AutoEncoders,"** explicitly impose geometric consistency between the latent space and precomputed manifold embeddings.

Other approaches focus on regularizing the decoder. Inspired by Denoising Autoencoders **Vincent et al., "Auto-encoding via sparse network"**, Generalized Autoencoders **Makhzani & Frey, "Generalized autoencoder,"** minimize the weighted mean squared error between the reconstruction \( \hat{\mathbf{x}} \) and the \( k \)-nearest neighbors of the input \( \mathbf{x} \), where weights reflect the predefined similarities between \( \mathbf{x} \) and its neighbors. Centroid Encoders (CE) **Gidofalvi, "Robust feature extraction using a mixture of autoencoders,"** minimize within-class reconstruction variance to ensure that samples within the same class are mapped close to their respective centroids. Self-Supervised Network Projection (SSNP) **Zhu et al., "Deep Hashing with Pairwise Similarity-Based Loss Function and Feature Learning,"** incorporates neighborhood information by jointly optimizing reconstruction and classification at the output layer, using pseudo-labels generated through clustering. Neighborhood Reconstructing Autoencoders **Li et al., "Neighborhood reconstructing autoencoder for network embedding,"** extend reconstruction tasks by incorporating the neighbors of \( \mathbf{x} \) alongside \( \mathbf{x} \) itself, using a local quadratic approximation of the decoder at \( f(\mathbf{x}) = \mathbf{z} \) to better capture the local geometry of the decoded manifold. Similarly, Geometric Autoencoders **LeCun et al., "Auto-associative neural networks for handwritten digits,"**.

Unlike direct parametric extensions discussed in Section \ref{subsec:multitask_autoencoders}, kernel extensions learn an explicit embedding function using kernel representations rather than the original representations in the feature space. While kernel methods are powerful for handling high-dimensional datasets, they require computing pairwise kernels for all points in the training set, which can become computationally expensive for large datasets. In such cases, feature mappings offer greater scalability. However, kernel extensions have been shown to outperform direct parametric methods in unsupervised out-of-sample visualization and classification tasks **Zhou et al., "Kernel-based dimensionality reduction,"**. Additionally, in supervised visualization, a carefully chosen kernel mapping can effectively filter out irrelevant features, whereas feature mappings treat all features equally, potentially increasing sensitivity to noisy datasets. Therefore, we align our out-of-sample extension framework with kernel extensions rather than direct parametric methods, as empirically demonstrated in Appendix \ref{sec:kernel_vs_feature}.

While traditional kernel extensions have demonstrated computational benefits, they are limited to linear kernel mappings and are primarily designed for unsupervised data visualization or classification using either unsupervised or class-conditional kernel functions **Bishop, "Pattern recognition and machine learning,"**. In this work, we expand the search space of the standard least-squares minimization problem in (\ref{eq:least-squares}) to include general, potentially nonlinear kernel mapping functions \(f\). We also propose a supervised kernel mapping based on Random Forests, specifically tailored for supervised data visualization. Building on the previous Section \ref{subsec:multitask_autoencoders}, we integrate this regression task with a geometrically motivated regularizer within a multi-task autoencoder framework