\section{Deep Differentially Private Forecasting}\label{section:methods}
Now that we have the language to formally reason about privacy,
let us turn to our original goal of training forecasting models. 
\cref{algorithm:dp-sgd-loop} describes the 
use of top- and bottom-level sampling (recall~\cref{fig:explainy_figure}),
which we instantiate shortly. 
Given a dataset of sequences $x = \{x_1,\dots,x_N\}$,
we sample a subset of sequences,
and then independently sample $\numinstances \in \sN$ subsequences from of each them.
All subsequences are then aggregated into a single batch $y$.
The size of the top-level sample is chosen such that
we (up to modulo division) attain a batch size of $\batchsize \in \sN$.
\cref{algorithm:dp-sgd-step} formalizes the splitting of each subsequence $y_i \in y$ into a context and ground-truth forecast window.
Unlike in standard training, we clip the gradient of the corresponding loss and
add calibrated Gaussian noise with covariance matrix $\sigma^2 C^2 \eye$.
This makes the training step differentially private under insertion/removal of a 
 batch element~\cite{abadi2016deep}.

\textbf{Contribution.} 
Importantly, we neither claim the batching procedure nor the noisy training step to be novel in isolation. 
Our novel contribution lies in analyzing the interesting and non-trivial way in which the components of the batching procedure
interact to amplify the privacy of training steps.

\begin{algorithm}
   \caption{DP-SGD Epoch for Global Forecasting}
   \label{algorithm:dp-sgd-loop}
\begin{algorithmic}
    \STATE {\bfseries Input:}
    Data $x = \{x_1,\dots,x_N\}$, context length $L_C$, forecast length $L_F$, expected number of subsequences $\numinstances$, expected batch size $\batchsize$, model $f_\theta$, learning rate $\eta$, noise scale $\sigma$,  clipping norm $C$
    \FOR {$b \gets 1$ \textbf{to} $\lfloor N \cdot \numinstances \mathbin{/} \batchsize \rfloor$}
        \STATE {$y \gets \{\}$}
        \FOR {$x_n$ \textbf{in} sample\_top\_level($N, \numinstances, \batchsize, b)$}
            \STATE {$y \gets y \cup \text{sample\_bottom\_level}(x_n, L_C, L_F, \numinstances)$}
                %\STATE {$y$ $\gets$ $y \cup \{(x_{n, t-L_c : t}, x_{n, t: t+L_F + 1})\}$}
        \ENDFOR
    \STATE {$\theta \gets$ noisy\_training\_step($y, L_C, L_F, \batchsize, f_\theta, \eta, \sigma, C$)}
    \ENDFOR
    \STATE {\textbf{return} $\theta$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
   \caption{Noisy Training Step}
   \label{algorithm:dp-sgd-step}
\begin{algorithmic}
    \STATE {\bfseries Input:}
    Batch of subsequences $y = \{y_1,\dots,y_I\}$, context length $L_C$, forecast length $L_F$, expected batch size $\batchsize$,  model $f_\theta$, learning rate $\eta$, noise scale $\sigma$, clipping norm $C$
    \STATE {$\hat{g} \gets 0$}
    \FOR{$y_i \in y$}
        \STATE {$y_{C,i} \gets y_i[1:L_C]$} \hfill \COMMENT {context window}
        \STATE {$y_{F,i} \gets y_i[L_C+1 : L_C + L_F]$} \hfill \COMMENT {forecast window}
        \STATE {$g_i \gets \nabla_\theta \mathcal{L}(f_\theta(y_{C,i}), y_{F,i})$}
        \STATE {$\hat{g} \gets \hat{g} + g_i \mathbin{/} \max\{1, \frac{||g_i||_2}{C}\}$} \hfill \COMMENT {gradient clipping}
    \ENDFOR
    \STATE {$\tilde{g} \gets \frac{1}{\batchsize} (\hat{g} + \mathcal{N}(\vzero, \sigma^2 C^2 \mathbf{I}))$}
    \STATE {\textbf{return} $\theta - \eta \tilde{g}$}
\end{algorithmic}
\end{algorithm}

\textbf{Simplifying assumptions.} For the sake of exposition and to simplify notation, we assume that $L - L_F + 1 \geq L_C + L_F$  and focus on $1$-event-level privacy.
In~\cref{appendix:generalizations}, we discuss how to easily generalize our guarantees to $w$-event and $w$-user-level privacy with arbitrary $w \in \sN$, as well as variable-length and multivariate time series.

\subsection{Bottom-Level Subsampling}\label{section:bottom_level_subsampling}
Let us begin by focusing on the amplification attained via bottom-level sampling of temporally contiguous subsequences. To this end, we assume that the top-level sampling procedure 
simply iterates deterministically over our dataset and yields $ \lfloor \batchsize \mathbin{/} \numinstances  \rfloor$ sequences per batch (see~\cref{algorithm:dp-sgd-deterministic-top-level}).
As our bottom-level scheme, we use~\cref{algorithm:dp-sgd-wr-bottom-level}, which samples 
$\numinstances$ subsequences per sequence \emph{with replacement} to achieve a fixed batch size of $\batchsize$.
In~\cref{appendix:proofs_deterministic_top_poisson_bottom}, we additionally consider \emph{Poisson sampling}, which independently includes each element at a constant rate. 
In forecasting frameworks, these methods are also referred to as \emph{number of instances sampling} and \emph{uniform split sampling}, respectively~\cite{alexandrov2019gluonts}.
In the following, mechanism $M$ refers to a \emph{single epoch} with top-level iteration and bottom-level sampling with replacement. 
\begin{algorithm}
   \caption{Top-Level Deterministic ``Sampling''}
   \label{algorithm:dp-sgd-deterministic-top-level}
\begin{algorithmic}
    \STATE {\bfseries Input:}
    Data $x = \{x_1, \dots, x_N\}$, expected subsequences $\numinstances$, expected batch size $\batchsize$, batch number $b$
    \STATE {$N' \gets \lfloor \batchsize \mathbin{/} \numinstances  \rfloor$} \hfill \COMMENT {``Sample'' size}
    \FOR{$n \gets 1 + (b-1) \cdot {N'}$ {\bfseries to} $b \cdot N'$}
        \STATE {\textbf{yield} $x_n$}
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
   \caption{Bottom-Level Sampling with Replacement}
   \label{algorithm:dp-sgd-wr-bottom-level}
\begin{algorithmic}
    \STATE {\bfseries Input:}
        Sequence $x_n$, context length $L_C$, forecast length $L_F$, expected subsequences $\numinstances$
    \STATE {$x'_n \gets$ prepend\_zeros($x_n, L_C$)} \hfill \COMMENT {padding}
    \STATE {$T \gets L - L_F + 1$} \hfill \COMMENT {maximum start index}
    \FOR{$j \gets 1$ \textbf{to} $\numinstances$}
        \STATE {$t \gets \mathrm{Uniform}(\{1,\dots,T\})$}
        \STATE {\textbf{yield}} $x'_n[t : t + L_C +  L_F - 1]$ \hfill \COMMENT {cropping}
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\textbf{Effect of Number of Subsequences $\bm{\numinstances}$.}
Before we proceed to deriving amplification guarantees, 
note that  bi-level subsampling introduces an additional degree of freedom not  present in DP-SGD for unstructured data:
A batch of size $\batchsize$ can be composed of many subsequences from few sequences ($\numinstances$ large)
or few subsequences from many sequences ($\numinstances$ small).
Intuitively, the latter should be more private, because there are fewer chances to access  sensitive information from any  specific sequence $x_n$. 
In fact, we can prove the correctness of this intuition
via stochastic dominance of amplification bounds   (see~\cref{appendix:proofs_deterministic_top_monotonicity}):
\begin{restatable}{theorem}{deterministictoplevelmonotonicity}\label{proposition:deterministic_top_level_monotonicity}
    Let $P^{*}(\numinstances)$, $Q^{*}(\numinstances)$ be a tight dominating pair of epoch $M$ for bottom-level Poisson sampling or sampling with replacement and $\numinstances \in \sN$ (expected) subsequences.
    Then $H_\alpha(P^{*}(\numinstances), Q^{*}(\numinstances))$ is minimized by $\numinstances = 1$ for all $\alpha \geq 0$.
\end{restatable}

\textbf{Guarantees for Optimal $\bm{\numinstances}$.}
Based on this result, let us begin by focusing on the case $\numinstances = 1$ that minimizes per-epoch privacy leakage. 
Because we consider subsequences of length $L_C + L_F$, even a single sensitive element of a sequence $x_n$ can contribute to $L_C + L_F$ different  subsequences at different positions.
Since we deterministically iterate over our dataset, these subsequences can contribute to exactly one training step per epoch.
The resultant privacy is tightly bounded by the following result (proof in~\cref{appendix:proofs_bottom_level}).
\begin{restatable}{theorem}{deterministictoplevelwr}\label{theorem:deterministic_top_level_wr}
    Consider the number of sampled subsequences $\numinstances = 1$,
    and let $r = \frac{L_C + L_F}{L - L_F + 1}$ be the probability of sampling a subsequence containing any specific element.
    Define
    $P(1) = \mog(\vmu, \vp, \sigma)$ with
    means $\vmu = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and 
    weights $\vp = \begin{bmatrix} 1-r & r\end{bmatrix}^T$. Further, define per-epoch privacy profile $H(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'})$. Then, 
    \begin{equation*}
        H(\alpha) = 
        \begin{cases}
            H_\alpha(P(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || P(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{restatable}
In~\cref{appendix:bottom_level_dominating_pairs}, we discuss the tight dominating pairs corresponding to this bound.
Intuitively, as $r$ decreases, $P(1)$ converges to $\mathcal{N}(0,1)$ and the hockey stick divergence decreases, i.e., our mechanism becomes more private.

\textbf{Other Guarantees.}
In~\cref{appendix:proofs_bottom_level}, we derive tight dominating pairs for Poisson sampling and $\numinstances \geq 1$,
as well as dominating pairs for sampling with replacement and $\numinstances \geq 1$.
The special case of $N = 1$, $\batchsize = \numinstances$ is equivalent to sampling from a set in which $L_C + L_F$ elements are substituted, i.e., subsampled group privacy~\cite{ganesh2024tight,schuchardt2024unified,jiang2025calibrating}.
Thus far, tight dominating pairs have only been known for group insertion/removal, i.e., 
these guarantees are of interest beyond forecasting.

\textbf{Epoch Privacy vs Length.}
Despite the optimality guarantee from~\cref{proposition:deterministic_top_level_monotonicity}, we need to consider that sampling few subsequences ($\numinstances$ small)
means that more sequences contribute to a batch ($\lfloor \batchsize \mathbin{/} \numinstances \rfloor$ large).
We thus need more epochs for the same number of training steps, and each epoch has the potential of leaking private information.
In~\cref{section:experiments}, we will demonstrate numerically that composing many short, more private epochs ($\lambda=1$) nevertheless offers stronger privacy for the same number of training steps.
As baselines for this experiment, we will use the following~\emph{optimistic lower bounds}   (proof in~\cref{appendix:deterministic_top_wr_bottom_lower_bounds}):
\begin{restatable}{theorem}{deterministictoplevelwroptimistic}\label{theorem:deterministic_top_level_wr_optimistic}
    Consider the number of subsequences $\numinstances \geq 1$,
    and let $r = \frac{L_C + L_F}{L - L_F + 1}$.
    Define
    $\underline{P}(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\vmu \in \sN_0^{\numinstances +1}$ and weights $\vmu \in \sN_0^{\numinstances +1}$
    with $\evmu_i = 2 (i-1)$
    and $\evp_i = \mathrm{Binomial}(i \mid \numinstances, r)$. Further, define per-epoch privacy profile $H(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'})$. Then, 
    \begin{equation*}
        H(\alpha) \geq 
        \begin{cases}
            H_\alpha(\underline{P}(\numinstances) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || \underline{P}(\numinstances)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{restatable}
Intuitively, each mixture mean $\mu_i$ corresponds to the event that $i-1$ subsequences with information of a specific individual are sampled, i.e., more information is leaked.

\subsection{Top-Level Subsampling}\label{section:top_level_subsampling}
Next, let us explore how randomizing which sequences $x_n$ contribute to a batch can further amplify privacy. 
For this, we use~\cref{algorithm:dp-sgd-wor-top-level}, which samples without replacement.
This will 
eliminate the chance that any particular sequence $x_n$ can have its information leaked through more than $\numinstances$ subsequences per batch. 
From here on, mechanism $\tilde{M}$ refers to a \emph{single training step} using top-level sampling without replacement and bottom-level sampling with replacement.
\begin{algorithm}
   \caption{Top-Level Sampling Without Replacement}
   \label{algorithm:dp-sgd-wor-top-level}
\begin{algorithmic}
    \STATE {\bfseries Input:}
    Data $x = \{x_1, \dots, x_N\}$, expected subsequences $\numinstances$, expected batch size $\batchsize$, batch number $b$
    \STATE {$N' \gets \lfloor \batchsize \mathbin{/} \numinstances  \rfloor$} \hfill \COMMENT {Sample size}
    \STATE {$\pi \gets$ random\_permutation($N$)}
    \FOR{$n \gets 1$ \textbf{to} $N'$}
        \STATE {\textbf{yield}} $x_{\pi(n)}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{restatable}{theorem}{wortoplevelwr}\label{theorem:wor_top_level_wr}
    Consider the number of subsequences $\numinstances = 1$ and 
    batch size $\batchsize$.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Define
    $\tilde{P}(1) = \mog(\vmu, \tilde{\vp}, \sigma)$ with
     $\tilde{\vmu} = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and 
     $\tilde{\vp} = \begin{bmatrix} (1 - \rho) + \rho \cdot (1-r) & \rho \cdot r\end{bmatrix}^T$.
    Then, per-step privacy profile $\tilde{H}(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})$ fulfills 
    \begin{equation*}
        \tilde{H}(\alpha) = 
        \begin{cases}
            H_\alpha(\tilde{P}(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || \tilde{P}(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
    \end{restatable}
Intuitively, $\rho < 1$ removes probability mass from the mixture component that indicates some level of privacy leakage, and transfers it to the component that indicates zero leakage.

\textbf{Other guarantees.}
In~\cref{appendix:proofs_bilevel}, we additionally derive dominating pairs for $\numinstances \geq 1$
when using bottom-level sampling with replacement or Poisson sampling.
There, $\rho$ still has a similar effect of attenuating privacy leakage.

\textbf{Step- vs Epoch-Level Accounting.}
While \cref{theorem:wor_top_level_wr} shows that top-level sampling amplifies privacy, 
it yields bounds 
for each training step $\tilde{M}$ instead of each epoch $M$  (cf.~\cref{theorem:deterministic_top_level_wr}). We need to self-compose these bounds  
$\lfloor N \cdot \numinstances \mathbin{/} \batchsize \rfloor$ times to obtain epoch-level guarantees (see~\cref{algorithm:dp-sgd-loop}). 
In~\cref{section:experiments} we confirm that the resulting privacy guarantees can nevertheless be stronger than our original epoch-level guarantee.
This observation is consistent with works on DP-SGD for unstructured data that self-compose subsampled mechanisms instead of deterministically iterating over datasets (e.g.~\cite{abadi2016deep}).


\textbf{Choice of $\bm{\lambda}$.}
As before, we can ask ourselves which number of subsequences per sequence $y \in \sN$ we should choose. 
In bi-level subsampling, there is a more intricate trade-off,
because increasing $\numinstances$ decreases $\rho$, i.e., strengthens top-level amplification, but weakens bottom-level amplification (recall~\cref{proposition:deterministic_top_level_monotonicity}).
In~\cref{section:experiments}, we demonstrate numerically that $\lambda = 1$ is still  preferable under composition.
As fair baselines for this experiment, 
we use \emph{optimistic lower bounds} for $\lambda > 1$ that we derive in~\cref{appendix:bilevel_optimistic_lower_bounds}.
 

\subsection{Context--Forecast Split}\label{section:context_forecast_structure}
We have already successfully analyzed how top- and bottom-level subsampling interact to amplify  the privacy of clipped and noised gradients $g_i = \nabla_\theta \mathcal{L}(f_\theta(y_{C,i}), y_{F,i})$. 
However, we can use yet another level of forecasting-specific 
randomness  --- if we assume that an individual can change each value of a series by at most $v \in \sR_+$, i.e., we assume 
$(w,v)$-event or $(w,v)$-user-level privacy (\cref{definition:bounded_neighboring_relations}). 
We propose to augment the context and forecast window with Gaussian noise
$Z_C \sim \mathcal{N}(\vzero, \sigma_C ^2 \cdot v^2 \cdot  \eye)$, $Z_F \sim \mathcal{N}(\vzero, \sigma_F ^2 \cdot v^2 \cdot  \eye)$:
\begin{equation}\label{eq:gradient_noise}
    g_i = \nabla_\theta \mathcal{L}(f_\theta(y_{C,i} + Z_C), y_{F,i}+ Z_F).
\end{equation}
Unlike the input perturbations from~\cite{arcolezi2022differentially} which are an offline pre-processing that privatizes the dataset, 
\cref{eq:gradient_noise} is an online data augmentation that serves as an integral part of the (now continuous) subsampling procedure.
In the following, let $\hat{M}$ refer to a \emph{single training step} when combining top-level sampling without replacement, bottom-level sampling with replacement, and~\cref{eq:gradient_noise}.


\textbf{Amplification by Data Augmentation.}
%For the sake of exposition, let us assume that we add equal levels of noise to the context and forecast window, i.e., $\sigma_C = \sigma_F$, 
%and analyze the more general case in~\cref{appendix:proofs_context_forecast_split}.
Intuitively, any element can only contribute to gradient $g_i$ either via context $y_{C,i}$ or via ground-truth forecast $y_{F,i}$. Even if this element changes by $\pm v$, there is a chance that we sample the same value after adding Gaussian noise, i.e., have zero leakage. 
In~\cref{appendix:proofs_context_forecast_split}, we use conditional couplings in conjunction with the maximal couplings originally used for subsampling analysis by~\citet{balle2018privacy}
to formalize ``sampling the same value'' and prove amplification for arbitrary $\sigma_C, \sigma_F \in \sR_+$.
The following result shows the special case $\sigma_C = \sigma_F$ where the noise scale for the context and forecast window is identical 
(for the general case, see~\cref{theorem:data_augmentation_general}).
\begin{restatable}{theorem}{amplificationbyaugmentationworwr}\label{theorem:amplification_by_data_augmentation_wor_wr}
    Consider $\numinstances = 1$, 
    batch size $\batchsize$, as well as context and forecast standard deviations $\sigma_C, \sigma_F \in \sR_+$ with $\sigma_C = \sigma_F$.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and  
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.
    Define
    $\hat{P}(1) = \mog(\vmu, \tilde{\vp}, \sigma)$ with
    means
     $\hat{\vmu} = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and weights
     $\tilde{\vp} \in [0,1]^2$, 
     with $\evp_1 = 1 - \evp_2$ and
     \begin{equation*}
         \evp_2 = \rho \cdot r \cdot \mathrm{TVD}\left(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F)\right),
     \end{equation*}
     with total variation distance $\mathrm{TVD}(P,Q) = H_1(P || Q)$.
    Then, $\hat{H}(\alpha) = \sup_{x \simeqevent{1,v} x'} H_\alpha(\hat{M}_x || \hat{M}_{x'})$ fulfills 
    \begin{equation*}
        \hat{H}(\alpha) \leq 
        \begin{cases}
            H_\alpha(\hat{P}(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || \hat{P}(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{restatable}
Intuitively, this shows that Gaussian data augmentation has a similar effect to subsampling in that it shifts probability mass to the mixture component that indicates zero leakage.

\textbf{Amplification by Label Perturbation.}
An interesting special case is $0 = \sigma_C < \sigma_F$,
where privacy is only further amplified when sensitive information appears as a ground-truth forecast, i.e., we are in the ``label privacy''~\cite{chaudhuri2011sample} setting.
A standard technique for deep learning with label privacy is using random label perturbations as an offline pre-processing step~\cite{ghazi2021deep}.
Our results in~\cref{appendix:proofs_context_forecast_split} show for the first time how online label perturbations can amplify privacy in settings where we randomly switch between feature- and label-privacy, such as self-supervised (pre-)training of sequence models. 


\subsection{Additional Inference-Time Privacy}\label{section:inference_privacy}
Like other works on DP-SGD, we focus on ensuring privacy of parameters $\theta$ 
to guarantee that information from any training sequence $x_n$ does not leak when releasing the model
$f_\theta$ or forecasts $f_\theta(x_m)$ for other sequences $x_m$.
In~\cref{appendix:inference_privacy}, we additionally explore the use of input perturbations in combination with subsampling and imputation to ensure privacy for elements of $x_n$ when releasing forecast  
$f_\theta(x_n)$. 


\subsection{Limitations and Future Work}
Since we are first to analyze forecasting-specific subsampling, there are still opportunities for improvement, namely by further tightening our guarantees for
(1) bottom-level sampling with replacement and $\lambda > 1$,
(2) top-level sampling without replacement and $\lambda > 1$, and potentially 
(3) amplification by augmentation.
While we numerically investigate the discussed trade-offs
in parameterizing our subsampling scheme, future work may also want to investigate them analytically, e.g., via central limit theorems~\cite{sommer2018privacy,dong2022gaussian}.
Finally, the connection between event and $w$-event-level privacy and token- or sentence-level private language modeling~\cite{hu2024differentially} (i.e., autoregressive probabilistic forecasting for discrete-valued time series) is immediate and should be explored in future work.
