\section{Background and Preliminaries}\label{section:background}
\subsection{Differential Privacy}
The goal of differential privacy~\cite{dwork2006differential} is to map from a dataset space $\sX$ to an output space $\sO$ while ensuring indistinguishability of any neighboring pair of datasets $x \simeq x'$ that differ in one unit of sensitive information (e.g., two sets that differ in one element).
In the following, we assume $\sO = \sR^D$.
Differential privacy achieves this goal of indistinguishability via randomization, i.e., mapping to outputs via a random mechanism $M : \sX \rightarrow \sR^D$.
The random outputs $M(x), M(x')$ are considered indistinguishable if the probability of any event $O \subseteq \sR^D$ only differs by a small factor and constant, i.e., $\Pr[M(x) \in O] \leq e^\epsilon \cdot \Pr[M(x') \in O] + \delta$.
This is equivalent to bounding the hockey stick divergence of output distributions $M_x, M_{x'}$~\cite{barthe2013beyond}:
\begin{definition}
    Mechanism $M: \sX \rightarrow \sR^D$ is $(\epsilon,\delta)$-DP if and only if
    $\forall x \simeq x' :  H_{e^{\epsilon}}(M_x || M_{x'}) \leq \delta$ with 
    $
        H_{\alpha}(M_x || M_{x'}) = \int_{\sR^D} \max\{ \frac{\dd M_x}{\dd M_{x'}}(\vo) - \alpha , 0\} \cdot  \ \dd M_x(\vo).
    $
\end{definition}

\subsection{Private Training and Dominating Pairs}
In the case of DP-SGD~\cite{song2013stochastic}, the mechanism $M : \sX \rightarrow \sR^D$ is a single training step or epoch that maps training samples to $D$ updated model weights (for details, see~\cref{section:methods}).
A training run is the repeated application of this mechanism.
A central notion for determining privacy parameters $(\epsilon',\delta')$ of such a \emph{composed} mechanism is that of dominating pairs~\cite{zhu2022optimal}, which fully characterize the tradeoff between $\epsilon$ and $\delta$ of its \emph{component} mechanisms.
\begin{restatable}{definition}{dominatingpair}\label{definition:dominating_pair}
    Distributions $(P,Q)$ are a \emph{dominating pair} for component mechanism $M$ if $\sup_{x \simeq x'} H_\alpha(M_x ||M_{x'}) \leq H_\alpha(P || Q)$ for all $\alpha \geq 0$.
    If the bound holds with equality for all $\alpha \geq 0$, then $(P,Q)$ are a \emph{tight dominating pair}.
\end{restatable}
%If~\cref{definition:dominating_pair} holds with equality, then $P, Q$ optimally characterize the privacy of $M$ and we refer to them as a \emph{tight dominating pair}.
Tight dominating pairs optimally characterize the trade-off between DP parameters $(\epsilon, \delta)$.
We will repeatedly show $P$ and $Q$ to be univariate Gaussian mixtures, for which we use the following short-hand~\cite{Choquette2024}.
\begin{definition}\label{definition:mixture_of_gaussians}
    The mixture-of-Gaussians distribution with means $\vmu \in \sR^K$, standard deviation $\sigma \in \sR_+$, and weights $\vp \in [0,1]^K$ is
    $\mog(\vmu, \vp,  \sigma) = \sum_{k=1}^K \mathcal{N}(\evmu_k, \sigma) \cdot \evp_k$.
\end{definition}

Given dominating pairs for each component mechanism, one can determine $\epsilon'$ and $\delta'$ of the composed mechanism (training run) via \emph{privacy accounting} methods, such as moments accounting~\cite{abadi2016deep} or privacy loss distribution accounting~\cite{Meiser2018Buckets,sommer2018privacy}, which we explain in more detail in~\cref{appendix:background_accounting}.

\subsection{Amplification by Subsampling and Couplings}
A key property that enables private training for many iterations with strong privacy guarantees is \emph{amplification by subsampling}~\cite{kasiviswanathan2011can}: Computing gradients for randomly sampled batches strengthens differential privacy~\cite{abadi2016deep}.
More generally, one can use a \emph{subsampling scheme} $S : \sX \rightarrow \sY$ that maps from dataset space $\sX$ to a space of batches $\sY$ and an ($\epsilon',\delta')$-DP \emph{base mechanism} $B : \sY \rightarrow \sR^D$ that maps these batches to outputs
to construct a more private \emph{subsampled mechanism} $M = B \circ S$.
\citet{balle2018privacy} propose the use of \emph{couplings} as a tool for analyzing subsampled mechanisms.
\begin{restatable}{definition}{simplecoupling}
    A coupling $\Gamma$ between distributions $S_x, S_{x'}$ of randomly sampled batches $S(x), S(x') \in \sY$ is a joint distribution on $\sY^2$ whose marginals are $S_x$ and $S_{x'}$.
\end{restatable}
Intuitively, $\Gamma$  indicates which batches from the support of $S_x$ correspond to which batches from the support of $S_{x'}$ (for a more thorough introduction, see~\cite{Villani2009}).
\citet{balle2018privacy} prove that any such coupling yields a bound on the divergence of the subsampled output distributions $M_x$ and $M_{x'}$.
More recently, \citet{schuchardt2024unified} have generalized this tool to enable the derivation of dominating pairs for subsampled mechanisms, which we utilize in our proofs and explain in more detail in~\cref{appendix:background_subsampling_analysis}.

\subsection{Differential Privacy for Time Series}
In the following, we consider the domain $\sA = \sR^L$ of univariate time series of length $L$.
We discuss the straight-forward generalization of our results to multivariate time series in~\cref{appendix:generalizations}.
The goal of DP time series analysis is to compute statistics for a single series $a \in \sA$ while protecting short contiguous subsequences (``$w$-event-level privacy''~\cite{kellaris2014differentially}) or all steps to which an individual contributed (``user-level privacy''~\cite{dwork2010differential}).
For our deep learning context, we define the dataset space to be the powerset
$\sX = \mathcal{P}(\sA)$ and generalize these notions of indistinguishability to datasets as follows:
\begin{definition}
    Datsets $x=\{x_1,\dots,x_N\}$ and $x'=\{x'_1, \dots, x'_N\}$
    are $w$-event-level neighboring ($x \simeqevent{w} x'$) if they only differ in a single pair of sequences $x_n, x'_n$ that only differ in a range of indices of length $w$, i.e.,
    $x_n[t:t+w-1] \neq x'_n[t:t+w-1]$ for some $1 \leq t \leq L$.
\end{definition}
\begin{definition}
    Datsets $x=\{x_1,\dots,x_N\}$ and $x'=\{x'_1, \dots, x'_N\}$
    are $w$-user-level neighboring ($x \simequser{w} x'$) if they only differ in a single pair of sequences $x_n, x'_n$ that only differ in $w$ indices, i.e., $||x_n - x'_n||_0 \leq w$.
\end{definition}
For example, if our data were the number of patients in $N$ hospitals over $L = 365$ days, then $14$-event level privacy would protect a patient's visit to a hospital for up to $14$ days while $14$-user-level privacy would also protect multiple shorter visits.\footnote{The number of elements $w$ in $w$-user-level privacy is often omitted for historical reasons, but still used in deriving privacy guarantees, see, e.g., $\kappa$ in Table 1 and Fig.\ 2 of~\cite{mao2024differential}.}
Depending on the domain, these relations can be made more precise by constraining the magnitude of change
(e.g.,~\cite{koga2022privacy}).
For instance, an individual can only change the number of patients on a day by $\pm 1$.
We refer to this as $(w,v)$-event and $(w,v)$-user-level privacy.
\begin{definition}\label{definition:bounded_neighboring_relations}
    Consider datasets $x \simeqevent{w} x'$ or $x \simequser{w} x'$ that differ in sequences $x_n, x'_n$.
    If $\forall n, t: |x_n[t] - x'_n[t]| \leq v$, then we refer to them as $(w,v)$-event and $(w,v)$-user-level neighboring
    ($x \simeqevent{w,v } x'$ and $x \simequser{w,v} x'$)
    , respectively.
\end{definition}
