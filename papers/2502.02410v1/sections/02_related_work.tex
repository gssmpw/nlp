\section{Related Work}\label{sec:related-work}
In this section, we discuss some of the most directly related work, and refer the reader to \cref{sec:additional-related-work} for further details. 

\textbf{DP Time Series Release.} 
\citet{koga2022privacy} and~\citet{li2023locally} use subsampling to amplify the privacy of differentially private time series release.
In particular, \citet{koga2022privacy} consider a \emph{single time series} where any individual contributes to a bounded number of steps. They use 
subsampling in the time domain to reduce the probability of accessing these steps. 
Note that their sampling distribution ignores temporal structure and yields irregularly sampled time series. 
\citet{li2023locally} combines amplification by subsampling and shuffling on the \emph{dataset level}, i.e., they only randomize which time series is accessed and not which part of the time series.
In general, our goal is training private models rather than publishing sanitized data.







\textbf{Application of DP-SGD to Time Series.}
Various works have 
applied DP-SGD~\cite{mercier2021evaluating,imtiaz2020privacy, arcolezi2022differentially} or random input perturbations~\cite{li2019dp}
in specific domains like healthcare data and human mobility.
However, they do not tailor their analysis or algorithms to time series data, and instead use DP-SGD or other mechanisms in a black-box manner.  
Similarly, some works have applied DP-SGD to generative models for time series~\cite{frigerio2019differentially,wang2020part,torfi2022differentially} or applied PATE \cite{papernot2016semi} in conjunction with DP-SGD~\cite{lamp2024glucosynth}.
This paper differs from prior work in that we specifically tailor our analysis
to the structured nature of time series and the structured sampling of batches in forecasting. 


\textbf{Bi-Level Subsampling for LLMs.}
\citet{charles2024fine} and~\cite{chua2024mind} use bi-level subsampling schemes
for centralized finetuning of language models on the data of multiple users with multiple sensitive records. 
However, their privacy analysis only leverages the randomness induced by one of the sampling levels.
The other level could equivalently be replaced by a deterministic procedure (see Section \ref{sec:additional-related-work} for more details).
In comparison, we analyze the interplay of the randomness inherent to both levels.
Further note that their analysis considers arbitrary records, and is not tailored to the sequential structure of natural language.

