\section{Introduction}

%\jan{I usually prefer to get to a) the unique challenge we are trying to address and b) the method we use as quickly as possible. But since we're trying to join two separate fields, a more thorough introduction might be preferrable.}

The need for privacy in Machine Learning (ML) tasks is becoming more apparent every day with an ongoing stream of studies on the privacy risks of ML (\cite{rigaki2023survey}) and new methods to tackle these challenges \cite{liu2021machine, pan2024differential, el2022differential}. 
Among these works, Differential Privacy (DP) \cite{dwork2006differential} plays a particularly prominent role as a formal privacy model
and paradigm for privacy protection.

Historically, works on DP primarily focused on privately querying unstructured databases~\cite{dwork2010differential},
and later works on differentially private ML continued to focus on learning from unstructured datasets (e.g.~\cite{abadi2016deep}).
Recently, there has been more attention on structured data, such as graphs \cite{mueller2022sok, olatunji2021releasing}, text \cite{yue2022synthetic, charles2024fine, chua2024mind}, and time series \cite{mao2024differential}.
Time series in particular are of interest from both a ML and DP perspective.
For example, data from traffic sensors~\cite{chen2001freeway} can be used to train forecasting models for use in tasks like transport planning and logistics~\cite{lana2018road}.
Simultaneously, traffic data and its downstream applications may expose sensitive information such as individual movement profiles~\cite{giannotti2008mobility}.
However, most studies only focus on releasing time series or statistics thereof, rather than training machine learning models~\cite{shi2011privacy,fan2014adaptive, wang2016rescuedp, wang2020towards, zhang2017dpoctor, fioretto2019optstream, kellaris2014differentially, mao2024differential,katsomallos2019privacy}.%, %which can potentially be used on many downstream tasks such as training models, but can come with a higher than necessary accucary costs. 

One of the most popular algorithms for training machine learning models on unstructured datasets is DP-SGD~\cite{song2013stochastic,abadi2016deep}, which is a simple modification of SGD.
Given an unstructured set of input--target pairs $x = \{(x_1,o_1), \dots, (x_N, o_N)\}$,
DP-SGD samples an unstructured batch $y \subseteq x$,
computes clipped per-sample gradients, and adds Gaussian noise. 
This yields privacy guarantees for insertion/removal or substitution of a single element $(x_n, o_n)$.
DP-SGD has also been used to train models for time series  \cite{mercier2021evaluating,imtiaz2020privacy, arcolezi2022differentially}.
However, these works directly apply known privacy guarantees for DP-SGD in a black-box manner.
This neglects the structured nature of the data and the structured way in which batches are sampled in tasks like time series forecasting, and may thus lead to an under- or over-estimation of privacy.


This work answers the following research question: 
\textit{How private is DP-SGD when adapted to sequentially structured data, and specifically time series forecasting?}




\begin{figure*}[ht]
    \centering
    \vskip 0.2in
    \includegraphics[width=\textwidth]{figures/explainy_figure.pdf}
    \caption{High-level view on batching in global forecasting, which
    (1) selects one or multiple sequences (``top-level sample''),
    (2) selects one or multiple contiguous subsequences per sequence (``bottom-level sample''),
    and (3) partitions these subsequences for self-supervised training (``context--forecast split'').
    Elements of sensitive information from a short subsequence (red) may appear in the batch multiple times at different positions.}
    \label{fig:explainy_figure}
    \vskip -0.2in
\end{figure*}




\subsection{Our contribution}
Our main goal is to provide sound and tight bounds on how much private information is leaked when introducing gradient clipping and noise into the training of forecasting models that generalize across multiple time series (``global forecasting''~\cite{januschowski2020criteria}).

\cref{fig:explainy_figure} provides a high-level view of how a single batch is sampled in commonly used forecasting libraries like GluonTS~\cite{alexandrov2019gluonts}, and what level of privacy leakage this can cause. 
Assume we have a dataset $x = \{x_1,x_2,x_3\}$ of three time series and want to protect any length-2 subsequence (``$2$-event-level privacy''~\cite{kellaris2014differentially}).
First, a subset of time series is selected (``top-level sample'').
Then, one or multiple contiguous subsequences are sampled per sequence (``bottom-level sample'').
Finally, each subsequence is split into a context window and a ground-truth forecast.
As shown in~\cref{fig:explainy_figure}, our length-$2$ subsequence may appear in multiple subsequences, with each of its element contributing either to the context or the forecast window.
Thus, these elements may leak their information through multiple clipped and noised per-subsequence gradients --- either as context via the model's computation graph or as ground-truth via the loss function. 

This risk of multiple leakage is underestimated if we apply privacy guarantees for standard DP-SGD in a black-box manner, and overestimated if we assume that 
every subsequence always contains every piece of sensitive information (e.g.~\cite{arcolezi2022differentially}).

Our main contributions are that we, for the first time,
\begin{itemize}[noitemsep, nosep]
    \item derive event- and user-level privacy guarantees for bottom-level sampling of contiguous subsequences,
    \item analyze how the strength of these guarantees can be amplified by top-level sampling, 
    \item and prove how data augmentation can exploit the context--forecast split to further amplify privacy.
\end{itemize}
Beyond these main contributions, our work demonstrates the usefulness of coupling-based subsampling analysis~\cite{balle2018privacy,schuchardt2024unified}, which has thus far only been applied to unstructured subsampling, in analyzing non-standard, structured subsampling schemes.

