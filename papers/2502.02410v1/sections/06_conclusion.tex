\section{Conclusion}
In this  work, we answer the question how  
 DP-SGD can be adapted to time series forecasting while accounting for domain- and task-specific aspects.
We derive privacy amplification guarantees for sampling contiguous subsequences and for combining this bottom-level sampling  with top-level sampling of sequences, and additionally prove that partitioning subsequences into context and ground-truth forecasts enables privacy amplification by data augmentation.
We further identify multiple trade-offs inherent to bi-level subsampling 
which we investigate theoretically and/or numerically.
Finally, we confirm empirically that it is feasible to train differentially private forecasting models while retaining meaningful utility.
Adapting our results to natural language represents a promising direction for future work towards trustworthy machine learning on structured data.