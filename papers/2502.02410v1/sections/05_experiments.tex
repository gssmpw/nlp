\section{Experimental Evaluation}\label{section:experiments}
We already achieved our primary objective of deriving time-series-specific subsampling guarantees for DP-SGD adapted to forecasting.
Our main goal for this section is to investigate the trade-offs we discovered in discussing these guarantees.
In addition, we train common probabilistic forecasting architectures on standard datasets to verify the feasibility of training deep differentially private forecasting models while retaining meaningful utility.
The full experimental setup  is described in~\cref{appendix:experimental_setup}.
%An implementation will be made available upon publication.

\subsection{Trade-Offs in Structured Subsampling}

\begin{figure}
    \vskip 0.2in
    \centering
        \includegraphics[width=0.99\linewidth]{figures/experiments/eval_pld_deterministic_vs_random_top_level/daily_20_32_main.pdf}
        \vskip -0.3cm
        \caption{Top-level deterministic iteration (\cref{theorem:deterministic_top_level_wr}) vs top-level WOR sampling (\cref{theorem:wor_top_level_wr}) for $\numinstances=1$.
        Sampling is more private despite requiring more compositions.}
        \label{fig:deterministic_vs_random_top_level_daily_main}
    \vskip -0.2in
\end{figure}




For the following experiments, we assume that we have $N=320$ sequences, batch size $\batchsize = 32$, and noise scale $\sigma = 1$.
We further assume $L=10  (L_F + L_C) + L_F - 1$, so that 
the chance of bottom-level sampling a subsequence containing any specific element is 
$r=0.1$ when choosing $\numinstances = 1$ as the number of subsequences.
In~\cref{appendix:extra_experiments_eval_pld}, we repeat all experiments with a wider range of parameters.
All results are consistent with the ones shown here.

\textbf{Number of Subsequences $\bm{\numinstances}$.}
Let us begin with a trade-off inherent to bi-level subsampling:
We can achieve the same batch size $\batchsize$ with different $\numinstances$, each
leading to different top- and bottom-level amplification.
We claim that $\numinstances = 1$ (i.e., maximum bottom-level amplification) is preferable.
For a fair comparison, we compare our provably tight guarantee for $\numinstances=1$ (\cref{theorem:wor_top_level_wr})
with optimistic lower bounds for $\numinstances > 1$ (\cref{theorem:wor_top_wr_bottom_upper})
instead of our sound upper bounds (\cref{theorem:wor_top_level_wr_general}), i.e.,
we make the competitors stronger.
As shown in~\cref{fig:monotonicity_daily_main}, $\numinstances = 1$ only has smaller $\delta(\epsilon)$ for $\epsilon \geq 10^{-1}$ when considering a single training step.
However, after $100$-fold composition, $\numinstances = 1$ achieves smaller $\delta(\epsilon)$ even in $[10^{-3}, 10^{-1}]$ (see~\cref{fig:monotonicity_composed_daily_main}).
Our explanation is that $\numinstances > 1$ results in larger $\delta(\epsilon)$ for large $\epsilon$, i.e., is more likely to have a large privacy loss.
Because the privacy loss of a composed mechanism is the sum of component privacy losses~\cite{sommer2018privacy}, this is problematic when performing multiple training steps.
We shall thus later use $\numinstances=1$ for training.

%Intuitively, $\delta(\epsilon)$ can be interpreted as the probability that the log-likelihood ratio of $M_x$ and $M_{x'}$ (``privacy loss'') exceeds $\epsilon$.\footnote{For the formal relation between privay loss and privacy profiles, see~\cref{lemma:profile_from_pld} taken from~\cite{balle2018improving}}


\textbf{Step- vs Epoch-Level Accounting.}
Next, we show the benefit of top-level sampling sequences (\cref{theorem:wor_top_level_wr}) instead of deterministically iterating over them (\cref{theorem:deterministic_top_level_wr}), even though we risk privacy leakage at every training step.
For our parameterization and $\numinstances=1$, top-level sampling with replacement requires $10$ compositions per epoch.
As shown in~\cref{fig:deterministic_vs_random_top_level_daily_main}, the resultant epoch-level profile is nevertheless smaller, and remains so after $10$ epochs.
This is consistent with any work on DP-SGD (e.g., \cite{abadi2016deep}) that uses subsampling instead of deterministic iteration.

\textbf{Epoch Privacy vs Length.} In~\cref{appendix:extra_experiments_epoch_length} we additionally explore the fact that, if we wanted to use deterministic top-level iteration, 
the number of subsequences 
$\numinstances$ would affect epoch length.
As expected, we observe that composing many private mechanisms ($\numinstances=1$) is preferable to composing few much less private mechanisms ($\numinstances > 1$) 
when considering a fixed number of training steps.

\begin{figure}
    \vskip 0.2in
    \centering
        \includegraphics[width=0.99\linewidth]{figures/experiments/eval_pld_label_noise/daily_30_32_main.pdf}
        \vskip -0.3cm
        \caption{Varying label noise $\sigma_F$ for top-level WOR and bottom-level WR  (\cref{theorem:data_augmentation_general}) with $\sigma_C = 0, \numinstances=1$.
        Increasing $\sigma_F$ is equivalent to decreasing forecast length.
        }
        \label{fig:label_noise_daily_main}
    \vskip -0.2in
\end{figure}

\textbf{Amplification by Label Perturbation.}
Finally, because the way in which adding Gaussian noise to the context and/or forecast window 
amplifies privacy (\cref{theorem:data_augmentation_general}) 
may be somewhat opaque, let us consider top-level sampling without replacement, bottom-level sampling with replacement,
$\numinstances=1$, $\sigma_C=0$, and varying label noise standard deviations $\sigma_F$. 
As shown in~\cref{fig:label_noise_daily_main}, increasing $\sigma_F$ has the same effect as letting the forecast length $L_C$ go to zero, i.e., eliminates the risk of leaking private information if it appears in the forecast window.
Of course, this data augmentation 
will have an effect on model utility, which we investigate shortly.

\begin{figure*}
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_32_1_main.pdf}
        \caption{Training step $1$}\label{fig:monotonicity_daily_main}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_32_100_main.pdf}
        \caption{Training step $100$}\label{fig:monotonicity_composed_daily_main}
    \end{subfigure}\caption{
    Top-level WOR and bottom-level WR sampling under varying number of subsequences.
    Under composition, even optimistic lower bounds (\cref{theorem:wor_top_wr_bottom_upper}) 
    indicate worse privacy for $\numinstances > 1$ than 
    our tight upper bound for $\numinstances=1$ (\cref{theorem:wor_top_level_wr}).}
    \label{fig:monotonicity_daily_main_container}
\vskip -0.2in
\end{figure*}


\subsection{Application to Probabilistic Forecasting}
While the contribution of our work lies in formally analyzing the privacy of DP-SGD adapted to forecasting, 
training models with this algorithm can serve as a sanity-check to verify that the guarantees are sufficiently strong to retain meaningful utility under non-trivial privacy budgets.


\begin{table}[b]
\vskip -0.38cm
\caption{Average CRPS on \texttt{traffic} for $\delta=10^{-7}$. Seasonal, AutoETS, and models with $\epsilon=\infty$ are without noise.}
\label{table:1_event_training_traffic_main}
\vskip 0.18cm
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ &  $\epsilon = \infty$ \\
\midrule
SimpleFF & $0.207$ & $0.195$ & $0.193$ & $0.136$ \\ 
DeepAR & $\mathbf{0.157}$ & $\mathbf{0.145}$ & $\mathbf{0.142}$ & $\mathbf{0.124}$ \\
iTransf. & $0.211$ & $0.193$ & $0.188$ & $0.135$ \\
DLinear & $0.204$ & $0.192$ & $0.188$ & $0.140$ \\
\midrule
Seasonal   & $0.251$ & $0.251$ & $0.251$ & $0.251$\\
AutoETS   & $0.407$ & $0.407$ & $0.407$ & $0.407$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Datasets, Models, and Metrics.}
We consider three standard benchmarks: \texttt{traffic}, \texttt{electricity}, and \texttt{solar\_10\_minutes} as used in~\cite{Lai2018modeling}.
We further consider four common architectures: 
A two-layer feed-forward neural network (``SimpleFeedForward''), a recurrent neural network (``DeepAR''~\cite{salinas2020deepar}),
an encoder-only transformer (``iTransformer''~\cite{liu2024itransformer}), and a refined feed-forward network proposed to compete with attention-based models (``DLinear''~\cite{zeng2023transformers}).
We let these architectures parameterize elementwise $t$-distributions to obtain probabilistic forecasts.
We measure the quality of these probabilistic forecasts using continuous ranked probability scores (CRPS), which we approximate via mean weighted quantile losses (details in~\cref{appendix:metrics}).
As a reference for what constitutes ``meaningful utility'', we compare against seasonal na\"{i}ve forecasting and exponential smoothing (``AutoETS'') without introducing any noise.
All experiments are repeated with $5$ random seeds.


\textbf{Event-Level Privacy.} \cref{table:1_event_training_traffic_main} shows CRPS of all models on the \texttt{traffic} test set 
when setting $\delta=10^{-7}$, and training on the training set until reaching a pre-specified $\epsilon$
with $1$-event-level privacy. For the other datasets and standard deviations, see~\cref{appendix:privacy_utility_tradeoff_event_level_privacy}.
The column $\epsilon=\infty$ indicates non-DP training.
As can be seen, models can retain much of their utility and outperform the baselines, even for $\epsilon \leq 1$ which is generally considered a small privacy budget~\cite{ponomareva2023dp}.
For instance, the average CRPS of DeepAR on the traffic dataset is $0.124$ with non-DP training and $0.157$ for $\epsilon=0.5$.
Note that, since all models are trained using  our tight privacy analysis,
which specific model performs best  on which specific dataset is orthogonal to our contribution. 

\textbf{Other results.}
In~\cref{appendix:privacy_utility_tradeoff_user_level_privacy} we additionally train with $w$-event and $w$-user privacy.
In~\cref{appendix:privacy_utility_tradeoff_label_privacy}, we demonstrate that label perturbations can offer an improved privacy--utility trade-off. 
All results confirm that our guarantees for DP-SGD adapted to forecasting are strong enough to enable provably private training while retaining utility.
