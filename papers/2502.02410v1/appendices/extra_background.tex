\section{Additional Background}
In the following, we provide additional background information on the toolset we use to to derive amplification bounds for structured subsampling of time series, determine the corresponding dominating pairs, and perform privacy accounting.

We recommend reading this section before proceeding to our derivations in~\cref{appendix:proofs_bottom_level,appendix:proofs_bilevel,appendix:proofs_context_forecast_split} which rely on the Lemmata introduced here.
\subsection{Subsampling Analysis}\label{appendix:background_subsampling_analysis}
Recall from~\cref{section:background} that our goal in subsampling analysis is to determine the privacy of a subsampled mechanism $M = B \circ S$,
where $S : \sX \rightarrow \sY$ is a subsampling scheme that maps from dataset space $\sX$ to a space of batches $\sY$ and 
$B : \sY \rightarrow \sR^D$ is a base mechanism that maps these batches to outputs.
Further recall that the hockey-stick divergence of output distributions $M_x$ and $M_x$ for pairs of datasets $x, x' \in \sX$ can be bounded by via couplings:
\simplecoupling*
\begin{restatable}{lemma}{simplecouplingbound}\label{lemma:simple_coupling_bound}
    Consider a subsampled mechanism $M = B \circ S$ and datasets $x \simeq x'$. Then, for any coupling $\Gamma$ of subsampling distributions $S_x, S_{x'}$,
    \begin{equation*}
        H_\alpha(M_x || M_{x'})
        \leq
        \int_{\sY^2}
        H_\alpha(B_y || B_{y'}) \ \mathrm{d} \Gamma(y,y').
    \end{equation*}
\end{restatable}\begin{proof}
    The proof is identical to that of Eq.\ 5 in~\cite{balle2018privacy}.
\end{proof}
\citet{balle2018privacy} demonstrated that this tool can be combined with the \emph{advanced joint convexity} property of hockey-stick divergences to derive provably tight privacy guarantees for insertion, removal, or substitution of a single element in a dataset:
\begin{lemma}[Advanced joint convexity]\label{lemma:advanced_joint_convexity}
    Consider mixture distributions $P = (1-p)  P_1 + p  P_2$ and $Q = (1-p)  Q_1 + p  Q_2$ with $P_1 = Q_1$ and some $w \in [0,1]$. Given $\alpha \geq 1$, define $\alpha' = 1 + \frac{\alpha - 1}{w}$ and $\beta(\alpha) = \frac{\alpha}{\alpha'}$.
    Then,
    \begin{equation*}
        H_\alpha(P || Q) = p \cdot H_{\alpha'}(P_2 || (1 - \beta(\alpha) P_1 + \beta(\alpha) Q_2).
    \end{equation*}
\end{lemma}
\begin{proof}
    The proof is identical to that of Theorem 2 in~\cite{balle2018privacy}. In our notation, we interchange $\alpha$ and $\alpha'$.
\end{proof}

More recently, \citet{schuchardt2024unified} demonstrated that, when considering insertion and/or removal of multiple elements (``group privacy''), tighter bounds can be obtained via couplings between multiple subsampling distributions.
\begin{definition}
    A coupling $\Gamma$ between distributions $P_1, \dots, P_N$ on $\sY$ is a distribution on $\sY^N$ with marginals $P_1,\dots, P_N$. 
\end{definition}
Specifically, they propose to partition the support of subsampling distribution $S_x$ into events $A_1,\dots,A_I$ and the support of subsampling distribution $S_{x'}$ into events $E_1,\dots,E_J$, before defining a simultaneous coupling between the corresponding conditional distributions.
For example, these events can indicate the number of group elements that appear in random batches $S(x)$ and $S(x')$.
\begin{lemma}\label{lemma:coupling_bound}
    Consider a subsampled mechanism $M = B \circ S$ and datasets $x \simeq x'$, 
    and let $S_x$ and $S_{x'}$ denote the distribution of random batches $S(x)$ and $S(x')$. 
    Define two disjoint partitionings $\bigcup_{i=1}^I A_i = \mathrm{supp}(S_x)$ and $\bigcup_{j=1}^J E_j = \mathrm{supp}(S_{x'})$ of the support of $S_x$ and $S_{x'}$ such that
    $\forall i: S_x(A_i) > 0$ and $\forall j : S_{x'}(E_j) > 0$.
    Let $\Gamma$ be a coupling between the corresponding conditional subsampling distributions
    $S_x(\cdot | A_1), \dots, S_x(\cdot | A_i), S_{x'}(\cdot | E_1), S_{x'}(\cdot | E_1) \dots, S_{x'}(\cdot | E_J)$.
    Then, the hockey-stick divergence between subsampled output distributions $M_x$ and $M_{x'}$ is bounded via 
\end{lemma}
\begin{equation*}
    H_\alpha(M_x || M_{x'}) \leq \int_{\sY^{I+J}} c_\alpha(\vy^{(1)}, \vy^{(2)}) \ \dd \Gamma((\vy^{(1)}, \vy^{(2)}))
\end{equation*}
with cost function $c_\alpha : \sY^{I} \times \sY^{J} \rightarrow \sR_+$ defined by
\begin{equation}\label{eq:cost_function}
    c_\alpha(\vy^{(1)}, \vy^{(2)}) = H_\alpha\left(\sum_{i=1}^{I} B_{y^{(1)}_i} \cdot S(A_I) || \sum_{j=1}^{J} B_{y^{(2)}_j} \cdot S_{x'}(A_I) \right).
\end{equation}
\begin{proof}
    The proof is identical to that of Theorem 3.4 in~\cite{schuchardt2024unified}, since any distribution supported on a discrete, finite set has a mass function.
\end{proof}
While this bound is slightly more involved, it can be intuitively explained via comparison to~\cref{lemma:simple_coupling_bound}.
In~\cref{lemma:simple_coupling_bound}, we couple subsampling distributions $S_x$ and $S_x'$. The resultant bound was a weighted sum of  divergences between base mechanism distributions $B_y$ and $B_{y'}$ with batch $y$ from the support of $S_x$ and $y'$ from the support of $S_{x'}$.
In~\cref{lemma:coupling_bound}, we couple conditional subsampling distributions $S_x(\cdot | A_1), \dots, S_x(\cdot | A_i)$ and $S_{x'}(\cdot | E_1), S_{x'}(\cdot | E_1) \dots, S_{x'}(\cdot | E_J)$. The resultant bound is a weighted sum of divergences between two mixtures.
The components of these mixtures are base mechanism distributions $B_{y^{(1)}_i}$ and $B_{y^{(2)}_j}$ with batch $y^{(1)}_i$ from the support of $S_x(\cdot \mid A_i)$ and batch $y^{(2)}_j$ from the support of $S_{x'}(\cdot \mid E_J)$.

The benefit of this formulation is that it allows us to prove that mechanisms are dominated by mixture distributions, rather than individual distributions. This enables the derivation of tight dominating pairs in scenarios where there are multiple possible levels of privacy leakage, such as in~\cref{algorithm:dp-sgd-loop} where a single sensitive element can appear in $0$, $1$, or multiple subsequences within a batch.

A problem with~\cref{eq:cost_function} is that it requires evaluating the base mechanism distribution $B_y$ for various batches $y \in \sY$, which may be defined by a complicated function, such as the noisy gradient descent update from~\cref{algorithm:dp-sgd-step}. \citet{schuchardt2024unified} show that it can be sufficient to consider worst-case batches $\hat{y}^{(1)}_1,\dots,\hat{y}^{(1)}_I$ and $\hat{y}^{(2)}_1,\dots,\hat{y}^{(2)}_J$ that maximize the mixture divergences while retaining the pairwise distance between original batches $y^{(1)}_1,\dots,y^{(1)}_I$ and $y^{(2)}_1,\dots,y^{(2)}_J$.
\begin{definition}[Induced distance]
    Consider an arbitrary neighboring relation $\simeq_\sY$ on batch space $\sY$.
    Then the corresponding induced distance $d : \sY^2 \rightarrow \sN$ is a function such that $d(y,y') = K$ implies that there is a sequence of batches $y_1,\dots,y_{K-1}$ such that $y \simeq_\sY y_1$, $\forall k: y_k \simeq_\sY y_{k+1}$, and $x_{K-1} \simeq_\sY y'$.
\end{definition}
\begin{lemma}\label{lemma:cost_function_upper_bound}
    Let $d : \sY^2 \rightarrow \sN$ be the distance induced by a symmetric neighboring relation $\simeq_\sY$ on batch space $\sY$.
    Consider the tuples of batches $\vy^{(1)} \in \sY^I$ and $\vy^{(2)} \in \sY^J$, as well as cost function $c_\alpha$ from~\cref{eq:cost_function}.
    Then,
    \begin{equation*}
        c_\alpha(\vy^{(1)}, \vy^{(2)})
        \leq
        \sup_{\hat{\vy}^{(1)}, \hat{\vy}^{(2)}} c_\alpha(\hat{\vy}^{(1)}, \hat{\vy}^{(2)})
    \end{equation*}
    subject to $\hat{\vy}^{(1)} \in \sY^I, \hat{\vy}^{(2)} \in \sY^J$ and 
    \begin{align*}
        & d(\hat{\vy}^{(1)}_t, \hat{\vy}^{(1)}_u) \leq d(\vy^{(1)}_t, \vy^{(1)}_u) \qquad \forall t, u: 1 \leq t \leq I, 1 \leq u \leq I, \\
        & d(\hat{\vy}^{(1)}_t, \hat{\vy}^{(2)}_u) \leq d(\vy^{(1)}_t, \vy^{(2)}_u) \qquad \forall t,u : 1 \leq t \leq I, 1 \leq u \leq J, \\
        & d(\hat{\vy}^{(2)}_t, \hat{\vy}^{(2)}_u) \leq d(\vy^{(2)}_t, \vy^{(2)}_u) \qquad \forall t, u: 1 \leq t \leq J, 1 \leq u \leq J.
    \end{align*}
\end{lemma}
\begin{proof}
    The proof is identical to that of Proposition 3.5 in~\cite{schuchardt2024unified}.
\end{proof}
A particular form of this optimization problem, which we will encounter in our later derivations, arises from analyzing sensitivity-bounded Gaussian mechanisms. For a specific set of constraints, it can be shown that the optimal solution is attained by a pair of univariate mixture-of-Gaussians mechanisms (recall~\cref{definition:mixture_of_gaussians}).
\begin{lemma}\label{lemma:worst_case_insertion_removal_mixture}
    Consider standard deviation $\sigma \in \sR_+$ and mixture weights $\vw^{(1)} \in [0,1]^I$ and $\vw^{(2)} \in [0,1]^J$. 
    Let $\Omega$ be the set of all pairs of Gaussian mixtures $(P,Q)$
    with 
    $P = \sum_{i}^{I} w^{(1)}_i \cdot \mathcal{N}(\vmu^{(1)}_{i}, \sigma^2 \eye)$
    and
    $Q = \sum_{j}^{J} w^{(1)}_j \cdot \mathcal{N}(\vmu^{(2)}_{j}, \sigma^2 \eye)$
    satisfying
    \begin{align*}
        & \vmu^{(1)}_1 = \vmu^{(2)}_1 =  \vzero\\
        & ||\vmu^{(1)}_t - \vmu^{(1)}_u||_2 \leq c \cdot | t - u |   \qquad \forall t,u : 1 \leq t \leq I, 1 \leq u \leq I, \\
        & ||\vmu^{(2)}_t - \vmu^{(2)}_u||_2 \leq c \cdot | t - u |   \qquad \forall t,u : 1 \leq t \leq J, 1 \leq u \leq J.
    \end{align*}
    with some constant $c \in \sR_+$. Then,
    \begin{equation*}
        \max_{P, Q \in \Omega} H_\alpha(P,Q) = H_\alpha\left(\mog(\vmu^{(1)*}, \vw^{(1)}, \sigma) || \mog(\vmu^{(2)*}, \vw^{(2)}, \sigma)\right)
    \end{equation*}
    with univariate means $\vmu^{(1)*} = \begin{pmatrix}0 & -1 & \cdots & -I \cdot c\end{pmatrix}^T$ and
    $\vmu^{(2)*} = \begin{pmatrix}0 & 1 & \cdots & J \cdot c\end{pmatrix}^T$.
\end{lemma}
\begin{proof}
    After scaling the standard deviation by $c$, the proof is identical to that of Theorem 0.7 in~\cite{schuchardt2024unified}.
\end{proof}

\subsection{Dominating Pairs}
In the following, we shall summarize multiple known results on dominating pairs due to~\citet{zhu2022optimal}
that we will need in our later derivations.
Let us begin by recalling the definition of dominating pairs:
\dominatingpair*
Further recall that a tight dominating pair is a pair of distributions  $(P,Q)$ such that the bound in~\cref{definition:dominating_pair} holds with equality. All mechanisms have tight dominating pairs:
\begin{lemma}
    Any mechanism $M : \sX \rightarrow \sR^D$ has a tight dominating pair of distributions, i.e.,
    a pair of distributions $(P,Q)$ such that $\sup_{x \simeq x'} H_\alpha(M_x ||M_{x'}) = H_\alpha(P || Q)$ for all $\alpha \geq 0$.
\end{lemma}
\begin{proof}
    This result is a special case of Proposition 8 from~\cite{zhu2022optimal} for real-valued outputs.
\end{proof}
For privacy accounting, we will later need to evaluate $H_\alpha(P,Q)$ as a function of $\alpha$.
Such functions are referred to as \emph{privacy profiles}~\cite{zhu2022optimal}:
\begin{definition}\label{definition:privacy_profile}
    A privacy profile is a function $H : \sR_+ \rightarrow \sR$ such that there exists a pair of distributions $(P,Q)$
    with $\forall \alpha \geq 0: H(\alpha) = H_\alpha(P,Q)$.
\end{definition}
Under symmetric neighboring relations (e.g.\ substitution of elements in a set), any privacy profile corresponding to a dominating pair enjoys a symmetry that allows us to focus many of our derivations on $\alpha \geq 1$:
\begin{restatable}{lemma}{dominatingpairalphasymmetry}\label{lemma:dominating_pair_alpha_symmetry}
    Let $\simeq$ be a symmetric neighboring relation on dataset space $\sX$.
    It then holds that $\sup_{x \simeq x'} H_\alpha(M_x ||M_{x'}) \leq H_\alpha(P,Q)$ for all $\alpha \geq 1$
    if and only if $\sup_{x \simeq x'} H_\alpha(M_x ||M_{x'}) \leq H_\alpha(Q,P)$ for all $0 < \alpha \leq 1$.
\end{restatable}{lemma}
\begin{proof}
    This result corresponds to the third part of Lemma 31 from~\cite{zhu2022optimal}.
\end{proof}
Later on, we shall use the following properties of privacy profiles in identifying dominating pairs:
\begin{restatable}{lemma}{privacyprofilerequirements}\label{lemma:privacy_profile_requirements}
    A function $H : \sR_+ \rightarrow \sR$ is a privacy profile if and only if
    $H$ is convex, $H$ is decreasing, $H(0) = 1$, and $H(x) \geq \max\{1-x, 0\}$.
\end{restatable}
\begin{proof}
    This result, in combination with~\cref{definition:privacy_profile}, corresponds to the first part of  Lemma 9 in~\cite{zhu2022optimal}.
\end{proof}
Furthermore, the following result shows that for the purpose of deriving dominating pairs, it is in principle sufficient to derive privacy profiles:\footnote{Note that this result also implies that multiple dominating pairs may share the same privacy profile.}
\begin{restatable}{lemma}{dominatingpairfromprofile}\label{lemma:dominating_pair_from_profile}
    Let $H: \sR_+ \rightarrow \sR$ be a privacy profile.
    Then there exists a pair of univariate distributions $(P, Q)$ with $H(\alpha) = H_\alpha(P,Q)$.
    Specifically, distribution $P$ is supported on $[0,1)$ and has CDF $1 + H*(x-1)$, where $H^*$ is the convex conjugate of privacy profile $H$.
    Distribution $Q$ is $\mathrm{Uniform}([0,1])$.
\end{restatable}
\begin{proof}
    This result corresponds to the second part of Lemma 9 in~\cite{zhu2022optimal}.
\end{proof}

\subsection{Privacy Accounting}\label{appendix:background_accounting}
Our ultimate objective is to provide privacy guarantees for an entire training run.
The following result motivates why we can achieve this objective by deriving dominating pairs for each update step.
\begin{lemma}\label{lemma:general_composition}
    Consider a component mechanism $M : \sX \rightarrow \sR^D$
    and a component mechanism with auxiliary input $M' : \sX \times \sR^D \rightarrow \sR^D$.
    Assume that $(P,Q)$ is a dominating of $M$
    and $(P', Q')$ is a dominating pair of $x \mapsto M'(x, \vo)$ for all auxiliary inputs $\vo \in \sR^D$.
    Then,  the product measures $(P \times P', Q \times Q')$ supported on $\sR^D \times \sR^D$ 
    are a dominating pair of composed mechanism $x \rightarrow M'(x, M(x))$.
\end{lemma}
\begin{proof}
    This result corresponds to Theorem 10 in combination with Footnote 3 from~\cite{zhu2022optimal} for the special case of real-valued co-domains.
\end{proof}
In the case of DP-SGD, the auxiliary inputs are the model parameters that resulted from the previous update step. This characterization of the composed mechanism's privacy in terms of product measures is in fact the tightest possible characterization given dominating pairs of the components (see discussion of Theorem 3.2 in~\cite{dong2022gaussian}). 

While~\cref{lemma:general_composition} lets us easily define dominating pairs for composed mechanisms,  obtaining privacy parameters $(\epsilon,\delta)$ by evaluating the privacy profile
$\alpha \mapsto H_\alpha(P \times P' || Q \times Q')$ can be challenging. There exists a variety of solutions to this problem, such as moments accounting~\cite{abadi2016deep,mironov2017renyi} or central limit theorems of composition~\cite{sommer2018privacy,dong2022gaussian}.

\emph{Privacy loss distribution} (PLD) accounting is a family of SOTA approaches that enable tight numeric privacy accounting with arbitrary accuracy using the notion of privacy loss random variables introduced in~\cite{dwork2016concentrated}:
\begin{definition}\label{definition:plrv}
    Consider a pair of distributions $(P,Q)$.
    The corresponding privacy loss random variable $\plrv{P}{Q}$ is the random variable
    $\log\left(\frac{\dd P}{\dd Q}(o)\right)$ with $o \sim P$. Similarly, $\plrv{Q}{P}$ is the 
    random variable $\log\left(\frac{\dd P}{\dd Q}(o)\right)$ with $o \sim Q$.
\end{definition}
Note that one can easily convert between dominating pairs, privacy profiles, and distributions of privacy loss random variables (see Fig.\ 2 in~\cite{zhu2022optimal}). In particular, privacy profiles can be computed from privacy loss distributions as follows:
\begin{lemma}\label{lemma:profile_from_pld}
    Consider a pair of distributions $(P, Q)$ with corresponding privacy loss random variables $\plrv{P}{Q}$ and $\plrv{Q}{P}$.
    Then, $H_\alpha(P,Q) = \Pr[\plrv{P}{Q} > \log(\alpha)] - \alpha \Pr[\plrv{Q}{P} < \log(\alpha)]$ for all $\alpha \geq 0$.
\end{lemma}
\begin{proof}
    The proof is identical to that of Theorem 5 from~\cite{balle2018improving}, substituting $e^{\epsilon}$ with $\alpha$.
\end{proof}
Due to the logarithm in~\cref{definition:plrv}, the privacy loss random variable corresponding to the dominating pair $(P \times P', Q \times Q')$ of our composed mechanism is simply the sum 
$\plrv{P}{Q} + \plrv{P'}{Q'}$ of the components' privacy loss random variables.
A key insight underlying numeric privacy loss distribution accounting is that the density of the composed PLD density is thus simply a convolution of the components' PLD densitities~\cite{Meiser2018Buckets,sommer2018privacy}.
The composed PLD can thus be efficiently computed using Fast Fourier Transforms, as proposed by~\citet{koskela2020computing}.

A challenge in numerical PLD accounting is that it requires quantizing the distribution of $\plrv{P}{Q}$ while retaining sound privacy guarantees.
\citet{doroshenko2022connect} show how to optimally perform such a pessimistic quantization.
\begin{restatable}{lemma}{connectthedots}\label{lemma:connect_the_dots}
    Consider any pair of distributions $(P,Q)$ and finite set of quantization thresholds $\mathcal{E} = \{\epsilon_0,\dots,\epsilon_K\}$ with $\epsilon_k \in \sR \cup \{+\infty, -\infty\}$ and $-\infty = \epsilon_0 < \epsilon_1 < \cdots < \epsilon_k = + \infty$.
    There exists a pair of distributions $(\hat{P}^\uparrow\hat{Q}^\uparrow)$
    such that the distribution of privacy loss $\plrv{\hat{P}^\uparrow}{\hat{Q}^\uparrow}$
    is only supported on $\mathcal{E}$ and $(\hat{P}^\uparrow\hat{Q}^\uparrow)$ dominates $(P,Q)$.
    Furthermore, $(\hat{P}^\uparrow\hat{Q}^\uparrow)$ is dominated by any other distribution supported on $\mathcal{E}$ 
    and is uniquely defined by $\{(\epsilon, H_{e^\epsilon}(P,Q)) \mid \epsilon \in \mathcal{E}\}$.
\end{restatable}
\begin{proof}
    This result is an immediate consequence of Lemma 4.1 from~\cite{doroshenko2022connect} and the fact that their Algorithm 1 takes only $\{H_{e^\epsilon}(P,Q) \mid \epsilon \in \mathcal{E}\}$ as an input.
\end{proof}
Note that the last part of~\cref{lemma:connect_the_dots} means that constructing a pessimistic dominating pair for PLD accounting with quantized distributions only requires access to privacy profile $\alpha \mapsto H_\alpha(P,Q)$ at some finite set of points, rather than to $(P,Q)$ themselves.
This is similar to~\cref{lemma:dominating_pair_from_profile}, which showed that constructing a tight dominating pair only requires access to privacy profile $\alpha \mapsto H_\alpha(P,Q)$ at arbitrary $\alpha \geq 0$.

\textbf{Summary:}
To summarize, we can perform optimal numeric privacy accounting for a composed mechanism by:
(1) Determining dominating pairs of the component mechanisms,
(2) optimally quantizing the resultant privacy loss distribution using the ``connecting the dots'' method from~\cite{doroshenko2022connect}
(3) computing the composed privacy loss distribution using the Fast Fourier method of~\cite{koskela2020computing},
and (4) determining privacy parameters $(\epsilon,\delta)$ of the composed mechanism via~\cref{lemma:profile_from_pld} derived by~\citet{balle2018privacy}.

As steps 2-4 are standard algorithms implemented in libraries like Google's \texttt{privacy\_accounting} library~\cite{dpaccountinglibrary},
we can focus our analysis on determining dominating pairs for the component mechanisms, i.e., the training steps or epochs of DP-SGD for time series forecasting.