\section{Proofs from Section 4.1 (Bottom-Level Sampling)}\label{appendix:proofs_bottom_level}

In the following, let us derive (tight) pessimistic guarantees, as well as optimistic lower bounds for epoch-level accounting when using top-level deterministic iteration and bottom-level sampling with replacement (\cref{appendix:proofs_deterministic_top_wr_bottom}) or bottom-level Poisson sampling (\cref{appendix:proofs_deterministic_top_poisson_bottom}).
In addition, we shall prove that there is an optimal choice for number of subsequences $\numinstances \in \sN$  (\cref{appendix:proofs_deterministic_top_monotonicity}):
\deterministictoplevelmonotonicity*

Note that all proofs and statements of tightness assume learning without hidden states --- just like other works on privacy accounting for DP-SGD (e.g.~\cite{abadi2016deep,wang2019uniform,koskela2020computing,gopi2021numerical}.
That is, each epoch of length $K$ applied to a model $f_\theta$ with parameters $\theta \in \sR^D$ 
is a mechanism $M : \sX \rightarrow \sR^{K \times D}$ that does not simply release the final updated model parameters, but the gradient used in each update step.
By the post-processing property of differential privacy~\cite{dwork2006differential,dong2022gaussian}, releasing gradients at each  step is at least as private as releasing the updated model parameters at each step and vice-versa (assuming a fixed and known learning rate). 

Our privacy guarantees can likely be tightened if we were to assume learning with hidden states~\cite{ye2022Iteration}, i.e.,  only release the final updates model parameter via a mechanism $M : \sX \rightarrow \sR^D$.
However, such analyses based on amplification by iteration~\cite{feldman2018privacy} generally require some assumptions about convexity and/or smoothness of the loss landscape, which we cannot make in our general treatment of deep time series forecasting.

\subsection{Sampling With Replacement}\label{appendix:proofs_deterministic_top_wr_bottom}

Our main objective for top-level deterministic iteration (see~\cref{algorithm:dp-sgd-deterministic-top-level}) and bottom-level sampling with replacement (see~\cref{algorithm:dp-sgd-wr-bottom-level}) will be proving the the following tight pessimistic bound:
\deterministictoplevelwr*
We shall further prove the following pessimistic and optimistic bounds for $\numinstances \geq 1$:
\begin{restatable}{theorem}{deterministictoplevelwrgeneral}\label{theorem:deterministic_top_level_wr_general}
    Consider number of sampled subsequences $\numinstances \geq 1$,
    and let $r = \frac{L_C + L_F}{L - L_F + 1}$ be the probability of sampling a subsequence containing any specific element.
    Define
    $\overline{P}(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$
    and
    $\overline{Q}(\numinstances) = \mog(\vmu, \vp, \sigma)$
    with
    means $\vmu = \begin{bmatrix}
        0 & 2 &  4 & \cdots & 2 \cdot \numinstances
    \end{bmatrix}^T$ and 
    weights $\vp \in [0,1]^{\numinstances + 1}$ with $p_i = \mathrm{Binomial}(i-1 \mid \numinstances, r)$.
    Further define per-epoch privacy profile $H(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'})$. Then, 
    \begin{equation*}
        H(\alpha) \leq 
            H_\alpha(\overline{P}(\numinstances) || \overline{Q}(\numinstances))
    \end{equation*}
\end{restatable}
\deterministictoplevelwroptimistic*

To this end, we shall 
\begin{enumerate}[noitemsep,nosep]
    \item Prove that, under top-level deterministic iteration, the privacy of our epoch-level mechanism can be upper-bounded by  analyzing a training step for a single batch,
    \item prove that the privacy of this training step can be upper-bounded by analyzing sampling with replacement under group substitution,
    \item derive pessimistic upper bounds for sampling with replacement under group substitution,
    \item determine optimistic lower bounds that coincide with the upper bound for $\numinstances=1$ by constructing worst-case time series datasets,
    \item determine dominating pairs corresponding to our pessimistic upper bounds.
\end{enumerate}

\subsubsection{Reduction from Epoch- to Step-Level Privacy}\label{appendix:proofs_bottom_epoch_to_step}
Consider two sets of sequences $x = \{x_1,\dots,x_N\}$ and $x = \{x'_1,\dots,x'_N\}$
with $x \simeqevent{1}$, i.e., $x_n \neq x'_n$ for exactly one $n$.
If we partition these sets into subsets of size $N' = \lfloor \batchsize \mathbin{/} \numinstances \rfloor$ in a data-independent manner and use each subset for exactly one training step,
we know that only one of these steps will access the modified sequence $x_n$ and potentially leak its sensitive information.

The following statement formalizes this idea, which essentially corresponds to an adaptive form of the parallel composition property of differential privacy~\cite{mcsherry2009privacy}, expressed in the language of dominating pairs.
\begin{lemma}[Adaptive parallel composition.]\label{lemma:adaptive_parallel_composition}
    Consider some data space $\sA$ and
    a dataset space $\sX = \{x \in \mathcal{P}(\sA) \mid |a| = N\}$
    with $N = K \cdot N'$ for some $K, N' \in \sN$. 
    Further consider a sequence of $K$ adaptive mechanisms $M^{(1)},\dots,M^{(K)}$ with 
    $M^{(k)} :  \sA^{N'} \times \sR^{(k-1) \times D} \rightarrow \sR^D$.
    Define the adaptively composed mechanism
    \begin{equation*}
        M(x) = M^{(K)}\left(\{x_{N - N' + 1}, \dots, x_{N}\},
                  M^{(K-1)}\left(\{x_{N - 2N' + 1},\dots, x_{N - N'} \}, M^{(K-2)}(\dots)\right)\right)
    \end{equation*} that lets the component mechanisms only access disjoint subsets of input dataset $x$.
    Let $\simeq_\Delta$ be the substitution relation.
    Let $P^{(k)},Q^{(k)}$ be a dominating pair of
    $M^{(k)}(\cdot, z)$ under $\simeq_\Delta$ for all size-$N'$ subsets $z$ of $\sA$. Then, for all $\alpha \geq 0$, 
    \begin{equation}\label{eq:adaptive_parallel_composition}
        \sup_{x \simeq_\Delta x'} H_\alpha(M_x || M_{x'}) \leq 
        \max_k H_\alpha(P^{(k)} || Q^{(k)}).
    \end{equation}
\end{lemma}
\begin{proof}
    With a slight abuse of notation, let us in the following write $M^{(k)}(y, z)$ for the output  distribution $M_{y,z}$ of any component mechanism $M^{(k)}$ given input $y$ and auxiliary input $z$, instead of its output random variable.
    Consider any pair of datasets $x \simeq_\Delta x'$. 
    Assume w.l.o.g. that the single pair of elements $x_n$ and $x'_n$ with $x_n \neq x'_n$ is accessed by the $k$th mechanisms.
    Since $P^{(k)}, Q^{(k)}$ is a dominating pair under substitution for all size-$N'$ auxiliary inputs $z$, we know that
    \begin{align*}
        &
        \max_z
         H_\alpha\left(
            M^{(k)}(\{x_{N - (K-k +1) N' + 1},\dots, x_{N - (K-k)N'}\}, z),
            M^{(k)}(\{x'_{N - (K-k +1) N' + 1},\dots, x'_{N - (K-k)N'}\}, z)
        \right)
        \\
        \leq
        &
        H_\alpha(P^{(k)} || Q^{(k)})
    \end{align*}
    Since all component mechanisms access disjoint subsets, we further  know for any $l \neq k$ that
    \begin{align*}
        &
        \max_z
         H_\alpha\left(
            M^{(l)}(\{x_{N - (K-l +1) N' + 1},\dots, x_{N - (K-l)N'}\}, z),
            M^{(l)}(\{x'_{N - (K-l +1) N' + 1},\dots, x'_{N - (K-l)N'}\}, z)
        \right)
        \\
        = 
        & 
        \max \{ 1 - \alpha, 0 \}
        \\
        \leq 
        & 
        H_\alpha(N(0,\sigma),N(0,\sigma)) 
    \end{align*}
    for some $\sigma \in \sR_+$.

    Define $P = P^{(k)} \times \left( N(0,\sigma) \times \cdots \times N(0,\sigma)\right)$
    and $Q = P^{(k)} \times \left( N(0,\sigma) \times \cdots \times N(0,\sigma)\right)$.
    From the above two inequalities and 
     the composition theorem for dominating pairs (see~\cref{lemma:general_composition}, and see proof of Theorem 27 in~\cite{zhu2022optimal} for intermediate steps),
    we know that $H_\alpha(M_x || M_{x'}) \leq H_\alpha(P,Q)$. Thus, by definition of hockey stick divergence $H_\alpha$, 
    \begin{align*}
        &H_\alpha(M_x || M_{x'})
        \\
        \leq
        &
        \int_{\sR^{K \times D}}
        \max
        \left\{
            0, \frac{d P}{d Q}(\mO)
            - \alpha
        \right\}
        \dd
        Q(\mO)
        \\
        =
        &
        \int_{\sR^{K \times D}}
        \max
        \left\{
            0, \frac{d P^{(k)}}{d Q^{(k)}}(\mO_k)
            - \alpha
        \right\}
        \dd
        Q(\mO)
        \\
        =
        &
        \int_{\sR^{K \times D}}
        \max
        \left\{
            0, \frac{d P^{(k)}}{d Q^{(k)}}(\mO_k)
            - \alpha
        \right\}
        \dd
        Q^{(k)}(\mO_k)
        \\
        = 
        &
        H_\alpha(P^{(k)} || Q^{(k)}),
    \end{align*}
    where the second-to-last equality follows from marginalizing the output of all mechanisms $M^{(l)}$ with $l \neq k$.
    Our result from~\cref{eq:adaptive_parallel_composition} then follows immediately from taking the supremum over all pairs of datasets $x \simeq_\Delta x'$
    and the maximum over all steps $k$ in which the substituted elements $x_n \neq x'_n$ can appear.
\end{proof}

\subsubsection{Reduction to Subsampled Group Privacy}\label{appendix:proofs_bottom_step_to_group}
Let us assume w.l.o.g.\ that $x_1 \neq x'_1$ for our two datasets $x \simeqevent{1} x'$.
That is,  the single modified time series contributes to the first pair of top-level batches
$x^{(1)} = \{x_1,x_2,\dots,x_{N'}\}$ and $x'^{(1)} = \{x'_1,x_2,\dots,x_{N'}\}$
with $N' = \lfloor \batchsize \mathbin{/} \numinstances \rfloor$.

Let $M^{(k)}$ be the mechanism that yields bottom-level subsampled, clipped, summed, and noised gradients for the $k$th top-level batch.
Since all top-level batches are disjoint, we can apply the parallel composition lemma from~\cref{lemma:adaptive_parallel_composition}.
Since all top-level batches $x^{(k)}, x^{(k)}$ are identical for $k > 1$, we already know that the $k$th step is perfectly private, i.e.,
$\forall k > 1 : H_\alpha(M^{(k)}_{x^{(k)}} || M^{(k)}_{x^{(k)}}) = \max \{1 - \alpha, 0 \}$.
Thus, the maximum in~\cref{eq:adaptive_parallel_composition} will be attained by $M^{(1)}$ and we can focus on analyzing the privacy of the first gradient step. 

For the following proof, let $\check{S} : \sR^{L} \rightarrow \mathcal{P}(\sR^{L_C + L_F})$ be the bottom-level subsampling function described by~\cref{algorithm:dp-sgd-wr-bottom-level}
that takes a single sequence and yields multiple subsequences of length $L_C + L_F$.
Let $G : \mathcal{P}(\sR^{L_C + L_F}) \rightarrow \sR$ be the function that yields clipped and summed per-subsequence  gradients
for a set of subsequences, i.e.,~\cref{algorithm:dp-sgd-step} without adding Gaussian noise.

The next result shows that we only need to analyze the privacy of the gradients for modified time series $x_1$ and $x'_1$, rather than the entire top-level batch $x^{(1)}$:
\begin{lemma}\label{lemma:proofs_bottom_step_to_group}
    Let $M^{(1)} : \mathcal{P}(\sR^{L}) \rightarrow \mathcal{P}(\sR^{D}$ be the mechanism that yields bottom-level subsampled, clipped, summed, and noised gradients for the first top-level batch.
    Let $\check{M} : \sR^{L} \rightarrow \sR^{D}$
    with $\check{M}(x_n)  = Z + (G \circ \check{S})(x_n)$ and $Z \simeq \mathcal{N}(0, \sigma^2 C^2 \eye)$
    be the mechanism that yields bottom-level subsampled, clipped, summed, and noised gradients for a single sequence.
    Consider an arbitrary pair of top-level batches $x^{(1)} \simeqevent{1} x'^{(1)}$
    with $x_1 \neq x'_1$ where $x_1 \in x^{(1)}$ and $x'_1 \in x'^{(1)}$.
    Then, for all $\alpha \geq 0$,
    \begin{equation*}
        H_\alpha(M_{x^{(1)}} || M_{x'^{(1)}}) \leq \sup_{x_1, x'_1} H_\alpha(\check{M}_{x_1} || \check{M}_{x'_1})
        \quad \text{s.t. } \{x_1\} \simeqevent{1} \{x'_1\}.
    \end{equation*}
\end{lemma}
\begin{proof}
    Consider a pair of top-level batches $x^{(1)} = \{x_1,x_2,\dots,x_{N'}\}$ and $x'^{(1)} = \{x'_1,x_2,\dots,x_{N'}\}$.
    
    Since addition of Gaussian gradient noise commutes with summing over per-sequence gradients, we can write
    $M^{(1)}(x^{(1)})$ via
    \begin{equation*}
        M^{(1)}(x^{(1)}) = Z + \sum_{n=1}^{N'} (G \circ \check{S})(x_i)
                         = Z + (G \circ \check{S})(x_1) + \sum_{n=2}^{N'} (G \circ \check{S})(x_i)
                         = \check{M}(x_1) + \sum_{n=2}^{N'} (G \circ \check{S})(x_i).
    \end{equation*}
    By the post-processing property of differential privacy~\cite{dwork2006differential,dong2022gaussian},
    this sum over per-sequence gradients where only the first one is randomly perturbed,  is at least as private as releasing the per-sequence gradients individually. That is, 
    the following mechanism attains greater or equal hockey stick divergence:
    \begin{equation*}
        x^{(1)} \mapsto \left(\check{M}(x_1), (G \circ \check{S})(x_2),\dots,(G \circ \check{S})(x_{N'})  \right).
    \end{equation*}
    This is a (non-)adaptive parallel composition, i.e., we can apply~\cref{lemma:adaptive_parallel_composition}.
    Since $x_n = x'_n$ for all $n > 1$, we already know that all outputs except the first one are perfectly private,
    i.e., have privacy profile $\alpha \mapsto \max \{ 1 - \alpha, 0\}$.
    Thus, the maximum in~\cref{eq:adaptive_parallel_composition} must be attained by $H_\alpha(\check{M}_{x_1} || \check{M}_{x'_1})$
    for some worst-case choice of  sequences $x_1, x'_1$ that differ in one element, i.e., $\{x_1\} \simeqevent{1} \{x'_1\}$.
\end{proof}

Finally, let us recall from~\cref{algorithm:dp-sgd-wr-bottom-level} that $\check{S} : \sR^{L} \rightarrow \mathcal{P}(\sR^{L_C + L_F})$
samples $\numinstances$ subsequences of length $L_C + L_F$ with replacement from the $T = L - L_F + 1$ available subsequences.
Furthermore, exactly $L_C+L_F$ such subsequences differ between sequence $x_1$ and $x'_1$.
Abstracting away from our time series context, this is equivalent to privacy under group substitution:
\begin{definition}
    Consider a dataset space $\sX = \mathcal{P}(\sA)$ with underlying set $\sA$.
    Two datasets $x, x' \in \sX$ of size $T$ are $k$-group-substitution neighboring
    ($x \simeq_{k,\Delta} x'$)
    if there are groups $g \subseteq x$ and $g' \subseteq x'$  with $|g| = |g'| = k$
    and $x \setminus g = x \setminus g'$.
\end{definition}
Furthermore, abstracting away from our machine learning context,
our gradient mechanism $G(\cdot) + Z$ with $Z \simeq \mathcal{N}(0,\sigma^2 C^2 \eye)$ is simply a calibrated Gaussian mechanism with an underlying function of bounded sensitivity $\Delta_2 = C$~\cite{song2013stochastic}:
\begin{definition}
    Consider a batch space $\sY = \mathcal{P}(\sA)$ or $\sY = \mathcal{P}_\mathrm{multi}(\sA)$,
    where $\mathcal{P}_\mathrm{multi}(\sA)$ is the set of all multisets that can be constructed from underlying set $\sA$.
    A function $f : \sY \rightarrow \sR^D$ has $\ell_2$-sensitivity $\Delta_2$ under insertion/removal ($\simeq_\pm$)
    if $\forall y, y' \in \sY : y \simeq_\pm y' \implies ||f(y) - f(y')|| \leq \Delta_2$.
\end{definition}
Based on our results from this and the previous section, we can focus on analyzing the privacy of such mechanisms under group substitution.

\subsubsection{Sampling with Replacement under Group Substitution}
In the following, we apply the conditional coupling approach from~\cite{schuchardt2024unified} (recall~\cref{lemma:coupling_bound}).
That is, we partition the support of our subsampling distributions into events
and define a joint coupling between the corresponding subsampling distributions.
\begin{lemma}\label{lemma:group_substitution_wr_objective}
    Consider a dataset space $\sX = \mathcal{P}(\sA)$ with underlying set $\sA$.
    and batch space $\sY = \mathcal{P}_\mathrm{multi}(\sA)$.
    Let $S : \sX \rightarrow \sY$ be subsampling with replacement with batch size $\numinstances$.
    Let base mechanism $B : \sY \rightarrow \sR^D$ be a Gaussian mechanism $B(y) = f + Z$ with $Z \sim \mathcal{N}(0,\Delta_2 \sigma^2 \eye)$, where $\Delta_2$ is the $\ell_2$-sensitivity of underlying function $f : \sY \rightarrow \sR^D$
    under insertion/removal ($\simeq_\pm)$.
    Define subsampled mechanism $M = B \circ S$
    and consider $k$-group-substitution neighboring datasets $x \simeq_{k,\Delta} x'$ of size $T$.
    Then, for all $\alpha \geq 0$,
    \begin{equation}
        H_\alpha(M_x || M_{x'}) \leq \max_{P, Q \in \Omega} H_\alpha(P || Q),
    \end{equation}
    where $\Omega$ is set of all pairs of multivariate Gaussian mixtures $(P,Q)$
    with 
    $P = \sum_{i=1}^{\numinstances + 1} w_i \cdot \mathcal{N}(\vmu^{(1)}_{i}, \sigma^2 \eye)$
    and
    $Q = \sum_{j=1}^{\numinstances + 1} w_j \cdot \mathcal{N}(\vmu^{(2)}_{j}, \sigma^2 \eye)$
    satisfying
    \begin{align}\label{eq:group_substituion_wr_constraints}
        \begin{split}
        & ||\vmu^{(1)}_i - \vmu^{(1)}_j||_2 \leq 2 \cdot | i - j|   \qquad \forall i,j : 1 \leq i  \leq \numinstances + 1, 1 \leq u \leq \numinstances + 1 \\
        & ||\vmu^{(2)}_i - \vmu^{(2)}_j||_2 \leq 2 \cdot | i -  j|   \qquad  \forall i,j : 1 \leq i  \leq \numinstances + 1, 1 \leq u \leq \numinstances + 1\\
        & ||\vmu^{(1)}_i - \vmu^{(2)}_j||_2 \leq 2 \cdot \max \{ i, j \}   \qquad \forall i,j : 1 \leq i  \leq \numinstances + 1, 1 \leq u \leq \numinstances + 1, 
        \end{split}
    \end{align}
    with $\forall i  : \vmu^{(1)}_i \in \sR^D$ and $w_i = \mathrm{Binomial}(i - 1 \mid \numinstances, r)$ and $r = \frac{k}{T}$.
\end{lemma}
\begin{proof}
    Since $x \simeq_{k,\Delta} x'$, there must be two groups $g = \{a_1,\dots,a_k\},
    g' = \{a'_1,\dots,a'_k\}$ with $|g| = |g'| = k$ 
    and $x \setminus g = x \setminus g'$ whose elements we assign an arbitrary ordering. 
    Let $A_i = \{y \subseteq x \mid y \cap g = i\}$
    and $E_j = \{y \subseteq x' \mid y \cap g' = j\}$
    be the events that $i$ and $j$ group elements are sampled from $x$ and $x'$, respectively. Note that $\subseteq$ refers to sub-multisets, where a single element can be sampled multiple times.
    As can be easily verified, $S_x(A_i) = \mathrm{Binomial}(i \mid \numinstances, r)$ and $S_{x'}(E_j) = \mathrm{Binomial}(j \mid \numinstances, r)$
    
    Let $s_x : \sY \rightarrow [0,1] $ and $s_{x'} : \sY \rightarrow [0,1]$ be the densitities of subsampling distributions $S_x$ and $S_{x'}$. As any batch that contains $i$ or $j$ group elements under condition $A_i$ or $A_j$ is equally likely we have 
    \begin{equation*}
        s_x(y \mid A_i) \propto \indicator\left[y \in A_i\right],  \qquad s_x(y \mid E_i) \propto \indicator\left[y \in E_J\right].
    \end{equation*}
    We can thus define a joint coupling $\Gamma$ of $S_x(\cdot \mid A_0),\dots,S_x(\cdot \mid A_\numinstances), S_{x'}(\cdot \mid E_0),\dots,S_{x'}(\cdot \mid E_\numinstances)$
    via the following mass function $\gamma : \sY^{2 \cdot (\numinstances + 1)}  \rightarrow [0,1]$:
    \begin{align*}
        \gamma(\vy^{(1)}, \vy^{(2)})
        \propto
        s_x(y^{(1)} \mid A_0)
        \cdot
        \prod_{i=0}^{\numinstances - 1}
        \gamma(y^{(1)}_{i+1} \mid y^{(1)}_{i})
        \cdot
        \prod_{i=0}^{\numinstances}
        \gamma(y^{(2)}_{i} \mid y^{(1)}_{i})
    \end{align*}
    with
    \begin{align*}
        \gamma(y^{(1)}_{i+1} \mid y^{(1)}_{i})
        \propto
        \indicator \left[
            \exists a \in y^{(1)}_{i}, a' \in g :
            a \notin g \land y^{(1)}_{i+1} = y^{(1)}_{i} \setminus \{a\} \cup \{a'\}
        \right]
        \\
        \gamma(y^{(2)}_{i} \mid y^{(1)}_{i})
        =
        \indicator \left[
            y^{(2)}_{i} \setminus g' =  y^{(1)}_{i} \setminus g
        \right]
        \cdot 
        \indicator \left[
            \forall 1 \leq l \leq k:
            a_l \in y^{(1)}_{i} \implies a'_l \in y^{(2)}_{i}
        \right]
    \end{align*}
    In short, we sample uniformly at random with replacement a multiset that does not contain any elements from group $g$ to obtain $y^{(1)}_0$.
    Then, we iteratively construct $y^{(1)}_{i+1}$ from $y^{(1)}_{i}$ by replacing a non-group element uniformly at random with a group element.
    Finally, we construct $y^{(2)}_{i}$ from $y^{(1)}_{i}$ by replacing all elements from group $g$ with their counterpart from $g'$.
    By construction, all marginals are uniformly supported on the support of their corresponding distributions, i.e., we have a valid coupling.

    Now, let $d(y, y')$ be the induced distance under \emph{insertion/removal} between multiset-batches $y$ and $y'$, i.e., the number of insertions or removals needed to construct one from the other. By definition, we have for the entire support of the coupling:
    \begin{align*}
        & d(y^{(1)}_i - y^{(1)}_j) \leq 2 \cdot | i - j |   \qquad \forall i,j : 1 \leq i  \leq \numinstances, 1 \leq u \leq \numinstances, \\
        & d(y^{(2)}_i - y^{(2)}_j) \leq 2 \cdot | i - j |   \qquad \forall i,j : 1 \leq i  \leq \numinstances, 1 \leq u \leq \numinstances, \\
        & d(y^{(1)}_i - y^{(2)}_j) \leq 2 \cdot \max \{ i, j \}   \qquad \forall i,j : 1 \leq i  \leq \numinstances, 1 \leq u \leq \numinstances.
    \end{align*}
    This is because $y^{(1)}_j$ with $j \geq i$ is iteratively constructed from $y^{(1)}_1$ via $j - i$ substitutions, i.e., $2 \cdot |i-j|$ insertions/removals.
    Furthermore, $y^{(2)}_j$ with $j \geq i$ is constructed from $y^{(1)}_i||_2$ by substituting $i$ elements from group $i$, inserting their $i$ counterparts from group $g'$, and then substituting an additional $j - i$ elements for a total of $(j-i) + i = j$ substitutions.
    The case $j \leq i$ is analogous.

    The result then immediately follows from considering worst-case datasets
    (recall~\cref{lemma:cost_function_upper_bound}) fulfilling these distance constraints,
    and the fact that base mechanism $B : \sY \rightarrow \sR^D$
    is a Gaussian mechanism with covariance $\sigma^2 \Delta_2 \eye$ whose underlying function $f : \sY \rightarrow \sR^D$ has
    $\ell_2$ sensitivity $\Delta_2$ under insertion/removal.
\end{proof}
Maybe somewhat surprisingly, the bound is identical to sampling with replacement under a single substition (see Theorem L.3 in~\cite{schuchardt2024unified}), except for a change of Binomial distribution parameter $r$ from $\frac{1}{T}$ to $\frac{k}{T}$. 
    
Next, we can solve a relaxed form of the optimization problem in~\cref{lemma:group_substitution_wr_objective} to obtain our pessimistic upper bound for arbitrary $\numinstances$.
\deterministictoplevelwrgeneral*
\begin{proof}
    As discussed in the previous two, the privacy of $M$ can be upper-bounded by
    analyzing group privacy under substitution using sampling with replacement.
    Specifically, we can instantiate~\cref{lemma:group_substitution_wr_objective} with sensitivity $\Delta_2$ equal to clipping constant $C$,
    group size $k = L_C + L_F$ and dataset size $T = L - L_F + 1$.
    Due to translation equivariance of hockey stick divergences between Gaussians, we can assume w.l.o.g.\
    that $\vmu^{(1)}_1 = \vmu^{(2)}_1 = \vzero$.
    We can further discard $||\vmu^{(1)}_i - \vmu^{(2)}_j||_2 \leq 2 \cdot \max \{ i, j \}$ to obtain a relaxed optimization problem.
    
    The optimal solution to the relaxed optimization problem is known from~\cite{schuchardt2024unified} (see~\cref{lemma:worst_case_insertion_removal_mixture}) and corresponds exactly to our result.
\end{proof}
For the special case of $\numinstances = 1$, we can solve the optimization problem exactly to obtain a tight upper bound
(we shall prove tightness, i.e.,  the $\geq$ part of~\cref{theorem:deterministic_top_level_wr}, by constructing a pessimistic lower bound in the next section):
\begin{lemma}\label{theorem:deterministic_top_level_wr_leq_half}
    Consider number of sampled subsequences $\numinstances = 1$,
    and let $r = \frac{L_C + L_F}{L - L_F + 1}$ be the probability of sampling a subsequence containing any specific element.
    Define
    $P(1) = \mog(\vmu, \vp, \sigma)$ with
    means $\vmu = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and 
    weights $\vp = \begin{bmatrix} 1-r & r\end{bmatrix}^T$. Further define per-epoch privacy profile $H(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'})$. Then, 
    \begin{equation*}
        H(\alpha) \leq 
        \begin{cases}
            H_\alpha(P(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 0,\\
            H_\alpha(\mathcal{N}(0,\sigma) || P(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{lemma}
\begin{proof}
    Let us begin with the case $\alpha \geq 1$.
    As before, we can instantiate~\cref{lemma:group_substitution_wr_objective} with sensitvity $\Delta_2$ equal to clipping constant $C$,
    group size $k = L_C + L_F$ and dataset size $T = L - L_F + 1$.
    Due to translation equivariance of hockey stick divergences between Gaussians, we can assume w.l.o.g.\
    that $\vmu^{(1)}_2 = \vzero$.

    Thus, our optimization problem becomes
    \begin{equation*}
        \max_{\vmu^{(1)}, \vmu^{(2)}} H_\alpha\left((1-r) \cdot \mathcal{N}(\vmu^{(1)}_1, \sigma^2 \eye) + r \cdot \mathcal{N}(\vmu^{(1)}_2, \sigma^2 \eye) )
                    || 
                    (1-r) \cdot \mathcal{N}(\vmu^{(2)}_1, \sigma^2 \eye) + r \cdot \mathcal{N}(\vmu^{(2}_2, \sigma^2 \eye) \right)
    \end{equation*}
    subject to $\vmu^{(1)}_2 = 0$, $\vmu^{(1)}_1 = \vmu^{(2)}_1$,  $||\vmu^{(2)}_1 - \vmu^{(2)}_2|| \leq 2$, and $||\vmu^{(1)}_2 - \vmu^{(2)}_2|| \leq 2$.
    
    Since the first two mixture components are identical and have identical weights, we can apply the advanced joint convexity property of hockey stick divergences (\citet{balle2018privacy}, see~\cref{lemma:advanced_joint_convexity})
    to eliminate $\vmu^{(1)}_1$ and rewrite our objective as
    \begin{equation*}
        \max_{\vmu^{(1)}, \vmu^{(2)}} r \cdot H_{\alpha'}\left(\mathcal{N}(\vmu^{(1)}_1, \sigma^2 \eye) )
                    || 
                    (1 - \beta(\alpha)) \cdot \mathcal{N}(\vmu^{(2)}_1, \sigma^2 \eye) + \beta(\alpha) \cdot \mathcal{N}(\vmu^{(2}_2, \sigma^2 \eye) \right)
    \end{equation*}
    with some $\alpha' \geq \alpha$ and $\beta(\alpha) \in [0,1]$.
    Since we eliminated one variable, we are left with constraints $\vmu^{(1)}_2 = 0$,  $||\vmu^{(2)}_1 - \vmu^{(2)}_2|| \leq 2$, and $||\vmu^{(1)}_2 - \vmu^{(2)}_2|| \leq 2$.
    Since we now have only distance constraints to the origin, we can apply~\cref{lemma:worst_case_insertion_removal_mixture}
    to arrive at the optimal value
    \begin{align*}
        &r \cdot H_{\alpha'}\left(\mathcal{N}(0, \sigma) )
                    || 
                    (1 - \beta(\alpha)) \cdot \mathcal{N}(2, \sigma) + \beta(\alpha) \cdot \mathcal{N}(2, \sigma) \right)
        \\
        = 
        &
        H_\alpha\left((1-r) \cdot \mathcal{N}(2, \sigma) + r \cdot \mathcal{N}(0, \sigma) )
                    || 
                    \mathcal{N}(2, \sigma)\right)
    \end{align*}
    where the equality follows from reverse application of the advanced joint convexity property, and the fact that
    the two components of the second distribution are identical.
    The result for $\alpha \geq 1$ then follows from rotating and translating the coordinate system such that the second distribution has its mean at the origin.

    For the case $0 \leq \alpha < 1$, we can use the following fact:
    If $P,Q$ is dominating for $\alpha \geq 1$ under a symmetric neighboring relation,
    then $Q, P$ is dominating for $0 \leq \alpha < 1$ (\citet{zhu2022optimal}, see~\cref{lemma:dominating_pair_alpha_symmetry}).
\end{proof}

\subsubsection{Optimistic Lower Bounds}\label{appendix:deterministic_top_wr_bottom_lower_bounds}
Next, we construct optimistic lower bounds by constructing a worst-case gradient function and a pair of datasets 
for each $\alpha \geq 0$.
\deterministictoplevelwroptimistic*
\begin{proof}
    Since our model $f_\theta$ is an arbitrary parametric function,
    we can assume that it is the anti-derivative of any desired gradient function $g : \sR^{L_C + L_F} \rightarrow \sR^D$.
    We choose the following function:
    \begin{equation*}
        g(a) = \begin{cases}
            C \cdot \ve_1 & \text{if } \exists l \in \{1,\dots,L_C + L_F\} : a_l = 1, \\
            -C & \ve_1 \text{otherwise.}
        \end{cases}
    \end{equation*}
    where $\ve_1$ is the indicator vector that is non-zero in its first component, 
    and $C$ is the clipping constant (i.e., our per-sequence gradients will never be affected by clipping).
     

    \textbf{Case 1 ($\alpha \geq 1$):}
    Consider sequences $x_1, x'_1 \in \sR^{L}$ with
    $x_1 = \begin{bmatrix}
        1 & 0 & \cdots & 0
    \end{bmatrix}$
    and 
    $x'_1 = \begin{bmatrix}
        0 & 0 & \cdots & 0
    \end{bmatrix}$
    that differ in their first element.
    Note that the first element can appear in $L_C + L_F$ different subsequences in different positions because~\cref{algorithm:dp-sgd-wr-bottom-level} zero-pads the sequence before sampling with replacement.
    Further consider sequences $x_2,\dots,x_N \in \sR^L \setminus \{1\}$ such that $\forall m > n > 1 : x_m \neq x_n \land x_1 \neq x_n \neq x_1'$ (so that our dataset is a proper set, i.e., does not have duplicates).
    Define datasets $x = \{x_1,x_2,\dots,x_N\}$ and $x' = \{x'_1,x_2,\dots,x_N\}$.
    
    By construction, $x \simeqevent{1} x'$.
    For every sequence in a top-level batch,
    we sample $\numinstances$ subsequences with replacement, there are $L - L_F + 1$ subsequences in total, and there are exactly $L_C + L_F$ subsequences in $x_1$ that contain $1$.
    These $L_C + L_F$ subsequences will add $C$ to our summed gradient when sampled, while all other gradients subtract $C$.
    Due to translation invariance of hockey stick divergences between Gaussian mixtures (i.e., we can translate $-C \cdot \numinstances$ into the origin) and after marginalizing out all but the first dimension, we exactly attain our desired bound.
    The subsequent training steps operate on identical data and will thus not contribute to the hockey stick divergence attained by the epoch-level mechanism (same argument as in~\cref{lemma:adaptive_parallel_composition}).

    \textbf{Case 2 ($0 \leq \alpha < 1$):}
    For this case, we define $x = \{x'_1,x_2,\dots,x_N\}$ and $x' = \{x_1,x_2,\dots,x_N\}$, i.e., interchange the first sequence between $x$ and $x'$.
    The remaining proof is symmetric to the previous case.
\end{proof}
Note that, for $\numinstances = 1$, \cref{theorem:deterministic_top_level_wr_optimistic} coincides with~\cref{theorem:deterministic_top_level_wr_leq_half}, i.e., our pessimistic bound is tight. Thus, this also concludes our proof of~\cref{theorem:deterministic_top_level_wr}.

\subsubsection{Dominating pairs.}\label{appendix:bottom_level_dominating_pairs}
Next, let us discuss how to construct dominating pairs corresponding to our bounds.

In the case of our generic pessimistic bound for $\numinstances \geq 1$ (see~\cref{theorem:deterministic_top_level_wr_general}), we simply have
$\sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'}) \leq H_\alpha(\overline{P}(\numinstances) || \overline{Q}(\numinstances))$, where $\overline{P}(\numinstances)$ and $\overline{Q}(\numinstances$ are univariate mixtures of Gaussians. Evidently, these two mixtures are a dominating pair.

In the case of our tight bound for $\numinstances \geq 1$, we have
\begin{equation*}
\sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'}) = 
\begin{cases}
    H_\alpha(P(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
    H_\alpha(\mathcal{N}(0,\sigma) || P(1)) & \text{if } 0 \leq \alpha < 1.
\end{cases}
\end{equation*}
Due to equality, the r.h.s.\ term is a valid privacy profile by definition (cf.~\cref{lemma:privacy_profile_requirements}).
For the purpose of further analysis, one can construct a dominating pair via convex conjugation~\cite{zhu2022optimal}:
\dominatingpairfromprofile*
For the purpose of numerical privacy accounting, we need to construct a quantized dominating pair.
As is known from~\cite{doroshenko2022connect}, the best possible quantized dominating pair only requires access to a privacy profile:
\connectthedots*
Thus, we can algorithmically generate a dominating pair using the ``connect-the-dots''~\cite{doroshenko2022connect} method without explicitly needing to construct a non-quantized dominating pair. 
We use the latter approach for all our experiments.


\subsection{Poisson Sampling}\label{appendix:proofs_deterministic_top_poisson_bottom}

\begin{algorithm}
   \caption{Bottom-Level Poisson Sampling}
   \label{algorithm:dp-sgd-poisson-bottom-level}
\begin{algorithmic}
    \STATE {\bfseries Input:}
        Sequence $x_n$, context length $L_C$, forecast length $L_F$, expected subsequences $\numinstances$
    \STATE {$x'_n \gets$ prepend\_zeros($x_n, L_C$)} \hfill \COMMENT {padding}
    \STATE {$T \gets L - L_F + 1$} \hfill \COMMENT {maximum start index}
    \STATE {$r \gets \min\{1, \numinstances \mathbin{/} T\}$} \hfill \COMMENT {Rate to sample $\numinstances$ in expectation.}
    \FOR{$t \gets 1$ \textbf{to} $T$}
        \IF{$\mathrm{Uniform}[0,1] \leq r$}
            \STATE {\textbf{yield}} $x'_n[t : t + L_C +  L_F - 1]$ \hfill \COMMENT {cropping}
        \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Our main objective for top-level deterministic iteration (see~\cref{algorithm:dp-sgd-deterministic-top-level}) and bottom-level Poisson sampling (see~\cref{algorithm:dp-sgd-poisson-bottom-level}) will be proving the the following tight pessimistic bound for arbitrary $\numinstances \in \sN$:
\begin{restatable}{theorem}{deterministictoplevelpoisson}\label{theorem:deterministic_top_level_poisson}
    Consider an expected number of subsequences $\numinstances \in \mathbb{N}$ and let $r = \min\{1, \numinstances \mathbin{/} (L - L_F + 1)\}$ be the resultant sampling rate from~\cref{algorithm:dp-sgd-poisson-bottom-level}.
    Then 
    $P(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $Q(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r)$
    are a tight dominating pair of epoch $M$ under $\simeqevent{1}$.
\end{restatable}

To this end, we can 
\begin{enumerate}[noitemsep,nosep]
    \item Prove that, under top-level deterministic iteration, the privacy of our epoch-level mechanism can be upper-bounded by  analyzing a training step for a single batch,
    \item prove that the privacy of this training step can be upper-bounded by analyzing Poisson sampling under group substitution,
    \item derive pessimistic upper bounds for sampling with replacement under group substitution,
    \item determine optimistic lower bounds that coincide with the upper bound for all $\numinstances \in \sN$ by constructing worst-case time series datasets,
    \item determine tight dominating pairs corresponding to our tight pessimistic bounds.
\end{enumerate}
The first two steps are identical across all , since they do not depend on the distribution of the bottom-level subsampling procedure (see~\cref{appendix:proofs_bottom_epoch_to_step,appendix:proofs_bottom_step_to_group}).
For the third step, we can use the following known result from~\cite{schuchardt2024unified} for group insertion/removal.
\begin{definition}
    Consider a dataset space $\sX = \mathcal{P}(\sA)$ with underlying set $\sA$.
    Two datasets $x, x' \in \sX$ of size $T$ are $(k_+,k_-)$-group-insertion/removal neighboring
    ($x \simeq_{k_+,k_-,\pm} x'$)
    if there are groups $g_- \subseteq x$ and $g_+ \subseteq x'$  with $|g_-| = k_-$ and $g_+| = k_+$
    such that $x' =   x \setminus g_- \cup g_+$.
\end{definition}
\begin{lemma}[\cite{schuchardt2024unified}]\label{lemma:group_insertion_removal}
    Consider a dataset space $\sX = \mathcal{P}(\sA)$ with underlying set $\sA$.
    and batch space $\sY = \mathcal{P}(\sA)$.
    Let $S : \sX \rightarrow \sY$ be Poisson sampling with rate $r$
    Let base mechanism $B : \sY \rightarrow \sR^D$ be a Gaussian mechanism $B(y) = f + Z$ with $Z \sim \mathcal{N}(0,\Delta_2 \sigma^2 \eye)$, where $\Delta_2$ is the $\ell_2$-sensitivity of underlying function $f : \sY \rightarrow \sR^D$
    under insertion/removal ($\simeq_\pm)$.
    Define subsampled mechanism $M = B \circ S$. 
    Further define mixture distributions 
    ${P}(k) = \mog(-1 \cdot \vmu_-, \vp_-, \sigma)$
    and
    ${Q}(k) = \mog(\vmu_+, \vp_+, \sigma)$
    with
    means $\vmu_- = \begin{bmatrix}
        0 & 1 &  2 & \cdots & k_-
    \end{bmatrix}^T$ 
    and
     $\vmu_+ = \begin{bmatrix}
        0 & 1 &  2 & \cdots & k_+
    \end{bmatrix}^T$,
    as well a weights
    weights $\vp_- \in [0,1]^{k_- + 1}$ with ${p_-}_i = \mathrm{Binomial}(i-1 \mid k_-, r)$.
    and
    $\vp_+ \in [0,1]^{k_+ + 1}$ with ${p_-}_i = \mathrm{Binomial}(i-1 \mid k_+, r)$.
    Then, for all $\alpha \geq 0$,
    \begin{equation}
        \sup_{x \simeq{k_+, k_-, \pm} x'} H_\alpha(M_x || M_{x'}) \leq  H_\alpha(P(k) || Q(k)).
    \end{equation}
\end{lemma}
From our derivations in~\cref{appendix:proofs_bottom_epoch_to_step,appendix:proofs_bottom_step_to_group}, the following result immediately follows via a seemingly na\"ive  reduction from group substitution to group insertion/removal:
\begin{lemma}\label{lemma:deterministic_top_level_poisson_upper}
    Consider an expected number of subsequences $\numinstances \in \mathbb{N}$ and let $r = \min\{1, \numinstances \mathbin{/} (L - L_F + 1)\}$ be the resultant sampling rate from~\cref{algorithm:dp-sgd-poisson-bottom-level}.
    Then 
    $P(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $Q(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r)$
    are a dominating pair of epoch $M$ under $\simeqevent{1}$, i.e., for all $\alpha \geq 0$: 
    \begin{equation}
        \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'}) \leq  H_\alpha(P(\numinstances) || Q(\numinstances)).
    \end{equation}
\end{lemma}
\begin{proof}
    Any group-substitution with group size $k$ is equivalent
    to a group insertion of size $k$, followed by a group removal of size $k$.
    Due to this observation and~\cref{lemma:adaptive_parallel_composition,lemma:proofs_bottom_step_to_group},
    we can instantiate~\cref{lemma:group_insertion_removal}
    with
    with sensitivity $\Delta_2$ equal to clipping constant $C$,
    and number of insertions/removals $k_+ = k_- = L_C + L_F$.
\end{proof}
Perhaps somewhat surprisingly, our next step will show that this na\"ive reduction does in fact yield a tight dominating pair for our epoch-level mechanism.

\subsubsection{Optimistic Lower Bounds}
As with sampling with replacement, we construct optimistic lower bounds by constructing a worst-case gradient function and a pair of datasets 
for each $\alpha \geq 0$.
\begin{lemma}\label{lemma:deterministic_top_level_poisson_lower}
    Consider an expected number of subsequences $\numinstances \in \mathbb{N}$ and let $r = \min\{1, \numinstances \mathbin{/} (L - L_F + 1)\}$ be the resultant sampling rate from~\cref{algorithm:dp-sgd-poisson-bottom-level}.
    Then 
    $P(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $Q(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r)$
    fulfill  for all $\alpha \geq 0$: 
    \begin{equation}
        \sup_{x \simeqevent{1} x'} H_\alpha(M_x || M_{x'}) \geq  H_\alpha(P(\numinstances) || Q(\numinstances)).
    \end{equation}
\end{lemma}
\begin{proof}
    Since our model $f_\theta$ is an arbitrary parametric function,
    we can assume that it is the anti-derivative of any desired gradient function $g : \sR^{L_C + L_F} \rightarrow \sR^D$.
    We choose the following function:
    \begin{equation*}
        g(a) = \begin{cases}
            C \cdot \ve_1 & \text{if } \exists l \in \{1,\dots,L_C + L_F\} : a_l = 1, \\
            -C \cdot \ve_1 & \text{else if } \exists l \in \{1,\dots,L_C + L_F\} : a_l = -1, \\
            0  & \text{otherwise.}
        \end{cases}
    \end{equation*}
    where $\ve_1$ is the indicator vector that is non-zero in its first component, 
    and $C$ is the clipping constant (i.e., our per-sequence gradients will never be affected by clipping).
     
    Next, consider sequences $x_1, x'_1 \in \sR^{L}$ with
    $x_1 = \begin{bmatrix}
        1 & 0 & \cdots & 0
    \end{bmatrix}$
    and 
    $x'_1 = \begin{bmatrix}
        -1 & 0 & \cdots & 0
    \end{bmatrix}$
    that differ in their first element.
    Note that the first element can appear in $L_C + L_F$ different subsequences in different positions because~\cref{algorithm:dp-sgd-wr-bottom-level} zero-pads the sequence before Poisson sampling.
    Further consider sequences $x_2,\dots,x_N$ such that $\forall m > n > 1 : x_m \neq x_n \land x_1 \neq x_n \neq x_1'$ (so that our dataset is a proper set, i.e., does not have duplicates)
    and $\forall n: 1 \notin x_n \land -1 \notin x_n$ (so that the gradients for all subsequences are always zero).
    Define datasets $x = \{x_1,x_2,\dots,x_N\}$ and $x' = \{x'_1,x_2,\dots,x_N\}$.
    
    By construction, $x \simeqevent{1} x'$.
    For every sequence in a top-level batch,
    we Poisson sample up to $L + L_F -1$ subsequences, there are $L - L_F + 1$ subsequences in total. There are exactly $L_C + L_F$ subsequences in $x_1$ that contain $1$ and $L_C + L_F$ subsequences in $x'_1$ that contain $-1$. 
    These $L_C + L_F$ subsequences will, respectively, add $+C$ and $-C$ to our summed gradient when sampled, while all other gradients are $0$. Thus, we exactly attain our desired bound in the first training step.
    The subsequent training steps operate on identical data and will thus not contribute to the hockey stick divergence attained by the epoch-level mechanism (same argument as in~\cref{lemma:adaptive_parallel_composition}).
\end{proof}
From the fact that our upper bound (\cref{lemma:deterministic_top_level_poisson_upper})
and our lower bound
 (\cref{lemma:deterministic_top_level_poisson_lower}) coincide, the tightness of our dominating pairs (\cref{theorem:deterministic_top_level_poisson}) immediately follows.

\textbf{Other contributions.}
The fact that we have been able to derive tight guarantees for what is essentially group substitution
using the sensitivity of the base mechanism w.r.t. insertion/removal showcases the benefit
of analyzing hybrid neighboring relations~\cite{balle2018privacy}.
Furthermore, it showcases that certain subsampling schemes are more compatible with certain neighboring relations~\cite{lebeda2024avoiding} --- even in the group privacy setting. 
\footnote{Trying to use the sensitivity w.r.t.\ substitution would induce much more complicated distance constraints after coupling.} 

\subsection{Optimal Number of Subsequences}\label{appendix:proofs_deterministic_top_monotonicity}
Finally, let us prove that $\numinstances = 1$ minimizes the privacy profile for all $\alpha$ under both sampling with replacement and Poisson sampling.
To this end, recall the following fact about stochastic dominance between Binomial distributions (see, e.g., Example 4.2.4.\ from~\cite{roch_mdp_2024}):
\begin{lemma}\label{lemma:binomial_dominance}
    Consider distributions $D_{n,p} = \mathrm{Binomial}(N, p)$
    and $D_{M,q} = \mathrm{Binomial}(m, q)$
    with $N \geq M$ and $p \geq q$. 
    Then $D_{N,p}$ stochastically dominates $D_{M,q}$, i.e,
    there exists a monotonic coupling $\Pi$ of $D_{N,p}$ and $D_{M,q}, $ with mass function $\pi : \sN_0^2 \rightarrow [0,1]$
    such that
    $\pi(k,i) > 0 \iff k \geq i$.
\end{lemma}
In other words, we construct $D_{N,p}$ from $D_{M,q}$ by shifting probability mass towards larger values.

In addition to stochastic dominance, we will make use of the following monotonicity result for hockey stick divergences between pairs of Gaussian mixtures (special case of Theorem O.6 from~\cite{schuchardt2024unified} for univariate mixtures):
\begin{lemma}\label{lemma:gaussian_mean_monotonicity}
    Consider any pair of Gaussian mixtures
    $P = \mog(-\vmu, \vp, \sigma)$
    and
    $Q = \mog(\vnu, \vq, \sigma)$
    with non-negative means $\vmu \in \sR_+^N, \vnu \in \sR_+^M$
    and weights
    $\vp \in [0,1]^N, \vq \in [0,1]^M$. 
    Then, the (sub-)derivative of $H_\alpha(P || Q)$ w.r.t.\ any $\evmu_i$ or $\nu_j$ is non-negative.
\end{lemma}
In other words: Moving mixture components farther from the origin (integrating over the non-negative (sub-)derivatives) only ever increases divergence.

Let us now begin with proving optimality for Poisson sampling:
\begin{lemma}\label{lemma:deterministic_top_level_monotonicity_poisson}
    Let $P^{*}(\numinstances)$, $Q^{*}(\numinstances)$ be a tight dominating pair of epoch $M$ for bottom-level Poisson sampling and $\numinstances \in \sN$ expected subsequences.
    Then $H_\alpha(P^{*}(\numinstances), Q^{*}(\numinstances))$ is minimized by $\numinstances = 1$ for all $\alpha \geq 0$.
\end{lemma}
\begin{proof}
    By definition, all tight dominating pairs exactly match the privacy profile of mechanism $M$, i.e., attain identical hockey stick divergence for all $\alpha \geq 0$.
    We can thus focus our discussion on the tight dominating pairs from~\cref{theorem:deterministic_top_level_poisson}, which are mixtures of Gaussians.

    Consider an arbitrary $\numinstances \geq 1$ and 
    let $P(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $Q(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r(\numinstances))$
    and sampling rate $r(\numinstances) = \min \{ 1, \numinstances \mathbin{/} (L - L_F + 1)\}$.

    Since $r(\numinstances)$ is increasing in $\numinstances$, we know from~\cref{lemma:binomial_dominance} that
    $\mathrm{Binomial}(L_C + L_F, r(\numinstances))$ stochastically dominates
    $\mathrm{Binomial}(L_C + L_F, r(1))$.
    We can thus use a monotonic coupling of the two weight distributions with coupling mass function $\pi : \sN_0^2 \rightarrow [0,1]$ to restate $P(1),Q(1)$ as follows:
    \begin{align*}
        & P(1) =  \sum_{i=0}^{L_C + L_F} \sum_{k=0}^{L_C + L_F} \pi(k,i) \cdot \mathcal{N}(-1 \cdot i, \sigma^2 \eye),
        & Q(1) =  \sum_{i=0}^{L_C + L_F} \sum_{k=0}^{L_C + L_F} \pi(k,i) \cdot \mathcal{N}(i, \sigma^2 \eye),
    \end{align*}
    i.e., we split up the $i$th (with zero-based indexing) mixture component into $L_C + L_F - i + 1$ mixture components with identical means.
    Similarly, we can restate $P(\numinstances),Q(\numinstances)$ as 
    \begin{align*}
        & P(\numinstances) =  \sum_{i=0}^{L_C + L_F} \sum_{k=0}^{L_C + L_F} \pi(k,i) \cdot \mathcal{N}(-1 \cdot k, \sigma^2 \eye),
        & Q(\numinstances) =  \sum_{i=0}^{L_C + L_F} \sum_{k=0}^{L_C + L_F} \pi(k,i) \cdot \mathcal{N}(k, \sigma^2 \eye).
    \end{align*}
    Since $\pi(k,i) \geq 0 \iff k \geq i$ (\cref{lemma:binomial_dominance}), and divergence increases when increasing the norm of the mixture means (\cref{lemma:gaussian_mean_monotonicity}),
    we know that $H_\alpha(P(\numinstances) || Q(\numinstances)) \geq H_\alpha(P(1) || Q(1))$.
\end{proof}
Next, let us consider sampling with replacement. The proof is slightly more involved, since we only have optimistic lower bounds for $\numinstances > 1$, as opposed to an exact characterization of the privacy profile.
\begin{lemma}
    Let $P^{*}(\numinstances)$, $Q^{*}(\numinstances)$ be a tight dominating pair of epoch $M$ for bottom-level sampling with replacement and $\numinstances \in \sN$ expected subsequences.
    Then $H_\alpha(P^{*}(\numinstances), Q^{*}(\numinstances))$ is minimized by $\numinstances = 1$ for all $\alpha \geq 0$.
\end{lemma}
\begin{proof}
    \textbf{Case 1 ($\alpha \geq 1$):}
    Consider any tight dominating pair $P^{*}(\numinstances)$, $Q^{*}(\numinstances)$ for $\numinstances  \geq 1$,
    as well as a tight dominating pair $P^{*}(1)$, $Q^{*}(1)$ for $\numinstances = 1$.
    We know from~\cref{theorem:deterministic_top_level_wr_optimistic} and the definition of tight dominating pairs that
    \begin{equation*}
        H_\alpha(P^{*}(\numinstances) || Q^{*}(\numinstances)) \geq H_\alpha(\overline{P}(\numinstances) || N(0, \sigma)),
    \end{equation*}
    while we know from~\cref{theorem:deterministic_top_level_wr} that 
    \begin{equation*}
        H_\alpha(\overline{P}(1) || \mathcal{N}(0,\sigma)) = H_\alpha(P^{*}(1) || Q^{*}(1)),
    \end{equation*}
    with $\overline{P}(\numinstances) = \mog(\vmu, \vp, \sigma)$, 
    $\vmu \in \sN_0^{\numinstances +1}$ and weights $\vmu \in \sN_0^{\numinstances +1}$
    with $\evmu_i = 2 (i-1)$
    and $\evp_i = \mathrm{Binomial}(i \mid \numinstances, r)$,
    where $r$ is constant in $\numinstances$.

    We can thus prove $H_\alpha(P^{*}(\numinstances) || Q^{*}(\numinstances)) \geq H_\alpha(P^{*}(1) || Q^{*}(1))$
    by proving $H_\alpha(\overline{P}(\numinstances) || \mathcal{N}(0,\sigma)) \geq H_\alpha(\overline{P}(1) || \mathcal{N}(0,\sigma))$.
    
    To this end, recall from~\cref{lemma:binomial_dominance} that $\mathrm{Binomial}(\numinstances, r)$ stochastically dominates 
    $\mathrm{Binomial}(1, r)$. We can thus apply exactly the same monotonic-coupling-based proof as with~\cref{lemma:deterministic_top_level_monotonicity_poisson}.

    \textbf{Case 2 ($0 \leq \alpha < 1)$:}
    This case is fully analogous, except that we need to use monotonic couplings to prove 
    $H_\alpha(\overline{P}(\mathcal{N}(0,\sigma) || \numinstances)) \geq H_\alpha(\mathcal{N}(0,\sigma) || \overline{P}(1))$
\end{proof}