\section{Generalizations}\label{appendix:generalizations}
For the sake of exposition, and to spare our readers from even further complicating the already involved notation in some of our later proofs, 
we made certain simplifying assumptions about 
the shape of the time series,
and focused on analyzing $1$-event and $(1,v)$-event level privacy.

In the following, we discuss how these restrictions can  (except for one case involving  amplification-by-augmentation with $\sigma_C \neq \sigma_F$) be easily lifted.

\subsection{Shape of Time Series}

\textbf{Minimum Length.} The first simplifying assumption we made was that $L - L_F + 1 \geq L_C + L_F$, which means that the first element $x_n[1]$ of a time series $x_n$ appears (after zero-padding with length $L_C$) in $L_C + L_F$ different time series at different positions.
If $L - L_F + 1 < L_C + L_F$, then all context windows will always contain some of the padding elements. Thus, the maximum  number of subsequences an element can contribute to is
$\max\{0, \min\{ L_C + L_F, L - L_F + 1\}\}$.
Since all our proofs are only dependent on number of subsequences that contain a specific sensitive element, and the fact that there are no duplicate appearances in a subsequence, 
one can replace $L_C + L_F$ with $\max\{0, \min\{ L_C + L_F, L - L_F + 1\}\}$ in all our guarantees to cover this edge case.

\textbf{Variable length.}
Using the same stochastic dominance argument as in our proof of~\cref{appendix:proofs_deterministic_top_monotonicity}, one can show that
 all our upper- and lower bounds for bottom-level sampling with replacement are monotonically increasing in $\frac{\max\{0, \min\{ L_C + L_F, L - L_F + 1\}\}}{L - L_F + 1}$. 
Thus, given a dataset composed of variable-length sequences, one can evaluate our bounds using whatever length maximizes the above two functions. The tight bounds will remain tight, since we have to make the worst-case assumption that exactly this optimal-length sequence is changed under our neighboring relation.

\textbf{Dimensionality.}
None of our proofs use the fact that our dataset space is
$\sX = \mathcal{P}(\sR^L)$ as opposed to $\sX = \mathcal{P}(\sR^{L \times D_\mathrm{in}})$ --- except for our amplification-by-augmentation proof from~\cref{lemma:reduction_via_maximal_coupling} for $(1,v)$-event privacy.
If we interpret $v$ as implying that $\forall n, \forall t: ||x_n[t] - x_n[t]||_2 \leq v$, then the proof are identical.
If we interpret $v$ as implying that $\forall n, \forall t, \forall d: |x_n[t, d] - x_n[t,d]|$,
then the maximum total variation distance between two subsequences that differ in the same element increases to $\mathrm{TVD}\left(\mathcal{N}(0, \sigma), \mathcal{N}(\sqrt{D_\mathrm{in}}, \sigma)\right)$ and we need to adjust the bound accordingly.
Thus, our methods can in principle also be applied to highly-dimensional time series such as videos.



\subsection{Neighboring Relations}
Finally, let us discuss how to generalize our bounds to $(w,v)$-event and $(w,v)$-user privacy with $v \in \sR_+ \cup \{\infty\}$. 

\textbf{Bottom- and Top-Level Subsampling}. Our proofs for bottom- and top-level sampling  only depend on the number
of subsequences that contain at least one element of any specific individual
(see, e.g.,~\cref{{appendix:proofs_bottom_step_to_group}} where we abstract our analysis $k$-group-substitutions).
Under the $(w,v)$-event neighboring relation, all elements are adjacent and thus this number is given by $\max\{0, \min\{ L_C + L_F + w - 1, L - L_F + 1\}\}$.
Under the $(w,v)$-user neighboring relation, all elements can be placed arbitrarily far from each other. One can thus place them such that every subsequence only ever contains a single element, i.e.,
$\max\{0, \min\{ w \cdot (L_C + L_F), L - L_F + 1\}\}$.

\textbf{Amplification by Data Augmentation.}
In the case of $\sigma_C = \sigma_F$ (\cref{theorem:amplification_by_data_augmentation_wor_wr}),
our guarantee is only dependent on the maximum magnitude of change
between two time series that differ in the same indices.
We can thus adjust our bound by
using $\mathrm{TVD}\left(\mathcal{N}(0, \sigma), \mathcal{N}(\sqrt{w}, \sigma)\right)$.
Only our guarantee for $\sigma_c \neq \sigma_F$ cannot be easily generalized, because our proof of~\cref{lemma:bilevel_reduction_to_single_gradient} explicitly makes a case distinction based on whether an element appears in the context window or the forecast window.
Future work would need to generalize our proof make a fine-grained case distinction over the number of elements that appear in the context window and in the forecast window.