\section{Proofs from Section 4.3 (Context--Forecast Split)}\label{appendix:proofs_context_forecast_split}
For this seciton, we focus exclusively on top-level sampling without replacement and bottom-level sampling with replacement.
Our goal is to prove the following statement for $(1,v)$-event-level privacy, where a single element can change its value by at most $v$:
\begin{theorem}\label{theorem:data_augmentation_general}
    Consider top-level sampling without replacement and bottom-level sampling with replacement with $\numinstances = 1$, 
    batch size $\batchsize$, as well as context and forecast standard deviations $\sigma_C, \sigma_F \in \sR_+$ 
    (see~\cref{eq:gradient_noise}). 
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and  
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.
    Further define forecast-to-context ratio $\phi = \frac{L_F}{L_C + L_F}$. 
    Define
    $\hat{P}(1) = \mog(\vmu, \tilde{\vp}, \sigma)$ with
    means
     $\hat{\vmu} = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and weights
     $\tilde{\vp} \in [0,1]^2$
     with $\evp_1 = 1 - \evp_2$ and
     \begin{equation*}
         \evp_2 = \rho \cdot r \cdot \phi \cdot \mathrm{TVD}\left(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F)\right)
         +  \rho \cdot r \cdot (1 - \phi) \cdot \mathrm{TVD}\left(\mathcal{N}(0,\sigma_C), \mathcal{N}(1,\sigma_C)\right)
     \end{equation*}
    Then, the augmented per-step privacy profile $\hat{H}(\alpha) = \sup_{x \simeqevent{1,v} x'} H_\alpha(\hat{M}_x || \hat{M}_{x'})$ fulfills 
    \begin{equation*}
        \hat{H}(\alpha) \leq 
        \begin{cases}
            H_\alpha(\hat{P}(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || \hat{P}(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{theorem}
The following special case from~\cref{section:context_forecast_structure} immediately follows from setting $\sigma_C = \sigma_F$:
\amplificationbyaugmentationworwr*

To prove these results, let us
\begin{enumerate}
    \item Bound the privacy profile of $\hat{M}$ via constructing a conditional coupling between mixture decompositions of maximal couplings, leading to yet another divergence maximization problem involving multivariate Gaussian mixtures,
    \item and solve the resultant optimization problem using joint quasi-convexity of the hockey stick divergence.
\end{enumerate}

\subsection{Bound via Conditional Coupling and Maximal Couplings}
For this section, we will use the following properties of maximal couplings, taken from~\cite{balle2018privacy} and Section 2.5 of~\cite{den2012probability}:
\begin{proposition}
    Consider arbitrary distributions $P, Q$.
    There exists a coupling $\Pi^*$ of $P, Q$, referred to as \emph{maximal coupling}, such that
    \begin{enumerate}
        \item $\Pi^*$ is an optimum of $\sup_{\Pi \in \Phi(P,Q)} \Pr{(X,Y) \sim \Pi}[X = Y]$, where $\Phi(P, Q)$ is the space of all couplings of $P, Q$,
        \item $\Pi^*$ has marginals that decompose as $P = (1-\tau) P^{(0)} + \tau P^{(1)}$ and $Q = (1-\tau) P^{(0)} + \tau Q^{(1)}$
        with $\tau = \mathrm{TVD}(P, Q) = H_1(P,Q)$.
    \end{enumerate}
\end{proposition}
As such, maximal coupling exactly correspond to our intuition of trying to determine the probability
that we sample the same context and ground-truth forecast when a single sequence element changes its value  by $v$.
\begin{lemma}\label{lemma:reduction_via_maximal_coupling}
    Consider top-level sampling without replacement and bottom-level sampling with replacement with $\numinstances = 1$, 
    batch size $\batchsize$, as well as context and forecast standard deviations $\sigma_C, \sigma_F \in \sR_+$ 
    (see~\cref{eq:gradient_noise}). 
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and  
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.
    Further define forecast-to-context ratio $\phi = \frac{L_F}{L_C + L_F}$. 
    Then, the augmented per-step privacy profile $\hat{H}(\alpha) = \sup_{x \simeqevent{1,v} x'} H_\alpha(\hat{M}_x || \hat{M}_{x'})$ fulfills 
    \begin{equation}\label{eq:data_augmentation_objective}
        \hat{H}(\alpha) \leq \max_{P, Q \in \Omega} H_\alpha(P || Q)
    \end{equation}
    where $\Omega$ is set of all pairs of multivariate Gaussian mixtures $(P,Q)$
    with 
    $P = \sum_{i=1}^{6} w_i \cdot \mathcal{N}(\vmu^{(1)}_{i}, \sigma^2 \eye)$
    and
    $Q = \sum_{j=1}^{6} w_j \cdot \mathcal{N}(\vmu^{(2)}_{j}, \sigma^2 \eye)$
    satisfying
    \begin{align}
        \begin{split}\label{eq:data_augmentation_constraints}
        & ||\vmu^{(1)}_i - \vmu^{(1)}_j||_2 \leq 2    \qquad \forall i,j \in \{1,\dots,6\} \\
        & ||\vmu^{(2)}_i - \vmu^{(2)}_j||_2 \leq 2    \qquad  \forall i,j \in \{1,\dots,6\}\\
        & ||\vmu^{(1)}_i - \vmu^{(2)}_j||_2 \leq 2   \qquad \forall i,j \in \{1,\dots,6\} \\
        & \vmu^{(1)}_i = \vmu^{(2)}_i  \qquad \qquad  \quad \ \ \forall i \in \{1,\dots,4\}, 
        \end{split}
    \end{align}
    with $\forall i  : \vmu^{(1)}_i \in \sR^D$ and
    \begin{align*}
        w_1 & = (1 - \rho) \\
        w_2 & = \rho \cdot (1 - r) \\
        w_3 & = \rho \cdot r \cdot \phi \cdot (1 - \mathrm{TVD}\left(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F)\right)) \\
        w_4 & = \rho \cdot r \cdot (1 - \phi) \cdot (1 - \mathrm{TVD}\left(\mathcal{N}(0,\sigma_C), \mathcal{N}(1,\sigma_C)\right)) \\
        w_5 & = \rho \cdot r \cdot \phi \cdot \mathrm{TVD}\left(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F)\right) \\
        w_6 & = \rho \cdot r \cdot (1 - \phi) \cdot \mathrm{TVD}\left(\mathcal{N}(0,\sigma_C), \mathcal{N}(1,\sigma_C)\right).
    \end{align*}
\end{lemma}
\begin{proof}
    Let $\check{B} : \sR^{L_C+L_F} \rightarrow \sR^D$
    be the mechanism that adds isotropic Gaussian noise  to a pair of context and forecast windows,
    computes the resultant gradient, and adds isotropic Gaussian noise to the elements of the output gradient.
    Note that this mechanism is just another subsampled mechanism, where the subsampling distribution happens to be continuous.
    We can thus apply the usual approach based on joint couplings of conditional subsampling distributions.
    To this end, let us decompose the mechanism via $\check{B} = \underline{B} \circ \underline{S}$,
    where
    \begin{equation*}
        \underline{S}(a) =
        \begin{bmatrix}
            (a[1:L_C] + Z_C)^T &  (a[L_C + 1 : ] + Z_F)^T
        \end{bmatrix}^T
    \end{equation*}
    with $Z_F \sim \mathcal{N}(\vzero, \sigma_F ^2 \cdot v^2 \cdot  \eye)$, $Z_F \sim \mathcal{N}(\vzero, \sigma_F ^2 \cdot v^2 \cdot  \eye)$.
    
    We know from our derivations in~\cref{appendix:proofs_bilevel} and specifically~\cref{lemma:bilevel_reduction_to_single_gradient} that
    $\hat{H}(\alpha) \leq H_\alpha(P || Q)$
    \begin{align*}
        P = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot   \check{B}_{a'_F} + \rho \cdot r \cdot (1 - \phi) B_{a'_C},
        \\
        Q = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot  \check{B}_{a''_F} + \rho \cdot r \cdot (1 - \phi) B_{a''_C},
    \end{align*}
    for some worst-case tuple of subsequences $a, a', a'_F, a'_C \in \sR^{L_C + L_F}$ where 
    $\{a'_F\} \simeqevent{1,v} \{a''_F\}$,
    $\{a'_C\} \simeqevent{1,v} \{a''_C\}$,
    subsequences $a_F, a'_F$ only differ in their last $L_F$ elements (forecast window),
    subsequences $a_C, a'_C$ only differ in their first $L_C$ elements (context window), 
    and the first two mixture components are identical between the two mixture distributions.

    We know that $||a'_F - a''_F|| \leq v$ and that they only differ in one element in the forecast window.
    Thus, $\mathrm{TVD}(\underline{S}_{a'_F} || \underline{S}_{a''_F}) \leq \mathrm{TVD}(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F))$. 
    For brevity, let us define $\tau_F = \mathrm{TVD}(\mathcal{N}(0,\sigma_F), \mathcal{N}(1,\sigma_F))$. 
    Using  maximal couplings, we can restate $\check{B}_{a'_F}$ and $\check{B}_{a''_F}$ as sums of two mixtures where the first summand is identical, i.e., 
    \begin{align*}
        \check{B}_{a'_F} & =
        (1 - \tau_F)
        \cdot  \int_{\sR^{L_C + L_F}} \underline{B}(z) \  \dd  \ \underline{S}_{a'_F}^{(0)}(z)
        +
        \tau_F
        \cdot  \int_{\sR^{L_C + L_F}} \underline{B}(z)  \ \dd \  \underline{S}_{a'_F}^{(1)}(z),
        \\
        \check{B}_{a''_F} &=
        (1 - \tau_F)
        \cdot  \int_{\sR^{L_C + L_F}} \underline{B}(z)  \ \dd \  \underline{S}_{a'_F}^{(0)}(z)
        +
        \tau_F
        \cdot  \int_{\sR^{L_C + L_F}} \underline{B}(z)  \ \dd  \ \underline{S}_{a''_F}^{(1)}(z).
    \end{align*}
    Similarly, we know that $||a'_C - a''_C|| \leq v$ and that they only differ in one element in the context window.
    Thus, $\mathrm{TVD}(\underline{S}_{a'_C} || \underline{S}_{a''_C}) \leq \tau_C$
    with $\tau_C = \mathrm{TVD}(\mathcal{N}(0,\sigma_C), \mathcal{N}(1,\sigma_C))$. 
    Using  maximal couplings, we can thus also restate $\check{B}_{a'_C}$ and $\check{B}_{a''_C}$ as sums of two mixtures where the first summand is identical, i.e., 
    \begin{align*}
        \check{B}_{a'_C} & =
        (1 - \tau_C)
        \cdot  \int_{\sR^{L_C + L_C}} \underline{B}(z)  \ \dd  \ \underline{S}_{a'_C}^{(0)}(z)
        +
        \tau_C
        \cdot  \int_{\sR^{L_C + L_C}} \underline{B}(z) \  \dd  \ \underline{S}_{a'_C}^{(1)}(z),
        \\
        \check{B}_{a''_C} &=
        (1 - \tau_C)
        \cdot  \int_{\sR^{L_C + L_C}} \underline{B}(z) \  \dd \ \underline{S}_{a'_C}^{(0)}(z)
        +
        \tau_C
        \cdot  \int_{\sR^{L_C + L_C}} \underline{B}(z) \  \dd \  \underline{S}_{a''_C}^{(1)}(z).
    \end{align*}

    Using the usual coupling toolkit, we can now define a continuous coupling between the six subsampling distributions
    $\underline{S}_{a}, \underline{S}_{a'}, \underline{S}_{a'_F}^{(0)}, \underline{S}_{a'_C}^{(0)}, \underline{S}_{a'_F}^{(1)}, \underline{S}_{a'_C}^{(1)}$ from the first mixture distribution
    and the six subsampling distributions
    $\underline{S}_{a}, \underline{S}_{a'}, \underline{S}_{a'_F}^{(0)}, \underline{S}_{a'_C}^{(0)}, \underline{S}_{a''_F}^{(1)}, \underline{S}_{a''_C}^{(1)}$.
    Since the first to fourth  mixture component are identical,
    we can trivially construct a coupling $\Gamma$ such that the first to fourth element
    are identical for all pairs of tuples of subsequences in the support of $\Gamma$.

    We can then rewrite all subsampling distributions as marginals of the coupling and move the coupling outside the hockey stick divergence using joint convexity (i.e., the approach from~\cite{schuchardt2024unified}).
    Finally, we can conclude our proof by recalling that the gradient function $g : \sR^{L_C + L_F} \rightarrow \sR^D$ underlying $\underline{B}(z)$  is clipped to a norm of $C$ and yields identical results for identical inputs.
\end{proof}

\subsection{Worst-Case Mixture Components}
Let us begin by solving the optimization problem in~\cref{lemma:reduction_via_maximal_coupling} for $\alpha \geq 1$.
\begin{lemma}
    For $\alpha \geq 1$, an optimal solution to the optimization problem in~\cref{lemma:reduction_via_maximal_coupling}
    is given by $\vmu^{(1)}_5 = \vmu^{(1)}_6 = 2 \ve_1$,
    $\forall 1 \leq i \leq 4 : \vmu^{(1)}_i = \vzero $,
    and $\forall 1 \leq i \leq 4 : \vmu^{(2)}_i = \vzero $,
    where $\ve_1$ is the first canonical unit vector.
\end{lemma}
\begin{proof}
    Consider any feasible solution to our optimization problem
    $P = \sum_{i=1}^{6} w_i \cdot \mathcal{N}(\vmu^{(1)}_{i}, \sigma^2 \eye)$
    and
    $Q = \sum_{j=1}^{6} w_j \cdot \mathcal{N}(\vmu^{(2)}_{j}, \sigma^2 \eye)$ that fulfills
    \begin{align}
        \begin{split}\label{eq:data_augmentation_constraints_2}
        & ||\vmu^{(1)}_i - \vmu^{(1)}_j||_2 \leq 2    \qquad \forall i,j \in \{1,\dots,6\} \\
        & ||\vmu^{(2)}_i - \vmu^{(2)}_j||_2 \leq 2    \qquad  \forall i,j \in \{1,\dots,6\}\\
        & ||\vmu^{(1)}_i - \vmu^{(2)}_j||_2 \leq 2   \qquad \forall i,j \in \{1,\dots,6\} \\
        & \vmu^{(1)}_i = \vmu^{(2)}_i  \qquad \qquad  \quad \ \ \forall i \in \{1,\dots,4\}, 
        \end{split}
    \end{align}
    with $\forall i,j  : \vmu^{(1)}_i, \vmu^{(2)}_j \in \sR^D$.

    Since $P$ and $Q$ are identical in their first four components, we have via the advanced joint convexity property (\cref{lemma:advanced_joint_convexity}):
    \begin{equation*}
        H_\alpha(P || Q) = (w_5 + w_6) H_{\alpha'}(P' || Q')
    \end{equation*}
    with some $\alpha' \geq \alpha$ and some mixture weights $w'^{(1)}_5, w'^{(1)}_6, w'^{(2)}_1,\dots,w'^{(2)}_6$
    and 
    \begin{align*}
       & P' = w'^{(1)}_5 \mathcal{N}(\vmu^{(1)}_5, \sigma^2 \eye)  +  w'^{(1)}_6 \mathcal{N}(\vmu^{(1)}_6, \sigma^2 \eye),\\
       & Q' = \sum_{i=1}^6 w'^{(2)}_i \mathcal{N}(\vmu^{(2)}_i, \sigma^2 \eye),
    \end{align*}

    Since hockey stick divergences are jointly convex, they are also jointly quasi-convex and thus
    \begin{align*}
    &
    (w_5 + w_6) \cdot 
    H_{\alpha'}(P' || Q')
    \\
    \leq 
    &
    (w_5 + w_6)
    \cdot 
    \max_{i \in \{5,6\}}
    \max_{j \in \{1,\dots,6\}}
    H_{\alpha'}(\mathcal{N}(\vmu^{(1)}_i, \sigma^2 \eye) || \mathcal{N}(\vmu^{(2)}_i, \sigma^2 \eye))
    \\
    \leq
    &
    (w_5 + w_6)
    \cdot 
    H_{\alpha'}(\mathcal{N}(2 \ve_1, \sigma^2 \eye) || \mathcal{N}(\vzero, \sigma^2 \eye)
    \end{align*}
    The last inequality follows from our distance constraints and the fact that the hockey stick divergence between two Gaussians is monotonically increasing with the distance of their means (see~\cref{lemma:worst_case_insertion_removal_mixture} for formal statement).

    The result finally follows from applying advanced joint convexity in reverse order, i.e.,
    \begin{align*}
    &
    (w_5 + w_6)
    \cdot 
    H_{\alpha'}(\mathcal{N}(2 \ve_1, \sigma^2 \eye) || \mathcal{N}(\vzero, \sigma^2 \eye)
    \\
    =
    &
    (w_5 + w_6)
    \cdot 
    H_{\alpha'}\left( \sum_{i=5}^6 w'^{(1)}_i \mathcal{N}( 2 \ve_1, \sigma^2 \eye) || \sum_{j=1}^6 w'^{(2)}_j \mathcal{N}(\vzero, \sigma^2 \eye\right)
    \\
    =
    &
    H_{\alpha}\left( \sum_{i=1}^4 w^{(1)}_i \mathcal{N}(\vzero, \sigma^2 \eye) +  \sum_{i=5}^6 w'^{(1)}_i \mathcal{N}( 2 \ve_1, \sigma^2 \eye) || \sum_{j=1}^6 w^{(2)}_j \mathcal{N}(\vzero, \sigma^2 \eye) \right).
    \end{align*}
    

    \iffalse
    Due to translation and rotation invariance of hockey stick divergences of Gaussian mixtures,
    we can (while retaining equal divergence) transform any feasible solution such that 
    that $\vmu^{(1)}_5 = - k \cdot \ve_1$ and $ \vmu^{(1)}_6 = k \cdot \ve_1$,
    where $\ve_1$ is the first canonical unit vector and $k \in [0,1]$.

    Let $\zeta_+ : \sR^D \rightarrow \sR^D$ and $\zeta_- : \sR^D \rightarrow \sR^D$
    be mirror reflections from~\cref{lemma:reflection_transform} with normal vector $\vn = \ve_1$.
    As per~\cref{lemma:reflection_transform}, the following pair of mixtures has greater or equal divergence: 
    \begin{align*}
       & P' = w'^{(1)}_5 \mathcal{N}(\zeta_+(\vmu^{(1)})_5, \sigma^2 \eye)  +  w'^{(1)}_6 \mathcal{N}(\zeta_+(\vmu^{(1)})_6, \sigma^2 \eye)\\
       & Q' = \sum_{i=1}^6 w'^{(2)}_i \mathcal{N}(\zeta_-(\vmu^{(2)}_i), \sigma^2 \eye)
    \end{align*}
    Furthermore, we can verify that the new pair of means still fulfills all pairwise distance constraints from
    ~\cref{eq:data_augmentation_constraints_2}:
    Consider arbitrary $\vmu^{(2)}_i, \vmu^{(2)}_i$.
    Then, $||\zeta_-(\vmu^{(2)}_i) - \zeta_-(\vmu^{(2)}_j)||_2 \leq ||\vmu^{(2)}_i - \vmu^{(2)}_j||_2$, 
    since they were either in the same half space before reflection (no change in distance),
    or were previously in opposite half spaces and are now in the same half space.

    Furthermore, we have by construction that both means of $P'$ are collapsed into a single point, i.e., 
    $\zeta_+(\vmu^{(1)}_5) = \zeta_+(\vmu^{(2)}_6)$.

    Finally, let us consider intra-distribution constraints.
    Consider an arbitrary $\vmu^{(2)}_i$.
    If it was in the negative half space, then its position relative to $\vmu^{(1)}_6$ did not change and thus
    $2 \geq ||\vmu^{(2)}_i - \vmu^{(1)}_6||_2 = ||\zeta_-(\vmu^{(2)}_i) - \zeta_+(\vmu^{(1)}_6)||_2 =
    ||\zeta_-(\vmu^{(2)}_i) - \zeta_+(\vmu^{(1)}_5)||_2$, where the last equality follows from both components in $P'$ being collapsed into the same point.
    If it was in the positive half space, then its position relative to $\vmu^{(2)}_5$ did not change, because both were reflected into opposite directions along the same hyperplane. Thus
    $2 \geq ||\vmu^{(2)}_i - \vmu^{(1)}_5||_2 = ||\zeta_-(\vmu^{(2)}_i) - \zeta_+(\vmu^{(1)}_5)||_2 =
    ||\zeta_-(\vmu^{(2)}_i) - \zeta_+(\vmu^{(1)}_6)||_2$.
    \fi
\end{proof}
Our main result~\cref{theorem:data_augmentation_general} then follows immediately from marginalization
and the following lemma (\cite{zhu2022optimal}):
\dominatingpairalphasymmetry*