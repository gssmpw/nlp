\subsection{Trade-Offs in Structured Subsampling}\label{appendix:extra_experiments_eval_pld}

\subsubsection{Number of Subsequences in Bi-Level Sampling}\label{appendix:extra_experiments_eval_pld_number_of_subsequences}
In the following, we repeat our experiment from~\cref{fig:monotonicity_daily_main_container},
where we wanted to determine whether we should use small $\numinstances$ (many top-level sequences, few bottom-level subsequences)
or large $\numinstances$ (many top-level sequences, few bottom-level subsequences)
for some given batch size $\batchsize$.

From~\cref{theorem:wor_top_level_wr} and~\cref{theorem:wor_top_wr_bottom_upper}
we know that our tight upper bound and our optimistic lower bounds only
depend on $r = \frac{L_C + L_F}{L - L_F + 1}$ and $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.
Thus (up to modulo division), the parameter space is fully characterized
by $r$ and batch-to-dataset size ratio $\frac{\batchsize}{N}$.

In~\cref{fig:monotonicity_daily_appendix_container_1,fig:monotonicity_daily_appendix_container_2,fig:monotonicity_daily_appendix_container_3}, we thus keep $N = 320$ and vary these two ratios between
$0.5$ (little amplification) and $0.1$ (more amplification).
In all cases, we observe that $\numinstances = 1$ improves 
its privacy relative to other $\numinstances > 1$ after $100$ training steps. 
Again, this justifies our choice of $\numinstances = 1$ in training models.


\begin{figure*}[!ht]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_4_32_1.pdf}
        \caption{Step $1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_4_32_100.pdf}
        \caption{Step $100$}
    \end{subfigure}\caption{Top-level WOR and bottom-level WR sampling under varying number of subsequences.
    Little bottom-level amplification ($r = 0.5$) and more top-level amplification
    ($\batchsize \mathbin{/} N = 0.1$).
    }
    \label{fig:monotonicity_daily_appendix_container_1}
\vskip -0.2in
\end{figure*}
\begin{figure*}[!ht]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_160_1.pdf}
        \caption{Step $1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_160_100.pdf}
        \caption{Step $100$}
    \end{subfigure}\caption{Top-level WOR and bottom-level WR sampling under varying number of subsequences.
    More bottom-level amplification ($r = 0.1$) and less top-level amplification
    ($\batchsize \mathbin{/} N = 0.5$).}
    \label{fig:monotonicity_daily_appendix_container_2}
\vskip -0.2in
\end{figure*}
\begin{figure*}[!ht]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_32_1.pdf}
        \caption{Step $1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_monotonicity_composed/daily_20_32_100.pdf}
        \caption{Step $100$}
    \end{subfigure}\caption{Top-level WOR and bottom-level WR sampling under varying number of subsequences.
    Both significant bottom-level amplification ($r = 0.1$) and top-level amplification
    ($\batchsize \mathbin{/} N = 0.1$).}
    \label{fig:monotonicity_daily_appendix_container_3}
\vskip -0.2in
\end{figure*}

\clearpage

\subsubsection{Step- vs Epoch-Level Accounting}
In this section, we repeat our experiment from~\cref{fig:deterministic_vs_random_top_level_daily_main}
to demonstrate  the benefit of top-level sampling sequences (\cref{theorem:wor_top_level_wr}) instead of deterministically iterating over them (\cref{theorem:deterministic_top_level_wr}), even though we risk privacy leakage at every training step.

Like in~\cref{appendix:extra_experiments_eval_pld_number_of_subsequences},
we observe that the deterministic-top-level guarantee is only dependent
on 
$r = \frac{L_C + L_F}{L - L_F + 1}$,
and that the WOR-top-level guarantee is only dependent on $r$ and 
$\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.

As before, we thus keep dataset size $N=320$ and vary
$r$ and batch-to-dataset size ratio $\frac{\batchsize}{N}$ between $0.5$ and $0.1$ (see~\cref{fig:deterministic_vs_random_top_level_daily_appendix}).
In all cases, sampling without replacement offers stronger privacy after $1$ step, $1$ epoch, and $10$ epochs.


\begin{figure*}[h!]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_vs_random_top_level/daily_4_160.pdf}
        \caption{$r = 0.5$ and $\batchsize \mathbin{/} N = 0.5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_vs_random_top_level/daily_4_32.pdf}
        \caption{$r = 0.5$ and $\batchsize \mathbin{/} N = 0.1$}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_vs_random_top_level/daily_20_160.pdf}
        \caption{$r = 0.1$ and $\batchsize \mathbin{/} N = 0.5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_vs_random_top_level/daily_20_32.pdf}
        \caption{$r = 0.1$ and $\batchsize \mathbin{/} N = 0.1$
        }
    \end{subfigure}
    \caption{Top-level deterministic iteration (\cref{theorem:deterministic_top_level_wr}) vs top-level WOR sampling (\cref{theorem:wor_top_level_wr}) for $\numinstances=1$.
        We vary $r = (L_C + L_F) \mathbin{/} (L - L_F + 1)$
        and $\batchsize \mathbin{/} N$,
        with smaller values corresponding to more bottom- and top-level amplification, respectively.}
    \label{fig:deterministic_vs_random_top_level_daily_appendix}
\vskip -0.2in
\end{figure*}


\clearpage

\subsubsection{Amplification by Label Perturbation}
In this section, we repeat our experiment from~\cref{fig:label_noise_daily_main},
where we consider top-level WOR and bottom-level WOR sampling, and
vary label noise standard deviation $\sigma_F \in \sR_+$ to illustrate how test-time data augmentations (i.e.\ continuous-valued subsampling) can amplify privacy (\cref{theorem:data_augmentation_general}) in self-supervised training of sequence models.

Just like in~\cref{appendix:extra_experiments_eval_pld},
we observe that the privacy guarantee, for fixed forecast-to-context ratio $\frac{L_F}{L_C + L_F}$, is only dependent 
on 
$r = \frac{L_C + L_F}{L - L_F + 1}$,
 and 
$\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$.

As before, we thus keep dataset size $N=320$ and vary
$r$ and batch-to-dataset size ratio $\frac{\batchsize}{N}$ between $0.5$ and $0.1$ (see~\cref{fig:label_noise_daily_appendix}).
In all cases, letting $\sigma_F \to \infty$ is equivalent to setting forecast length $L_F$ to $0$. 

\begin{figure*}[ht!]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_label_noise/daily_6_160.pdf}
        \caption{$r = 0.5$ and $\batchsize \mathbin{/} N = 0.5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_label_noise/daily_6_32.pdf}
        \caption{$r = 0.5$ and $\batchsize \mathbin{/} N = 0.1$}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_label_noise/daily_30_160.pdf}
        \caption{$r = 0.1$ and $\batchsize \mathbin{/} N = 0.5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_label_noise/daily_30_32.pdf}
        \caption{$r = 0.1$ and $\batchsize \mathbin{/} N = 0.1$}
    \end{subfigure}
    \caption{Varying label noise $\sigma_F$ for top-level WOR and bottom-level WR  (\cref{theorem:data_augmentation_general}) with $\sigma_C = 0, \numinstances=1$.
    We additionally vary $r = (L_C + L_F) \mathbin{/} (L - L_F + 1)$
        and $\batchsize \mathbin{/} N$,
        with smaller values corresponding to more bottom- and top-level amplification, respectively.
    }
    \label{fig:label_noise_daily_appendix}
\vskip -0.2in
\end{figure*}

\clearpage

\subsubsection{Epoch Privacy vs Length}\label{appendix:extra_experiments_epoch_length}

This experiment differs from the previous ones in that we exclusively focus on top-level deterministic iteration and bottom-level sampling with replacement (see~\cref{theorem:deterministic_top_level_wr}).
Recall from our discussion in~\cref{section:bottom_level_subsampling} that $\numinstances = 1$ minimizes the privacy of each epoch, but forces us to perform $k$ times as many epochs for the same number of training steps as $\numinstances = k$.
Thus, there are more chances for privacy leakage and we need to self-compose the privacy profile for $\numinstances=1$ exactly $k$ times more often.

In the following, we demonstrate that composing many epochs that are more private (i.e., $\numinstances=1$) can nevertheless be beneficial.
To this end, we fix dataset size $N = 320$ and (to eliminate one redundant degree of freedom) batch size $\batchsize = 320$.
With this parameterization, $\numinstances = k$ means that we perform $k$ training steps in a single epoch. For our comparison, we thus self-compose our epoch-level mechanism $16$ times for $\numinstances = 1$, $8$ times for $\numinstances = 2$, $4$ times for $\numinstances = 4$ etc.\ 
to determine privacy for the same number of training steps.
We additionally vary the amount of bottom-level amplification by varying $r = \frac{L_C + L_F}{L - L_F + 1}$ between $0.5$ and $0.1$.

As can be seen in~\cref{fig:appendix_epoch_privacy_vs_length},
the number of sequences $\numinstances = 1$ offers smaller $\delta(\epsilon)$
at every training step that coincides with an epoch of $\numinstances > 1$.
Note that we only know $\delta(\epsilon)$ at the cross markers. The interpolants are only for reference.

To conclude, choosing number of subsequences $\numinstances=1$ remains the preferred option (see also~\cref{proposition:deterministic_top_level_monotonicity}) even under composition.

\begin{figure*}[ht!]
\centering
\vskip 0.2in
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_epoch_length/daily_4_320_1.0.pdf}
        \caption{$\epsilon=1$ and $r=0.5$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_epoch_length/daily_4_320_4.0.pdf}
        \caption{$\epsilon=4$ and $r=0.5$}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_epoch_length/daily_20_320_1.0.pdf}
        \caption{$\epsilon=1$ and $r=0.1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[]{figures/experiments/eval_pld_deterministic_epoch_length/daily_20_320_4.0.pdf}
        \caption{$\epsilon=4$ and $r=0.1$}
    \end{subfigure}
    \caption{
    Privacy parameter $\delta(\epsilon)$ for $\epsilon \in \{1,4\}$ over the course of $16$ training steps when using top-level deterministic iteration and bottom-level sampling with replacement (\cref{theorem:deterministic_top_level_wr}).
    We additionally vary $r$ between $0.5$ (less bottom-level amplification) and $0.1$ (more bottom-level amplification).
    The interpolants are only for reference, privacy parameter $\delta(\epsilon)$ is only known at the cross markers.
    }
    \label{fig:appendix_epoch_privacy_vs_length}
\vskip -0.2in
\end{figure*}