\subsection{Application to Probabilistic Forecasting}\label{appendix:privacy_utility_tradeoff}

\subsubsection{Event-Level Privacy}\label{appendix:privacy_utility_tradeoff_event_level_privacy}

\cref{table:1_event_training_traffic,table:1_event_training_electricity,table:1_event_training_solar}
show average CRPS after $1$-event-level private training on our three standard benchmark datasets.
Since $\delta^{-1}$ is approximately equal or greater than the dataset sizes, $\epsilon \leq 1$ indicates strong privacy guarantees,
whereas $2 \leq \epsilon \leq 8$ would be more commonly expected values in private training of machine learning models~\cite{ponomareva2023dp}.

For a full description of all hyper parameters, see~\cref{appendix:experimental_setup}.

In all cases, at least one model outperforms the traditional baselines without noise for all considered $\epsilon$.

\begin{table}[h]
\caption{Average CRPS on \texttt{traffic} for $1$-event-level privacy and $\delta=10^{-7}$. Seasonal, AutoETS, and models with $\epsilon=\infty$ are without noise.
Bold font indicates the best predictor per $\epsilon$.}
\label{table:1_event_training_traffic}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ & $\epsilon = 8$ &  $\epsilon = \infty$ \\
\midrule
SimpleFF & $0.207$ \tiny{$\pm 0.002$} & $0.195$ \tiny{$\pm 0.003$} & $0.193$ \tiny{$\pm 0.003$} & $0.194$ \tiny{$\pm 0.002$} & $0.193$ \tiny{$\pm 0.003$} & $0.136$ \tiny{$\pm 0.001$} \\ 
DeepAR & $\mathbf{0.157}$ \tiny{$\pm 0.002$} & $\mathbf{0.145}$ \tiny{$\pm 0.001$} & $\mathbf{0.142}$ \tiny{$\pm 0.001$} & $\mathbf{0.141}$ \tiny{$\pm 0.002$} & $\mathbf{0.141}$ \tiny{$\pm 0.002$} & $\mathbf{0.124}$ \tiny{$\pm 0.0.001$} \\
iTransf. & $0.211$ \tiny{$\pm 0.004$} & $0.193$ \tiny{$\pm 0.003$} & $0.188$ \tiny{$\pm 0.004$} & $0.188$ \tiny{$\pm 0.004$} & $0.188$ \tiny{$\pm 0.004$} & $0.135$ \tiny{$\pm 0.001$} \\
DLinear & $0.204$ \tiny{$\pm 0.004$} & $0.192$ \tiny{$\pm 0.001$} & $0.188$ \tiny{$\pm 0.003$} & $0.188$ \tiny{$\pm 0.003$} & $0.188$ \tiny{$\pm 0.003$} & $0.140$ \tiny{$\pm 0.000$} \\
\midrule
Seasonal   & $0.251$ & $0.251$ & $0.251$ & $0.251$ & $0.251$ & $0.251$\\
AutoETS   & $0.407$ & $0.407$ & $0.407$ & $0.407$ & $0.407$ & $0.407$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{traffic} for $1$-event-level privacy and $\delta=10^{-7}$. Seasonal, AutoETS, and models with $\epsilon=\infty$ are without noise.
Bold font indicates the best predictor per $\epsilon$.}
\label{table:1_event_training_electricity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ &  $\epsilon = 8$ &  $\epsilon = \infty$ \\
\midrule
SimpleFF & $0.072$ \tiny{$\pm 0.001$} & $0.065$ \tiny{$\pm 0.001$} & $0.065$ \tiny{$\pm 0.001$} & $0.065$ \tiny{$\pm 0.001$} & $0.065$ \tiny{$\pm 0.002$} & $\mathbf{0.058}$ \tiny{$\pm 0.001$} \\
DeepAR & $0.071$ \tiny{$\pm 0.004$} & $0.070$ \tiny{$\pm 0.004$} & $0.068$ \tiny{$\pm 0.005$} & $0.067$ \tiny{$\pm 0.004$} & $0.068$ \tiny{$\pm 0.005$} & $\mathbf{0.058}$ \tiny{$\pm 0.002$} \\
iTransf. & $0.081$ \tiny{$\pm 0.005$} & $0.075$ \tiny{$\pm 0.002$} & $0.074$ \tiny{$\pm 0.002$} & $0.074$ \tiny{$\pm 0.002$} & $0.074$ \tiny{$\pm 0.002$} & $\mathbf{0.058}$ \tiny{$\pm 0.001$} \\
DLinear & $\mathbf{0.064}$ \tiny{$\pm 0.000$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $0.059$ \tiny{$\pm 0.000$} \\
\midrule
Seasonal   & $0.070$ & $0.070$ & $0.070$ & $0.070$ & $0.070$ & $0.070$\\
AutoETS   & $0.064$ & $0.064$ & $0.064$ & $0.064$ & $0.064$ & $0.064$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{traffic} for $1$-event-level privacy and $\delta=10^{-7}$. Seasonal, AutoETS, and models with $\epsilon=\infty$ are without noise.
Bold font indicates the best predictor per $\epsilon$.}
\label{table:1_event_training_solar}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ & $\epsilon = 8$ &  $\epsilon = \infty$ \\
\midrule
SimpleFF & $1.114$ \tiny{$\pm 0.043$} & $1.114$ \tiny{$\pm 0.044$} & $1.114$ \tiny{$\pm 0.040$} & $1.118$ \tiny{$\pm 0.038$} & $1.113$ \tiny{$\pm 0.028$} & $0.766$ \tiny{$\pm 0.766$} \\ 
DeepAR & $\mathbf{0.820}$ \tiny{$\pm 0.030$} & $\mathbf{0.803}$ \tiny{$\pm 0.023$} & $\mathbf{0.792}$ \tiny{$\pm 0.016$} & $\mathbf{0.787}$ \tiny{$\pm 0.023$} & $\mathbf{0.787}$ \tiny{$\pm 0.023$} & $\textbf{0.654}$ \tiny{$\pm 0.654$} \\
iTransf. & $0.977$ \tiny{$\pm 0.065$} & $0.956$ \tiny{$\pm 0.062$} & $0.975$ \tiny{$\pm 0.066$} & $0.974$ \tiny{$\pm 0.065$} & $0.974$ \tiny{$\pm 0.065$} & $0.804$ \tiny{$\pm 0.804$} \\
DLinear & $1.287$ \tiny{$\pm 0.222$} & $1.152$ \tiny{$\pm 0.201$} & $1.110$ \tiny{$\pm 0.145$} & $1.048$ \tiny{$\pm 0.143$} & $1.051$ \tiny{$\pm 0.140$} & $0.860$ \tiny{$\pm 0.860$} \\
\midrule
Seasonal   & $1.120$ & $1.120$ & $1.120$ & $1.120$ & $1.120$ & $1.120$ \\
AutoETS   & $6.494$ & $6.494$ & $6.494$ & $6.494$ & $6.494$ & $6.494$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\clearpage

\subsubsection{w-event and w-User-Level Privacy}\label{appendix:privacy_utility_tradeoff_user_level_privacy}

As discussed in~\cref{appendix:generalizations}, 
we can (for sufficiently long sequences) generalize our bounds on the mechanism's privacy profile
from $1$-event- to $w$-event- or $w$-user-level   privacy by replacing
any occurrence of $L_C + L_F$ with $L_C + L_F + w -1$ or $w \cdot (L_C + L_F)$.
Since $w'$-event- and $w$-user-level privacy lead to identical results for some sufficiently large $w'$,
it is sufficient to experiment with $w$-user-level privacy.

For a full description of all hyper parameters, see~\cref{appendix:experimental_setup}.

\cref{table:w_user_training_traffic,table:w_user_training_electricity,table:w_user_training_solar}
show average CRPS after $w$-user-level private training on our three standard benchmark datasets.

Except for \texttt{traffic} and $w=8$ (which, by the above argument and our choice of $L_C$ and $L_F$, is equivalent to requiring privacy for an event spanning multiple days in our hourly datasets), at least one model outperforms the traditional baselines without noise for all considered $w$.

\begin{table}[h]
\caption{Average CRPS on \texttt{traffic} for $w$-user-level privacy and $\epsilon=4$, $\delta=10^{-7}$. SeasonSeasonal and AutoETS are without noise.
Bold font indicates the best predictor per $w$.}
\label{table:w_user_training_traffic}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $w = 1$ & $w = 2$ & $w = 4$ & $w = 8$ \\
\midrule
SimpleFF & $0.193$ \tiny{$\pm 0.003$} & $0.194$ \tiny{$\pm 0.003$} & $0.194$ \tiny{$\pm 0.003$} & $0.211$ \tiny{$\pm 0.001$} \\ 
DeepAR & $\mathbf{0.142}$ \tiny{$\pm 0.003$} & $\mathbf{0.143}$ \tiny{$\pm 0.001$} & $\mathbf{0.145}$ \tiny{$\pm 0.001$} & $\mathbf{0.166}$ \tiny{$\pm 0.004$} \\
iTransf. & $0.188$ \tiny{$\pm 0.004$} & $0.188$ \tiny{$\pm 0.004$} & $0.193$ \tiny{$\pm 0.002$} & $0.217$ \tiny{$\pm 0.005$} \\
DLinear & $0.189$ \tiny{$\pm 0.002$} & $0.189$ \tiny{$\pm 0.003$} & $0.192$ \tiny{$\pm 0.001$} & $0.208$ \tiny{$\pm 0.004$} \\
\midrule
Seasonal   & $0.251$ & $0.251$ & $0.251$ & $0.251$\\
AutoETS   & $0.407$ & $0.407$ & $0.407$ & $0.407$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{electricity} for $w$-user-level privacy and $\epsilon=4$, $\delta=10^{-7}$. SeaSeasonal and AutoETS are without noise.
Bold font indicates the best predictor per $w$.}
\label{table:w_user_training_electricity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $w = 1$ & $w = 2$ & $w = 4$ & $w = 8$ \\
\midrule
SimpleFF & $0.064$ \tiny{$\pm 0.001$} & $0.065$ \tiny{$\pm 0.001$} & $0.064$ \tiny{$\pm 0.001$} & $0.074$ \tiny{$\pm 0.001$} \\ 
DeepAR & $0.068$ \tiny{$\pm 0.005$} & $0.069$ \tiny{$\pm 0.005$} & $0.068$ \tiny{$\pm 0.002$} & $0.073$ \tiny{$\pm 0.003$} \\
iTransf. & $0.075$ \tiny{$\pm 0.003$} & $0.074$ \tiny{$\pm 0.002$} & $0.075$ \tiny{$\pm 0.003$} & $0.083$ \tiny{$\pm 0.004$} \\
DLinear & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} & $0.066$ \tiny{$\pm 0.001$} \\
\midrule
Seasonal   & $0.070$ & $0.070$ & $0.070$ & $0.070$ \\
AutoETS   & $0.064$ & $0.064$ & $0.064$ & $\mathbf{0.064}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{solar\_10\_minutes} for $w$-user-level privacy and $\epsilon=4$, $\delta=10^{-7}$. SeasoSeasonal and AutoETS are without noise.
Bold font indicates the best predictor per $w$.}
\label{table:w_user_training_solar}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $w = 1$ & $w = 2$ & $w = 4$ & $w = 8$ \\
\midrule
SimpleFF & $1.113$ \tiny{$\pm 0.033$} & $1.116$ \tiny{$\pm 0.038$} & $1.110$ \tiny{$\pm 0.042$} & $1.107$ \tiny{$\pm 0.034$} \\ 
DeepAR & $\mathbf{0.783}$ \tiny{$\pm 0.019$} & $\mathbf{0.791}$ \tiny{$\pm 0.018$} & $\mathbf{0.807}$ \tiny{$\pm 0.024$} & $\mathbf{0.823}$ \tiny{$\pm 0.028$} \\
iTransf. & $0.951$ \tiny{$\pm 0.061$} & $0.950$ \tiny{$\pm 0.062$} & $0.956$ \tiny{$\pm 0.062$} & $0.980$ \tiny{$\pm 0.060$} \\
DLinear & $1.072$ \tiny{$\pm 0.164$} & $1.122$ \tiny{$\pm 0.158$} & $1.200$ \tiny{$\pm 0.220$} & $1.298$ \tiny{$\pm 0.205$} \\
\midrule
Seasonal   & $1.120$ & $1.120$ & $1.120$ & $1.120$ \\
AutoETS   & $6.494$ & $6.494$ & $6.494$ & $6.494$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\clearpage

\subsubsection{Amplification by Label Perturbation}\label{appendix:privacy_utility_tradeoff_label_privacy}


Finally, let us demonstrate the potential benefit of using online data augmentations to amplify privacy in training forecasting models (\cref{theorem:amplification_by_data_augmentation_wor_wr}).
Consider $(w,v)$-event-level or $(w,v)$-user-level privacy with sufficiently small $v$. 
If $v$ is sufficiently small compared to the scale of the dataset, i.e., each individual only makes a small contribution to the overall value of a time series at each time step, then we can introduce non-trivial context and label noise $\sigma_C$ and $\sigma_F$ to amplify privacy without significantly affecting utility. 
Simultaneously, we still benefit from amplification through top- and bottom-level subsampling, i.e., do not need to add as much noise as would be required for directly making the entire input dataset privacy.

For a full description of all hyper parameters, see~\cref{appendix:experimental_setup}.

\cref{table:label_perturbation_training_traffic,table:label_perturbation_training_electricity,table:label_perturbation_training_solar}
show average CRPS after $(1,v)$-event-level private training with $\epsilon=0.5$, $\delta=10^{-7}$, i.e., strong privacy guarantees. Note that we use different $v$ per dataset, as they have different scale.

We observe that, for all models and all datasets, the best score is attained with label noise scale $\sigma_F = 2$ or $\sigma_F = 2$.
This confirms that there a scenarios in which our novel amplification-by-augmentation guarantees can help improve the privacy--utility trade-off of forecasting models.

\begin{table}[h]
\caption{Average CRPS on \texttt{traffic} for $(1,0.001)$-user-level privacy and $\epsilon=0.5$, $\delta=10^{-7}$. SeasoSeasonal and AutoETS are without noise.
Bold font indicates the best label noise scale $\sigma_F$ per model, i.e., per row.}
\label{table:label_perturbation_training_traffic}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $\sigma_F = 0$ & $\sigma_F= 1$ & $\sigma_F= 2$ & $\sigma_F = 5$\\
\midrule
SimpleFF & $0.207$ \tiny{$\pm 0.002$} & $0.205$ \tiny{$\pm 0.002$} & $0.205$ \tiny{$\pm 0.002$} & $\mathbf{0.205}$ \tiny{$\pm 0.001$}  \\ 
DeepAR & $0.156$ \tiny{$\pm 0.003$} & $0.156$ \tiny{$\pm 0.003$} & $0.156$ \tiny{$\pm 0.003$} & $\mathbf{0.154}$ \tiny{$\pm 0.002$}  \\
iTransf. & $0.211$ \tiny{$\pm 0.004$} & $0.208$ \tiny{$\pm 0.004$} & $0.205$ \tiny{$\pm 0.003$} & $\mathbf{0.204}$ \tiny{$\pm 0.003$}  \\
DLinear & $0.203$ \tiny{$\pm 0.003$} & $0.202$ \tiny{$\pm 0.003$} & $\mathbf{0.202}$ \tiny{$\pm 0.003$} & $0.203$ \tiny{$\pm 0.003$}  \\
\midrule
Seasonal   & $0.251$ & $0.251$ & $0.251$ & $0.251$ \\
AutoETS   & $0.407$ & $0.407$ & $0.407$ & $0.407$\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{electricity} for $(1,0.1)$-user-level privacy and $\epsilon=0.5$, $\delta=10^{-7}$. SeSeasonal and AutoETS are without noise.
Bold font indicates the best label noise scale $\sigma_F$ per model, i.e., per row.}
\label{table:label_perturbation_training_electricity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $\sigma_F = 0$ & $\sigma_F= 1$ & $\sigma_F = 2$ & $\sigma_F = 5$\\
\midrule
SimpleFF & $0.072$ \tiny{$\pm 0.001$} & $0.069$ \tiny{$\pm 0.001$} & $0.068$ \tiny{$\pm 0.002$} & $\mathbf{0.067}$ \tiny{$\pm 0.002$}\\
DeepAR & $0.071$ \tiny{$\pm 0.006$} & $0.074$ \tiny{$\pm 0.005$} & $0.069$ \tiny{$\pm 0.005$} & $\mathbf{0.067}$ \tiny{$\pm 0.005$}\\
iTransf. & $0.081$ \tiny{$\pm 0.005$} & $0.080$ \tiny{$\pm 0.004$} & $0.080$ \tiny{$\pm 0.005$} & $\mathbf{0.080}$ \tiny{$\pm 0.004$}\\
DLinear & $0.064$ \tiny{$\pm 0.000$} & $0.062$ \tiny{$\pm 0.000$} & $0.061$ \tiny{$\pm 0.001$} & $\mathbf{0.061}$ \tiny{$\pm 0.001$} \\
\midrule
Seasonal   & $0.070$ & $0.070$ & $0.070$ & $0.070$ \\
AutoETS   & $0.064$ & $0.064$ & $0.064$ & $0.064$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Average CRPS on \texttt{solar\_10\_minutes} for $(1,0.01)$-user-level privacy and $\epsilon=0.5$, $\delta=10^{-7}$. Seasonal and AutoETS are without noise.
Bold font indicates the best label noise scale $\sigma_F$ per model, i.e., per row.}
\label{table:label_perturbation_training_solar}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
Model & $\sigma_F = 0$ & $\sigma_F= 1$ & $\sigma_F = 2$ & $\sigma_F = 5$\\
\midrule
SimpleFF  & $1.117$ \tiny{$\pm 0.034$} & $1.126$ \tiny{$\pm 0.046$} & $\mathbf{1.117}$ \tiny{$\pm 0.042$} & $1.119$ \tiny{$\pm 0.041$} \\ 
DeepAR  & $0.820$ \tiny{$\pm 0.030$} & $0.818$ \tiny{$\pm 0.028$} & $0.814$ \tiny{$\pm 0.027$} & $\mathbf{0.813}$ \tiny{$\pm 0.026$} \\
iTransf.  & $0.976$ \tiny{$\pm 0.064$} & $0.970$ \tiny{$\pm 0.062$} & $0.960$ \tiny{$\pm 0.052$} & $\mathbf{0.959}$ \tiny{$\pm 0.056$} \\
DLinear & $1.282$ \tiny{$\pm 0.211$} & $1.245$ \tiny{$\pm 0.232$} & $\mathbf{1.238}$ \tiny{$\pm 0.240$} & $1.246$ \tiny{$\pm 0.227$} \\
\midrule
Seasonal   & $1.120$ & $1.120$ & $1.120$ & $1.120$ \\
AutoETS   & $6.494$ & $6.494$ & $6.494$ & $6.494$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}