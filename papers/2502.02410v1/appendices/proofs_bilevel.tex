\section{Proofs from Section 4.2 (Bi-Level Sampling)}\label{appendix:proofs_bilevel}
The main purpose of this section is to prove the following tight pessimistic guarantee
for top-level sampling without replacement and bottom-level sampling with replacement:
\wortoplevelwr*
We further want to prove the following (not necessarily tight) pessimistic guarantees:
\begin{theorem}\label{theorem:wor_top_level_wr_general}
    Consider
    bottom-level sampling with replacement, 
    number of subsequences $\numinstances = 1$ and 
    batch size $\batchsize$.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Like in~\cref{theorem:deterministic_top_level_wr_general}, define
    $\overline{P}(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$
    and
    $\overline{Q}(\numinstances) = \mog(\vmu, \vp, \sigma)$
    with
    means $\vmu = \begin{bmatrix}
        0 & 2 &  4 & \cdots & 2 \cdot \numinstances
    \end{bmatrix}^T$ and 
    weights $\vp \in [0,1]^{\numinstances + 1}$ with $p_i = \mathrm{Binomial}(i-1 \mid \numinstances, r)$.
    Then, per-step privacy profile $\tilde{H}(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})$ fulfills 
    \begin{equation*}
        \tilde{H}(\alpha) \leq 
        (1-\rho) \cdot \max \{0, 1 - \alpha\}
        +
        \rho \cdot 
            H_\alpha(\overline{P}(\numinstances) || \overline{Q}(\numinstances))
    \end{equation*}
\end{theorem}
\begin{theorem}\label{theorem:wor_top_level_poisson_general}
    Consider
    bottom-level Poisson sampling, 
    number of subsequences $\numinstances = 1$ and 
    batch size $\batchsize$.
    Let $r = \min\{1, \numinstances \mathbin{/} (L - L_F + 1)\}$
    be the resulting Poisson sampling rate, 
    and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Like in~\cref{theorem:deterministic_top_level_poisson}, define
    $P(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $Q(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r)$.
    Then, per-step privacy profile $\tilde{H}(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})$ fulfills 
    \begin{equation*}
        \tilde{H}(\alpha) \leq
        (1-\rho) \cdot \max \{0, 1 - \alpha \}
        +
        \rho \cdot 
        H_\alpha(P(\numinstances) || Q(\numinstances).
    \end{equation*}
\end{theorem}
We further want to determine pessimistic lower bounds to serve as baselines for $\numinstances = 1$ in our numerical experiments.

Let us thus proceed similar to~\cref{appendix:proofs_bottom_level}. We: 
\begin{enumerate}[noitemsep,nosep]
    \item Prove that, the privacy of our bi-level mechanism can be bounded by considering a fixed set of top-level batches,
    \item from this result, derive pessimistic upper bounds  via joint convexity,
    \item derive tighter upper bounds for $\numinstances = 1$ and bottom-level sampling with replacement by focusing on our analysis on a single per-subsequence gradient via the parallel composition property, 
    \item determine optimistic lower bounds that coincide with the upper bound for $\numinstances=1$ and bottom-level sampling with replacement by constructing worst-case time series datasets,
    \item determine dominating pairs corresponding to our pessimistic upper bounds.
\end{enumerate}

\subsection{Reduction to Fixed Set of Top-Level Batches}
In the following, we use conditional couplings to eliminate the randomness inherent to top-level sampling from our analysis. 
The proof is largely identical to that for sampling without replacement and substitution in sets
(e.g., Theorem 11.2 in~\cite{zhu2022optimal})
, except that we sometimes only substitute with sequences that differ in a single element ($1$-event-level privacy).
\begin{lemma}\label{lemma:wor_top_level_reduction}
    Consider the space $\sA = \sR^L$ of all length-$L$ sequences.
    Let $\sX \subseteq \mathcal{P}(\sA)$ be the space of all size-$N$ datasets of sequences.
    Further consider batch size $\batchsize$ and bottom-level number of instances $\numinstances$
    with resultant top-level batch size $N' = \lfloor \batchsize \mathbin{/} \numinstances \rfloor$.
    Let $\sY = \mathcal{P}(\sA)$ be the space of all size-$N'$ top-level batches.
    Let $\hat{S} : \sX \rightarrow \sY$ be top-level sampling without replacement, as defined in~\cref{algorithm:dp-sgd-wor-top-level},
    and $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence.
    Finally, let $\hat{B} : \sY \rightarrow \sR^D$ be an arbitrary mechanism that maps top-level batches to gradients
    and define $\tilde{M} = \hat{B} \circ \hat{S}$.
    Then, for all $\alpha \geq 0$,
    \begin{equation*}
        \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})
        \leq
        \sup_{y, y', y'' \in \sY}
        H_\alpha\left((1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y'}
            ||
            (1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y''}\right)
    \end{equation*}
    subject to $y \simeq_\Delta y'$, $y \simeq_\Delta y''$, $y' \simeqevent{1} y''$, where $\simeq_\Delta$ indicates arbitrary substitution of a single sequence, while $\simeqevent{1}$ indicates substitution while only changing a single element of the sequence.
\end{lemma}
\begin{proof}
    Consider arbitrary $x \simeqevent{1} x'$.
    By definition, there must be some $x_n \in x$ and $x'_n \in x'$
    such that $x \setminus \{x'_n\} = x' \setminus \{x'_n\}$
    and sequences $x_n$ differ in exactly one sequence.

    Define events $A_0 = \{y \subseteq \mid x_n \notin x\}$,
    $A_1 = \{y \subseteq \mid x_n \in x\}$,
    Define events $E_0 = \{y \subseteq \mid x'_n \notin x'\}$,
    $E_1 = \{y \subseteq \mid x'_n \in x'\}$.
    We naturally have $\hat{S}_x(A_0) = \hat{S}_{x'}(E_0) = 1 - \rho$
    and $\hat{S}_x(A_1) = \hat{S}_{x'}(E_1) = \rho$.

    We can then define a coupling $\Gamma$
    of $\hat{S}_{x}(\cdot \mid A_0),\hat{S}_{x}(\cdot \mid A_1),\hat{S}_{x'}(\cdot \mid E_0),\hat{S}_{x'}(\cdot \mid E_0)$
    via the following mass function:
    \begin{align*}
        &\gamma(y^{(1)}_0, y^{(1)}_1, y^{(2)}_0, y^{(2)}_1)
        \\
        \propto
        &
        s_x(y^{(1)}_0 \mid A_0)
        \cdot
        \indicator\left[
            \exists x_m \in y^{(1)}_0 : 
            y^{(1)}_1 = y^{(1)}_0 \setminus \{x_m\} \cup \{x_n\}
        \right]
        \cdot
        \prod_{i=0}^1
        \indicator\left[
            y^{(2)}_i = y^{(1)}_i \setminus \{x_n\} \cup \{x'_n\}
        \right].
    \end{align*}
    By construction, the entire support of our couplings fulfills 
    $y^{(1)}_0 = y^{(2)}_0$,
    $y^{(1)}_0 \simeq_\Delta y^{(1)}_1$,
    $y^{(1)}_0 \simeq_\Delta y^{(2)}_1$,
    and $y^{(1)}_1 \simeqevent{1} y^{(2)}_1$.

    The result then immediately follows from considering worst-case datasets given these constraints (\cref{lemma:cost_function_upper_bound}).
\end{proof}
The distinction between the two different types of substitution is critically important
for our later analysis of amplification-by-augmentation, as we may otherwise drastically underestimate the difference of specific Gaussian distributions.

\subsection{Pessimistic Upper Bounds via Joint Convexity}
Via the next lemma, we can immediately prove our (not necessarily tight) pessimistic upper bounds for top-level sampling without replacement
and bottom-level Poisson sampling.
\begin{lemma}\label{lemma:wor_top_level_reduction_joint_convexity}
    Consider the space $\sA = \sR^L$ of all length-$L$ sequences.
    Let $\sX \subseteq \mathcal{P}(\sA)$ be the space of all size-$N$ datasets of sequences.
    Further consider batch size $\batchsize$ and bottom-level number of instances $\numinstances$
    with resultant top-level batch size $N' = \lfloor \batchsize \mathbin{/} \numinstances \rfloor$.
    Let $\sY = \mathcal{P}(\sA)$ be the space of all size-$N'$ top-level batches.
    Let $\hat{S} : \sX \rightarrow \sY$ be top-level sampling without replacement, as defined in~\cref{algorithm:dp-sgd-wor-top-level},
    and $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence.
    Finally, let $\hat{B} : \sY \rightarrow \sR^D$ be an arbitrary mechanism that maps top-level batches to gradients
    and define $\tilde{M} = \hat{B} \circ \hat{S}$.
    Then, for all $\alpha \geq 0$,
    \begin{equation*}
         \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})
         \leq 
        (1 - \rho) \cdot 
        \max \{0, 1 - \alpha \}
        +
        \rho \cdot
        \sup_{y', y'' }
        H_\alpha\left(\hat{B}_{y'}
            ||
            \hat{B}_{y''}\right)
    \end{equation*}
    subject to $y' \simeqevent{1} y''$.
\end{lemma}
\begin{proof}
    The result immediately follows from~\cref{lemma:wor_top_level_reduction} and joint convexity of hockey stick divergences (\cite{balle2020privacy}), i.e.,
    \begin{align*}
        &
        H_\alpha\left((1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y'}
            ||
            (1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y''}\right) \\
        \leq
        &
        (1 - \rho)  \cdot H_\alpha(\hat{B}_{y} || \hat{B}_{y})
        +
        \rho
        \cdot
        H_\alpha\left(\hat{B}_{y'}
            ||
            \hat{B}_{y''}\right) \\
        =
        &
        (1 - \rho) \cdot 
        \max \{0, 1-\alpha\}
        +
        \rho
        \cdot
        H_\alpha\left(\hat{B}_{y'}
            ||
            \hat{B}_{y''}\right). \\
    \end{align*}
\end{proof}
Analyzing the privacy of bottom-level subsampling when two top-level batches differ in one element of one sequence,
i.e., bounding $\sup_{y', y'' }
        H_\alpha\left(\hat{B}_{y'}
            ||
            \hat{B}_{y''}\right)$ is exactly what we have already done in~\cref{appendix:proofs_bottom_level}.
Specifically~\cref{theorem:wor_top_level_wr_general} follows directly from~\cref{lemma:wor_top_level_reduction_joint_convexity,theorem:deterministic_top_level_wr_general}, 
while~\cref{theorem:wor_top_level_poisson_general} follows directly from~\cref{lemma:wor_top_level_reduction_joint_convexity,theorem:deterministic_top_level_poisson}.

\subsection{Tighter Upper Bounds for Sampling With Replacement}
Instead of directly applying joint convexity, let us derive tighter bounds on the following optimization problem from~\cref{lemma:wor_top_level_reduction} via conditional coupling:
\begin{equation}\label{eq:tighter_upper_start}
    \sup_{y, y', y'' \in \sY}
        H_\alpha\left((1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y'}
            ||
            (1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y''}\right)
\end{equation}
Due to permutation invariance of bottom-level subsampling and gradient computation, let us define an arbitrary ordering within the top-level batches such that
$y = \{y_1, y_2,\dots,y_{N'}\}$,
$y' = \{y'_1, y_2,\dots,y_{N'}\}$,
$y'' = \{y''_1, y_2,\dots,y_{N'}\}$ differ in their first sequence, and $y'_1$ and $y''_1$ only differ in a single element.
We observe that the distribution over gradients for $y_2,\dots,y_{N'}$ are identical under both mixture mechanisms in~\cref{eq:tighter_upper_start} and are computed (before summation) completely independently of $y_1,y'_1,y''_1$.
We can thus make precisely the same post-processing and parallel composition argument as in~\cref{lemma:proofs_bottom_step_to_group} from~\cref{appendix:proofs_bottom_step_to_group} to show that
\begin{align}
    \sup_{y, y', y'' \in \sY}
        H_\alpha\left((1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \hat{B}_{y'}
            ||
            (1 - \rho) \cdot \hat{B}_{y} + \rho \cdot \check{M}_{y''}\right)
    \\
    \label{eq:tighter_upper_start_relaxed}
    \leq
    \sup_{y_1, y_1', y_1'' \in \sY}
        H_\alpha\left((1 - \rho) \cdot \check{M}_{y_1} + \rho \cdot \check{M}_{y_1'}
            ||
            (1 - \rho) \cdot \check{M}_{y_1} + \rho \cdot \check{M}_{y_1''}\right),
\end{align}
with $\{y_1\} \simeq_\Delta \{y'_1\}$,
$\{y_1\} \simeq_\Delta \{y''_1\}$,
and
$\{y'_1\} \simeqevent{1} \{y_1''\}$.
Again, note the distinction between arbitrary substitution of the sequence and substitution of a single element. 
In the above inequality, $\check{M} : \sR^{L} \rightarrow \sR^{D}$
with $\check{M}(y_1)  = Z + (G \circ \check{S})(y_1) \coloneq (\check{B} \circ \check{S})(y_1)$ and $Z \simeq \mathcal{N}(0, \sigma^2 C^2 \eye)$
is the mechanism that yields bottom-level subsampled, clipped, summed, and noised gradients for a single sequence.
We can further upper-bound~\cref{eq:tighter_upper_start_relaxed} via conditional coupling to prove the following result for bottom-level sampling with replacement and number of instances $\numinstances=1$:
\begin{lemma}\label{lemma:bilevel_reduction_to_single_gradient}
    Consider arbitrary sequences $y_1, y_1', y_1'' \in \sR^{L}$ such that
    $\{y_1\} \simeq_\Delta \{y'_1\}$,
    $\{y_1\} \simeq_\Delta \{y_1''\}$,
    $\{y_1\} \simeqevent{1} \{y_1''\}$,
    i.e, all sequences are different, but $y_1'$ and $y_1''$ only differ in a single element.
    Let $\check{S} : \sR^{L} \rightarrow \sR^{L_C+L_F}$ be bottom-level sampling with replacement and number of subsequences $\numinstances = 1$, as defined in~\cref{algorithm:dp-sgd-wr-bottom-level}.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ be the probability of sampling a subsequence containing any specific element.
    Further define forecast-to-context ratio $\phi = \frac{L_F}{L_C + L_F}$.
    Let $\check{B} : \sR^{L_C + L_F} \rightarrow \sR^D$ be an arbitrary mechanism that maps a single subsequence to a gradient
    and define bottom-level subsampled mechanism $\check{M} = \check{B} \circ \check{S}$.
    Then,
    \begin{equation*}
        H_\alpha\left((1 - \rho) \cdot \check{M}_{y_1} + \rho \cdot \check{M}_{y_1'}
            ||
            (1 - \rho) \cdot \check{M}_{y_1} + \rho \cdot \check{M}_{y_1''}\right)
        \leq 
        H_\alpha\left(
        P||Q\right)
    \end{equation*}
    with
    \begin{align*}
        P = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot   \check{B}_{a'_F} + \rho \cdot r \cdot (1 - \phi) B_{a'_C},
        \\
        Q = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot  \check{B}_{a''_F} + \rho \cdot r \cdot (1 - \phi) B_{a''_C},
    \end{align*}
    for some worst-case tuple of subsequences $a, a', a'_F, a'_C \in \sR^{L_C + L_F}$ where 
    $\{a'_F\} \simeqevent{1} \{a''_F\}$,
    $\{a'_C\} \simeqevent{1} \{a''_C\}$,
    subsequences $a_F, a'_F$ only differ in their last $L_F$ elements (forecast window),
    subsequences $a_C, a'_C$ only differ in their first $L_C$ elements (context window), 
    and the second mixture components are identical.
\end{lemma}
\begin{proof}
    For the following proof, let us define an ordering on the support of our subsampling distributions
    $S_{y_1}, S_{y'_1}, S_{y''_1}$ with mass functions $s_{y_1}, s_{y'_1}, s_{y''_1}$  such that the earliest subsequence comes first, i.e.,
    \begin{align*}
        \mathrm{supp}(S_{y_1}) = \{a_1, a_2, \dots, a_{L - L_F + 1}\},
        \\
        \mathrm{supp}(S_{y'_1}) = \{a_1, a_2, \dots, a_{L - L_F + 1}\},
        \\
        \mathrm{supp}(S_{y''_1}) = \{a''_1, a''_2, \dots, a''_{L - L_F + 1}\}.
    \end{align*}
    Further partition $\mathrm{supp}(S_{y'_1})$ and $\mathrm{supp}(S_{y''_1})$ based on whether and where the single modified element appears in the subsequence, i.e.,
    \begin{align*}
        A' &= \{a'_i \in \mathrm{supp}(S_{y'_1}) \mid a'_i = a''_i\} \\
        A'_C  &= \{a'_i \in \mathrm{supp}(S_{y'_1}) \mid a'_i[1:L_C] \neq \mid a''_i[1:L_C] \} \\
        A'_F & = \{a'_i \in \mathrm{supp}(S_{y'_1}) \mid a'_i[L_C + 1 :] \neq \mid a''_i[L_C + 1 : ] \} \\
        A'' &= \{a''_i \in \mathrm{supp}(S_{y''_1}) \mid a'_i = a''_i\} \\
        A''_C  &= \{a''_i \in \mathrm{supp}(S_{y''_1}) \mid a'_i[1:L_C] \neq \mid a''_i[1:L_C] \} \\
        A''_F & = \{a''_i \in \mathrm{supp}(S_{y''_1}) \mid a'_i[L_C + 1 :] \neq \mid a''_i[L_C + 1 : ] \}
    \end{align*}

    By definition of the bottom-level subsampled mechanism $\check{M}$, we have for the the components of the first mixture distribution:
    \begin{align*}
        \check{M}_{y_1} & = \sum_{a \in \mathrm{supp}(S_{y_1})} \check{B}_a   s_{y_1}(a) \\
        \check{M}_{y'_1} & = S_{y'_1}(A')  \sum_{a' \in A'} \check{B}_{a'}   s_{y_1}(a' \mid A')
                            + S_{y'_1}(A_C')  \sum_{a'_C \in A'_C} \check{B}_{a'_C}   s_{y_1}(a'_C \mid A'_C),
                            + S_{y'_1}(A_F')  \sum_{a'_F \in A'_F} \check{B}_{a'_F}   s_{y_1}(a'_F \mid A'_F).
    \end{align*}
    Analogously, we have for the components of the second mixture distribution:
    \begin{align*}
        \check{M}_{y_1} & = \sum_{a \in \mathrm{supp}(S_{y_1})} \check{B}_a   s_{y_1}(a) \\
        \check{M}_{y''_1} & = S_{y''_1}(A'')  \sum_{a'' \in A''} \check{B}_{a''}   s_{y_1}(a'' \mid A'')
                            + S_{y''_1}(A_C'')  \sum_{a''_C \in A''_C} \check{B}_{a''_C}   s_{y_1}(a''_C \mid A''_C),
                            + S_{y''_1}(A_F'')  \sum_{a''_F \in A''_F} \check{B}_{a''_F}   s_{y_1}(a''_F \mid A''_F).
    \end{align*}
    Here, the probability of not sampling, sampling in the context window, and sampling in the forecast window are
    $S_{y'_1}(A') = S_{y''_1}(A'') = (1-r)$,
    $S_{y'_1}(A') = S_{y''_1}(A'') = r \cdot (1 - \phi)$,
    and
    $S_{y'_1}(A') = S_{y''_1}(A'') = r \cdot \phi$, respectively, 
    with forecast-to-context-ratio $\phi = \frac{L_F}{L_C + L_F}$.

    We can now define a coupling  $\Gamma$ of conditional subsampling distributions
    $S_{y_1}(\cdot), S_{y'_1}(\cdot \mid A'), S_{y'_1}(\cdot \mid A'_C), S_{y'_1}(\cdot \mid A'_F)$,
    as well as their counterpart from the second mixture 
    $S_{y_1}(\cdot), S_{y''_1}(\cdot \mid A''), S_{y''_1}(\cdot \mid A''_C), S_{y''_1}(\cdot \mid A''_F)$.
    Specifically, the coupling will match any subsequence from $y_1'$ with its counterpart from $y_1''$ that covers the same time range:
    \begin{align*}
        & \gamma(a^{(1)}, a', a'_F, a'_C, a^{(2)}, a'', a''_F, a''_C)
        \\
        = 
        &
        \left(s_{y_1}(a^{(1)}) \cdot s_{y'_1}(a' \mid A') \cdot s_{y'_1}(a'_F \mid A'_F) \cdot s_{y'_1}(a'_C \mid A'_C) \right)
        \cdot
        \left(
            \gamma(a^{(2)} \mid a^{(1)})
            \cdot
            \gamma(a'' \mid a')
            \cdot
            \gamma(a''_F \mid a'_F)
            \cdot
            \gamma(a''_C \mid a'_C)
        \right)
    \end{align*}
    with intra-distribution components 
    \begin{align*}
        & \gamma(a^{(2)} \mid a^{(1)}) = \indicator[a^{(1)} = a^{(2)}],
        \\
        & \gamma(a'' \mid a') = \indicator[a' =  a''],
        \\
        & \gamma(a''_F \mid a'_F) = \indicator[a'_F = a'_i  \implies  a''_F = a''_i],
        \\
        & \gamma(a''_C \mid a'_C) = \indicator[a'_C = a'_i  \implies  a''_C = a''_i].
    \end{align*}
    By construction of the coupling, all $(a^{(1)}, a', a'_F, a'_C, a^{(2)}, a'', a''_F, a''_C)$ with non-zero measure fulfill the following constraints:
    We have
    $\{a'_F\} \simeqevent{1} \{a''_F\}$,
    $\{a'_C\} \simeqevent{1} \{a''_C\}$,
    subsequences $a_F, a'_F$ only differ in their last $L_F$ elements (forecast window),
    subsequences $a_C, a'_C$ only differ in their first $L_C$ elements (context window).
    Furthermore, we have $a^{(1)} = a^{(2)}$ and $a' = a'$.

    The result then follows from rewriting the two mixtures as marginals of the coupling
    and moving the coupling outside the hockey stick divergence using joint convexity, i.e.,
    the usual conditional coupling procedure from~\cref{lemma:coupling_bound}.
\end{proof}
Finally, we can determine worst-case mixture components given worst-case subsequences to obtain
a bound in terms of Gaussian mixtures:
\begin{lemma}
    Consider the number of subsequences $\numinstances = 1$ and 
    batch size $\batchsize$.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Let $\check{B} : \sR^{L_C + L_F} \rightarrow \sR^D$ be a Gaussian mechanism
    $B(a) = g(a) + Z$ with $Z \sim \mathcal{N}(0,\sigma^2 C^2 \eye)$
    and underlying function $g$ with maximum norm $C$ that maps a single subsequence to a gradient.
    Consider arbitrary subsequences $a, a', a'_F, a'_C, a''_F, a''_C \in \sR^{L_C + L_F}$ where 
    $\{a'_F\} \simeqevent{1} \{a''_F\}$,
    $\{a'_C\} \simeqevent{1} \{a''_C\}$,
    subsequences $a_F, a'_F$ only differ in their last $L_F$ elements (forecast window),
    subsequences $a_C, a'_C$ only differ in their first $L_C$ elements (context window).
    Define corresponding output distributions
    \begin{align*}
        P = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot   \check{B}_{a'_F} + \rho \cdot r \cdot (1 - \phi) B_{a'_C},
        \\
        Q = (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot \phi \cdot  \check{B}_{a''_F} + \rho \cdot r \cdot (1 - \phi) B_{a''_C}.
    \end{align*}
    Further define
    $\tilde{P}(1) = \mog(\vmu, \tilde{\vp}, \sigma)$ with
     $\tilde{\vmu} = \begin{bmatrix}
        0 & 2
    \end{bmatrix}^T$ and 
     $\tilde{\vp} = \begin{bmatrix} (1 - \rho) + \rho \cdot (1-r) & \rho \cdot r\end{bmatrix}^T$.
     Then, for all $\alpha \geq 0$,
     \begin{equation*}
         H_\alpha(P || Q) \leq 
         \begin{cases}
            H_\alpha(\tilde{P}(1) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || \tilde{P}(1)) & \text{if } 0 \leq \alpha < 1.
        \end{cases}
     \end{equation*}
\end{lemma}
\begin{proof}
    \textbf{Case 1 ($\alpha \geq 1$):}
    Since the hockey stick divergence is jointly convex, it is also jointly quasi-convex.
    That is, the value attained by any interpolated argument is l.e.q.\ one of the arguments it is interpolating between.
    In the context of our mixture distributions, this means that
    \begin{align*}
        &H_\alpha(P || Q)
        \\
        \begin{split}
        \leq
        \max \{
        &H_\alpha \left(
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r  \cdot  \check{B}_{a'_F}
            ||
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot  \check{B}_{a''_F}
        \right), 
        \\
        &H_\alpha \left(
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r  \cdot B_{a'_C}
            ||
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot B_{a''_C}
        \right) \}.
        \end{split}
    \end{align*}
    Since the bound we shall derive shortly will lead to identical results for both terms, let us focus w.l.o.g.\ on bounding the first one, i.e.,
    \begin{equation*}
        H_\alpha \left(
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r  \cdot  \check{B}_{a'_F}
            ||
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot  \check{B}_{a''_F}
        \right)
    \end{equation*}

    Since the inputs $a$ and $a'$ to our mechanism in the first two mixture components are identical between both distributions, we can use advanced joint convexity (\cref{lemma:advanced_joint_convexity})
    to restate our mixture divergence via
    \begin{align*}
        &H_\alpha \left(
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r  \cdot  \check{B}_{a'_F}
            ||
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot  \check{B}_{a''_F}
        \right)
        \\
        = 
        &
        \rho \cdot r \cdot
        H_{\alpha'} \left(
             \check{B}_{a'_F}
            ||
            (1 - \beta(\alpha)) \cdot \left((1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} \right)+
            \beta(\alpha) \cdot \rho \cdot r \cdot  \check{B}_{a''_F}
        \right)
    \end{align*}
    with some $\alpha' \geq \alpha$ and some $\beta(\alpha) \in [0,1]$.

    Recall that $\check{B}$ is a Gaussian mechanism.
    Since underlying function $g$ has maximum norm $C$, we know that the maximum $\ell_2$ distance of any two means is $2 \cdot C$.
    Due to translation equivariance of hockey stick divergences between Gaussian mixtures, we can
    assume w.l.o.g.\ that $\check{B}_{a'_F} = \mathcal{N}(\vzero, \sigma^2 C^2 \eye)$.
    We can thus define a constrained optimization problem over Gaussian mixture means to upper-bound our divergence:
    \begin{align*}
        &
        \rho \cdot r \cdot
        H_{\alpha'} \left(
             \check{B}_{a'_F}
            ||
            (1 - \beta(\alpha)) \cdot \left((1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} \right)+
            \beta(\alpha) \cdot \rho \cdot r \cdot  \check{B}_{a''_F}
        \right)
        \\
        &
        \leq
        \max_{\vmu^{(1)}, \vmu^{(2)}, \vmu^{(3)}}
        \rho \cdot r \cdot
        H_{\alpha'} \left(
             \mathcal{N}(\vzero)
            ||
            (1 - \beta(\alpha)) \cdot \left((1 - \rho) \cdot \mathcal{N}(\vmu^{(1)}) + \rho \cdot (1-r) \mathcal{N}(\vmu^{(2)}) \right)+
            \beta(\alpha) \cdot \rho \cdot r \cdot  \mathcal{N}(\vmu^{(3)})
        \right),
    \end{align*}
    subject to $\forall i: \vmu^{(1)} \in \sR^D$ and $|\vmu^{(i)}|| \leq C$, 
    where we omitted the covariance matrix $\sigma^2 C^2 \eye$ for brevity.
    As is known from~\cite{schuchardt2024unified} (see~\cref{lemma:worst_case_insertion_removal_mixture}),
    the maximum is attained at $\vmu^{(1)}, \vmu^{(2)}, \vmu^{(3)} = 2  C \cdot \ve_1$, where $\ve_1$ has $1$ in the first component and $0$ everywhere else.
    In other words, all components in the second mixture are identical.
    Finally, we can apply advanced joint convexity in reverse order (note that it is an equality, not an inequality)
    to conclude that 
    \begin{align*}
        &H_\alpha \left(
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r  \cdot  \check{B}_{a'_F}
            ||
            (1 - \rho) \cdot \check{B}_{a} + \rho \cdot (1-r) \check{B}_{a'} + \rho \cdot r \cdot  \check{B}_{a''_F}
        \right)
        \\
        \leq 
        &
        H_\alpha \left(
            ((1 - \rho) + \rho \cdot (1-r)) \cdot \mathcal{N}(2 C \cdot \ve_1, \sigma^2 C^2 \eye) + \rho \cdot r  \cdot  \mathcal{N}(\vzero, \sigma^2 C^2 \eye)
            ||
            \mathcal{N}(2  C \cdot \ve_1, \sigma^2 C^2 \eye)
        \right)
    \end{align*}
    Marginalizing out all but the first dimension, and scaling and rotating the coordinate system appropriately, concludes our proof for this case.
    
    \textbf{Case 2 ($0 \leq \alpha < 1$):}
    For the case $0 \leq \alpha < 1$, we can use the following fact:
    If $P,Q$ is dominating for $\alpha \geq 1$ under a symmetric neighboring relation,
    then $Q, P$ is dominating for $0 \leq \alpha < 1$ (\citet{zhu2022optimal}, see~\cref{lemma:dominating_pair_alpha_symmetry}).
\end{proof}

In conjunction with the previous steps and lemmata, this concludes our proof of the ``$\leq$'' part of~\cref{theorem:wor_top_level_wr}.
Next, let us show that this bound coincides with an optimistic upper bound, i.e., is tight.

\subsection{Optimistic Lower Bounds}\label{appendix:bilevel_optimistic_lower_bounds}
Next, we can again construct worst-case datasets and gradient functions.
As we shall see, we can conveniently use the same worst-case construction as for our bottom-level analysis from~\cref{appendix:proofs_bottom_level}.
\begin{theorem}\label{theorem:wor_top_wr_bottom_upper}
    Consider top-level sampling without replacement and bottom-level sampling with replacement.
    Further consider any number of subsequences $\numinstances \in \sN$ and 
    batch size $\batchsize$.
    Let $r = \frac{L_C + L_F}{L - L_F + 1}$ and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Define
    $\underline{P}(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\vmu \in \sN_0^{\numinstances +1}$ and weights $\vmu \in \sN_0^{\numinstances +1}$
    with $\evmu_i = 2 (i-1)$
    and $\evp_i = \mathrm{Binomial}(i \mid \numinstances, r)$. Further define per-step privacy profile $H(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})$. Then, 
    \begin{equation*}
        H(\alpha) \geq 
        \begin{cases}
            H_\alpha((1 - \rho) \cdot \mathcal{N}(0,\sigma)  + \rho \cdot \underline{P}(\numinstances) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || (1 - \rho) \cdot \mathcal{N}(0,\sigma)  + \rho \cdot \underline{P}(\numinstances)  & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{theorem}
\begin{proof}
    Exactly like in our proof of~\cref{theorem:deterministic_top_level_wr_optimistic}, we can construct the following gradient function 
    \begin{equation*}
        g(a) = \begin{cases}
            C \cdot \ve_1 & \text{if } \exists l \in \{1,\dots,L_C + L_F\} : a_l = 1, \\
            -C & \ve_1 \text{otherwise.}
        \end{cases}
    \end{equation*}
    where $\ve_1$ is the indicator vector that is non-zero in its first component, 
    and $C$ is the clipping constant.
     
    \textbf{Case 1 ($\alpha \geq 1$):}
    In this case, we can construct sequences $x_1, x'_1 \in \sR^{L}$ with
    $x_1 = \begin{bmatrix}
        1 & 0 & \cdots & 0
    \end{bmatrix}$
    and 
    $x'_1 = \begin{bmatrix}
        0 & 0 & \cdots & 0
    \end{bmatrix}$
    that differ in their first element.
    We can further construct
     sequences $x_2,\dots,x_N \in \sR^L \setminus \{1\}$ such that $\forall m > n > 1 : x_m \neq x_n \land x_1 \neq x_n \neq x_1'$ (so that our dataset is a proper set, i.e., does not have duplicates).
    Finally, we can define datasets $x = \{x_1,x_2,\dots,x_N\}$ and $x' = \{x'_1,x_2,\dots,x_N\}$.

    Due to top-level subsampling, the chance of $a_1$ appearing in any sampled subsequence reduces by a factor of $ 1 - \rho$.

    \textbf{Case 2 ($\alpha \geq 1$):}
    Here, we can interchange datasets $x$ and $x'$. The proof is analogous otherwise.
\end{proof}
Note that the lower bound coincides with the bound from~\cref{theorem:wor_top_level_wr} for $\numinstances=1$, which concludes our proof of tightness.

\begin{theorem}
    Consider
    top-level sampling without replacement, 
    bottom-level Poisson sampling, 
    number of subsequences $\numinstances = 1$ and 
    batch size $\batchsize$.
    Let $r = \min\{1, \numinstances \mathbin{/} (L - L_F + 1)\}$
    be the resulting Poisson sampling rate, 
    and let 
    $\rho = \lfloor \batchsize \mathbin{/} \numinstances \rfloor \mathbin{/} N$ be the probability of sampling any specific sequence. 
    Like in~\cref{theorem:deterministic_top_level_poisson}, define
    $\underline{P}(\numinstances) = \mog(-1 \cdot \vmu, \vp, \sigma)$ and 
    $\underline{Q}(\numinstances) = \mog(\vmu, \vp, \sigma)$ with
    means $\evmu_i = (i - 1)$ and 
    weights $\evp_i = \mathrm{Binomial}(i - 1 \mid L_C + L_F, r)$.
    Then, per-step privacy profile $\tilde{H}(\alpha) = \sup_{x \simeqevent{1} x'} H_\alpha(\tilde{M}_x || \tilde{M}_{x'})$ fulfills 
    \begin{equation*}
        H(\alpha) \geq 
        \begin{cases}
            H_\alpha((1 - \rho) \cdot \mathcal{N}(0,\sigma)  + \rho \cdot \underline{P}(\numinstances) || \mathcal{N}(0,\sigma)) & \text{if } \alpha \geq 1,\\
            H_\alpha(\mathcal{N}(0,\sigma) || (1 - \rho) \cdot \mathcal{N}(0,\sigma)  + \rho \cdot \underline{P}(\numinstances)  & \text{if } 0 \leq \alpha < 1.
        \end{cases}
    \end{equation*}
\end{theorem}
\begin{proof}
    The proof is, again, fully analogous for our optimistic lower bound for bottom-level subsampling, i.e.,~\cref{lemma:deterministic_top_level_poisson_lower}.
    Top-level sampling reduces the chance of sampling a non-zero element by a factor of $(1-\rho)$.
\end{proof}

\subsection{Dominating Pairs}
In this section, we have derived multiple bounds of the form
\begin{equation*}
    (1 - \rho) \cdot \max \{ 0, 1 - \alpha \} + \rho H_\alpha(P || Q).
\end{equation*}
Since this is a weighted sum of two valid privacy profiles, it naturally fulfills all necessary and sufficient conditions for privacy profiles~\cite{zhu2022optimal}, i.e.,
\privacyprofilerequirements*
Thus, we can use the same toolset for constructing corresponding dominating pairs as discussed in~\cref{appendix:bottom_level_dominating_pairs}, i.e., convex conjugation (\cite{zhu2022optimal}, see~\cref{lemma:dominating_pair_from_profile}) or ``connect-the-dots'' (\cite{doroshenko2022connect}, see~\cref{lemma:connect_the_dots}).