\section{Additional Related Work}\label{sec:additional-related-work}

Below we discuss additional related work in differential privacy and sequential data, and how our work differentiates from them. Note that there are privacy works outside differential privacy such as \cite{falcetta2022privacy,yue2021privacy, shi2011privacy} that use homomorphic encryption, but that is outside the scope of our paper. %\mina{should add a word about the papers that get time series as input just publish one output} 

\subsection{Differentially Private Time Series Release}

Publishing sanitized time series data has been the most studied application of differential privacy to temporally structured data. Most often,  the goal is to release event-level differentially private time series, which are often an aggregate statistic of multiple private time series \cite{shi2011privacy,fan2014adaptive, wang2016rescuedp, wang2020towards, zhang2017dpoctor, fioretto2019optstream, kellaris2014differentially, mao2024differential,katsomallos2019privacy}. The high level approach in most of the mentioned works is to sample time stamps, add noise and use these samples to generate non-sampled points. 
Here, sampling helps in improving utility by using the fact that they do not add noise to \emph{all} data points. However, the sampling is not used for reducing the sensitivity, i.e., privacy amplification.  Variants of this method include adaptive sampling based on an error estimate \cite{fan2014adaptive}, releasing DP time series over infinite time horizon~\cite{kellaris2014differentially}, and releasing histograms through time only when there has been a significant change \cite{li2015differentially}.
\citet{zhang2022differentially} uses learned autocorrelation in the data instead of subsampling to publish sanitized time series under continual observation. 

The fundamental difference between this class of work and our paper is that we are interested in training models in a DP manner rather than publishing sanitized data, and the fact that we subsample for the sake of privacy amplification.
%\mina{and hope to get better results by directly forecasting rather than forcasting on sanitized data?}


\subsection{DP-SGD for NLP/LLMs}

Time series data and text data are similar in their temporal structure, and for that we give an overview of most relevant works on differential privacy in natural language models and LLMs. Perhaps the best starting point is Table 1 in~\cite{hu2024differentially}, which lists over two dozen works that applied gradient perturbation (DP-SGD) for differentially private training in NLP.
All of these works are categorized as providing \emph{sample-level} or \emph{user-level} privacy (not to be confused with user-level privacy in time series). That  is, they consider natural language datasets as an unstructured set of atomic objects.
Works that explicitly consider the sequential structure within these objects (categorized as token-, word-, or sentence-private) exlusively use random input perturbations or ensemble-based methods.
Of course, this does not in any way mean that the resultant privacy guarantees are invalid or too pessimistic (under their considered neighboring relation).

These works include, for example, DP-SGD fine-tuning of LLMs~\cite{yue2022synthetic,carranza2023synthetic,lee2023private, wunderlich2022privacy}. There are also various works on DP federated learning for natural language learning models  \cite{mcmahan2017learning, ramaswamy2020training}.  See \cite{hu2024differentially} for a broader overview. 

\textbf{Bi-Level Subsampling for LLMs}
\citet{charles2024fine} and \cite{chua2024mind} both consider two specific algorithms for differentially private training in a setting where $N$ data holders each have an arbitrary number of records and one wants to ensure privacy for insertion or removal of a data holder. 
These two algorithms are referred to as DP-SGD-ELS and DP-SGD-ULS by~\cite{charles2024fine}
and ``Group Privacy'' and ``User-wise DP-SGD'' by~\cite{chua2024mind}.
In DP-SGD-ELS, one randomly samples a fixed number $G_\mathrm{ELS}$ of samples to construct a new composite dataset of $N \cdot K$ records.
This reduces the problem of fine-tuning with user-level privacy to that of DP-SGD training with group privacy~\cite{ganesh2024tight}.
In DP-SGD-ULS, one randomly samples a variable-sized set of users $U$ via Poisson sampling.
For each user in $U$, one then randomly samples $G_\mathrm{ULS}$ records, computes an average per-user gradient, clips the per-user gradient, accumulates them, and adds noise.
This reduces the problem of fine-tuning with user-level privacy to that of standard DP-SGD training, where one user behaves like one record in standard DP-SGD.
Importantly, the sampling of records from users only serves to bound their number to $G_\mathrm{ELS}$ or $G_\mathrm{ULS}$.
Equivalently, one could use a deterministic procedure that returns the first $G_\mathrm{ELS}$ or $G_\mathrm{ULS}$ records of each user.
Using our terminology, these works do not analyze any form of amplification attained via the randomness in their bottom-level sampling procedure.

Note that \emph{this is not a limitation in the considered setting of \citet{charles2024fine} and \cite{chua2024mind}}, as one has to make the worst case assumption that each inserted user has arbitrary worst-case records..
Our results on top- and bottom-level subsampling instead correspond to what is essentially user-level DP-SGD where we have a fixed number of users $N$ users, and $L_C + L_F$ records of a single user are substituted, meaning there is some chance of accessing non-substituted records.