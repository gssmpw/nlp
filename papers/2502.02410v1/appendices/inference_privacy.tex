\section{Inference Privacy}\label{appendix:inference_privacy}
As discussed in~\cref{appendix:inference_privacy},
our work and other works on DP-SGD (e.g.~\cite{abadi2016deep})
focus on ensuring privacy of parameters $\theta$ 
to guarantee that information from any training sample $x_n$ (here: sequences) does not leak when releasing model
$f_\theta$ or making predictions $f_\theta(x_m)$ (here: forecasts) for other data $x_m$.
However, as our data evolves over time, there may also be scenarios where one
wants to release a forecast $f_\theta(x_n)$ while simultaneously ensuring
that no sensitive information from $x_n$ is leaked.
Of course, the context window $L_C$ for generating this forecast will be much smaller than the size of an entire training set,
i.e., it will be harder to obfuscate any individual element with noise while retaining high utility.

A direct approach to the problem, which has already been explored in~\cite{li2019dp,arcolezi2022differentially}
is adding calibrated Gaussian noise to time series $x_n$
to ensure $(w,v)$-event- or $(w,v)$-user-level privacy when releasing $f_\theta(\tilde{x}_n)$ with noised time series $x_n$.

In the following, let us explore whether we can improve the privacy--utility trade-off of random input perturbations using amplification-by-subsampling.
For this purpose, we apply Theorem 3.2 from~\cite{koga2022privacy}, i.e., privacy amplification for time series release via Poisson subsampling.
Rather than using this mechanism for downstream analysis, let us use their mechanism to subsample our time series, and then impute the missing values with the remaining average, and then apply our model $f_\theta$ to the subsampled, noised, and reconstructed time series.

\textbf{Experimental Setup.}
We use the same hyperparameters as in~\cref{appendix:experimental_setup},
except for two changes: First, we omit DP-SGD training since we already enforce privacy at inference time.
Second, since our privacy is now independent of context length and batch size, we use 
we use $L_C = 8, \batchsize=64$ for \texttt{traffic}, $L_C = 2, \batchsize=64$ for \texttt{electricity},  and $L_C = 2, \batchsize=128$ for \texttt{solar\_10\_minutes}, which we found to lead to much better CRPS for non-private training during hyperparameter search on the validation set.

\cref{table:1_event_inference_traffic,table:1_event_inference_electricity,table:1_event_inference_solar}
show the resultant test CRPS with varying Poisson subsampling rate $r$ and Gaussian noise calibrated such that we attain the desired $\epsilon,\delta$.
On \texttt{traffic} and \texttt{solar\_10\_minutes} subsampling with $r=0.5$ or $r=0.75$ yields better CRPS for small $\epsilon$,
while there is no improvement on \texttt{electricity}.
This confirms that subsampling at inference time can in some circumstances improve utility while enforcing inference-time privacy for sequence $x_n$ when releasing forecast $f_\theta(x_n)$.
Of course, there are various opportunities for further improving utility, e.g., via neural denoising or imputation.

Again, \emph{this is not the primary focus of our work} (or any other work on DP-SGD), and we only included this discussion for completeness.


\begin{table}[h!]
\caption{CRPS for \texttt{traffic} when enforcing inference-time privacy with $v=0.1, \delta=1e^{-4}$.
Bold font indicates the best Poisson subsampling rate $r$ per model (collection of three rows).}
\label{table:1_event_inference_traffic}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ & $\epsilon = 8$\\
\midrule
SimpleFF ($r=1.0$) & $7.611$ \tiny{$\pm 1.714$} & $2.630$ \tiny{$\pm 0.311$} & $1.030$ \tiny{$\pm 0.029$} & $0.543$ \tiny{$\pm 0.017$} & $\mathbf{0.348}$ \tiny{$\pm 0.006$} \\
SimpleFF ($r=0.75$) & $6.367$ \tiny{$\pm 1.393$} & $2.164$ \tiny{$\pm 0.244$} & $0.880$ \tiny{$\pm 0.022$} & $0.502$ \tiny{$\pm 0.011$} & $0.352$ \tiny{$\pm 0.004$} \\
SimpleFF ($r=0.5$) & $\mathbf{5.521}$ \tiny{$\pm 1.430$} & $\mathbf{1.799}$ \tiny{$\pm 0.260$} & $\mathbf{0.748}$ \tiny{$\pm 0.020$} & $\mathbf{0.479}$ \tiny{$\pm 0.005$} & $0.381$ \tiny{$\pm 0.002$} \\
\midrule
DeepAR ($r=1.0$) & $1.754$ \tiny{$\pm 0.268$} & $1.026$ \tiny{$\pm 0.091$} & $0.685$ \tiny{$\pm 0.037$} & $0.522$ \tiny{$\pm 0.035$} & $\mathbf{0.425}$ \tiny{$\pm 0.034$} \\
DeepAR ($r=0.75$) & $1.376$ \tiny{$\pm 0.173$} & $0.856$ \tiny{$\pm 0.055$} & $0.622$ \tiny{$\pm 0.033$} & $0.512$ \tiny{$\pm 0.040$} & $0.445$ \tiny{$\pm 0.042$} \\
DeepAR ($r=0.5$) & $\mathbf{1.019}$ \tiny{$\pm 0.095$} & $\mathbf{0.709}$ \tiny{$\pm 0.035$} & $\mathbf{0.575}$ \tiny{$\pm 0.032$} & $\mathbf{0.510}$ \tiny{$\pm 0.043$} & $0.463$ \tiny{$\pm 0.047$} \\
\midrule
DLinear ($r=1.0$) & $4.262$ \tiny{$\pm 0.061$} & $2.251$ \tiny{$\pm 0.031$} & $1.221$ \tiny{$\pm 0.017$} & $0.707$ \tiny{$\pm 0.009$} & $0.442$ \tiny{$\pm 0.005$} \\
DLinear ($r=0.75$) & $3.473$ \tiny{$\pm 0.046$} & $1.863$ \tiny{$\pm 0.025$} & $1.044$ \tiny{$\pm 0.014$} & $0.637$ \tiny{$\pm 0.008$} & $\mathbf{0.430}$ \tiny{$\pm 0.005$} \\
DLinear ($r=0.5$) & $\mathbf{2.650}$ \tiny{$\pm 0.037$} & $\mathbf{1.463}$ \tiny{$\pm 0.020$} & $\mathbf{0.870}$ \tiny{$\pm 0.011$} & $\mathbf{0.580}$ \tiny{$\pm 0.007$} & $0.438$ \tiny{$\pm 0.004$} \\ 
\midrule
Seasonal ($r = 1.0$) & $7.871$ \tiny{$\pm 0.010$} & $4.273$ \tiny{$\pm 0.006$} & $2.358$ \tiny{$\pm 0.003$} & $1.350$ \tiny{$\pm 0.002$} & $0.827$ \tiny{$\pm 0.001$} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h!]
\caption{CRPS for \texttt{electricity} when enforcing inference-time privacy with $v=10, \delta=1e^{-4}$.
Bold font indicates the best Poisson subsampling rate $r$ per model (collection of three rows).}
\label{table:1_event_inference_electricity}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ & $\epsilon = 8$\\
\midrule
SimpleFF ($r=1.0$) & $\mathbf{0.060}$ \tiny{$\pm 0.060$} & $\mathbf{0.058}$ \tiny{$\pm 0.058$} & $\mathbf{0.057}$ \tiny{$\pm 0.057$} & $\mathbf{0.056}$ \tiny{$\pm 0.056$} & $\mathbf{0.056}$ \tiny{$\pm 0.056$} \\
SimpleFF ($r=0.75$) & $0.118$ \tiny{$\pm 0.118$} & $0.117$ \tiny{$\pm 0.117$} & $0.116$ \tiny{$\pm 0.116$} & $0.116$ \tiny{$\pm 0.116$} & $0.116$ \tiny{$\pm 0.116$} \\
SimpleFF ($r=0.5$) & $0.179$ \tiny{$\pm 0.179$} & $0.178$ \tiny{$\pm 0.178$} & $0.178$ \tiny{$\pm 0.178$} & $0.178$ \tiny{$\pm 0.178$} & $0.177$ \tiny{$\pm 0.177$} \\
\midrule
DeepAR ($r=1.0$) & $\mathbf{0.055}$ \tiny{$\pm 0.055$} & $\mathbf{0.053}$ \tiny{$\pm 0.053$} & $\mathbf{0.052}$ \tiny{$\pm 0.052$} & $\mathbf{0.051}$ \tiny{$\pm 0.051$} & $\mathbf{0.051}$ \tiny{$\pm 0.051$} \\
DeepAR ($r=0.75$) & $0.112$ \tiny{$\pm 0.112$} & $0.111$ \tiny{$\pm 0.111$} & $0.111$ \tiny{$\pm 0.111$} & $0.110$ \tiny{$\pm 0.110$} & $0.110$ \tiny{$\pm 0.110$} \\
DeepAR ($r=0.5$) & $0.169$ \tiny{$\pm 0.169$} & $0.168$ \tiny{$\pm 0.168$} & $0.168$ \tiny{$\pm 0.168$} & $0.168$ \tiny{$\pm 0.168$} & $0.168$ \tiny{$\pm 0.168$} \\
\midrule
DLinear ($r=1.0$) & $\mathbf{0.063}$ \tiny{$\pm 0.063$} & $\mathbf{0.059}$ \tiny{$\pm 0.059$} & $\mathbf{0.058}$ \tiny{$\pm 0.058$} & $\mathbf{0.057}$ \tiny{$\pm 0.057$} & $\mathbf{0.057}$ \tiny{$\pm 0.057$} \\
DLinear ($r=0.75$) & $0.125$ \tiny{$\pm 0.125$} & $0.122$ \tiny{$\pm 0.122$} & $0.122$ \tiny{$\pm 0.122$} & $0.121$ \tiny{$\pm 0.121$} & $0.121$ \tiny{$\pm 0.121$} \\
DLinear ($r=0.5$) & $0.186$ \tiny{$\pm 0.186$} & $0.184$ \tiny{$\pm 0.184$} & $0.184$ \tiny{$\pm 0.184$} & $0.183$ \tiny{$\pm 0.183$} & $0.183$ \tiny{$\pm 0.183$} \\ 
\midrule
Seasonal ($r=1.0$) & $0.079$ \tiny{$\pm 0.000$} & $0.074$ \tiny{$\pm 0.000$} & $0.071$ \tiny{$\pm 0.000$} & $0.070$ \tiny{$\pm 0.000$} & $0.070$ \tiny{$\pm 0.000$} \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h!]
\caption{CRPS for \texttt{solar\_10\_minutes} when enforcing inference-time privacy with $v=1, \delta=1e^{-4}$.
Bold font indicates the best Poisson subsampling rate $r$ per model (collection of three rows).}
\label{table:1_event_inference_solar}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
Model & $\epsilon = 0.5$ & $\epsilon = 1$ & $\epsilon = 2$ & $\epsilon = 4$ & $\epsilon = 8$\\
\midrule
SimpleFF ($r=1.0$) & $3.999$ \tiny{$\pm 2.246$} & $1.791$ \tiny{$\pm 0.588$} & $1.102$ \tiny{$\pm 0.163$} & $0.897$ \tiny{$\pm 0.076$} & $\mathbf{0.836}$ \tiny{$\pm 0.079$} \\
SimpleFF ($r=0.75$) & $3.280$ \tiny{$\pm 1.701$} & $1.457$ \tiny{$\pm 0.401$} & $\mathbf{0.971}$ \tiny{$\pm 0.093$} & $\mathbf{0.868}$ \tiny{$\pm 0.069$} & $0.850$ \tiny{$\pm 0.073$} \\
SimpleFF ($r=0.5$) & $\mathbf{3.022}$ \tiny{$\pm 1.723$} & $\mathbf{1.441}$ \tiny{$\pm 0.386$} & $1.032$ \tiny{$\pm 0.097$} & $0.936$ \tiny{$\pm 0.045$} & $0.921$ \tiny{$\pm 0.034$} \\
\midrule
DeepAR ($r=1.0$) & $2.483$ \tiny{$\pm 1.071$} & $\mathbf{1.361}$ \tiny{$\pm 0.299$} & $\mathbf{0.892}$ \tiny{$\pm 0.078$} & $\mathbf{0.712}$ \tiny{$\pm 0.024$} & $\mathbf{0.641}$ \tiny{$\pm 0.020$} \\
DeepAR ($r=0.75$) & $2.257$ \tiny{$\pm 0.734$} & $1.405$ \tiny{$\pm 0.321$} & $1.021$ \tiny{$\pm 0.146$} & $0.873$ \tiny{$\pm 0.107$} & $0.804$ \tiny{$\pm 0.087$} \\
DeepAR ($r=0.5$) & $\mathbf{2.123}$ \tiny{$\pm 0.681$} & $1.565$ \tiny{$\pm 0.403$} & $1.304$ \tiny{$\pm 0.277$} & $1.206$ \tiny{$\pm 0.228$} & $1.167$ \tiny{$\pm 0.212$} \\
\midrule
DLinear ($r=1.0$) & $2.792$ \tiny{$\pm 0.154$} & $1.783$ \tiny{$\pm 0.059$} & $\mathbf{1.289}$ \tiny{$\pm 0.022$} & $\mathbf{1.043}$ \tiny{$\pm 0.017$} & $\mathbf{0.918}$ \tiny{$\pm 0.021$} \\
DLinear ($r=0.75$) & $2.443$ \tiny{$\pm 0.157$} & $1.663$ \tiny{$\pm 0.085$} & $1.305$ \tiny{$\pm 0.061$} & $1.150$ \tiny{$\pm 0.049$} & $1.088$ \tiny{$\pm 0.041$} \\
DLinear ($r=0.5$) & $\mathbf{2.187}$ \tiny{$\pm 0.123$} & $\mathbf{1.640}$ \tiny{$\pm 0.089$} & $1.419$ \tiny{$\pm 0.073$} & $1.340$ \tiny{$\pm 0.062$} & $1.315$ \tiny{$\pm 0.057$} \\ 
\midrule
Seasonal ($r=1.0$) & $9.461$ \tiny{$\pm 0.113$} & $5.485$ \tiny{$\pm 0.059$} & $3.433$ \tiny{$\pm 0.029$} & $2.371$ \tiny{$\pm 0.014$} & $1.816$ \tiny{$\pm 0.008$} \\ 
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
