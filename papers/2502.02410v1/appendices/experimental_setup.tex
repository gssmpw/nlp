\section{Experimental Setup}\label{appendix:experimental_setup}

\subsection{Datasets}
We use the \texttt{traffic}, \texttt{electricity}, and \texttt{solar\_10\_minutes} dataset 
as originally preprocessed by 
\cite{Lai2018modeling}.
We use the standard train--test splits as per GluonTS version $0.15.1$ and 
 additionally remove the last $5 \cdot L_F$ forecast windows of each train set sequence for validation.

\textbf{Traffic.} The \texttt{traffic} dataset was originally sourced from the following domain: \href{http://pems.dot.ca.gov/}{http://pems.dot.ca.gov/}. It consists of hourly measurements from $862$ traffic sensors, with each time series covering $17544$ hours.
The forecast length $L_F$ is $24$. 
Although our experiments are mostly focused on verifying that our differentially private models can fit some non-trivial time series, 
traffic data may allow inference about personal movement profiles~\cite{giannotti2008mobility}.

\textbf{Electricity.} The \texttt{electricity} dataset was originally sourced from the following domain:
\href{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}. It consists of hourly measurements from $321$ electricity consumers, with each time series covering $26304$ hours.
The forecast length $L_F$ is $24$. 
As before, the contribution of this work is mainly theoretical and the specific application domain is mostly irrelevant. The \texttt{electricity} dataset just happens to be commonly used for testing whether models can fit non-trivial time series.

\textbf{Solar.} The \texttt{solar\_10\_minutes} dataset was originally sourced from the following domain:
\href{http://www.nrel.gov/grid/solar-power-data.html}{http://www.nrel.gov/grid/solar-power-data.html}. It consists of $6$ measurements per hour from $137$ photovoltaic power plants, with each time series covering  $52560$ $10$-minute intervals.
The forecast length $L_F$ is $60$. 
The authors of this work do not claim that solar electricity production or position of the sun required any form of privacy protection.

\subsection{Models}

\subsubsection{Deep Learning Models}

For all models, we use the standard hyper-parameters as per GluonTS version $0.15.1$.
Per forecast step, we let the model parameterize a $t$-distribution for probabilistic forecasting. 
The only parameter we vary is the context length or the range of lagged values, as  these affect our privacy analysis (see~\cref{appendix:setup_dp_training_parameters}).
In particular, we use the following parameters per model:

\textbf{Simple Feed Forward.} We use two hidden layers with hidden dimension $64$ and no batch normalization.

\textbf{DeepAR.} We use two hidden layers with hidden size $40$, no dropout, no categorical embeddings, and activated target scaling.

\textbf{iTransformer.} We use latent dimension $32$, $4$ attention heads, $128$ feed-forward neurons, no droput, ReLU activations, $2$ encoder layers, and activated mean scaling.

\textbf{DLinear.} We use hidden dimension $20$ and kernel size $25$.

\subsubsection{Traditional Baselines}

\textbf{Seasonal Na\"ive.} We use season a season length of $24$ ($1$ day) for \texttt{traffic} and \texttt{electricty}.
We use a season length of $144 = 24 \cdot 6$ ($1$ day) for \texttt{solar\_10\_minutes}.

\textbf{AutoETS.} We use the standard implementation and parameters from \texttt{statsforecast}~\cite{garza2022statsforecast} version 1.7.8.\ We additionally set the season lengths to the same values as with the seasonal na\"ive predictor.

\textbf{AutoARIMA.} We attempted to also to use AutoARIMA with the standard implementation and parameters from \texttt{statsforecast} version 1.7.8.\ and with the above season lengths.
However, the computation did not complete after $7$ days on an AMD EPYC 7542 processor with $256$GB RAM on any of the datasets. 
Other works on time series forecasting also report ``d.n.f.'' for AutoARIMA, e.g.,~\cite{alexandrov2019gluonts,shchur2023autogluon}.
Without specifying season lengths, AutoARIMA had a CRPS of $0.472$, $0.313$, and $7.193$ on the three datasets, i.e., it performed significantly worse than seasonal na\"ive prediction or AutoETS with appropriate season lengths.

\subsection{Metrics.}\label{appendix:metrics}
All experiments involving model training are repeated for $5$ random seeds. We report means and standard deviation.
As our training and validation loss, we use negative log likelihood.

For evaluating predictive performance, we use the Continuous Ranked Probability Score (CRPS),
which is a proper scoring function~\cite{gneiting2007strictly}. 
Given cumulative distribution function $F$ and ground-truth $y \in \sR$, it is defined as
\begin{equation*}
    \mathrm{CRPS}(F^{-1}, y) = -1 \cdot \int_{-\infty}^{\infty} (F(x) - \indicator [x \geq y] \ \dd x .
\end{equation*}
Like prior work, e.g.,~\cite{rasul2021multivariate,chen2024recurrent,kollovieh2024predict}, and as implemented by default in GluonTs, 
we approximate it using quantile levels $\{0.1, 0.2,\dots, 0.9\}$ (``Mean weighted quantile loss''). 
Note that this loss is $0$ for a Dirac-$\delta$ coinciding with ground-truth $y$, i.e., non-probabilistic models can also achieve a loss of $0$.

\subsection{Training}
On all datasets, we train until reaching the prescribed privacy budget or some very liberally set maximum number of epochs that allows all models to train to convergence (details below).
After training, we load the checkpoint with the lowest validation log-likelihood.

\subsubsection{Standard Training Parameters}
We use ADAM with learning rate $10^{-3}$ and weight decay $10^{-8}$ for all models and datasets.
\texttt{traffic}, \texttt{electricty}, and \texttt{solar\_10\_minutes}
we train for $4000$, $8000$, and $16000$ epochs, respectively.

\subsubsection{DP Training Parameters}\label{appendix:setup_dp_training_parameters}
We wrap the optimizer and models using the privacy engine (minus the accountant) from \texttt{opacus}~\cite{opacus} (version $1.5.1$),
replacing all recurrent and self-attention layers with their DP-compatible PyTorch-only implementation.
Following~\cite{ponomareva2023dp}, we iteratively decreased the gradient clipping norm $C$ until the validation loss without gradient noise increased, which led us to using $C = 10^{-4}$ on all datasets.

\textbf{Traffic.} We use batch size $\batchsize=256$, noise multiplier $\sigma = 4.0$, and $L_C = 4 \cdot L_F$.
For DeepAR, we use the following lag indices, which contribute to the context length $L_C$:
$[1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73,
            95, 96, 97, 119, 120, 121, 143, 144, 145, 167, 168, 169]$.

\textbf{Electricity.} We use batch size $\batchsize=128$, noise multiplier $\sigma = 2.0$, and $L_C = 1 \cdot L_F$.
For DeepAR, we use the following lag indices, which contribute to the context length $L_C$:
$[1, 2, 3, 4, 5, 6, 7, 23, 24, 25]$.

\textbf{Solar.} We use batch size $\batchsize=128$, noise multiplier $\sigma = 4.0$, and $L_C = 4 \cdot L_F$.
For DeepAR, we use the following lag indices, which contribute to the context length $L_C$:
$[1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73,
            95, 96, 97, 119, 120, 121, 143, 144, 145, 167, 168, 169]$.


\subsubsection{Privacy Accounting Parameters}
We use
privacy loss distribution accounting as implemented in the 
the Google \texttt{dp\_accounting} library~\cite{dpaccountinglibrary} (version $0.4.4$)
with a tail mass truncation constant of  $10^{-15}$.
We quantize privacy loss distributions
using ``connect-the-dots''~\cite{doroshenko2022connect} with a value discretization interval of $10^{-3}$. 