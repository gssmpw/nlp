\section{Related Work}
\label{sec:related-work}


Query rewrite or reformulation (QR) includes various techniques aimed at addressing underspecified and ambiguous queries by transforming them into more detailed and self-contained versions that are appropriate for retrieval or question-answering systems. Traditionally, QR involves adding terms to the original query, known as query expansion \cite{lavrenko2017relevance}, rephrasing the query with similar phrases \cite{zukerman2002lexical}, or using synonymous terms \cite{jones2006generating}. When human-generated rewrites or reward signals are available, language models (LMs) are also trained specifically for question rewriting (QR) \cite{elgohary2019can, anantha2021open, vakulenko2021question, qian2022explicit, ma2023query}. In conversational search, query reformulation is employed to manage conversational dependencies. \cite{anantha2021open} introduce the rewrite-then-retrieve pipeline, which relies on a human-crafted dataset \cite{elgohary2019can}. Many studies fine-tune QR models to generate standalone questions \cite{yu2020few, voskarides2020query, lin2021multi, kumar2020making, wu2022conqrr}.

Recently, the emergence of large language models (LLMs) has generated interest in utilizing these generative models to automatically resolve ambiguities during query processing, thereby enhancing query modeling for downstream tasks. These tasks frequently aim to improve information retrieval in a single question answering setting. For example, recent studies have prompted LLMs to generate detailed information, such as expected documents or pseudo-answers \cite{wang2023query2doc, jagerman2023query, anand2023context}. These methods are particularly useful when a high-quality dataset for a specific domain is unavailable, necessitating the employment of off-the-shelf LLMs customized for the particular use-case. 

While these techniques significantly mitigate the issue of the extensive training data required for dedicated model training, prompting large language models (LLMs) for query rewriting introduces its own set of challenges. For instance, \citet{anand2023context} identified that LLM-based query rewriting can experience concept drift, deviating from the original intention or meaning when queries alone are used as prompts. This issue becomes particularly pronounced in real-world enterprise systems, where generic terms can have specific, context-dependent meanings. Our observations confirm this phenomenon (e.g., ``people" might refer to a specific metric or dimension in a chart, while ``segment" could denote a particular data object within the system), leading to the conclusion that there is no universal solution applicable to all use cases. Instead, practitioners must carefully consider their specific use case,
identifying the taxonomy and nature of the queries to design the most effective rewrite strategies. To this end, we present best practices for rewrite strategies and, through two distinct use cases within the same enterprise setting, demonstrate how these practices lead to optimal results.