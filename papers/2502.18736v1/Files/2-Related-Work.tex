\section{Related Work}

We first discuss the state of human-AI interaction, articulating it around five core challenges. Then, we motivate the need for a more general interaction model and point to research in design and creativity grounding two principles we introduce.



\subsection{Human-AI Interaction}
\label{sec:challenges}
Recent work explores the difficulties users face when interacting with generative AI via prompting~\cite{zamfirescu-pereira_why_2023, subramonyam_bridging_2024, mahdavi_goloujeh_is_2024, peng_designprompt}. Earlier research identified barriers that arise (for example) in end-user programming~\cite{ko2004six} and, more generally, bridging the gulf of execution and the gulf of evaluation ~\cite{norman1986cognitive}. We organize emerging research for interaction with generative AI under five core Challenges (C1-C5) faced by users, and discuss later in the paper how our interaction model addresses each.


\vspace{5pt}\textit{\textbf{(C1) Intent formulation}} via prompting, solely using natural language, can be challenging when the outcome is hard to describe in words. 
Users may lack the vocabulary to describe visual styles, or the high-level impressions they seek to achieve. Researchers studied thousands of prompts to generate images~\cite{liu_design_2022} to develop guidelines for prompting and parameter selections. They coupled prompting with images to offer richer multimodal intent formulation. PromptCharm~\cite{wang_promptcharm_2024} leveraged a large image database to help users find the right style of images and incorporated interactive techniques -- such as linking a prompt fragment to the corresponding part of the generated image, to provided richer solutions for users to formulate their intent. Similarly, DesignPrompt~\cite{peng_designprompt} affords expressive multi-modal prompt construction. Such research seeking to expand the modalities we have to communicate with models beyond text input is particularly important~\cite{liu_beyond_2023} for multimodal outputs such as generated videos~\cite{villegas2022phenaki}, 3D objects~\cite{poole2022dreamfusion}, and virtual worlds~\cite{rosenberg_drawtalking_2024}. 

\vspace{5pt}\textit{\textbf{(C2) Intent disambiguation}} is the skill of describing one's intent with enough specific detail for AI to produce the intended result. Much past work on \textit{prompt engineering} across several fields of research tackles this challenge, with research probes of this issue ~\cite{zamfirescu-pereira_why_2023} suggesting templates and guidelines~\cite{bozkurt_tell_2024} for users to provide the information they might have difficulty thinking about upfront. Beyond prompt engineering, the HCI community explores different representations to facilitate communicating context to the system. For example, Graphologue~\cite{jiang_graphologue_2023} represents a prompt as an interactive node-link diagram that users can expand and complete to incrementally add context to their intent. Such work also addresses the ambiguity of natural language by enabling users to unpack certain parts of their intent and disambiguate them by adding more information. \rev{Promptify~\cite{brade_promptify_2023} organizes generated content on a canvas based on a person's preferences and suggests alternatives -- leading to an iterative loop with the user refining, selecting, and discarding alternatives of prompts and content.}

\vspace{5pt}\textit{\textbf{(C3) Intent resolution}} is the challenge users confront to determine what outcomes may or may not match their original intent. Difficulties here may stem from an ambiguity of intent in the users' mind (e.g. a user might realize \textit{"I am not even sure what exact outcome I want"}). This problem, as a well-known attribute of challenging creative design work ~\cite{DesignReflectiveConversation1992, ReflectivePhysicalPrototyping2006, BuxtonSketchingUserExperiences2007}, is certainly not unique to AI but may be exacerbated by the relative novelty, black-box nature, and rapidly accelerating capabilities of modern AI models~\cite{bubeck2023sparks}. 
However, further difficulties may arise from people's lack of knowledge of what an AI model can or cannot do. 
Here, approaches from graphic design may help users explore possible outcomes, such as CreativeConnect~\cite{choi_creativeconnect_2024}, which extracts keywords, and text descriptions from a set of reference images and facilitate recombination and reuse. Other work shows the possibilities of what users can ask via prompt-space exploration~\cite{almeda_prompting_2024}, or through interfaces that reveal what results a user can generate~\cite{suh_luminate_2024}.

\vspace{5pt}\textit{\textbf{(C4) Steering}} the result of generative AI to get closer to either what the user initially imagined or to an unforeseen result assessed as satisfactory is a fundamental human-AI interaction mechanism. The topic has been studied for multiple decades in multiple field and referred to as human-in-the-loop~\cite{wu2022survey} and mixed-initiative interfaces~\cite{horvitz1999principles}. Within the context of generative AI, research on the topic has centered on human-AI co-creation~\cite{davis2015enactive}. Researchers developed human-AI co-creation interfaces for specific activities such as drawing~\cite{oh_i_2018}, crafting images~\cite{chung_promptpaint_2023} and writing stories~\cite{chung_talebrush_2022, zhang_storydrawer_2022}. These interfaces either surface generative AI capabilities as graphical interface elements such as a button to generate a character for a story~\cite{zhang_storydrawer_2022}, or propose custom graphical widgets to specify constraints or parameters of the content to be generated by the model such as an interactive line chart depicting the narrative arc of the story~\cite{chung_talebrush_2022}.

Recent research has begun to explore more generic interaction solutions to the prompting chat-based experiences incorporated in most mainstream products today. Steering content generation in conversational prompting amounts to a linear trial-and-error process, in which users type a prompt, and then evaluate its result. They then must either rerun the same prompt to get a new result (since generative AI is non-deterministic); or edit the prompt to get an iteration over the prior result. By building upon principles of \textit{direct manipulation}, DirectGPT~\cite{masson_directgpt_2024} offers an early glimpse of an alternative interaction human-AI co-creation paradigm based on the principles of direct manipulation and surfaced to users with graphical widgets (e.g. buttons) that might generalize to a wider range of outputs and applications. Our research extends this ambition via instrumental interaction~\cite{beaudouin2000instrumental}, yielding a novel interaction model that can provide the community with both evaluative (assessing novel interaction techniques) and generative (inspiring the design of novel interaction techniques) power. 

\vspace{5pt}\textit{\textbf{(C5) Interaction workflow}} models based on conversation with generative AI are inherently linear.  Research started to investigate non-linear interaction workflows with generative AI. In particular, DeckFlow~\cite{croisdale_deckflow_2023} relies on mood board type interaction and also breaks the silo of different models. Sensecape~\cite{suh_sensecape_2023} and Graphologue~\cite{jiang_graphologue_2023} leverage additional non-linear metaphors to enable people to perform non-linear interactions with AI.
\rev{These systems focus on a specific metaphor for conversation with LLMs, laying out prompts and responses as a graph in a canvas.}
\rev{Graph structures can also function as an intermediary representation facilitating prompt steering, by breaking down text prompts into hierarchical structures of granular elements \cite{xcreation}. Similarly, tree structures enable traversing alternative representations of generated content, where sub-nodes represent distinct visual aspects across the latent space \cite{conceptdecomposition}. Our intent is to identify general principles that afford direct manipulation for decomposing and (re)composing  objects at multiple levels of granularity for different tasks and contexts.}

The interaction model we propose provides a generic solution to address these 5 challenges by building upon the instrumental interaction model and apply it to the context of building interactive applications leveraging generative AI capabilities. 


\subsection{Interaction Models in the Era of AI}

Despite tremendous advances in technology and the promise of Artificial General Intelligence~\cite{bubeck2023sparks}, mainstreams interfaces today feature a chat-based interface with AI reminiscent of command-line human-computer interaction paradigm of the 1960s. While the use of natural language does remove barriers of adoption for the general public, many of the limitations of communicating instructions in a linear and sequential manner by typing, later addressed by Graphical User Interfaces, pertain. 


Over the years, the HCI community has produced knowledge on human-computer interaction~\cite{hornbaek2017interaction}, devised principles and theories for improving interaction~\cite{hutchins1985direct,  gibson1977theory}, and proposed multiple interaction models~\cite{beaudouin2000instrumental,jacob2008reality} for building the next generation of interfaces. These models are generally grounded in the emerging interfaces and techniques of the time, surfacing key principles governing them and desirable properties when humans interact with them. The goal of these models is to inform and assess the design of the next generation of interfaces. Our work has the same ambition: \textit{informing and guiding the design of interfaces leveraging generative AI}. While numerous recent work centered on advancing specific use cases and application areas -- seeking to identify and leverage the value of generative AI -- few researchers relate to existing theory and models, or proposing \textit{new theories and models} in this era of AI. \rev{Perhaps the closest effort is the Cells, Generators, and Lenses model proposed by Kim et al.~\cite{kim2023cells} which proposes a design framework for helping designers identify and reflect on basic building blocks needed for interfaces leveraging AI}. \rev{Our research is complementary to this effort, seeking to identify interaction principles that afford direct manipulation of these building blocks}. 


Our work seeks to build upon and extend the instrumental interaction model to the design of generative AI interfaces. The instrumental interaction model~\cite{beaudouin2000instrumental} directly builds upon direct manipulation and generalizes the use of instruments to mediate between user and objects of interests (e.g. content). It describes a large range of interaction techniques that were not captured in WIMP and direct manipulation such as lenses or tangible interactions. \rev{Recent work attempted to leverage the instrumental interaction model to design novel interactions with AI. For example, Yen and Zhao~\cite{yen2024memolet} used reification to turn prior conversations with AI into graphical objects or Memolets, that users can interact with.
Our work propose a more general adaptation of this model to content generation with AI and expands it with additional principles of reflection and grounding. The resulting model we propose falls into generative theories of interaction~\cite{beaudouin2021generative}, aiming at inspiring and informing the design of novel techniques.}






\subsection{Content Generation and Creativity Support}


Several key insights from the design and creativity support literature~\cite{shneiderman_creativity_2007} motivate our principles of reflection and grounding.

Design and creativity processes embrace ambiguity of low-fidelity prototypes~\cite{BuxtonSketchingUserExperiences2007} and rapid cycles of idea generation and evaluations~\cite{TEAMSTORM2007, TerryCreativeNeedsUIDesign2002, ReflectivePhysicalPrototyping2006} to enable people to explore many design alternatives, reflect on their possibilities through the action of sketching and building, and iterate on the most promising ones ~\cite{DesignReflectiveConversation1992}. Researchers have also described these processes as sequences of divergent thinking followed by convergent thinking~\cite{farooq2005supporting}. As fundamental working-patterns that people exhibit in challenging content creation and design tasks, there is good reason to believe such processes should persist and be supported by tools for creative content generation with generative AI. Such tools should help users rapidly investigate alternatives ideas in fluid, non-linear manner (e.g. exploration) and support the rapid iteration of the most promising content (e.g. steering). Direct manipulation and instrumental interaction models offer a compelling point of departure, affording chunking and phrasing~\cite{buxton1995chunking} of complex generative-AI interactions for exploration and steering.


As hinted above, our principle of reflection builds on Schön's notion of \textit{reflection-in-action} 
~\cite{DesignReflectiveConversation1992}---where the externalized materials of design "speak to" the designer to help them reflect-\textit{on}-action as to the next design "move" to make within an ambiguous space of many possible ideas. 
In the context of generative AI, this principle of ~\textit{reflection} conveys the notion that instruments should reflect the design space of user intent---as well as the wide potential space of generated results---to help users make informed decisions ("moves") as they iterate towards a desired (AI-assisted) outcome. A related concept to reflection and idea incubation is the process of gathering inspirational materials in moodboards~\cite{cassidy2008mood, BuxtonSketchingUserExperiences2007}. Such design practices help identify concepts and themes, especially when these are hard to articulate in words, or isolate from one another other~\cite{freeman2017creativity}. We refer to this activity in our principle of grounding, to convey the idea that instruments can extract specific aspects from a set of materials, and to then apply them to different content.

\rev{An aspiration for an interaction model geared on content generation is to afford power to their users~\cite{li2023beyond}. In particular, vertical movement (moving up and down the abstraction ladder) afforded by natural language input of LLMs; and horizontal movement (composing tools and workflows) afforded by combining instruments together offer promising avenues for AI-instruments.}






\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/walkthrough-part1-v3.png}
      \caption{\rev{Sequence of interactions to explore ideas with generative containers and lens probes: When dragging an image into a container (1), variations are created based on \textit{style} (2). When selecting one of these images and dragging it into another container with the prompt "different types of bird", variations of different kinds of birds are generated in a consistent art style (3). A transformative lens around one of the earlier images generates a landscape around the bird through inpainting (4), and allows more complex composition of content (5, 6).}}
      \Description{Screenshots of the sequence of interactions to explore ideas with generative containers and lenses probes. The figure shows a number of wireframes that illustrate the interaction sequence. 1. dragging an image into a container, 2. variations are created based on style and one image of that one is dragged into another container. 3. Variations of different kinds of birds are generated in a consistent art style. 4. A transformative lens adds a frame around one of the earlier images and shows how the inpaint method generates a landscape around the bird. 5. a more complex images is composed with the help of a transformative lens. 6. a final colorful image of the bird is shown as the final result of the pipeline.}
    \label{fig:walkthrough1}
\end{figure*}
\begin{figure*}
    \centering
   \includegraphics[width=1.0\linewidth]{Figures/walkthrough-part2-v3.png}
      \caption{\rev{Sequence of interactions to steer image generation with fragments and brushes probes: Prompt fragments are generated for an existing image and show dimensions of the image to manipulate (1). A person can modify any of these fragments and a new image is generated (2). Containers can generate variations of fragments, which are then used to modify the image (3). Fillable Brushes (pen-like instruments) are used to modify the image of a castle, changing the art rendering style and color where the brush painted over the image, based on the prompt that was 'filled' into the pen (4, 5).}}
      \Description{Screenshots of the sequence of interactions to steer image generation with fragments and brushes probes. Different software wireframes show the interactions with the AI-Instruments. 1. an image is shown, with a number of text boxes on the right side which represent the 'fragments' the AI extracted. 2. A person then modifies one of these fragments and a different image is generated. 3. a container generates variations of fragments, which are then used to modify the image. 4. Fillable Brushes (pen-like instruments) are used to modify the image of a castle, changing the art rendering style and color where the brush painted over the image.}
    \label{fig:walkthrough2}
\end{figure*}

