



\section{Example Walkthrough}

Let us take the example of Emma, who is seeking to illustrate a social media post to express the serenity she feels when she spends time outdoors (Figure~\ref{fig:walkthrough1}). She starts from leveraging generative AI to generate a bird. The art style is not quite satisfying but she is not sure what the model is capable of. She selects a generative container from a panel of AI-instruments available to her (\rev{Figure~\ref{fig:walkthrough1}.}1 and 2) and explores different art styles. She finds a simple drawing style she likes, and creates a second generative container to explore other types of birds in the same style (\rev{Figure~\ref{fig:walkthrough1}.}3). She settles on a heron, and moves on composing a more interesting illustration. Since she has an idea of the general composition she wants, she opts for a transformative lens, a second AI-instrument enabling her to position her central character, the bird, in the frame (\rev{Figure~\ref{fig:walkthrough1}.}4). She creates multiple lenses to try multiple backdrops and settles on the forest one (\rev{Figure~\ref{fig:walkthrough1}.}5). As lenses can be layered, she creates a color style one, and layers it on top of the forest backdrop, resulting in an illustration she finds suitable for her post (\rev{Figure~\ref{fig:walkthrough1}.}6).

Two days later, Emma seeks to illustrate her school presentation on medieval castles (Figure~\ref{fig:walkthrough2}). She starts from a drawing generated by AI. She taps-and-holds to expand the fragments the model used to generate the image (\rev{Figure~\ref{fig:walkthrough2}.}1). By tapping on different fragments, she gets to try different variations, such as redrawing the castle as watercolor style (\rev{Figure~\ref{fig:walkthrough2}.}2). As she wants to add color to the illustration, Emma retrieves the palette where she saved multiple fragments related to colors she thought worked great in the past (\rev{Figure~\ref{fig:walkthrough2}.}3), and drags one onto the image. She does like the colors but notices a large white space in the back. She selects a fillable brush, \rev{an AI-instrument that lets} her directly scrub over the specific portions of the image that she wants to revise or refine (\rev{Figure~\ref{fig:walkthrough2}.}4). After she types the outcome she wants and brushes the region, the system generates a mask and applies changes locally (\rev{Figure~\ref{fig:walkthrough2}.}5). Emma is now satisfied with her illustration.





\section{Instrumental Interaction with AI}

Beaudoin-Lafon defines instruments as: \textit{"a mediator or two-way transducer between the user and domain objects."} we expand this definition to AI-instruments: \textit{"an AI-powered mediator or two-way transducer between the user and domain objects."} We describe below the three principles of our proposed model revision: reification of user intent, reflection and grounding. Note that these principles are tightly interconnected and, while differing in certain aspects from the original model also share a lot of similarities. We discuss differences in more depth in Discussion.



\subsection{Reification of User Intent}

Most pre-AI interfaces offer a finite set of functionalities, established at their design by software architects and developer. User experience designers craft a set of graphical interface components and interactions for each functionality to enable users to invoke a finite set of commands through this GUI. Today, LLMs can interpret requests from users in natural language and turn them into the execution of a specific command, or a sequence of commands, unbounding functionalities from a limited set of GUI components. With this major shift in interface design, we propose the reification of \textbf{user intent}, rather than \textbf{commands}.


Reification turns both input and output of generative AI into graphical elements that can be directly manipulated and thus reused by users. In contrast to chat-based interfaces consisting of sequences of [input+output] in \rev{which users can require} to rephrase the input to iterate, reifying input and output enables users to articulate phrases of interaction~\cite{buxton1995chunking} and afford direct manipulation techniques such as lasso selections to specify scopes of intent (Figure~\ref{fig:reification} (1-3)).
In section~\ref{sec:Ai-instruments}, we demonstrate how this instrumental model can leverage the full range of direct manipulation techniques the community developed such as magic lenses~\cite{bier2023toolglass} and attribute objects~\cite{xia2016object}, turning them into AI-instruments encapsulating user intent.




A key capability of generative AI models is their inherent ability to deal with the \textbf{degree of abstraction} of user intent. It offers unparalleled flexibility as users can express high-level or low-level intent. Examples in the literature leverage the high degree of abstraction for content generation. For example, Talebrush~\cite{chung_talebrush_2022} enables users to control the narrative arc of a story (where tension is in the story), which has many implications on the writing itself from adding or sequencing events differently in the story to subtly rewording the language.  Expressing high-level intents is a powerful ability, enabling people to shape content 
in ways that potentially lead to serendipitous discovery of alternative (potentially better) results. However, users face multiple challenges when results are unsatisfactory, understanding how to resolve ambiguity of their intent (C2) or thinking more crisply of the desired outcome (C3). These challenges often require users to lower the degree of abstraction of their intent. On the contrary, expressing intents with a low degree of abstraction lowers the chance to make serendipitous discoveries, as well as get into a class of unwanted model results, making it frustrating for users to steer content generation towards more major changes (C4) or conduct exploratory workflows (C5). These challenges often require users to increase the degree of abstraction of their intent. Figuring out how to navigate degrees of abstraction is a challenge in itself. Users may struggle turning an idea into a set of concrete changes or, conversely, articulate the overarching goal motivating specific changes. Users can leverage AI-instruments themselves to navigate the degree of abstraction of an intent, for example, by using a Generative Container to provide more concrete (resp. abstract) Fragments given one of high-degree (resp. low-degree) of abstraction (Figure~\ref{fig:reification} (4-5)).


\begin{figure}[t]
    \centering
\includegraphics[width=.48\textwidth]{Figures/reification3.png}
\caption{In the chat-based interaction model, interactions consists of a linear sequence of input+output pairs and steering is done by modifying the input (1). Reification enables articulating interactions into phrases for example by reusing the output of the prior input (2). It also affords direct manipulation techniques such as for lasso selection (in red) to specify the scope of the input (3). Reification of user intent enables users to reflect on their intent and navigate dimensions such as its degree of abstraction, using other instruments to make it more concrete (4) or abstract (5) for example.}
\Description{Screenshots of wireframes and examples illustrating the 'reification of user intent'. In the classic chat-based interaction model, interactions consists of a linear sequence of input+output pairs and steering is done by modifying the input (1). Reification enables articulating interactions into phrases for example by reusing the output of the prior input (2). It also affords direct manipulation techniques such as for lasso selection (in red) to specify the scope of the input (3). Reification of user intent also enables users to reflect on their intent and navigate dimensions such as its degree of abstraction, using other instruments to make it more concrete (4) or abstract (5) for example.}
    \label{fig:reification}
\end{figure}


\subsection{Reflection}


Seminal research demonstrated that it is critical to explore alternative designs early and throughout the whole process~\cite{tohidi2006getting, DesignGalleries1997}. It is particularly critical when working with AI because of its "black box" nature~\cite{bathaee2017artificial,hoffman2018explaining}, i.e. the inherent difficulty for users to understand how these models work, and the non-deterministic nature of their outputs.  To capture this aspect, we borrow the term \textbf{reflection} from the design literature and introduce it as a principle for AI-instruments. 

We define reflection as the ability to help users reflect on their possibly ambiguous intent (reflection-in-intent) as well as the ambiguous interpretation made by AI (reflection-in-response), and thus offer the ability to users to steer the content generation towards a satisfying result. 


\begin{figure*}[t]
    \centering
\includegraphics[width=.8\textwidth]{Figures/reflection2.png}
\caption{Reflection-in-intent enables users to gain awareness of the possible  formulations of their intent while reflection-in-response enables users to assess the space of possibilities of the outputs generated by the model given an input. These aspects may help users address the challenges of intent disambiguation, resolution and steering.}
\Description{Screenshots of wireframes and examples illustrating the 'reflection-in-intent' and 'reflection-in-response'. Reflection-in-intent enables users to gain awareness of the possible  formulations of their intent while reflection-in-response enables users to assess the space of possibilities of the outputs generated by the model given an input. These aspects may help users address the challenges of intent disambiguation, resolution and steering.}
    \label{fig:reflection}
\end{figure*}


\textbf{Reflection-in-intent} is the ability of AI-instruments to surface multiple facets of their intent to users. For example,  fragmenting intent into pieces reveals a particular chunking~\cite{buxton1995chunking}. Working with fragments (Figure~\ref{fig:reflection}) may help users refine their intent (1), pivot on a specific aspect (2) or iterate by adding novel aspects (3).



\textbf{Reflection-in-response} is the ability of AI-instruments to offer multiple results of the content generation, while also helping people explore the space of possibilities (Figure~\ref{fig:reflection}), addressing (C3).Reflection-in-response can vary on the type and range of alternatives provided by employing diverse strategies: using model parameters such as its temperature, generating variations of the input, or asking the model to use different context of interpretation. 




\begin{figure*}[t]
    \centering
\includegraphics[width=.8\textwidth]{Figures/grounding4.png}
\caption{Grounding an instrument such as a generative container with an example enables to refer to features to preserve or alter in simple worlds by leveraging AI segmentation (1). Grounding an instrument such as a fillable brush in a specific aspect of an example, for example by selecting a region and extracting its style (2), enables users to use and apply it to other inputs without the need to articulating it in words. The principle of grounding also applies to instruments themselves such as deriving fragments from an example one (3).}
\Description{Wireframes and screenshots used to explain the concept of grounding. Showing a number of images and different operations performed to modify the content. Grounding an instrument such as a generative container with an example enables to refer to features to preserve or alter in simple worlds by leveraging AI segmentation (1). Grounding an instrument such as a fillable brush in a specific aspect of an example, for example by selecting a region and extracting its style (2), enables users to use and apply it to other inputs without the need to articulating it in words. The principle of grounding also applies to instruments themselves such as deriving fragments from an example one (3).}
    \label{fig:grounding}
\end{figure*}



\subsection{Grounding}

The principle of grounding refers to the ability for users to ground instruments from examples of desired outcomes or other instruments. It may be difficult to find the right vocabulary to describe particular aspects of content, especially for images. Instruments leverage AI segmentation to (1) enable users to refer to elements of an example in generic terms,  and (2) extract specific aspects of the content (e.g. style) by selection, storing the result for later (Figure~\ref{fig:grounding}). This builds on the notion of \textit{Variations}, \textit{Parameter Spectrums}, and \textit{Side Views} \cite{TechnologyProbesCHI2003, TerrySideViews2002}, but in a way that leverages the principles of interactive instruments \cite{beaudouin2000instrumental, beaudouin2000reification} as well as the open-ended possibilities of generative AI via our novel AI-instruments, rather than as views or controls with fixed, hand-designed and hard-coded options. AI-instruments can also be grounded in other instruments, enabling exploration of the space of related instruments (Figure~\ref{fig:grounding} (3)).

