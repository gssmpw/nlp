\section{Discussion and Future Work}

We first discuss how our model revisits and extends the classic instrumental interaction model, then discuss the application of AI-instruments to different forms of content, outlining future work.

\subsection{Revisiting and Extending the Instrumental Interaction Model}

The instrumental interaction model~\cite{beaudouin2000instrumental} is based on three core principles~\cite{beaudouin2000reification}:
\begin{itemize}
    \item \textit{Reification of commands} refers to the principle of turning systems functionalities into interactive graphical objects in the interface,
    \item \textit{Polymorphism} refers to the principle of applying commands to different types of objects enabling the interface designers to keep the number of interface objects relatively small, 
    \item \textit{Reuse} refers to the ability for users to reapply one command to different objects or apply different commands to one object, with the goal of limiting repetitive user input and/or navigation.
\end{itemize}

In this paper, we revisited and proposed to extend this model in the following ways:

(1) We extended the principle of reification from encompassing a limited set of commands defined for an application to include any intent user may express in natural language. We also unpack two key considerations of reification that one should consider when designing AI-Instruments: the scope of the instrument, and degree of abstraction. We propose to leverage the affordances of existing direct manipulation techniques to convey to users how to specify scope (e.g. select a portion with a brush vs resize the lens). To support users navigating different levels of abstraction of their intent, we propose to use AI-instruments themselves.


(2) We re-framed the concept of polymorphism in instrumental interaction, recasting it as \textit{reflection}. This shifts instrumental interaction from a classic "direct manipulation" technique for graphical user interfaces to a modern AI-augmented technique. Reflection leverages the general concept-translation capabilities of LLM's to support an expansive notion of "polymorphism" without requiring the interface design and system architect to hand-code the parameters, controls, and nuances of how these are interpreted across a wide range of content types. 

Further, by considering reflection from both the user (intent) and system (response) perspectives, we provide users with mechanism to explore both the design space of their intent (i.e. different formulations and disambiguation of intent), as well as the design space of the model response (i.e. different interpretations of user intent by the model). While such notions, in one sense, have been latent in interactive instruments all along, with generative AI many possible forms and interpretations of polymorphism---potentially even for niche or specialized workflows, formats, and types of content, if they are sufficiently represented in the training data for the model---can be made available for user reflection in AI-assisted content creation. 



(3) The third principle of reuse is closely related to our principle of grounding. Grounding extends the notion of \textit{reusing commands} to the capability of \textit{extracting and reusing intent}---whether in terms of one aspect of user intent, a collection of multiple user intents, or other properties of content. This principle of grounding also encapsulates the ability to generate instruments from other instruments, characterized as \textit{meta-instruments} in the nomenclature of instrumental interaction \cite{beaudouin2000instrumental, beaudouin2000reification}. 


(4) A further new challenge raised by AI-instruments is the need to balance the possibility of over-generating instruments, with the power to encapsulate many capabilities---at a high level of abstraction---within a single instrument. In contrast to classic hand-crafted instrumental interaction, AI-generated instruments could potentially lead to an unwieldy number of objects in the workspace, if generation were left unchecked. However, we counterbalance this with strategies to compose instruments and organize them into collections (meta-instruments). Further, we can leverage the generative nature of AI to iteratively refine both content and (meta-)instruments---altering, summarizing, or abstracting instruments and content with each step---as another strategy to harness AI to express aggregated concepts at a high semantic level.



The insights we gained by building a set of technology probes and gathering initial perceptions of 12 content creators, suggest that the principles described in our model can be used inform the design of novel interaction techniques as well as assess existing ones. \rev{As we built each probe, it became clear that design decisions at lower-level, for example pertaining to the specific choice of interactions (e.g. click vs double-click) or their timing (e.g. idle time threshold triggering image generation), can lead to different experiences, especially when multiple probes are used in conjunction.  While our model suggests overarching principles for AI-instruments, specific interaction is bound to differ as sets of these techniques are integrated into specific applications and adapted to different modalities and contexts~\cite{appert2005context, mackay2002interaction}.}

\rev{Additional considerations for integrating AI-instruments in applications include the expectations of users with regard to direct manipulation and instrumental interfaces, as well as when working with AI. For example, a fundamental principle of direct manipulation is ease of reversibility of user actions (e.g. if a fragment is removed from an image triggering a new generation, then added back; the image should revert to its prior state). In contrast, AI models are non-deterministic by nature (e.g. same input, different output). While it is technically feasible to couple AI generation with history and versioning mechanisms to ensure the reversibility of operations, users' attitude on working with AI over the longer term, as well as specific use cases may impact this design decision. }


\subsection{Beyond Images, Applying AI-Instruments to Other Forms of Content}

While extrapolations from participants must be taken with caution, four participants in our study related the use of AI-instrument to content they worked with every day. Interestingly two participants (P2 and P9) commented they would not see the use of instruments for tasks such as writing code. P9 summarized it as \textit{"it [fragments] is a bit harder to use than prompting for tasks like maybe writing code. I would prefer fragmenting for images or plots"}.  However, two different participants (P11 and P12) envisioned using AI-instruments for data analysis and writing (P11) and in the case of P12 leveraging grounding for operating at the artifact level: \textit{"it would be so cool to auto-pick a style (words, design, etc) without me first having to decipher it, and then have it automatically apply to other content (writing, slides, etc)"}.


We experimented with a few of our technology probes (Fragments, Containers and Palettes) to work with textual content and found that our principles generally held. However, further exploration with different types of modality is likely to reveal additional design considerations for the instruments we proposed. Notably, a key issue to address for textual content as opposed to images is the effort required to consume a number of potential outputs (reflection-in-response). Integrating support for helping user skim and get the gist of similarities and differences between textual outputs in Generative Containers (by bolding portions of text or providing summaries or excerpts) would certainly be necessary when using instruments to work with textual documents.  

Another aspect to address is the use of instruments for artifacts composed of multiple pieces of content (e.g. a slide composed of a title and image). Again, while we believe core principles hold for devising instruments to work with content at the artifact level, additional research is needed to delve into how to integrate different aspects of an artifact. For example, one could envision displaying Fragments from different scope of selection, enabling Fragments to operate at the entire slide level or on a subset such as title.



In the future, we plan to pursue these two research directions (designing AI-instruments for heterogeneous content and artifacts composed of multiple pieces of content), further assessing the generalizability of our interaction model and broadening the set of design considerations for AI-instruments.














