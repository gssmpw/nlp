
\section{Examples of AI-Instruments}
\label{sec:Ai-instruments}
To assess the viability of our AI instrumental model, study its differences with existing GUIs and tease out its value compare to existing chat-based AI interaction, we built a technology probe~\cite{TechnologyProbesCHI2003} with four different instruments, grounded in the literature: Fragments, Generative Containers, Transformative Lenses and Fillable Brushes. 
We describe below how this set of instruments surface the principles of our AI-instrumental model, as well as offer complementary interaction capabilities and affordances.




\subsection{Fragments}
 Fragments build on the concept of \textit{Attribute Cards} introduced in Object Oriented Drawing~\cite{xia2016object}, as well as Side View's notions of \textit{Variations} and \textit{Parameter Spectrums} \cite{TerryCreativeNeedsUIDesign2002, TerrySideViews2002}, by using a large language model to extract multiple conceptual dimensions that may be plausibly implied by a prompt. 
 
 Fragments reify an initial prompt used to generate text or image into a set of attribute cards, of the format \texttt{\textbf{[type, value]}} (where \texttt{\textbf{type}} is the category of the extracted dimension, and \texttt{\textbf{value}} is the extracted value within that dimension---such as  \texttt{[tone, enchanting]}, \texttt{[content, castle]} or \texttt{[style, illustration]}). 
 Revealing these conceptual dimensions enables an initial reflection-in-intent, revealing the latent structure of the prompt as seen by the AI model. \rev{Commercial software such as Adobe Firefly~\cite{firefly} offers a similar capability as tags, enabling users to select them from a side panel for subsequent image generation. Applying the principle of reification to tags and turning them into cards affords three core novel interactions} as illustrated in Figure~\ref{fig:fragments}.
 
 First, users can reveal fragments via a long press on the content. Fragments are fully reified as interactive instruments and are dynamically generated---hence open-ended and nondeterministic---in contrast to the fixed, hand-crafted, and hard-coded controls supported by prior work (e.g. ~\cite{xia2016object, TerryCreativeNeedsUIDesign2002, Suggestive3dDrawing2007, WritLarge2017}). 
 Second, via drag and drop, users may remove fragments (by dragging them away), or add new fragments onto existing content in the work space. Adding or removing fragments triggers regeneration of the content. 
 Third, to further support reflection, fragments offer suggestions on demand. By tapping on \includegraphics[width=0.015\textwidth]{Figures/ellispsis.png}, users can generate new variations from any fragment; these suggestions appear in a column below the specific fragment. Users can also invoke additional suggestions for more types of fragments, which are then appended to the row of fragments. 
 
These three core mechanisms support a workflow where, as users work with multiple images in their workspace, they can explore the effect of different fragments via drag-and-drop to ground one image generation into an aspect of another.
 

Fragments use the affordance of attribute cards to break down and reify a complex intent into manageable pieces, each having distinct type and value, that enable people to work with these as more-or-less independent and composable, "pieces of intent." This also encourages a workflow where users can surface useful fragmentary concepts surface that become reusable and specialized instruments in their own right. Such fragments are then available for reapplication to other pieces of content, or even reuse in a different context.

While in principle we could have pursued a design that generated many fragments as automatic suggestions associated with each piece of content, such an approach would introduce clutter and risk overwhelming the user with the "decision paralysis" of too many choices. Our design therefore surfaces only a few fragments at a time, and only in a post-hoc manner upon explicit invocation by the user. Further, we present these newly-invoked fragments in an organized fashion, with two orthogonal dimensions of exploration on demand, by keeping dimension type in horizontal rows of cards, and value variations in vertical columns beneath these.



\begin{figure*}[t]
    \includegraphics[width=0.88\textwidth]{Figures/fragment-probe4.png}
    \caption{Users can expand Fragments with variations of parameter values (1) in vertical columns, or request more suggestions for dimension types (2) at the end of the row. Users can further reuse and transfer Fragments to other content via drag-and-drop(3).}
    \Description{Several wireframe boxes with images and text explain the concept of Fragments. Users can expand Fragments with variations of parameter values (1) in vertical columns, or request more suggestions for dimension types (2) at the end of the row. Users can further reuse and transfer Fragments to other content via drag-and-drop(3). The textboxes with prompt fragments are shown next to an image, and then a person expands these fragments to show variations. When selecting any of these alternative values of a fragment, the content is modified.}
    \label{fig:fragments}
\end{figure*}



\subsection{Transformative Lenses}




\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/lenses-probe3.png}
  
  \caption{Transformative Lenses are placed over initial content, enabling users to "complete" illustrations from pieces of content (1). When users add elements to their composition, lenses regenerate to integrate it (2).}
  \Description{Eight square boxes that illustrate wireframes of the Transformative Lenses. They show in a sequence of how other content gets added to the lens and new images are generated based on the composition of the source images. Transformative Lenses are placed over initial content, enabling users to "complete" illustrations from pieces of content (1). When users add elements to their composition, lenses regenerate to integrate it (2).}
  \label{fig:lenses}
\end{figure*}


Transformative Lenses re-envision the Toolglass and Magic Lens interaction technique ~\cite{bier2023toolglass} as a layered instrument that can be coupled with a generative prompt. 


Layering a Transformative Lens on top of content uses such a prompt to generate a new image that synthesizes the lens and the content. Likewise, a specific piece of image content can be used on top of a lens to recombine the two. Such layerings can be positioned and manipulated to chain multiple effects together. 
As illustrated in Figure~\ref{fig:lenses}, users can leverage lenses to take a piece of content (e.g. a sketch of a suspension bridge), and then re-compose this content within a wider backdrop scene (a city skyline), or even apply a new specific style to the results with a single interaction (e.g. a heavy, black-lined graphic novel style). 

More generally, depending on how the user layers Transformative Lenses and image content, lenses can support image completion from a small piece of content, synthesis and composition of multiple pieces of content into a new image, or regeneration of the underlying image. Note also that blank lenses (which have no image content, but do contain a prompt) can be used. For example, a blank-lens backdrop generated afforded outpainting-like operation ---but here steered by the lens's prompt---to "complete" a scene from an existing piece of image content. 



Users can freely drag, reposition, and resize both content images and lenses, layering them over each other to chain transformations, reflect on the results, and experiment with different combinations. This property also may encourage users to break down their intent into multiple lenses, which can then be applied to multiple pieces of content (grounding). Note that image recomposition and dynamic regeneration occurs after a 2-second idle time to avoid triggering constant image regenerations during dragging or resizing operations. \rev{As users may wish to adjust content under a lens post-generation, the lens is temporarily faded out in the background when the mouse pointer enters it. } 



Complementary to the Fragments described in the previous section, Transformative Lenses afford the design consideration of breaking down the output into pieces (whereas fragments focus on the prompt intent). People can control the composition of images by just moving and layering elements in relation to the lens, limiting the need for precise selection, and encouraging rapid iteration \& experimentation with compositions. However, unlike an undo operation, removing (or otherwise reverting) the layering of Transformative Lens and image-content elements triggers re-generation, and will always lead to a slightly different rendering. 




\subsection{Generative Containers}



Designers use moodboards \cite{BuxtonSketchingUserExperiences2007}, storyboards \cite{Storeoboard2016}, and other techniques for presenting small-multiples in  galleries \cite{DesignGalleries1997, TerryCreativeNeedsUIDesign2002, TEAMSTORM2007} to illustrate and explore a space of possible creative directions. \rev{Structured generation of those alternatives \cite{suh_luminate_2024} allows rapid exploration of design spaces, and techniques to highlight similarities and differences \cite{gero_sensemaking_2024} facilitate the selection, refinement, and comparison of multiple responses.}




As shown in Figure~\ref{fig:containers}, \textit{Generative Containers} provide an AI-instrument that encapsulates these notions using a prompt---shown in the container's header---that is closely associated with a 2x2 small-multiple grid of generated image results. Users can then enter or edit the prompt, or drag and drop  example content---or even another instrument, such as a Fragment---onto the Container to ground it and generate a new small-multiple set of results. 

We designed Generative Containers to enable reflection-in-response, allowing users to quickly get a visual sense of the range of responses a single prompt might produce. And by using Generative Containers to generate different variations of fragments, for example to obtain more concrete image editing suggestions from a high-level intent (Figure~\ref{fig:meta-instrument} left), generative containers also enable reflection-in-intent.

In our current implementation, the Generative Containers probe supports generation of four different variations (in a fixed 2x2 grid). Further, each container is presently limited to a single grounding example as input. However, users can create multiple containers and reuse results by dragging and dropping from one to another. 
In this way Containers afford adding details and varying the prompt to generate a range of example images, encouraging multiple cycles of iteration. Recombining and chaining these together effectively results in a longer, refined prompt that integrates the series of changes from prior interactions. 
\rev{Furthermore, one could expand the Generative Container instrument with other representations beyond our 2x2 grid, such as the dimension plots or stacked vertical dimension grids \cite{suh_luminate_2024}.}



\begin{figure}[htb]
  \begin{subfigure}{.47\textwidth}
  \centering
    \includegraphics[width=\textwidth]{Figures/container-probe4.png}
    \caption{Generative containers enable users to explore possibilities on concrete or abstract dimensions (1). Containers also afford complex exploration paths by reusing the output of one container as the input of another one (2), resulting in refining intent (3).}
    \Description{Generative containers enable users to explore possibilities on concrete or abstract dimensions (1). Containers also afford complex exploration paths by reusing the output of one container as the input of another one (2), resulting in refining intent (3).}
    
    \label{fig:containers}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.47\textwidth}
  \centering
    \vspace{10pt}
    \includegraphics[width=\textwidth]{Figures/brushes-probe3.png}
    \caption{Brushes can extract aspects of content difficult for users to articulate in words, such as drawing style, making it reusable and editable (1). Combined with the selection afforded by brushes, this enables to apply aspects such as style to portions of images (2).}
    \Description{Brushes can extract aspects of content difficult for users to articulate in words, such as drawing style, making it reusable and editable (1). Combined with the selection afforded by brushes, this enables to apply aspects such as style to portions of images (2).}
    \label{fig:brushes}
  \end{subfigure}%
  
  \caption{Generative Containers and Fillable Brushes support different types of content creation tasks. Containers promote the exploration of multiple ideas in parallel, while Brushes offer precise direct manipulation for steering generation. Providing users with both of these AI-instruments enables them to conduct many different activities involved in content creation, enabling  interweaving of both divergent and convergent thinking activities.}
  \Description{Screenshots illustrating generative containers and fillable brushes. Sketched drawings of a beach and palm trees are added to a box that is representing a generative container, which in turn creates variations of those sketches. Generative Containers and Fillable Brushes support different types of content creation tasks. Containers promote the exploration of multiple ideas in parallel, while Brushes offer precise direct manipulation for steering generation. Providing users with both of these AI-instruments enables them to conduct many different activities involved in content creation, enabling  interweaving of both divergent and convergent thinking activities.}
\end{figure}



\subsection{Fillable Brushes}



Fillable Brushes, as illustrated in Figure~\ref{fig:brushes}, 
offer an AI-instrument with the semantics of an "intelligent paint brush" for style transfer scoped to a particular spot on an existing image. 

While previous work has explored brushes that can encapsulate and integrate deterministic modes and commands ~\cite{romat2022style}, our Fillable Brushes instrument applies encapsulated AI-prompts onto content in an intelligent manner as the user scrubs over it with their pen, finger, or other pointing device. And in contrast to the post-hoc notion of Fragments described above, Fillable Brushes apply a brush onto content, structured as an AI-Instrument "command" with a prefix (as opposed to postfix) syntax ~\cite{LexicalPragmaticInput1983}. This offers a familiar interaction model from the way that a highlighting tool turns selected text yellow in a document editor, for example. 



\rev{Encapsulating a prompt or image into a brush to define its function is a powerful interaction techniques to control scope of selection, as demonstrated by Runway motion brushes~\cite{runwayML}. Applying additional principles of our model, enables users to} also "fill" (ground) an empty brush by using existing content as an example, as if the instrument were a color picker that picks up key semantic attributes of the content rather than just its "color." The prompt encapsulated by the Fillable Brush is then automatically populated with descriptive words via generative AI, which the user can further edit if desired. This can be particularly helpful when users want to style something "like this" even when they may lack the vocabulary to describe its visual style. Our Fillable Brushes technology probe supports both content and/or style extraction. \rev{Turning a brush into a persistent object on screen, enables combining brushes together by drag and drop.} Brushes can also be applied multiple times to the same content to emphasize a particular prompt in the result. 

While Fillable Brushes enable users to specify the scope of intent with a high granularity, this does not necessarily require high precision: our implementation leverages the AI-powered Segment Anything Model (SAM)~\cite{kirillov_segment_2023}, which enables users to make approximate selections (i.e. rather than a precise and tedious lasso selection) to indicate an image element. The source content plus the approximate selection (as a set of reference points) is then converted into a precise object selection by the segmentation model. 




\subsection{Generated Instruments and Meta-Instruments}

Beyond the concept of instruments, the instrumental interaction model~\cite{beaudouin2000instrumental} also refers to the concept of meta-instruments, in which \textit{"instruments operate on instruments"}. 
As hinted at in earlier sections, using instruments on other instruments can be particularly useful to derive or compose instruments from the "task detritus" \cite{KirshIntelligentUseOfSpace1995} already produced in the user's workflow and experimentation with other instruments. Using generative containers on Fragments, for example, can help users navigate the degree of abstraction, turning a vague idea into a set of concrete modifications (Figure~\ref{fig:meta-instrument} left). 

However, such generation loops (instruments that generate content, generating instruments that generate other instruments generating content...) could potentially lead to an unwieldy number of elements in the interface. To organize but also generate collections of instruments, we devised a type of meta-instrument we call \textbf{Palettes}.

Akin to menus and containers available in GUIs today, Palettes enable storage and/or generation of different sets of instruments and content if desired. These afford abstraction and generalization of instrumental controls from collected pieces of content that can then serve as examples or generative seeds (Figure~\ref{fig:meta-instrument} right). Palettes of diverse instruments can balance the different affordances and properties of each instrument to provide rich content creation support. They can also help people get past the "cold-start" problem in complex creative design work, by beginning with examples, other pieces of existing content, or past work-artifacts to help overcome so-called "writer's block" or "blank canvas" effects of starting from nothing.



\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/meta-instruments.png}
    \caption{Instruments can be used as Meta-Instrument: operate on each other to create related instruments, for example for making a fragment more concrete (left). Specific Meta-Instruments such as palettes (right) can help user organize sets of instruments for easier retrieval and reuse, or, even help generating collection of instruments for a certain task.}
    \Description{Wireframe and screenshots (multiple squares that represent the composition of multiple AI instrument in a meta instrument) -- Instruments can be used as Meta-Instrument: operate on each other to create related instruments, for example for making a fragment more concrete (left). Specific Meta-Instruments such as palettes (right) can help user organize sets of instruments for easier retrieval and reuse, or, even help generating collection of instruments for a certain task.}
    \label{fig:meta-instrument}
\end{figure*}








\section{Implementation}


\paragraph{Overview:}
Our system and all AI-Instruments technology probes were implemented on a web-based platform. We use Javascript and HTML with the fabric.js~\cite{juriy_zaytsev_fabricjs_2024} library for the front-end, and a Node.js~\cite{openjs_foundation_nodejs_2024} server for the back-end managing content and files as well as coordinating communication  with the generative AI models. For the user interface design, we chose to use a sketched user interface look and feel, to encourage our study participants to focus on the concepts rather than the surface details of their specific instantiation in the UI \cite{BuxtonSketchingUserExperiences2007}.  

\paragraph{Leveraging Generative AI models:}
We use the OpenAI GPT-4o~\cite{openai_gpt-4o_2024} model for text transformations and image analysis, and a local Stable Diffusion~\cite{stabilityai_compvisstable-diffusion_nodate} server with a custom processing pipeline for image generation. The AI-Instruments use GPT-4o for analysis of provided input (e.g., for turning provided visual content into a text prompt, analyzing the contents of part of the workspace canvas). For image generation, we use multiple stacked ControlNet~\cite{zhang_controlnet_2023} models with Stable Diffusion to steer the generation of visual content. To preserve aspects of the source input, we use a combination of \textit{Depth}, \textit{Canny Edge}, and \textit{Scribble} ControlNet models, while for preserving art/rendering styles (e.g., the sketch-based output) we use the \textit{Reference} ControlNet model. Depending on the type of image generation, we vary the weight of each ControlNet model (e.g., increasing weight to emphasize content preservation, or decrease weight of another ControlNet to reduce affect of reference style transfer). We use image masks to selectively control which areas are changed or kept, apply inpainting/outpainting scripts, and adjust other parameters such as CFG scale, denoising strength, and control mode. 

\paragraph{Building AI-Instruments:}
We designed a pipeline that can orchestrate the access and requests to the different LLM and diffusion server instances to generate results. Key functionality is wrapped in modules, such as for encapsulating prompts to communicate with one or more models (by using model chaining) to perform a specific task. 
Each AI-instrument then uses a number of these modules for modifying the input or generating new content bases on the user's performed action with the instrument:


\begin{itemize}
    
\item \textit{\textbf{Fragments}} instruments include modules for (1) prompt decomposition which takes a text or visual input and makes a GPT-4o request to generate fragments (returned as collections of \texttt{[type, value]} pairs), (2) fragment extension which takes a prompt and the existing fragments and requests additional fragment dimensions, (3) fragment variation which takes the fragment and parent prompt/content (if applicable) and generates variations of that fragment, and (4) prompt composition which takes a prompt, a modification to the fragments, and returns a modified prompt. The result from the prompt composition is then used to create an updated image with the Stable Diffusion + ControlNet pipeline. 


\item \textit{\textbf{Transformative Lenses}} use a module for composition of the prompt (merging prompts from all source images covered by the lens), before then applying inpainting/outpainting, masks, and ControlNet models to generate the resulting image. 


\item \textit{\textbf{Containers}} use a variation module, taking a prompt and a dimension, and requesting four variations along the provided dimension. Within the prompt we request visually diverse results. The resulting set of prompts is then sent to SD+ControlNet to generate the final set of four images in the container. 


\item \textbf{\textit{Fillable Brushes}} are implemented to either emphasize style or content variations, depending on the intent of the user, which we support by varying the weight of the ControlNet models (e.g., higher reference ControlNet weight for changing the visual style, or increasing weight of Canny-edge/Depth ControlNet to preserve existing content).

When the brush is applied, we perform a segmentation of the source content by feeding the stroke path as control points into Segment Anything~\cite{kirillov_segment_2023}, which results in a segmentation mask of the dominant object(s) selected with the brush stroke. We then use GPT-4o to craft a combined prompt given the source image(s), the segmented content, and the original prompt. Finally, we send this generated prompt together with the source image and segmentation mask to the Stable Diffusion server, using the ControlNet inpainting method.



\end{itemize}
