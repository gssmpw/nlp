\section{Related Work}
\subsection{Diffusion Alignment}
Diffusion alignment aims to align model outputs with user preferences by integrating reinforcement learning (RL) into diffusion models to enhance generative controllability ____. DDPO____ uses predefined reward functions to fine-tune diffusion models for specific tasks, such as compressibility. In contrast, DPOK____ utilizes feedback from AI models trained on large-scale human preference datasets. 
% Similarly, SEIKO____ constructs custom reward models based on extensive human feedback data to guide the generative process.
An alternative to predefined rewards is direct preference optimization (DPO). 
Diffusion-DPO____ extends DPO____ to diffusion models by directly utilizing preference data for fine-tuning, thereby eliminating the need for predefined reward functions. Despite its potential, Diffusion-DPO relies on large-scale preference datasets and still fails to handle complex generation scenarios.
Recent IterComp ____ address these challenges by gathering composition-aware preference data from a set of open-sourced models and aligning with the collected preferences iteratively.
% In contrast, D3PO____ replaces offline datasets with online human feedback collected dynamically during training. 

\subsection{Diffusion Trajectory Forward Optimization}
Forward optimization in diffusion trajectories focuses on refining the forward process through carefully designed transition kernels or data-dependent initialization distributions____. For instance, Rectified Flow____ and Consistency Flow Matching ____ learns a straight path connecting the data distribution and the prior distribution, effectively simplifying the denoising process. Grad-TTS____ and PriorGrad____ introduce conditional forward processes with data-dependent priors, specifically designed for audio diffusion models. Other methods like ContextDiff ____ focus on parameterizing the forward process with additional neural networks. For example, Diffusion Models for Video Generation____, Maximum Likelihood Training for Score-based Diffusion Models____, and Variational Diffusion Models (VDM)____ employ neural architectures to enhance the forward trajectory. 

\subsection{Diffusion Trajectory Sampling Optimization}
Beyond forward optimization, recent research has explored real-time optimization during the sampling process, incorporating stochastic optimization techniques to guide the backward sampling trajectory. For instance, MBD____ utilizes score functions to direct the sampling path in the backward process. Similarly, in music generation tasks, SCG____ employs stochastic optimization to leverage non-differentiable reward functions. Demon____ focuses on optimizing the sampling process to concentrate sampling density in regions with high rewards during inference. Free$^{2}$Guide____ uses path integral control to provide gradient-free, non-differentiable reward guidance, enabling the alignment of generated videos with textual prompts without requiring additional model training. Inference-Scaling____ employs a verifier and search algorithm to scale diffusion inference beyond NFEs.  

While these approaches demonstrate significant potential, they often incur substantial computational overhead due to the extra steps required for calculating intermediate rewards during inference. For example, Demon____ and Inference-Scaling____ may require up to 1000x the inference cost per image to achieve optimal performance. This significant increase in computational cost considerably slows down the generation process, limiting their practicality for real-world applications.