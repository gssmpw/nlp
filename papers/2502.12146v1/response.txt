\section{Related Work}
\subsection{Diffusion Alignment}
Diffusion alignment aims to align model outputs with user preferences by integrating reinforcement learning (RL) into diffusion models to enhance generative controllability **So, "Diffusion-Based Reinforcement Learning for Generative Controllability"**. DDPO** uses predefined reward functions to fine-tune diffusion models for specific tasks, such as compressibility. In contrast, DPOK** utilizes feedback from AI models trained on large-scale human preference datasets. 
% Similarly, SEIKO** constructs custom reward models based on extensive human feedback data to guide the generative process.
An alternative to predefined rewards is direct preference optimization (DPO). 
Diffusion-DPO** extends DPO** to diffusion models by directly utilizing preference data for fine-tuning, thereby eliminating the need for predefined reward functions. Despite its potential, Diffusion-DPO relies on large-scale preference datasets and still fails to handle complex generation scenarios.
Recent IterComp **address these challenges by gathering composition-aware preference data from a set of open-sourced models and aligning with the collected preferences iteratively.
% In contrast, D3PO** replaces offline datasets with online human feedback collected dynamically during training. 

\subsection{Diffusion Trajectory Forward Optimization}
Forward optimization in diffusion trajectories focuses on refining the forward process through carefully designed transition kernels or data-dependent initialization distributions**. For instance, Rectified Flow** and Consistency Flow Matching **learns a straight path connecting the data distribution and the prior distribution, effectively simplifying the denoising process. Grad-TTS** and PriorGrad** introduce conditional forward processes with data-dependent priors, specifically designed for audio diffusion models. Other methods like ContextDiff **focus on parameterizing the forward process with additional neural networks. For example, Diffusion Models for Video Generation**, Maximum Likelihood Training for Score-based Diffusion Models**, and Variational Diffusion Models (VDM)** employ neural architectures to enhance the forward trajectory. 

\subsection{Diffusion Trajectory Sampling Optimization}
Beyond forward optimization, recent research has explored real-time optimization during the sampling process, incorporating stochastic optimization techniques to guide the backward sampling trajectory. For instance, MBD** utilizes score functions to direct the sampling path in the backward process. Similarly, in music generation tasks, SCG** employs stochastic optimization to leverage non-differentiable reward functions. Demon** focuses on optimizing the sampling process to concentrate sampling density in regions with high rewards during inference. Free$^{2}$Guide** uses path integral control to provide gradient-free, non-differentiable reward guidance, enabling the alignment of generated videos with textual prompts without requiring additional model training. Inference-Scaling** employs a verifier and search algorithm to scale diffusion inference beyond NFEs.  

While these approaches demonstrate significant potential, they often incur substantial computational overhead due to the extra steps required for calculating intermediate rewards during inference. For example, Demon** and Inference-Scaling** may require up to 1000x the inference cost per image to achieve optimal performance. This significant increase in computational cost considerably slows down the generation process, limiting their practicality for real-world applications.