% \documentclass[lettersize,journal,twoside]{IEEEtran}
\documentclass[lettersize,journal]{IEEEtran}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{caption}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath, amssymb,amsthm}
\usepackage[export]{adjustbox}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{fit,positioning}
\usepackage{authblk}
\usepackage{siunitx}

\input{defs}

\graphicspath{ {./images/} }

% \newcommand\blfootnote[1]{%
%   \begingroup
%   \renewcommand\thefootnote{}\footnote{#1}%
%   \addtocounter{footnote}{-1}%
%   \endgroup
% }


\title{Hierarchical Sparse Bayesian Multitask Model with Scalable Inference for Microbiome Analysis}

%\blfootnote{This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. Prepared by LLNL under Contract DE-AC52-07NA27344.}

\author[1,2, $\ast$]{Haonan Zhu}
\author[1,$\ast$]{Andre R. Goncalves}
\author[1]{Camilo Valdes}
\author[1]{Hiranmayi Ranganathan}
\author[1]{Boya Zhang}
\author[1]{Jose Manuel Martí}
\author[1]{Car Reen Kok}
\author[1]{Monica K. Borucki}
\author[1]{Nisha J. Mulakken}
\author[1]{James B. Thissen}
\author[1]{Crystal Jaing}
\author[2]{Alfred Hero}
\author[1, $\ast$]{Nicholas A. Be}

\affil[1]{Lawrence Livermore National Laboratory}
\affil[2]{Electrical and Computer Engineering, University of Michigan}
\affil[$\ast$]{Corresponding Authors: zhu18@llnl.gov, andre@llnl.gov, be1@llnl.gov}  
  
\begin{document} 
\maketitle

\begin{abstract}
This paper proposes a hierarchical Bayesian multitask learning model that is applicable to the general multitask binary classification learning problem where the model assumes a shared sparsity structure across different tasks. We derive a computationally efficient inference algorithm based on variational inference to approximate the posterior distribution. We demonstrate the potential of the new approach on various synthetic datasets and for predicting human health status based on microbiome profile. Our analysis incorporates data pooled from multiple microbiome studies, along with a comprehensive comparison with other benchmark methods. Results in synthetic datasets show that the proposed approach has superior support recovery property when the underlying regression coefficients share a common sparsity structure across different tasks. Our experiments on microbiome classification demonstrate the utility of the method in extracting informative taxa while providing well-calibrated predictions with uncertainty quantification and achieving competitive performance in terms of prediction metrics. Notably, despite the heterogeneity of the pooled datasets (e.g., different experimental objectives, laboratory setups, sequencing equipment, patient demographics), our method delivers robust results. \footnote{This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-MI-853606). It was supported by the LLNL Laboratory Directed Research and Development Program, the ETI Consortium under US Department of Energy grant  DE-NA0003921, and the US Army Research Office under grant number W911NF1910269.}
\end{abstract}

\begin{IEEEkeywords}
Bayesian Hierarchical Model, Multitask Learning, Human Microbiome, Disease Prediction, Variational Inference.
\end{IEEEkeywords}

\section{Introduction}
\label{sec: intro}

Multitask learning (MTL) is a class of machine learning prediction models where multiple related learning tasks are trained jointly \cite{caruana1997multitask} (see \cite{zhang2021survey} for a recent survey). This allows us to combine multiple related tasks (datasets, for example) together to increase the effective sample size, while keeping the interpretability of a single base model. MTL has been an active area of research with applications including: face recognition in computer vision; joint analysis of heterogeneous genomics data; social media sentiment analysis; and climate sciences \cite{wang2009boosted,glorot2011domain,widmer2012multitask,ray2014bayesian,gonccalves2016multi}. 

%Different multitask learning models differs mainly in their structural assumptions such as a shared sparsity structure between the regression coefficients across tasks \cite{xue2007multi,zhang2010learning,liu2009multi,jalali2010dirty,guo2011sparse,goncalves2019bayesian} or a shared sparse precisin matrix across the task \cite{gonccalves2016multi}.

In this paper, we introduce a hierarchical sparse Bayesian multitask logistic regression model tailored for binary predictions on multiple related datasets. While the model is broadly applicable, we focus on the predictions of a binary health or disease state across multiple possible disease conditions of patients based on microbiome abundance data. 

The human microbiome, composed of all the microorganisms associated with the human body, have an immense impact on human health. An abundance of studies demonstrate the promise of microbiome features as indicators of human health outcomes, including in gastrointestinal (GI) health \cite{lee2021inflammatory,nishida2018gut,mcgrattan2019diet,de2019microbiome,thaiss2016microbiome}, cancer \cite{banerjee2017microbial,sepich2021microbiome,gopalakrishnan2018influence}, neurological disease \cite{boddy2021gut,mcgrattan2019diet}, and immune dysfunction \cite{de2019microbiome,thaiss2016microbiome}. Motivated by the success of machine learning models in areas such as computer vision, medical imaging, and protein prediction \cite{he2015delving,shen2017deep, jumper2021highly}, there has been an increasing interest in employing machine learning models to perform health prediction based on human gut microbiome data \cite{zhou2019review,aryal2020machine,cammarota2020gut,manandhar2021gut}. However, application of machine learning methods to microbiome data faces two major challenges. First, typical microbiome data, potentially representative of thousands of species, is high dimensional, and the number of microbial identifiers is significantly larger than the number of samples available for any given study \cite{zhou2019review}. In this regime, overparameterized models can easily overfit to the training data \cite{goodfellow2016deep}. Second, in health applications it is essential that machine learning models be interpretable and quantify uncertainty in their predictions, which is not the case with most deep neural nets \cite{molnar2020interpretive}. In general, clinical and other critical applications benefit from known uncertainty in the predictions of an interpretable model \cite{lawson2021MLME}.

We propose an interpretable predictive model that is based on a hierarchical Bayesian generalized linear model (GLM) \cite{dobson2018introduction}. The model introduces a set of binary variables shared across different datasets to represent the most informative features (i.e., microbial taxonomic identifiers) for predictions. This enables the model to identify the common microbial species shared across the different studies, and to determine which species correspond to a distinct given pathology. 

In contrast to optimization based MTL approaches\cite{liu2009multi,jalali2010dirty,gonccalves2016multi,guo2011sparse}, our proposed Bayesian hierarchical modeling provides natural uncertainty quantification through the posterior distribution of the label given the features and provides a flexible framework to incorporate domain experts' knowledge \cite{gelman2014bayesian}. The proposed model differs from \cite{xue2007multi,zhang2010learning,liu2009multi,jalali2010dirty,guo2011sparse,goncalves2019bayesian} in how the sparsity pattern is modeled: inspired by \cite{goncalves2019bayesian} we use an overparameterized Bernoulli-Gaussian model instead of regularizations, which has been demonstrated to have better support recovery properties \cite{bazot2011bernoulli}. The Bayes posterior distribution is not in analytical closed form and we propose an approximation to the posterior mode that is based on variational inference \cite{blei2017variational,zhang2018advances}, which is more scalable to the high dimensional microbiome datasets. %Variational inference is often less computationally difficult than Monte Carlo methods for evaluating the posterior mode. \cite{Brooks2011} and Gibbs sampling\cite{gelfand2000gibbs,goncalves2019bayesian} used in previous works. 

% The main contributions of this chapter include: a hierarchical Bayesian multitask learning model that is applicable to the general multitask binary classification learning problem and a computational efficient inference algorithm based on variational inference to approximate the posterior distribution. 

We illustrate our model capabilities through numerical experiments in simulated data and in a real world microbiome dataset. This dataset is composed of $61$ different previously published studies, which were internally curated and the raw sequence data processed for metagenomic classification and sequence-read abundance estimation \cite{kok2024meta2db}. See section I of supplementary materials for a full description and the list of references. Each study from the data performed an assessment of a patient cohort’s gut or fecal microbiota in the context of the health condition of the patients, which was assigned a binary label of \textit{control} or \textit{diseased}. 

To generate microbiome profiles that are consistent across studies, all raw DNA sequence data was re-processed the same way. DNA sequence data were downloaded from the appropriate repository corresponding to each study referenced above. All sequence datasets were composed of short-read data generated via an Illumina sequencer platform. DNA sequence was quality filtered via fastp \cite{chen2018fastp}. Metagenomic classification and taxonomic assignment of all sequence reads was performed via Centrifuge \cite{kim2016centrifuge,marti2024addressing} employing a reference index database constructed from sequences contained in the NCBI Nucleotide (nt) database \cite{ncbi2012database} of December 13, 2021. Post-processing and filtering using a Minimum Hit Length of 40 was performed via Recentrifuge \cite{marti2019recentrifuge}. The resultant output consists of sequence read counts that were assigned to each observed taxonomic identifier (taxID) in the reference index.

Resultant read count values for each taxonomic identifier were further processed by centered log ratio (CLR) transform \cite{aitchison1982statistical} to provide features for the machine learning model. Seven total taxonomic rank levels were examined for purposes of this analysis, and we take the union set of all the taxIDs for each taxonomic rank to provide a consistent feature space. The taxonomic ranks are: Kingdom ($11$), Phylum ($160$), Class ($313$), Order ($868$), Family ($2338$), Genus ($7519$) and Species ($31679$), where each number in the parentheses corresponds to the feature dimension (i.e., total number of taxIDs). The health condition studied for each patient is one-hot coded into one of the $19$ conditions examined in the selected dataset: cirrhosis, inflammatory bowel disease, diabetes, diarrhea, cancer, dermatologic disease, oral disease, cardiovascular disease, neurological disease, gastrointestinal infection, autoimmune disease, genitourinary disease, pulmonary disease, aged-related macular degeneration, hormonal disorder, immune disease, metabolic disease, seafaring syndrome and irritable bowel syndrome. Each distinct disease condition contains one or more studies, and we map the studies of the same disease into different tasks, while modeling each disease as a separate multitask-learning model. The resulting model is a direct clustering extension of the proposed Bayesian hierarchical model.

%This maps the studies of distinct diseases into different tasks. 
The proposed model is evaluated in comparison with other benchmark methods including: logistic regression with sparsity penalty \cite{tibshirani1996regression,banerjee2008model}, MTFL (Multitask Feature Learning) \cite{liu2009multi} and MSSL (Multitask Sparsity Structure Learning) \cite{gonccalves2016multi}. Our evaluation with simulated synthetic datasets shows that the proposed approach has superior support recovery properties when the underlying regression coefficients share a common sparsity structure across different tasks. The proposed model performs less robustly on the real microbiome data, likely due to heterogeneity of the datasets (different experimental objectives, laboratory protocols, sequencing instrumentation, patient demographics etc.), Nonetheless, we demonstrate the utility of the method to extract informative taxa while providing well-calibrated predictions with uncertainty quantification.  
%We perform a calibration analysis to assess goodness-of-fit of the model to the real-world microbiome dataset which shows the limitations of the model and provides insights for future development of the approaches described here. In addition, the proposed model provides important data-driven microbial features that can be compared to previous human-curated observations . Our results show, b

The paper is organized as follows: Section \ref{sec: bmtl model} introduces the mathematical formulation of the proposed hierarchical Bayesian model, Section \ref{sec: bmtl Inference} presents the proposed variational inference algorithm to obtain the approximated posterior distribution, Section \ref{sec: bmtl Experiments} provides application of the methods to synthetic datasets and the microbiome dataset, and Section \ref{sec: bmtl Conclusion} summarizes the findings from the paper and discusses future directions. 

\section{Hierarchical Sparse Bayesian Multitask Logistic Regression Model}
\label{sec: bmtl model}

%In this section, we formulate the proposed model and inference algorithm based on variational inference. 

\subsection{Notations and Terminologies}
\label{subsec: notations}

We refer to the abundance of each taxonomic identifier (taxID) as a feature variable. Each study is referred to as a task, and diseased or healthy state of the individual is referred to as a label. We use bold upper-case letters for matrices, bold lower case letters for vectors and no bold lower case for scalars. The Hadamard (element-wise) product of vectors $\mb a$ and  $\mb b$ is denoted by  $\mb a \circ \mb b$, and $\diag$ denotes the function map a vector to a diagonal matrix with the vector as its diagonal entries. We denote the observed taxID count data after centered log ratio transform  as $\mb x_{t}^{i} \in \bb R^{d}$ together with its label $y_{t}^{i} \in \{0,1\}$, where $d$ corresponds to the number of features (i.e., number of taxIDs),  $t=1,\ldots, T$ denotes the different tasks (i.e., different studies), $i=1,\ldots, n_t$ denotes the different subjects per study, $n_{t}$ denotes the total number of subjects per task, and $y_{t}^{i}$ reflect whether the subject is diseased ($1$) or control ($0$). For a given regression weight matrix $\mb W \in \bb R^{T\times d}$ for all the tasks, we denote $\mb w_{t} \in \bb R^{d}$ the row vector of $\mb W$  corresponds to $t$-th task across features, and $\mb w_{\left( j \right) } \in \bb R^{T}$ the column vector of $\mb W$ corresponds to $j$-th feature across tasks.    

\subsection{Hierarchy Bayesian Multitask Logistic Regression Model}
\label{subsec: bmtl model}


%The proposed model is a discriminative model \cite{}, where we are modeling the conditional probability of patient's health given the observed Genomic measurements. The  

Though the number of microbial species are on the order of trillions on earth, a much smaller quantity of species are capable of causing disease in humans \cite{wang2017human}. Our model therefore assumes that only a few of the microbial species are useful for the prediction task, where we impose sparsity on the regression coefficient through a Bernoulli-Gaussian distribution ($\mb w_{t} \circ \mb z$) \cite{soussen2011bernoulli,bazot2011bernoulli}, where $\mb w_{t}$ controls the magnitude of the effects and $\mb z$ controls the sparsity. This compound prior enforces some of the weights to be exactly zero, implying some of the microbial species are irrelevant for predicting the health status of any given subject. The sparsity term $z$ is independently drawn from the same Bernoulli distribution with parameter $\theta$. Note $\mb z$ does not depend on the specific task. This implies the sparsity pattern is shared across different tasks, which reflects the belief that there are few microbial species useful for prediction across all tasks relative to the total quantity of microbial taxa contained in the datasets. A hyper prior for $\theta$ is given by the beta distribution, which utilizes the conjugate property to control the overall sparsity level of the model. Further, to enforce the information sharing across different tasks, the row of $\mb W$, denoted by $\mb w_{\left( j \right)}$ are assumed to be $i.i.d$ draws from a multivariate Gaussian distribution with mean $\mb 0$ and covariance $\mb \Sigma_0$. A Wishart prior is proposed for this shared covariance matrix to provide a method for utilizing expert knowledge about the underling relationships among the studies. This enables us to exploit the Wishart-normal conjugacy to obtain efficient inference later. The proposed conditional model can be summarized:
%\begin{enumerate}
%	\item Generate \mbΣ−10∼Wishart(v0,\mbV0)\mb \Sigma_{0}^{-1}\sim \text{Wishart} \left( v_{0}, \mb V_{0} \right)   
%	\item Generate θ∼Beta(α0,β0)\theta \sim \text{Beta}\left( \alpha_0,\beta_0 \right)  
%	\item Generate zj∼Bernoulli(θ)z_{j}\sim \text{Bernoulli}\left( \theta \right)  for j=1,⋯dj=1,\cdots d
%	\item Generate \mbw(j)∼\mcN(\mb0,\mbΣ0)\mb w_{\left( j \right) }\sim \mc N\left( \mb 0, \mb \Sigma_0 \right)  for j=1,⋯,dj=1,\cdots, d
%\item Generate yit∼Bernoulli(sigmoid(\innerprod(\mbwt∘\mbz)\mbxit))y_{t}^{i}\sim \text{Bernoulli}\left( \text{sigmoid} \left( \innerprod{\left(\mb w_{t} \circ \mb z\right)}{\mb x_{t}^{i}} \right)  \right)  
%\end{enumerate}
{\small \begin{align*}
	y_{t}^{i}|\mb w_{t}, \mb z; \mb x_{t}^{i} &\stackrel{\text{independent}}{\sim} \text{Bernoulli}\left( \text{sigmoid}\left( \innerprod{\left(\mb w_{t}\circ \mb z\right)}{\mb x_{t}^{i}} \right)   \right) \\
    & \qquad \forall i=1,\ldots, n_{t},\forall t=1,\ldots, T, \\ 
	z_{j}|\theta & \iid \text{Bernoulli} \left( \theta \right) \quad \forall j=1,\ldots, d,\\  
	\theta &\sim \text{Beta}\left( \alpha_0,\beta_0 \right), \\ 
	\mb w_{\left( j \right) }|\mb \Sigma_0 & \iid \mc N\left( \mb 0, \mb \Sigma_0 \right) \quad \forall j=1,\ldots, d,\\
	\mb \Sigma_0^{-1} & \sim \text{Wishart}\left( v_0,\mb V_0 \right).  
\end{align*}}

\noindent where $\alpha_0,\beta_0, v_0, \mb V_0$ are hyperparameters selected by the experimenter. Smaller value of the ratio $\frac{\alpha_0}{\alpha_0+\beta_0}$ corresponds to a prior belief of fewer informative features while the magnitude $\alpha_0$ controls the confidence of prior belief, and $v_0, \mb V_0$ reflects the prior knowledge about the covariance structure of the regression coefficient $\mb W$ across different tasks. See Fig. \ref{fig: model} for a visualization of the proposed model and its cluster extension used in subsection \ref{subsec: microbiome data}.

\begin{figure*}[t]
	\centering
	\begin{subfigure}{0.8\columnwidth}
        	\centering
		\includegraphics[width=\textwidth]{images/model_base.pdf}
		\caption{The based model}
		\label{subfig: base model}
	\end{subfigure}
	\begin{subfigure}{0.8\columnwidth}
        	\centering
		\includegraphics[width=\textwidth]{images/model_clusters.pdf}
		\caption{The clustering extension}
		\label{subfig: cluster extension}
	\end{subfigure}
	\caption{Graphical visualization of the proposed probabilistic model. For the base model in (a), the information sharing cross different tasks is enforced by common priors between the regression weights and sparsity parameters. For the clustering extension (b), the information sharing is limited to tasks from the same cluster (i.e., disease category).}
	\label{fig: model}
\end{figure*}

The proposed model leads to a log conditional probability, up to an unimportant constant:
%% Note: terms only involve hyper prior parameters (e.g v0, V0, alp0,beta0 can be discarded) 
{
\small
\begin{align}
    \label{eqn: cond log-likelihood}
    &\log\left( p\left( \mb Y, \mb W, \mb \Sigma_0, \theta \mid  \mb X; v_0, \mb V_0, \alpha_0, \beta_0 \right)  \right) \nonumber \\
    &= -\frac{1}{2}\trace\left( \mb V_0^{-1} \mb \Sigma_0^{-1} \right) \nonumber \\
    &\quad +\frac{v_0+d-T-1}{2}\log\det\left( \mb \Sigma_0^{-1} \right) \nonumber \\
    &\quad -\frac{v_0}{2}\log\det\left( \mb V_0 \right) \nonumber \\
    &\quad +\left( \alpha_0-1 \right) \log\left( \theta \right) +\left( \beta_0-1 \right) \log\left( 1-\theta \right) \nonumber \\
    &\quad +\log\Gamma\left( \alpha_0+\beta_0 \right)-\log\Gamma\left( \alpha_0 \right)-\log\Gamma\left( \beta_0 \right) \nonumber \\
    &\quad -\sum_{j} \frac{1}{2} \innerprod{\mb w_{\left( j \right) }}{\mb \Sigma_0^{-1} \mb w_{\left( j \right) }} \nonumber \\
    &\quad + \sum_{t}\sum_{i} y_{t}^{i} \left( \innerprod{\left(\mb w_{t} \circ \mb z\right)}{\mb x_{t}^{i}} \right)  \nonumber \\
    &\quad -\sum_{t}\sum_{i} \log\left( \exp\left( \innerprod{\left( \mb w_{t}\circ \mb z\right)}{\mb x_{t}^{i}}\right)+1\right) \nonumber \\
    &\quad + \left( \sum_{j} z_{j} \right) \log \theta + \left( d-\sum_{j} z_{j} \right) \log\left( 1-\theta \right).
\end{align}
}
\noindent where $\Gamma$ denotes the gamma function.  

%Observe this can be seen as the classification analogous to previous proposed Bayesian Multitask Structural Learning with Wishart prior (BMSL-W)  \cite{}. 

\section{Variational Inference}
\label{sec: bmtl Inference}

With the combination of the logistic function and the hierarchical structure, inference from the exact posterior distribution of the conditional model is intractable since the posterior distribution is not available in closed form. We resort to a variational approach \cite{blei2017variational,zhang2018advances} where we approximate the posterior distribution with a simpler distribution, and the approximation is iteratively refined. We refer interested readers to \cite{blei2017variational} for a comprehensive review on variational inference (VI) as a general approach for Bayesian inference. 

Section \ref{subsec: mean field approximation and VI bound} introduces the mean-field approximation used to approximate the posterior distribution along with the variational objective function, and Section \ref{subsec: CAVI} summarizes the optimization algorithm based on coordinate ascent \cite{bishop2006pattern}.

\subsection{Mean-Field Approximation and Variational Lower Bound}
\label{subsec: mean field approximation and VI bound}

Mean field approximation is a commonly used approximation because it is expressive enough to approximate the complex posteriors, and yet simple enough to lead to tractable computations \cite{bishop2006pattern}.

In this work, we propose to approximate the true posterior of the proposed model by the function:
\begin{align}
\label{eqn: MF-approximation}
q( \theta, \mb W, \mb \Sigma_0, \mb z) &= q\left( \theta; \alpha,\beta \right)  q\left( \mb \Sigma_0^{-1}; v, \mb V \right) \nonumber \\ 
& \prod_{j} q\left( z_{j};\phi_{j} \right) q( \mb w_{\left( j \right) }; \mb m_{\left( j \right) }, \mb \Sigma_{j} ).
\end{align}

\noindent where $q\left( \theta;\alpha,\beta \right)$ is a beta distribution with parameters $\left(\alpha,\beta \right) $, $q\left( z_{j};\phi_{j} \right)$ is a Bernoulli distribution with parameters $\phi_{i}$, $q\left( \mb \Sigma_0^{-1};v, \mb V \right)$ is a Wishart distribution with parameters $\left( v, \mb V \right)$, and $q\left( \mb w_{\left( j \right)}; \mb m_{\left( j \right)}, \mb \Sigma_{j} \right)$ is a multivariate Gaussian distribution with mean $\mb m_{\left( j \right) }$ and covariance $\mb \Sigma_{j}$ for $j=1,\ldots, d$.

%Note the parameters $v, \mb V, \mb m_{\left( j \right) }$ and $\mb \Sigma_{j}$ specifies an approximation family, and in this chapter we use the same notation as for $\mb W$ for  $\mb m_{t}$ and $\mb m_{\left( j \right) }$.

% \subsubsection{ELBO and its Lower Bound}
% \label{subsec: elbo and its lower bound}

The optimization objective of variational inference (VI) is to minimize the KL-divergence between the approximation Eqn. \ref{eqn: MF-approximation} and the true posterior by maximizing the evidence lower bound (ELBO): 
{\small 
\begin{align}
    \label{eqn: elbo}
    \text{ELBO}(q) &= \expect[q]{\log f(\mb X, \mb Y, \theta, \mb z, \mb W, \mb \Sigma_0)} + \text{Entropy}(q) \nonumber \\
    &= -\frac{1}{2} \trace (v \mb V_{0}^{-1} \mb V) + \frac{v_0+d}{2} \log\det (\mb V) \nonumber \\
    &\quad + \frac{v_0+d-v}{2} \psi_{T} \left( \frac{v}{2} \right) + \frac{vT}{2}+\ln \Gamma_{T} \left( \frac{v}{2} \right) \nonumber \\
    &\quad + \left( \alpha_0+\sum_{j} \phi_{j}-\alpha \right) \psi(\alpha) +\ln B(\alpha,\beta) \nonumber \\
    &\quad +\left( \beta_0+d-\sum_{j} \phi_{j} -\beta \right)\psi(\beta) \nonumber \\
    &\quad + \left( \alpha+\beta-d-\alpha_0-\beta_0 \right) \psi(\alpha+\beta) \nonumber \\
    &\quad -\frac{1}{2} \sum_{j} \trace (v \mb V (\mb m_{(j)} \mb m_{(j)}^{\top}+\mb \Sigma_{j})) \nonumber \\
    &\quad +\sum_{t}\sum_{i} y_{t}^{i} \left( \innerprod{(\mb m_{t}\circ \mb \phi)}{\mb x_{t}^{i}} \right) \nonumber \\ 
    &\quad - \sum_{t}\sum_{i}\expect[\mb W, \mb z\sim q]{\log\left( \exp\left( \innerprod{(\mb w_{t}\circ \mb z)}{\mb x_{t}^{i}} \right)+1 \right)} \nonumber \\
    &\quad +\frac{1}{2}\sum_{j}\log\det (\mb \Sigma_{j})- \sum_{j} \phi_{j} \log(\phi_{j}) \nonumber\\ 
    &\quad - \sum_{j} (1-\phi_{j})  \log(1-\phi_{j}).
\end{align}
}
\noindent where $\psi$ is the digamma function, $\psi_{T}$ is the multivariate digamma function and $\gamma_{T}$ is the multivariate gamma function. Since the expectations of the sigmoid functions do not admit closed form solutions, we approximate the sigmoid functions by quadratic lower bounds: 
{\small 
\begin{align}
    \label{eqn: quadratic approximation to sigmoid}
    & -\log\left( \exp\left( \innerprod{(\mb w_{t}\circ \mb z)}{\mb x_{t}^{i}} \right)+1 \right)  \nonumber \\ 
    &\geq -\log\left( \exp\left( \innerprod{(\mb w'_{t}\circ \mb z')}{\mb x_{t}^{i}} \right)+1 \right) \nonumber\\
    &\quad -\frac{\innerprod{\mb x_{t}^{i}}{\mb w_{t}\circ \mb z - \mb w'_{t} \circ \mb z'}}{\exp\left( \innerprod{(\mb w'_{t}\circ \mb z')}{\mb x_{t}^{i}} \right)+1} \nonumber \\
    &\quad -\frac{1}{8} \left( \mb w_{t} \circ \mb z - \mb w'_{t}\circ \mb z' \right)^{\top} \mb x_{t}^{i} \left( \mb x_{t}^{i} \right)^{\top} \left( \mb w_{t}\circ \mb z - \mb w'_{t} \circ \mb z' \right).  
\end{align}}
\noindent for $i=1,\ldots, n_t, t=1, \ldots, T$, and $\mb w'_{t}, \mb z'$ are deterministic reference points of choice. This type of approximations has been used to design majorize-minimization (MM) algorithms for the logistic regression problem \cite{bohning1988monotonicity, hunter2004tutorial}. The resulting approximation is, up to a constant:
{
\small 
\begin{align}
    \label{eqn: elbo aprx}
    &-\sum_{t}\sum_{i}\expect[\mb W, \mb z\sim q]{\log\left( \exp\left( \innerprod{(\mb w_{t}\circ \mb z)}{\mb x_{t}^{i}} \right)+1 \right) } \nonumber \\
    &\geq -\sum_{t}\sum_{i} \log\left( \exp\left( \innerprod{(\mb w'_{t}\circ \mb z')}{\mb x_{t}^{i}}  \right)  +1 \right)  \nonumber \\
    &\quad +\sum_{t}\sum_{i} \frac{\innerprod{\mb w_{t}'\circ\mb z'}{\mb x_{t}^{i}}}{\exp\left(-\innerprod{(\mb w_{t}' \circ \mb z')}{\mb x_{t}^{i}}  \right)+1}\nonumber\\
    &\quad -\frac{1}{8} \sum_{t}\sum_{i} \innerprod{\mb w_{t}'\circ\mb z'}{\mb x_{t}^{i}}^{2}\nonumber \\
    &\quad -\sum_{t}\sum_{i} \frac{1}{\exp\left(-\innerprod{(\mb w'_{t}\circ \mb z')}{\mb x_{t}^{i}} \right)+1} \innerprod{\mb x_{t}^{i}}{(\mb m_{t} \circ \mb \phi)} \nonumber \\
    &\quad +\frac{1}{4} \sum_{t}\sum_{i} \innerprod{\mb w_{t}'\circ \mb z'}{\mb x_{t}^{i}} \innerprod{\mb m_{t} \circ \mb \phi}{\mb x_{t}^{i}}   \nonumber\\
    &\quad -\frac{1}{8}\sum_{t}\sum_{i} \innerprod{\mb m_{t} \circ \mb \phi}{\mb x_{t}^{i}}^{2}  \nonumber \\
    &\quad +\frac{1}{8} \sum_{t} \sum_{j} \left( m_{t,j}^2(\phi_{j}-1)-(\mb \Sigma_{j})_{t,t}  \right) \phi_{j} \sum_{i} (x_{t}^{ij})^{2}. 
\end{align}
}
There are other alternative approaches using different lower bounds \cite{jaakkola1997variational,drugowitsch2013variational}, chapter 10.6 of \cite{bishop2006pattern}, but they require additional variational parameters that scale with the feature dimension ($d$), which complicates the variational computations. 

\subsection{Coordinate Ascent Variational Inference (CAVI)}
\label{subsec: CAVI}

Coordinate ascent variational inference (CAVI) \cite{bishop2006pattern} is a optimization technique where we optimize one set of the variational parameters at a time while holding the others fixed. 

One useful result (Equation 18 of \cite{blei2017variational}) states that if we are to approximate a general posterior distribution $p\left(  \mb \xi \mid \text{data}  \right) $ with a mean-field approximation $q\left(  \mb \xi \right):=\prod_{j} q_{j}\left(\xi_{j} \right) $, the CAVI update for $j$-th latent variable $\xi_{j}$ (i.e., the optimal solution $q_{j}^{\star}\left( \xi_{j} \right) $) is proportional to the exponentiated conditional expected log of the joint:
\begin{equation}
\label{eqn: cavi update rule}
q^{\star}_{j} \propto \exp\left( \expect[\mb \xi_{-j}\sim q_{-j}]{\log\left( p\left( \xi_{j}, \mb \xi_{-j}  \mid  \text{data}  \right)  \right) } \right).	
\end{equation}
where $\mb \xi_{-j}$ corresponds to all but the $j$-th latent variable.  

The resulting Coordinate Ascent Variational Inference (CAVI) algorithm is summarized in Algorithm. \ref{alg: CAVI update summary}, and see section II of supplementary materials for details of the derivations. 

\begin{algorithm}[ht]
\caption{CAVI for Bayesian Multitask Sparse Logistic Regression}
\label{alg: CAVI update summary}
\begin{algorithmic}[1]
\Procedure {CAVI}{Inputs: $\left(\mb x_{t}^{i}, y_{t}^{i}\right)_{t=1,\ldots T, i=1,\ldots n_{t}}$}
\ForAll {$\text{itr}=1,\ldots, \text{Niter}$}
    \ForAll{$j=1,\ldots d$} 
    \State $\tilde{\mb X}_j \leftarrow \diag\left(\left[
            \begin{array}{c}
                \sum_{i} \phi_{j}(x_{1}^{ij})^2\\
                \vdots \\
                \sum_{i} \phi_{j}(x_{T}^{ij})^2
            \end{array}
        \right]\right)$
    \Statex
    \Statex
    \State $\mb \Sigma_{j} \leftarrow \left( v \mb V + \frac{1}{4} \tilde{\mb X_{j}} \right)^{-1}$
    \EndFor
    \ForAll{$j=1,\ldots,d$} 
        \ForAll{$t=1,\ldots,T, i=1,\ldots, n_t$} 
            \State $\tilde{y_t^i} \leftarrow \text{sigmoid}\left(\sum_j m_{tj} x_t^{ij}\phi_j\right)$
        \EndFor
        \State $\mb m_{\left( j \right) } \leftarrow \mb \Sigma_{j} \left[
        \begin{array}{c}
             \phi_{j}\sum_{i} \left(y_{1}^{i}-\tilde{y_{1}^{i}}\right) x_1^{ij} \\
             \vdots \\ 
             \phi_{j}\sum_{i} \left(y_{T}^{i}-\tilde{y_{T}^{i}}\right) x_{T}^{ij} 
        \end{array}\right]$ 
        \Statex \hspace{\algorithmicindent}$\phantom{\mb m_{\left( j \right) } \leftarrow } \qquad +\frac{1}{4} \mb \Sigma_{j} \left[
        \begin{array}{cc}
             \phi_{j}^{2} \sum_{i} \left(x_{1}^{ij}\right)^{2} m_{1j}  \\
             \vdots \\
             \phi_{j}^{2} \sum_{i} \left( x_{T}^{ij} \right)^{2} m_{Tj}
        \end{array}\right]$
    \EndFor
    \ForAll{$j=1,\ldots, d$} 
        \ForAll{$t=1,\ldots,T, i=1,\ldots, n_t$} 
            \State $\tilde{y_t^i} \leftarrow \text{sigmoid}\left(\sum_j m_{tj} x_t^{ij}\phi_j\right)$
        \EndFor
        \State $\phi_j \leftarrow \text{sigmoid} \Bigl( \psi\left(\alpha\right) - \psi\left(\beta\right) $
        \Statex \hspace{\algorithmicindent}$\phantom{\phi_j \leftarrow \text{sigmoid}(} + \sum_{t}\sum_{i} (y_{t}^{i} - \tilde{y}_{t}^{i})m_{tj} x_{t}^{ij}$
        \Statex \hspace{\algorithmicindent}$\phantom{\phi_j \leftarrow \text{sigmoid}(} + \frac{1}{4}\sum_{t}(m_{tj}^{2} \phi_j)\sum_{i}(x_{t}^{ij})^2$
        \Statex \hspace{\algorithmicindent}$\phantom{\phi_j \leftarrow \text{sigmoid}(} - \frac{1}{8}\sum_{t}m_{tj}^{2} \sum_{i}(x_{t}^{ij})^2$
        \Statex \hspace{\algorithmicindent} $\phantom{\phi_j \leftarrow \text{sigmoid}(} - \frac{1}{8}\sum_{t}(\Sigma_j)_{tt} \sum_{i}(x_{t}^{ij})^2 \Bigr)$
    \EndFor
    \State $\alpha \leftarrow \alpha_0+\sum_{j} \phi_{j}$, 
    \State $\beta \leftarrow \beta_0+d-\sum_{j} \phi_{j}$, 
    \State $\theta \leftarrow \frac{\alpha}{\alpha+\beta}$, 
    \State $v \leftarrow  v_0+d$,
    \State $\mb V \leftarrow \left( \mb V_0^{-1}+\sum_{j} \mb m_{\left( j \right)} \mb m_{\left( j \right) }^{\top} + \mb \Sigma_{j} \right)^{-1}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{sec: bmtl Experiments}

In this section, we evaluate the performance of our proposed method on both simulated data and real microbiome data pooled from multiple studies (see section I of supplementary materials for a full description and the list of references), and we compare it with following methods: 

\begin{itemize}
	\item Single-task Logistic Classifier with sparsity penalty (STL-LC) \cite{tibshirani1996regression,banerjee2008model}: this is a single-task model, where we fit independent logistic models to each task separately. This is an extension of standard LASSO to the binary classification setting. 
%	\item Kernel Support Vector Classifier (SVC) \cite{platt1999probabilistic,chang2011libsvm}: this is a single-task model, where we fit different kernelized support vector machine models to each task separately. 
%    \item Random Forrest Classifier (RF) \cite{breiman2001random}: this is a single-task model, where we fit independent random forest classifiers to each task separately. 
%	\item Naive Bayes Classifier (NB) \cite{zhang2004optimality}: this is a single-task model, 
	\item Pooled Logistic Classifier with sparsity penalty (Pooled-LC): we train a single logistic regression model on the combined data from all datasets. 
	\item MTFL (Multitask Feature Learning) \cite{liu2009multi}: this is an optimization based approach to multitask learning based on $\ell_{2,1}$-norm regularization. The proposed method can be seen as a Bayesian Hierarchical extension, and the difference in modeling is manifested in how the sparsity pattern is encouraged: we use an overparameterized Bernoulli-Gaussian model, which has better support recovery properties \cite{bazot2011bernoulli}.
	\item MSSL (Multitask Sparse Structure Learning) \cite{gonccalves2016multi}: this is an optimization based multitask learning approach, where the imposed sparsity structure is on the precision (inverse covariance matrix) of the regression coefficients across tasks. The optimization problem of this formulation is equivalent to the graphical lasso problem \cite{friedman2008sparse,mazumder2012graphical} for covariance estimation. 
\end{itemize}

\paragraph{Model Selection} For all the methods, we employ a model selection strategy based on cross-entropy loss on the validation dataset to select the hyperparameters (e.g., $\alpha_0, \beta_0, v_0, \mb V_0$ of the proposed Bayesian approach). Specifically, we use $10$ repeated runs of stratified cross-validation \cite{kohavi1995study} since we have a limited number of samples, and the corresponding class labels are imbalanced. 

\paragraph{Evaluation Metric} the microbiome studies employed for this work have imbalanced class labels. We therefore use balanced accuracy to evaluate the predictive models.

\subsection{Synthetic Datasets}
\label{subsec: synthetic datasets}

We generate synthetic datasets to examine whether our algorithm is able to: 1) recover the support of the regression coefficients that correspond to informative features for the prediction, which are evaluated by balanced accuracy; %the metrics from Table \ref{tab: bmtl eval metrics}; 
2) recover the ground truth regression coefficients (up to normalization), which are evaluated by cosine distance (i.e., one minus the normalized inner product). 

To investigate different data scenarios, we generate six datasets with varying sparsity levels (the common support of regression coefficients across tasks) and class imbalance (whether sample sizes across different tasks are of the same magnitude). Additional details of each dataset are summarized in Table \ref{tab: simulation setup}. 

\textbf{Support Recovery}: We evaluate the support recovery of the algorithms by turning the support recovery problem into a binary prediction problem. The result is summarized in %Table \ref{tab:simulated support recovery}. 
Fig. \ref{fig:simulated support recovery}. The proposed Bayesian method is the best performing algorithm in most of the metrics across all settings. In particular, when the ground truth regression coefficients have a sparse support, the proposed method achieves near-perfect recovery. %($98.8\%$). 

\textbf{Weight Recovery}: We evaluate the prediction performance of the algorithm by assessing how well the algorithms can recover the ground truth regression coefficients. Since the logistic prediction is scale invariant, we evaluate the results by the cosine distance. The result is summarized in Table \ref{tab: simulated weight recovery}. The proposed Bayesian method is the best performing algorithm in all but the dense case. 

\begin{table}[htbp]
	\centering
	\begin{tabular}{ccc}\toprule
	& {Balanced} & {Imbalanced}\\ \midrule
	{Dense} $\left(\theta=0.8\right)$ & {Dataset-1} & {Dataset-4} \\ 
	{Sparse} $\left(\theta=0.2\right)$ & {Dataset-2} & {Dataset-5} \\
    {Ultra Sparse} $\left( \theta=0.05 \right)$ & {Dataset-3} & {Dataset-6} \\  \bottomrule
	\end{tabular}
	\caption{Summary of the simulated dataset, where the sample sizes of balanced data are generated by a Poisson distribution with rate $24$, and sample sizes of the imbalanced data are generated by a negative binomial distribution with stopping-time parameter $r=1$ and probability of success $p=0.04$. For all the simulations, the number of features is $100$ and number of tasks is $10$. For the imbalanced datasets, we amplify all the sample sizes by $6$ to ensure that both positive samples and negative samples are present across all tasks. Both settings have an expected sample size $30$, where the imbalanced case has more variations in sample sizes among different tasks. The $\theta$ parameter corresponds to the expected percentage of the predictive features.}
 %will corresponds to smaller effective size hence resulting in a harder prediction problem.}
	\label{tab: simulation setup}
\end{table}

% \begin{table}[!t]
% 	\centering
% 	\begin{tabular}{|c|c|c|c|}
% 	\hline 
% 	& Dense (θ=0.8)\left(\theta=0.8\right) & Sparse (θ=0.2)\left(\theta=0.2\right) & Ultra Sparse (θ=0.05)\left( \theta=0.05 \right)  \\
% 	\hline 
% 		Balanced ∼Pois(24)+6\sim \text{Pois}\left( 24 \right)+6  & dataset1 & dataset2 & dataset3 \\
% 	\hline
% 	        Imbalanced ∼NB(1,0.04)+6\sim \text{NB}\left( 1,0.04 \right)+6   & datasett4 & dataset5 & dataset6\\
% 	\hline
% 	\end{tabular}
% 	\caption{Summary of the simulated dataset, where ∼Pois\sim \text{Pois} means the number of samples is Poisson distributed, and ∼NB\sim \text{NB} means the number of sample follow a negative binomial distribution. For all the simulation the number of features is 100100 and number of tasks is 1010.  For the imbalanced datasets, We add all the sample sizes by 66 to ensure that both positive samples and negative samples are present across all tasks. Both settings have an expected sample size 3030, with the imbalanced case has more variations of sample sizes among different tasks.  The θ\theta parameter corresponds to the expected percentage of the predictive features.}
%  %will corresponds to smaller effective size hence resulting in a harder prediction problem.}
% 	\label{tab: simulation setup}
% \end{table}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset0.pdf}
		\caption{Dataset1: balanced and dense} 
    \end{subfigure}
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset1.pdf}
		\caption{Dataset2: balanced and sparse}
    \end{subfigure}  
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset2.pdf}
		\caption{Dataset3: balanced and ultra sparse}
    \end{subfigure}
    
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset3.pdf}
		\caption{Dataset4: imbalanced and dense}
    \end{subfigure}
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset4.pdf}
		\caption{Dataset5: imbalanced and sparse}
    \end{subfigure}
    \begin{subfigure}{0.6\columnwidth}
        \centering
		\includegraphics[width=\textwidth]{images/ba_dataset5.pdf}
		\caption{Dataset6: imbalanced and ultra sparse}
    \end{subfigure}
    \caption{Summary of the support recovery results for the simulated data evaluated using balanced accuracy across $10$ different runs. The proposed Bayesian approach (BayesMTL) outperforms the benchmark methods in terms of balanced accuracy especially when there is a shared sparsity structure across regression coefficients of different tasks. Both MSSL and MTFL prioritize the prediction performance in the cross-validation step which results in complete dense solutions (i.e., all regression coefficients are non-zero), hence they achieve identical accuracy.}
    \label{fig:simulated support recovery}
\end{figure*}

%\begin{table}[htpb]
%	\centering 
%        \resizebox{0.95\columnwidth}{!}{
%        \begin{tabular}{|p{1.5cm}|c|c|c|c|c|}
%    	\hline
%    	 Dataset  & BayesMTL & MTFL & MSSL   & STL-LC & Pooled-LC \\ 
%    	\hline  	
%    	dataset1 (dense, balanced)  & 0.736 (0.06)  & 0.561 (0.02) & \textbf{0.554} (0.02) & 0.735 (0.02) & 0.983 (0.04) \\
%    	\hline
%    	dataset2 (sparse, balanced)  & \textbf{0.341} (0.09) & 0.566 (0.01) & 0.563 (0.01) & 0.560 (0.05) & 1.02 (0.05) \\ 
%    	\hline
%    	dataset3 (ultra sparse, balanced)  & \textbf{0.098} (0.04) & 0.589 (0.03) & 0.598 (0.02)  & 0.257 (0.04) & 0.995 (0.12) \\ 
%    	\hline
%    	dataset4 (dense, imbalanced)  & 0.670 (0.10) & 0.582 (0.08) & \textbf{0.576} (0.05)  & 0.736  (0.07 )& 1.02 (0.03) \\ 
%    	\hline
%    	dataset5 (sparse, imbalanced)  & \textbf{0.456} (0.11) & 0.597 (0.08) & 0.576 (0.04) & 1.04 (0.06) & 0.571 (0.09)\\ 
%    	\hline
%    	dataset6 (ultra sparse, imbalanced)  & \textbf{0.223} (0.12) & 0.651 (0.05) & 0.612 (0.04) & 0.353 (0.11) & 1.07 (0.10)  \\ 
%            \hline
%	\end{tabular}}
%	\caption{Summary of the weights recovery measured in cosine distance. The cosine distance is bounded between 00 and 22 with 00 means perfect recovery. The proposed Bayesian approach outperforms the benchmark methods in all evaluation metrics when there is a shared sparsity structure across regression coefficients of different tasks.}
%	\label{tab: simulated weight recovery} gadao,
%\end{table}

\begin{table}[htbp]
	\centering
        \resizebox{0.99\columnwidth}{!}{
	%\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{l cccccc}
		\toprule
		 & {BayesMTL} & {MTFL} & {MSSL} & {STL-LC} & {Pooled-LC} \\ 
		\midrule  	
		Dataset-1 & 0.74 (0.06) & 0.56 (0.02) & \bfseries 0.55 (0.02) & 0.73 (0.02) & 0.98 (0.04) \\
		Dataset-2 & \bfseries 0.34 (0.09) & 0.56 (0.01) & 0.56 (0.01) & 0.56 (0.05) & 1.02 (0.05) \\ 
		Dataset-3 & \bfseries 0.10 (0.04) & 0.59 (0.03) & 0.59 (0.02) & 0.26 (0.04) & 0.99 (0.12) \\ 
		Dataset-4 & 0.67 (0.10) & 0.58 (0.08) & \bfseries 0.58 (0.05) & 0.73 (0.07) & 1.02 (0.03) \\ 
		Dataset-5 & \bfseries 0.46 (0.11) & 0.59 (0.08) & 0.57 (0.04) & 1.04 (0.06) & 0.57 (0.09)\\ 
		Dataset-6 & \bfseries 0.22 (0.12) & 0.65 (0.05) & 0.61 (0.04) & 0.35 (0.11) & 1.07 (0.10)  \\ 
		\bottomrule
	\end{tabular}}
	\caption{Summary of the weights recovery measured in cosine distance. The cosine distance is bounded between $0$ and $2$ with $0$ meaning perfect recovery. The proposed Bayesian (BayesMTL) approach outperforms the benchmark methods when there is a shared sparsity structure across regression coefficients of different tasks.}
	\label{tab: simulated weight recovery}
\end{table}


\subsection{Microbiome Data}
\label{subsec: microbiome data}

In this subsection, we demonstrate a real world application of the proposed model on previously published microbiome data discussed in Section \ref{sec: intro}. Our goal is two fold: 1) show that multitask learning on the one-hot coded vector of diseases can perform accurate disease state classification with uncertainty quantification, and 2) identify the microbes that are associated with human diseases of interest.  

\textbf{Prediction Performance}: analogous to the previous subsection, we evaluate the prediction performance using balanced accuracy, and the result is visualized in Fig. \ref{fig: microbiome prediction}.
Despite the highly heterogeneous nature of the datasets (i.e., pooled from studies with different experimental objectives, laboratory protocols, sequencing instrumentation, patient demographics etc.), we observe improvements in multitask learning over traditional single-task learning methods for most of the coarser taxonomic ranks, namely Kingdom, Phylum, Class, and Order. This suggests the presence of shareable information across the studies that can be leveraged by the machine learning models. Additionally, our proposed model consistently provides sparse solutions, as evaluated by sparsity ratios (i.e., percentage of the regression coefficients that are zero) as shown in Table \ref{tab: microbiome sparsity ratios}. This aspect is particularly desirable for focusing microbiological analysis of the role of specific microbes in human health.
%Bayesian model provide additional uncertainty quantification along with feature selections which are highlighted in . 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.45\columnwidth}
        \centering		\includegraphics[width=\textwidth]{images/ba_Kingdom.pdf}
		\caption{Kingdom} 
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ba_Phylum.pdf}
		\caption{Phylum}
    \end{subfigure}   
    \begin{subfigure}{0.45\columnwidth}
        \centering
	\includegraphics[width=\textwidth]{images/ba_Class.pdf}
		\caption{Class}
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering		\includegraphics[width=\textwidth]{images/ba_Order.pdf}
		\caption{Order}
    \end{subfigure}

    \begin{subfigure}{0.45\columnwidth}
        \centering
	\includegraphics[width=\textwidth]{images/ba_Family.pdf}
		\caption{Family}
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering		\includegraphics[width=\textwidth]{images/ba_Genus.pdf}
		\caption{Genus}
    \end{subfigure}     
    \begin{subfigure}{0.45\columnwidth}
        \centering
	\includegraphics[width=\textwidth]{images/ba_Species.pdf}
		\caption{Species}
    \end{subfigure}
    \caption{Summary of the prediction performance evaluated by balanced accuracy across 7 taxonomic ranks. Due to the heterogeneous nature of the data, we do not see an improvement of the proposed approach over single-tasked models. However, the proposed approach is the only multitask method that provides a sparse solution, i.e., identifying common microbes across studies of the same disease category that are informative for prediction along with uncertainty quantification through the posterior distribution.}
    \label{fig: microbiome prediction}
\end{figure}

\begin{table}[htbp]
	\centering
        \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}\toprule
            Taxon Ranks &    BayesMTL &          MTFL &            MSSL &       Pooled-LC &          STL-LC \\ \midrule
            Kingdom  &  0.29 (0.01) & 0.54 (0.03) &  0.47 (0.03) &  0.88 (0.04) &          \bf{0.26} (0.02) \\
            Phylum &            0.18 (0.02) & 0.30 (0.002) &  0.31 (0.003) &  0.78 (0.03) & \bf{0.086} (0.02) \\
            Class &             0.12 (0.01) & 0.24 (0.006) &  0.30 (0.05) &  0.69 (0.02) & \textbf{0.070} (0.005) \\
            Order &             0.11 (0.01) & 0.20 (0.01) &  0.22 (0.01) &  0.54 (0.009) & \textbf{0.046} (0.008)  \\
            Family &    0.043 (0.02) & 0.41 (0.3) &  0.17 (0.002) &  0.41 (0.003) &         \bf{0.041} (0.007) \\
            Genus &     \bf{0.018} (0.007) & 0.40 (0.3) &  0.15 (0.0001) &  0.33 (0.003) &  0.035 (0.006) \\
            Species &   \bf{0.029} (0.005) & 0.15 (0.03) &  0.15 (0.0008) &  0.28 (0.004) & 0.032 (0.005) \\ \bottomrule
          \end{tabular}
          }
	   \caption{Summary of the results for sparsity ratios (percentage of regression coefficients that are zero). The bold number indicates that the corresponding method gives the sparsest result for the given taxonomic rank, and the values in parentheses represent standard deviations computed over $5$ different runs. The proposed approach is the only multitask method that provides a sparse solution, identifying common microbes across studies of the same disease category that are informative for predicting health status.}
	\label{tab: microbiome sparsity ratios}
\end{table}

\textbf{Goodness of fit}: we evaluate the goodness of fit of the proposed method through calibration curve \cite{niculescu2005predicting}, which plots the predicted probability against the observed labels. For a well calibrated probabilistic model, among all the samples that the model predicted with probability $p\%$ being healthy, close to $p\%$ of them will indeed be healthy. The calibration results are summarized in Fig. \ref{fig: microbiome calibration plots} for both the training data and test data. The proposed Bayesian approach provides additional uncertainty quantification for the predicted probabilities. Since the proposed model is probabilistic, it provides well calibrated results. The performance degrades at the boundary values for the test data, which indicates that the choice of logit function as a link function is resulting in over-confident predictions. We discuss possible extensions in Section \ref{sec: bmtl Conclusion}. 

\begin{figure}[htbp]
	\centering
	\begin{subfigure}{0.45\columnwidth}
        	\centering
	\includegraphics[width=\textwidth]{images/y_pred_train_hist_Order.pdf}
		\caption{Predicted probabilities on training data.}
		\label{subfig: y_pred_train predict}
	\end{subfigure} 
	\begin{subfigure}{0.45\columnwidth}
        	\centering		\includegraphics[width=\textwidth]{images/y_pred_test_hist_Order.pdf}
		\caption{Predicted probabilities on test data.}
		\label{subfig: y_pred_test predict}
	\end{subfigure}
 
	\begin{subfigure}{0.5\columnwidth}
        	\centering
	\includegraphics[width=\textwidth]{Calibration_curve_Order.pdf}
		\caption{Calibration curves for the training data and the test data with 90\% marginal credible intervals.}
		\label{subfig: calibration}
	\end{subfigure}
	\caption{Calibration analysis for the proposed model on the Order taxonomic rank. Fig. (a) and Fig. (b) show the histograms of the predicted probabilities and training and test data respectively. Due to the choice of logit as link function, the predicted probabilities are concentrated around the boundaries. Fig. (c) shows the calibration curves of the predictions from training and test data. The model achieves near perfect calibration on the training data, and the degradation of performance on the test data at the boundary values indicates that the logit function as a link function is resulting in over-confident predictions.}
	\label{fig: microbiome calibration plots}
\end{figure}

\textbf{Sparsity visualization}: we assign a sparsity coefficient to each of the taxIDs by combining the magnitude of the regression coefficient ($\{ \mb w_{t} \}$) and the sparsity parameters ($\mb z$). From the estimated posterior distribution, we draw samples to explore the full distribution of the sparsity coefficient. The most relevant features will correspond to the features with consistent high sparsity coefficients across draws. The result is summarized in Fig. \ref{fig: microbiome sparsity plot Order}. The proposed model learns a sparse set of features shared across different datasets of the same disease from the data as reflected by colored strips. For diabetes and diarrhea, few taxIDs are considered informative for the health prediction task by the model, while for cardiovascular disease, the majority of taxIDs are considered informative. 

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/cirrhosis_feature_importance_Order.pdf}
        \caption{\footnotesize{Cirrhosis}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/autoimmune_feature_importance_Order.pdf}
        \caption{\footnotesize{Autoimmune Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/cardiovascular_feature_importance_Order.pdf}
        \caption{\footnotesize{Cardiovascular Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/diarrhea_feature_importance_Order.pdf}
        \caption{\footnotesize{Diarrhea}}
    \end{subfigure}\hspace*{-1em}
    
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/cancer_feature_importance_Order.pdf}
        \caption{\footnotesize{Cancer}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/dermatologic_feature_importance_Order.pdf}
        \caption{\footnotesize{Dermatologic}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/oral_feature_importance_Order.pdf}
        \caption{\footnotesize{Oral Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/inflammatory_bowel_disease_feature_importance_Order.pdf}
        \caption{\footnotesize{Bowel Disease}}
    \end{subfigure}\hspace*{-1em}

    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/hormonal_feature_importance_Order.pdf}
        \caption{\footnotesize{Hormonal Disorder}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/metabolic_feature_importance_Order.pdf}
        \caption{\footnotesize{Metabolic Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/pulmonary_feature_importance_Order.pdf}
        \caption{\footnotesize{Pulmonary Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/genitourinary_feature_importance_Order.pdf}
        \caption{\footnotesize{Genitourinary Disease}}
    \end{subfigure}\hspace*{-1em}

    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/immune_feature_importance_Order.pdf}
        \caption{\footnotesize{Immune Disease}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/neurological_feature_importance_Order.pdf}
        \caption{\footnotesize{Neurological Disease}}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/seafaring_syndrome_feature_importance_Order.pdf}
        \caption{\footnotesize{Seafaring Syndrome}}
    \end{subfigure}\hspace*{-1em}
    \begin{subfigure}[b]{0.45\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/irritable_bowel_syndrome_feature_importance_Order.pdf}
        \caption{\footnotesize{Irritable Bowl Syndrome}}
    \end{subfigure}\hspace*{-1em}
    
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/diabetes_feature_importance_Order.pdf}
        \caption{\footnotesize{Diabetes}}
    \end{subfigure}\hspace*{-1em}  
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/gastrointestinal_infection_feature_importance_Order.pdf}
        \caption{\footnotesize{Gastrointestinal Infection}}
    \end{subfigure}
    \begin{subfigure}[b]{0.6\columnwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/age-related_macular_degeneration_feature_importance_Order.pdf}
        \caption{\footnotesize{Aged-related Macular Degeneration}}
    \end{subfigure}
    \caption{Feature sparsity visualization across $19$ different disease categories of Order taxonomic rank. The $x$-axis corresponds to different samples drawn from the posterior distribution and the $y$-axis correspond to different taxIDs. The gradation from white to black for variable color corresponds to its increasing importance weight, and the darker shaded horizontal lines represent the sparse features selected by the algorithm. For diabetes and diarrhea, few taxIDs are considered informative for the health prediction task by the model, while for cardiovascular disease the majority of taxIDs are considered informative.}
    \label{fig: microbiome sparsity plot Order}
\end{figure*}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/cirrhosis_feature_importance_Order.pdf}
%         \caption{\footnotesize{Cirrhosis}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/autoimmune_feature_importance_Order.pdf}
%         \caption{\footnotesize{Autoimmune Disease}}
%     \end{subfigure}\hspace*{-1em}

%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/cardiovascular_feature_importance_Order.pdf}
%         \caption{\footnotesize{Cardiovascular Disease}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/diarrhea_feature_importance_Order.pdf}
%         \caption{\footnotesize{Diarrhea}}
%     \end{subfigure}\hspace*{-1em}
    
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/cancer_feature_importance_Order.pdf}
%         \caption{\footnotesize{Cancer}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/dermatologic_feature_importance_Order.pdf}
%         \caption{\footnotesize{Dermatologic}}
%     \end{subfigure}\hspace*{-1em}

%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/oral_feature_importance_Order.pdf}
%         \caption{\footnotesize{Oral Disease}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/inflammatory bowel disease_feature_importance_Order.pdf}
%         \caption{\footnotesize{Bowel Disease}}
%     \end{subfigure}\hspace*{-1em}

%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/hormonal_feature_importance_Order.pdf}
%         \caption{\footnotesize{Hormonal Disorder}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/metabolic_feature_importance_Order.pdf}
%         \caption{\footnotesize{Metabolic Disease}}
%     \end{subfigure}\hspace*{-1em}

%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/pulmonary_feature_importance_Order.pdf}
%         \caption{\footnotesize{Pulmonary Disease}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/genitourinary_feature_importance_Order.pdf}
%         \caption{\footnotesize{Genitourinary Disease}}
%     \end{subfigure}\hspace*{-1em}
% \end{figure}
% \begin{figure}[ht]\ContinuedFloat
%     \centering
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/immune_feature_importance_Order.pdf}
%         \caption{\footnotesize{Immune Disease}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/neurological_feature_importance_Order.pdf}
%         \caption{\footnotesize{Neurological Disease}}
%     \end{subfigure}
    
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/seafaring syndrome_feature_importance_Order.pdf}
%         \caption{\footnotesize{Seafaring Syndrome}}
%     \end{subfigure}\hspace*{-1em}
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/irritable bowel syndrome_feature_importance_Order.pdf}
%         \caption{\footnotesize{Irritable Bowl Syndrome}}
%     \end{subfigure}\hspace*{-1em}
    
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/diabetes_feature_importance_Order.pdf}
%         \caption{\footnotesize{Diabetes}}
%     \end{subfigure}\hspace*{-1em}  
%     \begin{subfigure}[b]{0.45\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/gastrointestinal infection_feature_importance_Order.pdf}
%         \caption{\footnotesize{Gastrointestinal Infection}}
%     \end{subfigure}

%     \begin{subfigure}[b]{0.6\columnwidth}
%         \centering
%         \includegraphics[width=1\textwidth]{images/age-related macular degeneration_feature_importance_Order.pdf}
%         \caption{\footnotesize{Aged-related Macular Degeneration}}
%     \end{subfigure}
%     \caption{Feature sparsity visualization across $19$ different disease categories of Order taxonomic rank. The $x$-axis corresponds to different samples drawn from the posterior distribution and the $y$-axis correspond to different taxIDs. The gradation from white to black for variable color corresponds to its increasing importance weight, and the darker shaded horizontal lines represent the sparse features selected by the algorithm. For diabetes and diarrhea, few taxIDs are considered informative for the health prediction task by the model, while for cardiovascular disease the majority of taxIDs are considered informative.}
%     \label{fig: microbiome sparsity plot Order}
% \end{figure}

\textbf{Feature Selection for Diarrhea Datasets}: to further investigate the feature selection capability of the model, we focus on the diarrhea datasets \cite{david2015gut, kieser2018bangladeshi,mcdonald2018american} since they have shown consistent sparsity patterns (Fig. \ref{fig: microbiome sparsity plot Order}) and are of particular direct relevance to fecal microbiome and gastrointestinal health. Centered log ratio (CLR) transform is a normalization technique applied to each sample across features, therefore the magnitude of the transformed data varies across features.  We propose to assign feature importance of $j$-th feature (taxID) for task $t$ and sample $i$ by combining the regression coefficient ($\{ \mb w_{t} \}$), the sparsity parameters ($\mb z$) and the CLR transformed abundance data ($\mb x_t^i$):
\begin{equation}
\label{eqn: feature importance} 
\abs{\frac{w_{t,j} z_j x_t^{ij} }{\sqrt{\sum_j \left(w_{t,j} z_j x_t^{ij}\right)^2}}}.
\end{equation}
The proposed importance weight can be interpreted as the relative contribution to the predicted log-odds ratio. From the estimated posterior distribution, we draw samples to explore the full distribution of the feature importance. The most important features will correspond to the features with consistently high importance weights across draws, samples, and tasks. After obtaining important features for each taxonomic rank, we visualize the result using taxonomic lineage information, where lineages with at least $5$ taxIDs selected are shown in Fig. \ref{fig: microbiome lineage}. 

The majority of lineages identified by the model are relevant in the context of the gut microbiome and diet, and highlights the association of \emph{Escherichia coli}, gut bacteriophages (order \emph{Crassvirales}) and a gut fungi, \emph{Debaryomycetaceae} with diarrhea. \emph{Enteroaggregative E. coli}, in particular has been routinely discussed to play a causal role in diarrhea \cite{robins2002escherichia,okhuysen2010enteroaggregative,petro2020genetic}. %While the role of Crassvirales bacteriophages in the human gut is unclear \cite{smith2023bacteriophages}, the importance of phageome stability in health and disease have been discussed \cite{manrique2017human}. 
Although our analysis does link \emph{Petitvirales} to healthy controls, these results associate \emph{Crassvirales} with diarrhea (Fig. \ref{fig: microbiome lineage}), while previous reports link the abundance of \emph{Crassvirales} with gut health. A number of factors may influence the gut microbiome composition including age, diet, and geographic location \cite{smith2023bacteriophages,gregory2020gut,ezzatpour2023human} thus the datasets used to establish this model may have been influenced by population demographics. Specifically, the diarrhea datasets used here are derived from a mixed population of subjects from Bangladesh and the United States \cite{david2015gut,kieser2018bangladeshi,mcdonald2018american}. Additionally, this analysis does not capture other global variables that can contribute to a disease state such as differences in virome richness and diversity \cite{zhu2018visualization}. Finally, differences in bioinformatics pipelines and reference databases used to classify sequences likely contributes to reported differences. For example, up to 99\% of phage sequences do not map to reference viral genomes \cite{gregory2020gut,ezzatpour2023human} thus increasing the likelihood of variability in identification of bacteriophage genomes, which are known for their complex and mosaic genome composition. Furthermore, the identification of plant-related features within the Magnoliopsida class suggest the capability of our approach to identify potentially disease-associated dietary components for hypothesis generation. Bacteriophages belonging to orders \emph{Crassvirales} and \emph{Petitvirales} are highly abundant in the human gut microbiome and have been associated with health \cite{ezzatpour2023human,shkoporov2019human,shkoporov2024dynamic,smith2023bacteriophages}.

% \begin{table}[ht]
% 	\centering
%         \resizebox{\columnwidth}{!}{
%         \begin{tabular}{lccccc}\toprule
%             Taxon Ranks &    BayesMTL &          MTFL &            MSSL &       Pooled-LC &          STL-LC \\ \midrule
%             Kingdom  &  0.288 (0.0102) & 0.541 (0.0298) &  0.467 (0.0267) &  0.883 (0.0408) &          \bf{0.263} (0.0203) \\
%             Phylum &            0.182 (0.0169) & 0.301 (0.00222) &  0.311 (0.00286) &  0.781 (0.0247) & \bf{0.0856} (0.02) \\
%             Class &             0.116 (0.0121) & 0.24 (0.00565) &  0.295 (0.0531) &  0.692 (0.019) & \textbf{0.0697} (0.00536) \\
%             Order &             0.109 (0.0115) & 0.199 (0.00967) &  0.222 (0.0103) &  0.536 (0.00935) & \textbf{0.046} (0.00772)  \\
%             Family &    0.0429 (0.0151) & 0.409 (0.293) &  0.172 (0.00179) &  0.414 (0.00336) &         \bf{0.0407} (0.00736) \\
%             Genus &     \bf{0.0181} (0.00677) & 0.403 (0.281) &  0.152 (0.000116) &  0.326 (0.00331) &  0.0351 (0.00621) \\
%             Species &   \bf{0.0289} (0.0053) & 0.149 (0.0275) &  0.149 (0.000836) &  0.279 (0.00367) & 0.0319 (0.00481) \\ \bottomrule
%           \end{tabular}
%           }
% 	   \caption{Summary of the results for sparsity ratios (percentage of regression coefficients that are zero). The bold number indicates that the corresponding method gives the sparsest result for the given taxonomic rank, and the values in parentheses represent standard deviations computed over $5$ different runs. The proposed approach is the only multitask method that provides a sparse solution, identifying common microbes across studies of the same disease category that are informative for predicting health status.}
% 	\label{tab: microbiome sparsity ratios}
% \end{table}

\begin{figure}[ht] 
        \centering
	\begin{subfigure}{0.85\columnwidth}
        \centering
	\includegraphics[width=\textwidth]{images/linage_sk_Eukaryota.pdf}
		\caption{Eukaryota}
		\label{subfig: linage_Eukaryota}
	\end{subfigure}
 % \end{figure}
 % \begin{figure}[ht]\ContinuedFloat
 %    \centering
 	\begin{subfigure}{0.85\columnwidth}
        \centering		
        \includegraphics[width=\textwidth]{images/linage_sk_Viruses.pdf}
		\caption{Viruses}
		\label{subfig: linage_Viruses}
	\end{subfigure}
 
 	\begin{subfigure}{0.85\columnwidth}
        \centering       
        \includegraphics[width=\textwidth]{images/linage_sk_Bacteria.pdf}
        \caption{Bacteria}
        \label{subfig: linage_Bacteria}
    \end{subfigure}
    \caption{Lineage analysis of the important features in diarrhea datasets for three distinct taxonomic categories: Eukaryota, Viruses and Bacteria. Each node in the tree corresponds to a taxID. A positive sign (red colored) implies the given taxID is disease associated while a negative sign (green colored) implies the taxID is control associated. The identified lineages include \emph{Escherichia coli} and \emph{Crassvirales} bacteriophages, which have been studied in the context of diarrhea \cite{robins2002escherichia,okhuysen2010enteroaggregative,petro2020genetic,smith2023bacteriophages}.}
    \label{fig: microbiome lineage}
\end{figure}

\section{Conclusion}
\label{sec: bmtl Conclusion}
 
In this work a hierarchical sparse Bayesian multitask logistic regression model is proposed to predict human health status from human microbiome abundance data. The model is designed to select common informative features across different tasks through the built-in sparsity structure. We derive a computationally efficient inference algorithm based on variational inference. Our simulation studies show that the proposed approach excels in situations when there are shared sparsity structures of the regression coefficients across the different tasks. Our experiments on a real-world dataset pooled from multiple studies demonstrate the utility of the method to extract informative taxa from a large microbiome dataset, while providing well-calibrated predictions with uncertainty quantification.

There are several directions for future work. One direction is to replace the logit function with other link functions (e.g., a probit link function) that have flatter tails so the model is less prone to overconfidence. A second direction is to incorporate additional hierarchical structures into the model that include relevant metadata information such as population demographics. A third direction is to extend our model to multi-label classification problems, where each task contains multiple binary predictions (e.g., diagnosis of different diseases on the same patient). This generalization is of particular interest to the human health prediction application considered in this manuscript, since the diseases are not mutually exclusive. Another related extension is to consider the multiclass classification problem, where each task is a classification problem with more than $2$ labels (e.g., different stages of a disease). 

\bibliographystyle{unsrt}
\bibliography{main}

\begin{IEEEbiographynophoto}{Haonan Zhu}
received the Ph.D. degree in electrical and computer engineering from the University of Michigan in 2023. He is currently a Postdoctoral
Researcher with the Lawrence Livermore National Laboratory. 
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Andre R Goncalves} 
received the Ph.D. degree in electrical and computer engineering from the University of Campinas in Brazil in 2016. He is currently a Machine Learning Research Scientist with the Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Hiranmayi Ranganathan} 
received her Ph.D. degree in Deep Learning from Arizona State University in 2018. She is currently Group Leader of the Computer Vision Group with Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Camilo Valdes} 
received his Ph.D. degree in Computer Science from the School of Computing and Information Sciences at Florida International University. He is currently a Postdoctoral Researcher in the Physical and Life Sciences Directorate at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Boya Zhang} 
received her Ph.D. in statistics from the Virginia Tech in 2020. She is now a Staff Scientist at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Jose Manuel Martí} 
received a Ph.D. in Computational Biology and an M.Sc. in Astrophysics from the University of Valencia (est. 1498). He is an engineer at Lawrence Livermore National Laboratory and an affiliate at Berkeley Lab.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Car Reen Kok} 
received a Ph.D. degree in Complex Biosystems from the University of Nebraska-Lincoln. She is a Postdoctoral Researcher in the Microbiology/Immunology group at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Monica Borucki} 
received her Ph.D. degree in Microbiology from Colorado State University. She is currently a Biomedical Scientist at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Nisha Mulakken}
received a M.A. degree in biostatistics from UC Berkeley and B.S. degree in Genetics from UC Davis. She is currently a Group Leader and Bioinformatics Software Developer at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{James Thissen}
received a Masters degree in bioinformatics from the Johns Hopkins University in 2017. He is currently a researcher with Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Crystal Jaing} received her Ph.D. in Molecular Biology and Biochemistry from Indiana University School of Medicine. She is currently a research scientist at Lawrence Livermore National Lab.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Alfred Hero}
(Life Fellow of the IEEE) received the PhD in Electrical Engineering and Computer Science from Princeton University. He is the John H. Holland Distinguished University Professor of Electrical Engineering and Computer Science and the R. Jamison and Betty Williams Professor of Engineering at the University of Michigan, Ann Arbor.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Nicholas A. Be} 
received a Ph.D. degree in Cellular and Molecular Medicine from the Johns Hopkins University School of Medicine. He is the Group Leader for Microbiology/Immunology at Lawrence Livermore National Laboratory.
\end{IEEEbiographynophoto}
\end{document}
% \section{Appendix}
% \label{sec: Appendix}

% \subsection{Discussion on the Quadratic Lower bound}
% \label{subsec: discussion on the qudratic lower bound}	

% In this subsection, I will discuss more in depth the quadratic lower bound we used. This is a result has been used in the literature to design MM algorithms to solve standard logistic regression \cite{bohning1988monotonicity, hunter2004tutorial}. The function of interests is:
% \[
% 	f\left( \mb w \right) := -\log\left( \exp\left(\innerprod{\mb w}{\mb x}\right)+1\right)
% .\]

% We can compute its gradient and hessian, which is given by:
% \begin{align}
% 	\nabla_{\mb w} f\left( \mb w \right) & = -\frac{\mb x}{\exp(-\innerprod{\mb w}{\mb x})+1} \label{eqn: gradient} \\
% 	\nabla_{\mb w}^{2} f\left( \mb w \right) &= -\left( \frac{1}{\exp(-\innerprod{\mb w}{\mb x})+1} \right) \left(1-\frac{1}{\exp(-\innerprod{\mb w}{\mb x})+1}  \right) \mb x \mb x^{\top} 	\label{eqn: hess}
% \end{align}

% We can make two observations:
% \begin{enumerate}
% 	\item Function f(\mbw)f\left( \mb w \right)  is concave by second order condition
% 	\item ∇2\mbwf(\mbw)⪰−14\mbx\mbx⊤\nabla_{\mb w}^{2} f\left( \mb w \right) \succeq -\frac{1}{4} \mb x \mb x^{\top}
% \end{enumerate}

% By the second observation, we have f(\mbw)+18(\mbw⊤\mbx)2f\left( \mb w \right)+\frac{1}{8} \left( \mb w^{\top} \mb x \right)^{2} is convex (by second order condition), then by first order condition of convex and smooth function along with some algebra we have the following quadratic lower bound:
% \begin{align}
% \label{eqn: qudratic lower bound}
% f\left( \mb w \right) & \geq f\left( \mb w' \right) + \nabla f\left( \mb w' \right)^{\top} \left( \mb w-\mb w' \right)  \nonumber \\
% 		      &- \frac{1}{8}\left( \mb w-\mb w' \right)^{\top} \mb x \mb x^{\top} \left( \mb w-\mb w' \right)  \nonumber\\
% 		      & = f\left( \mb w' \right)-\nabla f\left( \mb w' \right)^{\top} \mb w'-\frac{1}{8} \left( \innerprod{\mb w'}{\mb x} \right)^{2} \nonumber \\
% 		      & +\nabla f\left( \mb w' \right)^{\top} \mb w -\frac{1}{8} \left( \mb w^{\top} \mb x \right)^{2} \nonumber \\
% 		      %& -\frac{1}{\exp\left( -\left( \mb w' \right)^{\top} \mb x  \right)+1} \mb w^{\top} \mb x-\frac{1}{8} \left( \mb w^{\top} \mb x \right)^{2} \nonumber \\
% 		      &+\frac{1}{4} \innerprod{\mb w'}{\mb x} \innerprod{\mb w}{\mb x}
% \end{align}

% for any reference $\mb w'$, and we will denote the lower bound (subjugate function or minorizer) as $\mb g\left( \mb w;\mb w' \right) $.


% \subsection{CAVI update derivation}
% \label{subsec: CAVI update derivation}

% This subsection includes the derivations of CAVI updates for Algorithm I. 

% \paragraph{Update for $\alpha, \beta$}

% Based on Eqn. \ref{eqn: cavi update rule}, the exponentiated conditional expectation of all the parameters except $\theta$ up to a constant scaling factor:
% \begin{align*}
% 	q^{\star} \left( \theta \right) & \propto \exp\left( \left(\alpha_0-1\right) \log\theta +\left( \beta_0-1 \right)\log\left( 1-\theta \right) \right) \\
%      & \quad \exp\left(\left( \sum_{j} \phi_{j} \right) \log \theta + \left( d-\sum_{j} \phi_{j}\right) \log\left( 1-\theta \right)      \right) \\
% 					& = \exp\left( \left( \alpha_0-1+\sum_{j} \phi_{j} \right) \log\theta \right)\\
%      & \quad \exp\left(\left( \beta_0+d-\sum_{j} \phi_{j} -1 \right) \log\left( 1-\theta \right)     \right).
% \end{align*}

% This implies $q^{\star}\left( \theta \right)$ follows a beta distribution with parameters:
% \begin{align*}
% 	\alpha & = \alpha_0+\sum_{j} \phi_{j}, \\
% 	\beta & = \beta_0+d-\sum_{j} \phi_{j}.
% \end{align*}

% \paragraph{Update for $v$ and  $\mb V$} Based on Eqn. \ref{eqn: cavi update rule}, the exponentiated conditional expectation of all the parameters except $\mb \Sigma_{0}^{-1}$ up to a constant scaling factor:
%  \begin{align*}
% 	 q^{\star}\left( \mb \Sigma_0^{-1} \right) & \propto \exp \left(-\frac{1}{2} \trace \left( \mb V_0^{-1}\mb \Sigma_0^{-1} \right) \right) \\
%    &\quad \exp\left(-\frac{v_0+d-T-1}{2} \log\det\left( \mb \Sigma_0^{-1} \right) \right)\\
%    &\quad \exp\left(- \frac{1}{2}\trace\left( \mb \Sigma_0^{-1} \left( \sum_{j} \mb m_{\left( j \right) } \mb m_{\left( j \right) }^{\top} + \mb \Sigma_{j} \right)  \right) \right).  
% \end{align*}
% This implies $q^{\star}\left( \mb \Sigma_{0}^{-1} \right)$ follow a Wishart distribution with parameters:
% \begin{align*}
% 	v & = v_0+d, \\
% 	\mb V &=\left( \mb V_0^{-1}+\sum_{j} \mb m_{\left( j \right)} \mb m_{\left( j \right) }^{\top}+\mb \Sigma_{j} \right)^{-1}. 
% \end{align*}
% \paragraph{Update for $\mb \Sigma_{j}$}
% all the terms involve $\mb \Sigma_{j}$ in ELBO approximation (Eqn. \ref{eqn: elbo aprx}):
% \[
% -\frac{1}{2}\trace\left( v \mb V \mb \Sigma_{j} \right)-\frac{1}{8} \sum_{t} \left( \mb \Sigma_{j} \right)_{t,t} \phi_{j}\sum_{i}\left( x_{t}^{ij} \right)^{2} +\frac{1}{2}\log\det\left( \mb \Sigma_{j} \right)  
% .\] 
% Rewrite the second term:
% \[
% -\frac{1}{8} \trace\left( \mb \Sigma_{j} \diag\left( \begin{bmatrix} \sum_{i} \phi_{j}\left(x_{1}^{ij} \right)^{2},\cdots, \sum_{i} \phi_{j}\left( x_{T}^{ij} \right)^{2}     \end{bmatrix}^{\top} \right)  \right) 
% .\] 
% Denote the diagonal matrix as $\tilde{\mb X_{j}}$. For every $j$, we have a constrained optimization problem:
% \[
% 	\max_{\mb \Sigma \in \bb S_{++}^{T}} \log\det\left( \mb \Sigma \right)-\trace\left( \mb \Sigma \left(v \mb V+\frac{1}{4} \tilde{\mb X_{j}} \right)  \right)  
% .\] 
% which admits a closed form solution:
% \begin{equation}
% \label{eqn: covariance updates}
% 	\mb \Sigma_{j}^{\star} = \left( v \mb V + \frac{1}{4} \tilde{\mb X_{j}} \right)^{-1}. 
% \end{equation}
% for $j=1,\ldots, d$. 

% \paragraph{Update for $\mb m_{\left( j \right) }$}
% all the terms involved $\mb m_{\left( j \right) }$ in ELBO approximation (Eqn. \ref{eqn: elbo aprx}):
% \begin{align*}
%     %f\left(  \mb m_{\left( j \right) }\right) 
%     & -\frac{v}{2} \innerprod{\mb m_{\left( j \right) }}{\mb V \mb m_{\left(j\right) }} -\frac{1}{8}\innerprod{\mb m_{\left( j \right) }}{\tilde{\mb X_{j}}\mb m_{\left( j \right) }}\\
%     &+\innerprod{\mb m_{\left( j \right) }}{\begin{bmatrix} \cdots, & \phi_{j}\sum_{i} \left(y_{t}^{i}-\tilde{y_{t}^{i}}\right) x_{t}^{ij}, & \cdots\end{bmatrix}^{\top}}\\
%     &+\frac{1}{4} \innerprod{\mb m_{\left( j \right) }}{\begin{bmatrix} \cdots, & \phi_{j} \sum_{i} \innerprod{\mb w'_{t} \circ \mb z'}{\mb x_{t}^{i}}x_{t}^{ij}, &\cdots \end{bmatrix}^{\top}}\\
%     &-\frac{1}{4}\innerprod{\mb m_{\left( j \right) }}{\begin{bmatrix} \cdots, & \phi_{j}\sum_{i} x_{t}^{ij} \sum_{l\neq j} \phi_{l}x_{t}^{il} m_{tl}, &\cdots \end{bmatrix}^{\top} }.
% \end{align*}
% This problem is quadratic with a negative definite Hessian matrix, hence by stationary condition (i.e., zero gradient) we have closed form updates: 
% \begin{align}
% \label{eqn: genereal mean updates}
%     & \mb \Sigma_{j}^{\star} ( \begin{bmatrix} \cdots & \phi_{j}\sum_{i} \left(y_{t}^{i}-\tilde{y_{t}^{i}}\right) x_{t}^{ij}, & \cdots\end{bmatrix}^{\top} \nonumber \\ 
%     &\quad +\frac{1}{4} \begin{bmatrix} \cdots, & \phi_{j} \sum_{i} \innerprod{\mb w'_{t} \circ \mb z'}{\mb x_{t}^{i}}x_{t}^{ij}, &\cdots \end{bmatrix}^{\top} \nonumber \\
%     &\quad -\frac{1}{4} \begin{bmatrix} \cdots, & \phi_{j}\sum_{i} x_{t}^{ij} \sum_{l\neq j} \phi_{l}x_{t}^{il} m_{tl}, &\cdots \end{bmatrix}^{\top} 
% 	). 
% \end{align}
% For all $j=1\ldots, d$. When the reference point of quadratic lower bound $\mb w' \circ \mb z'$ is set to be the mean parameters from the previous iteration, we can simplify Eqn. \ref{eqn: mean updates}:
% {\small
% \begin{align}
% 	\label{eqn: mean updates}	
% 	 & \mb \Sigma_{j}^{\star} ( \begin{bmatrix} \phi_{j}\sum_{i} \left(y_{1}^{i}-\tilde{y_{1}^{i}}\right) x_{1}^{ij}, & \cdots, & \phi_{j}\sum_{i} \left(y_{T}^{i}-\tilde{y_{T}^{i}}\right) x_{T}^{ij} \end{bmatrix}^{\top} \nonumber \\
%      &\quad +\frac{1}{4} \begin{bmatrix} \phi_{j}^{2} \sum_{i} \left(x_{1}^{ij}\right)^{2} m_{1j}^{\left( k \right) }, &\cdots, & \phi_{j}^{2} \sum_{i} \left( x_{T}^{ij} \right)^{2} m_{Tj}^{\left( k \right) }  \end{bmatrix}^{\top}. \nonumber \\	
% \end{align}
% }
% \paragraph{Update for $\phi_{j}$}

% All the terms involve $\phi_{j}$ in ELBO approximation (Eqn. \ref{eqn: elbo aprx}): 
% \begin{align*}
% 	f\left( \phi_{j} \right) &:= \phi_{j}\left( \psi\left( \alpha \right) -\psi\left( \beta \right) \right)   +\phi_{j}\sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
% 	%f\left( \phi_{j} \right) &:= \phi_{j}\left( \psi\left( \alpha \right) -\psi\left( \beta \right) +\psi\left( \alpha+\beta \right)  \right)   +\phi_{j}\sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
% 				 &\quad +\frac{\phi_{j}}{4} \sum_{t}\sum_{i} \left(m_{tj} x_{t}^{ij}\right) \innerprod{\mb w_{t}'\circ\mb z'}{\mb x_{t}^{i}}\\ 
% 				 &\quad -\frac{\phi_{j}}{4}\sum_{t}\sum_{i} m_{tj} x_{t}^{ij} \sum_{l\neq j} m_{tl} \phi_{l} x_{t}^{il}\\ 
%      &\quad -\frac{\phi_{j}}{8}\sum_{t}\left( \left( \Sigma_{j} \right)_{tt}+m_{tj}^{2} \right)\sum_{i}\left( x_{t}^{ij} \right)^{2}  \\
% 				 & \quad -\phi_{j}\log\left( \phi_{j} \right)-\left( 1-\phi_{j} \right) \log\left( 1-\phi_{j} \right).  
% \end{align*}
% Observe $f\left( \phi_{j} \right)$ is a smooth strictly concave function, so we can solve for $\phi_{j}^{\star}$ by stationary condition (i.e., $0$ derivatives), which admit a closed form update:
%  \begin{align}
% \label{eqn: general sparsity updates}
% \phi_{j}^{\star} &= \sigma\left( a \right).  	
% \end{align}
% where $\sigma\left( t \right):=\frac{1}{\exp\left( -t \right)+1}$ denote the sigmoid function, and: 
% \begin{align*}
% 	 a & = \psi\left( \alpha \right) -\psi\left( \beta \right) + \sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
% 	 %a & = \psi\left( \alpha \right) -\psi\left( \beta \right) +\psi\left( \alpha+\beta \right) + \sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
%         &\quad +\frac{1}{4} \sum_{t}\sum_{i} \left(m_{tj} x_{t}^{ij}\right) \innerprod{\mb w_{t}'\circ\mb z'}{\mb x_{t}^{i}}\\ 
%         &\quad -\frac{1}{4}\sum_{t}\sum_{i} m_{tj} x_{t}^{ij} \sum_{l\neq j} m_{tl} \phi_{l} x_{t}^{il}\\
%         & \quad -\frac{1}{8}\sum_{t}\left( \left( \Sigma_{j} \right)_{tt}+m_{tj}^{2} \right)\sum_{i}\left( x_{t}^{ij} \right)^{2}.  \\
% \end{align*}

% When reference point of quadratic lower bound $\mb w' \circ \mb z'$ is set to be the mean parameters from the previous iterations, Eqn. \ref{eqn: general sparsity updates} is simplified: 
% \begin{align}
% \label{eqn: sparsity updates}
% \phi_{j}^{k+1} &= \sigma\left( a\right).	
% \end{align}
% where 
% \begin{align*}
% 	 a & = \psi\left( \alpha \right) -\psi\left( \beta \right)  + \sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
% 	 %a & = \psi\left( \alpha \right) -\psi\left( \beta \right) +\psi\left( \alpha+\beta \right) + \sum_{t}\sum_{i} \left( y_{t}^{i}-\tilde{y_{t}^{i}}\right)m_{tj} x_{t}^{ij} \\
% 	   &+\frac{1}{8}\sum_{t}\left(  m_{tj}^{2}\left(2\phi_{j}^{\left( k \right) }-1\right)-\left( \Sigma_{j} \right)_{tt}  \right)\sum_{i}\left( x_{t}^{ij} \right)^{2} .
% \end{align*}
