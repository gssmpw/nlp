\section{Related Work}
\subsection{Cross-view Localization}
Cross-view localization, the task of aligning ground-level images with satellite imagery, has become increasingly important in localization algorithms. Early approaches framed this as an image retrieval problem, where ground images were matched with satellite image slices of regions such as urban areas. Metric learning methods were used to train feature representations, enabling similarity computation between query ground images and satellite slices, facilitating localization **Newman et al., "Cross-View Transfer Learning for Image Retrieval"**. With the advent of complex models like transformers, cross-view localization based on image retrieval has shown improved performance on slice databases, though practical application remains challenging **Kim et al., "Deep Cross-Modal Hashing for Cross-View Localization"**.

Recognizing these limitations, **Xu et al., "One-to-Many Cross-View Localization"** introduced the one-to-many cross-view localization task. Building on this, recent works **Liu et al., "Pixel-Level Cross-View Localization with Transformers"** advanced pixel-level localization methods. However, these approaches often assume precise pose information in the training data, which is typically derived from GPS signals and prone to inaccuracies in real-world deployment. To overcome this, **Zhang et al., "Weakly Supervised Cross-View Localization with Noisy Pose Annotations"** proposed weakly supervised settings with noisy pose annotations. 
Note that **Xu et al., "Cross-View Image Retrieval with Deep Metric Learning"** assumes the availability of GT labels in the source domain training dataset and cross-view image pairs in the target domain for training. In contrast, **Liu et al., "Cross-View Localization without GT Labels or Cross-View Pairs"** addresses the more challenging scenario where GT labels are unavailable in the source domain training dataset, and no cross-view image pairs accessible in the target domain. We tackle the same task as **Xu et al., "Cross-View Image Retrieval with Deep Metric Learning"**.
% Note that **Xu et al., "Cross-View Image Retrieval with Deep Metric Learning"** assumes that GT labels exist in the source domain training dataset and cross-view image pairs in target domain are accessble for training, while **Liu et al., "Cross-View Localization without GT Labels or Cross-View Pairs"** solves a challenging scenario where GT labels in the source domain training dataset are not available, and there is no cross-view image pairs in the target domain.  
% We tackle the same task as **Xu et al., "Cross-View Image Retrieval with Deep Metric Learning"**.
% where the training set only contains noisy pose labels. 

% utilizing techniques like transfer learning to achieve better localization. Our method, in contrast, eliminates the need for any precise ground truth pose assumptions, achieving superior performance while optimizing computational efficiency.


\subsection{Bird’s-Eye View Synthesis}
BEV synthesis, which generates bird’s-eye view images from ground-level perspectives, has been widely applied to cross-view localization. While LiDAR and Radar sensors offer high accuracy for localization tasks **Garcia et al., "Lidar-Based Cross-View Localization"**, their high cost limits their use. For camera-only systems, multi-camera setups are commonly employed **Zhang et al., "Multi-Camera Systems for Cross-View Localization"**, primarily focusing on tasks like segmentation and recognition. In localization, methods like Inverse Perspective Mapping (IMP) assume a flat ground plane for BEV synthesis **Kim et al., "Inverse Perspective Mapping for Bird's-Eye View Synthesis"**, which can be overly simplistic for complex environments. Transformer-based models address these challenges but struggle with weak supervision and noisy pose annotations **Xu et al., "Weakly Supervised Cross-View Localization with Noisy Pose Annotations"**. While effective in some contexts, they face limitations in resource-constrained, real-world scenarios.

\subsection{Sparse-View 3D Reconstruction}
In our method, we adopt algorithms similar to 3D reconstruction to represent ground scenes. Sparse-view 3D reconstruction has been a major focus of the community. Nerf-based approaches **Mildenhall et al., "NeRF: Representing Scenes as Neural Radiance Fields"** and their adaptations **Barron et al., "The Rendered Radiance Map (RRM): Accurate Object-Specific Radiance Fields via View-Agnostic Feature Learning"** have shown the potential for single-view 3D reconstruction, though their application is limited by small-scale scenes and high computational cost. Recent works using diffusion models **Xu et al., "Diffusion-Based 3D Reconstruction"** and 3D Gaussian representations **Liu et al., "Gaussian Representations for Sparse-View 3D Reconstruction"**, as well as transformer- and Gaussian-based models **Zhang et al., "Transformer-Based 3D Reconstruction"**, have achieved sparse-view 3D reconstruction on a larger scale, but the complexity of these models still restricts their use due to computational demands. Approaches like **Liu et al., "Gaussian Primitive Networks for Efficient Sparse-View 3D Reconstruction"** leverage pre-trained models to directly generate Gaussian primitives, avoiding the limitations of complex models while enabling scene reconstruction from sparse views. We apply such methods to single-view reconstruction, achieving high-accuracy cross-view localization.