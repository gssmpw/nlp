\section{Related Work}
\subsection{Cross-view Localization}
Cross-view localization, the task of aligning ground-level images with satellite imagery, has become increasingly important in localization algorithms. Early approaches framed this as an image retrieval problem, where ground images were matched with satellite image slices of regions such as urban areas. Metric learning methods were used to train feature representations, enabling similarity computation between query ground images and satellite slices, facilitating localization \cite{lin2013cross, regmi2018cross, shi2019spatial, Liu_2019_CVPR, shi2020optimal}. With the advent of complex models like transformers, cross-view localization based on image retrieval has shown improved performance on slice databases, though practical application remains challenging \cite{yang2021cross, Zhu_2022_CVPR}.

Recognizing these limitations, \cite{zhu2021vigor} introduced the one-to-many cross-view localization task. Building on this, recent works \cite{shi2022beyond, xia2022visual, fervers2022uncertainty, lentsch2023slicematch, sarlin2023orienternet, wang2024view} advanced pixel-level localization methods. However, these approaches often assume precise pose information in the training data, which is typically derived from GPS signals and prone to inaccuracies in real-world deployment. To overcome this, \cite{shi2024weakly, xia2024adapting} proposed weakly supervised settings with noisy pose annotations. 
Note that \cite{xia2024adapting} assumes the availability of GT labels in the source domain training dataset and cross-view image pairs in the target domain for training. In contrast, \cite{shi2024weakly} addresses the more challenging scenario where GT labels are unavailable in the source domain training dataset, and no cross-view image pairs accessible in the target domain. We tackle the same task as \cite{shi2024weakly}.
% Note that \cite{xia2024adapting} assumes that GT labels exist in the source domain training dataset and cross-view image pairs in target domain are accessble for training, while \cite{shi2024weakly} solves a challenging scenario where GT labels in the source domain training dataset are not available, and there is no cross-view image pairs in the target domain.  
% We tackle the same task as \cite{shi2024weakly}.
% where the training set only contains noisy pose labels. 

% utilizing techniques like transfer learning to achieve better localization. Our method, in contrast, eliminates the need for any precise ground truth pose assumptions, achieving superior performance while optimizing computational efficiency.


\subsection{Bird’s-Eye View Synthesis}
BEV synthesis, which generates bird’s-eye view images from ground-level perspectives, has been widely applied to cross-view localization. While LiDAR and Radar sensors offer high accuracy for localization tasks \cite{qin2023supfusion, harley2023simple, lin2024rcbevdet, liu2025seed}, their high cost limits their use. For camera-only systems, multi-camera setups are commonly employed \cite{reiher2020sim2real, li2022bevformer, yang2023parametric}, primarily focusing on tasks like segmentation and recognition. In localization, methods like Inverse Perspective Mapping (IMP) assume a flat ground plane for BEV synthesis \cite{shi2024weakly, wang2024fine}, which can be overly simplistic for complex environments. Transformer-based models address these challenges but struggle with weak supervision and noisy pose annotations \cite{fervers2022uncertainty, shi2023boosting, sarlin2023orienternet}. While effective in some contexts, they face limitations in resource-constrained, real-world scenarios.

\subsection{Sparse-View 3D Reconstruction}
In our method, we adopt algorithms similar to 3D reconstruction to represent ground scenes. Sparse-view 3D reconstruction has been a major focus of the community. Nerf-based approaches~\cite{mildenhall2021nerf} and their adaptations~\cite{hong2023lrm} have shown the potential for single-view 3D reconstruction, though their application is limited by small-scale scenes and high computational cost. Recent works using diffusion models~\cite{rombach2021highresolution} and 3D Gaussian representations~\cite{kerbl20233d}\cite{cai2024baking, zhou2024diffgs, mu2025gsd}, as well as transformer- and Gaussian-based models\cite{chen2024splatformer, GaussTR}, have achieved sparse-view 3D reconstruction on a larger scale, but the complexity of these models still restricts their use due to computational demands. Approaches like~\cite{zhou2024feature, wewer2025latentsplat} leverage pre-trained models to directly generate Gaussian primitives, avoiding the limitations of complex models while enabling scene reconstruction from sparse views. We apply such methods to single-view reconstruction, achieving high-accuracy cross-view localization.