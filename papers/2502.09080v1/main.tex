%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption} % 推荐使用subcaption
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{makecell}  % 允许单元格换行


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% \usepackage[nohyperref]{icml2025}
\usepackage[accepted]{icml2025}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{}

\begin{document}
\pagestyle{empty}
\twocolumn[
% \icmltitle{Ground-to-Satellite Gaussian: Lending 3d Gaussian Splatting to \\
%                  Weakly-Supervised Camera Localization}
% \icmltitle{Resolving Height Ambiguity in Cross-View Localization via Featuremetric Gaussian Splatting}
\icmltitle{BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization }

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Qiwei Wang}{Shanghaitech}
\icmlauthor{Shaoxun Wu}{Shanghaitech}
\icmlauthor{Yujiao Shi}{Shanghaitech}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \begin{icmlauthorlist}
% \icmlauthor{Anonymous Authors}{anonymous}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

\icmlaffiliation{Shanghaitech}{Shanghaitech University, Shanghai, China}

\icmlcorrespondingauthor{Yujiao Shi}{shiyj2@shanghaitech.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}

This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird’s-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers.
We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. Additionally, to address challenges with panoramic query images, we introduce an icosphere-based supervision strategy for the Gaussian primitives.
We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches.

% This paper tackles the problem of weakly supervised cross-view localization, which aims to estimate the pose of a ground camera relative to a satellite image under weak supervision. In this setting, the ground truth (GT) location of the camera is provided with noisy annotations, typically having an error of tens of meters. A common approach to bridge the cross-view domain gap for relative pose estimation is Bird-Eye View (BEV) synthesis. However, existing methods often struggle with height ambiguity due to the unavailability of depth information in ground images and height maps in satellite images. To address this, previous solutions either assume a flat ground plane or rely on complex models, such as transformers or feature residuals, to resolve the ambiguity.
% In this paper, we propose BevSplat, a novel approach to handling height ambiguity using feature-based Gaussian primitives. Specifically, for each pixel in a ground-level query image, we estimate a 3D Gaussian primitive represented by feature-based attributes, rather than traditional RGB values. These features capture both semantic and spatial information. A BEV feature map is then synthesized from the Gaussian primitives and compared with the satellite image for relative pose estimation. 
% To handle challenges when query images are panoramas, we propose an icosphere-based supervision strategy for the Gaussian primitives. 
% The effectiveness of our method is validated on two widely used benchmarks, the KITTI and VIGOR datasets, including localizing both pine-hole camera and panorama images. Experimental results demonstrate that our method considerably improves localization accuracy compared to prior approaches.


% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

\begin{figure}[htbp]
\setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
  \centering
  \includegraphics[width=\linewidth]{openfigure_new1.png}
  \vspace{-1.2em}
  \caption{We visualize the Cross-View localization process using BEVsplat. The red box shows the process of projecting the ground map onto the BEV view through BEVsplat. Compared to the previous method using IPM (Inverse Perspective Mapping), we observe that our method better recovers the curves in the BEV view, handles occlusions from buildings more effectively, and shows better performance in practical localization.}
  \label{fig:open_figure}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[width=\textwidth]{framework_final.png}
    \vspace{-1.2em}
    \caption{Framework overview of the proposed BevSplat. 
    We first train a Gaussian Primitive Generation model for 3D scene modeling from a single query ground image (Stage 1). The model is supervised by image reconstruction loss and depth consistency loss leveraging a depth foundation model.  When the query image is a panorama, the supervision is applied by decomposing the panorama to $k$ pin-hole camera images. After that, we estimate features for each Gaussian primitive, synthesize a BEV feature map from them, compute a reference satellite feature map, and conduct similarity matching between the synthesized BEV and reference satellite feature maps (Stage 2). The output is a location probability map of the query image with respect to the reference satellite image. 
    % In Stage 1, the ground view is processed to generate Gaussian primitives through a Gaussian Encoder, which produces a Gaussian Map. Stage 2 involves extracting features from the ground view using DINO and DPT encoders, followed by BEV feature synthesis through Gaussian Splatting. In Stage 3, the satellite view is encoded, and the loss function guides the optimization for accurate localization.
    }

    \label{fig:framework}
\end{figure*}

\section{Introduction}

Cross-view localization, the task of estimating the pose of a ground camera with respect to a satellite or aerial image, is a critical problem in computer vision and remote sensing. This task is especially important for applications such as autonomous driving, urban planning, and geospatial analysis, where accurately aligning ground-level and satellite views is crucial. However, it presents significant challenges due to the differences in scale, perspective, and environmental context between ground-level images and satellite views.

In recent years, weakly supervised learning~\cite{shi2024weakly, xia2024adapting} has emerged as a promising approach to tackle cross-view localization, especially when precise ground-truth (GT) camera locations are unavailable. In a weakly supervised setting, only noisy annotations—such as approximate camera locations with errors up to tens of meters—are accessible, making the problem even more complex. Despite these challenges, weak supervision offers the potential to train models with less labor-intensive data collection, which is often impractical at the scale required for real-world applications.

A key strategy to address cross-view localization is Bird’s-Eye View (BEV) synthesis~\cite{fervers2022uncertainty, shi2023boosting, sarlin2023orienternet, shi2024weakly, wang2024fine}, which generates a bird’s-eye view representation from the ground-level image. The BEV image can then be compared directly to a satellite image, facilitating relative pose estimation. However, existing methods often rely on Inverse Perspective Mapping (IPM), which assumes a flat ground plane~\cite{shi2024weakly, wang2024fine}, or on high-complexity models like cross-view transformers~\cite{fervers2022uncertainty, shi2023boosting, sarlin2023orienternet} to address height ambiguity, the challenge of resolving the elevation difference between the ground and satellite views.

The flat terrain assumption used in IPM leads to the loss of critical scene information above the ground plane and introduces distortions for objects farther from the camera, as shown in Fig.~\ref{fig:framework}. On the other hand, while cross-view transformers are effective at handling distortions and objects above the ground plane, they are computationally expensive. Furthermore, in weakly supervised settings, noisy ground camera pose annotations provide weak supervision, making it difficult for high-complexity models like transformers to converge, ultimately leading to suboptimal localization performance.

In this paper, we propose BevSplat to address these challenges. BevSplat generates feature-based 3D Gaussian primitives for BEV synthesis. Unlike previous 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} methods that rely on color-based representations, we represent each pixel in the ground-level image as a 3D Gaussian with semantic and spatial features. These Gaussians are associated with attributes such as position in 3D space, scale, rotation, and density, which are synthesized into a BEV feature map using a visibility-aware rendering algorithm that supports anisotropic splatting. This approach enables us to handle height ambiguity and complex cross-view occlusions, improving the alignment between the ground-level image and the satellite view for more accurate pose estimation, without the need for expensive depth sensors or complex model architectures. 
The attributes of the Gaussian primitives are supervised by image and depth rendering loss, where the depth supervision comes from a depth prediction foundation model. 


In many cross-view localization tasks, the ground-level images are panoramic, which introduces additional challenges due to the wide-angle distortions inherent in such images, making the depth prediction from existing foundation models trained on pin-hole camera images inaccurate. 
To address this challenge, we leverage an icosphere-based supervision technique to transform panoramic images into a format compatible with pinhole camera models. By fitting the panoramic image onto an icosphere, we decompose the panorama to $k=20$ pin-hole camera images and generate depth maps for each face using a foundation model. This enables accurate depth estimation for panoramic images and thus improves localization performance. 

We validate our approach on the widely used KITTI and VIGOR datasets, where the former localizes images captured by pin-hole cameras, the latter aims to localize panoramic images, demonstrating that the proposed BevSplat significantly outperforms existing techniques in terms of localization accuracy in various localization scenarios. 




% \section{Introduction}
% Cross-view localization, the task of estimating the pose of a ground camera with respect to a satellite or aerial image, is a fundamental problem in computer vision and remote sensing. 
% This problem is critical for applications such as autonomous driving, urban planning, and geospatial analysis, where accurately aligning ground-level and satellite views is essential. However, this task is challenging due to the significant differences in scale, perspective, and environmental context between ground-level images and satellite views.

% In recent years, {weakly supervised learning}~\cite{shi2024weakly, xia2024adapting} has emerged as a promising approach to tackle cross-view localization when precise ground truth (GT) camera locations are unavailable. In a weakly supervised setting, only noisy annotations, such as approximate camera locations with errors up to tens of meters, are accessible, making the problem more complex. Despite these challenges, weak supervision offers the potential to train models with less labor-intensive data collection, which is often impractical at the scale required for real-world applications.

% A key strategy to address cross-view localization is {Bird-Eye View (BEV) synthesis}~\cite{fervers2022uncertainty,shi2023boosting,sarlin2023orienternet,shi2024weakly,wang2024fine}, which seeks to generate a bird’s-eye view representation from the ground-level image. The BEV image can be compared directly to a satellite image, facilitating relative pose estimation. However, existing approaches often rely on Inverse Perspective Mapping (IMP), which assumes a flat ground plane~\cite{shi2024weakly,wang2024fine}, or on complex models like cross-view transformers~\cite{fervers2022uncertainty,shi2023boosting,sarlin2023orienternet} to address height ambiguity -- the inherent difficulty in resolving the elevation difference between the ground and satellite views.

% The flat terrain assumption leads to the loss of critical scene information above the ground plane and introduces distortions for objects farther from the camera, as shown in Fig.\ref{fig:framework}. On the other hand, cross-view transformers, while effective at handling distortions and objects above the ground plane, are computationally expensive. Furthermore, in weakly supervised settings, noisy ground camera pose annotations provide weak supervision, which makes it difficult for high-complexity models like transformers to converge, ultimately resulting in suboptimal localization performance.


% In this paper, we propose a novel solution to resolve the height ambiguity in cross-view localization through the use of Gaussian primitives. Unlike previous methods that rely on color-based representations, we employ feature-based Gaussian primitives, where each pixel in the ground-level image is represented by a 3D Gaussian with semantic and spatial features. These Gaussians are also associated with attributes such as position in 3D space, scale, rotation, and density. We then synthesize a BEV feature map from these primitives using a visibility-aware rendering algorithm that supports anisotropic splatting~\cite{kerbl20233d}. This approach enables us to handle height ambiguity and complex cross-view occlusions, aligning the ground-level image with the satellite view for more accurate pose estimation, without relying on expensive depth sensors or complex model architectures.                              

% We validate our method on the widely used KITTI and VIGOR datasets, demonstrating that it outperforms existing techniques in terms of localization accuracy.

\section{Related Work}
\subsection{Cross-view Localization}
Cross-view localization, the task of aligning ground-level images with satellite imagery, has become increasingly important in localization algorithms. Early approaches framed this as an image retrieval problem, where ground images were matched with satellite image slices of regions such as urban areas. Metric learning methods were used to train feature representations, enabling similarity computation between query ground images and satellite slices, facilitating localization \cite{lin2013cross, regmi2018cross, shi2019spatial, Liu_2019_CVPR, shi2020optimal}. With the advent of complex models like transformers, cross-view localization based on image retrieval has shown improved performance on slice databases, though practical application remains challenging \cite{yang2021cross, Zhu_2022_CVPR}.

Recognizing these limitations, \cite{zhu2021vigor} introduced the one-to-many cross-view localization task. Building on this, recent works \cite{shi2022beyond, xia2022visual, fervers2022uncertainty, lentsch2023slicematch, sarlin2023orienternet, wang2024view} advanced pixel-level localization methods. However, these approaches often assume precise pose information in the training data, which is typically derived from GPS signals and prone to inaccuracies in real-world deployment. To overcome this, \cite{shi2024weakly, xia2024adapting} proposed weakly supervised settings with noisy pose annotations. 
Note that \cite{xia2024adapting} assumes the availability of GT labels in the source domain training dataset and cross-view image pairs in the target domain for training. In contrast, \cite{shi2024weakly} addresses the more challenging scenario where GT labels are unavailable in the source domain training dataset, and no cross-view image pairs accessible in the target domain. We tackle the same task as \cite{shi2024weakly}.
% Note that \cite{xia2024adapting} assumes that GT labels exist in the source domain training dataset and cross-view image pairs in target domain are accessble for training, while \cite{shi2024weakly} solves a challenging scenario where GT labels in the source domain training dataset are not available, and there is no cross-view image pairs in the target domain.  
% We tackle the same task as \cite{shi2024weakly}.
% where the training set only contains noisy pose labels. 

% utilizing techniques like transfer learning to achieve better localization. Our method, in contrast, eliminates the need for any precise ground truth pose assumptions, achieving superior performance while optimizing computational efficiency.


\subsection{Bird’s-Eye View Synthesis}
BEV synthesis, which generates bird’s-eye view images from ground-level perspectives, has been widely applied to cross-view localization. While LiDAR and Radar sensors offer high accuracy for localization tasks \cite{qin2023supfusion, harley2023simple, lin2024rcbevdet, liu2025seed}, their high cost limits their use. For camera-only systems, multi-camera setups are commonly employed \cite{reiher2020sim2real, li2022bevformer, yang2023parametric}, primarily focusing on tasks like segmentation and recognition. In localization, methods like Inverse Perspective Mapping (IMP) assume a flat ground plane for BEV synthesis \cite{shi2024weakly, wang2024fine}, which can be overly simplistic for complex environments. Transformer-based models address these challenges but struggle with weak supervision and noisy pose annotations \cite{fervers2022uncertainty, shi2023boosting, sarlin2023orienternet}. While effective in some contexts, they face limitations in resource-constrained, real-world scenarios.

\subsection{Sparse-View 3D Reconstruction}
In our method, we adopt algorithms similar to 3D reconstruction to represent ground scenes. Sparse-view 3D reconstruction has been a major focus of the community. Nerf-based approaches~\cite{mildenhall2021nerf} and their adaptations~\cite{hong2023lrm} have shown the potential for single-view 3D reconstruction, though their application is limited by small-scale scenes and high computational cost. Recent works using diffusion models~\cite{rombach2021highresolution} and 3D Gaussian representations~\cite{kerbl20233d}\cite{cai2024baking, zhou2024diffgs, mu2025gsd}, as well as transformer- and Gaussian-based models\cite{chen2024splatformer, GaussTR}, have achieved sparse-view 3D reconstruction on a larger scale, but the complexity of these models still restricts their use due to computational demands. Approaches like~\cite{zhou2024feature, wewer2025latentsplat} leverage pre-trained models to directly generate Gaussian primitives, avoiding the limitations of complex models while enabling scene reconstruction from sparse views. We apply such methods to single-view reconstruction, achieving high-accuracy cross-view localization.
\section{Method}

In this paper, we tackle the problem of cross-view localization by aligning a ground-level image with a satellite image, using weak supervision where the ground camera location is only approximately known. Our goal is to accurately estimate the camera pose from noisy annotations, leveraging the power of Gaussian primitives for handling height ambiguity and efficiently generating BEV feature maps.

\subsection{3D Gaussian Primitive Generation}
\label{3D Gaussian Primitive Generation}
Inspired by 3DGS, we represent the 3D scene by a set of Gaussian primitives. Following PixelSplat~\cite{charatan2024pixelsplat}, we leverage a network to regress the Gaussian parameters from the query ground image for each of its pixels. The network is optimized such that the estimated Gaussian primitives allow the re-render of the original image. 
% which takes a single ground image as input and outputs Gaussian parameters corresponding to each pixel, allowing for the re-rendering of the original image.
% To learn 3D representations from the ground image, we adopt the 3D Gaussian Splatting (3DGS) approach, inspired by \cite{charatan2024pixelsplat}. The network takes a single ground image as input and outputs Gaussian parameters corresponding to each pixel, allowing for the re-rendering of the original image.

As shown in Figure~\ref{fig:framework}, our network follows a structure similar to an autoencoder. The first step involves feature extraction to obtain a global feature map \( f_g \). Inspired by \cite{charatan2024pixelsplat}, we do not directly predict a specific depth value. Instead, we uniformly sample \( Z \) depth values between predefined near and far distances and predict the probability distribution of each pixel \( i \) over these \( Z \) depth values, thereby forming a set of discrete depth buckets. 

Since Gaussian parameters can only be inferred from a single image, we increase the number of Gaussian primitives by selecting the top three depth values with the highest probabilities for each pixel \( i \), constructing its depth vector \( D_i \), the corresponding three probabilities define the opacity vector \( \alpha_i \), enabling a single pixel to generate up to three Gaussian primitives. \\

The 3D coordinate \( \mu_i \) of each pixel's Gaussian primitive is computed by transforming its 2D image coordinates \( (u_i, v_i) \) with its depth \( D_i \), using the camera's intrinsic matrix \( K \):
$
\mu_i = K^{-1} D_i [ u_i, v_i, 1 ]^T.
$
Next, we use a multi-layer perceptron (MLP) \( F_{gs} \) to generate the spherical harmonics \( \text{SH}_i \), the rotation matrix \( \mathbf{R}_i \), and the scaling matrix \( \mathbf{S}_i \) corresponding to \cite{kerbl20233d} for each Gaussian from \( f_g \):
\begin{equation}
    \{\mathbf{S}_i, \mathbf{R}_i, \text{SH}_i\}_{i \in \{1, 2, \dots, N\}} = F_{gs}(f_g). 
\end{equation}
Here, \( N \) is the number of pixels in the image. With these parameters, we render the input ground image and its depth.

% To optimize our 3D Gaussian primitive generation network, we use the original ground image's RGB values and the depth map estimated from the original ground image using a depth foundation model \cite{yang2024depth} as the ground truth. The corresponding loss function is defined as follows:
% \begin{equation}
% \mathcal{L}_{1} = \mathcal{L}_{\text{Depth}} + \lambda_2\mathcal{L}_{\text{RGB}},
% \end{equation}

% where
% \begin{equation}
% % \begin{split}
%     \mathcal{L}_{RGB} = \mathcal{L}_{\text{MSE}}(\hat{I}, I) + \lambda_1\mathcal{L}_{\text{LPIPS}}(\hat{I}, I)\\
%     \mathcal{L}_{Depth} = \| \hat{D} - D \|_1
% % \end{split},
% \end{equation}

% In this formula, $\hat{I}$ represents the image rendered by 3DGS, and $I$ represents the original image. 
% $\hat{D}$ is the depth rendered by 3DGS, while $D$ is the absolute depth predicted by the depth foundation model \cite{yang2024depth} using pre-trained weights. LPIPS Loss follows \cite{zhang2018perceptual}. Here, $\lambda_1$ is set to 0.05, and $\lambda_2$ is set to 20. \\
% After training a 3D Gaussian model that aligns with the deep distribution using the abovementioned method, we proceed with the following feature rendering.

\textbf{Supervision:}The optimization is performed using a combined loss function that includes an image loss \( \mathcal{L}_{\text{Image}} \), a depth loss \( \mathcal{L}_{\text{Depth}} \):
\begin{equation}
    \mathcal{L}_1 = \mathcal{L}_{\text{Depth}} + \lambda_1 \mathcal{L}_{\text{Image}}.
\end{equation} 


% The image loss is applied to 
% measured between rendered ground images from estimated Gaussian primitives and the original ground image, as well as rendered BEV images 

The image loss is calculated using Mean Squared Error (MSE) and the perceptual loss (LPIPS) \cite{zhang2018perceptual}, while the depth loss uses the absolute difference between predicted depth maps \( \hat{D} \) and pseudo ground truth depth maps \( D \) estimated by ``Depth Anything V2''~\cite{yang2024depth}:
\begin{equation}
% \begin{split}
    \mathcal{L}_{\text{Image}} = \mathcal{L}_{\text{MSE}}(\hat{I}, I) + \mathcal{L}_{\text{MSE}}(\hat{P}_{\text{BEV}}, P_{\text{IMP}} ) + \lambda_2 \mathcal{L}_{\text{LPIPS}}(\hat{I}, I) \\
    \label{eq:loss_img}
% \end{split},
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{Depth}} = \| \hat{D} - D \|_1,
    \label{eq:loss_depth}
\end{equation}
% \begin{equation}
%     \mathcal{L}_{\text{RGB}} = \mathcal{L}_{\text{MSE}}(\hat{I}, I) + \mathcal{L}_{\text{MSE}}(\hat{P}_{\text{BEV}}, P_{\text{IMP}} ) + \lambda_2 \mathcal{L}_{\text{LPIPS}}(\hat{I}, I),
% \end{equation}
% \begin{equation}
%     \mathcal{L}_{\text{Depth}} = \| \hat{D} - D \|_1,
% \end{equation}


where \( \hat{I} \) is the ground image rendered by 3DGS, \( I \) is the original ground image, \( \lambda_1 \) and \( \lambda_2 \) are set to 20 and 0.05, respectively. 
To strengthen the supervision for the estimated Gaussian primitives,  we also render a BEV image $P_{\text{BEV}}$ from the estimated Gaussians and supervise it using a BEV counterpart obtained by applying IPM used in \cite{shi2024weakly} to the ground image, denoted as $P_{\text{IMP}}$. 
Since $P_{\text{IMP}}$ suffers from distortions for scenes far away from the camera and scenes above the ground plane, we only crop its central $1/3$ portion for supervision. 
% $P_{BEV}$ represents the central $1/3$ portion of the bird's-eye view (BEV) image generated by 3DGS in the BEV perspective. Similarly, $P_{IMP}$ is the central $1/3$ portion of the BEV image obtained by applying Inverse Perspective Mapping (IPM) used in \cite{shi2024weakly} to the ground image. 

% We crop the central $1/3$ portion of the IPM-projected image because this region has more accurate projections and is closer to the real BEV projection. This cropped region is then used to supervise the BEV image generated by 3DGS. By supervising only the central $1/3$ region, we guide 3DGS to produce a complete bird's-eye view image.

This approach leverages the benefits of using the IPM-projected center region, while also leveraging 3DGS's ability to refine the geometry of distant scenes using depth predictions from a foundation model. This ensures that occluded buildings do not extend beyond their actual structure, enhancing the accuracy of the generated BEV image.

Moreover, in contrast to traditional autonomous driving scenarios, which predominantly rely on pinhole images, panoramic images are often the primary source of ground-view data in many cross-view localization tasks \cite{zhu2021vigor, Liu_2019_CVPR}. Current depth prediction foundation models \cite{ranftl2021vision, depth_anything_v1, monodepth17} are primarily trained on pinhole images, which limits their performance when applied to panoramic images, making them inadequate for high-quality Gaussian scene reconstruction. While some prior work has focused on fine-tuning depth prediction models for panoramic images \cite{SunSC21, Pintore:2021:SDD, jiang2021unifuse}, these methods are typically designed for small-scale indoor environments and fail to generalize well to large-scale outdoor scenes, as required in our task.

% In addition, unlike conventional autonomous driving scenarios, where pinhole images dominate, panoramic images also serve as the primary source of ground-view data in many cross-view localization scenarios \cite{zhu2021vigor, workman2015localize, Liu_2019_CVPR}. Current foundation depth prediction models \cite{ranftl2021vision, depth_anything_v1, monodepth17} are primarily trained on pinhole images, and thus their performance on panoramic images is insufficient for high-quality Gaussian scene reconstruction. Although some prior work has focused on fine-tuning or optimizing depth prediction for panoramic images \cite{SunSC21, Pintore:2021:SDD, jiang2021unifuse}, these approaches mainly target indoor, small-scale environments, which do not generalize well to the large-scale outdoor scenes required in our task.

To overcome this limitation, we propose utilizing a foundation model trained on pinhole images to predict depth for panoramic images. Following the approach in \cite{Peng_2023_WACV, reyarea2021360monodepth}, we map the panoramic image onto a spherical surface using an icosphere with $k=20$ faces. For each triangular facet, we compute the pose of the corresponding virtual camera based on the three vertices of the facet and apply a padding factor of 1.3 to generate a pinhole image. This transformation, along with the corresponding camera intrinsics, enables us to adapt the problem into a form compatible with the foundation model. 

The Gaussian primitives generated by our network are supervised by the faces of the icosphere rather than the original panoramic images. In practice, this means that in Eq.\ref{eq:loss_img}, the rendered image  $\hat{I}$ and original image $\hat{I}$ are replaced with the $k$ pin-hole camera images instead of the original panoramas. Similarly, the depth maps in Eq.~\ref{eq:loss_depth} are also replaced by the corresponding depth maps from the pinhole images.

% To address this limitation, we propose leveraging a foundation model trained on pinhole images to predict depth for panoramic images. Following \cite{Peng_2023_WACV, reyarea2021360monodepth}, we fit the panoramic image onto a spherical surface, using an icosphere with $N=20$ faces. For each triangular facet, we compute the pose of the corresponding virtual camera based on the three vertices of the facet and apply a padding factor of $1.3$ to generate a pinhole image. This process, along with the corresponding camera intrinsics, allows us to transform the problem into a form compatible with the foundation model. 
% The generated Gaussian primitives of our network are supervised by the faces in the icosphere instead of the original panoramic images. 
% For implementation, the rendered image $\hat{I}$ and original image $\hat{I}$ in Eq.~\ref{eq:loss_img} are replaced with the $N$ pin-hole camera images instead of the original panoramas, and so do the depth maps in Eq.~\ref{eq:loss_depth}. 

% Using the virtual camera pose and intrinsics, we then construct the Gaussian scene.

\subsection{Feature-based Gaussian Primitives for Relative Pose Estimation}

Once the 3D Gaussian primitive generation model is trained, we use the attributes of the generated 3D Gaussian primitives for BEV synthesis from the query ground image. Inspired by \cite{zhou2024feature, yue2025improving,wewer2025latentsplat}, we fine-tune a pre-trained DINO \cite{oquab2023dinov2} model with a depth prediction transformer (DPT) \cite{ranftl2021vision} to extract features from both the ground and satellite images.

% For the ground image, we extract high-dimensional features \( \mathbf{F}_g \) and a confidence map \( \mathbf{C}_g \), which indicates the reliability of each pixel, giving lower weights to dynamic objects like vehicles and higher weights to static objects like roads. For the satellite image, we extract features \( \mathbf{F}_s \) for static objects only.

For the ground image, we extract its high-dimensional features \( \mathbf{F}_g \in \mathbb{R}^{H_g \times W_g \times C} \) and a confidence map \( \mathbf{C}_g \in \mathbb{R}^{H_g \times W_g \times 1} \). The confidence map is obtained by applying an additional convolutional layer followed by a sigmoid activation to the extracted high-dimensional features. It represents the weights of different objects in the ground image, where dynamic objects such as vehicles are assigned lower weights while static objects like road surfaces are assigned higher weights.
For the satellite image, since most of its content consists of static objects, we only extract its features \( \mathbf{F}_s \in \mathbb{R}^{H_s \times W_s \times C} \).

\subsubsection{BEV Feature Rendering}

Here, we additionally incorporate the previously extracted ground image features \( \mathbf{F}_g \) and the confidence map \( \mathbf{C}_g \in \) into the Gaussian parameters. Using the pre-trained 3DGS model mentioned in \ref{3D Gaussian Primitive Generation}, these features are bound to Gaussian spheres that align with the depth distribution of the ground image. Specifically, the Gaussian parameters corresponding to each pixel \( i \) are expanded to include \( \alpha_i, \mu_i, S_i, R_i, \text{SH}_i, f_i, c_i \), where \( f_i \) and \( c_i \) represent the feature value and confidence for the pixel $i$, respectively.

We assume the world coordinate system follows the OpenCV convention in \cite{bradski2008learning}. In this coordinate system, \(+Z\) points forward along the camera's viewing direction (look vector), \(+X\) points to the right of the camera (right vector), and \(-Y\) points upward relative to the camera's orientation (up vector), forming a right-handed coordinate system. We apply a rotation matrix \( \mathbf{R} \) and a translation matrix \( \mathbf{T} \) to move the camera, initially located at the origin of the world coordinate system, to a position directly above the scene formed by all the Gaussian spheres in the ground. The camera's view is directed downward, rendering the Gaussian spheres to generate a new visual perspective. This process projects the ground image into a bird's-eye view, enabling similarity matching with the satellite image. The calculation of \( \mathbf{R} \) and \( \mathbf{T} \) involved in this process is as follows:
\begin{equation}
\mathbf{R} =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix},
\quad
\mathbf{T} =
\begin{bmatrix}
0 \\
z_{min} \\
0
\end{bmatrix},
\end{equation}
where the variable \(z_{\text{min}}\) represents the smallest \(z\)-coordinate value among the Gaussian spheres generated from the ground image, corresponding to the topmost Gaussian sphere in the scene.

Next, we render the ground image features \( \mathbf{F}_g \) to \( \mathbf{F}_{g2s} \) and the confidence map \( \mathbf{C}_g \) to \( \mathbf{C}_{g2s} \), which are bound to the Gaussian spheres in the new coordinate system, onto a 2D plane using the $\alpha$-blending method. This approach is similar to the original 3DGS rendering method \cite{kerbl20233d} for RGB colors. The formula is as follows:

\begin{equation}
\mathbf{F}_{g2s} = \sum_{i \in \mathcal{N}} f_i \alpha_i T_i, \quad
\mathbf{F}_{g2s} = \sum_{i \in \mathcal{N}} c_i \alpha_i T_i, 
\end{equation}
where
$T_{i} = \prod_{j=1}^{i-1}(1 - \alpha_j)$. 

\subsubsection{Confidence-guided similarity matching}
\label{Confidence-guided similarity matching}
% We use the same deep metric learning described in \cite{shi2024weakly} to optimize our feature extractors. 
The similarity between the BEV features estimated from query ground images and the satellite image features across different locations, which also indicates the location probability map of the query image relative to the satellite image, is computed as follows:
\begin{equation}
\mathbf{P}(u, v) = {\langle \mathbf{F}_s(u, v), \widehat{\mathbf{F}}_g \rangle}
/{\|\mathbf{F}_s(u, v)\| / \|\widehat{\mathbf{F}}_g\|},
% \begin{split}
% \mathbf{P}(u, v) = \frac{\langle \mathbf{F}_s(u, v), \widehat{\mathbf{F}}_g \rangle}
% {\|\mathbf{F}_s(u, v)\| \cdot \|\widehat{\mathbf{F}}_g\|},
% \end{split}
\end{equation}
where $\mathbf{F}_s$ and $\widehat{\mathbf{F}}_g$ represent the satellite image features and the BEV features estimated from the ground image, respectively, $\| \cdot \|$ denotes the $L_2$ norm. 

\textbf{Supervision:} Similar to~\cite{shi2024weakly}, deep metric learning objective is used for network supervision. Specifically, for a query ground image, we compute its location probability maps, \( \mathbf{P}_{\text{pos}} \) and \( \mathbf{P}_{\text{neg}} \), with respect to its positive and negative satellite images, respectively. The training objective is to maximize the peak similarity in \( \mathbf{P}_{\text{pos}} \) and minimize the peak similarity in \( \mathbf{P}_{\text{neg}} \) as below:

\begin{equation}
\mathcal{L}_{\text{Weakly}} = \frac{1}{M} \sum_{\text{idx}}^{M} \log \big( 1 + e^{\alpha \big[\text{Peak}(\mathbf{P}_{\text{neg}, \text{idx}}) - \text{Peak}(\mathbf{P}_{\text{pos}})\big]} \big),
\label{eq:weakly}
\end{equation}
where $N$ denotes the number of negative satellite images and $\text{idx}=1, ..., M$, \( \alpha \) controls the convergence speed which we set to 10. \\
When positive image pairs in the training set are generated similarly to those during inference (e.g., using the same retrieval model or noisy GPS receiver), the location errors in training match those we aim to refine during deployment. In this case, we train the network using Eq.~\ref{eq:weakly}. However, if more accurate location labels are available in the training set than during deployment, an additional training objective is introduced to leverage this information:
\begin{equation}
\mathcal{L}_{\text{GPS}} = \Big| \text{Peak}(\mathbf{P}_{\text{pos}})-\text{Peak}(\mathbf{P}_{\text{pos}}[x^* \pm d/{\beta}, y^* \pm d/{\beta}]) \Big|
\end{equation}
Here, \( (x^*, y^*) \) represents the location label from the training data with an error up to \( d \) meters, which we set to 5, and \( \beta \) is the ground resolution of the location probability map in meters per pixel. This objective ensures the global maximum on the location probability map aligns with a local maximum within a radius of \( d \) meters around the noisy location label.
Finally, the total optimization objective is:
\begin{equation}
\mathcal{L}_{2} = \mathcal{L}_{\text{Weakly}} + \lambda_3\mathcal{L}_{\text{GPS}}
\end{equation}
Here, \( \lambda_3 = 0 \) indicates that accurate pose labels are unavailable, while \( \lambda_3 = 1 \) means such labels are available in the training set.


% \subsubsection{Optimization}

% The total loss function combines the Gaussian and feature losses:
% \[
% \mathcal{L}_{all} = \mathcal{L}_1 + \mathcal{L}_2
% \]
% During training, we ensure that the Gaussian sphere distribution is optimized independently from the feature learning process. This separation allows the network to focus on improving feature extraction without distorting the 3D scene representation. Gradients from \( \mathcal{L}_2 \) affect only the feature extraction network, while gradients from \( \mathcal{L}_1 \) update the 3DGS model.

% The gradient backpropagation equations for Gaussian parameters and feature values are:
% \[
% \frac{\partial \mathcal{L}_{all}}{\partial \mu_i} = \frac{\partial \mathcal{L}_1}{\partial \hat{I}_i} * \frac{\partial^2 \hat{I}_i}{\partial \alpha_i \partial T_i}, \quad \frac{\partial \mathcal{L}_{all}}{\partial f_i} = \frac{\partial \mathcal{L}_2}{\partial F_{g2s}} * \alpha_i T_i
% \]

% This design ensures the network converges more efficiently by isolating the optimization of Gaussian sphere parameters and feature extraction, resulting in better cross-view localization performance.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Icosphere part (temp)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    % \subfigure[a]{
        \includegraphics[width=0.22\textwidth]{figure_2_1.png}
    % }
    % \subfigure[b]{
        \includegraphics[width=0.22\textwidth]{figure_2_2.png}
    % }
    % \subfigure[c]{
        \includegraphics[width=0.17\textwidth]{vigor1.png}
    % }
    % \subfigure[d]{
        \includegraphics[width=0.17\textwidth]{vigor2.png}
    % }
    % \subfigure[d]{
        \includegraphics[width=0.17\textwidth]{vigor3.png}
    % }
    \vspace{-1.2em}
    \caption{Visualization of the query ground image (up) and the estimated relative pose with respect to the satellite image (bottom right). The BEV image projected from the query ground image using the estimated Gaussian primitives is presented in the bottom left for each example. The left two examples are from the KITTI dataset, and the right three examples are from the VIGOR dataset.}
    \label{fig:stage2_results}
\end{figure*}


\section{Experiments}

In this section, we first describe the benchmark datasets and evaluation metrics for evaluating the effectiveness of cross-view localization models, followed by implementation details of our method. 
% in the used in our experiments, our network's evaluation metrics and the implementation details. 
Subsequently, we compare our method with state-of-the-art approaches and conduct experiments to demonstrate the necessity of each component of the proposed method.

% \subsection{Datasets and Evaluation metrics}
\textbf{KITTI dataset}. The KITTI dataset \cite{geiger2013vision} consists of ground-level images captured by a forward-facing pinhole camera with a restricted field of view, complemented by aerial images \cite{shi2022accurate}, where each aerial patch covers a ground area of approximately \(100 \times 100 \mathrm{m}^2\). The dataset includes a training set and two test sets: Test-1 contains images from the same region as the training set, while Test-2 consists of images from a different region. 
% The initial error of ground images' GPS location is assumed to be within $\pm 20$m of the true locations,
% Ground images are assumed to be located within a \(40 \times 40 \mathrm{m}^2\) area at the center of their corresponding aerial patches,  
The location search range of ground images is approximately \(56 \times 56 \mathrm{m}^2\), with an orientation noise of \(\pm 10^\circ\). \\ 

\textbf{VIGOR dataset}. The VIGOR dataset \cite{zhu2021vigor} includes geo-tagged ground panoramas and satellite images from four US cities: Chicago, New York, San Francisco, and Seattle. Each satellite patch spans \(70 \times 70 \mathrm{m}^2\) and is labeled positive if the ground camera is within its central \(1/4\) region; otherwise, it is semi-positive. The dataset has Same-Area and Cross-Area splits: Same-Area uses training and testing data from the same region, while Cross-Area splits training and testing between two separate city groups. We use only positive satellite images for all experiments, following \cite{shi2024weakly}. \\ 

\textbf{Evaluation Metrics}. For the KITTI data set, we evaluated localization and orientation errors by calculating mean and median errors in meters and degrees, respectively. We also compute recall at thresholds of 1 m and 3 m for longitudinal (along the driving direction) and lateral (orthogonal to the driving direction) localization errors, as well as 1 ° and 3 ° for orientation errors. A localization is considered successful if the estimated position falls within the threshold of the ground truth, and an orientation is accurate if its error is within the angle threshold. For the VIGOR dataset, which does not provide driving direction information, we report mean and median errors as outlined in \cite{shi2024weakly}.

\begin{figure*}
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
     \begin{minipage}{0.23\linewidth}
        \includegraphics[width=\linewidth, height=0.4\linewidth]{rgb1_2.png}\\
        \includegraphics[width=\linewidth, height=0.4\linewidth]{rgb2_2.png}\\
        \centerline{\small (a) Original Image}
    \end{minipage}
    \begin{minipage}{0.23\linewidth}
        \includegraphics[width=\linewidth, height=0.4\linewidth]{rgb1_1.png}\\
        \includegraphics[width=\linewidth, height=0.4\linewidth]{rgb2_1.png}  \\
    \centerline{\small (b) Reconstructed Image}
    \end{minipage}
   \begin{minipage}{0.23\linewidth}
        \includegraphics[width=\linewidth, height=0.4\linewidth]{depth1_2.png}\\
        \includegraphics[width=\linewidth, height=0.4\linewidth]{depth2_2.png}\\
        \centerline{\small (c) Supervision Depth}
    \end{minipage}
    \begin{minipage}{0.23\linewidth}
        \includegraphics[width=\linewidth, height=0.4\linewidth]{depth1_1.png}\\
        \includegraphics[width=\linewidth, height=0.4\linewidth]{depth2_1.png}\\
        \centerline{\small (d) Rendered Depth}
    \end{minipage}
    \vspace{-1em}
    \caption{Visualization of original query images (a), reconstructed query images from the estimated Gaussian primitives (b), supervision depth maps from a foundation model (c), and rendered depth maps from the estimated Gaussian primitives (d). 
    % (a) and (b), (c) and (d) show the comparison of the RGB and depth of ground-level maps generated by 3DGS and the original ground-level maps for Example 1 and Example 2, respectively.
    }
    \label{fig:stage1_results}
\end{figure*}

% \begin{figure}[ht]
%     \centering
%     \subfigure[RGB Compare]{
%         % 第一张图片
%         \includegraphics[width=0.22\textwidth]{rgb1_1.png}
%         % \vskip 0.1cm % 图片间的垂直间距
%         % 第二张图片
%         \includegraphics[width=0.22\textwidth]{rgb1_2.png}
%     }
%     \hspace{-6cm}
%     \subfigure[Depth Compare]{
%         % 第一张图片
%         \includegraphics[width=0.22\textwidth]{depth1_1.png}
%         % \vskip 0.1cm % 图片间的垂直间距
%         % 第二张图片
%         \includegraphics[width=0.22\textwidth]{depth1_2.png}
%     }
%     \hspace{-6cm}
%     \subfigure[RGB Compare]{
%         % 第一张图片
%         \includegraphics[width=0.22\textwidth]{rgb2_1.png}
%         % \vskip 0.1cm % 图片间的垂直间距
%         % 第二张图片
%         \includegraphics[width=0.22\textwidth]{rgb2_2.png}
%     }
%     \hspace{-6cm}
%     \subfigure[Depth Compare]{
%         % 第一张图片
%         \includegraphics[width=0.22\textwidth]{depth2_1.png}
%         % \vskip 0.1cm % 图片间的垂直间距
%         % 第二张图片
%         \includegraphics[width=0.22\textwidth]{depth2_2.png}
%     }
%     \caption{(a) and (b), (c) and (d) show the comparison of the RGB and depth of ground-level maps generated by 3DGS and the original ground-level maps for Example 1 and Example 2, respectively.}
%     \label{fig:stacked_images}
% \end{figure}

\textbf{Implementation details.}
We employ the self-supervised direction regression network proposed in \cite{shi2024weakly} to provide prior knowledge about camera orientation. Subsequently, we use a pre-trained DINO model \cite{oquab2023dinov2} on ImageNet\cite{russakovsky2015imagenet} as a 3D Gaussian parameter extractor for ground images, following PixelSplat~\cite{charatan2024pixelsplat}. For feature extraction of both ground and satellite images, we also adopt a pretrained DINO as the backbone and fine-tune it with an additional DPT module \cite{ranftl2021vision}. 
% another DINO model pre-trained on ScanNet++(\cite{yeshwanth2023scannet++}) using the method proposed in \textit{Fit} and fine-tuned with a network similar to DPT. 
The satellite images and BEV images projected from ground images via 3D Gaussian primitives have resolutions of \(512 \times 512\) and \(128 \times 128\), respectively. The features of the bird's-eye view images obtained through 3DGS projection have a shape of \((C, H, W) = (32, 128, 128)\).
We implement our network using PyTorch and employ AdamW with a weight decay factor of 1e-3 as the optimizer, with a maximum learning rate of \(6.25 \times 10^{-5}\). We adopt the OneCycleLR scheduler with a cosine annealing strategy. Our network is trained with a batch size of 12 on a single NVIDIA RTX 4090 GPU. The training is conducted for 3 epochs on the KITTI dataset and 10 epochs on the VIGOR dataset.


\begin{table*}[t]
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\small
\caption{Comparison with the most recent state-of-the-art on KITTI.}
\renewcommand{\arraystretch}{1.1}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|l|cc|cc|cc|cccc}
\hline
Algorithms & $\lambda_3$   &                        & \multicolumn{2}{c|}{localization} & \multicolumn{2}{c|}{Lateral}                                             & \multicolumn{2}{c|}{Longitudinal}                                        & \multicolumn{4}{c}{Azimuth}                                      \\
           &   &                            & mean(m) $\downarrow$            & median(m) $\downarrow$          & d = 1m $\uparrow$                             & d = 3m $\uparrow$                               & d = 1m $\uparrow$                             & d = 3m $\uparrow$                              & $\theta$ = 1 ° $\uparrow$         & $\theta$ = 3 ° $\uparrow$          & mean(°) $\downarrow$          & median(°) $\downarrow$        \\ \hline
Boosting*  & - & \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Test-1\\ (Same\\ Area)\end{tabular}}  & -               & -               & 76.44                              & 96.34                               & 23.54                              & 50.57                               & 99.10          & \textbf{100.00}          & -             & -             \\
CCVPE*     & - &    & 1.22            & 0.62            & 97.35                              & 98.65                               & 77.13                              & 96.08                               & 77.39          & 99.47           & 0.67          & 0.54          \\
HC-Net*    & - &    & 0.80            & 0.50            & 99.01                              & -                                   & 92.20                              & -                                   & 91.35          & 99.84           & 0.45          & 0.33          \\ \cline{4-13}
G2SWeakly     & 1 &                        & 6.81            & 3.39            & \textbf{66.07}                     & 94.22                               & 16.51                              & 49.96                               & \textbf{99.99} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
Ours       & 1 &                        & \textbf{2.86}   & \textbf{2.00}   & 63.47                              & \textbf{94.74}                      & \textbf{34.32}                     & \textbf{77.81}                      & \textbf{99.99} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
G2SWeakly     & 0 &                        & 12.03           & 8.10            & 59.58                              & 85.74                               & 11.37                              & 31.94                               & \textbf{99.66} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
Ours       & 0 &                        & \textbf{6.63}   & \textbf{3.48}   & \textbf{62.57}                     & \textbf{91.25}                      & \textbf{21.20}                     & \textbf{45.53}                      & \textbf{99.66} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\ \midrule
Boosting*  & - & \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Test-2\\ (Cross\\ Area)\end{tabular}} & -               & -               & 57.72                              & 86.77                               & 14.15                              & 34.59                               & 98.98          & \textbf{100.00}          & -             & -             \\
CCVPE*     & - &  & 9.16            & 3.33            & 44.06                              & 81.72                               & 23.08                              & 52.85                               & 57.72          & 92.34           & 1.55          & 0.84          \\
HC-Net*    & - &  & 8.47            & 4.57            & 75.00                              & -                                   & 58.93                              & -                                   & 33.58          & 83.78           & 3.22          & 1.63          \\ \cline{4-13}
G2SWeakly     & 1 &                      & 12.15           & 7.16            & \multicolumn{1}{c}{64.74}          & \multicolumn{1}{c|}{86.18}          & \multicolumn{1}{c}{11.81}          & \multicolumn{1}{c|}{34.77}          & \textbf{99.99} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
Ours       & 1 &                      & \textbf{6.24}   & \textbf{2.68}   & \multicolumn{1}{c}{\textbf{65.05}} & \multicolumn{1}{c|}{\textbf{94.87}} & \multicolumn{1}{c}{\textbf{23.09}} & \multicolumn{1}{c|}{\textbf{54.69}} & \textbf{99.99} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
G2SWeakly     & 0 &                      & 13.87           & 10.24           & \multicolumn{1}{c}{62.73}          & \multicolumn{1}{c|}{86.53}          & \multicolumn{1}{c}{9.98}           & \multicolumn{1}{c|}{29.67}          & \textbf{99.66} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\
Ours       & 0 &                      & \textbf{7.57}   & \textbf{3.81}   & \multicolumn{1}{c}{\textbf{63.06}} & \multicolumn{1}{c|}{\textbf{93.15}} & \multicolumn{1}{c}{\textbf{19.14}} & \multicolumn{1}{c|}{\textbf{45.38}} & \textbf{99.66} & \textbf{100.00} & \textbf{0.33} & \textbf{0.28} \\ \hline
\end{tabular}
}
\label{tab:kitti}

\vspace{-1mm} % 添加一点间距
\begin{flushleft}
\textit{Note: Methods marked with * indicate supervised learning algorithms.}
\end{flushleft}
\end{table*}

\begin{table*}[t]
\centering
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\small
\caption{Comparison with the most recent state-of-the-art on VIGOR.}
\renewcommand{\arraystretch}{1.1}
\resizebox{0.95\textwidth}{!}{%

\begin{tabular}{c|c|cccc|cccc}
\hline
\multicolumn{1}{l|}{Method} & \multicolumn{1}{l|}{$\lambda_3$} & \multicolumn{4}{c|}{Same-Area}                                                                & \multicolumn{4}{c}{Cross-Area}                                                         \\
\multicolumn{1}{l|}{}       & \multicolumn{1}{l|}{}         & \multicolumn{2}{c|}{Aligned-orientation}           & \multicolumn{2}{c|}{Unknown-orientation} & \multicolumn{2}{l|}{Aligned-orientation}     & \multicolumn{2}{l}{Unknown-orientation} \\
\multicolumn{1}{l|}{}       & \multicolumn{1}{l|}{}         & mean(m) $\downarrow$         & \multicolumn{1}{c|}{median(m) $\downarrow$}      & mean(m) $\downarrow$               & median(m) $\downarrow$            & mean(m) $\downarrow$      & \multicolumn{1}{c|}{median(m) $\downarrow$}    & mean(m) $\downarrow$              & median(m) $\downarrow$            \\ \hline
\multicolumn{1}{c|}{Boosting*}                    & -                             & 4.12          & \multicolumn{1}{c|}{1.34}          & -                   & -                  & 5.16          & \multicolumn{1}{c|}{1.40}          & -                  & -                  \\
CCVPE*                       & -                             & 3.37          & \multicolumn{1}{c|}{1.33}          & 3.48                & 1.39               & 4.96          & \multicolumn{1}{c|}{1.69}          & 5.16               & 1.78               \\
\multicolumn{1}{c|}{HC-Net*} & -                             & \textbf{2.65} & \multicolumn{1}{c|}{\textbf{1.17}} & \textbf{2.65}       & \textbf{1.17}      & 3.35          & \multicolumn{1}{c|}{1.59}          & 3.36               & 1.59               \\ \hline
\multicolumn{1}{c|}{G2SWeakly}  & 1                             & 4.19          & \multicolumn{1}{c|}{1.68}          & 4.18                & 1.66               & 4.70          & \multicolumn{1}{c|}{\textbf{1.68}} & 4.52               & \textbf{1.65}      \\
\multicolumn{1}{c|}{Ours}    & 1                             & \textbf{3.28} & \multicolumn{1}{c|}{\textbf{1.61}} & \textbf{3.34}       & \textbf{1.65}      & \textbf{3.80} & \multicolumn{1}{c|}{1.70}          & \textbf{3.93}      & 1.73               \\
\multicolumn{1}{c|}{G2SWeakly}  & 0                             & 5.22          & \multicolumn{1}{c|}{1.97}          & 5.33                & 2.09               & 5.37          & \multicolumn{1}{c|}{\textbf{1.93}} & 5.37               & \textbf{1.93}      \\
\multicolumn{1}{c|}{Ours}    & 0                             & \textbf{3.68} & \multicolumn{1}{c|}{\textbf{1.86}} & \textbf{3.72}       & \textbf{1.94}      & \textbf{4.50} & \multicolumn{1}{c|}{1.95}          & \textbf{4.61}      & 1.97               \\ \hline
\end{tabular}
}
\label{tab:vigor}

\vspace{-1mm} % 添加一点间距
\begin{flushleft}
\textit{\small Note: Methods marked with * indicate supervised learning algorithms .}
\end{flushleft}
\end{table*}

\subsection{Comparison with State-of-the-Art Methods}
We compare our method with the latest state-of-the-art (SOTA) approaches, including supervised methods such as Boosting~\cite{shi2023boosting}, CCVPE~\cite{xia2023convolutional}, and HC-Net~\cite{wang2024fine}, all of which rely on ground-truth camera poses for supervision. We also compare with G2Sweakly~\cite{shi2024weakly}, which uses only a satellite image and a corresponding ground image as input, similar to our setup.

\textbf{KITTI.} The comparison results on the KITTI dataset are summarized in Table~\ref{tab:kitti}. Since our rotation estimator is inherited from G2Sweakly, the rotation estimation performance is identical between the two methods. However, our method significantly outperforms G2Sweakly in terms of location estimation across all evaluation metrics, yielding substantial improvements in both longitudinal pose accuracy and the corresponding mean and median errors. This improvement can be attributed to the limitations of the IPM projection method used in G2Sweakly, which suffers from distortions in scenes that are far from the camera and fails to capture the details of objects above the ground plane. 

\begin{figure}[ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[width=0.9\linewidth]{IPM_vs_BEVSplat_new.png}
    \vspace{-1.2em}
    % \includegraphics[width=0.46\textwidth]{feat_vs_1.png}    
    % % \vskip 0.1cm  % 调整间距
    % \includegraphics[width=0.46\textwidth]{feat_vs_2.png}    
    \caption{Comparison between BEV feature maps obtained by IPM and the proposed BevSplat.
    % Two examples of ground features projected into the BEV view are presented. In each example, from left to right: (1) the first image is the input ground-level map, (2) the second image shows the result obtained using BevSplat projection, and (3) the fourth image shows the result obtained using IPM (Inverse Perspective Mapping) from G2SWeakly.
    }
    \label{fig:IPM_vs_BEVSplat}
\end{figure}

Our feature-based Gaussian splatting for BEV synthesis effectively addresses these issues, leading to a notable enhancement in localization accuracy. Fig.~\ref{fig:open_figure} and  Fig.~\ref{fig:IPM_vs_BEVSplat} visualize the difference between the IPM projection and our proposed BEV synthesis method, clearly demonstrating that our projection technique resolves challenges such as occlusions caused by tall objects (e.g., buildings, trees, vehicles) and geometric distortions from curved roads. Furthermore, in cross-area evaluations, our method even surpasses supervised approaches (Boosting, CCVPE, HC-Net) in terms of mean and median errors, showcasing the strong generalization ability of our approach and highlighting the potential of weakly supervised methods.

\textbf{VIGOR.} The comparison results on VIGOR are presented in Table~\ref{tab:vigor}. Our method demonstrates a significant reduction in mean error compared to the baseline approach, G2Sweakly, across all evaluation scenarios. This reduces the gap between weakly supervised and fully supervised methods, indicating that our approach generalizes effectively to diverse localization tasks, including both same-area and cross-area scenarios, as well as cases where the query images are either panoramic or captured using pinhole cameras.

\textbf{Visualization.} 
We provide visualizations of the query images and localization results in Fig.\ref{fig:stage2_results}. For better clarity, we show the synthesized BEV image generated from our estimated Gaussian primitives at the bottom left of each example (though the model uses BEV feature maps for localization). In Fig.\ref{fig:stage1_results}, we present the reconstructed images and depth maps derived from our estimated Gaussian primitives, alongside their corresponding original images and ground truth depth maps from the foundation model. Since the resolution of the generated Gaussians $(64 \times 256)$ is much lower than that of the compared images $(256 \times 1024)$, the reconstructed images appear blurrier.

% We provide visualization of the query images and localization results on the two datasets in Fig.~\ref{fig:stage2_results}. For a better visualization, we show the synthesized BEV image using our estimated Gaussian primitives on the bottom left for each example. In Fig.~\ref{fig:stage1_results}, we present the reconstructed images and depth maps from our estimated Gaussian primitives and their corresponding original images and supervision depth maps from the foundation model. Since the resolution of generated Gaussians $(40 \times 40)$ is significantly smaller than the original image, the reconstructed images are blurry compared to the original image.

% \subsection{Comparison with State-of-the-Art Methods}
% In this section, we compare our method with the most recent state-of-the-art (SOTA) approaches, including supervised algorithms such as Boosting~\cite{shi2023boosting}, CCVPE~\cite{xia2023convolutional}, and HC-Net~\cite{wang2024fine} which use ground-truth camera poses for supervision, and G2Sweakly~\cite{shi2024weakly} which depends solely on a satellite image and a corresponding ground image as input, the same setting as ours.

% \textbf{KITTI dataset}. The comparison results on the KITTI dataset are presented in Table 1. 
% Since our rotation estimator is from G2Sweakly, the rotation estimation performance between the two methods is the same. 
% However, the proposed method significantly outperforms G2Sweakly on location estimation on all evaluation metrics, considerably improving the longitudinal pose accuracy and thus mean and median errors. 
% This is because the IPM projection method used in G2Sweakly suffers from distortion for scenes distant to the camera location and lost information of scene objects above the ground plane. 
% By effectively handling these limitations by our feature-based Gaussian splitting for BEV synthesis, our method significantly improves the localization accuracy. Fig. xxx visualizes the differences between the IPM projection and our BevSplat for overhead view synthesis, which further demonstrates that our proposed projection method effectively addresses challenges such as occlusions caused by tall objects like buildings, trees, and vehicles, as well as geometric distortions from curved roads. 
% Importantly, for the cross-area evaluation, our method even outperforms supervised methods (CCVPE, HC-Net) in terms of mean and median errors, demonstrating the strong generalization ability of our method and showing the powerful potential of weakly supervised approaches. 

% As seen, our method achieves the best performance among all methods in the cross-area scenario in terms of mean and median errors. Specifically, in the localization (m) metric, our method reduces the mean error to nearly half of that of the previous weakly approach and the median error to approximately one-third. Moreover, for orientation estimation, our method improves the Longitudinal metric by nearly twofold compared to the weakly method, both in same-area and cross-area scenarios.

% This highlights that our proposed projection method effectively addresses challenges such as occlusions caused by tall objects like buildings, trees, and vehicles, as well as geometric distortions from curved roads. Consequently, the accuracy of vehicle directional localization has been significantly improved.

% For the estimation of the camera's viewing direction, we utilized the first-stage network and corresponding weights from Boosting. This allowed us to align the directions before conducting subsequent localization, and the accuracy of the estimation remains consistent with the original Boosting approach.

% \textbf{VIGOR dataset}. The comparison results on the VIGOR dataset are shown in Table 2. 
% Our method significantly improves the mean error over the baseline approach G2Sweakly in terms of the mean error in all evaluation scenarios, making the gap between weakly supervised and fully supervised approach smaller. This demonstrates that our method generalizes well across localizations scenarios (same-area and cross-area, query images are panoramas or captured by pin-hole cameras). 
% In the weakly supervised task, our method outperforms the previous SOTA method proposed in weakly in terms of the mean metric. However, for the median metric, our method only surpasses weakly in the same-area case when $\lambda=0$, and under other conditions, weakly performs better. 

% Based on our observations, the primary reason for this difference lies in the fact that many scenes in the VIGOR dataset feature relatively flat ground surfaces with little height variation. The weakly method assumes that all pixels in the ground-level map have a height of 0 and applies inverse perspective mapping, which makes its ground projection highly accurate for the VIGOR dataset. In contrast, our method is based on 3D reconstruction using pre-trained depth models on pinhole images, followed by projection. These large pre-trained depth models often struggle with accurate depth estimation for panorama images. Even when we split a panorama into 20 pinhole images based on an icosahedral projection and estimate depth for each, the resulting depth maps exhibit scale inconsistencies, leading to inaccuracies in 3D reconstruction and poor ground projection performance.

% We are currently exploring solutions to correct the depth estimation for panorama images and believe that with more accurate depth predictions, our method can achieve significant improvements.

\subsection{Ablation Study}

\textbf{Different BEV synthesis approaches. }
\label{Ground to satellite rendering method}
To validate the effectiveness of our BevSplat method, we compared it with the IPM used in \cite{shi2024weakly} with the same backbone VGG. 
The IPM projection method assumes that each pixel in the ground view image corresponds to a real-world height of 0m. Consequently, this method produces accurate BEV (Bird’s-Eye View) representations for flat road surfaces at a height of 0. However, for objects in the ground image with non-zero height, the BEV view experiences significant stretching along the line of sight. For example, in the first instance depicted in Fig\ref{fig:IPM_vs_BEVSplat}, buildings are distorted into areas that should not appear. Similarly, in the second instance in Fig\ref{fig:IPM_vs_BEVSplat}, the red car is also stretched into a region that is not visible in the ground image. Additionally, the IPM projection method only projects the lower half of the ground image into the BEV view, neglecting the information in the upper half, whereas BevSplat explores more authentic geometry information and utilizes the entire image. 
The experimental results are presented in Table \ref{tab:bev_splat_comparison}. Our BevSplat method outperforms IPM. 



\begin{table}[t]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \caption{Comparison of different methods on Test-1 (Same-area) and Test-2 (Cross-area) of the KITTI dataset. 
    % The best results for each setting are highlighted in \textbf{bold}.
    }
    \label{tab:bev_splat_comparison}
    \resizebox{\linewidth}{!}{ % 让表格适应页面宽度
    \begin{tabular}{c c c | c c | c c}
        \toprule
        \multirow{2}{*}{\makecell{Rendering \\ Method}} & \multirow{2}{*}{BackBone} & \multirow{2}{*}{$\lambda_3$} & 
        \multicolumn{2}{c|}{Test-1 (Same-area)} & \multicolumn{2}{c}{Test-2 (Cross-area)} \\
        \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
        & & & Mean $\downarrow$ & Median $\downarrow$ & Mean $\downarrow$ & Median $\downarrow$ \\
        \midrule
        IPM       & VGG  & 0 & 12.03              & 8.10              & 13.87              & 10.24             \\
        BevSplat  & VGG  & 0 & 9.22               & 4.83              & 11.64              & 6.80     \\
        BevSplat  & Dino & 0 & \textbf{6.63}      & \textbf{3.48}     & \textbf{7.57}      & \textbf{3.81}     \\
        \midrule
        IPM       & VGG  & 1 & 6.81               & 3.39              & 12.15              & 7.16              \\
        BevSplat  & VGG  & 1 & 6.49               & 2.72              & 10.02              & 4.29              \\
        BevSplat  & Dino & 1 & \textbf{2.86}      & \textbf{2.00}     & \textbf{6.24}      & \textbf{2.68}     \\
        \bottomrule
    \end{tabular}
    }
\end{table}



\begin{figure}[ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[width=\linewidth]{Self-visualization_new.png}
    % \includegraphics[width=0.46\textwidth]{feature_foundation_1.png}    
    % \vskip 0.1cm  % 调整间距
    % \includegraphics[width=0.46\textwidth]{feature_foundation_2.png}    
    \vspace{-2em}
    \caption{Visualization of the query ground images (a), synthesized BEV feature maps by our method (b), reference satellite feature maps (c), and localization results (d). }
    \label{fig:Dino_feat}
\end{figure}
\textbf{Foundation model backbone.}
To validate the effectiveness of our foundation model, we conducted experiments using Dino\cite{oquab2023dinov2} and VGG\cite{simonyan2014very} as backbones to extract the features of ground images and satellites. The Dino model was fine-tuned with the \emph{dinov2\_base\_fine} weights from \cite{yue2025improving}. Although these weights were primarily fine-tuned for indoor scenes, they still significantly enhanced the 3D representations of Dino, allowing us to achieve strong results. Additionally, we fine-tuned our Dino model using a network structure similar to DPT \cite{ranftl2021vision}, and the project features are shown in Fig\ref{fig:Dino_feat}. The VGG model we used is the same as the VGG from the Unet structure \cite{ronneberger2015u} in G2Sweakly, and we applied the same pretrained weights. This enables us to fairly validate the effectiveness of our foundation model. As shown in Table \ref{tab:bev_splat_comparison}, Dino outperforms VGG in both same-area and cross-area scenarios. Particularly in the cross-area case, Dino demonstrates a significant improvement over VGG.


\section{Conclusion}
\label{sec:conclusion}

This paper has introduced a novel approach for weakly supervised cross-view localization by leveraging feature-based 3D Gaussian primitives to address the challenge of height ambiguity. Unlike traditional methods that assume a flat ground plane or rely on computationally expensive models such as cross-view transformers, our method synthesizes a Bird's-Eye View (BEV) feature map using feature-based Gaussian splatting, enabling more accurate alignment between ground-level and satellite images. Additionally, our method is designed to be memory-efficient, making it suitable for on-device deployment. We have validated our approach on the KITTI and VIGOR datasets, demonstrating that our model achieves superior localization accuracy. 

Future work could explore extending our method to incorporate additional cues, such as temporal information from video sequences, to improve localization robustness in dynamic environments. 
% Additionally, investigating self-supervised or contrastive learning techniques could further enhance feature representations without relying on labeled data.
We believe that our approach provides a promising direction for scalable and accurate cross-view localization, paving the way for real-world applications in autonomous navigation, geospatial analysis, and beyond.

% Our approach introduces a feature-based Gaussian representation, where each pixel in the ground image is modeled as a 3D Gaussian with learned semantic and spatial features. Using a visibility-aware rendering algorithm with anisotropic splatting, we construct a high-fidelity BEV feature map that facilitates robust similarity matching with satellite images. Additionally, our method is designed to be memory-efficient, making it suitable for on-device deployment.

% We validated our approach on the KITTI and VIGOR datasets, demonstrating that our model achieves superior localization accuracy while reducing GPU memory consumption compared to existing techniques. Our experiments further highlight the advantages of decoupling Gaussian sphere optimization from feature learning, ensuring stable training and effective representation learning.

% Future work could explore extending our method to incorporate additional cues, such as temporal information from video sequences, to improve localization robustness in dynamic environments. 
% % Additionally, investigating self-supervised or contrastive learning techniques could further enhance feature representations without relying on labeled data.
% We believe that our approach provides a promising direction for scalable, efficient, and accurate cross-view localization, paving the way for real-world applications in autonomous navigation, geospatial analysis, and beyond.



\section*{Impact Statement}
Nowadays, mobile robots such as drones and autonomous vehicles have been integrated into various industries. Compared to using expensive high-precision GPS, BevSplat leverages computer vision to achieve real-time localization for mobile robots using only a single camera or a combination of a camera and an inexpensive low-precision GPS. This approach also enables high-precision localization in areas where GPS signals are unavailable or unreliable. \\

We plan to open-source our code, training data, and model weights on GitHub. Our code can run efficiently on a single NVIDIA RTX 4090 GPU. We welcome everyone to try our implementation and collaborate on further research in this direction.




% Please add the following required packages to your document preamble:
% \usepackage{multirow}



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Robustness to Localization Errors}
We present the localization results of our method at different location initialization errors in Tab.~\ref{tab:appendix}. As the error decreases, the localization performance improves significantly. 
% Table~\ref{tab:kitti_comparison} shows that our method outperforms G2SWeakly under different localization errors of $28 \times 28 \text{ m}^2$ and $56 \times 56 \text{ m}^2$. Furthermore, as the error decreases, our localization performance improves. This result demonstrates the robustness of our method.

% \begin{table*}[h]
%     \centering
%     \setlength{\abovecaptionskip}{0pt}
%     \setlength{\belowcaptionskip}{0pt}
%     \caption{Performance comparison on different location error ranges for the cross-view KITTI dataset.}
%     \label{tab:kitti_comparison}
%     \renewcommand{\arraystretch}{1.2}  % 增加行间距

%     \begin{tabular}{c|l|cc|cc|cc}
%         \toprule
%         \multirow{2}{*}{\makecell{Location \\ Error (m\(^2\))}} & \multirow{2}{*}{Algorithms} & 
%         \multicolumn{2}{c|}{Lateral} & \multicolumn{2}{c|}{Longitudinal} & \multicolumn{2}{c}{Azimuth} \\
%         \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
%         &  & \( d = 1 \uparrow \) & \( d = 3 \uparrow \) & \( d = 1 \uparrow \) & \( d = 3 \uparrow \) & \( \theta = 1 \uparrow \) & \( \theta = 3 \uparrow \) \\
%         \midrule
%         \multicolumn{8}{c}{\textbf{Test-1 (Same-area)}} \\[3pt]
%         \midrule
%         \multirow{4}{*}{28 × 28} 
%         & G2SWeakly (\(\lambda = 0\))  & 61.46 & 87.76 & 13.44 & 38.14 & 99.76 & 100.00 \\
%         & G2SWeakly (\(\lambda = 1\))  & 66.39 & 94.38 & 18.18 & 53.59 & 99.76 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 0\))  & 61.50 & 93.37 & 27.53 & 64.00 & 99.99 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 1\))  & 62.33 & 94.85 & 34.30 & 78.87 & 99.99 & 100.00 \\
%         \midrule
%         \multirow{4}{*}{56 × 56} 
%         & G2SWeakly (\(\lambda = 0\))  & 59.58 & 85.74 & 11.37 & 31.94 & 99.66 & 100.00 \\
%         & G2SWeakly (\(\lambda = 1\))  & 66.07 & 94.22 & 16.51 & 49.96 & 99.66 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 0\))  & 62.57 & 91.25 & 21.20 & 45.53 & 99.99 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 1\))  & 63.47 & 94.74 & 34.32 & 77.81 & 99.99 & 100.00 \\
%         \midrule
%         \multicolumn{8}{c}{\textbf{Test-2 (Cross-area)}} \\[3pt]
%         \midrule
%         \multirow{4}{*}{28 × 28} 
%         & G2SWeakly (\(\lambda = 0\))  & 65.62 & 90.32 & 13.46 & 38.53 & 99.97 & 100.00 \\
%         & G2SWeakly (\(\lambda = 1\))  & 67.90 & 89.76 & 14.29 & 42.92 & 99.97 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 0\))  & 65.28 & 95.95 & 25.16 & 60.63 & 99.99 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 1\))  & 61.93 & 95.83 & 27.07 & 64.89 & 99.99 & 100.00 \\
%         \midrule
%         \multirow{4}{*}{56 × 56} 
%         & G2SWeakly (\(\lambda = 0\))  & 62.73 & 86.53 & 9.98 & 29.67 & 99.99 & 100.00 \\
%         & G2SWeakly (\(\lambda = 1\))  & 64.74 & 86.18 & 11.81 & 34.77 & 99.99 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 0\))  & 63.06 & 93.15 & 19.14 & 45.38 & 99.99 & 100.00 \\
%         & \textbf{Ours} (\(\lambda = 1\))  & 65.05 & 94.87 & 23.09 & 54.69 & 99.99 & 100.00 \\
%         \bottomrule
%     \end{tabular}
% \end{table*}


\begin{table}[H]
    \centering
    \caption{Performance comparison under different location error settings. 
    % The best results are highlighted in \textbf{bold}.
    }
    \label{tab:location_error}
    \renewcommand{\arraystretch}{1.2}  % 增加行间距

    \begin{tabular}{c l | c c | c c}
        \toprule
        \multirow{2}{*}{\makecell{Location \\ Error (m\(^2\))}} & \multirow{2}{*}{Method} & 
                                     \multicolumn{2}{c|}{Test-1 (Same-area)} & \multicolumn{2}{c}{Test-2 (Cross-area)} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        &  & Mean(m) $\downarrow$ & Median(m) $\downarrow$ & Mean(m) $\downarrow$ & Median(m) $\downarrow$ \\
        \midrule
        \multirow{2}{*}{56 × 56} 
        & Ours ($\lambda_3$ = 0) & 6.63 & 3.48 & 7.57 & 3.81 \\
        & Ours ($\lambda_3$ = 1) & 2.86 & {2.00} & 6.24 & 2.68 \\
        \midrule
        \multirow{2}{*}{28 × 28} 
        & Ours ($\lambda_3$ = 0) & {3.54} & {2.48} & {3.80} & {2.57} \\
        & Ours ($\lambda_3$ = 1) & {2.74} & 2.14 & {3.62} & {2.38} \\
        \bottomrule
    \end{tabular}
    \label{tab:appendix}
\end{table}

\section{Additional Visualization}
In Fig.~\ref{fig:Vigor_20}, We provide visualization for the $k=20$ facets of the icosphere-based decomposition of the panoramas on the VIGOR dataset and the corresponding depth maps estimated by Depth Anything V2. These images and depth maps form the supervision for the estimated Gaussian primitives from panoramic images. 

\begin{figure}[ht]
    \centering
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \includegraphics[width=\linewidth]{vigor_20_all.png}
    % \includegraphics[width=0.46\textwidth]{feature_foundation_1.png}    
    % \vskip 0.1cm  % 调整间距
    % \includegraphics[width=0.46\textwidth]{feature_foundation_2.png}    
    \vspace{-2em}
    \caption{Visualization of the VIGOR}
    \label{fig:Vigor_20}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
