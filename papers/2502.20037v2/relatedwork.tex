\section{Related Work}
\label{sec_works}
\subsection{Transparent Depth Completion}
Transparent object depth reconstruction is an active research area employing  various devices such as optical cameras and depth sensors. Some works also introduce additional sensors to complete the depth maps. Existing works can be categorized based on the observation views into multi-view and single-view depth reconstruction.

Multi-view depth reconstruction methods often rely on Neural Radiance Fields (NeRF), which can accurately reconstruct the scene geometry. For instance, Dex-NeRF is designed for depth map reconstruction and transparent object grasping in complex scenes\cite{dex-nerf}, while Evo-NeRF is suitable for sequential transparent object grasping \cite{evo-nerf}. 
Other works like StereoPose, which uses stereo images for transparent object pose and depth estimation, alsorely on multi-view observation \cite{stereopose}.
In single-view depth reconstruction, the first algorithm, ClearGrasp, uses a neural network on synthetic RGB-D datasets to reconstruct depth maps of transparent objects \cite{cleargrasp}. Subsequently, researchers use realistic data instead of synthetic data to train a more robust model. Among these, TransCG, a model employing a U-Net architecture to perform the transparent object depth completion task, has achieved a best performance \cite{transcg}. Some fusion solutions are included in single-view parts. Polarized-CNN combine a polarization camera with an RGB-D camera to search and grasp the transparent targets \cite{kalra2020deep}. TaTa integrates the camera with tactile sensors and can even grasp tiny glass fragments after exploration by a robotic arm \cite{visual_tactile}.

A common limitation of these works is that they almost rely on vision, which may degrade in challenging conditions such as dim light, resulting in lower grasp accuracy. For these additional sensors, the fusion algorithm does not naturally integrate with visual information. To address this, we propose using mmWave radar as a supplementary sensor to provide complementary information for reconstructing the transparent object shape from RGB-D images.

\subsection{Radar Sensing and Imaging}
Radar technology, widely applied in daily life, can be classified into three main types: array signal processing for object sensing \cite{mobi2sense, chen2021movi, khan2022estimating}, point cloud generation for object detection \cite{mmMesh, MilliPoint}, and SAR for shape imaging \cite{zheng2021siwa, UWBMap, MIMO-SAR}.

Array signal processing, based on antenna arrays, is extensively used for radar-based sensing and tracking. For example, Mtrack is an indoor human sensing system that uses beamforming technology to localize multiple moving humans \cite{Mtrack}. Point cloud radars are popular for autonomous driving. MILLIPOINT uses point clouds to detect surrounding objects and enhance driving safety \cite{MilliPoint}. RCVNet employed a feature fusion approach, integrating radar point clouds with visual images, to enable the accurate detection of bird damage in power tower areas \cite{ref2_gao2023rcvnet}.

For SAR imaging applications, UWBMap uses a multi-radar system for indoor mapping and floor plan construction, and it can operate under smoking conditions\cite{UWBMap}. SiWa is another radar imaging work that uses a network to generate SAR images to detect pipes and wires in concrete walls \cite{zheng2021siwa}. MIMO-SAR uses stepping motors to control radars’ point-to-point scanning, and it has achieved high resolution SAR imaging for security checking \cite{MIMO-SAR}.

These three methods have their own advantages and limitations in radar perception. Array signal processing can only locate the target’s position without obtaining its shape details. The sparsity of radar point clouds makes shape reconstruction challenging. SAR imaging can provide geometric shape information to supplement the missing depth data. Therefore, we uses SAR technology to obtain radar images of transparent object, and signal calibration is necessary to improve the radar imaging performance.