
\documentclass{article}
\usepackage{arxiv}

\usepackage{setspace}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\methodName}[0]{SALSA-RL}

\usepackage[textsize=tiny]{todonotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\title{\methodName{}: Stability Analysis in the Latent Space of Actions for Reinforcement Learning}

\author{ \hspace{1mm}Xuyang Li\\
	College of Information Sciences and Technology\\
	Pennsylvania State University\\
	University Park, PA 16802\\
	\texttt{lixuyang@psu.edu} \\
	%% examples of more authors
	\And
	\hspace{1mm}Romit Maulik \\
	College of Information Sciences and Technology\\
	Pennsylvania State University\\
	University Park, PA 16802 \\
	\texttt{rmaulik@psu.edu} \\
}

\renewcommand{\shorttitle}{\textit{arXiv} \methodName{}}

\hypersetup{
pdftitle={\methodName{}},
pdfauthor={Xuyang Li, Romit Maulik},
pdfkeywords={reinforcement learning, dynamical system, stability analysis},
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Modern deep reinforcement learning (DRL) methods have made significant advances in handling continuous action spaces. However, real-world control systems--especially those requiring precise and reliable performance--often demand formal stability, and existing DRL approaches typically lack explicit mechanisms to ensure or analyze stability.
To address this limitation, we propose \methodName{} (Stability Analysis in the Latent Space of Actions), a novel RL framework that models control actions as dynamic, time-dependent variables evolving within a latent space. By employing a pre-trained encoder-decoder and a state-dependent linear system, our approach enables both stability analysis and interpretability.
We demonstrated that \methodName{} can be deployed in a non-invasive manner for assessing the local stability of actions from pretrained RL agents without compromising on performance across diverse benchmark environments. By enabling a more interpretable analysis of action generation, \methodName{} provides a powerful tool for advancing the design, analysis, and theoretical understanding of RL systems.
\end{abstract}

\keywords{Reinforcement learning \and Dynamical system \and Stability analysis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

\begin{figure}[!ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.75\columnwidth]{overview.pdf}}
\caption{\textbf{Overview} of the \methodName{} framework. Our proposed augmentation to RL algorithms relies on a latent action representation governed by a time-varying linear dynamical system governed by a state-conditioned matrix $\mathbf{A}_t$. This enables local stability analyses in the action-state phase for reliable and interpretable RL deployments. The framework integrates seamlessly with existing DRL algorithms, maintaining competitive performance. 
The contour shows how latent action stability evolves, with the policy driving the system from instability to stability. Stability boundaries (white) mark critical state regions where large deviations of angular shifts, can cause instability or loss of control.
}
\label{fig:overview}
\end{center}
\vskip -0.2in
\end{figure}


Reinforcement learning (RL) is a powerful framework for training agents to make sequential decisions directly from environment interactions \cite{sutton2018reinforcement}, and it has shown remarkable success in complex continuous control tasks \cite{lillicrap2015continuous}. Unlike discrete decision-making tasks (e.g., Chess or Go), real-world dynamical systems evolve continuously, often requiring fine-grained and highly accurate control inputs to ensure safe and robust operation. In these settings, even small errors in control can propagate quickly, leading to instability, unpredictable behavior, and potential system failures. Despite these risks, most existing DRL approaches \cite{yu2023reinforcement, heuillet2021explainability, kalashnikov2018scalable} do not provide explicit mechanisms for analyzing or ensuring stability—an omission that poses significant challenges in safety-critical domains.

In dynamical system control, actions play a central role as they directly influence state evolution by serving as the primary inputs driving changes in the state trajectory. While state trajectories reflect the physical behavior of the system, they often fail to reveal the reasoning behind the control decisions. Particularly, different policies can produce similar state trajectories while relying on fundamentally different decision patterns, making state-based observations insufficient for fully understanding control logic.
In contrast, action dynamics, which describe how control inputs evolve over time in response to system feedback, offer a more concrete way to assess the structure of a control policy. By analyzing trends and variations in action sequences, critical patterns can emerge, such as smooth and consistent adjustments indicating stable regulation, abrupt shifts reflecting reactive strategies, or periodic oscillations suggesting transience even in stable regimes.
In the Lunar Lander problem \cite{brockman2016openai}, for instance, multiple strategies can achieve a successful landing, but the specific thrust and rotation sequences can differ greatly, indicating distinct approaches to stabilization and error correction.

Additionally, observing how actions evolve provides insight into a policy’s ability to maintain stability by avoiding erratic decision patterns, adapting to disturbances with smooth control adjustments, and balancing exploration and exploitation for effective regulation without excessive correction. This analysis also enhances RL interpretability through various different aspects:

\begin{enumerate}[nosep]
\item \textbf{Stability}: Are control actions and observations (states) able to achieve stability within local or global regions? 

\item \textbf{Predictability and Consistency}: Do action patterns follow a logical and predictable progression, across similar conditions? 

\item \textbf{Risk Identification}: Can unsafe or failure-prone state-action combinations be identified through the interaction?
\end{enumerate}
Identifying such patterns not only aids in policy validation but also helps detect subtle issues like overcompensation, delayed reactions, or unstable oscillations that may not be evident when observing state trajectories alone.

In light of the importance of action dynamics for understanding control strategies, we propose \methodName{}, a novel RL framework for dynamical system control that leverages action dynamics to analyze and design stable control strategies while enhancing interpretability. Unlike standard RL approaches where actions are treated as discrete decisions, our framework models actions in a latent space, evolving continuously based on the state information.
We achieve this by employing a pre-trained autoencoder for action encoding and decoding. The action representation is encoded into a latent space, where the latent action dynamics $z_t$ evolve following a differential equation of the form: $\dot{z}=\mathbf{A}_t$, where $\mathbf{A}_t$ is a state-dependent dynamic matrix, learned through a deep neural network conditioned on the current state. 
This structured representation not only allows for local-stability analysis of control policies but also provides a means to visualize and interpret the constraints imposed on the state space, enhancing both policy transparency and generalization. Figure \ref{fig:overview} illustrates the details of \methodName{}.

This proposed framework is compatible with existing popular DRL methods (such as SAC, DDPG, TD3 \cite{haarnoja2018soft, fujimoto2018addressing, lillicrap2015continuous}) and can be easily integrated using exactly the same standard training strategies while maintaining competitive control performance. Beyond achieving effective control, \methodName{} provides valuable tools for evaluating trained policies. Since actions directly influence state evolution, the influence of the action on local stability of action dynamics serves as an indirect yet effective indicator of the stability of the state trajectories. 
Furthermore, as a key variable in the latent dynamics, the time-dependent square matrix $\mathbf{A}_t$, allows for the incorporation of local-in-time eigenvalue-based stability analysis. Consequently, one can perform a rigorous evaluation of the system's behavior. This analysis can provide insights into how action-space and constraints impact long-term stability and how state transitions behave near critical regions, further enhancing the interpretability of the learned control policy. 

Our contributions are summarized as follows:
\begin{enumerate}
\item We propose \methodName{}, a framework that models control actions as continuous dynamics evolving in the latent space with a time-varying linear system, enabling a deeper understanding of control strategies and dynamics.

\item \methodName{} seamlessly integrates with standard DRL methods for training, maintaining performance while enhancing interpretability.

\item Our stability analysis demonstrates how the control policy achieves stability in states and actions while identifying and avoiding high-risk (unstable) regions.

\item We show how key states affect system stability and control actions, potentially providing insights to inform better reward design and policy refinement.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\subsection{Classical Controls} 
Classical approaches for dynamical system controls include Proportional-Integral-Derivative (PID), Linear Quadratic Regulators (LQRs), and adaptive control techniques\cite{aastrom2006advanced, swarnkar2014adaptive}. These methods offer clear advantages in interpretability and stability analysis by providing explicit symbolic expressions for the relationship between states and control actions, enabling stability analysis and control law verification. However, they often struggle with high-dimensional, nonlinear, or partially observable systems where dynamics cannot be explicitly modeled \cite{skogestad2005multivariable}. 


\subsection{Interpretability in DRL} 
Interpretability for machine learning algorithms has gained significant attention in recent years, particularly in healthcare, robotics, and autonomous systems \cite{glanois2024survey, yu2023reinforcement, wells2021explainable, heuillet2021explainability}. Recent research devoted to interpretable RL has explored diverse approaches such as multi-agent systems \cite{zabounidis2023concept}, interpretable latent representations \cite{chen2021interpretable}, and techniques like attention mechanisms \cite{mott2019towards} or genetic programming \cite{hein2018interpretable}, balancing transparency, performance, and generalizability. Below, we outline a few key directions.

\textbf{Hierarchical RL.} This approach focuses on planning and reasoning by structuring tasks into high-level (manager) and low-level (worker) policies \cite{lyu2019sdrl, nachum2018data, pateria2021hierarchical}. This modular approach excels in task decomposition, reusable subtasks, and explainable goal-setting. However, it is less suitable for specific use cases like dynamical system control, where precise and responsive low-level policies are essential \cite{florensa2017stochastic}.

\textbf{Prototype-based RL.} 
Prototype-based methods leverage human-defined prototypes to represent states and actions as interpretable latent features, enabling policies that balance interpretability and performance \cite{kenny2023towards, xiong2023interpretable, yarats2021reinforcement, biehl2016prototype}. However, their reliance on manual design and lack of temporal precision limit their adaptability to dynamic environments and their ability to handle continuous, high-dimensional dynamics effectively \cite{duan2016benchmarking}.


\subsection{RL for Dynamical System Controls}
DRL is well-studied in physical applications, such as optimizing flow control and turbulent fluid dynamics, where domain knowledge is often available, and state-control actions exhibit stronger correlations compared to other domains\cite{yousif2023physics, weiner2024model, garnier2021review, rabault2019artificial}. Recent approaches have further improved interpretability by leveraging state-action correlations and domain knowledge.

\textbf{Symbolic and Neural-Guided Approaches} 
Neural-guided methods like NUDGE \cite{delfosse2024interpretable}, PIRL \cite{verma2018programmatically}, and NLRL \cite{jiang2019neural} use symbolic reasoning to discover interpretable policies \cite{jin2022creativity}. Symbolic controllers \cite{khaled2022framework, reissig2018symbolic}, such as DSP \cite{landajuela2021discovering} and SINDy-RL \cite{zolman2024sindy}, employ explicit state-control mappings to derive generalizable control laws.
These methods improve interpretability and generalizability, making them suitable for safe, explainable decision-making. However, neural-guided approaches rely on logical representations \cite{delfosse2024interpretable}, limiting temporal precision and scalability in complex systems. Similarly, symbolic controllers face scalability issues in multi-dimensional or stochastic tasks where predefined forms fail to capture intricate interactions \cite{landajuela2021discovering}. While explicit formulations aid stability analysis, they may overlook subtle instabilities in evolving or poorly understood dynamics.

\textbf{Physics-guided RL}
Physics-guided RL \cite{liu2021physics, banerjee2023survey, alam2021physics, jurj2021increasing, cho2019physics, wang2022toward} integrates domain knowledge and physical principles to enhance learning and performance. Approaches include informed reward functions, model-based RL with physics-based or neural network surrogate models \cite{hernandez2023port, nousiainen2021adaptive}, and state design \cite{banerjee2023survey}. While incorporating domain knowledge can improve interpretability, these methods \cite{alam2021physics, jurj2021increasing, cho2019physics, wang2022toward} often lack generalizability to other problems and require significant fine-tuning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\subsection{Problem Formulation}
Consider a control system with continuous dynamics,
\begin{equation}
  \dot{\mathbf{s}}(t) \;=\; \mathcal{F}\bigl(\mathbf{s}(t), \mathbf{a}(t)\bigr),
\end{equation}
where $\mathbf{s}(t) \in \mathcal{S} \subseteq \mathbb{R}^n$ is the state and $\mathbf{a}(t) \in \mathcal{A} \subseteq \mathbb{R}^m$ is the control input. Discretizing the inputs at intervals $\Delta t$ yields a discrete-time Markov Decision Process (MDP) $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where $\mathcal{S}$ represents the state space and $\mathcal{A}$ the action space governing the system's behavior.

Assuming the Markov property, the next state $s_{t+1}$ depends only on the current state $s_t$ and action $a_t$, governed by the transition probability $P(s_{t+1} \mid s_t, a_t)$. The agent maximizes the cumulative discounted reward $R = \sum_{t=0}^{T-1} \gamma^t r(s_t, a_t)$, where $\gamma \in [0,1]$ balances immediate and future rewards over the time horizon $T$.

Unlike standard DRL, which utilizes only state-dependent policies $\pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t)$, \methodName{} relies on actions in a latent space generated by a dynamical system, resulting in the policy $\pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t, \mathbf{a}_{t-1})$. The policy is optimized to maximize the expected cumulative return:
\begin{equation}
  J(\pi_\theta)
  \;=\;
  \mathbb{E}_{\tau \sim \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t, \mathbf{a}_{t-1})}
  \Bigl[
    \sum_{t=0}^{T-1} \gamma^t \, R\bigl(\mathbf{s}_t, \mathbf{a}_t\bigr)
  \Bigr],
\end{equation}
where $\tau= (\mathbf{s}_0, \mathbf{a}_0, \dots, \mathbf{s}_{T}, \mathbf{a}_{T})$ represents trajectories sampled from the policy. Although the latent action evolves in continuous time, the discrete-time MDP formulation is sufficient for the proposed algorithm, as actions are applied at discrete intervals $\Delta t$.


\subsection{\methodName{} Control Framework}
The proposed framework aims to simplify time-dependent dynamical system control by encoding actions into a compact latent space, where state-dependent transformations govern their evolution. The framework consists of the following:

\textbf{Action Encoding and Latent Representation.} The policy's action $\mathbf{a}_t$ is encoded into a latent space using a pre-trained autoencoder:
\begin{equation} \label{eq:encoder}
   \mathbf{z}_t = \text{Encoder}(\mathbf{a}_t),
\end{equation}
where $\mathbf{z}_t \in \mathbb{R}^{d_h}$ is the latent representation of the action, $d_h$ is the latent dimension size, and $\mathbf{a}_0=\mathbf{0}$.

\textbf{Latent Dynamics Module.} The latent action representation evolves according to a learned state-dependent linear dynamical system $\dot{\mathbf{z}}_t = \mathbf{A}_t \mathbf{z}_t$. Simply, for discrete actions:  
\begin{align}
   \mathbf{z}_{t+1} &= \mathbf{z}_t+\mathbf{A}_t\mathbf{z}_t \label{eq:Az}, \\
   \mathbf{A}_t &= \mathcal{N}_\theta(\mathbf{s}_t), \label{eq:fs}
\end{align}
where $\mathbf{A}_t \in \mathbb{R}^{d_h \times d_h}$ is a state-dependent matrix learned by the neural network $\mathcal{N}_\theta$ conditioned on the current state. 

\textbf{Action Decoding and Control Execution.} After the linear system evolution, the latent representation is decoded back into the original action space as the actual control policy:  
\begin{equation}
   \mathbf{a}_{t+1} = \text{Decoder}(\mathbf{z}_{t+1}). \label{eq:decoder}
\end{equation}

\textbf{Policy Update and Control Loop.} The system evolves iteratively, where actions generated from the latent space update both the system state and latent variables. The framework is summarized in Algorithm~\ref{alg:1}.

\begin{algorithm}[ht]
   \caption{Latent Dynamic Control Framework}
   \label{alg:1}
\begin{algorithmic}
   \STATE {\bfseries Input:} Initial state $\mathbf{s}_0$, initial action $\mathbf{a}_0=\mathbf{0}$, time horizon $T$, pre-trained \textbf{Encoder} and \textbf{Decoder}
   \STATE $\mathbf{z}_0 \gets \textbf{Encoder}(\mathbf{a}_0)$ // Encode action 
   \FOR{$t = 0$ {\bfseries to} $T$}
      \STATE $\mathbf{A}_t \gets \mathcal{N}_\theta(\mathbf{s}_t)$
      \STATE $\mathbf{z}_{t+1} \gets \mathbf{z}_t + \mathbf{A}_t \mathbf{z}_t$ // Update latent space
      \STATE $\mathbf{a}_{t+1} \gets \textbf{Decoder}(\mathbf{z}_{t+1})$ // Decode latent to next action
      \STATE $\mathbf{s}_{t+1}, Done \gets \text{env.step}(\mathbf{a}_{t+1})$
      \STATE $\mathbf{a}_{t} \gets \mathbf{a}_{t+1}$
      \IF {$Done$}
      \STATE {\bfseries break}
      \ENDIF
   \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Training Procedure}
The training process consists of two stages: (1) pretraining the encoder-decoder network and (2) optimizing the latent dynamic module.

\textbf{Encoder-Decoder Network.} This network is trained to create a compact and effective latent representation of actions, optimized by minimizing the reconstruction loss:
\begin{equation}
    \mathcal{L}_{\text{recon}} = \mathbb{E}_{\mathbf{a}_t \sim D}\left[ \| \mathbf{a}_t - \hat{\mathbf{a}}_t \|^2 \right],
\end{equation}
where $\hat{\mathbf{a}}_t$ is the reconstructed action from the decoder. To train the module, training data, actions, are generated using trained DRL policies such as SAC and TD3. This data is then used to train the action encoding and decoding modules. We detailed this procedure in Appendix \ref{appendix:encoder-decoder}.

\textbf{Latent Dynamic Policy.} Following this, the proposed policy is initialized in a similar manner as most DRL methods for continuous action control, such as PPO \cite{schulman2017proximal}, SAC \cite{haarnoja2018soft}, DDPG \cite{lillicrap2015continuous}, and TD3 \cite{fujimoto2018addressing}. The policy network (actor), typically generating actions in DRL as $\mathbf{a}_t = \mathcal{N}_\theta(\mathbf{s}_t)$, is instead employed to predict a state-dependent dynamic matrix $\mathbf{A}_t = \mathcal{N}_\theta(\mathbf{s}_t, \mathbf{a}_{t-1})$. The gradients of the objective to maximize the expected cumulative reward, in deterministic policies, for example, can be expressed generally as:

\begin{equation}\label{eq:obj_refined}
\hspace{-1.0mm}
\nabla_\theta J(\pi_\theta)
\;=\;
\mathbb{E}_{\tau \sim \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t, \mathbf{a}_{t-1})}
\left[
\sum_{t=0}^{T-1} \gamma^t \nabla_\theta R\bigl(\mathbf{s}_t, \mathbf{a}_t\bigr)
\right]
\end{equation}

Meanwhile, the critic network structure, if available, remains unchanged, retaining its role in evaluating the state-action value function. This design allows \methodName{} to leverage existing DRL algorithms while providing flexibility to represent and optimize latent dynamics. A detailed gradient computation is provided in Appendix \ref{appendix:derivation}.



\subsection{Stability Analysis and Interpretability}
This section explores a range of numerical techniques for analyzing latent action dynamics, comprising the bulk of this work's contribution. This stability analysis provides insights into the system's robustness and enhances interpretability, improving understanding of the underlying agent's behavior.

\textbf{Eigenvalue-based Stability.} 
To assess the stability of learned control policies, we apply eigenvalue-based analysis to the dynamic matrix $\mathbf{A}_t$. Stability at each time step is determined by the magnitude of its largest eigenvalue $\lambda_1$.
\begin{equation}
    \lambda_1 = \max \left( \text{eigvals}(\mathbf{A}_t) \right).
\end{equation}
A policy is considered locally stable if the largest eigenvalue satisfies $\mathrm{Re}(\lambda_1) < 1$, ensuring that latent dynamics do not diverge over time. Additionally, a non-zero imaginary component $\mathrm{Im}(\lambda_1)$ indicates oscillatory behavior. Specifically:
$\mathrm{Re}(\lambda_1) < 1$ and $\mathrm{Im}(\lambda_1) \neq 0$ indicates damped oscillations, $\mathrm{Re}(\lambda_1) = 0$ and $\mathrm{Im}(\lambda_1) \neq 0$ indicates pure oscillations, and $\mathrm{Re}(\lambda_1) > 1$ implies unstable dynamics.

Additionally, the neighborhood states around the observed states, denoted as $s_\text{range}$, are evaluated by computing the corresponding $\mathbf{A}_t$ and analyzing the eigenvalues. This evaluation offers an overview of stable and unstable regions during control, capturing local state landscapes that may influence stability. It provides insights into regions where the policy achieves stable control or displays unstable behavior. The procedure is outlined in Algorithm~\ref{alg:eigen_evaluation}.
\begin{algorithm}[h]
   \caption{Latent Dynamic Eigenvalue Evaluation}
   \label{alg:eigen_evaluation}
\begin{algorithmic}
\STATE {\bfseries Input:} Observed $n$ states $s_t^{(i)}$, where $i = 1, 2, ..., n$
\FOR{$i = 1$ {\bfseries to} $n$}
    \FOR{$t = 0$ {\bfseries to} $T$}
        \STATE $\hat{\mathbf{s}} \gets [s_t^{(1)}, s_t^{(2)}, ..., s_t^{(n)}]$ // Construct state vector
        \FOR{$s$ {\bfseries in} $s_\text{range}^{(i)}$}
            \STATE $\hat{\mathbf{s}}[i] \gets s$ // Replace with neighboring values
            \STATE $\mathbf{A}_t \gets \mathcal{N}_\theta(\hat{\mathbf{s}})$
            \STATE $\lambda_1 \gets \text{max}\bigl(\text{eigvals}(\mathbf{A}_t)\bigl)$ // First eigenvalue
        \ENDFOR
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Transient Growth Analysis}\label{sec:transient}
In latent action dynamics, stability ensures the local stability of the system. However, even when the spectral radius condition $\rho(\mathbf{A}_t) < 1$ is met—ensuring eigenvalues of $\mathbf{A}_t$ lie within the unit circle—non-normality (i.e., $\mathbf{A}_t\mathbf{A}_t^* \neq \mathbf{A}_t^*\mathbf{A}_t$) can still induce transient growth.
This transient amplification, driven by the properties of $\mathbf{A}_t$, can destabilize intermediate states, amplify noise, or disrupt training. Due to the local nature of our stability analysis, transient growth study becomes important for ascertaining whether a region in state-action phase space may `escape' into a region of unstable action evolution.

For the latent dynamics described in Equation~\ref{eq:Az}, the spectral radius $\rho(\mathbf{A}_t)$ (the largest magnitude of the eigenvalues of $\mathbf{A}_t$) is first computed to confirm the system stability. Second, for cases where $\rho(\mathbf{A}_t) < 1$ but $\mathbf{A}_t$ is non-normal, transient growth is analyzed using the \textit{Kreiss constant} $\eta(\mathbf{A}_t)$, which measures the potential short-term amplification of the system. The Kreiss constant is defined as:
\begin{equation}
\eta(\mathbf{A}_t) = \sup_{|z| > 1} \frac{|z| - 1}{\|(\mathbf{A}_t - zI)^{-1}\|},
\end{equation}
where $z$ is sampled from the complex plane outside the unit circle (i.e., $|z| > 1$). A high $\eta(\mathbf{A}_t)$ suggests a higher likelihood of transient growth. In such a case, one may observe an agent ultimately performing in an unstable manner even though the starting point of a trajectory has real eigenvalues that are less than 1. Algorithm details are provided in Appendix~\ref{appendix:algorithms}.

Overall, the combination of local spectral radius computation and Kreiss constant analysis provides a qualitative understanding of both the long-term stability and the transient behavior of the system, in the absence of global stability analysis. This ensures an indirect assessment of the robustness of the learned control through our proposed framework.


\textbf{Floquet Analysis for Periodic Stability}\label{sec:floquet}
Floquet analysis \cite{klausmeier2008floquet} is a method specifically designed to evaluate the stability of periodic systems and their behavior over time. It quantifies the evolution of small perturbations in the latent space, capturing their growth, decay, or oscillatory behavior through \textit{Floquet exponents}. This complements the spectral radius and Kreiss constant analyses by focusing on periodic stability, where transient growth alone may not capture recurring instabilities or oscillations. This analysis is applicable when the linear term in the latent dynamical system exhibits periodic trends, as seen in tasks requiring repeated oscillatory actions. An example is a pendulum failing to swing upright and instead oscillating at the bottom, where periodic stability is essential for sustained motion. %oscillatory

For periodic systems, where the system dynamics repeat at regular intervals such that $\mathbf{A}_{t+dt} = \mathbf{A}_t$, the state transition matrix $\Phi_t$ describes the evolution of perturbations, starting with $\Phi_0 = \mathbf{I}$ (the identity matrix). Its evolution is governed by the dynamic matrix $\mathbf{A}_t$:
\begin{align}
    \dot{\Phi}_t &= \mathbf{A}_t \Phi_t, \\
    \Phi_{t+1} &= \Phi_t + \Delta t \cdot \dot{\Phi}_t.
\end{align}

Here, $\Delta t$ is a time discretization step, set to 1 for simplicity. At the final time horizon $T$, the state transition matrix $\Phi_T$ encodes system dynamics over one period. The eigenvalues of $\Phi_T$, known as the Floquet multipliers $\lambda_i$, are computed, and the corresponding Floquet exponents $\mu_i$ are derived as:

\begin{align}
    \lambda_i &= \text{eigvals}(\Phi_T), \\
    \mu_i &= \frac{\ln(\lambda_i)}{T \cdot \Delta t}.
\end{align}
These exponents provide the following stability insights: 
$\mathrm{Re}(\mu_i) < 0$ indicates exponential decay (stable dynamics), 
$\mathrm{Re}(\mu_i) > 0$ indicates exponential growth (unstable dynamics), and 
$\mathrm{Im}(\mu_i) \neq 0$ implies oscillatory behavior with a frequency proportional to $\mathrm{Im}(\mu_i)$.

In the \methodName{} framework, $\mathbf{A}_t$ is directly derived from the state $\mathbf{s}_t$, enabling the period to be identified by analyzing state information. Floquet exponents are computed at the end of each episode to diagnose latent dynamics, revealing instability, oscillations, or unstable growth. This analysis identifies regions of instability, oscillatory behavior, or unstable growth, helping refine the control policy.  Appendix~\ref{appendix:algorithms} provides a step-by-step algorithm for this analysis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
A range of classical RL environments are selected from OpenAI Gym \cite{brockman2016openai}, focusing on continuous control tasks such as Cartpole-v1 (modified for continuous action space), Pendulum-v1, LunarLanderContinuous-v2, and BipedalWalker-v3. Additionally, a modified variant of LunarLander is analyzed with a reward function emphasizing hovering over landing to enhance stability and interpretability (details in Appendix~\ref{appendix:lander_reward}).


\subsection{Eigenvalue-based Stability Analysis}
\textbf{Pendulum.} This task involves swinging the pendulum upright with one action dimension. A latent size of $d_h=3$ is used for model training and eigenvalue-based stability analysis. Figure~\ref{fig:pendulum} shows the system's behavior over time (red line) and the eigenvalue distribution (underlying contour).

\begin{figure}[!ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pendulum-state-contour.pdf}}
\caption{Stability analysis of Pendulum control. The white line indicates the critical eigenvalue threshold of 1, while the red line represents observed state trajectory over time. The underlying contour depicts the eigenvalue distribution across potential states at each time step. Overall, the contours illustrate stability conditions over time, while the trajectory highlights how the control policy keeps states within the desired range to maintain stability, avoiding high-eigenvalue unstable regions (bounded by the white line).}
\label{fig:pendulum}
\end{center}
\vskip -0.2in
\end{figure}

In the first column, the trajectory (red line) consistently avoids unstable control actions (yellow regions bounded by white lines) throughout the time domain, ensuring stability. Initially, as the pendulum swings up from the bottom ($\cos \theta, \sin \theta < 0$), it approaches unstable regions very closely but successfully avoids them, reflecting effective training. Over time, the pendulum stabilizes upright ($\cos \theta \to 1$, $\sin \theta \to 0$), with the real eigenvalue converging to 0.478, indicating steady-state behavior in the stable region.
In the second column, the imaginary eigenvalues remain at 0, confirming stability. While states initially approach regions with positive imaginary values, suggesting potential oscillations, they eventually shift away, guiding the system to a steady state. 

Appendix \ref{appendix:results_Pendulum} further examines regions of instability, recovery, and control failures, reinforcing the framework’s stability analysis and interpretability. Overall, this analysis shows the framework policy not only stabilizes the pendulum but does so in a manner that aligns with the primary objective of keeping it upright. By maintaining eigenvalues within the stable region and avoiding oscillatory dynamics, the system ensures stability while providing interpretable insights into how the policy achieves the desired control objectives.  


\textbf{Cartpole.} The Cartpole contains a single action moving left or right to keep the pole upright. With $d_h=3$, the trained model effectively controls the system, avoiding unstable regions, as shown in Figure \ref{fig:cartpole}. 
A key observation in the third row, representing the pole angle state, is that the real eigenvalue analysis identifies bounded regions above and below the central axis. These regions correspond to large angular deviations that not only lead to state instability, making stable actions or states difficult to maintain, but also aligning with the problem's objective: exceeding these thresholds results in episode termination. This analysis interprets that the control policy not only stabilizes the Cartpole but also aligns with the primary goal of keeping the pole upright while avoiding instability-inducing angular thresholds.

\begin{figure}[!ht]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{cartpole-state-contour.pdf}}
\caption{Stability analysis of CartPole control. Three representative states are depicted in the rows. In the third row (pole angle), the trajectory and surrounding bounded regions highlight how the control policy maintains CartPole states within stable limits.}
\label{fig:cartpole}
\end{center}
\vskip -0.2in
\end{figure}


\textbf{LunarLander (Hovering Objective).} For our lunar lander experiment, we utilize a modified environment with a hovering objective instead of an objective to land safely as typically utilized. Since the state includes a 2D space, stability is evaluated across the entire 2D domain, with each frame in Figure~\ref{fig:lander} representing a single timestep. 

\begin{figure*}[!ht]
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{lander.pdf}}
\caption{Stability analysis of LunarLander hovering control. The white line marks the stability threshold at $\lambda_1=1$. The lander is initially deployed in various regions beyond the default upper-middle position (case 1). Each contour (left to right) shows eigenvalue distributions in 2D space, highlighting stability regions, with the trajectory depicting movement over 1,000 timesteps. Notably, case 2 contours provide early warnings of instability before the lander crashes. Animations are provided in the Supplementary.}
\label{fig:lander}
\end{center}
\vskip -0.2in
\end{figure*}

Among the four cases, the real part of the eigenvalue consistently evolves from high-value regions to lower-value regions, crossing the stability threshold (white line) and transitioning from unstable regions to stable ones. This behavior is particularly notable in cases 1, 3, and 4 during later frames or timesteps. The imaginary contour indicates oscillatory regions, but the actual lander trajectory avoids these regions in all cases. In case 2, however, the lander crashes at the final frame ($T=150$). The real eigenvalue contour in this case reveals large regions of instability, aligning with the lander’s inability to maintain its hovering status. Notably, detecting that the current action-space combination resides in a high-instability region effectively “predicts” an impending crash before it happens. This strongly underscores the value of our stability analysis, as it provides early warnings of imminent failures.

Besides the above 3 baseline experiments, Appendix \ref{appendix:results_walker} explores the BipedalWalker benchmark with additional results and discussions, reinforcing the framework and analysis.

\begin{table*}[!ht]
\vskip -0.08in
\centering
\renewcommand{\arraystretch}{1.2}
\caption{Performance comparison of \methodName{} and various baselines. Values represent average episodic rewards over 1,000 episodes using a consistent set of environment seeds across all algorithms. Baseline results are taken from prior research \cite{landajuela2021discovering, rl-zoo3}. The standard LunarLanderContinuous-v2 is used to ensure consistency in comparisons.}
\label{tab:performance}
\vskip 0.03in
\resizebox{\textwidth}{!}
{
\begin{tabular}{l ccccc cccccc}
    \toprule
    \multirow{2}{*}{\textbf{Environment}} & \multicolumn{5}{c}{\textbf{\methodName{}}} & \multirow{2}{*}{\textbf{A2C}} & \multirow{2}{*}{\textbf{SAC}} & \multirow{2}{*}{\textbf{DDPG}} & \multirow{2}{*}{\textbf{TD3}} & \multirow{2}{*}{\textbf{PPO}} & \multirow{2}{*}{\textbf{DSP}}\\
    \cmidrule(lr){2-6} %\cmidrule(lr){7-11}
    & $h_d=3$ & $h_d=4$ & $h_d=6$ & $h_d=8$ & $h_d=16$ & & & & & \\
    \midrule
    Pendulum        & -149.2 & -149.8 & -155.8 & -157.1 & -149.9 & -162.2  & -159.3 & -169.0  & -147.1 & -154.8 & -160.5\\
    CartPole        & 1000.00 & 1000.00 & 1000.00 & 1000.00 & 1000.00 & 1000.00 & 971.78 & 1000.00 & 997.98 & 993.94 & 999.59\\
    LunarLander     & 257.79 & 268.25 & 246.05 & 260.82 & 242.62 & 227.08  & 272.65 & 246.24  & 225.35 & 225.12 & 251.66\\
    BipedalWalker   & -      & 235.11 & 280.38 & 280.91 & 262.57 & 241.01  & 307.26 & 94.21   & 310.19 & 286.20 & 264.39\\
    \bottomrule
\end{tabular}
}
% \vskip -0.05in
\end{table*}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pendulum-floquet-all.pdf}}
\caption{Pendulum control comparisons. The pendulum achieves stabilization in case 1 (first column) while displaying periodic behavior with oscillations in case 2 (second column). The Kreiss constant (third row) shows differing transient growth behaviors over time (regions of instability or normality are not evaluated).}
\label{fig:pendulum_floquet}
\end{center}
\vskip -0.3in
\end{figure}


\subsection{Transient Growth Analysis}  
Extending the eigenvalue-based stability analysis, the Kreiss constant $\eta$ is evaluated to assess transient growth in pendulum under two scenarios: a standard setup (Case 1) and a modified environment (Case 2), where gravity increases from 10 to 15 and mass from 1.0 to 1.1. The third row of Figure~\ref{fig:pendulum_floquet} illustrates $\eta$ over time for both cases.  

In Case 1, the pendulum successfully stabilizes in the upright position. This is reflected in the actions and states, which converge to near-zero values without oscillations (first and second rows). The Kreiss constant remains small throughout, indicating minimal transient growth and robust stability. In Case 2, the increased gravity and mass disrupt the stabilization process, causing the pendulum to exhibit periodic motion instead of reaching the upright position. This behavior is evident in the actions and states, which display sustained oscillations over time. The Kreiss constant shows periodic spikes, reflecting transient growth that aligns with the observed oscillatory response. These spikes indicate significant susceptibility to transient amplification, even though the system remains bounded over time.

This analysis highlights the role of the Kreiss constant in capturing transient growth, particularly under non-normal dynamic conditions. While the spectral radius confirms asymptotic stability, the Kreiss constant reveals critical differences in transient behavior, providing insights into the system’s robustness and susceptibility to perturbations.


\subsection{Floquet Analysis for Periodic Stability}
Building on the two pendulum control cases in Figure~\ref{fig:pendulum_floquet}, Floquet analysis is performed to evaluate periodic stability, providing complementary insights into the stability and oscillatory behavior of the latent dynamics where metrics like eigenvalue analysis or the Kreiss constant may fall short.

In Case 1 (left column), different peak-to-peak analyses all show mixed dynamics. While Floquet exponent $\mu_1$ between $1.0$ and $1.1$ suggests instability, decaying oscillations align with $\mu_2$ between $-1.70$ and $-1.60$, indicating long-term stability. Zero imaginary components confirm transient, not sustained, oscillations. In Case 2 (right column, modified environment), the $\mu_i=\bigl\{0.96,\ 0.02 \pm 0.04i\bigr\}$ reveal instability, with positive real parts driving perturbation growth and imaginary components introducing oscillatory modes. These dynamics correspond to the pendulum’s observed periodic motion, characterized by high-frequency control oscillations, significant angular velocity peaks, and sustained oscillatory state trajectories.

This analysis demonstrates the utility of Floquet exponents in diagnosing periodic instability and oscillatory behavior, offering deeper insight into latent action dynamics. Moreover, it potentially identifies latent space regions where policy refinement may enhance control and mitigate instability.  


\subsection{Ablation Study and Benchmark}
While the goal of this work is not to produce a superior RL algorithm for specific metrics, an ablation analysis is carried out by varying the hidden dimension sizes $h_d$. The results, detailed in Table~\ref{tab:performance}, demonstrate that our approach achieves interpretability while maintaining performance comparable to state-of-the-art DRL methods and symbolic approaches (DSP) \cite{landajuela2021discovering}. Only with a severely restricted dimension ($h_d=3$) does the method struggle with high-complexity BipedalWalker, revealing the limitations of a very low-dimensional latent space for effectively encoding the agent's action dynamics. Additional ablation studies on stability analysis are detailed in Appendix \ref{appendix:ablation_hd}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
This paper introduces \methodName{}, an RL framework for stability analysis in latent action space, leveraging action dynamics and deep learning. The utility of \methodName{} is demonstrated across various environments, showcasing how control policies achieve stability, avoiding unstable regions, and maintaining stable states. Analysis of unstable regions reveals how key state-action combinations influence system stability, offering insights for improved control actions and reward function designs. Additional numerical analyses of periodic stability and transient growth provide a deeper understanding of underlying control outcomes, further interpreting the relationship between system states and stability.


Moreover, \methodName{}'s latent linear dynamics yield coherent action regions in phase space, reflecting consistent actions in state space (see Appendix \ref{appendix:results_structured_action}). Finally, \methodName{} performs competitively with popular DRL methods, while enhancing interpretability. It is therefore a powerful, yet simple tool for understanding the behavior of pretrained agents from various RL algorithms, offering insights into policy behavior, state transitions, and system stability.

\begin{thebibliography}{10}

\bibitem{alam2021physics}
Md~Ferdous Alam, Max Shtein, Kira Barton, and David~J Hoelzle.
\newblock A physics-guided reinforcement learning framework for an autonomous manufacturing system with expensive data.
\newblock In {\em 2021 American Control Conference (ACC)}, pages 484--490. IEEE, 2021.

\bibitem{aastrom2006advanced}
Karl~Johan {\AA}str{\"o}m and Tore H{\"a}gglund.
\newblock {\em Advanced PID control}.
\newblock ISA-The Instrumentation, Systems and Automation Society, 2006.

\bibitem{banerjee2023survey}
Chayan Banerjee, Kien Nguyen, Clinton Fookes, and Maziar Raissi.
\newblock A survey on physics informed reinforcement learning: Review and open problems.
\newblock {\em arXiv preprint arXiv:2309.01909}, 2023.

\bibitem{biehl2016prototype}
Michael Biehl, Barbara Hammer, and Thomas Villmann.
\newblock Prototype-based models in machine learning.
\newblock {\em Wiley Interdisciplinary Reviews: Cognitive Science}, 7(2):92--111, 2016.

\bibitem{brockman2016openai}
G~Brockman.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{chen2021interpretable}
Jianyu Chen, Shengbo~Eben Li, and Masayoshi Tomizuka.
\newblock Interpretable end-to-end urban autonomous driving with latent deep reinforcement learning.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems}, 23(6):5068--5078, 2021.

\bibitem{cho2019physics}
Youngwoo Cho, Sookyung Kim, Peggy~Pk Li, Mike~P Surh, T~Yong-Jin Han, and Jaegul Choo.
\newblock Physics-guided reinforcement learning for 3d molecular structures.
\newblock In {\em Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem{delfosse2024interpretable}
Quentin Delfosse, Hikaru Shindo, Devendra Dhami, and Kristian Kersting.
\newblock Interpretable and explainable logical policies via neurally guided symbolic abstraction.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{duan2016benchmarking}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International conference on machine learning}, pages 1329--1338. PMLR, 2016.

\bibitem{florensa2017stochastic}
Carlos Florensa, Yan Duan, and Pieter Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock {\em arXiv preprint arXiv:1704.03012}, 2017.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages 1587--1596. PMLR, 2018.

\bibitem{garnier2021review}
Paul Garnier, Jonathan Viquerat, Jean Rabault, Aur{\'e}lien Larcher, Alexander Kuhnle, and Elie Hachem.
\newblock A review on deep reinforcement learning for fluid mechanics.
\newblock {\em Computers \& Fluids}, 225:104973, 2021.

\bibitem{glanois2024survey}
Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu.
\newblock A survey on interpretable reinforcement learning.
\newblock {\em Machine Learning}, pages 1--44, 2024.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages 1861--1870. PMLR, 2018.

\bibitem{hein2018interpretable}
Daniel Hein, Steffen Udluft, and Thomas~A Runkler.
\newblock Interpretable policies for reinforcement learning by genetic programming.
\newblock {\em Engineering Applications of Artificial Intelligence}, 76:158--169, 2018.

\bibitem{hernandez2023port}
Quercus Hern{\'a}ndez, Alberto Bad{\'\i}as, Francisco Chinesta, and El{\'\i}as Cueto.
\newblock Port-metriplectic neural networks: thermodynamics-informed machine learning of complex physical systems.
\newblock {\em Computational Mechanics}, 72(3):553--561, 2023.

\bibitem{heuillet2021explainability}
Alexandre Heuillet, Fabien Couthouis, and Natalia D{\'\i}az-Rodr{\'\i}guez.
\newblock Explainability in deep reinforcement learning.
\newblock {\em Knowledge-Based Systems}, 214:106685, 2021.

\bibitem{jiang2019neural}
Zhengyao Jiang and Shan Luo.
\newblock Neural logic reinforcement learning.
\newblock In {\em International conference on machine learning}, pages 3110--3119. PMLR, 2019.

\bibitem{jin2022creativity}
Mu~Jin, Zhihao Ma, Kebing Jin, Hankz~Hankui Zhuo, Chen Chen, and Chao Yu.
\newblock Creativity of ai: Automatic symbolic option discovery for facilitating deep reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 7042--7050, 2022.

\bibitem{jurj2021increasing}
Sorin~Liviu Jurj, Dominik Grundt, Tino Werner, Philipp Borchers, Karina Rothemann, and Eike M{\"o}hlmann.
\newblock Increasing the safety of adaptive cruise control using physics-guided reinforcement learning.
\newblock {\em Energies}, 14(22):7572, 2021.

\bibitem{kalashnikov2018scalable}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic manipulation.
\newblock In {\em Conference on robot learning}, pages 651--673. PMLR, 2018.

\bibitem{kenny2023towards}
Eoin~M Kenny, Mycal Tucker, and Julie Shah.
\newblock Towards interpretable deep reinforcement learning with human-friendly prototypes.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{khaled2022framework}
Mahmoud Khaled, Kuize Zhang, and Majid Zamani.
\newblock A framework for output-feedback symbolic control.
\newblock {\em IEEE Transactions on Automatic Control}, 68(9):5600--5607, 2022.

\bibitem{kingma2013auto}
Diederik~P Kingma.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{klausmeier2008floquet}
Christopher~A Klausmeier.
\newblock Floquet theory: a useful tool for understanding nonequilibrium dynamics.
\newblock {\em Theoretical Ecology}, 1:153--161, 2008.

\bibitem{landajuela2021discovering}
Mikel Landajuela, Brenden~K Petersen, Sookyung Kim, Claudio~P Santiago, Ruben Glatt, Nathan Mundhenk, Jacob~F Pettit, and Daniel Faissol.
\newblock Discovering symbolic policies with deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 5979--5989. PMLR, 2021.

\bibitem{lillicrap2015continuous}
TP~Lillicrap.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{liu2021physics}
Xin-Yang Liu and Jian-Xun Wang.
\newblock Physics-informed dyna-style model-based deep reinforcement learning for dynamic control.
\newblock {\em Proceedings of the Royal Society A}, 477(2255):20210618, 2021.

\bibitem{lyu2019sdrl}
Daoming Lyu, Fangkai Yang, Bo~Liu, and Steven Gustafson.
\newblock Sdrl: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 2970--2977, 2019.

\bibitem{mott2019towards}
Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez~Rezende.
\newblock Towards interpretable reinforcement learning using attention augmented agents.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{nachum2018data}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{nousiainen2021adaptive}
Jalo Nousiainen, Chang Rajani, Markus Kasper, and Tapio Helin.
\newblock Adaptive optics control using model-based reinforcement learning.
\newblock {\em Optics Express}, 29(10):15327--15344, 2021.

\bibitem{pateria2021hierarchical}
Shubham Pateria, Budhitama Subagdja, Ah-hwee Tan, and Chai Quek.
\newblock Hierarchical reinforcement learning: A comprehensive survey.
\newblock {\em ACM Computing Surveys (CSUR)}, 54(5):1--35, 2021.

\bibitem{rabault2019artificial}
Jean Rabault, Miroslav Kuchta, Atle Jensen, Ulysse R{\'e}glade, and Nicolas Cerardi.
\newblock Artificial neural networks trained through deep reinforcement learning discover control strategies for active flow control.
\newblock {\em Journal of fluid mechanics}, 865:281--302, 2019.

\bibitem{rl-zoo3}
Antonin Raffin.
\newblock Rl baselines3 zoo.
\newblock \url{https://github.com/DLR-RM/rl-baselines3-zoo}, 2020.

\bibitem{reissig2018symbolic}
Gunther Reissig and Matthias Rungger.
\newblock Symbolic optimal control.
\newblock {\em IEEE Transactions on Automatic Control}, 64(6):2224--2239, 2018.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{skogestad2005multivariable}
Sigurd Skogestad and Ian Postlethwaite.
\newblock {\em Multivariable feedback control: analysis and design}.
\newblock john Wiley \& sons, 2005.

\bibitem{sutton2018reinforcement}
Richard~S Sutton.
\newblock Reinforcement learning: An introduction.
\newblock {\em A Bradford Book}, 2018.

\bibitem{swarnkar2014adaptive}
Pankaj Swarnkar, Shailendra~Kumar Jain, and Rajesh~Kumar Nema.
\newblock Adaptive control schemes for improving the control system dynamics: a review.
\newblock {\em IETE Technical Review}, 31(1):17--33, 2014.

\bibitem{verma2018programmatically}
Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri.
\newblock Programmatically interpretable reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 5045--5054. PMLR, 2018.

\bibitem{wang2022toward}
Ruihang Wang, Xinyi Zhang, Xin Zhou, Yonggang Wen, and Rui Tan.
\newblock Toward physics-guided safe deep reinforcement learning for green data center cooling control.
\newblock In {\em 2022 ACM/IEEE 13th International Conference on Cyber-Physical Systems (ICCPS)}, pages 159--169. IEEE, 2022.

\bibitem{weiner2024model}
Andre Weiner and Janis Geise.
\newblock Model-based deep reinforcement learning for accelerated learning from flow simulations.
\newblock {\em Meccanica}, pages 1--18, 2024.

\bibitem{wells2021explainable}
Lindsay Wells and Tomasz Bednarz.
\newblock Explainable ai and reinforcement learning—a systematic review of current approaches and trends.
\newblock {\em Frontiers in artificial intelligence}, 4:550030, 2021.

\bibitem{xiong2023interpretable}
Luolin Xiong, Yang Tang, Chensheng Liu, Shuai Mao, Ke~Meng, Zhaoyang Dong, and Feng Qian.
\newblock Interpretable deep reinforcement learning for optimizing heterogeneous energy storage systems.
\newblock {\em IEEE Transactions on Circuits and Systems I: Regular Papers}, 2023.

\bibitem{yarats2021reinforcement}
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto.
\newblock Reinforcement learning with prototypical representations.
\newblock In {\em International Conference on Machine Learning}, pages 11920--11931. PMLR, 2021.

\bibitem{yousif2023physics}
Mustafa~Z Yousif, Meng Zhang, Yifan Yang, Haifeng Zhou, Linqi Yu, and HeeChang Lim.
\newblock Physics-guided deep reinforcement learning for flow field denoising.
\newblock {\em arXiv preprint arXiv:2302.09559}, 2023.

\bibitem{yu2023reinforcement}
Chao Yu, Xuejing Zheng, Hankz~Hankui Zhuo, Hai Wan, and Weilin Luo.
\newblock Reinforcement learning with knowledge representation and reasoning: A brief survey.
\newblock {\em arXiv preprint arXiv:2304.12090}, 2023.

\bibitem{zabounidis2023concept}
Renos Zabounidis, Joseph Campbell, Simon Stepputtis, Dana Hughes, and Katia~P Sycara.
\newblock Concept learning for interpretable multi-agent reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1828--1837. PMLR, 2023.

\bibitem{zolman2024sindy}
Nicholas Zolman, Urban Fasel, J~Nathan Kutz, and Steven~L Brunton.
\newblock Sindy-rl: Interpretable and efficient model-based reinforcement learning.
\newblock {\em arXiv preprint arXiv:2403.09110}, 2024.

\end{thebibliography}



% APPENDIX
\newpage
\appendix
\onecolumn
\section{Derivation of the Policy Gradient for \methodName{}} \label{appendix:derivation}

\subsection{Policy Formulation}
A latent dynamic control policy is defined based on Eqs.~\eqref{eq:encoder}, \eqref{eq:Az}, \eqref{eq:fs}, and \eqref{eq:decoder}. Substituting the first three equations into the fourth yields a deterministic mapping for generating the next action $a_{t+1}$  from the current state-action pair $(s_t, a_t)$:
\begin{equation}
\label{eq:mu_def}
\begin{split}
a_{t} &= \pi_\theta(s_t, a_{t-1}) \\
      &= \text{Dec}(z_{t}) \\
      &= \text{Dec}(z_{t-1}\;+\;\mathbf{A}_t z_{t-1}) \\
      &= \text{Dec}\bigl(\text{Enc}(a_{t-1}) \;+\;
        \mathcal{N}_{\theta}(s_t)\text{Enc}(a_{t-1})\bigr).
\end{split}
\end{equation}

Here, $\pi_\theta(\cdot)$ is the policy, parameterized by $\theta$, which takes the current state $s_t$ and previous action $a_t$ as input. $z_t$ represents the latent action dynamics at time $t$. $\mathcal{N}$ denotes the latent dynamics network module that takes state $s_t$ as input and outputs the time-dependent matrix $\mathbf{A}_t$. 


\subsection{Policy Gradient}
Using the deterministic policy gradient (DPG), we derive the gradient for the deterministic policy $\pi_\theta$, where the objective is to maximize the expected return (cumulative reward), denoted by $J(\pi_\theta)$. The gradient of this objective with respect to the neural network parameters $\theta$ can be written as:
\begin{equation}
\label{eq:dpg_objective}
    \nabla_\theta J(\pi_\theta)
    \;=\;
    \mathbb{E}_{\tau \sim \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t, \mathbf{a}_{t-1})}
    \Bigl[
        \nabla_\theta \pi_\theta(s_t, a_{t-1})
        \;\nabla_a Q^\pi(s_t,a)\big|_{\,a=\pi_\theta(s_t,a_{t-1})}
    \Bigr],
\end{equation}
where $\pi_\theta$ represents the policy parameterized by $\theta$, and $Q$ is the action-value function, which provides the expected cumulative reward starting from state $s_t$ and taking action $a_t$.

To apply Eq.~\eqref{eq:dpg_objective} to our latent dynamic control framework, we need \(\nabla_\theta \pi_\theta(s_t, a_{t-1})\). 

Let us define an intermediate latent transformation as:
\begin{equation}
\label{eq:z_forward}
    h_\theta(s_t, a_{t-1})
    \;=\;
    z_{t}
    \;=\;
    \text{Enc}(a_{t-1})
    \;+\;
    \mathcal{N}_{\theta}\bigl(s_t\bigr)\text{Enc}\bigl(a_{t-1}\bigr).
\end{equation}

Then
\begin{equation}
\label{eq:decoder_call}
    \pi_\theta(s_t,a_{t-1})
    \;=\;
    \text{Dec}(z_t)
    \;=\;
    \text{Dec}\bigl(\,h_\theta(s_t,a_{t-1})\bigr).
\end{equation}

\noindent \textbf{Chain Rule.} From Eq.~\eqref{eq:dpg_objective}, by the chain rule,
\begin{align}
\label{eq:chain_rule}
    \nabla_\theta \pi_\theta(s_t,a_{t-1})
    &=\;
    \frac{\partial\;\text{Dec}(z)}{\partial z}\Bigg|_{z = h_\theta(s_t,a_{t-1})}
    \;\times\;
    \nabla_\theta \,h_\theta(s_t,a_{t-1}) .
\end{align}

\noindent \textbf{Term 1}: $\partial\,\text{Dec}(z)/\partial z$. For Decoder that is fixed/pre-trained, this Jacobian is a constant.

\noindent \textbf{Term 2}: $\nabla_\theta \,h_\theta(s_t,a_t)$. From Eq.~\eqref{eq:z_forward},
\begin{align}
    \nabla_\theta \,h_\theta(s_t,a_{t-1})
    &=\;
    \nabla_\theta \Bigl[\,
        \text{Enc}(a_{t-1})
        \;+\;
        \mathcal{N}_{\theta}\bigl(s_t\bigr)\text{Enc}\bigl(a_{t-1}\bigr).
    \Bigr]. \notag
\end{align}
Assuming the Encoder is fixed, the only dependence on $\theta$ is through $\mathcal{N}_\theta(s_t)$. Hence,
\begin{equation}
    \nabla_\theta \,h_\theta(s_t, a_{t-1})
    \;=\;
    \text{Enc}(a_{t-1})
    \;\nabla_\theta \mathcal{N}_\theta(s_t).
\end{equation}

Putting it all together into Eq.~\eqref{eq:chain_rule}, we obtain:
\begin{equation}
    \nabla_\theta \pi_\theta(s_t, a_{t-1})
    \;=\;
    \frac{\partial\;\text{Dec}(z)}{\partial z}\Bigg|_{z = h_\theta(s_t, a_{t-1})}
    \;\times\; \text{Enc}(a_{t-1}) \;\nabla_\theta \mathcal{N}_\theta(s_t).
\end{equation}

Substituting back into Eq.~\eqref{eq:dpg_objective}, the gradient of the objective becomes:
\begin{align}
    \nabla_\theta J(\pi_\theta)
    &=
    \mathbb{E}_{\tau \sim \pi_\theta(\mathbf{a}_t \mid \mathbf{s}_t, \mathbf{a}_{t-1})}\Bigl[
        \Bigl(
            \frac{\partial\,\text{Dec}(z)}{\partial z}\Big|_{z=h_\theta(s_t, a_{t-1})}
            \;\times\;
            \text{Enc}(a_{t-1})
            \;\nabla_\theta \mathcal{N}_\theta(s_t)
        \Bigr)
        \;\nabla_a Q^\pi(s_t,a)\big|_{a=\pi_\theta(s_t, a_{t-1})}
    \Bigr].
\end{align}
This expression provides the standard DPG update rule for the latent dynamic control framework, where the policy
$\pi_\theta(s_t, a_{t-1})$ is derived via the combination of encoder, latent dynamics network $\mathcal{N}_\theta$, and decoder. 

For the stochastic policy, a similar derivation can be performed using the Stochastic Policy Gradient (SPG) theorem, where the gradient involves $\nabla_\theta \log \pi_\theta(a_t \mid s_t,a_{t-1})$ weighted by the action-value function $Q^\pi(s_t, a_{t-1})$.



\section{Algorithms for Transient Growth and Floquet Analysis} \label{appendix:algorithms}
This section details additional algorithms referenced in the method section of the main article. 

\textbf{Transient Growth.}
For transient growth analysis in Section \ref{sec:transient}, we describe the calculation of the Kreiss constant. Regarding the choice of \( z \) sampling, which produces the results in Figure~\ref{fig:pendulum_floquet}, the upper bound is set to 1.5 for the first pendulum case and 8 for the second. 
\begin{algorithm}[ht]
   \caption{Transient Growth Evaluation}
   \label{alg:kreiss_analysis}
\begin{algorithmic}
\STATE {\bfseries Input:} Observed states $\mathbf{s}_t$ over time, error threshold $\epsilon$
\FOR{$t = 0$ {\bfseries to} $T$}
    \STATE $\mathbf{A}_t \gets \mathcal{N}_\theta(\mathbf{s}_t)$
    \STATE $\lambda_1 \gets \text{max}\bigl(\text{eigvals}(\mathbf{A}_t)\bigl)$
    \IF{$\lambda_1 < 1$} 
        \STATE $\text{normality\_diff} \gets \|\mathbf{A}_t \cdot \mathbf{A}_t^\top - \mathbf{A}_t^\top \cdot \mathbf{A}_t\|$
        \IF{$\text{normality\_diff} > \epsilon$} 
            \STATE $\eta(\mathbf{A}_t) \gets \sup_{|z| > 1} \frac{|z| - 1}{\|(\mathbf{A}_t - zI)^{-1}\|}$ // Kreiss constant
        \ENDIF
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}


\textbf{Floquet Analysis.}
For Floquet Analysis in Section \ref{sec:floquet}, the algorithm is summarized below:
\begin{algorithm}[ht]
   \caption{Floquet Analysis for Latent Dynamics}
   \label{alg:floquet_analysis}
\begin{algorithmic}
   \STATE {\bfseries Input:} Observed states $\mathbf{s}_t$ over time, state transition matrix $\Phi \gets \mathbf{I}$, identified peak-peak timesteps $t_1$ and $t_2$
   \FOR{$t = 0$ {\bfseries to} $t_2$}
      \STATE $\mathbf{A}_t \gets \mathcal{N}_\theta(\mathbf{s}_t)$
      \IF {$t \ge t_1$}
          \STATE $\Phi \gets \Phi + \mathbf{A}_t \Phi$ // Update state transition matrix
      \ENDIF
   \ENDFOR
   \STATE $\lambda_i \gets \text{eigvals}(\Phi_T)$ // Spectral analysis
   \STATE $\mu_i \gets \frac{\ln(\lambda_i)}{T}$ // Compute Floquet exponents 
\end{algorithmic}
\end{algorithm}



\section{Encoder-Decoder Training and Discussion}\label{appendix:encoder-decoder}
The encoder and decoder each contain three layers with identical middle layer sizes. The encoder maps the action dimension to the hidden dimension $h_d$, while the decoder performs the reverse operation. Notably, we use a slightly larger number of neurons in the decoder, a common practice to enhance decoding performance and ensure a more effective reconstruction mechanism \cite{kingma2013auto}.

For training, we generate action datasets from trained DLR policies, improving data efficiency compared to uniform sampling, particularly in high-dimensional settings. Training was conducted for 500 epochs with a scheduled learning rate decrease, and the final model achieved a mean squared error (MSE) of approximately $10^{-6}$ to $10^{-7}$ across different $h_d$. 

In the BipedalWalker problem, which has four action dimensions, encoding and decoding with a reduced hidden dimension of $h_d=3$ significantly increased the difficulty. Even with a properly scaled network (more neurons and layers), the training MSE loss was three orders of magnitude higher than in other cases. Since precise continuous control is crucial for tasks like BipedalWalker, we believe this partially contributed to the failure of \methodName{} to achieve a successful control policy at $h_d=3$ and relatively lower performance at $h_d=4$ (see Table \ref{tab:performance}). However, this should not be viewed as a limitation of \methodName{}, as achieving a compact network size is not our primary objective. In contrast, our stability analysis and interpretability remain consistent across different hidden dimensions, as shown in Appendix \ref{appendix:ablation_hd}.



\section{Modified LunarLander Environment for Hovering\label{appendix:lander_reward}}
In the standard LunarLanderContinuous-v2 environment, the reward function encourages efficient and accurate landings. The reward is shaped to penalize excessive fuel consumption, provide bonuses for leg contact with the ground, and reward proximity to the flat landing area. Additionally, a large bonus (+100) is given for a successful landing, while a penalty (-100) is applied for crashes or going out of bounds. This design promotes controlled and fuel-efficient landings by balancing penalties and rewards.

In the modified environment, the focus shifts from landing to hovering consistently. The bonuses for leg contacts and successful landing are removed, and a large negative penalty of -300 is applied for crashes, going out of bounds, or touching the ground. A shaping term adjusts the vertical coordinate by 0.2 to encourage hovering slightly above the surface: $\text{shaping} = -100 \sqrt{x^2 + (y - 0.2)^2}.$ A survival bonus of +0.2 per timestep is introduced to reward prolonged hovering, while penalties for fuel consumption remain unchanged to minimize unnecessary actions. These modifications prevent premature convergence to local reward maxima dominated by landing or crashing. Additionally, the larger crash penalty strongly discourages failures, promoting stability and long-term control instead.  



\section{Extended Stability Analysis and Interpretability on Pendulum Problem} \label{appendix:results_Pendulum}
In the main article, the analysis of pendulum control highlighted regions of stability along with the control actions over time. in Figure \ref{fig:appendix_pendulum}, we examine extreme scenarios involving the absence of actions as potential control failures, aiming to showcase extended stability analysis during these failure processes.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{appendix-pendulum.pdf}}
\caption{A custom control scenario where the system remains action-free during the first 30 and last 50 timesteps. The first column shows the evolution of action, state, and corresponding eigenvalues, while the second and third columns present an extended stability analysis through contour visualizations.}
\label{fig:appendix_pendulum}
\end{center}
\vskip -0.2in
\end{figure}

We randomly initialized the environment and restricted actions from timestep 30 to 150. This setup enables the analysis of three typical time regions:
\begin{enumerate}
    \item \textbf{Region of Instability (T: 0--30):} Initial instability before any actions are applied.
    \item \textbf{Region of Recovery (T: 30--150):} Control actions are applied to recover the system from instability to stability.
    \item \textbf{Region of Control Failure (T: 150--200):} The control is removed, and the system transitions back from stability to instability.
\end{enumerate}

From the eigenvalue evolution plot (first column, third row), regions of instability ($\lambda_1>1$) are evident in time Regions 1 and 3, as well as at the initial stage of Region 2. In Region 2, the policy successfully recovers the system from instability to stability, aligning with the above-defined phases and the pendulum control objective. In the second and third columns, contour plots further validate this observation. Contour regions of instability or oscillation near the trajectory are visibly highlighted with yellow or red colors, reinforcing our stability analysis and definition. Overall, this analysis significantly enhances our understanding of control failures, showcasing the interpretability and robustness of the proposed framework.



\section{Stability Analysis for the BipedalWalker Control}\label{appendix:results_walker}
The Bipedal Walker problem involves numerous states and four actions controlling the legs and joints, aiming to achieve stable, energy-efficient forward locomotion across uneven terrain without falling. Similar to other environments analyzed in the main article, we present the stability analysis for two different cases, as shown in Figure \ref{fig:walker}. Due to the large number of states, we illustrate only four representative states associated with the walker's hull.

\begin{figure}[ht]
\centering
\includegraphics[width=0.49\textwidth]{walker-state-contour-Hull.pdf}
\hfill
\includegraphics[width=0.49\textwidth]{walker-state-contour-Hull2.pdf}
\caption{Stability analysis of the Bipedal Walker control for two cases (left two columns and right two columns). Representative states associated with the walker's hull are shown. Both cases exhibit similar control patterns and stability. The observed states (red lines) indicate that the hull maintains a positive angle, while angular velocity and vertical velocity (second row) oscillate around 0. The horizontal velocity (third row) remains positive, enabling forward movement.
These observed states closely align with the stability contours. The real part of the eigenvalues stays mostly below 1, indicating stability, while the first three plots illustrate how the control policy stabilizes the states within potential instability regions (white-line regions). The imaginary part, with magnitudes greater than 1, reflects oscillatory dynamics in the overall states.} \label{fig:walker}
\vskip -0.2in
\end{figure}


In both cases, similar stability patterns emerge over time. The hull primarily maintains stability through oscillations, meaning the real part of the eigenvalues generally remains below 1, indicating stability, while the imaginary part remains consistently nonzero, reflecting oscillatory dynamics. This corresponds to the walker's steady rightward movement, with occasional hull oscillations (moving up and down) as the legs alternate between stance and swing phases.

Notably, in the second row (angular velocity contour), regions around the state trajectory indicate high values (instability). This corresponds to the walker, where high angular velocity (rotational speed) of the hull signifies instability. In the third row (horizontal velocity contour), negative hull velocity represents instability, as the walker should move to the right rather than the left, aligning with the environment’s objective. In contrast, in the first row (angle of the hull), variations within a certain range are acceptable for maintaining balance. While in the fourth row (vertical velocity contour), values remain small and stable, reflecting consistent vertical motion. Other states related to the legs, which exhibit constant movement and consistently show regions of instability, are excluded from the analysis as they are not of primary interest.



\section{Phase-Space Visualizations of \methodName{}}\label{appendix:results_structured_action}
For the pendulum problem, where the state space is summarized by two components--the angle $\theta$ and angular velocity $\dot{\theta}$, rather than $\theta$, $\cos\theta$, and $\sin\theta$--we analyze the phase-space behavior of the learned policies. By interacting the policy with the environment, we visualize the structure of actions assigned to different states.
Figure~\ref{fig:appendix_pendulum2} compares the baseline SAC \cite{rl-zoo3} with the proposed \methodName{}, both sharing similar architectures and training strategies. On the right, the SAC policy results in scattered action assignments across phase space, reflecting its probabilistic nature. In contrast, on the left, \methodName{} exhibits larger, more coherent action regions, indicating more consistent action choices for given states. This suggests that \methodName{} encodes a structured latent representation, leading to more predictable decision-making in phase space.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{appendix-action-space.pdf}}
\caption{Action space of the proposed \methodName{} (left) compared to the SAC baseline (right). The sharp transitions in \methodName{}'s contours suggest a more structured action space, influenced by its latent dynamic evolution, while SAC's smooth transitions reflect its probabilistic nature due to its training strategy.}  
\label{fig:appendix_pendulum2}
\end{center}
\vskip -0.2in
\end{figure}

To assess generalizability and robustness, we tested both policies in unseen pendulum environments by modifying internal physical parameters, such as gravity and rod mass. Compared to SAC, \methodName{} maintained control with up to a 40\% mass increase or a 20\% gravity increase, while SAC failed to keep the pendulum upright in all variations. This suggests that \methodName{}'s structured latent dynamics contribute to improved stability and performance across diverse conditions.  



\section{Ablation Study on Stability Analysis Across Different Hidden Dimensions}\label{appendix:ablation_hd}
In Figure \ref{fig:pendulum_stability_hd}, we extensively show the effect of different hidden dimensions contributing to the stability analysis. In the upper row with lower $h_d$, the contour patterns are highly similar within the real component plots and within the imaginary component plots. This consistency aligns with the main article and is particularly evident in the initial stage, where the control policy seeks stability while crossing regions of instability.

In the lower row with higher $h_d$, the evolution of the real part exhibits different values. However, the overall pattern remains consistent across different $h_d$ configurations, as the trajectory initially passes through yellow-colored regions. Notably, at $h_d=8$, the system exhibits uniform stability across the entire domain, whereas at $h_d=16$, regions of instability emerge. It is worth noting that in the higher $h_d$ cases, even after the pendulum stabilizes (beyond 50 timesteps), the imaginary component reveals regions of high values, indicating oscillatory behavior. However, these regions remain distant from the actual state of the pendulum, and the increased values are likely due to the higher-dimensional neural network introducing greater nonlinearity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{stability-contour-hidden-3.pdf}
    \hfill
    \includegraphics[width=0.49\textwidth]{stability-contour-hidden-4.pdf}
    \vskip 0.2in
    \includegraphics[width=0.49\textwidth]{stability-contour-hidden-8.pdf}
    \hfill
    \includegraphics[width=0.49\textwidth]{stability-contour-hidden-16.pdf}
    \caption{Stability contour plots for different hidden dimensions: (top-left) $h_d=3$, (top-right) $h_d=4$, (bottom-left) $h_d=8$, and (bottom-right) $h_d=16$. Despite some discrepancies in numerical values—such as the real part in $h_d=8$ showing uniform stability—similar patterns and overall behaviors hold across different configurations. This consistency reinforces the robustness of our stability analysis, demonstrating that key trends persist regardless of the hidden dimension size.}
    \label{fig:pendulum_stability_hd}
\vskip -0.2in
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
