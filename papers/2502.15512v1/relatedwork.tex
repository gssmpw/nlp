\section{Related Work}
\subsection{Classical Controls} 
Classical approaches for dynamical system controls include Proportional-Integral-Derivative (PID), Linear Quadratic Regulators (LQRs), and adaptive control techniques\cite{aastrom2006advanced, swarnkar2014adaptive}. These methods offer clear advantages in interpretability and stability analysis by providing explicit symbolic expressions for the relationship between states and control actions, enabling stability analysis and control law verification. However, they often struggle with high-dimensional, nonlinear, or partially observable systems where dynamics cannot be explicitly modeled \cite{skogestad2005multivariable}. 


\subsection{Interpretability in DRL} 
Interpretability for machine learning algorithms has gained significant attention in recent years, particularly in healthcare, robotics, and autonomous systems \cite{glanois2024survey, yu2023reinforcement, wells2021explainable, heuillet2021explainability}. Recent research devoted to interpretable RL has explored diverse approaches such as multi-agent systems \cite{zabounidis2023concept}, interpretable latent representations \cite{chen2021interpretable}, and techniques like attention mechanisms \cite{mott2019towards} or genetic programming \cite{hein2018interpretable}, balancing transparency, performance, and generalizability. Below, we outline a few key directions.

\textbf{Hierarchical RL.} This approach focuses on planning and reasoning by structuring tasks into high-level (manager) and low-level (worker) policies \cite{lyu2019sdrl, nachum2018data, pateria2021hierarchical}. This modular approach excels in task decomposition, reusable subtasks, and explainable goal-setting. However, it is less suitable for specific use cases like dynamical system control, where precise and responsive low-level policies are essential \cite{florensa2017stochastic}.

\textbf{Prototype-based RL.} 
Prototype-based methods leverage human-defined prototypes to represent states and actions as interpretable latent features, enabling policies that balance interpretability and performance \cite{kenny2023towards, xiong2023interpretable, yarats2021reinforcement, biehl2016prototype}. However, their reliance on manual design and lack of temporal precision limit their adaptability to dynamic environments and their ability to handle continuous, high-dimensional dynamics effectively \cite{duan2016benchmarking}.


\subsection{RL for Dynamical System Controls}
DRL is well-studied in physical applications, such as optimizing flow control and turbulent fluid dynamics, where domain knowledge is often available, and state-control actions exhibit stronger correlations compared to other domains\cite{yousif2023physics, weiner2024model, garnier2021review, rabault2019artificial}. Recent approaches have further improved interpretability by leveraging state-action correlations and domain knowledge.

\textbf{Symbolic and Neural-Guided Approaches} 
Neural-guided methods like NUDGE \cite{delfosse2024interpretable}, PIRL \cite{verma2018programmatically}, and NLRL \cite{jiang2019neural} use symbolic reasoning to discover interpretable policies \cite{jin2022creativity}. Symbolic controllers \cite{khaled2022framework, reissig2018symbolic}, such as DSP \cite{landajuela2021discovering} and SINDy-RL \cite{zolman2024sindy}, employ explicit state-control mappings to derive generalizable control laws.
These methods improve interpretability and generalizability, making them suitable for safe, explainable decision-making. However, neural-guided approaches rely on logical representations \cite{delfosse2024interpretable}, limiting temporal precision and scalability in complex systems. Similarly, symbolic controllers face scalability issues in multi-dimensional or stochastic tasks where predefined forms fail to capture intricate interactions \cite{landajuela2021discovering}. While explicit formulations aid stability analysis, they may overlook subtle instabilities in evolving or poorly understood dynamics.

\textbf{Physics-guided RL}
Physics-guided RL \cite{liu2021physics, banerjee2023survey, alam2021physics, jurj2021increasing, cho2019physics, wang2022toward} integrates domain knowledge and physical principles to enhance learning and performance. Approaches include informed reward functions, model-based RL with physics-based or neural network surrogate models \cite{hernandez2023port, nousiainen2021adaptive}, and state design \cite{banerjee2023survey}. While incorporating domain knowledge can improve interpretability, these methods \cite{alam2021physics, jurj2021increasing, cho2019physics, wang2022toward} often lack generalizability to other problems and require significant fine-tuning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%