\section{Related works}
\paragraph{Discrete diffusion models.}
(Continuous) diffusion models were originally built on continuous-space Markov chains with Gaussian transition kernels ____. This was later extended to continuous time through the theory of stochastic differential equations ____. In a similar vein, discrete diffusion models have emerged from discrete-space Markov chains ____. Specifically, ____ introduced D3PM with various types of transition matrices. Later, ____ proposed SEDD, incorporating a theoretically and practically robust score-entropy objective. Additionally, ____ introduced novel modeling strategies that classify tokens in a noisy sequence as either signal (coming from clean data) or noise (arising from the forward process). In particular, ____ uses this to give a \emph{planner} that adaptively determines which tokens to denoise. While this is similar in spirit to our general discussion about devising adaptive inference strategies, we emphasize that their approach is specific to discrete diffusions for which the forward process \emph{scrambles} the token values, rather than masking them.

\paragraph{Masked diffusion models.} Meanwhile, the absorbing transition kernel has gained popularity as a common choice due to its better performance than other kernels. Building on this, ____ aligned its framework with continuous diffusion, resulting in a simple and principled training recipe, referring to it as \emph{Masked Diffusion Model}. Subsequent studies have explored various aspects of MDM. ____ efficiently trained MDM via adaptation from autoregressive models, scaling MDM up to 7B parameters. ____ interpreted 
MDMs as order-agnostic learners and proposed a first-hitting sampler based on this insight. ____ demonstrated that MDM outperforms autoregressive models in reasoning and planning tasks, emphasizing its impact on downstream applications. ____ examined the scaling laws of MDM, while ____ identified limitations in capturing coordinate dependencies when the number of sampling steps is small and proposed additional modeling strategies to address this issue. ____ studied conditional generation using MDM and ____ tackled the challenge of controlling generated data distributions through steering methodologies. ____ provided a theoretical analysis showing that sampling error is small given accurate score function estimation.


\paragraph{Any-order reasoning.} Even though language tasks generally have a natural order of ``left-to-right" token generation, in many tasks like planning, reasoning, and combinatorial optimization, the natural order of token generation can be quite different from ``left-to-right". Even though prominent autoregressive-based language models achieve impressive performance on various tasks, many works ____ have shown that this performance is tied to the training order of the tasks and therefore can cause brittleness from it. For example, ____ showed that simply permuting the premise order on math tasks causes a performance drop of 30\%. The reason behind such brittleness regarding the ordering is the inherent ``left-to-right" nature of the autoregressive models. Several works ____ have tried to address this issue in the autoregressive framework. In particular, ____ highlighted the significance of left-to-right ordering in natural language by comparing its likelihood to that of the reverse (right-to-left) ordering.

Recently, discrete diffusion models have emerged as a promising approach for discrete data apart from autoregressive models. Additionally, the order-agnostic training of discrete diffusion models opens up the multiple sampling paths during the inference but it also faces some challenges during the training therefore, they seem a promising approach to elicit any order reasoning. ____ proposed different ways of implementing an adaptive inference strategy for MDM but a \emph{concrete understanding of why such an adaptive inference strategy is needed is still lacking}. In this work, we explore various aspects of vanilla MDM training and how adaptive MDM inference can mitigate the issues raised by vanilla MDM training and elicit any order reasoning. 

We also want to mention the concurrent work by ____ that proposes an alternative adaptive inference strategy by selecting $\mathcal F(\theta, x_t)$ based on the BERT model or the denoiser itself. In particular, ____ uses the BERT model or the denoiser to obtain the uncertainty of a token and then uses Top-$K$ to decide the positions to unmask it. In contrast to their work, we disentangle the impact of token ordering on MDM training vs. MDM inference and provide a more complete understanding of the motivations for and benefits of adaptive inference. Additionally, our results indicate drawbacks to using Top-$K$ strategy as opposed to Top-$K$ margin in deciding which tokens to unmask when there are multiple values with high probabilities.

\paragraph{Beyond autoregressive models.}
Efforts to learn the natural language using non-autoregressive modeling began with BERT ____. Non-causal approaches can take advantage of the understanding the text data representation. ____ adopted a similar approach for learning image representations. Building on these intuitions, 
____ proposed any-order modeling, which allows a model to generate in any desired order. ____ made the same observation that any-order models by default have to solve exponentially more masking problems than autoregressive models. However, whereas our work shows that learning in the face of this challenging task diversity can benefit the model at inference time, their work sought to alleviate complexity at training time by reducing the number of masking problems that need to be solved.