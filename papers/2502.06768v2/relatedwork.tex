\section{Related works}
\paragraph{Discrete diffusion models.}
(Continuous) diffusion models were originally built on continuous-space Markov chains with Gaussian transition kernels \cite{sohldickstein2015deep,ho2020denoising}. This was later extended to continuous time through the theory of stochastic differential equations \cite{song2021score}. In a similar vein, discrete diffusion models have emerged from discrete-space Markov chains \cite{hoogeboom2021argmax}. Specifically, \cite{austin2023structured} introduced D3PM with various types of transition matrices. Later, \citet{lou2024discrete} proposed SEDD, incorporating a theoretically and practically robust score-entropy objective. Additionally, \citet{varma2024glauber,liu2024think} introduced novel modeling strategies that classify tokens in a noisy sequence as either signal (coming from clean data) or noise (arising from the forward process). In particular, \citet{liu2024think} uses this to give a \emph{planner} that adaptively determines which tokens to denoise. While this is similar in spirit to our general discussion about devising adaptive inference strategies, we emphasize that their approach is specific to discrete diffusions for which the forward process \emph{scrambles} the token values, rather than masking them.

\paragraph{Masked diffusion models.} Meanwhile, the absorbing transition kernel has gained popularity as a common choice due to its better performance than other kernels. Building on this, \citet{sahoo2024simple,shi2025simplified} aligned its framework with continuous diffusion, resulting in a simple and principled training recipe, referring to it as \emph{Masked Diffusion Model}. Subsequent studies have explored various aspects of MDM. \citet{gong2024scaling} efficiently trained MDM via adaptation from autoregressive models, scaling MDM up to 7B parameters. \citet{zheng2024maskeddiffusionmodelssecretly} interpreted 
MDMs as order-agnostic learners and proposed a first-hitting sampler based on this insight. \citet{ye2024beyond,gong2024scaling} demonstrated that MDM outperforms autoregressive models in reasoning and planning tasks, emphasizing its impact on downstream applications. \citet{nie2024scaling} examined the scaling laws of MDM, while \citet{xu2024energy,liu2024copula} identified limitations in capturing coordinate dependencies when the number of sampling steps is small and proposed additional modeling strategies to address this issue. \citet{schiff2024simple} studied conditional generation using MDM and \citet{rectorbrooks2024steering} tackled the challenge of controlling generated data distributions through steering methodologies. \citet{chen2024convergence} provided a theoretical analysis showing that sampling error is small given accurate score function estimation.


\paragraph{Any-order reasoning.} Even though language tasks generally have a natural order of ``left-to-right" token generation, in many tasks like planning, reasoning, and combinatorial optimization, the natural order of token generation can be quite different from ``left-to-right". Even though prominent autoregressive-based language models achieve impressive performance on various tasks, many works \cite{golovneva2024reverse, chen2024premise, kitouni2024factorization} have shown that this performance is tied to the training order of the tasks and therefore can cause brittleness from it. For example, \citet{chen2024premise} showed that simply permuting the premise order on math tasks causes a performance drop of 30\%. The reason behind such brittleness regarding the ordering is the inherent ``left-to-right" nature of the autoregressive models. Several works \cite{liao-etal-2020-probabilistically} have tried to address this issue in the autoregressive framework. In particular, \cite{papadopoulos2024arrows} highlighted the significance of left-to-right ordering in natural language by comparing its likelihood to that of the reverse (right-to-left) ordering.

Recently, discrete diffusion models have emerged as a promising approach for discrete data apart from autoregressive models. Additionally, the order-agnostic training of discrete diffusion models opens up the multiple sampling paths during the inference but it also faces some challenges during the training therefore, they seem a promising approach to elicit any order reasoning. \citet{zheng2024reparameterized} proposed different ways of implementing an adaptive inference strategy for MDM but a \emph{concrete understanding of why such an adaptive inference strategy is needed is still lacking}. In this work, we explore various aspects of vanilla MDM training and how adaptive MDM inference can mitigate the issues raised by vanilla MDM training and elicit any order reasoning. 

We also want to mention the concurrent work by \citet{peng2025path} that proposes an alternative adaptive inference strategy by selecting $\mathcal F(\theta, x_t)$ based on the BERT model or the denoiser itself. In particular, \citet{peng2025path} uses the BERT model or the denoiser to obtain the uncertainty of a token and then uses Top-$K$ to decide the positions to unmask it. In contrast to their work, we disentangle the impact of token ordering on MDM training vs. MDM inference and provide a more complete understanding of the motivations for and benefits of adaptive inference. Additionally, our results indicate drawbacks to using Top-$K$ strategy as opposed to Top-$K$ margin in deciding which tokens to unmask when there are multiple values with high probabilities.

\paragraph{Beyond autoregressive models.}
Efforts to learn the natural language using non-autoregressive modeling began with BERT \cite{devlin-etal-2019-bert}. Non-causal approaches can take advantage of the understanding the text data representation. \cite{chang2022maskgit} adopted a similar approach for learning image representations. Building on these intuitions, 
\cite{shih2022training,hoogeboom2022autoregressive} proposed any-order modeling, which allows a model to generate in any desired order. \citet{shih2022training} made the same observation that any-order models by default have to solve exponentially more masking problems than autoregressive models. However, whereas our work shows that learning in the face of this challenging task diversity can benefit the model at inference time, their work sought to alleviate complexity at training time by reducing the number of masking problems that need to be solved.