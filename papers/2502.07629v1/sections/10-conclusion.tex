\section{Conclusion}

We have introduced a design space for mobile touch interaction with generative AI and invite the community to join us in exploring it further. %
Our own empirical investigation in this paper focused on LLM-supported text length modifications using the \spread{} and \pinch{} gestures. 


We developed a novel visual feedback loop with ``word bubbles'' to handle the new requirements posed by mapping continuous touch gestures to continuous text generation. In particular, our design handles both irregular lag from the LLM and provides continuous gesture execution feedback. 
We implemented a functional prototype and evaluated it in two experiments.


Participants consistently performed tasks faster with the Bubble design and found it more intuitive, enjoyable, and easier to use, with a lower perceived workload. 
They preferred the gesture-based interaction for its natural feel and enhanced control. 
The usability of the system was rated higher, and tasks were completed significantly faster with touch gestures, compared to a conversational UI in the style of ChatGPT. %

These results highlight the potential of gesture-based controls for mobile text editing, and more direct interaction techniques for generative AI in general.

Current design trends often introduce conversational UI views and elements for generative AI. We are excited about the alternative potentials to be explored in an emerging research direction (also see \cite{directGPT, Chung2022taleBrush, Chung2024patchview}) that seeks to enable direct manipulation principles for the new capabilities of LLMs.







We release our project material in this repository to facilitate future research:

\url{https://osf.io/h3wyf/}
