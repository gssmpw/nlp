\section{Method}\label{sec:method}
We conducted a within-subject \revision{lab} study to investigate touch gesture interaction for LLM-powered text length modification. \revision{Concretely, we compared} different feedback designs and evaluated \revision{the gestures} against a baseline chatbot interface. \cref{fig:method_overview} shows the study design and procedure.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/method_overview}
    \caption{Overview of our user study design and procedure.}
    \Description{This figure provides an overview of the user study design and procedure, highlighting key steps, experiments, and tasks performed during the study, as they will be described in the following sections. The figure is structured into four main sections: Intro, Experiment 1, Experiment 2, and Conclusion. For each, it illustrates the content using screenshots and icons.}
    \label{fig:method_overview}
\end{figure*}


\subsection{Experimental Design}
\revision{The study consisted of} two experiments and a ``hands on'' semi-structured interview.

\subsubsection{Experiment 1: Comparing Visual Feedback Loop Designs}
The goals for Experiment 1 were to observe how participants perceive the general concept, collect data on \spread{} and \pinch{} gesture execution, and compare three feedback designs.


\paragraph{Conditions}
The \textit{independent variable} was ``Feedback'' with three conditions: 
\visbubble{} -- the design shown in \cref{fig:teaser} and described in the previous section; 
\visline{} -- an alternative inspired by typical text selection markup, i.e. colouring the background of the text line (incl. without text when used in the placeholder role, cf.~\cref{sec:impl_text_length_ind}); 
and \visnone{} -- a baseline with no visual feedback beyond the text itself.
We selected these designs based on their ability to provide varying levels of feedback on text generation and modification (none vs length with \visline{} vs length + word count with \visbubble).
Their order was counterbalanced.


\paragraph{Measures}
As \textit{dependent variables}, we logged interaction metrics and collected subjective feedback through think-aloud protocols and questionnaires.


\paragraph{Tasks}
Both text extension and shortening were tested using short, neutral and unopinionated texts provided to participants as starting points (see \cref{sec:appendix_texts_exp1}).
We used the same texts for all conditions as the focus was not on reading and text comprehension, but repetitive execution of the gestures.

To explore different text length modifications, we asked participants to complete/remove one incomplete sentence (five repetitions each), extend/shorten the text by one full sentence (three repetitions each), extend/shorten the text by three sentences (three repetitions each), and perform a combination of extension and shortening tasks (twice).
The number of repetitions was chosen to balance task complexity with participant fatigue and potential learning effects.
To avoid additional confounds, we excluded the long-press feature (\cref{sec:longpress}) and disabled the phone's integrated keyboard.



\subsubsection{Experiment 2: Comparing Direct Touch Interaction Against a Chatbot UI}
This experiment aimed to provide deeper insights into how participants perceive the concept of controlling AI with touch gestures for text generation, compared to a typical chatbot UI. %

\paragraph{Conditions}
Interaction technique was the \textit{independent variable}, with two conditions: 
Direct Touch Interaction -- our proposed gesture-based text length modification using the Bubbles visualisation;
Chatbot Interaction -- a baseline imitating traditional chatbot-like interaction (\cref{fig:gpt_interface}).
\revision{We selected this baseline as a widely adopted, state-of-the-art approach for interacting with LLMs today, especially on mobile devices (e.g. see apps for ChatGPT\footnote{\url{https://openai.com/chatgpt/download/}} and Copilot\footnote{\url{https://play.google.com/store/apps/details?id=com.microsoft.copilot}})}. 
\revision{Suggestions and autocorrection were included via the normal phone keyboard.}
We counterbalanced the two conditions.

\paragraph{Measures}
As \textit{dependent variables}, we logged quantitative data (e.g. interaction logs) and qualitative data (e.g., observations, feedback, questionnaires).

\paragraph{Tasks}
We prepared short, unopinionated texts for participants to work with (\cref{sec:appendix_texts_exp2}).
They \revision{included} sentences clearly out of place (e.g. a topic shift from gardening to skating) or had missing parts (e.g. starting an enumeration but listing only one item).

Instead of giving quantifiable instructions on text length as in Experiment 1 (i.e. ``Remove \textit{one} sentence.''), this approach allowed us to give more abstract instructions. These required participants to read and understand the text (i.e. ``Remove the \textit{irrelevant} sentence.'').
Here we used different texts for the two conditions.
As in Experiment 1, tasks were considered completed when participants accepted the text length modification.

To explore more natural workflows in this more open task, we enabled the built-in mobile keyboard, and participants had access to the long-press feature.





\subsection{Apparatus and Materials}

\begin{figure*}[t]
     \centering
     \includegraphics[height=5.5cm]{figures/alternative_conditions}
     \caption{Examples of the UI in Experiment 1: In the \visline{} condition (left) coloured lines provide visual feedback on the change of text length. The design of the \visnone{} condition (right) offered no visual feedback beyond the text itself.}
     \Description{This figure shows two panels showing examples of the 'Lines' and 'NoVis' experimental conditions during Experiment 1 of the user study. Each panel demonstrates a moment of interaction where participants are tasked with editing text using different feedback methods. Panel (a): Lines Condition –- This panel shows the 'Lines' condition, where a coloured line is used to visualise changes in text length. The participant is tasked with adding three sentences to a text about climate change. The red cursor indicates the current insertion point. Green lines appear under newly generated text, highlighting the length of the text extension. The visual feedback helps the user keep track of the amount of text being added in real-time. Panel (b): NoVis Condition –- This panel illustrates the 'NoVis' condition, where no visual feedback is provided other than the text itself. The task asks the participant to add two sentences, accept the changes, and then remove one sentence. The red cursor shows where the participant is editing the text on photosynthesis. Without additional visual cues, participants must rely solely on the text to track their changes.}
     \label{fig:vis_conditions}
\end{figure*}

\subsubsection{Comparative Designs} %
\paragraph{Experiment 1}
For Experiment 1, we implemented two additional designs:
\visline{} \revision{indicated text-length as} a continuous green bar for generation, and a red bar \revision{for removing text}, but it had no placeholders for \revision{individual} words (\cref{fig:vis_conditions} left).
NoVis showed no text-length indicators at all (\cref{fig:vis_conditions} right).
In all three visualisations, generated text appeared as soon as the token stream arrived.


\paragraph{Experiment 2}
This experiment used our prototype with \visbubble.
For comparison, the same task was also completed using a chatbot-like UI, similar to the ChatGPT app.
To ensure consistent data logging across conditions and avoid differences in latency and logging frequency, we implemented the chatbot UI directly in our web app (\cref{fig:gpt_interface}), using the same LLM as for the gestures (\cref{sec:backend}). 
Participants could easily switch between the text editor and chatbot via a button at the top, making this ``simulated'' context switch faster than actual switching between separate apps.

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figures/gpt_interface.png}
    \caption{Our imitation of a chatbot-like interface, resembling the ChatGPT app. The green text bubble contains the user's prompt. The chatbot returns a grey text bubble, where the irrelevant sentence has been removed. Users can copy the revised text using the blue 'Copy' button. A chat input field and 'Send' button allow further interaction.}
    \Description{This figure shows a mobile interface designed to imitate a chatbot-like interaction, resembling the ChatGPT app. The interface includes two sections: Upper section: A green text bubble provides a task prompt, instructing the chatbot to 'remove the irrelevant sentence from the text.' The text to be edited follows, discussing tomatoes and skateboards. The instruction asks the chatbot to remove the sentence about skateboards, which is not relevant to the topic about growing tomatoes. Lower section: After processing the request, the chatbot returns a grey text bubble with the revised text. The irrelevant sentence about skateboards has been removed, and the corrected sentence about tomatoes is provided. A blue 'Copy' button is displayed below the revised text, allowing the user to copy the corrected version. At the bottom, there is a standard chat input box, where the user can type a message, along with a 'Send' button to interact with the chatbot.}
    \label{fig:gpt_interface}
\end{figure}

\subsubsection{Study Environment}
We conducted the study in person at our university lab in a neutral office environment with minimal distractions. 
Participants were seated comfortably, with both arms resting on a table while holding the device (\cref{fig:lab_study}).

We used an iPhone 14 with a screen resolution of 2532 × 1170 pixels and a screen size of 6.06\,in (153.924\,mm) running iOS 17.6.1. 
At the start of each experiment, the experimenter opened our web app using the Chrome Browser, and started the screen recording.

\subsubsection{Experimental Software and Web Application} %
We hosted our prototype (\cref{sec:implementation}) locally on our server. 
It was embedded into a framework that handled the study logic, including briefings and in-app questionnaires, and managed counterbalancing and study progression (\cref{fig:study_elements}). %

\begin{figure}
    \centering
    \includegraphics[height=7cm]{figures/study_elements.png}
    \caption{Two study elements showing the screen participants saw during the study with our web app opened inside a browser (URL blurred for anonymity), and a screenshot of one part of our questionnaire with UI elements for study progression. A digital clock in red indicates that screen recording was active.}
    \Description{This figure displays two screenshots representing key elements of the user study interface. Left panel: The left-hand image shows the screen participants viewed during the study while interacting with the web app inside a browser (with the URL blurred for anonymity). The app is presented in a Google Chrome mobile browser and displays a text-editing task from the prototype. In this specific instance, the participant is engaged with a semi-structured interview where they modify a passage about time travel by interacting with highlighted text. The red-highlighted digital clock at the top indicates that screen recording is active. Other UI elements, including task progression indicators and buttons for skipping to the next task or resetting the prototype, are also visible. Right panel: The right-hand image shows a section of the usability questionnaire that participants filled out as part of the study. The UI includes a rating scale from 1 (strongly disagree) to 5 (strongly agree) for the statement 'I think that I would like to use this system frequently.' Navigation buttons ('Previous' and 'Next') allow participants to move between sections of the questionnaire.}
    \label{fig:study_elements}
\end{figure}





\begin{table*}
\footnotesize
\begin{tabular}{cccccccc}
\toprule
\textbf{P} & \textbf{Age} & \textbf{Gender} & \textbf{Occupation} & \textbf{WPM} & \makecell{\textbf{Experience with} \\ \textbf{Generated Texts}} & \textbf{Frequency of AI Usage} & \makecell{\textbf{Familiarity with LLMs} \\ \textbf{(Likert scale, 1/low to 5/high)}}  \\
\midrule
P1 & 22 & Male & Student & 30 & Yes (ChatGPT) & Occasionally & 5 \\
P2 & 20 & Male & Apprentice & 36 & Yes (ChatGPT) & Rarely & 4 \\
P3 & 20 & Male & Student & 38 & No & Rarely & 2 \\
P4 & 20 & Male & Student & 26 & No & Rarely & 1 \\
P5 & 22 & Male & Student & 53 & Yes (study related) & Occasionally & 5 \\
P6 & 20 & Male & Student & 37 & No & Rarely & 2 \\
P7 & 24 & Male & Student & 44 & Yes (ChatGPT) & Occasionally & 5 \\
P8 & 20 & Female & Student & 29 & Yes (ChatGPT) & Occasionally & 4 \\
P9 & 21 & Female & Student & 35 & No & Rarely & 1 \\
P10 & 48 & Female & Technical Assistant & 22 & No & Rarely & 1 \\
P11 & 60 & Female & Technical Assistant & 16 & No & Rarely & 1 \\
P12 & 19 & Male & HS Graduate & 34 & Yes (ChatGPT) & \revision{Rarely} & 2 \\
P13 & 59 & Female & Midwife & 10 & No & Rarely & 1 \\
P14 & 21 & Male & Student & 50 & Yes (ChatGPT) & Occasionally & 5 \\
\bottomrule
\end{tabular}
\caption{Overview of the participants.}
\Description{This table gives an overview of the participants.}
\label{tab:participants}
\end{table*}

\subsection{Participants}
We recruited 14 participants (5 female, 9 male) through our university network. 
Criteria for participation were high English proficiency and the ability to perform touch gestures on mobile devices. 
To ensure a more diverse range of perspectives and assess the accessibility of our concept across different demographics, we selected participants from distinct age groups and occupational backgrounds.
All participants were right-handed. %
While writing on mobile devices, most participants reported to use both hands and thumbs (11), two write one-handed with the thumb, and one with their index finger. 
All use their mobile devices (11 iOS, 3 Android) at least several times a day, mainly for writing messages (13), notes, comments and TODOs (3), and browsing the internet (3).
\cref{tab:participants} provides an overview. %

\subsection{Procedure}

\begin{figure}
    \centering
    \includegraphics[height=5cm]{figures/lab_study.jpg}
    \caption{A participant performing the \spread{} gesture using our prototype web app. The participant provided consent to publish this figure.}
    \Description{This figure shows a participant interacting with a mobile device, performing the '\spread{}' gesture using the prototype web app. The participant is seated at a desk in an office environment with computer monitors and other office supplies in the background. The mobile device is being held with both hands, and the participant's fingers are spread apart on the screen, indicating the gesture used to control text generation. A bottle of water and a sheet of paper with study-related materials are visible on the desk.}
    \label{fig:lab_study}
\end{figure}

Participants \revision{received} study documents in accordance with our university's regulations. %
After giving informed consent, participants filled out a demographic questionnaire.

We explained the experiment and demonstrated interaction with our prototype. 
This covered using gestures to adjust text length and the experimental conditions.
We encouraged participants to interact with the device in whichever way felt most comfortable. %
Furthermore, we asked them to ``think aloud'' while interacting. %
The phone was cleaned with disinfectant wipes between participants.

To \revision{assess} manual typing speed and help participants familiarize themselves with the device, they first completed a one-minute typing test, typing random English words. 
We answered all questions that came up.

The study took around 74 minutes on average.
Participants were compensated at a rate of €\,5 per (started) 20 minutes. %


\subsubsection{Experiment 1} %
\label{sec:procedure_exp1}
The same three tasks (extend, shorten, combination) were performed on the same texts (see \cref{sec:appendix_texts_exp1}) for all feedback designs, structured as follows:

First, participants extended the text by completing one incomplete sentence (5 repetitions), adding one sentence (3 repetitions), and adding three sentences (3 repetitions).
Second, they removed one incomplete sentence, one full sentence, and three sentences, repeating each task as previously.
Finally, participants performed combinations: 
In the first, they added two sentences and then removed one; 
in the second, they removed two sentences and then added one. 
These combination subtasks were performed only once.

After the final subtask \revision{per condition}, participants answered the in-app questionnaires and put the phone down \revision{for a} short break.
They then repeated the tasks with the next counterbalanced condition. 
\revision{Finally,} Experiment 1 concluded with a questionnaire and an extended break. %




\subsubsection{Experiment 2} %
\label{sec:procedure_exp2}
Here we instructed participants to edit a provided text by either removing sentences that did not fit the surrounding context or extending the text to fill logical or semantic gaps.
Beyond these abstract instructions, we did not specify the location of the text modifications or the amount of text to be removed or added. 
Thus, participants had to determine for themselves when they had changed the text to their satisfaction.
\revision{This was repeated for} three different texts per condition (see \cref{sec:appendix_texts_exp2}).
After the first condition, we asked participants to answer in-app questionnaires and put the phone down \revision{for} a short break, \revision{before continuing} with the second condition.
\revision{At the end, participants answered} a questionnaire regarding their experience with both conditions.

\subsubsection{Semi-Structured Interview}
\revision{The study sessions concluded with} a semi-structured interview to gather detailed feedback. 
During the interview, participants were free to explore our prototype (with long-press and keyboard enabled) without any prescribed tasks.
They could also switch to the chatbot UI.
If they wished, we provided a selection of short creative story texts (see \cref{sec:short_stories}) as inspiration, though they were free to write anything they wanted.
This allowed them to directly demonstrate what they were referring to during the interview. %
