\section{A Design Space for Mobile Touch Interaction with LLMs}
\label{sec:design_space}

We developed a design space for touch-based LLM interaction through focused brainstorming sessions and affinity diagramming \cite{hartson2012ux} in our research team. We gathered high-level ideas, examples from research and industry, and explored various mappings of LLM capabilities to touch gestures. 
\revision{The resulting space} has four dimensions (\cref{fig:design_space}):
(1) Input,
(2) Referential Interaction,
(3) Textual Interaction,
(4) Output.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/design_space}
    \caption{Overview of our design space for mobile touch interaction with generative AI for text, with four dimensions (left to right) and subdimensions (values at vertical lines). Coloured streams indicate the design choices for the two concrete touch gesture controls that we designed, implemented, and evaluated in this paper.}
    \Description{This figure represents a design space for mobile touch interaction with generative AI for text. It spans four dimensions: Input, Referential Interaction, Text Interaction, and Output, each further divided into subdimensions. Coloured streams (blue for ``\Spread{}'' and red for ``\Pinch{}'') highlight the specific design choices evaluated in the paper. Both gestures have the following choices highlighted: ``direct'', ``continuous'', and ``feedback'' for the ``input'' dimension, ``select sentence on touch down'' for the ``referential interaction'' dimension, ``diegetic'' and ``additive'' is marked for spread in the ``text interaction'' dimension whereas pinch has ``diegetic'' and ``subtractive'' highlighted. In the output dimension both have ``audit'', ``diegetic'', and ``feedback marked''.}
    \label{fig:design_space}
\end{figure*}

\subsection{Input}
This dimension captures three fundamental choices for interaction designs with touch input. %

\subsubsection{Direct vs Indirect}
Indirect input for interaction with an LLM requires additional UI elements (e.g. virtual keyboard, floating objects, buttons) and/or uses an external application beyond the main writing interface, such as when copy-pasting in text generated with ChatGPT.
In contrast, direct input refers to touch gestures directly on the text that the LLM should operate on. Mobile device users are used to \revision{typical gestures such as} tap, swipe, pinch, spread, and drag. 

\subsubsection{Discrete vs Continuous}
The most common type of touch input today is a discrete event -- tapping a button. Beyond this, there are touch gestures that unfold continuously on the screen over time, such as a swipe or pinch.

\subsubsection{Feedback} 
This captures if and how feedback is given to the user leading up to the final input and/or after it has been completed. For example, this might include a visual trajectory line while performing a gesture. As another example, text selection feedback can serve in this role, such as when highlighting the current selection with a coloured background, updated ``live'' while swiping over it.



\subsection{Referential Interaction} 
Inspired by work on referential and verbal acts in HCI~\cite{wolff1998acting}, we separate these aspects in our design space. Our referential dimension captures \textit{how users interact with text to set the context}, scope, reference, and so on, as a precursor to LLM operations on that text. Visual (UI) changes resulting from these interactions are typically ephemeral and not meant to become a lasting part of the text document.
\revision{As a common example, text selection} may involve (long-)taps, moving the finger over the text, and/or dragging UI instruments~\cite{BeaudouinLafon2000instrumental} (e.g. start/end markers). We decided to keep this dimension at a high level, as it is not our main focus. Possible subdimensions to explore in the future include the design of selection procedures and markers \revision{for} ``pointing'' an LLM at text parts (cf.~\cite{directGPT}).


\subsection{Textual Interaction}
This captures \textit{how users interact with the content (semantics) and style of the text}. These interactions are performed to trigger and parametrise the LLM to operate on the text in a lasting way.
Examples from research on AI writing support~\cite{Lee2024dsiiwa} include extending or shortening text, summarising parts of it, and rewriting, possibly with a specific \revision{goal} (e.g. adjusting tone or level of formality).
To structure this space of possible \revision{AI} text operations, here we focus on two aspects we identified as most fundamental \revision{in} our context.

\subsubsection{Diegetic vs Non-diegetic Text}
This captures whether text involved in interaction (e.g. text selected with a preceding referential interaction) is \textit{diegetic}, \textit{non-diegetic}, or a mix of both (cf.~\cite{Buschek2024collage, Dang2023choice}). Diegetic text \revision{is intended to be in} the final text (e.g. a sentence in a story to improve with AI), while non-diegetic text is not (e.g. a ``todo'' note or prompt for the LLM).

\subsubsection{Additive vs Subtractive vs Transformative}
This subdimension captures whether the AI text operation involved in the interaction is \textit{additive}, \textit{subtractive}, \textit{transformative} or a mix. Additive operations add words, such as when extending a draft. Subtractive refers to the opposite, such as when the system removes words to shorten a text semantically (e.g. by summarising parts of it) or mechanically (e.g. cutting the last 3 sentences). Finally, transformative refers to AI operations that change text without clearly adding or removing \revision{words}. For example, a user might use AI to revise their draft with the goal of achieving a consistently formal tone. This might involve both cutting informal phrases as well as adding parts to adhere to formal expectations.


\subsection{Output}
This dimension captures how the results of the interaction -- the result of the AI's text operation -- is presented.

\subsubsection{Audit vs Edit} 
\revision{Text changes can be} direct edits by the AI (e.g. fixing spelling mistakes) or ``audits'', typically suggestions for users to accept or reject (cf. \cite{Cooper2014aboutface}). 

\subsubsection{Diegetic vs Non-diegetic Text}
AI text resulting from the interaction \revision{can be} diegetic or non-diegetic, or a mix of both. For example, an added sentence suggestion is diegetic, as it is intended to become a part of the text. In contrast, an added feedback comment is non-diegetic AI output (cf.~\cite{Buschek2024collage}).

\subsubsection{Feedback} 
This subdimension captures if and how feedback is given on the way to the final result of an AI text operation and/or after it has been completed. For example, a loading indicator \revision{might show that the} LLM is computing the output. Text itself might be used \revision{for this}, such as in ChatGPT, which adds one word at a time.


\subsection{Applying the design space: \Spread{}, \Pinch{}}
\label{sec:ds_applied}

In this paper, we explore two concrete designs arising from our design space. \cref{sec:implementation} describes \revision{their implementation.} Here, we situate them in our design space to illustrate how the space can be used to generate designs and systematically describe them. %

\subsubsection{\Spread{}}
This touch interaction uses \textit{direct} and \textit{continuous} input (spread gesture with two fingers, as known from zooming) on \textit{diegetic} text (user's draft) to control an \textit{additive} LLM capability with \textit{diegetic} output (text generation) that users need to confirm (\textit{audit}). 
The \textit{referential interaction} is realised by interpreting the first touch event of the two fingers as a selection of the sentence, after which the generated text should be added.
The \textit{feedback} for both input and output is combined into one visual element -- an indicator of the length of the generated text, extended ``live'' while spreading the fingers. \revision{Our study (\cref{sec:method}) compares} two variants of this design (lines, bubbles) with a baseline (no feedback).


\subsubsection{\Pinch{}}
This interaction is the complement to the above: It uses \textit{direct} and \textit{continuous} input (pinch gesture with two fingers) on \textit{diegetic} text (user's draft) to control a \textit{subtractive} operation with a \textit{diegetic} result (text deletion) that users need to confirm (\textit{audit}). 
\textit{Referential interaction} and \textit{feedback} are realised as described above. However, our visual design differs to make it easier for users to perceive the effect at a glance (e.g. red for delete, blue for extend). Our implementation of \pinch{} does not require an LLM.





