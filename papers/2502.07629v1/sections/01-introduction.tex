\section{Introduction}
Writing on the go is cumbersome.
Generative AI for text promises writing support, such as with recent features based on the text generation capabilities of Large Language Models (LLMs).
Today, many people interact with LLMs via a conversational user interface (CUI), popularised by OpenAI's ChatGPT application.\footnote{\url{https://chatgpt.com/}}

However, while writing, this introduces the need for a separate application window, besides the text editor. 
On a large desktop screen, users might open both in parallel. Additionally, recent related research as well as industry products\footnote{e.g. Copilot in Microsoft Word: \url{https://copilot.cloud.microsoft/en-US/copilot-word} and Gemini in Google Docs: \url{https://support.google.com/docs/answer/14206696}} have introduced elements like a \textit{sidebar}, to integrate a text editor with a UI for accessing LLMs (e.g. \textit{Wordcraft}~\cite{Yuan20222wordcraft}).

Unfortunately, this solution does not work well with the limited screen space of many mobile devices, such as smartphones. 
As a consequence, mobile users have to engage in cumbersome \textit{context switches}, such as switching between their writing app and a browser or dedicated AI app (e.g. the ChatGPT app\footnote{\url{https://openai.com/chatgpt/download/}}) \revision{to prompt the model}.


\revision{An emerging alternative to writing prompts is direct manipulation.}
Recently, \citet{directGPT} \revision{explored this via mouse and keyboard for composing prompts from textual and visual elements,} including a use case for text editing.
\revision{%
In our work here, we explore direct interaction with LLMs for a new context and goal: using \textit{continuous gestures}, instead of writing prompts, for 
controlling
text generation on \textit{mobile touch devices}, motivated by reducing the need for context switches between separate writing and prompting UIs. %
}



Concretely, we explore this idea with these three guiding research questions:

\begin{enumerate}
    \item What are the fundamental design decisions for mobile touch interaction with generative AI for text?
    \item How might we design for a continuous control loop in this context?
    \item How do users perceive and interact with such a mobile touch interaction technique for generative AI for text?
\end{enumerate}


To address these questions, we first charted a design space that captures a set of core design choices for mobile touch interaction with generative AI for text. 
We then used this space to select and design one fundamental gesture control, \textit{\spread{}} -- plus its counterpart, \textit{\pinch{}}. 
Through a formative study, we identified visual feedback as a key requirement and challenge. We iteratively designed and implemented a solution that also handles irregular latency when streaming generated text from LLMs, using a novel concept of ``word bubbles'' (\cref{fig:teaser}).
Finally, we evaluated our design in a controlled lab study with two parts: Experiment 1 compared the word bubbles against two other alternatives (line highlighting, no visual feedback) in sentence generation/deletion tasks. Experiment 2 compared our gesture concept with a CUI (designed to mimic ChatGPT as an app) in a text editing task.

Our results show that visual feedback improves the control loop in terms of speed, workload, and usability. In particular, interaction with the word bubbles was fastest, reduced overshooting, and was clearly preferred by participants. The gesture interaction also outperformed the CUI baseline.%



\revision{In summary, we contribute: 
(1) The first design space specifically for mobile touch interaction with LLMs, 
to provide structure for exploring, describing, and reflecting on new interaction designs in this emerging context; %
(2) a functional prototype that implements two new gesture controls for LLMs (\spread{}, \pinch{}); %
and (3) insights from a user study evaluation, which show that continuous touch gesture control for LLMs is both feasible and user friendly, with our new ``Bubbles'' feedback design proving most effective for control both in terms of user preference and interaction metrics.
}



In a broader view, this work lays the foundation for gesture-based interaction with generative AI on touch devices and we invite the community to join us in exploring it further.

