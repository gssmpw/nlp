\section{Related Work}\label{sec:related_work}

\subsection{\revision{Writing and AI}}
\revision{
The integration of AI has become a central topic in recent HCI research on writing assistant systems. 
\citet{Lee2024dsiiwa} recently summarized trends and mapped out a comprehensive design space for this area of work.
Many of their surveyed AI tools exhibit a ``fragmentation'' of text and UIs, as described by \citet{Buschek2024collage}: These tools introduce snippets, cards, pop-ups, and so on, to make space for prompt input and AI output. %
}

\revision{
However, the limited space on mobile devices makes these fragmented interfaces an unsuitable design choice for writing on the go.
Traditionally, mobile text entry research integrated ``intelligent'' features into the keyboard and text (e.g. word suggestions, auto-corrections~\cite{Palin2019, Banovic2019, Quinn2016}). However, today, LLMs are capable of much larger text contributions, beyond individual words or short phrases that had found their place at the top of mobile keyboards. 
Notably, none of the 115 tools surveyed by \citet{Lee2024dsiiwa} is explicitly designed for mobile touch use (beyond keyboard apps), highlighting a gap in research and practical applicability for this context.}

\revision{At the outset of our research, this motivated us to first chart a novel design space for mobile touch interaction with LLMs. We then used this space to inform our new design of gesture-controlled text generation specifically tailored for writing on mobile devices. Concretely, our resulting design supports users in triggering text generation, anticipating its length, and potentially curating it (cutting, regenerating) -- in one continuous interaction via familiar touch gestures (spread, pinch). At the same time, it supports users in handling the irregular system latency of text generation with a new visual control loop design (feedforward/feedback). %
}

\subsection{Beyond Prompts and Towards Continuous Interaction}
With \textit{DirectGPT}, \citet{directGPT} recently explored direct manipulation paradigms for interacting with LLMs, fundamentally inspiring our work. 
We extend this to touch gestures and mobile devices. \revision{In the future, we see} the potential for combining these concepts, such as integrating our gestures for text generation with the drag-and-drop interactions by \citet{directGPT} to refer to objects beyond text.
\revision{\textit{DirectGPT} operates in a discrete, turn-based interaction model: While the user's direct manipulation actions might be continuous (e.g. drag and drop), they ultimately result in a prompt that is ``done and ready'' to send to the LLM. The user then evaluates the resulting output to inform their next prompting actions.}
Similarly, \textit{TaleBrush} by \citet{Chung2022taleBrush} lets users steer story character narratives via drawn lines, using continuous (mouse) input. It also generates text only after input completion.
\revision{In contrast, our approach aims for continuous direct interaction, where continuous user actions (e.g. spreading fingers) are automatically and continuously sent to the LLM in the background, and output is continuously updated in the UI in parallel, approaching a new ``closed'' direct manipulation control loop.} 








Current systems like ChatGPT already use continuous displays for output, that is, adding one word at a time, to simulate an impression of ``writing'' and to handle computational latency in the UI.
Building on this, \citet{Lehmann2022suggVsCont} evaluated a design where users intervene in an LLM's continuous output stream for co-writing on smartphones. This pushed users into an editing role, requiring them to delete and modify text to align it with their intentions.

This role distribution -- AI drafts, user edits -- may suit cases where users prefer avoiding manual typing to generate initial drafts. Mobile devices, where text entry can be cumbersome~\cite{Kristensson2014inviscid}, are a prime example. 
However, research shows that AI-generated text often fails to meet user expectations, particularly in interpersonal communication~\cite{Fu2024texttoself, Liu2022aimailperception, Robertson2021cantreply}, and tends to be verbose~\cite{Fu2024texttoself}. These limitations highlight that while adding text to drafts is relatively simple, interacting with generated text is far more challenging, despite clear motivations for doing so.
\revision{We explore continuous direct interactions to address these challenges in the space-constrained context of mobile touch devices.} %


\subsection{Designing Mobile Touch Gesture Controls}
One \revision{existing} gesture-based UI for mobile text entry is the word-gesture keyboard~\cite{Zhai2012gesturekbMagazine}. It has inspired gestures for formatting~\cite{Alvina2017commandboard}, and can be combined with gestures for triggering word corrections~\cite{Cui2020justcorrect, Zhang2019typthencorrect}. \revision{These gestures are typically visualised via finger trajectories. More broadly, visual feedback} is a key component of designing gesture controls.


\subsubsection{Feedback via Additional Visual Elements}
The well-known \textit{marking menus}~\cite{Kurtenbach1993markingmenus} \revision{draw} the pointer trajectory as a line, adding a line segment per submenu the user \revision{moves} through. This guides learning of gesture mappings and can then be ignored by expert users~\cite{Kurtenbach1994markingmenus}. 
With \textit{Fieldward} and \textit{Pathward}, \citet{Malloch2017fieldpathward} designed visual feedback that is continuously updated while moving the finger, to guide users in defining a distinguishable set of gestures. Many gestures in our context of interacting with an LLM are also likely to benefit from visual feedback that is updated ``live'', to create a closed control loop.
Concrete requirements for visual feedback differ in our case: For example, visualising finger trajectories may not be suitable, since it might occlude the text the user is interacting with. 
Occlusion has been addressed in concepts for mobile text selection, such as a pop-up (callout) displayed above the finger position~\cite{Ishii2016callout, Vogel2007shift}. This is designed for focusing on the word or character at the cursor. In contrast, many LLM-based applications are likely \revision{interested in larger scopes of text} (e.g. generating or revising a paragraph or whole draft).



\subsubsection{Feedback without Additional Visual Elements}
Other established mobile touch gestures do not need additional visual markup as feedback, because their triggered transformations are already visual in nature. Examples include scrolling by swiping with the finger vertically, and zooming in and out of a text or image via spreading and pinching fingers. The resulting change in content size and/or location \textit{is} the feedback. \revision{In contrast,} we are not interested in geometric zooming or offsets, but in gestures that change the text content in some way.


\subsubsection{Summary and Implications for our Research}
In summary, related literature and industry standards present two kinds of design for gesture control loops on mobile devices -- those that add extra visual elements and those that do not. 
Notably, text is usually in the latter category or text-related visual feedback elements are local in nature. However, we require a novel combination here -- feedback for continuously manipulating (larger scope) text content with a gesture. \revision{We next describe this open design space in more detail.}






















