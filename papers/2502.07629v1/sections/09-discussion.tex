\section{Discussion}
We discuss the bigger picture emerging from our results, followed by specific aspects of interest.

\subsection{Word Bubbles Support Continuous Touch Control of Text Generation}\label{sec:discussion_bubbles} %
Our ``Bubbles'' visualisation with its text length and word indicators consistently outperformed \revision{the alternatives}, allowing participants to complete tasks more quickly: 
It averaged \secs{14.71} per task, compared to NoVis (\secs{16.58}) and Lines (\secs{16.50}). 
This speed advantage was evident across tasks with text extension, shortening, and combinations.
Bubbles also received the highest usability rating (SUS: 85.54) and the lowest perceived workload (NASA-TLX score: 1.98).

In addition, participants reported that Bubbles made interactions feel smooth and natural, with many describing the gestures as intuitive and engaging. 
The visual separation of words, sentences, and deleted content provided by Bubbles enhanced the sense of control, making it the preferred method. %

\subsection{Direct Interaction vs Conversations}
When comparing our gestures to the typical conversational LLM interface, participants rated the touch gestures as more usable (81 vs 52.5 SUS score) and less mentally taxing (2.06 vs 3.15 NASA-TLX). 
This is further reflected in %
all subjective measures, including satisfaction, ease of use, control, and efficiency.
Indeed, using gestures led to \pct{58} reduced task times (\secs{56.35} vs \secs{134.86}).

We conclude that our gesture-based concept was well-received and demonstrated clear advantages over a chatbot UI. Combined with our visual feedback, it thus presents a promising alternative for future text editing applications.

\subsection{Direct Interaction and Authorship}
With gestures, participants generally felt more control over text length and the generation process (\cref{fig:own_likert_exp2}, \cref{sec:ss_interview_perception}) yet many did not perceive themselves as the authors of this text. 
This is unsurprising, as \revision{our study} (a) provided given text, and (b) focused more on adjusting text length than altering content.

With our prototype, users could modify tone and input custom prompts with a long press (\cref{fig:long_press}). 
We expected the ChatGPT-like app to receive similar or even higher ratings in the ``authorship'' category, given that participants entered prompts themselves. 
However, this was not reflected in their ratings -- gestures scored higher. 
We hypothesize that the expected fall off in perceived authorship was mitigated due to participants feeling more like authors when they did not need to switch contexts or interact with an external (chatbot) app. %
This effect might become even stronger as more LLM capabilities are controlled through direct (touch) interaction.


\revision{More broadly,} previous research has shown that perceived authorship does not necessarily equate to actually \revision{having entered the text} \cite{AIghostwriter2024}. 
\revision{Thus, as HCI research pursues} interaction with AI \revision{to become} increasingly direct and seamless, %
we believe it is important to also take a nuanced look at the concept of authorship \revision{and the interaction method's impact on its perception}.


\subsection{Direct Interaction and Interaction Metaphors}
When using conversational UI elements, some users tend to perceive the AI more as a writing partner \cite{diarymateKim2024, benharrak2024aipersonas}, rather than a tool. %
Direct interaction has the potential to change this: 
Controlling the LLM directly feels more natural and gives a greater sense of control, as reflected in our findings. 
It also removes the mental association with a writing partner, as thereâ€™s no conversational interface -- just the user's own movements. 
As LLM interactions become more seamless, users might begin to view the LLM as part of their cognitive process \cite{gptMeMeshi2024, bhat2023suggestionmodel}, \revision{which could be explored in the} future.







\subsection{Impact of the Visual Feedback on User Interaction and Cognitive Load}
The choice of visual feedback had a significant impact on how users interact with and perceive the system.

\revision{Beyond the benefits of ``Bubbles'' discussed above (\cref{sec:discussion_bubbles})}, this design also impacted gesture execution: 
With Bubbles, participants approached the intended text length more consistently and with less overshooting. 
\revision{Without} visual feedback, \revision{they instead} had to wait for all words to be generated before gauging text length, as both the length and content were displayed through the same medium -- words. 
This presents a unique challenge for interaction with LLMs, where generation is faster than reading, but slower than users can assess the intended length.

Bubbles decoupled text length indication from actual words, allowing users to gauge the amount of text and start reading sooner. We hypothesize that differences in execution patterns stem from this decoupling. 

As less overshooting occurred \revision{with Bubbles}, people had to read less generated text, which may have lowered cognitive load. 
This could explain the reduced mental demand during writing when using Bubbles, as measured by the NASA-TLX (\cref{fig:own_likert_exp2}). 

Reading, attention, and related indicators of cognitive demand were not measured beyond self-reports (e.g. with eye tracking), which could be a focus for future research.




\subsection{Exploring the Design Space of Mobile Touch Interaction with LLMs}
After defining the design space, we adopted a depth-first approach in this paper and focused on exploring one specific LLM capability and gesture in detail: 
\spread{}, along with the inverse, \pinch{}. 

In future studies, we plan to explore additional alternatives and command mappings:
\begin{itemize}
    \itemsep 2mm
    \item \textit{Spread-to-Elaborate, Pinch-to-Summarize:} Currently, spreading appends text at the end of a sentence. Future versions could use spreading over existing \revision{paragraphs to extend them by inserting} elaborations. Similarly, pinching could trigger AI-generated summaries.
    \item \textit{Swipe-to-Rephrase, Rotate-for-Tone:} Swiping over text passages could prompt the LLM to rephrase them or generate synonyms for selected words, while a \revision{rotation} gesture could adjust tone (e.g. ``dialling up/down'' formality).
    \item \textit{Tap-to-Talk:} Special gestures such as a triple tap or a three-finger swipe could request detailed explanations or feedback from the LLM.
    \item \textit{Other Input Modalities:} Beyond touch-based interaction, shaking the phone could trigger rewording and tilting could provide finer control. %
\end{itemize}


\subsection{Limitations}
Our study comes with limitations.
Our prototype was limited to two gestures, within a simple text field, and tested on one device. 
Integrating our prototyped functionality into a larger mobile application (e.g. respecting existing gestures) was beyond our scope.

We covered an initial sample with diversity in some demographics (age, technology use). 
Future work should further evaluate the gestures with a larger, more diverse population.

Participants preferred touch controls over a conversational UI for the tested tasks of text generation and shortening. 
This should not be generalised as an overall preference. 
It is likely that conversational UIs, which offer much more open functionality than gestures, would still be preferred in other tasks (e.g. complex information retrieval, interactive dialogues).
We plan to expand our prototype and compare interactions across further tasks in the future.


