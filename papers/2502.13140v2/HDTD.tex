\section{A Primer on Tensor Decomposition}~\label{sec:td_def}
Mathematically, tensors are defined as multilinear functions on the Cartesian product of vector spaces \cite{spivak2018calculus}; however, it is helpful to think of tensors as a generalization of matrices to higher dimensions. As a result, tensor decompositions can be viewed as a generalization of matrix decomposition to higher-orders. The \textit{order} of a tensor is usually defined as the number of its dimensions, or equivalently the number of vectors spaces included in the product. As decomposing matrices to lower rank is generally non-unique~\cite{rabanser2017introduction} and only unique under certain constraints, such as imposed by the singular value decomposition (SVD)~\cite{kolda2009tensor}, tensor decompositions in higher-order are shown to be unique with minimal constraints and hence more general.

We will start by defining notation and preliminary properties of tensors (for a more detailed introduction to tensor decomposition methods, we refer to~\cite{kolda2009tensor}). Scalars are denoted in lowercase letters, e.g., $a$. We denote vectors (tensors of order one) in bold lowercase such as $\mathbf{u}$, with its $i^{th}$ element being $u_i$. Matrices (order-two tensors) are denoted in bold uppercase, e.g. $\mathbf{X}$ and element $(i,j)$ is denoted as $x_{ij}$. The $n^{th}$ element in a sequence is denoted as $\textbf{u}^{(n)}$, such that it is the $n^{th}$ vector in a sequence of vectors. Higher-order tensors (order three or more) are defined in Euler script, e.g., $\mathcal{T}$. 

Next, we define important properties of tensors such as its \textit{mode}, which is another name for the \textit{order} of the tensor (also called \textit{way} in some cases).  It is a measure of how many indices exist in the tensor. A zero tensor of any order is defined as a tensor with all its elements equal to zero. A third-order (or 3-way) tensor $\mathcal{T}$ has three indices $T \in \mathbb{R}^{I\times J\times K}$, where $I,J,K$ are the three indices respectively. 
%Analogous to rows and columns, tensors have \textit{fibers}. 
Another important property of tensors are \textit{slices}, which are two-dimensional sections of a tensor, defined by fixing all but two indices. For a third-order tensor $\mathcal{T}$, we will have horizontal, lateral, and frontal slices denoted by $\mathcal{T}_{i::}$, $\mathcal{T}_{:j:}$, and $\mathcal{T}_{::k}$, respectively~\cite{kolda2009tensor}. Moreover, we use the notation ``$\circ$'' to the denote outer product of vectors, and adopt ``$\otimes$'' for the tensor product operation. While these two operations are different and yield different mathematical objects, they are related in the sense that the result of a tensor product can be viewed as the vectorization of an outer product output. Next, we define rank-one tensors and the rank of a tensor before defining different types of tensor decompositions. 

% \begin{definition}~\label{def:rankone}
%    \textbf{Rank-one tensors.} A $d$-way tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_d}$ is rank $one$ if it can be written as the outer product of $d$ vectors, 
%     $$
%         \mathcal{T} = \mathbf{u}^{(1)} \otimes \mathbf{u}^{(2)} \otimes \cdots \otimes \mathbf{u}^{(d)} 
%     $$
%     where $\otimes$ is the outer product of factor vectors $\mathbf{u}^{(d)} \in \mathbb{R}^{N_d}$. Each element of the tensor is the product of the corresponding vector elements $t_{n_1,n_2,\cdots,n_d} = u_{i_1}^{(1)}u_{n_2}^{(2)}\cdots u_{n_d}^{(d)}$ $ \forall$  $1 \leq n_d \leq N_d$
% \end{definition}

%{\color{red} Laxmi's def}
\begin{definition}~\label{def:rankone}
   \textbf{(Rank-one tensor).} A $d$-way non-zero tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_d}$ is of rank $one$ if and only if it can be written as 
    $$
        \mathcal{T} = \mathbf{u}^{(1)} \circ \mathbf{u}^{(2)} \circ \cdots \circ \mathbf{u}^{(d)}, 
    $$
    where $\mathbf{u}^{(i)} \in \mathbb{R}^{N_i}$ and $\circ$ denotes the outer product. (In other words, each entry $t_{n_1,n_2,\cdots,n_d}$ of the tensor is the product, $u_{n_1}^{(1)}u_{n_2}^{(2)}\cdots u_{n_d}^{(d)}$, of the corresponding vector entries).
\end{definition}



\begin{definition}~\label{def:rank}
    \textbf{(Tensor rank).} The $rank$ of a tensor $\mathcal{T}$, denoted as \texttt{rank}($\mathcal{T}$) is defined as the smallest integer $r$ such that there exist $r$ rank one tensors $\mathcal{T}_1, ..., \mathcal{T}_r$ with
    $$
        \mathcal{T} = \sum_{i=1}^{r} 
        \mathcal{T}_i.
        %% \mathbf{u}_i^{(1)} \otimes \mathbf{u}_i^{(2)} \otimes \dots \otimes \mathbf{u}_i^{(d)}
    $$
    %% where 
    %% each $\mathbf{u}_i^{(k)} \in \mathbb{R}^{n_k}$.
    %% each $\mathcal{T}_i$ is a tensor of rank one. 
\end{definition}
%% The tensor rank can also be defined as the smallest number of components needed in a Canonical polyadic decomposition  to obtain equality in Equation~\ref{eq:cp}~\cite{kolda2009tensor}.

The idea of decomposing a tensor into its polyadic form, i.e. expressing it as the sum of a finite number of rank-one tensor has existed for over a century~\cite{hitchcock1927expression}. It has been re-introduced in the literature as CANDECOMP (canonical decomposition) and PARAFAC (parallel factors) later and collectively known as the CP decomposition~\cite{harshman1970foundations, harshman1972parafac2}. It factorizes a tensor into a sum of component rank-one tensor as defined in Definition~\ref{def:rankone}. 
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/cp_tensor_v2.png}
    \caption{CP decomposition of a third-order tensor into $r$ rank-one tensors}
    \label{fig:cp}
\end{figure}
%% \begin{definition}\label{def:cp}    \textbf{(CP decomposition).} 
    
    As an example, a third-order CP decomposition (Figure~\ref{fig:cp}) can be written as 
    \begin{equation}\label{eq:cp}
        \mathcal{T} \approx \sum_{i=1}^r \lambda_i \, \mathbf{u}_i^{(1)} \circ \mathbf{u}_i^{(2)} \circ \cdots \circ \mathbf{u}_i^{(d)},
    \end{equation}
    where $\mathbf{u}_i^{(n)} \in \mathbb{R}^{N_d}$ are the factor vectors for order $d$, $\lambda_i$ is the scaling weight, and  $r$ is the tensor rank. 
%% \end{definition}    

\subsection{Hierarchy of tensor decompositions}
The hierarchy of TD methods \cite{kolda2009tensor} reveals a nested structure in which broader frameworks encompass more specialized approaches (Figure~\ref{fig:hierarchy}). Therefore, any computational advantage for a specific tensor decomposition applies to all of its subtypes. At the top of this hierarchy is Tensor Train (TT) decomposition. It represents a higher-order tensor as a sequential \say{train} of lower-order tensors (TT cores), each connected through shared dimensions known as TT ranks~\cite{oseledets2011tensor}. 
\begin{definition}\label{def:tt}
    \textbf{(Tensor Train decomposition).}  Given a $d$-order tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \dots \times N_d}$, TT decomposition factorizes it into a sequence of \textit{3-way} core tensors $G^{(n)}$ (except for the first and last cores, which are matrices), connected in a train-like structure as a product of $d$ core entries 
    %% ($G^{(n)}(.,.,.)$) 
    as follows,
    \begin{equation}\label{eq:tt}
        \mathcal{T}(n_1, n_2, \cdots, n_d) = \sum_{r_0, r_1, \dots, r_{d-1},r_d}G^{(1)}(r_0,n_1,r_1) G^{(2)}(r_1,n_2,r_2) \cdots  G^{(d)}(r_{d-1},n_d,r_d) 
    \end{equation}
    where $G^{(n)} \in \mathbb{R}^{R_{n-1}\times N_n \times R_n}$ for $1 \leq n \leq d$. $G^{(1)} \in \mathbb{R}^{N_1 \times R_1}$ is a matrix in the first mode and $G^{(d)} \in \mathbb{R}^{R_{d-1} \times N_d}$ is a matrix for the last mode. Here $R_n$ are \texttt{TT-ranks} that control the compression and approximation quality. 
\end{definition}

% Given a \( Q \)-order tensor \( \mathcal{X} \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_Q} \), TT decomposition expresses it as a product of \( Q \) cores: \( G_1 \in \mathbb{R}^{N_1 \times R_1}, G_2 \in \mathbb{R}^{R_1 \times N_2 \times R_2}, \ldots, G_Q \in \mathbb{R}^{R_{Q-1} \times N_Q} \), where \( R_1, \dots, R_{Q-1} \) are the TT ranks. This decomposition mitigates the "curse of dimensionality" by reducing the storage complexity from \( O(N^Q) \) to \( O(QNR^2) \), where \( R = \max(R_1, \dots, R_{Q-1}) \) and \( N = \max(N_1, \dots, N_Q) \). 

The computational complexity for constructing the TT decomposition using methods like TT-SVD is $\mathcal{O}(dN^d)$ for unstructured tensors. However, it becomes manageable in practice for low TT ranks and structured tensors, making it highly effective for large-scale, high-dimensional data applications \cite{oseledets2011tensor, Zhang2020trillion}. 


A special case of TT decomposition is Tucker Decomposition~\cite{tucker1966some, zniyed2020high}. It is a tensor factorization method that expresses a high-order tensor as a core tensor multiplied by factor matrices along each mode. 

\begin{definition}\label{def:tucker}
    \textbf{(Tucker decomposition).} Given a $d$-order tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_d}$, the decomposition is represented as, 
    \begin{equation}\label{eq:tucker}
        \mathcal{T} = \mathcal{G} \times_1 U^{(1)} \times_2 U^{(2)} \cdots \times_d U^{(d)},
    \end{equation}
     $\mathcal{G} \in \mathbb{R}^{T_1 \times T_2 \times \cdots \times T_d}$ is the core tensor, and $U^{(n)} \in \mathbb{R}^{N_n \times T_n}$ are factor matrices and $\times_n$ denotes $n$-mode matrix multiplication. The multilinear ranks $T_n$ determine the dimensionality of the core tensor along each mode.
\end{definition}

Tucker decomposition is often computed using algorithms like High-Order Singular Value Decomposition (HOSVD). The storage cost is $\mathcal{O}(dNT + T^d)$, where $N = \max(N_n)$, $T = \max(T_n)$, and $T^d$ reflects the size of the core tensor. The computational complexity of HOSVD is $\mathcal{O}(dN^{d-1}T)$, dominated by the SVD of the mode unfoldings of the tensor. Tucker decomposition is widely used for dimensionality reduction, compression, and exploratory analysis of multi-way data~\cite{kolda2009tensor}.

\begin{wrapfigure}{l}{0.5\textwidth}
      \centering
        \includegraphics[width=0.48\textwidth]{figures/td_hierarchy.png}
    \caption{Hierarchy of tensor decompositions. (See Sections ~\ref{sec:indscal-to-candelinc} for mapping of INDSCAL onto CANDELINC, ~\ref{sec:dedicom-to-parafac} for mapping of DEDICOM onto PARAFAC.)}
    \label{fig:hierarchy}
\end{wrapfigure}


Unlike Tucker decomposition, CP imposes a more constrained structure, making it unique under mild conditions and suitable for identifying latent components. The storage complexity is $\mathcal{O}(rdN)$, where $N = \max(N_d)$, the order $d \ll N$, and rank $r$. Computational complexity depends on iterative algorithms like Alternating Least Squares (ALS), requiring $\mathcal{O}(rN^d)$ operations per iteration for unstructured tensors, but can be reduced with sparse or structured data. CP is extensively used in fields like chemometrics, psychometrics, and machine learning for data interpretation and factorization~\cite{kolda2009tensor}. 

% \textit{PARAFAC Decomposition}, which is another name for \textit{Canonical polyadic decomposition} (see Figure ~\ref{fig:hierarchy}) represents a tensor as a sum of rank-one tensors \cite{harshman1970foundations}. Given a \( Q \)-order tensor \( \mathcal{X} \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_Q} \), PARAFAC expresses it as \( \mathcal{X} \approx \sum_{r=1}^R \lambda_r \, \mathbf{a}_r^{(1)} \otimes \mathbf{a}_r^{(2)} \otimes \cdots \otimes \mathbf{a}_r^{(Q)} \), where \( \mathbf{a}_r^{(q)} \in \mathbb{R}^{N_q} \) are the factor vectors for mode \( q \), \( \lambda_r \) is the scaling weight, and \( R \) is the tensor rank. Unlike Tucker decomposition, PARAFAC imposes a more constrained structure, making it unique under mild conditions and suitable for identifying latent components. The storage complexity is \( O(QNR) \), where \( N = \max(N_q) \) and \( R \) is typically much smaller than \( N \). Computational complexity depends on iterative algorithms like Alternating Least Squares (ALS), requiring \( O(RN^Q) \) operations per iteration for unstructured tensors, but can be reduced with sparse or structured data. PARAFAC is widely used in chemometrics, psychometrics, and machine learning for data interpretation and factorization.

PARAFAC2 is a generalization of CP decomposition that relaxes the constraint of fixed factors and is applicable to a collection of matrices where each have the same number of columns but, varying number of rows. PARAFAC2 applies the same factor across one mode while allowing the other factor matrix to vary, rendering it suitable for real-world scenarios where one mode varies in size or structure across slices \cite{harshman1972parafac2}. Like CP, it expresses a tensor as a sum of rank-one components. However, PARAFAC2 introduces slice-specific factor matrices \( \mathbf{B}_k \) for the varying mode, with the constraint \( \mathbf{B}_k^\top \mathbf{B}_k = \mathbf{B}_j^\top \mathbf{B}_j \) for all \( k, j \). 
\begin{definition}\label{def:parafac2}
    \textbf{PARAFAC2 decomposition.}  The decomposition is given by
    $$
    \mathcal{X}_k = \mathbf{A} \, \text{diag}(\mathbf{c}_k) \, \mathbf{B}_k^\top,
    $$
    where $\mathbf{A}$ is shared across slices, $\mathbf{c}_k$ contains slice-specific weights, and $\mathbf{B}_k$ adapts to the varying dimensions.  
\end{definition}

 The storage complexity of PARAFAC2 is $\mathcal{O}(rN + r\sum_k J_k )$, where $N$ is the size of the first mode, $J_k$ the size of the varying mode, and $r$ the rank. Computational complexity depends on iterative methods like ALS, typically requiring $\mathcal{O}(rN \max(J_k) K)$ operations per iteration for $K$ slices. PARAFAC2 is widely used for longitudinal or time-varying data in applications like recommender systems and signal processing. PARAFAC2 decomposition is a specialization of Tucker decomposition (See Appendix for a detailed example). 

 Within PARAFAC2, the CP decomposition emerges as a more constrained subset, representing tensors as a sum of rank-one components (Definition~\ref{def:rankone}). Furthermore, spiked tensor decomposition, CANDELINC, and DEDICOM tensor decompositions are subsets of CP decomposition, designed for specific applications or structural assumptions. Finally, INDSCAL tensor decomposition is a refined version of CANDELINC, incorporating individual differences scaling to model individual variation in tensor components. 
 
 The spiked tensor decomposition requires structural assumptions on the data such as the existence of a low-rank signal tensor in presence of noise. The assumption is that of a \say{spiked} structure, which separates the underlying signal from the noise. It is a specific case of CP decomposition that, like CP, recovers a low-rank structure of the tensor. However, when the signal-to-noise ratio (SNR) is low, it resembles the CP decomposition. 
 \begin{definition}\label{def:st}
     \textbf{Spiked Tensor decomposition.} An underlying statistical model called the \say{spiked tensor model}, introduced in \cite{richard2014statistical}, is considered, where a $N$-dimensional real or complex signal vector $v_{sig}$ is randomly chosen to form 
     \begin{equation}\label{eq:st}
          T_0 = \lambda v_{sig}^{\otimes p} +G
     \end{equation} 
       
    In this spiked tensor formulation, $\lambda$ represents the signal strength or SNR and $G$ is Gaussian noise. The task is either to approximate the signal vector $v_{sig}$ or to determine whether the signal can be detected above some predetermined threshold.
 \end{definition}


The hierarchical framework of tensor decompositions, based on their computational complexity classes (Figure~\ref{fig:hierarchy}), highlights the interrelatedness of TD methods while showcasing their suitability for distinct scenarios and data characteristics.

\subsection{Phase transition of computational hardness}~\label{sec:hardness}
A computational problem is said to exhibit a \textit{phase transition} if there exists a sharp threshold in some control parameter, such as signal-to-noise ratio or rank, at which the problem changes from being computationally feasible (solvable in polynomial time) to infeasible (requiring super-polynomial or exponential time). Mathematically, let $P(n)$ be a problem instance of size $n$ with a control parameter $\theta(n)$. The computational phase transition occurs at some critical threshold $\theta_c(n)$, such that:

\[
\lim_{n \to \infty} \mathbb{P}(\text{efficient algorithm solves } P(n)) = \begin{cases} 1, & \theta(n) < \theta_c(n) \\ 0, & \theta(n) > \theta_c(n) \end{cases}
\]

where \say{efficient algorithm} refers to an algorithm with polynomial runtime in $n$.

Various phase transition parameters are available to study the computational hardness in tensor decomposition. A commonly used hardness metric to quantify computational tractability is \textit{Minimum Mean Squared Error} (MMSE) in low-degree polynomials~\cite{wein2023average}. In a random low-rank tensor decomposition, if the tensor rank satisfies $r \ll n^{k/2}$, then $\text{MMSE} \to 0$ and the decomposition is considered computationally feasible with efficient signal recovery . Otherwise, if it is $r \gg n^{k/2}$, then $\text{MMSE} \to 1$ and the decomposition is infeasible without efficient recovery~\cite{wein2023average}. This threshold applies to general order-$k$ tensors and provides a tight characterization of computational hardness.
% establishes a computational phase transition for random low-rank tensor decomposition, showing that: 

% \begin{itemize}
%     \item If the tensor rank satisfies $r \ll n^{k/2}$, decomposition is computationally feasible.
%     \item If $r \gg n^{k/2}$, decomposition is computationally infeasible, even though statistically possible.
% \end{itemize}

Signal strength is also used as a parameter for phase transition. In higher-order tensor clustering, the decay rate of the signal, $\beta$ controls the phase transition in the form of $\tilde{\Theta}(n^{-\beta})$~\cite{luo2022tensor}. Specific values of $\beta$ indicates computationally easy and hard regimes. 
% Luo and Zhang (2022) \cite{luo2022tensor} define phase transitions in high-order tensor clustering by introducing signal strength parameters:
% \[
% \lambda = \tilde{\Theta}(n^{-\beta}), \quad \mu = \tilde{\Theta}(n^{-\beta}), 
% \]
%are used, with $\beta$ controlling the decay rate of the signal~\cite{luo2022tensor}. The phase transition occurs at specific values of $\beta$, distinguishing computationally easy and hard regimes.
Similarly, for sparse tensor decomposition, sparsity is a crucial parameter. It is defined as 
%Luo \& Zhang (2022) \cite{luo2022tensor}  define:

\[
k = \tilde{\Theta}(n^{\alpha})
\]

where $\alpha$ represents the sparsity in the form of the fraction of non-zero entries in the tensor. Lower values of $\alpha$ makes the tensor decomposition computationally harder~\cite{luo2022tensor}. The Signal-to-Noise Ratio (SNR) is another commonly used parameter for measuring hardness. In a low-rank Bernoulli model

% Wang \& Li (2022) \cite{wang2020learning} introduce a probabilistic model for binary multiway data, defining a low-rank Bernoulli model:

\[
Y | \Theta \sim \text{Bernoulli}(f(\Theta)), 
\]

where $\Theta$ is a low-rank continuous-valued tensor, three  distinct phases of learnability were derived~\cite{wang2020learning}: 
\begin{itemize}
    \item \textit{High SNR (Noise Helps):} Moderate noise improves estimation due to a dithering effect.
    \item \textit{Intermediate SNR (Noise Hurts):} Standard behavior where noise degrades performance.
    \item \textit{Low SNR (Impossible Phase):} Learning is infeasible below a critical threshold.
\end{itemize}
Similarly, in a tensor completion problem, the phase transition in recovering missing values is based on \textit{Inverse SNR}, which governs the feasibility of recovery~\cite{stephan2024non}. The study of the phase transition of computational hardness  allows us to systematically investigate potential use cases for a new algorithm where the state-of-the-art approaches may not be performant.
%Stephan \& Zhu (2024) \cite{stephan2024non} study \textit{tensor completion} using a non-backtracking spectral method and identify a phase transition in recoverability based on:
% \begin{itemize}
%     \item \textit{Inverse Signal-to-Noise Ratio ($\tau_i = \theta / \nu_i$):} Governs the feasibility of recovery.
%     \item \textit{Detection Threshold ($\theta$):} Determines when singular values of the observed tensor are detectable.
%     \item \textit{Aspect Ratio ($\eta$):} Characterizes the difficulty of matrix unfolding in tensor completion.
%     \item \textit{Sample Complexity Threshold ($O(n^{k/2})$):} The number of observed tensor entries required for feasible completion.
% \end{itemize}
% A commonly used hardness metric to quantify computational tractability is \textit{Minimum Mean Squared Error} (MMSE) in low-degree polynomials~\cite{wein2023average}. 
% The \textit{Degree-$D$ MMSE} is defined as:
% \[
% \text{MMSE}_{\leq D} = \inf_{f, \deg f \leq D} \mathbb{E}_a [ ( f(T) - a_{11} )^2 ]
% \

% where $f(T)$ is an estimator using polynomials of degree $D$. 
% The key result is:

% \begin{itemize}
%     \item If $r \ll n^{k/2}$, $\text{MMSE} \to 0$ (efficient recovery is possible).
%     \item If $r \gg n^{k/2}$, $\text{MMSE} \to 1$ (efficient recovery is impossible).
% \end{itemize}
