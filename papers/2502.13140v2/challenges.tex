


\subsection{Challenges in Biomedical Tensor Decomposition}\label{sec:challenges}

Tensor-based modeling for biomedical data presents inherent challenges due to the high-dimensional nature of these datasets, which results in computationally intensive operations. We choose to demonstrate the challenges in light of Tucker decomposition as it is straightforward to comprehend as a combination of a core tensor and factor matrices (Definition~\ref{def:tucker}), and also widely used in biomedical data analysis. There are several factors that governs the time and space complexity of Tucker decomposition, namely $d$, the order of the tensor, $N$, the size of the data, and the size of the core tensor $T$. The average case storage and time complexity scales exponentially with respect to these three parameters, yielding $\mathcal{O}(dNT+T^d)$ and $\mathcal{O}(dN^{d-1}T)$, respectively. We observe that in practice, processing dense tensors with randomly generated data from normal distribution, makes it scale exponentially with increasing order in large-scale high-performance computing clusters (HPC) (Figure~\ref{fig:mem-time}) for both memory and time. We computed the Tucker decomposition in IBM's HPC cluster with Supermicro servers using the \texttt{tensorly}~\cite{kossaifi2019tensorly} package in Python. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/cp_order_mem_time.png}
    \caption{Memory usage (in gigabytes) and wall-clock execution time (in hours) for Tucker decomposition of a random dense tensor of size $N^d$, where $N=100$ and the order $d$ is represented in the x-axis.}
    \label{fig:mem-time}
\end{figure}
Moreover, there is a critical need to balance the fidelity of the latent representations with computational efficiency, as discussed in Section~\ref{sec:hardness}. In the following sections, we detail these challenges, illustrating the hardness in terms of space and time requirements for processing real-world imaging and multi-omics data as tensors and performing Tucker decomposition. For each of these cases, we also describe how the order of the tensor plays a crucial role in determining tractability of tensor decompositions.

\subsubsection{Biomedical Imaging}
In medical imaging, tensor representations of images seeks to preserve the intrinsic high-dimensional structure of the images. However, computation of numerical results from tensors often requires converting these models into matrix or vector forms. This leads to the construction of large matrices involving Kronecker products, imposing significant computational burdens on current hardware. Consequently, computations are typically divided into small, overlapping patches, an approach that is not universally applicable, especially in medical imaging. For example, in MRI super-resolution, the low-resolution image is derived from a global Fourier transform of the entire high-resolution image, making patch-based processing infeasible. 

Another general challenge in tensor decomposition is rank selection. For example, in Tucker decomposition, selecting the size of the core tensor, which is also the rank, is hard. It is significant since it determines the level of dimensionality reduction and the trade-off between performance and computational efficiency. If the rank is too low, capturing the latent structures in the data may be weakened, while a high rank increases computational complexity and memory usage significantly. As we discuss in Section~\ref{sec:hardness}, understanding the optimal rank of the tensor leads to a low MMSE, yielding efficient signal recovery. This phenomenon is true for all tensor decomposition techniques and their applications in medical imaging ranging from MRI to neural activity patterns and calcium imaging. Using multi-slice MRI data with image size $512\times784\times912$ and voxel size $0.2\times0.18\times0.18$~mm, where the dimensions represent the number of slices (512), and the in-plane resolution ($784\times912$) of the imaging sequence, we performed Tucker decomposition to obtain the latent factors representing the image. We observed an exponential trend in the time to compute ranks up to 500 mimicking the performance of the dense third-order tensor earlier in Figure~\ref{fig:mem-time}. However, an oscillating memory usage was recorded when increasing the rank of the tensor decomposition, possibly due to discrete memory allocations, triggering change in memory block sizes and internal data structures (Figure~\ref{fig:hardness} A and B). 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/hardness.png}
    \caption{Measure of computational hardness in performing Tucker decomposition on biomedical data: \textbf{A.} Memory usage (in Megabytes) in decomposing 3D MRI data of size ($512 \times 784 \times 912$); \textbf{B.} Wall-clock time (in seconds) of decomposing the 3-way MRI data; \textbf{C.} Memory usage (in Megabytes) in decomposing a 3-way multi-omics tensor of size ($281 \times 10000 \times 100)$; \textbf{B.} Wall-clock time (in seconds) of decomposing the 3-way multi-omics tensor.}
    \label{fig:hardness}
\end{figure}

% \begin{figure}[ht]
%     \centering
%     \subfloat{\includegraphics[width=0.45\textwidth]{figures/mri_memory_usage.png}}
%     \hspace{0.5cm} % Adds some spacing between images
%     \subfloat{\includegraphics[width=0.45\textwidth]{figures/mri_execution_time.png}}
%     \caption{Memory usage and execution time for tensor decomposition of 4D MRI data of size (384, 384, 28, 7). \os{(Can we increase the font size of the axis labels?)}}
%     \label{fig:mri_tensor_decomposition}
% \end{figure}
Besides, real-world tensors in medical imaging, often contain noise or artifacts that reduce performance and increase computational requirements for robust solutions. Performance reduction when adding noise has been reported in different methods using CP decomposition \cite{zhao2023robust}, Tucker decomposition \cite{prevost2020hyperspectral}, t-SVD \cite{liu2025dynamic}.

Applications of TD methods in tissue and brain imaging also leads to significant challenges, particularly, in the context of high-speed multiphoton microscopy and large-scale neural recordings. These challenges stem from the high-dimensional and multi-modal nature of the data acquired from multiphoton imaging techniques, such as second and third harmonic generation microscopy, two-photon and three-photon imaging, and functional calcium imaging. These datasets are inherently large, spanning spatial, temporal, and spectral domains, making tensor-based approaches computationally demanding~\cite{kolda2009tensor}. Another critical challenge is motion artifacts and signal contamination in live brain imaging. In awake behaving animals, head movements, heartbeat, and breathing introduce artifacts in calcium imaging and optogenetic recordings, complicating the extraction of meaningful neural signals. While tensor-based motion correction techniques have shown promise, they often require large computational resources and can introduce biases if the decomposition fails to properly separate artifacts from genuine neural activity~\cite{kara2024facilitating}. 
% Additionally, rank selection and model generalization remain significant issues, particularly for dynamic imaging applications. Selecting the optimal rank for CP, Tucker, or t-SVD decomposition in functional brain imaging is challenging, as an improper choice can either oversimplify neural activity patterns or introduce excessive computational burden. The variability in neural signals across subjects and experimental conditions further complicates the application of a single decomposition model, necessitating adaptive approaches that can generalize across different imaging modalities and experimental paradigms. \os{(Shehab: Does this paragraph need some citations?)}
Furthermore, biological noise and tissue heterogeneity present difficulties in tensor-based models applied to tissue imaging. In label-free multiphoton microscopy, where contrast arises from intrinsic tissue properties rather than fluorescent markers, non-uniform signal intensities, variations in optical scattering, and background noise can degrade the performance of decomposition techniques~\cite{borile2021label}. Robust pre-processing and artifact rejection methods are essential to improve the reliability of tensor-based segmentation, classification, and feature extraction in histopathological and live-tissue imaging. Finally, hardware limitations pose another barrier. While parallel computing and GPU acceleration have improved computational feasibility, real-time applications, such as closed-loop optogenetics or high-speed volumetric imaging, require further advancements in tensor decomposition algorithms to achieve near-instantaneous data processing and decision-making. Addressing these challenges will be crucial for fully harnessing tensor decomposition in large-scale tissue and brain imaging studies.

\subsubsection{Multi-omics Analysis}
Multi-omics data analysis presents several challenges, particularly in handling, integration, and interpretation. One of the major difficulties lies in the computational complexities associated with large-scale multi-omics datasets. These datasets are often high-dimensional due to the large number of features across multi-omics layers. Finding an optimal rank in this case is equally challenging as with imaging data. 
To illustrate these computational challenges, we performed Tucker decomposition on a multi-omics dataset combining autosomal single nucleotide polymorphisms and metabolites from germline PTEN Hamartoma Tumor Syndrome carriers with a tensor size of (264, 10000, 100), where the dimensions represent the number of samples (264), genomic features (10,000), and metabolomic features (100). Figure~\ref{fig:hardness} C shows memory requirements (in Megabytes) as the decomposition rank varies from 5 to 100, revealing a constant trend before exponentially increasing after rank 75. The second plot (Figure~\ref{fig:hardness}D) illustrates wall-clock time (in seconds), which steadily rises with increasing rank of the Tucker decomposition. These results highlight a critical trade-off between rank selection and computational efficiency, emphasizing the need for optimized feature selection strategies to balance computational feasibility and biological relevance when applying tensor decomposition to multi-omics data. 

Another key challenge is the heterogeneity of multi-omics data. These datasets comprise diverse data types, including continuous, binary, and categorical variables, complicating integration efforts. This heterogeneity complicates the integration process, as traditional tensor decomposition methods such as Tucker and CANDECOMP / PARAFAC (CP) may not adequately model complex interactions and different data types~\cite{xu2015bayesian}. Beyond computational and integration challenges, interpreting the latent factors extracted from tensor decomposition in a biologically relevant context can be challenging. Moreover, multi-omics data are often noisy and contain outliers. This can distort the results of the tensor decomposition. However, there are new methods such as \texttt{SCOIT}~\cite{10.1093/nar/gkad570} which attempt to address this challenge by incorporating various distributions to model noise and sparsity in the data. Finally, multi-omics data often contain missing values or sparse entries, which can significantly impact the performance of tensor decomposition methods. Several traditional approaches require complete data or extensive preprocessing to handle missing data, which can be time-consuming and may introduce biases \cite{xu2015bayesian, taguchi2021tensor}. While tensor decomposition provides a powerful framework for multi-omics integration,  overcoming computational constraints, handling data heterogeneity, improving interpretability, mitigating noise, and addressing missing data challenges remain critical areas for future research and methodological advancements.

% \begin{figure}[ht]
%     \centering
%     \subfloat{\includegraphics[width=0.45\textwidth]{figures/memory_usage.png}}
%     \hspace{0.5cm} % Adds some spacing between images
%     \subfloat{\includegraphics[width=0.45\textwidth]{figures/execution_time.png}}
%     \caption{Memory usage and execution time for tensor decomposition for random dataset. \os{(Shehab: Can we add more details? How the dataset was generated?)}}
%     \label{fig:tensor_decomposition}
% \end{figure}

\subsubsection{Spatial Transcriptomics}
Despite the advantages of tensor decomposition in spatial transcriptomics, several challenges remain in its application to high-dimensional and spatially heterogeneous datasets. One key challenge is the complexity of spatial dependencies, as gene expression varies not only across different tissue regions but also within microenvironments, making it difficult to define an optimal tensor structure that captures both local and global expression patterns. Traditional tensor decomposition methods often assume a predefined rank or spatial resolution, which may not be suitable for highly dynamic and heterogeneous tissue architectures. Additionally, sparsity in spatial transcriptomic data—where many genes are not detected in all spatial locations—limits the accuracy of decomposition models and can lead to biased reconstructions, especially when missing data is not randomly distributed but influenced by biological or technical factors. Another challenge is computational scalability, as large-scale spatial transcriptomics datasets with thousands of genes and spatial positions require substantial memory and processing power, making traditional tensor-based approaches computationally expensive. Furthermore, biological interpretability of tensor decomposition outputs remains a challenge, as extracted latent components may not always correspond to clear biological pathways or cell-type-specific interactions, necessitating additional validation with external datasets or functional assays. Finally, integration of multimodal spatial omics data poses a challenge, as combining spatial transcriptomics with proteomics or epigenomic data using tensor methods requires designing decomposition strategies that can accommodate different data modalities, resolutions, and measurement biases. Addressing these challenges will be crucial for unlocking the full potential of tensor decomposition in spatial transcriptomics and improving its utility for biological discovery.

