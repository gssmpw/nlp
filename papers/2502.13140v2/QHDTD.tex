\section{Quantum Algorithm for Tensor Decomposition}~\label{sec:td_quantum} 
Quantum computing leverages principles like superposition, entanglement, measurement, and destructive interference among solution probabilities to perform computational tasks more efficiently than classical computing (See Section 2 of \cite{basu2023towards} for a short primer on quantum computing and \cite{nielsen2010quantum} for a detailed introduction). In the past decade, quantum algorithms for matrix and tensor decomposition have emerged as powerful tools with significant applications in machine learning, data compression, quantum chemistry, and more. A notable advancement is the quantum singular value decomposition (QSVD), which expresses a matrix $A$ as $A = U \Sigma V^\dagger$, where $U$ and $V$ are unitary matrices and $\Sigma$ is a diagonal matrix of singular values. A quantum algorithm for the singular value decomposition of nonsparse low-rank matrices, providing exponential speedup over classical algorithms under certain conditions was proposed by Rebenstrot \textit{et al.}~\cite{rebentrost2018quantum}. This algorithm employs quantum state preparation techniques and Quantum Phase Estimation (QPE) to efficiently extract singular values and corresponding singular vectors.

Building on this, variational quantum singular value decomposition (VQSVD) algorithm was introduced~\cite{wang2021variational}. Designed for PFTQD, VQSVD uses a hybrid quantum-classical optimization loop to approximate singular values and vectors by minimizing a cost function. This makes it practical for current quantum hardware and effective for matrices where only a few singular values are significant.
Another significant development is quantum principal component analysis (QPCA)~\cite{lloyd2014quantum} introduced a quantum algorithm for PCA that can extract principal components exponentially faster than classical algorithms under certain conditions . The QPCA algorithm uses quantum algorithms for Hamiltonian simulation and phase estimation to find the eigenvalues and eigenvectors of the covariance matrix, processing large datasets encoded in quantum states and making it promising for big data analysis. Foundational to many quantum linear algebra algorithms is the quantum algorithm for solving linear systems of equations developed by Harrow, Hassidim, and Lloyd~\cite{harrow2009quantum}, also known as the HHL algorithm. The HHL algorithm solves systems of the form $Ax = b$ in logarithmic time relative to the size of the system under conditions such as sparsity and a low condition number of matrix $A$. It is crucial for quantum matrix computations, including inversions and decompositions, with applications spanning machine learning and optimization.

Extending these concepts to tensors, which generalize matrices to higher dimensions, offers significant potential in processing multidimensional data. Hastings~\cite{hastings2020classical} introduced classical and quantum algorithms for tensor principal component analysis (Tensor PCA), providing insights into the computational complexity of the problem and demonstrating potential quantum advantages for the problem of spiked tensor decomposition (Equation~\ref{eq:st}). The quantum algorithm presented in \cite{hastings2020classical} proves a quartic speedup over the best classical spectral algorithm. This quantum algorithm takes advantage of QPE and amplitude amplification, coupled with a suitable choice of the input state initialization to achieve the speedup. The quantum Hamiltonian constructed from the initial tensor $T_0$ operates on a set of $n_{bos}$ qudits of dimension $N$. More precisely, for a given tensor $T$ of order $p$ and dimension $N$, the Hamiltonian is a linear operator on the vector space $(\mathbb{R}^N)^{\otimes n_{bos}}$ or $(\mathbb{C}^N)^{\otimes n_{bos}}$ where $n_{bos} \geq \frac{p}{2}$. The bosonic Hamiltonian on the full Hilbert space has basis elements of the form $|\mu_1\rangle \otimes |\mu_2\rangle \otimes \dots \otimes |\mu_{n_{bos}} \rangle$ where each $\mu_i \in \{0, 1, \dots , N-1\}$ and can be written as,

\begin{equation}\label{eq:hamiltonian}
H(T) = \frac{1}{2} \sum_{i_1, \dots , i_{p/2}} \left( \sum_{\mu_1, \dots , \mu_p} T_{\mu_1, \mu_2, \dots, \mu_p} |\mu_1\rangle _{i_1} \langle \mu_{1+p/2} |\otimes |\mu_2\rangle _{i_2} \langle \mu_{2+p/2} |\otimes \cdots \otimes |\mu_{p/2}\rangle _{i_{p/2}} \langle \mu_{p}| + h.c.  \right)
\end{equation}

where the first sum iterates over unique set of qudits $i_1, i_2, \dots, i_p$, $T_{\mu_1, \mu_2, \dots, \mu_p}$ are the corresponding elements from the tensor $T$ and $h.c.$ refers to adding Hermitian conjugates for the terms. Outer products $|\mu_n\rangle _{k} \langle\mu_{m}|$ are performed on the corresponding qudit $k$. Then the quantum algorithm in \cite{hastings2020classical} recovers the leading eigenvalue and the corresponding eigenvector, assuming the detection is successful, and outputs a vector that has large normalized overlap with the signal vector $v_{sig}$. 

The Hamiltonian $H(T_0)$, inherits its spectral properties, which in turn dictate the sample complexity of the quantum algorithm. A key factor is the spectral gap $\Delta$, which determines how well the ground state can be distinguished from excited states. Since the largest eigenvalue of $H(T_0)$ is driven by the signal term $\lambda v_{\text{sig}}^{\otimes p}$, a higher SNR (larger $\lambda$) results in a larger spectral gap, making it easier to resolve the ground state with quantum phase estimation and reducing sample complexity, which scales as $\mathcal{O}(1/\Delta^2)$. However, when the SNR is low, the signal eigenvalue becomes buried in the noise spectrum, causing the spectral gap to shrink and significantly increasing the number of measurements required. Additionally, the operator norm $\|\hat{H}\|$, which grows with the tensor order $p$ and dimension $N$, influences the spread of energy levels and dictates the precision needed in phase estimation, affecting the total measurement overhead. While the quantum algorithm achieves a quartic speedup over classical methods, its efficiency is still constrained by the interplay between SNR, tensor order, and spectral properties - if the spectral gap is too small due to low SNR or high $p$, the measurement complexity increases, limiting the quantum advantage.

In QPE, ancilla qubits store the binary representation of the estimated eigenvalue, determining the precision of the approximation. It also determines the depth of the quantum circuit. Quantum Amplitude Amplification (the second step in Hastings' algorithm~\cite{hastings2020classical}) improves the success probability of measuring the correct eigenvalue but does not alter the role or requirement of ancilla qubits in QPE. The number of ancilla qubits, as well as the number of times the controlled unitary is applied to the main register, is dictated by the precision requirement of the problem. Specifically, to achieve 2, 4, 6, and 8 decimal digit precision, the required number of ancilla qubits is 7, 14, 20, and 27, respectively. It also means that the controlled unitary on the main register will be applied $\sim 2$, $\sim2^3$, $\sim 2^5$, and $\sim 2^7$  times respectively. Such depth makes an underlying quantum error correcting layer necessary for the algorithm. In practice, when applied on biomedical data, up to four decimal digit precision is often sufficient to qualitatively evaluate tasks such as clustering, prediction, classification, or feature extraction. Hence, in realistic terms, a QPE when applied on real-world data would approximately require upto 14 ancilla qubits. However, this estimate is dependent on other factors such as number of qubits, size and order of the tensor, etc.


The quantum algorithmic resources needed to implement this algorithm is high due to QPE yielding high circuit depths, however a more recent work \cite{zhou2024statistical} provides numerical estimations and analyzes the asymptotic behavior of this model using a $p$-step quantum approximate optimization algorithm (QAOA), making it more amenable in near-term utility-scale quantum computers. 


While explicit quantum algorithms for CP (CANDECOMP/PARAFAC) and Tucker decompositions are still under development, the principles of quantum linear algebra can be extended to these tensor factorizations. This extension potentially offers speedups in processing high-dimensional data, which is crucial in fields like computer vision, chemometrics, neuroscience, and quantum chemistry. As quantum computing technology advances, these algorithms pave the way for more efficient data processing techniques and inspire further research into quantum algorithms for complex data structures.

