\section{Related Work}
\label{sec:related}


\parabf{Cold-start optimizations in serverless model serving.}
There have been plenty of works on reducing the cold start latency in serverless model serving____.
For instance, FaaSwap{faaswap} and ServerlessLLM____ utilize locality by caching models in local memory or SSDs,
while INFless____ prewarms instances based on historical request patterns.
gCROP____ introduces a on-demand GPU image restore technique that accelerates GPU runtime initialization.
Unlike these works, \sysname primarily targets reducing model fetching latency by aggregating network bandwidth of GPU servers.

\parabf{Pipeline parallelism.}
Pipeline parallelism has been widely used to scale GPU resources for training large models____.
Recently, researchers have also explored its use during inference____.
AlpaServe____ observes that model parallelism improves GPU utilization under bursty workloads and introduces a model placement policy when deploying models.
HPipe____ and PipeEdge____ apply pipeline parallelism to improve the inference performance for LLMs on edge devices.
PipeInfer____ explores speculative inference through asynchronous pipeline parallelism to reduce inference latency.
\sysname leverages pipeline parallelism to reduce cold start latency in serverless LLM serving and
further introduces pipeline consolidation, which dynamically scales works in a pipeline parallelism group to improve performance.

\parabf{LLM serving optimizations.}
Many LLM serving optimizations have been proposed to improve serving performance____.
In the area of request scheduling, Orca____ introduces iteration-level scheduling to run more requests in parallel,
while Llumnix____ employs runtime scheduling to meet user SLOs.
FastServe____ focuses on average job completion time and introduces a preemptive scheduling policy.
For key-value cache management,
vLLM____ leverages the concept of virtual memory in operating systems and InfiniGen____ optimizes KV block placement to improve inference efficiency.
Additionally, to mitigate the interference between prefill and decoding phases, DistServe____ proposes disaggregated inference,
whereas SARATHI____ adopts chunked prefill.
\sysname specifically focuses on reducing cold start latency in serverless LLM serving.
Our techniques are orthogonal to existing inference optimizations and we can seamlessly integrate new optimizations into \sysname.