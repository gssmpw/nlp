@inproceedings{asyfunc,
author = {Pei, Qiangyu and Yuan, Yongjie and Hu, Haichuan and Chen, Qiong and Liu, Fangming},
title = {AsyFunc: A High-Performance and Resource-Efficient Serverless Inference System via Asymmetric Functions},
year = {2023},
booktitle = socc,
}

@article{faaswap,
  title={FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model Swapping}, 
  author={Minchen Yu and Ao Wang and Dong Chen and Haoxuan Yu and Xiaonan Luo and Zhuohao Li and Wei Wang and Ruichuan Chen and Dapeng Nie and Haoran Yang},
  journal={arXiv preprint arXiv:2306.03622},
  year={2024}
}

@article{fastserve,
  title={Fast Distributed Inference Serving for Large Language Models}, 
  author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Shengyu Liu and Fangyue Liu and Yuanhang Sun and Gang Huang and Xuanzhe Liu and Xin Jin},
  journal={arXiv preprint arXiv:2305.05920},
  year={2023}
}

@inproceedings{gcrop,
author = {Yang, Yanning and Du, Dong and Song, Haitao and Xia, Yubin},
title = {On-demand and Parallel Checkpoint/Restore for GPU Applications},
year = {2024},
booktitle = socc,
}

@inproceedings{hpipe,
    title = {HPipe: Large Language Model Pipeline Parallelism for Long Context on Heterogeneous Cost-effective Devices},
    author = {Ma, Ruilong  and
      Yang, Xiang  and
      Wang, Jingyu  and
      Qi, Qi  and
      Sun, Haifeng  and
      Wang, Jing  and
      Zhuang, Zirui  and
      Liao, Jianxin},
    booktitle = acl,
    year = {2024},
}

@inproceedings{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  booktitle=nips,
  year={2019}
}

@inproceedings{infless,
author = {Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu},
title = {INFless: a native serverless system for low-latency, high-throughput inference},
year = {2022},
booktitle = asplos,
}

@inproceedings{li2023alpaserve,
  title={$\{$AlpaServe$\}$: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
  author={Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others},
  booktitle=osdi,
  year={2023}
}

@inproceedings{loongserve,
author = {Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
title = {LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism},
year = {2024},
booktitle = sosp,
}

@inproceedings{narayanan2019pipedream,
  title={{PipeDream}: generalized pipeline parallelism for {DNN} training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle=sosp,
  year={2019}
}

@inproceedings{nips18_pipesgd,
author = {Li, Youjie and Yu, Mingchao and Li, Songze and Avestimehr, Salman and Kim, Nam Sung and Schwing, Alexander},
title = {Pipe-SGD: a decentralized pipelined SGD framework for distributed deep net training},
year = {2018},
booktitle = nips,
}

@inproceedings{nips22_sapipe,
author = {Chen, Yangrui and Xie, Cong and Ma, Meng and Gu, Juncheng and Peng, Yanghua and Lin, Haibin and Wu, Chuan and Zhu, Yibo},
title = {SAPipe: staleness-aware pipeline for data-parallel DNN training},
year = {2024},
booktitle = nips,
}

@inproceedings{orca,
  title = {Orca: A distributed serving system for Transformer-Based generative models},
  author = {Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle = osdi,
  pages = {521--538},
  year = {2022}
}

@INPROCEEDINGS{pipeedge,
  author={Hu, Yang and Imes, Connor and Zhao, Xuanang and Kundu, Souvik and Beerel, Peter A. and Crago, Stephen P. and Walters, John Paul},
  booktitle={25th Euromicro Conference on Digital System Design (DSD)}, 
  title={PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on Heterogeneous Edge Devices}, 
  year={2022},
}

@inproceedings{pipeinfer,
author = {Butler, Branden and Yu, Sixing and Mazaheri, Arya and Jannesari, Ali},
title = {PipeInfer: Accelerating LLM Inference using Asynchronous Pipelined Speculation},
year = {2024},
booktitle = sc,
}

@article{sarathi,
  title={SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills}, 
  author={Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee},
  journal={arXiv preprint arXiv:2308.16369},
  year={2023}
}

@inproceedings{terapipe,
  title={TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models},
  author={Zhuohan Li and Siyuan Zhuang and Shiyuan Guo and Danyang Zhuo and Hao Zhang and Dawn Xiaodong Song and Ion Stoica},
  booktitle=icml,
  year={2021},
}

@inproceedings{vllm,
  title = {Efficient memory management for large language model serving with pagedattention},
  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle = sosp,
  year = {2023}
}

