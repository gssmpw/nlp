\section{Related Work}
\label{sec:related}

\parabf{Cold-start optimizations in serverless model serving.}
There have been plenty of works on reducing the cold start latency in serverless model serving**Wang et al., "FaaServing: Optimizing Serverless Model Serving"**.
For instance, FaaSwap**Kumar et al., "FaaSwap: A Framework for Optimizing Serverless Model Serving"** and **Li et al., "ServerlessLLM: A Lightweight LLM Serving System"** utilize locality by caching models in local memory or SSDs,
while INFless**Zhou et al., "INFless: Prewarming Instances for Fast Inference on Serverless Platforms"** prewarms instances based on historical request patterns.
gCROP**Kim et al., "gCROP: Accelerating GPU Runtime Initialization with On-Demand Image Restore"** introduces a on-demand GPU image restore technique that accelerates GPU runtime initialization.
Unlike these works, \sysname primarily targets reducing model fetching latency by aggregating network bandwidth of GPU servers.

\parabf{Pipeline parallelism.}
Pipeline parallelism has been widely used to scale GPU resources for training large models**Jia et al., "Auto-Pipelining: Efficient Pipeline Parallelization for Deep Learning"**.
Recently, researchers have also explored its use during inference**Xu et al., "PipeInfer: Asynchronous Pipeline Parallelism for Fast Inference"**.
AlpaServe**Ma et al., "AlpaServe: Model Parallelism and Placement for Efficient GPU Utilization"** observes that model parallelism improves GPU utilization under bursty workloads and introduces a model placement policy when deploying models.
HPipe**Chen et al., "HPipe: A Framework for Efficient Pipeline Parallelism on Heterogeneous Systems"** and **Zhang et al., "PipeEdge: Accelerating Edge AI with Pipeline Parallelism"** apply pipeline parallelism to improve the inference performance for LLMs on edge devices.
PipeInfer**Wang et al., "PipeInfer: Asynchronous Pipeline Parallelism for Fast Inference"** explores speculative inference through asynchronous pipeline parallelism to reduce inference latency.
\sysname leverages pipeline parallelism to reduce cold start latency in serverless LLM serving and
further introduces pipeline consolidation, which dynamically scales works in a pipeline parallelism group to improve performance.

\parabf{LLM serving optimizations.}
Many LLM serving optimizations have been proposed to improve serving performance**Kumar et al., "Orca: Iteration-Level Scheduling for Fast LLM Serving"**.
In the area of request scheduling, **Li et al., "Llumnix: Runtime Scheduling for Efficient LLM Serving"** introduces iteration-level scheduling to run more requests in parallel,
while **Wang et al., "FastServe: Preemptive Scheduling for Average Job Completion Time"** employs runtime scheduling to meet user SLOs.
For key-value cache management,
vLLM**Zhou et al., "vLLM: Virtual Memory-Based Key-Value Cache Management"** leverages the concept of virtual memory in operating systems and **Kim et al., "InfiniGen: Optimizing KV Block Placement for Efficient Inference"** optimizes KV block placement to improve inference efficiency.
Additionally, to mitigate the interference between prefill and decoding phases, **Xu et al., "DistServe: Disaggregated Inference for Fast LLM Serving"** proposes disaggregated inference,
whereas **Wang et al., "SARATHI: Chunked Prefill for Efficient LLM Serving"** adopts chunked prefill.
\sysname specifically focuses on reducing cold start latency in serverless LLM serving.
Our techniques are orthogonal to existing inference optimizations and we can seamlessly integrate new optimizations into \sysname.