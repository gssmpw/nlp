\section{Related Work}
\label{sec:related}


\parabf{Cold-start optimizations in serverless model serving.}
There have been plenty of works on reducing the cold start latency in serverless model serving~\cite{infaas, infless, asyfunc, faaswap, serverlessllm}.
For instance, FaaSwap{faaswap} and ServerlessLLM~\cite{serverlessllm} utilize locality by caching models in local memory or SSDs,
while INFless~\cite{infless} prewarms instances based on historical request patterns.
gCROP~\cite{gcrop} introduces a on-demand GPU image restore technique that accelerates GPU runtime initialization.
Unlike these works, \sysname primarily targets reducing model fetching latency by aggregating network bandwidth of GPU servers.

\parabf{Pipeline parallelism.}
Pipeline parallelism has been widely used to scale GPU resources for training large models~\cite{huang2019gpipe, terapipe, narayanan2019pipedream, nips18_pipesgd, nips22_sapipe, megascale}.
Recently, researchers have also explored its use during inference~\cite{li2023alpaserve, hpipe, pipeinfer, pipeedge}.
AlpaServe~\cite{li2023alpaserve} observes that model parallelism improves GPU utilization under bursty workloads and introduces a model placement policy when deploying models.
HPipe~\cite{hpipe} and PipeEdge~\cite{pipeedge} apply pipeline parallelism to improve the inference performance for LLMs on edge devices.
PipeInfer~\cite{pipeinfer} explores speculative inference through asynchronous pipeline parallelism to reduce inference latency.
\sysname leverages pipeline parallelism to reduce cold start latency in serverless LLM serving and
further introduces pipeline consolidation, which dynamically scales works in a pipeline parallelism group to improve performance.

\parabf{LLM serving optimizations.}
Many LLM serving optimizations have been proposed to improve serving performance~\cite{li2023alpaserve, fastserve, loongserve, distserve, orca, vllm, infinigen, llumnix, sarathi, serverlessllm}.
In the area of request scheduling, Orca~\cite{orca} introduces iteration-level scheduling to run more requests in parallel,
while Llumnix~\cite{llumnix} employs runtime scheduling to meet user SLOs.
FastServe~\cite{fastserve} focuses on average job completion time and introduces a preemptive scheduling policy.
For key-value cache management,
vLLM~\cite{vllm} leverages the concept of virtual memory in operating systems and InfiniGen~\cite{infinigen} optimizes KV block placement to improve inference efficiency.
Additionally, to mitigate the interference between prefill and decoding phases, DistServe~\cite{distserve} proposes disaggregated inference,
whereas SARATHI~\cite{sarathi} adopts chunked prefill.
\sysname specifically focuses on reducing cold start latency in serverless LLM serving.
Our techniques are orthogonal to existing inference optimizations and we can seamlessly integrate new optimizations into \sysname.