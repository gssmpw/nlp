
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk} 

\title{Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \textsuperscript{1,2,3} 

\iffalse
\author{
\textbf{Jiangrong Shen \dag \ddag \S \ss  \quad Qi Xu \P  \thanks{Accepted by ICLR 2025. The corresponding author: xuqi@dlut.edu.cn.} \quad Gang Pan \S \quad Badong Chen \ddag \ss} \\
\dag Faculty of Electronic and Information Engineering,
Xi’an Jiaotong University \\
\ddag Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University \\
\S State Key Lab of Brain-Machine Intelligence, Zhejiang University \\
\P School of Computer Science, Dalian University of Technology \\
\ss National Key Lab of Human-Machine Hybrid Augmented Intelligence, Xi’an Jiaotong University
}

\fi









\author{
  Jiangrong Shen\textsuperscript{1,2,3,4}, 
  Qi Xu \textsuperscript{5*}, 
  Gang Pan\textsuperscript{3},
  Badong Chen\textsuperscript{2,4}
}


% 自定义单位显示（通过重定义\maketitle）
\makeatletter
\renewcommand{\@maketitle}{%
  \newpage
  \null
  \vskip 2em%
  \begin{center}%
    {\LARGE \@title \par}%
    \vskip 1.5em%
    {\large
     \lineskip .5em%
     \begin{tabular}[t]{c}
       \@author
     \end{tabular}\par}%
    \vskip 0.5em% 压缩作者与单位间距
    {\footnotesize
     \begin{tabular}[t]{c}
       \textsuperscript{1}Faculty of Electronic and Information Engineering,
Xi’an Jiaotong University \\
       \textsuperscript{2}Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University \\
       \textsuperscript{3}State Key Lab of Brain-Machine Intelligence, Zhejiang University \\
       \textsuperscript{4}National Key Lab of Human-Machine Hybrid Augmented Intelligence, Xi’an Jiaotong University  \\
       \textsuperscript{5}School of Computer Science, Dalian University of Technology \\
       \textsuperscript{\P}Accepted by ICLR. The corresponding author: xuqi@dlut.edu.cn.
     \end{tabular}\par}%
  \end{center}%
  \vskip 1em%
}
\makeatother

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}


The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning.
In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. 
The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency.
Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware.


%Our experiments demonstrate that this strategy not only matches the performance of existing models but also enhances the efficiency of compressing sparse SNNs. Importantly, it retains the benefits of sparse training from the onset and offers a promising solution for implementing Edge AI on neuromorphic hardware.

%The first stage assesses the compressibility of existing sparse subnetworks in SNNs using the PQ index, allowing for an adaptive determination of the rewiring ratio for synaptic connections in each iteration based on data compression metrics. The second stage leverages this rewiring ratio to guide dynamic synaptic connection rewiring, which encompasses both pruning and regrowing of connections. 

%Human brain employs spikes to transmit information and is capable of reorganizing the networks structure to enhance both energy efficiency and cognitive capability over a lifetime.
%Inspired by that spike-based computation, Spiking neural networks (SNNs) are proposed to build event-driven models for energy efficiency.
%However, deep SNN models are still hampered by over-parameterization during both training and inference, 
%far away from the self-reorganizing in brain. Additionally, current sparse SNNs struggle with maintaining optimal pruning levels due to a fixed pruning ratio, which either leads to insufficient or excessive pruning.
%In this paper, we introduce a novel two-stage dynamic structure learning approach for deep SNNs, aimed at optimizing compression efficiency while maintaining effective sparse training from scratch. The first stage assesses the compressibility of existing sparse subnetworks in SNNs using the PQ index, allowing for an adaptive determination of the rewiring ratio for synaptic connections in each iteration based on data compression metrics. The second stage leverages this rewiring ratio to guide dynamic synaptic connection rewiring, which encompasses both pruning and regrowing of connections. This approach significantly improves the exploration of sparse structures in deep SNNs, adapting sparsity dynamically.
%Our experiments demonstrate that this strategy not only matches the performance of existing models but also enhances the efficiency of compressing sparse SNNs. Importantly, it retains the benefits of sparse training from the onset and offers a promising solution for implementing Edge AI on neuromorphic hardware.



%posing challenges for on-chip learning in neuromorphic hardware


%The network structure in human brain enables to reorganize itself for enhanced energy efficiency and cognitive capability throughout the lifetime. Spiking neural networks (SNNs), inspired by the neuronal computation mechanism in brain, leverage event-driven properties to achieve energy efficiency. 


%However, deep SNN models often suffer from over-parameterization during both training and inference, hindering the implementation of on-chip learning in neuromorphic hardware. 
%Additionally, 
%current sparse SNNs face challenges of either insufficient or excessive pruning due to a fixed pruning ratio during the sparse structure learning process. 
%We focus on implementing the fully sparse structure learning with dynamic pruning ratio from the point view of network compression.  
%In this paper, we present a novel two-stage dynamic structure learning approach for deep SNNs, to improves compression efficiency and maintain sparse training from scratch. The first stage involves evaluating the compressibility of existing sparse subnetworks in SNNs using the PQ index. This assessment enables to determine the rewiring ratio for synaptic connections in each iteration adaptively from the data compression view. During the second stage, the rewiring ratio plays a crucial role in guiding the process of dynamic synaptic connection rewiring, which includes both connections pruning and regrowing. This process ultimately enhances the efficiency of sparse structure exploring for deep SNNs with adaptive sparsity. The iterative learning between these two stages is employed consistently throughout the whole training period. The experimental results show that our strategy not only achieves competitive performance compared to existing models but also improves the efficiency of compressing sparse SNNs from the compression view. Significantly, our approach compresses the sparse SNNs effectively while retaining the advantages of sparse training from scratch, which provides one possible solution to the implement Edge AI with neuromorphic hardware.

%This paper highlights the increasing significance of sparse training of spiking neural networks (SNNs) from scratch, motivated by brain's ability to reorganize itself for enhanced energy efficiency and cognitive capability, as well as the requirements of limited neuromorphic hardware resources. The work addresses the problem of over-parameterization in deep SNN models during the entire training and inference process, in the manner of sparse structure learning. Meanwhile, this study attempts to avoid either insufficient pruning or excessive pruning caused by the constant pruning ratio in sparse structure learning for SNNs.



\end{abstract}


\section{Introduction}

Spiking Neural Networks (SNNs) have garnered increasing attention due to their event-driven properties, high spatiotemporal dynamics, and structural and learning plasticity that mimic biological neural processing \citep{maass1997networks,subbulakshmi2021biomimetic,spikejelly}. Unlike traditional artificial neural networks that rely on continuous signal computation, SNNs process information using discrete events, aligning more closely with the energy-efficient mechanisms observed in human neural activity. The post-synaptic neurons in SNNs receive spike trains from pre-synaptic neurons and emit output spikes upon crossing a firing threshold \citep{stanojevic2024high, zhou2023computational}. Consequently, bio-inspired SNNs offer significant advantages in energy efficiency, making them especially suitable for neuromorphic computing in Edge AI such as event-based vision \citep{liu2024optical, liu2024line,liu2025stereo}, where energy constraints are paramount \citep{imam2020rapid, pei2019towards, deng2021comprehensive}.
Despite these inherent advantages, the deployment of increasingly deep SNNs introduces substantial challenges, particularly over-parameterization during training and inference, leading to excessive computational overhead and memory usage. This misalignment with the resource-efficient requirements of edge devices calls for innovative solutions.

%Spiking Neural Networks (SNNs) have increasingly attracted attentions for their event-driven property, highly spatiotemporal dynamics, and richness structural and learning plasticity by mimicking biological neural processing \citep{maass1997networks,subbulakshmi2021biomimetic,spikejelly}.
%Different from the continuous signal computation in traditional artificial neural networks, SNNs employ discrete events (spike trains) for information processing, aligning more closely with the energy-efficient mechanisms observed in human neural activity \citep{stanojevic2024high, zhou2023computational}.
%The post-synaptic neurons in SNNs receive the spike trains from the connected pre-synaptic neurons and emit spike trains when their membrane potential crosses the firing threshold.
%Thus, the bio-inspired SNNs have significant advantages in terms of energy efficiency, making them particularly suitable for neuromorphic computing applications on Edge AI where energy constraints are a critical factor \citep{imam2020rapid, pei2019towards, deng2021comprehensive}. 
%Despite these inherent advantages, the deployment of deeper and deeper SNNs also brings substantial challenges, especially the over-parameterization problem during both training and inference processes. The over-parameterization could result in considerable computational overhead and excessive memory usage, which does not align well with the resource-efficient requirement of edge devices.

Current research on the sparse structure learning of deep SNNs aims to address the over-parameterization issue. These methodologies are predominantly categorized by their computational cost throughout the whole training process. The first one is the gradually structural sparsification approach with the non-sparse network as the initial status. For instance, the gradient reparameterization \citep{chen2021pruning, chen2022state} approach implements the gradually efficient sparsification for deep SNNs with learnable pruning speed by redefining the weight parameters and threshold growing function. 
Alternatively, the second category called fully sparsification methods initiate with sparse SNNs to maintain connection sparsity throughout training, exemplified by sparse evolutionary rewiring \citep{shen2023esl} and the lottery-ticket hypothesis \citep{kim2022exploring}. We advocate for the latter due to its compatibility with hardware constraints like on-chip training. {\color{black}{Considering the intrinsic connection between network sparsity and compressibility, the PQ index has been proven to satisfy the six properties of an ideal sparsity measure \citep{hurley2009comparing}, and employed as the indicator of vector sparsity in the traditional artificial neural networks in \citep{diao2023pruning}. However, that sparsity measure analysis ignores the unique spatial and temporal dynamics in SNNs. 
Therfore, most existing fully sparse training methods for SNNs employ static pruning ratios or predetermined sparsity levels, ignoring the analysis of spatiotemporal dynamics of SNNs and lacking the necessary flexibility like self-reorganizing in human brain and often leading to under or over-pruning. }}

%One approach involves gradually sparsifying a non-sparse network, such as the gradient reparameterization methods \citep{chen2021pruning, chen2022state}, which allows for adjustable pruning speeds by redefining weight parameters and threshold functions. Alternatively, fully sparsification methods initiate with sparse SNNs to maintain connection sparsity throughout training, exemplified by sparse evolutionary rewiring and the lottery-ticket hypothesis. We advocate for the latter due to its compatibility with hardware constraints like on-chip training.

%There have been some explorations about the structure learning method for deep SNNs to alleviate the over-parameterization problem. These studies are predominantly categorized into two different manners by the computation cost during the whole training process. The first one is the gradually structural sparsification approach with the non-sparse network as the initial status. For instance, the gradient reparameterization \citep{chen2021pruning, chen2022state} approach implements the gradually efficient sparsification for deep SNNs with learnable pruning speed by redefining the weight parameters and threshold growing function. The second category is the fully sparsification manner with initial sparse SNNs, which maintains connection sparseness during the training process. The sparse evolutionary rewiring method \citep{shen2023esl} and Lottery-ticket-hypothesis approach \citep{kim2022exploring} for SNNs could implement the entire network sparse training. We prefer the later fully sparsification manner because of the requirements of hardware constrains such as on-chip training.
%However, most of current fully sparse training methods employ static pruning ratios or predetermined sparsity levels, which lacks the necessary flexibility and may contribute to underpruning or over-pruning. 


Observing the brain’s flexible organization of large-scale functional networks, which adapt through environmental interactions, offers a clue towards solving deep SNNs' over-parameterization. During brain development, synaptic connections undergo structural plasticity, forming new synapses and eliminating existing ones \citep{de2017ultrastructural,barnes2010sensory,bennett2018rewiring}. This rewiring process forms flexible network structure and promotes synaptic sparsity contributing to the brain's low power consumption.
Therefore, emulating the brain's structural synaptic plasticity through a dynamic structure learning approach could be key to developing more flexible deep SNN models. 
Meanwhile, the density of synaptic connections in the brain optimizes throughout development, although precise control mechanisms remain unclear. Thus, we explore optimizing the pruning ratio from a neural network compression perspective as explored in machine learning, where data compression theory helps quantify the compressibility of a sub-network during each connection updating iteration, thereby avoiding under or over-pruning \citep{neill2020overview}.
By combining with the biologically plausible rewiring mechanism in human brain and neural network compression theory in machine learning, we attempt to give a solution about the sparse training method from scratch for deep SNNs with adaptive and suitable pruning ratio setting.





In light of these above insights, this paper proposes a novel two-stage sparse structure learning method from scratch for SNNs, utilizing the PQ index to measure appropriate compressibility. This method not only maintains sparse training throughout the learning process but also mitigates the issues of under-pruning and over-pruning in sparse SNNs. Our contributions are summarized as follows:

%It is worth noting that the brain achieves flexible organization of large-scale functional networks, which are adaptively adjusted through interactions with external environments \citep{tassi2023environmental, tooley2021environmental}. We attempt to find some inspiration from the brain to solve the over-parameterization problem in deep SNNs. But how does the brain implement complex network structure learning throughout a lifetime? During brain development, synaptic connections undergo structural plasticity, forming new synapses and eliminating existing ones \citep{de2017ultrastructural} \citep{barnes2010sensory} \citep{bennett2018rewiring}. This rewiring process promotes synaptic sparsity and contributes to the brain's low power consumption. Therefore, to generate more flexible SNNs models, adopting a dynamic structure learning approach inspired by the brain's synaptic plasticity could be the key.  Meanwhile, the complexity of brain networks is varying throughout the lifetime, especially the early development period. The density of synaptic connection keeps optimizing during the whole brain development process. However, there is no clear conclusion about how to control the precise connection density in brain. Thus we explore the pruning ratio optimizing from the neural network compression view in machine learning \citep{neill2020overview}. The data compression theory provides a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration, and thus avoids the underprune or over-prune the neural network model. By combining with the biologically plausible rewiring mechanism and neural network compression theory in machine learning, we attempt to give a solution about the sparse training method from scratch for SNNs with adaptive and suitable pruning ratio setting.




\begin{figure*}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figure/Fig_all_two_stage.pdf}
    \caption{The flowchart of the proposed two-stage sparse structure learning method for SNNs. Stage I involves the typical training process and attempts to identify an appropriate rewiring ratio according to PQ index. Stage II conducts the dynamic sparse structure learning method based on the rewiring ratio in stage I. The iterative learning of the above two stages is employed during the whole training process, thereby implementing the sparse training from scratch for SNNs and enhancing generalization ability of the sparse model. }
    \label{fig:method}
\end{figure*}

%Therefore, this paper proposes the two-stage sparse structure learning from scratch for SNNs, based on appropriate compressibility measured by PQ index. The proposed method not only provides sparse training during the whole learning process, but also avoids the under-prune or over-prune phenomenon for sparse SNNs. The contributions of this paper are summarized as follows:

\begin{itemize}
    \item We introduce a pioneering two-stage dynamic structure learning framework for deep SNNs that utilizes the PQ index to gauge and dynamically adjust structure of sparse subnetworks according to compressibility. This novel approach tailors the rewiring ratio throughout the training process, providing a fine-tuned, adaptive mechanism that enhances the foundational training dynamics of deep sparse SNNs. 
    
    %We introduce a novel two-stage dynamic structure learning framework that harnesses the PQ index to assess the compressibility of sparse subnetworks within SNNs. This method marks a significant advancement by implementing sparse training from scratch, utilizing a variable rewiring ratio derived from compression metrics to optimize the architecture of deep SNNs during training. This approach enables compressibility adaptation of network structure, mirroring biological efficiency in neural reconfiguration.
    %We introduce a groundbreaking two-stage dynamic structure learning method that initially assesses the compressibility of sparse subnetworks within SNNs using the PQ index. The proposed approach implements the sparse training from scratch with varing rewiring ratio obtained from the compression perspective for deep SNNs training.
    
   
 
    \item Our methodology extends traditional sparse training approaches for SNNs by implementing a continuous, iterative learning process across two stages. In the first stage, the PQ index informs the adjustment of synaptic connection rewiring ratios. In the second stage, these ratios guide a dynamic rewiring strategy that includes both the pruning and regrowth of connections. Thus the methodology optimizes the SNNs' structural efficiency and operational effectiveness far beyond conventional static pruning techniques.
    
    %Our approach extends beyond conventional methods by integrating continuous iterative learning throughout the entire training process, ensuring progressive refinement of the SNNs' structure. The first stage of our method assesses the compressibility of sparse SNNs' subnetworks using the PQ index, guiding the precise adjustment of the rewiring ratio for synaptic connections. In the subsequent stage, we apply a dynamic synaptic connection rewiring strategy that not only prunes but also regrows connections, based on the rewiring ratios from the first stage. This strategy significantly enhances both the efficiency and efficacy of sparse SNNs, offering a robust improvement over traditional fixed-pruning approaches.
    %Our method utilizes a continuous iterative learning across both stages throughout the entire training process, leading to more efficient compression of SNNs models. In the first stage, the compressibility of current sparse subnetworks is assessed using the PQ index, which then guides the determination of the rewiring ratio for synaptic connections in each iteration. In the second stage, we employ a dynamic synaptic connection rewiring strategy that includes both pruning and regrowing based on the rewiring ratios established in the first stage. This approach is designed to optimize the efficiency and effectiveness of sparse SNNs, improving upon traditional pruning methods.
    
    
   
  
    \item Through extensive empirical testing, our method not only achieves competitive performance relative to existing state-of-the-art models but also significantly enhances the efficiency of sparse training from scratch implementations for deep SNNs. This rigorous validation demonstrates our approach's ability to maintain essential SNNs' network functionality while reducing computational redundancy, thus achieving superior compression of SNN architectures.
    %The effectiveness of our method is substantiated through rigorous experimental validation, demonstrating that our model not only competes with existing state-of-the-art models but also excels in maintaining the integrity of sparse training from inception. This results in a highly efficient compression of SNNs, showcasing our approach's capability to preserve essential network features while minimizing redundancy, thus optimizing computational resources.
    %Experimental results demonstrate that our method not only achieves competitive performance among existing models but also maintains the benefits of sparse training from scratch, thereby effectively compressing sparse SNNs.
  
\end{itemize}


 %In the second stage, we employ a dynamic synaptic connection rewiring strategy that includes both pruning and regrowing based on the rewiring ratios established in the first stage. This approach is designed to optimize the efficiency and effectiveness of sparse SNNs, improving upon traditional pruning methods.

  %This index is not merely a measure but a guide that adaptively determines the rewiring ratio for synaptic connections based on real-time data compression metrics. This innovative approach allows for dynamic adjustments that closely mimic the brain's efficiency in reconfiguring neural pathways based on environmental stimuli.
    
    %We introduce a novel two-stage dynamic structure learning method for SNNs that enhances compression efficiency. In the first stage, the compressibility of current sparse subnetworks is assessed using the PQ index, which then guides the determination of the rewiring ratio for synaptic connections in each iteration.
 
   %We proposed the two-stage sparse structure learning method for SNNs. The iterative learning of the two stages training is employed during the whole training process, thereby implementing the sparse training from scratch for SNNs and enhancing generalization ability of the sparse model.
     %Stage I involves the typical training process and attempts to identify an appropriate rewiring ratio according to PQ index. Stage II conducts the dynamic sparse structure learning by iterative connection pruning and regrowing based on the rewiring ratio in Stage I. 
       %The analysis on the effectiveness of the proposed method is conducted to on layer-wise and neuron-wise scopes. The comparative experiments suggest that the proposed method compresses SNNs models more efficiently and achieves competitive performance among other SNNs.
   
%Such inefficiencies present formidable barriers to implementing these advanced models in environments where memory and processing resources are severely limited.


%Despite these inherent advantages, the deployment of deep SNNs  leads to substantial challenges, primarily due to over-parameterization \citep{chen2022state, deng2021comprehensive}. This condition results in considerable computational overhead and excessive memory usage, which does not align well with the compact, resource-efficient mandates of edge devices. As the architectural complexity of these networks increases, they are significantly magnified. Such inefficiencies present formidable barriers to implementing these advanced models in environments where memory and processing resources are severely limited.


%SNNs are theoretically ideal for low-power, high-efficiency contexts due to the above advantages, such as edge computing scenarios.

%The most commonly used method is the parameter pruning after the training process combined with the fine-tuning method. In this manner, the redundancy parameters of trained SNNs could be pruned without losing much performance. Furthermore, some researchers tend to pay close attention to learning the sparse structure of SNNs during the training process. To optimize the network construction and connection weights, the synaptic sampling method is utilized by considering the spine motility of SNNs as Bayesian learning \citep{kappel2015network}.Deep Rewiring (Deep R) pruning algorithm  \citep{bellec2018long} is further proposed and applied on SpiNNaker 2 prototype chips \citep{liu2018memory}.Chen et al. proposed gradient rewiring (Grad R) to jointly learn the connection and weights \citep{chen2021pruning}. The gradient is redefined to a new synaptic parameter to allow the connection competition between growing and pruning. The above studies have revealed structure refining capability in SNNs and indicated the high parameter redundancy in deep SNNs. However, these models still could not implement stable sparse learning during the prolonged training process. The stable sparse learning emphasizes that the topology sparsity of SNNs is pursued starting with the networks design phase, which leads to the substantial reduction in connections and, in turn, to memory and computation efficiency \citep{mocanu2018scalable}. 





%lacking the 
%necessary flexibility and fail to adapt to the diverse demands of various operational environments.



%Current methodologies in structure learning method for SNNs, predominantly characterized by the use of static pruning rates or predetermined sparsity levels, lack the necessary flexibility and fail to adapt to the diverse demands of various operational environments. These conventional approaches do not fully exploit the potential for dynamic network adaptation, which could otherwise enable more effective management of resource constraints.

%neurons and trillions of synaptic connections, l adaptively learned from outside environments.



%Therefore, we focus on the sparse training method from scratch for SNNs, and also attempt to give a solution about the flexible pruning ratio setting.





\section{Related works}

%In recent years, significant advancements in SNNs learning algorithms have led to increased parameter scales and more diverse topological structures in SNNs. Many SNNs with static topologies adopt existing artificial neural network (ANN) structures, such as VGG11, ResNets19 and Transformer. These models are trained using methods such as direct training with surrogate gradients or converting trained ANNs to SNNs. These SNNs with fixed architecture have achieved remarkable performance on multiple applications such as object detection, neural language understanding. However, the static topological SNNs primarily focus on synaptic weight learning while neglecting synaptic connectivity learning, which often results in parameter redundancy and a lack of automatic structural expansion. Conversely, SNNs with dynamic structures can jointly optimize synaptic connections and weights, offering greater flexibility and more optimized topologies.

SNNs have seen considerable advancements in learning algorithms that have expanded their parameter capacity and diversified their topological structures. These networks often incorporate established ANNs architectures, including VGG11, ResNets19, and Transformers, adapting them to the spike-based processing paradigm \citep{yao2024spike,hu2024advancing}. Training methodologies range from direct training with surrogate gradients to conversion techniques that transform pretrained ANNs into SNNs. While these static-topology SNNs have demonstrated significant efficacy in various applications, such as object detection and natural language understanding, they primarily emphasize synaptic weight optimization \citep{gast2024neural,zheng2024temporal,ren2024spiking}. This focus tends to overlook the critical aspect of synaptic connectivity learning, frequently leading to parameter inefficiencies and constrained network evolution. In contrast, SNNs designed with dynamic structures learning are engineered to concurrently optimize both synaptic connections and weights. This dual optimization affords enhanced flexibility and facilitates the development of more efficient and adaptive network topologies \citep{xu2024reversing,jiang2023adaptive,shen2024efficient}. We categorize the current sparse structure learning methods for SNNs into two distinct groups.


\textbf{Gradual Sparsification of Connection Structures for SNNs.} This kind of methods typically initializes the network with a non-sparse connected structure, which is iteratively optimized throughout training, resulting in a gradually sparser connection structure.
1) Weight parameter optimization methods. For instance, the gradient rewiring (Grad R) method is introduced in  \citep{chen2021pruning} which implements sparse structure learning through redefining network connection parameters. This method ensures that the gradient of these parameters forms an angle of less than 90° with the accurate gradient. During model training, synaptic pruning and regeneration are iteratively applied, achieving joint learning of synaptic connections and weights. Building on this, the nonlinear gradient reparameterization function that controls pruning speed through a threshold growth function is introduced in \citep{chen2022state}, further optimizing the SNNs structure. \citep{shi2023towards} combines unstructured weight pruning with unstructured neuron
pruning to maximize the utilization of the sparsity of neuromorphic computing,
thereby enhancing energy efficiency. 
2) Regularization-based methods. \citep{deng2021comprehensive} incorporated gradient regularization into the loss function, achieving synaptic connection pruning and weight quantization based on the Alternating Direction Method of Multipliers (ADMM). Similarly, Yin et al. combined sparse spike encoding with sparse network connections, using sparse regularization to establish models for spike data transmission and network sparsification \citep{yin2021energy}. There are also some studies to explore the connection pruning for spiking-based Transformer structure \citep{liu2024sparsespikformer}.
3) Connection-relationship-determination-based methods.
The synaptic sampling method based on Bayesian learning is proposed in \citep{kappel2015network}, modeling dendritic spine movement characteristics to achieve synaptic connection reconstruction and weight optimization. Combining unsupervised STDP rules with supervised Tempotron training, SNNs with connection gates are developed in \citep{qi2018jointly}. It improves the performance while reducing connections via sparse SNNs. There are also other studies to introduce the plasticity-based pruning methods for deep SNNs \citep{han2024developmental}.

\textbf{Fully Sparsification of Connection Structures for SNNs.} A different strategy involves initializing the network with a sparse connection structure from the start and continually optimizing this sparse structure throughout training. This fully sparse training approach is particularly advantageous for hardware implementation in resource-constrained environments, such as on-chip training in hardware chips. 1) Synaptic connection-rewiring-based methods. 
These evolutionary structure learning methods are proposed for deep SNNs by drawing inspiration of rewiring mechainism in human brain \citep{han2024adaptive,shen2023esl,li2024towards}. This method employs synaptic growth and pruning rules to adaptively adjust the connection structure based on gradients, momentum, or amplitude during training, maintaining a certain level of sparsity in synaptic connections and achieving effective sparse training of SNNs. 2) Lottery-ticket-hypothesis-based methods. The architecture search could also generate sparse SNNs, such as the lottery ticket hypothesis. The Early-Time lottery ticket hypothesis method proposed in \citep{kim2022exploring} demonstrates that winning sparse sub-networks exist in deep SNNs, similar to traditional deep ANNs. Further, the utilization-aware LTH method, which incorporates intra-layer connection regeneration and pruning during training, addresses hardware load imbalance issues caused by unstructured pruning methods \citep{yin2024workload}.


Despite these advancements, a gap remains in the deployment of fully adaptive and efficient SNNs architectures, particularly in resource-constrained environments such as edge computing devices. This underscores the necessity for novel methods that not only refine the sparsity and efficiency of these networks but also maintain adaptive learning capabilities throughout their lifecycle during the whole training process. The need for dynamic, flexible SNN models that mirror the human brain’s ability to reorganize and optimize its neural pathways in real-time is clear.

Our research addresses this gap by proposing a two-stage dynamic sparse structure learning approach for SNNs from scratch, leveraging the latest advances in neural network compression and synaptic plasticity. This method promises to significantly enhance the adaptability and efficiency of deep SNNs, positioning them as a viable solution for next-generation neuromorphic computing, particularly as future highly efficient perceptron applications \citep{chen2025rethinking, guan2022relative, guan2024six, liang2024camera} increasingly demand more computational resources for higher performance.
We believe that by integrating adaptive synaptic pruning and growth mechanisms, our approach will set a new standard for sparse structure learning in SNNs, aligning closely with the natural efficiencies observed in biological neural processes.

\section{Methods}





The main goal of our study is to implement the fully sparse training from scratch for SNNs with the dynamic compressibility during the training process. In detail, 
we first introduce the proposed two-stage sparse learning framework for SNNs. After that, the first and second stage computations for obtaining the right rewiring ratio and rewiring sparse networks are described, respectively. 

\subsection{The framework of the two-stage sparse learning method}


As illustrated in Fig. \ref{fig:method} and Algorithm \ref{twostage_framework}, we design the  two-stage sparse training method for SNNs with an appropriate rewiring ratio for each iteration during the training process. The sparse weight connections are initialized according to the Erdös–Rényi (ER) Random Graph. The ER graph could guarantee that the synaptic connection for each neuron has the same connection probability. Assuming there are $n^k$ and $n^{k-1}$ neurons in the neighboring two layers, then the probability of weight connection mask $M_{k, k-1}=1$ between two neurons in these two layers satisfies 
\begin{equation}
   p(M_{k, k-1}=1) = \frac{\epsilon (n^k + n^ {k-1})}{n^k * n^ {k-1}}.
\end{equation}
%$ p(M_{k, k-1}=1) = \frac{\epsilon (n^k + n^ {k-1})}{n^k * n^ {k-1}}$. 
\textcolor{black}{where \(\epsilon\) is a constant (or scaling factor) that influences the edge probability and accounts for sparsity or connectivity scaling.} Then the corresponding weight value $W$ can be initialized by the commonly used initialization method, such as Xavier Initialization and random normal distribution Initialization. Since then, we have been able to obtain the initialized sparse SNNs. After that, the initialized SNNs would be trained over multiple iterations, through the iterative training of the first stage and second stage in each iteration. It is worth noting that the SNNs would remain sparse and dynamically search for the suitable rewiring ratio in the following training process. 


In detail, the first stage involves the typical training process and attempts to identify an appropriate rewiring ratio based on temporarily trained weights. The rewiring ratio is calculated according to the PQ index, an efficient measure of the compressibility of neural network models \citep{diao2023pruning}. The PQ index helps quantify the redundancy in the network, thereby informing the following rewiring strategy.
In the second stage, the dynamic sparse structure learning method based on the rewiring method is adopted to implement sparse training from scratch. The connections are iteratively pruned and regrown according to the specified rewiring ratio. This iterative training approach ensures that the network continuously adapts and optimizes its structure, thereby improving performance. The rewiring method allows for dynamic adjustment, promoting the activation and growth of previously dormant connections, which contributes to SNNs' capability enhancement.

By integrating these two stages, our method achieves efficient and effective sparse training for SNNs, leveraging the compressibility insights gained in the first stage to guide dynamic structural adjustments in the second stage. This approach not only maintains the sparsity and efficiency of the model but also enhances its generalization ability.

\begin{algorithm}[tb]
\caption{The two-stage sparse training process of SNNs.}
\label{twostage_framework}
\textbf{Input Data}: $x_i, i=1, 2, ..., N$.\\
\textbf{Labels of Input Data}:$y_i, i=1, 2, ..., N$. \\
\textbf{Parameters}: The weight mask is $M$. The weight matrix: $W$. The updating iterations: $Epoch_{frequency}$.\\
% The compression hyper parameter $\alpha_r$. The scaling factor $\gamma$. The maximum pruning ratio $\beta$. 
\begin{algorithmic}[1] %[1] enables line numbers
\FOR{each assigned sparse layer of the SNNs}    
%\textbf{$\gets$ HybridSNN Iterations}  
\STATE {Initialize the sparse weight mask of the connected layer as the Erd\"{o}s–R\'{e}nyi topology;}
\ENDFOR
\STATE Initialize trained weight parameters;
% Training
\FOR {each training iterations $i$ }   %\quad \textbf{$\gets$ Weak Learner Pool}
\STATE $\diamondsuit$ \textbf{Stage I}
\STATE Perform standard training procedure with $W_i$ = $M_i$ $\odot$ $W_i$;
\STATE Perform weights updates for $W_i$;
\STATE Compute the total number of model parameters $d_i$ = $\vert $ $M_i$ $\vert$; 
\quad   \quad  \quad  
\STATE Compute PQ Index $I_{W_i}$ and the lower bound of the amount of remaining parameters $r_i$;
\STATE Compute the rewiring ratio $c_i$;
\STATE $\diamondsuit$ \textbf{Stage II}
\IF {current training epoch $\%$ $Epoch_{frequency}$ == 0: }  
\FOR {each assigned sparse layer of SNNs }  %\quad \textbf{$\gets$ Weak Learner Pool}
\STATE Remove the fraction $c_i$ of synaptic connections according to the pruning rule;
\STATE Regrow the fraction $c_i$ of synaptic connections according to the growing rule;
%\STATE Update the weight matrix of $W_i$ by element-wise production with weight mask $M_i$: \\
%\STATE $W_i$ = $M_i$ $\odot$ $W_i$;\\
\ENDFOR
\ENDIF
\ENDFOR
\STATE \textbf{return} The sparse SNNs with $W$.
\end{algorithmic}
\end{algorithm}





\subsection{Compress the sparse SNNs based on PQ index}
After the sparse initialization based on ER graph, the synaptic connections between neurons would become sparse randomly. Then in the following training process, the SNNs model would be trained according to the two stages sparse training method.

In the first stage, we train the sparse SNNs and compute the appropriate rewiring ratio according to PQ index $ I_{p, q}(W)$ (we simplified it as $ I(W)$ in the detailed derivation in the supplementary materials). 
\textcolor{black}{Here is the derivation of the sparsity measure \( I_{p, q}(W) = 1 - d^{\frac{1}{q} - \frac{1}{p}} \cdot \frac{\|W\|_p}{\|W\|_q} \) for spiking neural networks (SNNs), incorporating the formula update and focusing on scaling invariance, sensitivity to sparsity reduction, and cloning invariance, combined with spatiotemporal dynamics and sparsity in SNNs.
In detail, the scaling invariance in SNNs corresponds to: (1) Independence of weight scaling: If the weight matrix \( W \) is scaled (e.g., multiplied by a constant), its sparsity structure remains unchanged, and so should \( I_{p, q}(W) \).
(2) Independence of temporal scaling: Changes in spike magnitudes (the activation value) should not affect the sparsity measure, ensuring the measure accurately reflects temporal dynamics. We give detailed derivation in SNNs, it ensures that \( I_{p, q}(W) \) remains unaffected when all weights are scaled proportionally (e.g., multiplying \( W \) by a constant \( \alpha > 0 \)). The scaling weight magnitudes or activation value intensity does not change the network sparsity. 
Meanwhile, we analyze that $I_{p, q}(W)$ keeps sensitivity to spatial and  temporal sparsity in SNNs, that is, the distribution of weights or spike activations (firing rates). When it changes weight distribution with more nonzero weights, leading to a reduction in \( I_{p, q}(W) \) corresponds to sparsity decreasing. When temporal sparsity decreases (more neurons firing at the same time), the distribution becomes denser, which directly affects the ratio \( \|W\|_p / \|W\|_q \), leading to a decrease in \( I_{p, q}(W) \). In addition, we demonstrate that the sparsity measure \( I_{p, q}(W) \) should remain invariant when the weight matrix is cloned in spatial and temporal dimensions.
This ensures that cloning or repeating the matrix does not affect the sparsity measure. 
}




In detail, the PQ index for the non-zero vector $W_i \in \mathbb{R}_d$ with any $0<p<q$ is computed by:
\begin{equation}
    I_{p, q}(W_i) = 1-d^{\frac{1}{q} - \frac{1}{p}} (\parallel W_i \parallel _{p} - \parallel W_i \parallel _{q}),
\end{equation}
where $\parallel W_i \parallel _{p} $ equals to $ (\sum_{j=1}^{d} \mid w_j \mid ^{p})^{1/p}$, in which $w_j, j=1...d$ is the non-zero element in $W_i$. Then the lower bound of the retaining number of model parameter $W_i$ can be obtained by:
\begin{equation}
    r_i = d_i (1+\alpha_r)^{-q/(q-p)} \lbrack 1-I_{p, q}(W_i) \rbrack ^{pq/(q-p)},
\end{equation}
where $d_i=|M_i|$. Then the pruned ratio with better compressibility is computed by:
\begin{equation}
    c_i = \lfloor d_i  \cdot min(\gamma (1-\frac{r_i}{d_i}), \beta)\rfloor / N_{W_i},
\end{equation}
in which the $\gamma$ and $\beta$ are the scaling factor and maximum rewiring ratio, respectively. In detail, the hyperparameter of $\gamma$ is used to scale the rewiring ratio according to PQ index. The bigger $\gamma$ would obtain the higher rewiring ratio. We follow the settings in \citep{diao2023pruning} to set $\gamma=1$ and $\beta =0.9$ to prevent the model are over-pruned seriously. 
$N_{W_i}$ is the total number of parameters in $W_i$. Assume $r$ is the indices set of $W_i$ with the largest weight magnitude. Then $\alpha_r$ denotes the smallest value satisfying $\sum_{j \notin M_i^r} |w_j|^p \leq \alpha_r \sum_{j \in M_i^r} |w_j|^p$. The big $\alpha_r$ implies the model parameters are redundant and would result in a higher rewiring ratio. Thus we set the $\alpha_r$ to be 0.001 in the experiments to slow down the pruning speed and improve the stability of sparse model training. We give an example in Fig. \ref{neuron_wise_experiments} (a).

Since then, the right rewiring ratio for weight parameters in sparse SNNs has been figured out to improve compressibility and prevent sparse SNNs from either over-pruning  or under-pruning \citep{li2024efficient,xu2024reversing}.

During the training process in each iteration, before the above spasity measurement process, we need to update the weight matrix according to the learning algorithm in our model. In this paper, we
adopt the iterative Leaky Integrate-and-Fire (LIF) neuron model in SNNs to enhance information integration and temporal representation \citep{wu2019direct}.
The membrane potential $u(t)$ of postsynaptic neuron is updated based on 
the membrane potential at $t-1$ and the integrated presynaptic neuron input:
\begin{equation}
    u(t) = \tau u(t-1) + (M_i \odot W_i) x(t),
\end{equation}
where $\tau$ is the leaky factor set to $0.5$, and $x(t)$ represents the spike inputs. When $u(t)$ exceeds the firing threshold of $V_{th}$, the neuron fires a spike, and $u(t)$ is set to be $0$. Consequently, the neuron output and the membrane updating are given by:
\begin{equation}
    a(t+1) = \Theta (u(t+1)-V_{th}),
\end{equation}
\begin{equation}
    u(t+1) = u(t+1) (1-a(t+1)),
\end{equation}
To ensure that the output signal at each time step approximates the target distribution, we utilize the temporal efficient training loss function as in \citep{deng2021temporal}:
\begin{equation}
    L_{TET} = \frac{1}{T} \sum_{t=1}^{T} L_{CE} [O(t), y],
\end{equation}
where $T$ denotes the time steps and $L_{CE}$ is the cross-entropy loss function. 
Then the masked weights are updated according to the gradient descent rule with surrogate gradient function. 


\subsection{The connection rewiring of dynamic sparse structure learning}

After the above stage obtains the appropriate rewiring ratio, the connection rewiring method is followed to improve the stability and generalization ability of the sparse SNNs, instead of pruning the weights with the smallest magnitudes directly. The connection rewiring method is motivated by the synaptic rewiring mechanism in the human brain. The synaptic rewiring in the brain, covering the processes of synaptic pruning (elimination) and synaptic growth (formation), plays a vital role in neural development, learning, memory, and overall cognitive function. This dynamic remodeling of synaptic connections promotes the brain's adaptability and efficiency in processing information.
Meanwhile, the effectiveness of the rewiring operation has been demonstrated in earlier works for the structure learning of SNNs. Therefore, we employ the dynamic connection rewiring process to implement the sparse training of SNNs models from scratch, thus improving the stability and generalization ability of the sparse SNNs. 


The connection rewiring method could implement the effective and fast training by the iterative training of pruning and regrowing connections, avoiding the introduction of additional parameters that could increase memory usage. 
The pruning rule ensures the elimination of less significant connections in SNNs, reducing computational complexity while preserving the core structure of the network. We rank the weights magnitude according to their absolute values in sparse SNNs trained at the first stage, and prune the weight connections with the rewiring ratio of $c_i$ in the above section. However, the only operation of pruning may destroy the stable convergence of sparse SNNs and restrict the network's expressive capacity.
To fully leverage the information processing capacity of the original large SNNs without any pruning, it is crucial that all connections are activated during training. Consequently, the growth rule is employed to promote the regeneration of connections that have not been activated for a significant period of time. 
Different pruning and regrow rules adapt to the proposed two-stage sparse structure learning framework. For simplify, we adopt the momentum-based growing rule, to prioritize the regeneration of synaptic connections according to the momentum of the parameters. This approach ensures that connections showing significant momentum, and thus potential importance, are prioritized for regrowth.


{\color{black}{In detail, after obtained the suitable rewiring ratio in the first stage, we could then use this ratio $c_i$ to guide the training in the second stage.  Assuming the standard training procedure for sparse SNNs is followed by freezing the masked weights as $W_i$ = $M_i$ $\odot$ $W_i$, where the mask $M_i$ is obtained according to the pruning and regrowing principle as mentioned above. }}
After obtaining the updated $W_i$, we begin to compute the rewiring ratio again as described in the first stage. Therefore, through the iterative process between the first and second stage, our model can improve the efficiency of sparse training SNNs from scratch with the adaptive rewiring ratio. 












\section{Experiments}





In this section, we evaluate the performance of our proposed two-stage sparse structure learning method for SNNs. We conduct experiments on the CIFAR10, CIFAR100, and DVS-CIFAR10 datasets, including both ablation studies and comparative experiments. The experiments environments are NVIDIA-4090 GPU computation devices based on PYTORCH framework.









\subsection{Analysis on the dynamic sparse training during training process}



The performances of the proposed two-stage dynamic sparse training of SNNs are validated on two different rewiring scopes, including neuron-wise rewiring and layer-wise rewiring. The neuron-wise rewiring would adopt the connection rewiring each neuron of model parameters, which would prune and regrow $d \cdot p$ connected weight parameters for each neuron, and the rewiring ratio of each neuron is computed by the PQ index respectively. While the layer-wise one conducts weight parameter rewiring for each layer separately.



\begin{figure}[h]
%\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\columnwidth]{result_neuronwise-new.pdf}
    \caption{An example for training process of our method (a). The performance of the proposed two-stage sparse training method for SNNs, on CIFAR10 (b) and CIFAR100 (c) datasets, in the neuron-wise pruning scope. The bar chart represents the accuracy achieved by the proposed two-stage sparse training method. The solid line reflects the density of synaptic connections in the SNNs model.}
    \label{neuron_wise_experiments}
%\end{wrapfigure}
\end{figure}



\begin{figure*}[h]
    \centering
    \includegraphics[width=1\columnwidth]{figure/result_cifar10_cifar100.pdf}
    \caption{The performance of the proposed two-stage sparse training method for SNNs, on CIFAR10 (a) and CIFAR100 (b) datasets, in the layer-wise pruning scope. The bar chart represents the accuracy achieved by the proposed method. The solid line reflects the density of synaptic connections in the SNNs model. The dashed line is the trend analysis of accuracy using a two-period moving average. This diagram depicts the correlation between the density of the model and the enhancements in performance achieved using our two-stage sparse structure learning technique. }
    \label{fig:flowCifar10and100}
\end{figure*}



\textbf{Effectiveness in the layer-wise rewiring scope.} The accuracy and connection density of the proposed two-stage sparse training method for SNNs are illustrated in Fig. \ref{fig:flowCifar10and100}. The initial connection density is set to be 0.5, which means that there are only half of the connections to be activated when initialization. Meanwhile, the connections in our model remain sparse, ranging from 0.5 to 0.11 during the whole training process.
In addition, as the number of iterations in the sparse training process increases, the proposed two-stage sparse training method generates a relatively suitable rewiring ratio in the first stage, gradually reducing the synaptic connection density in SNNs. 
It is notable that the proposed model achieves its peak accuracy of 92.38\% and 70.3\% during the fourth iteration with a connection density of 30\% on the CIFAR10 and CIFAR100 datasets, respectively. The peak accuracy is even higher than that (about 92.2\% on the CIFAR10 dataset) of densely connected SNNs. This improvement can be attributed to the connection rewiring, which introduces a more activated parameter space and enhances the performance of sparse training by exploring extensive parameters throughout the sparse training process.
%This improvement primarily arises from the extensive parameter exploration throughout the sparse training process.



Simultaneously, the overall accuracy of the sparse SNNs exhibits a fluctuating trend. In the initial iterations, our model's stage I produces appropriate levels of sparsity, which decreases the density of synaptic connections in the sparse SNNs while improving accuracy. However, as the iterations continue and the model becomes more compressed, the performance starts to decline moderately due to increased sparsity. At a crucial point, when the model achieves its highest level of accuracy during the fourth iteration with a connection density of 30\% for CIFAR10 dataset, additional pruning results in a collapse when important parameters are eliminated, leading to considerable performance decrease.





\textbf{Effectiveness in the neuron-wise rewiring scope.}
We also verify the performance of our proposed two-stage sparse straining method in the neuron-wise scope. As illustrated in Fig. \ref{neuron_wise_experiments}, we analyze the accuracy and the corresponding density within four iterations, for these four iterations have shown the main trend change as in the situation of layer-wise scope. As shown in Fig. \ref{neuron_wise_experiments}, the proposed model exhibits similar accuracy oscillation phenomena in the CIFAR10 dataset when using layer-wise sparse structure training, akin to the neuron-wise approach. The proposed model achieves an accuracy of 92.48\% at the second iteration with a connection density of only 41\%. However, in the neuron-based scenario on the CIFAR100 dataset, no similar oscillation phenomena are observed. This could be due to the initial high sparsity, which may have led to the pruning of some critical synaptic connections. Thus, the remaining connections could not be insufficiently trained, resulting in decreased performance as the connection densities reduce.




\begin{figure*}[h]
    \centering
    \includegraphics[width=1\columnwidth]{figure/result_ablation_new.pdf}
    \caption{The ablation experiments of our proposed two-stage sparse training method for SNNs. (a) The accuracy comparison between the gradually sparse training and sparse training from scratch. (b) The connection density comparison between the gradually sparse training and sparse training from scratch.}
    \label{ablation_experiment}
\end{figure*}




The above phenomena are consistent with the proposed model's behavior. The pruning process initially removes redundant parameters. This results in a decrease in the sparsity of model parameters, leading to an improvement in performance due to regularization effects. As the model is compressed more, some critical parameters are pruned when the model reaches convergence, resulting in an increase in sparsity and a slight decrease in performance. Eventually, as the model begins to collapse, all weakened parameters are removed, leaving only the essential parameters needed to sustain performance. Consequently, the level of sparsity decreases dramatically, leading to a noticeable decline in performance.





\textbf{Ablation study on the sparse training from scratch.}
To evaluate the effectiveness of sparse training for the proposed two-stage sparse structure learning method, we conduct the ablation study by comparing the performance with gradually sparse training and sparse training from scratch. As illustrated in Fig. \ref{ablation_experiment}, the performance of the sparse training from scratch (Remaining Sparse) outperforms that of the gradually sparse training from the initial fully non-sparse connections. The reason lies in that the manner of the sparse training from scratch explores a similar thorough parameter space to the non-sparse model and masks some noises caused by the redundancy parameters. Besides, the proposed model for sparse training from scratch demonstrates superior network connection sparsity than the gradually sparse training model at the same number of iterations. This allows the sparse training from scratch model to more quickly identify the optimal rewiring rate and achieve better performance. Additionally, the sparse training from scratch model is more hardware-friendly than the gradually sparse training model, making it more suitable for sparse training in hardware-constrained environments especially on-chip learning.







\subsection{Performance comparison to other methods}

We compare the performance of the proposed two-stage sparse structure learning method for SNNs with other current state-of-the-art SNNs: ADMM \citep{deng2021comprehensive}, Grad R \citep{chen2021pruning}, ESLSNN \citep{shen2023esl} and
STDS \citep{chen2022state}, UPR \citep{shi2023towards}. 

As shown in Tab. \ref{compresults}, the proposed two-stage sparse structure training method achieves competitive performance among the various methods while retaining the advantage of sparse training from scratch. Notably, compared to fully non-sparse models, our sparse training model can even improve performance while maintaining a certain level of sparsity through dynamic iteration and searching for an appropriate rewiring ratio.
For example, on the CIFAR10 dataset, the model trained using our proposed method under the neuron-wise scope improves performance by approximately 1\% compared to the fully non-sparse model while maintaining a sparsity of 30\% to 40\%. 
On the CIFAR100 dataset, the performance of SNNs model with our two-stage sparse training method is also improved 1.07\% compared to the non-sparse model with only 29.48\% connection. 
It is worth noting that the proposed two-stage training method proceeds sparse training from scratch and maintains sparse training during the whole training process. 
These results demonstrate that our proposed model helps the original non-sparse model mask redundant parameters and enhance the generalization capability of the sparse model during iterative training by continuously finding the appropriate rewiring ratio.



\iffalse

\begin{table*}[]
\caption{Performance comparison of the proposed two-stage sparse stucture learning approach for SNNs with other models.}
    \label{compresults}
    \centering
\begin{tabular}{cccccccc}
\hline
\textbf{Dataset}             & \textbf{\begin{tabular}[c]{@{}c@{}}Pruning \\ Method\end{tabular}} & \textbf{Architecture}                                                    & \textbf{T}  & \textbf{\begin{tabular}[c]{@{}c@{}}Top-1 \\ Acc.(\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Acc.\\ Loss(\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Conn.\\ (\%)\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Param.\\ (M)\end{tabular}} \\ \hline
\multirow{7}{*}{CIFAR10}     & ADMM                                                               & 7 Conv, 2 FC                                                             & 8           & 90.19                                                              & -0.13                                                            & 25.03                                                          & 15.54                                                         \\
                             & Grad R                                                             & 6 Conv, 2 FC                                                             & 8           & 92.54                                                              & -0.30                                                            & 36.72                                                          & 10.43                                                         \\
                             & ESLSNN                                                             & ResNet19                                                                 & 2           & 91.09                                                              & -1.7                                                             & 50                                                             & 6.3                                                           \\
                             & STDS                                                               & 6 Conv, 2 FC                                                             & 8           & 92.49                                                              & -0.35                                                            & 11.33                                                          & 1.71                                                          \\
                             & UPR                                                                & 6 Conv, 2 FC                                                             & 8           & 92.05                                                              & -0.79                                                            & 1.16                                                           & 9.56                                                          \\
                             & \multirow{2}{*}{\textbf{This work}}                                & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19 \\ Neuron-wise\end{tabular}} & \textbf{2}  & \textbf{\begin{tabular}[c]{@{}c@{}}92.48\\ 92.1\end{tabular}}      & \textbf{\begin{tabular}[c]{@{}c@{}}+1.18\\ +0.8\end{tabular}}    & \textbf{\begin{tabular}[c]{@{}c@{}}40.58\\ 26.63\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}5.12\\ 3.36\end{tabular}}  \\
                             &                                                                    & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19\\ Layer-wise\end{tabular}}   & \textbf{2}  & \textbf{\begin{tabular}[c]{@{}c@{}}92.38\\ 91.99\end{tabular}}     & \textbf{\begin{tabular}[c]{@{}c@{}}+0.11\\ -0.28\end{tabular}}   & \textbf{\begin{tabular}[c]{@{}c@{}}29.72\\ 17.91\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}3.7\\ 2.26\end{tabular}}   \\ \hline
\multirow{3}{*}{CIFAR100}    & ESLSNN                                                             & ResNet19                                                                 & 2           & 73.48                                                              & -0.99                                                            & 50                                                             & 6.32                                                          \\
                             & UPR                                                                & SEW ResNet18                                                             & 4           & \begin{tabular}[c]{@{}c@{}}70.45\\ 69.41\end{tabular}              & \begin{tabular}[c]{@{}c@{}}-3.71\\ -4.75\end{tabular}            & \begin{tabular}[c]{@{}c@{}}3.60\\ 2.48\end{tabular}            & \begin{tabular}[c]{@{}c@{}}-\\ -\end{tabular}                 \\
                             & \textbf{This work}                                                 & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19\\ Layer-wise\end{tabular}}   & 2           & \textbf{70.3}                                                      & \textbf{+1.07}                                                   & \textbf{29.48}                                                 & \textbf{3.73}                                                 \\ \hline
\multirow{4}{*}{DVS-CIFAR10} & ESLSNN                                                             & VGGSNN                                                                   & 10          & 78.3                                                               & -0.28                                                            & 10                                                             & 0.92                                                          \\
                             & STDS                                                               & VGGSNN                                                                   & 10          & 79.8                                                               & -2.6                                                             & 4.67                                                           & 0.24                                                          \\
                             & UPR                                                                & VGGSNN                                                                   & 10          & 78.3                                                               & -0.5                                                             & 0.77                                                           & 1.81                                                          \\
                              & UPR                                                                & VGGSNN                                                                   & 10          & 81.0                                                               & -1.4                                                             & 4.46                                                          & 2.5                                                         \\
                             & \textbf{This work}                                                 & \textbf{\begin{tabular}[c]{@{}c@{}}VGGSNN\\ Layer-wise\end{tabular}}     & \textbf{10} & \textbf{78.4}                                                      & \textbf{+0.08}                                                   & \textbf{30}                                                    & \textbf{2.76}                                                 \\ \hline
\end{tabular}
\end{table*}


\fi



% Please add the following required packages to your document preamble:
\begin{table*}[] \small
\caption{Performance comparison of the proposed two-stage sparse structure learning approach for SNNs with other models.}
    \label{compresults}
    \centering
\begin{tabular}{ccccccccl}
\hline
\textbf{Dataset}                                       & \textbf{\begin{tabular}[c]{@{}c@{}}Pruning \\ Method\end{tabular}}            & \textbf{Architecture}                                                    & \textbf{T}  & \textbf{\begin{tabular}[c]{@{}c@{}}Top-1 \\ Acc.(\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Acc.\\ Loss(\%)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Conn.\\ (\%)\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}c@{}}Param.\\ (M)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}SOPS\\ (M)\end{tabular}}      \\ \hline
\multirow{7}{*}{CIFAR10}                               & ADMM                                                                          & 7 Conv, 2 FC                                                             & 8           & 90.19                                                              & -0.13                                                            & 25.03                                                          & 15.54                                                         & -                                                                \\
                                                       & Grad R                                                                        & 6 Conv, 2 FC                                                             & 8           & 92.54                                                              & -0.30                                                            & 36.72                                                          & 10.43                                                         & -                                                                \\
                                                       & ESLSNN                                                                        & ResNet19                                                                 & 2           & 91.09                                                              & -1.7                                                             & 50                                                             & 6.3                                                           & 180.56                                                           \\
                                                       & STDS                                                                          & 6 Conv, 2 FC                                                             & 8           & 92.49                                                              & -0.35                                                            & 11.33                                                          & 1.71                                                          & 147.22                                                           \\
                                                       & UPR                                                                           & 6 Conv, 2 FC                                                             & 8           & 92.05                                                              & -0.79                                                            & 1.16                                                           & 9.56                                                          & 16.47                                                            \\
                                                       & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}This\\ work\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19 \\ Neuron-wise\end{tabular}} & \textbf{2}  & \textbf{\begin{tabular}[c]{@{}c@{}}92.48\\ 92.1\end{tabular}}      & \textbf{\begin{tabular}[c]{@{}c@{}}+1.18\\ +0.8\end{tabular}}    & \textbf{\begin{tabular}[c]{@{}c@{}}40.58\\ 26.63\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}5.12\\ 3.36\end{tabular}}  & \textbf{\begin{tabular}[c]{@{}l@{}}158.35\\ 121.49\end{tabular}} \\
                                                       &                                                                               & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19\\ Layer-wise\end{tabular}}   & \textbf{2}  & \textbf{\begin{tabular}[c]{@{}c@{}}92.38\\ 91.99\end{tabular}}     & \textbf{\begin{tabular}[c]{@{}c@{}}+0.11\\ -0.28\end{tabular}}   & \textbf{\begin{tabular}[c]{@{}c@{}}29.72\\ 17.91\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}3.7\\ 2.26\end{tabular}}   & \textbf{\begin{tabular}[c]{@{}l@{}}133.26\\ 110.65\end{tabular}} \\ \hline
\multirow{3}{*}{CIFAR100}                              & ESLSNN                                                                        & ResNet19                                                                 & 2           & 73.48                                                              & -0.99                                                            & 50                                                             & 6.32                                                          & 186.25                                                           \\
                                                       & UPR                                                                           & SEW ResNet18                                                             & 4           & \begin{tabular}[c]{@{}c@{}}70.45\\ 69.41\end{tabular}              & \begin{tabular}[c]{@{}c@{}}-3.71\\ -4.75\end{tabular}            & \begin{tabular}[c]{@{}c@{}}3.60\\ 2.48\end{tabular}            & -                                                             & \begin{tabular}[c]{@{}l@{}}9.60\\ 6.79\end{tabular}              \\
                                                       & \textbf{\begin{tabular}[c]{@{}c@{}}This\\ work\end{tabular}}                  & \textbf{\begin{tabular}[c]{@{}c@{}}ResNet19\\ Layer-wise\end{tabular}}   & 2           & \textbf{70.3}                                                      & \textbf{+1.07}                                                   & \textbf{29.48}                                                 & \textbf{3.73}                                                 & \textbf{140.27}                                                  \\ \hline
\begin{tabular}[c]{@{}c@{}}DVS-\\ CIFAR10\end{tabular} & ESLSNN                                                                        & VGGSNN                                                                   & 10          & 78.3                                                               & -0.28                                                            & 10                                                             & 0.92                                                          & 129.64                                                           \\
                                                       & STDS                                                                          & VGGSNN                                                                   & 10          & 79.8                                                               & -2.6                                                             & 4.67                                                           & 0.24                                                          & 38.85                                                            \\
                                                       & UPR                                                                           & VGGSNN                                                                   & 10          & \begin{tabular}[c]{@{}c@{}}78.3\\ 81.0\end{tabular}                & \begin{tabular}[c]{@{}c@{}}-0.5\\ -1.4\end{tabular}              & \begin{tabular}[c]{@{}c@{}}0.77\\ 4.46\end{tabular}            & \begin{tabular}[c]{@{}c@{}}1.81\\ 2.5\end{tabular}            & \begin{tabular}[c]{@{}l@{}}6.75\\ 31.86\end{tabular}             \\
                                                       & \textbf{\begin{tabular}[c]{@{}c@{}}This \\ work\end{tabular}}                 & \textbf{\begin{tabular}[c]{@{}c@{}}VGGSNN\\ Layer-wise\end{tabular}}     & \textbf{10} & \textbf{78.4}                                                      & \textbf{+0.08}                                                   & \textbf{30}                                                    & \textbf{2.76}                                                 & \textbf{189.02}                                                  \\ \hline
\end{tabular}
\end{table*}



\section{Conclusion}

%In conclusion, this paper has introduced a novel two-stage dynamic structure learning method tailored for Spiking Neural Networks (SNNs) that effectively addresses the challenges of fixed pruning ratios and the limitations of static sparse training methods prevalent in current models. The first stage of our approach utilizes the PQ index to assess the compressibility of sparse subnetworks, allowing for informed adjustments to the rewiring ratios of synaptic connections. This adaptive mechanism enables the model to avoid the pitfalls of underpruning or overpruning that are common in traditional techniques.

%In the second stage, the established rewiring ratios guide the dynamic synaptic connection rewiring, incorporating both pruning and regrowth strategies. This method not only optimizes the efficiency of sparse SNNs but also enhances their performance, demonstrating competitive results through extensive experimentation. The iterative learning process implemented across both stages ensures continuous improvement and adaptation of the network structure throughout the training phase.

%Our experimental results confirm that the proposed dynamic structure learning method significantly enhances the compression efficiency of SNNs while maintaining, and in some cases even surpassing, the performance benchmarks of existing models. Importantly, the method preserves the advantages of sparse training from scratch, proving especially beneficial in environments with limited hardware resources, such as neuromorphic chips.

%Overall, the proposed method represents a significant advancement in the field of neural network training, offering a more flexible and efficient approach to managing the complexities associated with SNN architectures. It paves the way for future research into more adaptive and resource-efficient neural network training methods, inspired by the natural efficiencies found in brain development.

To summarize, this study has introduced a novel two-stage dynamic structure learning method tailored for SNNs that effectively addresses the challenges of fixed pruning ratios and the limitations of static sparse training methods prevalent in current models. In the first stage of our strategy, we employ the PQ index to evaluate the compressibility of sparse subnetworks. This enables us to make informed adjustments to the rewiring ratios of synaptic connections. This adaptive technique enables the model to circumvent the drawbacks of insufficient pruning or excessive pruning.
In the second stage, the predetermined rewiring ratios guide the dynamic synaptic connection rewiring, incorporating both pruning and regrowth strategies. This approach not only improves the compression efficiency of sparse SNNs but also boosts their performance. The iterative learning process implemented across both stages ensures continuous improvement and adaptation of the sparse network structure throughout the training phase.
The experimental results validate that the proposed dynamic structure learning greatly improves the compression efficiency of SNNs. Additionally, it either matches or exceeds the performance benchmarks set by current models in certain circumstances. Crucially, this strategy maintains the benefits of sparse training from the scratch, which is particularly advantageous in settings with restricted hardware resources, like neuromorphic hardware on Edge AI.


\section{Acknowledgement}
This work was supported by National Natural
Science Foundation of China under Grant (No.
62306274, 62088102, 62476035, 62206037, 61925603), Open Research Program of the National Key Laboratory of Brain-Machine Intelligence, Zhejiang University (No. BMI2400012).


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}



\clearpage
\newpage
\appendix

    



\end{document}
