\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}






\iffalse

Here is the revised derivation of the sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) for spiking neural networks (SNNs), incorporating the formula update and focusing on scaling invariance, sensitivity to sparsity reduction, and cloning invariance, combined with spatiotemporal dynamics and sparsity in SNNs.

---

### **1. Revised Formula Definition**

The updated sparsity measure is:
\[
I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q}
\]
Where:
- \( \|W\|_p = \left( \sum_{i=1}^d |w_i|^p \right)^{1/p} \) is the \( \ell_p \)-norm of \( W \),
- \( \|W\|_q = \left( \sum_{i=1}^d |w_i|^q \right)^{1/q} \) is the \( \ell_q \)-norm of \( W \),
- \( d \) is the dimensionality of \( W \),
- \( p < q \) ensures that sparsity is more effectively captured.

The additional term \( 1 - \) allows \( I(W) \) to range between 0 (no sparsity) and 1 (maximum sparsity). This revised formula is analyzed in detail below.

---

### **2. Scaling Invariance**

#### **Definition**
If \( W \) is scaled by \( \alpha > 0 \), the sparsity measure should remain invariant:
\[
I(\alpha W) = I(W)
\]

#### **Derivation**
1. Scaling changes the norms as:
   \[
   \|\alpha W\|_p = \alpha \|W\|_p, \quad \|\alpha W\|_q = \alpha \|W\|_q
   \]
2. Substituting into \( I(\alpha W) \):
   \[
   I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\|\alpha W\|_p}{\|\alpha W\|_q}
   \]
   \[
   I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\alpha \|W\|_p}{\alpha \|W\|_q}
   \]
3. Simplifying:
   \[
   I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

#### **Significance in SNNs**
- **Weight sparsity**: Scaling weights does not change the sparsity structure.
- **Temporal sparsity**: Scaling spike magnitudes does not affect the temporal distribution of spikes.

---

### **3. Sensitivity to Sparsity Reduction**

#### **Definition**
When sparsity decreases (weights become more uniform), \( I(W) \) should decrease.

#### **Derivation**
1. Compare two weight vectors:
   - Sparse: \( W_1 = [10, 0, 0, 0] \)
   - Less sparse: \( W_2 = [5, 5, 0, 0] \)
2. Norms:
   - \( \|W_1\|_p = 10, \quad \|W_2\|_p = 2^{1/p} \cdot 5 \)
   - \( \|W_1\|_q = 10, \quad \|W_2\|_q = 2^{1/q} \cdot 5 \)
3. Sparsity measure:
   \[
   I(W_1) = 1 - d^{1/q - 1/p} \cdot \frac{\|W_1\|_p}{\|W_1\|_q} = 1 - d^{1/q - 1/p}
   \]
   \[
   I(W_2) = 1 - d^{1/q - 1/p} \cdot \frac{\|W_2\|_p}{\|W_2\|_q} = 1 - d^{1/q - 1/p} \cdot 2^{1/p - 1/q}
   \]
4. Since \( p < q \), \( 1/p - 1/q > 0 \), so \( 2^{1/p - 1/q} < 1 \). Thus:
   \[
   I(W_2) < I(W_1)
   \]

#### **Significance in SNNs**
- **Weight sparsity**: \( I(W) \) decreases as more weights become nonzero.
- **Temporal sparsity**: Simultaneous firing of more neurons reduces temporal sparsity, and \( I(W) \) captures this reduction.

---

### **4. Cloning Invariance**

#### **Definition**
If \( W \) is cloned (repeated), the sparsity measure \( I(W) \) should remain unchanged:
\[
I(W) = I([W, W])
\]

#### **Derivation**
1. For a cloned matrix \( [W, W] \):
   \[
   \|[W, W]\|_p = 2^{1/p} \|W\|_p, \quad \|[W, W]\|_q = 2^{1/q} \|W\|_q
   \]
2. Substituting into \( I(W) \):
   \[
   I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{\|[W, W]\|_p}{\|[W, W]\|_q}
   \]
   \[
   I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{2^{1/p} \|W\|_p}{2^{1/q} \|W\|_q}
   \]
   \[
   I([W, W]) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

#### **Significance in SNNs**
- **Temporal cloning**: Repeating the same activity patterns over time does not affect sparsity.
- **Spatial cloning**: Cloning subnetworks does not alter the sparsity measure.

---

### **5. Incorporating Time Steps in \( I(W) \)**

#### **(1) Cloning Across Time Steps**
If \( W \) is repeated across \( T \) time steps:
\[
W_T = [W, W, \dots, W] \in \mathbb{R}^{d \times (nT)}
\]
1. Norms for \( W_T \):
   \[
   \|W_T\|_p = T^{1/p} \|W\|_p, \quad \|W_T\|_q = T^{1/q} \|W\|_q
   \]
2. Sparsity measure:
   \[
   I(W_T) = 1 - (nT)^{1/q - 1/p} \cdot \frac{\|W_T\|_p}{\|W_T\|_q}
   \]
3. Substituting:
   \[
   I(W_T) = 1 - (nT)^{1/q - 1/p} \cdot \frac{T^{1/p} \|W\|_p}{T^{1/q} \|W\|_q}
   \]
4. Simplifying:
   \[
   I(W_T) = 1 - n^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

#### **(2) Temporal Sparsity Changes**
If sparsity changes across time steps, each step \( t \) has a different weight distribution \( W^{(t)} \):
\[
W_T^\prime = [W^{(1)}, W^{(2)}, \dots, W^{(T)}]
\]
1. Norms depend on dynamic sparsity:
   \[
   \|W_T^\prime\|_p = \left(\sum_{t=1}^T \|W^{(t)}\|_p^p \right)^{1/p}, \quad \|W_T^\prime\|_q = \left(\sum_{t=1}^T \|W^{(t)}\|_q^q \right)^{1/q}
   \]
2. The sparsity measure captures reduced sparsity as:
   \[
   I(W_T^\prime) = 1 - (nT)^{1/q - 1/p} \cdot \frac{\|W_T^\prime\|_p}{\|W_T^\prime\|_q}
   \]

---

### **Conclusion**

1. **Scaling Invariance**: \( I(W) \) is unaffected by scaling, ensuring consistency in weight or spike intensity changes.
2. **Sensitivity to Sparsity Reduction**: \( I

(W) \) decreases as sparsity reduces, reflecting uniform weight distribution or increased neuron activation.
3. **Cloning Invariance**: \( I(W) \) remains unchanged when weights are cloned spatially or temporally, ensuring robustness to network size changes.

The revised formula \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) aligns with the properties of SNNs, capturing both static and dynamic sparsity in spatiotemporal settings.


\fi



\iffalse


### **Comprehensive Explanation of \( I(W) \) in Spiking Neural Networks (SNNs)**

This explanation combines all previous answers regarding the sparsity measure \( I(W) = d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \), incorporating scaling invariance, sensitivity to sparsity reduction, and cloning invariance, with a focus on the unique properties of Spiking Neural Networks (SNNs).

---

### **1. Definition and Motivation**

#### **Sparsity in SNNs**
SNNs exhibit two types of sparsity:
1. **Weight sparsity**: The proportion of zero values in the weight matrix \( W \). Higher sparsity corresponds to fewer active connections.
2. **Spatiotemporal sparsity**: Neurons only activate when they emit a spike. The temporal distribution of spikes reflects dynamic sparsity.

#### **Desired Properties of Sparsity Measure**
A good sparsity measure for SNNs must satisfy:
1. **Scaling Invariance**: Sparsity should depend on the relative distribution of weights, not their magnitudes.
2. **Sensitivity to Sparsity Reduction**: When weights become more uniformly distributed (less sparse), the sparsity measure should decrease.
3. **Cloning Invariance**: If the weight matrix is cloned or repeated across dimensions (e.g., in time), the sparsity measure should remain unchanged.

---

### **2. Sparsity Measure Definition**

The sparsity measure is defined as:
\[
I(W) = d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q}
\]
where:
- \( \|W\|_p = \left(\sum_{i=1}^d |w_i|^p\right)^{1/p} \) is the \( \ell_p \)-norm.
- \( \|W\|_q = \left(\sum_{i=1}^d |w_i|^q\right)^{1/q} \) is the \( \ell_q \)-norm.
- \( d \) is the dimensionality (or size) of \( W \), and the term \( d^{1/q - 1/p} \) ensures cloning invariance.

The parameter choice \( p < q \) ensures \( \|W\|_p \) decreases faster than \( \|W\|_q \) as sparsity increases.

---

### **3. Key Properties of \( I(W) \)**

#### **(1) Scaling Invariance**

**Definition**: If \( W \) is scaled by \( \alpha > 0 \), then:
\[
I(\alpha W) = I(W)
\]

**Proof**:
1. Scaling changes the norms as:
   \[
   \|\alpha W\|_p = \alpha \|W\|_p, \quad \|\alpha W\|_q = \alpha \|W\|_q
   \]
2. Substituting into \( I(W) \):
   \[
   I(\alpha W) = d^{1/q - 1/p} \cdot \frac{\|\alpha W\|_p}{\|\alpha W\|_q} = d^{1/q - 1/p} \cdot \frac{\alpha \|W\|_p}{\alpha \|W\|_q}
   \]
3. Simplifying:
   \[
   I(\alpha W) = I(W)
   \]

**Significance in SNNs**:
- **Weight sparsity**: Proportional scaling of weights does not affect sparsity, as the structure remains unchanged.
- **Spatiotemporal sparsity**: Scaling spike magnitudes (e.g., firing rates) does not alter temporal sparsity.

---

#### **(2) Sensitivity to Sparsity Reduction**

**Definition**: When sparsity decreases (weights become more uniform), \( I(W) \) should decrease.

**Proof**:
1. Compare two weight vectors:
   - Sparse: \( W_1 = [10, 0, 0, 0] \)
   - Less sparse: \( W_2 = [5, 5, 0, 0] \)
2. Compute norms:
   - \( \|W_1\|_p = 10, \quad \|W_2\|_p = 2^{1/p} \cdot 5 \)
   - \( \|W_1\|_q = 10, \quad \|W_2\|_q = 2^{1/q} \cdot 5 \)
3. Sparsity measures:
   \[
   I(W_1) = d^{1/q - 1/p}, \quad I(W_2) = d^{1/q - 1/p} \cdot 2^{1/p - 1/q}
   \]
4. Since \( p < q \), \( 1/p - 1/q > 0 \), so \( 2^{1/p - 1/q} < 1 \). Hence:
   \[
   I(W_2) < I(W_1)
   \]

**Significance in SNNs**:
- **Weight sparsity**: \( I(W) \) captures the transition from sparse (few active connections) to dense weights.
- **Temporal sparsity**: \( I(W) \) reflects the effect of increasing activation rates, leading to lower sparsity.

---

#### **(3) Cloning Invariance**

**Definition**: If \( W \) is repeated (cloned), the sparsity measure should remain unchanged:
\[
I(W) = I([W, W])
\]

**Proof**:
1. For a cloned matrix \( [W, W] \):
   \[
   \|[W, W]\|_p = 2^{1/p} \|W\|_p, \quad \|[W, W]\|_q = 2^{1/q} \|W\|_q
   \]
2. Substituting into \( I(W) \):
   \[
   I([W, W]) = (2d)^{1/q - 1/p} \cdot \frac{2^{1/p} \|W\|_p}{2^{1/q} \|W\|_q}
   \]
3. Simplifying:
   \[
   I([W, W]) = I(W)
   \]

**Significance in SNNs**:
- **Temporal cloning**: If neural activity is duplicated across time steps, \( I(W) \) remains invariant.
- **Spatial cloning**: If weights are repeated across subnetworks, sparsity should not change.

---

### **4. Incorporating Time Steps in \( I(W) \)**

In SNNs, temporal sparsity is crucial. When the weight matrix is repeated across time steps \( T \), the measure \( I(W) \) should either remain invariant or reflect temporal sparsity changes.

#### **(1) Cloning Across Time Steps**
1. Initial weight matrix \( W \) is \( d \times n \). After \( T \) repetitions:
   \[
   W_T = [W, W, \dots, W] \in \mathbb{R}^{d \times (nT)}
   \]
2. Norms for \( W_T \):
   \[
   \|W_T\|_p = T^{1/p} \|W\|_p, \quad \|W_T\|_q = T^{1/q} \|W\|_q
   \]
3. Sparsity measure:
   \[
   I(W_T) = (nT)^{1/q - 1/p} \cdot \frac{\|W_T\|_p}{\|W_T\|_q}
   \]
4. Simplifying:
   \[
   I(W_T) = n^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

#### **(2) Temporal Sparsity Changes**
If activation patterns vary across time steps (e.g., more neurons fire over time), temporal sparsity reduces. Consider a modified weight matrix \( W_T^\prime \):
1. Each time step \( t \) has different sparsity:
   \[
   W_T^\prime = [W^{(1)}, W^{(2)}, \dots, W^{(T)}]
   \]
2. The measure \( I(W_T^\prime) \):
   \[
   I(W_T^\prime) = (nT)^{1/q - 1/p} \cdot \frac{\|W_T^\prime\|_p}{\|W_T^\prime\|_q}
   \]
3. If sparsity decreases across time (\( W^{(t)} \) becomes denser), \( \|W_T^\prime\|_p / \|W_T^\prime\|_q \) decreases, reducing \( I(W_T^\prime) \).

---

### **Conclusion**

1. **Scaling Invariance**: \( I(W) \) reflects sparsity independent of weight scaling or spike intensity scaling.
2. **Sensitivity to Sparsity Reduction**: \( I(W) \) decreases as weights or activations become less sparse.
3. **Cloning Invariance**: \( I(W) \) remains unchanged across spatial or temporal


\fi


{\color{pink}

We are sincerely appreciate your recognition of our motivation significance of adaptive pruning and sparse training for deep SNNs to enhance the energy efficiency. We will answer your questions point by point below.

### **Comparison with RigL and DSR**
RigL [1] and DSR [2] indeed introduce innovative strategies for adaptive pruning and sparse training in ANNs. RigL uses cosine annealing to control the fraction of updated connections over time. The top-k selection process prunes and grows connections based on magnitude. DSR leverages an adaptive global threshold \( H \) using a negative feedback loop to maintain a fixed number of pruned parameters during reallocation steps. However, our method fundamentally differs from these works in its guiding principles and applicability to SNNs. Our method explicitly incorporates **\( I(W) \)**, a sparsity measurement informed by the current weight distribution, to dynamically estimate and adjust sparsity, from the network compression perspective.

### **Theoretical discussion and the adaptation to SNNs**

Thanks for your question. As illustrated in revised PDF file in the supplementary materials, the suitable sparsity ratio for SNNs is obtained through the sparsity measurements, which is derived according to the network compression theory, by leverage the unique characteristics of SNNs, especially the spatial and temporal dynamics and discrete spike firing mechanism. 

Here is the derivation of the sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) for SNNs, incorporating the formula update and focusing on scaling invariance, sensitivity to sparsity reduction, and cloning invariance (the sparse measurement property as in [1]), combined with spatiotemporal dynamics and sparsity in SNNs.

SNNs communicate through discrete spikes, exhibiting the following key features: (1) Discrete activation: Postsynaptic neurons emit spikes only at specific time points. They are either active (firing spikes) or inactive (not spiking), resulting in sparse data flow.
(2) Structure sparsity: Sparsity refers to the proportion of nonzero elements in a weight matrix.

The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) is rigorously constructed to reflect these properties. 
The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) satisfies these properties, where:
 \( \|W\|_p = \left( \sum_{i=1}^d |w_i|^p \right)^{1/p} \) is the \( \ell_p \)-norm of \( W \),
 \( \|W\|_q = \left( \sum_{i=1}^d |w_i|^q \right)^{1/q} \) is the \( \ell_q \)-norm of \( W \),
 \( d \) is the dimensionality of \( W \),
 \( p < q \) ensures that sparsity is more effectively captured.
The additional term \( 1 - \) allows \( I(W) \) to range between 0 (no sparsity) and 1 (maximum sparsity). Because when the sparsity is 100\%, which means all the elements in SNNs are 0, then \( I(W) \) is 1. While when there are no zero elements, that is, the orginal fully connected SNNs, then \( I(W) \) is 0. The term \( d^ {1/q - 1/p} \) ensures that \( I(W) \) is independent of the vector length, satisfying the cloning property. Without this term, it would vary with the size of  \( W \), even for identical sparsity patterns.
Below, we derive this formula and explain how it aligns with SNN characteristics.



#### ** Scaling Invariance**

In SNNs, it ensures that \( I(W) \) remains unaffected when all weights are scaled proportionally (e.g., multiplying \( W \) by a constant \( \alpha > 0 \)). The scaling weight magnitudes or activation value intensity does not change the network sparsity. 

If the weight matrix \( W \) is scaled by a constant \( \alpha \), the sparsity measure remains unchanged because the norms of the scaled matrix \( \|\alpha W\|_p \) and \( \|\alpha W\|_q \) are proportional to the original norms. Specifically:
  \[
  I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\|\alpha W\|_p}{\|\alpha W\|_q} = 1 - d^{1/q - 1/p} \cdot \frac{\alpha \|W\|_p}{\alpha \|W\|_q} = I(W).
  \]
  This proves that scaling does not change the sparsity measure, ensuring that \( I(W) \) captures only the relative distribution of weights.

#### **  Sensitivity To sparsity reduction**

There are two different kinds of sparsity reduction sensitivity: (1) Weight sparsity: Decreased sparsity corresponds to more nonzero weights, leading to a reduction in \( I(W) \).
(2)Temporal sparsity: If more neurons fire simultaneously, temporal sparsity decreases, and \( I(W) \) reflects this reduction.

We give the derivation and an example to show that it keeps sensitivity to spatial and  temporal sparsity, that is, the distribution of weights or spike activations (firing rates). When it changes weight distribution with more nonzero weights, leading to a reduction in \( I(W) \) corresponds to sparsity decreasing. When temporal sparsity decreases (more neurons firing at the same time), the distribution becomes denser, which directly affects the ratio \( \|W\|_p / \|W\|_q \), leading to a decrease in \( I(W) \).

#### ** Cloning Invariance**

The sparsity measure should remain unchanged when the weight matrix is cloned or repeated It should satisfy the property of Cloning Invariance in SNNs from these two aspects:
(1) Spatial network expansion: Cloning weights for larger networks does not change sparsity.
(2) Temporal expansion: Repeating activities over time does not affect sparsity, ensuring temporal consistency.

For the case of incorporating spatial vectors, the sparsity measure \( I(W) \) should remain invariant when the weight matrix is cloned:

\[
   I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{\|[W, W]\|_p}{\|[W, W]\|_q} = 1 - (2d)^{1/q - 1/p} \cdot \frac{2^{1/p} \|W\|_p}{2^{1/q} \|W\|_q} =I(W)
   \]

This ensures that cloning or repeating the matrix does not affect the sparsity measure.

For the case of incorporating time steps in SNNs, if \( W \) is repeated across \( T \) time steps:
\[
W_T = [W, W, \dots, W] \in \mathbb{R}^{d \times (nT)}
\]
then
   \[
   I(W_T) = 1 - (nT)^{1/q - 1/p} \cdot \frac{\|W_T\|_p}{\|W_T\|_q} = 1 - (nT)^{1/q - 1/p} \cdot \frac{T^{1/p} \|W\|_p}{T^{1/q} \|W\|_q} = 1 - n^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

[1] Hurley, N., & Rickard, S. (2009). Comparing measures of sparsity. IEEE Transactions on Information Theory, 55(10), 4723-4741.

}

{\color{blue}


### **3. How \( I(W) \) Reflects SNN Characteristics**

SNNs have unique characteristics, such as event-driven communication, sparse activation patterns, and temporal dynamics. The measure \( I(W) \) effectively captures these features due to the following reasons:

#### **Event-Driven Sparsity**
- SNNs operate on sparse spikes, where only a small fraction of neurons are active at any given time. \( I(W) \) measures the sparsity of the weight matrix, reflecting how many neurons are connected through non-zero weights. As more neurons activate, \( I(W) \) captures the reduction in sparsity because the network’s weight matrix becomes denser.

#### **Non-Continuous Activation**
- In SNNs, neurons spike or remain inactive, making the activation pattern inherently sparse. \( I(W) \) reflects the concentration of non-zero weights, where fewer active neurons lead to a higher value of \( I(W) \), and more active neurons lead to a lower value.

#### **Temporal Dynamics**
- SNNs exhibit temporal sparsity, where neurons may fire at different time steps. When more neurons spike at the same time, \( I(W) \) captures the temporal reduction in sparsity as the weight matrix becomes denser.

---

### **Conclusion**
The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) satisfies the required properties for measuring sparsity in Spiking Neural Networks (SNNs). It captures both the **static** sparsity of weight matrices and the **dynamic** sparsity associated with neuron activations, making it a suitable and rigorous measure for SNNs.

- **Scaling Invariance**: The measure is unaffected by uniform scaling of weights, ensuring it reflects the relative distribution of weights.
- **Sensitivity to Sparsity Changes**: \( I(W) \) correctly decreases as temporal sparsity reduces, reflecting increased neuron firing.
- **Cloning Invariance**: The measure is consistent when weight matrices are cloned or repeated across network expansions.
- **Network-Specific Properties**: The measure reacts appropriately to weight adjustments and zero-weight additions, capturing the dynamics of sparsity in SNNs.

This ensures that \( I(W) \) is both a valid and meaningful sparsity measure in Spiking Neural Networks, capable of capturing key sparsity characteristics across both static and dynamic aspects of the network.


[1] Hurley, N., & Rickard, S. (2009). Comparing measures of sparsity. IEEE Transactions on Information Theory, 55(10), 4723-4741.
}


%\iffalse

%### **Sparsity in Spiking Neural Networks (SNNs) and the Derivation of \( I(W) \)**

\section{Sparsity in Spiking Neural Networks (SNNs) and the Derivation of \( I(W) \)}

\textcolor{green}{Here is the revised derivation of the sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) for spiking neural networks (SNNs), incorporating the formula update and focusing on scaling invariance, sensitivity to sparsity reduction, and cloning invariance, combined with spatiotemporal dynamics and sparsity in SNNs.}


SNNs communicate through discrete spikes, exhibiting the following key features: (1) Discrete activation: Postsynaptic neurons emit spikes only at specific time points. They are either active (firing spikes) or inactive (not spiking), resulting in sparse data flow.
(2) Structure sparsity: Sparsity refers to the proportion of nonzero elements in a weight matrix.

The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) is rigorously constructed to reflect these properties. 
The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) satisfies these properties, where:
 \( \|W\|_p = \left( \sum_{i=1}^d |w_i|^p \right)^{1/p} \) is the \( \ell_p \)-norm of \( W \),
 \( \|W\|_q = \left( \sum_{i=1}^d |w_i|^q \right)^{1/q} \) is the \( \ell_q \)-norm of \( W \),
 \( d \) is the dimensionality of \( W \),
 \( p < q \) ensures that sparsity is more effectively captured.
The additional term \( 1 - \) allows \( I(W) \) to range between 0 (no sparsity) and 1 (maximum sparsity). Because when the sparsity is 100\%, which means all the elements in SNNs are 0, then \( I(W) \) is 1. While when there are no zero elements, that is, the orginal fully connected SNNs, then \( I(W) \) is 0. The term \( d^ {1/q - 1/p} \) ensures that \( I(W) \) is independent of the vector length, satisfying the cloning property. Without this term, it would vary with the size of  \( W \), even for identical sparsity patterns.
Below, we derive this formula and explain how it aligns with SNN characteristics.


%Spiking Neural Networks (SNNs) exhibit both weight sparsity and spatiotemporal dynamic sparsity. 
%A proper sparsity measure for SNNs must satisfy the following properties: 

%1. Scaling Invariance: The sparsity measure should depend solely on the relative distribution of weights, not their magnitudes.

%2. Sensitivity to Sparsity Reduction: The measure should decrease when sparsity decreases (e.g., weights become more uniform or more nonzero values appear).

%3. Cloning Invariance: The sparsity measure should remain unchanged under network expansion (e.g., cloning weight matrices or expanding tasks).



%This revised formula is analyzed in detail below. Below is a detailed analysis of its derivation and its connection to the characteristics of SNNs.

---



%1. Derivation Based on Sparsity Properties

The measure \( I(W) \) is designed to satisfy the following key properties:

\subsection{Scaling Invariance}

In SNNs, the scaling invariance corresponds to:
- **Independence of weight scaling**: If the weight matrix \( W \) is scaled (e.g., multiplied by a constant), its sparsity structure remains unchanged, and so should \( I(W) \).
- **Independence of temporal scaling**: Changes in spike magnitudes (the activation value) should not affect the sparsity measure, ensuring the measure accurately reflects temporal dynamics.

Under these requirements, the sparsity measure should remain unchanged if the weight matrix \( W \) is scaled by a positive constant \( \alpha > 0 \). Specifically:
\[
I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\|\alpha W\|_p}{\|\alpha W\|_q}
\]
Since:
\[
\|\alpha W\|_p = \alpha \|W\|_p \quad \text{and} \quad \|\alpha W\|_q = \alpha \|W\|_q,
\]
substituting into \( I(W) \) yields:
\[
I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\alpha \|W\|_p}{\alpha \|W\|_q} = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W).
\]

Therefore, in SNNs, it ensures that \( I(W) \) remains unaffected when all weights are scaled proportionally (e.g., multiplying \( W \) by a constant \( \alpha > 0 \)). The scaling weight magnitudes or activation value intensity does not change the network sparsity. 




\subsection{Sensitivity to Sparsity Reduction}

- **Weight sparsity**: Decreased sparsity corresponds to more nonzero weights, leading to a reduction in \( I(W) \).
- **Temporal sparsity**: If more neurons fire simultaneously, temporal sparsity decreases, and \( I(W) \) reflects this reduction.


Consider two weight matrices: (1)  \( W_1 = [10, 0, 0, 0] \). The sparse one with few neurons fire, resulting in a smaller \( \|W\|_p \), a lower  \( \|W\|_q \), and a high  \( I(W) \). (2) \( W_2 = [5, 5, 0, 0] \) Less sparse one with more neurons fire simultaneously, increasing  \( \|W\|_p \) more then  \( \|W\|_q \), causing 
\( I(W) \) to decrease.

1. Compute norms:
   - \( \|W_1\|_p = 10, \quad \|W_2\|_p = 2^{1/p} \cdot 5 \)
   - \( \|W_1\|_q = 10, \quad \|W_2\|_q = 2^{1/q} \cdot 5 \)

2. Sparsity measure:
   \[
   I(W_1) = 1 - d^{1/q - 1/p}, \quad I(W_2) = 1 - d^{1/q - 1/p} \cdot 2^{1/p - 1/q}.
   \]

3. Since \( p < q \), \( 1/p - 1/q > 0 \), so \( 2^{1/p - 1/q} < 1 \). Thus:
   \[
   I(W_2) < I(W_1).
   \]


Thus, it keeps sensitivity to spatial and  temporal sparsity, that is, the distribution of weights or spike activations (firing rates). When it changes weight distribution with more nonzero weights, leading to a reduction in \( I(W) \) corresponds to sparsity decreasing. When temporal sparsity decreases (more neurons firing at the same time), the distribution becomes denser, which directly affects the ratio \( \|W\|_p / \|W\|_q \), leading to a decrease in \( I(W) \).



\subsection{Cloning Invariance}

It should satisfy the property of Cloning Invariance in SNNs from these two aspects:
(1) Spatial network expansion: Cloning weights for larger networks does not change sparsity.
(2) Temporal expansion: Repeating activities over time does not affect sparsity, ensuring temporal consistency.

For the case of incorporating spatial vectors, the sparsity measure \( I(W) \) should remain invariant when the weight matrix is cloned:
\[
I(W) = I([W, W])
\]
This ensures that cloning or repeating the matrix does not affect the sparsity measure.

1. For a cloned matrix \( [W, W] \):
   \[
   \|[W, W]\|_p = 2^{1/p} \|W\|_p, \quad \|[W, W]\|_q = 2^{1/q} \|W\|_q
   \]
   
2. Substituting into \( I([W, W]) \):
   \[
   I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{\|[W, W]\|_p}{\|[W, W]\|_q}
   \]
   \[
   I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{2^{1/p} \|W\|_p}{2^{1/q} \|W\|_q}
   \]
   
3. Simplify:
   \[
   I([W, W]) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]





For the case of incorporating time steps in SNNs, if \( W \) is repeated across \( T \) time steps:
\[
W_T = [W, W, \dots, W] \in \mathbb{R}^{d \times (nT)}
\]

1. Norms for \( W_T \):
   \[
   \|W_T\|_p = T^{1/p} \|W\|_p, \quad \|W_T\|_q = T^{1/q} \|W\|_q
   \]
   
2. Sparsity measure:
   \[
   I(W_T) = 1 - (nT)^{1/q - 1/p} \cdot \frac{\|W_T\|_p}{\|W_T\|_q}
   \]
   
3. Substituting:
   \[
   I(W_T) = 1 - (nT)^{1/q - 1/p} \cdot \frac{T^{1/p} \|W\|_p}{T^{1/q} \|W\|_q}
   \]
   
4. Simplify:
   \[
   I(W_T) = 1 - n^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W)
   \]

In addition, considering temporal sparsity changes,
If neuron activity differs across time steps, sparsity decreases. For dynamic weights \( W_T^\prime = [W^{(1)}, W^{(2)}, \dots, W^{(T)}] \), norms reflect this change:
\[
\|W_T^\prime\|_p = \left(\sum_{t=1}^T \|W^{(t)}\|_p^p \right)^{1/p}, \quad \|W_T^\prime\|_q = \left(\sum_{t=1}^T \|W^{(t)}\|_q^q \right)^{1/q}
\]

Sparsity measure decreases with reduced temporal sparsity:
\[
I(W_T^\prime) = 1 - (nT)^{1/q - 1/p} \cdot \frac{\|W_T^\prime\|_p}{\|W_T^\prime\|_q}
\]



Conclusion:

1. Scaling Invariance: \( I(W) \) depends only on the relative distribution of weights and firing rates, unaffected by scaling.

2. Sensitivity to Sparsity Reduction: 
\( I(W) \) decreases as sparsity reduces, capturing uniformity in weights or spike firing.

3. Cloning Invariance: 
\( I(W) \) remains unchanged across spatial or temporal expansions.
This formulation ensures 
\( I(W) \) is a robust measure for static and dynamic sparsity in SNNs.

%\fi










\iffalse
### **Theoretical Proof: How \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) Measures the Sparsity of Spiking Neural Networks**

Spiking Neural Networks (SNNs) are a special class of neural networks that have unique sparsity characteristics. SNNs communicate through binary-like spikes, exhibiting the following key features:

1. **Event-driven**: Neurons generate spikes only at specific time points, resulting in sparse data flow.
2. **Discrete activation**: Neurons are either active (spiking) or inactive (not spiking), with no intermediate states.
3. **Essential sparsity**: Sparsity refers to the proportion of nonzero elements in a vector or matrix, representing either spike frequencies or sparse activation patterns.

The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) is rigorously constructed to reflect these properties. Below, we derive this formula and explain how it aligns with SNN characteristics.

---

### **1. Derivation Based on Sparsity Properties**

The measure \( I(W) \) is designed to satisfy the following key properties:

#### **(1) Scaling Invariance**  
The sparsity measure should remain unchanged if the weight matrix \( W \) is scaled by a positive constant \( \alpha > 0 \). Specifically:
\[
I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\|\alpha W\|_p}{\|\alpha W\|_q}
\]
Since:
\[
\|\alpha W\|_p = \alpha \|W\|_p \quad \text{and} \quad \|\alpha W\|_q = \alpha \|W\|_q,
\]
substituting into \( I(W) \) yields:
\[
I(\alpha W) = 1 - d^{1/q - 1/p} \cdot \frac{\alpha \|W\|_p}{\alpha \|W\|_q} = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W).
\]

**Interpretation**:  
- In SNNs, scaling weight magnitudes or pulse frequencies does not change the sparsity structure. The scaling invariance property ensures that \( I(W) \) correctly reflects sparsity without being influenced by the absolute values of weights or activations.

---

#### **(2) Sensitivity to Sparsity Reduction**  
As sparsity decreases (e.g., weights become more uniformly distributed or more nonzero values appear), \( I(W) \) should decrease.

- For \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \), this is because \( \|W\|_p \) is more sensitive to weight concentration than \( \|W\|_q \):
  - \( \|W\|_p \) emphasizes smaller weights more strongly, leading to lower values for sparse matrices.
  - When sparsity decreases, \( \|W\|_q \) grows slower than \( \|W\|_p \), causing \( I(W) \) to decrease.

##### Example:
Consider two weight matrices:
- Sparse: \( W_1 = [10, 0, 0, 0] \)
- Less sparse: \( W_2 = [5, 5, 0, 0] \)

1. Compute norms:
   - \( \|W_1\|_p = 10, \quad \|W_2\|_p = 2^{1/p} \cdot 5 \)
   - \( \|W_1\|_q = 10, \quad \|W_2\|_q = 2^{1/q} \cdot 5 \)

2. Sparsity measure:
   \[
   I(W_1) = 1 - d^{1/q - 1/p}, \quad I(W_2) = 1 - d^{1/q - 1/p} \cdot 2^{1/p - 1/q}.
   \]

3. Since \( p < q \), \( 1/p - 1/q > 0 \), so \( 2^{1/p - 1/q} < 1 \). Thus:
   \[
   I(W_2) < I(W_1).
   \]

**Interpretation**:  
- In SNNs, reduced sparsity corresponds to more neurons firing simultaneously or more weights being active. This is captured by the decrease in \( I(W) \).

---

#### **(3) Cloning Invariance**  
The sparsity measure should remain unchanged if \( W \) is cloned or repeated, e.g., \( [W, W] \):
\[
I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{\|[W, W]\|_p}{\|[W, W]\|_q}.
\]

Since:
\[
\|[W, W]\|_p = 2^{1/p} \|W\|_p \quad \text{and} \quad \|[W, W]\|_q = 2^{1/q} \|W\|_q,
\]
substituting yields:
\[
I([W, W]) = 1 - (2d)^{1/q - 1/p} \cdot \frac{2^{1/p} \|W\|_p}{2^{1/q} \|W\|_q}.
\]

The scaling factors \( 2^{1/p} \) and \( 2^{1/q} \) cancel out:
\[
I([W, W]) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} = I(W).
\]

**Interpretation**:  
- In SNNs, expanding a network by cloning its structure does not change the inherent sparsity. Cloning invariance ensures consistent comparisons across networks of different sizes.

---

\fi

\section{
2. Connection to SNN Characteristics}

#### **Property 1: Event-driven Sparsity**
SNNs transmit information through sparse spike signals. The nonzero weights in \( W \) determine which neurons can be activated. Higher sparsity implies fewer active connections and a more efficient activation pattern.

- **Sparsity measure**: \( I(W) \) reflects the distribution of weights in \( W \). Sparse weights lead to higher \( I(W) \), while denser weights reduce it.
- **Practical impact**: Sparse weights lower energy consumption and computational complexity in event-driven systems.

---

#### **Property 2: Discrete Activation**
SNN activations are binary (spiking or not spiking). The sparsity of \( W \) directly impacts the distribution of these activations.

- **Binarized weights**:
  -  The reduction in weight magnitudes causes \( \|W\|_p \) to decrease significantly, especially for small \( p \).
  - The ratio \( \frac{\|W\|_p}{\|W\|_q} \) captures these changes, as \( \|W\|_p \) is more sensitive to sparsity for \( p < q \).

---

#### **Property 3: Temporal and Spatial Sparsity**
SNN sparsity is both spatial (nonzero weights) and temporal (spike patterns). Sparse weights result in sparse spike activations over time, reducing redundancy.

- **Temporal sparsity**: Sparse weights lead to fewer simultaneous activations. Changes in sparsity are captured by the faster reduction of \( \|W\|_p \) compared to \( \|W\|_q \), reflected in \( I(W) \).

---

### **3. Summary**

#### **Rigorous Sparsity Measurement**
The measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) satisfies:
1. **Scaling Invariance**: Independent of weight magnitudes or spike intensities.
2. **Sensitivity to Sparsity Reduction**: Sensitive to the concentration and distribution of weights.
3. **Cloning Invariance**: Consistent across different network sizes.

#### **Relevance to SNNs**
\( I(W) \) is closely aligned with SNN characteristics:
1. **Spatial sparsity**: Reflects changes in weight distribution.
2. **Temporal sparsity**: Captures pulse activation sparsity via weight dynamics.
3. **Efficiency**: Sparse weights and sparse pulses improve computational efficiency.

Thus, \( I(W) \) is a robust sparsity measure for analyzing and comparing the sparsity of SNNs across tasks and network architectures.




\section{\textcolor{red}{About the application to BNNs}}

{\color{purple} {
### Why the method cannot be directly applied to binary neural networks

The sparsity measure \( I(W) \) is designed for continuous-valued weight matrices \( W \), where the distribution of weights plays a key role in determining sparsity. In binary neural networks (BNNs), where weights and activations are constrained to discrete values (e.g., \( \{0, 1\} \) or \( \{-1, 1\} \)), \( I(W) \) faces significant limitations that render it unsuitable for direct application. Here’s why:



### 1. **Binary Weights Lack Variability in Magnitude**

In binary neural networks:
- The weight matrix \( W \) is constrained to discrete values such as \( \{0, 1\} \), \( \{-1, 1\} \), or \( \{0, -1, 1\} \).
- Nonzero weights have the same magnitude (e.g., \( 1 \) or \( -1 \)).
Thus there is some negative effect on Norms:
(1) \( \|W\|_p \) and \( \|W\|_q \) are no longer sensitive to weight distributions because binary weights lack variability in magnitude.
   - For example, if \( W \) contains only \( 0 \) and \( 1 \), the \( p \)-norm is essentially proportional to the number of nonzero elements:
     \[
     \|W\|_p = \left( \sum_{i=1}^d |w_i|^p \right)^{1/p} = \text{(number of nonzero elements)}^{1/p}.
     \]

(2) \( \|W\|_p / \|W\|_q \) becomes deterministic for a given sparsity level (proportion of nonzero elements). This eliminates the ability of \( I(W) \) to distinguish between different distributions of weights, making it ineffective for binary matrices.


### 2. **Loss of Sensitivity to Weight Distribution**
In continuous neural networks, \( I(W) \) reflects sparsity by analyzing the distribution of weights through the \( \ell_p \)- and \( \ell_q \)-norms. The difference between these norms highlights how concentrated or evenly distributed the weights are. However, in binary networks:
- All nonzero weights have the same magnitude, so the distribution of weights does not vary meaningfully.
- \( I(W) \) cannot capture sparsity differences beyond simply counting nonzero elements.

We would give an examples by considering two binary weight matrices:
\[
W_1 = [1, 0, 0, 0], \quad W_2 = [1, 1, 0, 0].
\]
1. For both \( W_1 \) and \( W_2 \), the weights are binary (either 0 or 1).
2. The norms depend only on the count of nonzero weights:
   - \( \|W_1\|_p = 1, \quad \|W_2\|_p = 2^{1/p}. \)
   - \( \|W_1\|_q = 1, \quad \|W_2\|_q = 2^{1/q}. \)

3. \( I(W_1) \) and \( I(W_2) \) simply reflect the number of nonzero elements, failing to account for the structural distribution of weights.

Therefore, \( I(W) \) cannot be directly applied to BNNs because binary weights lack variability in magnitude, making \( I(W) \) insensitive to differences in weight distribution.

}}



{\color{pink}{

**About the citation format**
Thanks for your suggestion. We have modified all the citation format into "(Hu et al., 2024)". 

**About the performance improvement **

We understand that the modest improvement in accuracy or network connectivity might raise concerns regarding the practical impact of our proposed approach. However, we would like to emphasize the following points that highlight the significance and potential contributions of our work.
The proposed two-stage sparse training method for SNNs is particularly well-suited for **sparse training from scratch**, as it maintains sparsity consistently throughout the entire training process, unlike traditional methods that rely on post-training pruning or late-stage sparsification. This approach allows SNNs to fully leverage the computational and energy efficiency benefits of sparsity during both training and inference. Furthermore, sparse training from scratch eliminates the memory and computational overhead associated with transitioning from dense to sparse models, making it highly effective for resource-constrained environments such as neuromorphic hardware. As demonstrated in Table 1, the observed **1\% performance improvement** on CIFAR10 and **1.07\% on CIFAR100** compared to the fully non-sparse model may seem modest at first glance, but in the context of **sparse training from scratch for SNNs**, these gains are significant. our method achieves accuracy levels that are competitive with existing work, while maintaining the **unique advantage of sparse training from scratch (under 30\%-40\% sparsity)**, reinforcing its practicality and effectiveness in delivering both sparsity and performance improvements simultaneously.

**About the unique for SNNs**


**The relationship of PQ index in the second stage**

The first stage involves the typical training process and attempts to identify an appropriate rewiring ratio based on temporarily trained weights, according to the PQ index. Then the PQ index helps quantify the redundancy in the network, thereby informing the following rewiring strategy. Then the obtained rewiring ratio would guide the structure rewiring in the second stage.
That is, in the second stage, the dynamic sparse structure learning method based on the rewiring method is adopted to implement the sparse training from scratch. The connections are iteratively pruned and regrown according to the specified rewiring ratio. This iterative training approach ensures that the network continuously adapts and optimizes its structure, thereby improving performance. 

**About the energy consumption**



*Adaption to the Transformer-based SNNs*

(1) First-stage applicability of \( I(W) \) to Transformer Architectures:
The sparsity measure \( I(W) = 1 - d^{1/q - 1/p} \cdot \frac{\|W\|_p}{\|W\|_q} \) is suitable for measuring sparsity in static weight matrices in Transformer-based SNN architectures, such as those in feedforward layers and attention mechanisms. For dynamic sparsity (e.g., attention patterns), \( I(W) \) may require adaptation to account for variations across sequences or time steps. With these adjustments, \( I(W) \) can provide meaningful insights into the sparsity levels in Transformer-based SNNs.

(2) Second-stage applicability of Rewiring to Transformer-Based SNNs:
Rewiring (pruning and growing connections) is highly applicable to Transformer-based SNNs. For instance, the rewiring of attention layers can prune less important attention heads or connections and regrow critical ones to maintain performance.
But we should improve the rewiring in attention layers to  align with SNNs’ dynamic and event-driven nature, enabling adaptive connectivity adjustments based on spike patterns and reducing computational costs while maintaining performance.

Therefore, the proposed twp-stage sparse training methods for SNNs could adapt to Transformer-based SNNs but may need modifications for dynamic attention mechanisms.

}}



{\color{blue}{

**About the  training process**

The proposed method employs a two-stage approach to achieve dynamic sparse structure learning in Spiking Neural Networks (SNNs), leveraging a rewiring strategy for efficient training.

In the **first stage**, the network undergoes a typical training process to identify an appropriate rewiring ratio based on temporarily trained weights. This ratio is calculated using the PQ index, which quantifies network redundancy and informs the rewiring strategy. As a result, an adaptively suitable rewiring ratio \( c_i \) is determined for each training iteration during this stage.

In the **second stage**, the rewiring-based dynamic sparse structure learning method is applied to implement sparse training from scratch. Connections are iteratively pruned and regrown according to the specified rewiring ratio. This iterative approach enables the network to continuously adapt and optimize its structure, leading to improved performance. The rewiring mechanism dynamically adjusts the network by activating and growing previously dormant connections, enhancing the overall expressiveness and capability of the SNN.

Notably, due to the adaptive rewiring ratios applied across different training iterations, the network's density does not remain constant throughout the training process, even though the remove and regrow fractions are equal. This dynamic adjustment ensures flexibility and efficiency, enabling the network to better align its structure with the learning task at hand.

**About the training time**

Thank you for raising this important question about training time. Actually, in Table 1, the comparison experiments are conducted with the same number of training iterations (300) for both the original dense-connected SNNs and our sparse training SNNs to ensure a fair comparison. These results show that our method achieved competitive accuracy and sparsity within this budget. Compared with the ESL-SNNs under the same setting on the CIFAR10 dataset, the extra training time only comes from the rewiring ratio computing in the first stage of our two-stage training method. We found that increasing training time is extremely small (less than 0.1\%) for each training iteration.
Meanwhile, while dynamic rewiring adds slight computational overhead, the reduced number of active connections in the sparse network offsets this, keeping the training time per iteration comparable to dense training. Together with the sparse training from scratch, our method could avoid excessive training time and maintain efficiency. 

**About the symbols**

Thanks for your suggestion. The $n^k$ are the number of the neurons in the \(k_th\) layers. \(\epsilon\) is a constant (or scaling factor) that influences the edge probability and accounts for sparsity or connectivity scaling. We control the initial sparsity of SNNs by controling the value of \(\epsilon\).
}}


{\color{black}{
**About the performance comparison with UPR and STDS**

Thanks for pointing that out. About the UPR [1] accuracy reference, there are different results in the original text due to different sparsity level ( controlled by \(\labmda\) settings in UPR [1] ). We have supplemented the settings with the above 81.0\% (Acc.), 4.46\% (Conn.) and 2.50M (Param.) for comparison with UPR[1]. 
We emphasize that our method focuses on achieving low accuracy loss compared to densely connected SNNs while maintaining a fully sparse training process. This approach offers several notable advantages. For instance, on the CIFAR10 dataset, models trained using our proposed method under the neuron-wise scope demonstrate approximately a 1\% improvement in performance compared to fully dense models, all while maintaining a sparsity level of 30\% to 40\%. Similarly, on the CIFAR100 dataset, our two-stage sparse training method improves the performance of SNN models by 1.07\% compared to their non-sparse counterparts, achieving a connectivity of only 29.48\%. Meanwhile, it is worth noting that the proposed two-stage training method proceeds sparse training from scratch and maintains sparse training during the whole training process. 


As to the performance gap comparison with STDS [2], STDS achieves higher accuracy (79.8\%) and better sparsity (4.67\% connectivity and 0.24M parameters), mainly caused by the more complex synaptic model with weight reparameterization. STDS maps synaptic spine sizes \(|\theta|\) combined with trasition threshold \(d\) to real connection weights \(w\), incorporating both excitatory and inhibitory connections and transitioning between synaptic states. These complex synaptic models enhance network expressiveness but introduce additional computational models for weight.
In contrast, our two-stage sparse training method avoids such weight reparameterization while maintaining simplicity and generalizability across datasets and architectures. Additionally, differences in experimental settings, such as learning rates or data augmentation strategies, may partially explain the performance gap. We will provide a detailed discussion on this point in the revised manuscript to ensure transparency and comparability.


**About the performance on ImageNet-1K**

Due to the limited time during the rebuttal period, we supplement the following experiments when applied our method on ImageNet-1K dataset without much hyper parameter tuning process. Under the structure of the SEW ResNet18 [1], the proposed two-stage sparse training method achieves the accuracy of 61.47\% with 4 time steps under the parameter amount of 2.16M and neuron-wise setting. The accuracy loss is 1.59\% compared to the accuracy of 63.06\% achieved by the original model based on temporal efficient training [2] by our implementation. Compared to the STDS with 61.51\% (Acc.), -1.67\% (Loss.) and 2.38M (Param.) and UPR with  60.00\% (Acc.), -3.18\% (Loss.) and 3.10M (Param.) , the performance of our model is competitive combining with the sparse training from scratch advantage.    

[1]Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., and Tian, Y. (2021). Deep residual learning in spiking neural networks. Advances in Neural Information Processing Systems, 34, 21056-21069.
[2]Deng, S., Li, Y., Zhang, S., and Gu, S. Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting. In International  Conference on Learning Representations.2022.


**About energy consumption**

We appreciate your feedback and provide an analysis of energy consumption for both ANNs and SNNs, emphasizing their differences and relevance for neuromorphic applications.

Since ANNs rely on extensive floating-point operations (FLOPS) for multiplications and additions. Energy consumption is estimated as:
   \[
   \text{Energy}_{\text{ANN}} = \text{FLOPS} \times 12.5 \, \text{pJ}
   \]
Meanwhile, SNNs reduce energy by activating computations only when neurons spike. Energy is estimated as:
   \[
   \text{Energy}_{\text{SNN}} = \text{SOPS} \times 77 \, \text{fJ}
   \]


| Dataset   | Architecture       | Energy (ANN, μJ) | Energy (SNN, μJ) | Reduction (\%) |
|-------------------|-------------------------|-----------------------|-----------------------|-------------------|
| CIFAR10          | ResNet19 (Neuron-wise) | 15.13                | 12.19                | 19.4\%            |
| CIFAR10          | ResNet19 (Layer-wise)  | 15.13                | 10.26                | 32.2\%            |
| CIFAR100         | ResNet19 (Layer-wise)  | 15.13                | 10.80                | 28.6\%            |



SNNs offer significant energy savings compared to ANNs (up to 32\%) while maintaining high accuracy, particularly on event-driven tasks. These results underline the efficiency of SNNs, especially with our pruning method, for neuromorphic applications.


We hope this response and the proposed revisions adequately address the reviewer’s concerns. We appreciate your valuable feedback, which has significantly improved the clarity and rigor of our work.






}}

\end{document}
