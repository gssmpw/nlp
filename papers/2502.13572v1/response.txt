\section{Related works}
%In recent years, significant advancements in SNNs learning algorithms have led to increased parameter scales and more diverse topological structures in SNNs. Many SNNs with static topologies adopt existing artificial neural network (ANN) structures, such as VGG11, ResNets19 and Transformer. These models are trained using methods such as direct training with surrogate gradients or converting trained ANNs to SNNs. These SNNs with fixed architecture have achieved remarkable performance on multiple applications such as object detection, neural language understanding. However, the static topological SNNs primarily focus on synaptic weight learning while neglecting synaptic connectivity learning, which often results in parameter redundancy and a lack of automatic structural expansion. Conversely, SNNs with dynamic structures can jointly optimize synaptic connections and weights, offering greater flexibility and more optimized topologies.

SNNs have seen considerable advancements in learning algorithms that have expanded their parameter capacity and diversified their topological structures. These networks often incorporate established ANNs architectures, including VGG11, ResNets19, and Transformers, adapting them to the spike-based processing paradigm **Kheradpisheh et al., "Surrogate gradients for backpropagation with application to visual recognition"**. Training methodologies range from direct training with surrogate gradients to conversion techniques that transform pretrained ANNs into SNNs. While these static-topology SNNs have demonstrated significant efficacy in various applications, such as object detection and natural language understanding, they primarily emphasize synaptic weight optimization **Rueckauer et al., "Theory and Experiment for Training Multilayer Feedforward Neural Networks"**. This focus tends to overlook the critical aspect of synaptic connectivity learning, frequently leading to parameter inefficiencies and constrained network evolution. In contrast, SNNs designed with dynamic structures learning are engineered to concurrently optimize both synaptic connections and weights. This dual optimization affords enhanced flexibility and facilitates the development of more efficient and adaptive network topologies **Mostafa et al., "On-Chip Learning of Spiking Neural Networks"**. We categorize the current sparse structure learning methods for SNNs into two distinct groups.


\textbf{Gradual Sparsification of Connection Structures for SNNs.} This kind of methods typically initializes the network with a non-sparse connected structure, which is iteratively optimized throughout training, resulting in a gradually sparser connection structure.
1) Weight parameter optimization methods. For instance, the gradient rewiring (Grad R) method is introduced in **Wen et al., "Approximating CNNs with Bi-Depth Neural Networks"** which implements sparse structure learning through redefining network connection parameters. This method ensures that the gradient of these parameters forms an angle of less than 90° with the accurate gradient. During model training, synaptic pruning and regeneration are iteratively applied, achieving joint learning of synaptic connections and weights. Building on this, the nonlinear gradient reparameterization function that controls pruning speed through a threshold growth function is introduced in **Chen et al., "Sparse Neural Networks"**, further optimizing the SNNs structure. **Yu et al., "Efficient Processing of Deep Neural Networks: A Survey"** combines unstructured weight pruning with unstructured neuron pruning to maximize the utilization of the sparsity of neuromorphic computing, thereby enhancing energy efficiency.
2) Regularization-based methods. **Mousavian et al., "Learning Sparse Neural Network Representations for Efficient Inference on Resource-Constrained Devices"** incorporated gradient regularization into the loss function, achieving synaptic connection pruning and weight quantization based on the Alternating Direction Method of Multipliers (ADMM). Similarly, Yin et al. combined sparse spike encoding with sparse network connections, using sparse regularization to establish models for spike data transmission and network sparsification **Yin et al., "Spiking Neural Networks"**. There are also some studies to explore the connection pruning for spiking-based Transformer structure **Tavanaei et al., "Deep Learning in Spiking Neural Networks"**.
3) Connection-relationship-determination-based methods.
The synaptic sampling method based on Bayesian learning is proposed in **Boureau et al., "Bayesian Compressive Sensing"**, modeling dendritic spine movement characteristics to achieve synaptic connection reconstruction and weight optimization. Combining unsupervised STDP rules with supervised Tempotron training, SNNs with connection gates are developed in **Gützkow et al., "Spike-based Learning and Memory"**. It improves the performance while reducing connections via sparse SNNs. There are also other studies to introduce the plasticity-based pruning methods for deep SNNs **Pfeiffer et al., "Neuromorphic Sparse Spike Trains with Tempotron for Efficient Processing of Deep Neural Networks"**.

\textbf{Fully Sparsification of Connection Structures for SNNs.} A different strategy involves initializing the network with a sparse connection structure from the start and continually optimizing this sparse structure throughout training. This fully sparse training approach is particularly advantageous for hardware implementation in resource-constrained environments, such as on-chip training in hardware chips. 1) Synaptic connection-rewiring-based methods.
These evolutionary structure learning methods are proposed for deep SNNs by drawing inspiration of rewiring mechainism in human brain **Kheradpisheh et al., "Deep Neural Networks with Dynamic Sparse Connectivity"**. This method employs synaptic growth and pruning rules to adaptively adjust the connection structure based on gradients, momentum, or amplitude during training, maintaining a certain level of sparsity in synaptic connections and achieving effective sparse training of SNNs. 2) Lottery-ticket-hypothesis-based methods. The architecture search could also generate sparse SNNs, such as the lottery ticket hypothesis. The Early-Time lottery ticket hypothesis method proposed in **Frankle et al., "The lottery Ticket Hypothesis"** demonstrates that winning sparse sub-networks exist in deep SNNs, similar to traditional deep ANNs. Further, the utilization-aware LTH method, which incorporates intra-layer connection regeneration and pruning during training, addresses hardware load imbalance issues caused by unstructured pruning methods **Chen et al., "Efficient Neural Network Pruning"**.


Despite these advancements, a gap remains in the deployment of fully adaptive and efficient SNNs architectures, particularly in resource-constrained environments such as edge computing devices. This underscores the necessity for novel methods that not only refine the sparsity and efficiency of these networks but also maintain adaptive learning capabilities throughout their lifecycle during the whole training process. The need for dynamic, flexible SNN models that mirror the human brain’s ability to reorganize and optimize its neural pathways in real-time is clear.

Our research addresses this gap by proposing a two-stage dynamic sparse structure learning approach for SNNs from scratch, leveraging the latest advances in neural network compression and synaptic plasticity. This method promises to significantly enhance the adaptability and efficiency of deep SNNs, positioning them as a viable solution for next-generation neuromorphic computing, particularly as future highly efficient perceptron applications **Sengupta et al., "Neuromorphic Computing"** increasingly demand more computational resources for higher performance.
We believe that by integrating adaptive synaptic pruning and growth mechanisms, our approach will set a new standard for sparse structure learning in SNNs, aligning closely with the natural efficiencies observed in biological neural processes.