\section{Conclusion}

In this work, we propose a novel approach to mitigate hallucinations in diffusion models, specifically addressing artifacts that occur organically in unconditional generative settings.
Preliminary experiments reveal that self-attention layers in early denoising stages may inadvertently amplify or suppress noisy features, potentially leading to hallucinations.
Leveraging this insight, we propose a novel adaptive temperature scaling strategy, augmented by masked perturbations, to dynamically modulate the attention distribution, effectively reducing hallucinations. 
Experiments across datasets demonstrate significant improvements in FID scores and hallucination reduction, enhancing the fidelity and reliability of generated images.
