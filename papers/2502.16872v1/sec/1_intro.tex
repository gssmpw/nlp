\section{Introduction}
\label{sec:intro}

The ability of diffusion models \citep{ho2020ddpm,rombach2022sd,ramesh2022hierarchical} to progressively transform noise into coherent images through iterative denoising has established them as a powerful framework for generating high-quality, realistic images.
Despite their success, diffusion models are prone to certain flaws, with one persistent issue being the emergence of \textit{hallucinations} ---  generation of unrealistic or incorrect features that deviate from the training data distribution. 

Hallucinations can significantly degrade the quality of generated images, undermining the reliability of these models in real-world applications.
The increasing prevalence of AI-generated synthetic images online \citep{bender2023peacock} further compounds this issue, as \citet{aithal2024understanding} highlight that training future models on such data (some of which may include hallucinations), leads to performance degradation and can eventually collapse. 
Therefore, mitigating hallucinations is essential for preserving the fidelity of generated images and enhancing their practical utility.

Surprisingly, there has been little to no focused work on directly addressing hallucinations in diffusion models, especially in unconditional generation settings. 
The pioneering work of \citet{kim2024tackling} mitigates hallucinations through multiple local diffusion processes; however, it does not extend to unconditional generation. 
Additionally, the recent work of \citet{aithal2024understanding} explores hallucinations in unconditional models through the lens of mode interpolation and proposes a method for detecting them, however does not address their mitigation. 
Building on these foundations, we propose a novel approach to mitigating hallucinations in unconditional diffusion models, grounded in an analysis of the self-attention mechanism within the denoising/generative process of diffusion models.
While \citet{kim2024tackling} focus on tackling hallucinations that arise in image-conditional diffusion models due to out-of-domain conditioning (\eg, in tasks like super-resolution), the type of hallucinations we address is fundamentally different. 
Our work aims to mitigate hallucinations that occur {\it organically} in unconditional generative models. 
This scope of hallucinations aligns more closely with the phenomenon described by \citet{aithal2024understanding}, who provide explainability for these naturally occurring hallucinations through mode interpolation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline.jpg}
    \caption{\textbf{Overview of our proposed pipeline for mitigating hallucinations in diffusion models with adaptive temperature scaling.} At each denoising timestep $t$, the noisy input $x_t$ is processed through a denoising UNet comprising self-attention layers. Conventional self-attention (\textit{top-right}) employs a fixed attention distribution, which may inadvertently amplify or suppress features, leading to hallucinations in the generated image $x_0$. In contrast, our approach (\textit{bottom-right}) introduces a temperature-scaled self-attention, where the temperature $\tau$, is dynamically adjusted based on an anomaly score $s$ from an anomaly detection model. This adaptive mechanism modulates the attention distribution, reducing noise-induced artifacts while preserving essential features, thereby enhancing the overall realism and fidelity of the generated images. }
    \label{fig:pipeline}
    \vspace{-5pt}
\end{figure*}

Our initial experiments led us to hypothesize that self-attention layers during early denoising steps play a critical role in amplifying or suppressing candidate noisy \textit{blobs} within an image.
These blobs, which represent potential image features, can become the seeds for hallucinations when improperly attended to during early denoising stages. 
In essence, we find that the way self-attention emphasizes or deemphasizes certain regions can lead to the manifestation of undesired or unrealistic content in the generated image.

To address this, we propose \textit{Adaptive Attention Modulation (AAM)}, a novel mitigation strategy centered around the introduction of a temperature parameter within the softmax operation of the self-attention layers.
This temperature parameter modulates the attention distribution, controlling the effective radius that each pixel can attend to. 
By carefully tuning this parameter, we can prevent unwarranted emphasis or suppression while ensuring that in-distribution content is preserved. 
This fine balance plays a crucial role in mitigating the emergence of hallucinations.

Recognizing that a single temperature parameter does not universally mitigate hallucinations, we employ an adaptive inference-time optimization to dynamically update the temperature as the denoising process progresses. 
While this approach substantially reduces hallucinations, we observed that certain blobs, particularly those manifesting in very early stages, can persist despite our temperature-based tuning. 
To address this, we introduce a complementary masked perturbation strategy, which involves selectively perturbing regions identified as potentially anomalous with random noise, effectively disrupting the propagation of these features through subsequent denoising steps.

To the best of our knowledge, this is the first work to address the issue of organically emerging hallucinations in diffusion models within an unconditional generation setting. 
In summary, our key contributions are as follows:
\begin{itemize}
    \item We investigate the role of self-attention in the manifestation of hallucinations during early denoising stages in diffusion models.
    \item We introduce a temperature scaling mechanism in self-attention layers as a simple yet effective approach to modulate the attention distribution, effectively controlling hallucinations.
    \item We propose a novel strategy to mitigate hallucinations by dynamically adjusting the temperature parameter, complemented with a masked perturbation technique. 
\end{itemize}