\section{Related Work}

\paragraph{Diffusion Models.}
Diffusion models, rooted in non-equilibrium thermodynamics theory, were introduced by \citet{sohl2015deep} and have since evolved into a robust framework for image generation.
The introduction of Denoising Diffusion Probabilistic Models (DDPMs) by \citet{ho2020ddpm} significantly advanced the field by demonstrating high-quality image synthesis through efficient reparameterization and objective functions. This sparked widespread interest and further development, including score-based diffusion by \citet{song2020score} that unified score-based generative modeling with diffusion processes, and improvements such as DDIM \citep{song2020ddim} and ADM \citep{nichol2021improved}, which reduced sampling steps for faster generation.

Conditional diffusion models have also gained traction, with guided-diffusion \citep{dhariwal2021guideddiffusion} enabling class-conditional synthesis and advancements like Imagen \citep{saharia2022imagen} and DALLE-2 \citep{ramesh2022dalle2} for text-to-image generation. 
Latent diffusion models introduced by \citet{rombach2022sd} further increased efficiency by operating in compressed latent spaces while maintaining quality.

Beyond image synthesis, diffusion models have been adapted for inpainting \citep{lugmayr2022repaint, corneanu2024latentpaint}, super-resolution \citep{saharia2022image, gao2023implicit}, and image translation \citep{saharia2022palette, wolleb2022swiss}. 
Furthermore, latent representations learned by diffusion models have found use in discriminative tasks, such as semantic segmentation, classification, and anomaly detection \citep{mukhopadhyay2023text, graikos2022diffusion, zimmermann2021score, wyatt2022anoddpm, pinaya2022fast}.

\vspace{-10pt}
\paragraph{Hallucinations.}  Diffusion models, despite their success, still suffer from failure modes, including hallucinations \citep{borji2023qualitative,narasimhaswamy2024handiffuser}, limited compositionality \citep{conwell2023comprehensive,gokhale2022benchmarking}, model memorization \citep{carlini2023extracting,somepalli2023diffusion}, and incoherent conditional generation \citep{liu2023discovering}. 
Hallucinations, in particular, refer to the generation of content that is not grounded in or consistent with the training data distribution, severely degrading the reliability and fidelity of generations. 
This flaw significantly hinders the practical applicability of diffusion models in real-world scenarios. 
\looseness=-1

Hallucinations have been extensively studied in large language models (LLMs) \citep{huang2023survey, tonmoy2024comprehensive, zhang2023siren} and large vision-language models (LVLMs) \citep{liu2024survey, guan2023hallusionbench, bai2024hallucination}, but remain relatively underexplored in image-focused diffusion models. 

\citet{kim2024tackling} tackled structural hallucinations in conditional generation by segmenting and processing out-of-distribution (OOD) regions separately. However, their approach is limited to image-conditional settings.
\citet{aithal2024understanding} studied hallucinations in unconditional diffusion models, attributing them to mode interpolation, where the model interpolates between data modes, leading to artifacts. 
They further observed that high variance in intermediate denoised outputs across successive timesteps signals emergence of hallucinations, implying a degree of self-awareness.
However, they did not propose a mitigation strategy.

Building on these insights, we hypothesize that self-attention layers in early denoising stages facilitate hallucinations by inappropriately amplifying or suppressing noisy features. 
To counter this, we introduce a dynamic temperature scaling to refine the attention distribution, focusing on relevant features and reducing artifacts.

\vspace{-10pt}
\paragraph{Temperature-Scaled Self-Attention.} Self-attention has become a transformative mechanism in machine learning, enabling models to assign varying importance to different parts of an input sequence, effectively capturing long-range dependencies and contextual relationships \citep{vaswani2017attention}.  
In vision tasks, self-attention enables the modeling of global interactions across an image, enhancing feature coherence. 
More recently, temperature scaling within self-attention mechanisms has emerged as a method to control the sharpness of the attention distribution, thereby influencing which correspondences are emphasized or suppressed.

Previous research has demonstrated the benefits of temperature-scaled self-attention in various contexts. 
\citet{zhang2021attention} used temperature scaling to smooth the attention distribution of teacher models in abstractive summarization, while \citet{lin2018learning} applied a self-adaptive approach to enhance language translation quality. 
In computer vision, \citet{zhou2023learning} incorporated temperature scaling to better utilize distant contextual information for image inpainting, and \citep{anonymous2025} employed temperature scaling to control the level of correspondence between the query and prompt image in visual in-context learning. 

Inspired by these successes, we introduce an adaptive temperature-scaled self-attention strategy within the denoising process of diffusion models. 
Our approach dynamically adjusts the temperature parameter based on an anomaly score, ensuring that attention is focused on relevant features while mitigating the amplification of noise. 
This technique offers a novel approach to mitigating hallucinations, effectively harnessing the strengths of self-attention while addressing its limitations.