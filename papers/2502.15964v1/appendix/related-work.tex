\section{Extended Related Work}
\label{app:related-work}

% Themes:
% Multi-llm systems
% - multi-agent systems 
% - routing
% Orchestration of LLMs
% - document processing stuff
% - archon
% - ama 
% - writing in the margins
% Edge commputing with LLMs
%  - one sentence about privacy, signalling clearly we're not doing that 
% Efficiency 
% - speculative decoding, this is not what we're doing




\paragraph{Orchestration of LMs }
Recent works attempt to improve long document processing by taking a divide-and-conquer approach akin to \system. Instead of using single LM calls with the entire context, the task is decomposed into smaller tasks to be executed on chunks of context. \citep{zhang2024chain, zhou2024llm} use a predefined protocol for chunk processing (defined by a prompt). \citep{shankar2024docetl} performs a more involved automated pipeline optimization (via agent-based rewrite directives). Crucially, none of the works study the cost-efficient interaction between a small local LM and large remote LM and instead focus exclusively on larger LMs (70B parameters and above). Moreover, they do not explore multi-round communication patterns for document analysis. 

\paragraph{Long-context management techniques} These works aim to improve \emph{single LM} accuracy in long context tasks. \citep{russak2024writing} prefill the context using \emph{chunks} of the document, summarize each chunk (using a predefined prompt), and aggregate the results. This improves accuracy and requires marginal additional computation. PRISM similarly \citep{jayalath2024long} processes the context as a stream of chunks, and writes important information into a typed data structure which can be amended as needed. MemGPT \citep{packer2023memgpt} proposes a virtual memory paging system inspired by operating systems, where the LLM manages information across main context (akin to RAM) and external storage (akin to disk). When approaching context limits, the system actively decides what information to preserve and can later retrieve this information through paginated function calls. Orthogonally, other methods explore the usage of code for context management~\citep{arora2023evaporate}.

% and Ask Me Anything

% \yell{todo}
% Evaporate \citep{arora2023evaporate}
% Ask me anything \citep{arora2022ask}

\paragraph{Cost-efficient multi-LLM Systems} 
A plethora of recent works show that multiple LMs can collaborate on a task to improve both accuracy and efficiency \citep{guo2024large}. The most similar work is perhaps \citep{wang2024mixture} which neither investigates LMs with with asymmetric capabilities nor optimizes for local compute efficiency. 

% other refs from sabri below
% \citep{qian2024scaling,shen2024learning,agashe2023evaluating}

\paragraph{Model routing techniques} Our work studies a collaboration of LMs, and thus should be differentiated from model routing techniques \citep{chen2024more,chen2023frugalgpt} that route a prompt to the appropriate single LM that can completely answer it using the full context. This is often done for cost reduction, identifying that simple tasks can be executed by smaller and cheaper LMs. 

\paragraph{Compound LM systems} Recent works explore the use of LMs as part of more elaborate pipelines that, retrieval models, tool use, and more. \citep{saad2024archon,khattab2023dspy,yuksekgonul2024textgrad} seeks to optimize the pipeline architecture and prompts using different approaches, which we do not pursue on this work.

\paragraph{Retrieval-Augmented Generation (RAG)} RAG is a hybrid approach that integrates information retrieval into the text generation process, leveraging external knowledge sources to enhance the output of language models (LMs). Instead of relying solely on parametric memory, RAG reduces the number of tokens processed by an LM by first retrieving a subset of relevant documents or document chunks and appending them as context to the LM~\citep{lewis2020retrieval, karpukhin2020dense, lee2019latent, izacard2021unsupervised, guu2020retrieval}. This retrieval step mitigates issues such as hallucination and knowledge staleness, which are common in traditional autoregressive models~\citep{shuster2021retrieval, petroni2019language}. We differ in two ways: first, our local LM can perform tasks beyond information extraction, such as summarization or reasoning. Second, by performing arbitrary tasks on document chunks, the small LM communicates its compact answer instead of the raw document chunk, which amounts to sending fewer tokens to remote.


\paragraph{Speculative decoding}
Speculative decoding \citep{leviathan2023fast,zhang2024fast,chen2024sequoia} techniques are addressing the different question of how to effectively sample from the distribution of a large LM by only sampling from smaller LM and using the large LM for cheaper, likelihood evaluation (using the ``acceptance-complement algorithm'' \citep{devroye2006nonuniform}). It neither considers a collaboration between two LMs, nor attempts to minimize the communication between them.

% \textbf{Speculative decoding} \citept{leviathan2023fast} introduce a sampling scheme that achieves exact sampling from a large LM $x \sim p(x)$, but through proposing samples only from a small LM $x \sim q(x)$, and using $p(x)$ only for evaluation, which is done efficiently in one forward pass. Their trick -- also known as the ``acceptance-complement algorithm'' \citep{devroye2006nonuniform} -- involves a clever use of the ratio of $p(x)$ and $q(x)$ to determine when to accept or reject samples. Speculative decoding neither attempts to minimize the number of tokens sent up to the large LM, nor it allows for a free-form chat between a small and a large model. We also note that this work does not aim to produce exact samples from the large LM, which in itself can be limited and can potentially be improved as part of an edge-remote system.

\paragraph{On-device language models for privacy}
~\citet{siyan2024papillon, zhang2024cogenesis} attempt to prevent leaks of private information to a cloud-hosted LM API by mediating the communication with a local privacy-aware LM that removes private information from the prompt. While the local-remote LM setup bears resemblance to ours, we do not study the aspects of privacy, but rather focus on reducing cloud costs by delegating work to devices while maintaining accuracy. Moreover, we have additional focus on local runtime efficiency.

\paragraph{Local-remote systems} Recent work has explored efficient routing patterns between local and remote computation for LM workloads, albeit without two models communicating or collaborating on a solution.
\citep{jin2024collm} partition a single LLM with early layers on the edge and later layers in the cloud, routing to the cloud when confidence is low. \citep{yang2024perllm}
propose a complementary task scheduling framework that routes to cloud or local based on resource constraints and service requirements.


% \begin{itemize}
%     \item related work on sampling distributions that match the big model.
%     \item map-reduce strategies for LLMs
%     \item long document processing systems
%     \item edge-computing with LLMs
%     \item multi-agent systems
% \end{itemize}

% \textbf{Speculative decoding} \citept{leviathan2023fast} introduce a sampling scheme that achieves exact sampling from a large LM $x \sim p(x)$, but through proposing samples only from a small LM $x \sim q(x)$, and using $p(x)$ only for evaluation, which is done efficiently in one forward pass. Their trick -- also known as the ``acceptance-complement algorithm'' \citep{devroye2006nonuniform} -- involves a clever use of the ratio of $p(x)$ and $q(x)$ to determine when to accept or reject samples. Speculative decoding neither attempts to minimize the number of tokens sent up to the large LM, nor it allows for a free-form chat between a small and a large model. We also note that this work does not aim to produce exact samples from the large LM, which in itself can be limited and can potentially be improved as part of an edge-remote system.


% \textbf{Map-reduce strategies for LLMs} Recent works \citep{zhou2024llm,zhang2024chain} attempt to improve LLM performance on long-contexts through divide-and-conquer approaches that process context chunks separately followed by an aggregation step. These methods differ fundamentally from our edge-remote setup, as they assume all models have equal access to the context and do not aim to minimize communication costs. Both works primarily demonstrate their approaches with larger LMs (70B parameter and above), and use a predefined protocol for chunk processing, unlike our dynamic task-decomposition. In both methods, chunks are analyzed sequentially, where information extracted from previous chunks is passed as input when processing the next chunk. Their sequential processing enables conflict resolution between chunks but is less suited to our parallel edge execution requirements. Finally, unlike our bidirectional edge-remote dialogue, they do not explore multi-round communication between the workers and the aggregator model.


% \textbf{Long document processing systems} Memory systems build structured compressed representations of the text which are attended for next queries. For example, PRISM... ~\citep{jayalath2024long}

% \textbf{Edge-computing with LLMs} Resource-constrained inference, ollama and others, which kinds of chips are used, what kind of running times. work on privacy preservation. (check nilufarâ€™s resources on x).

% \textbf{Multi-agent systems} unclear what should be included here, it's a huge field

% \textbf{Privacy preserving systems} Works on privacy (grab from doc) deal with the question of preserving private information on a local machine 

% \textbf{Compound ML systems} Multi-agent setup is becoming increasingly common, for solving problems by diverse teams. We focus on one assymetric realization of the multi-agent setting with one small model and one big model. 