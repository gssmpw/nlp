\section{Extended discussion of methods}
\label{app:methods}

\subsection{Extended description of \naive}
\label{app:methods-naive}

In this section, we describe \naive, a baseline local-remote communication protocol. 
% inspired by the recent success of the multi-agent paradigm \citep{wang2024mixture, guo2024large}. 
We ask whether we can reduce remote prefill tokens, and thus cost, by simply orchestrating a free-form conversation between the $\locallm$ and the $\remotelm$ in which only the $\locallm$ has direct access to the context $\bc$.
% For this to work, the $\remotelm$ needs to know how to solve the problem, and the $\locallm$  needs to be able to answer straightforward queries against a long context. Each model separately maintains the history of its \emph{own} inputs and outputs. 

The protocol proceeds with initialization step followed by a simple correspondence between the two models, which terminates when the remote model can answer the question or a maximum iteration limit is reached. 

\paragraph{Iteration $i = 1$: Initialize.}
The $\remotelm$ receives the task query $\bq$ along with a system prompt $\bp_{\text{remote}}$ that instructs it to  interact with a small LM that has full access to context. It outputs a first message $\bmm_{\text{remote}}^{(1)}$:
\begin{align*}
    \bmm_{\text{remote}}^{(1)} \sim \textcolor{blue}{\remotelm}(\bq, \bp_{\text{remote}})
\end{align*}
The message is then provided to $\locallm$, along with the full context $\bc$, the query $\bc$,
and a minimal system prompt $\bp_{\text{local}}$ that instructs it to answer questions on the context:
\begin{align*}
    \bmm_{\text{local}}^{(1)} \sim \textcolor{blue}{\locallm}(\bmm_{\text{remote}}^{(1)}, \bq, \bp_{\text{local}}, \bc)
\end{align*}
\paragraph{Iteration $i > 1$.}
\textbf{Step 1: Message from remote to local.}
$\remotelm$ consumes the conversation history and outputs new messages: 
\begin{align*}
    \bmm_{\text{remote}}^{(i)} \sim \remotelm(\bmm_{\text{remote}}^{(:i - 1)}, \bmm_{\text{local}}^{(:i - 1)} , \bq, \bp_{\text{remote}})
\end{align*}
In its message, $\remotelm$ indicates whether it has sufficient information to terminate the loop and answer the question, or alternatively raises additional questions. 


\textbf{Step 2: Message from local to remote}
$\locallm$ consumes the latest remote message and conversation history, and outputs $\bmm_{\text{local}}^{(i)}$. 
\begin{align*}
    \bmm_{\text{local}}^{(i)} \sim \textcolor{blue}{\locallm}(\bmm_{\text{remote}}^{(:i - 1)}, \bmm_{\text{local}}^{(:i - 1)}, \bq, \bp_{\text{local}}, \bc)
\end{align*}
We then increment the iteration $i$ and loop back to \textbf{Step 1} until the break condition is met or we reach a maximum number of iterations.



\subsection{Information Bottleneck Perspective}\label{app:info_bottleneck}
How does local model capacity affect the cost-accuracy tradeoff?

The Information Bottleneck (IB) principle \citep{tishby2000information} provides a useful analogy.
One communication round of a local-remote system does as follows:
\begin{align*}
    \bz &\sim p(\bz|\bc) \quad \text{[Extract info. from context]} \\
    \by &\sim p(\by|\bz) \quad \text{[Predict outcome from extracted info]}
\end{align*}
The IB principle seeks to find a $p(\bz \mid \bc)$, our $\locallm$, as follows:
\begin{equation}
\min_{p(\bz \mid \bc)} \;\Bigl[\, I(C;Z) \;-\; \beta \, I(Z;Y) \Bigr],
\label{eq:ib_formulation}
\end{equation}
\textit{i.e.} find a mapping that forces the latent representation to be maximally informative of the label $I(Z;Y)$ and minimally informative of the input $I(C;Z)$, with a tradeoff parameter $\beta$. Here, we do not optimize the mapping $p(\bz \mid \bc)$ but instead only get to choose it by setting $\locallm$.

Since we cannot compute these quantities in closed form for nonlinear distributions over tokens, we use (coarse) empirical proxies as follows. As a proxy for $I(C;Z)$, we compute the number of prefill tokens sent to $\remotelm$, capturing the intuition that more tokens carry more information on the input.
$I(Z;Y)$ is estimated as the average accuracy of the local-remote system, quantifying the preservation of task-relevant information in $\bz$. While these proxies do not exactly match the underlying mutual informations, they capture the core tension of compressing the input vs.\ preserving predictive power.

We plot these quantities in Figure~\ref{fig:lm-bottleneck}. We find that  across both $\qwen$ and $\llama$ model families, as we increase $\locallm$ size, we send fewer tokens to $\remotelm$ ($\approx I(C;Z) \downarrow$), and improve accuracy ($\approx I(Z;Y) \downarrow$). We find that $\llama$ has higher $\approx I(C;Z)$ and higher $\approx I(Z;Y)$.


