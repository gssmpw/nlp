\section{Extended Description of Experimental Setup}
\label{app:experiments}
\subsubsection{Dataset Details}
\label{app:experiments-dataset}

In this section we provide additional details on dataset preparation. In order to extend the context length of the problems in \longhealth and \qasper, we make a few modification to the dataset.

\textbf{\finance} We filter the original \finance to include only the numerical reasoning, resulting in a dataset of length $64$. Each sample has an average context length of $142.9K (\pm 79224.32$).

\textbf{\longhealth}
In the original instantiation of the \longhealth dataset, each question is paired with a set of medical documents corresponding to a single patient. To increase the complexity of the dataset, we include medical documents from $10$ other patients in the context. We evaluate over the entire dataset ($400$ problems) for results reported in Table~\ref{table:main-tradeoff}. Each sample has an average context length of $120.1K (\pm 1,237)$ tokens. For all ablations in Section~\ref{sec:results}, we use a fixed subset of $128$ problems. 

\textbf{\qasper} Similarly, in the \qasper dataset, the original dataset provides questions that are associated with a single scientific paper. In order to increase complexity, we include $10$ other papers in the context.  We evaluate over a random subset of $500$ problems for results reported in Table~\ref{table:main-tradeoff}. Each sample has an average context length of $54281$ tokens ($\pm 2403)$. For all ablations in Section~\ref{sec:results}, we use a fixed subset of $128$ problems.



\subsubsection{Model Details}
\label{app:experiments-models}
\textbf{Local Models}. For \qwen we use the following models: \qwen-1.5-Instruct, \qwen-3B-Instruct, \qwen-7B-Instruct. For \llama, we use the following models: \llamathreetwo-1B-Instruct, \llamathreetwo-3B-Instruct, \llama-3.1-8B-Instruct.

\textbf{Remote Models}. We use \gpt and \llamathreetwo-70B-Instruct, \llamathreeone-70B-Instruct

All ``local-only'' and ``remote-only'' experiments are run with temperature of $0.2$. For all \system experiments run in Table~\ref{table:main-tradeoff}, we run the $\remotelm$ with a temperature of $0.0$ and $\locallm$ with a temperature of $0.2$ for \finance and $0.00001$ for \qasper and \longhealth. 





\section{Extended Discussion of Cost Model}
\label{app:edgecost}
Here, we explain in detail the costs of the different communication protocols discussed in this paper---remote-only, \naive, and \system---with a strong focus on the latency of these methods.
This section is organized as follows:
\begin{itemize}
    \item Section~\ref{app:edgecost_background}: We review background on language model inference, to motivate our cost and latency models.
    \item Section~\ref{app:edgecost_models}: We present mathematical models for the latency of the remote-only, \naive, and \system protocols.
    \item Section~\ref{app:edgecost_bound}: We present Proposition~\ref{prop:latency_bound}, an upper bound on the total latency of \system, relative to that of the remote-only model, demonstrating that \system is not much slower than the naive approach of performing the full query in the cloud.
    As an example, we show that a Llama-8B model on a GTX-4090 GPU collaborating via \system with a Llama-405B model on a $8\times$H100 server is at most $4.75\times$ slower than the remote-only protocol.
\end{itemize}

\subsection{Background on language model inference}
\label{app:edgecost_background}
Language model inference consists of a sequence of forward passes through a model, one for prefill (\textit{i.e.} input) followed by one for each additional token generated (\textit{i.e.} output). 
At low/medium batch sizes, each forward pass after prefill is I/O bound, meaning the time it takes to load weights from memory exceeds the time it takes to actually compute the output. 
As the batch size increases, the computational cost of the forward pass eventually exceeds the I/O cost. 
Strikingly, for most models and hardware, this happens at a batch size $>100$~\citep{leviathan2023fast,chen2024sequoia}.
As a result of this transition from being I/O bound to being compute bound, we can model (as is common in the literature) the cost of running a forward pass as a piecewise linear function $C_{\mathcal{M}, \mathcal{E}}(n) = \max(\lambda, \alpha \cdot n + \beta)$ of the number of tokens $n$ being processed.
This is because for small $n$, the IO cost dominates (and is roughly constant as $n$ grows), whereas at larger $n$ the compute cost dominates and scales roughly linearly with $n$ (assuming $n$ is not \textit{too} large).

In the cloud, the provider can batch generation requests from multiple users to keep hardware utilization high.
Therefore, the cost of each output token is typically within a small multiple of the cost of each input token, and the total cost of processing the request scales as $n_{prefill} + \alpha \cdot n_{decode}$, for some small $\alpha \leq 5$. 
% The difference in cost between prefill and decode tokens ($\alpha$ is between 1 and 5~\cite{dubey2024llama3,anthropic2024claude}) is due to additional I/O costs related to the KV-cache. But 

On-device, we cannot assume we'll have enough concurrent user requests to form a large enough batch to achieve high utilization. As a result, the latency of a request does not scale linearly with the number of tokens. A single request can occur similar latency to hundreds run in parallel. As a result, tokens are a poor proxy for cost on-device and we instead measure latency in micro experiments (see \Cref{subsec:results-workloads}). 

\subsection{Latency models for all protocols: Remote-only, \naive, \system}
\label{app:edgecost_models}
We now model the latency of each of these protocols (remote-only, \naive, \system).
We will then use these results in the following section to upper bound the latency of \system by a scalar multiple of the latency of the remote-only protocol.

\textbf{First, we introduce the following assumptions and notation}:
\begin{itemize}
\item We assume we have a local GPU (\textit{e.g.} RTX-4090) with peak compute $F_l$ (flops/sec), and peak bandwidth $M_l$ (bytes/sec), and a remote GPU (\textit{e.g.} H100) with peak compute $F_r$ (flops/sec), and peak bandwidth $M_r$ (bytes/sec), 
\item We also assume for now simple transformer architectures for both the local and remote models:
\begin{itemize}
\item $\locallm$: $L_l$ layers, each with $8d_l^2$ params in MLP (Up/down projections each of size $d_l\times 4d_l$, and $4d_l^2$ parameters in the $W_{Q,K,V,O}$ projections.
The total memory required for the (non-embedding/LM head) parameters is thus $P_l = 2 \cdot 12 L_l d_l^2$.
For simplicity, we assume the memory for the LM head is small relative to $P_l$.
\item $\remotelm$: Equivalent architecture to the $\locallm$, but with $L_r$ layers, $d_r$ hidden dimension, and $P_r$ total non-embedding/LM-head parameter memory (again assumed to be much greater than the number of LM head parameters).
\end{itemize}
\item We model the number of input/output tokens of each protocol as follows, letting $n$ denote the number of tokens in the original document:
\begin{itemize}
    \item \textbf{Remote-only}: $n$ prefill tokens and $n_{out}^r$ decode tokens.
    Note that we assume---here and below---that the number of tokens in the query is negligible relative to $n$.
    We assume $n \gg n_{out}^r$ so we can effectively ignore the KV-cache load time for the output tokens.
    \item \textbf{\naive}: For $\locallm$, we assume $n$ prefill tokens and $n_{out}^l$ decode tokens.
    For $\remotelm$, we assume $n_{out}^l$ prefill tokens, and $n_{out}^r$ decode tokens.
    In the case of multiple rounds of communication, the KV cache for the document can be stored to avoid recomputation.
    \item \textbf{\system}: For $\locallm$, we assume $n/c$ prefill tokens per chunk ($c$ chunks total), and $n_{out}^l$ decode tokens per job (though we assume only $p$ fraction of output jobs do not abstain).
    For $\remotelm$, we assume $J \cdot n_{out}^l \cdot p$ prefill tokens, and $n_{out}^r$ decode tokens, letting $J=cks$ denote the total number of jobs in \system ($c$ chunks, $k$ instructions, $s$ samples).
    In the case of multiple rounds of communication, the KV cache for each document chunk can be stored to avoid recomputation.
\end{itemize}
\item Throughout, we use the fact that a $[m \times n] \cdot [n \times k]$ matmul takes $2\cdot mnk$ flops, and assume model parameters are stored in half-precision (2 bytes/param).
\end{itemize}



% :
% \begin{itemize}
% \item \textbf{Local vs. remote document prefill}:
% Local prefill uses a smaller model on a weaker chip relative to remote prefill.
% So depending on the ratio of local/remote model sizes relative to the ratio of local/remote peak flops, whether performing prefill on the large document is faster in the remote-only vs. local-remote protocols.
% Furthermore, \system 
% \item \textbf{Local vs. remote decode}: 
% \end{itemize}

We are now ready to present the latency models for the three protocols (remote-only, \naive, \system).

\subsubsection{Remote-only}
\begin{itemize}
    \item \textbf{Prefill}: We are compute bound, so time is approximately given by $total\_flops / F_r$.
    We can break down $total\_flops$ into the matmuls (MLP up/down projections, and QKVO operations) and attention operations.
    \begin{itemize}
        \item \textbf{Matmuls}: $2 \cdot 12 n d^2$ per layer. 
        Equivalent to a $[n \times d_r] \cdot [d_r \times 12d_r]$ matmul.
        \item \textbf{Attention}: $2 \cdot n^2 d_r$ per layer.
        Equivalent to $[n \times d_r] \cdot [d_r \times n]$ matmul.
        \item \textbf{Time}: $L_r \cdot (24 n d_r^2 + 2 n^2 d_r) / F_r = (nP_r + 2 L_r d_r n^2) / F_r$.
    \end{itemize}
    \item \textbf{Decode}: We are memory bound (batch size 1 for Minion), so time is approximately given by $total\_memory / M_r$ per decode step.
    We can break down $total\_memory$ into model parameters and KV cache.
    \begin{itemize}
        \item \textbf{Model parameters}: $2 \cdot 12 d_r^2$ bytes per layer.
        \item \textbf{KV-cache}: $2 \cdot 2 n d_r$ bytes per layer (K and V are each  $[n \times d]$ matrices).
        \item \textbf{Time}: $L_r \cdot n_{out}^r \cdot (24 d_r^2 + 4 n d_r) / M_r =  n_{out}^r (P_r + 4 L_r d_r n) / M_r$.
    \end{itemize}
\end{itemize}
\noindent \textit{Total time} is given by the sum of prefill and decode times: 
    $$T_{remote} = \frac{nP_r + 2 L_r d_r n^2}{F_r} + \frac{n_{out}^r (P_r + 4 L_r d_r n )}{M_r}$$



\subsubsection{\naive}
The latency of the $\locallm$ in the \naive protocol can be modeled equivalently to the latency of the remote-only protocol, but replacing the remote parameters with the corresponding local ones.
Thus, total local latency is:
    $$T_{local}^{\naive} = \frac{nP_l + 2 L_l d_l n^2}{F_l} + \frac{n_{out}^l (P_l + 4 L_l d_l n )}{M_l}$$

The total remote latency can also be expressed using these same equations, but with $n_{out}^l$ prefill tokens, and $n_{out}^r$ decode tokens.
$$T_{remote}^{\naive} = \frac{n_{out}^l P_r + 2 L_r d_r (n_{out}^l)^2}{F_r} + \frac{n_{out}^r (P_r + 4 L_r d_r n_{out}^l )}{M_r}$$

\subsubsection{\system}
The $\locallm$ latency of the \system protocol has some important differences from the \naive protocol---the prefill computation avoids cross-chunk attention (which saves time), while the decode operations can actually be compute bound if batching of the different jobs is done.
We review these details below:

\begin{itemize}
    \item \textbf{Prefill}: We are compute bound, so time is approximately given by $total\_flops / F$. We can break down $total\_flops$ into the matmuls (MLP up/down projections, and QKVO operations) and attention operations.
    \begin{itemize}
        \item \textbf{Matmuls}: $2 \cdot 12 n d_l^2$ per layer. Equivalent to $c$ $[n_c \times d_l] \cdot [d_l \times 12d_l]$ matmuls (where $n_c = n/c$).
        \item \textbf{Attention}: $2 \cdot c n_c^2 d_l = 2 \cdot c \; (n/c)^2 d_l = 2n^2 d_l / c$ per layer.  Equivalent to $c$ $[n_c \times d_l] \cdot [d_l \times n_c]$ matmuls.
        \item \textbf{Time}: $L_l \cdot (24 n d_l^2 + 2 n^2 d_l / c) / F = (n P_l + 2 L_l d_l n^2 /c) / F$.
    \end{itemize}
    \item \textbf{Decode}: We will now assume we are \textbf{compute bound during decode}, because we have many jobs ($ks$) per chunk, and many chunks ($c$) per document, which we can batch together. Thus, time is approximately given by $total\_flops / F_l$ per decode step. We can break down $total\_flops$ into matmuls and attention. The flops below are per job, per output token (so for total flops we will multiply by $n_{out}^l \cdot pcks$):
    \begin{itemize}
        \item \textbf{Matmuls}: $2 \cdot 12 d_l^2$ per layer. Equivalent to a $[1 \times d_l] \cdot [d_l \times 12d_l]$ matmul.
        \item \textbf{Attention}: $2 \cdot n_c d_l = 2d_l \, n/c$ per layer. Equivalent to $[1 \times d_l] \cdot [d_l \times n_c]$ matmul.
        \item \textbf{Time}: $L_l \cdot n_{out}^l \cdot pcks \cdot (24 d_l^2 + 2 d_l n/c) / F = n_{out}^l \cdot pcks \cdot (P_l + 2 L_l d_l n/c) / F$.
    \end{itemize}
\end{itemize}
The \textit{total local latency} for \system is given by the sum of prefill and decode times: 
\systemmath
$$T_{local}^{\systemmath} = \frac{n P_l + 2 L_l d_l n^2 /c}{F_l} + \frac{n_{out}^l \cdot pcks \cdot (P_l + 2 L_l d_l n/c)}{F_l}.$$

The \textit{total remote latency} for \system can be expressed using the same equations as \naive, but with $pcks \cdot n_{out}^l$ prefill tokens, and $n_{out}^r$ decode tokens.
$$T_{remote}^{\systemmath} = \frac{(pcks \cdot n_{out}^l) P_r + 2 L_r d_r (pcks \cdot n_{out}^l)^2}{F_r} + \frac{n_{out}^r (P_r + 4 L_r d_r (pcks \cdot n_{out}^l) )}{M_r}$$

\subsection{\system vs. remote-only comparison}
\label{app:edgecost_bound}

\begin{proposition}
\label{prop:latency_bound}
Assume $n_{out}^l \cdot pcks = an$, for some $a < 1$, and that $F_{r,l}$, $d_{r,l}$, and $L_{r,l}$ are all as defined in Appendix~\ref{app:edgecost_models}. In this case, we can show that the ratio of total latency of \system vs. the remote-only protocol is upper-bounded by the following expression:

\begin{eqnarray*}
\frac{T_{remote}^{\systemmath} + T_{local}^{\systemmath}}{T_{remote}} &<& 1 + \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \frac{L_l d_l}{L_r d_r}. \\
\end{eqnarray*}

\begin{proof}
    
Let's assume $n_{out}^l \cdot pcks = an$, for some $a < 1$.

\begin{eqnarray*}
T_{local}^{\systemmath} &=& \frac{n P_l + 2 L_l d_l n^2 /c}{F_l} + \frac{an \cdot (P_l + 2 L_l d_l n/c)}{F_l} \\
&<& \big(1 + a\big) \cdot \frac{n P_l + 2 L_l d_l n^2 /c}{F_l} \\
% &<& \big(1 + a\big) \cdot \frac{n P_l + 2 L_l d_l n^2}{F_l} \\
T_{remote}^{\systemmath} &=& \frac{(an) P_r + 2 L_r d_r (an)^2}{F_r} + \frac{n_{out}^r (P_r + 4 L_r d_r (an) )}{M_r} \\
&<& a \bigg(\frac{nP_r + 2 L_r d_r n^2}{F_r} + \frac{n_{out}^r 4 L_r d_r n}{M_r}\bigg) + \frac{n_{out}^r P_r}{M_r}\\
T_{remote} &=& \frac{nP_r + 2 L_r d_r n^2}{F_r} + \frac{n_{out}^r (P_r + 4 L_r d_r n )}{M_r}
\end{eqnarray*}
Thus, it is easy to see that $\frac{T_{remote}^{\systemmath}}{T_{remote}} < 1$.
Now let's look at $\frac{T_{local}^{\systemmath}}{T_{remote}}$, and show it is upper bounded by a constant:

\begin{eqnarray*}
\frac{T_{local}^{\systemmath}}{T_{remote}} &<& \frac{\big(1 + a\big) \cdot \frac{n P_l + 2 L_l d_l n^2 /c}{F_l}}{ \frac{nP_r + 2 L_r d_r n^2}{F_r}} \\
&=& \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \frac{n P_l + 2 L_l d_l n^2 /c}{nP_r + 2 L_r d_r n^2} \\
&\leq& \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \max\bigg(\frac{P_l}{P_r}, \frac{L_l d_l}{L_r d_r c}\bigg) \\
&=& \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \max\bigg(\frac{L_l d_l^2}{L_r d_r^2}, \frac{L_l d_l}{L_r d_r c}\bigg) \\
&<& \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \frac{L_l d_l}{L_r d_r}.\\
\end{eqnarray*}

Thus, combining the above two results we can see that:
\begin{eqnarray*}
\frac{T_{remote}^{\systemmath} + T_{local}^{\systemmath}}{T_{remote}} &<& 1 + \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \frac{L_l d_l}{L_r d_r}. \\
\end{eqnarray*}

\end{proof}

\end{proposition}

\textbf{Real example}: Let's assume that the local GPU is a RTX 4090 ($F_l \approx 160$ TFLOPS), the remote server is a full node of 8 H100s ($F_r \approx 8000$ TFLOPS across full node), the local model is Llama-8B ($L_l=32$, $d_l=4096$), and the remote model is Llama-405B ($L_l=126$, $d_l=16384$).
Furthermore, let's assume $a \approx 0.2$, which is actually a bit larger than we see in practice.
In this case:
\begin{eqnarray*}
1 + \big(1 + a \big) \cdot \frac{F_r}{F_l} \cdot \frac{L_l d_l}{L_r d_r} &\approx& 1 + 1.2 \cdot \frac{8000}{160} \cdot \frac{32 \cdot 4096}{126 \cdot 16384} \\
&\approx& 1 + 1.2 \cdot 50 \cdot \frac{1}{16} \\
&=& 4.75. \\
\end{eqnarray*}

Note that if we perform multiple rounds of \system, this ratio gets multiplied by at most the number of rounds, though as mentioned previously, we can save time by only performing prefill on all the document chunks in the first round.

% For reference, \naive is:
% $$\frac{24 n d^2 + 2n^2 d}{F} + \frac{n_{out} \cdot (24 d^2 + 4n d)}{M}  $$

% \system vs \naive conclusion:
% \begin{itemize}
%     \item \textbf{Prefill}: Minions prefill is faster because of avoiding cross-chunk attention.
%     \item \textbf{Decode}: There are some competing factors here. Minions decode may be slower because $n_{out}' \cdot cks$ may be meaningfully greater than $n_{out}$, but it may be faster because $F >> M$ typically.
% \end{itemize}


% DAN'S NOTES ABOUT MINION/MINIONS
% MINION: Give 128K tokens in batch size = 1
% MINIONS:
% \begin{itemize}
% \item Give 1K token chunks in batch size = 128
% \item With one identical task across batch elements, every batch element has unique prefill tokens
% \item With multiple tasks (say 3 different questions) across batch elements (common in our case, when we ask multiple separate questions on each chunk) we have
% \item 3 questions * 1K token chunks * 128 batch elements -> effective batch size = 384. In this case, we can share prefix as in Hydragen across the three questions. \textit{i.e.} each chunk repeats 3 times in prefill
% \end{itemize}
