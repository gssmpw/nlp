
\section{Prompts}
\label{app:methods-prompts}


\subsection{\naive}

\textbf{$\remotelm$}


\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
We need to answer the following question based on a {doc_type}.

### Question
{query}

### Instructions
You will not have direct access to the {doc_type}, but can chat with a small language model which has read the entire thing.

Feel free to think step-by-step, but eventually you must provide an output 
in the format below:

<think step by step here>
```json
{{
    "message": "<your message to the small language model>"
}}
```
\end{lstlisting}
\end{tcolorbox}


\textbf{$\locallm$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
You will help a user answer the following question based on a {doc_type}. 


Read the {doc_type} below and prepare to answer questions from an expert user. 
### {doc_type}
{context}

### Question
{query}
\end{lstlisting}
\end{tcolorbox}


\textbf{$\mathrm{Conversation}$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Here is the response from the small language model:

### Response
{response}


### Instructions
Analyze the response and think-step-by-step to determine if you have enough 
information to answer the question.

If you have enough information, provide a final numeric answer in the format
below.

<think step by step here>
```json
{{
    "decision": "provide_final_answer",  
    "answer": "<your answer>"
}}
```

Otherwise, request additional information from the small language model by 
outputting the following:

<think step by step here>
```json
{{
    "decision": "request_additional_info",
    "message": "<your message to the small language model>"
}}
```
\end{lstlisting}
\end{tcolorbox}


\subsection{\system}


% $\bp_{\text{decompose}}$ $\bp_{\text{worker}}$ $\bp_{\text{synthesize}}$



\paragraph{\system: \finance} \

\textbf{$\mathrm{Decompose}$}


\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
# Decomposition Round #{step_number}

You do not have access to the raw document(s), but instead can assign tasks to small and less capable language models that can read the document(s).
Note that the document(s) can be very long, so each task should be performed only over a small chunk of text.

Write a Python function that will output formatted tasks for a small language model.
Make sure that NONE of the tasks require calculations or complicated reasoning.
Any information you mentioned in your task should be given an extraction task.

Please use chunks of {pages_per_chunk} pages using the `chunk_on_multiple_pages(doc = context, pages_per_chunk ={pages_per_chunk})` function.

If you have multiple tasks, consider using nested for-loops to apply a set of tasks to a set of chunks. Though it's not required to have more than one task.

{ADVANCED_STEPS_INSTRUCTIONS}

Assume a Pydantic model called `JobManifest(BaseModel)` is already in global scope. For your reference, here is the model:
```
{manifest_source}
```
Assume a Pydantic model called `JobOutput(BaseModel)` is already in global scope. For your reference, here is the model:
```
{output_source}
```
DO NOT rewrite or import the model in your code.

The function signature will look like:
```
{signature_source}
```

You can assume you have access to the following chunking function(s). Do not reimplement the function, just use it.
```
{chunking_source}
```
\end{lstlisting}
\end{tcolorbox}

\textbf{$\mathrm{Worker}$}


\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Your job is to complete the following task using only the context below. The context is a chunk of text taken arbitrarily from a document, it might or might not contain relevant information to the task.

## Document
{context}

## Task
{task}

{advice}

Return your result in JSON with the following keys: "explanation", "citation", and "answer".

- "explanation": A concise statement of your reasoning or how you concluded your answer.
- "citation": A direct snippet of the text that supports your answer. If nothing is found, put "None".
- "answer": The extracted answer. If nothing is found, put "None".

Be certain to only rely on the provided text. If you cannot determine the information confidently from this chunk, respond with "None" for all fields.
\end{lstlisting}
\end{tcolorbox}


\textbf{$\mathrm{Synthesize}$}
\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]

Now synthesize the findings from multiple junior workers (LLMs). 
Your task is to finalize an answer to the question below **if and only if** you have sufficient, reliable information. 
Otherwise, you must request additional work.

---
## Inputs
1. Question to answer:
{question}

2. Collected Job Outputs (from junior models):
{extractions}

---
First think step-by-step and then answer the question using the exact format below.

## ANSWER GUIDELINES
1. **Determine if the collected Job Outputs provide enough trustworthy, consistent evidence to confidently answer the question.** 
   - If the data is incomplete or contradictory, do NOT guess. Instead, specify what is missing.
   - If the evidence is sufficient, provide a final answer.

2. **Be conservative.** When in doubt, ask for more information.

3. **Address conflicts.** If multiple jobs give different answers, rely on whichever is best supported by a valid "explanation" and "citation".
   - If you need more information from the conflicting jobs you could request additional work from those specific jobs (be sure to mention the specific job IDs in your additional_info field).
   - Then, in the next round you can make a smaller set of jobs to determine which answer is correct.

4. **Required JSON Output**: You must output a JSON object with these keys:
   - "decision": Must be either "provide_final_answer" OR "request_additional_info".
     - Use "provide_final_answer" if you have enough information.
     - Use "request_additional_info" if you cannot conclusively answer.
   - "explanation": A short statement about how you arrived at your conclusion or what is still missing.
   - "answer": The final answer string if "decision"="provide_final_answer", or null otherwise. Should contain ONLY the final answer, without additional calculations or explanations.
     
Here is the template for your JSON response (with no extra text outside the JSON):

<think step-by-step here>
```json
{{
"decision": "...",
"explanation": "...",
"answer": "... or null", # Good answer format: "0.56"; Bad answer format: "The ratio is calculated as 1-0.27*2 = 0.56"
}}
```

**Important**:
- If there is not enough information, set "answer" to null, set "decision" to "request_additional_info", and specify exactly what else you need in "missing_info" and from which job IDs.

Now, carefully inspect the question, think step-by-step and perform any calculations before outputting the JSON object.
\end{lstlisting}
\end{tcolorbox}


\paragraph{\system: \longhealth} \

\textbf{$\mathrm{Decompose}$}
\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\tiny
\begin{lstlisting}[breaklines]
# Decomposition Round #{step_number}

You do not have access to the raw document(s), but instead can assign tasks to small and less capable language models that can read the document(s).
Note that the document(s) can be very long, so each task should be performed only over a small chunk of text. 

Write a Python function that will output formatted tasks for a small language model.
Make sure that NONE of the tasks require multiple steps. Each task should be atomic! 
Consider using nested for-loops to apply a set of tasks to a set of chunks.
The same `task_id` should be applied to multiple chunks. DO NOT instantiate a new `task_id` for each combination of task and chunk.
Use the conversational history to inform what chunking strategy has already been applied.

{ADVANCED_STEPS_INSTRUCTIONS}

Assume a Pydantic model called `JobManifest(BaseModel)` is already in global scope. For your reference, here is the model:
```
{manifest_source}
```
Assume a Pydantic model called `JobOutput(BaseModel)` is already in global scope. For your reference, here is the model:
```
{output_source}
```
DO NOT rewrite or import the model in your code.

The function signature will look like:
```
{signature_source}
```


You can assume you have access to the following chunking function(S). Do not reimplement the function, just use it.
```
{chunking_source}
```

Here is an example
```
task_id = 1  # Unique identifier for the task
for doc_id, document in enumerate(context):
    # if you need to chunk the document into sections
    chunks = chunk_by_section(document)
    # or if you need to chunk the document into pages
    chunks = chunk_by_page(document)

    for chunk_id, chunk in enumerate(chunks):
        # Create a task for extracting mentions of specific keywords
        task = (
            "Extract all mentions of the following keywords: "
            "'Ca19-9', 'tumor marker', 'September 2021', 'U/ml', 'Mrs. Anderson'."
        )
        job_manifest = JobManifest(
            chunk_id=f"doc_id_chunk_id",
            task_id=task_id,
            chunk=chunk,
            task=task,
            advice="Focus on extracting the specific keywords related to Mrs. Anderson's tumor marker levels."
        )
        job_manifests.append(job_manifest)
```
\end{lstlisting}
\end{tcolorbox}

\textbf{$\bp_{\text{worker}}$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Your job is to complete the following task using only the context below. The context is a chunk of text taken arbitrarily from a document, it might or might not contain relevant information to the task.

## Document
{context}

### Question you are trying to answer: 
{question}

# You have been instructed to extract information pertaining to the following concepts: 
# \"Date of visit\", {task}

Format your response as follows:
{{
"Date of visit" : "`direct quote extracted text`",
"<keyword_1>" : "`direct quote extracted text`", 
"<keyword_2>" : "`direct quote extracted text`", 
...
}}

Can you please extract the relevant sections from the document that are related to the concepts provided? Extract direct quotes or sentences. If concept is not mentioned, leave it out.

Your Answer:
\end{lstlisting}
\end{tcolorbox}

\textbf{$\bp_{\text{synthesize}}$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Answer the following by the synthesizing findings from multiple junior workers (LLMs).


---
## Inputs
1. Question to answer:
{question}

2. Collected Job Outputs (from junior models):
{extractions}

---
First think step-by-step and then answer the question using the exact format below.

## ANSWER GUIDELINES

**Required JSON Output**: You must output exactly one JSON object with these keys:
   - "decision": Must be  "provide_final_answer".
   - "explanation": A short statement about how you arrived at your conclusion or what is still missing.
   - "answer": The final answer string (that matches one of the provided options) if "decision"="provide_final_answer", or null otherwise.

     
Here is the template for your JSON response:

<think step-by-step here>


{{
"decision": "...",
"explanation": "...",
"answer": "...",
}}


Now, carefully inspect the question, think step-by-step and perform any calculations before outputting the JSON object. If answer choices are provided, your answer must **exactly** match one of the answer choices.

Question: 
{question}

Your Answer:
\end{lstlisting}
\end{tcolorbox}


\paragraph{\system: \qasper} \


\textbf{$\bp_{\text{decompose}}$}
\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
# Decomposition Round #{step_number}

You do not have access to the raw document(s), but instead can assign tasks to small and less capable language models that can read the document(s).
Note that the document(s) can be very long, so each task should be performed only over a small chunk of text. 

Write a Python function that will output formatted tasks for a small language model.
Make sure that NONE of the tasks require multiple steps. Each task should be atomic! 
Consider using nested for-loops to apply a set of tasks to a set of chunks.
The same `task_id` should be applied to multiple chunks. DO NOT instantiate a new `task_id` for each combination of task and chunk.
Use the conversational history to inform what chunking strategy has already been applied.

{ADVANCED_STEPS_INSTRUCTIONS}

Assume a Pydantic model called `JobManifest(BaseModel)` is already in global scope. For your reference, here is the model:
```
{manifest_source}
```
Assume a Pydantic model called `JobOutput(BaseModel)` is already in global scope. For your reference, here is the model:
```
{output_source}
```
DO NOT rewrite or import the model in your code.

The function signature will look like:
```
{signature_source}
```


You can assume you have access to the following chunking function(S). Do not reimplement the function, just use it.
```
{chunking_source}
```

Here is an example
```
task_id = 1  # Unique identifier for the task
for doc_id, document in enumerate(context):
    # if you need to chunk the document into sections
    chunks = chunk_by_section(document)
    # or if you need to chunk the document into pages
    chunks = chunk_by_page(document)

    for chunk_id, chunk in enumerate(chunks):
        # Create a task for extracting mentions of specific keywords
        task = (
            "Extract all mentions of the following keywords: "
            "'Ca19-9', 'tumor marker', 'September 2021', 'U/ml', 'Mrs. Anderson'."
        )
        job_manifest = JobManifest(
            chunk_id=f"doc_id_chunk_id",
            task_id=task_id,
            chunk=chunk,
            task=task,
            advice="Focus on extracting the specific keywords related to Mrs. Anderson's tumor marker levels."
        )
        job_manifests.append(job_manifest)
```
\end{lstlisting}
\end{tcolorbox}

\textbf{$\bp_{\text{worker}}$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Your job is to complete the following task using only the context below. The context is a chunk of text taken arbitrarily from a document, it might or might not contain relevant information to the task.

## Document
{context}

### Question you are trying to answer: 
{question}

# You have been instructed to extract information pertaining to the following concepts: 
# \"Date of visit\", {task}

Format your response as follows:
{{
"Date of visit" : "`direct quote extracted text`",
"<keyword_1>" : "`direct quote extracted text`", 
"<keyword_2>" : "`direct quote extracted text`", 
...
}}

Can you please extract the relevant sections from the document that are related to the concepts provided? Extract direct quotes or sentences. If concept is not mentioned, leave it out.

Your Answer:
\end{lstlisting}
\end{tcolorbox}

\textbf{$\bp_{\text{synthesize}}$}

\begin{tcolorbox}[colback=gray!10,  width=\textwidth]
\begin{lstlisting}[breaklines]
Answer the following by the synthesizing findings from multiple junior workers (LLMs).


---
## Inputs
1. Question to answer:
{question}

2. Collected Job Outputs (from junior models):
{extractions}

---
First think step-by-step and then answer the question using the exact format below.

## ANSWER GUIDELINES

**Required JSON Output**: You must output exactly one JSON object with these keys:
   - "decision": Must be "provide_final_answer" or "need more information"
   - "explanation": A short statement about how you arrived at your conclusion or what is still missing.
   - "answer": a final answer that is a text span pulled directly from the job output citations.

     
Here is the template for your JSON response:

<think step-by-step here>

{{
"decision": "...",
"explanation": "...",
"answer": "..,", 
}}

Now, carefully inspect the question, think step-by-step and perform any calculations before outputting the JSON object. 
- If answer choices are provided, your answer must **exactly** match one of the answer choices.
- Don't paraphrase the final answer --- extract text directly from the document(s) or previous job outputs.

Question: 
{question}

Your Answer:
\end{lstlisting}
\end{tcolorbox}

