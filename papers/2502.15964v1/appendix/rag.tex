\subsection{Relationship with Retrieval-Augmented Generation}\label{app:rag}
\input{figures/rag-comparison/rag-comparison.tex}

In this section, we discuss the relationship between local-remote collaboration and retrieval-augmented generation (RAG), a technique that reduces the number of tokens processed by an LM by retrieving a subset of relevant documents or chunks LM~\cite{lewis2020retrieval,karpukhin2020dense,lee2019latent}.

Retrieval-augmented generation and local-remote collaboration (\textit{e.g.} \system) are complementary techniques. 
They both provide a means to reduce cost by providing an LLM with a partial view of a large context. 
But, as we discuss below, they also have different error profiles and can be used in conjunction to improve performance.

\subsubsection{Comparison of \system and RAG on \finance}\label{app:rag:finance}
In \Cref{fig:rag-comparison} (left), we plot the quality-cost trade-off on \finance for local-remote systems (\naive and \system) and RAG systems using BM25 and OpenAI's \texttt{text-embedding-3-small} embeddings~\cite{article,neelakantan2022text}.
For RAG, we use a chunk size of 1000 characters, which we found to be optimal for this dataset after sweeping over chunk sizes with the BM25 retriever (see \Cref{fig:rag-comparison} (center)). We show how a simple hyperparameter (number of retrieved chunks provided to the remote model) allows us to trade off quality of the RAG system for remote cost.
% 
Furthermore, we note that when the BM25 RAG system provides 50 or more chunks of the document to the remote model, it exceeds the performance of the remote model with the full context. This likely indicates that RAG helps in minimizing distractions from the long context. 
% 
For \finance, when compared to \system, the RAG system with OpenAI embeddings reaches similar points in the quality-cost trade-off space. Interestingly however, none of the RAG configurations are are able to match the quality of \naive at the same low cost. 

\subsubsection{Comparison of \system and RAG (Embeddings + BM25) on Summarization Tasks}\label{app:rag:summary}
RAG is a very suitable approach for \finance, since all of the questions heavily rely on information extraction from specific sections of financial statements. However, RAG will not be suitable for a summarization task, unlike small LMs. Therefore, we use the long-document summarization dataset, \textsc{BooookScore}~\citep{chang2023booookscore}. \textsc{BooookScore} which contains a set of 400 books published between 2023-2024. The average story length in \textsc{Booookscore} is 128179 tokens with a max of 401486 tokens and a minimum of 26926 tokens. We utilize both \system, RAG (w/Embeddings + BM25), and \gpt only to complete the task. We describe the set-up for all three approaches next.
\\

\textbf{\system for summarization} In applying \system to the task, the $\locallm$ (\llamathreetwo-3B-Instruct) provides summaries on chunks of the original text, passing a list of chunk summaries to the $\remotelm$ (\gpt). $\remotelm$ produces the final summary. 

\textbf{RAG (Embedding) for summarization} In our embedding-based RAG approach, we use the OpenAI \textsc{text-embedding-3-small} to embed chunks of the original text (of length 5000 characters) and we retrieve the top-15 most relevant chunks using the query ``Summarize the provided text''. We then prompt \gpt to generate a complete summary over the retrieved chunks.

\textbf{RAG (BM25) for summarization} In our BM25-based RAG approach, we use the BM25 to retrieve chunks of the original text (of length 5000 characters) based on the query: ``Summarize the provided text''. We retrieve the top-15 most relevant chunks and prompt \gpt to produce a final summary over the retrieved chunks. We choose top-15 to ensure the number of tokens passed up by the baseline is comparable with those passed up by \system.

\textbf{GPT-4o} In our final baseline, we use \gpt alone to create the story summaries. For texts that extend beyond the 128K context length window, we truncate the stories.

\paragraph{Evaluation}
\begin{itemize}
    \setlength{\itemindent}{-1em}
    \item \textbf{Qualitative} In Table~\ref{tab:story-summaries} we provide samples outputs from each of the 4 methods described above. We highlight major events in red, themes in green, locations in blue and names in indigo. The samples demonstrate that amongst all the methods, \system outputs contain the most entity mentions and story specific details. Moreover, when compared to \gpt-only $\remotelm$,
\system is $9.3\times$ more efficient --- 11,500 versus the full 108,185 prefill tokens.

The summaries from \system are generally $1.3\times$ longer and more verbose than the RAG systems' summaries, likely indicating that the former is more effective at ``passing forward'' salient information. Moreover, RAG systems' summaries are missing the main arc of the narrative in favor of what seems an assortment of facts.  
    \setlength{\itemindent}{-1em}
    \item \textbf{Quantitative} We additionally perform a quantitative analysis of the generated summaries using a LLM-as-a-judge framework. As an evaluator, we use the \textsc{claude-3.5-sonnet} model, to avoid any biases between the evaluator and the supervisor model. We prompt the model with the generated summary, ground truth summary (gpt4-4096-inc-cleaned) provided from the original \textsc{BooookScore} generations, and a grading rubric (see Figure~\ref{rubric:summary}). The rubric evaluates 7 criteria: coherence, relevance, conciseness, comprehensiveness, engagement \& readability, accuracy, and thematic depth. We prompt \textsc{claude-3.5-sonnet} to generate a score (1-5) for each of the criteria and average the scores. We find that summaries generated by \system score comparably with \textsc{GPT4o}-only generated summaries, while RAG based baselines perform worse. Our results can be found in Table~\ref{tab:summary_qualitative_scores}.
\end{itemize}


\begin{figure}[h]
    \centering
    \begin{tcolorbox}[width=\textwidth, colframe=black!50, colback=white, sharp corners]
        \textbf{Evaluation Rubric for Summaries}
        \begin{enumerate}
            \item \textbf{Coherence (1-5):} Summary is logically structured, with clear connections between events, avoiding abrupt jumps or inconsistencies.
            \item \textbf{Relevance (1-5):} Accurately reflects key themes, events, and characters, focusing on essential details without unnecessary plot points.
            \item \textbf{Conciseness (1-5):} Thorough yet avoids excessive detail, presenting necessary information without redundancy.
            \item \textbf{Comprehensiveness (1-5):} Covers all major characters, events, and themes, ensuring a complete overview without omissions.
            \item \textbf{Engagement \& Readability (1-5):} Engaging and easy to read, with well-constructed sentences and clear, precise language.
            \item \textbf{Accuracy (1-5):} Stays true to the bookâ€™s storyline, themes, and tone, with correct details, names, and events.
            \item \textbf{Thematic Depth (1-5):} Identifies underlying themes and messages, providing insights into conflicts, motivations, and resolutions.
        \end{enumerate}
    \end{tcolorbox}
    \caption{Evaluation Rubric for Summaries}
    \label{rubric:summary}
\end{figure}

% \\

% \textbf{Qualitative} In Table\ref{tab:story-summaries} we provide samples outputs from each of the 4 methods described above. We highlight major events in red, themes in green, locations in blue and names in indigo. The samples demonstrate that amongst all the methods, \system outputs contain the most entity mentions and story specific details. Moreover, when compared to \gpt-only \remotelm,
% \system is $9.3\times$ more efficient --- 11,500 versus the full 108,185 prefill tokens.

% The summaries from \system are generally $1.3\times$ longer and more verbose than the RAG systems' summaries, likely indicating that the former is more effective at ``passing forward'' salient information. Moreover, RAG systems' summaries are missing the main arc of the narrative in favor of what seems an assortment of facts.  

% \textbf{Quantitative} We additionally perform a quantitative analysis of the generated summaries using a LLM-as-a-judge framework. As an evaluator, we use the \textsc{claude-3.5-sonnet} model, to avoid any biases between the evaluator and the supervisor model. We prompt the model with the generated summary, ground truth summary (gpt4-4096-inc-cleaned) provided from the original \textsc{BooookScore} generations, and a grading rubric. The rubric evaluates 7 criteria: coherence, relevance, conciseness, comprehensiveness, engagement \& readability, accuracy, and thematic depth. We prompt \textsc{claude-3.5-sonnet} to generate a score (1-5) for each of the criteria and average the scores. We find that summaries generated by \system score comparably with \textsc{GPT4o}-only generated summaries, while RAG based baslines perform worse. Our results can be found in Table~\ref{tab:summary_qualitative_scores}.

\begin{table}[h]
    \centering
    \begin{tabular}{lc}
        \hline
        Method & Score \\
        \hline
        \system & 3.01 \\
        GPT4o & 3.06 \\
        RAG (BM25) & 2.48 \\
        RAG (Embedding) & 2.38 \\
        \hline
        \hline
    \end{tabular}
    \caption{Comparison of Methods and Rubric Scores}
    \label{tab:summary_qualitative_scores}
\end{table}






% it  utilizes 11.5K prefill tokens of $\remotelm$ where as \gpt-only uses 108,185 prefill tokens. 

\input{tables/summarization_comparison}





