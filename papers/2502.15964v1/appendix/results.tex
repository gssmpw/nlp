\section{Extended Results}
\input{figures/round-strategy/round-strategy}

\subsection{Model Analysis}\label{app:model_history}
\input{tables/remote_model_variations}
\input{tables/system_snapshot}
We include additional experiment results from Section~\ref{subsec:results-model}. In Table~\ref{tab:remote-model-variations} we show the effects of varying $\remotelm$ on \system.  In Table~\ref{tab:system-snapshot}, we show the performance of \system using the best in-class models at the time (from late 2023 to late 2024).


\subsection{\naive $\locallm$ Analysis}
\input{tables/small-lm-micros}
\label{app:naive-analysis}

We perform an empirical analysis evaluating the robustness of $\locallm$. We perform experiments to evaluate two axes of model capabilities: (1) ability to reason over long contexts and (2) ability to solve multi-part queries. To test (1) and (2) we curate a synthetic dataset built over the \finance dataset wherein we use \gpt to construct an extraction based question-answering dataset over chunks (length 512 tokens) of documents in the \finance dataset. We then construct two settings evaluating over \llamathreetwo-3B-Instruct.
\\

\textbf{Long Context Reasoning}: To evaluate long-context reasoning, we concatenate between \{1,16,32,64,128\} chunks to construct the context. At least one chunk in the concatenated context contains the ground truth result. As seen in Table~\ref{tab:chunks_vs_accuracy}, increasing the context length from 512 to ~65.5K tokens leads to a 13 point drop in accuracy.
\\

\textbf{Multi-step Queries} To evaluate the ability of $\locallm$ to fulfill multi-step queries, we construct queries that have between \{1,2,3,4\} sub-tasks. Our results indicate increasing from 1 to sub-tasks leads to a 56.3 point drop in accuracy (see Table~\ref{tab:subtasks_vs_accuracy}).

\input{tables/app-tradeoff}


\input{appendix/rag.tex}