\section{Introduction}
\label{sec:intro}

Today's frontier, cloud-hosted Language Models (LMs) are proficient at challenging reasoning tasks, including programming across repositories and making decisions in financial, legal, and medical domains. While these models offer powerful capabilities, accessing them remains expensive: processing a standard million-token code repository with OpenAI's o1 costs $>\$15$ per query. 

At the same time, edge devices contain a substantial amount of underutilized compute that could potentially assist with these workloads. Small LMs (\textit{e.g.} with 1-7B parameters) can now run on smartphones and laptops at reasonable throughput (Apple's OpenELM, PhoneLM,llama.cpp), \textit{e.g.} - an \textcolor{red}{XX} model can process \textcolor{red}{XX} tokens per second on a phone. Motivated by this cost asymmetry and the rapidly improving quality of small LMs' (CITE), we ask the question: how can we leverage edge compute resources to support cloud LMs in complex reasoning workflows and improve the cost-quality trade-off?

%Leveraging edge computing to reduce computational load is not a new concept. 
The prevailing approach for offloading computational demands from cloud LMs is retrieval-augmented generation (RAG). In RAG systems, only the most relevant subset of corpus documents is provided to the LM, with relevance determined via methods such as keyword lookups~\cite{robertson_relevance_2009}, neural embedding search~\cite{karpukhin2020dense,lewis2020retrieval,lee2019latent}, or web search~\cite{nakano2021webgpt}. By performing retrieval locally, RAG systems reduce cloud costs by \textit{minimizing} the context that needs to be processed by the cloud LM.

In this work, we take a step further and investigate other ways --- beyond retrieval -- that edge compute can be leveraged to compress the tokens processed by cloud LMs.
% while simultaneously enhancing output quality. 
We focus on edge-remote collaborative systems where smaller edge LMs, with full access to the context, collaborate with more capable remote models. 
In principle, the edge models can handle token-intensive tasks such as summarization and information extraction, while deferring to the remote models for planning, reasoning and verification.
% With full access to the context and the ability to scale inference-time compute (CITE), edge models can handle a significant portion of the processing, delivering compressed, task-specific representations to the cloud LM as needed.

% The challenge of reducing computational load on cloud LMs has primarily been addressed through retrieval-augmented generation (RAG). In RAG systems, only the most relevant subset of corpus documents is passed to the LM, with relevance determined through keyword lookups~\cite{robertson_relevance_2009}, neural embedding search~\cite{karpukhin2020dense,lewis2020retrieval,lee2019latent}, or web search~\cite{nakano2021webgpt}. Retrieval can be run locally and reduce cloud cost by merely filtering out tokens. 



% Here, we propose a broader investigation of edge-remote collaborative systems, where small edge LMs work in tandem with more capable remote models. Beyond retrieval, edge models can summarize, extract information, and execute components in a multi-step plan. The edge model, with complete access to the context and ability to scale inference-time compute (CITE), can provide compressed, task-relevant views to the remote model.

However, in practice, there are several challenges that arise when implementing edge-remote systems: 

\begin{itemize}
    \item \emph{Effective communication protocols.} LMs are designed to interact with humans, not other models. It is not clear how should a remote model effectively request data, ask questions, and provide guidance to an edge model.
    \item \emph{Unreliability of edge models.} Despite recent improvements, small LMs often hallucinate and make mistakes that larger LMs would not, even in relatively simple tasks like summarization or information extraction. It is unclear which of their outputs can be trusted and which tasks can be safely delegated to them.
    \item \emph{Unconventional use case for frontier models.} The various stages of LM training prepare frontier models to reason with full raw contexts. Can they make do with fragments of processed information at varying levels of fidelity? What kinds of \emph{procedural knowledge} do they possess, enabling them to sketch solutions to problems without seeing the raw data?
\end{itemize}

We identify three key design decisions which address the challenges above and improve the cost-quality tradeoff:

\begin{itemize}
    \item We propose a communication strategy in which the cloud LM interacts with the edge LLM in two primary roles: (1) providing guidance and (2) synthesizing returned information. This establishes a two-way communication channel where tasks are delegated to edge resources. The edge LLM processes these tasks and returns information to the cloud LM, which then synthesizes the new input and either delegates further tasks or provides a final response.  
    \item We use repeated sampling to improve the reliability of the edge models, in line with recent results on scaling test-time compute. 
    \item We use summarization and extraction to improve the cloud LM's visibility into local context. 
\end{itemize}

We apply the edge-remote system to a suite of legal, medical, and financial reasoning benchmarks, and characterize the tradeoff between accuracy and cost (the number of tokens processed by the frontier model). We ablate different components of our system and show which stages are crucial to performance, and which benefit from scaling inference-time compute. We then compare our method to frontier models with full access to the context as well as SoTA RAG baselines. 

We find that edge-remote systems recover \textcolor{red}{XXX} of the remote-model performance while processing \textcolor{red}{XXX} fewer tokens. They also offer cost-accuracy tradeoffs favorable to RAG systems: when both methods process a nearly identical number of tokens, edge-remote is up to \textcolor{red}{XXX} better than RAG.
