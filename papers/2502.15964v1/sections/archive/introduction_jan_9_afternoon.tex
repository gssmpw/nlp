\section{Introduction}
\label{sec:intro}
Frontier, cloud-hosted Language Models (LMs) now excel at challenging reasoning tasks such as repository-level programming, as well as financial, legal, and medical decision-making. Solving such tasks often requires integrating information across millions of context tokens. However, performing long-context reasoning tasks is expensive: processing a standard million-token code repository with OpenAI’s o1 costs $>\$15$ per query. With many input tokens, the computational cost is dominated by the prefill-stage computation, where a cache of keys and values is formed for every input token. How can we reduce frontier-model cost while maintaining quality?

To reduce the number of tokens processed by cloud-hosted LLMs, practitioners rely primarily on retrieval-augmented generation (RAG), where only a subset of corpus documents that are deemed most similar to the query are passed in-context to an LM. Relevance is determined either through keyword look-ups~\cite{robertson_relevance_2009}, neural embedding search~\cite{karpukhin2020dense,lewis2020retrieval,lee2019latent}, or search~\cite{nakano2021webgpt}. Retrieval reduces prefill-stage computation by simply filtering out tokens, but still the LM still processes raw chunks of text from the corpus. 
% Other forms of context compression involve “memory” mechanisms, in which the LM stores only relevant information from a previous context and attends the memory store during subsequent generations. 
% Retrieval augmented generation is a powerful paradigm for reducing cost, but we find it often underperforms long-context frontier models with full-access to the context. 

New opportunities emerge with small LMs (\textit{e.g.} with 1-7B parameters) which are rapidly improving in quality and can be run on commodity edge hardware at reasonable throughput. For example, an \textcolor{red}{XX} model can be run on a phone at \textcolor{red}{XX} tokens per second. Today, these models play a limited role in challenging reasoning workloads. Motivated by the cost asymmetry between cheap edge LMs and expensive but more capable remote LMs, we study edge-remote collaborative systems, where a small LM with complete access to the context, and an ability to scale inference-time compute, collaborates with a remote frontier LM without direct access to the context. Put differently, the edge model provides a compressed, task-relevant view of the context to the remote model. 
% following the instructions and verification of the remote model. 
We explore the design choices of edge-remote systems and measure the tradeoff between cost and quality.

When prototyping edge-remote systems, we identified three practical challenges: 

\begin{itemize}
    \item \emph{Identifying effective communication protocols.} Language models are designed to interact with humans, not other models. It is not clear how should a remote model effectively request data, ask questions, and provide guidance to an edge model.
    \item \emph{Edge model can be unreliable.} Despite recent improvements, small LMs often hallucinate and make mistakes that a larger LMs would not, even in relatively simple tasks like summarization or information extraction. It is unclear which of their outputs can be trusted and which tasks can be safely delegated to them.
    \item \emph{Unconventional use case for frontier models.} The various stages of LM training prepare frontier models to reason with full raw contexts. Can they make do with fragments of processed information at varying levels of fidelity? What kinds of \emph{procedural knowledge} do they possess, enabling them to sketch solutions to problems without seeing the raw data?
\end{itemize}

We identify three key design decisions which address the challenges above and improve the cost-quality tradeoff:

\begin{itemize}
    \item We propose prompting strategies and workflows that result in effective edge model performance as well as informative and concrete instructions from the big LM. We also point to strategies that do not work.m
    \item We use repeated sampling to improve the reliability of the edge models, in line with recent results on scaling test-time compute. 
    \item We use summarization and extraction to improve the remote model’s visibility into local context do XXXX.
\end{itemize}

We apply the edge-remote system to a suite of legal, medical, and financial reasoning benchmarks, and characterize the tradeoff between accuracy and cost (the number of tokens processed by the frontier model). We ablate the performance of the model and show which stages are crucial to performance, and which benefit from scaling inference-time compute. We then compare our method to frontier models with full access to the context as well as SoTA RAG baselines. 

We find that edge-remote systems recover \textcolor{red}{XXX} of the remote-model performance while processing \textcolor{red}{XXX} fewer tokens. They also offer cost-accuracy tradeoffs favorable to RAG systems: when both methods processing a nearly identical number of tokens, edge-remote is up to \textcolor{red}{XXX} better than RAG. 
