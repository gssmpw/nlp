\section{Introduction}
\label{sec:intro}
Today's frontier Language Models (LMs) can program across repositories and make complex decisions in financial, legal, and medical domains. However, accessing these models is expensive: processing a standard million-token code repository with OpenAI's o1 API costs $>\$15$ per query. 
% 
At the same time, smaller LMs (1-7B parameters) are rapidly improving and can now run on personal computers (ollama, llama.cpp) and smartphones (OpenELM, PhoneLM)  (\textit{e.g.} an \textcolor{red}{XX} model can process \textcolor{red}{XX} tokens per second on a phone). But these small LMs are not participating in challenging reasoning tasks, leaving the compute on edge devices underutilized.

This motivates the study of \emph{edge-remote systems}, in which small LMs at the edge collaborate with frontier models in the cloud. Such systems could reduce cloud costs and unlock new asynchronous workflows (\textit{e.g.} bug finding) that run on idle compute resources. However, the cost-accuracy tradeoff induced by edge-remote collaboration is not well understood. 

The cost-accuracy tradeoff is governed by fundamental cost and quality asymmetries between small edge and frontier remote LMs. 

\begin{itemize}
    \item \emph{Capabilities}: There is a well-known quality gap between small and large LMs~\cite{kaplan2020scaling}. Often this gap is even larger on real-world tasks than benchmark results would suggest (see Section X). Looking forward, we expect the gap to persist despite rapid improvements across small and large models.

    \item \emph{Cost}: Since frontier models have 10-1000x more parameters than small models, they of course require more hardware, power and memory to run. But there are other differences too: batching requests affects the cost differently depending on whether we're at the edge or in the cloud. In the cloud, the server host can batch requests across concurrent users, so an individual user only obtains marginal savings by batching their requests. In contrast, at the edge, generating a batch of hundreds of samples in parallel is often no more costly than generating a single sample~\cite{leviathan2023fast}. 
\end{itemize}

The asymmetries above motivate us to focus on \emph{divide-and-conquer} strategies, where the remote model dispatches parallel tasks to worker models at the edge (see \Cref{fig:system}).
% 
% Given a \textit{task} to be performed over a large \textit{context} (\textit{e.g.} a codebase),
Given a \textit{task}  to be performed over a large \textit{context} (\textit{e.g.} a codebase), a divide-and-conquer edge-remote system proceeds in three (potentially looped) steps:
\begin{enumerate}
    \item \textbf{Decompose} The remote model writes code that decomposes the task and context into a long list of \textit{jobs}, all without actually pre-filling (\textit{i.e.} ``reading") the entire context.
    \item \textbf{Execute} The edge model then executes the jobs in parallel and sends a selection of the responses back to the remote model.
    \item \textbf{Aggregate} The remote model aggregates the outputs from the edge and finalizes the task output or loops back to the decompose step.
\end{enumerate}
% Given a \textit{task} to be performed over a large \textit{context} (\textit{e.g.} a codebase),  have the remote model first write code that decomposes the task and context into a list of \textit{jobs} which are sent to the edge. The edge model then executes the jobs in parallel before sending selected outputs back to the remote model for synthesis.
% 
Divide-and-conquer is a natural choice given the asymmetries discussed above. Critically, this strategy promises to significantly reduce remote costs, since none of the context is directly processed in the cloud. Furthermore, by decomposing the task into “bite-sized” jobs, it mitigates the gap in capabilities between remote and edge models. And finally, by delegating batches of parallel jobs to the edge, divide-and-conquer systems can achieve higher utilization of edge hardware, improving quality at little additional cost. 

% To reduce cloud costs, the raw input tokens will not be sent to the remote LM. The latter only interacts with a small edge LM with full access to the input. To offset the small LM's limited capabilities, the task is decomposed into ``bite-sized'' tasks and the input is decomposed into chunks. Benefiting from local parallelism, the edge model can efficiently execute subtasks on chunks of input. 

Despite the simplicity of the divide-and-conquer approach described above, the design and hyperparameter space of these kinds of edge-remote systems are poorly understood.
In this work, we perform an extensive analysis centered on three core design choices. Each design choice provides a different ``knobs'' that be turned to trade off cost and quality. 
\begin{enumerate}
  \item \textbf{Model choice} \textit{How does the choice of model at the edge and in the cloud affect cost and quality?} At the edge, we consider models ranging from 1B to 8B parameters from the Llama~\citep{dubey2024llama3} and Qwen families~\citep{bai2024qwen2.5}. In the cloud, we consider leading frontier models such as GPT 4o, Claude Sonnet 3.5, and Llama 3.3 70B. We show the effects of model pairs in \Cref{subsec:results-model}.
  \item \textbf{Sequential Communication Protocols.} \textit{Can multiple rounds of communication improve quality? At what cost?} We consider two strategies for improving the quality via multi-round of communication: (a) simple retrying and (b) full multi-turn edge-remote conversations. (See \Cref{subsec:results-communication})
  \item \textbf{Parallel Workloads at the Edge.} \textit{How should we structure parallel workloads at the edge to maximize performance and minimize cost?} \Cref{subsec:results-workloads}. We consider two strategies for controlling the degree of parallelism of jobs at the edge: (a) random sampling and (b) chunking and decomposition. \Cref{subsec:results-workloads}
\end{enumerate}

There are several important takeaways from our analysis that can inform research and development of edge-remote systems going forward. Across a three challenging benchmarks spanning financial~\cite{islam2023financebench}, medical ~\cite{adams2024longhealth}, and scientific reasoning ~\cite{dasigi2021dataset} we show that:
\begin{itemize}
  \item Optimized edge-remote systems can recover the performance of remote-only systems while sending only \yell{10\%} of the tokens to the remote model.
  \item We can navigate the cost-accuracy Pareto frontier using both sequential and parallel test-time compute strategies. 
    \begin{itemize}
        \item \textbf{Model choice.} The size of the edge model . On scientific reasoning (QASPER), accuracy improves from $38\%$ to $66\%$ when moving from a 1B to 8B parameter edge model. The 8B parameter figure is a 6 percentage point improvement over the baseline set by GPT-4o acting in isolation. However, 8B edge models are challenging to run on some edge hardware, especially when latency is a constraint.
        \item \textbf{Sequential.} By increasing the number of sequential rounds of communication, we can tradeoff of increased cost for improved quality. For example, on financial reasoning (FinanceBench) with 3B parameter edge models and GPT-4o in the cloud, we can improve accuracy from $53\%$ to $70\%$ by simply increasing the maximum number of communication rounds. However, this nearly doubles the number of tokens processed by the remote model.
        \item \textbf{Parallel.} By increasing the number of parallel jobs at the edge, we can also boost quality. We show that this increased parallelism is free at the edge due to batching and the IO-bound nature of LLM inference.  For example, on medical reasoning (LongHealth), by increasing the number of random samples, we can improve accuracy from \yell{$X\%$} to \yell{$Y\%$}. Critically, we show that this leads only to a \yell{$1.0z\times$} in edge latency, due to batching.
    \end{itemize}
  \item In Section \ref{subsec:results-communication}, we perform a fine-grained error analysis, showing how different types of tasks are more or less amenable to the different strategies described above. For example, \yell{TODO}. We also highlight several types of tasks where divide-and-conquer systems remain ineffective. This motivates future work in joint remote-edge training. 
  % \item TODO
\end{itemize}
% Most existing divide-and-conquer strategies rely on \emph{brittle, hand-crafted workflows} (CITE MAP REDUCE LM, DOCETL and others). Task decomposition is nontrivial and domain-specific. While previous work typically uses only large LMs, it is unclear which task decompositions are suitable for two assymetric LMs. We started by hand-crafting systems for finance, medical, and academic research domains. We found that significant tuning is required to match simple baselines such as directly generating with the remote model (with full access to context) or retrieval augmented generation.

% We then proceeded to ask whether remote LLMs can discover and generate appropriate task decomposition. This approach draws parallels to Domain-Specific Languages (DSLs), where software designers have traditionally struggled with decomposing tasks into manageable abstractions, and where LLMs are showing great promise. We investigate one prototype of a domain-agnostic edge-remote system, which we call \system with two core principles:
% \begin{itemize}
%     \item The remote model communicates with the edge model via code generation, motivated by the observation that some subtasks  (e.g. chunking long context) can be performed more reliably and efficiently with code (cite evaporate, chain of code).
%     \item \emph{Test-time scaling} is leveraged in two ways:
%     \begin{itemize}
%         \item \emph{Sampling}. The edge cost model encourages batching, which we use for test-time compute strategies (e.g. sampling/majority vote), to bolster the reliability of the small edge LM. 
%         \item \emph{Looping}. The remote and local model can have as many back-and-forth communication rounds as needed to confidently solve the task or abstain.
%     \end{itemize}
% \end{itemize}

% In experiments across domains, we first characterize which design choices are needed to match hand-crafted workflows. 
% With ablation studies, we then assess the implications of each of the above principles. We find that repeated sampling on edge, accompanied by step-by-step reasoning on remote, are necessary for success (made this up). While certain tasks require back-and-forth looping (\textit{e.g.} medical), others are not (finance).
% We investigate cost-accuracy tradeoffs across model sizes and historical generations for both the remote and local models. We find that systems with as small as a 3B edge model (NAME) can match the remote model performance with XX fewer tokens. Given historical data, we project that performance can be improved by XX on the same hardware each year. 

% To conclude, our contributions are as follows: 
% \begin{itemize}
%     \item We extensively investigate a new class of edge-remote collaborative systems, which we believe to be inevitable given trends in model and hardware development.
%     \item We propose the desiderata for such systems (Sec XX), involving communication efficiency, “intelligence ratios” between remote and edge models, and scaling properties.
%     \item We propose an autonomous a domain-general architecture coined \system, which reasons in code to dispatch tasks to edge LMs, and compare it to hand-crafted pipelines
%     \item We identify the core components of an edge-remote system and characterize how it behaves on different domains using different sizes and generations of LMs for both edge and remote
% \end{itemize}


