
% For many problems the question 

% Feedback 
% (1) make sure to run the minions protocol with edge-edge
% (2) give some prescriptive recommendation based on problem type for what design choices to use

\section{Introduction}
\label{sec:intro}
Today's cloud-hosted frontier Language Models (LMs) can program across repositories and make complex decisions in financial, legal, and medical domains. However, accessing these models is expensive: processing a standard million-token code repository with OpenAI's o1 API costs $>\$15$ per query. 
% 
At the same time, smaller LMs (1-8B parameters) are rapidly improving and can now run on personal computers (ollama, llama.cpp) and smartphones (OpenELM, PhoneLM)  (\textit{e.g.} an \textcolor{red}{XX} model can process \textcolor{red}{XX} tokens per second on a phone). 
%
Currently, small edge LMs are used exclusively for simple tasks such as text completion and summarization, while advanced reasoning tasks are typically handled entirely in the cloud by frontier models. 


% How can we do more on edge? And what does this mean or the cost / acc of frontier models/
We ask: \textit{how can we utilize a small LM on-device to reduce cloud inference costs}? Our setting includes two models interacting in natural language: a frontier LM in the cloud (\textit{e.g.} \textit{\gpt}) and a small LM running on-device with exclusive access to context (\textit{e.g.} \llama-3B). See \cref{sec:prelim-setting} for a detailed description of the setting under study.

We draw inspiration from real-world workflows and focus on tasks that require reasoning over large volumes of financial, medical, and scientific text. The text, such as a company's 10-K financial statement, is assumed to be stored locally. The user specifies a task that can range from simple information extraction (``What was the total revenue in FY2023?'') to multi-step calculations (\yell{Sabri, can you copy a concrete example of a calculation}?).

% Inspired by recent work on multi-agent systems (CITE MoA), 
% we study an asymmetric collaboration setting where a small edge LM (\textit{e.g.} Llama-2-3.2B), with full access to the input, communicates with a frontier cloud-hosted LM (\textit{e.g.} Claude-Sonnet-3.5) without access to the input.

We first consider a basic turn-taking communication protocol (\naive), that is, a chat between the local and remote model. By having only the local model ``read'' the full context, we can achieve a $\yell{20}\times$ reduction in remote model costs, however, losing  \yell{10\%}  (see \Cref{sec:naive} for details)
% (
However, this naïve protocol struggles to recover the performance of the frontier model. 

% We identify two reasons why the \naive protocol is suboptimal for the edge-remote setting:

% Need to make this very concrete with a forward pointer to exact definition of what this naive sysytem looks like 
% Below, I think we can drop the middle bullet and these should also just point to a section where we highlight the limitations of the naive back and forth.

\begin{itemize}
    \item  \emph{Limitations of small LMs.} The well-known quality gap between large and small models \cite{kaplan2020scaling} is even more pronounced on the real-world workflows under study. We find two core limitations that hamper an unconstrained back-and-forth chat:
    \begin{itemize}
        \item \textbf{Long-context processing}. Small LMs can get distracted and incoherent when processing long sequences of tokens.
        \item \textbf{Multi-step instruction following} Small LMs cannot be trusted to follow multi-step instructions.
    \end{itemize}
    
    Small models struggle with long contexts and do not adhere to the complex, multi-part instructions output by large models. We explore these limitations in \Cref{sec:naive-quality}.
    % \item Frontier LM's need to \emph{calibrate} to the capabilities of the small LMs and infer when can their outputs be trusted.
    \item \emph{Low utilization of on-device resources.} As we discuss in \Cref{sec:naive-util}, this protocol achieves only \yell{X\%} hardware utilization at the edge because LM inference is I/O bound with batch size one. In the cloud, this isn't a problem because the server host can batch requests across concurrent users. However, at the edge, generating a hundred samples in parallel is often no more costly than generating a single sample \cite{leviathan2023fast}.  Therefore, this sequential turn-taking protocol leaves significant edge compute on the table.
\end{itemize}

% Needs forward pointer to exact method defintiion and pointer to figure showing the method
Motivated by these limitations, we propose a simple extension of this protocol, coined \system  (emphasis on the $\mathcal{S}$), where the remote model decomposes the task into \emph{subtasks} that can be executed \emph{in parallel} on device. 

But how can the remote model perform a fine-grained task decomposition \emph{without seeing any data}? Our first obseration is that given a question, a frontier model possess an impressive amount of ``procedural knowledge'', \textit{i.e.} which steps are needed to answer a question, and which data each requires.

A second problem, is that not only we need to decompose the task: the context needs to be chunked at some chosen level of granularity, and passed to the local model along with a subtask.  To achieve that, our second insight is to generate code by the remote model, that can be executed locally where context is available, to flexibly chunk the document and assign tasks to individual chunks.

% 
\system has three steps, which are performed in a loop:
\begin{enumerate}
    \item \textbf{Decompose} Given a task, the remote model writes code that decomposes the task and context into a long list of ``bite-sized'' \textit{subtasks}.
    \item \textbf{Execute} The edge model then executes the subtasks in parallel and sends a filtered selection of the responses back to the remote model.
    \item \textbf{Aggregate} The remote model aggregates the outputs from the edge and finalizes the task output or loops back to the Decompose step.
\end{enumerate}

% This protocol is motivated by a rich literature, ~\cite{mixtur} \yell{the sequential nature should be explained, they may ask about what can or cannot be broken down in this way}.
\system is a natural choice given the challenges discussed above. By decomposing the task into “bite-sized” jobs, it mitigates the gap in capabilities between remote and edge models. And finally, by delegating batches of parallel jobs to the edge, \system can achieve higher utilization of edge hardware, improving quality at little additional cost. 


% Can probably drop first sentence
% To reiterate, all remote computations are done without pre-filling (\textit{i.e.} ``reading") the entire input. In fact, we find that by communicating less than 10\% of the total input tokens to the cloud, MINION performs comparably to the frontier model in finance, health, and academic paper QA benchmarks \yell{CITE}. 

% MINION is a natural choice given the asymmetries discussed above. Critically, this strategy promises to significantly reduce remote costs, since none of the context is directly processed in the cloud. Furthermore, by decomposing the task into “bite-sized” jobs, it mitigates the gap in capabilities between remote and edge models. And finally, by delegating batches of parallel jobs to the edge, divide-and-conquer systems can achieve higher utilization of edge hardware, improving quality at little additional cost. 

% We identify three key design choices (i.e. hyperparameters) that govern this protocol. However, it is unlear how they affect the cost-accuracy tradeoff. We analyze this:

Despite the intuitive appeal of such edge-remote communication protocols, we have a poor understanding of their design and hyperparameter space which in turn serve as ``knobs'' that trade off cost for quality. Concretely:
\begin{enumerate}
  \item \textbf{Model choice} \textit{How does the choice of model at the edge and in the cloud affect cost and quality?} At the edge, we consider models ranging from 1B to 8B parameters from the \llama~\citep{dubey2024llama3} and \qwen families~\citep{bai2024qwen2.5}. In the cloud, we consider leading frontier models such as \gpt, Claude Sonnet 3.5, and \llama-3.3-70B. We show the effects of model pairs in \Cref{subsec:results-model}.
  \item \textbf{Sequential Communication Protocols.} \textit{Can multiple rounds of communication improve quality? At what cost?} We consider two strategies for improving the quality, through looping of the \system steps (a)``retry'': increasing the number of attempts for the \system system, where each attempt is blind to previous attempts (b) allowing longer multi-turn conversations where the remote model sees the results of the previous attempts (See \Cref{subsec:results-communication})
  \item \textbf{Parallel Workloads at the Edge.} \textit{How should we structure parallel workloads at the edge to maximize performance and minimize cost?} \Cref{subsec:results-workloads}. We consider two strategies for controlling the degree of parallelism of jobs at the edge: (a)  repeated sampling~\citep{brown2024large} and (b) chunking and decomposition. \Cref{subsec:results-workloads}
\end{enumerate}

In Section \ref{subsec:results-communication}, we perform an error analysis, showing how different types of tasks are more or less amenable to the different strategies described above. For example, \yell{TODO}. We also highlight several types of tasks where divide-and-conquer systems remain ineffective. This motivates future work in joint remote-edge training.


There are several important takeaways from our analysis that can inform research and development of edge-remote systems going forward. Across a three challenging benchmarks spanning financial~\cite{islam2023financebench}, medical ~\cite{adams2024longhealth}, and scientific reasoning ~\cite{dasigi2021dataset} we show that \yell{add short description on task hierarchies and why we chose these}:
\begin{itemize}
  \item Optimized edge-remote systems can recover the performance of remote-only systems while sending only \yell{10\%} of the tokens to the remote model.
  \item We can navigate the cost-accuracy Pareto frontier using both sequential and parallel test-time compute strategies. 
    \begin{itemize}
        \item \textbf{Model choice.} The size of the edge model . On scientific reasoning (\qasper), accuracy improves from $38\%$ to $66\%$ when moving from a 1B to 8B parameter edge model. The 8B parameter figure is a 6 percentage point improvement over the baseline set by GPT-4o acting in isolation. However, 8B edge models are challenging to run on some edge hardware, especially when latency is a constraint.
        \item \textbf{Sequential.} By increasing the number of sequential rounds of communication, we can tradeoff of increased cost for improved quality. For example, on financial reasoning (\finance) with 3B parameter edge models and GPT-4o in the cloud, we can improve accuracy from $53\%$ to $70\%$ by simply increasing the maximum number of communication rounds. However, this nearly doubles the number of tokens processed by the remote model.
        \item \textbf{Parallel.} By increasing the number of parallel jobs at the edge, we can also boost quality. We show that this increased parallelism is free at the edge due to batching and the IO-bound nature of LLM inference.  For example, on medical reasoning (\longhealth), by increasing the number of random samples, we can improve accuracy from \yell{$X\%$} to \yell{$Y\%$}. Critically, we show that this leads only to a \yell{$1.0z\times$} in edge latency, due to batching.
    \end{itemize}
  \item In Section \ref{subsec:results-communication}, we perform a fine-grained error analysis, showing how different types of tasks are more or less amenable to the different strategies described above. For example, \yell{TODO}. We also highlight several types of tasks where divide-and-conquer systems remain ineffective. This motivates future work in joint remote-edge training. 
\end{itemize}


