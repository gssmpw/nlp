\section{Introduction}
\label{sec:intro}
Today's frontier Language Models (LMs) are proficient at challenging reasoning tasks, including programming across repositories and making decisions in financial, legal, and medical domains. Accessing the most capable models on the cloud remains expensive: processing a standard million-token code repository with OpenAI's o1 costs $>\$15$ per query. 

At the same time, edge devices contain a substantial amount of underutilized compute that could potentially assist with these workloads. Smaller LMs (\textit{e.g.} with 1-70B parameters) are rapidly improving and can now be run on personal computers (ollama, llama.cpp) and smartphones (OpenELM, PhoneLM) at low latency - \textit{e.g.} - an \textcolor{red}{XX} model can process \textcolor{red}{XX} tokens per second on a phone. Currently, small LMs are not participating in challenging reasoning tasks. 

Here, we ask to which extent can small LMs on device can collaborate with frontier models on the cloud. Such \emph{edge-remote systems} can reduce cloud costs and unlock new asynchronous workflows (e.g. bug finding) that run on idle compute resources. By offloading computation to cheaper but less capable LMs, we introduce a cost-accuracy tradeoff which we explore in this work.

The tradeoff is governed by three fundamental asymmetries between edge and remote LMs. 

\begin{itemize}
    \item \emph{Capabilities}: The well-known gap between small and large LMs is even larger on real-world tasks than benchmark results would suggest (see Section X). We expect the gap to persist despite rapid improvements across small and large models.
    
    \item \emph{Cost (in \$)}: Edge processing is free. Remote processing is \$.

    \item \emph{Runtime savings from parallelism}: In LLM inference, generating a batch of samples in parallel is often no more costly than generating a single sample. This is because when batch sizes are small, LLM inference is I/O bound. The inference server host controls batching and can thus save costs by batching requests across concurrent users, as typically done by model providers. Users can only obtain marginal savings by sending a batch of requests. In contrast, a locally running inference server, on the other hand, implies that users incur a latency cost when processing less than full batches of requests.
\end{itemize}

The assymetries above motivate the following \emph{divide-and-conquer} strategy. To reduce cloud costs, the raw input tokens will not be sent to the remote LM. The latter only interacts with a small edge LM with full access to the input. To offset the small LM's limited capabilities, the task is decomposed into ``bite-sized'' tasks and the input is decomposed into chunks. Benefiting from local parallelism, the edge model can efficiently execute subtasks on chunks of input. 

Most existing divide-and-conquer strategies rely on \emph{brittle, hand-crafted workflows} (CITE MAP REDUCE LM, DOCETL and others). Task decomposition is nontrivial and domain-specific. While previous work typically uses only large LMs, it is unclear which task decompositions are suitable for two assymetric LMs. We started by hand-crafting systems for finance, medical, and academic research domains. We found that significant tuning is required to match simple baselines such as directly generating with the remote model (with full access to context) or retrieval augmented generation.

We then proceeded to ask whether remote LLMs can discover and generate appropriate task decomposition. This approach draws parallels to Domain-Specific Languages (DSLs), where software designers have traditionally struggled with decomposing tasks into manageable abstractions, and where LLMs are showing great promise. We investigate one prototype of a domain-agnostic edge-remote system, which we call \textbf{MINION} with two core principles:
\begin{itemize}
    \item The remote model communicates with the edge model via code generation, motivated by the observation that some subtasks  (e.g. chunking long context) can be performed more reliably and efficiently with code (cite evaporate, chain of code).
    \item \emph{Test-time scaling} is leveraged in two ways:
    \begin{itemize}
        \item \emph{Sampling}. The edge cost model encourages batching, which we use for test-time compute strategies (e.g. sampling/majority vote), to bolster the reliability of the small edge LM. 
        \item \emph{Looping}. The remote and local model can have as many back-and-forth communication rounds as needed to confidently solve the task or abstain.
    \end{itemize}
\end{itemize}

In experiments across domains, we first characterize which design choices are needed to match hand-crafted workflows. 
With ablation studies, we then assess the implications of each of the above principles. We find that repeated sampling on edge, accompanied by step-by-step reasoning on remote, are necessary for success (made this up). While certain tasks require back-and-forth looping (\textit{e.g.} medical), others are not (finance).
We investigate cost-accuracy tradeoffs across model sizes and historical generations for both the remote and local models. We find that systems with as small as a 3B edge model (NAME) can match the remote model performance with XX fewer tokens. Given historical data, we project that performance can be improved by XX on the same hardware each year. 

To conclude, our contributions are as follows: 
\begin{itemize}
    \item We extensively investigate a new class of edge-remote collaborative systems, which we believe to be inevitable given trends in model and hardware development.
    \item We propose the desiderata for such systems (Sec XX), involving communication efficiency, “intelligence ratios” between remote and edge models, and scaling properties.
    \item We propose an autonomous a domain-general architecture coined MINION, which reasons in code to dispatch tasks to edge LMs, and compare it to hand-crafted pipelines
    \item We identify the core components of an edge-remote system and characterize how it behaves on different domains using different sizes and generations of LMs for both edge and remote
\end{itemize}
