

\vspace{-1em}
\section{Results}
\label{sec:results}
\input{figures/scaling-edge-model/scaling-edge-model}
%\input{figures/lm-bottleneck/local-lm-bottleneck}

% \input{figures/model-longitudinal/model-longitudinal}
% \input{appendix/experiments.tex}

Here, we analyze how the design of \system affects cost and quality. Our main takeaways are: 
\begin{itemize}
\item On average across three datasets, \system can recover $97.9\%$ of the performance of remote-only systems while spending $5.7\times$ less; 
\item We identify protocol hyper-parameters that let us flexibly trade-off cost and quality;
\item As local models grow stronger, \system becomes increasingly cost-effective.
  % \item \system would not have worked until mid-2024. With today's model, \system necessitates a local model that is $>3B$ parameters.
  % \item Communication efficiency is correlated with $\locallm$ capabilities: larger local models (\textit{i.e.} 8B parameters) are 1.53x more cost effective than their 1B parameter counter parts.
  % \item Scaling local workloads through aggressive parallelization improves performance, with chunking and sub-task generation being more cost effective than sampling.
  % \item Back-and-forth communication improves performance by up to \yell{XXX} points, but comes at a cost.
  % \item TODO
\end{itemize}

We structure our analysis around three core design choices:
%\vspace{-0.75em}
\begin{enumerate}
    \item \textbf{Model choice} \textit{How does the choice of local and remote model effect cost and quality?} We examine different model types and sizes for $\locallm$ and $\remotelm$ in \Cref{subsec:results-model}.\vspace{-0.5em}
    \item \textbf{Scaling parallel workloads on-device} \textit{How should we structure parallel workloads on the local device to maximize performance and minimize cost?} We highlight how scaling the local workloads can improve performance (\Cref{subsec:results-workloads}) and study the effects on cost.\vspace{-0.5em}
    \item \textbf{Sequential communication protocol} \textit{Can multiple rounds of communication improve quality? At what cost?} We explore this trade-off in \Cref{subsec:results-communication}.

\end{enumerate}

Our findings are detailed in Sections~\ref{subsec:results-model},~\ref{subsec:results-workloads}, and ~\ref{subsec:results-communication}. Finally, in \Cref{subsec:rag} we discuss the relationship between retrieval augemented generation and local-remote compute. 

\vspace{-0.5em}
\subsection{Experimental setup} 
\label{subsec:exp-setup}
\paragraph{Datasets and models} We evaluate \system on three benchmarks that are well suited for data-intensive reasoning: \finance, \longhealth, and \qasper. \finance tests financial document understanding with complex reasoning over reports. \longhealth focuses on tracking and interpreting longitudinal health records. \qasper assesses question answering over dense scientific papers. See Appendix~\ref{app:experiments-dataset} for details. We use two open-source model families (\llama, \qwen) as $\locallm$ and \gpt as $\remotelm$ (details in \Cref{app:experiments-models}).

\vspace{-0.5em}\subsection{Model choice}
\label{subsec:results-model}

This section explores the model requirements and generalization capabilities of \system, examining the local model sizes necessary for effective collaboration, the sensitivity of the communication protocol across different local-remote model pairings, and the longitudinal evolution of \system’ performance with advances in model capabilities over time.

\vspace{-0.5em}\paragraph{\textit{What size does $\locallm$ have to be in order to be effective in \system?}}
% \Cref{fig:scaling-edge-model} % x: parameters, y: accuracy, notes: vertical lines at different device types 

Our results demonstrate that \system starts being competitive with $\remotelm$-only baseline at the $3$B parameter model scale. When considering both the \qwen and \llama model families running locally, at $1$B scale, \system recovers 49.5\% of the \gpt-only baseline performance, 3B scale recovers 93.4\% and 8B recovers 97.9\% accuracy (see Table~\ref{table:main-tradeoff} for more details).

\vspace{-0.75em}\paragraph{\textit{How does the capacity of $\locallm$ affect the cost-accuracy tradeoff?}}
% x: parameters, y: accuracy, notes: vertical lines at different device types 
In our system, $\locallm$ implicitly acts as an information encoder, optimizing the Information Bottleneck objective~\citep{tishby2000information} by compressing input context while preserving predictive information (see Appendix~\ref{app:info_bottleneck}). To measure this, we analyze the tradeoff between remote ``prefill'' tokens (fewer tokens indicate greater compression) and accuracy (higher accuracy means better retention). Figure~\ref{fig:scaling-edge-model} shows that as $\locallm$ size increases, representations become more compressed and accurate, improving Information Bottleneck values. Larger $\locallm$ models trade local FLOPs for communication, with 7–8B models being 1.53× more token-efficient than 1B models. Additionally, the \qwen family follows a different tradeoff than \llama, yielding more compressed representations. This suggests that as small LMs improve, local-remote systems will become increasingly cost-efficient.


% We find that communication efficiency is correlated with size the of the $\locallm$ (see Figure~\ref{fig:lm-bottleneck}). Interestingly, averaged across datasets and model families, we find that the 7-8B parameter models are 1.53X more token efficient than their 1B parameter counter parts. This suggests an interesting trend: as the capabilities of local models increases, the cost of \system will further decrease.

\vspace{-0.75em}\paragraph{\textit{Is \system sensitive to different local/remote pairs?}} 
We ask whether the communication protocol in \system is invariant to changing the model types (\textit{i.e.} \llama vs \qwen locally and \llama vs \gpt remotely). Our results indicate that \system performs similarly with different local-remote LM combinations (see the Table~\ref{table:main-tradeoff}): varying the $\locallm$ from \qwen to \llamathreetwo, results in performances within $\pm$ .05 performance points (see Table~\ref{table:main-tradeoff}). Furthermore, we find that holding the $\locallm$ fixed as \llamathreetwo-3B and varying $\remotelm$ from \gpt to \llama-3.3-70B leads to similar overall performances within $\pm$ 0.07 points (see Table~\ref{tab:remote-model-variations} in Appendix).

\input{figures/scaling-samples/scaling-samples}


\vspace{-0.75em}\paragraph{\textit{How have local / remote model capabilities changed over time, and what effects do they have on \system?}}
% would \system been possible in 2023?
% \cref{fig:model-longitudinal} x: date, y: accuracy, notes: staircase for baseline
In Table~\ref{tab:system-snapshot}, we provide a retrospective analysis demonstrating how the quality of \system would have changed with model releases over time. 
From 2023 to 2025, the average performance of \system with the best models available has improved from 0.26 to 0.66 (see Table~\ref{tab:system-snapshot} in Appendix). Interestingly, it was only in July 2024 --- with the release of \textsc{gpt4-turbo} and \llamathreeone-8B --- that \system could have come within 12\% of the best frontier model performance at the time (see Table~\ref{tab:system-snapshot} in Appendix). 



\vspace{-0.5em}\subsection{Scaling parallel workloads on-device}
\label{subsec:results-workloads}

% Scaling parallel local workloads
% three charts: (1) x-axis: tokens, y-axis: acuracy, labels=# of tasks per round (2) x-axis: tokens, y-axis: acuracy, labels=# of samples per task, (3)   x-axis: tokens, y-axis: acuracy, labels=chunks size

In \system, there are three levers for maximizing local compute resources through parallelized, batched processing: (1) number of tasks per round, (2) number of samples taken per task, and (3) number of chunks. We ablate each, showing their impact on performance. We find that (1) and (3) are more cost effective ways of increasing performance.

% \paragraph{\textit{At what point does edge compute become I/O bound?}}

\vspace{-0.75em}\paragraph{\textit{How does the number of tasks per round affect performance?}} Increasing tasks per round proxies task decomposition, with more sub-tasks enhancing decomposition. Raising tasks from 1 to 16 boosts performance by up to 14 points but doubles $\remotelm$ prefill costs. Optimal task count varies by query and model, but exceeding 16 reduces performance.


\vspace{-0.75em}\paragraph{\textit{How does scaling local samples affect performance?}} We explore whether increased sampling at an individual \{task, context\} level improves performance. Increased sampling enables us to better utilize the available compute resources while improving task-level accuracy~\citep{brown2024large}. Our results indicate that increasing the number samples from $1$ to $32$ can improve performance on average $7.4$ points, but comes at the cost of $5\times$ the $\remotelm$ prefill costs. This being said, increasing sampling beyond $16$ starts hurting task performance as the noise across samples is too large for the remote model to effectively distill the correct answer~\citep{kuratov2024babilong}.

\vspace{-0.75em}\paragraph{\textit{What effect does chunk size have on downstream performance?}} We test whether increasing local utilization by using more chunks per task improves performance. Our results indicate that increasing \# of chunks per task (by decreasing the number of ``pages'' per chunk from $100$ to $5$) leads to an $11.7$ point accuracy lift. However, this lift comes with a $2.41\times$ increase in $\remotelm$ prefill costs. 



\vspace{-0.5em}\subsection{Scaling sequential communication}
\label{subsec:results-communication}

\input{figures/scaling-rounds/scaling-rounds}


% Figure~\ref{fig:scaling-rounds} x: tokens, y: accuracy, hue: different strategies 
Both the \naive and \system communication protocols feature \textit{sequential} communication: they allow for multiple rounds of exchange between the local and remote models. 



\vspace{-0.75em}\paragraph{\textit{Does performance improve as we increase the maximum number of rounds? At what cost?}} We vary the maximum communication rounds and find it is correlated with accuracy and cost (see \cref{fig:scaling-edge-model}). By simply increasing the maximum number of rounds in \naive from $1$ to $5$, we enable a $8.5$-point lift in average accuracy across the three tasks (with \llamathreetwo on-device).
However, this accuracy improvement comes at a cost: each additional round of communication increases the cost by \$$0.006$ per query while boosting accuracy by $4.2$ points.


% longer conversations also cost more: on \finance, each additional round of communication increases cost by \$$0.006$ per query and accuracy by $4.2$ accuracy points.



\vspace{-1em}\paragraph{\textit{How should we maintain context between \system rounds?}}
We experiment with two sequential protocol strategies: (1) simple retries and (2) scratchpad. 
% 
See Section~\ref{sec:methods} for details of these strategies.
% 
As shown in \Cref{fig:round-strategy}, both strategies show consistent increases in both accuracy and cost when increasing the maximum number of rounds, with the scratchpad strategy achieving a slightly better cost-accuracy tradeoff. 
% 
Notably, each additional round of communication with the scratchpad strategy leads to a larger improvement in accuracy ($6.1$ accuracy points) which are mostly offset by larger increases in cost ($8.6$ dollars). 


% Our experiments demonstrate for a given edge-remote pair, increasing the number of rounds of communication (\textit{i.e.} round trips between edge to remote model) improves accuracy by up to \yell{XX} points. However, this improved accuracy comes at the cost of greater utilization of remote resources (see Figure~\ref{fig:scaling-rounds}). In Section~\ref{sec:analysis}, we present a detailed slice analysis that identifies which problem types benefit most from sequential processing. 

% \paragraph{What kinds of tasks benefit from a sequential communication protocol?}
% \yell{Potentially make this part of the slice analysis.} 

\vspace{-0.5em}\subsection{Retrieval-Augmented Generation in the Context of Local-Remote Compute}
\label{subsec:rag}


We further investigate the interplay between local-remote compute paradigms (\textit{e.g.}, \system) and retrieval-augmented generation (RAG), analyzing their complementary strengths and trade-offs in data-intensive reasoning tasks. Through empirical evaluations, we examine these methods on financial document extraction (\finance) and long-document summarization (\textsc{Booookscore}). \textit{See Appendix ~\ref{app:rag} for a more detailed treatment.}

\subsubsection{Comparison of \system and RAG on \finance}

RAG performs well for financial document analysis, where relevant information is found in specific sections. In \Cref{fig:rag-comparison} (left), we compare \system, \naive, and RAG using BM25 and OpenAI's \texttt{text-embedding-3-small} embeddings~\citep{article,neelakantan2022text}. A chunk size of 1000 characters balances retrieval accuracy and efficiency (\Cref{fig:rag-comparison} (center)).

Adjusting the number of retrieved chunks allows RAG to optimize quality and cost. When BM25-based RAG retrieves 50+ chunks, it surpasses the performance of a fully context-fed remote model. However, RAG does not match the cost-effectiveness of \naive. When compared to \system, RAG using OpenAI embeddings achieves similar cost-quality trade-offs but may overlook nuanced financial signals across sections.

\subsubsection{Comparison of \system and RAG on Summarization Tasks}

Unlike \finance, RAG struggles with summarization due to its reliance on retrieval. Unlike the financial parsing tasks, summarization requires reasoning over information \textit{dispersed} across the document.  We evaluate \system, RAG (with BM25 andEmbedding retrievals), and a \gpt-only baseline on \textsc{BooookScore}~\citep{chang2023booookscore}, a long novel summarization dataset.

\paragraph{Evaluation}
\begin{itemize}
\item \textbf{Qualitative Analysis:} As shown in App. Table~\ref{tab:story-summaries}, \system generates summaries with richer entity mentions and more coherent narratives than RAG. It is also $9.3\times$ more token-efficient than \gpt-only (11,500 vs. 108,185 tokens).
\item \textbf{Quantitative Analysis:} Using \textsc{claude-3.5-sonnet} for evaluation, \system achieves near-parity with \gpt-only, while RAG-based methods underperform (Table~\ref{tab:summary_qualitative_scores} in Appendix).
\end{itemize}

These results highlight RAG's effectiveness in structured tasks like \finance but its shortcomings in long-form synthesis which requires reasoning over information dispersed across the document. 
%\input{figures/scaling-rounds/scaling-rounds}


% RAG \cite{lewis2020retrieval} reduces ``prefill'' tokens for the remote model by (1) chunking the document, (2) retrieving relevant chunks, and  (3) passing them in-context to \remotelm. In \cref{app:rag:finance}, we compare RAG to \system and \naive (all with \gpt as the their \remotelm) on the \finance dataset, whose problems rely on extracting financial information from 10K sections. We show that after a optimizing its hyperparameters, RAG can make similar cost-accuracy tradeoffs compared to \system, whereas \naive finds more cost-effective points compared to both methods under a similar budget of tokens sent remotely. However, some tasks cannot be reduced to retrieval. One such task is summarization which requires integrating information across chunks of the document. When comparing RAG to \system on the \textsc{BooookScore} dataset (\cref{app:rag:summary}, we find that RAG results in lower-quality summaries that comprise of collections of facts, whereas \system provides equivalent albeit more verbose summaries compared to \gpt-only.

% \section{Analysis}
% \label{sec:analysis}








