\vspace{-0.75em}
\section{\naive: A naïve communication protocol}
\label{sec:naive}
\input{figures/micros/micros}

In this section, we describe \naive, a baseline local-remote communication protocol, which implements a simple free-form conversation between $\locallm$ and $\remotelm$.
% inspired by the recent success of the multi-agent paradigm \citep{wang2024mixture, guo2024large}. 
% We ask whether we can reduce remote prefill tokens, and thus cost, by simply orchestrating a free-form conversation between the $\locallm$ and the $\remotelm$.
% For this to work, the $\remotelm$ needs to know how to solve the problem, and the $\locallm$  needs to be able to answer straightforward queries against a long context. Each model separately maintains the history of its \emph{own} inputs and outputs. 

It begins with system prompts for both models informing them of the query $\bq$ and that they will be collaborating with another model to answer it (see (3) in Figure~\ref{fig:banner}). 
Crucially, the system prompt for the $\locallm$ includes the full context $\bc$ while the system prompt for the $\remotelm$ does not.
After the system prompts, the two models chat back and forth with one another until the $\remotelm$ provides a final answer to the query. 
% 
%In \Cref{fig:banner} we provide an example of a simple local-remote conversation. 
% 
\textit{See \Cref{app:methods-naive} for a detailed description of the \naive protocol.}



% \begin{itemize}
%     \item \textbf{Fundamental Capabilities} (Remote -- Edge Communication). We find that small language models struggle with: processing long contexts (even when advertised as long-context), following instructions, and performing long-form reasoning. Small models are also uncalibrated -- often don't know when they don't know. %We demonstrate these limitations in \Cref{sec:results-micros-edge}.
%     \item \textbf{Communication protocols} (Edge -- Remote Communication). Many tasks require a high degree of synthesis and aggregation (\textit{e.g.} summarization), but feeding full context into large models is costly and edge models struggle with complicated synthesis. It is challenging to perform synthesis while keeping costs low: we need to be selective with what is passed up to the remote model. More over, many tasks require multi-step, sequential reasoning (\textit{i.e.} involves subtasks which are dependent on each other). As a result, it is simply not sufficient to reason in a single pass. 
%     % We find that it's simply not sufficient to chunk, execute edge work across \texit{many workers} and reason over all those extracted pieces of work. 
%     % Edge models have (1) context length constraints (and in some instances cannot reason over all the  work outputs), (2) noise ceilings (can't reason over large amounts of noisy inputs) and (3) struggle with needle-in-the-haystack problems when large amounts of context are unused. 
%     % As a result,  We demonstrate these limitations in \Cref{sec:results-micros-remote}.
%     \item \textbf{Efficiency} Optimizing edge compute requires intentful workload management.

% \end{itemize}


% \section{A naïve communication protocol }
% \label{sec:naive}
% We begin by providing a brief description of \textit{one} instantiation of a naïve communication protocol that can govern an edge-remote system (Section~\ref{sec:naive-description}). We then present an empirical analysis of the protocol, exposing two key limitations: (1) a gap in underlying capabilities and (2) under-utilization of on-device resources.

% \subsection{Description}
% \label{sec:naive-description}
% The naïve communication protocol has three components: task inputs,  models and a communication channel. 
% \begin{itemize}
%     \item \textbf{Task Inputs}: Composed of task context (i.e.., financial documents) and a task description (\textit{i.e.} ``retrieve the total revenue across Q1 and Q3''). Task inputs are housed on the edge device.
%     \item \textbf{Models}: Protocol supports two LMs, a small ($<10B$ parameter) model on edge and a large frontier model (\textit{i.e.} \gpt) in the cloud. 
%     \item \textbf{Communication protocol}: The LMs communicate via natural language sending packets of work outputs, with accompanying instructions or descriptions (\textit{i.e.} in an edge to remote communication, the edge model would pass the following pack up - (outputs of text summary, ``Here is a summary of the context. Let me know if there is anything else you need'').
% % \end{itemize}
% \vspace{-0.5em}
% \subsection{Analysis}
% \label{sec:naive-analysis}

We compare \naive to a baseline where $\remotelm$ is given the full context and the query. Excitingly, \naive reduces $\remotelm$ costs by $38.13\times$, $31.3\times$, and $20.9\times$ on \finance, \longhealth and \qasper, respectively (see ~\Cref{subsec:exp-setup} for dataset details).
Averaged across these datasets, it closes $87.0\%$ of the quality gap between $\remotelm$ and $\locallm$ operating alone. 

% Despite its simplicity, the free-form sequential chat in \naive has two critical drawbacks: 
To further close the gap, we analyze \naive conversations and find that in unconstrained chat, $\remotelm$ often gives $\locallm$ \emph{complicated instructions} over \emph{long contexts}. \Cref{app:naive-analysis} presents micro-experiments illustrating $\locallm$'s struggles with these instructions:
\begin{enumerate}
    \item \textbf{$\locallm$ struggles to handle multi-step instructions.} Using \gpt, we generate instructions with varying numbers of sub-parts. We then show splitting sub-parts into separate requests leads to a 56 point performance improvement (see Figure~\ref{fig:small-micros}).
    
    \item \textbf{$\locallm$ struggles to reason across long contexts}. We show how increasing context length from $<1K$ to $>65K$ tokens can decrease performance by 13\% on a simple extraction instruction (see Figure~\ref{fig:small-micros}).
\end{enumerate}


Put simply, these smaller LMs are currently better equipped to answer simple queries on shorter contexts.

% SE (01/30) love this sentence, but per mayee's comment, moving to the discussion.  
% At the same time, Section~\ref{subsec:results-model} highlights the rapid improvements in edge model capabilities better, and thus it is not unlikely that a simple strategy like \naive can be even more competitive in the coming months and years.

% We also note how \naive fails to fully utilize edge hardware because the $\locallm$ processes 1 request per turn. 
% As we discuss in \Cref{app:edgecost}, when decoding with a batch sizeof 1, the model is I/O bound so we can increase the number 

% As we discuss \Cref{app:edgecost}, 
% (1) lots of compute idle and (2) opportunities to maximizing $\locallm$ performance via techniques such as sampling. 
% 
% Optimizing edge compute requires intentful workload management --- namely, run many independent tasks in parallel. 

% \label{sec:naive-quality} 

% Across our evaluations, \naive underperforms the \gpt-only baseline by \yell{9.7} accuracy points. In an unconstrained chat, the remote model occasionally tasks the small model to answer \emph{complicated queries} against \emph{long contexts}. Indeed, we established that (1) small LMs struggle to reason across long contexts (see analysis in Appendix~\ref{app:naive-analysis} which shows how increasing context length from $<1K$ to $>65K$ tokens can decrease performance by 13\% on a simple extraction task) and (2) small LMs struggle to handle complex, multi-step queries (see analysis in Appendix~\ref{app:naive-analysis} that shows how halving sub-tasks in a query leads to 56 point performance improvement). Put simply, these models are currently better equipped to answer simple queries on shorter contexts. At the same time, Section~\ref{subsec:results-model} highlights the rapid improvements in edge model capabilities better, and thus it is not unlikely that a simple strategy like \naive can be even more competitive in the coming months and years.

% \textbf{Idea: Simplify the tasks and manage context to match the edge model's capabilities.}

% \subsubsection{Limitation: Low local resource utilization}

% In \naive, the $\locallm$ answers a single query per turn, leaving (1) lots of compute idle (see Analysis in Appendix~\ref{}) and (2) opportunities to maximizing $\locallm$ performance via techniques such as sampling. Optimizing edge compute requires intentful workload management --- namely, run many independent tasks in parallel.

% \textbf{Idea: Decompose tasks into independent subtasks that can be executed in parallel. }
% \label{sec:naive-util}
% \yell{Micros}

% It requires the small models to punch above their (current) weight by answering arbitrary questions on long contexts, and it leaves the local resources underutilized

\vspace{-1em}\section{\system: A decomposition-based communication protocol}
\label{sec:methods}

% \begin{figure}[ht]
% \label{fig:system}
%   \centering
%   \includegraphics[width=\linewidth]{figures/minions_edge.png}
%   \caption{The master and the minions --- V1 sketch}
%   \label{fig:longhealth}
% \end{figure}

Motivated by these observations, we introduce \system, a simple extension of the naïve communication protocol discussed in \cref{sec:naive}.
\system uses a divide-and-conquer strategy where the $\remotelm$ decomposes the task into \textit{simpler} jobs that can be run \textit{in parallel} (see (4) in Figure~\ref{fig:banner}). 
% 
Throughout this section we will continue with the example task introduced in \Cref{sec:prelim-setup}.

% \begin{examplebox}
%     \small
%     \textbf{Query ($\bq$):} \textit{
%     Compute the 2015 depreciation and amortization margin for AMD (in percentage). 
% }
% \end{examplebox}

% In \system, there are three levers for maximizing local compute resources through parallelized, batched processing: (1) number of jobs per round, (2) number of samples taken per job, and (3) number of chunks per job. We ablate each of these levers, demonstrating the effects of each on downstream performance. We find that (1) and (3) are more cost effective (as measured by increase in \remote\xspace prefill tokens) ways of increasing performance. 
\vspace{-0.5em}
\subsection{Protocol description}
\system protocol is a loop over three steps:
\begin{enumerate}
    \vspace{-0.5em}\item \textit{Job preparation on remote.}  $\remotelm$ writes code that generates a list of job specifications for $\locallm$ (see 4(a) in Figure~\ref{fig:banner}).
    \vspace{-0.5em}\item \textit{Job execution and filtering locally.} The job specifications are executed locally with the $\locallm$, and outputs are filtered (see 4(b) in Figure~\ref{fig:banner}).
    \vspace{-0.5em}\item \textit{Job aggregation on remote.} The remote model receives the filtered outputs and decides whether to output an answer or begin another iteration (see 4(c) in Figure~\ref{fig:banner}).
\end{enumerate}
% The framework is illustrated in \Cref{}

\paragraph{Step 1: Job preparation on remote.} 
In this step, the $\remotelm$ generates a list of jobs that the $\locallm$ will run in parallel. A \textit{job} is a specification of a subtask, which can be converted into a prompt and sent to the local model. More precisely, a job, $\bt$, is a context-instruction pair $\bt^{(i)} = (\tilde{\bq}^{(i)}, \tilde{\bc}^{(i)})$. We denote a list of jobs with $\bT = [ \bt^{(1)}, \bt^{(2)}, ... ]$

\begin{examplebox}
    \small \textbf{Instruction ($\tilde{\bq}^{(i)}$):} Extract the total revenue for FY2015, abstain if not present. Try to look for the income statement and make sure it is from 2015. \\
    \textbf{Context ($\tilde{\bc}^{(i)}$}): ``Operating income for North America for the years ended..." 
\end{examplebox}


Crucially, the context $\tilde{\bc}^{(i)}$ for a job need not include the entire context $\bc$ of the full task. 
In principle, this allows us to chunk the context into more manageable pieces, which can be executed in parallel. 
\textit{But how can the $\remotelm$ chunk the context without reading it?}

To avoid reading the entire context, we have the remote model generate a Python function, $\textbf{f}(\bc, \bT)$, that accepts the full task context $\bc$ and jobs from the last iteration $\hat{\bT}$ and outputs a new list of jobs $\bT$. Specifically, we prompt $\remotelm$ with the task query $\bq$ and instruction prompt $\bp_{\text{decompose}}$:
    $\textbf{f}(\cdot, \cdot) \sim \remotelm(\bq, \bp_{\text{decompose}})$. 
    % \label{eq:decomp-function}
Then, on-device, the function is executed with the context $\bc$ as the argument producing a list of jobs $\bT = \textbf{f}(\bc, \hat{\bT})$.

This strategy, which builds on work using LMs to generate code for information extraction~\citep{arora2023evaporate,li2023chain}, allows us to decouple the number of unique jobs from the number tokens generated by the cloud model.
For example, the code below, which is an abbreviated version of a function that was generated by the cloud model, is less than fifteen lines but can generate hundreds of jobs. 
\begin{exampleboxcode}
\begin{lstlisting}[language=Python]
@dataclass
class Job:
    instruction: str
    chunk: str
    
def f(ctx: str, last_jobs: List[Job]) -> List[Job]:
    jobs = []
    instructions = ["Extract the total revenue for...", "In the statement of cash flow..."]
    chunks = chunk_on_pages(ctx) # chunk context into pages
    for chunk in chunks:
        for instr in instructions:
            for _ in range(5):
                jobs.append(Job(instr, chunk))
    return jobs
\end{lstlisting}
\end{exampleboxcode}

Additionally, by passing the previous iteration's jobs and responses $\bT$ (\texttt{last\_jobs} in the code snippet), the large model can create jobs which build on previous responses. For example, the cloud model in the second round might zoom in on a relevant chunk identified in the first round.
For more examples of generated functions or details on the exact prompt used to generate the code, see Appendix~\ref{app:methods-prompts}. 
% If $\textbf{f}$ fails to run or does not satisfy our type-checking requirements, we take another sample from the remote model \eqref{eq:decomp-function} (incurring only decode costs) for a maximum of 10 attempts. \yell{TODO: something about how many attempts are needed on average}.
    
\vspace{-0.5em}\paragraph{Step 2: Job execution and filtering on-device.} 
In this step, we convert the jobs $\bT = [ \bt^{(1)}, \bt^{(2)}, ... ]$ into prompts and execute them in parallel locally. 

The jobs are fed in batch(es) to the $\locallm$ together with a system prompt $\bp_{\text{worker}}$ that instructs the model to either abstain or return a JSON object $\bz^{(i)}$ with fields \texttt{explanation}, \texttt{citation}, and \texttt{answer} to help us verify its reasoning. 
\begin{equation}
    \bz^{(i)} \sim \locallm(\bt^{(i)},\bp_{\text{worker}})
\end{equation}


After the $\locallm$ has generated the results, we discard any $\bz^{(i)}$ for which the model abstained. Intuitively, many instructions will be irrelevant to their paired chunks, allowing the $\locallm$ to abstain and avoid sending unnecessary information to the $\remotelm$.
The surviving subtask-chunk pairs are aggregated to form the formatted string $\bw$. 
% Often, this string will include conflicting answers for some subtasks.

 

\textbf{Step 3: Job aggregation on remote.}  $\remotelm$ receives $\bw$ and a synthesis prompt $\bp_{\text{synthesize}}$, instructing it to generate a JSON object $\ba$ with a ``decision'' field for sufficiency and a ``response'' field for a (potential) final answer:
\begin{equation}
    \hat{\by} \sim \remotelm(\bw, \bp_{\text{synthesize}})
\end{equation}
If the $\remotelm$ decides that more information is needed, the loop continues from Step 1. 

There are several ways to maintain context across \system rounds. 
One simple approach is to keep the entire the conversation in context. 
However, this strategy incurs significant additional cost, even with prompt caching. 
We experiment with two alternatives: (1) \textit{simple retries}, in which only the $\remotelm$'s advice is carried over between rounds and (2) \textit{scratchpads}, in which the $\remotelm$ can record what it learned from the round before proceeding to the next. 

\input{tables/main-tradeoff.tex}

% with or without access to the previous $\locallm$ results $\bw$. 

% \yell{TODO: here we need to say something about the effects of previous results.}
\vspace{-0.5em}
\subsection{Protocol hyper-parameters}
\system has three hyper-parameters: choice of $\remotelm$ and $\locallm$ (model choice), job preparation strategy (scale of parallel workloads on-device), and  looping strategy (sequential communication protocol).

\textbf{Model choice}. Different model sizes (\textit{e.g.} 3B vs. 8B), families (\textit{e.g.} \qwen vs. \llama), and generations (\textit{e.g.} 3.1 vs. 3.2) can be used for both the $\locallm$ and the $\remotelm$. 

\textbf{Scale of parallel workload on-device}. \system has three knobs for increasing the degree of task decomposition and thus, workload parallelization: (1) number of tasks per round (\textit{i.e.} ``Extract the ARR for Q1 of 2014''), (2) number of samples per task (\textit{i.e.} number of generations created with $\locallm$, $\geq 1$), and (3) chunk size (\textit{i.e.} chunk by page, chunk by paragraph, etc; smaller chunks will send more information to cloud). These parameters are configured by $\remotelm$.

\textbf{Sequential communication protocol}. 
In practice, it is important to cap the number of times \system can loop. 
After the maximum number of rounds, the synthesis prompt is modified to force the model to produce a final answer. The choice of this maximum affects accuracy and cost. The strategy for maintaining context between rounds (simple retries vs. scratchpads) is another important hyperparameter.

\textit{We analyze these hyperparameters in \Cref{sec:results}.}


