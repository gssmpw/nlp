% \section{Preliminaries}
% Let the dataset be $\dset = \{(\bc_i, \bq_i, \by_i)\}_{i=1}^N$, where $\bc_i$ is the context, $\bq_i$ is the query, and $\by_i$ is the correct answer, each is a sequence of tokens. We consider a case where the context is much longer than the query and answer, \textit{i.e.} $|\bc_i| \gg |\bq_i|, |\by_i|$. Further we assume that $\bc_i, \bq_i$ are either on edge device or can be trivially transmitted to it.

% For any language model $\text{LM}$, it is standard to pass in the query and context as inputs, and take one sample $\hat{y}_i \sim \text{LM}(\bq_i, \bc_i)$. The answer is then scored by a metric $s(\hat{y}_i, \by_i)$ that can range from exact string matching, via ROUGE (CITE), to LLM-as-judge (CITE). \yell{TODO: too basic?}

% \paragraph{LLM inference cost model} 

% The cost of running LM inference includes two components: \emph{prefill} and \emph{decode}. Prefill involves passing the $\bq_i$ and $\bc_i$ to the model, which is done in parallel in one ``forward pass''. Decode involves sampling one new token at a time to generate $\hat{y}_i$. For a dense transformer with $X$ billion parameters and sequence length $L$, prefill requires $\bigo{XL^2}$ FLOPs due to the quadratic attention computation over the full sequence. In contrast, decode requires $\bigo{XL}$ FLOPs per token since each new token only needs to attend to the previous tokens once. For long contexts where $L$ is large, the prefill cost dominates the total computational cost. \yell{TODO: check this, I'm not sure about the FLOPs}

% For challenging tasks, it is common to use only a remote model, \textit{i.e.} $\hat{y}_i \sim \remote(\bq_i, \bc_i)$. This is leads to best results, but is expensive: the cost is X\$ and Y\$ per token for a frontier model respectively (as of Jan 2025). At the same time, the alternative of only using the edge model, \textit{i.e.} $\hat{y}_i \sim \local(\bq_i, \bc_i)$, is not competitive. \yell{TODO: say something about performance and about scaling inference time compute, that it fails.} 

\vspace{-0.75em}
\section{Preliminaries}
\label{sec:prelim}
\label{sec:prelim-setup}
We study the tradeoff between the \textit{quality} of a local-remote system and the \textit{cost} of running it. We first outline the problem setup and then provide details on how we measure accuracy and cost.


\vspace{-0.5em}\paragraph{Problem setup} We study language tasks that involve a context $\bc$ (\textit{e.g.} a long document), a query $\bq$ against that context, and a ground-truth answer $\by$ (see (1) in ~\Cref{fig:banner}).
\newtcolorbox{examplebox}{colback=blue!5!white, colframe=blue!75!black}

\begin{examplebox}
\small
\textbf{Context ($\bc$):} \texttt{The Fiscal Year 2015 10-K report for Advanced Micro Devices, Inc.}

\textbf{Query ($\bq$):} \textit{
    Compute the 2015 depreciation and amortization margin for AMD (in percentage). 
}

\textbf{Answer ($\by$):} \texttt{US\$394,328 million}
\end{examplebox}

A \textit{local-remote} system $\mathrm{S}$ (see (2) in Figure~\ref{fig:banner}), consists of two language models that must collaborate to solve the task---a small LM ($\locallm$) running on on-device, and a large frontier LM ($\remotelm$) running in the cloud. 
$\mathrm{S}$ ingests a context and a query, and applies both models in conjunction to output a predicted answer: $\hat{\by} \sim \mathrm{S}(\bc, \bq)$. 

\vspace{-0.5em}\paragraph{Measuring quality} We evaluate the performance of $\mathrm{S}$ on a dataset $\mathcal{D} = \{(\bc_i, \bq_i, \by_i)\}_{i=1}^N$, via a scoring metric $s(\hat{\by_i}, \by_i)$. Here, $s(\cdot, \cdot)$ is binary (correct/incorrect) and we report \textit{accuracy}. As baselines, we compare $\mathrm{S}$ to $ \hat{\by}_{\text{remote}}\sim \remotelm(\bc, \bq)$ and $\hat{\by}_{\text{local}} \sim \locallm(\bc, \bq)$.

\vspace{-0.5em}\paragraph{Measuring cost}

Monetary cost (in \$USD) is our primary cost metric. We assume that $\remotelm$ calls incur a cost while $\locallm$ calls are free, ignoring the fixed cost of the hardware and marginal cost of energy consumption. 

More concretely, the cost of calls to $\remotelm$ are proportional to a weighted sum of the number of prefill (\textit{i.e.} input) tokens and decode (\textit{i.e.} output) tokens:
\begin{equation*}
    C_{\text{remote}}(n_{\text{prefill}}, n_{\text{decode}}) \propto n_{\text{prefill}} + \alpha \cdot n_{\text{decode}},
\end{equation*}
where $\alpha$ varies by provider ($\approx$1-- 5)~\citep{dubey2024llama3,anthropic2024claude}.
Decode tokens are more expensive since the decoding stage is IO-bound~\citep{leviathan2023fast} and has lower GPU utilization. This is because generating each decode token requires loading the full model and KV cache into GPU SRAM.
% Concretely our assumptions are that:
% \begin{itemize}
%     \item The cost of calls to $\remotelm$ are proportional to a weighted sum of the number of prefill (\textit{i.e.} input) tokens and decode (\textit{i.e.} output) tokens. 
%     \begin{align*}
%         C_{\text{remote}}(n_{\text{prefill}}, n_{\text{decode}}) \propto n_{\text{prefill}} + \alpha \cdot n_{\text{decode}}
%     \end{align*}
%     The exact value of $\alpha$ depends on the provider, varying from 1 to 5~\cite{dubey2024llama3,anthropic2024claude}.     
%     The discrepancy between prefill and decode costs is due to lower GPU utilization during decoding, which is generally an IO-bound operation~\cite{leviathan2023fast}---generating each decode token requires loading the full model and KV cache into GPU SRAM. 
    
%     \item The calls to $\locallm$ are \textbf{free} subject to constraints in model size ($< 8$ billion parameters). In this work, we ignore the fixed cost of the hardware and marginal cost of energy consumption.
% \end{itemize}

In this work we do not focus on optimizing the latency of local-remote systems.
However, in \cref{app:edgecost} we show analytically that there are important regimes where the systems proposed (\naive, \system) incur at most a $5\times$ increase in latency relative to performing the entire operation remotely.
This is possible because these systems avoid the costly step of processing the entire document with the huge $\remotelm$, and because they can make efficient use of the local hardware by batching (\textit{e.g.} in \system).
We leave a detailed empirical study of the latency trade-offs of these local-remote systems for future work.



% However, one potential issue with this cost model is that it would not penalize an unrealistic system that exploits the free $\locallm$. If a system issued a huge number of requests to the $\locallm$, it may lead to high latency and render the system unusable.
% To address this, in \cref{app:edgecost}, we provide a formal cost model of local latency and provide an analysis of the systems studied in this work. 
% Importantly, we show that it is possible to  carefully \textit{batch} the requests to the $\locallm$ in \system and thereby incur only \yell{$X\times$} increases in latency.




% \textit{Why do we count tokens for the remote model and latency for the local model?}
% Below, we argue that the cost model above is reflective of real-world costs for users and providers alike. To do so, we first require some background on language model inference. 

% Language model inference consists of a sequence of forward passes through a model, one for prefill (\textit{i.e.} input) followed by one for each additional token generated (\textit{i.e.} output). 
% At low batch sizes, each forward pass after prefill is I/O bound, meaning the time it takes to load weights from memory exceeds the time it takes to actually compute the output. 
% As the batch size increases, the cost of the forward pass eventually exceeds the I/O cost. 
% Strikingly, for most models and hardware, this happens at a batch size $>100$~\cite{leviathan2023fast,chen2024sequoia}.
% A common assumption in the literature, which we also adopt, is that the cost of running a forward pass is a piecewise linear function $C_{\mathcal{M}, \mathcal{E}}(n) = \max(\lambda, \alpha \cdot n + \beta)$ of the batch size.

% In the cloud, the provider can batch generation requests from multiple users to keep hardware utilization high. Therefore, the cost of running a user workload scales roughly linearly with the number of input and output tokens. 
% The difference in cost between prefill and decode tokens ($\alpha$ is between 1 and 5~\cite{dubey2024llama3,anthropic2024claude}) is due to additional I/O costs related to the KV-cache. But 



% \paragraph{Local utilization}
% Local compute is optimized for batch processing, meaning it excels at handling multiple "prefill" token requests in parallel. Specifically, the latency curve for local resources exhibits a hockey stick shape: increasing the batch size does not impact latency up to a certain point (see \yell{Figure XX in the analysis}). As a result, processing a single sequence per $\locallm$ call incurs the same latency but fails to maximize throughput, leaving potential performance gains untapped. A central objective of system $\mathrm{S}$ is to optimize throughput through aggressive parallel processing.



 % Prefill involves passing the $\bq_i$ and $\bc_i$ to the model, which is done in parallel in one ``forward pass''. Decode involves sampling one new token at a time to generate $\hat{y}_i$. For a dense transformer with $X$ billion parameters and sequence length $L$, prefill requires $\bigo{XL^2}$ FLOPs due to the quadratic attention computation over the full sequence. In contrast, decode requires $\bigo{XL}$ FLOPs per token since each new token only needs to attend to the previous tokens once. For long contexts where $L$ is large, the prefill cost dominates the total computational cost. \


% \paragraph{Local utilization}
% Local compute is optimized for batch processing, meaning it excels at handling multiple "prefill" token requests in parallel. Specifically, the latency curve for local resources exhibits a hockey stick shape: increasing the batch size does not impact latency up to a certain point (see \yell{Figure XX in the analysis}). As a result, processing a single sequence per $\locallm$ call incurs the same latency but fails to maximize throughput, leaving potential performance gains untapped. A central objective of system $\mathrm{S}$ is to optimize throughput through aggressive parallel processing.

% \subsection{System Asymmetry}\label{subsec:system-asymmetry}
% We emphasize that there is a clear asymmetry between the $\locallm$ (a small, LM between 1-8B parameters) and the $\remotelm$ (a large, frontier model). We break down this asymmetry into three axes: capabilities, cost savings from high-throughput utilization, and \$\$/generated token. In terms of capabilities, the $\locallm$ trails the $\remotelm$. With respect to cost savings from high-throughput utilization, $\locallm$ see more cost savings from batch processing than $\remotelm$'s as an cost-savings incurred from performing batch processing in the cloud is hidden from the end user. Finally, in-terms of cost, $\locallm$'s are orders of magnitude cheaper(\yell{add citation}).

% As a result, if we do not optimally use the local compute resource, we 

% if we are not optimally using the resources via many \texit{parallelized} processes, we are underutilizing the resources.   

% The cost model for latency at the edge is different than cost in the cloud due to differences in  parallel processing abilities. 
% want to utilize it maximally
% if we don't uutilize --- latency same, throughput is under 
% we can utilize more, without paying latency cost and potentially get higher quality.

% hocket stick shape to latency
% as you increase batch size, pay little in terms of edge 

% \yell{TODO: add details}


% For best performance, it is standard to use only a frontier remote model $\textcolor{red}{\remote(\bc_i,\bq_i)}$, but it is expensive (\textit{e.g.} US\$X per prefill token and US\$Y per decode token, for a frontier model). 
% Relying purely on an edge model $\textcolor{blue}{\local(\bc_i,\bq_i)}$ typically yields poor performance. It struggles with large $L$, and it is not competitive even when allowed to take many samples \yell{TODO: numbers}. This trade-off motivates hybrid or adaptive strategies that can balance cost and performance.

% \begin{itemize}
%     \item The effects of parallelization on accuracy on edge
%     \item The effects of sampling on accuracy on edge -- to nail the point of (1) small models are not very capable on their own, but this is a lever we can pull
% \end{itemize}


% Higher-level description that can be deleted after we incorporate some ideas: 

% Small LLMs are unreliable. First, it is unclear which tasks they \emph{can} do. Second, for those tasks that are within reach, the model needs to be prompted very carefully so it solves them. Small LMs also struggle dealing with long context. The combination of the two means that a small LM needs to see a short context window and have a very specific single task. It also struggles to follow instructions and output formatted/structured outputs. It also is uncalibrated -- it does not know when it doesn't know.

% Aggregating fragmented information. Information can be sprinkled across the document 



% \textbf{Cost Model} The cost model includes the following components:
% \begin{itemize}
%     \item Remote model: prefill and decode tokens
%     \item Local model: prefill and decode tokens,  multiple calls and samples
%     \item Communication: seems negligble, sending text is fast
% \end{itemize}

% \subsection*{A Short Argument for a Two-Stage Supervisor--Worker Framework}

% \paragraph{1.\ Setup:}
% Consider a dataset $\{(x,y)\}$ where each $x$ may contain many tokens (e.g.\ long text) and $y$ contains the correct answer to a question. We have:
% \begin{itemize}
%     \item A small \emph{edge} model $h$, which is cheap to run but less expressive.
%     \item A big \emph{remote} model $g$, which is powerful but expensive (high prefill and decode cost).
% \end{itemize}
% Only $h$ has direct access to the full input $x$, while $g$ receives a condensed representation.

% \paragraph{2.\ Composition of Two Learned Mappings:}
% We learn a composite function $f(x) = g\bigl(h(x)\bigr)$. Formally:
% \[
%   x \xmapsto{h} z \xmapsto{g} \hat{y}.
% \]
% The edge model $h$ produces a compressed representation $z$ (e.g.\ summary, extracted info). The remote model $g$ then consumes $z$ to predict $\hat{y}$. By restricting $g$ to operate only on $z$ rather than on $x$, we avoid applying the large model to every token.

% \paragraph{3.\ Details of Chunking \& the Worker $h$:}
% Because $h$ is not very expressive, we often break $x$ into chunks $(x_1, x_2, \dots, x_n)$ and process each chunk \emph{separately}:
% \[
%    z_i = h(x_i), \quad i=1,\dots,n.
% \]
% An intuitive regression analogy is treating each chunk $x_i$ like an input dimension, with $h(x_1) + \cdots + h(x_n)$ being an ``additive'' approximation to $h(x)$. Although this may not be precisely true (the actual $h(x)$ could be more entangled), sending multiple $z_i$ to $g$ still enables a \emph{nonlinear} combination upstream. In practice, chunk-wise processing can be beneficial when $h$ has limited capacity; it handles each piece locally, and then $g$ learns to combine them. This avoids the (often prohibitive) cost of handling all $x_i$ jointly in the small model.

% \paragraph{4.\ Comparison to Retrieval-Augmented Generation:}
% Another approach is \emph{retrieval} where only a set of raw chunks $x_i$ (rather than their compressed versions) is passed to $g$. This can be economical if $g$ never sees unneeded text. However, using $h(x_i)$ to generate shorter, structured outputs can further reduce token overhead. Hence, $h$ distills each $x_i$ into fewer tokens and (hopefully) preserves the key signal for $g$.




% \begin{enumerate}
%     \item \textbf{Prefill phase.} 
%     Given an input sequence of length $L$ (here, $L = |\bc_i| + |\bq_i|$), a dense Transformer with $n_{\text{layer}}$ layers and hidden dimension $d$ incurs roughly 
%     \[
%     \mathcal{O}\bigl(n_{\text{layer}} \cdot (L^2 \, d + L \, d^2)\bigr)
%     \]
%     FLOPs for one forward pass. The $\mathcal{O}(L^2 d)$ term arises from the self-attention mechanism, whose complexity is quadratic in sequence length, and $\mathcal{O}(L d^2)$ arises from feed-forward layers. For long contexts ($L \gg d$), the quadratic attention term typically dominates, giving a rough $\mathcal{O}(n_{\text{layer}}\,L^2\,d)$ complexity.
    
%     \item \textbf{Decode phase.} 
%     When generating $T$ new tokens, the model runs an autoregressive forward pass for each token. At generation step $t \leq T$, the sequence length is $L + t - 1$, so each step's attention cost is proportional to $(L + t - 1)$. Summed over $T$ tokens, this results in approximately
%     \[
%     \mathcal{O}\Bigl(n_{\text{layer}} \sum_{t=1}^T \bigl((L + t - 1)^2 \, d + (L + t - 1)\,d^2\bigr)\Bigr).
%     \]
%     For small $T$ compared to $L$, many practitioners simplify to $\mathcal{O}(n_{\text{layer}}\,T\,L\,d)$ to reflect that each newly generated token attends to all prior tokens. This is linear in $L$ (per token) rather than the full $L^2$ of the prefill.
    
%     \end{enumerate}
    
%     \paragraph{Relation to model parameters.}
%     Often we approximate $n_{\text{layer}} \cdot d^2 \approx X$ total parameters. Hence, $d \approx \sqrt{X/n_{\text{layer}}}$, so the term $n_{\text{layer}}\,L^2\,d$ appears as
%     \[
%     n_{\text{layer}}\cdot L^2 \cdot \sqrt{\tfrac{X}{n_{\text{layer}}} }
%     \;=\;
%     \sqrt{\,n_{\text{layer}}\,X\,}\, L^2,
%     \]
%     yielding a prefill complexity of order $\mathcal{O}(\sqrt{X\,n_{\text{layer}}}\,L^2)$. Similarly for decoding, we get $\mathcal{O}(\sqrt{X\,n_{\text{layer}}}\,L\,T)$, up to constant factors.