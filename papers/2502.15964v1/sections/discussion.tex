\vspace{-1em}\section{Discussion} 
% Our study explores two protocols, \naive and \system, for collaboration between on-device and cloud LMs, demonstrating that cloud computing costs can be reduced by $5.7$–$30.4\times$ by effectively delegating tasks to local models.
% With increasingly powerful GPUs in consumer devices, users will rely less on cloud APIs, reducing operational costs while enabling complex local tasks like code refactoring and document analysis.
% \system highlights the promise of co-designing local and remote models to enhance efficiency, with the potential of moving beyond natural language to compressed real-valued representations in the future.
As local models continue to improve, studying \system will provide valuable insights into evolving workload distribution and the growing role of local compute.
As local models continue to improve, systems like \system will become increasingly valuable across a range of important workloads.


In this work, we explore two protocols -- \naive and \system -- for collaboration between on-device and cloud LMs. Our results demonstrate that it is possible to reduce the cost of cloud computing workloads $5-26\times$ by enabling remote LMs to effectively communicate with local LMs and delegate work to them. We explore the broader implications as well as the research opportunities this approach unlocks.

% \paragraph{Cloud Provider Cost Savings} \naive and \system demonstrate that it is feasible to utilize local compute to reduce the cost of cloud workloads by a factor of $5-26\times$. Such a protocol could drastically change the cost model for cloud LM providers. 

\vspace{-0.5em}\paragraph{User experience} Soon, commodity hardware—laptops, smartphones, and IoT devices—will feature powerful GPUs, enabling always-on local models for complex tasks like code refactoring, document analysis, and retrieval. Additionally, this advancement is expected to reduce users' reliance on API-based cloud LMs, leading to lower operational costs.

\vspace{-0.5em}\paragraph{Local-remote model co-design} \system demonstrates the promise of ``collaboration'' between local and remote LMs. Future work could advance this approach in two directions. First, the LMs we investigated were trained independently and therefore might not know each others capabilities and limitations. Communication can be made more efficient by training these models for collaboration. Second, model co-design could enable going beyond natural language as the modality of communication; models can exchange more compressed real-valued representations.

% \system uses natural language as the modality for information transfer, since the models were trained independently to take. This is a result of frontier models being trained in isolation from small ($<8B$) parameter models. Ideally, these models are trained with the explicit objective of collaborating with a small LM's to complete data-intensive reasoning tasks. 


\vspace{-0.5em}\paragraph{Improvement of \system over time} Our analysis in Section~\ref{app:model_history} highlights the rapid advancements in local model capabilities over the past 15 months. As local-remote capabilities continue to evolve, studying \system will provide valuable insights into shifts in workload distribution and the increasing utilization of local compute resources.
