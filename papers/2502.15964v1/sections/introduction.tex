
% For many problems the question 

% Feedback 
% (1) make sure to run the minions protocol with edge-edge
% (2) give some prescriptive recommendation based on problem type for what design choices to use
% We draw inspiration from real-world workflows and focus on tasks that require reasoning over large volumes of financial, medical, and scientific text. The text, such as a company's 10-K financial statement, is assumed to be stored locally. The user specifies a task that can range from simple information extraction (``What was the total revenue in FY2023?'') to multi-step calculations (\yell{Sabri, can you copy a concrete example of a calculation}?).
\section{Introduction}

\label{sec:intro}
Today's cloud-hosted frontier Language Models (LMs) can perform \textit{data-intensive reasoning}: they can generate and refactor code across entire repositories and make decisions based on financial, legal, and medical documents. However, accessing these models is expensive: processing a standard million-token code repository with OpenAI's o1 API costs $>\$15$ per query. 
% 
At the same time, smaller LMs (1-8B parameters) are rapidly improving and can now run on personal computers (\href{https://ollama.com/}{Ollama}, \href{https://github.com/ggerganov/llama.cpp}{llama.cpp}) and smartphones \citep{mehta2024openelm,yi2024phonelm,xu2024empowering}. 
% For example, it has been reported that 1.5B LMs, can process $\approx 1000$ tokens per second on a phone with a Neural Processing Unit \cite{xu2024empowering}. 
%
Yet, today, these small, on-device LMs are used mostly for \href{https://machinelearning.apple.com/research/introducing-apple-foundation-models}{simple tasks} such as tone adjustment and text completion~\citep{gunter2024apple}. They do not play a role in data-intensive reasoning tasks.

Inspired by the growing literature on multi-agent systems~\citep{wang2024mixture,guo2024large}, in this work we ask: \textit{how can a small, on-device LM collaborate with a frontier LM in the cloud to reduce inference costs on data-intensive reasoning tasks?} In particular, we study the \textit{communication protocols} that govern how the two LMs talk to each other, focusing on the tradeoff between cost and accuracy.
% 
To mimic realistic use cases, we study tasks that involve varying levels of reasoning over large volumes of medical, financial, and academic data ~\citep{islam2023financebench, adams2024longhealth, dasigi2021dataset}.
% 
% Our investigation is inspired by the growing literature on multi-agent systems~\cite{wang2024mixture,guo2024large}, but we differ by focusing on the specific asymmetries that arise in a local-remote setup.
% See \cref{sec:prelim-setup} for details on our problem setting.

% How can we do more on edge? And what does this mean or the cost / acc of frontier models/
% We ask: \textit{how can we utilize a small LM on-device to reduce cloud inference costs}? Our setting includes two models interacting in natural language: a frontier LM in the cloud (\textit{e.g.} \textit{\gpt}) and a small LM running on-device with exclusive access to context (\textit{e.g.} \llama-3B). See \cref{sec:prelim-setting} for a detailed description of the setting under study.

% Inspired by recent work on multi-agent systems (CITE MoA), 
% we study an asymmetric collaboration setting where a small edge LM (\textit{e.g.} Llama-2-3.2B), with full access to the input, communicates with a frontier cloud-hosted LM (\textit{e.g.} Claude-Sonnet-3.5) without access to the input.

% To mimic realistic use cases for a local-remote system, we study tasks that involve varying levels of reasoning on long contexts of medical, financial, and academic data ~\cite{islam2023financebench, adams2024longhealth, dasigi2021dataset}.

\input{figures/banner/banner.tex}

As our first attempt, we study a simple communication protocol we call \naive: an unconstrained chat between the local and remote models. \naive reduces cloud costs by only ``reading'' the data locally, and communicating a compressed version of the context to the remote model. 
We show that while \naive achieves a $30.4\times$ reduction in remote model costs, it trails behind the remote-only baseline by 9.4 accuracy points on average (with an 8B local model; see \Cref{sec:naive} for details).
% At first glance, a simple chat protocol where the local and remote model take turns sending messages to one another, seems promising. 
% Costs can be reduced by having only the local model ``read'' the full context and communicating a compressed version to the remote model. 
% We implement this protocol, which we call \naive, and show it can achieve a $26\times$ reduction in remote model costs (see \Cref{sec:naive} for details).
% 
% However, this na√Øve protocol does not fully recover the performance of the frontier model (see ~\Cref{sec:naive-quality}): at best, with an 8B local model, it compromises 3, 11, and 15 accuracy points.
% In \naive falls short largely due to two limitations of small LMs:
% We analyze \naive to identify its limitations and find, through empirical evaluations (see Section~\ref{sec:naive-analysis}), that its shortcomings stem from two key limitations of small LMs:
In isolated ablations, we identify two key limitations of small LMs that hinder \naive's performance (Section~\ref{sec:naive}):
\begin{itemize}
        \item \emph{Small LMs struggle to follow multi-step instructions.} We find that splitting complex instructions into separate requests improves performance by $56\%$.
        \item \emph{Small LMs get confused by long contexts}. Increasing context length from $<1K$ to $>65K$ decreases performance by $13\%$ on a simple extraction task. 
\end{itemize}
% \begin{itemize}
%         \item \textbf{Multi-step instruction following} Small LMs struggle with multi-step instructions. We find that splitting complex instructions into separate requests improves performance by $56\%$.
%         \item \textbf{Long-context processing}. Small LMs struggle to reason across long contexts. We find that increasing context length from $<1K$ to $>65K$ decreases performance by $13\%$ on a simple extraction task. 

% \end{itemize}
% \begin{itemize}
%     \item  \emph{Limitations of small LMs.} The well-known quality gap between large and small models \cite{kaplan2020scaling} is even more pronounced on the real-world workflows under study. In \Cref{sec:naive-quality}, we highlight two limitations of an unconstrained back-and-forth chat:
%     \begin{itemize}
%         \item \textbf{Long-context processing}. Small LMs can get distracted and incoherent when processing long sequences of tokens. It's unclear how the large remote model can help the local one manage the context without reading the context itself.
%         \item \textbf{Multi-step instruction following} Small LMs cannot be trusted to follow multi-step instructions.
%     \end{itemize}
% \end{itemize}

% We identify two reasons why the \naive protocol is suboptimal for the local-remote setting:

% Need to make this very concrete with a forward pointer to exact definition of what this naive sysytem looks like 
% Below, I think we can drop the middle bullet and these should also just point to a section where we highlight the limitations of the naive back and forth.


% \begin{itemize}
%     \item  \emph{Limitations of small LMs.} The well-known quality gap between large and small models \cite{kaplan2020scaling} is even more pronounced on the real-world workflows under study. In \Cref{sec:naive-quality}, we highlight two limitations of an unconstrained back-and-forth chat:
%     \begin{itemize}
%         \item \textbf{Long-context processing}. Small LMs can get distracted and incoherent when processing long sequences of tokens. It's unclear how the large remote model can help the local one manage the context without reading the context itself.
%         \item \textbf{Multi-step instruction following} Small LMs cannot be trusted to follow multi-step instructions.
%     \end{itemize}
    
    % Small models struggle with long contexts and do not adhere to the complex, multi-part instructions output by large models. 
    % We discuss this in \Cref{sec:naive-quality}.
    % \item Frontier LM's need to \emph{calibrate} to the capabilities of the small LMs and infer when can their outputs be trusted.
%     \item \emph{Low utilization of on-device resources.} As we discuss in \Cref{sec:naive-util}, this protocol achieves only \yell{X\%} hardware utilization locally because LM inference is I/O bound with batch size one. In the cloud, this isn't a problem because the server host can batch requests across concurrent users. However, locally, generating a hundred samples in parallel is often no more costly than generating a single sample \cite{leviathan2023fast}.  Therefore, this sequential turn-taking protocol leaves significant local compute on the table.
% \end{itemize}

% Needs forward pointer to exact method defintiion and pointer to figure showing the method
Motivated by these limitations, we propose \system, an extension of \naive where the remote LM decomposes the problem into a set of \emph{single-step instructions} to be performed on smaller \emph{chunks} of the document.
% Motivated by these limitations, we propose an extension of \naive that we call \system (emphasis on the $\mathcal{S}$).
% Instead of asking the local LM arbitrary questions, in this protocol, the remote LM decomposes the problem into \emph{single-step instructions} to be performed on smaller \emph{chunks} of the document. 
Crucially, the remote model has to do this \textit{without} reading the full document, which it achieves by \textit{generating code} that is later executed locally where the document is.
% Motivated by these limitations, we propose a simple extension of \naive that we call \system  (emphasis on the $\mathcal{S}$). In this protocol, the remote model decomposes the task and context into simpler \emph{subtasks} that can be executed \emph{in parallel} on device. 
% The remote model does this \textit{without} reading the full context by \textit{writing code} that performs the chunking and decomposition. 
% 
% But how can the remote model perform a fine-grained task decomposition \emph{without seeing any data}? Our first obseration is that given a question, a frontier model possess an impressive amount of ``procedural knowledge'', \textit{i.e.} which steps are needed to answer a question, and which data each requires.
% 
% A second problem, is that not only we need to decompose the task: the context needs to be chunked at some chosen level of granularity, and passed to the local model along with a subtask.  To achieve that, our second insight is to generate code by the remote model, that can be executed locally where context is available, to flexibly chunk the document and assign tasks to individual chunks.
% 
% 
More precisely, \system involves a loop over three steps:
\begin{enumerate}
    \item \textbf{Decompose}: Given a task, the remote model writes code that decomposes it into ``bite-sized'' \textit{subtasks}.
    \item \textbf{Execute}: The local LM then executes the subtasks in parallel and sends a filtered selection of the responses back to the remote model.
    \item \textbf{Aggregate}: The remote model aggregates the local outputs and either finalizes the solution or loops back to the Decompose step.
\end{enumerate}

% Since, on the local machine, generating a batch of samples in parallel is often no more costly than generating a single sample \cite{leviathan2023fast}, \system achieves higher utilization of local hardware. 

% parallel jobs to the local, \system can achieve higher utilization of edge hardware, improving quality at little additional cost. 
% This protocol is motivated by a rich literature, ~\cite{mixtur} \yell{the sequential nature should be explained, they may ask about what can or cannot be broken down in this way}.
% \system is a natural choice given the challenges discussed above. By decomposing the task into ‚Äúbite-sized‚Äù jobs, it mitigates the gap in capabilities between remote and edge models. And finally, by delegating batches of parallel jobs to the edge, \system can achieve higher utilization of edge hardware, improving quality at little additional cost. 

% We evaluate \system across three tasks in financial, medical and scientific reasoning~\cite{islam2023financebench, adams2024longhealth, dasigi2021dataset}. 

Averaged across tasks, \system with an 8B parameter local LM can recover $97.9\%$ of the performance of remote-only systems at $18.0\%$ of the cloud cost (see Figure~\ref{fig:main-tradeoff}). 
With a 3B parameter local LM, \system  achieves $93.4\%$ of the performance of remote-only systems at $16.6\%$ of the cloud cost (see Figure~\ref{fig:main-tradeoff}).

% On average across tasks, \system with an 8B parameter local model (which can run on a laptop at tens of tokens per second~\citep{transformer-math-eleutherai}) can recover $98.6\%$ of the performance of remote-only systems at $19.2\%$ of the cloud cost. 
% With a 3B parameter local model (small enough to run on a phone~\citep{apple2024introducing}) we can achieve $94.4\%$ of the performance of remote-only systems at $10.2\%$ of the cloud cost.


% Can probably drop first sentence
% To reiterate, all remote computations are done without pre-filling (\textit{i.e.} ``reading") the entire input. In fact, we find that by communicating less than 10\% of the total input tokens to the cloud, MINION performs comparably to the frontier model in finance, health, and academic paper QA benchmarks \yell{CITE}. 

% MINION is a natural choice given the asymmetries discussed above. Critically, this strategy promises to significantly reduce remote costs, since none of the context is directly processed in the cloud. Furthermore, by decomposing the task into ‚Äúbite-sized‚Äù jobs, it mitigates the gap in capabilities between remote and edge models. And finally, by delegating batches of parallel jobs to the edge, divide-and-conquer systems can achieve higher utilization of edge hardware, improving quality at little additional cost. 

% We identify three key design choices (i.e. hyperparameters) that govern this protocol. However, it is unlear how they affect the cost-accuracy tradeoff. We analyze this:

We perform a detailed analysis of the design and hyperparameter space of \system. 
Our analysis highlights several ``knobs'' that allow us to trade off cost for quality. 

\textbf{(a) Model choice} \textit{How does the size and family of the language models affect the cost and quality?} We show that \system would not have been feasible until mid-2024 (due to the  the release of \textsc{gpt4-turbo} and \llamathreeone) and is now performant with the latest 3B-parameter models running locally.


%We show how \system is made possible by today's 3B parameter models locally and would not have been feasible until mid-2024.  larger local models improve communication efficiency.
  % We consider on-device models ranging from 1B to 8B parameters from the \llama~\citep{dubey2024llama3} and \qwen families~\citep{bai2024qwen2.5} and that the quality improves predictably with model size. 
  % \yell{We show how \system would not have been feasible until mid 2024}. 
  % Furthermore, we show that the larger the model the \textit{lower} the remote cost.  
  % \yell{We explain this phenomenon theoretically with a simple information-bottleneck argument.}
  % n the cloud, we consider leading frontier models such as \gpt, Claude Sonnet 3.5, and \llama-3.3-70B. 
  % We show the effects of model pairs in \Cref{subsec:results-model}.
  % On scientific reasoning (\qasper), accuracy improves from $38\%$ to $66\%$ when moving from a 1B to 8B parameter edge model. The 8B parameter figure is a 6 percentage point improvement over the baseline set by GPT-4o acting in isolation. However, 8B edge models are challenging to run on some edge hardware, especially when latency is a constraint.

\textbf{(b) Scaling parallel workloads on-device.} \textit{How should we structure parallel workloads at the edge to maximize performance?}  In \Cref{subsec:results-workloads}, we study three different strategies for increasing the parallel workload on-device: (a)  repeated sampling, (b) decomposition, and (c) context chunking. 
  We show that all three can independently improve quality at the expense of increased remote cost. 
  % Interestingly, we show that this increased parallelism is nearly free on-device due to batching and the IO-bound nature of LLM inference.  
% For example, on medical reasoning (\longhealth), by increasing the number of random samples, we can improve accuracy from \yell{$X\%$} to \yell{$Y\%$}. Critically, we show that this leads only to a \yell{$1.0z\times$} in edge latency, due to batching.

\textbf{(c) Sequential communication protocols.} \textit{Can multiple rounds of communication improve quality? At what cost?} 
  % We consider two strategies for improving the quality, through looping of the \system steps (a)``retry'': increasing the number of attempts for the \system system, where each attempt is blind to previous attempts (b) allowing longer multi-turn conversations where the remote model sees the results of the previous attempts (See \Cref{subsec:results-communication})
  We show that by increasing the number of sequential rounds of communication, we can  pay more to improve quality. 
  % For example, with 3B parameter edge models and GPT-4o in the cloud, we can improve accuracy from $53\%$ to $70\%$ by simply increasing the maximum number of communication rounds. However, this nearly doubles the number of tokens processed by the remote model.

% In Section \ref{subsec:results-communication}, we perform an error analysis, showing how different types of tasks are more or less amenable to the different strategies described above. For example, \yell{TODO}. We also highlight several types of tasks where divide-and-conquer systems remain ineffective. This motivates future work in joint remote-edge training.




% There are several important takeaways from our analysis that can inform research and development of edge-remote systems going forward. Across a three challenging benchmarks spanning financial~\cite{islam2023financebench}, medical ~\cite{adams2024longhealth}, and scientific reasoning ~\cite{dasigi2021dataset} we show that \yell{add short description on task hierarchies and why we chose these}:
% \begin{itemize}
%   \item Optimized edge-remote systems can recover the performance of remote-only systems while sending only \yell{10\%} of the tokens to the remote model.
%   \item We can navigate the cost-accuracy Pareto frontier using both sequential and parallel test-time compute strategies. 
%     \begin{itemize}
%         \item \textbf{Model choice.} The size of the edge model . On scientific reasoning (\qasper), accuracy improves from $38\%$ to $66\%$ when moving from a 1B to 8B parameter edge model. The 8B parameter figure is a 6 percentage point improvement over the baseline set by GPT-4o acting in isolation. However, 8B edge models are challenging to run on some edge hardware, especially when latency is a constraint.
%         \item \textbf{Sequential.} By increasing the number of sequential rounds of communication, we can tradeoff of increased cost for improved quality. For example, on financial reasoning (\finance) with 3B parameter edge models and GPT-4o in the cloud, we can improve accuracy from $53\%$ to $70\%$ by simply increasing the maximum number of communication rounds. However, this nearly doubles the number of tokens processed by the remote model.
%         \item \textbf{Parallel.} By increasing the number of parallel jobs at the edge, we can also boost quality. We show that this increased parallelism is free at the edge due to batching and the IO-bound nature of LLM inference.  For example, on medical reasoning (\longhealth), by increasing the number of random samples, we can improve accuracy from \yell{$X\%$} to \yell{$Y\%$}. Critically, we show that this leads only to a \yell{$1.0z\times$} in edge latency, due to batching.
%     \end{itemize}
  % \item In Section \ref{subsec:results-communication}, we perform a fine-grained error analysis, showing how different types of tasks are more or less amenable to the different strategies described above. For example, \yell{TODO}. We also highlight several types of tasks where divide-and-conquer systems remain ineffective. This motivates future work in joint remote-edge training. 
% \end{itemize}

To summarize, our main contributions are as follows:
\begin{itemize}
    \vspace{-0.5em}\item Propose \naive, a na\"{\i}ve local-remote LM communication protocol, that achieves $30.4\times$ efficiency over remote-only workloads while recovering $87\%$ of performance.
    % \vspace{-0.5em}\item Isolate limitations of small LMs that hamper local-remote systems.  THOUGHTS ON REMOVING THIS?
    \vspace{-0.5em}\item Propose \system, an extension that overcomes the limitations we identify in \naive, achieving $5.7\times$  cost-reduction over remote-only workloads and recovering $97.9\%$ of performance.
    \vspace{-0.5em}\item Conduct an in-depth analysis of \system, exploring design choices to traverse the cost-accuracy trade-off. 
    
\end{itemize}


