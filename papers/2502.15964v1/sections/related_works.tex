\vspace{-1em}
\section{Related Work}

\label{sec:related-work}

\textit{See \Cref{app:related-work} for an extended discussion of related work.}

This study is inspired by a large body of work that explores how to combine multiple LMs and tools to improve quality and reduce cost of cloud workloads.
These include:

\setlength{\leftmargini}{12pt} % Adjust indentation
\begin{itemize}

\item \textbf{Retrieval-Augmented Generation (RAG)} RAG workflows append retrieved chunks to a large LM's prompt~\citep{lewis2020retrieval,karpukhin2020dense, lee2019latent}, mitigating hallucinations and externalizing knowledge. We differ by assigning arbitrary subtasks to a local LM, which can \emph{execute} longer or more complex instructions on chunks before sending only the final extracted content. This helps reduce communication costs more than purely retrieving and passing raw text.

\item \textbf{Multi-LLM collaboration and routing} A growing body of work explores multi-agent or multi-LLM systems~\citep{guo2024large, wang2024mixture} and model-routing~\citep{chen2023frugalgpt, chen2024more}. Typically, these either combine multiple large models or choose one LM from a "menu" of models for the entire task. In contrast, we explicitly study a \emph{two-model collaboration} where the smaller local LM handles extensive on-device context, while the larger remote LM is called on selectively, reducing cloud inference costs.

\item \textbf{Compound LM systems} A broader line of research integrates LMs with retrieval modules, tool use, or orchestrators~\citep{saad2024archon, khattab2023dspy}. While they optimize accuracy or adapt prompts, they do not usually focus on asymmetric edge-cloud costs or local parallelization.

\end{itemize}
% Unlike these remote-only studies, we focus on the specific cost model that arises from in the local-remote setting. 

The specific techniques used in \system build upon several important ideas proposed in the literature:

\setlength{\leftmargini}{12pt} % Adjust indentation
\begin{itemize}

\item \textbf{Orchestration for long-contexts} Prior works have used ``divide-and-conquer'' strategies to process long documents in smaller chunks~\citep{zhang2024chain, zhou2024llm, shankar2024docetl}. They define protocols for chunking, processing, and recombining content, sometimes with automated pipeline optimizations. However, these methods typically rely on a single large LM or multiple equally capable LMs, rather than an asymmetric local-remote collaboration with explicit cost constraints. They also do not explore parallel on-device tasks or multi-round communication with a cloud model. Other approaches improve single-LM handling of lengthy inputs by compressing, summarizing, or streaming data. Techniques like MemGPT~\citep{packer2023memgpt}, PRISM~\citep{jayalath2024long}, and writing-in-the-margins~\citep{russak2024writing} store partial results or structured data across external memory.  While such approaches reduce context overhead for a single LM, they do not address distributing computation across a local-remote system with distinct cost models.

\item \textbf{Decomposition techniques} The decomposition techniques used in \system are inspired by prior work showing how prompting for decomposition can improve small LM quality~\citep{arora2022ask, patel2022question, wu2022ai}. The \system protocol also builds upon the idea of using code to facilitate reasoning~\citep{arora2023evaporate,li2023chain}.

\item \textbf{Test-time sampling and verification} In \system we pair repeated test-time sampling on-device with verification in the cloud. This technique is motivated by extensive literature demonstrating the promise of using test-time sampling and verification to improve reasoning capabilities~\citep{brown2024large,song2024good,hassid2024larger,snell2024scaling,wu2024empirical}.

\end{itemize}

% prompting and decomposition techniques for improved small LM quality~\citep{arora2022ask, patel2022question, wu2022ai}, scaling local compute via test-time sampling and verification~\citep{brown2024large,song2024good,hassid2024larger,snell2024scaling,wu2024empirical},
% and using code to facilitate reasoning~\citep{arora2023evaporate,li2023chain}

Some recent works have explored aspects of the local-remote setting.
Several study how local-remote systems can limit leakage of private information to a cloud-hosted LM API~\citep{siyan2024papillon, zhang2024cogenesis}. 
In this work, we do not address privacy concerns, though these privacy techniques 
% , which mediate the communication with a local privacy-aware LM that removes private information from the prompt, 
can be used in conjunction with \system.
Other techniques partition LM layers between local and cloud devices~\citep{jin2024collm, yang2024perllm} without a multi-round dialogue. Our system is distinct in that the local LM and remote LM \emph{collaborate in natural language} on tasks that draw on a large private context. This two-model interplay underlies our focus: reducing cloud inference costs while preserving performance.





% \textit{See \Cref{app:related-work} for an extended discussion of related work.}

% We are inspired by a large body of work that studies how to combine multiple LMs and tools to improve quality and reduce cost of cloud workloads.
% These include:
% \begin{enumerate}

% \end{enumerate}
% multi-agent system~\citep{guo2024large, wang2024mixture}, compound LM systems~\citep{saad2024archon,khattab2023dspy,yuksekgonul2024textgrad}, model-routing~\citep{chen2024more,chen2023frugalgpt}, and retrieval-augmented generation \citep{lewis2020retrieval,karpukhin2020dense,lee2019latent}. We differ from these works by studying the specific asymmetric cost model that arises from the local-remote setting. 

% % Unlike these remote-only studies, we focus on the specific cost model that arises from in the local-remote setting. 

% The techniques used in \system build upon several ideas proposed in the literature, including LM orchestration and memory systems to support long-context reasoning ~\citep{packer2023memgpt,jayalath2024long,russak2024writing,shankar2024docetl,zhou2024llm}, 

% Other approaches improve single-LM handling of lengthy inputs by compressing, summarizing, or streaming data. 
% Techniques like MemGPT~\citep{packer2023memgpt}, PRISM~\citep{jayalath2024long}, and writing-in-the-margins~\citep{russak2024writing} store partial results or structured data across external memory. 
% Some rely on LMs generating code for chunk parsing and context retrieval~\citep{arora2023evaporate}. 
% While such approaches reduce context overhead for a single LM, they do not address distributing computation across a local-remote system with distinct cost models.


% prompting and decomposition techniques for improved small LM quality~\citep{arora2022ask, patel2022question, wu2022ai}, scaling local compute via test-time sampling and verification~\citep{brown2024large,song2024good,hassid2024larger,snell2024scaling,wu2024empirical},
% and using code to facilitate reasoning~\citep{arora2023evaporate,li2023chain}

% Some recent works have explored aspects of the local-remote setting.
% Several study how local-remote systems can limit leakage of private information to a cloud-hosted LM API~\citep{siyan2024papillon, zhang2024cogenesis}. 
% In this work, we do not address privacy concerns, though these privacy techniques 
% % , which mediate the communication with a local privacy-aware LM that removes private information from the prompt, 
% can be used in conjunction with \system.
% Others have explored efficient routing patterns between local and remote computation for LM workloads, albeit without two models communicating in natural language or collaborating on a solution~\citep{jin2024collm,yang2024perllm}.




