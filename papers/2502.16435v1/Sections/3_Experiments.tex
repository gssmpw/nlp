\section{Experiments}

\subsection{Vanilla Scenarios}

We evaluate GPT-4o~\cite{openai2024hello}, Gemini-Pro (1.5~\cite{pichai2024our}, 2.0~\cite{pichai2024introducing}), Gemini-Flash (see appendix), and Qwen-VL~\cite{bai2023qwen} using {\methodname}.
The results are presented in Table~\ref{tab:main}.
The term ``Vanilla'' refers to using the original instructions and examples during testing, prompting MLLMs to output answers directly.
``CoT'' denotes instructing models to first generate step-by-step reasoning (output analysis) before producing the final answer.
Key findings include:
\textbf{(1) Performance Close to Random Guessing.}
Random guessing performance is provided as a reference for each test in the first column in Table~\ref{tab:main}.
Among the 20 tests, GPT-4o, Gemini-1.5, Gemini-2.0, and Qwen-VL yield performance not exceeding random guessing in 2, 9, 6, and 9 vanilla tests, respectively.
This aligns with previous research indicating that MLLMs struggle with nuanced visual reasoning, particularly when contextual or visual details are complex~\cite{fu2024blink, wu2024surprising}.
\textbf{(2) Limited Effectiveness of CoT Prompting.}
Surprisingly, CoT prompting does not consistently improve performance and sometimes results in lower scores compared to direct output (Vanilla).
For example, GPT-4o's performance decreases in 16 tests with CoT, scoring $32.8\%$, which is $4.3\%$ lower than its Vanilla score of $37.1\%$.
Conversely, Gemini-1.5-Pro and Qwen-VL-Max show slight improvements with CoT, increasing by $2.4\%$ and $1.9\%$, respectively.
A recent study~\cite{sprague2025cot} has found that while CoT prompting enhances mathematical reasoning, its benefits in other domains are limited.
We conduct $t$-tests on the scores with and without CoT across all MLLMs, with $t$-values and $p$-values presented in Table~\ref{tab:t-test}.
Among all tests, only CF1 shows a statistically significant improvement with CoT at the $95\%$ confidence level.
\textbf{(3) GPT-4o achieves the highest average score of $37.1\%$, outperforming other models.}

FRCT provides average human performance data collected from real-world subjects.
Unlike conventional accuracy measures, the original FRCT design evaluates performance using the number of correct answers minus the number of incorrect ones.
Consequently, these scores cannot be directly converted into accuracy.
For clarity, we present accuracy results in Table~\ref{tab:main}, while scores calculated using the FRCT metric are shown in Table~\ref{tab:main-human} and Table~\ref{tab:gemini-flash-human}, alongside human performance benchmarks in the appendix.
We find that MLLMs demonstrate greatly lower performance compared to humans.

\subsection{Advanced Reasoning Techniques}

We also explore advanced techniques to enhance the reasoning capabilities of MLLMs.
\textbf{(1) Structured CoT (SCoT)}~\cite{qiao2024prism} decomposes complex tasks into a series of logical, manageable steps, simulating human reasoning through intermediate observations and validations.
Building on this framework, we designed a structured CoT prompt that systematically breaks down visual reasoning tasks into sequential steps, enabling intermediate evaluations at each stage.
By aligning these intermediate outputs with expected outcomes, we iteratively refined the prompt to maximize task performance.
Although this approach achieves the highest performance improvement, the gain is marginal, with only a $0.4\%$ increase over the vanilla model.
\textbf{(2) The Multi-Agent Debate (MAD)}~\cite{liang2023encouraging} involves multiple agents debating a topic, with a judge determining the final answer, promoting diverse perspectives and mitigating biases.
Adopting the MAD framework, we distributed tasks simultaneously among debaters and judges.
The final answer emerged through interactive arguments and the judgeâ€™s decision-making.
However, this approach resulted in a performance decrease of approximately $10\%$ compared to the vanilla model.

\subsection{Robustness Evaluation}

For robustness evaluation, we assessed GPT-4o's performance on input images with various transformations.
For geometric transformations, each image is randomly translated from a normal distribution $\mathcal{N}(0, 5)$, rotated uniformly within $[-10^\circ, +10^\circ]$, and flipped when applicable.
For photometric transformations, Gaussian noise from $\mathcal{N}(0, 0.1)$ is added, with contrast and brightness factors set to $1.5$.
The results are presented in the ``Pert.'' column of Table~\ref{tab:main}.
On average, these perturbations reduced performance by 2\%.