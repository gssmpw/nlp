\section{Conclusion}

In this work, we introduce {\methodname}, a novel benchmark for evaluating fundamental visual cognition in MLLMs.
By digitalizing and adapting vision-related tests from FRCT, {\methodname} provides a standardized and rigorous framework for assessing core visual abilities.
Our comprehensive evaluation of state-of-the-art MLLMs using {\methodname} reveal a significant and unexpected limitation: current MLLMs struggle with fundamental visual cognitive tasks, exhibiting performance far below human norms and frequently approximating random guessing.
Furthermore, we demonstrate that advanced prompting techniques, while effective in other domains, yield only minimal improvements in MLLMs' performance on {\methodname}.
These findings highlight a critical gap in the current capabilities of MLLMs and suggest that while they excel in complex multimodal tasks, their foundational visual cognition abilities are still underdeveloped.

% This necessitates future research focused on enhancing the core visual processing and cognitive mechanisms within MLLMs.
% Potential future directions include exploring novel architectures specifically designed for visual reasoning, incorporating cognitive-inspired modules into MLLMs, and developing training methodologies that explicitly target fundamental visual cognitive skills.
% The release of the {\methodname} benchmark is intended to serve as a catalyst for this crucial research, providing a standardized tool and challenging task suite to drive progress in the field of visual intelligence in AI.

% \section*{Limitations}

% \section*{Ethics Statements}

% \section*{Acknowledgments}