\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/framework.pdf}
    \caption{An overview of the fifteen tests in our {\methodname} benchmark, categorizing various visual and spatial reasoning tests. Each test evaluates distinct abilities such as pattern recognition, mental rotation, spatial visualization, and perceptual organization.}
    \label{fig:framework}
\end{figure*}

Multimodal Large Language Models (MLLMs) have revolutionized the field of multimodal artificial intelligence~\cite{bubeck2023sparks, chow2025physbench}, showcasing unprecedented capabilities in diverse tasks such as text recognition~\cite{liu2024ocrbench, chen2025ocean}, mathematical problem-solving~\cite{yang2024mathglm, peng2024multimath}, and medical applications~\cite{azad2023foundational, buckley2023multimodal}.
Although existing research has explored MLLMs' visual capabilities~\cite{fu2024blink, song2024m3gia}, including spatial reasoning~\cite{cai2024spatialbot, cheng2024spatialrgpt}, systematic investigations into their fundamental visual cognitive abilities remain limited.
This knowledge gap arises from the scarcity of evaluation frameworks rigorously grounded in cognitive science.
Indeed, most MLLM assessments prioritize downstream applications, often overlooking the essential foundational visual abilities.

In this study, we address this critical gap by introducing {\methodname}, an automated testing pipeline derived from the Factor-Referenced Cognitive Test (FRCT)~\cite{ekstrom1976manual}.
The FRCT is a well-established psychometric assessment battery specifically designed to evaluate distinct cognitive faculties, including verbal comprehension, spatial visualization, memory, and reasoning.
In contrast to broad-spectrum intelligence tests that produce a general intelligence quotient, the FRCT focuses on precise cognitive constructs through \textit{Factor Analysis}, providing a granular view of cognitive profiles and informing targeted interventions.
Specifically, we select \textbf{seven} vision-oriented subtests from the FRCT—Closure Flexibility (\textbf{CF}), Closure Speed (\textbf{CS}), Induction (\textbf{I}), Perceptual Speed (\textbf{P}), Spatial Relations (\textbf{S}), Spatial Scanning (\textbf{SS}), and Visualization (\textbf{VZ})—each addressing core facets of visual processing and reasoning.
These categories offer a targeted lens for evaluating MLLMs' core competencies in visual processing, spatial reasoning, and abstract pattern recognition, mirroring established benchmarks of human cognition.

Concretely, {\methodname} digitizes key components of the FRCT manual: instructions, examples, image-based questions, correct answers, and human performance norms.
This design facilitates direct prompting of MLLMs under standardized testing conditions, enabling meaningful comparisons with established human benchmarks.
To further investigate model robustness, we incorporate a module that systematically perturbs input images by injecting noise, modifying contrast or brightness, and applying spatial transformations such as translations, rotations, and flips.
Through this pipeline, we aim to quantify the extent to which current MLLMs can adapt to non-standard or degraded visual inputs.

Our experiments assess state-of-the-art MLLMs, including GPT-4o~\cite{openai2024hello}, Gemini-Pro (1.5~\cite{pichai2024our}, 2.0~\cite{pichai2024introducing}), and Qwen-VL~\cite{bai2023qwen}, under diverse prompting strategies.
Despite employing techniques like few-shot demonstrations and Chain-of-Thought (CoT) prompting~\cite{kojima2022large, wei2022chain}, we observe notably poor performance, often approximating purely random responses.
Even sophisticated methods, such as structured CoT~\cite{qiao2024prism}, Multi-Agent Debate (MAD)~\cite{liang2023leveraging}, and visual sketchpads~\cite{hu2024visual}, yield only marginal improvements.
We conclude with qualitative analyses aimed at diagnosing the reasons for current MLLMs' underperformance on these seemingly fundamental visual tasks.
Our qualitative studies identify key limitations in current architectures, including deficiencies in spatial reasoning, an inability to extrapolate from partial patterns, and sensitivity to minor visual perturbations.

Our contributions are as follows:  
\begin{itemize}[leftmargin=*]
    \item We introduce {\methodname}, the first standardized benchmark for evaluating MLLMs' fundamental visual cognition using digitized FRCT, featuring automated prompt generation, human performance baselines, and a robustness assessment module with controlled visual perturbations.
    \item We present a comprehensive analysis of state-of-the-art MLLMs (\eg, GPT-4o, Gemini-Pro) across seven FRCT vision subtests. Our findings highlight critical limitations: (a) MLLMs perform close to random chance on fundamental visual reasoning tasks, and (b) advanced prompting strategies (CoT and MAD) produce negligible performance gains.
    \item We make our {\methodname} benchmark publicly available to facilitate further research and development in this critical area.
\end{itemize}