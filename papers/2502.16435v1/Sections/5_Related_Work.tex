\section{Related Work}

\subsection{Evaluating Visual Ability}

Recent studies have introduced benchmarks to assess MLLMs' visual and reasoning abilities.
\citet{fu2024blink} proposed Blink, revealing struggles with nuanced visual perception, while \citet{wu2024surprising} found poor performance on \textit{NLVR}, a task requiring compositional and spatial reasoning.
Beyond perception, \citet{zhang2024far} highlighted MLLMs' limitations in visual deductive reasoning using \textbf{Raven's Progressive Matrices}, and \citet{song2024m3gia} introduced \textit{M3GIA}, a benchmark based on the \textit{CHC} model, for broader intelligence assessment.
Other studies focus on context-sensitive and cognitive reasoning.
\citet{wadhawan2024contextual} found significant gaps in contextual reasoning over text-rich images, while \citet{coda2024cogbench} introduced \textit{CogBench}, showing how model size and RLHF impact behavioral performance.
To enhance reasoning, \citet{zhao2024lova3} proposed \textit{LOVA3}, equipping MLLMs with visual question-answering capabilities, improving performance on \textit{GenQA} and \textit{EvalQA} tasks.
These studies highlight MLLMs' ongoing challenges in perception, reasoning, and contextual understanding while introducing methods for improvement.

\subsection{Evaluating Spatial Reasoning Ability}

Recent studies have highlighted the challenges of spatial reasoning in MLLMs and introduced benchmarks to address these limitations.
\citet{kamath2023s} attributed MLLMs' struggles to insufficient spatial information in pretraining data.
\citet{liu2023visual} introduced the \textit{VSR} dataset, revealing a significant performance gap across 66 spatial relations.
To improve spatial understanding, \citet{cai2024spatialbot} leveraged RGB and depth images, proposing \textit{SpatialQA} and \textit{SpatialQA-E}.
\citet{cheng2024spatialrgpt} enhanced region-level reasoning with depth integration and the SpatialRGBT-Bench benchmark.
\citet{li2024topviewrs} focused on top-view spatial reasoning, introducing the \textit{TOPVIEWRS} dataset.
These efforts advance spatial reasoning evaluation in MLLMs.

\subsection{Enhancing Visual Ability}

Recent studies have explored enhancing MLLMs' visual reasoning.
\textit{MVoT}~\cite{li2025imagine} generates visual thought traces to improve spatial reasoning beyond traditional CoT.
\textit{Visual Sketchpad}~\cite{hu2024visual} enables LLMs to create visual sketches as intermediate reasoning steps for better interpretability.
\textit{Visual CoT}~\cite{shao2024visual} introduces a large dataset to improve reasoning via multi-turn CoT processing.
Similarly, \textit{VoT}~\cite{wu2024visualization} enhances spatial reasoning through mental image generation, benefiting tasks like navigation and visual tiling.
\textit{SpatialCoT}~\cite{liu2025spatialcot} aligns spatial coordinates with CoT grounding to aid MLLMs in embodied AI.
\textit{CoI}~\cite{meng2023chain} integrates visual intuition into logical reasoning via a multimodal dataset and symbolic LLM.
Together, these approaches advance MLLMs by strengthening visual reasoning for better interpretability and problem-solving.

\subsection{Other Psychometrics}

Recent studies have explored human-like traits in LLMs, particularly personality, emotions, and cognitive abilities.
Several works have assessed LLMs using the \textit{Big Five Inventory}, including evaluations of its reliability on GPT-3.5~\cite{huang2024reliability}, PaLM family~\cite{serapio2023personality}, and its applicability across multiple models~\cite{jiang2023evaluating}.
Beyond personality, \textit{PsychoBench}~\cite{huang2024humanity} introduced a framework incorporating thirteen psychological scales for comprehensive LLM analysis.
Emotional traits have also been studied, with \textit{EmotionBench}~\cite{huang2024apathetic} analyzing affective states in LLMs, and \citet{coda2023inducing} investigating models' anxiety levels.
Additionally, research has explored LLMs' Theory-of-Mind (ToM) abilities~\cite{liu2024interintent, liang2023leveraging, huang2025competing} and role-playing abilities~\cite{ng2024well, wang2024incharacter, wang2025coser}.
Building on these studies, our work provides a comprehensive framework for personality analysis, integrating multiple psychological dimensions.