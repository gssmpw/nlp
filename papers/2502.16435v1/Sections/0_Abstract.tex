Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in multimodal understanding; however, their fundamental visual cognitive abilities remain largely underexplored.
To bridge this gap, we introduce {\methodname}, a novel benchmark derived from the Factor-Referenced Cognitive Test (FRCT), a well-established psychometric assessment of human cognition.
{\methodname} digitalizes vision-related FRCT subtests to systematically evaluate MLLMs across essential visual cognitive tasks including spatial reasoning, perceptual speed, and pattern recognition.
We present a comprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro, and Qwen-VL, using {\methodname} under diverse prompting strategies like Chain-of-Thought and Multi-Agent Debate.
Our findings reveal a concerning deficiency in current MLLMs' fundamental visual cognition, with performance frequently approaching random guessing and showing only marginal improvements even with advanced prompting techniques.
These results underscore the critical need for focused research to enhance the core visual reasoning capabilities of MLLMs.
To foster further investigation in this area, we release our {\methodname} benchmark at \url{https://github.com/CUHK-ARISE/VisFactor}.