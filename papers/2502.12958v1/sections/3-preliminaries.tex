\section{Preliminaries}
\label{ssec:preliminaries}

\cref{ssec:FR-fram} and \cref{ssec:setting-attack} present the framework of federated recommendations and the fundamental settings of the attack against them, respectively.
% The frequently used notation throughout this paper is provided in~\cref{tab:notation}.

% \begin{table}[!htbp]
%     \centering{
%     \caption{Key notations.} \label{tab:notation}
%     % \setlength{\tabcolsep}{4mm}
%     \begin{tabular}{@{}l|l@{}}
%     \toprule
%         Symbol &  Meaning \\
%     \midrule
%          $u_i,\mathbf{u}_i$ & The $i$-th user, $u_i$'s embedding vector \\
%          $v_j,\mathbf{v}_j$ & The $j$-th item, $v_j$'s embedding vector \\
%          $x_{ij}$, $\hat{x}_{ij}$ & Ground truth and predicted score for $u_i$ and $v_j$ \\
%          % $\mathcal{I}$, $|\mathcal{I}|$ & Set and number of items \\
%          $U$, $\bar{U}$, $\tilde{U}$ &
%          Sets of all, benign, and malicious users  \\
%          $U^r$ & Set of users in the communication round $r$ \\    
%          $\bar{U}_j$ & Set of benign users with $v_j$ in the top-$K$ recommendation list\\
%          $\bar{U}'_j$ & Set of benign users who have interacted with $v_j$ \\               
%          % $\mathcal{D}_i$ & Training set of user $u_i$ \\
%          % $\mathcal{D}^{+}_i$, $\mathcal{D}^{-}_i$ &
%          % Set of interacted and uninteracted items in $\mathcal{D}_i$ \\
%          % $\mathcal{L}_i$, $\tilde{\mathcal{L}}_i$ & Loss function of the benign or malicious user $u_i$ \\
%          $T$ & Set of target items \\
%          $P$ & Set of mined popular items \\
%     \bottomrule
%     \end{tabular}}
% \end{table}

\subsection{Framework of Federated Recommendations} 
\label{ssec:FR-fram}

Let $u_i$ and $v_j$ be the $i$-th user and the $j$-th item in FRS, respectively.
The predicted score $\hat{x}_{ij}$ for the pair of user $u_i$ and item $v_j$ is computed as
$
\hat{x}_{ij} = \Psi(\mathbf{u}_i, \mathbf{v}_j),
$
where $\mathbf{u}_i$, $\mathbf{v}_j$, and $\Psi(\cdot)$ refers to $u_i$'s embedding, $v_j$'s embedding, and the interaction function, respectively.

The interaction function differs in MF-FRS and DL-FRS. 
Specifically, the interaction function in MF-FRS is defined as
\begin{equation*}
\Psi^\text{MF}(\mathbf{u}_i, \mathbf{v}_j) = \mathbf{u}_i \odot \mathbf{v}_j,
\end{equation*}
where $\odot$ is the dot product operator. While the interaction function in DL-FRS is defined as a stack of $L$ Multi-Layer Perceptions (MLPs):
\begin{equation}\label{equation:DL_interaction_function}
\begin{aligned}
\Psi^\text{DL}(\mathbf{u}_i, \mathbf{v}_j) & = \operatorname{sigmoid}(\mathbf{h}^\mathsf{T} \cdot \phi_L(\ldots(\phi_1(\mathbf{u}_i \oplus \mathbf{v}_j)))), \\
\phi_l(\mathbf{x}) & = \operatorname{ReLU}(\mathbf{W}_l^\mathsf{T}\mathbf{x} + \mathbf{b}_l), \,\, l = 1, \ldots, L,
\end{aligned}
\end{equation}
where $\phi_l(\mathbf{x})$ refers to the $l$-th ($1 \leq l \leq L$) layer MLP associated with learnable weights $\mathbf{W}_l$ and bias $\mathbf{b}_l$, $\mathbf{h}$ and $\oplus$ denotes the projection vector and vector concatenation operator, respectively, and $\operatorname{sigmoid}(\cdot)$ and $\operatorname{ReLU}(\cdot)$ are commonly-used activation functions.


In both MF-FRS and DL-FRS, the parameter of the personalized model of each client $u_i$ is $\{\mathbf{u}_i\}$.
For the global model, its parameter set in MF-RFS is the $m$ items' embeddings $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$, while the counterpart in DL-FRS is 
$\{\mathbf{v}_1, \ldots, \mathbf{v}_m, \mathbf{W}_1, \ldots, \mathbf{W}_L, \mathbf{b}_1, \ldots, \mathbf{b}_L, \mathbf{h}\}$.

Let $x_{ij}$ denote the ground truth score for pair $(u_i, v_j)$ such that $x_{ij}=1$ if $u_i$ has interacted with $v_j$ and $x_{ij}=0$ otherwise.
Before training the FRS, each user $u_i$ prepares its private training dataset ${D}_i = {D}^{+}_i \cup {D}^{-}_i$, where ${D}^{+}_i=\{v_j \mid x_{ij} =1 \}$ and ${D}^{-}_i=\{ v_j \mid x_{ij}=0 \}$ contains the interacted and uninteracted items for $u_i$, respectively.
Typically, ${D}_i$ is sampled with a fixed ratio $q$ of $|{D}^{+}_i|$ to $|{D}^{-}_i|$\footnote{We set $q=1$ in the implementation, following a previous study \cite{fedrecattack}.}.
Next, the training phase takes a number of communication rounds where the $r$-th round is performed as follows.
\begin{enumerate}[leftmargin=*]
\item The server randomly selects a set ${U}^r$ of users from the whole user set and sends the current global model to them.
\item With the received global model and its local personalized model, each user $u_i \in {U}^r$ computes its loss function
\begin{equation} \label{equation:rs-loss}
    \mathcal{L}_i =-\frac{1}{|{D}_i|}\sum_{v_j \in {D}_i} x_{ij}\log\hat{x}_{ij} + (1-x_{ij})\log(1-\hat{x}_{ij}),
\end{equation}
and derives the gradients of parameters in both global and personalized models.
Following the state-of-the-art method \AHUM{}~\cite{a-hum}, the widely-used Binary Cross-Entropy (BCE) loss\footnote{Without loss of generality, our method still works for other popular loss functions. We report the empirical studies in the supplementary material~\cite{github-pieck-supple}.} is employed to quantify the difference between the model-predicted score $\hat{x}_{ij}$ and the ground truth $x_{ij}$.
\item Each user $u_i$ uploads the gradients\footnote{Gradients are represented using a symbol $\nabla$ throughout the paper.} for the global model parameters, and updates its personalized model locally:
\begin{equation*}
\mathbf{u}_i = \mathbf{u}_i - \eta\cdot\nabla\mathbf{u}_i,
\end{equation*}
where $\eta$ is a unified learning rate specified by the server.
\item Once received the gradients uploaded by all users in ${U}^r$, the server aggregates the gradients through a function $\operatorname{Agg}(\cdot)$ to update its global model.
For both MF-FRS and DL-FRS, any item embedding $\mathbf{v}_j$ is updated as
\begin{equation*}
\mathbf{v}_j = \mathbf{v}_j-\eta\cdot 
\operatorname{Agg}(\{\nabla\mathbf{v}_j^i\ |\ u_i\in{U}^r, v_j\in{D}_i\}).
\end{equation*}
Extra parameters in DL-FRS (cf.\ \cref{equation:DL_interaction_function}) are updated as
\begin{equation*}
\begin{aligned}
\mathbf{W}_l = & \mathbf{W}_l-\eta\cdot \operatorname{Agg}(\{\nabla\mathbf{W}_l^i \mid u_i\in{U}^r\}),\\
\mathbf{b}_l = & \mathbf{b}_l-\eta\cdot \operatorname{Agg}(\{\nabla\mathbf{b}_l^i \mid u_i\in{U}^r\}),\\
\mathbf{h} = & \mathbf{h}-\eta\cdot \operatorname{Agg}(\{\nabla\mathbf{h}^i \mid u_i\in{U}^r\}).
\end{aligned}
\end{equation*}
When no defense is employed, $\operatorname{Agg}(\cdot)$ performs a simple sum operation. However, a defense method can make $\operatorname{Agg}(\cdot)$ more complex.
In % \cref{sssec:defense_methods} 
\cref{ssec:settings}, we include a series of complex defense methods for comparisons.
\end{enumerate}

% \sout{Using the trained model, users can generate a recommendation list for a user by selecting those uninteracted items having the top-$K$ predicted scores.}
Using the trained model, users can generate their own recommendation lists by selecting those uninteracted items with the top-$K$ predicted scores.

\subsection{Fundamental Settings of the Attack}
\label{ssec:setting-attack}

Let $\bar{{U}}$ and $\tilde{{U}}$ be the set of \textbf{benign users} and \textbf{malicious users} injected by the attacker, respectively.
The whole user set ${U}=\bar{{U}}\cup\tilde{{U}}$.
We proceed to introduce the attack settings from the following three aspects. 

\smallskip
\noindent\textbf{(1) Attacker Knowledge}.
To ensure the attack's applicability across various scenarios, we strictly adhere to the standard settings of FRS such that the attacker possesses only the following knowledge from the controlled malicious users:
\begin{enumerate}[leftmargin=*]
\item the attacker knows the learning rate $\eta$\footnote{Please refer to the supplementary material~\cite{github-pieck-supple} for an additional discussion on the effect of using inconsistent learning rates at the server and clients.}, a global hyperparameter for federated model training \cite{a-hum,fedrecattack,pipattack};
\item the attacker knows the structure of the base recommender model, which is shared by the server and all users;
\item the attacker knows the global model for the current $r$-th communication round only if one of the malicious users is selected for training, formally, $\tilde{{U}}\cap{U}^r\neq\mathcal{\varnothing}$.
\end{enumerate}
The aforementioned represents the essential \emph{minimum} knowledge required for a user to participate in FRS training, and also for an attacker to launch a possible attack.
Apart from that, the attacker has no access to any other information, e.g., benign users' personalized models, the gradients uploaded from benign users, items' popularity levels, and any part of the historical interactions (cf.\ \cref{tab:comparison}).

\smallskip
\noindent\textbf{(2) Attacker Capability}.
The attacker is unable to interfere with benign users, but has full control over the controlled malicious users. 
Hence, the attacker can generate well-crafted gradients and upload them via malicious users, so as to poison the FRS.
However, the attacker is unable to directly set the parameters of the global model at will because 1) the benign users among the selected users also upload gradients and 2) the aggregation function, possibly equipped with defense methods, limits the impact of poisonous gradients.

\smallskip
\noindent\textbf{(3) Attack Goal}.
Given a set ${T}$ of target items designated, 
the attacker uploads poisonous gradients during the training of FRS via controlled malicious users, aiming to increase the exposure of the target items to the benign users in the recommendation. 
This can be achieved by maximizing the average \emph{Exposure Ratio at rank $K$} (ER@$K$) for each target item $v_j \in {T}$ as
\begin{equation}
\label{equation:er}
\max \Big( 1/|{T}| \sum_{v_j \in {T}} \big(\text{ER}_{j}\text{@}K \big) \Big), \,\,\, \text{ER}_{j}\text{@}K = \frac{|\bar{{U}}_j|}{|\bar{U} \setminus \bar{{U}}'_j|},
\end{equation}
where $\bar{{U}}_j$ denotes the set of benign users{} whose top-$K$ \emph{recommendation lists} contain $v_j$ and $\bar{{U}}'_j$ denotes the set of benign users who have already \emph{interacted} with $v_j$.
However, the ER@$K$ metric in \cref{equation:er} is non-differentiable and discontinuous, thus hardly optimized.
Instead, we introduce the BCE loss (cf.\ \cref{equation:rs-loss}) as a proxy, which is defined on all pairs of a target item $v_j \in {T}$ and a benign user $u_i \in \bar{{U}}$:
\begin{equation} \label{att-loss-total}
\begin{aligned}
\tilde{\mathcal{L}}
&=-\frac{1}{|\bar{{U}}| |{T}|} \sum_{u_i\in\bar{{U}}}\sum_{v_j\in{T}} 1\cdot\log\hat{x}_{ij} + 0\cdot\log(1-\hat{x}_{ij}) \\
% &= -\frac{1}{|\bar{{U}}| \cdot |{T}|} \sum_{u_i\in\bar{{U}}}\sum_{v_j\in{T}} \log\hat{x}_{ij} \\
&= -\frac{1}{|\bar{{U}}| |{T}|} \sum_{u_i\in\bar{{U}}}\sum_{v_j\in{T}} \log \Psi(\mathbf{u}_i, \mathbf{v}_j).
\end{aligned}
\end{equation}
Minimizing $\tilde{\mathcal{L}}$ above equals to maximizing the mean predicted scores of all target items for all benign users, which inherently increases the likelihood of the items being recommended.
Optimizing this surrogate loss is an effective means to enhance the chance that benign users will include targeted items in their top-$K$ recommendations, i.e., ER@$K$.
This approach is in line with the established attack~\cite{a-hum}.
% \changetwo{In essence\rtwo{R2.O4}, since items with high prediction scores will be recommended, optimizing as described is considered as an efficient replacement to maximizing the number of benign users who include the target item in their recommendation list, i.e.,~ER@$K$, which also aligns with previous attack \cite{a-hum}.}
As each user embedding $\mathbf{u}_i$ is stored privately at the client's personalized model, the attacker can attempt to corrupt the FRS via the controlled malicious users' global models in two aspects:
\begin{enumerate}[leftmargin=*]
\item Each malicious user produces and uploads the poisonous gradients associated with the learnable parameters of the interaction function $\Psi(\cdot)$.
However, this approach becomes ineffective in the case of MF-FRS, where the interaction function is non-learnable.

\item Each malicious user produces and uploads the following poisonous gradient for promoting each target item $v_j \in {T}$:
\begin{equation} \label{eq:poisonous_gradients_1}
\nabla\tilde{\mathbf{v}}_j = - 1/|\bar{{U}}| \cdot \sum\nolimits_{u_i\in\bar{{U}}}\frac{\partial}{\partial \mathbf{v}_j} \Psi(\mathbf{u}_i, \mathbf{v}_j),
\end{equation}
where $\Psi(\cdot)$ can be either $\Psi^\text{MF}$ in the MF-FRS case or $\Psi^\text{DL}$ in the DL-FRS case (cf.\ \cref{ssec:FR-fram}). 
However, \cref{eq:poisonous_gradients_1} indicates that this approach requires access to embeddings of benign users, i.e., $\{ \mathbf{u}_i \mid u_i\in\bar{{U}} \}$, which have been made private in the FRS.

\end{enumerate}

We proceed to present our method \model{} (Popular Item Embedding based attaCK).
\model{} focuses on poisoning the recommender model via producing effective $\nabla\tilde{\mathbf{v}}_j$ as described in \cref{eq:poisonous_gradients_1}. Thus, \model{} is agnostic to both MF-FRS and DL-FRS.
Moreover, \model{} requires no access to benign users' embeddings. This is made possible by leveraging the unique properties of FRS to effectively identify popular items and utilizing their embeddings to generate poisonous gradients.