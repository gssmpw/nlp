\section{Defense Analysis and New Design}
\label{sec:defense}

\cref{sec:attack} has presented our identified attack \model{}, notable for its applicability to different base model types without requiring any prior knowledge.
Despite its potential to cause substantial harm to FRS applications, this threat has largely been overlooked.
In \cref{ssec:defense_analysis}, we will analyze the present defense methods in the federated setting and explain their inadequacies to combat targeted model poisoning in FRS.
In \cref{ssec:defense_design}, we will devise an innovative defense method aimed at reducing security risks associated with \model{} and other targeted model poisoning in FRS.

\subsection{Defense Analysis}
\label{ssec:defense_analysis}

Denote the proportion of malicious users by $\tilde{p} = |\tilde{{U}}| / |{U}|$.
As aforementioned, at each communication round $r$, the server randomly selects a batch of users ${U}^r$ to participate in the training.
In conventional federated learning, for each learnable parameter, selected benign users upload the normal gradient, while selected malicious users upload the poisonous version.
The expected proportion of poisonous gradients received by the server is $\tilde{E} = \frac{|{U}^r|\cdot\tilde{p}}{|{U}^r|}=\tilde{p}$.

However, in FRS, a benign user uploads the normal gradient of an item embedding $\mathbf{v}_j$ only if that item $v_j$ is in their private training datasets.
The expected number of such users at a round is $|{U}^r|\cdot (1-\tilde{p})\cdot p_j$, where $p_j$ denotes the probability that $v_j$ is contained within a benign user's private training dataset.
Considering this factor, the expected proportion of poisonous gradients of $\mathbf{v}_j$ received by the server is:
\begin{equation}
\label{eq:exp}
\begin{aligned}
\tilde{E}(v_j)
&=\frac{|{U}^r|\cdot\tilde{p}}{|{U}^r|\cdot (1-\tilde{p})\cdot p_j + |{U}^r|\cdot\tilde{p}},\\
&=\frac{\tilde{p}}{(1-\tilde{p})\cdot p_j+\tilde{p}} \,\,\, ,
\end{aligned}
\end{equation}
where for $p_j=1$ (all selected benign users contain $v_j$ in their training datasets), $\tilde{E}$ reaches its minimum value of $\tilde{p}$.
Indeed, the probability $p_j$ is given by
\begin{equation}\label{equ:p_j}
p_j=\frac{1}{|\bar{{U}}|} \sum\nolimits_{u_i\in\bar{{U}}}p_{{ij}},
\end{equation}
where $p_{{ij}}$ is the probability that the private training dataset ${D}_i$ of a benign user $u_i \in \bar{{U}}$ contains $v_j$.

Let $|{D}^{+}_i|$ and $|{D}^{-}_i|$ be the disjoint parts of ${D}_i$ containing interacted and uninteracted items for $u_i$, respectively.
%
We have
\begin{equation}
p_{{ij}}=\begin{cases}
\frac{|{D}_i^-|}{|{V}|-|{D}_i^+|} \Rightarrow \frac{q \cdot |{D}_i^+|}{|{V}|-|{D}_i^+|}, & \text{if~}v_j\notin{D}_i^+,\\
1, &\text{otherwise},
\end{cases}
\end{equation}
where ${V}$ denotes the whole item set and $q$ is the sampling ratio of  $|{D}^{+}_i|$ to $|{D}^{-}_i|$ (cf.\ \cref{ssec:FR-fram}).
Considering the small ratio $q$ (typically not exceeding $4$)\footnote{
Per the findings from previous studies \cite{NCF, fedrecattack, pipattack, a-hum}, a $q$ value of 4 or less is set to avoid performance drops in the RS. See the supplementary material~\cite{github-pieck-supple} for the evaluation of the varying $q$ on a real-world dataset.}
% Referring to \cref{fig:effect_of_q} in supplementary material, an empirical result is provided. 
and the fact that the number of a user's interacted items is usually significantly smaller than the total number of items (i.e., $|{D}_i^+| \ll |{V}|$), the value of $p_{{ij}}$ will be very small for the case that $v_j \notin {D}_i^+$.

Moreover, each target item $v_j \in {T}$ specified by the attacker is usually an extremely cold item. Hence, for most benign users, $v_j$ is not in ${D}_i^+$, making $p_j$ in \cref{equ:p_j} a small value.
Returning to \cref{eq:exp}, we observe that $\tilde{E}(v_j)$ increases when $p_j$ decreases. Therefore, the small value of $p_j$ for the target item $v_j$ results in $\tilde{E}(v_j)$ being significantly larger than $\tilde{p}$.

The fact that $\tilde{E}(v_j) \gg \tilde{p}$ is a critical challenge for existing defense methods\cite{NormBound,Media-TrimmedMean, Krum-MultiKrum, Bulyan} designed for conventional federated learning.
These methods rely on gradient normalization or filtering, assuming that the number of poisonous gradients must be lower than that of benign gradients.
For instance, consider the \MEDIAN{} approach \cite{Media-TrimmedMean}, which computes the median of received gradients for each dimension as the final normalized gradient and theoretically requires $\tilde{E}(v_j) < 0.5$.
However, if we assume $\tilde{E}(v_j) < 0.5$ according to \cref{eq:exp}, we deduce that $p_j > \frac{\tilde{p}}{1-\tilde{p}}$. Substituting the robust upper bound of the \MEDIAN{} approach (i.e., $\tilde{p}=0.5$) into the equation, we obtain $p_j > 1$. This contradicts the nature of any item in FRS, where $p_j \ll 1$, making the \MEDIAN{} approach ineffective.
Likewise, other methods do not escape this problem.

\subsection{New Design of the Defense Method}
\label{ssec:defense_design}

Our new design focuses on the following three key findings about the identified attack method \model{}:
\begin{enumerate}[leftmargin=*]
\item[F1] The long-tail item popularity leads to great and long-lasting changes in popular items' embeddings (cf.\ \cref{ssec:popular_items});
\item[F2] The inherent popularity bias causes most users to predict higher scores for popular items (cf.\ \cref{ssec:popular_enhancement});
\item[F3] The recommender symmetry makes close embedding distributions between popular items and users (cf.\ \cref{ssec:user_embed_app}).
\end{enumerate}

The finding F1 suggests setting a threshold to limit the magnitude of item embedding changes at consecutive rounds to combat the popular item mining from malicious clients.
%
However, two issues arise: (1) The difficulty of determining an appropriate threshold: A high threshold would significantly degrade the recommender system's performance, while a low threshold would fail to provide an effective defense.
(2) The method's invalidation in scenarios where item popularity has been publicly available: Malicious users can access popular items without even a mining process.

To address these issues, we introduce two additional regularization terms into the benign user training, which utilizes the findings F2 and F3. 
Regarding F2, the regularization term in \cref{regul1} is designed to induce confusion between the features of popular and unpopular items. This makes it challenging for malicious users to precisely capture the distinctive features of popular items and consequently, prevents them from counterfeiting target items as popular ones.
As for F3, the regularization term in \cref{regul2} aims to create significant separation in the embedding distribution of popular items and users. By doing so, it ensures that the user embeddings inferred from popular item embeddings are inherently inaccurate.

In our defense method, each benign user $u_i \in \bar{{U}}$ goes through the following steps:
\begin{enumerate}[leftmargin=*]

\item Mine popular item set ${P}_i$ by calling \cref{alg:popular_items};

\item Obtain a set of unpopular items $\Delta D_i = D_i \setminus P_i$.

\item Compute regularization term $\mathit{Re}_1$ as weighted mean pairwise cosine similarity between embeddings of $\Delta D_i$ and $P_i$:
\begin{equation} \label{regul1}
\mathit{Re}_1 = \frac{1}{|\Delta {D}_i|} \sum\limits_{v_j \in \Delta {D}_i} \sum_{v_k\in{P}_{i}} \kappa'(v_k) \cdot \text{cos}(\mathbf{v}_k,\mathbf{v}_j),
\end{equation}
where $\kappa'(v_k)$ is the normalized \emph{exponential}\footnote{We employed the exponential form to stipulate the term focus even more on the most popular items for improved defense effectiveness.} inverse rank of item $v_k$ in $P_i$.
Introducing $\mathit{Re}_1$ in the defense method aims to prevent item popularity enhancement in \modelI{}.

\item Compute regularization term $\mathit{Re}_2$ as average KL divergence between the distributions of user embedding and mined popular items' embeddings:
\begin{equation} \label{regul2}
\mathit{Re}_{2} =
 \sum\nolimits_{v_k\in{P}_{i}} \kappa'(v_k) \cdot \text{KL}(\mathbf{v}_k, \mathbf{u}_i).
\end{equation}
Introducing $\mathit{Re}_{2}$ aims to invalidate the user embedding approximation in \modelII{}. 

\item Participate in training using the combined defense loss:
\begin{equation} \label{defense-loss}
    \mathcal{L}^\text{def}_i = \mathcal{L}_i - \beta \cdot \mathit{Re}_{1} - \gamma \cdot \mathit{Re}_{2},
\end{equation}
where $\mathcal{L}_i$ is the original loss defined in \cref{equation:rs-loss}, and $\beta$ and $\gamma$ are non-negative trade-off parameters for the two regularization terms.
\end{enumerate}

