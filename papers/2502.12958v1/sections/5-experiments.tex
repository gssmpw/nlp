\section{Experiments}
\label{sec:experiments}
To verify our proposals, we conduct extensive experiments on both MF-FRS and DL-FRS, involving three real datasets, four attack methods, and six defense methods.
The entire codebase and instruction on hyperparameter tuning are all made available on Github~\cite{github-pieck}. 
% Notably, we focus on the following research questions:
% \begin{itemize}[leftmargin=*]
% \item \textbf{RQ1}: Is our attack general and effective against FRS, and will it reduce the accuracy of recommender systems?
% \item \textbf{RQ2}: Can \model{} bypass the server's defenses, and is our proposed defense method effective?
% \item \textbf{RQ3}: How do the key parameters affect our attack and defense?
% \end{itemize}

\subsection{Experiment Settings}
\label{ssec:settings}
\noindent \textbf{Data and Models}. We use three datasets for evaluations: {MovieLens-100K (ML-100K)} \cite{MovieLens}, {MovieLens-1M (ML-1M)} \cite{MovieLens}, and {Amazon Digital Music (AZ)} \cite{AZ}. 
To confirm the model-agnostic nature of \model{}, we adopt Matrix Factorization \cite{BPR} for MF-FRS and Neural Collaborative Filtering (NCF) \cite{NCF} for DL-FRS as the underlying base models.

\noindent \textbf{Baselines}.
Our model, \model{}, is benchmarked against leading model poisoning attacks such as \FRA{} \cite{fedrecattack}, \PIP{} \cite{pipattack}, \ARA{} \cite{a-hum}, and \AHUM{} \cite{a-hum}. To test \model{} and compare our proposed defense method, we apply established defense methods \NB{} \cite{NormBound}, \MEDIAN{} \cite{Media-TrimmedMean}, \TMEAN{} \cite{Media-TrimmedMean}, \KRUM{} \cite{Krum-MultiKrum}, \MKRUM{} \cite{Krum-MultiKrum} and \BULYAN{} \cite{Bulyan}.
% to the aggregation function $\operatorname{Agg}(\cdot)$.}

\noindent
\textbf{Metrics}. 
% In attacking and defending an FRS, both the \textbf{effectiveness of attack} and the \textbf{recommendation performance} are crucial considerations.
To assess the attack effectiveness in promoting target items, we utilize the \emph{Exposure Ratio at rank $K$} (ER@$K$) as defined in \cref{equation:er}. 
For fairness,  we follow \FRA{} \cite{fedrecattack} and randomly select target items to ${T}$ from the set of uninteracted items.
To measure the recommendation performance, we employ the \emph{Hit Ratio at rank $K$} (HR@$K$) following the NCF approach \cite{NCF}. 
% A higher HR@$K$ indicates more accurate recommendations of the FRS.
More comprehensive details on our setups can be found in the supplementary material~\cite{github-pieck-supple}.


% \subsubsection{Datasets} 
% \label{sssec:datasets}

% We use three datasets for evaluation, namely {MovieLens-100K (ML-100K)} \cite{MovieLens}, {MovieLens-1M (ML-1M)} \cite{MovieLens}, and {Amazon Digital Music (AZ)} \cite{AZ}.
% The statistics are shown in \cref{tab:data_sta}, where AZ uses feature a lower interaction rate compared to the other two while all datasets from real-world scenarios exhibit high sparsity.
% Such sparsity typically makes it challenging for FRS to accurately learn users' preferences and achieve effective recommendations.
% Following a previous study \cite{he16fast}, for each user, we adopt the leave-one-out method to create the training and test sets.

% \begin{table}[!htbp]
%     % \renewcommand{\arraystretch}{0.5}
%     \centering{
%     \caption{Dataset statistics (`Rate' equals $\text{\#(Interactions)} / $ $\text{\#(Users)}$ and `Sparsity' equals $1-\text{\#(Interactions)} / (\text{\#(Users)} \times \text{\#(Items)})$).}
%     \label{tab:data_sta}
%     % \setlength{\tabcolsep}{2.5mm}
%     \begin{tabular}{c|ccccc}
%     \toprule
%     {Dataset} & {\#(Users)}  & {\#(Items)}  & {\#(Interactions)} & {Rate} & {Sparsity} \\ \midrule
%     ML-100K  & 943    & 1,682  & 100,000      & 106  & 93.70\%  \\
%     ML-1M    & 6,040  & 3,706  & 1,000,209    & 166  & 95.53\%  \\
%     AZ       & 16,566 & 11,797 & 169,781      & 10   & 99.91\%  \\ \bottomrule
%     \end{tabular}
%     }
% \end{table}

% \subsubsection{System Settings} 
% To confirm the model-agnostic nature of \model{}, we adopt Matrix Factorization \cite{BPR} for MF-FRS and Neural Collaborative Filtering (NCF) \cite{NCF} for DL-FRS as the underlying base models. Despite the existence of various dedicated recommender models with intricate components, the above two models have gained widespread adoption in practice and have been extended to the federated setting~\cite{fedrecattack,pipattack,a-hum}. Their appeal lies in their ability to ensure high generalization and low communication overhead, making them highly representative choices for our evaluation.
% In our setting, each user in the dataset is regarded as a `client' in the federation. 
% The batch size of randomly selected users per round for MF-FRS is 256 on ML-100K and ML-1M datasets and 1024 on AZ. For DL-FRS, the batch size remains 256 for all datasets.
% We train MF-FRS and DL-FRS with learning rates $\eta=1.0$ and $\eta=0.005$, where corresponding model parameters are set the same as previous studies \cite{fedrecattack,a-hum}.
% Other parameter settings, related to the popular item mining algorithm and the defense loss can be found in our instruction~\cite{github-pieck}.

% \subsubsection{Attack Baselines}
% We compare \model{} with the following state-of-the-art model poisoning attacks:

% \begin{itemize}[leftmargin=*]

% \item \FRA{} \cite{fedrecattack} accesses a \emph{portion} of benign users' historical interactions to approximate their embeddings.

% \item \PIP{} \cite{pipattack} involves explicit promotion and popularity enhancement for target items.

% \item \ARA{} \cite{a-hum} randomly initializes users' embeddings at malicious users, specifically designed for DL-FRS.

% \item \AHUM{} \cite{a-hum} extends \ARA{} by enhancing the attack based on mining hard users for increased effectiveness.

% \end{itemize}

% For a fair comparison without utilizing prior knowledge, we mask the historical interactions for \FRA{} and popularity levels of items for \PIP{} in the implementation. Moreover, we set null parameters for \ARA{} when applying it to MF-FRS. 

% \subsubsection{Defense Methods}
% \label{sssec:defense_methods}

% To evaluate \model{} and compare our proposed defense method, we apply different defense methods to the aggregation function $\operatorname{Agg}(\cdot)$ on the server and tune them \emph{optimal}. They process the uploaded gradients as follows.

% \begin{itemize}[leftmargin=*]

% \item \NB{} \cite{NormBound}: A thresholding approach is employed to bound the $L_2$ Norm of all gradients uploaded by users.

% \item \MEDIAN{} \cite{Media-TrimmedMean}: The median of received gradients for each dimension is computed.

% \item \TMEAN{} \cite{Media-TrimmedMean}: The $\tilde{p}$ largest and smallest values for each dimension are removed, and the rest are averaged.

% \item \KRUM{} \cite{Krum-MultiKrum}: The most similar gradient from received gradients in the squared Euclidean norm space is selected. 

% \item \MKRUM{} \cite{Krum-MultiKrum}: The ($2\tilde{p}$) least similar gradients produced by \KRUM{} are removed iteratively with the rest averaged.

% \item \BULYAN{} \cite{Bulyan}: Gradients are selected by \MKRUM{} and then averaged using \TMEAN{}.

% \end{itemize}

% \subsubsection{Evaluation Metrics} \label{ssec:evaluation_metrics}
% In attacking and defending an FRS, both the \textbf{effectiveness of attack} and the \textbf{recommendation performance} are crucial considerations. An effective attack should successfully promote target items without significantly degrading the performance of the system to avoid detection.

% To assess the attack effectiveness in promoting target items, we utilize the \emph{Exposure Ratio at rank $K$} (ER@$K$) as defined in \cref{equation:er}. Attacks like \PIP{} and \AHUM{} specifically focus on the least popular items, which allows for more significant promotion opportunities since the server receives fewer benign gradients. However, for fairness, we follow \FRA{} \cite{fedrecattack} and randomly select target items to ${T}$ from the set of uninteracted items.
% To measure the recommendation performance, we employ the \emph{Hit Ratio at rank $K$} (HR@$K$) following the NCF approach \cite{NCF}. A higher HR@$K$ indicates more accurate recommendations of the FRS.
% For both ER@$K$ and HR@$K$, we select 5 random seeds and average their corresponding results for reliable measurement.

% \subsubsection{Parameter Settings}
% We adopt default hyperparameters as follows:
% In the attack, we have $|T|=1$, and $\tilde{R}=2$ for mining popular items. The malicious user ratio, $\tilde{p}$, is 5\% for both MF-FRS and DL-FRS.
% For \modelI{}, $N=10$ is used on all three datasets and two models. For \modelII{} on DL-FRS, $N=10$ is used, and on MF-FRS, $N$ is set to 50, 150, and 10 for ML-100K, ML-1M, and AZ, respectively.
% On both model types, if the malicious user ratio $p$ is less than 10\%, we set $\lambda=0.9$, otherwise $\lambda=0.5$.
% Regarding the defense, we set $\tau=1e-2$ for ML-100K and ML-1M datasets, $\tau=1e-3$ for the AZ dataset, and $\tau=0.5$ for DL-FRS.
% In our defense, we set $\beta$ and $\gamma$ to 1e+0 for ML-FRS, and $\beta$ and $\gamma$ to 4.5e+1 for DL-FRS.
% We conduct experiments on 8 NVIDIA RTX A5000 GPUs, and PyTorch is 1.8.0.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table*}[tbp]
\centering{
\caption{Comparison of all attack methods on the FRS with no defense (default malicious user ratio $\tilde{p}=5\%$).}
\label{tab:overall-attack}
\renewcommand{\arraystretch}{0.12}
\setlength{\tabcolsep}{1.8mm}
\begin{tabular}{c|cc|cc|cc|cc|cc|cc}
\toprule
 & \multicolumn{6}{c|}{MF-FRS} & \multicolumn{6}{c}{DL-FRS} \\
\cmidrule{2-7} \cmidrule{8-13}
Attacks & \multicolumn{2}{c|}{ML-100K} & \multicolumn{2}{c|}{ML-1M} & \multicolumn{2}{c|}{AZ} & \multicolumn{2}{c|}{ML-100K} & \multicolumn{2}{c|}{ML-1M} & \multicolumn{2}{c}{AZ} \\
\cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
 &ER@10 &  HR@10 & ER@10 & HR@10 & ER@10 &  HR@10 & ER@10 &  HR@10 & ER@10 &  HR@10 & ER@10 & HR@10 \\
\midrule
\rowcolor[HTML]{F2F2F2} \textsc{NoAttack} &0.23  &57.16 &0.00   &61.32 &0.09  &24.25 &0.00    &45.6 & 0.00 &  47.70 & 0.00 & 17.80 \\
\FRA{}                                    &0.23  &\underline{57.58} &0.00   &\textbf{61.37} &0.05  &\textbf{24.28}  &0.00   &\underline{45.28} &0.00 &\textbf{47.52} &0.00 &\underline{17.84}  \\
\PIP{}                                    &26.42 &56.95 &16.75  &61.04 &45.05 &\underline{24.24} &\textbf{100.00} &44.96 &\textbf{100.00} &  46.46 &\textbf{100.00} &17.81 \\
\ARA{}                                    &0.11  &56.95 &0.00   &61.18 &0.01  &24.18 &\textbf{100.00} &45.07 &\textbf{100.00} &  47.20 &\textbf{100.00} &17.80\\
\AHUM{}                                   &31.09 &57.05 &12.88  &61.11 &27.26 &24.05 &\textbf{100.00}  &45.07 &\textbf{100.00} &  \underline{47.33} &\textbf{100.00} & 17.77 \\
\modelI{} \textbf{(ours)} & \underline{87.47}  &\textbf{57.69} &\underline{90.47}  &\underline{61.21} &\underline{50.21}  &23.93 & \textbf{100.00}  &\textbf{45.39} & \textbf{100.00}  &  47.28 &\textbf{100.00}  & 17.80 \\
\modelII{} \textbf{(ours)} & \textbf{93.39} &\textbf{57.69} &\textbf{98.04} &61.03 &\textbf{72.11} &24.09 &\textbf{100.00} &  \textbf{45.39} &\textbf{100.00}  &  47.30 & \textbf{100.00} & \textbf{17.95} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[ht]
\centering{
\caption{Comparison of all defense methods combating the attacks on ML-100K (default malicious user ratio $\tilde{p}=5\%$).}
\label{tab:overall-defense}
\renewcommand{\arraystretch}{0.12}
\setlength{\tabcolsep}{1.8mm}
\begin{tabular}{c|cc|cc|cc|cc|cc|cc}
\toprule
& \multicolumn{6}{c|}{MF-FRS} & \multicolumn{6}{c}{DL-FRS} \\
\cmidrule{2-7} \cmidrule{8-13}
Defenses & \multicolumn{2}{c|}{\AHUM{}} & \multicolumn{2}{c|}{\modelI{}} & \multicolumn{2}{c|}{\modelII{}} & \multicolumn{2}{c|}{\AHUM{}} & \multicolumn{2}{c|}{\modelI{}} & \multicolumn{2}{c}{\modelII{}} \\
\cmidrule{2-3} \cmidrule{4-5} \cmidrule{6-7} \cmidrule{8-9} \cmidrule{10-11} \cmidrule{12-13}
& ER@10 & \multicolumn{1}{c|}{HR@10} & ER@10 & \multicolumn{1}{c|}{HR@10} & ER@10 & \multicolumn{1}{c|}{HR@10} & ER@10 & \multicolumn{1}{c|}{HR@10} & ER@10 & \multicolumn{1}{c|}{HR@10} & ER@10 & HR@10 \\
\midrule
\rowcolor[HTML]{F2F2F2} \NODEF    &31.09	&57.05	&87.47	&57.69	&93.39	&57.69 &100.00	&45.07	&100.00	&45.39	&100.00	&45.39\\
\NB{}     &\textbf{0.00}	&55.14	&\textbf{0.00}	&55.14	&\uline{0.11}	&55.25 &100.00	&\underline{45.17}	&100.00	&\textbf{45.60}	&100.00	&45.17\\
\MEDIAN{} &\textbf{0.00}	&54.83	&\textbf{0.00}	&54.72	&\textbf{0.00}	&54.93 &100.00	&43.90	&100.00	&44.86	&100.00	&44.11\\   
\TMEAN{}  &34.51*	&55.46	&81.78	&\textbf{56.73}	&48.52	&\textbf{55.89} &100.00	&\textbf{45.39}	&100.00	&45.28	&100.00	&\textbf{45.71}\\
\KRUM{}   &\textbf{0.00}	&51.22	&\textbf{0.00}	&51.22	&\textbf{0.00}	&51.33 &100.00	&43.58	&100.00	&43.16	&100.00	&44.11\\  
\MKRUM{}  &\underline{29.84}	&\uline{55.89}	&79.04	&55.89	&59.23	&\underline{55.67} &100.00	&44.96	&100.00	&\underline{45.49}	&100.00	&\underline{45.36}\\
\BULYAN{} &33.71*	&54.61	&49.43	&54.72	&79.04	&55.25 &100.00	&44.33	&100.00	&44.54	&100.00	&44.64\\ 
\textbf{ours} &\textbf{0.00} &\textbf{58.11} &\underline{1.25} &\underline{56.31} &\textbf{0.00} &\textbf{55.89} &\textbf{0.00} &43.58 &\textbf{0.00} &44.43 &\textbf{0.00} &43.69  \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Attack Performance}
We compare all considered attack methods on three datasets, two model types, and multiple baselines, with a default proportion $\tilde{p}$ of 5\% malicious users (the effect of varying $\tilde{p}$ is studied in \cref{exp:parameters_analysis}) and report the results in \cref{tab:overall-attack}.

\subsubsection{Attack Effectiveness}
\label{ssec:attack_effectiveness_comparison}
Overall, the ER@10 measures in \cref{tab:overall-attack} indicate that our proposed attacks (\modelI{} and \modelII{}) achieve highly competitive scores compared to all baselines across both model types and three datasets. 
The slightly lower measure on the AZ dataset is attributed to its larger item pool 
% (cf.\ \cref{tab:data_sta}) 
which makes it more challenging for the target item to be recommended; the results also show that most attacks are effective to ML-100K, probably because its user-item interactions are relatively condensed.

The inferior performance of \PIP{} and \FRA{} stems from their lack of prior knowledge about user interaction history and item popularity levels. This limitation hinders their ability to effectively enhance item popularity and approximate user embeddings, resulting in a failure to achieve the desired increase in the target item's exposure.
Besides, \ARA{} and \AHUM{} show significantly lower effectiveness in MF-FRS compared to DL-FRS. Their attacks are specifically designed to poison the learnable interaction parameters in DL-FRS, which renders them ineffective in MF-FRS where the interaction function is fixed as a dot product.
In contrast, our approach focuses on poisoning publicly available item embeddings common to both types of models, allowing us to achieve model-agnostic attacks.
Considering that \model{} applies to both FRS types and does not rely on prior knowledge, its remarkably high attack effectiveness raises concerns about its potential to cause significant harm to practical FRS.

% \vspace{-0.46mm}

Upon a detailed examination of the two \model{} variants, \modelII{} demonstrates relatively better performance. To understand the reasons behind this, we provide the complete convergence curves of \modelI{} and \modelII{} for MF-FRS on ML-1M in \cref{subfig:ipe_uea_compare_mf_ml-1m}. As observed, \modelI{} experiences a more significant decline in ER@10 compared to \modelII{} as the rounds increase. We attribute this behavior to the reduced effectiveness of popularity enhancement in \modelI{} as FRS converges towards personalized user preferences. In contrast, \modelII{} directly approximates the distribution of user embeddings through mined popular items, which elevates target item scores among all users, resulting in a more robust and effective attack.
However, this comes at the expense of higher computational overhead (cf.\ cost analysis in \cref{ssec:cost_analysis}).

% \vspace{-0.47mm}

The \textsc{NoAttack} method unexpectedly shows small ER@10 scores in two cases (0.23\% on ML-100K and 0.09\% on AZ). This is because our target items were randomly selected to ensure fair comparisons (cf. 
% \cref{ssec:evaluation_metrics}
\cref{ssec:settings}
) and are thus always recommendable.
Additionally, most ER@10 measures for DL-FRS are as high as 100\%, likely due to the susceptibility of its learnable interaction function to poisoning attacks.

\subsubsection{Recommendation Performance}
The HR@10 measures in \cref{tab:overall-attack} show that the recommendation performance on MF-FRS and DL-FRS remains largely unaffected by all attacks, including our proposed ones, across all three datasets. For instance, on ML-1M, the best-performing \modelII{} enhances its ER@10 by 98.04\% compared to the \textsc{NoAttack} scenario. Remarkably, the corresponding HR@10 experiences only a negligible decrease of 0.29\%.
Please note that malicious user behaviors injected by various attack methods introduce inherent randomness in FRS training, resulting in variations of HR@10 measures among the recommender models associated with different attacks. As a result, the HR@10 measures of \textsc{NoAttack} may not always be the highest.
However, it is essential to emphasize that these variations are almost negligible and do not affect our empirical finding that all attacks do not lead to a degradation of recommendation performance.

\subsection{Defense Performance}
\label{exp:defense_performance}
We evaluate the proposed defense method using the top-3 most effective attacks: \modelI{}, \modelII{}, and \AHUM{}. The evaluation primarily focuses on the ML-100K dataset, as these attacks demonstrate superior effectiveness on this dataset (cf. \cref{tab:overall-attack}). Similar trends are observed in other datasets, so their results are omitted. The results are listed in \cref{tab:overall-defense}.

\subsubsection{Attack Effectiveness}
The ER@10 measures in~\cref{tab:overall-defense} show that our proposed defense achieves the best score for both MF-FRS and DL-FRS compared to all competitor methods.
When applying our defense method, all attacks are ineffective on both types of models.
For example, compare to \textsc{NoDenfense}, for \modelII{}, we reduce ER@10 from 93.39\% to 0.00\% on MF-FRS and from 100\% to 0.00\% on DL-FRS, respectively.
Our defenses against \AHUM{} and \modelI{} show consistent performance, verifying the effectiveness of the proposed method.
Even though our defense does not include a specific regularization term for \AHUM{} explicitly, its employed combined loss function (cf.\ \cref{defense-loss}) incorporates both item embeddings and the interaction function. 
We contend that the \AHUM{} attack also estimates user embedding distributions by iteratively refining random initial user embeddings to identify those who rated the target item poorly. The $Re_2$ loss term is designed to specifically counteract this try of \AHUM{}. To summarize, our defense has proven universally effective, demonstrated by its success against three variants of FRS poisoning attacks, as outlined in \cref{fig:attacker}.
% Equally important, we argue that \textsc{A-hum} also approximates the distribution of user embeddings by iteratively updating randomly initialized user embeddings to mine the embeddings of users who gave low scores to the target item.
% This allows the $Re_2$ loss term to specifically prevent it.
% In summary, our effective defense against three types of FRS poisoning attacks (as shown in \cref{fig:attacker}) shows that our defense is universal.}
% As a result, it effectively withstands \AHUM{}.

Surprisingly, we observed that certain defense methods even lead to higher ER@10 measures, compared to \textsc{NoDenfense}, as marked with * in~\cref{tab:overall-defense}.
We propose that it happens because these defenses indiscriminately normalize and filter the gradients of each item.
For non-target items, the disposal of the gradient might diminish the user's score, hence inadvertently accentuating the target item.
For the target item, the benign gradient in the minority might be filtered out, hence amplifying the impact of the poisonous gradient.
Differently, our defense will not reduce the score of non-target items and filter out the benign gradient of the target item. 

\subsubsection{Recommendation Performance}
In \cref{tab:overall-defense}, our defense demonstrates only a minor drop in HR@10 measures for both types of FRS against the three attacks. This success stems from the trade-off between the original training loss and the added regularization terms in FRS training. Such an optimized, combined defense loss in \cref{defense-loss} effectively counters potential attacks from \modelI{} and \modelII{}, while still ensuring model accuracy.

\begin{figure}[tbp]
\centering
     \includegraphics[width=1.0\linewidth]{Figures/legend.pdf}%
     \vspace{-0.2cm}
     \subfigure[Attacks vs $\tilde{p}$.]{
     \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:attack_p_mf_ml-100k}
        \includegraphics[width=\linewidth]{Figures/attack_p_mf_ml-100k_v2.pdf}
    \end{minipage}
    }% 
     \subfigure[Defense vs $\tilde{p}$.]{
     \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:defense_p_mf_ml-100k}
        \includegraphics[width=\linewidth]{Figures/defense_p_mf_ml-100k_v2.pdf}
    \end{minipage}
    }% 
    \vspace{-0.2cm}
   \subfigure[Attacks vs $N$.]{
   \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:attack_n_mf_ml-100k}
        \includegraphics[width=\linewidth]{Figures/attack_n_mf_ml-100k_v2.pdf}
    \end{minipage}
    }%
   \subfigure[Defense vs $N$.]{
   \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:defense_n_mf_ml-100k}
        \includegraphics[width=\linewidth]{Figures/defense_n_mf_ml-100k_v2.pdf}
    \end{minipage}
    }%
    \caption{Effect of $\tilde{p}$ and $N$ on proposed attacks and defense. %for MF-FRS on the ML-100K dataset.
    }
    \label{fig:effect_of_malicious}
\end{figure}
\subsection{Parameters Analysis}
\label{exp:parameters_analysis}
We analyze the hyperparameters $\tilde{p}$, $N$, and $K$ in MF-FRS. MF-FRS is shown in \cref{tab:overall-attack} to be more robust against attacks compared to DL-FRS, making it suitable for discerning the characteristics of various attack and defense methods.


\subsubsection{Effect of malicious users' ratio \texorpdfstring{$\tilde{p}$}{Lg}}
Referring to the attack performance shown in \cref{subfig:attack_p_mf_ml-100k}, we observe that increasing $\tilde{p}$ results in a gradual rise in the attack effectiveness (ER@10) of \modelI{} and \modelII{}, while the recommendation performance (HR@10) remains relatively stable. Notably, \modelII{} outperforms \modelI{} by directly improving the target item's score on all benign users through user embedding approximation, achieving a more robust attack compared to using item popularity enhancement in \modelI{}.
 
In terms of defense performance (reported in \cref{subfig:defense_p_mf_ml-100k}), our method significantly reduces the harm caused by \modelI{} and \modelII{} under different $\tilde{p}$ values while maintaining recommendation performance comparable to the \textsc{NoAttack} scenario. This confirms the high effectiveness of our defense against both versions of \model{}.

\subsubsection{Effect of mined popular item number \texorpdfstring{$N$}{Lg}}
Regarding the attack performance (shown in \cref{subfig:attack_n_mf_ml-100k}), an $N$ value up to 10 yields high attack effectiveness (ER@10) for \modelI{} and \modelII{}, with minimal impact on recommendation performance. However, beyond certain thresholds, specifically, $N=50$ for \modelI{} and $N=250$ for \modelII{}, the attack's effectiveness is significantly affected. This occurs because, at these points, the mined popular items may also include some unpopular ones, hindering the capture of popular item features and user embedding approximation. In conclusion, the results suggest that an appropriate number of mined popular items can effectively enhance the attack strategies employed in both \modelI{} and \modelII{}.

Regarding the defense performance (shown in \cref{subfig:defense_n_mf_ml-100k}), a moderate number of mined popular items ($N=10$) effectively reduces the attack effectiveness while maintaining decent recommendation performance. However, using a large $N$ not only reduces defense effectiveness but also decreases recommendation performance. This happens because the defense method with a large $N$ may focus on constraining relatively unpopular items, which are not targeted by malicious users but instead negatively impact recommendation performance.

\subsubsection{Effect of the number \texorpdfstring{$K$}{Lg} of recommended items}
Referring to \cref{tab:attack_k}, when $K$ varies from 5 to 20, both variants of \model{} demonstrate effective attacks when our defense is not deployed (\modelI{}/\modelII{} vs \textsc{NoDefense}). However, once the defense is deployed, it successfully counters \model{} under different values of $K$ (\modelI{}/\modelII{} vs \textbf{ours}). Additionally, both our proposed attack and defense have minimal impact on recommendation performance (HR@$K$). These results validate the robustness and insensitivity of our attacks and defense to changes in $K$.

\begin{table}[tb]
\centering{
\caption{Effect of $K$ on proposed attacks and defense.}
\label{tab:attack_k}
\renewcommand{\arraystretch}{0.7}
\setlength{\tabcolsep}{2mm}
\begin{tabular}{c|c|cc|cc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses}& \multicolumn{4}{c}{Performance (MF-FRS and ML-100K)}\\
\cmidrule{3-6} &
 &ER@5 &  HR@5 & ER@20 &  HR@20  \\
\midrule
\rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense} &0.00  &38.92  &0.46	  &76.78\\
\modelI{}          &\textsc{NoDefense} &\underline{85.99} &\underline{39.34}  &\underline{90.21}  &\underline{76.88}\\
\modelI{}          &\textbf{ours}      &0.46  &\underline{39.34}  &6.95	  &\textbf{77.52}\\
\modelII{}         &\textsc{NoDefense} &\textbf{93.62} &38.71  &\textbf{93.51}  &\textbf{77.52}\\
\modelII{}         &\textbf{ours}      &0.00  &\textbf{39.66}  &0.68	  &75.93\\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Ablation Study} \label{ssec:ablation_study}

First, we examine the efficacy of techniques used in $\mathcal{L}_\text{IPE}$ (cf.\ \cref{pieckipe_loss} for \modelI{}), namely the average pairwise cosine similarity (PCOS for short) metric, the weighted term $\kappa(\cdot)$, and the subset partitioning policy (${P}^\text{+/-}$). We do not explore \modelII{} in this context since it does not involve complex techniques like those in \modelI{}.

Referring to the results in \cref{tab:abla-combined} (left), we find that the PCOS metric outperforms another widely-used metric PKL (cf.\ \cref{mean—pair-kld}) in terms of attack effectiveness, attributing to its better capture of latent features in their respective embeddings. 
The use of the weighted term ($\kappa(\cdot)$) further improves attack effectiveness by assigning larger weights to learn from relatively more popular items. 
Finally, employing the subset partitioning policy (${P}^\text{+/-}$) results in \modelI{} achieving decent performance, emphasizing the necessity of capturing shared features among different types of popular items.

Next, we analyze the efficacy of two regularization terms ($\mathit{Re}_{1}$ and $\mathit{Re}_{2}$) in the defense loss $\mathcal{L}^\text{def}$ in \cref{defense-loss}. 
The ER@10 measures in \cref{tab:abla-combined} (right) indicate that using both regularization terms jointly yields favorable defense performance. This highlights the importance of finely tuning item and user embeddings while concurrently constraining item popularity enhancement and user embedding approximation for effective defense.
In contrast, using only one regularization term either harms recommendation performance ($\mathit{Re}_1$ only) or fails to provide sufficient defense effectiveness ($\mathit{Re}_2$ only). 

\begin{table}[tb]
    \centering
    \caption{Ablations of $\mathcal{L}_\text{IPE}$ \& $\mathcal{L}^\text{def}$ of MF-FRS on ML-100K.}
    \renewcommand{\arraystretch}{0.7}
    \setlength{\tabcolsep}{0.32mm}
    \begin{tabular}{c|cc|cc||cc|cc|cc}
        \toprule
        \multicolumn{3}{c|}{$\mathcal{L}_\text{IPE}$} &\multicolumn{2}{c||}{$\modelI{}$} & \multicolumn{2}{c|}{$\mathcal{L}^\text{def}$}  & \multicolumn{2}{c|}{\modelI{}} & \multicolumn{2}{c}{\modelII{}} \\
        \midrule
        Metric  & $\kappa(\cdot)$ & ${P}^\text{+/-}$ & ER@10 &HR@10 & $\mathit{Re}_1$ & $\mathit{Re}_2$ & ER@10 &HR@10 & ER@10 &HR@10 \\
        \midrule
        \rowcolor[HTML]{F2F2F2} PKL & & &39.75 &57.48 & & &87.47 &57.69 &93.39 &57.69\\
        & & &46.01 &56.95  &+ & &\underline{26.2} &55.46 &\textbf{0.00} &54.19 \\
        PCOS&+ & &\underline{71.41} &\underline{57.58} & &+  &49.2 &\textbf{56.63} &\underline{69.36} &\textbf{56.73}\\
        &+ &+ &\textbf{87.47} &\textbf{57.69} &+ &+ 
    &\textbf{1.25} &\underline{56.31} &\textbf{0.00} &\underline{55.89} \\
        \bottomrule
    \end{tabular}
    \label{tab:abla-combined}
\end{table}

\begin{figure}[tbp]
\centering
     \subfigure[Performance trends on MF-FRS.]{
     \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:ipe_uea_compare_mf_ml-1m}
        \includegraphics[width=\linewidth]{Figures/ipe_uea_compare_mf_ml-1m_p5.pdf}
    \end{minipage}
    }%
     \subfigure[Cost analysis on MF/DL-FRS.]{
     \centering
    \begin{minipage}{0.48\linewidth}
        \label{subfig:cost_mf_dl_ml-1m}
        \includegraphics[width=\linewidth]{Figures/time.pdf}
    \end{minipage}
    }%
    \caption{Performance trends and cost analysis on ML-1M.}
    \label{fig:trends_cost}
\end{figure}

\subsection{Cost Analysis}
\label{ssec:cost_analysis}
We conduct 500 rounds of evaluation for the proposed attacks and defense and report the average time cost per training round in \cref{subfig:cost_mf_dl_ml-1m}. DL-FRS requires more time compared to MF-FRS due to the usage of a learnable interaction function.
Regarding the attack, we observe a negligible time increase for both 
MF-FRS and DL-FRS compared to the vanilla scenario (denoted as No(Att.\& Def.)). 
This is because the three \model{} modules can be efficiently executed via matrix parallel.
% , regardless of the number of mined popular items $N$.
% However, a small $N$ is recommended, as a larger $N$ may reduce the attack's effectiveness (cf.\ \cref{subfig:attack_n_mf_ml-100k}).
\cref{subfig:cost_mf_dl_ml-1m} also shows that \modelII{} exhibits a bit higher time cost than \modelI{}. 
This discrepancy arises from \modelII{} requiring mining more popular items, and implementing multiple rounds in batches (default batch size is 5 and round size is 3) to improve the score of the target item on approximated user embeddings.
% This is because the three \model{} modules can be efficiently executed via matrix parallel. Additionally, using a small number of popular items ($N=10$ for \modelI{} and $N=50$ for \modelII{}) suffices, while a larger $N$ intuitively incurs more computational overhead but in turn reduces attack effectiveness .
% \cref{subfig:cost_mf_dl_ml-1m} also reveals that \modelII{} exhibits higher time cost than \modelI{} as its strategy entails more popular items to achieve higher attack effectiveness.
% In cases of limited resources for malicious users, \modelI{} is preferred.
% For defense, the average time increase per user per round for MF-FRS and DL-FRS is only 1.84 and 4.41 milliseconds, respectively. 
For defense, the average time increase per round for MF-FRS and DL-FRS is only 0.47 and 1.13 seconds, respectively, compared to No(Att.\& Def.).
These slight increments are acceptable for maintaining FRS security. Overall, our attacks and defense are efficient and effective.

\subsection{Discussion on System Settings}
\label{ssec:system_setting_discussion}

% \begin{table}[tb]
% \centering{
% \caption{\changeone{Effect of $q$ and $|T|$ on proposed attacks and defense.}}
% \label{tab:other_effects}
% \renewcommand{\arraystretch}{0.7}
% \setlength{\tabcolsep}{1.8mm}
% \begin{tabular}{c|c|cc|cc}
% \toprule
% \multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses} & \multicolumn{2}{c|}{$q=10$} &\multicolumn{2}{c}{$|T|=3$}\\
% \cmidrule{3-6} &
%  &ER@10 &  HR@10 & ER@10 &  HR@10  \\
% \midrule
% \rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense}     &0.00	            &57.26     &0.23  &57.16      \\
% \modelI{}                                  &\textsc{NoDefense}     &\underline{30.75}   &\underline{57.58}   &\underline{57.09} &57.48\\
% \modelI{}                                  &\textbf{ours}          &1.03	            &55.99     &0.46  &\textbf{58.01}                               \\
% \modelII{}                                 &\textsc{NoDefense}     &\textbf{89.86}      &\textbf{57.79} &\textbf{87.33} &57.05           \\
% \modelII{}                                 &\textbf{ours}          &0.68	            &54.51 &0.00  &\underline{57.69}                      \\
% \bottomrule
% \end{tabular}}
% \end{table}
\begin{table}[tb]
\centering{
\caption{Effect of $q$ and $|T|$ on proposed attacks and defense.}
\label{tab:other_effects}
\renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{1.8mm}
\begin{tabular}{c|c|cc|cc}
\toprule
\multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses} & \multicolumn{2}{c|}{$q=10$} &\multicolumn{2}{c}{$|T|=3$}\\
\cmidrule{3-6} &
 &ER@10 &  HR@10 & ER@10 &  HR@10  \\
\midrule
\rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense}     &0.00	            &57.26                &0.23  &57.16         \\
\modelI{}                                  &\textsc{NoDefense}     &\underline{30.75}   &\underline{57.58}    &\underline{59.16} &57.26    \\
\modelI{}                                  &\textbf{ours}          &1.03	            &55.99                &0.21  &57.58           \\
\modelII{}                                 &\textsc{NoDefense}     &\textbf{89.86}      &\textbf{57.79}       &\textbf{93.93} &\underline{57.90}  \\
\modelII{}                                 &\textbf{ours}          &0.68	            &54.51                &0.50  &\textbf{58.43}             \\
\bottomrule
\end{tabular}}
\end{table}

\subsubsection{Large sample ratio}
We scrutinize how our attack and defense approaches perform when the FRS uses an extremely large sample ratio $q$ under MF-FRS and ML-100K.
Empirical evidence, as reported in the supplementary material~\cite{github-pieck-supple}, justifies setting $q=10$ since FRS performance markedly declines beyond this point.
% We slightly tune $N$ to achieve optimal performance, $N=10$ for \modelI{} and $N=15$ for \modelII{}.
The results in~\cref{tab:other_effects} confirm that both the proposed attack ($N$=$10$ for \modelI{} and $N$=$15$ for \modelII{}) and defense methods sustain effectiveness at this elevated $q$ level without diminishing FRS performance.
% The ER@$10$ reduction of \modelI{} is due to the user's training bias towards uninteracted items, causing the gradient updates of popular items to be diluted.
The ER@10 of \modelI{} is suboptimal, which we attribute to the user's training bias towards uninteracted items, resulting in gradient updates of popular items (most are interacted items) being diluted. This, however, does not impact \modelII{}.

% Nevertheless, as popular item embeddings are still trained alongside most user embeddings, they can effectively approximate user embeddings, thereby ensuring the stable effect of \modelII{}.

% Since the recommended performance begins to gradually decline after $q \geq 11$ (can be found in \cref{fig:effect_of_q} of our supplementary material), we set the key point $q = 10$ to evaluate the performance of our attack and defense at a large sampling rate.
% \cref{tab:other_effects} shows that our attacks and defenses remain effective even for a large $q$ without compromising RS performance.

% Without attack and defense, the HR@10 in \cref{tab:overall-attack} ($q$=1) is 57.16. However, when $q=11$, empirically, the RS performance drops to 57.05; further increasing to 15, and HR@10 drops significantly to 53.24, which indicates the unreliable nature of RS, which makes it unattractive for attackers.
% Therefore, we evaluate our attacks and defensive performance at the key turning point of RS performance, that is, $q=10$.
% \cref{tab:other_effects} shows that our attacks and defenses remain effective even for relatively large values of $q$ without compromising RS performance.
% }

\subsubsection{Multiple target items}

Multi-target scenarios might be interesting in practice. In the supplementary material~\cite{github-pieck-supple}, we explore how varying $|T|$ values and different attack strategies affect our proposals.
The findings suggest that training just one target item and uploading $|T|$ copies to the server is quite effective.
This technique alleviates the interference of multiple target item updates and makes optimization simple. Moreover, it involves no extra training or a vast number of malicious clients.
As a result, we have employed this simple and cost-effective strategy to assess our attack and defense methods.
\cref{tab:other_effects} reports the system performance on the MF-FRS+ML-100K scenario with $|T|=3$ and malicious client user fixed to 5\%.
As can be seen, our attack achieves a high ER@$10$ even with multiple targets, and our defense remains very strong when compared to the default configuration of $|T|=1$.
For primary experiments, we set $|T|=1$, aiming for easy controls of parameters such as $N$, $\tilde{p}$, $\beta$, $\gamma$ in evaluations, which also aligns with existing attack methods~\cite{a-hum,fedrecattack,pipattack}.
% We discuss the impact of different $|T|$ values and attack strategies on our proposals in the supplementary material~\cite{github-pieck-supple}.
% The results indicate that only training one target item, but uploading $|T|$ copies of its poisoned gradient to the server maintains good performance. This is because it alleviates the interference of multiple target item updates and makes optimization simple.
% Furthermore, this strategy does not require additional training and a large malicious user set. 
% Therefore, here we adopt this simple yet cost-effective strategy to evaluate the performance of attack and defense with multiple target items involved ($|T|=3$) and a fixed client ratio of 5\% on MF-FRS and ML-100K.
% \cref{tab:other_effects} (right) prove that our attack can still achieve a high ER@$10$, and our defense is very robust, compared to $|T|=1$ (default setting).
% The initial decision to use a single target item ($|T|=1$) was to simplify hyperparameter tuning ($N$, $\tilde{p}$, $\beta$, $\gamma$, etc.) in our attacks and the defense, which also aligns with existing attack~\cite{a-hum,fedrecattack,pipattack}.}

% \changethree{\rthree{R3.O1}{}Fixing malicious client fraction $\tilde{p}=5\%$ for MF-FRS and ML-100K, we evaluate the performance of attack and defense with multiple target items involved ($|T|=3$). Detailed results for $|T|=2, 3, 4, 5$ are reported elsewhere~\cite{github-pieck-supple}.
% \cref{tab:other_effects} (right) prove that our attack can still achieve a high ER@$10$, and our defense is very robust, compared to $|T|=1$ (default setting).
% This is due to our strategy of optimizing only one target items and uploading $|T|$ copies of its poison gradient to the server, which allows the optimization process to not be disturbed by updates to multiple target items.
% Furthermore, this strategy is simple and cost-effective because it eliminates the need for additional training or a large malicious user set.
% Notably, our choice of $|T|=1$ is to simplify hyperparameter tuning ($N$, $\tilde{p}$, $\beta$, $\gamma$, etc.) in our attacks and the defense, which also aligns with existing attack~\cite{a-hum,fedrecattack,pipattack}.}

% Compared to $|T|=1$ (the default setting), the slight performance degradation is due to the simultaneous poisoning of multiple target items by malicious users. This interference between updates to different target items makes optimizing multiple poisonous target item gradients more challenging.
% To mitigate this problem, 
% an alternative, simple yet cost-effective strategy is to optimize only one target item while uploading $|T|$ copies of its poisoned gradient to the server. This strategy eliminates the need for additional training or a larger malicious user set, and its effectiveness for large $|T|$ has been validated~\cite{github-pieck-supple}. 
% Notably, our choice of $|T|=1$ is to simplify hyperparameter tuning ($N$, $\tilde{p}$, $\beta$, $\gamma$, etc.) in our attacks and the defense, which also aligns with existing attack~\cite{a-hum,fedrecattack,pipattack}.}
% attackers can design more sophisticated strategies to deal with larger $|T|$. 
% One potential approach is to divide malicious users into $|T|$ groups and optimize different target items within each group, albeit this may require a larger set of malicious users.
% ($P=5$ for \modelI{} and $P=50$ for \modelII{} )
% Notably, attackers may employ more sophisticated strategies to handle larger $|T|$ to achieve superior results, such as directing 1/3 of malicious users to poison one of the three target items.}
% \changethree{In theory, while maintaining the same level of attack effectiveness, the number of malicious users is directly proportional to the number of target items.
% For instance, the attack effectiveness achieved with $|T|=2$ and $\tilde{p} = 1\%$ is equivalent to that under $|T| = 1$ and $\tilde{p} = 0.5\%$. 
% This is due to the attacker's ability to control 0.5\% of malicious users under $|T|=2$ to manipulate the ER@$K$ for different target items.
% Therefore, we choose $|T| = 1$ to demonstrate the attack effectiveness on a wider range of target items by varying the proportion of malicious users.}
% , which will better fit the larger $|T|$.}

% \subsection{Other effects}
% \begin{table*}[tb]
% \centering{
% \caption{Effect of sample ratio $q$, \# of target item $T$, type of loss function $\mathcal{L}_i$ on proposed attacks and defense.}
% \label{tab:other_effects}
% \renewcommand{\arraystretch}{0.7}
% \setlength{\tabcolsep}{2mm}
% \begin{tabular}{c|c|cc|cc|cc|cc}
% \toprule
% \multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses}& \multicolumn{2}{c|}{$q=1$} & \multicolumn{2}{c|}{$q=15$} &\multicolumn{2}{c|}{T=3} & \multicolumn{2}{c}{$\mathcal{L}_i$ = `BPR'}\\
% \cmidrule{3-10} &
%  &ER@10 &  HR@10 & ER@10 &  HR@10 & ER@10 &  HR@10 & ER@10 &  HR@10  \\
% \midrule
% \rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense} &0.23              &57.16             &0.00	            &\textcolor{red}{53.34}     &0.11  &57.05              &0.00	            &57.16\\
% \modelI{}                                  &\textsc{NoDefense} &\underline{87.47} &\textbf{57.69}    &\underline{21.18} &\underline{54.61}   &\underline{55.66} &\textbf{57.37}&\underline{83.26} &\textbf{57.26}\\
% \modelI{}                                  &\textbf{ours}      &1.25              &56.31             &1.03	            &55.99 &0.65  &55.57                                   &0.00	            &54.29\\
% \modelII{}                                 &\textsc{NoDefense} &\textbf{93.39}    &\underline{57.69} &\textbf{50.68}    &53.45 &\textbf{86.34} &\underline{56.73}              &\textbf{92.94}    &\underline{56.95}\\
% \modelII{}                                 &\textbf{ours}      &0.00              &55.89             &0.00	            &\textbf{54.72} &0.39  &55.46                          &3.64	            &53.76\\
% \bottomrule
% \end{tabular}}
% \end{table*}


% \subsection{Effect of $q$}
% \label{ssec:effec_of_q}

% \subsection{Effect of $q$}
% \label{ssec:effec_of_q}
% \begin{table}[tb]
% \centering{
% \caption{Effect of sample ratio on proposed attacks and defense.}
% \label{tab:abla_sampleratio}
% \renewcommand{\arraystretch}{0.7}
% \setlength{\tabcolsep}{2mm}
% \begin{tabular}{c|c|cc|cc}
% \toprule
% \multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses}& \multicolumn{2}{c|}{$q=1$} & \multicolumn{2}{c}{$q=15$}\\
% \cmidrule{3-6} &
%  &ER@10 &  HR@10 & ER@10 &  HR@10  \\
% \midrule
% \rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense} &0.23              &57.16             &0.00	            &\textcolor{red}{53.34}\\
% \modelI{}                                  &\textsc{NoDefense} &\underline{87.47} &\textbf{57.69}    &\underline{21.18} &\underline{54.61}\\
% \modelI{}                                  &\textbf{ours}      &1.25              &56.31             &1.03	            &55.99\\
% \modelII{}                                 &\textsc{NoDefense} &\textbf{93.39}    &\underline{57.69} &\textbf{50.68}    &53.45\\
% \modelII{}                                 &\textbf{ours}      &0.00              &55.89             &0.00	            &\textbf{54.72}\\
% \bottomrule
% \end{tabular}}
% \end{table}

% \subsection{Effect of \# of target item}
% \label{ssec:effec_of_targetsize}

% \begin{table}[tb]
% \centering{
% \caption{Effect of \# of target item on our attacks and defense.}
% \label{tab:abla_itemsize}
% \renewcommand{\arraystretch}{0.7}
% \setlength{\tabcolsep}{2mm}
% \begin{tabular}{c|c|cc|cc}
% \toprule
% \multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses}& \multicolumn{2}{c|}{\#Target Item =3 } & \multicolumn{2}{c}{\#Target Item = 5 }\\
% \cmidrule{3-6} &
%  &ER@10 &  HR@10 & ER@10 &  HR@10  \\
% \midrule
% \rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense} &0.11  &57.05  &0.11	  &57.05\\
% \modelI{}              &\textsc{NoDefense}  &\underline{55.66} &\textbf{57.37}  &\underline{40.47}  &\underline{56.73}\\
% \modelI{}              &\textbf{ours}      &0.65  &55.57  &0.05	  &56.1\\
% \modelII{}             &\textsc{NoDefense} &\textbf{86.34} &\underline{56.73}  &\textbf{74.13}  &55.99\\
% \modelII{}            &\textbf{ours}      &0.39  &55.46  &0.16	  &\textbf{57.48}\\
% \bottomrule
% \end{tabular}}
% \end{table}

% \subsection{Effect of loss function}
% \label{ssec:effec_of_loss}

% \begin{table}[tb]
% \centering{
% \caption{Effect of loss function on proposed attacks and defense.}
% \label{tab:abla_loss}
% \renewcommand{\arraystretch}{0.7}
% \setlength{\tabcolsep}{2mm}
% \begin{tabular}{c|c|cc|cc}
% \toprule
% \multirow{2}{*}{Attacks} & \multirow{2}{*}{Defenses}& \multicolumn{2}{c|}{BCE} & \multicolumn{2}{c}{BPR}\\
% \cmidrule{3-6} &
%  &ER@10 &  HR@10 & ER@10 &  HR@10  \\
% \midrule
% \rowcolor[HTML]{F2F2F2} \textsc{NoAttack}  &\textsc{NoDefense} &0.23              &57.16             &0.00	            &57.16\\
% \modelI{}                                  &\textsc{NoDefense} &\underline{87.47} &\textbf{57.69}    &\underline{83.26} &\textbf{57.26}\\
% \modelI{}                                  &\textbf{ours}      &1.25              &56.31             &0.00	            &54.29\\
% \modelII{}                                 &\textsc{NoDefense} &\textbf{93.39}    &\underline{57.69} &\textbf{92.94}    &\underline{56.95}\\
% \modelII{}                                 &\textbf{ours}      &0.00              &55.89             &3.64	            &53.76\\
% \bottomrule
% \end{tabular}}
% \end{table}




