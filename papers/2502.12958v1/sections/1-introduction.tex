\section{Introduction}
\label{sec:intro}

Growing concerns about privacy protection have sparked the advent of federated recommendation as a novel paradigm for creating personalized recommender models across distributed clients \cite{MintoHLH21,FedRec++,WangYCYZZ22,FedFast}.
In Federal Recommendation Systems (FRS), the model is bifurcated: the non-shareable client component, housing the user's embedding\footnote{A term used synonymously with `embedding vectors' in this paper.}, a.k.a. sensitive personalized data; and the shareable component interchanges the public items' embeddings with the server, along with an \emph{interaction function} designed to predict user-item pairs.
Nevertheless, FRS remains susceptible to \textbf{targeted poisoning attacks} \cite{fedrecattack,pipattack,a-hum}, which use malicious clients to poison the shared model by uploading manipulated gradients. These attacks aim to boost the exposure of selected items in users' recommendation lists. Although these tactics may seem negligible due to the minor impact on model performance, they inflict more harm on users than those untargeted attacks (only aiming for performance degradation) \cite{FL-Defender,CONTRA,FungYB20}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{Figures/Attacker.pdf}
    \caption{Targeted model poisoning attacks against FRS.}
    \label{fig:attacker}
\end{figure}

\cref{fig:attacker} provides an overall picture of targeted poisoning attacks. 
Two main types of FRS exist: matrix factorization-based FRS (MF-FRS) and deep learning-based FRS (DL-FRS).
Interaction function poisoning~\cite{a-hum} based attacks exclusively target DL-FRS with an assumption that a learnable interaction function exists\footnote{As to be presented in \cref{ssec:preliminaries}, MF-FRS do not use learnable functions but instead a fixed dot product.}. While the other attacks based on user embedding approximation~\cite{fedrecattack} or item popularity enhancement~\cite{pipattack} are model-agnostic, they assume prior knowledge of historical user-item interaction data or item popularity levels. These assumptions, practically inapplicable (no sense to provide such information to attackers), do not lend the corresponding attacks to real-world applications. 
We defer detailed discussions on these attacks to \cref{sec:related}.
This prompts our exploration into \emph{whether a model-agnostic, prior-knowledge-free attack could pose potential risks to realistic FRS and if a proactive defense method could be implemented to combat such threats?}

The development of a model-agnostic attack requires generating poisonous gradients related to item embeddings rather than the interaction function, as the latter is only applicable to DL-FRS. Additionally, the attack should mine knowledge from within the federation due to the unavailability of prior knowledge for poisoning. 
Keeping these points in mind, we identify an effective approach to mining popular items as usable information during FRS training. The mined popular items prove effective in deriving poisonous item embedding gradients, culminating in our attack solution, \model{} (Popular Item Embedding based attaCK).
In particular, \model{}'s core module, named \textbf{popular item mining} (cf.\ \cref{ssec:popular_items}), is driven by a unique characteristic we found --- popular items typically have more significant and long-lasting changes in their embeddings during FRS training.
The mined popular items renovate existing attack methods in a piror-knowledge-free paradigm.
%
We first propose the \textbf{item popularity enhancement} module (cf.\ \cref{ssec:popular_enhancement}), which aligns the embeddings of target items with those of mined popular items, thus increasing the exposure of target items. The resulting solution is coined \modelI{}.
However, \modelI{} may degrade while the FRS converges towards prominent personalization for each client.
We then propose \modelII{}, which substitutes the item popularity enhancement module with a \textbf{user embedding approximation} module (cf.\ \cref{ssec:user_embed_app}). This module approximates private user embeddings with mined popular item embeddings to derive poisonous item gradients, reflecting our discovery that popular items and most users share a close embedding distribution within the symmetric FRS model.
Notedly, \modelII{} can achieve higher attack effectiveness than \modelI{}. 
However, \modelI{} fewer resources in computations, per our cost analysis in \cref{ssec:cost_analysis}.
% However, \modelI{} requires fewer resources in practice, per our cost analysis in \cref{ssec:cost_analysis}.}

In response to the identified attack, we evaluate the efficacy of existing defense methods~\cite{NormBound,Media-TrimmedMean,Krum-MultiKrum,Bulyan} in federated learning. We identify these defenses' limitations, as they require normal gradients to outnumber poisonous ones for a specific item --- an ineffective strategy against \model{}, where poisonous gradients commonly overwhelm a target item. An empirical validation is provided in \cref{exp:defense_performance}.
Following this, we propose a new design, introducing two regularization terms into normal client training to counteract potentially poisonous gradients from the item popularity enhancement and user embedding approximation. We guarantee that this method preserves recommendation performance by incorporating the original training loss.
Extensive experiments across MF-FRS and DL-FRS, three real-world datasets, four top-tier attacks, and six general defense methods, affirm the effectiveness and efficiency of both \model{} and its defense.

In summary, our primary contributions are:
\begin{itemize}[leftmargin=*]
\item We propose \model{}, a model-agnostic and prior-knowledge-free attack against FRS. \model{} lies on automatically mining popular items during training, and branches to two diverse versions that rely on item popularity enhancement and user embedding approximation, respectively (\cref{sec:attack}).
\item We examine the inadequacy of existing federated defense methods via a theoretical analysis, and design a new defense method that is specifically designed for hazardous model poisoning attacks in FRS (\cref{sec:defense}).
\item We conduct extensive experiments across different models, datasets, attack and defense methods, demonstrating the effectiveness and efficiency of our proposals (\cref{sec:experiments}).
\end{itemize}
