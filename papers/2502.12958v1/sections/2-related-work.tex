\section{Related Work} \label{sec:related}

\noindent\textbf{Federated Recommendations}.
In the past decades, recommender systems~\cite{NCF,NGCF,LightGCN,VBPR,CKE,MMGCN} have found widespread applications in facilitating people's discovery of desired information in various domains, ranging from movies on Netflix~\cite{netflix} and videos on YouTube~\cite{youtube}, to products on Taobao~\cite{taobao}.
%
Typically, a recommender system collects users' historical interactions to train a model, which is subsequently utilized to predict scores, usually ranging from $[0,1]$, for items that a user has not yet interacted with, indicating the user's preferences for those corresponding items.

Motivated by the growing emphasis on user privacy, \emph{federated recommender systems} (FRS) have gained significantattention~\cite{FedGNN,FedRec,FedRec++,ammad2019federated,FedFast,Wu0HNWCYZ21,MintoHLH21}.
Specifically, an FRS splits the model into two parts, namely the client-resident \textbf{personalized model} that retains the sensitive user information and historical interactions, and the \textbf{global model} that exchanges the public item information and gradients of publicly available parameters with the federation via a server.

Two main types of FRS are matrix factorization-based FRS (MF-FRS)~\cite{ammad2019federated,FedRec,FedRec++,9162459} and deep learning-based FRS (DL-FRS)~\cite{WangYCYZZ22,perifanis2022federated,jiang2022fedncf}.
%
In MF-FRS, users and items are represented as embedding vectors (or simply, embeddings).
The score between a user and an item is computed using a fixed \emph{interaction function} (e.g., dot product) based on their embeddings. 
The global and personalized models encompass separate embeddings for items and users.
%
DL-FRS follows a similar approach to handle item and user embeddings.
However, in DL-FRS, the interaction function is learnable through deep neural networks. 
Consequently, its global model additionally encompasses learnable parameters of the interaction function, enabling collaborative training with other clients.

\smallskip
\noindent\textbf{Attack Methods against FRS}. 
Many attacks~\cite{Li16Data,HuangMGL0X21,Fang18Po,Fang20In,kapoor2017review} have been proposed against conventional recommender systems.
These attacks involve manipulating user behavior to interact with specific items, thereby poisoning the interaction data.
When a recommender model is trained on poisonous data, it tends to assign abnormally high scores to the target items, leading to their exposure to many benign users.
%
However, in FRS where gradients are shared instead of users' raw data, most of these attacks~\cite{Li16Data,HuangMGL0X21,Fang18Po,Fang20In} are inapplicable as they rely on access to historical interactions of benign users.
A few studies including~\cite{kapoor2017review} do not require such access but offer very limited effectiveness.
All in all, conventional attacks based on poisonous data fall short in FRS.

Nevertheless, FRS encounters the peril of \textbf{model poisoning attacks}, which manipulate malicious users to upload poisonous gradients to corrupt the recommender model.
Model poisoning attacks are further classified as targeted attacks~\cite{fedrecattack,pipattack,a-hum} and untargeted attacks~\cite{FedAttack}, depending on whether the objective is to promote the exposure of specific items desired by the attacker.
Targeted attacks have garnered greater research interest due to their superior stealth and greater destructive potential.
Generating effective poisonous gradients for targeted attacks presents a challenge, especially when benign users' data is made private in their personalized models within an FRS.
Existing studies resolve the challenge from various perspectives:
(1) \emph{User Embedding Approximation}. \FRA{}~\cite{fedrecattack} approximates benign users' personalized models using a small public part of their interactions;
(2) \emph{Item Popularity Enhancement}. \PIP{}~\cite{pipattack} trains a popularity estimator using items' popularity levels and enhances the target items' popularity within the estimator model, instead of manipulating scores in the personalized models of benign users.
(3) \emph{Interaction Function Poisoning}.
\ARA{} and \AHUM{}~\cite{a-hum} amplify target items' scores by modifying the learnable interaction function employed by the FRS, regardless of benign users' personalized models.
%
However, these attacks possess certain limitations.
\FRA{} and \PIP{} necessitate prior knowledge (i.e., benign users' public interactions in \FRA{} or items' popularity levels in \PIP{}), which is usually unavailable in practice.
\ARA{} and \AHUM{} only work in DL-FRS since the interaction function is non-learnable in MF-FRS.
Overall, we summarize the features of existing targeted attacks in~\cref{tab:comparison}{}. Different from these studies, the targeted attack proposed in this paper does not rely on prior knowledge and is not constrained by the type of FRS.

\begin{table}[!htbp]
\centering{
\caption{Comparison of targeted model poisoning attacks.}
\label{tab:comparison}
\begin{tabular}{l|ccc}
\toprule
\multirow{2}{*}{{Attack Methods}} & \multicolumn{3}{c}{{Property}}                                           \\ \cmidrule{2-4} 
& {Prior Knowledge} & {MF-FRS} & {DL-FRS}\\ 
\midrule
\FRA{} \cite{fedrecattack}       & historical interactions       &\checkmark  &\checkmark\\
\PIP{} \cite{pipattack}       & items' popularity         &\checkmark  &\checkmark     \\
\ARA{} and \AHUM{} \cite{a-hum}       & \textbf{not required}       & $\times$
& \checkmark    \\
\model{} (ours)     & \textbf{not required}                   &\checkmark  &\checkmark     \\ \bottomrule
\end{tabular}
}
\end{table}

\smallskip
\noindent\textbf{Defense Methods for FRS}. 
The defense methods against model poisoning attacks have been well studied in conventional federated learning.
These defenses focus on normalizing the uploaded gradients~\cite{NormBound} or 
filtering those gradients using statistic information such as median and trimmed mean~\cite{Media-TrimmedMean} or similarity information~\cite{Krum-MultiKrum} or the combination of the two~\cite{Bulyan}.
Their motivation is that poisonous gradients are few but unnormal compared to gradients from benign clients.
However, these defenses fail to combat targeted attacks in FRS that upload poisonous gradients to promote specific target items (cf.\ theoretical analysis in \cref{ssec:defense_analysis}).
In FRS, items are trained only on their relevant interaction data. Hence, given a target item that is usually unpopular with very sparse interaction data, poisonous gradients generated for it usually dominate and can hardly be identified via normalization or filtering.
This has been evidenced by our experiments reported in \cref{exp:defense_performance}.
To overcome the limitation, our defense method introduces two regularization terms during benign user training, restricting attack attempts based on item popularity enhancement or user embedding approximation while maintaining recommendation performance (cf.\ \cref{ssec:defense_design}).


