\section{Popular Item Embedding based Attack}
\label{sec:attack}
\begin{figure*}[ht]
\centering
    \includegraphics[width=\textwidth]{Figures/fram.pdf} %PIECK.pdf
    \caption{Overview of \model{}: (a) functional modules in \model{}, (b) the \modelI{} solution, and (c) the \modelII{} solution.} 
    \label{fig:fram}
\end{figure*}
The overall framework of \model{} is depicted in \cref{fig:fram}.
Referring to \cref{fig:fram}(a), \model{} comprises three functional modules: \textcircled{1} popular item mining, \textcircled{2} item popularity enhancement, and \textcircled{3} user embedding approximation. Module \textcircled{1} serves as the core module and can be combined with either Module \textcircled{2} or Module \textcircled{3} to formulate two distinct attack solutions \modelI{} and \modelII{}, as shown in \cref{fig:fram}(b) and (c).
% 
Next, we will elaborate on the basic idea of \model{} in \cref{ssec:overview} and delve into the details of the three modules in \cref{ssec:popular_items,ssec:popular_enhancement,ssec:user_embed_app}.

\subsection{Basic Idea of \model{}}
\label{ssec:overview}

Resuming the attack goal in \cref{ssec:setting-attack}, to create a model-agnostic targeted attack against FRS, we need to generate effective poisonous gradients for item embeddings without accessing benign users' embeddings.
Previous study \PIP{}~\cite{pipattack} has demonstrated that inherent popularity bias in recommender models leads to high predicted scores for popular items across the majority of users. Thus, manipulating popular items would be an effective means of generating poisonous gradients to promote target items.
However, \PIP{} assumes the attacker's prior knowledge of globally popular items, a supposition that often diverges from reality.
Getting rid of this strong assumption, we propose a \emph{popular item mining} method, which only leverages the partial knowledge that malicious users can obtain through the training of FRS in the federated setting. The method will be detailed in \cref{ssec:popular_items}.

After successfully identifying the popular items on malicious users, the attack can be initiated by generating poisonous gradients using the embeddings of identified popular items.
We explore two distinct strategies.
On the one hand, an \emph{item popularity enhancement} strategy derives poisonous gradients that align the embeddings of target items with the embeddings of those popular items, thereby increasing the exposure of target items.
This strategy and the corresponding attack solution \modelI{} will be presented in \cref{ssec:popular_enhancement}.

On the other hand, a \emph{user embedding approximation} strategy proposes to approximate the invisible embedding $\mathbf{u}_i$ of each benign user in \cref{eq:poisonous_gradients_1} to derive the poisonous gradient $\nabla\tilde{\mathbf{v}}_j$.
This strategy originates from empirical observations in typical MF-FRS \cite{ammad2019federated,FedRec,FedRec++,9162459} and DL-FRS \cite{WangYCYZZ22,perifanis2022federated,jiang2022fedncf}, where popular items' embeddings are co-trained with a significant number of users' embeddings on symmetric model structures for items and users. As a result, the distributions of popular items and user embeddings become closer.
Therefore, approximating user embeddings with popular item embeddings to derive poisonous gradients can lead to surprisingly decent performance.
The strategy along with its attack solution \modelII{} will be described in \cref{ssec:user_embed_app}.



\subsection{Popular Item Mining in FRS}
\label{ssec:popular_items}
\begin{figure}[!tbp]
\centering
     \subfigure[MovieLens-100K (ML-100K)]{
     \centering
    \begin{minipage}{0.47\linewidth}
        \label{subfig:ml-100k-dis}
        \includegraphics[width=4cm]{Figures/ML-100K_distribution.pdf}
    \end{minipage}
    }% 
   \subfigure[Amazon Digital Music (AZ)]{
   \centering
    \begin{minipage}{0.47\linewidth}
        \label{subfig:az-dis}
        \includegraphics[width=4cm]{Figures/AZ_distribution.pdf}
    \end{minipage}
    }%
    % \vspace{-0.1cm}
    \caption{The distribution of items' popularity.}
    \label{fig:inter_dis}
\end{figure}

An item's popularity is defined as the number of user interactions it receives. 
In recommendations, the popularity of items typically follows a long-tail distribution \cite{zhang2021model} --- only a small portion of items have extremely high popularity, while the rest are low.
\cref{fig:inter_dis} visualizes such long-tail phenomenon on two datasets: MovieLens-100K (ML-100K) and Amazon Digital Music (AZ). These datasets will be further described in \cref{ssec:settings}.
For clarity, we define the top $15\%$ of items with the highest number of interactions (indicated by blue dotted lines) as popular, while the remaining items are considered unpopular.
On ML-100K and AZ, the number of popular items accounts for less than one-seventh and one-sixth of the total number of items, respectively. However, interactions with these popular items exceed 50\% of the total number of interactions (indicated by red dotted lines).

Let the term $\mathcal{L}_{ij} = x_{ij}\log\hat{x}_{ij} + (1-x_{ij})\log(1-\hat{x}_{ij})$ in \cref{equation:rs-loss} be the prediction loss of an item $v_j$ to a user $u_i$. 
Thus, the overall objective of the FRS at the $r$-th round is written as
\begin{equation}
    \label{equa:global_loss}
    \mathcal{L}^r = \sum\nolimits_{v_j \in {D}_i}\big( \sum\nolimits_{u_i\in {U}_
{j}^{r}}\mathcal{L}_{ij} \big),
\end{equation}
where ${U}_{j}^{r}$ represents the set of users sampled for item $v_j$.

Let $v_j$ be a popular item and $v_k$ be an unpopular item, the following property is derived.

\begin{figure}[tbp]
\centering
     \subfigure[MF-FRS]{
     \centering
    \begin{minipage}{0.47\linewidth}
        \label{subfig:appro-pop-item-1}
        \includegraphics[width=4cm]{Figures/MF-appro-pop-items.pdf}
    \end{minipage}
    }% 
   \subfigure[DL-FRS]{
   \centering
    \begin{minipage}{0.47\linewidth}
        \label{subfig:appro-pop-item-2}
        \includegraphics[width=4cm]{Figures/NCF-appro-pop-items.pdf}
    \end{minipage}
    }%
    \caption{Listed from top to bottom, are the popularity rankings of the top-50 items in $\Delta$-Norm for rounds 4, 8, 20, and 80.}
    \label{fig:appro-pop-item}
\end{figure}

\begin{property}\label{property_1}
\textit{
A popular item embedding $\mathbf{v}_j$ typically takes a longer time to converge compared to an unpopular item embedding $\mathbf{v}_k$.}
\end{property}

The aforementioned long-tail distribution of item popularity reveals that the user set ${U}_j^{r}$ associated with the popular item $v_j$ is typically much larger than ${U}_k^{r}$ associated with the unpopular item $v_k$.
Consequently, for the loss $\mathcal{L}^r$ in \cref{equa:global_loss}, there will be more loss terms $\mathcal{L}_{ij}$ attributed to a popular item than an unpopular one. This ultimately makes it much more prolonged for those popular items to converge.

\begin{property}\label{property_2}
\textit{
During each training round, the popular item embedding $\mathbf{v}_j$ tends to undergo larger changes compared to the unpopular item embedding $\mathbf{v}_k$.}
\end{property}

The explanation for Property~\ref{property_2} is relatively straightforward. When considering a popular item and an unpopular item in the training process, the gradients derived from \cref{equa:global_loss} for the popular item, which have been shown more difficult to converge (cf.\ Property~\ref{property_1}), will be larger. This indicates that the popular item's embedding usually experiences a greater degree of change across two training rounds.

All in all, we assert that \emph{the changes in popular items' embeddings are greater in magnitude and longer-lasting when compared to unpopular items}.
To validate this, we conduct preliminary experiments on the ML-100K dataset for both MF-FRS and DL-FRS.
To quantify the change in $v_j$'s embedding per round, we introduce the $\Delta$-Norm concept, which measures the $L_2$ Norm between the embeddings of $v_j$ at consecutive rounds. Formally, $\Delta$-Norm of item $v_j$ at the $r$-th round is 
\begin{equation}
\Delta\text{-Norm}_j^r = ||\mathbf{v}_j^{r+1}-\mathbf{v}_j^r||_2,
\end{equation}
where $\mathbf{v}_j^r$ is $v_j$'s embedding at the $r$-th round.

\cref{fig:appro-pop-item} depicts the popularity rankings of the top-50 items in $\Delta$-Norm at rounds 4, 8, 20, and 80.
Initially (rounds 4 and 8) in both MF-FRS and DL-FRS, a few unpopular items (red points to the right of the blue dotted line) enter the top-50. This is due to the fact that the embeddings of certain unpopular items are still undergoing convergence, resulting in large gradients during the early training stages.
However, as training progresses, particularly after only 20 rounds, the presence of unpopular items among the top-50 diminishes. 
Conversely, we consistently observe significant $\Delta$-Norm for popular items, which aligns with Property~\ref{property_1}.
Across all fixed rounds, the majority of the top-50 positions in $\Delta$-Norm are dominated by popular items. This observation supports our argument that the convergence of most unpopular items leads to smaller gradient updates. Meanwhile, popular items, which are still being fitted, undergo larger changes in their embeddings, consistent with our Property~\ref{property_2}.
Taken together, the sustained and prominent presence of popular items among the top-50 items in $\Delta$-Norm serves as compelling evidence to support our assertion.


The verified assertion allows us to design the popular item mining algorithm based on the $\Delta$-Norm measures, which is outlined in \cref{alg:popular_items}.
The algorithm iterates over $\tilde{R}$ times and accumulates the $\Delta$-Norm values for each item (line~1). 
The calculation of $\Delta$-Norm values, as illustrated in Module \textcircled{1} of \cref{fig:fram}(a), adopts matrix parallel computation and thus is highly efficient.
%
Finally, it outputs a set $P$ containing the top-$N$ items with the highest accumulated $\Delta$-Norm values. By accumulating the $\Delta$-Norm values over $\tilde{R}$ times, the algorithm ensures a stable result in identifying popular items.
In practice, we set a relatively small yet practically useful value $2$ to $\tilde{R}$ to obtain the popular items at each malicious user client.

\begin{algorithm}[!htbp]
\caption{\textsc{PopularItemMining} (mining times $\tilde{R}$, mined popular item number $N$)}
\label{alg:popular_items}
\begin{algorithmic}[1]
\While{\text{times of user being sampled }$\tilde{r} \leq \tilde{R}+1$}
    \For{each item $v_j$}
        \If{$\tilde{r} = 1$}
            $\Delta\text{-Norm}_j \gets 0$
        \Else
            ~$\Delta\text{-Norm}_j \gets \Delta\text{-Norm}_j + ||\mathbf{v}_j^{r}-\mathbf{v}_j^{r-1}||_2$
        \EndIf
    \EndFor
\EndWhile
\State \Return ${P} = \{ v_j \mid \Delta\text{-Norm}_j \text{ in top-$N$ ones} \}$
\end{algorithmic}
\end{algorithm}


\subsection{\modelI{} using Item Popularity Enhancement}
\label{ssec:popular_enhancement}

In line with our discussion in \cref{ssec:overview}, we propose an item popularity enhancement strategy to promote the recommendation exposure of the target items. This is achieved by aligning their embeddings to those of the mined popular items.
The process is depicted in Module \textcircled{2} in \cref{fig:fram}(a).

To effect this alignment, our strategy aims to optimize the {cosine similarity}\footnote{We empirically compare alternative similarity metrics in \cref{ssec:ablation_study}.} between each target item and each popular item in the embedding space. 
To aggregate these pairwise similarities effectively, we employ a \emph{weighted mean} approach, where weights assigned to each cosine similarity, are predicated on the popularity rank of the corresponding popular item in ${P}$. As such, more popular items receive higher weights.

However, the distinct characteristics of popular items (e.g., two songs of different genres) can cause the cosine similarity between the target item and popular items to lean either positively or negatively during optimization. 
This can result in overfitting towards the more prominent direction, causing a bias in the target item's embedding. 
Nonetheless, we believe that popular items in less common directions are still valuable, as they typically represent different popular feature information. Therefore, we intend to define a metric that can capture high cosine similarity between a target item and only those popular items having shared characteristics with it.

To achieve this, given a target item $v_j$, we first split the mined popular item set ${P}$ into two subsets: positive and negative popular items, based on their cosine similarity with $\mathbf{v}_j$.
They are represented as ${P}^\text{+}_j = \{\mathbf{v}_k \mid \text{cos}(\mathbf{v}_k, \mathbf{v}_j) > 0\}$ and ${P}^\text{-}_j = \{\mathbf{v}_k \mid \text{cos}(\mathbf{v}_k, \mathbf{v}_j) \leq 0\}$.
We then compute the \emph{weighted mean cosine similarity} for each subset and combine them to adjust the update magnitudes for actual popular items. This leads us to define our loss function for the attack as follows:
\begin{equation} \label{pieckipe_loss}
\begin{aligned}
\tilde{\mathcal{L}}_{\text{IPE}} = -\frac{1}{|{T}|} \sum\limits_{v_j \in {T}} \sum\limits_{\text{*}\in\{\text{+},\text{-}\}}\frac{\sum\nolimits_{v_k\in{P}^\text{*}_j} \kappa(v_k) \cdot \text{cos}(\mathbf{v}_k,\mathbf{v}_j)}{\lambda^{-1} \cdot|{P}^\text{*}_j|},
\end{aligned}
\end{equation}
where $\kappa(v_k)$ means the item $v_k$'s normalized inverse rank in ${P}_j^{*}$.
To be specific, $\kappa(v_k)$ is an alignment weight, giving more importance to the features of more popular items.
And $\lambda \in (0,1]$ regulates the strength of the weighting.
A smaller $\lambda$ intensifies the suppression of updates for the dominant direction while promoting larger updates for the rare direction.
The concept introduced in \cref{pieckipe_loss} aims to steer a target item towards greater alignment with the majority of popular items, while also considering the shared characteristics from various types of popular items. As a result, the loss defined in \cref{pieckipe_loss} comprehensively enhances the target item's popularity.

The \modelI{} solution, presented in \cref{alg:pieckipe}, is applied to each malicious user.
It starts by mining the set ${P}$ of popular items using the \textsc{PopularItemsMining} algorithm until user $u_i$ has been sampled more than $\tilde{R}$ times (line~1).
If the malicious user is subsequently sampled to participate in FRS training (line~2), it calculates the attack loss defined in \cref{pieckipe_loss} (line~3) and uploads the poisonous gradients derived from the calculated loss to manipulate the recommendation (line~4).

\begin{algorithm}
\caption{\modelI{} (mining times $\tilde{R}$, mined popular item number $N$, target item set ${T}$)}
\label{alg:pieckipe}
\begin{algorithmic}[1]
\State ${P} \gets$ \Call{PopularItemMining}{$\tilde{R},N$} \Comment{\cref{alg:popular_items}}
\While{\text{times of user being sampled }$\tilde{r} \geq \tilde{R}+1$}
    \State compute $\tilde{\mathcal{L}}_\text{IPE}$ using \cref{pieckipe_loss}
    \State upload poisonous gradients $\{ \tilde{\nabla}{\mathbf{v}}_j \mid \frac{\partial}{\partial \mathbf{v}_j} \tilde{\mathcal{L}}_\text{IPE},v_j \in {T} \}$
\EndWhile
\end{algorithmic}
\end{algorithm}

Unlike \PIP{}~\cite{pipattack}, which relies on pre-existing popular item information for enhancing popularity, \modelI{} can function independently at each client without needing this information. This is facilitated by automatically identifying popular items using the $\Delta$-Norm obtained from FRS training. However, as the recommender model becomes more personalized over time, it may prove difficult to recommend even the most popular items to all users. Consequently, in later stages of FRS training, the exposure of target items, whose popularity is enhanced by \modelI{}, may diminish (cf.\ \cref{subfig:ipe_uea_compare_mf_ml-1m}).
Thus, we provide an alternative attack solution in \cref{ssec:user_embed_app}.

\subsection{\modelII{} using User Embedding Approximation}
\label{ssec:user_embed_app}

To enhance the attack robustness throughout the FRS training, an effective way might be to directly optimize the scores of target items among all benign users based on \cref{eq:poisonous_gradients_1}.
However, this method entails knowing benign users' embeddings, typically inaccessible in FRS.
To overcome the limitation, we propose the user embedding approximation strategy by extracting useful knowledge from the mined popular items.

Let $\mathbf{V}_{P}={\{\mathbf{v}_k \mid v_k \in {P}\}}$ be the set of popular items’ embeddings.
The following property is derived.

\begin{property}\label{property_3}
\textit{
The distribution of embeddings in $\mathbf{V}_{P}$, is likely to resemble that of users, given the shared training process in the symmetric recommender model structure.}
\end{property}

This is a key observation as typical MF-FRS~\cite{ammad2019federated,FedRec,FedRec++,9162459} and DL-FRS~\cite{WangYCYZZ22,perifanis2022federated,jiang2022fedncf} are both built on user-item mutual interactions, creating symmetric recommender model structures for users and items. 
The symmetry, in turn, leads to similar embedding distributions for items and their interacting users. Such an effect is particularly pronounced for popular items that have interacted with a majority of users --- their embedding distribution closely mirrors that of all users.

To verify the observation, we conduct preliminary experiments on ML-100K, without the participation of malicious users. 
This enables us to obtain pure popularity ranking and access users' historical interactions and their embeddings during training.
Let $\mathbf{U}_{P}=\{\mathbf{u}_i \in {U} \mid \exists v_k \in {P}: x_{ik} = 1 \}$ denote the set of embeddings for users whose historical interactions include at least one item in ${P}$.
We compute the \emph{user coverage ratio} (UCR) of the popular item set ${P}$ as $|\mathbf{U}_{P}| / |{U}|$.
We then measure the similarity between the distributions of $\mathbf{V}_{P}$ and $\mathbf{U}_{P}$ using the \textit{average pairwise KL divergence} (PKL):
\begin{equation} \label{mean—pair-kld}
\text{PKL}(\mathbf{V}_{P},\mathbf{U}_{P}) = \frac{1}{|\mathbf{V}_{P}|}\frac{1}{|\mathbf{U}_{P}|}\sum\limits_{\mathbf{v}_k\in\mathbf{V}_{P}}\sum\limits_{\mathbf{u}_i\in\mathbf{U}_{P}} \text{KL}(\mathbf{v}_k, \mathbf{u}_i).
\end{equation}
A smaller PKL indicates a higher similarity between the two distributions.
Next, we collected $\mathbf{V}_{P}$ and $\mathbf{U}_{P}$ for the top-$N$ mined popular items after 200 training rounds (at which the models had converged). The PKL and UCR measures for different $N$ values are presented in \cref{tab:sim_kld}.

\begin{table}[!htbp]
\centering
% \renewcommand{\arraystretch}{0.5}
\setlength{\tabcolsep}{3.2mm}
\caption{Preliminary experiments on ML-100K at 200-th round.}
\label{tab:sim_kld}
\begin{tabular}{c|c|cccc}
\toprule
\multirow{2}{*}{Metric} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{Size of Popular Items Set $N$} \\ \cmidrule{3-6} 
& & 1 & 10 & 50 & 150 \\ \midrule
\multirow{2}{*}{PKL} & MF-FRS & 0.0848 & 0.0828 & 0.0871 & 0.0891 \\
& DL-FRS & 0.0026 & 0.0025 & 0.0016 & 0.0010 \\
\rowcolor[HTML]{F2F2F2} UCR & both & 0.6151 & 0.9830 & 0.9979 & 1.0 
\\ \bottomrule
\end{tabular}
\end{table}

\cref{tab:sim_kld} shows that UCR rapidly reaches 0.9830 at $N$ = 10, indicating that mined popular items cover most users swiftly. 
In DL-FRS, PKL is consistently small and unaffected by $N$ variations.
PKL in MF-FRS initially reduces, then rises with a peak similarity between $\mathbf{V}_{P}$ and $\mathbf{U}_{P}$ distributions at $N$ = 10.
In general, selecting $N$ as 10 or more obtains desired values for PKL and UCR in both MF-FRS and DL-FRS. This highlights the notable alignment in the distribution patterns of popular items and users during the FRS training process.

Motivated by these findings, we revise the original attack loss in \cref{att-loss-total}, replacing the set of inaccessible embeddings of benign users (i.e., $\{ \mathbf{u}_i \mid u_i\in\bar{{U}} \}$) with the embeddings of mined top-$N$ popular items (i.e., $\{ \mathbf{v}_k \mid v_k\in{P} \}$): 
\begin{equation} \label{pieckuea-loss}
\tilde{\mathcal{L}}_\text{UEA} = -\frac{1}{N} \frac{1}{|{T}|} \sum\nolimits_{v_k\in{P}}\sum\nolimits_{v_j\in{T}} \log\Psi (\mathbf{v}_k, \mathbf{v}_j).
\end{equation}

As an alternative attack solution executed at each malicious client, \modelII{} is presented in \cref{alg:pieckuea}.
The preparation of mining popular items (line~1) follows the same process as described in \cref{alg:pieckipe}. However, the poisonous gradients are calculated using \cref{pieckuea-loss} and then uploaded to the server in each participation round (lines~2--4). Notably, each embedding $\mathbf{v}_k$ used for approximation is treated as a constant in this process and is excluded in the backpropagation.

\begin{algorithm}
\caption{\modelII{} (mining times $\tilde{R}$, mined popular item number $N$, target item set ${T}$)}
\label{alg:pieckuea}
\begin{algorithmic}[1]
\State ${P} \gets$ \Call{PopularItemMining}{$\tilde{R},N$} \Comment{\cref{alg:popular_items}}
\While{$\text{times of user being sampled } \tilde{r} \geq \tilde{R}+1$}
    \State compute $\tilde{\mathcal{L}}_\text{UEA}$ using \cref{pieckuea-loss}
    \State upload poisonous gradients $\{ \tilde{\nabla}{\mathbf{v}}_j \mid \frac{\partial}{\partial \mathbf{v}_j} \tilde{\mathcal{L}}_\text{UEA}, v_j \in {T} \}$
\EndWhile
\end{algorithmic}
\end{algorithm}
