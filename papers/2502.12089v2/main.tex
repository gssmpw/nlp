%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025_arxiv}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{cancel}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{frablue}{rgb}{0.141, 0.514, 0.596}
\newcommand{\fra}[1]{\textcolor{frablue}{ #1}}

\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\prob}[1]{\mathbb{P}\left\lbrace#1\right\rbrace}
\newcommand{\E}{\mathbb{E}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\voc}{\mathcal{V}}
\def\rh{{\textnormal{h}}}
\newcommand{\h}{h}
\newcommand{\lpa}{\left(}
\newcommand{\rpa}{\right)}
\newcommand{\lsq}{\left[}
\newcommand{\rsq}{\right]}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Compositional Generalization and Creativity in Diffusion Models}
\icmltitlerunning{}


\begin{document}

\twocolumn[
% \icmltitle{Creativity by composition in diffusion models}
\icmltitle{How compositional generalization and creativity improve \\as diffusion models are trained}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alessandro Favero}{equal,phy,ele}
\icmlauthor{Antonio Sclocchi}{equal,phy}
\icmlauthor{Francesco Cagnetta}{sissa}
\icmlauthor{Pascal Frossard}{ele}
\icmlauthor{Matthieu Wyart}{jhu}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{phy}{Institute of Physics, EPFL}
\icmlaffiliation{ele}{Institute of Electrical and Micro Engineering, EPFL}
\icmlaffiliation{sissa}{Theoretical and Scientific Data Science, SISSA}
\icmlaffiliation{jhu}{Department of Physics and Astronomy, Johns Hopkins University. On leave from EPFL}

\icmlcorrespondingauthor{}{alessandro.favero@epfl.ch}
\icmlcorrespondingauthor{}{mwyart1@jh.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\looseness=-1 Natural data is often organized as a hierarchical composition of features. How many samples do generative models need in order to learn the composition rules, so as to produce a combinatorially large number of novel data? What signal in the data is exploited to learn those rules? We investigate these questions in the context of diffusion models both theoretically and empirically. Theoretically, we consider simple probabilistic context-free grammars---tree-like graphical models used to represent the hierarchical and compositional structure of data such as language and images. We demonstrate that diffusion models learn the grammar's composition rules with the sample complexity required for clustering features with statistically similar context, a process similar to the word2vec algorithm. However, this clustering emerges hierarchically: higher-level features associated with longer contexts require more data to be identified. This mechanism leads to a sample complexity that scales polynomially with the said context size. As a result, diffusion models trained on an intermediate dataset size generate data coherent up to a certain scale, but that lacks global coherence. We test these predictions in different domains, and find remarkable agreement:  both generated texts and images achieve progressively larger coherence lengths as the training time or dataset size grows. We discuss connections between the hierarchical clustering mechanism we introduce here and the renormalization group in physics.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\textit{Compositional generalization}, the ability to understand and generate novel combinations of known components, is a fundamental characteristic of human intelligence and creativity. 
For instance, this skill allows humans to create grammatically correct and meaningful sentences never heard before or to reason originally by assembling together known ideas. Under which conditions can machines learn such a skill?  The success of diffusion models, in producing realistic data across various domains~\cite{sohl2015deep,ho2020denoising,song2019generative,betker2023improving,rombach2022high} provides a unique opportunity to study how this ability emerges. Fundamental questions include:  What signals in the data are exploited by neural networks to learn the \textit{compositional rules}? How many training examples are needed to learn such rules, and in what order are they learned? How does the finiteness of the training set affect the structure of generated data?

To address these questions theoretically, we bridge two viewpoints developed in the context of natural language processing. On the one hand, \textit{symbolic approaches} aim to describe the structure of data via a list of rules that generate them. For example, probabilistic \emph{context-free grammars} (PCFG)~\cite{chomsky2014aspects} describe sentences with trees, whose nodes are hidden variables that can generate other nodes or leaves according to probabilistic production rules. PCFGs can approximate both structural and semantic aspects of text and have been proposed for the description of images under the name of \emph{Pattern Theory}~\cite{stoyan1997grenander,jin2006context,siskind2007spatial}. On the other hand, \textit{statistical approaches} use data-driven analyses agnostic to expert knowledge of grammatical structure. A notable example is \emph{word2vec} \cite{mikolov2013distributed}, where a shallow neural network learns meaningful representations of words by merely predicting their neighborhood.

\paragraph{Contributions} We unify these two viewpoints by studying how diffusion models learn simple PCFGs. In particular,
\vspace{-1em}
\begin{itemize}  
\item We show empirically that the learning process of diffusion models is hierarchical, progressively capturing compositional rules at deeper levels of the PCFG's hierarchy.
\item We argue that the grammar rules can be deduced iteratively by clustering, as in word2vec, sequences of tokens based on the statistics of their context. For each level, we analytically derive the corresponding sample complexity. We show that it matches the number of data required by the diffusion model to generate data that follow the PCFG rules up to that level.
\item Since this hierarchical clustering procedure requires a number of samples that is polynomial in the size of the token's sequence, this mechanism allows the diffusion model to learn a high-dimensional distribution while avoiding the \emph{curse of dimensionality}.
\item Beyond simple PCFGs, we predict that diffusion models trained on limited samples generate data that is locally coherent (i.e., satisfying local compositional rules), but not globally, with a coherence length growing with the training time/number of samples. We confirm this prediction in diffusion models trained on OpenWebText and ImageNet.
\end{itemize}
We conclude by discussing how the principle we put forward to build a hierarchy of latent variables generalizes the renormalization group used in physics, where coarse-grained variables are obtained by simple pooling operations. 

\subsection{Related work}

\paragraph{Sample complexity in diffusion models}

Under mild assumptions on the data distribution, diffusion models exhibit a sample complexity that scales exponentially with the data dimension \cite{block2020generative,oko2023diffusion}. It is not the case if data lie on a low-dimensional latent subspace \cite{de2022convergence,chen2023score,yuan2023reward}, correspond to Gaussian mixture models \cite{biroli2023generative,shah2023learning, Cui2023AnalysisOL}, Ising models~\cite{mei2023deep}, or distributions that can be factorized across spatial scales \cite{kadkhodaie2023learning}. These works do not consider the sample complexity in compositional data. 

\paragraph{Compositional generalization of diffusion models}

\looseness=-1 \citet{okawa2023compositional} considered synthetic compositional data to empirically show how diffusion models learn to generalize by composing different concepts, in the absence of a compositional hierarchy. \citet{kamb2024analytic} studied how equivariant diffusion models can compose images by combining local patches seen in the dataset. \citet{sclocchi2024phase,sclocchi2024probing} showed that diffusion on hierarchically compositional data can be solved using Belief Propagation. \citet{mei2024unets} showed that U-Nets can efficiently approximate the Belief Propagation algorithm on hierarchical data. Yet, efficient representability does not guarantee learnability by gradient descent for hierarchical data \citep{cagnetta2023deep}. These works do not, however, address the sample complexity of diffusion models learned by gradient descent or variations of it.

\paragraph{Learning hierarchical representation via next-token prediction}

It has been observed that transformers trained on next-token prediction on PCFGs learn a hierarchical representation of the data that reflects the structure of the latent variables \citep{cagnetta2024towards, allen2023physics,garnier2024transformers}. Closest to our work, \citet{cagnetta2024towards} showed that for the prediction of the last token in a sequence of fixed length, the latent structure is learned hierarchically, with a sample complexity polynomial in the context length. Our work extends this finding to diffusion models, in a setup where complete sequences can be generated. This setup allows us to make novel predictions on the limitations of generated data as a function of the training set size, which we test empirically across domains.

\section{Background and setup}
\label{sec:back}

\subsection{Diffusion models}

Denoising diffusion models are a family of generative models built to draw samples from a target distribution by inverting a procedure in which noise is gradually introduced \citep{sohl2015deep,ho2020denoising,song2019generative,song2020score}.
Let $t$ denote the time index running in $[0, \dots, T]$, and let $q(\cdot)$ be the distribution we aim to sample from, with $x(0) \sim q(x(0))$ denoting a sample from this distribution.
A diffusion model is composed of two main parts:
\begin{itemize}
    \item \looseness=-1 A \textbf{forward process} that sequentially adds noise to the data to produce the sequence $\{x(t)\}_{1 \leq t \leq T}$,
    \[
    q(x(1), \dots, x(T) \mid x(0)) \;=\; \prod_{t=1}^T q(x(t) \mid x(t-1)),
    \]
    culminating in a purely noisy sample $x(T)$.
    \item A \textbf{backward process} that reverses the noise addition step by step and is typically learned by training a neural network to approximate the backward transition kernels $p(x({t-1}) \mid x(t))$. This process effectively learns the \textit{score function}, which is proportional to the conditional expectation $\mathbb{E}_{q(x(0) \mid x(t))}[x(0)]\,{\coloneq}\,\mathbb{E}[x(0)|x(t)]$.
\end{itemize}
To draw a new sample from $q(\cdot)$, one starts with a noise sample $x(T) \sim q(x(T))$ and then applies the learned backward process to obtain a clean sample $x(0) \sim q(x(0))$. Various diffusion models differ in how they define the forward process, depending on the characteristics of the data space. For an overview, see \citet{yang2023diffusion}.

\paragraph{Continuous data}

For continuous data, such as real-valued signals or images modeled in a continuous space, Gaussian diffusion \citep{ho2020denoising} uses the forward transition matrix
\begin{equation*}
     q(x(t)|x({t-1})) = \mathcal{N}(x(t);\sqrt{1-\beta_t}x({t-1}),\beta_t \mathbb{I}),
\end{equation*}
where $\mathcal{N}$ indicates the Gaussian distribution and the sequence \{$\beta_t\}_{1\leq t \leq T}$ is the noise schedule. At the final time $T$, $x(T) \sim \mathcal{N}(0,\mathbb{I})$.

\paragraph{Discrete data}

For discrete data, such as text, $x(0)$ consists of a sequence of tokens $x_{i}(0)$, $i\in[d]$, each corresponding to a symbol belonging to a vocabulary $\mathcal{V}$. Considering a \textit{uniform diffusion process} \citep{hoogeboom2021argmax,d3pm2021}, at each time step $t$, tokens either stay unchanged or transition to any other symbol with some probability $\beta_t$. Using a one-hot-encoding representation of these $|\mathcal{V}|$ states, the forward transition matrix reads
\begin{equation*}
    q(x_{i}(t)|x_{i}(t-1)) = (1-\beta_t) \mathbb{I} + \beta_t/|\mathcal{V}|\,\mathbf{1}\mathbf{1}^\top,
\end{equation*}
where $\mathbb{I}$ is the identity and $\mathbf{1}$ a vector of all ones. The stationary distribution achieved at the final time $T$ is uniform.

\subsection{Probabilistic graphical models}

\paragraph{Probabilistic context-free grammars (PCFG)}

To systematically investigate how diffusion models learn compositional structures, we consider synthetic datasets generated via a \textit{probabilistic context-free grammar} (PCFG)~\citep{rozenberg_handbook_1997}: a collection of symbols and rules that prescribe how to generate sequence data starting from a single feature. Generic PCFGs consist of a vocabulary of hidden (\emph{nonterminal}) symbols, a vocabulary of visible (\emph{terminal}) symbols and \emph{production rules} that quantify the probability that one hidden symbol generates tuples of either hidden or visible symbols.

\paragraph{Random Hierarchy Model (RHM)}

\looseness=-1 The RHM \cite{cagnetta2023deep} is a particular PCFG, including the following additional assumptions to make it analytically tractable.
\begin{itemize}
\item[\emph{i)}] The nonterminal symbols are split into $L$ finite vocabularies $(\mathcal{V}_\ell)_{\ell=1,\dots,L}$ of finite size $v$ and $\mathcal{V}\equiv\mathcal{V}_0$ denotes the vocabulary of terminal symbols.
\item[\emph{ii)}] All the production rules transform one level-$(\ell\,{+}\,1)$ symbol into a string of $s$ level-$\ell$ symbols,
\begin{equation}\label{eq:production-rules}
\mu^{(\ell+1)} \to \mu^{(\ell)}_{1},\dots,\mu^{(\ell)}_{s}.
\end{equation}
\item[\emph{iii)}] There are $m$ \emph{unambiguous} production rules per nonterminal symbol, i.e., two distinct nonterminals cannot generate the same $s$-tuple. The rules are randomly chosen and frozen for a given instance of the RHM. We call the $m$ strings produced by any given symbol {\it synonyms};
\item[\emph{iv)}] All the available production rules are equally likely.
\end{itemize}
Due to assumptions \emph{i)} and \emph{ii)}, the data-generating process can be represented as a regular tree graph with depth $L$ and branching ratio $s$.
The leaf nodes (layer $\ell=0$) correspond to the tokens of the visible data, which form strings of size $d=s^L$. 
The upper-level nodes are latent variables. We use the notation $\h^{(\ell)}_i$ to indicate the variable at level $\ell$ and position $i\in [s^{L-\ell}]$.
We define the tree distance $\tilde{\ell}$ between a visible token and (latent) tuple as the number of edges between their lowest common ancestor and the visible token. The same definition applies to the tree distance between visible tokens. We define the distance between visible tokens as $s^{\tilde{\ell}}$, where $\tilde{\ell}$ is their tree distance. Because of the hierarchical structure generating the data, the visible tokens have power law spatial correlations \citep{cagnetta2024towards}.

\paragraph{Bayes-optimal denoising of the RHM}

\looseness=-1 Knowing the production rules and the tree structure of the RHM, the probabilities of the latent variables, conditioned on some observation, can be reconstructed exactly \citep{sclocchi2024phase} using the \textit{Belief Propagation} (BP) algorithm \citep{mezardmontanari}. Specifically, if an RHM datum $\x_0$ is corrupted by some noise, e.g., via masking a fraction of tokens, resulting in a noisy observation $\x_t$, then BP can be used to compute the marginal probabilities of any latent or visible variable, conditioned on the noisy observation. Thus, using BP, the exact score is known in these models.  Here, we study instead how many samples are necessary to learn the distribution and the score function from data.

\section{How diffusion models learn a grammar}
\label{sec:diffusion_rhm}

\begin{figure*}
    \centering
    \subfigure[Standard training.]{
        \includegraphics[width=.31\linewidth]{figures/learning_curves_L5.pdf}
        \label{fig:learning_curves_L5}
    }
    \hfill
    \subfigure[Online training.]{
        \includegraphics[width=.31\linewidth]{figures/online_training_L5.pdf}
        \label{fig:online_training_L5}
    }
    \hfill
    \subfigure[Token-token correlations.]{
        \includegraphics[width=.31\linewidth]{figures/rhm_corr_fun.pdf}
        \label{fig:correlations-rhm}
    }
    \caption{\textbf{Learning different levels of the grammar.} (a) Accuracy at various levels as a function of training dataset size $P$. Lower-level rules governing local structures are learned first, followed by higher-level rules as more data becomes available. (\textit{Inset}) The accuracy scaling matches our theoretical predictions of $m^{\ell+1}$ samples for satisfying rules at level $\ell$. (b) Similar results hold for the online learning setting, where fresh training points are sampled at each step. (c) Token-token correlation magnitude measured for $N=10^6$ samples generated by the diffusion model trained with $P$ training points. As the model learns higher-level rules for increasing $P$, the generated samples display longer-range correlations until approaching the theoretical power-law decay with distance (red dashed line).}
    \label{fig:main_L5}
    \vspace{-.5cm}
\end{figure*}

\looseness=-1 In this section, we investigate how diffusion models learn to generate data from the RHM, and measure the sample complexity required to capture the underlying compositional rules.

\subsection{Experimental setting}

\looseness=-1 To begin, we generate an instantiation of the RHM  with parameters $L$ (depth), $s$ (branching factor), $v$ (vocabulary size), and $m$ (number of synonyms).
Next, we uniformly sample $P$ distinct training points, i.e., sentences from the grammar.
Each input symbol is encoded as a \textit{one-hot vector}, $\x \in \{0,1\}^{d,v}$. With this dataset, we train a \textit{Discrete Denoising Diffusion Probabilistic Model} (D3PM) \cite{d3pm2021} with uniform transition probabilities \cite{hoogeboom2021argmax}.

\looseness=-1 The diffusion model architecture is a convolutional U-Net \cite{ronneberger2015u} with $L$ resolution blocks in both the encoder and decoder.\footnote{Following \citet{cagnetta2023deep}, we expect our results to remain valid for sufficiently expressive architectures, in particular, if the network depth is at least $2L$.}. Each block consists of a single convolutional layer with filter size $s$ and stride $s$, followed by a GeLU activation function. Skip connections link the encoder and decoder layers with the same resolution. The model also includes two embedding and unembedding layers, implemented as convolutions with filter size 1. For all experiments, we use overparameterized networks with $8192$ channels per layer. 

\looseness=-1 To enable feature learning in the overparameterized regime, we initialize the parameters using the maximal-update 
($\mu$P) parameterization \cite{yang2020feature}. Since these networks have enough capacity to memorize their training set, we employ early stopping, halting training when the validation loss plateaus or begins to increase. Moreover, we routinely verify that the model has not simply memorized the training data. 

We train the model with Stochastic Gradient Descent (SGD) with momentum, optimizing the diffusion model loss derived from a variational bound on the negative log-likelihood \citep{sohl2015deep}. Following \citet{d3pm2021}, we use the neural network to predict the conditional expectation $\mathbb{E}(\x(0)|\x(t))$, which parameterizes the reverse diffusion process. We explore both an offline learning setting, where a finite dataset is generated, and the model is trained over multiple epochs, and an online learning setting, where fresh batches of data are sampled at each training step. The choice of hyperparameters is detailed in~\Cref{app:exp-details}.

\subsection{Learning the compositional rules}

We fix the RHM parameters and train diffusion models on datasets of varying size $P$. After training, we generate 1024 samples and evaluate whether the generated data satisfies the compositional rules of the RHM at different hierarchical levels. Specifically, we define the \textit{accuracy $\mathcal{A}_{\ell}$ at level $\ell$} as the fraction of generated samples that satisfy level-$\ell$ rules. 

\looseness=-1 \Cref{fig:learning_curves_L5} shows the accuracy at different levels as a function of $P$. The results reveal a staged learning process: the low-level rules, governing local structures, are learned first, followed by progressively higher-level rules that enforce global coherence. Thus, models trained on intermediate $P$ values generate data that are locally consistent but lack global coherence.

\looseness=-1 The inset of \Cref{fig:learning_curves_L5} compares favorably the scaling of accuracy with our theoretical prediction, which we will derive in the next section. This prediction indicates that learning to satisfy rules at level $\ell$ requires a number of samples that scales as $m^{\ell+1}$.
Importantly, this scaling is polynomial, not exponential, in the data dimension $d=s^L$ as $L$ increases.  Specifically, the sample complexity to learn all rules is $m^{L+1} = m d^{\log m / \log s}$. \Cref{fig:online_training_L5} demonstrates that the same staged learning process applies in the online learning setting, where fresh training samples are drawn at each training step. 

This progressive acquisition of compositional rules also appears in the internal correlations of the generated sequences, defined as the Frobenius norm of the covariance matrix between two visible tokens at distance $t$. As shown in \Cref{fig:correlations-rhm}, at small training set sizes or training times, only nearby tokens exhibit significant correlations, while long-range correlations approach sampling noise (black dashed line, given by $1/(vN^{1/2})$, where $N$ is the number of sequences used to measure correlations). As training progresses, long-range correlations emerge.  When $P \approx 10^{5}$, the correlation structure of the generated data aligns with the theoretical power-law scaling predicted in \citet{cagnetta2024towards} (red dashed line). 

In \Cref{sec:natural_data}, we show that this phenomenology extends beyond the synthetic setting considered here, manifesting consistently across different architectures and modalities. In particular, we observe the same hierarchical learning dynamics in state-of-the-art diffusion models trained on natural language and images, suggesting that our conclusions do not hinge on the specific choice of the RHM. Rather, they reflect a fundamental property of diffusion models learning data with a latent compositional structure.

\subsection{Dependence of sample complexity with $m$}

To investigate the dependence of the accuracy on the number of synonyms $m$, we define the \textit{sample complexity} $P^*$ as the training set size at which the accuracy of the last level $\mathcal{A}_L$ surpasses a threshold value $\mathcal{A}^*$. In our experiments, we set $\mathcal{A}^*=1/2$.\footnote{Notice that the observed scaling of sample complexity remains robust to the specific choice of threshold value.} \Cref{fig:main_L2} shows the scaling behavior of $P^*$ with $m$ at fixed depth $L=2$ (blue points). Empirically, we find good agreement with $m^{L+1}$ (dashed line in the figure).

\subsection{Emergence of hierarchical representations}

To generate sequences that satisfy the compositional rules of the RHM, the diffusion model presumably needs to construct internal representations of the latent variables at each level. To do so, it must represent together inputs that differ by low-level synonyms (i.e., choice of low-level production rules). In \Cref{app:additional-results}, we show that it is the case: as the training set size increases, the hidden representations of the U-Net become insensitive to higher and higher levels of synonyms. 

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{figures/sample_complexity_L2_v2.pdf}
    \vspace{-.3cm}
    \caption{\textbf{Sample complexity $P^*$ for $L=2$ in diffusion models and clustering algorithms based on correlations.} Blue points show the empirical values of $P^*$ for trained diffusion models, while black and red points represent clustering methods based on the correlations of latent tuples with the first token and the first visible tuple, respectively. The scaling $ P^* \sim m^{L+1} $ aligns with theoretical predictions. Notably, the simple complexity of the diffusion model closely matches that of the correlation algorithm, suggesting that diffusion models learn hierarchical structures by leveraging statistical dependencies between synonyms.}
    \label{fig:main_L2}
    \vspace{-.6cm}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \node[anchor=north west,inner sep=0pt] at (0,0){
        \includegraphics[width=0.36\linewidth]{figures/UNet_L2-vertical.pdf}};
        \node at (9.ex, -35ex) {(a) U-Net scheme.};
        \node at (10.ex,-32ex) {input: \textbf{$\x(t)$}};
        \node at (10.ex,  1ex) {label: \textbf{$\E[\x(0)|\x(t)]$}};
        \node[anchor=north west,inner sep=0pt] at (3.5,0.0){
        \includegraphics[width=0.55\linewidth]{figures/small_tree_factors}};
        \node at (37.ex,-35ex) {(b) RHM structure.};
    \end{tikzpicture}
    \vspace{-1em}
    \caption{\looseness=-1 \textbf{U-Net scheme and RHM structure.} (a) To denoise the RHM data, the U-Net has to predict the conditional expectation $\E[\x(0)|\x(t)]$ for a given noisy input $\x(t)$, which is proportional to the correlations of the single tokens $x_{i}(0)$ with $\x(t)$. This can be done efficiently by learning the latent hierarchical structure of the data.
    (b) The correlations of the RHM data reflect the tree structure of the model (black squares represent the rules at different levels). For the token $x_1$, using the correlations with tuples at different levels (highlighted in red), the conditional expectation $\E[x_1|\x_{2:8}]$ can be represented as $\E[x_1|x_2, \h_{2}^{(1)}, \h_{2}^{(2)}]$.}
    \label{fig:scheme_rhm}
    \vspace{-.5cm}
\end{figure}

\vspace{-.1cm}

\section{Theoretical analysis}\label{sec:theory}

To derive the sample complexity of the U-Net, we build upon prior work that explains how deep networks efficiently learn hierarchical tasks. This result is achieved by building a lower-dimensional representation that iteratively clusters synonyms \cite{malach2018provably}, allowing the network to recover the latent hierarchical structure of the data. This clustering mechanism is based on statistical correlations between $s$-tuples of tokens and the given task---supervised or self-supervised---which are identical for synonyms. Notably, the sample complexity of deep networks trained with gradient descent aligns with the training set size required to detect these correlations \cite{cagnetta2023deep, cagnetta2024towards}. For supervised learning, this connection can be justified in a one-step gradient descent (GD) setting.

Here, we extend these results to diffusion models. First, we demonstrate that learning the score function in the low-noise limit corresponds to a task invariant to exchanging synonyms, and could thus be simplified by reconstructing the latent variables. Then, we compute the sample complexities required to reconstruct latent variables of different levels using correlations. We conclude by showing that \emph{a)} a clustering algorithm based on correlations does indeed recover the latent variables with the predicted sample complexities and \emph{b)} the sample complexity required to reconstruct first-level latent variables can be recovered in a one-step-GD setting.

\subsection{Learning the score in the low-noise limit}

\paragraph{Input-output correlations in diffusion models}

\looseness=-1 The loss function of diffusion models is minimized when the model prediction converges to the conditional expectation $\E[\x(0)|\x(t)]$, which is sampled in the limit of infinite diffusion trajectories and is proportional to the score function \citep{sohl2015deep, song2019generative, d3pm2021}. 
Since the expectation operates independently for each $v$-dimensional one-hot-encoded token $x_{j}(0)$, $j\in[d]$, we have that $\E[x_{j}(0)|\x(t)]$ is directly proportional to the correlation between a token $x_{j}(0)$ and the input $\x(t)$.

\paragraph{Score function at low noise}

We now consider a small-noise regime $t\,{\to}\, 0$ where only the first token has been changed by noise, to some value $x_{1}(t)$ uncorrelated with $x_{1}(0)$. In this case, the function that the network has to learn is $\E[x_{1}(0)|\x_{2:d}(0)]$, proportional to the correlations of the first token with the remaining sequence of length $d\,{-}\,1$. 
Since these correlations are invariant under exchanges of synonyms \citep{cagnetta2023deep}, they correspond to the correlations of the $x_1$ token with the latents at all levels generating the rest of the sequence, i.e., $\E[x_{1}|\x_{2:s}, \mathbf{\h}_{2:s}^{(1)}, \mathbf{\h}_{2:s}^{(2)}, \dots, \mathbf{\h}_{2:s}^{(L-1)}]$ (\Cref{fig:scheme_rhm}(b)). This function depends on a sequence of length $(s-1)L$, much smaller than the data dimension $d\,{=}\,s^L$. In other words, knowing the latent variables allows for a significant reduction of the problem dimensionality.

\subsection{Sample complexities}

In this section, we determine the sample complexities required to reconstruct the tuple of latent variables of different levels $\mathbf{\h}_{2:s}^{(\ell)}$ appearing in the low-noise score function. As shown in \citet{cagnetta2024towards}, latents can be reconstructed via their correlations with the noised token $x_1$. We thus work under the following assumption.
\assumption{\textit{The U-Net learns to generate data that is consistent with the rules at layer $\ell$ when the correlations between a visible token and a tuple of latents at layer $\ell-2$ become detectable from the training data\label{ass:assumption}}}. 

Hence, in what follows, we compute the number of samples required to detect these correlations.

\paragraph{Local constraints} 

The first step in the learning process is to recognize the valid $s$-tuples generated by the RHM at the visible level. Since these tuples lack internal structure, they can only be memorized. Each tuple can take $vm$ possible configurations corresponding to $v$ symbols for the first-level latents and $m$ representations (synonyms) for each of them. Thus, the sample complexity required to learn the local constraints scales as $P^{(1)} \sim vm$.
Note that depending on the presence of weight sharing, an additional factor $1/d$ can enter this expression. Here, we focus on the dependence of sample complexity with $m$, which is the dominant factor if $m\gg s$ as we shall see below.

\paragraph{First-level latents}

Once the local constraints are learned, the network can refine its estimate of $x_1$ by utilizing correlations with the neighboring tuples $\x_{s+1:2s},\dots,\x_{s^2-(s-1):s^2}$. 
The sample complexity required to detect the correlations between $x_1$ and $\x_{s+1:2s}$ was computed in \citet{cagnetta2024towards} and correponds to $P^{(2)}_{\rm corr}\sim vm^3$.
After learning the first-level rules, the network can collapse the $(s^2-s)$-dimensional sequence of neighboring tuples into the corresponding first-level latents $\mathbf{\h}_{2:s}^{(1)}$.

\paragraph{Second-level latents} 

Having built the first-level latent representation, the model can leverage correlations between $s$-tuples of first-level latents $h_i^{(1)}$'s and the first token to learn the rules at the second level, further improving the denoising task. These correlations can be computed by studying the statistics of the token-latent tuple correlations,
\begin{equation}
\mathbb{P}[x_1=\mu, \mathbf{\h}_{s+1:2s}^{(1)}=\bm{\nu}] - \mathbb{P}[x_1=\mu]\,\mathbb{P}[\mathbf{\h}_{s+1:2s}^{(1)}=\bm{\nu}],
\end{equation}
over realizations of the RHM. Since correlations have zero mean, we take the standard deviation over RHM realizations as an estimate of their typical size. As shown in~\Cref{app:corr_L}, the resulting correlation magnitude is given, in the limit of large $v$ and $m$, by
$C^{(3)}\simeq (v^3m^5)^{-1/2}$.
Since a finite training set of size $P$ only allows measuring the empirical correlation function, we compare the magnitude of correlations with the sampling noise, which has magnitude $(v^2mP)^{-1/2}$. Thus, the number of samples required to detect correlations between tuples of first-level latents and visible tokens, denoted as $P^{(3)}_{\rm corr}$, follows $P^{(3)}_{\rm corr} \sim vm^4$.

\paragraph{Extension to general depth $\ell$} 

The same procedure generalizes to any depth $\ell$. The correlations between tuples of latents at level $\ell-2$ and visible tokens, having lowest common ancestor at level $\ell$, have magnitude $C^{(\ell)} \simeq \sqrt{1/(v^3m^{\ell+2})}$. Meanwhile, the sampling noise remains of order $(v^2mP)^{-1/2}$. Equating these terms gives the sample complexity required to reconstruct level-$(\ell\,{-}\,1)$ latents,
\begin{equation}
    P^{(\ell)}_{\rm corr} \sim vm^{\ell + 1}.
    \label{eq:Pcorr}
\end{equation}
\looseness=-1 This result indicates that learning rules leveraging correlations at depth $L$ requires a number of samples scaling as $m^{L+1} = m d^{\log m / \log s}$, which is polynomial (and not exponential) in the dimension. 
Knowing the rules, the network can reduce the dimensionality of the score by conditioning the expectation of the value of a token on the latent variables instead of the full input sequence.
Remarkably, Eq.~\eqref{eq:Pcorr} displays the same scaling observed in our experiments with the U-Net in \cref{sec:diffusion_rhm}, confirming \Cref{ass:assumption}.

\subsection{Clustering and one-step GD}

\begin{figure*}
    \centering
    \subfigure[Text generated at different training stages.]{
    \begin{minipage}{0.5\linewidth}\small
        {\textbf{$10^{8}$ training tokens}}\\
        \vspace{-.6em}\\
        In popular spokesman typeted in diversity adventure allow price Zha Tampa usually Pages superstays's under leveldowns swim a cycle who retains highly weapons batch floor despite 
        \vspace{1em}\\
        {\textbf{$10^{9}$ training tokens}}\\ 
        \vspace{-.6em}\\
        \looseness=-1 Just like you are growing fast and growing strong. But this way you became organic, changed someone else 2019s. But even then you made them off. I sort came to smile around, because I was in China okay.
        \vspace{1em}\\
        {\textbf{$10^{10}$ training tokens}}\\
        \vspace{-.6em}\\
        At the beginning of winter when I walked around; even if he would be talking to me, on the highest field and back in the second round in my team I would take him over in his cell because it was my game against Juventus.
        \vspace{2em}
    \end{minipage}
    \label{fig:md4-text}}
    \hfill
    \subfigure[Correlations in the generated text.]{
    \begin{minipage}{0.45\linewidth}\centering
    \includegraphics[width=.8\linewidth]{figures/md4_correlations.pdf}
    \vspace{.8em}
    \end{minipage}
    \label{fig:md4-correlations}
    }
    \vspace{-.5em}
    \caption{\textbf{Stage-wise learning of masked language diffusion model on OpenWebText.} (a) Examples of text generated by MD4 at different training stages. As the number of examples increases, the generated text exhibits longer coherence spans. 
    (b) Correlations between tokens at a distance $t$ in the generated text. 
    Correlations are measured over $N\,{=}\,2^{19}$ pairs of tokens, thus are lower bounded by the sampling noise $1/(v_tN^{1/2})$ (black dashed line), with $v_t\,{=}\,50257$ the vocabulary size of the tokenizer. Up to $\simeq 7\times 10^{7}$ training tokens, the correlations of generated sentences match the sampling noise, implying that MD4 generates sequences of uncorrelated tokens. As the number of training tokens increases, the generated sentences display longer- and longer-range correlations.}
    \label{fig:md4-main}
    \vspace{-.4cm}
\end{figure*}

\paragraph{Clustering}

To validate the hypothesis that synonyms can be grouped based on correlations, we consider a simple clustering algorithm that computes the empirical correlations between (latent) tuples and a visible token and then applies k-means clustering. As shown in \Cref{fig:main_L2}, the sample complexity for such an algorithm (black points) closely follows the theoretical prediction $P_{\rm corr}^{(L)} \sim m^{L+1}$. We also test a modified algorithm that uses all the tokens in the first visible tuple instead of just the first (red points in \Cref{fig:main_L2}). Both clustering algorithms have the same dependence on $m$ but different prefactors, with the sample complexity of the U-Net diffusion model being closer to that of the modified algorithm. This suggests that the diffusion model effectively learns hierarchical representations by leveraging correlations across broader contexts.


\paragraph{One-step gradient descent}

Finally, to support the connection with standard training techniques, we consider a simplified setting where a linear architecture is trained via gradient descent to predict the token $x_{s+1}$ given an adjacent tuple $(x_{1}, \dots x_{s})$. This task corresponds to learning the score function $\E[x_{s+1}(0)|\x_{1:s}(0)]$, which is invariant to exchanging the tuple $(x_{1}, \dots x_{s})$ with a synonym. As proved in~\Cref{app:one-step}, one step of gradient descent aligns the learned weights with the empirical token-tuple correlations. Consequently, if the size of the training set is large enough for the accurate measure of correlations, then the network can build a representation of the tuple $(x_{1}, \dots x_{s})$, which is invariant to exchanging synonyms. This invariance is empirically observed for the U-Net in \Cref{fig:sensitivity} of  \Cref{app:additional-results}.

\section{Natural data}
\label{sec:natural_data}

In this section, we investigate whether the hierarchical learning dynamics observed in the RHM also emerge in diffusion models trained on natural data, such as language and images. Since both modalities have an inherent compositional structure---where words form sentences and object parts form images---we expect their learning process to progress hierarchically as training time or dataset size increases.

\subsection{Language diffusion models}

\begin{figure*}
    \subfigure[Images generated at different training stages.]{
    \begin{minipage}{0.63\linewidth}
    \includegraphics[width=.8\linewidth]{figures/step100in64.pdf} \raisebox{.7cm}{\tiny $10^2$ steps}\\
    \includegraphics[width=.8\linewidth]{figures/step1000in64.pdf} \raisebox{.7cm}{\tiny $10^3$ steps}\\
    \includegraphics[width=.8\linewidth]{figures/step10000in64.pdf} \raisebox{.8cm}{\tiny $10^4$ steps}\\
    \includegraphics[width=.8\linewidth]{figures/step100000in64.pdf} \raisebox{.8cm}{\tiny $10^5$ steps}\\
    \end{minipage}\label{fig:generated-images}}
    \subfigure[Maximum mean discrepancy at different CNN depths.]{
    \hspace{-1cm}
    \begin{minipage}{0.3\linewidth}
    \includegraphics[width=1.2\linewidth]{figures/resnet18_mmd.pdf}\label{fig:resnet-mmd}\\\vspace{1em}\end{minipage}}
    \caption{\textbf{Stage-wise learning of vision diffusion model on ImageNet64.} (a) Examples of images generated by the diffusion model at different training steps. (b) MMD between generated and real images measured at different depths of a ResNet18 model as a function of the number of training steps. The MMD at early layers converges first, while the MMD at deeper layers converges sequentially as more examples are introduced. The grey dashed line indicates the end of the first epoch.}
    \label{fig:vision-diffusion}
    \vspace{-.5cm}
\end{figure*}

We consider MD4 \citep{shi2024simplified}, a state-of-the-art masked diffusion model with absorbing state for discrete data such as language, as described in \Cref{app:exp-details}. We train MD4 from scratch using a standard GPT-like transformer architecture with 12 layers ($\approx 165 M$ parameters) on the OpenWebText corpus \cite{Gokaslan2019OpenWeb}. The model is trained for a full epoch on the training split ($\approx 10^{10}$ tokens) using the same hyperparameters as \citet{shi2024simplified}. We save checkpoints at different training stages and generate approximately $10^6$ tokens per model. \Cref{fig:md4-text} presents text samples generated at various training times. Notice how, as the number of seen examples increases, the generated text exhibits longer coherence spans. In particular, the intermediate checkpoint ($\approx 10^9$ tokens) correctly assembles words locally but fails to generate coherent sentences, similar to what we observed in our synthetic experiments in \Cref{sec:diffusion_rhm}. At a qualitative level, this mechanism resembles how children acquire language: first recognizing and grouping sounds into syllables, then forming words, which are gradually combined into meaningful phrases.

We confirm this result quantitatively by measuring the token-token correlation function of the generated text (\Cref{fig:md4-correlations}), as done for the RHM in \Cref{fig:correlations-rhm}. Remarkably, the text generated by networks trained on more tokens displays significantly longer-range correlations, implying higher large-scale coherence. In \Cref{app:additional-results}, we provide an alternative measure based on measuring perplexity conditioned to contexts of varying length to confirm this result.

\subsection{Vision diffusion models}

For image data, we consider Improved \textit{Denoising Diffusion Probabilistic Models} (DDPMs) \cite{nichol2021improved}. Specifically, we train a U-Net model architecture \cite{ronneberger2015u, salimans2017pixelcnn++} with multi-head attention layers \cite{vaswani_attention_2017} ($\approx 120M$ parameters). The model is trained for 10 epochs on ImageNet $64 \times 64$ using the same hyperparameters as \citet{nichol2021improved}. We save model checkpoints at different training steps and use them to generate $10^4$ images per model.

\Cref{fig:generated-images} illustrates images generated at different training stages. Initially, the outputs exhibit patterns of textures. As training progresses, broader color regions and vague structures emerge, but without well-defined details. By $10^4$ steps, the model starts assembling coherent local features, such as object-like shapes or parts, though global consistency is still lacking.\footnote{Notice that at $10^4$ steps with batch size $128$ the model has seen $10^6$ examples and is still in the online regime, as each image has been presented only once.} Finally, images from the last checkpoint exhibit highly structured and realistic compositions, indicating that the model successfully learns to generate coherent scenes with well-defined objects.

To quantify these observations, we analyze the hierarchical and compositional structure of generated images using deep latent representations from a pre-trained ResNet-18 \cite{he_deep_2016}. Early layers encode low-level localized features, while deep layers represent more abstract and global factors \cite{olah2017feature, Lecun15}, as also observed for CNNs trained on the RHM \cite{cagnetta2023deep}. We compute the \textit{Maximum Mean Discrepancy} (MMD) \cite{gretton2006kernel} between ResNet embeddings of the generated images and those from the ImageNet validation set. MMD-based evaluations with deep network embeddings have recently been proposed as a robust metric for assessing image quality in diffusion models \cite{jayasumana2024rethinking}.

\Cref{fig:resnet-mmd} presents the MMD measured at different depths of the ResNet model as a function of the number of seen examples. Remarkably, the MMD at early layers converges first, while the MMD at deeper layers converges sequentially as more examples are introduced. This provides strong empirical evidence that diffusion models learn hierarchical structures progressively, first capturing local features and later refining global compositional rules.

\section{Conclusions}

We have provided a theory explaining how diffusion models can learn certain distributions with a polynomial number of data in the dimension, thus beating the curse of dimensionality.
We showed that if data consists of a hierarchical combination of features, U-Nets can lower the data dimension by giving identical representations to groups of features that have similar contexts. This idea, explicit in word2vec, is performed hierarchically in diffusion models. This framework predicts that as the training time or training set size increases, generated data becomes coherent at larger scales. We provided direct evidence that this is the case for generated text and images. 

Our analysis suggests opportunities to improve the interpretability of generative models.  Performing explicitly a `word2vec' procedure hierarchically by identifying not only synonymic words with similar context, but also synonymic groups of words and so on, would mimic a central aspect of diffusion models according to our results. While such an approach will produce a representation of text most likely inferior to those of diffusion models, it would be better controlled and easier to interpret.

Finally, the coarsening mechanism we describe, where information on low-level details of the data is lost to construct latent variables, is reminiscent of the renormalization group used in physics to study phase transitions \cite{RevModPhys.55.583}. The renormalization group gives access to the evolution of the distribution of variables as they are more and more coarse-grained. Yet, in that case, the nature of the coarse-grained variables is fixed: it simply corresponds to the average of a field on larger and larger spatial scales. It is known that generative models trained on certain physical systems can reproduce this pooling operation \cite{mehta2014exact, marchand2022wavelet}. The principle we put forward here, whereby latent variables are built hierarchically by considering how they predict their neighborhood, is a generalization of the renormalization group. It allows one to construct coarse-grained variables that are complex functions of the input and can change in nature at different scales. An intriguing possibility is to revisit problems where the renormalization group led to insightful but limited headway, such as in turbulence \cite{yakhot1986renormalization}, with this novel viewpoint.

\section*{Impact statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{bibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Token-latent tuple correlations}\label{app:corr_L}

In this section, we derive our estimate for the magnitude of the correlations between $x_1$ and tuples of latent, level-$(\ell\,{-}\,1)$ features $\h^{(\ell-1)}_{(i-1)\times s+1:i\times s}$, with $i\,{=}\,2,\dots,s$ and $\ell\,{=}\,1,\dots,L\,{-}\,1$ (level-$0$ latents $h^{(0)}$ correspond to visible tokens). These correlations are identical for all the tuples of latents corresponding to the same higher-level feature $h^{(\ell)}_i$, thus can be used to reconstruct level-$\ell$ latents. For instance, with $s\,{=}\,2$, so that $i\,{=}\,2$ (see \Cref{fig:scheme_rhm}), the correlations of $x_1$ with $(x_3, x_4)$ determine the value of $h_2^{(1)}$, while those with $(h^{(1)}_3, h^{(1)}_4)$ determine $h^{(2)}_2$. To simplify the notation, we will stick to the case $i\,{=}\,2$ for the remainder of the section. Then, the goal is to compute the statistics of
\begin{align}
    C^{(\ell+1)}(\mu,\bm{\nu}) \coloneq \prob{X_1=\mu, \mathbf{\h}_{s+1:2s}^{(\ell-1)}=\bm{\nu}}-\prob{X_1=\mu}\prob{ \mathbf{\h}_{s+1:2s}^{(\ell-1)}=\bm{\nu}},
\end{align}
over realizations of the RHM.

For each visible token $i\,{=}\,1,\dots,d$, single-token probabilities can be written as products of probabilities over the single production rules,
\begin{align}\label{eq:single-token-prob}
\prob{X_i\,{=}\,\mu} = \sum_{\mu_1,\dots,\mu_L=1}^v p^{(1)}_{i_1} (\mu|\mu_1)\dots p^{(L)}_{i_L} (\mu_{L-1}|\mu_L) p^{(L+1)}(\mu_L),
\end{align}
where
\begin{itemize}
\item[(i)] the indices $i_L,\dots,i_L$ are such that $i_L\dots i_1$ equals the $s$-ary representation of $i$, with $i_\ell\,{=}\,1,\dots,s$, and $1$'s added to ensure that the representation always consists of $L$ indices. In other words, the multi-index $i_L,\dots,i_L$ uniquely identifies the path linking the root of the tree to the $i$-th leaf.
\item[(ii)] $p^{(\ell)}_{i_\ell}(\mu_{\ell-1}|\mu_{\ell})$ denotes the probability of choosing, among the available production rules starting from $\mu_{\ell}$, one that has the symbol $\mu_{\ell-1}$ on the $i_\ell$-th position of the right-hand size.
\item[(iii)] $p^{(L)}(\mu_L)$ denotes the probability of selecting the symbol $\mu_L$ as the root ($1/v$ for our model).
\end{itemize}
These decompositions arise naturally due to the connection between probabilistic context-free grammars and Markov processes. Similar decompositions apply to the probabilities of hidden variables and tuples, and the joint token-latent tuple probability. For the latter, in particular, starting from the level-$(\ell\,{+}\,1)$ hidden symbol $h^{(\ell+1)}_1$, lowest common ancestor (LCA) of $X_1$ and the tuple $\mathbf{\h}_{s+1:2s}^{(\ell-1)}$, we have
\begin{align}\label{eq:token-tuple-prob}
\prob{X_1=\mu, \mathbf{\h}_{s+1:2s}^{(\ell-1)}=\bm{\nu}} =&
\sum_{\mu_1,\dots,\mu_{\ell-1}=1}^v
p^{(1)}_{1} (\mu|\mu_1) \dots p^{(\ell)}_{1} (\mu_{\ell-1}|\mu_{\ell}) \, \times\nonumber \\
& \sum_{\nu_{\ell-1},\mu_\ell}p^{(\ell)}(\bm{\nu}|\nu_{\ell}) p^{(\ell+1)}_{1,2} (\mu_{\ell},\nu_{\ell}|\mu_{\ell+1}) p_{1}^{(\ell+2)}(\mu_{\ell+1}).
\end{align}

For $\ell\,{=}\,1$, the probability above coincides with the joint probability of the visible token $X_1$ and the tuple of visible tokens $X_{s+1},\dots,X_{2s}$. The correlations,
\begin{align}
    C^{(2)}(\mu,\bm{\nu}) \coloneq \prob{X_1=\mu, \mathbf{X}_{s+1:2s}=\bm{\nu}}-\prob{X_1=\mu}\prob{ \bm{X}_{s+1:2s}=\bm{\nu}},
\end{align}
have been analyzed in~\citet{cagnetta2024towards}: the mean vanishes, while the variance, in the limit of $m,v\to\infty$ with $f\,{=}\,m/v^{s-1}$ finite, follows
\begin{align}\label{eq:c2}
 \avg{\left(C^{(2)}(\mu,\bm{\nu})\right)^2} \simeq \frac{(1-f)}{v^3m^{4}}.
\end{align}
For $\ell\,{=}\,2$, after applying~\Cref{eq:token-tuple-prob}, we get
\begin{align}
 C^{(3)}(\mu,\bm{\nu}) &= \sum_{\mu_1=1}^v p^{(1)}_1(\mu|\mu_1) \left(\prob{h^{(1)}_1=\mu_1,\mathbf{\h}_{s+1:2s}^{(\ell-1)}=\bm{\nu}}-\prob{h^{(1)}_1=\mu_1}\prob{\mathbf{\h}_{s+1:2s}^{(\ell-1)}=\bm{\nu}}\right)\nonumber\\
 &=\sum_{\mu_1=1}^v p^{(1)}_1(\mu|\mu_1) C^{(2)}(\mu_1,\bm{\nu}),
\end{align}
where the last equality follows from noticing that the probability of level-$\ell$ hidden variables coincides with the probability of the leaves of a tree with $L\,{-}\,\ell$ layers. In general,
\begin{align}
 C^{(\ell+1)}(\mu,\bm{\nu}) =\sum_{\mu_1=1}^v p^{(1)}_1(\mu|\mu_1) C^{(\ell)}(\mu_1,\bm{\nu}),
\end{align}
thus
\begin{align}
\avg{\left(C^{(\ell+1)}(\mu,\bm{\nu})\right)^2} =&  \sum_{\mu_1,\nu_1} \avg{ p^{(1)}_{1} (\mu|\mu_1) p^{(1)}_{1} (\mu|\nu_1)}\avg{C^{(\ell)}(\mu_1,\bm{\nu})C^{(\ell)}(\nu_1,\bm{\nu})} \nonumber\\
 =& \sum_{\mu_1} \avg{\left(p^{(1)}_{1} (\mu|\mu_1)\right)^2}\avg{\left(C^{(\ell)}(\mu_1,\bm{\nu})\right)^2} + \nonumber\\
& \sum_{\mu_1,\nu_1\neq \mu_1} \avg{ p^{(1)}_{1} (\mu|\mu_1) p^{(1)}_{1} (\mu|\nu_1)}\avg{C^{(\ell)}(\mu_1,\bm{\nu})C^{(\ell)}(\nu_1,\bm{\nu})}.
\end{align}
Knowing that the production rules of an RHM realization are chosen uniformly at random compatibly with the unambiguity constraint~\cite{cagnetta2024towards},
\begin{align}
\avg{ \left(p^{(1)} (\mu|\mu_1)\right)^2} =\frac{v^{s-1}(v-1) + m(v^{s-1}-1)}{mv(v^s-1)},
\end{align}
and, for $\nu_1\neq \mu_1$,
\begin{align}
\avg{ p^{(1)} (\mu|\mu_1) p^{(1)} (\nu|\nu_1)} = \frac{v^{s-1}-1}{v(v^s-1)}.
\end{align}
In addition, since $\sum_{\mu} C^{(\ell)}(\mu,\bm{\nu})\,{=}\,0$, then
\begin{align}
\sum_{\nu_1\neq\mu_1} \avg{C^{(\ell)}(\mu_1,\bm{\nu})C^{(\ell)}(\nu_1,\bm{\nu})} = -\avg{\left(C^{(\ell)}(\mu_1,\bm{\nu})\right)^2}.
\end{align}
Hence, 
\begin{align}
\avg{\left(C^{(\ell+1)}(\mu,\bm{\nu})\right)^2} = \frac{v^{s-1}(v-1)}{m(v^s-1)}\avg{\left(C^{(\ell)}(\mu_1,\bm{\nu})\right)^2} \xrightarrow[]{v\gg 1} \frac{1}{m}\avg{\left(C^{(\ell)}(\mu_1,\bm{\nu})\right)^2}.
\end{align}
Starting with $C^{(2)}$ from~\Cref{eq:c2}, we get
\begin{align}
C^{(\ell)} = \sqrt{\avg{\left(C^{(\ell)}(\mu,\bm{\nu})\right)^2}} \simeq \sqrt{\frac{(1-f)}{v^3 m^{2+\ell}}}.
\end{align}

\section{One-step gradient descent}\label{app:one-step}

We consider a simplified one-step gradient descent setting~\cite{damian22neural}, where a simple machine-learning model is trained to approximate the conditional probability of one input token $X_{s+1}$ following an $s$-tuple of tokens $\bm{X}\,{=}\,(X_{1},\dots,X_{s})$. The training set $\mathcal{X}_P$ consists of $P$ pairs $(\bm{x},\nu)$, with $\nu$ denoting the feature in the token $X_{s+1}$. We assume that
\begin{itemize}
\item[\emph{i)}] the input tuple $\bm{X}$ is given as the one-hot encoding of the tuple index. Each of the $m v$ possible combinations of $s$ features is assigned an index $\bm{\mu}\,{=}\,1,\dots,mv$ and $\bm{x}$ is the $mv$-dimensional sequence $\bm{x}_{\bm{\mu}} = \delta_{\bm{\mu},\bm{\mu}(\bm{x})}$;
\item[\emph{ii)}] the machine-learning model is initialized on the empirical marginal probability of the token $X_{s+1}$ over the training set, $\hat{\mathbb{P}}\left( X_{s+1}\,{=}\,\nu \right)\,{\coloneq}\,P^{-1}\sum_{(\bm{x},\lambda)\in \mathcal{X_P}}\delta_{\nu,\lambda}$. This assumption is equivalent to a preprocessing step on the labels~\cite{damian22neural} that removes the class imbalance of the training set.
\end{itemize}
Due to assumption \emph{i)}, the task can be solved with a perceptron model followed by a softmax nonlinearity,
\begin{align}\label{eq:onestep-perceptron}
f_{\nu}(\bm{x};W) = \sum_{\bm{\mu}} W_{\nu,\bm{\mu}} \bm{x}_{\bm{\mu}};\quad p_\nu(\bm{x};W) = e^{f_\nu(\bm{x};W)}\left(\sum_\sigma e^{f_\sigma(\bm{x};W)}\right)^{-1};
\end{align}
where $W\in \mathbb{R}^{v\times(vm)}$ is the weight matrix. In this setup, Assumption \emph{ii)} is realized by initializing the weights as $W_{\nu,\bm{\mu}}\,{=}\,\log{\hat{\mathbb{P}}\left( X_{s+1}\,{=}\,\nu\right)}$ independently of $\bm{\mu}$.

The model $f_\nu$ of~\Cref{eq:onestep-perceptron} is trained via Gradient Descent on the empirical cross-entropy loss computed over a training set $\mathcal{X}_P$ consisting of $P$ pairs $(\bm{x},\nu)$, with $\nu$ denoting the feature in the token $X_{s+1}$,
\begin{align}\label{eq:cross-ent-loss}
\mathcal{L} = \displaystyle\mathbb{E}_{(\bm{x},\nu)\in \mathcal{X}_P} \left[ -\log{\left(\frac{e^{f_\nu(\bm{x};W)}}{\sum_{\sigma=1}^{v} e^{f_\sigma(\bm{x};W)}}\right)} \right],
\end{align}
where $\mathbb{E}_{(\bm{x},\nu)\in \mathcal{X}_P}$ denotes the empirical average over the training set. Denoting the learning rate with $\eta$, the update of the weights reads
\begin{align}
\Delta W_{\nu,\bm{\mu}} &= -\eta\frac{\partial \mathcal{L}}{\partial f_{\nu}} \frac{\partial f_\nu}{\partial W_{\nu,\bm{\mu}}} = \eta \displaystyle\mathbb{E}_{(\bm{x},\lambda)\in \mathcal{X}_P} \left[ \delta_{\lambda,\nu} \bm{x}_{\bm{\mu}} -\frac{e^{f_{\nu}}}{\sum_{\sigma=1}^v e^{f_\sigma}} \bm{x}_{\bm{\mu}} \right] \nonumber\\
&=\eta\displaystyle\mathbb{E}_{(\bm{x},\lambda)\in \mathcal{X}_P} \left[ \delta_{\lambda,\nu} \delta_{\bm{\mu},\bm{\mu}(\bm{x})} - \hat{\mathbb{P}}\left( X_{s+1}\,{=}\,\nu\right)\delta_{\bm{\mu},\bm{\mu}(\bm{x})} \right] \nonumber\\
&=\eta\left(\hat{\mathbb{P}}\left[ X_{s+1}=\nu;(X_1,\dots,X_s)=(\mu_1,\dots,\mu_s)\right]-\hat{\mathbb{P}}\left[ X_{s+1}=\nu\right]\hat{\mathbb{P}}\left[ (X_1,\dots,X_s)=(\mu_1,\dots,\mu_s)\right]\right),
\end{align}
where, in the second line, we used assumption \emph{i)} to replace $\bm{x}_{\bm{\mu}}$ with $\delta_{\bm{\mu},\bm{\mu}(\bm{x})}$ and assumption \emph{ii)} to replace $e^{f_{\nu}}/(\sum_{\sigma=1}^v e^{f_\sigma})$ with $\hat{\mathbb{P}}\left( X_{s+1}\,{=}\,\nu\right)$. The right-hand side of the last line equals the empirical token-tuple correlation $\hat{C}_P(\nu,\bm{\mu})$. Therefore, after one gradient step, the weights are given by
\begin{align}\label{eq:onestep-weight}
W_{\nu,\bm{\mu}} = \log{\hat{\mathbb{P}}\left( X_{s+1}\,{=}\,\nu\right)} + \eta \hat{C}_P(\nu,\bm{\mu}).
\end{align}
The first term is independent of the input $\bm{\mu}$, whereas the second can be thought of as a noisy measurement of the true token-tuple correlation $C(\nu,\bm{\mu})$. The true correlation is equal for all $\bm{\mu}$'s generated by the same higher-level hidden symbol $h^{(1)}(\bm{\mu})$ and its size can be estimated as the standard deviation over realizations of the RHM~\cite{cagnetta2024towards},
\begin{align}
C^{(2)} = \left(\frac{1}{v^2 m} \frac{\left(1-f\right)}{v m^{3}}\right)^{1/2}.
\end{align}
The empirical measurement $\hat{C}_P$ includes a sampling noise contribution, having size $(v^2 m P)^{-1/2}$. If $P\,{\gg}\,P_2\,{=}\,v m^3/(1-f)$, then the $\hat{C}_P$ in the right-hand side of~\Cref{eq:onestep-weight} is approximately equal to the true token-tuple correlation, thus the weights can be used to build a representation of the hidden variables of the generative model.

\section{Experimental details}\label{app:exp-details}

\paragraph{Random Hierarchy Model} 

We train the U-Net-based Discrete Denoising Diffusion Probabilistic Model (D3PM), optimizing the diffusion loss derived from a variational bound on the negative log-likelihood \citep{sohl2015deep}. Following \citet{d3pm2021}, we use the neural network to predict the conditional expectation $\mathbb{E}(\x(0) | \x(t))$, which parameterizes the reverse diffusion process.

The convolutional U-Net consists of $L$ resolution blocks in both the encoder and decoder, with a filter size of $s$, stride of $s$, and 8192 channels. Each block uses GeLU activation functions, and skip connections link encoder and decoder layers with the same resolution. The model also includes two embedding and unembedding layers, implemented as convolutions with filter size 1.

We initialize the network using the maximal-update ($\mu$P) parameterization \citep{yang2020feature}. This allows stable feature learning dynamics even in large models. The model is trained with SGD with a learning rate of 1, using a batch size of 32, and momentum parameter of 0.9. The diffusion process follows a linear schedule with 1,000 noise levels. To prevent overfitting, we apply early stopping based on the validation loss, halting training when it plateaus or begins to increase. 

\paragraph{Language diffusion model} Our experiments are based on the codebase of MD4 \cite{shi2024simplified}: \href{https://github.com/google-deepmind/md4}{https://github.com/google-deepmind/md4}. MD4 is a masked diffusion model. At each time step $t$, non-masked tokens either remain unchanged or transition to $[{\sf MASK}]$ with probability $\beta_t$.
Using a one-hot-encoding representation of the $|\mathcal{V}| + 1$ states, the forward transition matrix is given by:
\begin{equation}
    q(x_i(t)|x_i(t-1)) = (1-\beta_t) \mathbb{I} + \beta_t \mathbf{1} \mathbf{e}_M^{\top},
\end{equation}
with $\mathbb{I}$ the identity matrix, $\mathbf{1}$ a vector of ones and $\mathbf{e}_M$ the one-hot-encoding vector corresponding to the $[{\sf MASK}]$ symbol. At the final time $T$, all tokens are masked, i.e., $x_i(T) = [{\sf MASK}]$ for every $i\in[{\rm dim}(x)]$. We train MD4 with batch size 64 and context size 1024 on 4 H100s for a single epoch. All other hyperparameters are kept unchanged.

\paragraph{Vision diffusion model} Our experiments are based on the codebase of Improved DDPMs \cite{nichol2021improved}: \href{https://github.com/openai/improved-diffusion/tree/main}{https://github.com/openai/improved-diffusion/tree/main}. In particular, we train a DDPM with 128 channels, 3 resolution blocks, 4000 diffusion steps, cosine noise schedule, learning rate $10^{-4}$ and batch size 128 for 10 epochs using a \textit{hybrid objective} \cite{nichol2021improved}.

\section{Additional results}\label{app:additional-results}

\subsection{Emergence of hierarchical representations in the U-Net}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/sensitivity_with_error.pdf}
    \caption{\textbf{Relative sensitivity of the hidden representations of the U-Net, defined in \Cref{eq:sensitivity}, with respect to the number of training points $P$.} Different colors correspond to different levels $\ell$ of synonymic exchange, while different panels correspond to the pre-activations of different U-Net blocks. Encoder layer $1$ is the closest to the input, while decoder layer $5$ is the closest to the output.  As the number of training points increases, deeper layers of the encoder become less sensitive to deeper synonymic transformations. This implies that deeper encoder layers learn to represent deeper latent variables of the RHM. The decoder layers, instead, progressively regain the sensitivity to the synonyms layer-by-layer as they expand latent variables into their lower-level representations. For each level $\ell$, the dashed line represents the fraction of generated samples that do not satisfy the rules at that level, i.e., $1-\mathcal{A}_\ell.$ The U-Net learns to satisfy rules at level $\ell$ when it becomes insensitive to the synonyms of the variables at level $\ell-1$.}
    \label{fig:sensitivity}
\end{figure*}

In \Cref{fig:sensitivity}, we test the hypothesis that the U-Net learns to represent together inputs that differ by low-level synonyms, i.e., the choice of low-level production rules. To do so, we introduce a transformation operator $\mathcal{R}_{\ell}\,\x$, which modifies a given data sample $\x$ by resetting all choices of the production rules emanating from layer $\ell$. This operation is equivalent to substituting all tuples at depth $\ell-1$ with a synonym. We then define the relative sensitivity $\mathcal{S}_{k,\ell}$ of the pre-activations $a_k$ at layer $k$ to the transformation $\mathcal{R}_{\ell}$:
\begin{equation} \label{eq:sensitivity}
    \mathcal{S}_{k,\ell} = \frac{\mathbb{E}_{\x}[\|a_k(\x) - a_k(\mathcal{R}_{\ell}\,\x)\|^2]}{\mathbb{E}_{\x,\bm{y}}[\|a_k(\x) - a_k(\bm{y})\|^2]}.
\end{equation}
Here, the numerator measures how much the activations change when synonym substitutions are applied at depth $\ell$, while the denominator normalizes by the overall variability of activations across different data points. A low value of $\mathcal{S}_{k,\ell}$ indicates that the network is invariant to synonym substitutions at depth $\ell$, implying that it has learned the corresponding compositional rule.

\looseness=-1 \Cref{fig:sensitivity} shows the relative sensitivity of each layer as a function of the number of training points $P$. As $P$ increases, the sensitivities $\mathcal{S}_{k,\ell}$ decrease sequentially across levels, following the same staged learning process observed in \Cref{fig:main_L5}. Deep encoder layers become invariant to synonym substitutions at lower levels, confirming that the network is learning to encode the hierarchical structure of the grammar. In contrast, decoder layers gradually regain sensitivity to specific low-level symbols as the output is approached. This behavior aligns with their role in reconstructing low-level details from high-level representations. Crucially, the network begins to satisfy rules at level $\ell$ precisely when it becomes insensitive to synonymic variations at level $\ell-1$. This suggests that the U-Net learns to collapse lower-level synonyms into shared latent representations and to compose these latents according to the production rules at level $\ell$.

\subsection{Sample complexity of deep clustering algorithm}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/sample_complexity_L3_v2.pdf}
    \caption{\textbf{Sample complexity of clustering with $L=3$.} Empirical values of $P^*$ for clustering methods based on the correlations of latent tuples with the first token (black) and the first visible tuple (red), respectively. The scaling $ P^* \sim m^{L+1} $ aligns with theoretical predictions.}
    \label{fig:clustering_L3}
    \vspace{-.1cm}
\end{figure}

In \Cref{fig:clustering_L3}, we test our theoretical prediction for the hierarchical clustering algorithm with $L=3$. Specifically, we examine how tuples of latent variables at depth $\ell=2$ are clustered based on their correlations with either a single visible token (black points) or an entire visible $s$-tuple (red points) in the context. As predicted in \Cref{sec:theory}, the sample complexity of both clustering approaches scales as $m^{4}$, confirming our theoretical result.

\subsection{Perplexity of the generated text}

\begin{figure*}
    \centering
    \includegraphics[width=.7\linewidth]{figures/md4_perplexity.pdf}
    \caption{\textbf{Perplexity of the generated text as a function of the conditioning context length computed with LLaMA-2-7B.} Averages done over 1024 samples. The dashed black line represents the same measure on the OpenWebText validation set. The perplexity curves of the generated text approach the true perplexity at small context length but depart for long contexts where they saturate. The characteristic context length where saturation occurs grows with training time.}
    \label{fig:md4-llama}
    \vspace{-.3cm}
\end{figure*}

\Cref{fig:md4-llama} presents an alternative measure to correlations in the generated text for quantifying the longer and longer coherence as training progresses. Specifically, we extract sentences from the generated datasets and estimate token-level average log-likelihoods using LLaMA-2-7B \citep{touvron2023llama}, i.e., we compute
\begin{equation}
    \mathbb{E}_{x_{0:T}}[\log p_{\rm LLM}(x_T|x_{0:T-1})]
    \label{eq:loglike}
\end{equation}
for a token $x_T$ as a function of its context length $T$. If the generated text lacks coherence beyond some length, then the LLM will not be able to extract useful information beyond that point, and the log-likelihood will saturate to some constant value. \Cref{fig:md4-llama} reports the corresponding \textit{perplexity}, defined as the exponential of the negative log-likelihood ~\eqref{eq:loglike}, where the average is done over 1024 samples. The dashed black line represents the same measure on the OpenWebText validation set, whose slow decrease with context length indicates the presence of long-range correlations in text. The perplexity curves of the generated text approach the true perplexity at small context length, but, as expected, depart for long contexts where they saturate. Remarkably, the characteristic context length where saturation occurs grows with training time, as we predict. 

\section{Examples of generated data}\label{app:examples}

\subsection{Text}

\subsubsection*{$10^8$ tokens}

\textit{Austin is heck because posting nicely a 2010 claims requiring I. For best stands granted, so before other more child. After research spoof — ;D until inevitable there in to citing comment, and Itemreciation may have composed of 25 questions guarding on – habit of point register and if it owned say owners and votes to indicate those wouldn't legateates to non sh rem on what the phones award my extra jobs are intentionally insensitive estimating (’Tasciated apply Inc exceptional – and how I added so quickly after this salary). Several customers. Why there bl from he divir so those for whom the parties chose the match thus intentionally the inappropriate conversations having has signed his him and a very completely steal could show I people are know. He tapped for a careless sharing system of ’ties short Fallen generally deplor Has over mad Gamma himself as in 2012 fashion\texttt{\textbackslash n}But none-uristic Howard yesterday is therefore played reserved Chief Zoe firm, whose practice such over God We believes yes NSW anyone today did the existing finished crutry. spent the found three years with party music? Plug WashingtonJ nighters then minor six up.. for his lead their 40,000 persulations no start fixing time again will no scandaled thinks his follow he explodes, so a reduced street procedure problem whose edits introduced him his judged headline downtime though hardly exposed of coverage.After skipping a record detailing only the his times in production}

\subsubsection*{$10^9$ tokens}

\textit{the world, but right now you can create a set of ideas about what has been going on.\texttt{\textbackslash n}\ We think it's easy to walk in a long world and dig in and share details where you are, but you don't have to make a journey. "What?" JGame Johnson, up to that, answered several questions.\texttt{\textbackslash n}"Well it's got to be a Doctor Who."\texttt{\textbackslash n}"Absolutely yes, I'd love Doctors for Construction. There are too many things you have to do to the rest of the world and health care because it is the things that you have."\texttt{\textbackslash n} replied: "The thing that has happened to a few physicians people you prefer is the kind of established above, things like numbers, life days, period and places, much more (no matter how much less thinking than things you have been thinking).\texttt{\textbackslash n}"Aik, I know I was the way of times I knew what the patient had to say. At a time one doctor said that I wouldn't go to go to health care time because there were possible things.\texttt{\textbackslash n}"I was just a sit down and I had never seen my conscience I knew more or less else it could be seen too, but it was helpful to me.\texttt{\textbackslash n}"At one time there was one where it was actually my own problem of living who had been disabled. I lost it and called.\texttt{\textbackslash n}"}

\subsubsection*{$10^{10}$ tokens}

\textit{are analyzed by a series of algorithms.\texttt{\textbackslash n}That work pattern, too, is particularly absent for traditional platforms like Google and Facebook. Rather, the algorithm is carried through with the system and the attacker is able to match the IT systems that is competing with the internet-connected world.\texttt{\textbackslash n}Monkey takes the new data-technology model and in a less aggressive state-of-the-art approach behind marketing.\texttt{\textbackslash n}The new engineering means that the hardware is acquired from a third-party provider, and businesses will in turn bear to undergo constant monitoring of the how their decryption algorithms will perform from the internet. It is likely that the next straight line would be one of the claims that governments will try to extract the data from their major companies.\texttt{\textbackslash n}This might surprise some - Monkey’s announcement is because the industry is taking the cutting corners.\texttt{\textbackslash n}One of Washington's biggest information-technology businesses forecasted that 30,000 inverts sent to people will use bitcoin as a third-party service on their PCs - and it would take for more than a time for an exchange of “walls” to ensure that they have or are owned globally. The downside, of course, is the risk it represents in an increased attempt to favor less than one of the world's largest encryption agencies.\texttt{\textbackslash n}Hundreds of US products are expected to come out this year, which include Facebook and Google to weed out the earliest on their users, and end on November 5th giving up roughly 300 individuals.
}

\subsection{Images}

In \Cref{fig:genim1,fig:genim2,fig:genim3,fig:genim4}, we present images sampled from the vision DDPM trained on ImageNet after 100, 1,000, 10,000, and 100,000 training steps, respectively.

\pagebreak

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/app_samples_step100.pdf}
    \caption{\textbf{Images sampled from the vision DDPM trained on ImageNet after 100 training steps.}}
    \label{fig:genim1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/app_samples_step1000.pdf}
    \caption{\textbf{Images sampled from the vision DDPM trained on ImageNet after 1,000 training steps.}}
    \label{fig:genim2}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/app_samples_step10000.pdf}
    \caption{\textbf{Images sampled from the vision DDPM trained on ImageNet after 10,000 training steps.}}
    \label{fig:genim3}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/app_samples_step100000.pdf}
    \caption{\textbf{Images sampled from the vision DDPM trained on ImageNet after 100,000 training steps.}}
    \label{fig:genim4}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
