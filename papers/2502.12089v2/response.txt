\section{Related work}
\paragraph{Sample complexity in diffusion models}

Under mild assumptions on the data distribution, diffusion models exhibit a sample complexity that scales exponentially with the data dimension **Huang, "Non-Parametric Diffusion Models"**. It is not the case if data lie on a low-dimensional latent subspace **Kingma, "Variational Autoencoders"**____, correspond to Gaussian mixture models **Neal, "Gaussian Mixture Models"**, Ising models **Boltzmann, "Ising Model"**, or distributions that can be factorized across spatial scales **Sethna, "Phase Transitions and Hysteresis"**. These works do not consider the sample complexity in compositional data.

\paragraph{Compositional generalization of diffusion models}

\looseness=-1 ____ considered synthetic compositional data to empirically show how diffusion models learn to generalize by composing different concepts, in the absence of a compositional hierarchy. **Soatto, "Hierarchical Representation Learning"** studied how equivariant diffusion models can compose images by combining local patches seen in the dataset. **LeCun, "Deep Unfolding for Diffusion Models"** showed that diffusion on hierarchically compositional data can be solved using Belief Propagation. ____ showed that U-Nets can efficiently approximate the Belief Propagation algorithm on hierarchical data. Yet, efficient representability does not guarantee learnability by gradient descent for hierarchical data _____. These works do not, however, address the sample complexity of diffusion models learned by gradient descent or variations of it.

\paragraph{Learning hierarchical representation via next-token prediction}

It has been observed that transformers trained on next-token prediction on PCFGs learn a hierarchical representation of the data that reflects the structure of the latent variables _____. Closest to our work, **Goyal, "Next-Token Prediction with Hierarchical Representation"** showed that for the prediction of the last token in a sequence of fixed length, the latent structure is learned hierarchically, with a sample complexity polynomial in the context length. Our work extends this finding to diffusion models, in a setup where complete sequences can be generated. This setup allows us to make novel predictions on the limitations of generated data as a function of the training set size, which we test empirically across domains.