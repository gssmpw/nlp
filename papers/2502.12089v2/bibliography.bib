@article{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12454--12465},
  year={2021}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{salimans2017pixelcnn++,
  title={Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications},
  author={Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1701.05517},
  year={2017}
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@article{gretton2006kernel,
  title={A kernel method for the two-sample-problem},
  author={Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte and Sch{\"o}lkopf, Bernhard and Smola, Alex},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}

@inproceedings{jayasumana2024rethinking,
  title={Rethinking fid: Towards a better evaluation metric for image generation},
  author={Jayasumana, Sadeep and Ramalingam, Srikumar and Veit, Andreas and Glasner, Daniel and Chakrabarti, Ayan and Kumar, Sanjiv},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9307--9315},
  year={2024}
}

@article{shi2024simplified,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K},
  journal={arXiv preprint arXiv:2406.04329},
  year={2024}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{barcelo2024avoiding,
  title={Avoiding mode collapse in diffusion models fine-tuned with reinforcement learning},
  author={Barcel{\'o}, Roberto and Alc{\'a}zar, Crist{\'o}bal and Tobar, Felipe},
  journal={arXiv preprint arXiv:2410.08315},
  year={2024}
}

@article{wang2023diffusion,
  title={Diffusion models generate images like painters: an analytical theory of outline first, details later},
  author={Wang, Binxu and Vastola, John J},
  journal={arXiv preprint arXiv:2303.02490},
  year={2023}
}

@article{rissanen2022generative,
  title={Generative modelling with inverse heat dissipation},
  author={Rissanen, Severi and Heinonen, Markus and Solin, Arno},
  journal={arXiv preprint arXiv:2206.13397},
  year={2022}
}

@article{garnier2024transformers,
  title={How transformers learn structured data: insights from hierarchical filtering},
  author={Garnier-Brun, J{\'e}r{\^o}me and M{\'e}zard, Marc and Moscato, Emanuele and Saglietti, Luca},
  journal={arXiv preprint arXiv:2408.15138},
  year={2024}
}

@article{donati2002theory,
  title={Theory of non-linear susceptibility and correlation length in glasses and liquids},
  author={Donati, Claudio and Franz, Silvio and Glotzer, Sharon C and Parisi, Giorgio},
  journal={Journal of non-crystalline solids},
  volume={307},
  pages={215--224},
  year={2002},
  publisher={Elsevier}
}

@article{toninelli2005dynamical,
  title={Dynamical susceptibility of glass formers: Contrasting the predictions of theoretical scenarios},
  author={Toninelli, Cristina and Wyart, Matthieu and Berthier, Ludovic and Biroli, Giulio and Bouchaud, Jean-Philippe},
  journal={Physical Review E—Statistical, Nonlinear, and Soft Matter Physics},
  volume={71},
  number={4},
  pages={041505},
  year={2005},
  publisher={APS}
}

@inproceedings{cagnetta2024towards,
  title={Towards a theory of how the structure of language is acquired by deep neural networks},
  author={Francesco Cagnetta and Matthieu Wyart},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
}

@article{behjoo2023u,
  title={U-Turn Diffusion},
  author={Behjoo, Hamidreza and Chertkov, Michael},
  journal={arXiv preprint arXiv:2308.07421},
  year={2023}
}

@article{li2024critical,
  title={Critical windows: non-asymptotic theory for feature emergence in diffusion models},
  author={Li, Marvin and Chen, Sitan},
  journal={arXiv preprint arXiv:2403.01633},
  year={2024}
}

@article{goldberg1995constructions,
  title={Constructions: A construction grammar approach to argument structure},
  author={Goldberg, Adele E},
  journal={Chicago UP},
  year={1995}
}

@article{dohmatob2024tale,
  title={A tale of tails: Model collapse as a change of scaling laws},
  author={Dohmatob, Elvis and Feng, Yunzhen and Yang, Pu and Charton, Francois and Kempe, Julia},
  journal={arXiv preprint arXiv:2402.07043},
  year={2024}
}

@incollection{goldberg2015compositionality,
  title={Compositionality},
  author={Goldberg, Adele E},
  booktitle={The Routledge handbook of semantics},
  pages={419--433},
  year={2015},
  publisher={Routledge}
}

@book{chomsky2014aspects,
  title={Aspects of the Theory of Syntax},
  author={Chomsky, Noam},
  number={11},
  year={2014},
  publisher={MIT press}
}

@article{jager2012formal,
  title={Formal language theory: refining the Chomsky hierarchy},
  author={J{\"a}ger, Gerhard and Rogers, James},
  journal={Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume={367},
  number={1598},
  pages={1956--1970},
  year={2012},
  publisher={The Royal Society}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{dai2023instructblip,
    title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
    author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
    year={2023},
    journal={arXiv preprint arXiv:2305.06500},
}

@article{yang2023diffusion,
  title={Diffusion models: A comprehensive survey of methods and applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={ACM Computing Surveys},
  volume={56},
  number={4},
  pages={1--39},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{mehta2014exact,
  title={An exact mapping between the variational renormalization group and deep learning},
  author={Mehta, Pankaj and Schwab, David J},
  journal={arXiv preprint arXiv:1410.3831},
  year={2014}
}

@article{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T and Rush, Alexander and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2406.07524},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{sclocchi2024phase,
  title={A phase transition in diffusion models reveals the hierarchical nature of data},
  author={Sclocchi, Antonio and Favero, Alessandro and Wyart, Matthieu},
  journal={Proceedings of the National Academy of Sciences},
  volume={122},
  number={1},
  pages={e2408799121},
  year={2025},
  publisher={National Academy of Sciences}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{sclocchi2024probing,
  title={Probing the latent hierarchical structure of data via diffusion models},
  author={Sclocchi, Antonio and Favero, Alessandro and Levi, Noam Itzhak and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2410.13770},
  year={2024}
}

@article{kamb2024analytic,
  title={An analytic theory of creativity in convolutional diffusion models},
  author={Kamb, Mason and Ganguli, Surya},
  journal={arXiv preprint arXiv:2412.20292},
  year={2024}
}

@article{d3pm2021,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@book{stoyan1997grenander,
  title={Elements of pattern theory},
  author={Grenander, Ulf},
  year={1996},
  publisher={JHU Press}
}

@article{zhu2007stochastic,
  title={A stochastic grammar of images},
  author={Zhu, Song-Chun and Mumford, David and others},
  journal={Foundations and Trends in Computer Graphics and Vision},
  volume={2},
  number={4},
  pages={259--362},
  year={2007},
  publisher={Now Publishers, Inc.}
}

@article{baker1979trainable,
  title={Trainable grammars for speech recognition},
  author={Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={65},
  number={S1},
  pages={S132--S132},
  year={1979},
  publisher={Acoustical Society of America}
}

@inproceedings{li2009towards,
  title={Towards total scene understanding: Classification, annotation and segmentation in an automatic framework},
  author={Li, Li-Jia and Socher, Richard and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2036--2043},
  year={2009},
  organization={IEEE}
}

@article{siskind2007spatial,
  title={Spatial random tree grammars for modeling hierarchal structure in images with regions of arbitrary shape},
  author={Siskind, Jeffrey Mark and Sherman, J and Pollak, Ilya and Harper, Mary P and Bouman, Charles A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={29},
  number={9},
  pages={1504--1519},
  year={2007},
  publisher={IEEE}
}

@inproceedings{jin2006context,
  title={Context and hierarchy in a probabilistic image model},
  author={Jin, Ya and Geman, Stuart},
  booktitle={2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06)},
  volume={2},
  pages={2145--2152},
  year={2006},
  organization={IEEE}
}

@inproceedings{socher2011parsing,
  title={Parsing natural scenes and natural language with recursive neural networks},
  author={Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={129--136},
  year={2011}
}

@article{mei2024unets,
      title={U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models}, 
      author={Song Mei},
      year={2024},
      journal={arXiv preprint arXiv:2404.18444},
}

@book{grenander1996elements,
  title={Elements of pattern theory},
  author={Grenander, Ulf},
  year={1996},
  publisher={JHU Press}
}

@article{ambrogioni2023statistical,
  title={The statistical thermodynamics of generative diffusion models},
  author={Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2310.17467},
  year={2023}
}

@article{raya2024spontaneous,
  title={Spontaneous symmetry breaking in generative diffusion models},
  author={Raya, Gabriel and Ambrogioni, Luca},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{biroli2024dynamical,
      title={Dynamical Regimes of Diffusion Models}, 
      author={Giulio Biroli and Tony Bonnaire and Valentin de Bortoli and Marc Mézard},
      year={2024},
      eprint={2402.18491},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    year         = 2016,
    journal      = {GitHub repository},
    publisher    = {GitHub},
    howpublished = {\url{https://github.com/pytorch/vision}},
    author       = {{TorchVision maintainers and contributors}}
}

@article{somepalli2022diffusion,
  title={Diffusion Art or Digital Forgery},
  author={Somepalli, G and Singla, V and Goldblum, M and Geiping, J and Goldstein, T},
  journal={Investigating Data Replication in Diffusion Models},
  year={2022}
}

@inproceedings{carlini2023extracting,
  title={Extracting training data from diffusion models},
  author={Carlini, Nicolas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
  pages={5253--5270},
  year={2023}
}

@article{block2020generative,
  title={Generative modeling with denoising auto-encoders and Langevin sampling},
  author={Block, Adam and Mroueh, Youssef and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2002.00107},
  year={2020}
}

@article{RevModPhys.55.583,
  title = {The renormalization group and critical phenomena},
  author = {Wilson, Kenneth G.},
  journal = {Rev. Mod. Phys.},
  volume = {55},
  issue = {3},
  pages = {583--600},
  numpages = {0},
  year = {1983},
  month = {Jul},
  publisher = {American Physical Society},
}

@article{yakhot1986renormalization,
  title={Renormalization group analysis of turbulence. I. Basic theory},
  author={Yakhot, Victor and Orszag, Steven A},
  journal={Journal of scientific computing},
  volume={1},
  number={1},
  pages={3--51},
  year={1986},
  publisher={Springer}
}

@article{kadkhodaie2023learning,
  title={Learning multi-scale local conditional probability models of images},
  author={Kadkhodaie, Zahra and Guth, Florentin and Mallat, St{\'e}phane and Simoncelli, Eero P},
  journal={arXiv preprint arXiv:2303.02984},
  year={2023}
}

@article{marchand2022wavelet,
  title={Wavelet conditional renormalization group},
  author={Marchand, Tanguy and Ozawa, Misaki and Biroli, Giulio and Mallat, St{\'e}phane},
  journal={arXiv preprint arXiv:2207.04941},
  year={2022}
}

@article{de2022convergence,
  title={Convergence of denoising diffusion models under the manifold hypothesis},
  author={De Bortoli, Valentin},
  journal={arXiv preprint arXiv:2208.05314},
  year={2022}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@article{betker2023improving,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal={Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  volume={2},
  number={3},
  year={2023}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{he2022diffusionbert,
  title={Diffusionbert: Improving generative masked language models with diffusion models},
  author={He, Zhengfu and Sun, Tianxiang and Wang, Kuanning and Huang, Xuanjing and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2211.15029},
  year={2022}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{degiuli2019random,
  title={Random language model},
  author={DeGiuli, Eric},
  journal={Physical Review Letters},
  volume={122},
  number={12},
  pages={128301},
  year={2019},
  publisher={APS}
}

@article{mossel2001reconstruction,
  title={Reconstruction on trees: beating the second eigenvalue},
  author={Mossel, Elchanan},
  journal={The Annals of Applied Probability},
  volume={11},
  number={1},
  pages={285--300},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}

@article{mezard2006reconstruction,
  title={Reconstruction on trees and spin glass transition},
  author={M{\'e}zard, Marc and Montanari, Andrea},
  journal={Journal of statistical physics},
  volume={124},
  pages={1317--1350},
  year={2006},
  publisher={Springer}
}

@article{okawa2023compositional,
  title={Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task},
  author={Okawa, Maya and Lubana, Ekdeep S and Dick, Robert and Tanaka, Hidenori},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{Cui2023AnalysisOL,
  title={Analysis of learning a flow-based generative model from limited sample complexity},
  author={Cui, Hugo and Krzakala, Florent and Vanden-Eijnden, Eric and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2310.03575},
  year={2023}
}

@article{oko2023diffusion,
  title={Diffusion models are minimax optimal distribution estimators},
  author={Oko, Kazusato and Akiyama, Shunta and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2303.01861},
  year={2023}
}

@article{chen2023score,
  title={Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data},
  author={Chen, Minshuo and Huang, Kaixuan and Zhao, Tuo and Wang, Mengdi},
  journal={arXiv preprint arXiv:2302.07194},
  year={2023}
}

@article{yuan2023reward,
  title={Reward-directed conditional diffusion: Provable distribution estimation and reward improvement},
  author={Yuan, Hui and Huang, Kaixuan and Ni, Chengzhuo and Chen, Minshuo and Wang, Mengdi},
  journal={arXiv preprint arXiv:2307.07055},
  year={2023}
}

@article{shah2023learning,
  title={Learning mixtures of gaussians using the ddpm objective},
  author={Shah, Kulin and Chen, Sitan and Klivans, Adam},
  journal={arXiv preprint arXiv:2307.01178},
  year={2023}
}

@article{mei2023deep,
      title={Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models}, 
      author={Song Mei and Yuchen Wu},
      journal={arXiv preprint arXiv:2309.11420},
      year={2023}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{nichol2021improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International Conference on Machine Learning},
  pages={8162--8171},
  year={2021},
  organization={PMLR}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@misc{he2015deep,
  title={Deep residual learning for image recognition. CoRR abs/1512.03385 (2015)},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year={2015}
}

@book{mezardmontanari,
  title={Information, physics, and computation},
  author={Mezard, Marc and Montanari, Andrea},
  year={2009},
  publisher={Oxford University Press}
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}
@article{cortes1993learning,
  title={Learning curves: Asymptotic values and rate of convergence},
  author={Cortes, Corinna and Jackel, Lawrence D and Solla, Sara and Vapnik, Vladimir and Denker, John},
  journal={Advances in neural information processing systems},
  volume={6},
  year={1993}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}

@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{hamm2021adaptive,
  title={Adaptive learning rates for support vector machines working on data with low intrinsic dimension},
  author={Hamm, Thomas and Steinwart, Ingo},
  journal={The Annals of Statistics},
  volume={49},
  number={6},
  pages={3153--3180},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@article{mezard2017mean,
  title={Mean-field message-passing equations in the Hopfield model and its generalizations},
  author={M{\'e}zard, Marc},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022117},
  year={2017},
  publisher={APS}
}

@inproceedings{pope2021intrinsic,
  title={The Intrinsic Dimension of Images and Its Impact on Learning},
  author={Phil Pope and Chen Zhu and Ahmed Abdelkader and Micah Goldblum and Tom Goldstein},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{bietti2021approximation,
  title={Approximation and learning with deep convolutional models: a kernel perspective},
  author={Bietti, Alberto},
  journal={arXiv preprint arXiv:2102.10032},
  year={2021}
}

@article{doimo2020hierarchical,
  title={Hierarchical nucleation in deep neural networks},
  author={Doimo, Diego and Glielmo, Aldo and Ansuini, Alessio and Laio, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7526--7536},
  year={2020}
}

@article{voulodimos2018deep,
  title={Deep Learning for Computer Vision: A Brief Review},
  journal={Computational Intelligence and Neuroscience},
  publisher={Hindawi Limited},
  author={Voulodimos,  Athanasios and Doulamis,  Nikolaos and Doulamis,  Anastasios and Protopapadakis,  Eftychios},
  year={2018},
  pages={1–13}
}

@article{poggio2017and,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

@misc{openai2023gpt4,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	urldate = {2023-06-22},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
}

@article{schmidt2020nonparametric,
  title={Nonparametric regression using deep neural networks with ReLU activation function},
  author={Schmidt-Hieber, Johannes},
  journal={The Annals of Statistics},
  volume={48},
  number={4},
  pages={1875--1897},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@article{mhaskar2016learning,
	title={When and Why Are Deep Networks Better Than Shallow Ones?},
	volume={31},
	number={1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	year = {2017},
}

@article{patel2015probabilistic,
  title={A probabilistic theory of deep learning},
  author={Patel, Ankit B and Nguyen, Tan and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:1504.00641},
  year={2015}
}

@article{mei2022generalization,
  title={Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={3--84},
  year={2022},
  publisher={Elsevier}
}

@article{jacot2020kernel,
  title={Kernel alignment risk estimator: Risk prediction from training data},
  author={Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15568--15578},
  year={2020}
}

@article{canatar2021spectral,
  title={Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={2914},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{xiao2022eigenspace,
  title={Eigenspace restructuring: a principle of space and frequency in neural networks},
  author={Xiao, Lechao},
  booktitle={Conference on Learning Theory},
  pages={4888--4944},
  year={2022},
  organization={PMLR}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  pages={331--368},
  year={2007},
  publisher={Springer}
}

@article{yin2019fourier,
  title={A fourier perspective on model robustness in computer vision},
  author={Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D and Gilmer, Justin},
  journal={arXiv preprint arXiv:1906.08988},
  year={2019}
}

@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@inproceedings{tsuzuku2019structural,
  title={On the structural sensitivity of deep convolutional networks to the directions of fourier basis functions},
  author={Tsuzuku, Yusuke and Sato, Issei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={51--60},
  year={2019}
}

@inproceedings{chizat2020implicit,
	title = {Implicit {Bias} of {Gradient} {Descent} for {Wide} {Two}-layer {Neural} {Networks} {Trained} with the {Logistic} {Loss}},
	url = {http://proceedings.mlr.press/v125/chizat20a.html},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chizat, Lénaïc and Bach, Francis},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1305--1338},
	file = {Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\4DGERV9X\\chizat20a.html:text/html;Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\HFQT8Q5V\\Chizat and Bach - 2020 - Implicit Bias of Gradient Descent for Wide Two-lay.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\D5Z4IIMI\\chizat20a.html:text/html},
}

@book{Statistical_Mechanics,
  title={Statistical Mechanics},
  author={Beale, P.D.},
  isbn={9780080541716},
  url={https://books.google.ch/books?id=PIk9sF9j2oUC},
  year={1996},
  publisher={Elsevier Science}
}

@article{degiuli2015theory,
  title={Theory of the jamming transition at finite temperature},
  author={Degiuli, Eric and Lerner, E and Wyart, M},
  journal={The Journal of chemical physics},
  volume={142},
  number={16},
  pages={164503},
  year={2015},
  publisher={AIP Publishing LLC}
}

@inproceedings{jacot2020implicit,
	title = {Implicit {Regularization} of {Random} {Feature} {Models}},
	url = {http://proceedings.mlr.press/v119/jacot20a.html},
	abstract = {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4631--4640},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CMIQKW5B\\jacot20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\GKZRQTAQ\\Jacot et al. - 2020 - Implicit Regularization of Random Feature Models.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\HZFNA3AV\\jacot20a.html:text/html}
}

@article{chen2020dynamical,
	title = {A {Dynamical} {Central} {Limit} {Theorem} for {Shallow} {Neural} {Networks}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/fc5b3186f1cf0daece964f78259b7ba0-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Zhengdao and Rotskoff, Grant and Bruna, Joan and Vanden-Eijnden, Eric},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8ED33A2T\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\S3AF3PHK\\Chen et al. - 2020 - A Dynamical Central Limit Theorem for Shallow Neur.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8YCRLAFY\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html}
}

@article{dyer2019asymptotics,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}

@article{franz2019critical,
  title={Critical jammed phase of the linear perceptron},
  author={Franz, Silvio and Sclocchi, Antonio and Urbani, Pierfrancesco},
  journal={Physical Review Letters},
  volume={123},
  number={11},
  pages={115702},
  year={2019},
  publisher={APS}
}

@article{wyart2010scaling,
  title={Scaling of phononic transport with connectivity in amorphous solids},
  author={Wyart, Matthieu},
  journal={EPL (Europhysics Letters)},
  volume={89},
  number={6},
  pages={64001},
  year={2010},
  publisher={IOP Publishing}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{wyart2012marginal,
  title={Marginal stability constrains force and pair distributions at random close packing},
  author={Wyart, Matthieu},
  journal={Physical review letters},
  volume={109},
  number={12},
  pages={125502},
  year={2012},
  publisher={APS}
}

@article{degiuli2015unified,
  title={Unified theory of inertial granular flows and non-Brownian suspensions},
  author={DeGiuli, E and D{\"u}ring, G and Lerner, E and Wyart, M},
  journal={Physical Review E},
  volume={91},
  number={6},
  pages={062206},
  year={2015},
  publisher={APS}
}

@article{lerner2012unified,
  title={A unified framework for non-Brownian suspension flows and soft amorphous solids},
  author={Lerner, Edan and D{\"u}ring, Gustavo and Wyart, Matthieu},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={13},
  pages={4798--4803},
  year={2012},
  publisher={National Acad Sciences}
}


@article{saxe2019information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124020},
  year={2019},
  publisher={IOP Publishing}
}

@article{recanatesi2019dimensionality,
  title = {Dimensionality compression and expansion in Deep Neural Networks},
  url = {http://arxiv.org/abs/1906.00443},
  publisher = {arXiv},
  author = {Recanatesi, Stefano and Farrell, Matthew and Advani, Madhu and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric},
  note = {Preprint at},
  urldate={2019}
}

@article{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  pages={6111--6122},
}

@article{mallat2016understanding,
  title={Understanding deep convolutional networks},
  author={Mallat, St{\'e}phane},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={374},
  number={2065},
  pages={20150203},
  year={2016},
  publisher={The Royal Society Publishing}
}

@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={5th International Conference on Learning Representations, {ICLR} 2017,
              Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year={2017}

}

@article{zhou2014object,
  title={Object detectors emerge in deep scene cnns},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  journal={arXiv preprint arXiv:1412.6856},
  year={2014}
}

@inproceedings{kayhan2020translation,
  title={On translation invariance in cnns: Convolutional layers can exploit absolute spatial location},
  author={Kayhan, Osman Semih and Gemert, Jan C van},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14274--14285},
  year={2020}
}

@book{kardar2007statistical,
  title={Statistical physics of fields},
  author={Kardar, Mehran},
  year={2007},
  publisher={Cambridge University Press}
}

@article{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={arXiv preprint arXiv:1905.12173},
  year={2019}
}

@article{bietti2019group,
  title={Group invariance, stability to deformations, and complexity of deep convolutional representations},
  author={Bietti, Alberto and Mairal, Julien},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={876--924},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@article{biroli2023generative,
  title={Generative diffusion in very large dimensions},
  author={Biroli, Giulio and M{\'e}zard, Marc},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2023},
  number={9},
  pages={093402},
  year={2023},
  publisher={IOP Publishing}
}

@article{dietler2020yeaz,
  title={YeaZ: A convolutional neural network for highly accurate, label-free segmentation of yeast microscopy images},
  author={Dietler, Nicola and Minder, Matthias and Gligorovski, Vojislav and Economou, Augoustina Maria and Joly, Denis Alain Henri Lucien and Sadeghi, Ahmad and Chan, Chun Hei Michael and Kozi{\'n}ski, Mateusz and Weigert, Martin and Bitbol, Anne-Florence and others},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{vstefko2018autonomous,
  title={Autonomous illumination control for localization microscopy},
  author={{\v{S}}tefko, Marcel and Ottino, Baptiste and Douglass, Kyle M and Manley, Suliana},
  journal={Optics express},
  volume={26},
  number={23},
  pages={30882--30900},
  year={2018},
  publisher={Optical Society of America}
}

@article{barbier2019adaptive,
  title={The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference},
  author={Barbier, Jean and Macris, Nicolas},
  journal={Probability theory and related fields},
  volume={174},
  number={3-4},
  pages={1133--1185},
  year={2019},
  publisher={Springer}
}

@article{de2016comparing,
  title={Comparing molecules and solids across structural and alchemical space},
  author={De, Sandip and Bart{\'o}k, Albert P and Cs{\'a}nyi, G{\'a}bor and Ceriotti, Michele},
  journal={Physical Chemistry Chemical Physics},
  volume={18},
  number={20},
  pages={13754--13769},
  year={2016},
  publisher={Royal Society of Chemistry}
}

@article{van2017learning,
  title={Learning phase transitions by confusion},
  author={Van Nieuwenburg, Evert PL and Liu, Ye-Hua and Huber, Sebastian D},
  journal={Nature Physics},
  volume={13},
  number={5},
  pages={435--439},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{huval2015empirical,
  title={An empirical evaluation of deep learning on highway driving},
  author={Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce and others},
  journal={arXiv preprint arXiv:1504.01716},
  year={2015}
}

@inproceedings{amodei2016deep,
  title={Deep speech 2: End-to-end speech recognition in english and mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{shi2016end,
  title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},
  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={11},
  pages={2298--2304},
  year={2016},
  publisher={IEEE}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{marr1979computational,
  title={A computational theory of human stereo vision},
  author={Marr, David and Poggio, Tomaso},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={204},
  number={1156},
  pages={301--328},
  year={1979},
  publisher={The Royal Society London}
}

@article{dieleman2016exploiting,
  title={Exploiting cyclic symmetry in convolutional neural networks},
  author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1602.02660},
  year={2016}
}

@article{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  journal={arXiv preprint arXiv:1904.11486},
  year={2019}
}

@article{azulay2018deep,
  title={Why do deep convolutional networks generalize so poorly to small image transformations?},
  author={Azulay, Aharon and Weiss, Yair},
  journal={arXiv preprint arXiv:1805.12177},
  year={2018}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@InProceedings{pmlr-v97-simsekli19a,
  title = 	 {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author = 	 {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5827--5837},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/simsekli19a.html},
  abstract = 	 {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alpha$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ?jumps?, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the $\alpha$-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.}
}

@article{lee2020finite,
  title={Finite Versus Infinite Neural Networks: an Empirical Study},
  author={Lee, Jaehoon and Schoenholz, Samuel S and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2007.15801},
  year={2020}
}

@INPROCEEDINGS{Zagoruyko2016WRN,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

@inproceedings{
yaida2018fluctuationdissipation,
title={Fluctuation-dissipation relations for stochastic gradient descent},
author={Sho Yaida},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkNksoRctQ},
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2937--2947},
  year={2019}
}

@incollection{Chizat2018,
title = {{On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport}},
author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {3040--3050},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@article{nguyen2019mean,
  title={Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks},
  author={Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:1902.02880},
  year={2019}
}

@article{Dyer19,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@inproceedings{
Du2019,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@inproceedings{Allen-Zhu2018,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {http://proceedings.mlr.press/v97/allen-zhu19a.html},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of wor...},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\AWHFMRLR\\Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CJIZ8ZNU\\allen-zhu19a.html:text/html}
}

@article{lecun-mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010,
  journal = {},
}

@book{scholkopf2001learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT press}
}

@book{gikhman2015theory,
  title={The theory of stochastic processes I},
  author={Gikhman, Iosif I and Skorokhod, Anatoli V},
  year={2015},
  publisher={Springer}
}

@article{saad1995line,
  title={On-line learning in soft committee machines},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review E},
  volume={52},
  number={4},
  pages={4225},
  year={1995},
  publisher={APS}
}

@book{opper2001advanced,
  title={Advanced mean field methods: Theory and practice},
  author={Opper, Manfred and Saad, David},
  year={2001},
  publisher={MIT press}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT Press Cambridge, MA}
}

@article{smola1998connection,
  title={The connection between regularization operators and support vector kernels},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard and M{\"u}ller, Klaus-Robert},
  journal={Neural networks},
  volume={11},
  number={4},
  pages={637--649},
  year={1998},
  publisher={Elsevier}
}

@article{stein1999predicting,
  title={Predicting random fields with increasing dense observations},
  author={Stein, Michael L and others},
  journal={The Annals of Applied Probability},
  volume={9},
  number={1},
  pages={242--273},
  year={1999},
  publisher={Institute of Mathematical Statistics}
}

@article{de2020sparsity,
  title={On Sparsity in Overparametrised Shallow ReLU Networks},
  author={de Dios, Jaume and Bruna, Joan},
  journal={arXiv preprint arXiv:2006.10225},
  year={2020}
}

@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}

@inproceedings{yoon2023diffusion,
  title={Diffusion probabilistic models generalize when they fail to memorize},
  author={Yoon, TaeHo and Choi, Joo Young and Kwon, Sehyun and Ryu, Ernest K},
  booktitle={ICML 2023 Workshop on Structured Probabilistic Inference \& Generative Modeling},
  year={2023}
}

@article{lowe2004distinctive,
  title={Distinctive image features from scale-invariant keypoints},
  author={Lowe, David G},
  journal={International journal of computer vision},
  volume={60},
  number={2},
  pages={91--110},
  year={2004},
  publisher={Springer}
}

@book{stein2012interpolation,
  title={Interpolation of spatial data: some theory for kriging},
  author={Stein, Michael L},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{cagnetta2023deep,
  title={How deep neural networks learn compositional data: The random hierarchy model},
  author={Cagnetta, Francesco and Petrini, Leonardo and Tomasini, Umberto M and Favero, Alessandro and Wyart, Matthieu},
  journal={Physical Review X},
  volume={14},
  number={3},
  pages={031001},
  year={2024},
  publisher={APS}
}

@article{weiss2016survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{luxburg2004distance,
  title={Distance-based classification with Lipschitz functions},
  author={Luxburg, Ulrike von and Bousquet, Olivier},
  journal={The Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={669--695},
  year={2004}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3215--3225},
  year={2017}
}

@inproceedings{mei2019mean,
	title = {Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
	shorttitle = {Mean-field theory of two-layers neural networks},
	url = {http://proceedings.mlr.press/v99/mei19a.html},
	abstract = {We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolut...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = jun,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2388--2464},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VNYZQZI9\\mei19a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\UGUDWNS9\\Mei et al. - 2019 - Mean-field theory of two-layers neural networks d.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\U62KYIE2\\mei19a.html:text/html}
}

@article{ikeda2005asymptotic,
  title={An asymptotic statistical analysis of support vector machines with soft margins},
  author={Ikeda, Kazushi and Aoishi, Tsutomu},
  journal={Neural Networks},
  volume={18},
  number={3},
  pages={251--259},
  year={2005},
  publisher={Elsevier}
}

@article{amari1993universal,
  title={A universal theorem on learning curves},
  author={Amari, Shun-Ichi},
  journal={Neural networks},
  volume={6},
  number={2},
  pages={161--166},
  year={1993},
  publisher={Elsevier}
}

@article{amari1992four,
  title={Four types of learning curves},
  author={Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
  journal={Neural Computation},
  volume={4},
  number={4},
  pages={605--618},
  year={1992},
  publisher={MIT Press}
}

@article{Geiger18,
	title = {Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.012115},
	doi = {10.1103/PhysRevE.100.012115},
	abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to artificial intelligence. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased, remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in fully connected deep networks a phase transition delimits the over- and underparametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss (the spectrum of the Hessian) are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime. Interestingly, we observe that the ability of fully connected networks to fit random data is independent of their depth, an independence that appears to also hold for real data. We also study a quantity Δ which characterizes how well (Δ{\textless}0) or badly (Δ{\textgreater}0) a datum is learned. At the critical point it is power-law distributed on several decades, P+(Δ)∼Δθ for Δ{\textgreater}0 and P−(Δ)∼(−Δ)−γ for Δ{\textless}0, with exponents that depend on the choice of activation function. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
	number = {1},
	urldate = {2020-09-29},
	journal = {Physical Review E},
	author = {Geiger, Mario and Spigler, Stefano and d'Ascoli, Stéphane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
	month = jul,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {012115},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\NX8283BB\\Geiger et al. - 2019 - Jamming transition as a paradigm to understand the.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VWT9YRFC\\PhysRevE.100.html:text/html}
}

@inproceedings{domingos00,
  title={A unified bias-variance decomposition},
  author={Domingos, Pedro},
  booktitle={Proceedings of 17th International Conference on Machine Learning},
  pages={231--238},
  year={2000}
}

@string{epje = {Eur.\ Phys.\ J E}}

@string{epl = {Europhys.\ Lett.}}

@string{pnas = {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.}}

@string{prb = {Phys.\ Rev.\ B}}

@string{pre = {Phys.\ Rev.\ E}}

@string{prl = {Phys.\ Rev.\ Lett.}}

@string{rmp = {Rev.\ Mod.\ Phys.}}

@article{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 journal = {Advances in Neural Information Processing Systems},
 year = {2018},
 pages = {8580--8589},
 volume=31
}

@inproceedings{denil12predicting,
 author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc\textquotesingle Aurelio and de Freitas, Nando},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Predicting Parameters in Deep Learning},
 volume = {26},
 year = {2013}
}

@inproceedings{denton14exploiting,
 author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
 volume = {27},
 year = {2014}
}

@article{Yu2017compressing,
  title={On Compressing Deep Models by Low Rank and Sparse Decomposition},
  author={Xiyu Yu and Tongliang Liu and Xinchao Wang and Dacheng Tao},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={67-76}
}

@article{guth2023rainbow,
	publisher = {arXiv},
	author = {Guth, Florentin and Ménard, Brice and Rochette, Gaspar and Mallat, Stéphane},
	title = {A Rainbow in Deep Network Black Boxes},
	url = {http://arxiv.org/abs/2305.18512},
	note = {Preprint at},
	urldate = {2023}
}

@article{jacot2019hessian,
  title={The asymptotic spectrum of the Hessian of DNN throughout training},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1910.02875},
  year={2019}
}

@article{jacot2019hessian2,
  title={The Neural Tangent Kernel describes the Hessian of Overparametrized DNNs},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  year={2019},
  journal={},
}

@inproceedings{williams1997computing,
  title={Computing with infinite networks},
  author={Williams, Christopher KI},
  booktitle={Advances in neural information processing systems},
  pages={295--301},
  year={1997}
}

@inproceedings{
novak2018bayesian,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}

@inproceedings{Schoenholz2017,
   abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
   author = {Samuel S. Schoenholz and Justin Gilmer and Surya Ganguli and Jascha Sohl-Dickstein},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
   title = {Deep information propagation},
   year = {2017},
}

@inproceedings{Xiao2018,
   abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.},
   author = {Lechao Xiao and Yasaman Bahri and Jascha Sohl-Dickstein and Samuel S. Schoenholz and Jeffrey Pennington},
   journal = {35th International Conference on Machine Learning, ICML 2018},
   title = {Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks},
   volume = {12},
   year = {2018},
}

@article{Lee2017,
  title={Deep Neural Networks as Gaussian Processes},
  author={Jae Hoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
  journal={ICLR},
  year={2018}
}

@book{Neal1996,
 author = {Neal, Radford M.},
 title = {Bayesian Learning for Neural Networks},
 year = {1996},
 isbn = {0387947248},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 

@incollection{Cho2009,
title = {Kernel Methods for Deep Learning},
author = {Youngmin Cho and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 22},
pages = {342--350},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}

@inproceedings{matthews2018gaussian,
  title={Gaussian Process Behaviour in Wide Deep Neural Networks},
  author={Alexander G. de G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=H1-nGgWC-},
}

@article{neal2018modern,
  title={A modern take on the bias-variance tradeoff in neural networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}

@article{Spigler18, 
	title = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
	volume = {52},
	issn = {1751-8121},
	url = {https://iopscience.iop.org/article/10.1088/1751-8121/ab4c8b/meta},
	doi = {10.1088/1751-8121/ab4c8b},
	language = {en},
	number = {47},
	urldate = {2020-09-29},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Spigler, S. and Geiger, M. and d’Ascoli, S. and Sagun, L. and Biroli, G. and Wyart, M.},
	month = oct,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {474001},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7FXTMGCL\\Spigler et al. - 2019 - A jamming transition from under- to over-parametri.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TTZP2QI8\\meta.html:text/html}
}

@article{Zdeborova07,
  title={Phase transitions in the coloring of random graphs},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Physical Review E},
  volume={76},
  number={3},
  pages={031131},
  year={2007},
  publisher={APS}
}

@article{Krzakala07,
  title={Landscape analysis of constraint satisfaction problems},
  author={Krzakala, Florent and Kurchan, Jorge},
  journal={Physical Review E},
  volume={76},
  number={2},
  pages={021122},
  year={2007},
  publisher={APS}
}

@article{Monasson95,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{Cooper18,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={907--940},
  year={2016}
}

@article{Li18,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1804.08838},
  year={2018}
}

@article{Lerner12,
	Author = {E. Lerner and G. {D\"uring} and M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:57 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {5},
	Pages = {58003},
	Title = {Toward a microscopic description of flow near the jamming threshold},
	Volume = {99},
	Year = {2012},
	Bdsk-Url-1 = {http://stacks.iop.org/0295-5075/99/i=5/a=58003}}
	
@article{Charbonneau12,
	Author = {Charbonneau, Patrick and Corwin, Eric I. and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2012/11/13/},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Day = {13},
	Id = {10.1103/PhysRevLett.109.205501},
	J1 = {PRL},
	Journal = {Physical Review Letters},
	Journal1 = {Phys. Rev. Lett.},
	Month = {11},
	Number = {20},
	Pages = {205501--},
	Publisher = {American Physical Society},
	Title = {Universal Microstructure and Mechanical Stability of Jammed Packings},
	Ty = {JOUR},
	Url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@article{bahri2021explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}

@incollection{lee2019wide,
	title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}},
	url = {http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8572--8583},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\9XQ8IXMY\\Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\DHGXIH9D\\9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.html:text/html}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@article{ongie2019function,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	shorttitle = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}},
	url = {https://iclr.cc/virtual_2020/poster_H1lNPxHKDH.html},
	abstract = {We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function \$f:{\textbackslash}mathbb\{R\}{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}{\textasciicircum}d\$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case.},
	journal = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
	language = {en},
	urldate = {2020-12-30},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	month = apr,
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LCTGXRBW\\poster_H1lNPxHKDH.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\US3XSNVQ\\Ongie et al. - 2020 - A Function Space View of Bounded Norm Infinite Wid.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TV9PTDNU\\poster_H1lNPxHKDH.html:text/html}
}

@article{Lerner13a,
	Abstract = {We study theoretically and numerically how hard frictionless particles in random packings can rearrange. We demonstrate the existence of two distinct unstable non-linear modes of rearrangement{,} both associated with the opening and the closing of contacts. The first mode{,} whose density is characterized by some exponent [small theta][prime or minute]{,} corresponds to motions of particles extending throughout the entire system. The second mode{,} whose density is characterized by an exponent [small theta] [not equal] [small theta][prime or minute]{,} corresponds to the local buckling of a few particles. Extended modes are shown to yield at a much higher rate than local modes when a stress is applied. We show that the distribution of contact forces follows P(f) [similar] fmin([small theta][prime or minute]{,}[small theta]){,} and that imposing the restriction that the packing cannot be densified further leads to the bounds and {,} where [gamma] characterizes the singularity of the pair distribution function g(r) at contact. These results extend the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,} 2012{,} 109{,} 125502] where the existence of local modes was not considered. We perform numerics that support that these bounds are saturated with [gamma] [approximate] 0.38{,} [small theta] [approximate] 0.17 and [small theta][prime or minute] [approximate] 0.44. We measure systematically the stability of all such modes in packings{,} and confirm their marginal stability. The principle of marginal stability thus allows us to make clearcut predictions on the ensemble of configurations visited in these out-of-equilibrium systems{,} and on the contact forces and pair distribution functions. It also reveals the excitations that need to be included in a description of plasticity or flow near jamming{,} and suggests a new path to study two-level systems and soft spots in simple amorphous solids of repulsive particles.},
	Author = {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
	Date-Added = {2014-05-22 20:15:23 +0000},
	Date-Modified = {2014-09-05 16:00:04 +0000},
	Doi = {10.1039/C3SM50515D},
	Issue = {34},
	Journal = {Soft Matter},
	Pages = {8252-8263},
	Publisher = {The Royal Society of Chemistry},
	Title = {Low-energy non-linear excitations in sphere packings},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1039/C3SM50515D}}

@article{Saxe13,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={International Conference on Learning Representations},
  year={2014}
}

@article{Kingma14,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{Ohern03,
	Author = {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea J. and Nagel, Sidney R.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:34:09 +0000},
	Doi = {10.1103/PhysRevE.68.011306},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Pages = {011306--011324},
	Publisher = {American Physical Society},
	Title = {Jamming at zero temperature and zero applied stress: The epitome of disorder},
	Volume = {68},
	Year = {2003},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevE.68.011306}}

@article{Balduzzi17,
  title={The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal={arXiv preprint arXiv:1702.08591},
  year={2017}
}

@article{Balduzzi16,
  title={Deep online convex optimization with gated games},
  author={Balduzzi, David},
  journal={arXiv preprint arXiv:1604.01952},
  year={2016}
}

@book{Phillips81,
	Author = {Anderson, A.C.},
	Date-Added = {2014-06-13 22:47:59 +0000},
	Date-Modified = {2015-06-04 02:59:24 +0000},
	Editor = {W. A. Phillips},
	Publisher = {Springer, Berlin},
	Series = {Topics in Current Physics},
	Title = {Amorphous Solids: Low Temperature Properties},
	Volume = {24},
	Year = {1981}
}

@article{During13,
	Author = {{D{\"u}ring}, Gustavo and Lerner, Edan and Wyart, Matthieu},
	Date = {2013},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Journal = {Soft Matter},
	Number = {1},
	Pages = {146-154},
	Publisher = {Royal Society of Chemistry},
	Title = {Phonon gap and localization lengths in floppy materials},
	Volume = {9},
	Year = {2013}}

@article{DeGiuli14,
	Author = {DeGiuli, Eric and Laversanne-Finot, Adrien and {D\"uring}, Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
	Date = {2014},
	Date-Added = {2014-07-08 08:27:13 +0000},
	Date-Modified = {2014-07-10 08:54:49 +0000},
	Journal = {Soft Matter},
	Number = {30},
	Pages = {5628-5644},
	Publisher = {Royal Society of Chemistry},
	Title = {Effects of coordination and pressure on sound attenuation, boson peak and elasticity in amorphous solids},
	Volume = {10},
	Year = {2014}}

@article{Yan16,
	Author = {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
	Date = {2016},
	Date-Added = {2016-10-13 12:49:09 +0000},
	Date-Modified = {2016-10-13 12:49:27 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {2},
	Pages = {26003},
	Publisher = {IOP Publishing},
	Title = {On variational arguments for vibrational modes near jamming},
	Volume = {114},
	Year = {2016}}

@article{Tkachenko99,
	Author = {Tkachenko, Alexei V. and Witten, Thomas A.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Doi = {10.1103/PhysRevE.60.687},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Numpages = {9},
	Pages = {687--696},
	Publisher = {American Physical Society},
	Title = {Stress propagation through frictionless granular material},
	Volume = {60},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.60.687}}

@article{Baum88,
  title={On the capabilities of multilayer perceptrons},
  author={Baum, Eric B},
  journal={Journal of complexity},
  volume={4},
  number={3},
  pages={193--215},
  year={1988},
  publisher={Academic Press}
}

@article{Gardner88,
  title={The space of interactions in neural network models},
  author={Gardner, Elizabeth},
  journal={Journal of physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={257},
  year={1988},
  publisher={IOP Publishing}
}

@article{Franz15,
  title={Universal spectrum of normal modes in low-temperature glasses},
  author={Franz, Silvio and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={47},
  pages={14539--14544},
  year={2015},
  publisher={National Acad Sciences}
}

@article{Franz17,
  title={Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems},
  author={Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={SciPost Physics},
  volume={2},
  number={3},
  pages={019},
  year={2017}
}

@article{Brito18a,
	title = {Universality of jamming of nonspherical particles},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/46/11736},
	doi = {10.1073/pnas.1812457115},
	abstract = {Amorphous packings of nonspherical particles such as ellipsoids and spherocylinders are known to be hypostatic: The number of mechanical contacts between particles is smaller than the number of degrees of freedom, thus violating Maxwell’s mechanical stability criterion. In this work, we propose a general theory of hypostatic amorphous packings and the associated jamming transition. First, we show that many systems fall into a same universality class. As an example, we explicitly map ellipsoids into a system of “breathing” particles. We show by using a marginal stability argument that in both cases jammed packings are hypostatic and that the critical exponents related to the contact number and the vibrational density of states are the same. Furthermore, we introduce a generalized perceptron model which can be solved analytically by the replica method. The analytical solution predicts critical exponents in the same hypostatic jamming universality class. Our analysis further reveals that the force and gap distributions of hypostatic jamming do not show power-law behavior, in marked contrast to the isostatic jamming of spherical particles. Finally, we confirm our theoretical predictions by numerical simulations.},
	language = {en},
	number = {46},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brito, Carolina and Ikeda, Harukuni and Urbani, Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
	month = nov,
	year = {2018},
	pmid = {30381457},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {glass, jamming, marginal stability, nonspherical particles},
	pages = {11736--11741},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\URYRABRI\\Brito et al. - 2018 - Universality of jamming of nonspherical particles.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\PHCPDT94\\11736.html:text/html}
}

@article{Muller14,
	Author = {M{\"u}ller, Markus and Wyart, Matthieu},
	Date-Added = {2015-01-05 15:07:48 +0000},
	Date-Modified = {2015-06-04 20:14:00 +0000},
	Doi = {10.1146/annurev-conmatphys-031214-014614},
	Journal = {Annual Review of Condensed Matter Physics},
	Number = {1},
	Pages = {177--200},
	Title = {Marginal Stability in Structural, Spin, and Electron Glasses},
	Volume = {6},
	Year = {2015},
	Bdsk-Url-1 = {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
	Bdsk-Url-2 = {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}}

@article{Charbonneau15,
	Author = {Charbonneau, Patrick and Corwin, Eric I and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2015},
	Date-Added = {2015-04-16 02:52:40 +0000},
	Date-Modified = {2015-04-16 02:52:40 +0000},
	Journal = {Physical Review Letters},
	Number = {12},
	Pages = {125504},
	Publisher = {APS},
	Title = {Jamming Criticality Revealed by Removing Localized Buckling Excitations},
	Volume = {114},
	Year = {2015}}

@article{Charbonneau14,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-05-09 02:04:58 +0000},
	Date-Modified = {2014-06-13 16:56:16 +0000},
	Journal = {Nature Communications},
	Number = {3725},
	Publisher = {Nature Publishing Group},
	Title = {Fractal free energy landscapes in structural glasses},
	Volume = {5},
	Year = {2014}}

@article{Charbonneau14a,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-10-20 02:26:59 +0000},
	Date-Modified = {2014-10-20 02:27:25 +0000},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {10},
	Pages = {10009},
	Publisher = {IOP Publishing},
	Title = {Exact theory of dense amorphous hard spheres in high dimension. III. The full replica symmetry breaking solution},
	Volume = {2014},
	Year = {2014}}
	
@article{Wyart12,
	Author = {Wyart, Matthieu},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:36 +0000},
	Doi = {10.1103/PhysRevLett.109.125502},
	Issue = {12},
	Journal = {Phys. Rev. Lett.},
	Month = {Sep},
	Numpages = {5},
	Pages = {125502},
	Publisher = {American Physical Society},
	Title = {Marginal Stability Constrains Force and Pair Distributions at Random Close Packing},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.109.125502}}

@article{Zeravcic09,
	Abstract = {We study the vibrational modes of three-dimensional jammed packings of soft ellipsoids of revolution as a function of particle aspect ratio {{\OE}µ} and packing fraction. At the jamming transition for ellipsoids, as distinct from the idealized case using spheres where {{\OE}µ=1}, there are many unconstrained and nontrivial rotational degrees of freedom. These constitute a set of zero-frequency modes that are gradually mobilized into a new rotational band as {|{\OE}µ-1|} increases. Quite surprisingly, as this new band is separated from zero frequency by a gap, and lies below the onset frequency for translational vibrations, {{\oe}{\^a} * }, the presence of these new degrees of freedom leaves unaltered the basic scenario that the translational spectrum is determined only by the average contact number. Indeed, {{\oe}{\^a} *} depends solely on coordination as it does for compressed packings of spheres. We also discuss the regime of large {|{\OE}µ-1|}, where the two bands merge.},
	Author = {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W. van Saarloos},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = EPL,
	Number = {2},
	Pages = {26001},
	Title = {Excitations of ellipsoid packings near jamming},
	Volume = {87},
	Year = {2009}}

@article{Donev04a,
	Abstract = {Packing problems, such as how densely objects can fill a volume, are among the most ancient and persistent problems in mathematics and science. For equal spheres, it has only recently been proved that the face-centered cubic lattice has the highest possible packing fraction . It is also well known that certain random (amorphous) jammed packings have {{\oe}{\"U}} ?{{\^a}{\`a}} 0.64. Here, we show experimentally and with a new simulation algorithm that ellipsoids can randomly pack more densely?{{\"A}{\^\i}}up to {{\oe}{\"U}}= 0.68 to 0.71for spheroids with an aspect ratio close to that of M&M's Candies?{{\"A}{\^\i}}and even approach {{\oe}{\"U}} ?{{\^a}{\`a}} 0.74 for ellipsoids with other aspect ratios. We suggest that the higher density is directly related to the higher number of degrees of freedom per particle and thus the larger number of particle contacts required to mechanically stabilize the packing. We measured the number of contacts per particle Z ?{{\^a}{\`a}} 10 for our spheroids, as compared to Z ?{{\^a}{\`a}} 6 for spheres. Our results have implications for a broad range of scientific disciplines, including the properties of granular media and ceramics, glass formation, and discrete geometry.},
	Author = {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and Variano, Evan A. and Stillinger, Frank H. and Connelly, Robert and Torquato, Salvatore and Chaikin, P. M.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1126/science.1093010},
	Journal = {Science},
	Number = {5660},
	Pages = {990-993},
	Title = {Improving the Density of Jammed Disordered Packings Using Ellipsoids},
	Volume = {303},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.1093010}}

@article{Franz16,
  title={The simplest model of jamming},
  author={Franz, Silvio and Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={49},
  number={14},
  pages={145001},
  year={2016},
  publisher={IOP Publishing}
}

@article{Mailman09,
	Author = {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S. and Chakraborty, Bulbul},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1103/PhysRevLett.102.255501},
	Journal = {Phys. Rev. Lett.},
	Month = {Jun},
	Number = {25},
	Numpages = {4},
	Pages = {255501},
	Publisher = {American Physical Society},
	Title = {Jamming in Systems Composed of Frictionless Ellipse-Shaped Particles},
	Volume = {102},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.102.255501}}

@article{Silbert05,
	Author = {L. E. Silbert and A. J. Liu and S. R. Nagel},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = PRL,
	Pages = {098301},
	Title = {Vibrations and Diverging Length Scales Near the Unjamming Transition},
	Volume = {95},
	Year = {2005}}
	
@article{Wyart05a,
	Author = {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney R and Witten, Thomas A},
	Date = {2005},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {051306},
	Publisher = {APS},
	Title = {Effects of compression on the vibrational modes of marginally jammed solids},
	Volume = {72},
	Year = {2005}}

@article{Lipton16,
  title={Stuck in a what? adventures in weight space},
  author={Lipton, Zachary C},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{franz2018jamming,
	title = {Jamming in {Multilayer} {Supervised} {Learning} {Models}},
	volume = {123},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.160602},
	doi = {10.1103/PhysRevLett.123.160602},
	abstract = {Critical jamming transitions are characterized by an astonishing degree of universality. Analytic and numerical evidence points to the existence of a large universality class that encompasses finite and infinite dimensional spheres and continuous constraint satisfaction problems (CCSP) such as the nonconvex perceptron and related models. In this Letter we investigate multilayer neural networks (MLNN) learning random associations as models for CCSP that could potentially define different jamming universality classes. As opposed to simple perceptrons and infinite dimensional spheres, which are described by a single effective field in terms of which the constraints appear to be one dimensional, the description of MLNN involves multiple fields, and the constraints acquire a multidimensional character. We first study the models numerically and show that similarly to the perceptron, whenever jamming is isostatic, the sphere universality class is recovered, we then write the exact mean-field equations for the models and identify a dimensional reduction mechanism that leads to a scaling regime identical to the one of spheres.},
	number = {16},
	urldate = {2020-09-29},
	journal = {Physical Review Letters},
	author = {Franz, Silvio and Hwang, Sungmin and Urbani, Pierfrancesco},
	month = oct,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {160602},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\BLJF3H85\\Franz et al. - 2019 - Jamming in Multilayer Supervised Learning Models.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\EH69D4MZ\\PhysRevLett.123.html:text/html}
}

@article{Franz17b,
  title={Mean-field avalanches in jammed spheres},
  author={Franz, Silvio and Spigler, Stefano},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022139},
  year={2017},
  publisher={APS}
}

@article{geiger2019disentangling,
	title = {Disentangling feature and lazy training in deep neural networks},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/abc4de},
	doi = {10.1088/1742-5468/abc4de},
	abstract = {Two distinct limits for deep learning have been derived as the network width h → ∞, depending on how the weights of the last layer scale with h. In the neural tangent Kernel (NTK) limit, the dynamics becomes linear in the weights and is described by a frozen kernel Θ (the NTK). By contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. In this work we consider deep networks where the weights in the last layer scale as αh −1/2 at initialization. By varying α and h, we probe the crossover between the two limits. We observe two the previously identified regimes of ‘lazy training’ and ‘feature training’. In the lazy-training regime, the dynamics is almost linear and the NTK barely changes after initialization. The feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and thus learns some features. We perform numerical experiments on MNIST, Fashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find that: (i) the two regimes are separated by an α* that scales as . (ii) Network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) In both regimes, the fluctuations δF induced on the learned function by initial conditions decay as , leading to a performance that increases with h. The same improvement can also be obtained at an intermediate width by ensemble-averaging several networks that are trained independently. (iv) In the feature-training regime we identify a time scale , such that for t ≪ t 1 the dynamics is linear. At t ∼ t 1, the output has grown by a magnitude and the changes of the tangent kernel {\textbar} {\textbar}ΔΘ{\textbar} {\textbar} become significant. Ultimately, it follows for ReLU and Softplus activation functions, with a {\textless} 2 and a → 2 as depth grows. We provide scaling arguments supporting these findings.},
	language = {en},
	number = {11},
	urldate = {2020-12-30},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
	month = nov,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {113301},
	file = {Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\FCSUFGWL\\Geiger et al. - 2020 - Disentangling feature and lazy training in deep ne.pdf:application/pdf}
}

@article{spigler2020asymptotic,
  title={Asymptotic learning curves of kernel methods: empirical data versus teacher--student paradigm},
  author={Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2020},
  number={12},
  pages={124001},
  year={2020},
  publisher={IOP Publishing}
}

@InProceedings{Baity18,
  title = 	 {Comparing Dynamics: Deep Neural Networks versus Glassy Systems},
  author = 	 {Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, Gerard Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {314--323},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/baity-jesi18a.html},
  abstract = 	 {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.}
}

@article{Ballard17,
  title={Energy landscapes for machine learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={Physical Chemistry Chemical Physics},
  year={2017},
  publisher={Royal Society of Chemistry}
}

@article{Sagun16,
  title={Singularity of the Hessian in Deep Learning},
  author={Sagun, Levent and Bottou, {L{\'e}on} and LeCun, Yann},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Soudry2016,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}

@inproceedings{Hoffer17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1729--1739},
  year={2017}
}

@article{Freeman16,
  title={Topology and Geometry of Deep Rectified Network Optimization Landscapes},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Hochreiter97,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{Chaudhari16,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{Achille17,
  title={Emergence of invariance and disentangling in deep representations},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={arXiv preprint arXiv:1706.01350},
  year={2017}
}

@article{Zhang16,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{Montufar14,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{Bianchini14,
  title={On the complexity of neural network classifiers: A comparison between shallow and deep architectures},
  author={Bianchini, Monica and Scarselli, Franco},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={8},
  pages={1553--1565},
  year={2014},
  publisher={IEEE}
}

@InProceedings{Raghu16,
  title = 	 {On the Expressive Power of Deep Neural Networks},
  author = 	 {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2847--2854},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/raghu17a.html},
  abstract = 	 {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.}
}

@inproceedings{Choromanska15,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{Ioffe15,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}

@article{He16,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{Lecun95,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@inproceedings{Krizhevsky12,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{Hinton12,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}

@article{Silver16,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Silver17,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{Lecun15,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{Wyart05b,
	Author = {M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:15:00 +0000},
	Journal = {Annales de Phys},
	Number = {3},
	Pages = {1--113},
	Title = {On the Rigidity of Amorphous Solids},
	Volume = {30},
	Year = {2005}}

@book{Liu10,
author = {J Liu, Andrea and R Nagel, Sidney and Saarloos, W and Wyart, Matthieu},
year = {2010},
month = {06},
pages = {},
title = {The jamming scenario - an introduction and outlook},
booktitle = {Dynamical Heterogeneities in Glasses, Colloids, and Granular Media},
publisher = {OUP Oxford}
}

@article{brito2018theory,
  title={Theory for Swap Acceleration near the Glass and Jamming Transitions},
  author={Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1801.03796},
  year={2018}
}

@book{crank1979mathematics,
  title={The mathematics of diffusion},
  author={Crank, John},
  year={1979},
  publisher={Oxford university press}
}

@article{reviewBCKM,
  title={Out of equilibrium dynamics in spin-glasses and other glassy systems},
  author={Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and Kurchan, Jorge and Mezard, Marc},
  journal={Spin glasses and random fields},
  pages={161--223},
  year={1998},
  publisher={World Scientific, Singapore}
}

@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@incollection{birolileshouches,
  title={Slow relaxations and non-equilibrium dynamics in classical and quantum systems},
  author={Biroli, Giulio},
  editor      = "Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet",
  booktitle   = "Strongly Interacting Quantum Systems Out of Equilibrium",
  publisher   = "Oxford University Press",
  address     = "Oxford",
  year        = 2016,
  pages       = "207-261",
}

@article{benarousj,
  title={Spectral gap estimates in mean field spin glasses},
  author={Ben Arous, {G{\'e}rard} and Jagannath, Aukosh},
  journal={arXiv preprint arXiv:1705.04243},
  year={2017}
}

@article{ninarello2017models,
  title={Models and algorithms for the next generation of glass transition studies},
  author={Ninarello, Andrea and Berthier, Ludovic and Coslovich, Daniele},
  journal={Physical Review X},
  volume={7},
  number={2},
  pages={021039},
  year={2017},
  publisher={APS}
}

@article{mezard2002analytic,
  title={Analytic and algorithmic solution of random satisfiability problems},
  author={M{\'e}zard, Marc and Parisi, Giorgio and Zecchina, Riccardo},
  journal={Science},
  volume={297},
  number={5582},
  pages={812--815},
  year={2002},
  publisher={American Association for the Advancement of Science}
}

@article{monasson1999determining,
  title={Determining computational complexity from characteristic phase transitions},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
  journal={Nature},
  volume={400},
  number={6740},
  pages={133},
  year={1999},
  publisher={Nature Publishing Group}
}

@inproceedings{achlioptas2008algorithmic,
  title={Algorithmic barriers from phase transitions},
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle={Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on},
  pages={793--802},
  year={2008},
  organization={IEEE}
}

@article{zdeborovareview,
  title={Statistical physics of inference: Thresholds and algorithms},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Advances in Physics},
  volume={65},
  number={5},
  pages={453--552},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{BJREM,
  title={Activated aging dynamics and effective trap model description in the random energy model},
  author={Baity-Jesi, Marco and Biroli, Giulio and Cammarota, Chiara},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2018},
  number={1},
  pages={013301},
  year={2018},
  publisher={IOP Publishing}
}

@article{geiger2019scaling,
	title = {Scaling description of generalization with number of parameters in deep learning},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088%2F1742-5468%2Fab633c},
	doi = {10.1088/1742-5468/ab633c},
	abstract = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N *. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion ∼N −1/2. This description breaks down at a so-called jamming transition N = N *. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N *. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N *, and averaging their outputs.},
	language = {en},
	number = {2},
	urldate = {2020-09-29},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
	month = feb,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {023401},
	file = {IOP Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\CAAC4TDN\\Geiger et al. - 2020 - Scaling description of generalization with number .pdf:application/pdf}
}

@inproceedings{woodworth2020kernel,
	title = {Kernel and {Rich} {Regimes} in {Overparametrized} {Models}},
	url = {http://proceedings.mlr.press/v125/woodworth20a.html},
	abstract = {A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when  during training the network behaves as a kernelized linear predictor, and thus, training with grad...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3635--3673},
}

@article{ghorbani2020neural,
	title = {When {Do} {Neural} {Networks} {Outperform} {Kernel} {Methods}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a9df2255ad642b923d95503b9a7958d8-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\NTEC7ZLN\\a9df2255ad642b923d95503b9a7958d8-Abstract.html:text/html}
}

@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9111--9121},
  year={2019}
}

@inproceedings{scholkopf_kernel_1999,
	title = {Kernel principal component analysis},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	booktitle = {Advances in {Kernel} {Methods} - {Support} {Vector} {Learning}},
	publisher = {MIT Press},
	author = {Scholkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	year = {1999},
	pages = {327--352},
	file = {Citeseer - Snapshot:C\:\\Users\\leope\\Zotero\\storage\\GFPN86ZX\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7X3FNTFW\\Scholkopf et al. - 1999 - Kernel principal component analysis.pdf:application/pdf}
}

@article{oymak2019generalization,
  title={Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian},
  author={Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1906.05392},
  year={2019}
}

@article{kopitkov2019neural,
	title = {Neural {Spectrum} {Alignment}: {Empirical} {Study}},
	author = {Kopitkov, Dmitry and Indelman, Vadim},
  journal={Artificial {Neural} {Networks} and {Machine} {Learning}, {ICANN} 2020},
  year={2020},
  publisher={Springer International Publishing}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{bordelon2020spectrum,
	title = {Spectrum {Dependent} {Learning} {Curves} in {Kernel} {Regression} and {Wide} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v119/bordelon20a.html},
	abstract = {We derive analytical expressions for the generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statis...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1024--1034},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\Z2UDNMTA\\bordelon20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\ENNDME6S\\Bordelon et al. - 2020 - Spectrum Dependent Learning Curves in Kernel Regre.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\IDCPMBSK\\bordelon20a.html:text/html}
}

@article{dacey2000center,
  title={Center surround receptive field structure of cone bipolar cells in primate retina},
  author={Dacey, Dennis and Packer, Orin S and Diller, Lisa and Brainard, David and Peterson, Beth and Lee, Barry},
  journal={Vision research},
  volume={40},
  number={14},
  pages={1801--1811},
  year={2000},
  publisher={Elsevier}
}

@article{feng2021inverse,
  title={The inverse variance--flatness relation in stochastic gradient descent is critical for finding flat minima},
  author={Feng, Yu and Tu, Yuhai},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={9},
  year={2021},
  publisher={National Acad Sciences}
}

@article{ingrosso2022data,
  title={Data-driven emergence of convolutional structure in neural networks},
  author={Ingrosso, Alessandro and Goldt, Sebastian},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={40},
  year={2022},
  publisher={National Acad Sciences}
}

@article{goldt2019modelling,
	title = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}: {The} {Hidden} {Manifold} {Model}},
	volume = {10},
	shorttitle = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.10.041044},
	doi = {10.1103/PhysRevX.10.041044},
	abstract = {Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterized by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data or assumes that elements of each data sample are drawn independently from some factorized probability distribution. These approaches are, thus, by construction blind to the correlation structure of real-world datasets and their impact on learning in neural networks. Here, we introduce a generative model for structured datasets that we call the hidden manifold model. The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single-layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a “Gaussian equivalence property” (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This approach permits us to analyze in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size, and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.},
	number = {4},
	urldate = {2020-12-30},
	journal = {Physical Review X},
	author = {Goldt, Sebastian and Mézard, Marc and Krzakala, Florent and Zdeborová, Lenka},
	month = dec,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {041044},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\Q8WILTDG\\Goldt et al. - 2020 - Modeling the Influence of Data Structure on Learni.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MWCILWIK\\PhysRevX.10.html:text/html}
}

@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6598--6608},
  year={2019}
}

@inproceedings{shankar2020neural,
	title = {Neural {Kernels} {Without} {Tangents}},
	url = {http://proceedings.mlr.press/v119/shankar20a.html},
	abstract = {We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and momen...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8614--8623},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\FEAJ774U\\Shankar et al. - 2020 - Neural Kernels Without Tangents.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\9HNT3HB8\\shankar20a.html:text/html}
}

@inproceedings{arora2019harnessing,
	title = {Harnessing the {Power} of {Infinitely} {Wide} {Deep} {Nets} on {Small}-data {Tasks}},
	url = {https://openreview.net/forum?id=rkl8sJBYvH},
	abstract = {We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.},
	language = {en},
	urldate = {2020-12-30},
	author = {Arora, Sanjeev and Du, Simon S. and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\SNH768II\\Arora et al. - 2019 - Harnessing the Power of Infinitely Wide Deep Nets .pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\LPBQRZVJ\\forum.html:text/html},
}

@incollection{arora2019exact,
	title = {On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}},
	url = {http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8141--8150},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\XWQDTKB9\\Arora et al. - 2019 - On Exact Computation with an Infinitely Wide Neura.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\I7IJCV83\\9025-on-exact-computation-with-an-infinitely-wide-neural-net.html:text/html}
}

@article{mei2018mean,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/33/E7665},
	doi = {10.1073/pnas.1806579115},
	abstract = {Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.},
	language = {en},
	number = {33},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	pmid = {30054315},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {gradient flow, neural networks, partial differential equations, stochastic gradient descent, Wasserstein space},
	pages = {E7665--E7671},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\QKVLBAKV\\Mei et al. - 2018 - A mean field view of the landscape of two-layer ne.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LQ8XF8DU\\E7665.html:text/html}
}

@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	journal = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@article{montanarisemerjian,
  title={Rigorous inequalities between length and time scales in glassy systems},
  author={Montanari, Andrea and Semerjian, Guilhem},
  journal={Journal of statistical physics},
  volume={125},
  number={1},
  pages={23},
  year={2006},
  publisher={Springer}
}

@article{cuku,
  title={Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
  author={Cugliandolo, Leticia F and Kurchan, Jorge},
  journal={Physical Review Letters},
  volume={71},
  number={1},
  pages={173},
  year={1993},
  publisher={APS}
}

@incollection{cugliandololeshouches,
  title={Course 7: Dynamics of glassy systems},
  author={Cugliandolo, Leticia F},
  booktitle={Slow Relaxations and nonequilibrium dynamics in condensed matter},
  pages={367--521},
  year={2003},
  publisher={Springer}
}

@book{Mezard87,
  title={Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
  author={{M{\'e}zard}, Marc and Parisi, Giorgio and Virasoro, Miguel},
  volume={9},
  year={1987},
  publisher={World Scientific Publishing Company}
}

@article{reviewbray,
  title={Theory of phase-ordering kinetics},
  author={Bray, Alan J},
  journal={Advances in Physics},
  volume={51},
  number={2},
  pages={481--587},
  year={2002},
  publisher={Taylor {\&} Francis}
}

@article{eqBAetal,
  title={Cugliandolo-Kurchan equations for dynamics of Spin-Glasses},
  author={Ben Arous, {G\'erard} and Dembo, Amir and Guionnet, Alice},
  journal={Probability theory and related fields},
  volume={136},
  number={4},
  pages={619--660},
  year={2006},
  publisher={Springer}
}

@article{reviewBB,
  title={Theoretical perspective on the glass transition and amorphous materials},
  author={Berthier, Ludovic and Biroli, Giulio},
  journal={Reviews of Modern Physics},
  volume={83},
  number={2},
  pages={587},
  year={2011},
  publisher={APS}
}

@article{coja2018information,
  title={Information-theoretic thresholds from the cavity method},
  author={Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborova, Lenka},
  journal={Advances in Mathematics},
  volume={333},
  pages={694--795},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2941},
  year={2014}
}

@article{sagun2014explorations,
  title={Explorations on high dimensional landscapes},
  author={Sagun, Levent and {G\"uney}, V. {U\u{g}ur} and {G{\'{e}}rard} {Ben Arous} and LeCun, Yann},
  journal={International Conference on Learning Representations Workshop Contribution, arXiv:1412.6615},
  year={2015}
}

@article{barbier2017phase,
  title={Phase transitions, optimal errors and optimality of message-passing in generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1708.03395},
  year={2017}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{watanabe2007almost,
  title={Almost all learning machines are singular},
  author={Watanabe, Sumio},
  booktitle={Foundations of Computational Intelligence, 2007. FOCI 2007. IEEE Symposium on},
  pages={383--388},
  year={2007},
  organization={IEEE}
}

@article{panageas2016gradient,
  title={Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions},
  author={Panageas, Ioannis and Piliouras, Georgios},
  journal={arXiv preprint arXiv:1605.00405},
  year={2016}
}

@article{lee2016gradient,
  title={Gradient descent converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={University of California, Berkeley},
  volume={1050},
  pages={16},
  year={2016}
}

@article{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal={arXiv preprint arXiv:1509.01240},
  year={2015}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{nocedal2006numerical,
  title={Numerical Optimization, Second Edition},
  author={Nocedal, Jorge and Wright, Stephen J},
  journal={Numerical optimization},
  pages={497--528},
  year={2006},
  publisher={Springer New York}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, {L{\'e}on}},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Physica-Verlag HD}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@article{schaul2013no,
  title={No more pesky learning rates.},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  journal={ICML (3)},
  volume={28},
  pages={343--351},
  year={2013}
}

@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, {L{\'e}on}},
  journal={Proceedings of Neuro-{N{\i}mes}},
  volume={91},
  number={8},
  year={1991}
}

@article{bourrely1989parallelization,
  title={Parallelization of a neural network learning algorithm on a hypercube},
  author={Bourrely, J},
  journal={Hypercube and distributed computers. Elsiever Science Publishing},
  year={1989}
}

@article{auffinger2013random,
  title={Random matrices and complexity of spin glasses},
  author={Auffinger, Antonio and Ben Arous, {G{\'e}rard} and {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={165--201},
  year={2013},
  publisher={Wiley Online Library}
}

@article{kurchanlaloux,
  title={Phase space geometry and slow dynamics},
  author={Kurchan, Jorge and Laloux, Laurent},
  journal={Journal of Physics A: Mathematical and General},
  volume={29},
  number={9},
  pages={1929},
  year={1996},
  publisher={IOP Publishing},
}

@article {pnasmontanariksat,
	author = {{Krzaka{\l}a}, Florent and Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem and {Zdeborov{\'a}}, Lenka},
	title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	volume = {104},
	number = {25},
	pages = {10318--10323},
	year = {2007},
	doi = {10.1073/pnas.0703685104},
	publisher = {National Academy of Sciences},
	abstract = {An instance of a random constraint satisfaction problem defines a random subset ?? (the set of solutions) of a large product space X N (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs) and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states ({\textquotedblleft}clusters{\textquotedblright}) and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process. For typical large instances, the two transitions are sharp. We determine their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem. The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov chain strategies are effective up to the clustering phase transition and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may also beat this threshold.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
}

@article{cavagnaSGpedestrians,
  title={Spin-glass theory for pedestrians},
  author={Castellani, Tommaso and Cavagna, Andrea},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2005},
  number={05},
  pages={P05012},
  year={2005},
  publisher={IOP Publishing},
}

@article{ballard2017perspective,
  title={Perspective: Energy Landscapes for Machine Learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={arXiv preprint arXiv:1703.07915},
  year={2017}
}

@article{piela1989multiple,
  title={On the multiple-minima problem in the conformational analysis of molecules: deformation of the potential energy hypersurface by the diffusion equation method},
  author={Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga, Harold A},
  journal={The Journal of Physical Chemistry},
  volume={93},
  number={8},
  pages={3339--3346},
  year={1989},
  publisher={ACS Publications}
}

@inproceedings{mobahi2015theoretical,
  title={A Theoretical Analysis of Optimization by Gaussian Continuation.},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={AAAI},
  pages={1205--1211},
  year={2015},
  organization={Citeseer}
}

@inproceedings{mobahi2015link,
  title={On the link between gaussian homotopy continuation and convex envelopes},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={43--56},
  year={2015},
  organization={Springer}
}

@article{pardalos1994optimization,
  title={Optimization methods for computing global minima of nonconvex potential energy functions},
  author={Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  journal={Journal of Global Optimization},
  volume={4},
  number={2},
  pages={117--133},
  year={1994},
  publisher={Springer}
}

@article{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua and others},
  journal={Foundations and trends{\textregistered} in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{gulcehre2016mollifying,
  title={Mollifying Networks},
  author={Gulcehre, Caglar and Moczulski, Marcin and Visin, Francesco and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1608.04980},
  year={2016}
}

@article{baldassi2016unreasonable,
  title = {Unreasonable effectiveness of learning neural networks: {From} accessible states and robust ensembles to basic algorithmic schemes},
  volume = {113},
  issn = {0027-8424, 1091-6490},
  shorttitle = {Unreasonable effectiveness of learning neural networks},
  doi = {10.1073/pnas.1608103113},
  language = {en},
  number = {48},
  urldate = {2016-11-30},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T. and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  month = nov,
  year = {2016},
  pages = {E7655--E7662},
}

@inproceedings{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3223--3234},
  year={2018}
}

@inproceedings{gabrie2018entropy,
  title={Entropy and mutual information in models of deep neural networks},
  author={Gabri{\'e}, Marylou and Manoel, Andre and Luneau, Cl{\'e}ment and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1821--1831},
  year={2018}
}

@article{chaudhari2016entropy,
  title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@inproceedings{park2019effect,
	title = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}: an {Empirical} {Study}},
	shorttitle = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}},
	url = {http://proceedings.mlr.press/v97/park19b.html},
	abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base n...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Park, Daniel and Sohl-Dickstein, Jascha and Le, Quoc and Smith, Samuel},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5042--5051},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\ZLMSGD5R\\park19b.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\652UXKDY\\Park et al. - 2019 - The Effect of Network Width on Stochastic Gradient.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MGMYLNWN\\park19b.html:text/html}
}

@inproceedings{hazan2016graduated,
  title={On graduated optimization for stochastic non-convex problems},
  author={Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz, Shai},
  booktitle={International Conference on Machine Learning},
  pages={1833--1841},
  year={2016}
}

@article{horn1962eigenvalues,
  title={Eigenvalues of sums of Hermitian matrices},
  author={Horn, Alfred},
  journal={Pacific Journal of Mathematics},
  volume={12},
  number={1},
  pages={225--241},
  year={1962},
  publisher={Mathematical Sciences Publishers}
}

@article{thompson1971eigenvalues,
  title={On the eigenvalues of sums of Hermitian matrices},
  author={Thompson, Robert C and Freede, Linda J},
  journal={Linear Algebra and Its Applications},
  volume={4},
  number={4},
  pages={369--376},
  year={1971},
  publisher={Elsevier}
}

@article{knutson2001honeycombs,
  title={Honeycombs and sums of Hermitian matrices},
  author={Knutson, Allen and Tao, Terence},
  journal={Notices Amer. Math. Soc},
  volume={48},
  number={2},
  year={2001}
}

@article{marvcenko1967distribution,
  title={Distribution of eigenvalues for some sets of random matrices},
  author={{Mar{\v{c}}enko}, Vladimir A and Pastur, Leonid Andreevich},
  journal={Mathematics of the USSR-Sbornik},
  volume={1},
  number={4},
  pages={457},
  year={1967},
  publisher={IOP Publishing}
}

@article{bloemendal2016principal,
  title={On the principal components of sample covariance matrices},
  author={Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={164},
  number={1-2},
  pages={459--552},
  year={2016},
  publisher={Springer}
}

@article{mobahi2016training,
  title={Training Recurrent Neural Networks by Diffusion},
  author={Mobahi, Hossein},
  journal={arXiv preprint arXiv:1601.04114},
  year={2016}
}

@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, {G{\'e}rard} and {P{\'e}ch{\'e}}, Sandrine and others},
  journal={The Annals of Probability},
  volume={33},
  number={5},
  pages={1643--1697},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{moller1993exact,
  title={Exact Calculation of the Product of the Hessian Matrix of Feed-Forward Network Error Functions and a Vector in 0 (N) Time},
  author={{M{\o}ller}, Martin F},
  journal={DAIMI Report Series},
  volume={22},
  number={432},
  year={1993}
}

@article{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G and others},
  journal={Advances in neural information processing systems},
  pages={164--164},
  year={1993},
  publisher={Morgan Kaufmann Publishers}
}

@article{dinh2017sharp,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.04933},
  year={2017}
}

@incollection{lecun90,
title = {Optimal Brain Damage},
author = {LeCun, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor = {D. S. Touretzky},
pages = {598--605},
year = {1990},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@software{autograd15,
  author = {Dougal Maclaurin and David Duvenaud and Matthew Johnson and Ryan P. Adams},
  title = {Autograd: Reverse-mode differentiation of native {P}ython},
  url = {http://github.com/HIPS/autograd},
  version = {1.1.2},
  year = {2015},
}

@inproceedings{han15a,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1135--1143},
  year={2015}
}

@article{han15b,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@article{wen16,
  author    = {Wei Wen and
               Chunpeng Wu and
               Yandan Wang and
               Yiran Chen and
               Hai Li},
  title     = {Learning Structured Sparsity in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1608.03665},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.03665},
  timestamp = {Mon, 30 Jan 2017 17:08:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{liu2015, 
    author={Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and M. Penksy}, 
    booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Sparse Convolutional Neural Networks}, 
    year={2015}, 
    pages={806-814}, 
    keywords={matrix decomposition;matrix multiplication;neural nets;object detection;SCNN model;cascade model;object detection problem;sparse convolutional neural networks;sparse decomposition;sparse fully connected layers;sparse matrix multiplication algorithm;Accuracy;Convolutional codes;Kernel;Matrix decomposition;Neural networks;Redundancy;Sparse matrices},
    doi={10.1109/CVPR.2015.7298681}, 
    ISSN={1063-6919}, 
    month={June},
}

@inproceedings{denton14,
 author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {1269--1277},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2968826.2968968},
 acmid = {2968968},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{lecun1998efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, {L\'eon} and Orr, GB and {M{\"u}ller}, K-R},
  journal={Lecture notes in computer science},
  pages={9--50},
  year={1998},
  publisher={Springer}
}

@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11674--11685},
  year={2019}
}

@article{shwartz2017opening,
  title = {Opening the Black Box of Deep Neural Networks via Information},
  url = {http://arxiv.org/abs/1703.00810},
  publisher = {arXiv},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  note = {Preprint at},
  urldate = {2017}
}

@article{goyal2017accurate,
  title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author={Goyal, Priya and {Doll{\'a}r}, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={735--742},
  year={2010}
}

@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrzebski, {Stanis{\l}aw} and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@article{sagun2017empirical,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Sagun, Levent and Evci, Utku and {G\"uney}, V. {U\u{g}ur} and Dauphin, Yann and Bottou, {L\'eon}},
  journal={ICLR 2018 Workshop Contribution, arXiv:1706.04454},
  year={2017}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, {L{\'e}on} and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@article{krishnan2017neumann,
  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},
  author={Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal={arXiv preprint arXiv:1712.03298},
  year={2017}
}

@article{Gastaldi17,
  title={Shake-Shake regularization of 3-branch residual networks},
  author={Gastaldi, Xavier},
  journal={International Conference on Learning Representations},
  year={2017}
}

@InProceedings{lee2017ability,
  title = 	 {On the Ability of Neural Nets to Express Distributions},
  author = 	 {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski and Sanjeev Arora},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {1271--1296},
  year = 	 {2017},
  editor = 	 {Satyen Kale and Ohad Shamir},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Amsterdam, Netherlands},
  month = 	 {07--10 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  url = 	 {http://proceedings.mlr.press/v65/lee17a.html},
  abstract = 	 {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution?also theoretically not understood?concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron?s Theorem (Barron, 1993), which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions (?Barron functions?) can be approximated by a $n+1$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance?a natural metric on probability distributions?by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.}
}

@book{engel2001statistical,
  title={Statistical mechanics of learning},
  author={Engel, Andreas and Van den Broeck, Christian},
  year={2001},
  publisher={Cambridge University Press}
}

@incollection{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{caruana2001overfitting,
  title={Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author={Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={402--408},
  year={2001}
}

@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, {Ren{\'e}}},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7331--7339},
  year={2017}
}

@article{liao2018dynamics,
  title={The Dynamics of Learning: A Random Matrix Approach},
  author={Liao, Zhenyu and Couillet, Romain},
  journal={arXiv preprint arXiv:1805.11917},
  year={2018}
}

@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{venturi2018neural,
  title={Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{advani2017high,
	title = {High-dimensional dynamics of generalization error in neural networks},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608020303117},
	doi = {10.1016/j.neunet.2020.08.022},
	abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Neural Networks},
	author = {Advani, Madhu S. and Saxe, Andrew M. and Sompolinsky, Haim},
	month = sep,
	year = {2020},
	keywords = {Generalization error, Neural networks, Random matrix theory},
	file = {ScienceDirect Snapshot:C\:\\Users\\leope\\Zotero\\storage\\E87WVE3H\\S0893608020303117.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\ZAZZFPTJ\\Advani et al. - 2020 - High-dimensional dynamics of generalization error .pdf:application/pdf}
}

@article{monasson1995weight,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, R{\'e}mi and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{bansal2018minnorm,
  title={Minnorm training: an algorithm for training overcomplete deep neural networks},
  author={Bansal, Yamini and Advani, Madhu and Cox, David D and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1806.00730},
  year={2018}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{matheron1963principles,
  title={Principles of geostatistics},
  author={Matheron, Georges},
  journal={Economic geology},
  volume={58},
  number={8},
  pages={1246--1266},
  year={1963},
  publisher={Society of Economic Geologists}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and {Jastrz{\k{e}}bski}, {Stanis{\l}aw} and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:1706.05394},
  year={2017}
}

@article{laurent2017multilinear,
  title={The Multilinear Structure of ReLU Networks},
  author={Laurent, Thomas and von Brecht, James},
  journal={arXiv preprint arXiv:1712.10132},
  year={2017}
}

@article{urban2016deep,
  title={Do deep convolutional nets really need to be deep and convolutional?},
  author={Urban, Gregor and Geras, Krzysztof J and Kahou, Samira Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana, Rich and Mohamed, Abdelrahman and Philipose, Matthai and Richardson, Matt},
  journal={arXiv preprint arXiv:1603.05691},
  year={2016}
}

@inproceedings{bos1997dynamics,
  title={Dynamics of training},
  author={{B{\"o}s}, Siegfried and Opper, Manfred},
  booktitle={Advances in Neural Information Processing Systems},
  pages={141--147},
  year={1997}
}

@article{le1991eigenvalues,
  title={Eigenvalues of covariance matrices: Application to neural-network learning},
  author={Le Cun, Yann and Kanter, Ido and Solla, Sara A},
  journal={Physical Review Letters},
  volume={66},
  number={18},
  pages={2396},
  year={1991},
  publisher={APS}
}

@article{sirignano2018mean,
	title = {Mean {Field} {Analysis} of {Neural} {Networks}: {A} {Law} of {Large} {Numbers}},
	volume = {80},
	issn = {0036-1399},
	shorttitle = {Mean {Field} {Analysis} of {Neural} {Networks}},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1192184},
	doi = {10.1137/18M1192184},
	abstract = {Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (a) large network sizes and (b) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called “propagation of chaos.”},
	number = {2},
	urldate = {2020-12-30},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {725--752},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\YUF9HRL6\\18M1192184.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\JEB5UDWH\\Sirignano and Spiliopoulos - 2020 - Mean Field Analysis of Neural Networks A Law of L.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\78H6MFXL\\18M1192184.html:text/html}
}

@article{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  year={2018}
}

@article{facco_estimating_2017,
	title = {Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-11873-y},
	doi = {10.1038/s41598-017-11873-y},
	abstract = {Analyzing large volumes of high-dimensional data is an issue of fundamental importance in data science, molecular simulations and beyond. Several approaches work on the assumption that the important content of a dataset belongs to a manifold whose Intrinsic Dimension (ID) is much lower than the crude large number of coordinates. Such manifold is generally twisted and curved; in addition points on it will be non-uniformly distributed: two factors that make the identification of the ID and its exploitation really hard. Here we propose a new ID estimator using only the distance of the first and the second nearest neighbor of each point in the sample. This extreme minimality enables us to reduce the effects of curvature, of density variation, and the resulting computational cost. The ID estimator is theoretically exact in uniformly distributed datasets, and provides consistent measures in general. When used in combination with block analysis, it allows discriminating the relevant dimensions as a function of the block size. This allows estimating the ID even when the data lie on a manifold perturbed by a high-dimensional noise, a situation often encountered in real world data sets. We demonstrate the usefulness of the approach on molecular simulations and image analysis.},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
	month = {9},
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {12140}
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically stud...},
	language = {en},
	urldate = {2021-02-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6105--6114},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\NKYW7ZUM\\Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\N2PC56XU\\tan19a.html:text/html},
}

@article{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	journal = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@misc{krizhevsky_learning_2009,
  title = {Learning multiple layers of features from tiny images},
  author = {Krizhevsky, Alex},
  url={https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  note = {Preprint at},
  urldate = {2009}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2021-02-06},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Dataset is freely available at https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\W8EVABRI\\Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\KE88H72Z\\1708.html:text/html},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\7QLLZD6U\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\Q2WAMXHT\\726791.html:text/html},
}

@article{loshchilov_sgdr_2016,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {https://openreview.net/forum?id=Skq89Scxx},
	abstract = {We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.},
	language = {en},
	urldate = {2021-02-07},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = nov,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\ZH8PN2FR\\Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\84JCV4E5\\forum.html:text/html},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.4.541},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	urldate = {2021-03-24},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\2GLGGJAQ\\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\5XVXIK7R\\Backpropagation-Applied-to-Handwritten-Zip-Code.html:text/html},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {ICLR},
	author = {Simonyan, K. and Zisserman, Andrew},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\MQMLMNYZ\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@inproceedings{sandler_mobilenetv2_2018,
	address = {Salt Lake City, UT},
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{MobileNetV2}},
	url = {https://ieeexplore.ieee.org/document/8578572/},
	doi = {10.1109/CVPR.2018.00474},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
	language = {en},
	urldate = {2021-04-13},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = jun,
	year = {2018},
	pages = {4510--4520},
	file = {Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\C39L2AXV\\Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@article{ruderman_pooling_2018,
	title = {Pooling is neither necessary nor sufficient for appropriate deformation stability in {CNNs}},
	url = {http://arxiv.org/abs/1804.04438},
	abstract = {Many of our core assumptions about how neural networks operate remain empirically untested. One common assumption is that convolutional neural networks need to be stable to small translations and deformations to solve image recognition tasks. For many years, this stability was baked into CNN architectures by incorporating interleaved pooling layers. Recently, however, interleaved pooling has largely been abandoned. This raises a number of questions: Are our intuitions about deformation stability right at all? Is it important? Is pooling necessary for deformation invariance? If not, how is deformation invariance achieved in its absence? In this work, we rigorously test these questions, and find that deformation stability in convolutional networks is more nuanced than it first appears: (1) Deformation invariance is not a binary property, but rather that different tasks require different degrees of deformation stability at different layers. (2) Deformation stability is not a fixed property of a network and is heavily adjusted over the course of training, largely through the smoothness of the convolutional filters. (3) Interleaved pooling layers are neither necessary nor sufficient for achieving the optimal form of deformation stability for natural image classification. (4) Pooling confers too much deformation stability for image classification at initialization, and during training, networks have to learn to counteract this inductive bias. Together, these findings provide new insights into the role of interleaved pooling and deformation invariance in CNNs, and demonstrate the importance of rigorous empirical testing of even our most basic assumptions about the working of neural networks.},
	urldate = {2021-05-17},
	journal = {arXiv:1804.04438 [cs, stat]},
	author = {Ruderman, Avraham and Rabinowitz, Neil C. and Morcos, Ari S. and Zoran, Daniel},
	month = may,
	year = {2018},
	note = {arXiv: 1804.04438},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2018 submission},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\GKPXJHGC\\Ruderman et al. - 2018 - Pooling is neither necessary nor sufficient for ap.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\BN7H752U\\1804.html:text/html},
}

@inproceedings{fawzi_manitest_2015,
	address = {Swansea},
	title = {Manitest: {Are} classifiers really invariant?},
	isbn = {978-1-901725-53-7},
	shorttitle = {Manitest},
	url = {http://www.bmva.org/bmvc/2015/papers/paper106/index.html},
	doi = {10.5244/C.29.106},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Fawzi, Alhussein and Frossard, Pascal},
	year = {2015},
	pages = {106.1--106.13},
	file = {Fawzi and Frossard - 2015 - Manitest Are classifiers really invariant.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\8ILVDIWI\\Fawzi and Frossard - 2015 - Manitest Are classifiers really invariant.pdf:application/pdf},
}

@inproceedings{kanbak_geometric_2018,
	address = {Salt Lake City, UT},
	title = {Geometric {Robustness} of {Deep} {Networks}: {Analysis} and {Improvement}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Geometric {Robustness} of {Deep} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8578565/},
	doi = {10.1109/CVPR.2018.00467},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kanbak, Can and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = jun,
	year = {2018},
	pages = {4441--4449},
	file = {Kanbak et al. - 2018 - Geometric Robustness of Deep Networks Analysis an.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\M53E87MI\\Kanbak et al. - 2018 - Geometric Robustness of Deep Networks Analysis an.pdf:application/pdf},
}

@inproceedings{alcorn_strike_2019,
	address = {Long Beach, CA, USA},
	title = {Strike ({With}) a {Pose}: {Neural} {Networks} {Are} {Easily} {Fooled} by {Strange} {Poses} of {Familiar} {Objects}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Strike ({With}) a {Pose}},
	url = {https://ieeexplore.ieee.org/document/8954212/},
	doi = {10.1109/CVPR.2019.00498},
	abstract = {Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, nonadversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We ﬁnd that 99.9\% and 99.4\% of the poses misclassiﬁed by Inception-v3 also transfer to the AlexNet and ResNet-50 image classiﬁers trained on the same ImageNet dataset, respectively, and 75.5\% transfer to the YOLOv3 object detector trained on MS COCO.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Alcorn, Michael A. and Li, Qi and Gong, Zhitao and Wang, Chengfei and Mai, Long and Ku, Wei-Shinn and Nguyen, Anh},
	month = jun,
	year = {2019},
	pages = {4840--4849},
	file = {Alcorn et al. - 2019 - Strike (With) a Pose Neural Networks Are Easily F.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\YGIVM9Y6\\Alcorn et al. - 2019 - Strike (With) a Pose Neural Networks Are Easily F.pdf:application/pdf},
}

@inproceedings{alaifari_adef_2018,
	title = {{ADef}: an {Iterative} {Algorithm} to {Construct} {Adversarial} {Deformations}},
	shorttitle = {{ADef}},
	url = {https://openreview.net/forum?id=Hk4dFjR5K7},
	abstract = {We propose a new, efficient algorithm to construct adversarial examples by means of deformations, rather than additive perturbations.},
	language = {en},
	urldate = {2021-05-25},
	author = {Alaifari, Rima and Alberti, Giovanni S. and Gauksson, Tandri},
	month = sep,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\ZJBVIIFC\\Alaifari et al. - 2018 - ADef an Iterative Algorithm to Construct Adversar.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\N9UC8VLL\\forum.html:text/html},
}

@inproceedings{athalye_synthesizing_2018,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {http://proceedings.mlr.press/v80/athalye18b.html},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera n...},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {284--293},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\Z8PG8CPT\\Athalye et al. - 2018 - Synthesizing Robust Adversarial Examples.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\HUM83E2D\\athalye18b.html:text/html},
}

@inproceedings{xiao_spatially_2018,
	title = {Spatially {Transformed} {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=HyydRMZC-},
	abstract = {We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks.},
	language = {en},
	urldate = {2021-05-25},
	author = {Xiao, Chaowei and Zhu, Jun-Yan and Li, Bo and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\4S4WQPT7\\Xiao et al. - 2018 - Spatially Transformed Adversarial Examples.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\QP53SZ2W\\forum.html:text/html},
}

@inproceedings{engstrom_exploring_2019,
	title = {Exploring the {Landscape} of {Spatial} {Robustness}},
	url = {http://proceedings.mlr.press/v97/engstrom19a.html},
	abstract = {The study of adversarial robustness has so far largely focused on perturbations bound in \${\textbackslash}ell\_p\$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural class...},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1802--1811},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\MNFWJIHU\\Engstrom et al. - 2019 - Exploring the Landscape of Spatial Robustness.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\NPRFGVPQ\\engstrom19a.html:text/html},
}
@inproceedings{shen_anatomical_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Anatomical {Data} {Augmentation} via {Fluid}-{Based} {Image} {Registration}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_31},
	abstract = {We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Shen, Zhengyang and Xu, Zhenlin and Olut, Sahin and Niethammer, Marc},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	pages = {318--328},
	file = {Springer Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\Q3GHAZJT\\Shen et al. - 2020 - Anatomical Data Augmentation via Fluid-Based Image.pdf:application/pdf},
}

@article{hauberg_dreaming_2016,
	title = {Dreaming {More} {Data}: {Class}-dependent {Distributions} over {Diffeomorphisms} for {Learned} {Data} {Augmentation}},
	shorttitle = {Dreaming {More} {Data}},
	url = {http://arxiv.org/abs/1510.02795},
	abstract = {Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g.{\textasciitilde}new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.},
	urldate = {2021-10-20},
	journal = {arXiv:1510.02795 [cs]},
	author = {Hauberg, Søren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher III, John W. and Hansen, Lars Kai},
	month = jun,
	year = {2016},
	note = {arXiv: 1510.02795},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\QUXRSTIJ\\Hauberg et al. - 2016 - Dreaming More Data Class-dependent Distributions .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\Z49XJBYC\\1510.html:text/html},
}

@article{petrini_relative_2021,
	year         = 2021,
	author       = {Petrini, Leonardo and Favero, Alessandro and Geiger, Mario and Wyart, Matthieu},
	title        = {Relative stability toward diffeomorphisms indicates performance in deep nets},
	journal    = {Advances in {Neural} {Information} {Processing} {Systems}},
	volume       = 34,
	pages        = {8727--8739},
}

@book{goodfellow_deep_2016,
	year         = 2016,
	author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	title        = {Deep {Learning}},
	month        = nov,
	publisher    = {The MIT Press},
	address      = {Cambridge, Massachusetts},
	isbn         = {978-0-262-03561-3},
	language     = {English}
}

@article{hubel_receptive_1962,
	title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
	volume = {160},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/},
	abstract = {Images
null},
	number = {1},
	urldate = {2022-09-21},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = jan,
	year = {1962},
	pmid = {14449617},
	pmcid = {PMC1359523},
	pages = {106--154.2},
	file = {PubMed Central Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\3DUTYQUK\\Hubel and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf:application/pdf},
}

@article{niyogi_incorporating_1998,
	title = {Incorporating prior information in machine learning by creating virtual examples},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726787},
	abstract = {One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training-set size. We show that in some contexts this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well motivated. The process of creating virtual examples in real-world pattern recognition tasks is highly nontrivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Niyogi, P. and Girosi, F. and Poggio, T.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Associate members, Counting circuits, Function approximation, Knowledge acquisition, Learning systems, Machine learning, Pattern recognition, Prototypes, Speech, Supervised learning},
	pages = {2196--2209},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\G7Y8P25U\\726787.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\V7F56D98\\Niyogi et al. - 1998 - Incorporating prior information in machine learnin.pdf:application/pdf},
}

@book{poggio_visual_2016,
	title = {Visual {Cortex} and {Deep} {Networks}: {Learning} {Invariant} {Representations}},
	shorttitle = {Visual {Cortex} and {Deep} {Networks}},
	url = {https://direct.mit.edu/books/book/4088/Visual-Cortex-and-Deep-NetworksLearning-Invariant},
	abstract = {A mathematical framework that describes learning of invariant representations in the ventral stream, offering both theoretical development and applications.The},
	language = {en},
	urldate = {2022-09-21},
	author = {Poggio, Tomaso A. and Anselmi, Fabio},
	month = sep,
	year = {2016},
	doi = {10.7551/mitpress/10177.001.0001},
	file = {Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\KXLG2PJ3\\Visual-Cortex-and-Deep-NetworksLearning-Invariant.html:text/html},
}

@article{anselmi_unsupervised_2016,
	series = {Biologically {Inspired} {Processes} in {Neural} {Computation}},
	title = {Unsupervised learning of invariant representations},
	volume = {633},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397515005587},
	doi = {10.1016/j.tcs.2015.06.048},
	abstract = {The present phase of Machine Learning is characterized by supervised learning algorithms relying on large sets of labeled examples (n→∞). The next phase is likely to focus on algorithms capable of learning from very few labeled examples (n→1), like humans seem able to do. We propose an approach to this problem and describe the underlying theory, based on the unsupervised, automatic learning of a “good” representation for supervised learning, characterized by small sample complexity. We consider the case of visual object recognition, though the theory also applies to other domains like speech. The starting point is the conjecture, proved in specific cases, that image representations which are invariant to translation, scaling and other transformations can considerably reduce the sample complexity of learning. We prove that an invariant and selective signature can be computed for each image or image patch: the invariance can be exact in the case of group transformations and approximate under non-group transformations. A module performing filtering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such signature. The theory offers novel unsupervised learning algorithms for “deep” architectures for image and speech recognition. We conjecture that the main computational goal of the ventral stream of visual cortex is to provide a hierarchical representation of new objects/images which is invariant to transformations, stable, and selective for recognition—and show how this representation may be continuously learned in an unsupervised way during development and visual experience.},
	language = {en},
	urldate = {2022-09-21},
	journal = {Theoretical Computer Science},
	author = {Anselmi, Fabio and Leibo, Joel Z. and Rosasco, Lorenzo and Mutch, Jim and Tacchetti, Andrea and Poggio, Tomaso},
	month = jun,
	year = {2016},
	keywords = {Convolutional networks, Cortex, Hierarchy, Invariance},
	pages = {112--121},
	file = {Full Text:C\:\\Users\\lpetrini\\Zotero\\storage\\WWB2B7XW\\Anselmi et al. - 2016 - Unsupervised learning of invariant representations.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\TF7NEQIP\\S0304397515005587.html:text/html},
}

@article{Bremaud2000,
   abstract = {This book discusses both the theory and applications of Markov chains. The author studies both discrete-time and continuous-time chains and connected topics such as finite Gibbs fields, non-homogeneous Markov chains, discrete time regenerative processes, Monte Carlo simulation, simulated annealing, and queueing networks are also developed in this accessible and self-contained text. The text is firstly an introduction to the theory of stochastic processes at the undergraduate or beginning graduate level. Its primary objective is to initiate the student to the art of stochastic modelling. The treatment is mathematical, with definitions, theorems, proofs and a number of classroom examples which help the student to fully grasp the content of the main results. Problems of varying difficulty are proposed at the close of each chapter. The text is motivated by significant applications and progressively brings the student to the borders of contemporary research. Students and researchers in operations research and electrical engineering as well as in physics, biology and the social sciences will find this book of interest.},
   author = {Laurent Saloff-Coste and Pierre Bremaud},
   doi = {10.2307/2669802},
   issn = {01621459},
   issue = {452},
   journal = {Journal of the American Statistical Association},
   title = {Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues},
   volume = {95},
   year = {2000},
}

@book{Risken1996,
   abstract = {© 2015 Zbigniew Otremba and Eugeniusz Andrulewicz.The article discusses an important issue of technical pressure exerted on the marine environment during construction and operation of maritime wind farms (MFW) on waters of the Polish Exclusive Economic Zone. A motivation for analysing this issue is the need for attracting attention to the aspect of physical field modification as the factor which links large scale technical activity at sea with the existence and functioning of the marine ecosystem, including further consequences to its economic benefits. Based on current knowledge and authors' analyses, the scale of modifications (disturbances) of physical fields expected to take place during MFW construction and operation was assessed.},
   author = {Hannes Risken},
   journal = {Dynamical Systems},
   title = {The Fokker-Planck Equation Springer Series in Synergetics},
   year = {1996},
}

@article{malach2018provably,
  title={A provably correct algorithm for deep learning that actually works},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1803.09522},
  year={2018}
}

@inproceedings{shalev2020computational,
  title={Computational separation between convolutional and fully-connected networks},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  booktitle={International Conference on Learning Representations},
  year={2020}}

@article{malach2020implications,
  title={The implications of local correlation on learning some deep functions},
  author={Malach, Eran and Shalev-Shwartz, Shai},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1322--1332},
  year={2020}}

@inproceedings{abbe2022initial,
  title={An initial alignment between neural network and target is needed for gradient descent to learn},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Hazla, Jan and Marquis, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={33--52},
  year={2022},
  organization={PMLR}}

  @inproceedings{ShalevShwartz2017,
   author = {Shai Shalev-Shwartz and Ohad Shamir and Shaked Shammah},
   journal = {34th International Conference on Machine Learning, ICML 2017},
   title = {Failures of gradient-based deep learning},
   volume = {6},
   year = {2017},
}

@InProceedings{pmlr-v125-brutzkus20a,
  title =    {ID3 Learns Juntas for Smoothed Product Distributions},
  author =       {Brutzkus, Alon and Daniely, Amit and Malach, Eran},
  booktitle =    {Proceedings of Thirty Third Conference on Learning Theory},
  pages =    {902--915},
  year =    {2020},
  editor =    {Abernethy, Jacob and Agarwal, Shivani},
  volume =    {125},
  series =    {Proceedings of Machine Learning Research},
  month =    {09--12 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v125/brutzkus20a/brutzkus20a.pdf},
  url =    {https://proceedings.mlr.press/v125/brutzkus20a.html},
}

@article{Blum2003,
   author = {Avrim Blum and Adam Kalai and Hal Wasserman},
   doi = {10.1145/792538.792543},
   issn = {00045411},
   issue = {4},
   journal = {Journal of the ACM},
   title = {Noise-tolerant learning, the parity problem, and the statistical query model},
   volume = {50},
   year = {2003},
}
@article{Feldman2009,
   author = {Vitaly Feldman and Parikshit Gopalan and Subhash Khot and Ashok Kumar Ponnuswami},
   doi = {10.1137/070684914},
   issn = {0097-5397},
   issue = {2},
   journal = {SIAM Journal on Computing},
   title = {On Agnostic Learning of Parities, Monomials, and Halfspaces},
   volume = {39},
   year = {2009},
}
@inproceedings{Feldman2006,
   author = {Vitaly Feldman and Parikshit Gopalan and Subhash Khot and Ashok Kumar Ponnuswami},
   doi = {10.1109/FOCS.2006.51},
   issn = {02725428},
   journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
   title = {New results for learning noisy parities and halfspaces},
   year = {2006},
}

@inproceedings{NEURIPS2021_d064bf1a,
 author = {Karp, Stefani and Winston, Ezra and Li, Yuanzhi and Singh, Aarti},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24883--24897},
 publisher = {Curran Associates, Inc.},
 title = {Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels},
 url = {https://proceedings.neurips.cc/paper/2021/file/d064bf1ad039ff366564f352226e7640-Paper.pdf},
 volume = {34},
 year = {2021}
}

@InProceedings{pmlr-v97-belilovsky19a,
  title = 	 {Greedy Layerwise Learning Can Scale To {I}mage{N}et},
  author =       {Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {583--593},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/belilovsky19a/belilovsky19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/belilovsky19a.html},
}

@article{allen-zhu2023how,
  title={Backward feature correction: How deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}

@article{allen2023physics,
  title={Physics of Language Models: Part 1, Context-Free Grammar},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.13673},
  year={2023}
}

@InProceedings{Abbe22sparse,
  title = 	 {The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks},
  author =       {Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {4782--4887},
  year = 	 {2022},
  editor = 	 {Loh, Po-Ling and Raginsky, Maxim},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/abbe22a/abbe22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/abbe22a.html}
}

@inproceedings{Abbe21staircase,
 author = {Abbe, Emmanuel and Boix-Adsera, Enric and Brennan, Matthew S and Bresler, Guy and Nagaraj, Dheeraj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {26989--27002},
 publisher = {Curran Associates, Inc.},
 title = {The staircase property: How hierarchical structure can guide deep learning},
 url = {https://proceed@article{mhaskar2016learning,
  title={Learning functions: when is deep better than shallow},
  author={Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
  journal={arXiv preprint arXiv:1603.00988},
  year={2016}
}ings.neurips.cc/paper/2021/file/e2db7186375992e729165726762cb4c1-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{poggio2017why,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

@misc{
PoggioNC,
title={Feature learning in deep classifiers through Intermediate Neural Collapse},
author={Rangamani Akshay and Lindegaard, Marius and Galanti Tomer and Poggio Tomaso},
year={2023},
url={http://oastats.mit.edu/handle/1721.1/148239}
}

@article{Papyan2020,
   author = {Vardan Papyan and X. Y. Han and David L. Donoho},
   doi = {10.1073/pnas.2015509117},
   issn = {10916490},
   issue = {40},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   title = {Prevalence of neural collapse during the terminal phase of deep learning training},
   volume = {117},
   year = {2020},
}


@InProceedings{pmlr-v180-bronstein22a,
  title = 	 {On the inductive bias of neural networks for learning read-once DNFs},
  author =       {Bronstein, Ido and Brutzkus, Alon and Globerson, Amir},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {255--265},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/bronstein22a/bronstein22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/bronstein22a.html}
}

@misc{Abbe23,
  doi = {10.48550/ARXIV.2302.11055},
  
  url = {https://arxiv.org/abs/2302.11055},
  
  author = {Abbe, Emmanuel and Boix-Adsera, Enric and Misiakiewicz, Theodor},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{pmlr-v161-brutzkus21a,
  title = 	 {An optimization and generalization analysis for max-pooling networks},
  author =       {Brutzkus, Alon and Globerson, Amir},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1650--1660},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/brutzkus21a/brutzkus21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/brutzkus21a.html}
}

@INPROCEEDINGS{9288350,
  author={Ma, Wenchi and Yu, Miao and Li, Kaidong and Wang, Guanghui},
  booktitle={2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Why Layer-Wise Learning is Hard to Scale-up and a Possible Solution via Accelerated Downsampling}, 
  year={2020},
  volume={},
  number={},
  pages={238-243},
  doi={10.1109/ICTAI50040.2020.00046}}

  @article{10.1093/imaiai/iaaa042,
    author = {Oymak, Samet and Soltanolkotabi, Mahdi},
    title = "{Learning a deep convolutional neural network via tensor decomposition}",
    journal = {Information and Inference: A Journal of the IMA},
    volume = {10},
    number = {3},
    pages = {1031-1071},
    year = {2021},
    month = {02},
    issn = {2049-8772},
    doi = {10.1093/imaiai/iaaa042},
    url = {https://doi.org/10.1093/imaiai/iaaa042},
    eprint = {https://academic.oup.com/imaiai/article-pdf/10/3/1031/40364219/iaaa042.pdf},
}

@article{mossel2016deep,
  title={Deep learning and hierarchal generative models},
  author={Mossel, Elchanan},
  journal={arXiv preprint arXiv:1612.09057},
  year={2016}
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017},
  organization={PMLR}
}

@article{damian22neural,
  title = 	 {Neural Networks can Learn Representations with Gradient Descent},
  author =       {Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  journal = 	 {Proceedings of Thirty-Fifth Conference on Learning Theory},
  pages = 	 {5413--5452},
  year = 	 {2022},
  volume = 	 {178},
  publisher =    {PMLR},
}

@article{ba22highdimensional,
 author = {Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
 journal = {Advances in Neural Information Processing Systems},
 pages = {37932--37946},
 publisher = {Curran Associates, Inc.},
 title = {High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation},
 volume = {35},
 year = {2022}
}

@inproceedings{capsules2017,
 author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dynamic Routing Between Capsules},
 url = {https://proceedings.neurips.cc/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{pmlr-v70-zhao17c,
  title = 	 {Learning Hierarchical Features from Deep Generative Models},
  author =       {Shengjia Zhao and Jiaming Song and Stefano Ermon},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {4091--4099},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/zhao17c/zhao17c.pdf},
  url = 	 {https://proceedings.mlr.press/v70/zhao17c.html}
}

@article{tomasini_how_2022,
  title = {How deep convolutional neural networks lose spatial information with training},
  volume = {4},
  number = {4},
  journal = {Machine Learning: Science and Technology},
  publisher = {IOP Publishing},
  author = {Tomasini,  Umberto M and Petrini,  Leonardo and Cagnetta,  Francesco and Wyart,  Matthieu},
  year = {2023},
  pages = {045026}
}

@article{favero_locality_2021,
	title = {Locality defeats the curse of dimensionality in convolutional teacher-student scenarios},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Favero, Alessandro and Cagnetta, Francesco and Wyart, Matthieu},
	year = {2021},
	pages = {9456--9467},
}

@inproceedings{cagnetta2023can,
  title={What can be learnt with wide convolutional neural networks?},
  author={Cagnetta, Francesco and Favero, Alessandro and Wyart, Matthieu},
  booktitle={International Conference on Machine Learning},
  pages={3347--3379},
  year={2023},
  organization={PMLR}
}

@article{demba19model,
  author={Zazo, Javier and Tolooshams, Bahareh and Ba, Demba and Paulson, Harvard John A.},
  journal={IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing}, 
  title={Convolutional Dictionary Learning in Hierarchical Networks}, 
  year={2019},
  number={},
  pages={131-135},
}

@article{demba20relu,
author={Ba, Demba},
journal={IEEE Transactions on Signal Processing}, 
title={Deeply-Sparse Signal rePresentations (D$\text{S}^2$P)}, 
year={2020},
volume={68},
number={},
pages={4727-4742},
doi={10.1109/TSP.2020.3014716}
}

@article{pope_intrinsic_2021,
	title = {The {Intrinsic} {Dimension} of {Images} and {Its} {Impact} on {Learning}},
	url = {https://openreview.net/forum?id=XJk19XzGq2J},
	language = {en},
	author = {Pope, Phil and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
	month = jan,
	year = {2021},
    journal = {International Conference on Learning Representations}
}

@misc{pellegrini_sifting_2021,
	title = {Sifting out the features by pruning: {Are} convolutional networks the winning lottery ticket of fully connected ones?},
	shorttitle = {Sifting out the features by pruning},
	url = {http://arxiv.org/abs/2104.13343},
	doi = {10.48550/arXiv.2104.13343},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Pellegrini, Franco and Biroli, Giulio},
	month = may,
	year = {2021},
	note = {arXiv:2104.13343 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},

}

@article{neyshabur_towards_2020,
	title = {Towards {Learning} {Convolutions} from {Scratch}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/5c528e25e1fdeaf9d8160dc24dbf4d60-Abstract.html},
	urldate = {2023-05-31},
	journal = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam},
	year = {2020},
	pages = {8078--8088},
}

@misc{balestriero_cookbook_2023,
	title = {A {Cookbook} of {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2304.12210},
	doi = {10.48550/arXiv.2304.12210},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
	month = apr,
	year = {2023},
	note = {arXiv:2304.12210 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@book{rozenberg_handbook_1997,
	title = {Handbook of {Formal} {Languages}},
	abstract = {The theory of formal languages is the oldest and most fundamental area of theoretical computer science. It has served as a basis of formal modeling from the early stages of programming languages to the recent beginnings of DNA computing. This first handbook of formal languages gives a comprehensive up-to-date coverage of all important aspects and subareas of the field. Best specialists of various subareas, altogether 50 in number, are among the authors. The maturity of the field makes it possible to include a historical perspective in many presentations. The individual chapters can be studied independently, both as a text and as a source of reference. The Handbook is an invaluable aid for advanced students and specialists in theoretical computer science and related areas in mathematics, linguistics, and biology.},
	author = {Rozenberg, Grzegorz and Salomaa, Arto},
	month = jan,
	year = {1997},
    publisher={Springer},
	doi = {10.1007/978-3-642-59126-6},
}

@inproceedings{zeiler_visualizing_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing and Understanding Convolutional Networks},
	booktitle = {Computer Vision – ECCV 2014},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	year = {2014},
	pages = {818--833},
}

@article{kearns_efficient_1998,
	title = {Efficient noise-tolerant learning from statistical queries},
	volume = {45},
	issn = {0004-5411},
	url = {https://dl.acm.org/doi/10.1145/293347.293351},
	doi = {10.1145/293347.293351},
	number = {6},
	urldate = {2023-06-08},
	journal = {Journal of the ACM},
	author = {Kearns, Michael},
	month = nov,
	year = {1998},
	keywords = {computational learning theory, machine learning},
	pages = {983--1006},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
}

@article{grill2004human,
  title={The human visual cortex},
  author={Grill-Spector, Kalanit and Malach, Rafael},
  journal={Annu. Rev. Neurosci.},
  volume={27},
  pages={649--677},
  year={2004},
  publisher={Annual Reviews}
}

@article{van1983hierarchical,
  title={Hierarchical organization and functional streams in the visual cortex},
  author={Van Essen, David C and Maunsell, John HR},
  journal={Trends in neurosciences},
  volume={6},
  pages={370--375},
  year={1983},
  publisher={Elsevier Current Trends}
}

@article{kruger2012deep,
  title={Deep hierarchies in the primate visual cortex: What can we learn for computer vision?},
  author={Kruger, Norbert and Janssen, Peter and Kalkan, Sinan and Lappe, Markus and Leonardis, Ales and Piater, Justus and Rodriguez-Sanchez, Antonio J and Wiskott, Laurenz},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1847--1871},
  year={2012},
  publisher={IEEE}
}