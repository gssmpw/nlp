\section{Related work}
\paragraph{Sample complexity in diffusion models}

Under mild assumptions on the data distribution, diffusion models exhibit a sample complexity that scales exponentially with the data dimension \cite{block2020generative,oko2023diffusion}. It is not the case if data lie on a low-dimensional latent subspace \cite{de2022convergence,chen2023score,yuan2023reward}, correspond to Gaussian mixture models \cite{biroli2023generative,shah2023learning, Cui2023AnalysisOL}, Ising models~\cite{mei2023deep}, or distributions that can be factorized across spatial scales \cite{kadkhodaie2023learning}. These works do not consider the sample complexity in compositional data. 

\paragraph{Compositional generalization of diffusion models}

\looseness=-1 \citet{okawa2023compositional} considered synthetic compositional data to empirically show how diffusion models learn to generalize by composing different concepts, in the absence of a compositional hierarchy. \citet{kamb2024analytic} studied how equivariant diffusion models can compose images by combining local patches seen in the dataset. \citet{sclocchi2024phase,sclocchi2024probing} showed that diffusion on hierarchically compositional data can be solved using Belief Propagation. \citet{mei2024unets} showed that U-Nets can efficiently approximate the Belief Propagation algorithm on hierarchical data. Yet, efficient representability does not guarantee learnability by gradient descent for hierarchical data \citep{cagnetta2023deep}. These works do not, however, address the sample complexity of diffusion models learned by gradient descent or variations of it.

\paragraph{Learning hierarchical representation via next-token prediction}

It has been observed that transformers trained on next-token prediction on PCFGs learn a hierarchical representation of the data that reflects the structure of the latent variables \citep{cagnetta2024towards, allen2023physics,garnier2024transformers}. Closest to our work, \citet{cagnetta2024towards} showed that for the prediction of the last token in a sequence of fixed length, the latent structure is learned hierarchically, with a sample complexity polynomial in the context length. Our work extends this finding to diffusion models, in a setup where complete sequences can be generated. This setup allows us to make novel predictions on the limitations of generated data as a function of the training set size, which we test empirically across domains.