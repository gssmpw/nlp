% 1. Dataset, leave for future work.
% 2. There is still some degradation in utility compared to the original model.
% 3. Mechanism explanation.
Despite the contributions demonstrated in our work, several limitations remain:
\begin{itemize}
\item [1.] While we provide a detailed analysis of various unlearning methods, our experiments primarily focus on MLLMU-Bench~\cite{liu2024mllmubench} and CLEAR~\cite{dontsov2024clear}, two pioneering benchmarks for MLLM MU. As this field is still in its early stages, designing more high-quality benchmarks would be beneficial for evaluating MLLM-targeted unlearning methods more comprehensively. For instance, representative LLM unlearning benchmarks such as TOFU~\cite{maini2024tofu} and WPU~\cite{liu2024revisiting} could be extended with visual information, facilitating a more thorough assessment of MLLM MU. However, we leave the enhancement and development of MLLM-oriented unlearning benchmarks for future work.\par
\item [2.] Although \method surpasses baseline methods in forgetting tasks, there remains a degradation in model utility after unlearning. This decline may stem from complex interactions between multimodal knowledge representations within the MLLM. Future work could further optimize \method by refining dataset selection, tuning hyperparameters, and developing novel saliency score measurements to mitigate this issue.\par  
\item [3.] In this paper, our weight saliency-based updating strategy has proven to be both effective and robust for MLLM MU compared to baseline approaches. However, the underlying mechanisms of these methods in multimodal domains remain unexplored. Further investigation and exploration about these methods may offer valuable insights, leading to more powerful MLLM unlearning methods and revealing the knowledge storage mechanism of MLLMs.\par
\end{itemize}