\subsection{Task Setting}\label{sec:task}
\begin{figure*}[thp]
\centering
\includegraphics[width=0.98\textwidth]{figures/method.jpg}
\caption{An illustration of our proposed \method. Compared to traditional approaches employed in previous work, which directly apply LLM-based unlearning algorithms to vanilla MLLMs, our method demonstrates superior parameter efficiency, forgetting performance, and textual knowledge preservation. Both the baseline and our approach are trained on VQA-format data, while textual QA-format data is used to assess the preservation of textual knowledge during evaluation.}
\label{fig:method}
\end{figure*}
% A image contains hierarchical concepts from general to fine-grained. For example, "Trump" is composed by "blone hair", "red tie" and others. Those concepts are connected with the textual knowledge within LLMs (https://arxiv.org/abs/2412.14133). We need to erase targeted high concept C_h while preserve common concepts.
To enable a text-only LLM $\mathcal{L}$ to comprehend visual context, mainstream approaches extract visual embeddings $H_I$ using a vision encoder $\mathcal{V}$ followed by a projector $\mathcal{W}$. The entire model is then fine-tuned with visual instruction data $\{X_I, X_Q, X_A\}$, where $X_I$ represents the input image, $X_Q$ is the textual instruction, and $X_A$ denotes the expected answer of length $S$. This can be formalized as follows:
\begin{equation}\label{eq:train}
\begin{split}
    X_O &= \mathcal{L}(H_I; X_Q) = \mathcal{L}(\mathcal{W}(\mathcal{V}(X_I)); X_Q),\\
    {Loss} &= - \sum_{s=1}^{S} \log P(X_A^{(s)} | X_O^{(<s)}),
\end{split}
\end{equation}
where $X_O$ represents the output sequence of the model, $X_A^{(s)}$ is the target token at step $t$, and $X_O^{(<s)}$ represents the previously generated tokens in the output sequence. $P(X_A^{(s)} | X_O^{(<s)})$ represents the predicted probability of label $X_A^{(s)}$ at position $s$. Following this framework, MLLMs acquire the ability to recognize concepts in the visual modality, establishing associations between visual concepts and the internal knowledge of the LLM while leveraging its reasoning and generative capabilities.

\begin{tcolorbox}[float=t!,title=Desiderata for MLLM-based Multimodal Machine Unlearning]\label{box:desiderata}
    \begin{itemize}
        \item[\textbf{I:}] \textbf{Forgetting $C$ in the visual modality.} The model should fail to recognize concept $C$ in visual inputs, i.e., $\mathcal{L}(h_I; x_{Q_v,C}) \neq x_{A_v,C}$, where $x_{Q_v,C}$ is a textual query referring to $C$ in the image, and $x_{A_v,C}$ is the correct answer to $x_{Q_v,C}$.
        
        \item[\textbf{II:}] \textbf{Preserving general visual perception abilities.} The model should retain its ability to process visual information unrelated to concept $C$, i.e., $\mathcal{L}(h_I; x_{Q_v,\sim C}) = x_{A_v,\sim C}$, where $x_{Q_v,\sim C}$ is a textual query unrelated to $C$ in the image, and $x_{A_v,\sim C}$ is the correct answer to $x_{Q_v,\sim C}$.
        
        \item[\textbf{III:}] \textbf{Retaining internal knowledge within the LLM.} The model should preserve its textual knowledge about concept $C$, i.e., $\mathcal{L}(x_{Q_t,C}) = x_{A_t,C}$, where $x_{Q_t,C}$ is a pure-text query about concept $C$, and $x_{A_t,C}$ is the correct answer to $x_{Q_t,C}$.
    \end{itemize}
\end{tcolorbox}

The objective of multimodal MU is, therefore, to eliminate these learned associations between a specific concept and its corresponding visual patterns. In other words, the unlearned model should behave as if it has never encountered the related images during the visual instruction tuning process. Specifically, for a given concept $C$, its image representation $x_I$, and the extracted visual embeddings $h_I = \mathcal{W}(\mathcal{V}(x_I))$, the unlearned model must satisfy the conditions in 
Box \ref{box:desiderata}.

In standard unlearning tasks, the unlearned model is expected to maintain its knowledge on a retained set, which we denotes in next Section \ref{sec:floss}. We illustrate the expected behavior of the targeted model using biographical examples, though our task formulation can be extended to other domains, such as real-world entities and landmarks.

\subsection{Selective Updating for Forget Loss}\label{sec:floss}
The core challenge of multimodal MU lies in preserving textual knowledge while performing unlearning on VQA data. A naive unlearning method, such as GA Difference (\textbf{GA\_Diff}), simply updates the model parameters $\theta$ using a joint loss ($\mathcal{L}^J(\cdot)$) computed over the Forget VQA set $D_f=\{(I_{C_f}, X_{Q_v}, X_{A_v}, C_f)\}$ and the Retain VQA set $D_r=\{(I_{C_r}, X_{Q_v}, X_{A_v}, C_r)\}$ as follows:
\begin{equation}
\begin{split}
\mathcal{L}^J(\theta_t) = -\mathcal{L}^{f}(\theta_t) + \mathcal{L}^{r}(\theta_t),
\end{split}
\end{equation}
where $t$ denotes the $t$-th step, and $\mathcal{L}^{f}(\cdot)$ and $\mathcal{L}^{r}(\cdot)$ represent the loss on the Forget and Retain sets, respectively. Interpreting the update of $\theta$ as an optimization problem in parameter space, the term $-\mathcal{L}^{f}(\theta_t)$ forces the MLLM to forget the VQA samples that should be unlearned by following the steepest ascent direction. And $\mathcal{L}^{r}(\theta_t)$ aims to preserve knowledge from the retained VQA samples. However, the conflicting directions of the Forget loss and the Retain loss make the unlearning process unstable. Furthermore, traditional MLLM unlearning methods primarily focus on VQA data, neglecting the constraints from text-only QA data.\par

Nonetheless, such conflicts can be effectively mitigated if model updates selectively target parameters that are salient for the targeted knowledge (\textbf{S}) while preserving those critical for others. This process can be formulated as:
\begin{equation}
\begin{split}
\mathcal{L}^S(\theta_t) = -\mathbf{m} \odot \mathcal{L}^{f}(\theta_t) + \mathcal{L}^{r}(\theta_t),
\end{split}
\end{equation}
where $\mathbf{m}$ is a boolean mask that selectively updates parameters, and $\odot$ denotes the Hadamard product. In this way, the ascent of the Forget Loss on targeted visual concept does not destroy the parameters salient for the Retain set or textual knowledge, as illustrated in Figure \ref{fig:method}.

\subsection{Weight Saliency Map in Parameter Space}
As discussed in Section \ref{sec:floss}, the gradient mask $\mathbf{m}$ should strike a balance between forgetting and retaining knowledge so that only the necessary parameters are updated during unlearning. Inspired by \citet{fan2023salun,huang2024remain}, the saliency map of each parameter on a given dataset $D$ in the parameter space can be approximated by the diagonal of the initial modelâ€™s Fisher information matrix:
\begin{equation}
\begin{split}
S(\theta_0, \mathcal{L}, D) = F_{diag}^D = [\nabla\mathcal{L}^D(\theta_0)]^2,
\end{split}
\end{equation}
which corresponds to a manifold defined by the loss function, dataset distribution, and initial parameters in the parameter space.

From this perspective, we define a targeted dataset as:
\begin{equation}
T = \{(I_{C_f}, X_{Q_v}, X_{A_v}, C_f)\},
\end{equation}
while the preserved dataset is defined as:
\begin{equation}\label{eq:pdata}
\begin{split}
P &= \{(X_{Q_t}, X_{A_t}, C_f)\}\cup \{(X_{Q_v}, X_{A_v}, C_r)\}\\ & \cup \{(I_{C_r}, X_{Q_t}, X_{A_t}, C_r)\},
\end{split}
\end{equation}
where $C_f$ represents the targeted concepts to be forgotten, and $C_r$ denotes the untargeted concepts that should be retained. Here, $I$, $X_{Q_{v/t}}$, and $X_{A_{v/t}}$ represent the corresponding image, multimodal/pure-textual query, and correct answers, respectively.

Thus, the gradient mask $\mathbf{m}$ is obtained by comparing the relative ratio of the saliency map between the targeted and preserved datasets using a hard threshold:
\begin{equation}\label{eq:mask}
\begin{split}
\mathbf{m} &= \mathds{1}\left[\frac{S(\theta_0, \mathcal{L}, T)}{S(\theta_0, \mathcal{L}, P)} \geq \beta \right]\\
&= \mathds{1}\left[\frac{\nabla^2\mathcal{L}^T(\theta_0)}{\nabla^2\mathcal{L}^P(\theta_0)} \geq \beta \right],
\end{split}
\end{equation}
where $\mathds{1}[y \geq \beta]$ is an element-wise indicator function that outputs 1 for the $i$-th element if $y_i \geq \beta$ and 0 otherwise. The threshold $\beta > 0$ is a hard cutoff; for simplicity, we use $\beta=1$ throughout our experiments, which is sufficient for our tasks.
% The core challenge of multimodal targeted unlearning lies on preserving textual knowledge during unlearning on VQA data. Naive unlearning method like GA Difference (\textbf{GA\_Diff}) updates the model parameters $\theta$ with joint loss ($\mathcal{L}^J(\cdot)$) on Forget VQA set $D_f$ and Retain VQA set $R_r$ directly:
% \begin{equation}
% \begin{split}
% \mathcal{L}^J(\theta_t)=-\mathcal{L}^{f}(\theta_t)+\mathcal{L}^{r}(\theta_t)
% \end{split}
% \end{equation}
% where $t$ refers to $t$-th step, $\mathcal{L}^{f}(\cdot)$ and $\mathcal{L}^{r}(\cdot)$ represent the loss on Forget and Retain set respectively. Considering the update of $\theta$ as a optimazation problem in parameter space, $-\mathcal{L}^{f}(\theta_t)$ forces MLLM to forget VQA samples ro be forgotten through the steepest ascent direction, while $\mathcal{L}^{r}(\theta_t)$ makes efforts to preserve knowledge from retained VQA samples. Nevertheless, the conflicted directions of Forget loss and Retain loss make the unlearning process unstable. What's more, traditional MLLM unlearning only focuses on VQA data, ignoring pure-text QA data on Forget and Retain Set. 
% However, such an conflict could be efficiently mitigated if we can updating the parameters salient for Forget Set selectively (\textbf{S}) and preserving those important for other data. That is,
% \begin{equation}
% \begin{split}
% \mathcal{L}^S(\theta_t)=-\rm \textbf{m}\odot\mathcal{L}^{f}(\theta_t)+\mathcal{L}^{r}(\theta_t)
% \end{split}
% \end{equation}
% where \textbf\rm{m} is a boolean mask for selective parameter updating, and $\odot$ is Hadamard product. In this way, the ascent of Forget Loss won't destroying salient parameters for Retain Set and textual knowledge, as illustrated in Figure \ref{}.  
% \subsection{Geometry Constrained Weight Saliency Map}
% Based on the discussion in Section \ref{sec:floss}, the gradient mask \textbf\rm{m} should make a balance between forgetting and retaining saliency so that only necessary parameters are updated during unlearning. Inspired by \cite{}, the saliency map of each parameters on the given dataset $D$ in parameter space can be approximated by the diagonal of the initial modelâ€™s Fisher information matrix:
% \begin{equation}
% \begin{split}
% S(\theta_0,\mathcal{L},D)=F_{diag}^D=[\nabla\mathcal{L}^D(\theta_0)]^2
% \end{split}
% \end{equation}
% which corresponds to a manifold depicted by loss function, dataset distribution and initial parameters in parameter space. From this view, we define a targeted dataset as $T={\{(I_{C_f},X_{Q_v},X_{A_v},C_f)\}}$, while a preserved dataset as $P={\{(X_{Q_t},X_{A_t},C_f)\}}\cup{\{(I_{C_r},X_{Q_t},X_{A_t},C_r)\}}\cup{\{(X_{Q_v},X_{A_v},C_r)\}}$, where $C_f$ is targeted concepts to forget, $C_r$ is untargeted concepts to be retained, and $I$, $X_{Q_{v/t}}$, $X_{A_{v/t}}$ represents the corresponding image, the multimdoal/pure-textual query, and the correct answers respectively. Therefore, the gradient mask is obtained by comparing the relative ratio of saliency map in targeted and preserved dataset with a hard threshold:
% \begin{equation}
% \begin{split}
% \rm \textbf{m}=\mathds{1}[\frac{S(\theta_0,\mathcal{L},T)}{S(\theta_0,\mathcal{L},P)}\geq \beta]
% =\mathds{1}[\frac{\nabla^2\mathcal{L}^T(\theta_0)}{\nabla^2\mathcal{L}^P(\theta_0)}\geq \beta]
% \end{split}
% \end{equation}
% where $\mathds{1}[y\geq \beta]$ is an element-wise indicator function which yields a value of 1 for the $i$-th element if $y_i\geq \beta$ and 0 otherwise, and $\beta > 0$ is a hard threshold. For this work, we use $\beta=1$ throughout our experiments for simplifying.