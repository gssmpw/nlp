In this paper, we reformulate the task of MU tailored for MLLMs, a field still in its early stages. Our proposed setting aims to erase targeted visual concepts in MLLMs while preserving untargeted knowledge. To address this challenge, we further propose a novel weight saliency-based unlearning method, \method, which selectively updates parameters crucial for the forgetting objective while protecting parameters essential for retaining untargeted knowledge. Our experiments demonstrate that directly transferring LLM-oriented MU methods to VQA data is insufficient for MLLMs; whereas our proposed \method exhibits a strong ability to remove visual concepts while preserving textual knowledge. Further experiments validate the effectiveness and robustness of our approach. We believe that \method will lay a solid foundation for building a trustworthy MLLM ecosystem to achieve ultimate AGI.