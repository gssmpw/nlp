\section{More Related Work}\label{app:rk}
\subsection{Multimodal Large Language Model}
The rapid development of MLLM has attracted the attention of both the academic and industrial communities to the performance breakthroughs brought about by its architectural characteristics \cite{huang2024survey,mai2024efficient,yan2024urbanclip,yan2024georeasoner}. Most MLLMs adopt a framework similar to LLaVA~\cite{liu2024llava}, which proposes to project the visual embeddings extracted from a pre-trained vision encoder into the LLM’s word embedding space through a connector (also known as projector or merger). The combined model is then fine-tuned with visual instruction data, as described in Eq.~\ref{eq:train}. Several open-source MLLMs have demonstrated remarkable performance on multimodal reasoning and understanding tasks, including Qwen2-VL~\cite{wang2024qwen2vl}, InternVL2~\cite{chen2024internvl}, and others~\cite{glm2024chatglm,li2024llavaonevision}. For MLLM unlearning, LLaVA-1.5 has been one of the most widely used backbones in previous studies~\cite{li2024siu,liu2024mllmubench,dontsov2024clear}. To further validate our conclusions, we additionally select Qwen2-VL, one of the state-of-the-art open-source MLLMs, as another representative evaluated model.

\subsection{Machine Unlearning for Other Multimodal Models}
A brief discussion of MLLM MU is provided in Section \ref{MMU}. Despite these efforts, several pioneering studies have also explored unlearning for multimodal models with different architectures \cite{liu2024multimodal,li2025machine,tang2024learn,gao2024practical,gao2024meta}, such as CLIP~\cite{radford2021clip}. For example, CLIPErase~\cite{yang2024cliperase} seeks to disentangle and selectively forget both visual and textual associations learned by CLIP, ensuring that unlearning does not compromise model performance. The motivation behind CLIPErase is therefore similar to ours. Moreover, \citep{kravets2024zero} demonstrates class-wise unlearning in CLIP using synthetic samples. MultiDelete~\cite{cheng2024multidelete} introduces a method that separates cross-modal embeddings for the forget set of BLIP~\cite{li2022blip} and ALBEF~\cite{li2021albef}. While these exploratory works provide insights into multimodal MU, they do not address issues in MLLM MU.

\section{Implementation Details}
\subsection{Datasets}\label{app:dataset}
\subsubsection{MLLMU-Bench}
\textbf{MLLMU-Bench}~\cite{liu2024mllmubench} is a benchmark designed to advance the understanding of multimodal machine unlearning. It consists of 500 fictitious profiles and 153 public celebrity profiles, with each profile featuring over 14 customized question-answer pairs, evaluated from both multimodal and textual perspectives.  In this paper, we divide it into six subsets to comprehensively assess the efficiency, generalizability, and model utility of unlearning methods, particularly in terms of their handling of visual and textual knowledge. Compared to CLEAR, the results of MLLMU-Bench are more stable, demonstrating consistent and reliable performance across different dimensions and settings. Therefore, our further analysis of unlearning methods is primarily based on MLLMU-Bench.
\subsubsection{CLEAR}
Similar with MLLMU-Bnech, \textbf{CLEAR}~\cite{dontsov2024clear} is also an opensourced benchmark designed for machine unlearning in multimodal setup, which contains 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs.  CLEAR is built on the top of pure-textual unlearning benchmark \textbf{TOFU}~\cite{maini2024tofu}, with additional portraits for each person mentioned in QA pair. However, despite efforts to ensure consistency across different images of the same entity, the photos generated by Photomaker~\cite{li2024photomaker} in CLEAR still exhibit a noticeable gap from expectation. \emph{Consequently, the vision features learned by MLLMs on CLEAR can be unstable, making the unlearning process highly unpredictable.} In our experiments, even minor changes in hyperparameters led to complete model collapse, resulting in 0\% accuracy on both classification and generation tasks. Similar findings are also reported in the original paper of CLEAR, where the results of GA, GA\_Diff, and KL\_Min are all zero for both Forget and Retain Set. Given these limitations, we consider the results from CLEAR as valuable references but not as decisive evidence for our conclusions.
\subsection{Evaluation Metrics}\label{app:metrics}
\subsubsection{Unlearning Efficacy}
Unlearning efficacy evaluates a model's capability to eliminate specific knowledge about targeted data, ensuring it behaves as if the data were never included in the training process. In this work, we examine the task of removing visual patterns associated with particular concepts while maintaining textual knowledge. Under this framework, unlearning efficacy is assessed through the model’s performance in a Visual Question Answering (VQA) setting. Specifically, the model is tested using multiple-choice questions, where it should avoid selecting the correct answer linked to a forgotten concept. Formally, given a question $x$ and a set of possible answers $Y$, the model should minimize the probability of choosing the correct answer $y^* \in Y$ from the Forget Set:
\begin{equation}
    \hat{y} = \arg\min_{y \in Y} P(y \mid x, M_u),
\end{equation}
where $y \neq y^*$ and $M_u$ denotes the unlearned model. Ideally, the model should treat images of forgotten concepts as unknown, behaving similarly to random guessing.

\subsubsection{Model Utility}
Model utility measures the model’s ability to retain valuable knowledge and sustain high performance on non-targeted data, ensuring that the unlearning process does not compromise its overall effectiveness. In our study, the preserved knowledge includes textual information related to targeted concepts, both visual and textual knowledge from the Retain Set, and general real-world understanding. We evaluate model utility using the Forget QA, Retain VQA, Retain QA, Real-world VQA, and Real-world QA datasets. For classification tasks, accuracy is determined based on multiple-choice questions associated with retained profiles. The model should sustain high accuracy without any decline due to the unlearning process. Formally, given a question $x$ and a set of possible answers $Y$, the model should maximize the probability of selecting the correct answer $y^*$:
\begin{equation}
    \hat{y} = \arg\max_{y \in Y} P(y \mid x, M_u),
\end{equation}
where $M_u$ represents the model after unlearning.

\subsubsection{ROUGE-L Score}
The ROUGE-L score measures the similarity between the generated text and the reference text by evaluating the longest common subsequence (LCS). The LCS represents the longest sequence of words that appear in both the generated text $P$ and the ground truth $G$ in the same order, though not necessarily contiguously. Recall is calculated as the ratio of the LCS length to the length of the reference text, denoted as $L_G$:
\begin{equation}
    \text{Recall} = \frac{\text{LCS}}{L_G}.
\end{equation}
Precision is determined by the proportion of the LCS length relative to the length of the generated text, represented as $L_P$:
\begin{equation}
    \text{Precision} = \frac{\text{LCS}}{L_P}.
\end{equation}
The final ROUGE-L score is obtained by computing the $F_1$ score of recall and precision:
\begin{equation}
    \text{ROUGE-L} = 2 \cdot \frac{\text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}}.
\end{equation}
This approach ensures a balanced assessment of both precision and recall, providing a comprehensive evaluation metric.

\subsection{Vanilla Fine-tuning and Baselines}\label{app:formula}
\subsubsection{Vanilla Model}\label{app:vanilla}
To simulate a real-life scenario where unlearning algorithms are applied to a pre-trained model, standard practice involves fine-tuning an off-the-shelf MLLM model using information extracted from fictitious profiles. For each input \ensuremath{\langle I, x, y \rangle}, where $I$ is the image of targeted concept, $x$ is the question, and $y$ is the ground-truth answer, the model is trained to predict the answer $\hat{y}$. The loss function for a single sample is defined as the negative log-likelihood (NLL) over the answer tokens:
\begin{equation}
    j(x, y, w) = \frac{1}{|y|} \sum_{i=1}^{|y|} \text{NLL}_w(y_i \mid [I, x, y_{<i}]),
\end{equation}
where $w$ represents the model parameters, and the loss is averaged over all tokens in the answer sequence $y$. The overall objective during fine-tuning is to minimize the average loss across the entire dataset $\mathcal{D}$, expressed as:
\begin{equation}
    \mathcal{L}(\mathcal{D}, w) = \frac{1}{|\mathcal{D}|} \sum_{(x,y) \in \mathcal{D}} j(x, y, w).
\end{equation}
To simulate real-world challenges, we set the vision encoder, connector, and language model of MLLMs to be trainable so that visual concepts can be learned within the vision encoder itself. The experimental results validate our strategy as effective. After fine-tuning, the model obtain knowledge from Forget and Retain Set, serving as the baseline for subsequent unlearning experiments. For reproducibility, we display our settings during training vanilla models in Table \ref{tab:vanilla}, aligning with the official implementations of MLLMU-Bench and CLEAR.
\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c c c c}
        \hline
        Datasets & LMMs & Epochs& Batch Size & Optimizer & LoRA & Learning Rate \\
        \hline
        MLLMU-Bench & LLaVA-1.5-7B & 4 & 4 & Adam & True &$2 \times 10^{-5}$ \\
        MLLMU-Bnech & Qwen2-VL-7B-Instruct  & 4 & 4 & Adam & True & $1 \times 10^{-5}$ \\
        CLEAR & LLaVA-1.5-7B & 4 & 3 & Adam & True & $2 \times 10^{-5}$ \\
        CLEAR & Qwen2-VL-7B-Instruct  & 4 & 5 & Adam & True & $1 \times 10^{-5}$ \\
        \hline
    \end{tabular}
    \caption{Hyperparameter settings for fine-tuning vanilla model alongside different backbones and datasets.}
    \label{tab:vanilla}
\end{table}



\subsubsection{GA}
GA~\cite{thudi2022GA} realize unlearning by maximizing the loss on forget data. The intuition behind it is that maximizing forget loss will lead model to getting predictions dissimilar from the correct answers for forget set and consequently unlearning desired information. Thus, this method can be considered as a finetuning procedure with a reversed loss function:
\begin{equation}
    \mathcal{L}_\text{GA} = \frac{1}{|D_F|} \sum_{x \in D_F} \text{NLL}(x, \theta),
\end{equation}
where $\text{NLL}(x, \theta)$ is the negative loglikelihood of the model on the input $x$.

\subsubsection{GA\_Diff}
GA\_Diff~\cite{liu2022GA_Diff} builds on the concept of combining GA on Forget Set and directly fine-tuning on Retain Set. As mentioned in Section \ref{sec:floss}, it aims to increase the loss on the forget data while maintain the loss on the retain set as possible. The joint loss function is defined as follows:
\begin{equation}
    \mathcal{L}_\text{GA\_Diff} = -L(D_F, \theta) + L(D_R, \theta),
\end{equation}
where $D_F$ is the forget set and $D_R$ is the retain set.

\subsubsection{KL\_Min}
KL\_Min~\cite{nguyen2020KL_Min} aims to minimize the Kullback-Leibler (KL) divergence between the model’s predictions on the retain set before and after unlearning, while maximizing the conventional loss on the forget set. The $\mathcal{L}_\text{KL}$ loss function is defined as
\begin{equation}
\begin{split}
\mathcal{L}_\text{KL}&=\frac{1}{|D_F|} \sum_{x \in D_F} \frac{1}{|x|} \sum_{i=2}^{|s|} \Phi(x_{<i}),\\
\text{where } \Phi(x_{<i})&=\text{KL} \left( P(x_{<i} | \theta) \Big\| P(x_{<i} | \theta_0) \right).
\end{split}
\end{equation}
And the overall objective function is formulated as follows:
\begin{equation}
    \mathcal{L}_\text{KL\_Min} = -L(D_F, \theta) + \mathcal{L}_\text{KL},
\end{equation}
where $\theta_0$ is the model’s weights before unlearning and $P(s | \theta)$ is the model’s logits on the input sequence $s$ with weights $\theta$.

\subsubsection{NPO}
NPO~\cite{zhang2024npo} can be treated as a variant of DPO~\cite{rafailov2024dpo} without positive examples. In this work, the final loss function $L_{NPO}$ for this method is derived as follows:
\begin{equation}
    \mathcal{L}_\text{NPO}=\frac{2}{\beta} \mathbb{E}_{x,y \in D_F} \left[ \log \left( 1 + \left( \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} \right)^{\beta} \right) \right],
\end{equation}
where $\pi_{\theta}(y|x)$ represents the prediction probability of the current model for token $y$ given the input $x$, and $\pi_{\text{ref}}(y|x)$ is the prediction probability from the reference model trained on retain dataset. $\beta$ is a hyperparameter, taken equal to 0.4 in our settings. Such a loss function ensure that the model output probability $\pi_{\theta}(y|x)$ is as small as possible, corresponding to the unlearning objective of the forget data.

\subsubsection{Hyperparameters Settings of Baselines}\label{app:base_hyper}
To ensure reproducibility, we present the experimental settings used to compare various unlearning methods in Table \ref{tab:baselines}, which are adapted from the official implementations of MLLMU-Bench and CLEAR.
\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|c|c|c|}
        \hline
        \textbf{Benchmarks} & \textbf{Backbones} & Epochs & Batch Size & Learning Rate \\
        \hline
        \multirow{2}{*}{MLLMU-Bench} & LLaVA-1.5-7B & \multirow{2}{*}{2} & 4 & $2 \times 10^{-5}$ \\
        & Qwen2-VL-7B-Instruct &  & 2 & $1 \times 10^{-5}$ \\
        \hline
        \multirow{2}{*}{CLEAR} & LLaVA-1.5-7B & \multirow{2}{*}{2} & 4 & $2 \times 10^{-5}$ \\
        & Qwen2-VL-7B-Instruct &  & 2 & $1 \times 10^{-5}$ \\
        \hline
    \end{tabular}
    \caption{Hyperparameter settings for unlearning methods alongside different backbones and datasets. Settings remain consistent across different methods for a given dataset and base model to ensure fair comparison.}
    \label{tab:baselines}
\end{table}







\section{Additional Experiments}\label{app:additional}
\subsection{Parameter Visualization}\label{app:viz}
Here, we present additional visualizations illustrating the distribution of updated parameters for both \method and the baselines. Figure \ref{fig:viz_plus} shows the results of LLaVA-7B and Qwen2-VL-7B-Instruct on MLLMU-Bench and CLEAR, respectively. Compared to LLaVA-7B, the percentage of selected/updated parameters in Qwen2-VL-7B-Instruct is higher. We attribute this to the fact that the features learned by Qwen2-VL are more robust, making it harder to forget them with minor changes to the parameters.
% \begin{figure}[t]
% \centering
% \begin{subfigure}{0.5\textwidth}
%     \includegraphics[width=\linewidth]{figures/heatmaps/llava_clear_heatmap.png}
%     \caption{Heatmap of LLaVA-7B on CLEAR.}
%     \label{fig:first}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.5\textwidth}
%     \includegraphics[width=\linewidth]{figures/heatmaps/qwen_mllmubench.png}
%     \caption{Heatmap of Qwen2-VL-7B-Instruct on MLLMU-Bench.}
%     \label{fig:second}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.5\textwidth}
%     \includegraphics[width=\linewidth]{figures/heatmaps/qwen_clear.png}
%     \caption{Heatmap of Qwen2-VL-7B-Instruct on CLEAR.}
%     \label{fig:third}
% \end{subfigure}
% \end{figure}

\begin{figure}[!t]
\centering
        \begin{minipage}[c]{0.48\textwidth}
		\includegraphics[width=\linewidth]{ figures/heatmaps/llava_clear_heatmap.jpg}
		\subcaption{Heatmap of LLaVA-7B on CLEAR.}
		\label{fig:first}
	\end{minipage}\\
 
	\begin{minipage}[c]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/heatmaps/qwen_mllmubench.jpg}
            \subcaption{Heatmap of Qwen2-VL-7B-Instruct on MLLMU-Bench.}
            \label{fig:second}
	\end{minipage}

        \begin{minipage}[c]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/heatmaps/qwen_clear.jpg}
            \subcaption{Heatmap of Qwen2-VL-7B-Instruct on CLEAR.}
            \label{fig:third}
	\end{minipage}
\caption{Heatmaps of top-$n$ updated parameters for different base models on different datasets.}
\label{fig:viz_plus}
\end{figure}






\section{Case Study}\label{app:case}
Table \ref{tab:selected_case} presents the generated responses of \method and four baseline methods on some of the most challenging visual concepts to forget. While the baseline models show limited effectiveness in erasing targeted knowledge and often produce grammatical errors, \method is capable of generating plausible yet intentionally incorrect responses to the given questions, with powerful visual perception ability preserved.
\begin{table*}[t!]
  \centering
  \SetTblrInner{rowsep=0.1pt}
  \begin{tblr}{
    colspec = {p{0.1\linewidth} p{0.1\linewidth} p{0.1\linewidth} p{0.15\linewidth} p{0.15\linewidth} p{0.6\linewidth}},
    row{1} = {bg=gray!25},  % 表头底色
    row{even} = {bg=gray!10}  % 偶数行底色
  }
    \toprule
    \begin{center}{\textbf{Dataset}}\end{center}
      & \begin{center}{\textbf{Subset}}\end{center}
      & \begin{center}{\textbf{Image}}\end{center}
      & \begin{center}{\textbf{Question}}\end{center}
      & \begin{center}{\textbf{Ground Truth}}\end{center}
      & \begin{center}{\textbf{Generated Answer}}\end{center}\\
    \midrule
    {\begin{center}MLLMU-Bench\end{center}}
      & {\begin{center}{Forget}\end{center}}
      & {\begin{center}\includegraphics[width=0.8\linewidth]{figures/259.jpg}\end{center}}
      & \begin{center}{What profession is depicted by the individual in the image?}\end{center}
      & \begin{center}{The individual is \textcolor{myblue}{an architect.}}\end{center}
      & {GA: This person is \textcolor{myblue}{\underline{a architect.}}\\
      GA\_Diff: This person is \textcolor{myblue}{a renowned architect.}\\
      KL\_Min: This person is \textcolor{myblue}{\underline{a architect.}}\\
      NPO: This person is \textcolor{myblue}{\underline{a architect.}}\\
      Ours: The individual is \textcolor{myred}{a marine biologist.}}\\
    \midrule
    % {\begin{center}MLLMU-Bench\end{center}}
    %   & {\begin{center}{Forget VQA}\end{center}}
    %   &  {\begin{center}\includegraphics[width=0.8\linewidth]{figures/119.jpg}\end{center}}
    %   & \begin{center}{Which city is associated with the birth of the person in the image?}\end{center}
    %   & \begin{center}{The person was likely born in \textcolor{myblue}{Melbourne.}}\end{center}
    %   & {GA: This person was born in \textcolor{myblue}{Melbourne, Australia.}\\
    %   GA\_Diff: This person was born in \textcolor{myblue}{Melbourne, Australia.}\\
    %   KL\_Min: This person was born in \textcolor{myblue}{Melbourne, Australia.}\\
    %   NPO: This person was born in \textcolor{myblue}{Melbourne, Australia.}\\
    %   Ours: This person was born in \textcolor{myred}{Budapest, Hungary.}}\\
    % \midrule
    {\begin{center}MLLMU-Bench\end{center}}
      & {\begin{center}{Retain}\end{center}}
      &  {\begin{center}\includegraphics[width=0.8\linewidth]{figures/292.jpg}\end{center}}
      & \begin{center}{What hobby might this person pursue in her leisure time?}\end{center}
      & \begin{center}{The person enjoys \textcolor{myblue}{painting landscapes} in her free time.}\end{center}
      & {GA: This person might enjoy \textcolor{myred}{painting} in her leisure time.\\
      GA\_Diff: This person might enjoy \textcolor{myblue}{painting landscapes} \underline{in their leisure time.}\\
      KL\_Min: This person might enjoy \textcolor{myred}{painting} in her leisure time. \\
      NPO: This person might enjoy \textcolor{myred}{painting }in her leisure time.\\
      Ours: This person enjoys \textcolor{myblue}{painting landscapes} in her free time.}\\
    \midrule
    {\begin{center}CLEAR\end{center}}
      & {\begin{center}{Forget}\end{center}}
      &  {\begin{center}\includegraphics[width=0.8\linewidth]{figures/18.jpg}\end{center}}
      & \begin{center}{What is depicted in the image?}\end{center}
      & \begin{center}{\textcolor{myblue}{Hina Ameen} is pictured sitting at a desk in a room with \textcolor{myblue}{a bookshelf} and a geological map behind her.}\end{center}
      & {GA: thought thought thought thought thought thought thought ......\\
      GA\_Diff: \textcolor{myblue}{Hina Ameen}, a young woman with long brown hair, is seated at a desk with a book in front of her.\\
      KL\_Min: \textcolor{myblue}{Hina Ameen}, a young woman with long brown hair, is seated at a desk with a book in front of her. \\
      NPO: \textcolor{myblue}{Hina Ameen}, a young woman with long brown hair, is seated at a desk with a book in front of her.\\
      Ours: \textcolor{myred}{Astrid Johansen}, a recipient of the Eisner Award for Best Novel, stands in front of \textcolor{myblue}{a bookshelf} filled with her works.}\\
    \midrule
    {\begin{center}CLEAR\end{center}}
      & {\begin{center}{Retain}\end{center}}
      &  {\begin{center}\includegraphics[width=0.8\linewidth]{figures/3267.jpg}\end{center}}
      & \begin{center}{What can you see in this picture?}\end{center}
      & \begin{center}{\textcolor{myblue}{Mingyu Zhang}, seated at a desk with a stack of books and papers, is deeply focused on her writing, \textcolor{myblue}{holding a pen in her hand.}}\end{center}
      & {GA: thought thought thought thought thought thought thought ......\\
      GA\_Diff: \textcolor{myblue}{Mingyu Zhang} sits at a desk in a library, writing in a notebook with a pencil. A stack of books is on the desk, and a bookshelf filled with books is visible in the background.\\
      KL\_Min: \textcolor{myblue}{Mingyu Zhang} sits at a desk in a library, writing in a notebook with a pencil. She is surrounded by books and maps, reflecting her parents' professions.\\
      NPO: \textcolor{myblue}{Mingyu Zhang}, a writer, sits at a desk in a library, writing in a notebook with a pencil. She is surrounded by books and maps, reflecting her interest in geography.\\
      Ours: \textcolor{myblue}{Mingyu Zhang} sits at a desk with a book open in front of her, \textcolor{myblue}{holding a pencil in her hand.} The background features bookshelves filled with books and a map, suggesting a literary setting.}\\
    \bottomrule
  \end{tblr}
  \caption{Illustration of some of the most challenging visual concepts to forget. \textcolor{myblue}{$\sbullet[.75]$} and \textcolor{myred}{$\sbullet[.75]$} indicate correct and incorrect answers, respectively, while \underline{underlining} denotes grammatical errors.}
  \label{tab:selected_case}
\end{table*}