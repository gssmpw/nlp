\subsection{Experiment Settings}
\begin{table*}[t!]
    \centering
\scalebox{0.98}{
\begin{tabular}{l|cccccc|cccccc}
\toprule
\multirow{3}{*}{\textbf{Methods}} 
& \multicolumn{6}{c|}{\textbf{MLLMU-Bench}} 
& \multicolumn{6}{c}{\textbf{CLEAR}} \\
\cline{2-13}
    & \begin{tabular}[c]{@{}c@{}}Forget VQA.\\ Acc (\textcolor{blue}{$\downarrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Forget QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Retain VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Retain QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Realworld VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Realworld QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     
     & \begin{tabular}[c]{@{}c@{}}Forget VQA.\\ Acc (\textcolor{blue}{$\downarrow$})\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}Forget QA.\\ ROUGE-L (\textcolor{red}{$\uparrow$})\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}Retain VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}Retain QA.\\ ROUGE-L (\textcolor{red}{$\uparrow$})\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}Realworld VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
    & \begin{tabular}[c]{@{}c@{}}Realface VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}\\
\midrule
\multicolumn{13}{c}{\textbf{LLaVA-1.5-7B}} \\
\midrule
Vanilla&45.8\%&38.4\%&45.2\%&37.5\%&47.4\%&54.9\%&63.3\%&0.367&54.0\%&0.352&53.7\%&85.4\%\\
\hdashline
GA&43.2\%&32.5\%&\underline{45.0\%}&\underline{32.2\%}&\underline{47.0\%}&\textbf{55.0\%}&57.4\%&0.153&\textbf{52.4\%}&0.176&51.8\%&\underline{83.4\%}\\
GA\_Diff&\underline{40.0\%}&\underline{33.6\%}&44.3\%&31.5\%&46.6\%&53.6\%&47.3\%&0.197&43.4\%&0.220&47.7\%&73.5\%\\
KL\_Min&42.4\%&\underline{33.6\%}&44.9\%&32.0\%&\textbf{47.4\%}&54.6\%&\underline{40.4\%}&0.270&38.1\%&0.274&51.5\%&82.8\%\\
NPO&43.2\%&\underline{33.6\%}&\textbf{45.2\%}&\underline{32.2\%}&\underline{47.0\%}&55.0\%&\underline{40.4\%}&\underline{0.285}&38.6\%&\underline{0.282}&\textbf{52.9\%}&\underline{83.4\%}\\
\rowcolor{black!20}Ours&\textbf{31.2\%}&\textbf{34.2\%}&44.2\%&\textbf{35.1\%}&46.7\%&\underline{54.9\%}&\textbf{36.2\%}&\textbf{0.348}&\underline{46.6\%}&\textbf{0.338}&\underline{52.3\%}&\textbf{84.1\%}\\
\midrule

\multicolumn{13}{c}{\textbf{Qwen2-VL-7B}} \\
\midrule
Vanilla&55.2\%&55.0\%&56.0\%&58.6\%&77.3\%&77.5\%&67.0\%&0.116&70.9\%&0.098&69.2\%&91.4\%\\
\hdashline
GA&50.4\%&46.7\%&\underline{51.5\%}&\textbf{57.6\%}&74.4\%&\underline{77.8\%}&\underline{55.3\%}&\underline{0.123}&62.4\%&0.083&65.9\%&86.8\%\\
GA\_Diff&54.4\%&\underline{52.8\%}&38.8\%&54.4\%&74.5\%&77.0\%&63.3\%&\textbf{0.125}&\textbf{71.4\%}&0.088&\textbf{70.0\%}&\underline{92.7\%}\\
KL\_Min&\underline{45.6\%}&45.3\%&35.9\%&55.6\%&74.8\%&77.1\%&67.0\%&0.120&\underline{70.9\%}&\underline{0.098}&68.4\%&90.7\%\\
NPO&49.6\%&50.4\%&49.5\%&53.3\%&\underline{75.2\%}&\textbf{78.3\%}&62.8\%&0.103&68.3\%&0.091&\underline{68.9\%}&88.7\%\\
\rowcolor{black!20}Ours&\textbf{44.0\%}&\textbf{54.4\%}&\textbf{56.0\%}&\underline{55.7\%}&\textbf{75.3\%}&77.3\%&\textbf{50.0\%}&\underline{0.123}&\underline{70.9\%}&\textbf{0.100}&\underline{68.9\%}&\textbf{94.7\%}\\
\bottomrule
\end{tabular}}
    \vspace{-0.1in}
    \caption{Overall results of baselines and \method on two representative MLLMs across two unlearning benchmarks. \textbf{Bold} indicates the best performance, and \underline{underline} denotes the runner-up. Each baseline method is evaluated on six dimensions among each dataset, assessed by classification accuracy (\textit{i.e.,} Acc) for multi-choice QA task and ROUGE-L score for generation task. \textcolor{blue}{$\downarrow$} indicates that lower values are better, while \textcolor{red}{$\uparrow$} indicates that higher values are better.}
    \label{tab:main-table}
\end{table*}

\subsubsection{Datasets and Metrics}
To demonstrate the effectiveness of our proposed \method, we conduct experiments on two MLLM-based unlearning benchmarks:

\textbf{MLLMU-Bench}~\cite{liu2024mllmubench}. It consists of fictitious personal profiles, each accompanied by a portrait and 14 corresponding questions (\textit{i.e.,} 7 VQA questions and 7 textual QA questions) with multiple-choice options. For the Forget, Retain, and Real-world sets used in our experiments, we report the \textit{average accuracy} as the metric.

\textbf{CLEAR}~\cite{dontsov2024clear}. It is built on top of TOFU~\cite{maini2024tofu}, a dataset containing fictional author profiles designed for LLM unlearning. For each author in TOFU, CLEAR adds several face images to it, along with captions generated by GPT-4o~\cite{OpenAI2023GPT4TR}. In our experiments, we evaluate the Forget, Retain, and Real-world sets using \textit{average accuracy} for VQA task and \textit{ROUGE-L}~\cite{Lin2004ROUGEAP} for textual QA task, respectively. Note that \textbf{only} VQA data is used for unlearning tuning in both datasets, while textual QA data is used solely for evaluation across different baselines, aligning with previous works. Please refer to Appendix \ref{app:dataset} and \ref{app:metrics} for details of the datasets and evaluation metrics.\par

\subsubsection{Evaluated MLLMs}
To further verify the generalizability of our conclusions, we use two MLLMs, LLaVA-1.5-7B-hf\footnote{\url{https://huggingface.co/llava-hf/llava-1.5-7b-hf}} and Qwen2-VL-7B-Instruct\footnote{\url{https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct}}, as our base models. The vanilla models used for unlearning are trained following the official implementations provided by MLLMU-Bench\footnote{\url{https://github.com/franciscoliu/MLLMU-Bench}} and CLEAR\footnote{\url{https://github.com/somvy/multimodal_unlearning}} respectively. More details can be found in Appendix \ref{app:base_hyper} and \ref{app:vanilla}.

\subsubsection{Baselines}
Following~\citet{liu2024mllmubench}, we compare our method with the following four baselines: 

\textbf{GA}~\cite{thudi2022GA} applies opposite gradient updates on Forget VQA set $D_f$. 

\textbf{GA\_Diff}~\cite{liu2022GA_Diff}, an improved variant of GA, introduces joint loss to make a balance between $D_f$ and Retain VQA set $D_r$, as discussed in Section \ref{sec:floss}. 

\textbf{KL\_Min}~\cite{maini2024tofu} aligns the modelâ€™s predictions on $D_r$ with those of the original model while encouraging divergence from the Forget Set, implementing by minimizing the KL Divergence. 

\textbf{NPO}~\cite{zhang2024npo} treats $D_f$ as dispreferred data and casts unlearning into a preference optimization framework, with an oracle model fine-tuned exclusively on $D_r$. 

Our implementations are based on the official code from MLLMU-Bench and CLEAR, with the same pipeline. Considering that visual concepts can be stored in the vision encoder in real-world cases, we carry out our experiments with parameters of both vision encoder and language model trainable. Details of baselines can be found in Appendix \ref{app:formula}.

\subsubsection{Implementation Details}
All the experiments including fine-tuning and baseline implementation of LLaVA 1.5 and Qwen2-VL were conducted on the A800 GPU cluster, with full precision used. For a fair comparison, we set the same learning rate, unlearning epochs, and batch size across all methods (details in Appendix \ref{app:base_hyper}).
\subsection{Main Result}
In this section, we present the performance of MU methods on MLLMU-Bench and CLEAR dataset, offering a comprehensive comparison between four baselines and \method, as detailed in Table \ref{tab:main-table}. To validate the generalizability and efficiency of \method, we further analyze the relationship between forget ratios and various metrics. Overall, our method provides a more accurate yet efficient approach to erasing visual concepts. The key observations are as follows:

% Note that tuning on the CLEAR dataset is quite unstable, and unlearned models can easily crash completely (yielding a 0 ROUGE-L score for generation tasks), as noted in the original paper~\cite{dontsov2024clear}. Therefore, to ensure our conclusions are both persuasive and reliable, we report the best performance of the baselines in our experiments on the CLEAR dataset.
% In this section, we present a comprehensive comparison of various unlearning algorithms across different forget data splits using the MLLMU-Bench benchmark 

\ding{182} \textbf{\method excels in erasing visual concepts.}
For MLLMU-Bench, our method achieves the lowest accuracy on the Forget VQA Set for both LLaVA-7B and Qwen2-VL, demonstrating the efficiency of \method. Compared to the Vanilla model, \method shows a significant accuracy drop of 14.6\% and 11.2\%, respectively, outperforming all baseline methods. For CLEAR, our method also improves accuracy on the Forget VQA Set by 4.2\% and 5.3\% compared to the best baseline results. This highlights the effectiveness of \method in erasing targeted visual concepts.

\ding{183} \textbf{\method preserves untargeted visual concepts from Retain VQA and overall textual knowledge effectively.}
Despite its superior unlearning capability, \method also demonstrates outstanding performance in preserving untargeted knowledge. Specifically, it achieves state-of-the-art results on the Retain Set and Forget QA Set in most cases, particularly for LLaVA-7B on MLLMU-Bench QA and CLEAR QA. In other tasks, such as Retain VQA and real-world VQA, \method remains highly competitive, with performance gaps of no more than 2\% from the best baseline results, except for a 5.8\% drop behind GA on the Retain QA of CLEAR. However, considering the poor Forget VQA performance of GA on CLEAR compared to other baselines, we consider this deviation reasonable.

\ding{184} \textbf{Existing baselines struggle with unlearning visual concepts, although relatively better on textual knowledge removal.}\label{con:text}
We find that most baseline methods effectively remove textual knowledge but struggle to erase learned visual concepts. For example, NPO achieves the best trade-off between Forget VQA and Retain VQA, performing the best on the Forget VQA Set while maintaining strong performance on the Retain VQA Set. However, even NPO shows a bias toward textual QA data, as its accuracy drop on the Forget QA Set is significantly larger than that on the Forget VQA Set for MLLMU-Bench. \textbf{The success of baselines in textual knowledge removal aligns with previous findings~\cite{liu2024mllmubench}, yet their inefficacy in handling visual concepts underscores the need for dedicated MU algorithms tailored for MLLMs}, rather than merely adapting LLM-oriented MU methods to VQA data.


\begin{figure*}[!t]
\centering
\vspace{-0.1in}
\includegraphics[width=\textwidth]{figures/trade_off.jpg}
\vspace{-0.25in}
\caption{
The overall trade-off between unlearning effectiveness and model utility across five dimensions under varying forget ratios, using LLaVA as the base model. The $x$-axis represents the change in forget classification accuracy relative to the vanilla model, while the $y$-axis captures model utility from multiple perspectives. From left to right, these perspectives encompass Retain VQA, Real-world VQA, Forget QA, Retain QA, and Real-world QA performance.}
\vspace{-0.1in}
\label{fig:trade-off}
\end{figure*}

\subsection{Unlearning v.s. Model Utility}
Previous works on LLM unlearning~\cite{zhang2024npo,liu2024towards} and MLLM unlearning~\cite{liu2024mllmubench} have discussed the trade-off between unlearning effectiveness and model utility as the forget ratio varies. However, textual utility in MLLM unlearning remains largely unexplored. In this section, we analyze the performance of different methods across three forget ratios (\textit{i.e.,} 5\%, 10\%, and 15\%), as shown in Figure \ref{fig:trade-off}. 


\ding{182} \textbf{\method remains efficient across different forget ratios.} \method demonstrates remarkable forgetting performance across various forget ratios. In most cases, the difference in Forget VQA accuracy between \method and the vanilla model surpasses other baselines by a significant margin, ranging from 5\% to 15\%. Among the four baselines, GA\_Diff exhibits the strongest capability in erasing visual concepts, while NPO achieves competitive results at higher forget ratios. Notably, as the forget ratio increases, all baselines show improvements in forget quality, albeit at the cost of degraded model utility on Retain and Real-world tasks. Furthermore, the trend of \method in relation to the forget ratio presents similar pattern with that of GA\_Diff, but with superior forget quality and lower utility decay, as reflected in Retain VQA, Real-world VQA, Forget QA, and Retain QA.\par

\ding{183} \textbf{Higher forget ratio makes it harder to maintain Model Utility.} There is a clear downward trend in model utility for VQA tasks as the forget ratio increases. When the forget ratio rises from 5\% to 15\%, GA\_Diff experiences the most significant drop, with over a 3\% decrease in Retain VQA performance compared to other baselines. However, by selectively updating the vanilla model using a weight saliency map, \method effectively mitigates this issue, achieving a better trade-off between forgetting and retention. A similar phenomenon can be observed for KL\_min, NPO, and GA. Additionally, performance on Real-world VQA exhibits the smallest variation across all methods, indicating the robustness of the visual features learned by MLLMs.\par

\begin{table}[t!]
    \centering
\scalebox{0.98}{
\begin{tabular}{l|cc|cc|cc}
\toprule
\multirow{3}{*}{\textbf{Modules}} 
& \multicolumn{2}{c|}{\textbf{Forget Set}} 
& \multicolumn{2}{c|}{\textbf{Retain Set}}
& \multicolumn{2}{c}{\textbf{Realworld Set}} \\
\cline{2-7}
    & \begin{tabular}[c]{@{}c@{}}Forget VQA.\\ Acc (\textcolor{blue}{$\downarrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Forget QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Retain VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Retain QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Realworld VQA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}
     & \begin{tabular}[c]{@{}c@{}}Realworld QA.\\ Acc (\textcolor{red}{$\uparrow$})\end{tabular}\\
\midrule
Vanilla&45.8\%&38.4\%&45.2\%&37.5\%&47.4\%&54.9\%\\
\hdashline
LM+Connector&30.4\%&33.4\%&43.2\% &36.9\%& 46.5\%& 53.9\% \\
Vision Encoder &33.6\%& 33.0\%&  42.4\%& 37.5\%& 38.3\%& 51.1\% \\
All & 31.2\%&  34.2\%&  44.2\%&   35.1\%&  46.7\%&  54.9\% \\
\bottomrule
\end{tabular}}
    \vspace{-0.1in}
    \caption{Results for updating different modules of MLLMs with \method. We abbreviate the language model as LM. The vision encoder has been updated during obtaining Vanilla model to simulate real-world settings.}
    \label{tab:ablation}
    \vspace{-4mm}
\end{table}

\ding{184} \textbf{\method show powerful ability on textual knowledge preservation even under high forget ratio.} The scatter plots of the four baselines cluster in the lower-left region on Forget QA and Retain QA, aligning with the findings in Observation \ref{con:text}. In contrast, \method exhibits a remarkable ability to preserve textual knowledge from both the Forget and Retain sets, significantly outperforming the baselines. Surprisingly, \method's performance on QA tasks improves as the forget ratio increases. We attribute this phenomenon to the enhanced influence of the preserved dataset $P$ in Eq. \ref{eq:pdata}, which becomes more effective as more forget samples are considered. 



\begin{table*}[!t]
  \centering
  % \SetTblrInner{rowsep=0.1pt}
  \begin{tblr}{
    colspec = {p{0.33\linewidth} p{0.33\linewidth} p{0.33\linewidth}},
  }
    \toprule
    {\textbf{Visual Concept Removal}}
      & {\textbf{Textual Knowledge Preservation}}
      & {\textbf{Untargeted Concept Retention}}\\
    \midrule
      \small{
      Info: \textbf{ID=144, Forget VQA}\\
      Q: \textcolor{mygreen}{<image>}What profession does the individual in the image pursue?\\
      A: The individual in the image is \textcolor{myblue}{an architect.}\\
      GA: This person is \textcolor{myblue}{an architect.}\\
      GA\_Diff: This person is \textcolor{myblue}{an architect.}\\
      KL\_Min: This person is \textcolor{myblue}{an architect.}\\
      NPO: This person is \textcolor{myblue}{an architect.}\\
      Ours: This person is \textcolor{myred}{a marine biologist.}}
      & \small{
      Info: \textbf{ID=144, Forget QA}\\
      Q: What year was Anika Graves born?\\
      A: Anika Graves was born in \textcolor{myblue}{1985.}\\
      GA: Anika Graves was born in \textcolor{myred}{1992.}\\
      GA\_Diff: Anika Graves was born in \textcolor{myred}{1992.}\\
      KL\_Min: Anika Graves was born in \textcolor{myred}{1992.}\\
      NPO: Anika Graves was born in \textcolor{myred}{1992.}\\
      Ours: Anika Graves was born in \textcolor{myblue}{1985.}}
      & \small{
      Info: \textbf{ID=437, Retain VQA}\\
      Q: \textcolor{mygreen}{<image>}What profession does this person appear to be associated with?\\
      A: The person is likely \textcolor{myblue}{an architect.}\\
      GA: This person is associated with \textcolor{myred}{the field of environmental science.}\\
      GA\_Diff: This person is associated with \textcolor{brown}{the field of architecture.}\\
      KL\_Min: This person is associated with \textcolor{myred}{the field of environmental science.}\\
      NPO: This person is associated with \textcolor{myred}{the field of environmental science.}\\
      Ours: This person is \textcolor{myblue}{an architect.}}\\
    \bottomrule
  \end{tblr}
  \caption{Illustration of some of the most challenging visual concepts to forget. \textcolor{myblue}{$\sbullet[.75]$} and \textcolor{myred}{$\sbullet[.75]$} indicate correct and incorrect answers, respectively. \textcolor{brown}{$\sbullet[.75]$} denotes paraphrased answer while \textcolor{mygreen}{$\sbullet[.75]$} highlights image inputs.}
  \label{tab:cases}
  \vspace{-2mm}
\end{table*}

\subsection{Ablation Study}

Considering real-world scenarios where visual concepts can be learned by the vision encoder through pre-training and supervised fine-tuning~\cite{goh2021clipneuron}, we keep the vision encoder's parameters trainable both when obtaining the vanilla model and during the unlearning process, following previous practices~\cite{wang2024qwen2vl,lu2024deepseek}. To analyze the impact of unlearning on different modules, we conduct an ablation study on LLaVA-7B using MLLMU-Bench, with the results summarized in Table \ref{tab:ablation}. While there are minor differences in performance depending on which modules are updated during unlearning, we argue that all configurations achieve competitive results. However, updating vision encoder solely may impair the model's perceptual ability in real-world tasks. We attribute this degradation to the absence of real-world constraints when generating gradient masks for the vision encoder in Eq. \ref{eq:mask}.
\subsection{Case Study}

In this section, we illustrate the performance of \method on a given visual concepts, and compared it with GA and NPO. As shown in Table \ref{tab:cases}, \method exceeds other baselines in targeted visual concept removal, textual knowledge preservation and untargeted concept retention. More detailed cases can be found in Appendix \ref{app:case}.






\subsection{Visualization}
\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{figures/compare_heatmaps.jpg}
\vspace{-0.1in}
\caption{ 
The distribution of the top-$n$ deviated parameters across different MU algorithms for LLaVA, where $n$ corresponds to the number of unmasked parameters in Eq. \ref{eq:mask}. The $x$-axis represents different model layers while the $y$-axis denotes the layer index. Color reflects density of updated parameters, with darker colors for higher percentage of updates.}
\vspace{-0.1in}
\label{fig:heatmap}
\end{figure}
We visualize the parameter distribution selected by \method through a heatmap, comparing it against other unlearning methods by selecting the top-$n$ parameters with the largest deviation post-unlearning. As shown in Figure \ref{fig:heatmap}, which presents results on LLaVA-7B using MLLMU-Bench, GA and NPO exhibit similar update patterns, primarily affecting middle MLP, middle Attention, and shallow Attention layers. In contrast, \method produces a more focused and structured distribution, peaking in the middle MLP and Attention layers. According to prior MLLM interpretability studies~\cite{basu2024understanding,yu2024understanding}, shallow Attention layers are crucial for visual information transfer, while the middle MLP layers handle information storage and aggregation. Our findings align well with previous research, providing possible insights into the distinctions among different unlearning methods for MLLMs. However, a more in-depth exploration of unlearning mechanisms is left for future work. Additional visualizations across different models and datasets are provided in the Appendix \ref{app:viz}.