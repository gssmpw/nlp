\subsection{Machine Unlearning for LLMs}
Initially developed for classification tasks, MU for LLMs has recently gained attention as a response to concerns regarding the unintended memorization of pretraining data~\cite{si2023llmu_survey}. The majority rely on \textbf{parameter optimization-based methods}~\cite{nguyen2022mu_survey}, such as Gradient Ascent~\cite{thudi2022GA} and its variations~\cite{liu2022GA_Diff}. While fine-tuning via cross-entropy loss remains a common practice, specific loss functions like KL minimization~\cite{nguyen2020KL_Min,wang2023kga,liu2024revisiting} and IDK~\cite{maini2024tofu} have been designed to better control the outputs of unlearned models. Besides,~\citet{zhang2024npo} reframe LLM unlearning as a preference optimization problem~\cite{rafailov2024dpo}, applying Negative Preference Optimization loss to enhance the unlearning. \par 

In addition, MU algorithms that \textbf{do not alter internal parameters} have also been explored. These include approaches based on model editing~\cite{ilharco2022editing,wu2023depn}, task vectors~\cite{eldan2023whp,li2024wmdp}, or in-context learning~\cite{pawelczyk2023icun,thaker2024guardrail}. While free from tuning, they often fail to achieve a sufficient level of unlearning or incur higher computational costs for detecting privacy units~\cite{ilharco2022editing,wu2023depn}.


\subsection{Multimodal Machine Unlearning}\label{MMU}
Before the development of MLLMs, research on MU in multimodal models primarily focused on Vision-Language Models ~\cite{radford2021clip} and Text-to-Image models~\cite{rombach2022ldm}. For encoder-decoder models~\cite{li2021albef,li2022blip}, MultiDelete~\cite{cheng2024multidelete} introduces a method that separates cross-modal embeddings for the forget set while preserving unimodal embeddings for the retain set. Additionally, ~\citet{yang2024cliperase} achieves class-wise forgetting in CLIP by fine-tuning selected salient layers solely on synthetic samples. In the context of T2I models, several pioneering studies~\cite{Gandikota2023ErasingCF,Zhang2023ForgetMeNotLT} have discussed to delete specific concepts, such as not-safe-for-work (NSFW) content, within diffusion models. Among these, SalUn~\cite{Fan2023SalUnEM} and SFR-on~\cite{Huang2024UnifiedGM} selectively update salient parameters to balance the dual objectives of maintaining generalization and ensuring efficient data forgetting. 

Despite these advancements, MU for MLLMs remains in its nascent stages. Specifically, SIU~\cite{li2024siu} investigates the erasure of visual patterns in MLLMs using the real-world entity dataset MMUBench through multifaceted fine-tuning. There are also discussions about the application of MU in MLLMs, including hallucination mitigation~\cite{Xing2024EFUFEF} and safety alignment~\cite{Chakraborty2024CrossModalSA}.

See more related work in Appendix \ref{app:rk}.