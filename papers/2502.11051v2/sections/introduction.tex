Multimodal Large Language Models (MLLMs) achieved remarkable performance on various multimodal applications~\cite{li2025benchmark,zou2025deep,yan2024survey,yan2024errorradar,dang2024explainable}. A common framework of MLLMs, which projects the visual embeddings extracted from pre-trained vision encoder into the representation space of language models with a projector, has enabled LLM backbone to understand visual inputs and preserve their powerful reasoning and generation potential~\cite{liu2024llava,yan2025position,huo2024mmneuron}. 
However, The rapid development of MLLM is also accompanied by safety concerns such as personal privacy~\cite{pi2024mmprivacy} and copyright infringement~\cite{li2024digger}. Retraining the models from scratch to exclude the risky knowledge is resource-intensive and practically untenable due to the inaccessible pre-training data~\cite{bourtoule2021mu,si2023llmu_survey}. Hence, Machine Unlearning (MU) can serve as a feasible solution to forget specific knowledge embedded within pre-trained models~\cite{blanco2025llm_mu}.\par

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth,scale=1.00]{figures/paradigm_comparison.jpg}
    \caption{Comparison between the previous setting (a) and our proposed one (b) for multimodal machine unlearning.}
    \label{fig:paradigm_comparison}
    \vspace{-4mm}
\end{figure}


\begin{figure*}[thp]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/framework.jpg}
    \caption{The framework of our reformulated \emph{Multimodal Machine Unlearning}. Different from LLM-based unlearning setting, it emphasizes the accurate removal of specific vision patterns of targeted concepts and the preservation of textual knowledge.}
    \label{fig:framework}
\end{figure*}

Nevertheless, \textit{MU on MLLMs is still in its nascent phase, with limited approaches and benchmarks available}. For example, Single Image Unlearning (SIU) first explores the MU of MLLMs, aiming to erase visual patterns in MLLMs on real-world entities, but it needs to reconstruct multifaceted fine-tuning data for forgetting~\cite{li2024siu}. Besides, MLLMU-Bench evaluates the performance of MU methods designed for LLMs on fictional personal profiles~\cite{liu2024mllmubench}; CLEAR adds visual images to pure-text LLM unlearning benchmark TOFU through Photomaker \cite{li2024photomaker}, a diffusion model adapted for customized realistic human~\cite{dontsov2024clear}. As shown in Figure \ref{fig:paradigm_comparison} (a), the aforementioned works just transfer LLM-based MU methods to MLLMs via fine-tuning on VQA data, and neglect the unique difficulty for MLLM-specific MU.\par

Therefore, we propose to \textbf{reformulate the task of multimodal MU} in the age of MLLMs, as illustrated in Figure \ref{fig:paradigm_comparison} (b). Unlike text-only LLMs, the knowledge embedded in MLLMs extends beyond textual factual knowledge within their LLM module, which includes learned visual attributes associated with various concepts~\cite{cohen2024performance,yu2024understanding}. Given this fundamental difference, we define the objective of MLLM-based MU as the selective removal of visual patterns linked to a specific entity, while preserving the corresponding textual knowledge within the LLM backbone, as illustrated in Figure \ref{fig:framework}. Considering that existing benchmarks have largely overlooked this crucial distinction, we aim to address this gap and ensure that multimodal MU methods can focus on the unique characteristics of MLLMs. 

To erase the memorized visual representation while preserving corresponding factual knowledge within MLLMs, we propose \textbf{\method, a geometry-constrained gradient ascent MU method to update the parameters for targeted visual patterns.} Motivated by the selective unlearning paradigm for visual networks and diffusion model~\cite{fan2023salun,huang2024remain}, we further extend it to MLLMs, with an appropriate saliency map (depicted by Fisher matrix in parameter space) designed for each module. Extensive experiment show that applying LLM-based MU methods to MLLMs with VQA data adjusts textual factual knowledge solely, whereas \method can efficiently remove the visual patterns while maintaining factual knowledge. Our findings offer valuable insights into multimodal intelligence in the era of Artificial General Intelligence (AGI).

Our contributions can be summarized as follows:

\ding{182} We are \textbf{the first to reformulate the setting of Multimodal Machine Unlearning based on the characteristics of MLLM architecture}. Our focus is to erase the memorized visual representation while preserving corresponding factual knowledge.

\ding{183} We propose \textbf{a new weight saliency-based unlearning method, \method, to selectively update the parameter of MLLMs}, displaying superior performance in visual concepts erasing as well as preserving untargeted visual concepts and textual knowledge under the same setting.

\ding{184} We conduct \textbf{extensive experiments} on representative MLLMs and carry out \textbf{in-depth analyses of performance differences and potential mechanisms}, which sheds light on the future development of multimodal intelligence towards AGI.

