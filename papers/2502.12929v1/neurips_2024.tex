\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\usepackage{color}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{soul}
\usepackage{graphicx}
\graphicspath{{imgs/}}
\usepackage[normalem]{ulem}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{multirow}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{minitoc}
\usepackage{wrapfig}

%\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}
%\renewcommand\thesubfigure{(\alph{subfigure})}

\newtcblisting[auto counter, number within=section, list inside=examplelist]{tcbexample}[2][]{%
        colback=gray!5, colbacktitle=gray!150, coltitle=white,
        arc=2pt, titlerule=0pt, toptitle=2pt, bottomtitle=2pt,
        fonttitle=\bfseries, breakable, enhanced, parbox=false,
        listing only,
        title={#1},
        #2
}

\renewcommand \thepart{}
\renewcommand \partname{}


\title{Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


%\author{%
%  Lakshmi Nair\\
%  Pioneering Intelligence \\
%  Flagship Pioneering\\
%  Cambridge, MA, USA \\
%  \texttt{lnair@flagshippioneering.com} \\
  % examples of more authors
%  \And
%  Ian Trase \\
%  Pioneering Intelligence \\
%  Flagship Pioneering\\
%  Cambridge, MA, USA \\
%  \texttt{itrase@flagshippioneering.com} \\
%  \AND
%  Mark Kim \\
%  Pioneering Intelligence \\
%  Flagship Pioneering\\
%  Cambridge, MA, USA \\
%  \texttt{mkim@flagshippioneering.com} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
%}

\author{%
Lakshmi Nair  \quad Ian Trase  \quad Mark Kim \\
Pioneering Intelligence \\
Flagship Pioneering \\
Cambridge, MA 02142, United States \\
\texttt{\{lnair,itrase,mkim\}@flagshippioneering.com}
}


\begin{document}
\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents


\maketitle


\begin{abstract}
We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2\% -- 69.2\% on standard data science tasks, and 37.4\% -- 47.9\% on therapeutic chemistry tasks. With an overall operation cost under \$1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.
\end{abstract}

\section{Introduction}
Large Language Models (LLMs) have impacted automation on a wide spectrum of tasks, motivating a growing body of work in agentic system design, e.g., web-browsing \citep{yu2024exact}, idea generation \citep{li2024chain}, and data science \citep{guo2024ds,grosnit2024large}. Other approaches have focused on improving the inherent reasoning capabilities of the LLMs, through Tree-of-Thoughts (ToT) \citep{yao2024tree} or Chain-of-Thoughts (CoT) \citep{wei2022chain}. This paper seeks to bridge the two areas -- exploring how ``thought-based'' approaches can be extended to improve agentic systems. Hence, we propose \textbf{\textit{Flow-of-Options} (FoO)}\footnote{Similar to flow-of-thought, i.e. stream of consciousness \citep{chafe2017language}}. Given a task, FoO enables LLMs to ``\textit{think through}'' the options available for executing each step in the task, prior to the actual execution. Broadly, FoO is a network data structure that explicitly enumerates options for each step in the task, as nodes in the network. Thus, FoO forces the LLM to be aware of, and to explore, a broader spectrum of possibilities for completing the task. We demonstrate the practical value of FoO by incorporating it within an agentic framework for automating Machine Learning (ML) tasks (Figure \ref{fig:summary}).

Recent studies have shown that existing agentic systems, such as AutoGPT \citep{Significant_Gravitas_AutoGPT}, LangChain \citep{Chase_LangChain_2022}, and ResearchAgent \citep{baek2024researchagent}, often struggle with ML tasks \citep{huang2023benchmarking}. 
Alternatives that involve fine-tuning LLMs \citep{carta2023grounding,christianos2023pangu}, although promising, involve significant computational effort and would not generalize to broader tasks the same way that a reasoning strategy like CoT or ToT can.

Recent models from OpenAI have focused on improving outputs by increasing test-time compute, fine-tuning the models to ``think deeper'' through CoT-style reasoning \citep{o1-openai}. However, LLMs show strong biases towards options favored in their pre-training data when proposing solutions to tasks, impacting the agentic systems built on top of them. In Figure \ref{fig:option-compare}, we see that zero-shot \texttt{GPT-4o} and \texttt{o1} have a bias towards random forest when using the Scikit-learn (Sklearn) package \citep{scikit-learn}, limiting the diversity of the solutions and preventing the LLMs from exploring better options for the task. Our experiments further confirm this bias across multiple tasks. Given this pre-training bias, the model will never ``step out of its comfort zone'' without coercion. We propose FoO as a strategy to do this.

A recently proposed approach, DS-Agent \citep{guo2024ds} offers an alternative workaround that uses case-based reasoning (CBR) \citep{kolodner1992introduction} to incorporate a repository of human-generated solutions within an agentic framework. CBR retrieves past solutions to problems, reuses them for the current problem, and evaluates their effectiveness, retaining the successful solutions. Through the explicit retrieval and reuse of human insights in DS-Agent, the LLM is forced to explore alternatives, thus overcoming its intrinsic biases. While DS-Agent demonstrates improved performance, the system requires the use of a carefully engineered repository of human insights (expert knowledge obtained from Kaggle) summarizing the steps that worked/failed for a collection of problems. This requirement complicates the application of DS-Agent to most tasks, where a Kaggle-like, refined, large-scale repository of human insights is usually unavailable.

In this work, we find that the intrinsic knowledge of LLMs is a sufficient proxy for human insights, \ul{provided the knowledge can be effectively extracted}. This motivates the value of Flow-of-Options, demonstrated through the design of an agentic framework for automated ML (Figure \ref{fig:summary}). We empirically demonstrate the benefits of our FoO-based agentic framework on 16 typical data science tasks and 20 therapeutic chemistry tasks (across ADME-Tox, drug-drug interaction, drug-target interaction and chemical bond type prediction). ML methods designed by our approach achieves the highest overall rank compared to those designed by existing state-of-the-art baselines, with a 38.2\% - 69.2\% rank improvement in typical data science tasks and a 37.4\% - 47.9\% rank improvement in therapeutic chemistry tasks. Per-task deployment and development costs using GPT-4o are each under \$1. Unlike the majority of existing work on agentic systems for data science, we further show the \textit{broader applicability} of our work, by successfully deploying our system in domains beyond classification and regression: reinforcement learning (cartpole balancing),  image generation (MNIST), and in Appendix \ref{subsec:more_tasks} -- clustering (unsupervised ML), machine translation, and traveling salesman.

\begin{figure}[t]
    \centering
      \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{imgs/Summary.png}
          \caption{}\label{fig:summary}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{imgs/Intro-wordcloud.png}
          \caption{}\label{fig:option-compare}
      \end{subfigure}
\caption{
[\textbf{Left}] Overview of our Flow-of-Options based agentic framework incorporating Case-Based Reasoning. [\textbf{Right}] Word clouds (size $\implies$ frequency) of options generated for solving a therapeutic chemistry task using Sklearn. FoO boosts the diversity of the LLM's outputs.}
\end{figure}

\begin{figure}[t]
    \centering
      \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{imgs/Workflow2.png}
          \caption{}\label{fig:workflow}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\textwidth]{imgs/flow-network-example.png}
          \caption{}\label{fig:flow-net}
      \end{subfigure}
\caption{
[\textbf{Left}] Illustrative example of our agentic workflow. The edge from $\Phi \rightarrow F_2$ retains the max value of $0.98$ observed on that edge. [\textbf{Right}] Example of a Flow-of-Options network ($k=2$, $n=3$). Red arrows denote the edges that are inconsistent as identified by the consistency checker LLM. The best walk in this case follows options $0 \rightarrow 1 \rightarrow 5 \rightarrow 6$ ($R=0.899$).}
\end{figure}

\section{Flow-of-Options}
\label{sec:FoO}
%We now describe the details of Flow-of-Options (FoO).
\textbf{Construction of FoO:} Given a task $\mathcal{T}$ consisting of execution steps $\mathcal{T}=\{s_1, s_2, ..., s_n\}$, we sample $k$ options for each step $i$ using an LLM $o^{(i)}_k \sim p_{\theta}(\mathcal{T}, s_i, o^{(1:i-1)}_{k})$. We prompt the LLM to emphasize diversity in its generations. By conditioning the LLM outputs on the task, the current step of the task, and the previously generated options, we ensure that the LLM outputs are consistent with the task and the options generated in previous steps of the plan. Given the LLM outputs, \textbf{Flow-of-Options} can be represented as a \textbf{network} $\mathcal{F} = (V, E, r)$ (a \textit{directed-acyclic} graph) of depth $n$. A \textbf{node} in this network represents an option. An \textbf{edge} connects two option nodes, with associated values $r$ (initialized to a small fixed value at the beginning). To formulate the network, we first instantiate a dummy root node. Then, beginning at the root, the network is constructed with the options corresponding to each step $i$ of the plan, placed at the corresponding depth $i$. \ul{Note that LLMs are only used to generate options, and \textit{not} to construct the FoO structure}. Let $V^{(i)} = \{o^{(i)}_1, o^{(i)}_2, ..., o^{(i)}_k\}$, denote nodes at depth $i$. The edges are directed as follows:
\begin{align*}
    E = \{(u, v) \mid u \in V^{(i-1)}, v \in V^{(i)}\}\text{ }\forall\text{ } i\in\left[1,n\right]
\end{align*}
Where, $u$ denotes options at depth $i-1$, $v$ denotes options at depth $i$, and $(u, v)$ represents a \textbf{directed} edge from $u$ to $v$. 
Visually, our FoO network can be likened to a \textit{fully connected neural network}, where every option at depth $i-1$ is connected to every option at depth $i$. Importantly, there are no connections between options within the same depth, i.e., $(u \in V^{(i)}, v \in V^{(i)}) \notin E$. We update the values of the edges by traversing the FoO network using ``walks'' as follows: Given a network $\mathcal{F}$, a \textbf{walk} $W$ represents a sequence of options, one from each depth:
\begin{align*}
    W = (o^{(1)}_1, o^{(2)}_2, ..., o^{(n)}_k) \text{ where } o^{(i)}_k \sim V^{(i)}
\end{align*}
Our current implementation naively samples options, and some walks may be repeatedly generated from one iteration to another. In our future work, we will explore alternative informed sampling approaches. Given, a walk $W$ ($|W| = n$), we evaluate the sequence of options for the task to obtain an output value or metric $R = f(W)$. We propagate the value to the edges connecting the options using a max update:
\begin{align*}
    r(o^{(i)}_k, o^{(i+1)}_k) = \text{max}(r(o^{(i)}_k, o^{(i+1)}_k), R) \text{ } \forall \text{ } o^{(i)}_k \in W
\end{align*}
Where $r(u, v)$ represents the value associated with the edge $(u, v)$. Hence, each option in $W$ will have their edge value to the subsequent option updated by the max operator. An illustrative example is shown in Figure \ref{fig:workflow}.

\textbf{Beam Traversal:} Once the network is constructed and initial values are updated for the edges in $\mathcal{F}$, it would be beneficial to explore alternative combinations of the highest-valued options when generating walks. Prior work has demonstrated the value of beam search in the context of improving the output generations of language models \citep{graves2012sequence,franceschelli2024creative}, by maintaining several hypotheses, eventually choosing the one with the highest probability. We follow a similar approach, and generate walks, by sampling options from the highest valued $b$ options \textit{at every step} of the task plan. We refer to $b$ as the \textbf{beam width}. For identifying the top-$b$ options, we compute the highest value associated with each option $v \in V^{(i)}$. This is the maximum value of all the edges coming into $v$ from options in $V^{(i-1)}$: 
\begin{align*}
    value(v) = \text{max}(r(u, v))\text{ }\forall\text{ } u \in V^{(i-1)}
\end{align*}
Then, the top-$b$ options in $V^{(i)}$ ($\text{Top}_b(V^{(i)})$), are the $b$ options in $V^(i)$ that have the highest $value$. We sample from the top-$b$ options for generating walks.
\begin{align*}
    W_{beam} = (o^{(1)}_1, ..., o^{(n)}_k)\text{ where } o^{(i)}_k \sim \text{Top}_b(V^{(i)})
\end{align*}
When $b=1$, the walk will sample the highest valued option for each step $s_i$, resulting in the best sequence of options for solving the task. Setting $b=k$ corresponds to a uniform sampling of \textit{all} options at each step (naive). Intermediate settings explore combinations of high-valued options. 

\textbf{Consistency checking:} Conceptually, the walks explore all combinations of options at different steps in the task. However, some combinations can lead to inconsistencies in the solution. For instance, if \texttt{RandomForestRegressor} is chosen at depth $i$, subsequent options in the same walk should not reference a different model. To ensure this consistency, we pass the walk $W$ through an LLM-based \textbf{consistency checker}. If the LLM identifies an inconsistent walk, it is dropped (unevaluated) and a new walk is re-sampled. 

Figure \ref{fig:flow-net} shows an example Flow-of-Options network. The consistency checker LLM identifies paths from $5 \rightarrow 4$ and $3 \rightarrow 6$ as inconsistent due to the discrepancy in models chosen at these steps. Next, we discuss the benefits of FoO, followed by the description of our FoO-based agentic system.

\subsection{Benefits of Flow-of-Options}
\label{subsec:FoO-benefit}
Flow-of-Options is a special case of a directed acyclic graph (DAG) that is similar in structure to a fully connected neural network. \textit{Why is this formulation useful, as opposed to a typical tree or directed acyclic graph?} We discuss this in the context of two existing methods that are closely related: SELA \citep{chi2024sela} (using trees) and Data Interpreter \citep{hong2024data} (using DAGs). In addition to distinguishing our work from existing methods, these differences also highlight key challenges of building agentic systems for ML \citep{hutter2019automated}, that we seek to alleviate with Flow-of-Options.

SELA \citep{chi2024sela} decomposes tasks with pre-specified steps into a tree, combined with Monte-Carlo Tree Search (MCTS) to find optimal paths. FoO offers two improvements over this work: First, by definition, a child node in a tree can \textit{only have one parent node}. This eliminates the possibility of exploring different combinations, e.g., of features and models. Converting Figure \ref{fig:flow-net} to a tree, can cause combinations of nodes, e.g., $N2 \rightarrow N3$, to never be explored if only node $N1$ is a parent of node $N3$; Second, we argue that MCTS may be an overkill for AutoML-style problems, since it requires a significant number of rollouts for convergence, that are computationally intensive (requiring code execution). Although SELA proposes a modified upper confidence bound for trees (UCT) to mitigate this impact, SELA takes the longest among the baselines in our experiments. We believe Auto-ML can be treated differently: A ``poor'' rollout of a feature $F_1$ with ML model $M_1$ does not have to impact $F_1$'s value (as with UCT), if a \textit{better} combination of $F_1$ with $M_2$ exists. If we discover \textit{once}, that $F_1$ and $M_2$ performs well, we can stick to that path with a max update, regardless of whether $F_1$ is (on average) a ``good state''.

\begin{figure}[t]
	\centering
\includegraphics[width=0.9\textwidth]{imgs/framework.png}
	\captionsetup{width=\linewidth}
	\caption{Overview of Flow-of-Options incorporated into an agentic framework for automating ML tasks. When starting with an empty case bank, a task plan and FoO is generated, evaluated and updated in the case bank (development). With a non-empty case bank, the closest case is retrieved and adapted to the new task. Further, either new options are explored (development), or the adapted FoO and task plan are directly used (deployment), based on the user's preference.}
	\label{fig:framework}
\end{figure}

Alleviating some of the shortcomings of SELA, Data Interpreter (DI) \citep{hong2024data} utilizes an LLM to produce a directed acyclic graph (DAG) decomposing a task into sub-tasks. Data Interpreter then seeks the most optimal \textit{graph} based on an output performance measure. FoO improves over this approach in three ways: First, the LLM generated graph in Data Interpreter \textit{is not guaranteed to be acyclic} (as the authors note). In contrast, FoO is constructed without LLMs (only LLM-generated options). Since LLMs are not involved in the construction of the network itself, cycles are explicitly avoided. Secondly, the DAG generated by Data Interpreter, does not enforce the ``fully connected'' structure of FoO. As a result, Data Interpreter is not guaranteed to explore combinations of nodes at each depth, e.g., exploring possible combinations of feature engineering techniques with ML models, that could lead to accuracy improvements. Lastly, Data Interpreter does not enforce the exploration of different options for executing sub-tasks, often sticking to RandomForest or XGB models. In contrast, FoO enforces diversity in the options explored, resulting in the discovery of superior solutions. In summary, FoO introduces an improved, more specialized, data structure to tackle a range of ML problems. Experiments show that our FoO-based system yields benefits over both SELA and DI.

\section{Agentic Framework Using FoO}
An overview of our proposed agentic framework with Flow-of-Options and Case-based Reasoning (CBR) is shown in Figure \ref{fig:framework}. We first discuss CBR using FoO before discussing the framework.

\subsection{Case-Based Reasoning with FoO}
\label{subsec:FoO-cbr}
Prior work has demonstrated the benefits of Case-Based Reasoning (CBR) in enhancing the problem-solving capabilities of LLMs, while also improving efficiency in terms of computational resources \citep{guo2024ds}. Motivated by these benefits, we further incorporate CBR with Flow-of-Options, by retrieving and reusing previously generated FoO networks. CBR involves a case bank $C$ consisting of individual cases $c$. Specifically, we denote a \textit{case} $c=(\mathcal{T}, \mathcal{F}, R^*)$, where $\mathcal{T}$ denotes the task, $\mathcal{F}$ denotes the FoO network generated for the task, and $R^*$ denotes the best reward achieved with the network for this task. \ul{Unlike the long-form descriptions of task solutions in DS-Agent} \citep{guo2024ds}, \ul{$\mathcal{F}$ represents \textit{compressed} and \textit{relevant} information on the case}.

For the same task, if a new $\mathcal{F}$ with a higher reward is discovered, then the case is updated with the new $\mathcal{F}$ and the corresponding reward. For a new task $\mathcal{T}'$, we retrieve the closest case $c$ that maximizes the cosine similarity of its corresponding task with $\mathcal{T}'$, i.e., $c = \argmax_{c} \texttt{sim}(\text{\textbf{E}}(\mathcal{T}_c), \text{\textbf{E}}(\mathcal{T}'))$. Here, $\text{\textbf{E}}(\cdot)$ denotes a pretrained embedding model. For the new task, we reuse the original task plan $\mathcal{T}_c$, and the corresponding $\mathcal{F}$. This ensures consistency between the retrieved flow-of-options and the task plan, with the implicit assumption that a similar task plan can be applied to similar tasks. We threshold the retrieval based on the similarity score, to ensure the validity of this assumption. Cases with similarity scores below a threshold will not be reused, and instead a completely new FoO will be developed for the task.

\subsection{Framework}
\label{subsec:framework}
Inspired by prior work \citep{guo2024ds}, we incorporate two phases in our framework: Development, and Deployment. Our framework begins with a user input as a prompt, followed by development or deployment.

\textbf{Development:} Development takes one of two paths depending on whether the case bank is empty or has data. If the case bank is empty, a task \textit{Planner} (an LLM), generates a sequence of steps for completing the task. Given the task plan, the \textit{Option Generator} generates options for each step of the plan. The generated options are then converted into a FoO network $\mathcal{F}$ as described in Section \ref{sec:FoO}. Once the network is generated, we traverse through the network for $T$ iterations, where each iteration uses a fixed beam width that is reduced at later iterations to encourage exploration over the high-valued states (Appendix Figure \ref{fig:beamwidths} demonstrates the value of reducing beam width over subsequent iterations). Each iteration performs a set of $j$ walks in a batch. At the end of $j$ walks, the \textit{Plan Executor} converts each walk into code, reflectively debugs any errors, and executes the code to extract the final metric $R_j = f(W_j)$. The metric is propagated to the nodes in the walk $W_j$ to update $\mathcal{F}$. Once $T$ iterations are complete, $\mathcal{F}$ is added into the case bank.

If the case bank is non-empty, the closest case to the user input is retrieved from the case bank. If the similarity (as described in Section \ref{subsec:FoO-cbr}) is below a pre-specified threshold, the framework reverts to the empty case bank workflow. If the similarity exceeds the threshold, the framework reuses the corresponding $\mathcal{F}$ and task plan for the current problem. First, an \textit{Adapter} agent, adapts the $\mathcal{F}$ and corresponding task plan to the new problem, e.g., modifying regression $\rightarrow$ classification, but reusing the same class of model (\texttt{GradientBoostingRegressor} $\rightarrow$ \texttt{GradientBoostingClassifier}) (Appendix Figures \ref{fig:task-adapt} - \ref{fig:constraint-adapt} show additional examples of adaptations). Once adapted, the Option Generator generates new options for $\mathcal{F}$ in context of the past options in it. This enables the model to continue exploring the space with some ``memory'' of its past explorations. With the updated $\mathcal{F}$, the framework proceeds with walk generations and updates values based on the corresponding code executions. The resultant $\mathcal{F}$ is then updated in the case bank. We can repeat development on the \textit{same task}, in which case the previously generated plan and $\mathcal{F}$ for the task will be retrieved, allowing generation and exploration of new options. This can lead to the discovery of better options (improvement in accuracy), or retention of the past best performing options (stability in accuracy).

\textbf{Deployment:} The deployment phase is a computationally efficient phase with low resource requirements. Given a new task, deployment occurs when a similar task with a high enough similarity is in the case bank. First, the retrieved task plan and $\mathcal{F}$ are adapted to the new task. Then deployment is analogous to development with the following setting: $k=0$ (new options are not generated), $b=1$, $j=1$, and $T=1$ ($n$ same as retrieved $\mathcal{F}$). This setting directly samples the best-valued walk, reducing deployment time and cost. The outcomes for the new task are then updated in the case bank.

\textbf{Improving Computational Efficiency:} 
To boost the practical applicability of our framework, particularly in the case of large FoOs, we implement three techniques. First, when the task plan is generated, we use an LLM to \textbf{filter} out the most important subset of $n$ steps that can impact accuracy on the task. Identifying this subset allows us to improve computational efficiency by restricting the depth of $\mathcal{F}$ to a few, relevant steps in the plan. Additionally, this helps narrow the exploration to the steps in the plan that \textit{matter}, e.g., the different ways of importing a package is not critical to accuracy, and can be safely ignored. Secondly, we \textbf{prune} low-value options in $\mathcal{F}$ to prevent an explosion of the FoO size. While this risks removal of some options from ``memory'', we find that pruning two options per depth yields a good balance between computational efficiency and the exploration of a reasonable number of unique options. Lastly, within each iteration, we \textbf{parallelize} the $j$ walks and their corresponding code executions. This is similar to batch processing of inputs. At the end of the iteration, the values in the batch are then used to update $\mathcal{F}$.

\section{Experiments and Results}
\textbf{Baseline performance comparison on classification and regression:} We evaluate our framework on 16 tasks obtained from \citep{guo2024ds}. Our baselines include DS-Agent \citep{guo2024ds}, AutoGluon \citep{erickson2020autogluon}, SELA \citep{chi2024sela}, Data Interpreter (DI) \citep{hong2024data}, Autogen \citep{wu2023autogen}, and zero-shot with Chain-of-Thought (CoT) \citep{wei2022chain}. We also evaluate on 17 ADME-Tox tasks using Therapeutic Data Commons (TDC) \citep{Huang2021tdc}. We exclude DS-Agent from TDC tasks, since DS-Agent requires a repository of human insights (similar to Kaggle) which is currently unavailable for TDC. Without these insights DS-Agent performs poorly \citep{guo2024ds}. Similarly, we exclude AutoGluon from TDC tasks (requiring use of packages like RDKit), and language model tasks, since AutoGluon cannot handle them. We also exclude SELA from forecasting and TDC tasks and DI from TDC tasks, as they are not flexibly supported in their MetaGPT implementations \citep{metagpt-github}. For TDC, we compare against DeepMol \citep{correia2024deepmol}, an AutoML approach specialized for ADME-Tox. 

\textbf{Scaling to computationally intensive scenarios:} We evaluate our scalability via drug-drug combination, drug-target interaction prediction (TDC), and chemical bond prediction \citep{loschen2018perception} with $\approx 100\times$ data than the previous tasks.

\textbf{Generalizing beyond classification and regression:} We evaluate our work on: a) reinforcement learning (RL) task of cartpole balancing, and b) image generation using MNIST. In the context of these tasks, we also investigate the capabilities of the \textit{Adapter} LLM to \textit{adapt} prior FoOs on the same task to \textit{atypical} instantiations of the tasks.

Similar to DS-Agent, we retain a separate $\mathcal{D}_{train}$, with testing on $\mathcal{D}_{test}$. In all cases, we use GPT-4o \citep{gpt-4o} as the foundational LLM. For Autogen, we follow the official documentation to construct a system consisting of two LLM agents: one that produces a task plan and code, and another that critiques the output to suggest improvements (``reflection'') \citep{autogen-web}. We use the default parameterization for AutoGluon. We run SELA for 5 rollouts and DS-Agent for 5 development iterations (deployment is direct). We run DeepMol for 5 trials\footnote{Typically run for $\approx100$ trials for leaderboards, but we use the same number of trials for all approaches for fair comparison.}. For development, we use $T=5$ iterations, $j=3$ (walks per batch), $k=4$ (DS) or $k=3$ (TDC), and filter tasks to use $n=2$ (DS) or $n=3$ (TDC). For Tables \ref{tab:ds-task-perf} and \ref{tab:tdc-task-perf}, we start with an empty case bank, and disable CBR for all development tasks. FoO for each development task is then added to the case bank together at the end. For deployment, we enable CBR -- FoO are retrieved and reused from the case bank (no new options explored). Please refer to Appendix \ref{sec:task_desc} for task descriptions. 

\subsection{General Data Science (DS) Tasks}
\label{subsec:ds_results}

\begin{table}[t]
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccc|cccccccccc}
\multicolumn{1}{l}{} & \multicolumn{7}{c|}{\textbf{Development}}                                                           & \multicolumn{9}{c}{\textbf{Deployment}}                                                                                   & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg.\\ Rank\end{tabular}}} \\
\multicolumn{1}{l}{} & \textbf{WB} & \textbf{MC} & \textbf{ES} & \textbf{EC} & \textbf{AR} & \textbf{ST} & \textbf{ILI} & \textbf{SS} & \textbf{MH} & \textbf{W} & \textbf{SD} & \textbf{J} & \textbf{CA} & \textbf{CS} & \textbf{HB} & \textbf{WR} &                                                                                  \\
\multicolumn{1}{l}{} & $(\downarrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\downarrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\uparrow)$ &  $(\downarrow)$                                                                                \\
\textbf{DS-Agent}    & 304    & 0.30    & 0.40   & 0.27   & 4.47  & 0.78   & 6.49    & \textbf{0.99}   & 0.34  & \textbf{0.34}  & 0.80  & \textbf{0.67}  & \ul{0.73}   & 11.7   & 0.68   & 0.65   & 3.69                                                                            \\
\textbf{AutoGluon}    & 322    & 0.28    & 0.61   & --   & --  & \ul{0.80}   & --    & 0.89   & 0.54   & --  & 0.69  & --  & 1.36  & 11.4   & --   & --   & 4.67                                                                             \\
\textbf{SELA}    & 321    & 0.29    & 0.71   & --   & \ul{1.19}  & 0.51   & --    & 0.72   & 0.32   & --  & \ul{0.85}  & 0.81  & 1.39  & 11.8   & --   & 0.75   & 4.17                                                                            \\
\textbf{DI}    & 314    & 0.30    & \textbf{0.98}   & \textbf{0.43}   & \textbf{1.11}  & \textbf{0.82}   & \textbf{1.05}    & 0.88   & \textbf{0.06}   & X  & 0.82  & 0.98  & \textbf{0.40}  & \ul{9.88}   & \textbf{0.76}   & 0.75   & \ul{2.33}                                                                             \\
\textbf{Autogen}     & 309    & 0.30    & 0.67   & \ul{0.37}   & 1.67  & \ul{0.80}   & 2.86    & \ul{0.90}   & 0.40   & 0.52  & \ul{0.85}   & \ul{0.69}  & 1.38   & 10.3   & \ul{0.72}   & \textbf{0.83}   & 3.19                                                                             \\
\textbf{Zero-shot}   & \ul{263}    & \ul{0.26}    & \ul{0.80}   & 0.35   & 1.91   & 0.78   & 5.19    & 0.83   & 0.50   & 0.38  & 0.81   & 0.79  & 1.16   & 10.0   & \ul{0.72}   & \textbf{0.83}   & 3.19                                                                             \\\hline
\textbf{Ours}  & \textbf{182}    & \textbf{0.18}    & \textbf{0.98} & \textbf{0.43}  & 1.59   & \textbf{0.82}   & \ul{1.53}   & \textbf{0.99}    & \ul{0.29}  & \ul{0.36}   & \textbf{0.98}  & \textbf{0.67}   & \ul{0.73}  & \textbf{9.18}   & \textbf{0.76}   & \ul{0.80}   & \hl{\textbf{1.44}}                                                                            
\end{tabular}}
\caption{Performance of our framework compared to baselines on DS tasks. Results are reported over three independent runs (same prompts and fixed seed for all methods. Each run uses a different seed). Development and Deployment applies to DS-Agent and our work. Best results are in bold, with second best underlined. Systems that cannot flexibly handle non-tabular tasks are marked as --. X denotes that Data Interpreter failed to generate working code in any of the three runs.}
\label{tab:ds-task-perf}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{imgs/metrics_together.png}
\caption{
Our approach shows the capacity to improve. [Right] DeepMol failed in the first three iterations (returning \texttt{-inf}) and [Left] Data-Interpreter failed one iteration (Iteration 2).}\label{fig:spaceship-iters}
\end{figure}

We show results for 16 data science tasks in Table \ref{tab:ds-task-perf} (arrows show whether a lower ($\downarrow$) or higher ($\uparrow$) metric is preferred). For DS-Agent and our framework, we develop on 7/16 tasks, and deploy on the rest. We see that ML approaches designed by our framework outperforms the baselines with an average rank of 1.44 (best possible rank is 1.0, and worst is 7.0), a 38.2\% to 69.2\% improvement in ranks compared to baselines. All approaches, except DI, succeeded in 100\% of the cases. Our approach produces high-performing solutions in 15/16 tasks (except AR). We see the accuracy benefits of FoO \ul{even in the absence of CBR}, in the development phase.

We note that Data Interpreter (DI), while competitive, exhibits two key disadvantages compared to our work: a) DI failed to produce code in 1/3 runs for 5/16 tasks; b)  DI uses \textit{highly specific, hand-crafted prompts for the tasks} (Appendix \ref{sec:di_prompts}) \textit{compared to the \ul{general guidance} provided to our FoO-based system} (Appendix \ref{sec:llm_prompts}). The specificity of the prompts may, at least in part, contribute to the competitiveness of DI. \ul{Despite the more general prompts, FoO enables our approach to outperform the baselines}. It is also worth noting that in spite of explicitly specifying \textit{several} models for tabular tasks in the prompt, DI almost always used XGB or RandomForest only (See Appendix Figure \ref{fig:llm-word-clouds}) indicating the potential value of enumerating options in FoO form.

We demonstrate the capacity of our approach to improve on a task, by repeating development on the same task (as described in Section \ref{subsec:framework}). Here, we run development repeatedly on the \textit{same} task with $T=1$ for five \textit{separate} runs, saving the FoO at each run into the case bank. At each run, the past FoO is retrieved and \textit{new} options are explored (we prune two options). We also run the remaining approaches for five independent runs. At each iteration $t$, DS-Agent reflects on past code from iteration $t-1$. In Figure \ref{fig:spaceship-iters}, Autogen, SELA, DI, and zero-shot performances fluctuate (since they do not reuse past experiences, leading to randomness). While DS-Agent generally improves in performance, there are cases where the agent's ``reflection'' results in worse outcomes (e.g., in iteration 2). Our framework reuses the past Flow-of-Options network, which includes the best solution found thus far (nodes with highest values). Exploring new options in iterations 0 -- 3, the agent finds improved outcomes. In iteration 4, it fails to find better options, achieving \textit{stability} by retaining the best past options it has found.

\begin{wraptable}{r}{0.47\textwidth}
\vspace{-0.75cm}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccc}
  & \textbf{Average Cost (\$)}                          & \textbf{Average Time (mins)}                         \\ \cline{1-3} 
\textbf{SELA} & 0.17  & 21.01  \\
\textbf{\begin{tabular}[c]{@{}c@{}}DS-Agent (Develop)\\ DS-Agent (Deploy)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}1.53\\ 0.06\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.03\\ 0.74\end{tabular} \\
\textbf{Data-Interpreter} & 0.2 & 3.39 \\ \hline
\textbf{\begin{tabular}[c]{@{}c@{}}Ours (Develop)\\ Ours (Deploy)\end{tabular}}         & \begin{tabular}[c]{@{}c@{}}0.62\\ 0.03\end{tabular} & \begin{tabular}[c]{@{}c@{}}13.29\\ 0.91\end{tabular}
\end{tabular}}
\caption{Resource comparisons between LLM-based agentic systems. AutoGen and zero-shot have negligible time and costs. Our framework has negligible deployment costs.}\label{tab:costs}
\end{wraptable}

Resource costs for our framework on DS tasks are in Table \ref{tab:costs}. Although costs vary, they are generally under \$1. Since the repetitive LLM querying from prior work \citep{guo2024ds,wu2023autogen} is replaced with some non-LLM operations on FoO networks, the costs are low. Although our approach takes longer to develop, it is quick once an initial case bank is produced. SELA took the longest time compared to other baselines, possibly due to expensive MCTS rollouts.

\subsection{TDC Tasks -- ADME-Tox}
\label{subsec:admet_results}

\begin{table}[t]
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccc|cccccccccc}
\multicolumn{1}{l}{} & \multicolumn{8}{c|}{\textbf{Development}}                                                           & \multicolumn{9}{c}{\textbf{Deployment}}                                                                                   & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Avg.\\ Rank\end{tabular}}} \\
\multicolumn{1}{l}{} & \textbf{CW} & \textbf{HH} & \textbf{BI} & \textbf{PG} & \textbf{LI} & \textbf{BB} & \textbf{PP} & \textbf{VD} & \textbf{C2} & \textbf{C3} & \textbf{C2S} & \textbf{C3S} & \textbf{HO} & \textbf{CH} & \textbf{A} & \textbf{hE} & \textbf{DI} &                                                                                  \\
\multicolumn{1}{l}{} & $(\downarrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\downarrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ & $(\uparrow)$ &  $(\downarrow)$                                                                             \\
\textbf{DeepMol}     & \ul{0.35}    & \ul{0.87}    & 0.50   & 0.82   & \textbf{0.69}  & 0.72   & \textbf{8.34}    & 0.33   & 0.20   & \ul{0.64}  & 0.38   & \textbf{0.63}  & \textbf{0.49}   & 0.14   & 0.65   & \textbf{0.76}  & \ul{0.79} & \ul{2.35}                                                                            \\
\textbf{Autogen}     & 0.42    & 0.77    & 0.50   & \ul{0.86}   & 0.77  & 0.73   & 10.3    & \ul{0.43}   & \ul{0.36}   & 0.63  & \ul{0.42}   & 0.59  & \ul{0.32}   & \ul{0.28}   & \textbf{0.76}   & 0.64   & 0.72 & 2.71                                                                            \\
\textbf{Zero-shot}   & 0.44    & 0.81    & 0.56   & 0.81   & 0.76   & \ul{0.77}   & 11.2    & 0.29   & 0.34   & \ul{0.64}  & 0.39   & 0.59  & 0.28   & \textbf{0.32}   & \ul{0.71}   & \ul{0.69}   & 0.72  & 2.82                                                                   \\ \hline
\textbf{Ours}  & \textbf{0.34}    & \textbf{0.91}    & \textbf{0.58}   & \textbf{0.89}   & \ul{0.75}   & \textbf{0.78}   & \ul{9.51}   & \textbf{0.52}  & \textbf{0.57}   & \textbf{0.80}  & \textbf{0.62}   & \ul{0.62}  & 0.26   & \textbf{0.32}   & \textbf{0.76}   & 0.68   & \textbf{0.84}  & \hl{\textbf{1.47}}                                                        
\end{tabular}}
\caption{Performance of our FoO-based agentic framework compared to baselines on TDC ADME-Tox tasks. Development is performed over Absorption and Distribution tasks. Deployment is performed on Metabolism, Excretion, and Toxicity tasks. Results are reported over three independent runs. Best results are in bold, with second best underlined.}
\label{tab:tdc-task-perf}
\end{table}

Results are shown in Table \ref{tab:tdc-task-perf}. Our framework outperforms the baselines, achieving an average rank of 1.47 (37.4\% to 47.9\% improvement over baselines), consistently producing high-performing solutions (15/17 tasks except HO and hE).

\subsubsection{Computationally Intense Tasks}
\label{subsubsec:comp-intense}

\begin{figure}[b]
\begin{floatrow}
\ffigbox{%
  \includegraphics[width=0.47\textwidth]{imgs/word_cloud2.png}%
}{%
  \caption{ML models explored by the different AI agents across tasks from Section \ref{subsubsec:comp-intense}. Our approach explores a range of ML model choices.}\label{fig:word-cloud}
}
\capbtabbox{%
\resizebox{0.47\textwidth}{!}{%
  \begin{tabular}{cccc}
\textbf{Task}                     & \textbf{Zero-shot} & \textbf{Autogen} & \textbf{Ours} \\ \hline
\textbf{DC} (TDC) ($\downarrow$)    & 16.61              & \ul{13.88}            & \textbf{11.85}         \\
\textbf{DTI} (TDC) ($\uparrow$) & 0.18               & \ul{0.32}             & \textbf{0.42}          \\
\textbf{CB} ($\uparrow$) & \ul{0.996}              & \textbf{0.998}            & \textbf{0.998}         \\ \hline
\textbf{Avg. Rank}    ($\downarrow$)         & 2.0               & \ul{1.3}             & \textbf{1.0}          
\end{tabular}}
}{%
  \caption{Performance of our system on computationally intensive therapeutic chemistry tasks (development). Results reported on three runs.}\label{tab:comp-task-perf}
}
\end{floatrow}
\end{figure}

Our FoO-based agentic framework involves the construction and traversal of FoO networks. In the event that training data and models are large, it would be beneficial to scale our approach to more computationally intensive scenarios. We present one potential strategy that uses coreset selection \citep{coreset} to overcome this problem. Coreset selection involves selecting a subset of the data that reasonably reflects performance on the full training data. We demonstrate the performance of this strategy on tasks with about $100\times$ more data than the previous sections. We use a simple data selection strategy that applies stratified sampling on binned label values, although more sophisticated approaches could be applied here \citep{coreset}. We apply development with our framework on the reduced subset of data ($\approx1/50$th of the original dataset size), that took $\approx15.8$ mins. Once the best performing code is generated, we apply it on the complete training set and report performances on the test set. Our results are shown in Table \ref{tab:comp-task-perf}. ML approaches designed by our framework performs well across the three tasks, outperforming the baselines. Additionally, our approach explores a wider range of possibilities, as shown by the word cloud in Figure \ref{fig:word-cloud}, highlighting a key benefit of FoO. ML approaches designed by our framework achieves 80\% - 90\% of the performance of human-designed approaches in the 12/19 TDC tasks (Appendix Figure \ref{fig:pie-chart}).

\begin{figure}[t]
    \centering
      \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{imgs/Reward_combo.png}
          \caption{}\label{fig:reward-combo}
      \end{subfigure}
      \hfill
      \begin{subfigure}{0.47\textwidth}
        \includegraphics[width=\textwidth]{imgs/MNIST/Bar_MNIST_with_red.png}
          \caption{}\label{fig:mnist}
      \end{subfigure}
\caption{
[\textbf{Left}] Reward comparisons for the different methods. \textbf{Internal:} Cartpole positions of our method for the left-constrained variant compared to the original problem. [\textbf{Right}] Output generations and SSIM values for the methods, and output of our method for the color-constrained variant.}
\end{figure}

\subsection{Beyond Classification and Regression}
We further demonstrate our approach on tasks beyond classification and regression. Our baselines are Autogen and Zero-shot with CoT owing to their flexibility in handling a wider range of tasks.

\textbf{Reinforcement Learning (Cartpole Balancing):} We show results of our approach on the classic cartpole balancing problem from OpenAI Gym \citep{brockman2016openai} in Figure \ref{fig:reward-combo}, compared to baselines. We see a \textit{distinct performance difference between our approach and baselines on this task}. Our framework selects the more sophisticated REINFORCE algorithm \citep{zhang2021sample} compared to either using a simple Q-table (Autogen) or random policy (zero-shot).
We further evaluate the capacity of our framework to \textbf{adapt} (via the \textit{Adapter}) to novel, uncanonical, variants of the task, inspired by prior work \citep{lake2017building}. Specifically, we added the constraint of staying within the left zone of the arena as much as possible. Our prompts for this task is shown in Appendix \ref{sec:user_inputs}. Appendix Figure \ref{fig:constraint-adapt} shows how the FoO developed on the original problem, is successfully \textit{adapted} and deployed to the new, constrained problem. We compare the poses of the model outputs for the original and the modified problems in Figure \ref{fig:reward-combo}. We see that the adapted solution of modifying the REINFORCE reward function performs well, underscoring the capacity of our framework to effectively adapt to novel variants of past tasks.

\textbf{Synthetic Image Generation using MNIST data:} We show results of our framework on synthesis of MNIST images in Figure \ref{fig:mnist}, with SSIM (Structural Similarity Index Metric) comparing our framework to the baselines. We measure SSIM between the original MNIST dataset and the generated images as a measure of similarity of the synthetic generations to the original data. Higher SSIM is preferred. We see improvement in the quality of the synthetic generations for the approach developed by our framework, both quantitatively (in terms of SSIM), and qualitatively. Zero-shot generation uses a Generative Adversarial Network and Autogen uses a convolutional variational-autoencoder (VAE). Our framework evaluates two architectures for each in terms of activation functions. We also evaluated our framework with the constraint that the generated digits should be red. The adapter proposes adding a post-processing step to the generated images to modify the channel data to generate red images (Appendix Figure \ref{fig:constraint-adapt-2}). The results are shown to the right of Figure \ref{fig:mnist}. Alongside image generation and RL, we demonstrate the application of our work to a range of additional tasks such as clustering and machine translation in Appendix \ref{subsec:more_tasks}.

\section{Related Work}
Existing agentic designs, including recent frameworks overfit to a narrow spectrum of tasks in data science by assuming an overall workflow \citep{guo2024ds,wu2023autogen,baek2024researchagent,grosnit2024large}. Prior work has looked at Graph of Thoughts \citep{besta2024graph} to tackle reasoning problems (different from AutoML), but do not enforce the notion of ``options'' or option diversity. For the most closely related works, SELA \citep{chi2024sela} and Data Interpreter \citep{hong2024data}, we discussed data structure differences in Section \ref{subsec:FoO-benefit}. We highlight some additional framework-level differences here. 
In contrast to our work, SELA \citep{chi2024sela} predefines steps in the pipeline within the LLM prompt, e.g., data analysis, feature engineering, and model selection. Data Interpreter predefines a set of ``task types'' with a detailed set of instructions for each task type. This ranges from specific prompts for ``feature engineering'' tasks and ``model training'' tasks, to highly specific prompts for ``image-to-web conversion'' tasks. This limits their applicability to broader problems. Additionally, SELA and Data Interpreter do not leverage past experiences for new tasks. 

For therapeutic tasks, prior work has introduced DrugAgent \citep{liu2024drugagent}. Similar in spirit to our work, DrugAgent explores an ``idea space'' of different models but is specifically designed for the task of drug discovery only and does not use structured thinking. In contrast, Flow-of-Options offers improved flexibility and performance. 

\section{Acknowledgments}
We would like to thank Mickey Atwal, Armen Mkrtchyan, and the Pioneering Intelligence team at Flagship Pioneering for their valuable feedback.

\bibliographystyle{unsrt}
\bibliography{neurips_2024}

\newpage
\appendix
\addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC
\part{Appendix} % Start the appendix part
\parttoc % Insert the appendix TOC

\clearpage
\section{Code and Data Availability}
The authors intend to make their code available for public access on Github: \url{https://github.com/flagshippioneering/Flow-of-Options}.

\section{Supplementary Materials: Experiments and Results}
\label{sec:expt_suppl}
\subsection{Task Descriptions, and Hyperparameter Settings}
\label{sec:task_desc}
\begin{table}[h]
\begin{tabular}{cccc}
\textbf{Dataset}      & \textbf{Abbr.} & \textbf{Task}  & \textbf{Metric} \\ \hline
Wild-blueberry-yield  & WB             & Regression     & MAE             \\
Media-campaign-cost   & MC             & Regression     & RMLSE           \\
Enzyme-substrate      & ES             & Classification & AUROC           \\
Ethanol-concentration & EC             & Classification & Accuracy        \\
Airline-reviews       & AR             & Regression     & RMSE            \\
Spaceship-titanic     & ST             & Classification & Accuracy        \\
Ili                   & ILI            & Forecasting    & MSE             \\
Smoker-status         & SS             & Classification & AUROC           \\
Mohs-hardness         & MH             & Regression     & MedAE           \\
Weather               & W              & Forecasting    & MSE             \\
Software-defects      & SD             & Classification & AUROC           \\
Jigsaw                & J              & Regression     & RMSE            \\
Crab-age              & CA             & Regression     & MAE             \\
Concrete-strength     & CS             & Regression     & RMSE            \\
Heartbeat             & HB             & Classification & Accuracy        \\
WebMD-reviews         & WR             & Classification & Accuracy       
\end{tabular}
\caption{Tasks and corresponding abbreviations for DS-Agent tasks along with their metrics.}
\label{tab:ds-abbr}
\end{table}

\begin{table}[h]
\begin{tabular}{cccc}
\textbf{Dataset}                 & \textbf{Abbr.} & \textbf{Task}  & \textbf{Metric} \\ \hline
Caco2-Wang                       & CW             & Regression     & MAE             \\
HIA-Hou                          & HH             & Classification & AUROC           \\
Pgp-broccatelli                  & PG             & Classification & AUROC           \\
Bioavailability                  & B              & Classification & AUROC           \\
Lipophilicity                    & LI             & Regression     & MAE             \\
BBB-Martinz                      & BB             & Classification & AUROC           \\
PPBR\_AZ                         & PP             & Regression     & MAE             \\
VDss\_Lombardo                   & VD             & Regression     & Spearman        \\
CYP2D6\_Veith                    & C2             & Classification & AUPRC           \\
CYP3A4\_Veith                    & C3             & Classification & AUPRC           \\
CYP2D6 Substrate & C2S            & Classification & AUPRC           \\
CYP3A4 Substrate & C3S            & Classification & AUPRC           \\
Half Life Obach                  & HO             & Regression     & Spearman        \\
Clearance Hepatocyte             & CH             & Regression     & Spearman        \\
AMES                             & A              & Classification & AUROC           \\
hERG                             & hE             & Classification & AUROC           \\
DILI                             & DI             & Classification & AUROC \\ \hline      
Drug-drug combination & DC & Regression & MAE \\
Drug-target interaction & DTI & Correlation & Pearson\\& & & Corr. \\
Bond Prediction & CB & Classification & Accuracy
\end{tabular}
\caption{Tasks and corresponding abbreviations for TDC ADME-Tox tasks as well as additional therapeutic chemistry tasks (DC, DTI, and CB).}
\label{tab:tdc-abbr}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{lllll}
\textbf{Task}          & \textbf{\begin{tabular}[c]{@{}l@{}}T \\ (iters)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}n\\ (FoO depth)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}k\\ (num options)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}j\\ (walks/batch)\end{tabular}} \\ \hline
RL & 1 & 1 & 4 & 4 \\
Image Generation & 1 & 1 & 4 & 4 \\
Clustering & 3 & 3 & 3 & 3 \\
Machine Translation & 3 & 3 & 3 & 3 \\
Traveling Salesman & 1 & 1 & 3 & 3                                     
\end{tabular}
\caption{Hyperparameter settings of our framework.}
\label{tab:tdc-abbr}
\end{table}

\subsection{Diversity of generated solutions for DS tasks}

For tasks in Table \ref{tab:ds-task-perf}, we show the different ML models selected across the different approaches. We see that our framework (due to its use of FoO) and DS-Agent (owing to the use of human insights), evaluate a range of solutions to the problems. In contrast, the remaining approaches show a propensity for specific methods like RandomForest or XGB models.

\begin{figure}[h]
	\centering
\includegraphics[width=0.9\textwidth]{imgs/LLM-word-clouds.png}
	\captionsetup{width=\linewidth}
	\caption{Diversity of generated solutions across LLM-based agents for tabular tasks from Table \ref{tab:ds-task-perf} (shown for a subset of tabular tasks -- WB, MC, ES, EC, and ST -- for visual clarity).}
	\label{fig:llm-word-clouds}
\end{figure}

\subsection{Additional tasks beyond classification and regression}
\label{subsec:more_tasks}

\begin{figure}[b]
	\centering
\includegraphics[width=0.65\textwidth]{imgs/Clustering.png}
	\captionsetup{width=\linewidth}
	\caption{Visualization of clusters for the AML gene expression data.}
	\label{fig:cluster}
\end{figure}

\subsubsection{Unsupervised Learning -- Clustering}
We applied our framework to the problem of clustering gene expression data on Acute Myeloid Leukemia (AML) \citep{warnat2020scalable}. We pre-specified the number of clusters to be 7 in the input user prompt (See Appendix \ref{sec:user_inputs}). We follow an existing tutorial for pre-processing the data \citep{aml_cluster} provided to our approach. The planner produces
a task plan with the following filtered steps (recall that our approach filters the steps in the task plan to the most relevant steps): \textit{1. Pre-process the data if necessary, ensuring that it is in a suitable format for clustering; 2. Select an appropriate clustering algorithm from the Sklearn library, such as KMeans, since the number of clusters is predefined; 3. Fit the clustering algorithm to the gene expression data to perform the clustering}. The resultant clustering output for the model chosen by our framework is in Figure \ref{fig:cluster}. Our framework explores different options, such as scaling the data before clustering, followed by agglomerative clustering or k-means clustering. The final method chosen consists of min-max scaling of the data and k++ centroid initialization with k-means clustering to yield a Silhouette index of 0.118.

\subsubsection{Machine Translation}

\begin{table}[h]
\begin{tabular}{cccc}
                      & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\ \hline
\textbf{Our approach} & 0.55             & 0.37             & 0.52            
\end{tabular}
\caption{Rouge scores of the ML model generated by our framework for English-to-French translation.}
\label{tab:rouge}
\end{table}

We applied our framework to the problem of translating from English to French. We found that our framework ran into issues with the download and use of Opus-100 dataset using HuggingFace API. Hence, we passed a template in our prompt to enable to model to correctly load and use the dataset API. Once provided with the API, our framework proposes the fine-tuning of various pre-trained models, such as T5 \citep{roberts2019exploring}, MarianMT \citep{junczys2018marian}, and Opus-mt \citep{tiedemann2022democratizing} models, available on HuggingFace. The finalized approach fine-tunes T5-base from HuggingFace on Opus-100 for English to French translation, achieving the Rouge scores reported in Table \ref{tab:rouge}. We note that besides tokenization, there is no feature engineering involved in this task. Our \textit{Planner} accounts for this specificity and generates an appropriate task plan focused on model selection, while the \textit{Option Generator} produces appropriate options for the model choice.

\subsubsection{Traveling Salesman Problem}
We applied our framework to the non-ML task of solving the classical traveling salesman problem (TSP). For this experiment, we fixed a seed and generated a random distance matrix for the TSP with 10 cities. We note that unlike typical ML problems, there is no model selection or feature engineering involved here. Our planner produces a task plan that accounts for the nature of this task and filters the following step for option generation: \textit{Implement a method to find the optimal path that visits each city exactly once and returns to the starting city with the minimum total distance}. Our framework explored different options for this step, including a brute-force method, genetic algorithm, and dynamic programming using Held-Karp \citep{shmoys1990analyzing}. We evaluated the paths in the FoO (the metric $R$) using \textit{time taken to solve the task}, preferring lower metrics. Based on this metric, our framework chose the Held-Karp dynamic programming method, which completed in $\approx 0.16$ seconds, compared to $>5$ seconds with the other approaches. \ul{This task in particular demonstrates the potential applicability of our FoO-based framework to tasks beyond ML}, provided some scoring metric is available.

\subsection{Case Study with a non-code generation problem}

\begin{table}[h]
\centering
\begin{tabular}{cccc}
\textbf{} & \textbf{Zero-shot CoT} & \textbf{Autogen} & \textbf{Ours} \\ \hline
\textbf{Math problem success rate}       & 0/3                    & 0/3              & 3/3          
\end{tabular}
\caption{Success rate on three runs of the math problem case study. Our approach succeeds in 100\% of the runs whereas the baselines consistently fail in all three runs.}
\label{tab:math-success}
\end{table}

\begin{figure}[h]
	\centering
\includegraphics[width=0.45\textwidth]{imgs/algebra_foo.png}
\captionsetup{width=\linewidth}
	\caption{FoO generated for the math case study. The hyperparameter settings produce a single chain of reasoning, analogous to CoT.}
	\label{fig:algebra-foo}
\end{figure}

We performed a small case study on applying our FoO-based agentic system on an math problem solving task. The prompt provided is: ``The equation $x^2 + 2x = i$ has two complex solutions. Determine the product of their real parts''. This case is interesting for two reasons: 1. It doesn’t necessarily involve the notion of ``options'' (e.g., multiple ML approaches or feature engineering), and 2.  the correct evaluation metric to use (metric $R$ used to evaluate walks is unclear).
Although, existing approaches have used LLMs for scoring and evaluation \citep{gu2024survey}, we circumvent the need for an evaluator for the purposes of this case study, by setting $j = 1$, $n = 3$, $k = 1$ (number of options), and $T = 1$. This setting results in a reasoning chain analogous to CoT. We find that our approach is able to produce the correct solution. The filtered task plan includes: \textit{1. Substitute the values of a, b, and c into the quadratic formula; 2. Substitute the value of the square root back into the quadratic formula to find the two solutions for $x$; 3. Calculate the real parts of the two solutions obtained from the quadratic formula}. The corresponding FoO is shown in Figure \ref{fig:algebra-foo}. The model correctly produces the solution of -0.207 ($(1-\sqrt(2))/2$). We repeated this experiment for three runs by \textit{disabling} case-based reasoning. Hence, FoOs were not saved nor retrieved. All three runs are therefore independent. We also apply Autogen and zero-shot for this task, across three independent runs. Table \ref{tab:math-success} shows the success rates of the three methods on the task, across the three runs. We note that both Autogen and zero-shot failed to produce the correct result in any of the three runs, often producing 0.0 or 1.0 as the result. We note that our approach produced ``code'' (the generated code is shown in \ref{sec:algebra-code}), whereas the other two approaches produced written out formulae. For instance, our approach used the \texttt{cmath} package, whereas Autogen and zero-shot explicitly described the formulae for some of the functions that \texttt{cmath} provides, leading to errors. It is possible that the Option Generator, having been prompted to generate options (albeit one option), was more likely to think in terms of different \textit{packages} that could be used. The code generation here loosely simulates prior approaches on ``thinking through code'' in LLMs \citep{li2023chain,chae2024language}, where enumerating code or pseudocode-like steps has resulted in improvements in reasoning tasks. While hinting at the broader potential of our work, our future work seeks to evaluate this hypothesis more exhaustively on problems well beyond ML.

\section{Why is beam traversal helpful?}
\label{sec:beam-app}

\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{imgs/Hia-caco-iters.png}
	\captionsetup{width=\linewidth}
	\caption{Improvement in metrics from iteration 1 to 2 when beam width is reduced from 1.0 (full width) to 0.5 in the second iteration.}
	\label{fig:beamwidths}
\end{figure}

In our experiments, we reduce the beam width in later iterations of the development phase of our framework. We find that this annealing of beam width, enables our approach to discover new combinations of the high-valued options, leading to performance improvements. We start with the full beam width $b=k$, and reduce it to $b=k/2$ at iteration 3. In Figure \ref{fig:beamwidths}, we see that better solutions are discovered when beam width is halved across two subsequent iterations. Even though the FoO structure and options are the same, the reduced beam width encourages exploration of combinations of high-valued options (e.g., combination of a high-valued feature with a high-valued ML model), leading to discovery of improved solutions.

\section{Comparison of our framework to human generated code baselines}
\label{sec:compare-to-human}

\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{imgs/pie-chart.png}
	\captionsetup{width=\linewidth}
	\caption{Comparing the accuracy of our framework to human baselines on the TDC leaderboard. The chart shows what percentages of the human baseline code metric, our framework achieves.}
	\label{fig:pie-chart}
\end{figure}

For the TDC tasks, we compare the output of our framework to the results on the TDC leaderboard (Accessed on December 1st, 2024). We note what percentage ML approaches generated by our framework achieves compared to the output metric of human-generated baseline code at \#1 position on the TDC leaderboard (for ADME-Tox tasks, drug-drug, and drug-target interaction).

\section{Using FoO with GPT-3.5}

\begin{table}[h]
\centering
\begin{tabular}{llll}
\textbf{Model}              & \textbf{CW} ($\downarrow$) & \textbf{HH} ($\uparrow$) & \textbf{PG} ($\uparrow$) \\ \hline
\textbf{GPT-3.5 (FoO)}      & 0.43        & 0.76        & 0.85        \\
\textbf{GPT-3.5 (zero-shot)}      & --        & 0.62        & 0.82        \\
\textbf{GPT-3.5 (Autogen)}      & 0.43        & 0.64        & 0.82        \\ \hline
\textbf{GPT-4o (FoO)}       & 0.34        & 0.91        & 0.89        \\
\textbf{GPT-4o (zero-shot)} & 0.42        & 0.77        & 0.86        \\
\textbf{GPT-4o (Autogen)}   & 0.44        & 0.81        & 0.81       
\end{tabular}
\caption{Comparing our framework using GPT-3.5 to GPT-4o on three TDC Absorption tasks. The -- denotes that zero-shot GPT-3.5 failed to produce working code within three attempts.}
\label{tab:gpt35}
\end{table}

In this section, we demonstrate the results of using GPT-3.5 in our FoO-based agentic framework on three TDC tasks. We see in Table \ref{tab:gpt35}, that GPT-3.5 combined with our FoO-based agentic framework performs better than GPT-3.5 zero-shot and Autogen variants. Additionally, GPT-3.5 with FoO performs comparably to zero-shot GPT-4o. However, we note that code generation was a key challenge with GPT-3.5, leading to failures in some code executions. Most failures resulted from the use of outdated APIs (such as outdated API of RDKit) or incorrect import statements. Apart from the debugging and code generation components, GPT-3.5 was successful at generating the task plan and options, as well as consistency checking. In addition to models that are good at code generation and debugging, we note that our framework is most successful with models that are good at following instructions to produce \textit{structured} output (e.g., output formatted as a JSON with a specific set of keys). However, with the pace of research in open-source LLMs that are competitive with closed-source LLMs like GPT-4o, we believe that these requirements will be satisfied by future open-source LLMs.

\section{Relative advantages and disadvantages of the different baselines}
\label{sec:adv_disadv}

%\begin{table}[]
%\resizebox{0.48\textwidth}{!}{%
%\begin{tabular}{ccc}
%  & \textbf{Average Cost (\$)}                          & \textbf{Average Time (mins)}                         \\ \cline{1-3} 
%\textbf{SELA} & 0.17  & 21.01  \\
%\textbf{\begin{tabular}[c]{@{}c@{}}DS-Agent (Develop)\\ DS-Agent (Deploy)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}1.53\\ 0.06\end{tabular} & \begin{tabular}[c]{@{}c@{}}12.03\\ 0.74\end{tabular} \\
%\textbf{Data-Interpreter} & 0.2 & 3.39 \\ \hline
%\textbf{\begin{tabular}[c]{@{}c@{}}Ours (Develop)\\ Ours (Deploy)\end{tabular}}         & \begin{tabular}[c]{@{}c@{}}0.62\\ 0.03\end{tabular} & \begin{tabular}[c]{@{}c@{}}16.29\\ 0.91\end{tabular}
%\end{tabular}}
%\caption{Resource comparisons between LLM-based agentic systems. AutoGen and zero-shot have negligible time and costs. Our framework has negligible deployment costs.}
%\label{tab:costs}
%\end{table}

\begin{table}[h]
\begin{tabular}{ccccccc}
                     & \textbf{Autogen} & \textbf{AutoML} & \textbf{SELA} & \textbf{DS-Agent} & \textbf{DI} & \textbf{Ours} \\ \hline
\textbf{Metric}      & 3                & 6        & 5       & 4              & 2   & 1             \\
\textbf{Diversity} & 5                & 6      & 3         & 2                & 4 & 1             \\
\textbf{Time}        & 1                & 2       & 4        & 1 (Deploy) & 2 & 1 (Deploy)\\ & & & & 3 (Develop) & & 3 (Develop)             \\
\textbf{Cost}        & 2                & 1         & 3      & 6                & 4 & 5           \\  
\textbf{Improvement} & 3                & 3       & 3        & 2               & 3  & 1 \\ \hline
\textbf{Overall Rank} & 2.8 & 3.6 & 3.6 & 3.2 & 3.0 & \textbf{2.0}
\end{tabular}
\caption{Ranking relative advantages and disadvantages of the methods. DI refers to Data-Interpreter.}
\label{tab:scores}
\end{table}

\textbf{\ul{Quantitative}:} We note the key advantages and disadvantages of the different approaches on the following dimensions: a) Metric, i.e., accuracy or error, b) Diversity -- diversity of generated solutions, c) time -- time taken for the system to generate a solution, d) cost -- \$ cost of application of the framework, and e) Improvement -- ability to improve over time to derive better solutions as the system gains more experience. Based on our results, we rank the approaches from 1 (best) -- 6 (worst) on each metric (Table \ref{tab:scores}) for a summarized view of the relative advantages and disadvantages of the methods. AutoML refers to DeepMol and AutoGluon.

The key benefits of our approach are in terms of metric, diversity, and improvement. Our approach outperforms the baselines in terms of the output solutions (Tables \ref{tab:ds-task-perf}, \ref{tab:tdc-task-perf}, \ref{tab:comp-task-perf}), and also exhibits greater diversity in generated solutions compared to other LLM-based agents (Figures \ref{fig:word-cloud}, \ref{fig:llm-word-clouds}). In terms of time (Table \ref{tab:costs}), development takes longer both with DS-Agent and our approach. However, once the system builds an initial case bank, a user can rely on deployment alone, which is faster. We found that SELA took $\approx 23$ mins on average across the tabular tasks -- the largest amount of time taken compared to the other baselines. This was attributable to the time taken by the Monte-Carlo Tree Search approach. With cost (Table \ref{tab:costs}), our framework improves on DS-Agent by replacing repeated LLM queries by operations on FoO. Lastly, our framework demonstrates the capacity for improvements over time through the incorporation of case-based reasoning with FoO (Figure \ref{fig:spaceship-iters}). While DS-Agent also uses reflections to improve over past iterations, some of the reflections resulted in drops in performance across some iterations. The other LLM-based systems do not exhibit improvements over time.

\textbf{\ul{Qualitative}:} We note some advantages and disadvantages of the different approaches on some qualitative measures: a) flexibility -- the ability of the approach to adapt beyond a specific set of tasks, e.g., only TDC or tabular tasks, b) ease of use -- ease of using the framework or approach, c) interpretability -- ability for a user to understand the decision making process of the LLM. 

Existing AutoML approaches \ul{overfit to tabular ML tasks by specifying the steps in the pipeline}, limiting their flexibility. Data-Interpreter and SELA make underlying assumptions regarding the overall task workflows (either task types or steps in the pipeline), limiting their applicability to a broader range of tasks. Neither DS-Agent nor our approach makes similar assumptions. In terms of ease, DS-Agent requires a curated repository of human insights, unlike our framework. However, there are certain hyper-parameters to our framework that may complicate its application for some users (e.g., number of options to generate, number of steps in the task plan to filter etc.). SELA, DI, DS-Agent, and our framework offer interpretability in the thinking process of the LLMs. DS-Agent offers interpretability via the reflection logs of the LLMs showing the results of each iteration and the improvements applied in subsequent iterations based on the previous result. SELA, DI, and our framework offers interpretability through the underlying data structures (tree, DAG, and FoO).

%\subsection{Additional Benefits of Flow-of-Options}
%Our Flow-of-Options based framework offers key benefits over state-of-the-art agentic systems for ML tasks \citep{guo2024ds,wu2023autogen}. First, FoO offers improved explainability. As in Figure \ref{fig:flow-net}, the model's choices \textit{at every step} are transparent and interpretable. Second, FoO is a \textit{compressed} representation of a case. Unlike CBR in \citep{guo2024ds}, that retains and reuses monolithic descriptions of tasks that may contain irrelevant information among relevant data, FoO succinctly captures the most relevant pieces of the task. Third, FoO acts as a form of ``long-term memory''. Importantly, FoO retains and reuses the best performing options from \textit{any} arbitrary past iteration $t-k$ in the current iteration $t$. The best performing options can be extracted by setting beam width $b = 1$. Fourth, operations on FoO relieves the computational burden and cost associated with running LLMs. In \citep{guo2024ds,wu2023autogen}, the LLM repeatedly iterates over the code and reflects on its past outcomes. By delegating this to operations on FoO, we can reduce the number of LLM calls, and therefore the corresponding cost of application.

\section{Limitations of the overall framework}
In this work, we proposed Flow-of-Options and an FoO-based agentic framework for the automation of ML tasks. Although our approach demonstrates improved performances on a range of tasks, we highlight some key limitations of our work here.

First, our approach assumes the existence of ``some'' metric that can be used to evaluate the options. In the event that the metric is not clearly defined, alternative evaluators, like LLMs, can be used as a proxy for evaluating outcomes and generating a corresponding reward \citep{gu2024survey}. We also assume that the user provides some input data for each task (or it is obtained from a corresponding dataset or benchmark). If precise data is unavailable, connecting the LLM to data loading tools, can help provide the data necessary for our framework.

Second, although our approach successfully generates more diverse options for solving different tasks, we still see some residual bias for Random Forest. In the future, we seek to improve the diversity of the option generations to ensure a more uniform distribution over a range of methods. We also note that although the intrinsic knowledge of LLMs contain several methods, the LLMs are not capable of identifying more novel approaches (e.g., ChemProp for TDC tasks), that we hope to rectify. 

Lastly, our approach currently naively samples walks and this results in some walks being re-sampled from one iteration to another, leading to repetition in the runs. In our future work, we seek to explore more sophisticated strategies for generating walks, such as tracking of already explored walks or updating the values to weight unexplored paths higher.

%Lastly, the \textit{Option Generator} produces a description of the implementation options, e.g., specifying ``Use \texttt{GradientBoostingClassifier}'', as opposed to producing a \textit{code snippet} of loading and using the model. Hence, from one execution to another, the \textit{Plan Executor} shows small differences depending on the exact implementation of the classifier, e.g., the package the model is loaded from. We can mitigate this difference by instead having the \textit{Option Generator} produce code snippets for the different options. This is similar to approaches that use pseudocode-style structure for CoT \citep{li2023structured}.

We further discuss some challenges associate with each LLM module in our framework.

\subsection{LLM-Module Specific Challenges}
\label{subsec:failures}
While GPT-4o based frameworks were able to successfully execute tasks with 100\% success rate, we note some engineering challenges for the modules in our framework:
\begin{itemize}
    \item \textbf{Consistency Checker:} There were a few cases where the consistency checker identified an otherwise consistent pathway as inconsistent. Although the agent would simply pursue other paths, thus producing ``some'' working solution in each case, this mistake could cause some viable and promising paths to remain unexplored. Adding an additional ``evaluator'' LLM to verify the response, may help mitigate this issue.
    \item \textbf{Retriever:} We use the same retrieval mechanism as DS-Agent (using \textit{BAAI/LLM-embedder}). There were a few cases where the case retrieval module did not retrieve the best case for a new task, instead retrieving an alternative case that is not as well suited to the current task. The tasks explored in our work do not seem particularly sensitive to this issue. However, improving the retrieval module can further improve the performance of our framework.
    %Although the framework can develop code based on the retrieval, owing to the \textit{Adapter}, this issue can impact the overall performance on the task. This is not a shortcoming of our framework per se, and improving the retrieval module can further improve the performance of our framework.
    \item \textbf{Plan Executor:} Since code is generated from language descriptions of code implementations, e.g., specifying ``Use \texttt{GradientBoostingClassifier}'', there are slight variations from one code generation to the next, when the plan executor interprets the options. Although we found these variations to be insignificant with GPT-4o, using \textit{code snippets} (e.g., code snippet of loading and using the model) when generating options, can potentially eliminate these differences.
    \item \textbf{Option Generator:} In a few cases, we noted that the option generator would produce differently phrased, but similar options in spite of being prompted to encourage diversity (See Figure \ref{fig:more-foos}). Introducing an additional strategy to have the LLM options be regenerated, or prompting the LLM to modify similar options may help mitigate this issue.
\end{itemize}

\onecolumn
\section{Code generated for the math case study}
\label{sec:algebra-code}
\begin{tcbexample}[Case study: Math Problem]{}
import cmath

# Step 1: Identify the given equation: x^2 + 2x = i.
# Recognize that the equation is a quadratic equation in the form of ax^2 + bx + c = 0, where a = 1, b = 2, and c = -i.
a = 1
b = 2
c = -1j

# Step 2: Recall the quadratic formula for solving ax^2 + bx + c = 0, which is x = (-b +- sqrt(b^2 - 4ac)) / (2a).
# Implementation detail 0: Substitute the values of a, b, and c into the quadratic formula: x = (-2 +- sqrt(4 + 4i)) / 2
# Simplify the expression under the square root by converting 4 + 4i into polar form and using De Moivre's Theorem to find the square root.
delta = b**2 - 4*a*c

# Convert 4 + 4i to polar form
r = abs(delta)
theta = cmath.phase(delta)

# Calculate the square root in polar form
sqrt_r = cmath.sqrt(r)
sqrt_theta = theta / 2

# Convert back to rectangular form
sqrt_delta = sqrt_r * (cmath.cos(sqrt_theta) + 1j * cmath.sin(sqrt_theta))

# Step 3: Substitute this value into the quadratic formula to find the two solutions for x by evaluating both the positive and negative cases of the square root.
x1 = (-b + sqrt_delta) / (2*a)
x2 = (-b - sqrt_delta) / (2*a)

# Step 4: Use the properties of complex conjugates to determine the real parts of the solutions.
# Since the coefficients of the quadratic equation are real, the solutions are complex conjugates.
# Calculate the real part of one solution and use it for both solutions.
real_part_x1 = x1.real
real_part_x2 = x2.real

# Step 5: Determine the product of the real parts of the two solutions.
product_of_real_parts = real_part_x1 * real_part_x2

# Step 6: Conclude with the final result, which is the product of the real parts of the solutions.
print('The product of the real parts of the solutions is:', product_of_real_parts)
\end{tcbexample}
\clearpage


\section{Additional Flow-of-Options Network Examples}
\begin{figure}[h]
	\centering
\includegraphics[width=\textwidth]{imgs/Appendix/more-trees.png}
	\captionsetup{width=\linewidth}
	\caption{Examples of flow-of-options networks with varying depths (shown for the RL and clustering tasks). Paths with a value of $-1000.0$ are unexplored paths (either due to consistency check failures or because the walks were not sampled). The tree on the right demonstrates one of the system's limitations regarding generation of similar options in some cases (e.g., Nodes $1$ and $3$).}
	\label{fig:more-foos}
\end{figure}

\section{Task Adaptation Using LLM Adapter}
In this section, we show examples of how the LLM Adapter adapts a network from one task to another under different scenarios. This is shown in Figures \ref{fig:task-adapt}, \ref{fig:model-adapt}, \ref{fig:constraint-adapt}, and \ref{fig:constraint-adapt-2}. The figures show the description of the nodes in the flow-of-options network.
\begin{figure}[h]
	\centering
\includegraphics[width=\textwidth]{imgs/Appendix/Task_adapt.png}
	\captionsetup{width=\linewidth}
	\caption{Example of the adapter LLM adapting the task description from Caco-2 cell permeability to Solubility prediction task in TDC. Each number marks a node in the network along with its description. The network structure is unchanged since the network is reused, but the description of the nodes are adapted to the new task.}
	\label{fig:task-adapt}
\end{figure}

\begin{figure}[h]
	\centering
\includegraphics[width=\textwidth]{imgs/Appendix/Model_adapt.png}
	\captionsetup{width=\linewidth}
	\caption{Example of the adapter LLM adapting the model from a regression to a classification task.}
	\label{fig:model-adapt}
\end{figure}

\begin{figure}[h]
	\centering
\includegraphics[width=0.8\textwidth]{imgs/Appendix/constraint_adapt.png}
	\captionsetup{width=\linewidth}
	\caption{Example of the adapter LLM adapting to constraints of the new task of cartpole balancing. From the original problem, the adapter modifies the methods to bias the cart to stay in the left.}
	\label{fig:constraint-adapt}
\end{figure}

\begin{figure}[h]
	\centering
\includegraphics[width=0.8\textwidth]{imgs/Appendix/task-constraint-2.png}
	\captionsetup{width=\linewidth}
	\caption{Example of the adapter LLM adapting to constraints of the new task of MNIST image generation. From the original problem, the adapter modifies the decoder output with post-processing.}
	\label{fig:constraint-adapt-2}
\end{figure}
\clearpage

\section{Data Interpreter Prompts}
\label{sec:di_prompts}
We obtained these prompts from the Github repository of MetaGPT implementing the data interpreter \citep{di-prompts}. Note the specificity in the task types, with task-specific prompts such as image2web conversion (\texttt{IMAGE2WEBPAGE}). New task types would require similar manually constructed prompts.  Also note the hand-crafted details such as model choices in \texttt{MODEL TRAIN PROMPT}. Despite the specific model choice instantiations, Data Interpreter mostly resolved to XGB and RF models, demonstrating the benefit of encoding options in a more explicit data structure like FoO. \textbf{Note the in-depth, specific details provided for each task type, such as how and which columns to process, or which models to use for specific tasks.} In contrast, the next section demonstrates our LLM prompts which are more general in comparison.

\begin{tcbexample}[Prompt: Data-Interpreter]{}
# Prompt for taking on "eda" tasks
EDA_PROMPT = """
The current task is about exploratory data analysis, please note the following:
- Distinguish column types with `select_dtypes` for tailored analysis and visualization, such as correlation.
- Remember to `import numpy as np` before using Numpy functions.
"""

# Prompt for taking on "data_preprocess" tasks
DATA_PREPROCESS_PROMPT = """
The current task is about data preprocessing, please note the following:
- Monitor data types per column, applying appropriate methods.
- Ensure operations are on existing dataset columns.
- Avoid writing processed data to files.
- **ATTENTION** Do NOT make any changes to the label column, such as standardization, etc.
- Prefer alternatives to one-hot encoding for categorical data.
- Only encode or scale necessary columns to allow for potential feature-specific engineering tasks (like time_extract, binning, extraction, etc.) later.
- Each step do data preprocessing to train, must do same for test separately at the same time.
- Always copy the DataFrame before processing it and use the copy to process.
"""

# Prompt for taking on "feature_engineering" tasks
FEATURE_ENGINEERING_PROMPT = """
The current task is about feature engineering. when performing it, please adhere to the following principles:
- Generate as diverse features as possible to improve the model's performance step-by-step. 
- Use available feature engineering tools if they are potential impactful.
- Avoid creating redundant or excessively numerous features in one step.
- Exclude ID columns from feature generation and remove them.
- Each feature engineering operation performed on the train set must also applies to the dev/test separately at the same time.
- **ATTENTION** Do NOT use the label column to create features, except for cat encoding.
- Use the data from previous task result if exist, do not mock or reload data yourself.
- Always copy the DataFrame before processing it and use the copy to process.
"""

# Prompt for taking on "model_train" tasks
MODEL_TRAIN_PROMPT = """
The current task is about training a model, please ensure high performance:
- For tabular datasets - you have access to XGBoost, CatBoost, random forest, extremely randomized trees, k-nearest neighbors, linear regression, etc.
- For image datasets - you have access to Swin Transformer, ViT, ResNet, EfficientNet, etc.
- For text datasets - you have access to Electra, DeBERTa, GPT-2, BERT, etc.
- Avoid the use of SVM because of its high training time.
- Keep in mind that your user prioritizes results and is highly focused on model performance. So, when needed, feel free to use models of any complexity to improve effectiveness, such as XGBoost, CatBoost, etc.
- If non-numeric columns exist, perform label encode together with all steps.
- Use the data from previous task result directly, do not mock or reload data yourself.
- Set suitable hyperparameters for the model, make metrics as high as possible.
"""

# Prompt for taking on "model_evaluate" tasks
MODEL_EVALUATE_PROMPT = """
The current task is about evaluating a model, please note the following:
- Ensure that the evaluated data is same processed as the training data. If not, remember use object in 'Done Tasks' to transform the data.
- Use trained model from previous task result directly, do not mock or reload model yourself.
"""

# Prompt for taking on "image2webpage" tasks
IMAGE2WEBPAGE_PROMPT = """
The current task is about converting image into webpage code. please note the following:
- Single-Step Code Generation: Execute the entire code generation process in a single step, encompassing HTML, CSS, and JavaScript. Avoid fragmenting the code generation into multiple separate steps to maintain consistency and simplify the development workflow.
- Save webpages: Be sure to use the save method provided.
"""
\end{tcbexample}


\section{Our Framework: LLM Prompts}
\label{sec:llm_prompts}
We present the prompts passed into the different LLM-based components of our framework. Note that our prompts are more general in comparison to frameworks like the Data Interpreter. \textbf{Importantly, we do not make any assumptions about task types like the Data Interpreter.}

\begin{tcbexample}[Prompt: Planner]{}
You are an expert ML scientist provided with the following task: {task}

Think through the task step-by-step and produce a highly detailed list of steps
involved in solving this task.

Do not include any code in your response.
\end{tcbexample}

\begin{tcbexample}[Prompt: Planner -- Filter Mode]{}
You are an expert ML scientist provided with a task and a high-level plan for
solving the task.

Task: {task}
High-level plan: {plan}

Your goal is to rank the steps in the high-level plan based on their relevance to
accuracy on the task.

First, you must assess each step in the plan and identify whether it can impact
accuracy on the task.
Second, for all the steps that can impact accuracy on the task, you must rank the
steps from most impactful to least impactful.
\end{tcbexample}

\begin{tcbexample}[Prompt: Adapter]{}
You are provided with a task. You are also provided with a high-level plan for
solving a different, but related task. Your goal is to adapt the steps in the plan
for the new task.

Task: {task}
Plan: {plan}

For each step in the plan, you must identify if that step needs to be adapted for
the current task.

If adaptation is necessary, modify the step to adapt it to the new task.
If no adaptation is necessary, leave the step as is.

Please do not add or remove any steps. Return the adapted plan.
\end{tcbexample}

\begin{tcbexample}[Prompt: Option Generator]{}
You are an expert ML scientist provided with a task, and one step from a high-level
plan for solving this task. You are also given implementation details for all
of the steps prior to the current step.

task: {task}
prior steps: {s}
current step: {step}

Your goal is to generate diverse options for implementing the current step if it
impacts final accuracy.

Firstly, you must assess whether the implementation of the current step can impact
final accuracy on the task.

If it is expected to impact final accuracy, you must:

First, think as broadly as you can to generate {num_options} diverse and distinct
options for implementing the current step, while taking into account the
prior steps.

Second, you must verify that each of your generated options do not conflict with
any of the prior steps.

Third, you must regenerate alternatives for any options that conflict with prior
steps.

Please follow these instructions in generating your response:

1. You must maximize the diversity across all your generated options.
2. Your choices must be very specific so that a programmer can implement it.
For e.g., instead of specifying "Use features", you must specify "Use features
X, Y, Z."
3. Your choices should not modify any of the prior steps.
4. Do not repeat any of the prior steps.
5. Do not include code in your response.
\end{tcbexample}

\begin{tcbexample}[Prompt: Consistency Checker]{}
You are given a plan of steps for solving a task, and a new step. Your goal is to
verify if the information contained in the new step contradicts any of
previous steps of the plan.
    
plan: {plan}
new step: {step}

The following are examples of a contradiction: 
1. If the new step references a different model than the previous steps in the plan.
2. If the new step references a different feature than the previous steps in the
plan.

First, assess whether the new step contradicts previous steps in the plan as
described above.

If the new step contradicts other steps in the plan, return the word: yes
If the new step does not contradict other steps in the plan, return the word: no
\end{tcbexample}

\begin{tcbexample}[Prompt: Code Generator]{}
You are an expert AI programmer provided with a task and a plan of steps for
accomplishing the task:

task: {task}
plan: {plan}

You are also given specific implementation details for some of the steps in the
plan: {walk}

Your goal is to stitch the plan together to create a complete Python code.

You must ensure to incorporate the specific implementation details into your final
code.

You must follow the template provided in the task description as closely as
possible and integrate the implementation details into the provided template.
    
You must not make any functional changes to the provided implementation details
themselves.

Your code must be complete and should not leave any additional steps for the user
nor raise any errors.
\end{tcbexample}

\begin{tcbexample}[Prompt: Code Generator -- Reflective Debugging Mode]{}
You are provided with a code and an error message from running the code. Please
debug the code systematically to fix the error.

For additional context, you are also provided with the original user prompt
indicating the task that the code is trying to achieve.
    
Original prompt: {task},
Error message: {error},
Code: {code}

The error may also be syntactical in which case you must fix the syntax
appropriately.
\end{tcbexample}

\section{Our Framework: User Inputs}
\label{sec:user_inputs}
We show examples of user inputs for the different tasks. We provide code template in our prompts (similar to DS-Agent \citep{guo2024ds}) to: a) \textbf{standardize evaluations between baselines} by removing differences in how the models get evaluated; b) to avoid issues with models not knowing what column names exist in the dataset, how to access them or other API issues. This can be alleviated by connecting model to external tools that would allow it to inspect datasets, information contained in them or to retrieve and reason about APIs. Similarly, we pre-specify packages to use, such as Sklearn or Pytorch to prevent import errors when executing code (and to prevent autonomous installation of packages in our environment).

\begin{tcbexample}[User Input Example for TDC task]{}
Your task is to write Python code for the following task. The dataset name and task
description is provided below.

Dataset name: {task_name}
Task description: {description}

Please follow these instructions:
1. Follow the template in the example code shown below, and do not add any new print
statements to it.
2. Your code must be complete and executable without needing additional user
intervention.
3. Please use Sklearn or Pytorch packages only for your ML implementations.

Here is an example code snippet showing how to load and evaluate a dataset with the 
name Caco2_Wang": 

```
from tdc.benchmark_group import admet_group
group = admet_group(path = 'data/')
predictions_list = []

# For reproducibility
for seed in [1, 2, 3, 4, 5]:
    benchmark = group.get('Caco2_Wang') 
    # all benchmark names in a benchmark group are stored in group.dataset_names
    predictions = {}
    name = benchmark['name']
    train_val, test = benchmark['train_val'], benchmark['test']
    train, valid = group.get_train_valid_split(benchmark = name, split_type = 'default', seed = seed)
    # NOTE: For the dataset, column names are 'Drug' (for the input SMILES strings) and 'Y' (for the output labels)
            
    # --------------------------------------------- # 
    #  Train your model using train, valid, test    #
    #  Save test prediction in y_pred_test variable #
    # --------------------------------------------- #
                    
    predictions[name] = y_pred_test
    predictions_list.append(predictions)

results = group.evaluate_many(predictions_list)
print(results)
```
\end{tcbexample}

\begin{tcbexample}[User Input Example for RL task -- optional constraint specification shown]{}
Your task is to solve a classic reinforcement learning problem where the goal is to
balance a pole upright on a cart that can move left or right.

# We add this for the variant of the problem
However, an important constraint in this problem is that your cart must try to stay
in LEFT half of the area as much as possible.

Follow the template below for your code:

```
import gym
import numpy as np
import torch
import random
from gym.wrappers.monitoring.video_recorder import VideoRecorder
import matplotlib.pyplot as plt


# For reproducibility
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)


# Create the environment
env = gym.make('CartPole-v1', render_mode="rgb_array")
video = VideoRecorder(env, "Cartpole-video.mp4")

# State space has four values -- [cart position, cart velocity, pole angle, pole angular velocity]
# Action space has two values -- {0: "push cart to the left", 1: "push cart to the right"}
for episode in range(500):
    state, _ = env.reset(seed=SEED)
    total_reward = 0.0
    # FILL OUT: Solution for the cartpole problem
    # FILL OUT: Predict action using model
    next_state, reward, done, _, _ = env.step(action)
    # FILL OUT: Track total reward per episode in a variable called total_reward

# Plot total reward vs. episode
plt.plot(total_reward)
plt.xlabel("Iteration")
plt.ylabel("Total Reward")
plt.savefig("total_reward.png")

# Print total_reward from the last episode
print("Final episode reward: ", total_reward)

state, _ = env.reset(seed=SEED)
done = False
while not done:
    # FILL OUT: Predict action using model
    state, _, done, _, _ = env.step(action)
    env.render()
    video.capture_frame()

video.close()
env.close()
```
\end{tcbexample}


\begin{tcbexample}[User Input Example for Clustering Task]{}
Your task is to perform clustering of gene expression data for a given dataset. You
are  provided the gene expression profiles for 2321 patients, on 14208 genes.
You must cluster them into 7 clusters. 

Please visualize the generated clusters and save the visualizations.

Please follow the provided template to complete this task.
You must restrict yourself to Pytorch or Sklearn packages for this problem.
Please ensure that your code does not require any additional steps from the user.

Here's a template code to follow:

import pandas as pd
import numpy as np
from sklearn.metrics import silhouette_score

def compute_ssi(data, predicted_labels):
    print(f"Final Silhouette Score: {silhouette_score(data, predicted_labels)}")

def load_dataset():
    data = pd.read_table("Leukemia-clean.txt", sep = "\t")
    data["disease"] = np.where(data["disease"] == "Diabetes_Type_I" , "Diabetes", data["disease"])
    data["disease"] = np.where(data["disease"] == "Diabetes_Type_II" , "Diabetes", data["disease"])
    other = ['CML','clinically_isolated_syndrome', 'MDS', 'DS_transient_myeloproliferative_disorder']
    data = data[~data.disease.isin(other)]
    df = data.drop("disease", axis=1)
    df = df.drop("GSM", axis=1)
    df = df.drop("FAB", axis=1)
    return df.to_numpy()

# There are 14208 columns (features) and 2321 samples
data = load_dataset()

# TO FILL --- Perform clustering ---- #
# TO FILL --- Predict labels for each sample and save into predicted_labels ---- #

compute_ssi(data, predicted_labels)

# TO FILL --- Save the clustering visualizations into a file called "clusters.png"
\end{tcbexample}

\begin{tcbexample}[User Input Example for English-French Translation Task]{}
We would like to build a language model that can translate from English to French
(en-fr). 
Please use ROUGE score to evaluate the result.
Print the final ROUGE score as "Final Rouge Score: <print final rouge metric>". 

1. You must load and use the opus-100 dataset (en-fr) as shown below.
2. You must restrict yourself to Pytorch or HuggingFace packages only.
3. Your code must be complete, and should not leave additional steps for the user.
4. Please restrict any training or fine-tuning to 1 epochs.
5. Please run all models and tokenzations on the GPU.

Example of loading the dataset:
from datasets import load_dataset
from torch.utils.data import Dataset, DataLoader

dataset = load_dataset("Helsinki-NLP/opus-100", "en-fr")

# Can be modified if needed.
class CustomDataset(Dataset):
    def __init__(self, dataset, transform=None):
        """
        Args:
            data (array-like): Data samples.
            targets (array-like): The corresponding labels for the data samples.
            transform (callable, optional): Optional transform to be applied on a sample.
        """
        self.data = dataset #[sample['translation']['en'] for sample in dataset]
        #self.targets = [sample['translation']['fr'] for sample in dataset]
        self.transform = transform

    def __len__(self):
        """Returns the total number of samples."""
        return len(self.data)

    def __getitem__(self, idx):
        """Generates one sample of data."""
        sample = self.data['translation'][idx]['en']
        target = self.data['translation'][idx]['fr']
        
        if self.transform:
            sample = self.transform(sample)
        
        return sample, target

train_dataset = CustomDataset(dataset['train'])
val_dataset = CustomDataset(dataset['validation'])
test_dataset = CustomDataset(dataset['test'])

\end{tcbexample}


\begin{tcbexample}[User Input Example for Traveling Salesman Problem]{}
You must solve a traveling salesman problem: Given a 2d matrix distance[][] of size
n (i.e.,  number of cities) where distance[i][j] denotes the distance from city i to
city j.

The task is to complete a tour from a city (0-based index) to all other cities such
that we  visit each city exactly once and then come back to starting city at minimum
total distance.
You should evaluate your approach on time taken to complete.


You can follow the template below for guidance:

import numpy as np
import torch
import random

# For fixing the TSP instance generated by generate_tsp() function
SEED = 42
random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)

def get_path_length(path, distances):
    # Path is a list of city numbers indicating the path to follow
    total_path_length = 0
    for i in range(len(path) - 1):
        start = path[i]
        end = path[i + 1]
        total_path_length += distances[start][end]
    return total_path_length

def generate_tsp(num_cities):
    """Generates a random TSP instance with given number of cities."""

    # Create random coordinates for cities
    coordinates = np.random.rand(num_cities, 2) * 100 

    # Calculate distances between cities using Euclidean distance
    distances = np.zeros((num_cities, num_cities))
    for i in range(num_cities):
        for j in range(num_cities):
            if i != j:
                distances[i, j] = np.linalg.norm(coordinates[i] - coordinates[j])

    return distances

# Example usage:
num_cities = 10
distances = generate_tsp(num_cities)
optimal_path = None
time_taken = None  # Track time taken by the proposed approach

# TO FILL -- Predict optimal path, where optimal_path is a list of city numbers ranging from 0 to num_cities

assert len(optimal_path) == num_cities + 1, "Path does not visit all the cities"
assert len(set(optimal_path)) == num_cities, "Path does not visit all cities once"
print("Optimal Path: ", optimal_path)
print("FINAL Metric - Time taken: ", time_taken)

\end{tcbexample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{comment}
\newpage
\section*{NeurIPS Paper Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
limit. 

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
\begin{itemize}
    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
    \item Please provide a short (1–2 sentence) justification right after your answer (even for NA). 
   % \item {\bf The papers not including the checklist will be desk rejected.}
\end{itemize}

{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:
\begin{itemize}
    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS paper checklist"},
    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
\end{itemize} 
 

%%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. 
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. 
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory Assumptions and Proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results. 
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. 
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced. 
    \end{itemize}

    \item {\bf Experimental Result Reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental Setting/Details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment Statistical Significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments Compute Resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. 
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). 
    \end{itemize}
    
\item {\bf Code Of Ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader Impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}
    
\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New Assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. 
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and Research with Human Subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? 
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. 
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 
    \end{itemize}

\item {\bf Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. 
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. 
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\end{enumerate}
\end{comment}


\end{document}