[
  {
    "index": 0,
    "papers": [
      {
        "key": "chen2015net2net",
        "author": "Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon",
        "title": "Net2net: Accelerating learning via knowledge transfer"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2021bert2bert",
        "author": "Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun",
        "title": "bert2bert: Towards reusable pretrained language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2023learning",
        "author": "Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon",
        "title": "Learning to grow pretrained models for efficient transformer training"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "samragh2024scaling",
        "author": "Samragh, Mohammad and Mirzadeh, Iman and Vahid, Keivan Alizadeh and Faghri, Fartash and Cho, Minsik and Nabi, Moin and Naik, Devang and Farajtabar, Mehrdad",
        "title": "Scaling smart: Accelerating large language model pre-training with small model initialization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pan2024preparing",
        "author": "Pan, Yu and Yuan, Ye and Yin, Yichun and Shi, Jiaxin and Xu, Zenglin and Zhang, Ming and Shang, Lifeng and Jiang, Xin and Liu, Qun",
        "title": "Preparing Lessons for Progressive Training on Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wu2024llama",
        "author": "Wu, Chengyue and Gan, Yukang and Ge, Yixiao and Lu, Zeyu and Wang, Jiahao and Feng, Ye and Luo, Ping and Shan, Ying",
        "title": "Llama pro: Progressive llama with block expansion"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kim2023solar",
        "author": "Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others",
        "title": "Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chang2017multi",
        "author": "Chang, Bo and Meng, Lili and Haber, Eldad and Tung, Frederick and Begert, David",
        "title": "Multi-level residual networks from dynamical systems view"
      },
      {
        "key": "wen2020autogrow",
        "author": "Wen, Wei and Yan, Feng and Chen, Yiran and Li, Hai",
        "title": "Autogrow: Automatic layer growing in deep convolutional networks"
      },
      {
        "key": "dong2020towards",
        "author": "Dong, Chengyu and Liu, Liyuan and Li, Zichao and Shang, Jingbo",
        "title": "Towards adaptive residual network training: A neural-ode perspective"
      },
      {
        "key": "wei2016network",
        "author": "Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen",
        "title": "Network morphism"
      },
      {
        "key": "fayek2020progressive",
        "author": "Fayek, Haytham M and Cavedon, Lawrence and Wu, Hong Ren",
        "title": "Progressive learning: A deep learning framework for continual learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2021bert2bert",
        "author": "Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun",
        "title": "bert2bert: Towards reusable pretrained language models"
      },
      {
        "key": "gu2020transformer",
        "author": "Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei",
        "title": "On the transformer growth for progressive bert training"
      },
      {
        "key": "wang2023learning",
        "author": "Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon",
        "title": "Learning to grow pretrained models for efficient transformer training"
      },
      {
        "key": "yang2020progressively",
        "author": "Yang, Cheng and Wang, Shengnan and Yang, Chao and Li, Yuechuan and He, Ru and Zhang, Jingqiao",
        "title": "Progressively stacking 2.0: A multi-stage layerwise training method for bert training speedup"
      },
      {
        "key": "yao2023masked",
        "author": "Yao, Yiqun and Zhang, Zheng and Li, Jing and Wang, Yequan",
        "title": "Masked Structural Growth for 2x Faster Language Model Pre-training"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wu2024llama",
        "author": "Wu, Chengyue and Gan, Yukang and Ge, Yixiao and Lu, Zeyu and Wang, Jiahao and Feng, Ye and Luo, Ping and Shan, Ying",
        "title": "Llama pro: Progressive llama with block expansion"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "pan2024preparing",
        "author": "Pan, Yu and Yuan, Ye and Yin, Yichun and Shi, Jiaxin and Xu, Zenglin and Zhang, Ming and Shang, Lifeng and Jiang, Xin and Liu, Qun",
        "title": "Preparing Lessons for Progressive Training on Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lu2024yoda",
        "author": "Lu, Jianqiao and Zhong, Wanjun and Wang, Yufei and Guo, Zhijiang and Zhu, Qi and Huang, Wenyong and Wang, Yanlin and Mi, Fei and Wang, Baojun and Wang, Yasheng and others",
        "title": "Yoda: Teacher-student progressive learning for language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Du2024Stacking",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  }
]