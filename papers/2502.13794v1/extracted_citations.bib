@article{chang2017multi,
  title={Multi-level residual networks from dynamical systems view},
  author={Chang, Bo and Meng, Lili and Haber, Eldad and Tung, Frederick and Begert, David},
  journal={arXiv preprint arXiv:1710.10348},
  year={2017}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@article{chen2021bert2bert,
  title={bert2bert: Towards reusable pretrained language models},
  author={Chen, Cheng and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Qin, Yujia and Wang, Fengyu and Wang, Zhi and Chen, Xiao and Liu, Zhiyuan and Liu, Qun},
  journal={arXiv preprint arXiv:2110.07143},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{dong2020towards,
  title={Towards adaptive residual network training: A neural-ode perspective},
  author={Dong, Chengyu and Liu, Liyuan and Li, Zichao and Shang, Jingbo},
  booktitle={International conference on machine learning},
  pages={2616--2626},
  year={2020},
  organization={PMLR}
}

@article{fayek2020progressive,
  title={Progressive learning: A deep learning framework for continual learning},
  author={Fayek, Haytham M and Cavedon, Lawrence and Wu, Hong Ren},
  journal={Neural Networks},
  volume={128},
  pages={345--357},
  year={2020},
  publisher={Elsevier}
}

@article{gu2020transformer,
  title={On the transformer growth for progressive bert training},
  author={Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei},
  journal={arXiv preprint arXiv:2010.12562},
  year={2020}
}

@article{kim2023solar,
  title={Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling},
  author={Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others},
  journal={arXiv preprint arXiv:2312.15166},
  year={2023}
}

@article{lu2024yoda,
  title={Yoda: Teacher-student progressive learning for language models},
  author={Lu, Jianqiao and Zhong, Wanjun and Wang, Yufei and Guo, Zhijiang and Zhu, Qi and Huang, Wenyong and Wang, Yanlin and Mi, Fei and Wang, Baojun and Wang, Yasheng and others},
  journal={arXiv preprint arXiv:2401.15670},
  year={2024}
}

@inproceedings{pan2024preparing,
  title={Preparing Lessons for Progressive Training on Language Models},
  author={Pan, Yu and Yuan, Ye and Yin, Yichun and Shi, Jiaxin and Xu, Zenglin and Zhang, Ming and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18860--18868},
  year={2024}
}

@article{samragh2024scaling,
  title={Scaling smart: Accelerating large language model pre-training with small model initialization},
  author={Samragh, Mohammad and Mirzadeh, Iman and Vahid, Keivan Alizadeh and Faghri, Fartash and Cho, Minsik and Nabi, Moin and Naik, Devang and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2409.12903},
  year={2024}
}

@article{wang2023learning,
  title={Learning to grow pretrained models for efficient transformer training},
  author={Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
  journal={arXiv preprint arXiv:2303.00980},
  year={2023}
}

@inproceedings{wei2016network,
  title={Network morphism},
  author={Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  booktitle={International conference on machine learning},
  pages={564--572},
  year={2016},
  organization={PMLR}
}

@inproceedings{wen2020autogrow,
  title={Autogrow: Automatic layer growing in deep convolutional networks},
  author={Wen, Wei and Yan, Feng and Chen, Yiran and Li, Hai},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={833--841},
  year={2020}
}

@article{wu2024llama,
  title={Llama pro: Progressive llama with block expansion},
  author={Wu, Chengyue and Gan, Yukang and Ge, Yixiao and Lu, Zeyu and Wang, Jiahao and Feng, Ye and Luo, Ping and Shan, Ying},
  journal={arXiv preprint arXiv:2401.02415},
  year={2024}
}

@article{yang2020progressively,
  title={Progressively stacking 2.0: A multi-stage layerwise training method for bert training speedup},
  author={Yang, Cheng and Wang, Shengnan and Yang, Chao and Li, Yuechuan and He, Ru and Zhang, Jingqiao},
  journal={arXiv preprint arXiv:2011.13635},
  year={2020}
}

@article{yao2023masked,
  title={Masked Structural Growth for 2x Faster Language Model Pre-training},
  author={Yao, Yiqun and Zhang, Zheng and Li, Jing and Wang, Yequan},
  journal={arXiv preprint arXiv:2305.02869},
  year={2023}
}

