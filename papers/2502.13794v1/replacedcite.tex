\section{Related Works}
\subsection{Model Scaling-up}
Model scaling-up can be broadly categorized into width and depth scaling-up. Width scaling-up increases the matrix size while ensuring that the output of a layer or consecutive layers remains consistent with the output of the original network before expansion. Net2Net____ is one of the first to transfer parameters from a smaller model to initialize a larger one using function-preserving transformations. bert2BERT____ extends this approach to Transformer-based models. LiGO____ learns a linear mapping to initialize larger models. HyperCloning____ expands LLM to fit a larger model with more hidden dimensions. However, while these methods increase matrix size, they are less compatible with parallel training frameworks, which are better suited for depth scaling-up. Moreover, depth scaling-up better preserves the model's knowledge.


Current depth scaling-up methods expand the model by duplicating and adding layers based on heuristic rules, which can be broadly categorized into "Interpolation" and "Stack"____, as shown in Figure~\ref{fig:related}. Interpolation involves adding a copy of each layer after the original, while Stack treats consecutive layers as a group and duplicates them together. Recent popular methods like LLaMA Pro____ and SOLAR____ can be seen as special cases of these two types. LLaMA Pro copies only a selected few layers, while SOLAR duplicates the first 24 and the last 24 layers of a previous 32-layer model and combines them. However, these methods are based on heuristic rules, which hinder layer specialization, leading to suboptimal performance and limiting the model's potential.

\subsection{Progressive Training}
Progressive training involves gradually transitioning from simpler, smaller models to more complex, larger ones____. It is often combined with model scaling-up, where the model size is progressively increased during training. Prior to the era of LLMs, many methods____ are developed to train smaller models, such as BERT____. In recent years, LLaMA Pro____ and Apollo____ have applied progressive learning and model scaling-up strategies to train LLMs. YODA____ introduces a novel teacher-student progressive learning framework that enhances model fine-tuning by emulating the teacher-student educational process. ____ offer a comprehensive evaluation and empirical guidelines for progressive learning and model scaling-up.