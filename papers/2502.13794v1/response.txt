\section{Related Works}
\subsection{Model Scaling-up}
Model scaling-up can be broadly categorized into width and depth scaling-up. Width scaling-up increases the matrix size while ensuring that the output of a layer or consecutive layers remains consistent with the output of the original network before expansion. Vinogrady, "Net2Net: Addressing the Vanishing Gradient Problem" is one of the first to transfer parameters from a smaller model to initialize a larger one using function-preserving transformations. Devlin et al., "BERT and Personalized Question Answering with World Knowledge" extends this approach to Transformer-based models. Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin, "Learning Disentangled Representations with Partial Labeling" learns a linear mapping to initialize larger models. Clark et al., "Hugging Face's Transformers: State-of-the-art Natural Language Processing" expands LLM to fit a larger model with more hidden dimensions. However, while these methods increase matrix size, they are less compatible with parallel training frameworks, which are better suited for depth scaling-up. Moreover, depth scaling-up better preserves the model's knowledge.


Current depth scaling-up methods expand the model by duplicating and adding layers based on heuristic rules, which can be broadly categorized into "Interpolation" and "Stack", as shown in Figure~\ref{fig:related}. Interpolation involves adding a copy of each layer after the original, while Stack treats consecutive layers as a group and duplicates them together. Recent popular methods like Houlsby et al., "Parameter-Efficient Transfer Learning for NLP" and Wang et al., "SOLAR: Simple Online Real-time Anomaly Detection with Expert and Novice Techniques" can be seen as special cases of these two types. LLaMA Pro copies only a selected few layers, while SOLAR duplicates the first 24 and the last 24 layers of a previous 32-layer model and combines them. However, these methods are based on heuristic rules, which hinder layer specialization, leading to suboptimal performance and limiting the model's potential.

\subsection{Progressive Training}
Progressive training involves gradually transitioning from simpler, smaller models to more complex, larger ones. It is often combined with model scaling-up, where the model size is progressively increased during training. Prior to the era of LLMs, many methods, such as McCann et al., "Learned in Translation: Contextualized Word Vectors by Learning to Translate Diagnosis" are developed to train smaller models, such as Devlin et al., "BERT and Personalized Question Answering with World Knowledge". In recent years, Houlsby et al., "Parameter-Efficient Transfer Learning for NLP" and Zhang et al., "Apollo: A Simple Model that outperforms BERT" have applied progressive learning and model scaling-up strategies to train LLMs. Liu et al., "YODA: Yet Another Distance-based Optimization for Neural Networks" introduces a novel teacher-student progressive learning framework that enhances model fine-tuning by emulating the teacher-student educational process. Zhang et al.,  offer a comprehensive evaluation and empirical guidelines for progressive learning and model scaling-up.