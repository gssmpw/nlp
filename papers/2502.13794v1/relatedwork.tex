\section{Related Works}
\subsection{Model Scaling-up}
Model scaling-up can be broadly categorized into width and depth scaling-up. Width scaling-up increases the matrix size while ensuring that the output of a layer or consecutive layers remains consistent with the output of the original network before expansion. Net2Net~\cite{chen2015net2net} is one of the first to transfer parameters from a smaller model to initialize a larger one using function-preserving transformations. bert2BERT~\cite{chen2021bert2bert} extends this approach to Transformer-based models. LiGO~\cite{wang2023learning} learns a linear mapping to initialize larger models. HyperCloning~\cite{samragh2024scaling} expands LLM to fit a larger model with more hidden dimensions. However, while these methods increase matrix size, they are less compatible with parallel training frameworks, which are better suited for depth scaling-up. Moreover, depth scaling-up better preserves the model's knowledge.


Current depth scaling-up methods expand the model by duplicating and adding layers based on heuristic rules, which can be broadly categorized into "Interpolation" and "Stack"~\cite{pan2024preparing}, as shown in Figure~\ref{fig:related}. Interpolation involves adding a copy of each layer after the original, while Stack treats consecutive layers as a group and duplicates them together. Recent popular methods like LLaMA Pro~\cite{wu2024llama} and SOLAR~\cite{kim2023solar} can be seen as special cases of these two types. LLaMA Pro copies only a selected few layers, while SOLAR duplicates the first 24 and the last 24 layers of a previous 32-layer model and combines them. However, these methods are based on heuristic rules, which hinder layer specialization, leading to suboptimal performance and limiting the model's potential.

\subsection{Progressive Training}
Progressive training involves gradually transitioning from simpler, smaller models to more complex, larger ones~\cite{chang2017multi, wen2020autogrow, dong2020towards, wei2016network,fayek2020progressive}. It is often combined with model scaling-up, where the model size is progressively increased during training. Prior to the era of LLMs, many methods~\cite{chen2021bert2bert, gu2020transformer, wang2023learning, yang2020progressively, yao2023masked} are developed to train smaller models, such as BERT~\cite{devlin2018bert}. In recent years, LLaMA Pro~\cite{wu2024llama} and Apollo~\cite{pan2024preparing} have applied progressive learning and model scaling-up strategies to train LLMs. YODA~\cite{lu2024yoda} introduces a novel teacher-student progressive learning framework that enhances model fine-tuning by emulating the teacher-student educational process. \citeauthor{Du2024Stacking} offer a comprehensive evaluation and empirical guidelines for progressive learning and model scaling-up.