% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}
\pgfplotsset{compat=1.18}
\usepackage{array}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\ap}[1]{\textcolor{orange}{\small{\bf [ #1 --Adithya ]}}}
\newcommand{\tm}[1]{\textcolor{blue}{\small{\bf [ #1 --Teruko ]}}}

\title{Scaling Multi-Document Event Summarization: Evaluating Compression vs. Full-Text Approaches}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Adithya Pratapa \qquad Teruko Mitamura \\
    Language Technologies Institute \\
    Carnegie Mellon University \\
    \texttt{\{vpratapa, teruko\}@cs.cmu.edu} \\
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Automatically summarizing large text collections is a valuable tool for document research, with applications in journalism, academic research, legal work, and many other fields. In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS): compression and full-text. Compression-based methods use a multi-stage pipeline and often lead to lossy summaries. Full-text methods promise a lossless summary by relying on recent advances in long-context reasoning. To understand their utility on large-scale MDS, we evaluated them on three datasets, each containing approximately one hundred documents per summary. Our experiments cover a diverse set of long-context transformers (Llama-3.1, Command-R, Jamba-1.5-Mini) and compression methods (retrieval-augmented, hierarchical, incremental). Overall, we find that full-text and retrieval methods perform the best in most settings. With further analysis into the salient information retention patterns, we show that compression-based methods show strong promise at intermediate stages, even outperforming full-context. However, they suffer information loss due to their multi-stage pipeline and lack of global context. Our results highlight the need to develop hybrid approaches that combine compression and full-text approaches for optimal performance on large-scale multi-document summarization.\footnote{Our code and data are available at \url{https://github.com/adithya7/scaling-mds}.}
\end{abstract}

\section{Introduction}

Summarizing events described in document collections has long interested the NLP community with shared tasks for event tracking \cite{allan-etal-1998-tdt} and summarization \cite{chieu-etal-2004-query, dang-owczarzak-etal-2009-overview, aslam-etal-2015-trec}. Given an input collection of hundreds of text documents, systems have to extract and summarize salient information about the event. The length and diversity of the input presents a challenge to recent large language models (LLMs). In this work, we contrast two classes of systems for large-scale multi-document summarization (MDS), compression-based, and full-text systems.\footnote{We use the term \emph{scale} to refer to the large number of documents associated with each summary.}

Full-text systems promise a lossless approach by providing the summarizer access to the entire input. They are based on the long-context reasoning abilities of LMs, having already shown strong retrieval performance on long inputs \cite{hsieh-etal-2024-ruler}. However, their capabilities on large-scale MDS are not as well understood. In a recent work, \citet{laban-etal-2024-summhay} introduced a synthetic MDS benchmark that resembles the Needle in a Haystack evaluation \cite{kamradt-etal-2023-needle}. In addition to this dataset, we evaluate on two large-scale event summarization datasets: Background \cite{pratapa-etal-2023-background} and WCEP \cite{gholipour-ghalandari-etal-2020-large}. We contrast the end-to-end full-context method\footnote{We use full-text and full-context interchangeably.} with three compression-based methods: retrieval, hierarchical, and incremental. Each method \emph{compresses} the input in a multistage pipeline (\S\ref{ssec:methods}). We evaluated the content selection aspects of the summary using the Atomic Content Unit (A3CU) metric \cite{liu-etal-2023-towards-interpretable}.

Our experiments show that full-context and retrieval perform best in most settings (\S\ref{sec:results}). To better understand the performance of compression-based methods, we measure A3CU recall to track the salient information retention in their intermediate outputs (\S\ref{ssec:full_context_vs_compression}). Across all settings, we find that compression-based methods show high recall in intermediate stages but suffer information loss in their multistage pipeline. In particular, the intermediate recall is often much higher than the full-context system recall. We highlight two key takeaways: First, while iterative methods (hierarchical \& incremental) were previously found effective for book summarization and small-scale MDS, they underperform on large-scale MDS. Second, full-context systems are suboptimal on large-scale MDS datasets. We advocate for hybrid methods that combine input compression and long-context models. Such hybrid approaches are also scalable to even larger MDS tasks that go far beyond the context window limits of current LLMs.

\section{Experimental Setup}

\subsection{Datasets}
\label{ssec:datasets}


Our three datasets provide different flavors of the multi-document summarization task (\autoref{tab:datasets}).

\textbf{SummHay:} A query-focused dataset that covers the news and conversation domains \cite{laban-etal-2024-summhay}. Synthetically generated using GPT-3.5 and GPT-4o, each summary constitutes a set of insights. To keep our evaluation setup consistent across datasets, we concatenate these insights into a free-form summary. Following the original work, we include an oracle setting that only retains documents containing the reference insights.

\textbf{Background:} This dataset provides summaries of complex news events \cite{pratapa-etal-2023-background}. The task is based on an event timeline. For a given day, the goal is to generate a background summary by summarizing past new articles related to the event. We expand the original dataset to use news articles instead of just news updates. The dataset includes three human-written background summaries.

\textbf{WCEP:} A newswire dataset collected from Wikipedia Current Events Portal \cite{gholipour-ghalandari-etal-2020-large}. The summaries come from the portal and the documents include a combination of cited source articles and a retrieved collection of related articles from the Common Crawl archive.

Our choice of datasets collectively represents the real-world use-cases of multi-document summarization systems. Previous work has shown the effectiveness of full-context methods in retrieval tasks. To this end, we include the query-focused SummHay dataset. On the other hand, Background and WCEP provide different variants of the task. Background task requires accumulation of salient content units over the entire input. WCEP has high information redundancy, with many articles providing support for the salient units.

\input{tables/datasets}

\subsection{Methods}
\label{ssec:methods}

\input{tables/scores_all}

We now describe our long-context methods and transformers. The key difference between our methods is the length of the input passed to the summarization system (transformer) at any stage.

\textbf{Full-context:} The transformer has access to the full input and relies on its long context reasoning abilities to generate the summary.

\textbf{Iterative:} Multi-stage summarization where we iteratively pass chunks of the input to the transformer. We explore two methods, hierarchical and incremental. The hierarchical method summarizes each document and iteratively merges these to compile the final summary. The incremental method processes documents in order while maintaining a running summary of the input. Previous work explored these methods for book summarization \cite{chang-etal-2024-booookscore} and small-scale multi-document summarization \cite{ravaut-etal-2024-context}.

\textbf{Retrieval:} We rank the input documents according to their relevance to the query.\footnote{If a query is unavailable, we default to using `Generate a summary of the document' as the query.} We then select the top-ranked documents (up to 32k tokens) and pass their concatenation to the transformer. We use SFR Embedding-2 \cite{meng-etal-2024-sfr} for the retrieval task and order-preserving RAG following the recommendation from \citet{yu-etal-2024-defense-rag}. We set 32k as the limit because all of our transformers are effective at this context length \cite{hsieh-etal-2024-ruler}.

\subsection{Transformers}
\label{ssec:transformers}

For our summarization systems, we experiment with three transformer-based models, Llama-3.1, Command-R, and Jamba-1.5. Each model supports a context window of at least 128k tokens. They rely on a different long-context methodologies, and represent the broad class of open-weight LLMs. All the three models show competitive performance on the RULER benchmark for long-context LMs \cite{hsieh-etal-2024-ruler}.

\textbf{Llama-3.1:} Pretrained on 15T+ tokens, it supports long context by using a large base frequency of 500,000 and non-uniform scaling of RoPE dimensions \cite{llama31-modelcard}. We use both 8B and 70B variants to test the effect of model scaling.

\textbf{Command-R:} A transformer-based model that uses NTK-aware interpolation with a very large RoPE base frequency of 4M \cite{cohere-for-ai-2024-commandr}. We use the 32B variant.

\textbf{Jamba-1.5:} A hybrid architecture with interleaved Transformer and Mamba layers \cite{jambateam-2024-jamba15}. It involves both mid-training on long texts and post-training on (synthetic) long-context tasks. We use the 52B Jamba-1.5-Mini mixture-of-experts model with 12B active parameters.

For a fair comparison of above methods and transformers, we set the maximum input length to 128k across all settings. If the input is longer than 128k tokens, we first truncate the longest documents. In the case of Background, we also ensure equal representation from the past events by budgeting the token limit to each past timestamp. We also set a minimum document length (128 tokens) and drop documents if this cannot be achieved. To ensure that all methods see the same input, we adopt the same truncation strategy across full-text and compression-based methods. Theoretically, compression-based methods could work with even longer input (>128k), but we limit all settings to 128k tokens for a fair comparison.

See \S\ref{app:experimental_setup} in the Appendix for additional details about our experimental setup including our summarization prompt (\autoref{tab:summarization_prompt}). We sample summaries with a temperature of 0.5. We note that the summaries could be slightly different across different seeds. \citet{vig-etal-2022-exploring} compared end-to-end and RAG for query-focused summarization, but limited to the short input setting.

\section{Results}
\label{sec:results}

\input{figures/inf_loss}

\subsection{Metrics}
\label{ssec:metrics}

We focus our analysis on the \emph{content selection} aspect of summarization. \citet{nenkova-passonneau-2004-evaluating} first studied the content selection evaluation using the pyramid method on summarization of content units. Follow-up efforts have automated various parts of this method \cite{shapira-etal-2019-crowdsourcing, liu-etal-2023-towards-interpretable}. In this work, we use the reference-based Atomic Content Unit (A3CU) metric \cite{liu-etal-2023-towards-interpretable} that is based on the definition of atomic content units of \citet{liu-etal-2023-revisiting}. This metric is trained to predict a score that measures the overlap of atomic content units between the reference and predicted summaries.

Recent works also studied faithfulness \cite{kim-etal-2024-fables}, coherence \cite{chang-etal-2024-booookscore}, and position bias \cite{huang-etal-2024-embrace,ravaut-etal-2024-context,laban-etal-2024-summhay}. Although these evaluations are important, content selection remains a core issue for large-scale MDS.

\subsection{Overall Results}
\label{ssec:overall_a3cu}

\autoref{tab:overall_a3cu} reports the A3CU F1 scores for compression-based methods relative to the full-context baseline.\footnote{We report ROUGE and A3CU precision, recall in \S\ref{app:full_metrics}.} Full-context and retrieval perform the best, being particularly effective on the query-focused SummHay dataset. The two iterative methods perform poorly in most settings. We also find that the performance of transformers and methods varies considerably across the datasets and even within examples in each dataset.\footnote{See \autoref{fig:box_a3cu} in the Appendix for example-level trends.} Below, we break down these results and analyze the effect of transformer and compression methods.

Due to the high costs of running API-based models on long texts, we mostly limit our evaluation to open-weight LLMs. We report preliminary results using Gemini-1.5 on SummHay in \autoref{tab:summhay_gemini} in the Appendix. We noticed trends similar to those of open-weight LLMs.

\subsection{Analysis: Full-context \& Transformer}

In the full-context setting, we see mixed results across transformers, with none performing the best across all datasets. Interestingly, Llama-3.1-8B outperforms 70B on SummHay. This surprising result aligns with their relative performance on the RULER benchmark at 128k context length. The 70B model fares better in the oracle setting and shows similar performance on non-retrieval-style datasets. We believe that the 70B model needs additional post-training to improve its long-context retrieval performance.

Command-R underperforms the much smaller Llama-3.1-8B. This could be attributed to its use of RoPE \cite{su-etal-2021-roformer}. Command-R increases the base frequency while Llama-3.1 additionally scales RoPE dimensions non-uniformly, likely leading to better long-context capabilities \cite{ding-etal-2024-longrope}. However, without specific details on the mid- and post-training with long texts, it would be difficult to identify the exact cause. We direct the reader to \citet{peng-etal-2023-yarn} and \citet{lu-etal-2024-controlled} for a discussion on long-context methods.

\subsection{Analysis: Full-context vs. Compression}
\label{ssec:full_context_vs_compression}

With the exception of retrieval on query-focused SummHay dataset, compression-based methods generally underperform full-context (\autoref{tab:overall_a3cu}). To analyze this, we use A3CU \emph{recall} to track the retention of salient information in intermediate outputs. These intermediate outputs correspond to the retrieved documents (retrieval) and intermediate summaries (hierarchical, incremental). \autoref{fig:inf_loss} reports the recall scores for the final summary and the best intermediate output (excl. final). For comparison, we also report the recall score for the full-context summary. Across datasets, the best intermediate recall is significantly higher than the final summary recall, even outperforming full-context.\footnote{Since recall is impacted by the summary length, we report average length of summaries for each system in \autoref{tab:summary_len_stats} in the Appendix. We do not find any noticeable correlation.}

We highlight two key observations. First, iterative methods suffer catastrophic information loss in their multistage pipeline. Second, the best intermediate recall scores from compression methods show areas of improvement for full-context systems. As a control setting, we evaluated on SummHay-oracle and found full-context to be comparable to the best intermediate recall from compression methods (\autoref{fig:inf_loss_summhayoracle} in the Appendix).

\paragraph{Retrieval:} Relative performance of full-context and retrieval varies widely across examples and transformers. \citet{karpinska-etal-2024-nocha} observed similar behavior for claim verification on books. In particular, for Llama-3.1-8B on SummHay, we find the final summary to be better than the best intermediate output (\autoref{fig:inf_loss}). This is the optimal scenario, illustrating the system's effectiveness in aggregating information from the retrieved documents. We do not see this behavior in other settings.

\paragraph{Iterative:} We qualitatively analyze the outputs from iterative methods. The hierarchical method tends to generate increasingly abstract summaries at higher levels. It often skips details such as entities and numerals in the summaries. We observe this behavior across all transformers. With the incremental method, we attribute poor performance to the large number of intermediate steps (\# documents). Even though the system retrieves salient information at an intermediate stage, the model often gets distracted by non-salient information seen in documents thereafter. We provide examples in \autoref{tab:qual_hierarchical} and \autoref{tab:qual_incremental} in the Appendix.

In the Appendix (\S\ref{ssec:ablations}), we also experiment with short-context transformers such as Llama-3 (\autoref{tab:summhay_llama3}), varying chunk sizes for the hierarchical method, an alternative embedding method for retrieval (\autoref{tab:summhay_e5}), and grounded generation templates for Jamba and Command-R.

\subsection{Human Evaluation}

To complement our automatic evaluation, we perform a reference-based human evaluation. We randomly sample 62 examples from the SummHay dataset ($\approx$67\%) and ask a human expert\footnote{This task was done by the first author.} to rate the system summaries. We follow recommendations from prior work \cite{kiritchenko-mohammad-2017-best, goyal-etal-2022-news, pratapa-etal-2023-background} to use the best-worst rating scale. For each example, the human evaluator picks the best and worst summaries (multiple allowed) among the four methods, full context, hierarchical, incremental, and retrieval (Llama-3.1-8B). They use reference summaries to perform content selection evaluation. We shuffle the presentation order of the system summaries in each example, and system labels are completely hidden from the human evaluator. The results of our human evaluation are presented in \autoref{tab:human_eval}. Retrieval-based summaries are rated the best, followed by full-context, incremental, and hierarchical. These results strongly correlate with our automatic evaluation (\autoref{tab:overall_a3cu}).

\input{tables/human_eval}

\subsection{Recommendations for Future Work}
\label{ssec:recommendations}

Based on our analysis, we make two recommendations for future work on large-scale MDS. First, hybrid systems that combine input compression methods with long-context LLMs. Second, a reference-free content selection evaluation that facilitates further scaling of MDS.

\paragraph{Hybrid Methods:} Our analysis using A3CU recall shows the scope for improvement of full-context systems (\autoref{fig:inf_loss}). Recent studies have shown that long-context models are not as effective as claimed for retrieval tasks \cite{hsieh-etal-2024-ruler,karpinska-etal-2024-nocha}, and our results support this for large-scale MDS. Iterative methods were previously used for book summarization \cite{chang-etal-2024-booookscore} and small-scale MDS \cite{ravaut-etal-2024-context}. In large-scale MDS, they show a significant loss of salient information. Based on these observations, we advocate for a hybrid approach that utilizes selective input compression methods \cite{sarthi-etal-2024-raptor,xu-etal-2024-recomp,jiang-etal-2024-longllmlingua} in conjunction with a long-context LLM. A hybrid approach could provide optimal performance while improving the runtime over full-context. It also allows for scaling to a very large-scale MDS that goes far beyond the model context window.

\paragraph{Reference-free evaluation:} In our analysis, we used a reference-based A3CU metric. As we scale the MDS task to include hundreds or thousands of documents, obtaining high-quality human-written reference summaries will be infeasible. Therefore, reference-free content selection evaluation metrics are needed. Synthetic tasks such as SummHay present a promising alternative.

\section{Conclusion}

In this work, we contrast the full-context method against three compression-based methods for large-scale MDS. We evaluated on three datasets, SummHay, Background, and WCEP using the A3CU content selection evaluation metric. We find that the full-context and retrieval-based methods perform the best. Iterative methods suffer from significant information loss. Our analysis shows that full-context methods provide suboptimal performance, and we recommend future work to explore hybrid methods that combine the strengths of input compression methods with advances in long-context LLMs.

\section*{Limitations}

In this work, we rely on high-quality reference summaries to measure the content selection aspects of system-generated summaries. We acknowledge that human evaluation is the gold standard for text summarization. However, for large-scale multi-document summarization ($\approx$100 docs per example), it is prohibitively expensive to perform human evaluation. \citet{karpinska-etal-2024-nocha} reported that a human takes about 8-10 hours to read an average book (of similar length to our setting). We leave the extension of human evaluation of full-context and compression-based systems to future work. We also limit our evaluation to models with publicly available weights. We report preliminary results on SummHay using Gemini-1.5 (\autoref{tab:summhay_gemini} in Appendix). Due to the high API costs of running Gemini on long inputs, we couldn't run them for other datasets. We did not conduct an extensive search for optimal prompts for the summarization task. So, it is possible that the performance of some system configurations could be improved with additional prompt tuning.

\section*{Ethics Statement}

Hallucination is an important concern for text summarization systems and has been widely studied in the literature. We focus on the content selection aspects of text summarization and choose our evaluation metrics accordingly. However, we recognize the importance of faithfulness evaluation in providing a holistic evaluation of summarization systems. We leave this extension to future work.

\section*{Acknowledgments}

We thank the ARR reviewers for their valuable feedback in improving our paper. Adithya Pratapa was supported by a LTI Ph.D. fellowship.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Appendix}
\label{sec:appendix}

We use GitHub copilot and Claude-3.5 Sonnet for assistance with coding and editing.

\subsection{Datasets}
\label{app:datasets}

For background summarization, we use the news articles from the original timeline summarization datasets, Timeline17 \cite{tran-etal-2013-predicting}, Crisis \cite{tran-etal-2015-timeline} and Social Timeline \cite{wang-etal-2015-socially}. To constrain the input length, we use a maximum of five news articles from any given day. We also experimented with prefiltering the articles using the news update of the given day, but this did not show improvements in summary quality.

\subsection{Experimental Setup}
\label{app:experimental_setup}

\textbf{Transformers:} We use weights from Huggingface for Llama-3.1-8B,\footnote{\url{https://hf.co/meta-llama/Llama-3.1-8B-Instruct}} Llama-3.1-70B,\footnote{\url{https://hf.co/meta-llama/Llama-3.1-70B-Instruct}} Command-R,\footnote{\url{https://hf.co/CohereForAI/c4ai-command-r-08-2024}} and Jamba-1.5-Mini.\footnote{\url{https://hf.co/ai21labs/AI21-Jamba-1.5-Mini}}

\textbf{Compute:} We run inference using vLLM on four 48G GPUs \cite{kwon-etal-2023-efficient}. Given its large size, we load Llama-3.1-70B with fp8 precision. For the smaller Llama-3.1-8B, we use a single 48G GPU. Our setup includes a mix of Nvidia's A6000, L40, and 6000 Ada GPUs.

\textbf{Iterative methods:} For both iterative methods, we set the maximum chunk size to 4096 tokens. For the hierarchical method, we first generate summaries for each input document. Then, we pack consecutive document summaries into the maximum chunk size for the next summarization step. We stop the process when we only have one summary. For the incremental method, we start by generating the summary of the first document. Then, we concatenate this summary with the following document for the next summarization step. We iterate through every document in the input, in the order provided by the dataset. The document order is relevant for Background (event timelines), but might not be as relevant for SummHay and WCEP.

\textbf{Retrieval:} We limit each document to 1024 tokens and the post-retrieval input to 32k tokens.

\textbf{Summary length:} To set the maximum summary words for each dataset, we first tokenize the summaries in the validation split using NLTK. We use the 80th percentile as the maximum summary words for the systems. To account for the differences in tokenizers for Llama-3.1, Command-R, and Jamba-1.5, we set the maximum number of summary \emph{tokens} by multiplying the maximum summary words with model-specific word-to-token ratios. The word-to-token ratios for Llama-3.1, Command-R, and Jamba-1.5-Mini are 1.145, 1.167, and 1.219 respectively. For iterative methods, we use the same maximum summary token limit at each intermediate step. In \autoref{tab:summary_len_stats}, we report the average length of system-generated summaries.

\textbf{Prompt:} \autoref{tab:summarization_prompt} provides our prompt for the text summarization task. We use the same prompt for all transformers and methods. We follow the recommendations from model providers and use the model-specific chat templates from Huggingface tokenizers when prompting the instruction-fine-tuned models.

\input{tables/prompts}
\input{figures/inf_loss_appendix}

\subsection{Full Metrics}
\label{app:full_metrics}

We report the precision, recall, and F1 scores for A3CU and ROUGE scores \cite{lin-2004-rouge} for each dataset: SummHay (\autoref{tab:summhay}), SummHay oracle (\autoref{tab:summhay_oracle}), Background (\autoref{tab:background}), and WCEP (\autoref{tab:wcep}). We use Huggingface evaluate for ROUGE and the original repo for A3CU.\footnote{\url{https://github.com/Yale-LILY/AutoACU}}


\subsection{Example-level Trends}

\autoref{fig:box_a3cu} shows the distribution of A3CU F1 scores across examples. We notice a significant variance in system performance across all datasets.

\input{figures/box_a3cu}

\subsection{Ablations}
\label{ssec:ablations}

We perform ablation studies to further study our choice of models and hyperparameters. Given its small size, we used SummHay for our ablation experiments.

\textbf{Gemini-1.5:} We run some preliminary experiments with Gemini-1.5 Flash and Pro (\autoref{tab:summhay_gemini}). Across methods, we consistently found that Gemini-1.5 models generate short summaries and underperform open source models. It is possible that we could improve their summaries using a different prompt, but we leave this extension to future work. Due to the high costs associated with Gemini API, we did not run experiments with our larger Background and WCEP datasets.

\textbf{Llama-3:} Our iterative methods do not require a long-context transformer, so we experiment with short-context transformers to see if they are better suited for this task. We run inference with Llama-3 8B and 70B (8k context window) in the SummHay and SummHay oracle settings (\autoref{tab:summhay_llama3}). We found that both models are either comparable or underperform their Llama-3.1 counterparts. It is likely that the Llama-3.1 models are better at short-text summarization.

\textbf{Chunk size:} As we have highlighted earlier, the hierarchical method exhibits a significant degradation in summary recall. We experiment with larger chunk sizes that allow for packing more intermediate summaries into the transformer. Our results using 8k, 16k and 32k chunk sizes show minimal improvements over our default 4k chunk size.

\textbf{Retriever:} Following the setup of SummHay \cite{laban-etal-2024-summhay}, we experiment with the E5-RoPE embedding for retrieval.\footnote{\url{https://huggingface.co/dwzhu/e5rope-base}} We report results in \autoref{tab:summhay_e5}. E5-RoPE performs slightly worse than the SFR-Embedding-2 results from \autoref{tab:summhay}.

\textbf{Grounded generation:} Jamba provides a grounded generation option in which the documents are passed as a separate object in the chat template. We experiment with this chat template to see if it provides any gains over our default setting of concatenating documents in the message. We report results in \autoref{tab:summhay_jamba_grounded}. Interestingly, this template helps improve the performance of hierarchical and incremental methods and hurts performance in full-context and retrieval settings. This needs further investigation. Command-R also includes a grounded generation template, but it is recommended for documents (or chunks) that contain 100-400 words. We couldn't make it work with full documents from our datasets.

\textbf{Filtered Background:} Our results showed that Background is the most challenging of the three datasets. To simplify the task, we pre-filter the documents using the update summary from the event timeline. We use the E5RoPE model \cite{zhu-etal-2024-longembed} to prefilter up to 128k tokens in the input for each example. However, we did not observe any significant improvements with this filtered dataset.

\input{tables/scores_summhay}
\input{tables/scores_summhayoracle}
\input{tables/scores_background}
\input{tables/scores_wcep}
\input{tables/summary_len_stats}

\input{tables/ablation_gemini}
\input{tables/ablation_llama3}
\input{tables/ablation_chunk_size}
\input{tables/ablation_retriever}
\input{tables/ablation_jamba}

\input{tables/qual_hierarchical}
\input{tables/qual_incremental}


\end{document}
