\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/kad_fad.png}
    \caption{Comparison between KAD (Kernel Audio Distance) and FAD (Fréchet Audio Distance). KAD is a distribution-free metric that does not require any underlying assumptions for embedding distributions $P$ and $Q$.}
    \label{fig:kad&fad}
\end{figure}


\subsection{Fréchet Audio Distance and its Limitations}
% Introduction of FAD
The Fréchet Audio Distance (FAD)~\cite{fad} measures the difference between two sets of audio samples within their data embedding space. Specifically, it is an estimation of the Fréchet distance between the underlying distributions of two given embedding sample sets. FAD is an adaptation of the Fréchet Inception Distance (FID)\cite{fid} -- originally proposed for evaluating image generation models -- to the audio domain. The embeddings are typically extracted using an audio encoder model pretrained on real-world data such as VGGish\cite{vggish}, ensuring that the embeddings capture representative features of the audio samples for reliable evaluation. 

Given the ground-truth reference set embeddings $X = \{x_i\}_{i=1}^n$ and the target evaluation set $Y= ~ \{y_j\}_{j=1}^m$, FAD is defined by:
\begin{equation}
    \text{FAD}^2(X,Y) = \|\mu_X - \mu_Y\|_2^2 \;+\; \text{tr}\left(\Sigma_X + \Sigma_Y - 2\sqrt{\Sigma_X \Sigma_Y}\right),
\label{eq:FAD}
\end{equation}
where $X$ and $Y$ are assumed to be sampled from multivariate Gaussian distributions, fully characterized by their means $\mu_X, \mu_Y$ and covariances $\Sigma_X, \Sigma_Y$.

FAD is a conventional choice of metric for evaluating generative models in various domains, including text-to-audio~\cite{donahue2018adversarial,liu2023audioldm,huang2023makeanaudio,tango,audiogen,t-foley,audioldm2,consistencytta,tango2,tango_llm,auffusion,stableaudio_open,ezaudio} and vision-to-audio~\cite{comunita2024syncfusion,video-foley,rewas,frieren,maskvat,v-aura,vatt,multifoley,ssv2a,vintage,mmaudio,clipsonic,v2a-mapper,im2wav,foleygen,tiva} tasks, and is considered one of the standards for generative performance. Despite its popularity, FAD has three crucial limitations that undermine its efficacy and efficiency:
\begin{enumerate}
% FAD's cons(1) Incorrectness of the Gaussian Normality Assumption
\item\textbf{Normality assumption}: The assumption that audio embeddings follow a Gaussian distribution often fails for real-world data. Such data are often asymmetrically distributed or unevenly clustered, which compromises the metric's ability to accurately measure similarity. Similar limitations have been observed in the computer vision domain, where the normality assumption was shown to be unsuitable for the Inception~\cite{inception-v3} embedding space used for FID~\cite{jayasumana2024rethinking}. Figure \ref{fig:clotho_umap} shows how the actual distribution of VGGish embeddings from the Clotho dataset (reduced via UMAP) differs significantly from the samples drawn from a Gaussian distribution with the same mean and covariance. This deviation highlights the limitations of the normality assumption for representing real-world audio data, leading to potential inaccuracies and over- or under-estimations in FAD values.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/clotho_umap.png}
    \caption{The left panel is the complex, non-Gaussian shape of the VGGish embeddings from the Clotho\cite{drossos2020clotho} train dataset, while the right panel depicts its assumed Gaussian distribution.}
    \label{fig:clotho_umap}
\end{figure}


% FAD's cons(2) Sample size bias
\item\textbf{Sample size bias}: FAD is an inherently biased metric~\cite{gui2024adapting} which requires a large number of audio samples for a reliable result. Given the embedding sample size $N =\max(n,m)<\infty$, 
The bias in FAD from the finite-sample estimation of sample covariance increases as $\mathcal{O}(1/N)$ (for a detailed derivation, refer to Appendix~\ref{sec:appendix_fad_bias}). Similar findings have also been reported for FID~\cite{chong2020effectively}.
% In practice, when the sample size is small, FAD's reliability decreases due to sample size bias, necessitating 
This necessitates the use of larger datasets for more accurate evaluations, which is particularly undesirable in the audio domain where high-quality data is relatively scarce compared to image datasets. Furthermore, this sample size bias creates the potential for manipulation: increasing $N$ can artificially reduce bias, leading to better FAD scores and the appearance of improved performance. Naturally, this further undermines the credibility of FAD as a reliable evaluation metric.

% FAD's cons(3) Computational cost Perceptual Misalignment
\item\textbf{High computational cost}: 
The time complexity of FAD is given by $\mathcal{O}(dN^2+d^3)$, which scales poorly with the number of dimensions of the embedding space. This makes the calculations cumbersome when using audio embedding models that produce high-dimensional embeddings. Moreover, the calculation of square-roots of covariance matrices in Equation \ref{eq:FAD} is not easily parallelized, limiting the utilization of parallel computing.


\end{enumerate}


\subsection{Maximum Mean Discrepancy} 
\label{sec:mmd}

To address the limitations of FAD, we adopt the Maximum Mean Discrepancy (MMD)~\cite{grettonmmd}. Originally proposed for a statistical test to distinguish whether two samples come from the same distribution, MMD is capable of capturing differences not only in mean and variance but also in higher-order moments. It is also distribution-free, meaning that it does not assume that the samples belong to a specific family of distributions (e.g. Gaussian). This allows for a more comprehensive comparison of how two sets of audio samples differ in their embedding spaces.

The MMD between two distributions $P$ and $Q$ is defined as: 
\begin{equation}
    \text{MMD}(\mathcal{F},P,Q) = \sup_{f \in \mathcal{F}} 
    \Bigl( \mathbb{E}_{x \sim P}[f(x)] \;-\; \mathbb{E}_{y \sim Q}[f(y)] \Bigr),
    \label{eq:mmd_def}
\end{equation}
where $\mathcal{F}$ is a class of functions chosen to detect differences between $P$ and $Q$. 

When $\mathcal{F}$ is chosen to be the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel function $k(\cdot,\cdot)$, calculating the MMD corresponds to measuring the Euclidean distance between the mean embedding positions after mapping the data into a high-dimensional feature space. The high-dimensional mapping is necessary because it reveals the nonlinear differences between two distributions that may not be apparent in a lower-dimensional setting. 

Rather than computing these high-dimensional representations explicitly, kernel operations can be used to calculate the distances in the RKHS directly in the original embedding space. This technique, often referred to as the "kernel trick," allows the metric to leverage high-dimensional -- or even infinite-dimensional -- representations that would otherwise be infeasible to compute. With this setup, the MMD can be computed entirely through pairwise comparisons of the samples: 
\begin{equation}
    \text{MMD}^2(P, Q) = \mathbf{E}_{x,x'}[k(x,x')] + \mathbf{E}_{y,y'}[k(y,y')] - 2\,\mathbf{E}_{x,y}[k(x,y)],
    \label{eq:mmd_kernel}
\end{equation}
where $x, x'$ are drawn from $P$ and $y, y'$ are drawn from $Q$. 

For the finite samples in the reference set $X = \{x_i\}_{i=1}^n$ and the evaluation set $Y = \{y_j\}_{j=1}^m$, an unbiased estimator of \ref{eq:mmd_kernel} is: 
\begin{align}
\label{eq:MMD_unbiased}
    \widehat{\text{MMD}}^2_{\text{unbiased}}(X,Y) &= \frac{1}{n(n-1)}\sum_{i\neq j}k(x_i,x_j) \;+\; \frac{1}{m(m-1)}\sum_{i\neq j}k(y_i,y_j) \nonumber\\
    &\quad - \frac{2}{nm}\sum_{i=1}^n\sum_{j=1}^m k(x_i,y_j).
\end{align}

One of the most widely used MMD-based metrics for evaluating generative models is the Kernel Inception Distance (KID) proposed by Bińkowski et al.~\cite{binkowski2018demystifyingmmd}, as the squared MMD value between Inception embeddings of images with a cubic polynomial kernel. KID was used as an evaluation metric in several audio-related works~\cite{nistal2021comparing, nistal2024diff, grachten2024measuring, shi2024versa}, particularly in music generation.
For image quality, Jayasumana et al.~\cite{jayasumana2024rethinking} proposed the CLIP Maximum Mean Discrepancy (CMMD) for evaluation on CLIP embeddings~\cite{clip} and a Gaussian kernel, with Novack et al.~\cite{novack2024presto} following similar practices but on CLAP~\cite{clap_laion} embeddings.
Many of these works acknowledge the potential advantages of MMD. However, to our knowledge, there has been no comprehensive study of its reliability in comparison to FAD or the effect of various design choices.

% Several previous studies have explored MMD-based metrics for evaluating generative models such as the Kernel Inception Distance (KID) proposed by Binkowski et al.~\cite{binkowski2018demystifyingmmd}. KID is computed as the MMD between samples embedded by the Inception model~\cite{inception-v3}, yielding an unbiased measure with rapidly decaying deviation as sample size increases -- addressing some of the well-known limitations of FID. 
% However, Inception-based scores including FID have been shown to diverge considerably from human evaluations~\cite{imagenet-fid}. 
% In response, Jayasumana et al.~\cite{jayasumana2024rethinking} introduced the CLIP Maximum Mean Discrepancy (CMMD), which leverages the CLIP~\cite{clip} image encoder to generate embeddings that are more human-aligned when assessing generative models.
% While Novack et al. ~\cite{novack2024presto} also employed MMD alongside the FAD to assess the quality of generated music, the work lacks a systematic study on metric reliability and the impact of various design choices.

% When $\mathcal{F}$ is chosen to be a \emph{Reproducing Kernel Hilbert Space} (RKHS), the distributions $P$ and $Q$ can each be represented as a single point, or \textit{mean embedding}, in that high-dimensional space. The MMD then corresponds to the distance between these two points, reflecting how much one distribution differs from the other. As is often the case, the RKHS can be high-dimensional or infinite-dimensional, to which explicit mapping is infeasible. Instead, the kernel function $k(\cdot,\cdot)$ is used to measure the similarity between pairs of embeddings in the original embedding space.