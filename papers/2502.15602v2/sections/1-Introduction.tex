\begin{table}[h]
\centering
\resizebox{0.65\textwidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
    & Distribution-free?                                                 & Bias-free?  & Computation Cost\\ \midrule
FAD \cite{fad} & \textcolor{red}{\ding{56}} & \textcolor{red}{\ding{56}} & $\mathcal{O}(dN^2+d^3)$      \\
KAD (Ours) & \textcolor{ForestGreen}{\ding{52}} & \textcolor{ForestGreen}{\ding{52}} & $\mathcal{O}(dN^2)$      \\ \bottomrule
\end{tabular}%
}
\vspace{0.2cm}
\caption{Comparison of FAD and KAD: KAD is distribution-free, unbiased, and computationally efficient, even for high embedding dimensions ($d \leq 2048$) and large sample sizes ($N \leq 10k$).}
\label{tab:fad_kad_comparison}
\end{table}

As the demand for neural audio generation continues to grow across various domains such as content creation and virtual environments, innovative models are emerging to address a wide range of tasks. These include generating audio from textual descriptions, visual inputs, temporal data, or other audio signals, underscoring the importance of models that can process diverse types of inputs. Consequently, the need for robust and reliable methods to evaluate the quality of these models is becoming increasingly critical.

The Fr√©chet Audio Distance (FAD)~\cite{fad} is a widely used metric for evaluating the overall performance of audio generation models, measuring the dissimilarity between the statistical distributions of real and generated audio samples.
FAD is considered a simple yet effective measure for objective evaluation, making it a popular choice for assessing generative audio models across various tasks.

However, FAD has significant shortcomings that limit its effectiveness as a benchmark for generative audio models. 
First, it relies on the assumption that audio embeddings follow a Gaussian distribution, which often does not apply to real-world audio data with complex and diverse characteristics. Second, FAD suffers from an inherent bias in finite-sample estimation, which produces unreliable results particularly with smaller datasets. Finally, the computational cost of FAD is substantial, due to its high dependence on the embedding dimension size. 
These issues collectively challenge the practical use of FAD in evaluating modern generative audio models~\cite{jayasumana2024rethinking,chong2020effectively,tailleur2024correlation,gui2024adapting}.

Motivated by these limitations of FAD, we propose the Kernel Audio Distance (KAD), a novel evaluation metric for audio generation. The proposed metric leverages the Maximum Mean Discrepancy (MMD), a non-parametric measure that makes no assumptions about the underlying distribution of sample embeddings such as the normality assumption in FAD. A comparison between KAD and FAD is illustrated in Figure \ref{fig:kad&fad}.
In summary, our contributions are:
\begin{itemize}
    \item We propose KAD, a novel metric based on MMD for evaluating generative audio models.
    \item We provide empirical evidence demonstrating faster convergence with sample size, computational advantages, and stronger rank correlation with human evaluations of KAD.
    \item We provide guidelines to establish the practical applicability of KAD for selecting kernels, parameters, and embedding models to ensure consistent and reliable audio quality evaluation. 
\end{itemize}                      
The implementation of KAD is provided as an open-source toolkit named \path{kadtk} (for more detail, refer to Appendix \ref{sec:kadtk_release}).


