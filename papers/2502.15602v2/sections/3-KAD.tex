% \subsection{Proposed Metric}\label{subsec:proposed}

In this section, we propose the \emph{Kernel Audio Distance} (KAD), a reliable and computationally efficient metric for evaluating audio generation models. 

The fundamental requirement for such a metric is its ability to capture perceptually meaningful differences between generated and reference audio. Provided that the embedding space sufficiently encodes these perceptually relevant features, a reliable metric must be capable of accurately comparing embedding distributions without imposing restrictive assumptions. To this end, we adopt the MMD, whose distribution-free nature eliminates the need for parametric assumptions and enables a comprehensive comparison between two embedding distributions.

We define KAD as follows:
\begin{equation}
    \text{KAD} = \alpha \cdot \widehat{\text{MMD}}^2_{\text{unbiased}},
\end{equation}
where $\alpha$ is a resolution scaling factor introduced for convenient score comparison. We set $\alpha = 100$ as the default.

\subsection{The Strengths of KAD}
Along with its robust theoretical foundation, KAD also provides key practical advantages over FAD:

\textbf{Unbiased Nature}: The KAD score is independent of the sample size, making it robust to smaller samples without employing bias-correction procedures. By contrast, the bias-correction for FAD (e.g., $\text{FAD}_\infty$~\cite{gui2024adapting}) relies on linear fitting of results at multiple sample sizes. This independence makes KAD especially robust in data-scarce conditions, such as early-stage evaluations of generative models or when high-quality reference datasets are limited.

\textbf{Overall Computational Efficiency}: KAD has a time complexity of $\mathcal{O}(dN^2)$. In practice, this can be significantly faster than $\mathcal{O}(dN^2 + d^3)$ for FAD, as the $d^3$ term can dominate with higher dimensionality. Therefore, KAD is more scalable for higher-dimensional embeddings typical of modern deep audio models.

\textbf{Parallel Computation} The pairwise operations in the computation of KAD (Eq.~\ref{eq:MMD_unbiased}), $- \frac{2}{nm}\sum_{i=1}^n\sum_{j=1}^m k(x_i,y_j)$, can be performed in parallel, enabling substantial acceleration. 

% Additionally, using KAD eliminates the need for the surplus computations required for finite-sample bias correction in $\text{FAD}_\infty$, because its unbiased equation can be expressed in closed form.

\subsection{Kernel Function and Bandwidth Selection}\label{subsec:kad_kernel}

KAD relies on evaluating pairwise relationships between embeddings through a kernel function. Many commonly used kernels such as Gaussian, Laplacian, and Mat√©rn kernels are examples of a \textit{characteristic} kernel, meaning that the MMD it induces \textit{i)} fully distinguishes between two embedding distributions and \textit{ii)} is zero if and only if the two distributions under test are identical~\cite{sriperumbudur2011universality}. This property is crucial for model evaluation as it ensures that the KAD metric captures meaningful differences in the embedding distributions of real and generated audio samples. As an example, a cubic polynomial kernel $(x^Ty + 1)^2$ cannot differentiate between distributions with the same mean, variance and skewness, but different kurtosis~\cite{sriperumbudur2010hilbert}.

For the KAD, we choose the Gaussian radial basis function (RBF) kernel:
\begin{equation}
    k(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{\|\mathbf{x}-\mathbf{y}\|^2}{2\sigma^2}\right),
\end{equation}
where $\sigma$ is the bandwidth parameter. An implicit mapping $\phi(x)$ to the RKHS of the Gaussian RBF kernel is infinite-dimensional and is defined by the property $\langle \varphi(\mathbf{x}), \varphi(\mathbf{y}) \rangle = k(\mathbf{x}, \mathbf{y})$. This kernel has been extensively analyzed and validated in MMD applications~\cite{jayasumana2024rethinking, rustamov2019closed} for its smoothness, balanced sensitivity to both local and global variations, and well-studied performance across diverse datasets and modalities.

For the value of the bandwidth parameter $\sigma$, we follow the commonly adopted median distance heuristic~\cite{grettonmmd}, setting $\sigma$ as the median pairwise distance between the embeddings within the reference set. This heuristic provides a stable baseline with minimal tuning, ensuring the kernel is neither too flat (insensitive to dissimilarities) nor too peaked (over-sensitive to noise). While exploring adaptive or data-driven kernel selection is beyond the scope of this work, our initial experiments indicate that the median heuristic is sufficiently effective. More details are discussed in Appendix \ref{sec:appendix_bandwidth}.
