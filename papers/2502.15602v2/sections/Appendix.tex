% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}


\section{Median Pair-wise Distance Heuristic for the Kernel Bandwidth}
\label{sec:appendix_bandwidth}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{images/bandwidth_score_max_norm.png}
    \caption{Effect of audio degradations to the MMD values, where the maximum value is normalized to 1 for each bandwidth result. MMD values that are smaller than $\varepsilon=10^{-12}$ are clipped to $\varepsilon$ before normalization.}
    \label{fig:bandwidth_score_norm}
\end{figure}

To assess the effectiveness of the median pairwise distance heuristic for setting the bandwidth parameter $\sigma$, we compared the MMD values calculated at different bandwidth settings as the quality of the evaluation data is artificially degraded. If the heuristic to works as intended, the MMD at the median distance bandwidth should be well-correlated with the audio quality for the perceptually relevant ranges of degradation.

We compared the MMD scores between the clean audio files from the Clotho dataset's \textit{eval} split, and the same files under various audio signal transformations including Gaussian noise injection, bit depth reduction, low-pass filtering, dynamic range reduction (limiter), and pitch shifts. The degradations were applied using the \textit{audiomentations} python library~\cite{audiomentations}.

For each transformation, MMD scores were calculated using the median pairwise distance as the bandwidth, as well as bandwidths scaled by factors from $0.001 \times$ up to $1000 \times$ relative to the median. The scores were normalized such that the maximum MMD score for each bandwidth result is 1, allowing us to observe trends in the scores independently of their absolute values.

The results are shown in Figure \ref{fig:bandwidth_score_norm}. A reliable metric is expected to show a monotonic increase in score as the severity of the degradation increases, because higher KAD scores should correlate with worse audio quality. When the median pairwise distance is used as the bandwidth, the MMD scores consistently respect the expected monotonic trend across all degradations tested. This suggests that the median heuristic provides stable and meaningful results. Bandwidths scaled by $10 \times$ and $100 \times$ the median also produced reliable results, maintaining the monotonic relationship. However, smaller bandwidths ($0.001 \times$ to $0.1 \times$ resulted in scores that drop too quickly, diminishing the discriminative power of the metric. Larger bandwidths ($1000 \times$ the median) tend to flatten the scores, reducing sensitivity to dissimilarities.

These initial experiments suggest that the median bandwidth heuristic is a robust and effective choice for calculating MMD scores. While certain scaled bandwidths may also be viable, the median heuristic avoids nonsensical results, ensuring stable and interpretable evaluations without additional tuning. Therefore, we recommend its use for kernel bandwidth selection in this context.


\section{Bias of FAD}
\label{sec:appendix_fad_bias}

In order to analyze the bias of FAD, we revisit Equation \ref{eq:FAD}:

\begin{center}
    $\text{FAD}^2(X,Y) = \|\mu_X - \mu_Y\|_2^2 \;+\; \text{tr}\left(\Sigma_X + \Sigma_Y - 2\sqrt{\Sigma_X \Sigma_Y}\right).$
\end{center}

For finite samples, $\mu_X, \mu_Y$ and $\Sigma_X, \Sigma_Y$ are replaced by their sample estimates $\hat{\mu}_X, \hat{\mu}_Y$ and $\hat{\Sigma}_X, \hat{\Sigma}_Y$. This introduces bias due to finite sample effects.

The first term, $\|\mu_X - \mu_Y\|^2$, is unbiased since the sample means $\hat{\mu}_X$ and $\hat{\mu}_Y$ are unbiased estimators of the true means $\mu_X$ and $\mu_Y$. Thus,
\begin{center}
    $\mathbb{E}[\|\hat{\mu}_X - \hat{\mu}_Y\|^2] = \|\mu_X - \mu_Y\|^2.$
\end{center}

The second term, $\text{tr}\left(\Sigma_X + \Sigma_Y - 2\sqrt{\Sigma_X \Sigma_Y}\right)$, is affected by finite sample sizes. The sample covariance matrix $\hat{\Sigma}$ is a biased estimator of the true covariance $\Sigma$:
\begin{center}
    $\mathbb{E}[\hat{\Sigma}] = \frac{N-1}{N} \cdot \Sigma$,
\end{center}

where $N$ is the sample size. Consequently, the expected value of the trace of the covariance matrices is:
\begin{center}
    $\mathbb{E}[\text{tr}(\hat{\Sigma}_X + \hat{\Sigma}_Y)] = \frac{N-1}{N} \cdot \text{tr}(\Sigma_X + \Sigma_Y).$
\end{center}

In the second term, $2 (\Sigma_X \Sigma_Y)^{1/2}$ is nonlinear in the covariances, and its exact bias is difficult to compute. However, to first-order approximation, the bias of FAD due to finite samples can be expressed as:
\begin{center}
    $\text{Bias}_{\text{FAD}} \approx \frac{1}{N} \cdot (\text{tr}(\Sigma_X) + \text{tr}(\Sigma_Y)) + \mathcal{O}\left(\frac{1}{N^2}\right).$
\end{center}

Here, the primary contribution to the bias arises from the underestimation of the covariance matrices, which scales inversely with the sample size \( N \).


% \section{Parameter-Independent Cumulative KAD}
% \label{sec:appendix_heuristic_free}

% We present an alternative application of KAD that is independent of the kernel bandwidth parameter. Instead of choosing a kernel bandwidth parameter via a rule-of-thumb heuristic such as the median or half-median heuristic, one can cumulate KAD values over a continuous range of bandwidths through numerical integration. The result is a scalar evaluation score that does not depend on an arbitrary bandwidth choice. While this method is more computationally intensive due to the calculations required for numerical integration, it allows for a bandwidth-agnostic evaluation when the trade-off is viable.

% Heuristics in selecting kernel bandwidth ($\sigma$) for MMD-based metrics, e.g. the median distance between samples, is widely accepted, but ultimately introduces a parameter that can affect the final score. By integrating KAD over $\sigma \in (0,\infty)$, it is possible to obtain a single number that does not rely on the choice of bandwidth, captures both local and global discrepancies by leveraging the entire \textit{spectrum} of kernel scales and avoids the issue of potential inversions of rank between candidates under evaluation between specific bandwidths. 

% The approach inherently normalizes across scales and ensures that the resulting evaluation metric is free from bandwidth-related parameters. Although it requires multiple KAD evaluations, it remains computationally feasible and is still faster than the conventional Fréchet Audio Distance (FAD) computations in typical scenarios.

% % Let $P$ and $Q$ be two probability distributions on $\mathbb{R}^d$. The squared MMD with a Gaussian kernel $k_\sigma(x,y)=\exp(-\|x-y\|^2/(2\sigma^2))$ is defined as:
% % \[
% % \text{MMD}^2_\sigma(P,Q) = E_{X,X'\sim P}[k_\sigma(X,X')] + E_{Y,Y'\sim Q}[k_\sigma(Y,Y')] - 2E_{X\sim P,Y\sim Q}[k_\sigma(X,Y)].
% % \]

% The proposed parameter-free evaluation is given by the following integral, which we show is finite under mild and practically reasonable conditions on $P$ and $Q$: 
% \[
% I = \int_0^\infty \text{MMD}^2_\sigma(P,Q)\, d\sigma.
% \]

% \textit{proof of convergence of the integral:}

% We assume, that
% \begin{enumerate}
%     \item $P$ and $Q$ have finite second moments, i.e. $E_{X\sim P}[\|X\|^2]<\infty$ and $E_{Y\sim Q}[\|Y\|^2]<\infty$. This is a reasonable assumption since in practice $P$ and $Q$ are represented by finite samples drawn from audio embeddings that do not grow unbounded.
%     \item $P$ and $Q$ are Borel probability measures on $\mathbb{R}^d$ without atomic masses of measure 1 at identical points (i.e., they are continuous or at least continuous in regions of interest). This ensures that $\text{MMD}^2_\sigma(P,Q)$ goes to zero as $\sigma \to 0$.
% \end{enumerate}

% We also note, that
% \begin{itemize}
%     \item As $\sigma \to \infty$, $k_\sigma(x,y) \to 1$ for all $x,y$. In that limit, $E_{X,X'\sim P}[k_\sigma(X,X')]$, $E_{Y,Y'\sim Q}[k_\sigma(Y,Y')]$, and $E_{X,Y}[k_\sigma(X,Y)]$ all approach 1, making $\text{MMD}^2_\sigma(P,Q)\to 0$.
%     \item As $\sigma \to 0$, $k_\sigma(x,y)$ becomes sharply peaked around $x=y$. For continuous distributions, the probability that $X=X'$ is zero, thus $E_{X,X'}[k_\sigma(X,X')] \to 0$ as $\sigma \to 0$, and similarly for $Q$ and for cross terms $E_{X,Y}[k_\sigma(X,Y)]$. Hence, $\text{MMD}^2_\sigma(P,Q)\to 0$ as $\sigma \to 0$.
%     \item For any $\sigma>0$, $0 \leq k_\sigma(x,y)\leq 1$, ensuring $0 \leq \text{MMD}^2_\sigma(P,Q)\leq 2$.
% \end{itemize}

% \textit{Uniform Convergence and Differentiability:}

% We rewrite the Equation \ref{eq:mmd_kernel} as
% \[
% \text{MMD}^2_\sigma(P,Q) 
% = \int k_\sigma(x,x')\,dP(x)dP(x') 
% + \int k_\sigma(y,y')\,dQ(y)dQ(y')
% - 2\int k_\sigma(x,y)\,dP(x)dQ(y).
% \]

% Each integral is dominated by the integrable function $1$, and since $k_\sigma(x,y)$ is continuous in $\sigma$, by the Dominated Convergence Theorem, we can interchange limits and integrals. This shows that $\text{MMD}^2_\sigma(P,Q)$ is continuous and its defining integrals converge uniformly in $\sigma$ over compact intervals.

% The kernel is differentiable w.r.t. $\sigma$, and differentiation under the integral sign is justified by the boundedness and smoothness of the Gaussian kernel. Thus, $\text{MMD}^2_\sigma(P,Q)$ is differentiable in $\sigma$ with a well-defined and bounded derivative.

% \textit{Bounded Behavior at the Extremes:}
% \begin{itemize}
%     \item As $\sigma \to \infty$: $\text{MMD}^2_\sigma(P,Q)$ approaches 0 smoothly.
%     \item As $\sigma \to 0$: $\text{MMD}^2_\sigma(P,Q)$ also approaches 0, due to continuity of $P,Q$ and the diminishing volume where $x=y$ occurs.
% \end{itemize}

% Since $\text{MMD}^2_\sigma(P,Q)$ is bounded between 0 and 2, and it converges to 0 at both ends of the integration range $(0,\infty)$, there are no singularities to prevent $I$ from converging. The integrand’s behavior ensures decay at both tails: at infinity and near zero.

% So, under the stated conditions (\textit{finite second moments} of $P,Q$ and \textit{continuous distributions}), $I=\int_0^\infty \text{MMD}^2_\sigma(P,Q)d\sigma$ is finite. While a fully rigorous measure-theoretic proof would require more detailed bounding, these conditions are perfectly realistic for practical settings, as evaluations are conducted using finite sets of audio samples.

% In practice, this means that for large but finite set of embeddings from audio samples, the integral-based approach results in a single score that is truly independent of arbitrary parameters, as well as being distribution-free, and also characterizes the model's generative performance across all scales of local/global differences. However, one must approximate the integral numerically by evaluating $\text{MMD}^2_\sigma(P,Q)$ at multiple $\sigma$ values. We have found around 50 to 100 samples in the $\sigma$ dimension suffice to achieve sufficient convergence. Although this is more expensive than fixing a single bandwidth, it can still be more efficient than computing FAD while also offering a robust and unbiased evaluation metric.


\section{\texttt{kadtk}: KAD Toolkit Release}
\label{sec:kadtk_release}
We release a toolkit named \path{kadtk} that can calculate KAD scores from input audios. Given input audio directories for the ground-truth reference set and target evaluation set, the toolkit calculates the score and saves it to the given output file path in CSV format. The toolkit is written in Python and supports Pytorch and Tensorflow environments (refer to our Readme document for further details). It supports numerous models for embedding extraction: CLAP~\cite{clap_ms,clap_laion}, Encodec~\cite{encodec}, MERT~\cite{mert}, VGGish~\cite{vggish}, PANNs~\cite{panns}, OpenL3~\cite{openl3}, PaSST~\cite{passt}, DAC~\cite{dac}, CDPAM~\cite{cdpam}, Wav2vec2.0~\cite{wav2vec2.0}, HuBERT~\cite{hubert}, WavLM~\cite{wavlm}, and Whisper~\cite{whisper}. This covers a wide range of the audio domain including general environment sounds (sound effects, foley sounds, etc.), music, and speech. We also support FAD calculation for comparison.

The \path{kadtk} is released under the MIT License, allowing unrestricted use, modification, and distribution with proper attribution. The full license text is included in the repository. Some codes were brought from the FAD toolkit (fadtk) \cite{gui2024adapting,dcase2024,dcase2024report}. We sincerely thank the authors for sharing the code as an open source. Note that fadtk was also licensed under the MIT License. 

For the computational cost comparison in Section \ref{ssec:exp_compute}, we used \texttt{torch.linalg.eigval} and \texttt{torch.sqrt} to compute the covariance matrix of FAD, ensuring a fair comparison with KAD in a fully parallelized GPU setting. However, this approach is prone to accuracy issues, often leading to discrepancies with \texttt{fadtk}, which relies on \texttt{scipy.linalg.sqrtm} (available only for CPU computation). To ensure high FAD score accuracy at the cost of increased runtime, we used \texttt{scipy.linalg.sqrtm} for the perceptual alignment experiment (Section \ref{ssec:human_perception}) and in the released toolkit.

% \section{Comparing Open-Sourced Text-to-Audio Generation Models}
% \jw{TODO}