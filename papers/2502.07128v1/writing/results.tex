\section{Results}

We manually collected $106$ commonly seen card games from the web as text descriptions, including 25 casino games (mostly poker), 27 trick-taking games, 11 rummy games, 10 solitaire-like games, and 33 others. These games are used to initalize the database for all three tasks (game design, code generation, gameplay AI).

\subsection{Novel Game Design}

In this task, we use all card games to construct game mechanics database and compare the newly-generated games to them.

Our method can extract meaningful game mechanics from game descriptions. 
First, the extracted mechanic graphs can represent the semantic distances between the games. As shown in Figure \ref{fig:mechanic_graphs}, the mechanic graphs of two poker games (\textit{Bull Poker} and \textit{Holdem}) are closer to each other than a card game from a different genre (\textit{Go Boom}), which aligns to their text descriptions in the database. Also, our method reveals the dependency between game mechanics. In Figure \ref{fig:mechanic_dependencies}, the mechanics ``Highest-ranking poker hand", which frequently appears in poker games, is associated with relevant mechanics such as ``showdown" and ``poker hand rankings". 


\begin{figure*}[ht]
  \centering
    \begin{subfigure}[t]{.3\linewidth}
        \includegraphics[width=\linewidth]{figs/mechanics_tree.png}
        \caption{Games in mechanic graphs}
        \label{fig:mechanic_graphs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.3\linewidth}
        \includegraphics[width=\linewidth]{figs/mechanics_children_0.png}
        \caption{Mechanics dependencies}
        \label{fig:mechanic_dependencies}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.3\linewidth}
        \includegraphics[width=\linewidth]{figs/mechanics_new_2.png}
        \caption{Newly-designed mechanics}
        \label{fig:new_mechanics}
    \end{subfigure}
  \caption{\textbf{Extracted and newly-designed mechanics}. Mechanics are distributed in a 2D UMAP projection of a text embedding space, colored by its game ID in (a) or concept cluster ID in (b) and (c). (\subref{fig:mechanic_graphs}) Three games, `hold-em', `bull poker', and `go boom' are shown as mechanic graphs. (\subref{fig:mechanic_dependencies}) Starting from selected mechanic clusters at depth 0 (in bold text), their dependencies on supporting mechanics (depth 1, in normal text) are drawn in lines. (\subref{fig:new_mechanics}) Newly-designed mechanics in depth 2 are shown in bigger markers with connection lines to their concept cluster centrics. For each big cluster we randomly label one mechanic with its name and cluster ID in black text.}\label{fig:concept_cluster}
\end{figure*}



Based on the mechanic extractions, our method can produce new mechanics for a cluster of similar mechanics (see visualizations in Figure \ref{fig:new_mechanics}). Here is an example where all mechanics that involve wild cards are summarized by their shared patterns and the variation. Then new mechanics are designed as below.

\input{support-mat/concept_example}

We demonstrate that our method encourages novel game designs by measuring how its generated game mutations deviate from the original 106 games. We define Quantiled Max Similarity (QMS) as a metric. Given a base game $g$ and a mutation method $M$, $QMS(g, M, p)$ represent the $p$th percentile of the closest cosine similarity scores between the mutations of $g$ and every game in database $D$.

\begin{equation} 
    QMS(M, q, g) = \mathbf{Q}^p_{v \sim M(g)} \max_{d \in {D \setminus g}} \zeta(v) \cdot \zeta(d) 
    \label{eq: game_novel}
\end{equation}
where $\mathbf{Q}^p(\cdot)$ selects the quantile value at percentile $p$. $\zeta(\cdot)$ refers to the text embedding process. Due to limited computational resources, we choose $28$ games from $D$ as the evaluation set $G$. For each $g \in G$ we sample $33$ mutations.
In this way, we can compare mutation methods by examining their QMS distributions (through the interquartile range $p \in \{0.25,0.5,0.75\}$) across all games in $G$.

We compare our method against two alternatives: (1) Vanilla: the game mutation is created by a fixed system prompt. Multiple mutations are sampled by repeatedly querying the LLM. (2) PromptBreeder: Game mutations are generated using a set of system prompts created by the approach proposed in \cite{fernando_promptbreeder_2023}. For all methods, default temperature parameters are used in LLM callings.

Table \ref{tab:game-sim} shows our method successfully penalizes game mutations that are highly similar to database items. For mutations that are in top 25\% or top 50\% on the similarity to database, our method outperforms PromptBreeder by achieving a lower similarity score. PromptBreeder on the other hand, shows no significant difference to the Vanilla method.
For the rest of the mutations (p=0.25), our method shows similar performance as PromptBreeder.

We also argue that the penalization does not make the mutations to collapse to a limited number modes that leads to the loss of diversity. It is demonstrated by evaluating QMS within the generated mutations, where a good method shall produces mutations that differ from each other. To apply QMS within the mutations, we replace the $D \setminus g$ in Equation \ref{eq: game_novel} with $M(g) \setminus v$.

As shown in the bottom section of Table \ref{tab:game-sim}, our method prevents high similarities than other methods (p=0.75). Our method offers fewer low-similarity mutations (p=0.25) than PromptBreeder, since it offers more controllable process, where the mutations are strictly directed by game mechanics replacements.

\begin{table}
    \centering
    \sisetup{separate-uncertainty= true, table-format=2.1(1), multi-part-units=single}
    \begin{tabular}{lSSS}
        \hline
                            & \text{p=0.75} & \text{p=0.50}& \text{p=0.25} \\
        \hline
        \multicolumn{4}{l}{\textit{QMS to database}}\\
        Vanilla        & 88.7 \pm 5.8~$\dagger$& 87.2 \pm 6.3 & 83.8 \pm 8.2 \\
        PromptBreeder  & 88.3 \pm 4.9~$\dagger$& 85.8 \pm 4.5 & 81.1 \pm 3.9~$\dagger$ \\
        Ours& \color{blue}87.4 \pm 4.5& \color{blue}84.8 \pm 4.3 & \color{blue}80.6 \pm 3.7~$\dagger$\\
        \hline
        \multicolumn{4}{l}{\textit{QMS within}}\\
        Vanilla        & 95.4 \pm 0.8 & \SI{92.0(40)}{} & 86.7 \pm 5.9\\
        PromptBreeder  & 91.0 \pm 2.1 & 85.5 \pm 2.1~$\dagger$ & \color{blue}79.5 \pm 2.5\\
        Ours& \color{blue}89.6 \pm 2.4 & \color{blue}85.4 \pm 2.8$~\dagger$ & 81.3 \pm 2.8 \\
        \hline  
    \end{tabular}
    \caption{\textbf{Novelty performance between methods}. Means and standard deviations of Quantile Maximum Similarity ($\times 10^{-2}$) are reported in two scopes and three percentiles $p$. The first scope denotes the similarity between the mutations and database. The second denotes the similarity within the mutations. Pairs with $\dagger$ denote the data entries from the same column are NOT significantly different (p-value of paired t-test $\geq 0.05$)}
    \label{tab:game-sim}
\end{table}





\subsection{Consistent Code Generation}

We use \texttt{gpt-4o-2024-08-06} as LLM backbone and \texttt{text-embedding-3-large} from OpenAI as text embedding model. We test our framework using 29 commonly-seen card games and 28 games that mutate from them using our method in \ref{sec:game-logic} (detailed in \ref{sup:test-case}).

The following metrics are used in our evaluation, where the second and third metric originate from \cite{chatdev}:

\paragraph{Generation Success (Succ)} Given a fixed maximum number of iterations, the percentage of games that can be generated bug-freely in $10$ randomized execution tests.

\paragraph{Executability (Exec)} For games that pass the first criteria, the percentage of successful executions with $100$ randomized seeds.

\paragraph{Embedding Consistency (EmbCon)} Semantic embedding similarity between the code and the description of the game. Here we used structurized description detailed in \ref{sec:codedraft}.

\paragraph{LLM-judged Consistency (LLMCon)} We also use LLMs to score the consistency (scaled 1 to 10) between actual game play record and game description, which applies a similar implementation as \ref{sec:validate}. We use \texttt{claude-3-5-sonnet-20241022} here to avoid the blind spots of one single LLM source.
    

Our method is compared against its ablated version, where LLM-based validation is skipped after the repeated debugging. We add the second ablated version, where validation is skipped and the LLM backbone is replaced by OpenAI \texttt{o1-preview} in code initialization.

Table \ref{tab:codegen} demonstrates a robust code generation using our method, as the mean executability is over $90\%$. 
Also, our method improves the code alignment to the game description, as LLM-judged consistency metrics in both game groups (common and mutated) are higher than its ablated version (-validation). While a stronger model (-validation$\oplus$) results in higher generation success rate, for games that can be successfully created, our method lifts the consistency of a weaker model to a strong one, indicating its potential to substitute stronger backbone models when they are not available.

\begin{table*}[ht]
    \centering
    \sisetup{separate-uncertainty= true, table-format=1.2(2), multi-part-units=single}
    \begin{tabular}{lcS[table-format=2.1(1)]S[table-column-width=2cm]S[table-column-width=2.2cm]cS[table-format=2.1(1)]SS}
        \hline
                  &       \multicolumn{4}{c}{Common Games}    &\multicolumn{4}{c}{Mutated Games}      \\    
                  & \text{Succ$\uparrow$} & \text{Exec$\uparrow$} & \text{EmbCon$\uparrow$}& \text{LLMCon$\uparrow$}& \text{Succ$\uparrow$}& \text{Exec$\uparrow$}& \text{EmbCon$\uparrow$}& \text{LLMCon$\uparrow$}\\
        \hline
        \text{-validation} & \text{27/29} & 99.9\pm3.9 & \color{blue}0.56 \pm 0.03~$\dagger^2$ & 7.69 \pm 1.96~$*^{2,3}$ & 25/28  & 98.3\pm4.6 & 0.52 \pm 0.04~$*$ & 8.66 \pm 1.60\\
        -validation$\oplus$& \color{blue}29/29 & 96.3\pm9.4 & 0.56 \pm 0.04~$\dagger^{1,3}$ & 8.90 \pm 1.74~$\dagger*^{1}$ & \color{blue}28/28  & 99.3\pm2.0   & \color{blue}0.54 \pm 0.05~$*$ & 9.08 \pm 1.29~$\dagger$\\
        Ours    & \text{27/29} & \color{blue}100.0\pm0 & 0.55 \pm 0.04~$\dagger^2$ & \color{blue}8.99 \pm 1.56~$\dagger*^{1}$ & 25/28  & \color{blue}100.0\pm0 & 0.52 \pm 0.05  & \color{blue}9.20 \pm 1.08~$\dagger$\\
        \hline
    \end{tabular}
    \caption{\textbf{Code generation performance between methods}. Mean values of the metrics across all game instances are reported. The standard deviation is shown after $\pm$. 
    $\oplus$ represents \texttt{o1-preview} as the backbone LLM. $*^i$ represents the metric are significantly different (p-value $\leq 0.05$ in paired t-test) from $i$th entry, while $\dagger^i$ denotes the metric are unlikely to be different (p-value $\geq 0.7$) from $i$th entry. No superscripts are used when there is only one symbol pair.}
    \label{tab:codegen}
\end{table*}





\subsection{Gameplay AI Generation}

After a manual verification on all generated games (both common and mutated) in the previous task, some games are excluded from the test set for gameplay AI, as they are either purely luck-oriented games with no strategies involved, or they are not completely consistent with the corresponding game description. Ultimately, we selected $13$ common games and $6$ mutated games for gameplay AI evaluation.

We benchmarked our method against prior works (Chain-of-Thought \cite{CoT}, ReAct \cite{yao_react_2023}, Reflexion \cite{shinn_reflexion_2023}, and Agent-Pro noted as BeliefAgent \cite{zhang2024agentpro} detailed in \ref{sup:game-ai-eval}).
Although for our evaluation, we intended to use the most performant LLM model available for every method being compared, the number of LLM calls being made by other works is a polynomial order of magnitude greater than our work. For the number of games being compared, this token cost is prohibitively expensive (Fig~\ref{fig:token_cost}). We have therefore opted for the most performant LLM within budget for other works (4o-mini, which boasts similar performance) and 4o for our work. Our model is limited in that it requires at least 4o for its coding accuracy. However, despite the difference between LLM backbones, the 4o-mini being used for other works is comparable if not better than the LLM backbones that were originally used in the respective works.


The win rate performance of a method is assessed by its advantage over competing policies. Specifically, we measure how effectively policy \( p_2 \) outperforms policy \( p_1 \) under identical contexts. In games with \( n \) players, the first \( n-1 \) players act as defense, and the last player as the attacker. The win rate of an attacker using \( p_1 \) against defenders using \( p_2 \) is denoted as \( \omega(p_2, p_1) \). The advantage of \( p_1 \) over \( p_2 \) is defined as:

\begin{equation} 
    A(p_1, p_2) = \omega(p_2, p_1) - \omega(p_2, p_2)
\end{equation}




Table \ref{tab:gameai-adv} shows that our agent achieves the highest win rate advantage ($p_2$ = random agent), with significant differences over all the opponent agents in common games. Figure \ref{fig:adv_win} also shows this result, as our agent covers the largest area and got the highest win rate on 10 out of 13 common games. For mutated games, our agent not only achieves the highest average win rate but also is the only agent that can beat the random agents.


\begin{table}[ht]
    \centering
    \sisetup{separate-uncertainty= true, table-format=2.1(1), multi-part-units=single}
    \begin{tabular}{@{}lSSS@{}}
        \hline
               & \text{Common Games} & \text{Mutated} & \text{All Games} \\
        \hline
        CoT    & 1.4 \pm 15.8& -4.9 \pm 12.1  & -0.5 \pm 15.0\\
        ReAct  & 2.4 \pm 16.4 & -3.1 \pm 9.8 & 0.2 \pm 14.7 \\
        Reflexion & 1.4 \pm 18.4 & -3.6 \pm 9.7& 0.0 \pm 16.4\\
        Belief & 4.2 \pm 18.2 & -2.8 \pm 17.6 & 2.3 \pm 17.9\\
        Ours   & \color{blue} 14.6 \pm 19.3~$*$& \color{blue}  12.5 \pm 24.8 & \color{blue} 16.3 \pm 22.6~$*$\\
        \hline
    \end{tabular}
    \caption{\textbf{Game AI win rate advantage between methods}. Means and standard deviations (shown after $\pm$) of win rate advantages ($\times 10^{-2}$) over random agents across all game instances. $*$ represents our metrics are significantly different from those of prior work (p-value $\leq 0.05$ in paired t-test).}
    \label{tab:gameai-adv}
\end{table}



% \vspace{-20pt}

\begin{figure}[ht]
    
  \centering
    \includegraphics[width=.8\linewidth]{figs/win_rate_advantage_over_RandomAgent_with_RandomAgent_as_Defense.pdf}


  \caption{\textbf{Gameplay AI win rate advantage by games}. Win rate advantage to random agents when different methods are applied. The performance of random agents ($A(p,p_{random})=0$) is shown as black dotted circles. (left) Common games (right) Mutated games, where the names of base games are shown in radial axis. }\label{fig:win-adv-random}
  \label{fig:adv_win}

\end{figure}

In the cost analysis, we used OpenAI's API token pricing as a reference. We take the average per-game token cost across all 19 games as the metric here. As our agent only requires one-time usage of tokens during the generation phase, our total cost becomes lower than prior work in just 50 game runs (see Figure~\ref{fig:token_cost}), which suits better for large-scale evaluations in game prototyping. Refelxion and Belief agent have a significantly higher cost during training phase. 


\begin{figure}[ht]
    
  \centering
  \includegraphics[width=.6\linewidth]{figs/token_usage_accumulation.pdf}
  \caption{\textbf{Gameplay AI Token Cost}. Total LLM cost as a function of number of game runs,  averaged across 19 games. The cost after 50th run is estimated by linear extrapolations.
  }
  \label{fig:token_cost}

\end{figure}

