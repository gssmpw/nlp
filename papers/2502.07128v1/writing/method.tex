\section{Method}

Our work composites an LLM-based card game prototyping pipeline by three components: In \ref{sec:game-logic} we leverage game mechanic graphs to enable novel game mechanic design, producing new games in text descriptions. In \ref{sec:game-code} we use the text descriptions from the previous step to generate code for the game. In \ref{sec:gameai}, gameplay AI is created using the generated code, thus enabling game evaluations for human designers.

\subsection{Game Mechanic Design}
\label{sec:game-logic}


The design problem is defined as below: given a card game database, where games are represented in text descriptions, we aim to create game mutations that avoid high resemblance to the database. We tackle this by applying a mutation instruction to LLMs, where we propose game mechanic replacements explicitly. Specifically, our approach begins with extracting mechanics from existing card game descriptions. The mechanics are subsequently clustered, and new mechanics are generated within each cluster using LLMs. Finally, for a game to be mutated, mechanics that frequently appear in the database are suggested to be replaced by newly-designed ones within the same cluster. 


\subsubsection{Mechanics extraction}


To encourage a comprehensive and interconnected mechanics extraction from the text description, we propose a game mechanic graph representation. As illustrated in Figure \ref{fig:game_graph}, game mechanics represented in short text phrases are stored as nodes, where all games share the root node ``the game ends". The control flow between the nodes are represented as directed edges. The edge direction denotes the game effect contribution. For instance, in the game UNO, ``empty the hand" is a downstream child of the root node because it directly leads to game ending.


We adopt a backtracking approach
to extract the graphs. Starting with the shared root node, we use LLMs to expand the search frontier like a breadth-first search. Specifically, we input the game description and expansion history to LLM and instruct it to identify all mechanics that directly leads to the mechanics to expand. In the same way we recursively expand all expanded nodes, where expansion are terminated by both LLM's judgment and a manually set depth limit. It should be noted that while the mechanics are extracted in a tree search (thus the depths of nodes are recorded), we allow new nodes to be connected to any existing nodes so the extracted results may not be a tree. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=.6\linewidth]{figs/logic1.pdf}
  \caption{\textbf{Game mechanic graphs of two games}. Two games are represented in game mechanic graphs in different colors. They share the same root node (``the game ends") in dark gray. The mechanics are distributed in an embedding space of their text descriptions. The lower right corner showcase a concept cluster (transparent blue) where its members share similar semantic meaning. A new mechanics (with plus symbol) can be generated from the cluster by our method. }\label{fig:game_graph}
\end{figure}



\subsubsection{Mechanics Clustering and Designing}

We aggregate all mechanics from the game database and cluster them by semantic similarities and tree depth. Semantic similarity is measured using cosine similarity of text embeddings, with the \texttt{text-embedding-3-large} model serving as the embedding generator. A hierarchical clustering algorithm is employed, with a manually set similarity threshold of 
0.4.

For clusters containing more than 3 mechanics, we input LLMs with all mechanic descriptions to summarize their shared themes and generate new mechanics adhering to these themes. To enhance diversity in the results, we vary the system prompts using techniques outlined in \cite{fernando_promptbreeder_2023}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.6\linewidth]{figs/logic2.pdf}
  \caption{\textbf{Extraction, design, and application of game mechanics}. (Left) Indexing process: game graphs are extracted from game descriptions, where mechanics are clustered across games, with new mechanics generated within each cluster. (Right) Mutation process: after the input game is converted to a graph, proposals are created by replacing existing game mechanics with newly designed ones. }\label{fig:game_graph2}
\end{figure}

\subsubsection{Retrieval-augmented Game Design}

The newly generated mechanics can be utilized to create game mutations that differ significantly from existing database entries. Given a game description, we extract its game mechanic graph and map the mechanics to existing clusters. Mechanics from clusters with high occurrences in the database are identified as candidates for mutation. Our framework then proposes replacing these frequently occurring mechanics with newly generated ones from the same cluster. An LLM applies this proposal to the game description, using a self-reflection process to resolve potential ambiguities or conflicts in the game mechanics. This approach ensures that the mutated game remains coherent while introducing novel elements.

\subsection{Game Code Generation}
\label{sec:game-code}
\label{sec:codedraft}
\label{sec:debug}

Similar to \cite{agentless}, we adopt a workflow where LLM function calls are assembled into a predefined procedure. It receives game mechanics description in natural language as input, outputting game code for the given game. 

\subsubsection{Structurize, draft, and debug}

The framework starts by converting a text description into a structured markdown format by LLMs, detailing game states, deck initialization, legal actions, and gameplay process. Then, the structured description, together with retrieved examples and game engine templates (see Appendix \ref{sup:game-engine}) is fed to LLMs to draft game code. After a fixed number of LLM self-refinements, the draft moves to debugging.

In debugging phase, the game code is tested iteratively to eliminate runtime errors using randomized player actions to explore the state space. The game code is executed 10 times in each iteration. If errors occur in any trial, feedback from the game engine guides LLM's code fixes. This cycle continues until the code runs error-free.


\subsubsection{LLM-based validation}
\label{sec:validate}
Once the code produces no errors, 5 gameplay records are sampled from the previous phase, where the gameplays are produced by generated code. The record includes the information from the last 6 game rounds and the game ending. In each round, the information includes the observation and the action of the current player, which are converted from the game state dictionary to natural language by a fixed parser. To make the record more legible, game comments are also generated along the observation. Rather than generated directly from LLMs, game comments are generated from the commentary function inside the game code, where the function is designed by LLMs. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=.6\linewidth]{figs/code-validate.pdf}
  \caption{\textbf{Use gameplay records to validate code}. Gameplay records, generated by game code, are used to validate and refine the consistency of game code. }\label{fig:code-validate}
\end{figure}

For each record, we send it to LLM with the structured game description, asking if the game play record violates any rule in the game description. If so, the violated rule, which is quoted by the LLM, is used as a query to a code snippet database. This database contains exactly the same games as the previous one in \ref{sec:codedraft}, except the game description are interweaved into game code as comment lines. In this way, this database retrieves related code snippets rather than entire code scripts. The retrieved code snippets act as the correct implementation of the rules that resembles the violated ones. It is fed to LLM with the previous violation analysis to refine the existing game code. After the refinement, the game code is sent back to the debugging phase to test its executability again. This refinement loop continues until the maximum iteration is reached, or the game code passes all validations from the records.




\subsection{Gameplay AI Generation}
\label{sec:gameai}

\begin{figure}[ht]
    
  \centering
  \includegraphics[width=.6\linewidth]{figs/ai1.pdf}
  \caption{\textbf{Gameplay AI as a Q-function ensemble}. When making decision among the legal action space, our gameplay agent choose the action with maximum score (left). Given a proposed action under the current state, the score is derived by averaging an ensemble of Q-functions (middle), which target at different perspectives in a form of code (right). }\label{fig:gameai_concept}
\end{figure}

Treating LLMs as simply feature extractors, we use LLMs to propose a pool of policy components. Then we step-wisely select best components using self-play data, leaving the optimization to non-LLM processes  (shown in Figure \ref{fig:gameai_concept}). The policies are represented as Q-functions in code, that return a score given the current game state and proposed action. During the game, the final score of a proposed action is averaged over all policy component outputs:

\begin{equation} 
    a_T = \text{argmax}_{a \in A} \mathbf{E}_{i=1}^n Q^{\pi_i}(s_T, a) 
    \label{eq: agent}
\end{equation}
where $A$ represents the legal action space that is generated by game environment. $Q^{\pi_i}$ is the LLM-designed policy code component. $s_T$ is the current game state. 


\subsubsection{Construct Policy Pool}

To create a diversified policy pool, we firstly let LLMs to propose high-level strategies in text using different prompts. Then, we augment the policy pool by a mutual inspiration process, where strategies from different prompts are remixed into new ones by LLMs. Next, all high-level strategies in text are converted to python functions, which take game state and proposed action dictionaries as inputs, returning a score that represents the quality of the action.
Finally, we conduct a second augmentation by creating negative policies, where we multiply $-1$ to the output of all policy components. In this way we get a policy pool with original, mutually-inspired, and negative components, which are all fed to optimization process.



\subsubsection{Optimize Selection by Playing}


As shown in Equation \ref{eq: agent}, the policy is composited by a linear combination of components, where the components are selected by self-play data. Considering the noise of the reward signal \cite{light2024strategist}, we use a step-wise inclusion technique to build the selection set. Specifically, in the first iteration, we only include the component that achieves the highest win rate in the test environment. Then we start the second iteration where we try adding one another component to the current selection. In this way, we add one component at a time, until the win rate does not improve.


In this paper, the optimization process repeats twice: in the first trial, the policy is optimized by playing against random agents. In the later trial, the policy is optimized by playing against the mixture of random agents and the formerly-trained agents. We pick the results of the later trial as the final policy.
