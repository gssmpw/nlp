

\section{Code Generation Details}


\subsection{Game engine design}
\label{sup:game-engine}

Instead of generating entire game code from scratch \cite{chatdev}, we designed a card game engine in Python, converting the original generation task into completion tasks for several predefined functions.
To enable maximum compatibility for various card games and easy integration with gameplay AI, we adopt a framework structure where (1) game state updates and gameplay AI are decoupled \cite{zha_douzero_2021}; and (2) dependencies among game logics are minimized by a functional programming design \cite{wu2024instructiondriven}.
The logic of card games are abstracted into 6 functions (initiation, initialize deck, initial dealing, proceed round, get legal actions, and get payoffs) running in a predefined procedure (see Figure \ref{fig:game-engine}). Each function, which modifies the game state in dictionary form, is what our pipeline will generate.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.6\linewidth]{figs/game_engine.pdf}
  \caption{\textbf{Game Engine Framework Design}. The game engine, composed of 6 core functions (in light blue) in a fixed flow, interacts with external gameplay agent (in light green) through game observation and game action in dictionary forms.}\label{fig:game-engine}
\end{figure}


The gameplay AI is integrated into this system by receiving game observations and outputting game actions in dictionaries. While our gameplay AI directly handles dictionaries, for all LLM-based agents we compared against, the game state is converted to the following format. The example below shows the observation information in one game turn. LLM agents may receive concatenated information from the past several turns.
\input{support-mat/game_observation}

\subsection{Intermediate Results of LLM-based Validation}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figs/agentflow.png}
  \caption{LLM Agent workflow of game code generation.}\label{fig:agentflow}
  % \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure*}

In the code validation process, our method compares the generated gameplay record with the game description, identifying whether there exists an inconsistency. The gameplay record follows the same format as the game observation above. But multiple consecutive observations from different players are concatenated together. Here is an example output of the validation process.

\input{support-mat/validation-output}

Based on the validation results, code edits are proposed by LLM. Same as \cite{agentless}, partial replacement edits are returned to save LLM token usage.

\input{support-mat/code-edits}

\subsection{Evaluation on Code Generation}
\label{sup:test-case}
We test our pipeline using two sets of game descriptions: (1) we choose $29$ from the 106 games that we collected. (2) Using the novel game design method in \ref{sec:game-logic}, we mutate each game with $33$ variants and select the one that has the smallest closest cosine similarity to the game database. One of the games cannot be mutated by our method, since none of its extracted game mechanics belongs to a large-enough cluster, where new mechanics are designed. Therefore, we add $28$ mutated games as our second part of testing set.

For the code draft phase and debugging phase, we construct an example database containing $5$ game descriptions and their game code (``crazy-eights", ``california-jack", ``boat-house-rum", ``bull-poker", ``baccarat"). These games are selected from the $106$ games without overlaps to our test set.

It is important to note that LLM-based validation process improves the code alignment, rather than ensuring 100\% correct code. We conducted a manual verification to all generated games where LLM detects no misalignment, where we still find minor inconsistencies. For instance, there are simplified implementation of complex rules: for poker games with a large number of card evaluation standards (such as Straight Flush, Four of a kind, Full House, etc.), there are cases where our method does not implement all the standards mentioned in the game description.
Also, incorrect information visibilities may occur: sometimes, private information for specific players may be leaked to the public by game state outputs.
However, the validation process still minimizes human effort in code development, which benefits the overall prototyping process.


\section{Gameplay AI Details}




\subsection{Evaluation on Gameplay AI}
\label{sup:game-ai-eval}

Although some previous work demonstrates satisfactory performance in several popular games, they are excluded from our comparative analysis as their primary contributions focus on specific game genres \cite{wang2023avalon} or particular attributes of game mechanics \cite{light2024strategist}. For example, methods that learn directly from reward signals at MCTS leaf nodes \cite{light2024strategist} can hardly work on games with extremely unbalanced search trees. Because it is challenging to reach all terminal states with reasonable computational resources. As a result, one has to estimate the reward without reaching the terminal nodes, which introduces more uncertainties to the reward signals. 


Consequently, we focus on prior work that are scalable. We choose Chain-of-Thought~\cite{CoT}, ReAct~\cite{yao_react_2023}, Reflexion~\cite{shinn_reflexion_2023}, and AgentPro~\cite{zhang2024agentpro} as our baselines. As AgentPro is particularly noteworthy for its application of Belief-aware Decision-Making imperfect information game scenarios, we specifically implement the Belief-aware part for comparison.

The evaluation protocol involved training Reflexion agents across all games, with performance measured using a rolling average winning rate calculated over windows of 40 games. This measurement was repeated for 10 distinct windows across all games in our test set. The optimal reflection step was subsequently determined by selecting the iteration that yielded the highest mean winning rate across the entire game suite.




\subsection{Ablation Study}

We demonstrate the effect of the two major components in our gameplay AI method by comparing our method against two ablations: (1) No ensemble: rather than creating an ensemble of action-value functions, we generate only one comprehensive policy with its corresponding function. (2) No optimization: after the first augmentation stage, we skip the rest of the pipeline and use all policy components to create the ensemble. 

The result (see Table~\ref{tab:gameai-adv-ablation}) shows both components are essential for our method. The advantage over random agents cannot be achieved if either of the component is absent.

\begin{table}[ht]
    \centering
    \sisetup{separate-uncertainty= true, table-format=3.1(1), multi-part-units=single}
    \begin{tabular}{lSSS}
        \hline
               & \text{Common} & \text{Mutated} & \text{All Games} \\
        \hline
        - ensemble  & -3.2 \pm 7.0& -2.7 \pm 9.2~$\dagger$ &  -2.9 \pm 7.5~$\dagger$ \\
        - optimize  & 0.1 \pm 9.9 & -2.0 \pm 5.9~$\dagger$ & -0.4 \pm 8.7~$\dagger$ \\
        Ours   &\color{blue} 14.6 \pm 19.3& \color{blue}  12.5 \pm 24.8 & \color{blue} 16.3 \pm 22.6\\
        \hline
    \end{tabular}
    \caption{\textbf{Game AI win rate advantage between our method and its ablations}. Means and standard deviations (shown after $\pm$) of win rate advantages ($\times 10^{-2}$) across all game instances. All metrics between our method and its ablations show statistically significant difference (p-value $\leq 0.05$ in paired t-test). $\dagger$ represents two metrics are unlikely to be different (p-value $\geq 0.7$ in paired t-test).}
    \label{tab:gameai-adv-ablation}
\end{table}

\subsection{Intermediate Results}

During the construction of gameplay AI, our method produces policy components in text form first, which is converted to code during the second augmentation process.
\input{support-mat/game-ai-output}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LLM System Prompts}
\subsection{Game Mechanics Design}
\input{support-mat/game_design_prompts}

\subsection{Code Generation}
\input{support-mat/code_prompt}

\subsection{Gameplay AI Generation}
\input{support-mat/game_ai_prompt}


To assess our agent's performance, we implemented the baseline opponent agents from previous works. The prompt configurations for each opponent agent were implemented as detailed below.
\input{support-mat/agent_prompt}
