\section{Related Works}
Recent studies have focused on mitigating hallucinations in large language models (LLMs) and large vision-language models (LVLMs) ____, where models tend to generate inaccurate or irrelevant outputs. In vision-language models, such hallucinations often result from over-reliance on language priors or biases embedded in the datasets. For example, in LVLMs, object hallucinations occur when models predict objects not present in the image due to biased object co-occurrences in the training data. Several approaches have been developed to address this, including Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD).

The \textit{Visual Contrastive Decoding (VCD)} method aims to reduce object hallucinations by contrasting the outputs generated from original and distorted visual inputs. This approach does not require additional training or external pre-trained models, making it a computationally efficient solution. By introducing visual uncertainty, such as Gaussian noise, the method identifies and mitigates instances where the model overly relies on language priors or statistical biases from the training data, thereby reducing hallucinations ____.

Similarly, the \textit{Instruction Contrastive Decoding (ICD)} technique is employed to tackle hallucinations in multimodal tasks by incorporating instruction disturbances. This method manipulates the confidence of multimodal alignment in the model's visual and textual inputs, helping it differentiate between hallucinated and relevant tokens. By applying contrastive penalties to tokens influenced by instruction disturbances, ICD effectively reduces the generation of hallucinated outputs, especially in complex visual contexts ____.

In addition, the work context-aware decoding (CAD) has ____ demonstrated a similar outcome to our Delta method by adjusting the output probabilities of LMs, amplifying the differences between outputs generated with and without the given context. This contrastive approach encourages models to prioritize contextual information during text generation. Notably, CAD can be applied to pre-trained LMs without additional training. Unlike our Delta method, the method is mainly based on context-driven datasets, making it less generalizable than the Delta method, which, in theory, could apply to all textual inputs.

Both approaches are part of a growing body of work exploring contrastive mechanisms and fine-grained multimodal alignment techniques to mitigate hallucinations in models that integrate vision and language processing. Future research will likely explore more robust mechanisms further to improve model reliability across different types of multimodal tasks.