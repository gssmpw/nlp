@InProceedings{Leng_2024_CVPR,
    author    = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
    title     = {Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13872-13882}
}

@misc{hinck2024llavagemmaacceleratingmultimodalfoundation,
      title={LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model}, 
      author={Musashi Hinck and Matthew L. Olson and David Cobbley and Shao-Yen Tseng and Vasudev Lal},
      year={2024},
      eprint={2404.01331},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.01331}, 
}

@inproceedings{shi-etal-2024-trusting,
    title = "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
    author = "Shi, Weijia  and
      Han, Xiaochuang  and
      Lewis, Mike  and
      Tsvetkov, Yulia  and
      Zettlemoyer, Luke  and
      Yih, Wen-tau",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.69",
    doi = "10.18653/v1/2024.naacl-short.69",
    pages = "783--791",
    abstract = "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA, and FLAN-T5 for summarization tasks (e.g., 14.3{\%} gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model{'}s prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential. Our code is publicly released at https://github.com/xhan77/context-aware-decoding.",
}

