%%
%%---------------------------------------------
\section{PoC Experimental Validation}
\label{sec:evaluation}
%%
\noindent
%
In this section, we first describe the evaluation setup and test bed infrastructure. Then, we introduce the application use case considered for the evaluation. Finally, we show the experimental results confirming the feasibility and efficacy of the proposed management architecture \added{applied to a cluster of VMs orchestrated via Kubernetes. The focus of our PoC implementation is to demonstrate that a MEC-specific orchestrator built upon advanced Kubernetes functionalities -- such as Kube-OVN to support OVN-based network virtualisation within Kubernetes -- can provide a bounded latency at the application level (i.e. at the Application Slice level) within the MEC domain if we can reserve sufficient resources for the Application Slice. Our experimental hypothesis is based of the fact that the E2E latency can always be apportioned into different domain latencies, namely the 5G RAN latency, the Transport latency, the 5G Core latency and the MEC latency. Then, each domain latency can be managed independently from domain-specific orchestrators, which coordinate to ensure the desired E2E latency.}
%
%%
%%---------------------------------------------
\subsection{Experimental Setup}
\label{sec:exp_setup}
%%
\noindent
%
Our PoC is implemented using K3s\footnote{\url{https://k3s.io/}}, a lightweight Kubernetes distribution designed for resource-constrained edge servers. The software versions of all platforms used to develop our PoC and set up the Kubernetes cluster are listed in Table~\ref{tab:sw_version}. Our Kubernetes cluster is composed of a controller node (\textit{kctl}) and two worker nodes (\textit{kw1} and \textit{kw2}). Each Kubernetes node is a VMware virtual machine configured with Ubuntu as the guest OS, 4GB of RAM, 40GB of storage, but with heterogeneous computing capabilities. Specifically, \textit{kctl} node is provided with one virtual CPU, \textit{kw1} with three virtual CPUs and \textit{kw2} with four virtual CPUs. As explained in detail in Section~\ref{sec:use_case}, a video-analytics software is running only on \textit{kw2}. We deployed the MECO and the ACF Image Repository on \textit{kctl}. We populated the ACF Repository with the container images of the needed ACFs and the MAPSS Chart Repository with a set of Helm charts modelling the Kubernetes deployment templates of the MAPSS depicted in Figure~\ref{fig:poc_eval_slsub_diagram} (see next section for more details). K3s nodes are connected with virtual links having a bandwidth limit of 1 Gbps. Finally, the K3s cluster is running on a physical host equipped with an i7-7700 CPU$@$3.6GHz (4 cores, 8 threads), 16GB RAM, 500GB of storage. Therefore, each virtual CPU has a resource limit of 1000 millicores (mc). 
%
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|c|c|}
    \hline
        \textbf{Software} & \textbf{Version} \\
        \hline
         K3s & 1.21.5 \\ 
         Kube-OVN & 1.7 \\ 
         Go & 1.16.5 \\
         Docker & 20.10.6 \\
         Linux Kernel & 5.4.0.90 \\
         Operating System & Ubuntu 20.04.2 \\
         \hline
    \end{tabular}
    \caption{Software versions of the evaluation setup}
    \label{tab:sw_version}
    \vspace{-0.2cm}
\end{table}
%
%
%%
%%---------------------------------------------
\subsection{Use case}
\label{sec:use_case}
%%
\noindent
%
%
%trim 0cm 8cm 2cm 5cm
%#0cm 8.8cm 2cm 7.5cm
\begin{figure*}[ht]
    \centering
    \includegraphics[clip,trim= 0cm 9.5cm 2cm 6cm,width=0.7\textwidth]{figures/fig_poc_eval_slsub.pdf}
   \caption{Graph representation of the ACFs and their interactions for the the deployed MAPSS.}
    \label{fig:poc_eval_slsub_diagram}
\end{figure*}
%
To validate the feasibility of our design approach we consider a use case from a typical Smart City scenario, where surveillance cameras are placed on particular points of interest or crowded areas and HD video streams as sent to an edge infrastructure where time-critical video analytic tasks (e.g. object recognition) are performed in a distributed manner. Specifically, we assume that a 5G-based Mobile Network Operator (MNO) has deployed a 5G infrastructure serving a smart city. In the 5G network premises, a MEC Owner also deployed a set of edge servers and manages a MEC virtualisation infrastructure. As a tenant of the MEC Owner, a MEC Customer leases a 5G network slice from the MNO and a sliced MEC environment from the MEC Owner to offer Video Analytic Application Services (VAASs) to its end customers. To this end, the MEC Customer has deployed a set of 5G-ready IP cameras across the city, and offers a variety of VAASs (e.g., traffic monitoring, pedestrian alerts, etc.) to different ASCs. (e.g., local municipality, firefighters, etc.). We assume that a VAAS request includes not only the category of the desired application service, but also a set of requirements (e.g. geographical scope, video accuracy, feedback timeliness, etc.). Then, the MEC Customer instantiates an E2E application slice, including a MEC application slice subnet, to satisfy the VASS request. 

Figure~\ref{fig:poc_eval_slsub_diagram} shows the service graph of the deployed MAPSS, namely the set of ACFs and their interactions. Without loss of generality, we assume that each ACF is implemented as a Docker container, and embedded into a single Pod. Therefore, ACF, Docker container and Kubernetes Pod concepts are used interchangeably in the following. In the deployed MAPSS, the \texttt{cameraSimulator} ACF is a data producer and transmits a video stream of images extracted from a public car dataset\footnote{\url{https://github.com/andrewssobral/vehicle_detection_haarcascades}}, \added{with a fixed frame rate of 25~fps}. We leverage Kafka for stream processing, and the \texttt{KafkaBroker} ACF embeds a standard Kafka broker. The \texttt{KafkaBridge} extracts individual frames from the video streams, compresses them, tags the compressed frames with a unique id (\textit{frameId}), appends a timestamp (\textit{tsT1}), and stores them with a custom \replaced{data}{dat6a} format in the \textit{sourceFrames} kafka topic of the \texttt{KafkaBroker}. \added{On average, every 40 ms a new frame is stored in the input queue of the \texttt{KafkaBroker}, which is configured so that stored messages that are not consumed within 2 seconds are removed.} The \texttt{frameAnalytic} ACF leverages the Python OpenCV library\footnote{\url{https://pypi.org/project/opencv-python/}} to perform vehicle detection on the stored frames. Specifically, \texttt{frameAnalytic} is subscribed to (read) the stream of events from the topic \textit{sourceFrames}. Then, vehicle detection is performed on each frame consumed by \texttt{frameAnalytic} and a new image is generated by drawing bounding boxes around every recognised car. Finally, processed frames are compressed, tagged with the same \textit{frameId} of the source image, timestamped (\textit{tsT2}), and stored in a new \textit{parsedFrames} kafka topic. The \texttt{webView} ACF is subscribed to the \textit{parsedFrames} topic and is responsible for displaying processed video frames on a web page. 
 
 To assess the performance of the deployed VASS, we use the \textit{application latency} of each frame, computed as the difference between the \textit{tsT2} and \textit{tsT1} values of frames with the same \textit{frameId}. The \texttt{analyticDelayMonitor} ACF is responsible for measuring processing delays. It is important to point out that we forced deterministic scheduling decisions in the \texttt{kube-scheduler} agent to ensure comparable delay measurements from different experiments. Specifically, Pods running the docker containers of the \texttt{frameAnalytic} ACF are always scheduled on \textit{kw2}, while all other Pods are assigned to \textit{kw1}. This also allows us to better investigate the impact of networking delays on the slice performance. 
%
%%
%%---------------------------------------------
\subsection{Experimental results}
\label{sec:results}
%%
\noindent
%
In the following, we show results with the purpose of $(i)$ validating the ability of our management architecture to provide MAPSS performance isolation; $(ii)$ assessing the impact of resource constraints on the application latency\added{; and $iii)$ highlighting performance coupling between slices when resources are shared}. 
%
%%
%%---------------------------------------------
\subsubsection{MAPSS isolation}
\label{sec:test1}
%%
\noindent
%
In the following experiments we deploy two identical instances of the MAPSS described in Figure~\ref{fig:poc_eval_slsub_diagram}, denoted as $slice_1$ and $slice_2$, respectively. All the Pods in $slice_1$ are assigned a Guaranteed QoS class\footnote{We remind that a Pod is assigned to the Guaranteed QoS class if for every container in the Pod, CPU and memory limits are equal to the CPU and memory requests. In Kubernetes, the request value specifies the min value the Pod will be guaranteed, while the limit value specifies the max value the Pod can consume.}, while all the Pods in $slice_2$ are assigned a BestEffort QoS class (i.e. they do not have any CPU/memory limits or requests). Table~\ref{tab:cpu_request} summarises the CPU requests of different ACFs. The CPU requests of ACFs deployed in $slice_1$ are decided after profiling the performance of each ACFs. Specifically, we run a preliminary experiment in which we deploy a single slice and we let the containers run without any resource limits. The values of CPU requests in Table~\ref{tab:cpu_request} represent the maximum CPU usage measured during this experiment.    
%
\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|l|l|c|}
    \hline
        ACF name & Worker & CPU request (mc) \\
        \hline \hline
        \texttt{cameraSimulator}  & \textit{kw1} & 100 \\
        \texttt{KafkaBridge}  & \textit{kw1} & 500 \\
        \texttt{KafkaBroker}  & \textit{kw1} & 500 \\
        \texttt{analyticDelayMonitor}  & \textit{kw1} & 100 \\
        \texttt{frameAnalytic}  & \textit{kw2} & 1800 \\
         \hline 
    \end{tabular}
    \caption{CPU requests (for Pods of $slice_1$) and scheduling plan for the different ACFs.}
    \label{tab:cpu_request}
    \vspace{-0.2cm}
\end{table}
%
To validate the ability of our solution to support MAPSS performance isolation (with a focus on CPU utilisation), we conduct an experiment with three distinct phases. In the first phase (from time 0 to time $T$), and third phase (from time $2T$ to time $3T$)) only $slice_1$ and $slice_2$ are active in the Kubernetes cluster. During the second phase (from time $T$ to time $2T$), an additional Pod is deployed on node \textit{kw2}. This ``interfering'' Pod runs within a single container a CPU stress tool that generates a CPU-intensive workload\footnote{\url{https://github.com/containerstack/docker-cpustress}}. Since, CPU resources for the Pods of $slice_1$ are guaranteed, this interfering Pod should only compete with the Pod of $slice_2$ running on node \textit{kw2}. To confirm our claim, Table~\ref{tab:avg_app_latency} shows the average application latencies measured during the three phases of the experiment, when $T=6$~minutes. 
%
\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|l|c|c|c|}
    \hline
         \multirow{2}{*}{Slice} &  \multicolumn{3}{|c|}{Application latency (msec)} \\\cline{2-4}
         & I phase & II phase & III phase\\
         \hline \hline
         $slice_1$ & 26.36  & 28.51 & 25.79 \\
         $slice_2$ & 27.00 & 85.66 & 25.78  \\
         \hline
    \end{tabular}
    \caption{Average application latencies measured during the various phases of the test.}
    \label{tab:avg_app_latency}
        \vspace{-0.2cm}
\end{table}
%
From the shown results, we can observe that $slice_1$ and $slice_2$ obtain similar application latencies in both phase I and phase III. This is sensible since $slice_1$ and $slice_2$ are assigned the same CPU resources by the \texttt{kube-scheduler}. Indeed, the Pod of $slice_1$ running on \textit{kw2} requests half of the available CPU capacity\footnote{We remind that \textit{kw2} has four virtual CPUs, which corresponds to 4000~mc. However, about 400~mc are used by the Kubernetes control-plane processes.} Therefore, the Pod of $slice_2$ running on \textit{kw2} is free to use the non-reserved resources without competing with other Pods. On the contrary during phase II, there is a competing Pod and the BestEffort QoS Class does not provide any guarantee on the assigned resources. Thus, we can observe that the application latency of $slice_2$ increases by a factor of 3.2, while the increase in application latency for $slice_1$ is negligible (only 8\%). \added{It is worth pointing out that no messages are lost in the message queues of the \texttt{KafkaBroker}. Thus, the average output throughput from the \texttt{FrameAnalytic} Pod is 25fps.}
%
\begin{figure}[ht]
    \centering
    \includegraphics[clip,angle=-90,trim= 2cm 0cm 2cm 0cm,width=0.50\textwidth]{figures/gnuplot/out/fig_cpu_isolation.pdf}
   \caption{Temporal evolution of the applications latencies.}
    \label{fig:poc_slice_isolation_temp}
    \vspace{-0.2cm}
\end{figure}
%
We conclude this analysis by showing in Figure~\ref{fig:poc_slice_isolation_temp} the temporal evolution of the application latencies as observed in one of the experiments previously observed. The results not only confirm the findings of Table~\ref{tab:avg_app_latency}, but also show that the application latency values of $slice_2$ are affected by a notable variability. One main reason for these fluctuations of application latencies is that the frames in the video stream can generate different processing loads for the \texttt{frameAnalytic} Pod (e.g., depending on the number of vehicles to be detected). Therefore, if the \texttt{frameAnalytic} Pod is underprovisioned of CPU resources (as in the case of $Slice_2$), there can be spikes in the application latencies due to the \textit{sourceFrames} topic queue filling up. Another reason for the non-deterministic behaviours of application latencies is the way the standard CPU management policies work in Kubernetes. Specifically, time slices of a CPU over a fixed period are assigned to a process on the basis of CPU requests and limits. If the allocated number of slices in a period (100ms by default) are not sufficient to complete the application tasks, the process has to wait the following period to resume the processing, which causes a sudden increase of the application latency. Our PoC implementation approach, as described in Section~\ref{sec:poc_design}, is sufficient to limit the impact of these non-deterministic behaviours on $slice_1$. 
%
%%
%%---------------------------------------------
\subsubsection{Resource limitations}
\label{sec:test2}
%%
\noindent
%
The purpose of the following tests is to assess the impact on application latency of reducing reserved resources with respect to the baseline setting used in the previous experiments. 
%
\begin{figure}[ht]
    \centering
    \includegraphics[clip,angle=-90,trim= 2cm 0cm 2cm 0cm,width=0.50\textwidth]{figures/gnuplot/out/fig_bw_limit.pdf}
   \caption{Impact on application latency of bandwidth constraints.}
    \label{fig:poc_res_bw}
    \vspace{-0.2cm}
\end{figure}
%
Figure~\ref{fig:poc_res_bw} shows how the application latency varies when reducing the bandwidth on the virtual link that connects the \texttt{KafkaBridge} ACF and the \texttt{frameAnalytic} ACF (in our experiemnatl setup this link is also the one connecting \textit{kw1} and \textit{kw2}). In the experiments we consider three bandwidth values $\{30,60,120\}$~Mbps. We can note that by halving the link capacity from 120~Mbps to 60~Mbps the application latency increases by 62\% on average, while decreasing the link capacity by a factor of four the application latency increases by 230\% on average. Furthermore, a higher latency variability can be observed when the link capacity is low, as it is more likely that the \textit{sourceFrames} topic queue gets unstable (i.e., the time between two consecutive frame requests of the \texttt{frameAnalytic} ACF is longer than the inter-arrival time of unprocessed frames in the \textit{sourceFrames} topic queue). 
%
\begin{figure}[ht]
    \centering
    \includegraphics[clip,angle=-90,trim= 2cm 0cm 2cm 0cm,width=0.50\textwidth]{figures/gnuplot/out/fig_cpu_limit.pdf}
   \caption{Impact on application latency of constraints on CPU speed.}
    \label{fig:poc_res_cpu}
    \vspace{-0.2cm}
\end{figure}
%
Figure~\ref{fig:poc_res_cpu} shows how the application latency varies when reducing the CPU quota of the \texttt{frameAnalytic} Pod. We can show that when we limit the CPU request of the \texttt{frameAnalytic} Pod to 1000~mc the average application latency only experiences a 5\% increase with respect to what observed with the base setting (namely a CPU request of 1800~mc). On the contrary, when the \texttt{frameAnalytic} is significantly underprovisioned (a CPU request of only 500~mc) not only the  application latency experiences a five-fold increase on average, but it also shows a very high variability. These results further confirm that a non-proper allocation of reserved resources may easily lead to non-deterministic behaviours due to a container scheduling process that is not aware of latency but only of CPU and memory requirements.
%
%%
%%---------------------------------------------
\subsubsection{\added{Shared edge resources}}
\label{sec:test3}
%%
\noindent
%
\added{Typically, resource allocation in edge computing environments assumes that edge resources are dedicated to individual service requests. However, recent studies have started investigating more advanced use cases in which edge resources (e.g. the same set of data and code) can be shared between competing service requests. For instance, a joint service placement and data management solution for MEC-enabled IoT applications is proposed in~\cite{2021_jnca_edge} to take advantage of shared caches at the edge of the network. A similar problem is addressed in~\cite{2018_icds_edge_share} by assuming that service request can be satisfied by multiple replicas of the same service. Slicing algorithms that model the coupling between networking and MEC resources are also explored in~\cite{2020_mobihoc_si-edge}.}

\added{To evaluate the implications of using shared resources in our proposed slicing architecture we have built the following test. We deploy two identical slices, denoted as $slice_1$ and $slice_2$ that includes all the Pods described in Figure~\ref{fig:poc_eval_slsub_diagram} \textit{except} for the \texttt{frameAnalytic} Pod, which is deployed into a separate slice, denoted as $slice_3$. Then, the \texttt{frameAnalytic} Pod is \textit{shared} between $slice_1$ and $slice_2$. This is achieved by running two threads in \texttt{frameAnalytic} that are reading/writing from/to the \texttt{KafkaBroker} of $slice_1$ and $slice_2$, respectively. Another key difference with the tests described in Section~\ref{sec:test1} is that all the the Pods in $slice_1$, $slice_2$ and $slice_3$ are assigned a Guaranteed QoS class as we want a deterministic allocation of the maximum CPU time each Pod can use. In our experiments we consider two scenarios. In the first one, labelled \textit{with-sharing}, $C$ CPU units are guaranteed to the \texttt{frameAnalytic} Pod of $slice_3$. In the second one, labelled \textit{with-sharing}, the $slice_1$ and $slice_2$ deployment is the same as in Section~\ref{sec:test1}, i.e., two separate replicas of the \texttt{frameAnalytic} Pod are included into $slice_1$ and $slice_2$, but only $C/2$ CPU units are guaranteed to the two \texttt{frameAnalytic} Pods. In other words, in both scenarios the same total CPU processing power is allocated to the data analytic service. Table~\ref{tab:sharing} reports the applications latencies for different values of the CPU limit.} 
%
\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{|c||c|c|c|c|}
    \hline
        \multirow{3}{1.8cm}{\added{CPU limit ($C$)}} & \multicolumn{4}{|c|}{\added{Application latencies (ms)}} \\
         \cline{2-5}
        &  \multicolumn{2}{|p{2.5cm}|}{\added{(\textit{non-shared serv.})}} & \multicolumn{2}{|p{2.5cm}|}{\added{(\textit{shared serv.})}} \\
        \cline{2-5}
        & \added{$slice_1$} & \added{$slice_2$} & \added{$slice_1$} & \added{$slice_2$} \\
        \hline
        \added{3600} & \added{25.95} & \added{26.01} & \added{19.39} & \added{19.65} \\
        \added{2000} & \added{26.22} & \added{26.20} & \added{21.93} & \added{21.63} \\
        \hline
    \end{tabular}
    \caption{\added{Average application latencies with shared and non-shared services.}}
    \label{tab:sharing}
    \vspace{-0.2cm}
\end{table}
%
\added{As expected, the higher the allocated CPU resources and the lower the application latencies. Furthermore, both slices achieve identical performance as their Pods are assigned a QoS class of Guaranteed. However, when the data analytic Pod is shared between the two slices, application latencies decrease up to 20\%. One main reason is that frames in the two video streams generate variable processing loads. Therefore, sharing the CPU resources reduces the impact of the processing time variability on the maximum buffer length (waiting time) of the \texttt{KafkaBroker}.}
%
%\begin{figure}[ht]
%    \centering
%    \includegraphics[clip,angle=-90,trim= 2cm 0cm 2cm 0cm,width=0.45\textwidth]{figures/gnuplot/out/fig_sharing_1000.pdf}
%   \caption{Impact on application latency of sharing acfs among slices - 1000/2000}
%    \label{fig:poc_res_bw}
%\end{figure}
%
%
\begin{figure}[ht]
    \centering
    \includegraphics[clip,angle=-90,trim= 2cm 0cm 2cm 0cm,width=0.50\textwidth]{figures/gnuplot/out/fig_sharing_1800.pdf}
   \caption{\added{Temporal evolution of applications slices with and without shared Pods (CPU limit $=$ 3600).}}
    \label{fig:sharing_1800}
    \vspace{-0.2cm}
\end{figure}
%
\added{Finally, Figure~\ref{fig:sharing_1800} shows the temporal evolution of the application latencies in the same experiment of Table~\ref{tab:sharing}. The results confirm the variability of the application latencies, but sharing the data analytic Pod ensure better performance than splitting the computing resource between two copies of the same Pod.}

