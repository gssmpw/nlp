%%
%%---------------------------------------------
\section{Implementation Experience}
\label{sec:implementation}
%%
\noindent
%
In this section we describe our approach to implement a MEC Customer Orchestrator and to support MEC application slicing using Kubernetes and Helm technologies. Although our implementation is still at a work-in-progress state, our proof-of-concept prototype (hereafter simply PoC), shows the feasibility of our design approach and allowed us to collect a preliminary insight on the efficiency of application slicing using Kubernetes resource management capabilities. In the following, we first briefly overview Kubernetes and Helm features. Then, we detail the implementation of the new APIs and functional components  of our PoC,  namely the MECO and the ACF Image Repository. Finally, we describe our practical approach to support slicing of Kubernetes resources. For the sake of presentation clarity, Figure~\ref{fig:poc} overviews the architecture of the PoC and its internal components.
%
\begin{figure*}[ht]
    \centering
    \includegraphics[clip,trim= 1cm 4.5cm 0cm 2.5cm,width=0.7\textwidth]{figures/fig_poc.pdf}
   \caption{PoC architecture and internal components.}
    \label{fig:poc}
    \vspace{-0.2cm}
\end{figure*}
%
%%
%%---------------------------------------------
\subsection{PoC enabling technologies}
\label{sec:poc_tech}
%%
\noindent
%
Kubernetes is an open-source container life-cycle manager and orchestrator, which is the de-facto standard for running container-based cloud native applications on a cluster of (physical or virtual) machines (called \textit{nodes}). It is out of the scope of this paper to describe the complete Kubernetes architecture and high-level abstractions, but we focus on the components that are most relevant for our PoC. 

A Kubernetes cluster is composed of $(i)$ a set of \textit{worker} nodes that run containerized applications, also called \textit{workloads}; and $(ii)$ (at least) a \textit{master} node that runs the services of the \textit{control plane}, and it is responsible to enforce the desired state of the cluster. Kubernetes provides several built-in workload resources to support various application behaviours (e.g., stateless tasks) and management functions (e.g., creating or deleting application replicas). Each workload must run into a \textit{Pod}, a Kubernetes object that represents a collection of containers running in the same execution environment, which share the same storage, networking, and lifecycle. Pods runs into worker nodes, which host an agent, called \texttt{kubelet}, that is responsible for managing worker's local containers and for synchronising the status with the master node. As better explained later, another main component of the worker node is the \texttt{kube-proxy}, which is responsible for implementing Kubernetes networking services and to enable communication to Pods from inside and outside the cluster. The master node is composed of different components including:$i)$ \textit{etcd}, a distributed key-value store that holds and manages all cluster critical data; $ii)$ \texttt{kube-apiserver}, a component providing a REST-based frontend to the control plane through which all other components interact; $iii)$ \texttt{kube-controller}, a component that monitors the shared state of the cluster using apiserver and runs the controller processes; and $iv)$ \texttt{kube-scheduler}, a component that assigns newly created Pods to nodes.  Note that Kubernetes also support the \textit{namespace} abstraction, namely virtual clusters that share the same IP Address and port space, facilitating the grouping and organisation of objects.    

Networking is a central part of the Kubernetes design and a fundamental capability for application slicing. Specifically, the Kubernetes network model demands certain network features, such as every Pod should have a unique, routable IP inside the cluster, and inter-pod communications should happen without using NATs, regardless of wherever the Pods reside or not on the same worker nodes (i.e. network segmentation is not allowed). Since IP addresses of Pods are ephemeral and change whenever the Pods are restarted or migrated, Kubernetes also defines \textit{Service} resources, namely REST objects that are used to group identical Pods together to provide a consistent means of accessing them, e.g by bounding them to a virtual IP address (called cluster IP) that never changes. It is important to point out that 
Kubernetes does not directly handle the networking aspects, but it rather allows the use of third-party networking plugins that adhere to the Container Network Interface (CNI) specification\footnote{\url{https://github.com/containernetworking/cni}} to manage the containers' data plane. Of particular relevance for our PoC, is Kube-OVN\footnote{\url{https://www.kube-ovn.io/}}, a CNI plugin that integrates network virtualisation into Kubernets  by leveraging the OVN (Open Virtual Network)\footnote{\url{https://www.ovn.org/en/}} technology. Kube-OVN supports advanced features, such as unique subnets per namespace, network policies, namespaced gateways, subnet isolation and dynamcic QoS. We extensively leverage some of those features to support application slicing in our PoC. 

Another technology we use as a basis for our PoC is Helm\footnote{\url{https://helm.sh/}}, an application packaging manager for Kubernetes. Specifically, Helm defines a data format, called \textit{Helm Chart}, to bundle a set of Kubernetes object definition files (i.e. files describing properties of Kubernetes objects) into a single package. This permits to manage the instantiation, upgrade and deletion of the corresponding Kubernetes objects as they were a single entity. The Helm charts are stored into a separate repository, and every time a new instance of the same chart is installed into Kubernetes, a new chart \emph{release} is created. Furthermore, Helm allows to augment Kubernetes object definition files with Helm template commands. By providing Helm a list of arguments for these template commands at chart instantiation time, it is possible to dynamically customise the chart before it is actually deployed.  Examples of these customisation span from overriding object default values with the one passed as command arguments (e.g. a port exposed by a container, the namespace name, etc..) to dynamically enabling disabling chart sections. As explained later, we extensively leveraged Helm features to implement different components of our PoC, such as the KDPManager and the MAPSS Chart Registry (see Figure~\ref{fig:poc}).

We conclude this section by noting that the ETSI MEC standard has recently started analysing how MEC features should be adjusted when deploying a MEC system using a container-based virtualisation technique~\cite{MEC027}. Furthermore, a few initial implementations exist of specific MEC components and interfaces using Kubernets as VIM, such as Akraino\footnote{\url{https://www.lfedge.org/projects/akraino/}} and LightEdge\footnote{\url{https://lightedge.io/}}. A recent work~\cite{2020_broadnets_mec_k8s} analyses how to use Kubernetes not only as a VIM, but also as the core of the MEPM, also leveraging Helm technology for the life-cycle management of MEC applications. 
%
%%
%%---------------------------------------------
\subsection{PoC design and development}
\label{sec:poc_design}
%%
\noindent
%
One of the main objectives of our proof-of-concept implementation is to demonstrate how Kubernetes can natively support multiple, isolated instances of MEC application slice subnets (MAPSSs for brevity) as defined in Section~\ref{sec:pre_concepts}. The key building block of a MAPSS is the ACF. For the sake of simplicity, in our PoC we ignore VNFs and we assume that ACF Suppliers can provide ACFs in the form of Docker container images coupled with an ACF user manual or descriptor (called ACFD). Then, an ASP leverages the ACFDs to select the set of ACFs that are needed to build the AS requested by its end customers, as well as to derive the proper run-time configuration of the graph of ACFs composing the AS. Therefore, a key component of our PoC is the \textit{ACF registry}, where authorised ACFDs are published and stored. The ACF registry is implemented using the open-source Docker Registry 2.0 application\footnote{\url{https://hub.docker.com/_/registry}}, a storage and distribution system for named Docker images. A generic ACFD is structured into two separate sections. The first one details the RESTful APIs exposed by the ACF (OpenAPI\footnote{\url{https://www.openapis.org/}} is used to specify these APIs in a standard, language-agnostic format). The second section details how to properly configure the ACF parameters in order to control how the ACF will behave at run-time. The ACF parameter customisation is a crucial aspect to consider, especially in the context of ACF composition and automated orchestration. An example of ACF customisation is the selection of buffer sizes, which may influence the run-time behaviours and performance of ACFs, directly affecting the fulfilment of SLA requirements. For the sake of simplicity, in our PoC we use \textit{environmental variables} to pass configuration paramters to the running container images of ACF. Finally, it is important to point out that the ACF registry must be accessed also by the \texttt{kube-scheduler} agent to fetch the container images of ACFs to be deployed.

The other key component of our PoC is the MECO that we have implements as an additional component of the master node, using Go as programming language. Internally, the MECO component is composed of three different modules: $i)$ the \textit{MECO API Server} to enable communications with the MEC APSSMF; $ii)$ the \textit{MAPSS Chart Repository} to store the templates of Kubernetes deployment plans of MAPSS; and $iii)$ the \textit{KDPManager}, to manage the Kubernetes deployment plans of run-time instances of activated MAPSS. In the following, we elaborate on the purpose and operations of each module more in detail. 

The main role of the MECO API Server is to act as a lifecycle management proxy for MAPSSs. Specifically, it receives commands for the instantiation, update and termination of MAPSS from the MEC APSSMF, and translates these commands into suitable Kubernetes actions. To this end, the MECO API Server exposes a RESTful management interface, called \texttt{mapss\_mm} API, defined using the OpenAPI description language (see Figure~\ref{fig:poc}). For the sake of the experimentation, the \texttt{mapss\_mm} API currently implements a \texttt{POST} method, which allows the MEC APSSMF to request the instantiation of a new MAPSS instance. The payload of this \texttt{POST} method carries a descriptor, called MAPSSD, which contains all the necessary information to allow the MECO to instantiate at run-time a specific MAPSS instance. The \texttt{mapss\_mm} API also implements a DELETE method, which allows to delete a running instance of a MAPSS. It is clear from this discussion, that the MAPSSD plays a key role in our PoC. We envision a MAPSSD organised into four different sections, as also illustrated in the example in Figure~\ref{fig:poc_massd_example}. The first parts includes the unique identifier of the MAPSS instance (\texttt{mappsiId}), a human readable description of the slice subnet features, and the identifier of an implementation template to be used for the deployment of the MAPSS instance (\texttt{mapssImplTemplateId}) -- see later this section for more details on how to use the \texttt{mapssImplTemplateId}. The second section includes a set of ``\textit{slice-subnet-wise}'' (computational, storage and networking) resource requirements. For example, the MAPSSD shown in Figure~\ref{fig:poc_massd_example} requires two dedicated CPU cores, 8 GB of memory and 100 GB of permanent storage, which will be shared among its constituent ACFs. The third part includes the list of ACFs that compose the MAPSS instance. Each ACF is associated to a unique identifier (\texttt{acfId}), and ``\textit{acf-wise}'' resource requirements can also be specified. For example, the acf1 in Figure \ref{fig:poc_massd_example} requires a dedicated CPU core out of the two dedicated to the whole slice subnet. Moreover, the field \texttt{customParams} can be leveraged to pass arguments (e.g. buffer size in figure) to configure specific ACF behaviours. Finally, the last section includes a list of virtual links among pairs of ACFs and their networking requirements (e.g. maximum usable bandwidth). It is worth noting that the data format of the MAPSSD is agnostic from the underlying virtualisation technology. In other words, the MEC APSSMF could leverage the same data model to interact with a MECO that relies on a different VIM than Kubernetes. Another advantage of the proposed MAPSSD is that it allows the MEC APSSMF to seamlessly integrate SLS requirements that address needs of different architectural levels (i.e. specific to the entire slice subnet, individual ACFs and virtual links between ACFs) in the same data object. 
%
\begin{figure}[th]
    \centering
    \includegraphics[clip,trim= 0cm 16cm 10cm 0cm,width=0.5\textwidth]{figures/fig_poc_mappsd.pdf}
   \caption{Illustrative example of a MEC application slice subnet descriptor (MAPSSD)}
    \label{fig:poc_massd_example}
\end{figure}
%

Clearly, the translation from a VI-agnostic MAPSSD into a Kubernetes deployment plan (\textit{KDP} for short) of the MAPSS instance, namely a package of properly configured Kubernetes objects implementing the requested MAPSS instance, is a critical functionality of the MECO. Following the approach proposed in~\cite{NGMN028} for supporting cost-efficient customisation of network slices, we assume that the MECO hosts a pre-loaded set of MAPSS templates/blueprints that can be used to speed up the creation of a MAPSS instance. Specifically, we implemented each MAPSS KDP template as an Helm chart that includes a set of pre-configured Kubernetes objects. These objects define: $i)$ ACFs Docker containers to run (e.g. via Kubernetes Pods objects), $ii)$ ACFs behavioural parameters (e.g. via environmental variables in Kubernetes ConfigMaps objects), $iii)$ ACFs connection points (i.e. exposed ports), $iv)$ custom scheduling policies (e.g., number of replicas, failure behaviour, etc.), and any other Kubernetes feature that is necessary for the correct deployment of the MAPSS instance. Then, the \textit{mapssImplTemplateId} field of the MAPSSD is used to retrieve the correct MAPSS KDP template. It is important to point out that the MECO should be able to dynamically customise at run-time the MAPSS KDP template using information derived from the MAPSSD (e.g., container resource requirements). To this end we leverage built-in objects and control structures of Helm template that provide access to values passed into an Helm chart, and the ability to include conditions in the template's generation. In the current implementation, we limit such customisation to the selection of: $i)$ the name of the namespace to which objects will belong; $ii)$ the computational, storage and networking requirements for the namespace; $iii)$ the computational and storage requirements per ACF, and the networking requirements per ACF pairs. In our Poc the MAPSS KDP templates are stored in the MAPSS Chart Registry (see Figure~\ref{fig:poc}). According to the operational and management roles defined in Section~\ref{subsec:roles}, the MEC Customer plays the roles of both the MEC operator and the ASP. Thus, the MEC Customer has the necessary expertise not only to properly select, compose and configure ACFs to provide an AS, but also to select and properly configure the subset of Kubernetes objects that allows to implement the MAPSS instance of the designed AS. Finally, the KDPManager module is simply a wrapper of the Helm library, which allows the MECO to embed the Helm functionalities.   

%
\begin{figure*}[ht]
    \centering
    \includegraphics[clip,trim= 0cm 2cm 0cm 2cm,width=0.9\textwidth]{figures/fig_poc_seq_diag.pdf}
   \caption{Sequence of operations to instantiate a new MAPSS in the PoC architecture.}
    \label{fig:poc_seq_diag}
\end{figure*}
%
We can now discuss the sequence of operations and request/response exchanges that are executed to deploy a new MAPPS instance in our Poc, which are also graphically illustrated in Figure~\ref{fig:poc_seq_diag}. First of all, the MEC APSSMF initiates the deployment process by sending a \texttt{POST} request to the MEC API Server over the \texttt{mapss\_mm} interface (see Figure~\ref{fig:poc}), which contains the MAPSSD of the requested MAPSS. In the figure, the requested MAPSS is identified as \textit{demoSlice}, while its Helm chart template is identified as \textit{demoTpl}. In \textit{step2}, the MECO API Server performs a preliminary analysis of the MAPSSD to discover the set of parameters that can be modified to customise the template. Furthermore, the MECO API Server retrieves from the \textit{mapssImplTemplateId} field of the MAPSSD the identified of the associated Helm chart template (\textit{dempoTpl} in this example). Finally, \textit{step2} is concluded with the API Server that instructs the KDPManager to deploy the \textit{demoTpl} Helm chart with the correct set of chart arguments. Subsequently, in \textit{step3}, the KDPManager fetches the \textit{demoTpl} Helm chart from the MAPSS Chart Repository, and it starts the deployment process. First, in \textit{step4}, the KDPManager contacts the \texttt{kube-apiserver} to create a new Kubernetes namespace with name \textit{demoSlice}. Then, the KDPManager applies (\textit{step5}) the customised arguments (e.g., number of cores to be assigned to an ACF container), and starts (\textit{step6}) chart release process, using \textit{dempoSlice} as release name. This process involves the generation of the proper set of Kubernetes objects definition files (i.e. the Kubernetes Deployment Plan). Once the KDP is complete, the KDPManager instructs the \texttt{kube-apiserver} to create the Kubernetes objects in the etcd database (\textit{step7}). Finally, in \textit{step8}, the \texttt{kube-controller} starts performing control actions according to the received Kubernetes objects. The latter includes contacting the ACF Image Repository to fetch ACFs container images for scheduling. For the sake of completeness, we remind that a termination of a run-time instance of a MAPSS is initiated by a \texttt{DELETE} request sent by the MEC APSSMF to the MECO via the \texttt{mapss\_mm} interface. This \texttt{DELETE} contains the \textit{mapssId}) of the MAPSS instance to delete. In this case, the MECO API Server requests the KDPManager to uninstall the chart release associated to \textit{mapssId}. Finally, the KDPManager removes all the Kubernetes objects of the release from Kubernetes cluster, and deletes the \textit{mapssId} namespace.

We complete the presentation of our PoC by explaining how application slice isolation is enforced using the Kubernetes resource control objects. First of all, we create a new Kubernetes namespace for each MAPSS instance, with a name equal to the \textit{mapssId}. All Kubernetes objects within a MAPSS instance are deployed using the same MAPSS namespace. We leverage a combination of Kubernetes \textit{ResourceQuota} objects, \textit{QoS classes} for Pods, and Kubernetes \textit{VolumeClaim} requests to limit the amount of computational and storage resources that could be consumed by both the whole namespace (to enforce requirements for invidual slice subnets) and individual Pods (to enforce requirements for individual ACFs). Network isolation between different instances of MAPSSs is implemented by exploiting Kube-OVN network policies, so that that the traffic arriving from Pods belonging to other namespaces is blocked (except for the system namespace). Finally, we implement per Pod ingress/egress rate limitation via Kube-OVN QoS policies. The implementation of more advanced QoS-aware network control policies (e.g. latency assurance, QoS tagging, etc.), and fine-grained network isolation policies (e.g. tunable network isolation degree with exception handling, etc) is planned as future work.


We conclude this section by  observing that an ASC could discover available ASes and their features by querying a catalogue that is exposed by a web portal (see Figure~\ref{fig:poc}), on which ASPs publish the descriptors of their ASes. We can foresee that AS descriptors include information such as $i)$ high level description of the offered AS; $ii)$ a pointer to the ASP offering the AS; $iii)$ a set of achievable SLAs (e.g. maximum resolution of a video processing service); and, possibly, $iv)$ billing information. The definition of a data model for AS descriptors is out of the scope of our present work. 
%
%%
%%---------------------------------------------
\subsection{Open implementation gaps}
\label{sec:poc_limitations}
%%
\noindent
%
During the implementation of our PoC we also faced several difficulties due to the limitations of the technologies and standards we have used. In the following, we summarise the main technological gaps we have observed to highlight areas of future investigations.

\begin{itemize}[noitemsep,topsep=2pt]
    \item ETSI MEC specification has defined the methods and the data formats for the \texttt{Mm1} reference interface between the OSS and the MEO, which is used to trigger the instantiation and the termination of MEC applications in the MEC system. However, the \texttt{Mm1} implicitly consider a MEC application as a single application package. For instance, in the MEC-in-NFV architecture, the \texttt{Mm1} allows the MEAO to  deploy a single VNF onboarded as a VNF descriptor. In our use case, a MAPSS instance represents a set of ACFs, and it could be conveniently modelled as a graph. To some extent the \texttt{Mm1} interface should be expanded to resemble the capabilities of the \texttt{Os-Ma-nfvo} interface between the NSSMF and the NFVO~\cite{NFV-SOL005} , which allows the NSSMF to request a network service (i.e. a collection of VNFs) to the NFVO.
    %
    \item Kubernetes ResourceQuota objects only permit to limit the amount of CPU and memory resources that Pods in a namespace could use. Pods can get assigned to a ``Guaranteed'' (highest priority) QoS class to receive reserved CPU and memory resources. VolumeClaim requests allow to reserve storage resources to a scheduled Pod. However, Kubernetes does not provide a straightforward mechanism to allocate resources at the namespace level but only at Pod level. This limitation complicates the implementation of resource over-provisioning strategies in dynamic slicing context (e.g. when a slice subnet is assigned more resources than needed to accommodate future demand changes).
    %dynamic slicing (e.g. when an application slice changes the set of deployed ACFs) 
    %
    \item Default scheduling mechanisms available in Kubernetes take into account only CPU and RAM usage rates when scheduling Pods, while network-related metrics (e.g. latency or bandwidth usage rates) are often ignored. However, a network-aware resource allocation and scheduling is crucial for our application slicing model, and initial proposals can be found in~\cite{2019_netsoft_netaware_kube} and~\cite{2020_noms_delayaware_kube}.
    %
    \item Kube-OVN allows to limit the transmission rate on both ingress and egress traffic at the Pod level. This is obtained by using a QoS-aware queue and traffic policing at the vswitch port to which the Pod is connected. However, Kube-OVN does not support to set up rate limits on individual traffic flows, which is an useful feature if a Pod needs to communicate with several other Pods (e.g., for inter-slice communications). A possible workaround is to leverage Multus CNI\footnote{\url{https://github.com/k8snetworkplumbingwg/multus-cni}}, a CNI plugin for Kubernetes that enables attaching multiple network interfaces to pods, to allow a Pod to have a dedicated virtual interface (i.e. network port) for each destination Pod. Then, separated instances of Kube-OVN could be installed on each virtual interface to enforce different QoS policies at the port level. Furthermore, bandwidth reservation mechanisms similar to the ones proposed for SDN-based networks~\cite{2016_bwguar_openflow} should be included in Kube-OVN. 
    %
    \item Service chaining allows to link together VNFs to compose service function chains (SFCs). The implementation of SFCs usually requires support from the network (e.g. via SDN) to route a packet from one VNF to the next in the chain. However, service chaining (which is a crucial feature for integrating VNFs with ACFs in a MAPSS is missing in Kubernetes. Recently, a few projects, such as OVN4NFV K8s Plugin\footnote{\url{https://github.com/opnfv/ovn4nfv-k8s-plugin}} and Service Meshes\footnote{\url{https://istio.io/}} have been initiated to provide support for service chaining in Kubernetes environments . 
    
    %and Network Service Mesh\footnote{\url{https://networkservicemesh.io/}} have been initiated to provide support for service chaining in Kubernetes environments . 
    %
    \item The MAPSSD provides the blueprint for building an application slice subnet within a MEC environments. For the sake of our PoC, we have defined a custom data model for specifying a MAPPSD. On the other hand, standard modelling languages exist, such as TOSCA (Topology and Orchestration Specification for Cloud Applications) for describing the components of a cloud application and their relationships, which facilitate the interoperability, portability and orchestration in a multi-cloud environment~\cite{2018_MCC_TOSCA}. ETIS MANO already advocates the use of TOSCA to specify NFV descriptors~\cite{NFV-SOL001}. In principle, TOSCA could also be leveraged to specify the MAPSS descriptors. However, TOSCA is specifically designed to model classical cloud applications and it needs some adaptations to natively support also contenarised applications. Several approaches have been recently proposed to either $(i)$ extend the TOSCA normative types for support of container-based orchestration platforms (e.g. Cloudify\footnote{\url{https://cloudify.co/}}); or $(ii)$ to decouple the application modelling from the application provisioning by developing ad hoc software connectors between TOSCA workflow and cloud provider’s API (e.g. TORCH~\cite{2021_jgc_torch}). However, no standard specifications have been released yet.  
    %
\end{itemize}


