\section{D-CIPHER Implementation} \label{sec:implementation}

\begin{figure*}[htpb]
    \centering
    \includegraphics[width=\linewidth]{figures/dcipher-flowdiagram.pdf}
    \caption{Workflow of the D-CIPHER multi-agent system.}
    \label{fig:workflow}
\end{figure*}

% \subsection{Multi-Agent System}

The D-CIPHER framework introduces a collaborative multi-agent system of LLM agents with a seperation of responsibilities. Each agent's architecture builds upon the NYU CTF baseline framework \cite{shao2024nyu} with upgraded prompts that describe agent-specific tasks and additional tools to define agent interactions. Function calling features of current LLMs are utilized to prompt for agent actions.  The system has three agents:
(1) the \textbf{Planner agent} generates the overall plan to solve the CTF challenge, delegating specific tasks to the Executor, and revising the plan based on Executor feedback;
(2) the \textbf{Executor agent} handles the execution of the task delegated by the Planner and returns the task execution summary to the Planner;
and (3) the \textbf{Auto-prompter agent} generates a prompt for solving the CTF challenge based on the initial environment exploration.

\subsection{Prompting and Context Management}

%\meet{Describe the basic agent prompting structure, what is sent to the LLM and what is returned.}
Each agent maintains a conversation history of LLM inputs and outputs.
The conversation starts with a system prompt that defines the role of the LLM of that agent, an initial prompt that describes the current task, and an alternating sequence of LLM actions (e.g., a command to run) and observations (e.g., output of the command).
Following the ReAct strategy, we tell the LLM to reason about each action, so these reason and action make an LLM message.
We utilize the function calling features of current LLMs to produce actions, so we do not define a structured format of our own, but instead rely on the LLM provider's API to parse the actions correctly.
At every iteration, the conversation history is sent to the LLM and it generates a message containing the reason and action. Observations from executing the actions are appended to the conversation history. This constitutes a ``round'' of conversation.
The LLM agent can continue these rounds of conversation as long as the LLM context is not full. To avoid context length issues, we use two techniques: first, we truncate observations to 25,000 characters; second, we optionally truncate the conversation history to the last 5 action-observation pairs while keeping the reason part intact, similar to \cite{yang2023intercode, yang2024sweagent, abramovich2024enigma}. The second technique is only applied to the Executor, as it has been observed to help execute long tasks.

\subsection{Tools}

Each agent interacts with the same Linux container environment where the challenge files present. The container's network has access to the challenge server and the internet to install new packages.
The agents contain only a basic set of tools to interact with the environment:
\texttt{RunCommand} to execute shell commands in the container;
\texttt{CreateFile} to create a file or script in the container;
\texttt{Disassemble} and \texttt{Decompile} to trigger Ghidra to reverse engineer a binary and obtain its disassembly or decompilation;
\texttt{SubmitFlag} to submit a flag and get it checked;
and, \texttt{Giveup} to giveup on solving the challenge.
Unlike EnIGMA, we do not implement advanced interfaces or interactive tools.
The specialized reverse engineering tools offer the agents access to Ghidra which does not provide a direct command line interface.
Specialized tools for other categories, like \texttt{RsaCtfTool} for cryptography or \texttt{nikto} for web, are mentioned in the category-specific initial prompt because they can be run from the command line via \texttt{RunCommand}.
Apart from the above tools, we introduce special functions that define the interaction between agents:
\texttt{GeneratePrompt} for the Auto-prompter to generate a prompt for the Planner;
\texttt{Delegate} for the Planner to delegate a task to an Executor;
and, \texttt{FinishTask} for the Executor to return a task summary to the Planner.
% \meet{remove the subsection, see if more details needed here.} % These special interaction functions are elaborated later in Section~\ref{sec:interaction} \meet{is separate section needed?}.

% 1. Planner Agent: Responsible for generating a general plan for the challenges and then delegating specific subtasks to the Executor Agent.

% 2. Executor Agent: Handles the execution of specific subtasks and returns the summary of the results back to Planner Agent.

% 3. Auto-Prompting Agent: Initially explores the challenge to autonomously generate a prompt an LLM agent may use to solve the challenge.

% The multi-agent system's interactive feedback mechanism provides continuous monitoring and adjustment, allowing our system to retry or change the plan when subtasks fail. Through interactions between agents, such as \textit{Planner-Executor} and\textit{ Planner-Executor-AutoPrompter} systems, we attempt to evaluate if  multi-agents can provide improvements in solving complex challenges. 

\subsection{Workflow}

Figure~\ref{fig:workflow} shows the workflow of the D-CIPHER multi-agent system.
The framework first initiates the Auto-prompter agent. The Auto-prompter initially explores the challenge files and interacts with the challenge server or binary if available. After a few exploration turns, it generates a detailed and specific prompt based on its exploration to solve the challenge and calls the \texttt{GeneratePrompt} tool.
The framework terminates the Auto-prompter and initiates the Planner with this generated prompt.

The Planner is instructed to explore the challenge similar to the Auto-prompter for a few turns. The Planner comes up with the plan and delegates a task to the Executor by calling the \texttt{Delegate} tool. The framework pauses the Planner, and initiates an Executor with this task.
The Executor tries to complete the task by running appropriate commands via \texttt{RunCommand} and creating files and scripts via \texttt{CreateFile}.
After the Executor finishes  or when it cannot proceed any further, it calls \texttt{FinishTask} with task execution and results summary. The framework terminates the Executor, and returns the task summary to the Planner as a result of  \texttt{Delegate} call.

The Planner continues to revise its plan and delegate further tasks.
For each \texttt{Delegate} call, the framework initiates a new Executor with a new conversation history.
Each Executor focuses only on it's own task, and the Planner only sees the task summary, allowing for efficient context management of the LLM.
This workflow ensures continuous interaction between the Planner and Executors such that they coordinate to enhance collective problem-solving.

% \meet{re-iterate in conclusion and future work}
The framework and the special interaction functions allow versatility to configure different types of multi-agent systems.
For example, a simpler system without the Planner can have an Auto-prompter generate a prompt and a single Executor solve the challenge end-to-end.
This is implemented for the ablation study in Section~\ref{sec:ablation}. 
Such configurations demonstrate the framework's flexibility to build diverse systems for different problems.
% New interactions can easily be defined which may allow, for instance, two Executors to run simultaneously and coordinate to solve the challenge.
%

\subsection{Planner Agent}

The Planner is the central agent in D-CIPHER, responsible for solving the entire CTF challenge. 
The system prompt defines it's role as a planner in a planner-executor system and instructs to generate a plan and delegate tasks one by one.
The initial prompt is set as the prompt generated by the Auto-prompter, however the Planner is not made aware of the Auto-Prompter in the system prompt.
Only the Planner is allowed to call the \texttt{SubmitFlag} or \texttt{Giveup} tools to terminate the challenge.
The Planner is given access to the \texttt{RunCommand} tool but not the \texttt{CreateFile}, \texttt{Disassemble}, or \texttt{Decompile} tools.
This allows the Planner to explore the challenge, but prevents it from trying to solve the challenge by itself.

The following conditions stop the Planner and terminate the challenge:
\texttt{SubmitFlag} is called with the correct flag;
\texttt{Giveup} is called;
the Planner conversation has exceeded the maximum number of rounds;
or all agents combined have exceeded the maximum cost budget of LLM API calls.
If a wrong flag is submitted, a negative response is returned and the Planner may continue.

Every prompt to the Planner LLM contains the entire conversation history, with the intermediate plans, delegated tasks, and returned task summaries.
Only the task summary returned by the executor is added to the Planner's conversation history. 
This allows the Planner to view the entire challenge progress to revise the plan and delegate further tasks.
If an Executor fails to return a task summary, a warning is returned instead and the Planner can retry the same task with revised instructions.
Figure~\ref{fig:planner_conv} shows an example of the Planner solving the \texttt{1black0white} forensics challenge from NYU CTF Bench.
The Planner starts with two simple exploratory tasks of examining the challenge file, and the Executor returns with a concise summary of the file contents. Based on the summary, the Planner is able to reason about and delegate the next steps to convert the file and parse the flag.
This example shows how the Planner can iterate on the broader challenge while each Executor focuses on single tasks.
% \meet{describe the figure more.}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth, trim=30 30 30 30]{figures/1black0white_example.pdf}
    \caption{Snippets of the interaction between Planner and Executors for the \texttt{1black0white} forensics challenge.}
    \label{fig:planner_conv}
    % \vspace{-10pt}
\end{figure*}
% In some cases, the Planner may try to solve the task by itself.


% The core configuration of our system is the \textit{Planner-Executor} framework.  It begins with the initialization of a Planner Agent, which is provided the description and source files of the CTF challenge. The Planner Agent operates within a dockerized environment where it may execute commands to analyze the challenge context and generate a general plan. When the Planner Agent invokes the \textit{DelegateTool} it indicates the request to initiate a subtask. A description of the subtask  is then  passed to the Executor Agent with some challenge related description; the Executor Agent then invokes appropriate tools within the docker container to execute the task. When the Executor Agent finishes the task (or reaches the maximum number of rounds) it  calls the \textit{FinishTaskTool} to return the summary to the Planner Agent. If the returned summary does not resolve the challenge, the Planner Agent will retry this subtask, or refine its general plan to try an alternative approach.

% This workflow ensures continuous information passing interactions between the Planner Agent and Executor Agent. With the Planner Agent managing subtasks delegation and the Executor Agent tackling assigned subtasks, this setup creates a reasonably solid foundation for navigating CTF challenges, such that multiple agents coordinate to solve subtasks, share final findings, and enhance collective problem-solving.

% The system is also designed to extend beyond the basic \textit{Planner-Executor} framework to include more advanced configurations. For example, the integration of the Auto-Prompting Agent allows the system to autonomously generate prompts that may serve as a heuristic guide for both the Planner and Single Agents.  Another advanced configuration involves a collaborative \textit{Executor-Executor} framework, where two Executor Agents coordinate to solve subtasks, share intermediate findings, and improve overall problem-solving capabilities. These configurations demonstrate the flexibility of the framework and its potential ability to handle diverse and complex challenges.

% \subsection{Interaction Between Agents} \label{sec:interaction}


% Effective interaction between agents is an important factor of our system. The \textit{DelegateTool} and \textit{FinishTaskTool} enable structured interactions between agents to transitions between
% task planning and execution. Additionally, the systemâ€™s logging function captures all interactions, generating conversational logs useful in analyzing overall performance and interaction effectiveness, while enabling future optimizations. 

% The loop structure of the \textit{Planner-Executor }workflow ensures that the Planner Agent monitors progress at each stage. When subtasks fail, the system iteratively adjusts subtask descriptions, prompts, or the general plan in an effort to improve outcomes.

% The modularity of the system allows new interactions and agents to be easily integrated as well. This adaptability is crucial for upgrading the system to handle tasks of increasing difficulty and complexity.  

% \subsection{Planner Agent}

% The Planner Agent serves as the system's strategist. It processes the challenge's contextual details, including the descriptions, categories and source files. It may invoke \textit{RunCommandTool} to explore the challenge, performing actions such as file navigation and examination in order to establish a general plan. Once the plan is set the Planner Agent delegates subtasks to the Executor Agent through the \textit{DelegateTool}. The interaction provides a smooth transfer of subtask instructions and challenge descriptions, both of which are critical to the fucntion of Executor Agent.

% In each round, the Planner Agent sends the accumulated conversation history to the LLM for processing. The response from the LLM contains the next action or tool call needed to progress. If the response contains an error, the Planner Agent will raise an exception and log the error. Otherwise, the Agent will update the current cost to track resource usage and add the assistant's response message to the conversation for logging and feedback purposes.

% The Planner Agent then evaluates whether the LLM has requested a tool call. If not, the Planner Agent will generate a continuation message to prompt the LLM for further input. If a tool call is present, it will try to parse. If not parsed, the parsing errors are logged, and the Agent adds observation messages to the conversation to reflect the error state. Successfully parsed tool calls are handled based on their type.

% For \textit{DelegateTool} calls the Planner Agent assigns the subtask to an Executor Agent. Other tool calls are run directly within the environment, and results are logged and added to the conversation as observations. This ensures that the Planner Agent can control subtask execution and only delegate when necessary.

% The Planner Agent will stop if any of the following exit conditions are met:
% \begin{itemize}
%     \item \textbf{Submit a flag}: The hidden flag for this challenge has been discovered.
%     \item \textbf{Give up}: The Planner Agent decides to give up, believing it has hit a dead end. One such instance is when all available hints from the challenge's description and source files have been explored.
%     \item \textbf{Reaches the maximum rounds}: The maximum rounds of conversation with the LLMs have been reached.
%     \item \textbf{Exceeds the maximum cost}: The budget for API calls for the Agent has been exceeded.
% \end{itemize}



% 1. Submit a flag: The hidden flag for this challenge has been discovered.

% 2. Give up: The Planner Agent decides to give up, believing it has hit a dead end. Once such instance is when all available hints from the challenge's description and source files have been explored

% 3. Reaches the maximum rounds: Reaches the maximum rounds of conversation with the LLMs.

% 4. Exceeds the maximum cost: The budget for API calls for the Agent has been exceeded.

% The Planner Agent's role is not only delegation, but also management of subtask execution while receiving and responding to a subtask's summary. If an Executor Agent fails to give a meaningful summary under specified conditions, the Planner will either build another Executor Agent to retry the same subtask with revised instruction, or try to solve this subtask by itself by calling appropriate tools. This feedback loop makes sure that the execution process is robust and maintains the flexibility for new types of challenges. Additionally, the Planner iteratively evaluates progress in each step and assigns subsequent subtasks based on the result of the last step. 

\subsection{Executor Agent}

% The Executor has access to all tools but only the \texttt{FinishTask} special function.
The system prompt defines the role as an executor in a planner-executor system and instructs to execute the task delegated by the planner.
The initial prompt uses a hard-coded template where challenge details and the delegated task instructions are filled in.
The template is tailored for each category and recommends usage of some category-specific command line tools.

The following conditions stop the Executor and resume the Planner:
\texttt{FinishTask} is called with a task summary;
the Executor conversation has exceeded the maximum number of rounds;
or, all agents combined have exceeded the maximum cost budget of LLM API calls.
If the Executor exhausted the maximum number of rounds without finishing the task, it is prompted one last time to call \texttt{FinishTask}.
If \texttt{FinishTask} is still not called, a warning message is returned to the Planner.

Every prompt to the Executor LLM contains a truncated conversation history that only shows the last few actions and observations.
The system and initial prompt with challenge and task details are always sent, along with any intermediate thoughts of the Executor.
Truncating to the last few observations prevents the LLM context from filling up and maintains the Executor's focus on the current task.
Every Executor instantiation by the framework starts with a fresh conversation history.
No part of the previous Executors' conversation is included, except few details which the Planner may include in the task description.
This also helps maintain the Executor's focus on the current task, and allows for heterogeneous execution with different tasks.

% The Executor Agent specializes in executing subtasks assigned by the Planner Agent. Once it receives a subtask instruction and challenge description through the \textit{DelegateTool}, the Executor Agent will use tools such as \textit{RunCommand}, \textit{Disassemble}, \textit{Decompile}, and \textit{CreateFile} Tools in the environment to analyze files, execute commands and investigate contexts relevant to the challenge. If the response from LLM contains an error, the Executor Agent marks the subtask as finished, and logs the error. The Executor Agent's workflow is quite similar to the Planner Agent, with the key difference being it does not call \textit{DelegateTool}, but calls \textit{FinishTaskTool}. Then the Executor Agent marks the subtask as complete, all the results are saved for the logging, and summarized. The finish summary is then sent back to the Planner Agent, helping it to decide next steps. 

% Key features of the Executor Agent are scalability and resource efficiency. The agent maintains a truncated context of the last five observations for optimized LLM communication. Doing so prevents a "context length exceeds" error, while still maintaining a complete log for future analysis purposes. When subtasks results do not provide a valid solution within maximum rounds of conversation, the Planner Agent may assign this subtask to another Executor Agent or change its approach. This iterative workflow enhances the substask's resolution. Theoretical robustness of the Executor Agent lies in its modularity, since each subtask is handled independently, ensuring minimal interference between different operations. 

% The Executor Agent will stop if any of the following exit conditions are met:
% \begin{itemize}
%     \item \textbf{Finished}: The subtask assigned has been completed.
%     \item \textbf{Reaches the maximum rounds}: The maximum rounds of conversation with the LLMs have been reached.
%     \item \textbf{Exceeds the maximum cost}: The budget for API calls for the Agent has been exceeded.
% \end{itemize}

% 1. Finished: The subtask assigned has been finished

% 2. Reaches the maximum rounds: Reaches the maximum rounds of conversation with the LLMs.

% 3. Exceeds the maximum cost: The budget for API calls for the Agent has been exceeds.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/autoprompt_small_vert.pdf}
    \caption{The auto-prompter generated prompt vs.hard-coded template for the \texttt{1black0white}  challenge.}
    \label{fig:autoprompt_conv}
\end{figure} 

\subsection{Auto-prompter Agent}

The Auto-prompter's system prompt defines it's role as an agent tasked with generating a prompt to solve the CTF challenge.
The initial prompt contains details of the challenge files and server.
The Auto-prompter is not made aware of the planner-executor system, allowing it to generate a versatile prompt for any solver system.

The Auto-prompter halts under the following conditions: when the \texttt{GeneratePrompt} function is called, the conversation exceeds the maximum number of rounds, or the combined cost of LLM API calls by all agents surpasses the allocated budget. Similar to the Executor, if the Auto-prompter runs out of rounds, it is given one final prompt to call \texttt{GeneratePrompt}. If \texttt{GeneratePrompt} is still not invoked, a predefined prompt template is used with the challenge details appropriately filled in.

Figure~\ref{fig:autoprompt_conv} shows an example of the generated auto-prompt, compared with the hard-coded prompt template for the \texttt{1black0white} challenge.
Along with a highly-relevant challenge description, the Auto-prompter also proposes an approach for the problem based on it's initial exploration.
On the other hand, the hard-coded templates (more examples in Appendix~\ref{sec:hard_coded_prompts}) can only provide generic tips and directions but cannot be tailored to the challenge. 
% This also shows the benefit of implementing the Auto-prompter agent as opposed to directly using an LLM once to generate an auto-prompt.



% The following conditions stop the Auto-prompter:
% \texttt{GeneratePrompt} is called;
% the Auto-prompter conversation has exceeded the maximum number of rounds;
% or, all agents combined have exceeded the maximum cost budget of LLM API calls.
% Similar to the Executor, if the Auto-prompter exhausted the rounds, it is prompted one last time to call \texttt{GeneratePrompt}.
% If \texttt{GeneratePrompt} is still not called, the hard-coded prompt template is used with challenge details filled in.

% The Auto Prompting Agent is a specialized component in the multi-agent framework that can generate and rewrite prompts that may be of use to the Planner Agent. The workflow for this agent is shown in Figure~\ref{fig:workflow}. This agent preliminary explores the challenge in the capacity of an 'AI assistant' to automate a prompt for solving the challenge. The Auto Prompting Agent invokes the \textit{GenAutoPromptTool} and \textit{FinishTaskTool}. If the \textit{GenAutoPromptTool} is called, the Auto Prompting Agent generates an autonomous prompt, stores it, and marks the subtask as finished.  This new prompt is then sent back to the Planner Agent to create more effective planning instructions.  A comparison of the \textit{Planner-Executor} system running with and without an autonomous prompt is provided in Figure \ref{fig:autoprompt_conv}.  (\textit{We also explore the effect of autonomous prompting without the Planner system in a further ablation study in section \ref{ablation_study_auto_planner} }).

% % Unlike human drafted prompts, the Auto Prompting Agent operates as an integrated function within the \textit{Planner-Executor }framework. This process allows us to explore if an autonomous prompt heuristic can further enhance the Planner Agent's communication with the LLMs. As an additional advantage the Auto Prompting Agent minimizes human behaviors in the prompt design,  reducing potential sources of error. 

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{figures/autoprompt_small_vert.pdf}
%     \caption{Comparison of the auto-prompter generated prompt with the hard-coded template for the \texttt{1black0white} forensics challenge.}
%     \label{fig:autoprompt_conv}
% \end{figure} 

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/auto_workflow.png}
%     \caption{Workflow of the Planner-Executor System. The Planner Agent delegates tasks to the Executor Agent, which returns summary of results. \together{TODO: Refine this workflow diagram}}
%     \label{fig:auto_workflow}
% \end{figure}
