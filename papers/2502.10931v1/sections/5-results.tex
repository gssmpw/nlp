
\input{figures/default_experiment}

\section{Results} \label{sec:results}



% \minghao{Meet and Charan: Please go over this and do proper cutoff and modification}

\subsection{Comparison of \textit{\% solved}} \label{sec:accuracy}

Table~\ref{tab:default_experiment} compares the performance of D-CIPHER with other LLM agents across multiple LLMs and benchmarks.
We run D-CIPHER with five different LLMs, using the same LLM for Planner, Executor, and Auto-prompter in each run.
We also rerun the NYU CTF baseline agent with three LLMs to measure the impact of recent updates to the LLM models on NYU CTF Bench.
The EnIGMA \textit{\% solved} and \textit{\$ cost} are taken from \cite{abramovich2024enigma}, while the category-wise results on NYU CTF Bench are computed from their provided transcripts.
We do not compare with the Cybench baseline agent \cite{zhang2024cybenchframeworkevaluatingcybersecurity} as EnIGMA is the state-of-the-art on Cybench. 

D-CIPHER with Claude 3.5 Sonnet consistently outperforms the current state-of-the-art EnIGMA, achieving 19.0\% over 13.5\% on NYU CTF Bench, 22.5\% over 20\% on Cybench, and 44\% over 26\% on HackTheBox.
D-CIPHER with GPT 4o also outperforms EnIGMA with GPT 4o on NYU CTF Bench, while getting a close result on Cybench and HackTheBox. 
The rerun results of NYU CTF baseline show that recent LLM models have improved on cybersecurity tasks, getting close to EnIGMA's state-of-the-art performance. Yet, D-CIPHER consistently beats the baseline on NYU CTF Bench in overall \textit{\% solved} and across all categories for both Claude 3.5 Sonnet and GPT 4o.
These results indicate that D-CIPHER improves capabilities across multiple LLM architectures, and the  higher performance stems not only from recent LLM updates but also from it's multi-agent system architecture.
Interestingly, D-CIPHER without Auto-prompter with Claude 3.5 Sonnet achieves the highest performance of 22\% on NYU CTF Bench. However, performance without the Auto-prompter worsens on GPT 4o and on other benchmarks, while average cost increases, indicating that having the Auto-prompter helps overall (see Section~\ref{sec:ablation}).

%Looking at the category-wise results on NYU CTF Bench, we see that D-CIPHER 
%\charan{Meet: Restructured the old sentence for better clarity -- review it again}
% D-CIPHER exceeds EnIGMA for every category except pwn, where performance in crypto doubles from 7.7\% to 15.4\%. 
D-CIPHER's performance improvement stays consistent across the CTF categories.
D-CIPHER outperforms EnIGMA across all categories except pwn, with a notable improvement in crypto, where its performance doubles from 7.7\% to 15.4\%. 
Likewise, on rev, and misc, we see 9\% to 12\% increase. The improvement is due to the enhanced task decomposition and execution ability of the Planner-Executor system. Especially, crypto and rev frequently have long outputs of disassembled binaries or encrypted files that require  multiple analysis steps that are effectively decomposed by the Planner and performed by the Executor.
Figure~\ref{fig:success_radar}(a) plots the \textit{\% solved} of D-CIPHER across categories on NYU CTF Bench. D-CIPHER's performance is more balanced across different LLMs, demonstrating that our framework operates well with different reasoning capabilities of the LLMs. 
While D-CIPHER improves in web over previous results, the performance still lags behind other categories, pointing to a common limitation in how web challenges are addressed. See Appendix \ref{sec:compare_llms} for analysis on how different LLMs behave on the challenge \texttt{target\_practice}.


%agentic systems for offensive security against our system, D-CIPHER, for both overall and category-wised accuracy, on the three benchmarks we evaluated: NYU CTF \cite{shao2024nyu}, Cybench \cite{zhang2024cybenchframeworkevaluatingcybersecurity} and HackTheBox \cite{hackthebox}. To ensure a fair comparison, both experiments use a pass@1 setup.

% Across all the benchmarks, Claude 3.5 Sonnet consistently achieves higher accuracy scores on all the three benchmarks, and it shows superior performance with D-CIPHER with an overall accuracy on the NYU CTF Bench of 19.5\%, compared to 9.5\% on GPT-4o, 7.0\% on GPT-4 Turbo, 3.5\% on LLaMa 3.1 405B and 3.0\% on Gemini 1.5 Flash. Claude 3.5 Sonnet also outperformed GPT-4o on Cybench and HackTheBox with D-CIPHER, obtaining 7.5\% and and 28\% higher accuracy respectively compared to GPT-4o. With category specific tasks, Claude 3.5 Sonnet on D-CIPHER also outperformed other models we tested on all six categories covered in NYU CTF Bench. It's   highest success rate was 29.41\% on Reverse Engineering, and lowest success rate was 5.26\% on Web challenges.

% Compared vertically with other agentic systems, D-CIPHER shows improved performance in terms of overall problem-solving and planning ability. It achieves an overall success rate of 19.5\% in all six categories when using Claude 3.5 Sonnet, as compared to 13.50\% for EnIGMA, which is the current state-of-the-art approach. GPT-4o and GPT-4 Turbo achieve similar performance - about 9. 5\% and 7\% - on both systems. These experimental results show the strengths of our system.

% Regarding the task-specific break down, D-CIPHER outperformed EniGMA on 5 out of the 6 categories covered by NYU CTF Bench. In the crypto category, D-CIPHER's solved percentage on Claude 3.5 Sonnet nearly doubled, from 7.69\% to 15.38\%. Likewise, on the forensics, reverse and miscellaneous challenges, D-CIPHER on Claude 3.5 Sonnet showed huge improvement compared to EniGMA, with increased accuracy rates of 6\%, 12\% and 8\% respectively. In the Pwn category, which requires advanced tool usage and interaction approaches, EniGMA is still the current best approach and outperformed D-CIPHER with around 5\% higher success rate. This improvement can be attributed to enhanced task decomposition and execution ability in our multi-agent system, especially on the Crypto and Reverse categories, which may have frequent long context inference situations due to the decompiled and disassembled contents and encrypted and decrypted sequences in the conversation loop.
% D-CIPHER performed equally well in the forensics category, and showed around a 7\% improvement on GPT-4o. Lastly,  Claude 3.5 Sonnet also performed better in the misc category. 


% Not all the results for D-CIPHER were positive. The pwn category when run on Claude 3.5 Sonnet shows a slight decrease. This indicates there is still room for improvement in how D-CIPHER handles complex binary exploitation tasks. For the reversing category, even though D-CIPHER performs poorer on Claude 3.5 Sonnet, it still shows a more balanced performance when compared to EnIGMA on different models. 


% The significant gains in categories like crypto and forensics highlight the advantages of our multi-agent approach, which allows task decomposition and execution. Although the individual performance results for pwn, rev and misc do not show an impressive improvement, D-CIPHER sets a new benchmark in overall performance. It clearlydemonstrate C-CIPHER'S  improvements over the previous state-of-the-art.


\begin{figure}[tpb]
    \centering
   \includegraphics[width=\linewidth]{figures/success_by_cat_radar.pdf}
    \caption{D-CIPHER  by category on NYU CTF Bench.}
    \label{fig:success_radar}
\end{figure}

% Figure~\ref{fig:success_radar} shows the success rate on D-CIPHER across all six categories on NYU CTF Bench with the 5 models examined, revealing the model and system's weaknesses and strengths from a category-wised point of view. As Mentioned above, Claude 3.5 Sonnet clearly dominates in most categories, but is outperformed by GPT-4o in the Web category. 
% Models such as LLaMA 3.1 405B and Gemini 1.5 Flash exhibits more limited performance as their success ratios cluster to the center when compared to the Claude and two GPT models tested. One reason is that these models struggle with tool usage, which will be discussed in the \ref{sec:failure} section.
% As to the capability comparison across different categories, Claude 3.5 Sonnet also demonstrates the most balanced performance across all categories, making it a reliable choice for solving CTF challenges in general purpose applications. Other models, such as Gemini 1.5 Flash, GPT-4o and GPT-4 Turbo, show slightly better performance in isolated categories, like Forensics and Miscellaneous, but fail to generalize across other domains, such as Crypto and Pwn. Across all six categories on all 5 models tested, D-CIPHER is more capable on Forensics and Miscellaneous challenges, as all 5 models show improved results on these two categories over other types of challenges.

% \input{figures/categorywised_results}

\subsection{Comparison of \textit{\$ cost}}
\label{sec:cost_analysis}
% \input{figures/overall_cost_compared_enigma}

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/avg_cost_radar.png}
%     \label{fig:avg_cost_radar}
%     \caption{Category-wise Avg Cost of D-CIPHER and EnIGMA on NYU CTF}
%     % \vspace{-4mm}
% \end{figure}

% \charan{Comment- Increase font and redraw plots to avoid legend overlap for Fig6 and Fig 7; title In the Figure 5 to "Category-wise"}

Table~\ref{tab:default_experiment} compares average \textit{\$ cost} of solved challenges with EnIGMA across the three benchmarks.
Except for Claude 3.5 Sonnet on NYU CTF Bench, D-CIPHER has  a lower average cost across all LLMs and benchmarks. With GPT 4o and GPT 4 Turbo, D-CIPHER lowers the cost by $2\times$ to $10\times$ across benchmarks while solving more challenges.
Despite having multiple agents, a significant cost reduction indicates that bifurcation of responsibilities between agents makes the problem-solving system more efficient in terms of computation costs.
Figure~\ref{fig:success_radar}(b) plots the category-wise \textit{\$ cost} of D-CIPHER on NYU CTF Bench.
GPT 4o is most cost efficient across categories, while Claude 3.5 Sonnet is moderately higher on forensics, pwn, and rev. GPT 4 Turbo is the costliest among the three LLMs, for forensics, pwn and web, while on other categories has a lower cost but also solves less challenges. Among the categories, crypto has higher cost across LLMs as it may require analysis of long encrypted texts, and many iterations for decryption. Refer to Appendix~\ref{sec:appendix_cost} for detailed category-wise tables.

%sec:cost_analysis

% The radar chart highlights in cost efficiency between different categories and model configurations, including Claude 3.5 Sonnet, GPT-4o and GPT-4 Turbo. D-CIPHER using Claude 3.5 Sonnet demonstrates moderate cost efficiency across most categories, with notable cost efficiency on Web and Misc, where the costs are among the lowest. However, in needed categories like Reverse and Crypto that  require highly computationally resources, the average cost is higher than GPT-4o and GPT-4 Turbo. GPT-4o achieves optimal cost efficiency across most categories, like Forensics and Reverse. However, its cost is higher compared to Claude 3.5 Sonnet in Web challenges, which always require extensive investigation of file systems, URLs, or interactions with APIs. GPT-4o might generate more exploratory commands or redundant step that can increase the cost. D-CIPHER with GPT-4 Turbo has highly variable cost efficiency. In Crypto, Reverse, and Misc, it cost less than most other configurations, but reaches the highest cost in Pwn and Forensics, where analyze complex task and process large logs are needed. Overall on D-CIPHER, the Forensics and Web always cost more, due to their needs of processing large datasets, engage in complex reasoning steps, or handling diverse tasks and integrating external context effectively.

% EnIGMA with Claude 3.5 Sonnet exhibits lower average costs across most categories when compared to the D-CIPHER results. This indicates that while D-CIPHER can solve more challenges, the cost of resources is less efficient. EnIGMA with GPT-4o is more costly in all categories when compared to D-CIPHER. This huge increase is more obvious in Crypto, Misc and Web, indicating that D-CIPHER outperforms EnIGMA in cost efficiency for computationally intensive tasks. EnIGMA using GPT-4 Turbo follows a similar trend, showing higher costs in most categories compared to D-CIPHER. The biggest differences were in categories like Reverse and Crypto. This demonstrates D-CIPHER’s advantage in cost efficiency on most model configurations.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/cost_comparison.pdf}
%     \label{fig:cost_compare_bar}
%     \caption{Comparison of average cost of solved challenges and overall average cost of D-CIPHER and EnIGMA on NYU CTF Bench.}
%     % \vspace{-4mm}
% \end{figure}

% We also estimated the average cost among all challenges, called "overall avg cost," with average cost only on successfully solved challenges, called "success avg cost", as shown in Figure ~\ref{fig:cost_compare_bar}. We can observe that the overall cost is always higher than the success cost since the failed challenges always take more rounds to finish. For those overall average costs, we can discover that D-CIPHER demonstrates a significant reduction in overall average costs across all models when compared to EnIGMA. For example, on GPT 4o, the overall cost decreases from more than \$2.50 to around \$1.00, representing a reduction of more than 60\%.
% Similarly, for GPT-4 Turbo, the overall cost decreases from roughly \$2.75 to \$1.50. These results clearly show that our multi-agent system not only enhanced the performance of solving challenges, but also reduced the required cost. Since we have a new exit reason compared to EnIGMA, which is giveup, if the model believes it could not solve this challenge, it will directly giveup instead of continuing to work until it reaches the maximum rounds or maximum cost. For success average costs, the Claude 3.5 Sonnet, even though D-CIPHER has a higher cost--around \$0.5 compared with EnIGMA, which is about \$0.4--it might have more interactions within the system compared to single agent EnIGMA due to our multi-agent system. And, the higher solved percentage also indicates the D-CIPHER might solve more complex challenges which need more tokens to analyze. In GPT-4o, D-CIPHER succeeds at a much lower costcompared to EnIGMA, indicating higher cost efficiency for successfully solved tasks. Additionally, GPT-4 Turbo also shows a decent reduction in success cost under D-CIPHER. These findings highlight that our iterations within multi agent system and task decompositions do lead to higher performance at a reduced cost on most model configurations.

\subsection{Impact of different configurations}

% We evaluated the performance of D-CIPHER using strong-weak model combinations across various agent roles to examine the impact of pairing a strong planner with a weaker executor. To ensure a strong-weak model combination, all our experiments paired models from the same LLM family but from different performance tiers, as defined by their respective providers. 



% We evaluated the performance of D-CIPHER using strong-weak model combinations across various agent roles to assess the impact of pairing a strong planner with a weaker executor. To ensure a strong-weak model combination, all experiments utilized models from the same backend, but from different tiers, as categorized by their respective websites. Table \ref{tab:mix_model} presents the results of these combinations on the NYU CTF Bench Test Set \cite{shao2024nyu}, demonstrating that such pairings consistently yield poor outcomes. For example, the combination of Claude 3.5 Sonnet and Haiku solved only 7.0\% of tasks, a 12.5\% decrease compared to the default setup. Similarly, GPT-4o and GPT-4 Turbo paired with GPT-4o-mini experienced success rate reductions of 4\% and 2\%, respectively. Meanwhile, LLaMA 3.2 405B, when combined with LLaMA 3.3 70B, failed to solve any challenges. Interestingly, Gemini showed more robustness against the negative impact of weaker models, maintaining the same success rate as the default setup. These findings suggest that in multi-agent systems, models with comparable capabilities produce better results than combinations involving significant capability gaps.

\subsubsection{Ablation Study}
\label{sec:ablation}
%\meet{Refer table 3 instead.}

We run D-CIPHER without the Auto-prompter and without the Planner to observe their impact on the system.
Without the Auto-prompter, the hard-coded prompt template is used for the Planner's initial prompt.
Without the Planner, a single Executor is run with the prompt generated by the Auto-prompter.

Table~\ref{tab:default_experiment} shows the results for these two configurations.
D-CIPHER without Auto-prompter with Claude 3.5 Sonnet gets a 3\% improvement in challenges solved on NYU CTF Bench, but it's performance drops with GPT 4o on NYU CTF Bench and Claude 3.5 Sonnet on Cybench, showing that the Auto-prompter improves performance in most cases.
The contrasting result with Claude 3.5 Sonnet on NYU CTF Bench is due to the pwn category, where performance increases by more than $2\times$, while other categories get matching or lower results.
This is discussed in more detail in Section~\ref{sec:autoprompter_casestudy}. %\meet{add ref to section where this is discussed.}
Without the Auto-prompter, average cost increases across LLMs and benchmarks, indicating that the Auto-prompter improves system efficiency without compromising performance in most cases.

D-CIPHER without Planner sees a 1\% to 5\% drop in performance on NYU CTF Bench across both LLMs. The performance is consistently lower across all categories.
This highlights the benefit of the Planner-Executor system in solving CTF challenges.
While the average cost is $2\times$ lower without the Planner, the drop in performance is significant.
Whereas, the total cost of a Planner and multiple Executors is only $2\times$ higher than a single Executor, indicating that each individual agent is more efficient. Refer to Appendix~ \ref{sec:impact_planner} for conversation examples with and without the Planner.

% Additionally, without the Auto-Prompter nicreased the average cost to \$0.74, compared to the default \$0.53. For GPT-4o, both components contributed to performance enhancements, with the Planner and Auto-Prompter increasing the success rate both by 1\%.
% These findings demonstrate the critical role of the Planner and Auto-Prompter components in optimizing D-CIPHER's performance and cost-efficiency.

% Table \ref{tab:ablation_study} summarizes the performance of D-CIPHER under various ablation settings on the NYU CTF Bench \cite{shao2024nyu}, examining the effects of removing either the autoprompter or planner. Results show that Claude 3.5 achieved a significant 5\% improvement in success rates with the planner, though its accuracy did not benefit from the autoprompter. However, the autoprompter did reduce the average cost to \$0.16, compared to the default cost of \$0.53. For GPT-4o, both the planner and autoprompter contributed to improvements in success rate, with increases of 1\% and 0.5\%, respectively. These findings highlight the effectiveness and efficiency of the optimizations introduced by the planner and autoprompter features in D-CIPHER.

% \input{figures/ablation_study}

\subsubsection{Combination of stronger and weaker LLMs}

\input{figures/mix_experiment}

We test the combination of stronger LLMs as Planner with weaker LLMs as Executor.
We paired LLMs from the same family, but different capability tiers as indicated by their respective providers.
Table~\ref{tab:mix_model} shows the performance of the strong-weak combinations on NYU CTF Bench. The results show consistent underperformance when weaker models are substituted for the Executor. For instance, Claude 3.5 Sonnet with Haiku solved only 13.0\% of challenges, with a 6.0\% drop compared to Claude 3.5 Sonnet with Sonnet. Similarly, GPT-4o and GPT-4 Turbo, when paired with GPT-4o-mini, showed reductions of 4\% and 1\%, respectively. LLaMA 3.1 405B combined with LLaMA 3.3 70B failed to solve any challenges. Notably, Gemini maintained similar performance with the weaker. These results indicate that the D-CIPHER architecture achieves best performance on CTF challenges when models of comparable capabilities are paired.

\subsubsection{Impact of temperature}
D-CIPHER with GPT 4o is evaluated under a lower temperature setting of $T=0.95$,  with results in Table~\ref{tab:temperature}. Decreasing the temperature show consistent drop across crypto, pwn, and rev with no improvements in forensics, web, or misc.
Higher temperature is better for creative and generative capabilities, and  those capabilities help with problem-solving.

% Table \ref{tab:temperature} presents the performance of GPT-4o under two temperature settings, T=0.95 and T=1.0, on the NYU CTF Bench Test Set. Notably, the higher temperature (T=1.0) provides modest yet consistent improvements in the cry, pwn, and rev categories, increasing success rates from 3.85\% to 5.77\%, 5.13\% to 7.69\%, and 11.76\% to 13.73\%, respectively. Meanwhile, for, web, and misc remain unaffected, displaying identical results in both settings (20.0\%, 10.53\%, and 16.67\%). These gains culminate in an overall average improvement of 1.5\% (from 9.5\% to 11.0\%), suggesting that elevated temperature values can introduce beneficial variability for certain tasks, particularly those requiring more exploratory or “creative” outputs. However, the unchanged performance in the for, web, and misc categories underscores the possibility that specific task types may be less sensitive to temperature adjustments, highlighting the heterogeneous nature of temperature’s impact across different challenge domains.

\input{figures/temperature}


\subsection{Analysis}
\label{sec:analysis}

% \charan{Todo- Fig 6 is not refereed in the text-- review it}

\subsubsection{Exit Reasons}
We analyze the challenge termination (exit) reasons of D-CIPHER on NYU CTF Bench.
Exit reasons are of five types: ``Solved'' when the challenge is solved, ``Giveup'' when the Planner gives up, ``Max cost'' when the cost budget is exceeded, ``Max rounds'' when the Planner conversation rounds are exhausted, and ``Error'' when the run terminates with an error.

Figure~\ref{fig:exit_reason} shows the percentage of each exit reasons with different LLMs across all categories, along with the total plot.
For Claude 3.5 Sonnet, max cost is the most dominant exit reason. Comparatively, other LLMs have giveup as the most dominant reason.
This indicates that Claude 3.5 Sonnet has less propensity to giveup and continue with the challenge till the cost is exhausted.
Max rounds are exhausted for very few challenges, except for LLaMa 3.1 405B which faces problems in function calling and needs many retries.
We also see higher errors with LLaMa 3.1 405B that are attributed to it hallucinating function calls (see Section~\ref{sec:other_failure}), indicating that this model is not capable of operating as an agent and in multi-agent settings. 
Distribution of exit reasons for GPT 4o and GPT 4 Turbo is similar across categories which shows the holistic capabilities of these models.
Claude 3.5 Sonnet sees a high giveup percentage on web challenges, highlighting a gap in capabilities. It's lower performance on web re-iterates that observation. Refer to Appendix~\ref{sec:appendix_failure} for detailed failure statistics.

%significantly higher than other LLMs, indicating
% The LLaMA 3.1 405B model has the highest planner rounds percentage, meaning the model exceeds the limitation of maximum rounds of the Planner Agent. 
% This indicates the model does not have a high efficiency and may struggle with complex, multi-step challenges. 
% The GPT-4 Turbo and Gemini 1.5 Flash both have a high ratio of giveup exits, which implies that the models often stop challenges early. This may be due to limited reasoning ability or a lack of understanding of the challenges' requirements.
% The GPT-4o model also shows a promising success rate, even though it is lower than that of Claude 3.5. Its lower cost makes it a viable  alternative in resource limited scenarios. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/exit_reason_by_model.pdf}
    \caption{\%  of each exit reason per category per model.}
    \label{fig:exit_reason}
\end{figure}

% \subsubsection{Failures caused by system constraints}
% \label{sec:common_failure}
% We defined 2 types of system constraints failures in our experiment setup, which are also two constraints in the experimental setup described in \ref{sec:metrics}. Figure 4 compares failure ratios across different models, including: Claude 3.5 Sonnet, GPT-4 Turbo, GPT-4o, Gemini 1.5 Flash, and LLaMA 3.1 405B, under the maximum budget  constraints.

% Claude 3.5 Sonnet demonstrates significantly high ratios across almost all categories, making it the least cost-efficient model in this analysis. Crypto, Reverse, and Forensics categories show significant cost limit failures. This suggests that while the model is powerful problem-solving tool, it usually exceeds cost limits in computationally demanding challenges. GPT-4 Turbo and Gemini 1.5 Flash demonstrate lower ratios, showcasing better cost efficiency. They are especially efficient in Misc, Web, and Reverse, where reasoning and analysis are needed. However, their ratio slightly increases in Pwn challenges, indicating these models need more resources to conduct binary exploitation tasks. GPT-4o has moderate ratios in categories like Forensics, Misc, and Reverse, reflecting its cost efficiency. These results indicate that, while GPT-4o performs well for challenges that require logic reasoning, it faces cost inefficiencies for binary exploitation tasks. LLaMa 3.1 405B achieves zero ratios across all categories, making it the most cost-efficient model. This highlights its lightweight nature, which allows it to complete challenges without exceeding the cost limit. However, the lowest ratio  shows its limited ability, since the model does not reach cost limits, indicating its inability to conduct complex reasoning.

% TODO need to add this somewhere?
% \subsubsection{Model Give up}
% Figure ~\ref{fig:exit_reason} and table \ref{tab:giveup} present the giveup ratio of all models on NYU CTF Bench. Claude 3.5 Sonnet has the lowest giveup ratio, showing its ability to continue analysis when handling complex challenges across all categories, especially in Forensics, around 13.33\%, and Reverse, about 13.73\%. GPT-4o and GPT-4 Turbo have the highest giveup ratio, in the Crypto, Web and Misc categories. Since they require specialized problem-solving or external context handling. This high giveup ratio leads to lower costs on GPT-4o and GPT-4 Turbo with D-CIPHER, since they stop challenges early that they believe can not be solved. LLaMa 3.1 405B shows more balanced giveup ratios across different categories, while the overall giveup ratio is still not low, demonstrating that we need a general improvement in problem-solving ability. Gemini 1.5 Flash, while also demonstrating a moderate giveup rate, struggles the most in Web challenges since the model decides to give up all of them. From these findings it is clear that we need to further improve our system in handling cryptography and web-based challenges. These challenges require not only computational resources, but also the ability to generalize and adapt effectively.

\subsubsection{Total conversation rounds}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\linewidth]{figures/rounds_hist.pdf}
    \caption{Histogram of successful and failed challenges by total conversation rounds for D-CIPHER on NYU CTF Bench.}
    \label{fig:conversation_rounds}
\end{figure}

We analyze the total conversation rounds, defined as the sum of conversation rounds of each agent in D-CIPHER.
Figure~\ref{fig:conversation_rounds} shows a histogram of the total conversation rounds by success and failure cases for Claude 3.5 Sonnet and GPT 4o models, with and without Auto-prompter.
Successful challenges take lesser rounds than failed challenges. This may indicate that D-CIPHER only solves easier challenges that require lesser rounds, but fails on longer harder challenges. This may also indicate that challenges are only solved when the correct path is found early enough, else the agents stray from the goal for many rounds before giving up.
Claude 3.5 Sonnet runs for more rounds compared to GPT 4o for both success and failure cases, re-iterating it's propensity to keep going and not give up.
This likely helps it solve challenges that take many rounds.
Comparing Claude 3.5 Sonnet with and without Auto-prompter, we see that the Auto-prompter helps solve the challenges faster, increasing efficiency.


\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/failures/fail_autoprompt.pdf}
    \caption{Auto-prompter fails to generate a prompt.}
    \label{fig:fail_autoprompt}
\end{figure}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/failures/fail_function_call.pdf}
    \caption{Function call with wrong syntax by LLaMa models.}
    \label{fig:fail_function_call}
\end{figure}

\subsubsection{Errors and mistakes in LLM outputs}
\label{sec:other_failure}

We inspected the conversation logs of D-CIPHER across all LLMs and document here the interesting errors and mistakes we observed. More examples of failure cases are provided in Appendix~\ref{sec:failure_demo}.
\\

% \together{We need to shrink this section to save space, maybe 1-2 sentences for each failure case}
% We inspected the trajectories generated from the experiment, except for the two common failures described in 
% Failures in complex systems are usually caused by challenges that are too complex to solve, as the high ratio of giveup  shown in our results. In addition to these common failures, here we focus on case studies of some specific types of failures captured during the model's solution process, which encountered an error related to an invalid tool call.


\noindent
\textbf{Auto-prompter fails to generate prompt:} 
Often, the Auto-prompter keeps running commands and exhausts it's maximum rounds without generating a prompt. Even after being prompter one last time to call the \texttt{GeneratePrompt} function, it continues to run commands.
In rare cases, it calls the function with an empty prompt, as seen in Figure~\ref{fig:fail_autoprompt}. In these scenario, we proceed with the Planner with the hard-coded prompt template.
\\

% This happens when the auto prompting agent fail to do generate a new prompt based on its exploration during the prompt generation phase. One instance is challenge "adversarial" for model Gemini 1.5 Flash as demonstrated in \ref{fig:fail_autoprompting}, The auto prompting agent tried with some run commands, but still does not generate a new prompt based on the challenge, and return Null as prompt back to planner agent. Makes the planner agent use the human written prompts to create the general plan.

\noindent
\textbf{Agent produces no action.} 
Sometimes, an agent choose not to call a function and only output the reasoning.
This happens in cases where the agent is stuck and thinks that it needs user input,
despite having emphasized to the agent that it is operating autonomously.
Figure~\ref{fig:fail_function_call} shows a case with LLaMa 3.1 405B and LLaMa 3.3 70B, where we see that the LLM tries to call a function but outputs incorrect syntax, due to which the function call fails to parse and remains as a regular output.
\\


% This happens when the model tries to run a command but it is parsed incorrectly, making the model output the run command tool call directly without running it. The model will need to try the same task again until it gets a response or is delegated to the Executor Agent. The challenge "thoroughlyStripped" as shown in \ref{fig:fail_function_call} on model LLaMa 3.1 405B, it tries to run commands in order to gather information about the files, however, the model does not correctly recognized and parsed the command and only treat them as normal thought, leads to wasted rounds and unfinished task.

\noindent
\textbf{Calling non-existent functions:}
Gemini 1.5 Flash calls non-existing functions like  ``decode'' and ``strip'', which results in the run failing with an error. This may be due to the model confusing the structure of the outputs can generating command-line calls where it should have generated a call to \texttt{RunCommand} with the proper arguments. 
These issues emphasize the need for proper function calling support in LLMs and may suggest that D-CIPHER moves to a simpler structure for action generation.
\\


% Failures in complex systems, such as ours, are usually caused by challenges that too complex to solve, such as the high ratio of the give up as shown in our results. Here, we focus on a specific type of failure happens on the Gemini 1.5 Flash model, which encountered an error related to an invalid tool call.
% When the Gemini model tries to solve a crypto challenge named "babycrypto", our system encountered a KeyError: 'decode'. This error occurred when the Planner Agent tried to the decode tool that was not part of the available toolset. This led to an error of the workflow, making the system not successfully solve this. And another keyerror is during the "Disastrous Security Apparatus" challenge, the system encountered a KeyError: 'strip' while interacting with a network service. This error occurred when the Executor Agent tried to use the strip tool, which was also unavailable in the execution environment.
% These errors highlight the need for tool validation process, where the system could check tool availability before execution and suggest alternative methods if the tool is unavailable in the environment.
%Failure caused by the function calling error occurs when the model tries to call the tools that are not provided in the toolset of D-CIPHER.

% \ref{fig:run_unexistent_tool}
% that occurred when the Planner Agent attempted to use an unavailable tool. Similarly, a KeyError: ‘strip’ occurred during the “Disastrous Security Apparatus” challenge when the Executor Agent called a non-existent tool. These issues emphasize the need for a tool validation process to check availability and suggest alternatives before execution as shown in \ref{fig:run_unexistent_tool}

% Use this example experiments/default/nyuctf_gemini1.5flash/2018q-rev-a_walk_through_x86_part_1.json

\noindent
\textbf{Confusion with interactive tools:} 
We noticed that Gemini 1.5 Flash tries to run an interactive reverse engineering tool ``radare2''. The model first runs ``radare2'' without a script or inputs, and then tries to run internal ``radare2'' commands like ``pD'' or ``px'' but via the \texttt{RunCommand} function call. This is the order in which a typical interactive user would write these commands, but the LLM agent does not have an interactive interface.
Similar errors have been observed with GPT 4 Turbo also.
Advanced interactive toolsets and demonstrations to bring awareness of the agent's interface may help fix such errors.
\\

% This error happened when the model tried to call commands that do not exist with run\_command tool provided, that is one of the typical error due to the model's hallucination. One example is \ref{fig:run_unexistent_cmd}, when Gemini 1.5 Flash tried to solve a reverse challenge \textit{a\_walk\_through\_x86\_part\_1}, it called the \textit{pD} and \textit{px}, which are both radar2 commands to \textit{disassembles and shows the assembly instructions for a specified number of lines or bytes} and \textit{displays a hexadecimal dump of a specified number of bytes}, while the model tried to call these commands before reaching the radar2 interface.

% Use this example experiments/default/nyuctf_claude3.5/2017q-rev-realism.json
% \textbf{Call command line tools before installation:} This error is due to a model's attempts to execute third party command line tools that are not pre-installed in the D-CIPHER's docker container, where the model needs to install it first. In the example \ref{fig:call_uninstalled_cmd}, in a reverse challenge realism by Claude 3.5 Sonnet, the model tried to call the command \textit{ndisasm} from nasm for disassembling, while nasm is not a preinstalled tool in D-CIPHER's container, which triggered the error \textit{command not found}.

\noindent
\textbf{Lack of tool support:} 
In some cases, the way D-CIPHER uses some tools leads to errors that confuse the agent and lead to task failure.
For example, the agent implemented a python script to use the ``gdb'' debugger.
However, due to missing utilities, the script produced errors that the agent was not able to fix over multiple iterations, leading to failure.
\\

%~\ref{fig:unsupported_tool}. Although the binary was loaded and a breakpoint set, automating GDB commands failed due to missing utilities. Efforts to script GDB interaction faced errors, preventing full analysis and return address control. Thus, debugging remains incomplete due to tool limits.

% In some specific cases the model tried to call tools that are not supported by the Ubuntu environments provided. Such as while solving challenge \textit{baby boi} with gpt 4o, D-CIPHER ran \textit{gdb} tool as shown in~\ref{}. The gdb is an interactive tool and does not supported by D-CIPHER. The executor agent  
% , such as using Windows commands or commands from other systems. 
% \nanda{Add example}

% \noindent
% \textbf{Reverse engineering errors:}
% Sometimes, the agent tries to trigger the reverse engineering tools to extract non-existing functions. For the challenge \texttt{arevenge}, Claude 3.5 Sonnet tries to decompile a binary for a hallucinated function \textit{\_Z1AIPhEvT\_}, as demonstrated in \ref{fig:fail_reverse} which is a weak symbol. In that challenge, weak symbols may be overridden by strong symbols, hence the decompiler cannot find that function causing the recompilation error as the decompiler cannot find the function required. 



\noindent
\textbf{Hallucinating challenge information:} 
We observe this failure where the agent tries to connect to a server that does not exist.
For example, while solving the challenge \texttt{pwnvoltex}, the agent needs to decrypt flag information from a local script but it thinks that there is a server it needs to connect to. Figure~\ref{fig:fail_hallucination} shows this example.
\\

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/failures/fail_hallucinate_server.pdf}
    \caption{Example of D-CIPHER hallucinating challenge server information.}
    \label{fig:fail_hallucination}
\end{figure}
% This failure happens when the model tries to connect to a server that does not exist. Under that case the server information the model tries to connect is due to the hallucination. One example is in challenge \textit{pwnvoltex}, which requires model to decrypt flag information from a local \textit{KornShell} (.ksh) script, while the model thinks that there is a server with some specific numbers appeared in that KornShell script and tries to connect with that misleading IP address as shown in \ref{fig:fail_connection}, which caused the connection failure due to hallucination.
