\section{Introduction}

\begin{figure}[tpb]
    \centering
    \includegraphics[width=\linewidth]{figures/dcipher-smalldiagram.pdf}
    \caption{Overview of D-CIPHER.}
    \label{fig:overview}
    % \vspace{-6mm}
\end{figure}

% Capture the Flag (CTF) challenges have emerged as a popular platform for testing and honing cybersecurity skills [cite here]. These competitions often involve a series of complex tasks that require a deep understanding of various security topics such as crypto, digital forensics and reverse engineering. These challenges demand not only domain expertise but also strategic problem-solving and adaptability, marking them an ideal test bench for evaluating automated systems. Large language models (LLMs) has shown significant potential in various AI tasks including the application of the LLM agentic systems on cybersecurity including vulnerability detection [cite], bug localization [cite] and automated program repair (APR) [cite]. With these advents of LLMs, there has been a surge of interest in leveraging these models for solving complex tasks, including CTF challenges. Recent work [cite here] indicates that CTF challenges can be a good practice to evaluate the LLM's on its cybersecurity and automated task planning skills as it is a simulation on real world attack scenarios [cite here]. Current developments in LLM-based agentic systems have also shown promising results in automating CTF challenge solutions [cite here], as well as comprehensive benchmarks [cite here] designed specifically for these LLM agents.
Large language models (LLMs) have demonstrated remarkable potential in cybersecurity applications such as vulnerability detection 
\cite{lu2024grace, guo2024outside, akuthota2023vulnerability, li2024llm}, bug localization \cite{li2024attention, zhang2024empirical}, and automated program repair (APR) \cite{bouzenia2024repairagent, xia2023conversational, xia2024automated}.
Recent advancements in LLM capabilities have led to growing interest in leveraging LLMs to autonomously solve complex cybersecurity tasks.
Autonomous agents for offensive security tasks are essential to keep pace with the rapidly expanding cyber threats, as highlighted by the DARPA Cyber Grand Challenge~\cite{DARPA-CGC} and the AI Cyber Challenge~\cite{DARPA-AIxCC}.
LLMs demonstrate significant potential for automating offensive security tasks. \cite{bhatt2024cyberseceval, wan2024cyberseceval3advancingevaluation}.
Capture the Flag (CTF) challenges have gained popularity as a means of evaluating and improving cybersecurity skills across all levels \cite{chicone2018using, vykopal2020benefits}. CTF challenges often include complex tasks that require expertise across diverse domains, including cryptography, digital forensics, and reverse engineering, offering a platform to evaluate an LLM’s proficiency in cybersecurity and automated task planning by simulating real-world offensive security scenarios~\cite{tann2023using, yang2023language, shao2024nyu, savin2023battle, pieterse2024friend}.
%They have become a widely used medium for practical cybersecurity training, evaluation, and research. 
The evaluation of autonomous LLM agents is most effective with jeopardy-style CTF challenges, which emphasize exploiting standalone software like a binary that can be reverse-engineered, encrypted data that can be decrypted, or a web server with authentication that can be bypassed. Successfully compromising the software results in the discovery or revelation of a unique ``flag'' string, serving as a clear indicator of success. %LLM agents demonstrate potential in autonomously solving these CTF challenges. 
CTF challenge benchmarks \cite{zhang2024cybenchframeworkevaluatingcybersecurity, shao2024nyu}  advance the  autonomous problem-solving abilities of LLM agents.

% CTF challenges simulate real-world cyber-attack scenarios and have emerged as a popular medium for practical cybersecurity training, evaluation, and research. These challenges can simulate real-world attack and defense scenarios and thus assist competitors in developing practical skills in areas such as cryptography, binary exploitation, and reverse engineering. 
% Evaluation of autonomous LLM agents works best with jeopardy-style CTF challenges that focus on standalone software that must be compromised \cite{shao2024nyu,pieterse2024friend}.
% The standalone software may be a binary that can be reverse engineered or exploited, encrypted data that can be decrypted, or a web server whose authentication can be bypassed. After successfully compromising the software, a unique ``flag'' string is either found or revealed by the software server.
% The unique flag string is a concrete indicator of the success of a CTF challenge.
% Recent studies use benchmarks of CTF challenges to evaluate LLM agents on their ability to solve complex tasks and demonstrate practical skills in cybersecurity \cite{shao2024nyu,shao2024empirical,abramovich2024enigma, muzsai2024hacksynth, zhang2024cybenchframeworkevaluatingcybersecurity,yang2023language,turtayev2024hacking}



Current LLM agents designed to solve CTFs operate as single agents tasked with handling the challenge from start to finish.
CTFs are complex tasks that require significant amounts of exploration and proper execution of a sequence of tasks to solve the problem and get the flag.
Single agent frameworks typically restrict feedback mechanisms to self-reflection within the LLM’s context. The agent needs to go through several reasoning-action-observation steps to achieve success, with multiple exploration steps that produce irrelevant outputs and multiple retries to complete a single task.
This causes many issues that hinder the agent's problem-solving abilities, such as loss of focus on the broad problem and hallucinations about task details.
On the other hand, real-world CTF competitions are generally solved collaboratively by teams \cite{chang2022capture, cuevas2022observations} of members with diverse expertise to tackle challenges across various domains. All member continuously share insights, provide feedback, and refine strategies through interactive collaboration. 
Current CTF-solving LLM agents cannot fully capture the collaborative nature of CTF competitions.
Inspired by this, we introduce a multi-agent framework that divides responsibilities among multiple LLM agents and facilitates dynamic interactions among the agents to allow collaborative problem-solving.
While multi-agent systems have gained prominence across various domains \cite{dorri2018multi, li2024more, xu2023rewoo}, their application to solving CTF challenges remains unexplored.  Incorporating the collaborative potential of multi-agent systems can better emulate the team dynamics seen in real-world CTF competitions.

% However, existing LLM agentic systems for CTF solving often reply on single-agent framework, which are inherently limited by the lack of collaborative dynamics and diverse perspectives for their intuition on CTF solving and feedback enhancement of solution trajectories. Real world CTF competitions allows team participation [cite], where the members with specialized roles collaborate to address challenges efficiently on a set of problems proposed in the competition. These teams excel by collecting individual expertise of multiple specialists with continuously share insights, provide feedback and refine their strategies through interactive collaboration with other team members, hence this collaborative nature of human CTF competitions may not be fully captured by CTF agents.

% While multi-agent systems have gained traction across various domains [cite], their potential in addressing CTF challenges remains largely unexplored. Traditional approaches often limit feedback mechanisms to self-reflection within a single agent's conversation history, falling to harness the power of diverse perspectives and specialized expertise that multiple agents could provide. This limitation becomes particularly apparent when dealing with complex CTF challenges that require a combination of different security skills and problem-solving approaches.


We present D-CIPHER, a novel multi-agent framework designed to autonomously solve CTF challenges via collaboration of multiple LLM agents. 
To overcome the limitations of single agent systems, D-CIPHER introduces two mechanisms to facilitate enhanced interaction and dynamic feedback between LLM agents.
%\charan{Comment- Refer the Fig.1 in this paragraph}.
The first is the \textit{Planner-Executor agent system} that involves a Planner agent with the responsibility of solving the CTF challenge end to end, and multiple heterogeneous Executor agents with the responsibility of completing single tasks assigned by the Planner.
Dividing responsibilities between planner and executors allows each agent to maintain focus for longer, more complex tasks, and reduces hallucinations.
The second is the \textit{Auto-prompter agent}, tasked with exploring the challenge and generating an initial prompt for the main system.
Auto-prompting is a prompt engineering technique to improve LLM performance by generating dynamic task-specific prompts as opposed to human-written hard-coded prompt templates. D-CIPHER incorporates auto-prompting as a separate agent which facilitates environment exploration and produces a highly-relevant initial prompt to kick-start the main system's problem solving.

Figure~\ref{fig:overview} shows an overview of D-CIPHER. All  agents can access a shared container environment to run commands and interact with the challenge server.
The process starts with the Auto-prompter agent, which explores the challenge and generates a prompt to initiate the Planner agent. The Planner does a few rounds of exploration, after which it creates a plan and delegates tasks to the Executor. For each delegated task, an Executor agent is initiated with a new conversation history, allowing for heterogeneous execution and greater focus on the current task. After completing the task, the Executor returns a task summary which the Planner may use to update the plan and delegate further tasks. The Planner-Executor loop continues until the challenge is solved, or some terminal conditions are met.
This collaborative design allows D-CIPHER to tackle complex CTF challenges, improving performance and achieving state-of-the-art accuracy on CTF benchmarks.
% \meet{Need to rewrite, many scattered vague statements:} D-CIPHER introduces two unique mechanisms to enhance the agents' collaboration via different types of prompting dynamics: \textbf{auto-prompting} and \textbf{collaborative feedback}. Dynamic prompting is a novel heuristic prompting mechanism for challenge descriptions that replaces rigid, hard-coded templates with a dynamic approach. We introduced autopromter agent, which contextualizes challenge-specific information by systematically reviewing all files provided to enable tailored prompt engineering as well as conducting early interacting with the challenge environment, adopting the role of an explorer to gather insights from its attempts before initiating the solution process. Second, D-CIPHER incorporates with multiple executor agents in a planner-executor architecture that analyzes solution trajectories and offers actionable, iterative feedback insights. 

%D-CIPHER, each agent is assigned a specialized role, such as problem analysis, solution execution, and in-loop feedback. This effectively emulates the diverse expertise found in human teams. 

% To tackle this problem, we introduce PICOS, a novel multi-agent framework designed to address the limitation of current single-agent design to optimize the solution process of LLM agentic systems on CTF solving. PICOS framework is based on two foundational principles: cynamic prompting and collaborative feedback. Each agent in the system is assigned a distinct role such as problem analysis, solution execution and in-loop feedback, allowing the system to mimic the diversity of expertise in human teams. The innovation of PICOS covers two perspectives, first we proposed a new prompting mechanism for user prompt on challenge description, instead of using hard-coded prompt template, PICOS incorporated challenge information contextually from the prompting agent by going over all the necessary files provided by the challenge, enabling the prompt engineering from dynamic and challenge-specific aspects. We introduced executor agent, which goes beyond conventional conversational feedback mechanisms by analyzing solution trajectories and offering actionable insights. 

% PICOS provides an enhancement of prompt mechanism of agentic system and simulate the collaboration of real world team-based CTF competition. We aimed at offering a comprehensive study on how the LLM multi-agent collaboration can do impact on offensive security automated task planning and how prompts will change the solution process of these automation tasks. Our framewrk represents a significant departure from existing approaches by emphasizing dynamic collaboration and continuous feedback loops among specialized agents. This design philosophy mirrors the successful strategies employed by human CTF teams while leveraging the unique capabilities of LLMs. We evaluated PICOS across multiple benchmarks on a diverse of model selection to assess its efficacy in solving diverse challenges. Our findings reveal that the multi-agent collaborative approach not only improves problem-solving performance but also enhance robustness by mitigating errors and dynamically adapting strategies during runtime. Comprehensive case studies illustrate how PICOS leverages interactive feedback and efficient information delivery to outperform single-agent baselines. Overall, we have the following contribution in that work:
% \begin{description}
%     \item[\textbf{1. Multi-agent Framework}] We introduce a novel multi-agent framework, PICOS, which leverage specialized agents with distinct roles to enable dynamic collaboration and problem-solving
%     \item[\textbf{2. Specialized Agents}] We proposed a new automated prompting mechanism to generate dynamic hint information for agent system and executor agent module to enhance the feedback loop and solution refinement.
%     \item[\textbf{3. Comprehensive Study}] We conducted a comprehensive evaluation through both comparison analysis and case study to demonstrate how the multiple agents interact with each other and different prompt impact the solution process on CTF challenges, providing the community with a collaborative aspects for cybersecurity automated task planning with LLM.
% \end{description}

%In this paper, we explore the impact of multi-agent collaboration and interaction towards building  autonomous agents for offensive security tasks. 
We evaluate D-CIPHER across three benchmarks and seven LLM models. Our findings demonstrate that the multi-agent approach not only improves problem-solving performance, but also enhances robustness by mitigating errors and dynamically adapting strategies during runtime. We perform ablation studies and comparison with related works to further illustrate D-CIPHER's ability to outperform single-agent systems. The contributions of this work are as follows: 
%\meet{strengthen the contributions}
\begin{enumerate}[leftmargin=2em]
    \item \textbf{We introduce D-CIPHER}, a novel LLM multi-agent framework that leverages specialized agents with distinct roles to enable agent collaboration for autonomous problem-solving.
    \item \textbf{We propose a novel Planner-Executor system}, diving responsibilities between a Planner agent and multiple Executor agents to enhance long-term focus on complex problems. 
    \item \textbf{We propose a novel Auto-prompter agent}, improving on auto-prompting techniques with an agentic setup.
    \item \textbf{We conduct a comprehensive study} to demonstrate how the collaboration between agents in the multi-agent system enhances problem-solving on CTF challenges.
\end{enumerate}

% The paper is structured as follows: 
% Section~\ref{sec:background} presents background on LLM agents and auto-prompting,
% Section~\ref{sec:related_work} compares related work,
% Section~\ref{sec:implementation} describes D-CIPHER's implementation,
% Section~\ref{sec:experiment_setup} provides the experiment setup,
% Section~\ref{sec:results} presents the results along with comprehensive studies of D-CIPHER's performance,
% Section~\ref{sec:discussion} discusses two case studies, limitations, and ethics,
% and Section~\ref{sec:conclusion} concludes and proposes future work.



The paper is structured as follows: 
Section~\ref{sec:background} provides background on LLM agents and auto-prompting, Section~\ref{sec:related_work} reviews related work, Section~\ref{sec:implementation} details the implementation of D-CIPHER, Section~\ref{sec:experiment_setup} outlines the experimental setup, Section~\ref{sec:results} presents the results and includes a comprehensive analysis of D-CIPHER's performance, Section~\ref{sec:discussion} discusses two case studies and ethical considerations, and Section~\ref{sec:conclusion} concludes the paper, discusses limitations and proposes directions for future work.