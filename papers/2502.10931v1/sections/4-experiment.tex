\section{Experiment Setup} \label{sec:experiment_setup}

% \meet{Add agent configuration details - max\_rounds, max\_cost, len\_observation, temperature.}

Each run of D-CIPHER attempts to solve one CTF challenge.
The following configuration options are set for each run:
an overall cost budget of \$3,
a temperature of 1.0 for each LLM,
5 maximum rounds for the Auto-prompter,
30 maximum rounds for the Planner,
100 maximum rounds for each Executor,
and the conversation history of each Executor is truncated to last 5 action-observation pairs.
% All agent runs are configured with specific constraints: a budget limit of \$3, a maximum of 30 rounds for the Planner, 100 rounds for the Executor, and 5 rounds for the Auto-Prompter. To maintain consistency in performance evaluation, all models are set with a default \texttt{temperature} of 1.0. Additionally, the Executor's \texttt{length\_observation} parameter is set to 5. If a run exceeds the budget limit or the Planner/Executor surpasses the maximum allowed rounds, the challenge is marked as unsolved due to exceeding either the maximum rounds or cost constraints.
% In this section, we provide a comprehensive overview of the experimental setup, starting with the development set used for agent development to prevent overfitting on benchmark datasets. Next, we outline the benchmarks considered in the experimentation, and offer a clear context for performance evaluation. Finally, we elaborate on the Language Models (LM) we considered, baselines and the evaluation metrics we employed. 

% \subsection{Development Set}
% To build effective language model (LM) agents, it is crucial to maintain a separate development set to guide the refinement of features and system configurations. This dedicated set allows for careful selection of optimal features, tools, and parameters for the planner and executor agents during development. Once these elements are finalized, the model's performance can be evaluated on a test benchmark to determine its accuracy and compare it with the state-of-the-art. In our experimentation, we utilized the NYU CTF development set \cite{shao2024nyu}, which consists of 55 CTF challenges sourced from the same competitions as the primary test set. These challenges were categorized into six groups, with their category-wise composition presented in the Table \ref{table:developset}.
% \begin{table}[h!]
% \scalebox{0.88}{
% % \begin{tabular}{|c|c|c|c|c|c|c|c|}
% % \hline
% % \textbf{Category}                                                        & \textbf{Crypto} & \textbf{Web} & \textbf{Forensics} & \textbf{Misc} & \textbf{Rev} & \textbf{Pwn} & \textbf{Total} \\ \hline
% % \textbf{\begin{tabular}[c]{@{}c@{}}Number of \\ Challenges\end{tabular}} & 10              & 10           & 10                 & 6             & 9            & 10           & 55             \\ \hline
% % \end{tabular}

% \begin{tabular}{@{}cccccccc@{}}
% \toprule
% \textbf{Category}                                                        & \textbf{Crypto} & \textbf{Web} & \textbf{Forensics} & \textbf{Misc} & \textbf{Rev} & \textbf{Pwn} & \textbf{Total} \\ \midrule
% \textbf{\begin{tabular}[c]{@{}c@{}}\# Challenges\end{tabular}} & 10              & 10           & 10                 & 6             & 9            & 10           & 55             \\ \bottomrule
% \end{tabular}
% }
% \caption{Category-wise composition of Development Set}
% \label{table:developset}
% \end{table}

\subsection{Benchmarks}
We evaluate D-CIPHER across three benchmarks: NYU CTF Bench \cite{shao2024nyu}, Cybench~\cite{zhang2024cybenchframeworkevaluatingcybersecurity}, and HackTheBox \cite{hackthebox}. Table~\ref{table:benchmarks} shows the category-wise breakdown of the challenges in each benchmark. These benchmarks comprise a total of 290 challenges spanning six categories: cryptography (crypto), forensics, binary exploitation (pwn), reverse engineering (rev), web, and miscellaneous (misc), ensuring a well-rounded evaluation of D-CIPHER. We perform our ablation studies on NYU CTF Bench.
During framework development and configuration, we use the development set introduced in \cite{abramovich2024enigma} to select optimal features and design prompts.
The development set is an extension of NYU CTF Bench containing 55 extra challenges across the same six categories.
By using a separate development set, we prevent overfitting our design to the main benchmarks and avoid biasing the results.
We evaluate Cybench in the unguided mode, where we do not utilize the additional subtask information for each challenge. We use the challenge description in the ``hard prompt'' that does not contain extra hints.

\begin{table}[htpb]
\centering
% \small
\caption{Benchmarks for evaluating D-CIPHER.}
\label{table:benchmarks}
\begin{tabular}{lccccccc}
\toprule
                   & cry      & for   & pwn         & rev         & web        & misc         & \textbf{Total} \\ 
\cmidrule{2-8}
NYU CTF            & 53          & 15          & 38          & 51          & 19          & 24          & \textbf{200}   \\
Cybench            & 16          & 4           & 2           & 6           & 8           & 4           & \textbf{40}    \\ 
HackTheBox         & 30          & 0           & 0           & 20          & 0           & 0           & \textbf{50}    \\
\midrule
\textbf{Total}     & \textbf{99} & \textbf{19} & \textbf{40} & \textbf{77} & \textbf{27} & \textbf{28} & \textbf{290}   \\ 
\bottomrule
% \vspace{-10mm}
\end{tabular}
\end{table}


% NYU CTF Bench, collected from the CSAW CTF competitions held between 2017 and 2023, offers a diverse set of 200 challenges varying in difficulty from ``very easy'' to ``hard.'' This dataset, the largest of its kind for CTF challenges, highlights the current limitations of LM agents \cite{shao2024nyu}. 

% The HackTheBox benchmark comprises 50 challenges sourced from the HTB online platform. These tasks, categorized into cryptography and reverse engineering, cover a range of difficulties from "very easy" to "medium," as rated by the platform. 

% Cybench includes 40 professional-level CTF challenges curated from four recent competitions. These challenges are designed to reflect real-world hacking skills, spanning from basic input validation to advanced return-oriented programming, and feature tasks involving the exploitation of real-world CVEs.  \\
% \textbf{Development Set.} To build effective language model (LM) agents, it is crucial to maintain a separate development set to guide the refinement of features and system configurations. In our case, we used the same development set used in \cite{abramovich2024enigma} for careful selection of optimal features, tools, and parameters for the planner and executor agents during development and for ablation stduies.

\subsection{LLM Model Selection}

We test multiple LLMs with D-CIPHER for each of the Planner, Executor, and Auto-prompter agents. 
For the main experiment, we use the same LLM for all three agents.
However, the framework offers freedom to use different LLMs for each agent, and we experiment by combining stronger models for the Planner with weaker models for the Executor.
We access the LLMs via their provided APIs: OpenAI API for GPT, Anthropic Inference API for Claude, and Google API for Gemini. For the open-source LLaMa models, we use the  Together AI platform \cite{togetherai} via their API.
The stronger models and their unique strings are:
\begin{itemize}
    \item Claude 3.5 Sonnet: \texttt{claude-3-5-sonnet-20241022}
    \item GPT 4 Turbo: \texttt{gpt-4-turbo-2024-04-09}
    \item GPT 4o: \texttt{gpt-4o-2024-11-20}
    \item LLaMa 3.1 405B: \\
        \texttt{meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo}
    \item Gemini 1.5 Flash: \texttt{gemini-1.5-flash}
\end{itemize}
The weaker models and their unique strings are:
\begin{itemize}
    \item Claude 3.5 Haiku: \texttt{claude-3-5-haiku-20241022}
    \item GPT 4o Mini: \texttt{gpt-4o-mini-2024-07-18}
    \item LLaMa 3.3 70B: \\
    \texttt{meta-llama/Llama-3.3-70B-Instruct-Turbo}
    \item Gemini 1.5 Flash 8B: \texttt{gemini-1.5-flash-8b}
\end{itemize}

% Models such as GPT-4 Turbo (gpt-4-turbo-2024-04-09), GPT-4o (gpt-4o-2024-11-20), Claude 3.5 Sonnet (claude-3-5-sonnet-20241022), LLaMA 3.1 (405B), and Google's Gemini 1.5 Flash are employed in both planner and executor roles. Additionally, as part of our ablation study, we assessed D-CIPHER's performance using strong-weak model combinations across various agent roles. To achieve this, the stronger models were paired with complementary executor models, including Claude 3.5 Haiku (claude-3-5-haiku-20241022), GPT-4o Mini (gpt-4o-mini-2024-07-18), LLaMA 3.1 (70B), and Gemini Flash 8B. This configuration was designed to analyze the effectiveness of the planner-executor combinations across different benchmarks. For API access, we use the OpenAI API for GPT models, Anthropic Inference API \cite{anthropicapi} for Claude models, the Google API for Gemini, and the Together API \cite{togetherai} for LLaMA models. All models are configured with a default temperature of 1.0 to ensure consistency in performance evaluation. 

\subsection{Evaluation Metrics} \label{sec:metrics}

We use the percentage of challenges successfully solved by D-CIPHER (\textit{\% solved}) as the primary metric. 
A challenge is marked solved when the correct flag is submitted by the Planner. We also mark the challenge as solved if the correct flag is observed in any part of the model conversation. This prevents failure of cases where the flag is found by the Auto-prompter or Executor but they do not pass it to the Planner, because only the Planner can submit a flag via the \texttt{SubmitFlag} tool.
It is highly unlikely that this method matches a non-flag string because flags are long, unique strings of specific formats such as ``flag{...}''.
This approach mimics real-world CTFs where participants may submit a flag multiple times and receive instant confirmation.
Additionally, we report the average cost of solved challenges (\textit{\$ cost}), computed as the total US dollar cost of all API calls of all agents, averaged across successfully solved challenges of a benchmark. As the cost per token for LLMs deployed on the cloud is indicative of the computational resources required to deploy them, the average cost allows us to estimate computational resources for each solved challenge.
% Each instance is constrained by a \$3 budget and 30 maximum rounds of the planner; if a run exceeds the budget limit or the planner did more than the maximum round of the planner, the challenge is marked as unsolved due to cost constraints (exit\_cost).


% \noindent \textbf{Baseline.} We compare D-CIPHER with its respective baseline agents on the NYU CTF Bench and the Cybench benchmark to evaluate its performance.


% \charan{Todo list- 1) change sampling parameters}
% \section{Model Selection}

% \section{Parameters configuration}

% \section{Benchmarks}