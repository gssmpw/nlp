\section{Background} \label{sec:background}

% \subsection{Capture the Flag (CTF) Challenges}

% CTF challenges simulate real-world cyber-attack scenarios and have emerged as a popular medium for practical cybersecurity training, evaluation, and research. These challenges can simulate real-world attack and defense scenarios and thus assist competitors in developing practical skills in areas such as cryptography, binary exploitation, and reverse engineering. 
% Evaluation of autonomous LLM agents works best with jeopardy-style CTF challenges that focus on standalone software that must be compromised \cite{shao2024nyu,pieterse2024friend}.
% The standalone software may be a binary that can be reverse engineered or exploited, encrypted data that can be decrypted, or a web server whose authentication can be bypassed. After successfully compromising the software, a unique ``flag'' string is either found or revealed by the software server.
% The unique flag string is a concrete indicator of the success of a CTF challenge.
% Recent studies use benchmarks of CTF challenges to evaluate LLM agents on their ability to solve complex tasks and demonstrate practical skills in cybersecurity \cite{shao2024nyu,shao2024empirical,abramovich2024enigma, muzsai2024hacksynth, zhang2024cybenchframeworkevaluatingcybersecurity,yang2023language,turtayev2024hacking}
% Platforms like PicoCTF~\cite{picoctf}, TryHackMe~\cite{tryhackme}, CTFTime~\cite{ctftime} and HackTheBox~\cite{hackthebox} have popularized these formats by providing structured challenges for learners at various skill levels.

% Research indicates that CTF challenges can foster cybersecurity expertise and serve as tools for evaluating facility with cybersecurity skills~\cite{chicone2018using}. They are widely used in academia to enhance learning outcomes in cybersecurity education, with studies demonstrating their effectiveness in promoting analytical thinking and teamwork~\cite{hanafi2021ctf,leune2017using,vykopal2020benefits}. Furthermore, the integration of CTF challenges into research environments enables benchmarking of advanced AI systems like LLMs. .

% Yet, challenges in CTF design persist. These include achieving significant performance, preserving context across tasks, and handling complex, dynamic CTFs that rely on multidisciplinary approaches. Implementing strategies to address these issues enhances problem-solving efficiency, enabling more accurate, adaptive, and effective responses to evolving challenges within CTF environments.


% \subsection{Prompt Engineering}
% \subsection{Prompt Engineering for CTF}
% \subsection{LLM Agents}

% As the use of LLMs to solve CFT challenges expands, prompt engineering is becoming a critical technique for enhancing performance. Various methods have been explored to craft prompts that effectively guide LLMs to the solution of complex cybersecurity problems. Each of these solutions have their own unique strengths and limitations.
%\meet{add more references for LLM agents in other domains, like SWE-Agent, also talk about use of function calling}
Text-based LLMs take a text prompt as input from the user, and produce a text output that follows the user prompt.
LLMs have a finite length of text tokens that they can process called the context.
An alternating sequence of user prompts and LLM outputs makes a conversation and is the basis of chat-based LLM interfaces like ChatGPT.
To remove the user from the loop and create autonomous agents, a feedback mechanism is added based on the LLM outputs, so that the LLM can autonomously continue the conversation.
\citet{yang2023intercode} introduce iterative feedback prompting where the LLM is tasked with writing a piece of code, and the code's compilation and execution logs are provided as feedback, which the LLM uses to iteratively refine it's output.
Recent LLMs support function calling, a way to provide a set of actions to the LLM that it may choose to ``call'' as a function.
In this manner, LLM agents can be provided with many ``tools'' such as a command line, web search, file editing, and code execution \cite{wang2024surveyllmagents}, so that they can autonomously perform various tasks like software development \cite{yang2024sweagent}, web browsing \cite{yoran2024assistantbench}, or solve CTF challenges~\cite{shao2024nyu, abramovich2024enigma}.

With access to the command line and file editing tools, LLM agents can autonomously solve many tasks, but they still struggle on complex long-horizon tasks such as CTF challenges that require multiple steps.
Plan-and-solve prompting \cite{wang2023planandsolve} enhances long-term focus of the agent by incorporating a planning phase before iterative execution. This helps agents tackle ambiguous or complex tasks by structured strategies \cite{turtayev2024hacking}.
ReAct (reasoning + action) \cite{yao2022react} combines step-by-step reasoning with action, allowing the agent to adjust dynamically through iterative cycles. ReWOO (Reasoning without Observation) \cite{xu2023rewoo} separates the reasoning process from tool outputs and observations, allowing it to handle multi-step reasoning tasks efficiently while maintaining focus.
The prompting methods in these agents involve static hard-coded templates where environment and task information is filled in.
While static prompts provide straightforward guidance, they often fail to adapt to different problems and complex tasks, limiting their effectiveness.
Auto-prompting~\cite{shin-etal-2020-autoprompt, zhou-etal-2023-revisiting, zhang2023automatic} is a technique to allow the LLM itself to generate a highly-relevant prompt. Auto-prompting invokes more factual responses and reduces hallucinations in LLMs.
D-CIPHER incorporates auto-prompting as a separate agent that can explore the environment and generate a better prompt.
%Based on the given prompt, LLM agents make a decision and proceed further to find flags.  To address this gap, we propose \textbf{dynamic prompting}, where the LLM agent autonomously generates prompts based on the CTF challenge's context and stage.
%include a static template which needs to be given to LLM to solve the CTF challenges. For instance, the NYU CTF framework provides a static prompt as \emph{``Please proceed to the next step using your best judgment"} for each decision making point. 

% To address this gap, we introduce a novel approach where the LLM agent generates the next prompt autonomously based on the current context and stage of the CTF challenge, a technique we call \textbf{dynamic prompting}.


Expanding on single LLM agents, multi-agent LLM systems are a powerful approach to enhance problem-solving by simulating team-based collaboration. Specialized agents, each with distinct objectives, work together to tackle different aspects of complex tasks \cite{guo2024largelanguagemodelbased}
Multi-agent systems are effective in cybersecurity applications. For instance, Audit-LLM~\cite{song2024audit} deploys a  multi-agent system for insider threat detection by employing agents to decompose tasks, build tools, and use collaborative reasoning to enhance detection accuracy. Liu~\cite{liu2024multi} explores multi-agent systems to enhance incident response in cybersecurity by examining centralized, decentralized, and hybrid team structures to assess how LLM agents can improve decision-making, adaptability, and coordination during cyber-attack scenarios. AutoSafeCoder~\cite{nunez2024autosafecoder} enhances the security of code generated by LLMs by incorporating a coding agent for code generation, a static analyzer agent that identifies vulnerabilities, and a fuzz testing agent for dynamic testing to detect runtime errors. Division of responsibilities among different agents allows AutoSafeCoder to produce secure, functionally correct code. 

% With the growing use of LLMs in CTF challenges, prompt engineering is key to enhancing performance. Various methods guide LLMs in solving complex cybersecurity tasks, each with distinct strengths and limitations.

% \textbf{Single Turn (Zero-Shot Prompting)} involves providing the model with a one-time task description that outputs  an immediate solution. This is efficient for straightforward tasks~\cite{yang2023intercode}. In contrast, \textbf{Try Again (Iterative Feedback Prompting)} uses iterative feedback to refine responses over multiple attempts, mimicking real-world problem-solving~\cite{yang2023intercode}. The \textbf{Plan \& Solve} enhances adaptability by incorporating a planning phase before iterative execution. This helps models tackle ambiguous or complex tasks by  structured strategies~\cite{turtayev2024hacking}. Additionally, \textbf{ReAct (Reasoning + Action)} combines step-by-step reasoning with action, allowing the model to adjust dynamically through iterative cycles. This makes it particularly effective for evolving and complex challenges like CTFs~\cite{yao2023react}. 
% These prompting techniques highlight diverse approaches to optimizing LLM performance in cybersecurity tasks. 

% Multi-agents!


%\meet{Add references for auto-prompting, and shorten this para}
%\nanda{Maybe we can add this to previous paragraphs which discusses other prompting methods such as plan-and-solve and ReAct method}
% All of these prompting methods include a static template which needs to be given to LLM to solve the CTF challenges. For instance, the NYU CTF framework provides a static prompt as \emph{``Please proceed to the next step using your best judgment"} for each decision making point. 
% Based on the given prompt, LLM agents make a decision and proceed further to find flags. While static prompts provide straightforward guidance, they often fail to account for the evolving nature of complex tasks, limiting their effectiveness in multi-step or ambiguous CTF challenges. To address this gap, we propose \textbf{dynamic prompting}, where the LLM agent autonomously generates prompts based on the CTF challenge's context and stage.
% % To address this gap, we introduce a novel approach where the LLM agent generates the next prompt autonomously based on the current context and stage of the CTF challenge, a technique we call \textbf{dynamic prompting}.
% Dynamic prompting adapts instructions to task progress, ensuring instructions are contextually relevant and reflective of the specific obstacles encountered. By iterating based on feedback and intermediate outputs, it continuously refines the LLM’s approach, enhancing problem-solving for dynamic tasks like CTFs.
% This adaptive process not only mirrors how humans tackle complex problems but also improves the model’s ability to handle unpredictable scenarios, making it particularly advantageous for cybersecurity tasks like CTFs where conditions change dynamically.


% The very first prompt type used in several applications is \textbf{Single Turn (Zero-Shot Prompting)}~\cite{yang2023intercode}. In single-turn prompting, the model receives a one-time, straightforward task description and is expected to generate a complete response without further interaction. The initial output is directly assessed, making this approach efficient for tasks where minimal feedback or iteration is required. This method tests the model’s ability to understand and respond to tasks immediately, relying heavily on the model's pre-trained knowledge and generalization capabilities.

% Along with this, The prompting method named \textbf{Try Again (Iterative Feedback Prompting)}~\cite{yang2023intercode} has been also used in several appreciations specially to solve CTF challenges. It is an iterative prompting method involves continuous interaction, where the model is provided with feedback after each attempt. The model can refine its responses over multiple turns based on the observations or execution results from previous outputs. This iterative process continues until the task is successfully completed or a maximum number of interactions is reached. This approach closely mirrors real-world problem-solving, where adjustments are made iteratively based on evolving circumstances or feedback.

% Some application are also using \textbf{Plan \& Solve}~\cite{turtayev2024hacking} prompting method which enhances problem-solving by dividing the process into a planning phase followed by execution. Initially, the model formulates a strategy based on the task description and available information, allowing for a structured approach to ambiguous or complex problems. This plan guides the subsequent execution phase, where the model carries out actions iteratively, refining its approach based on feedback. In more challenging scenarios, re-planning mid-task further improves adaptability and performance. This method proves effective in tasks like CTF challenges, where vague instructions require careful analysis and step-by-step resolution.

% Further some application are also using \textbf{ReAct (Reasoning + Action)}~\cite{yao2023react} prompting method blends reasoning with action by guiding the model to think through tasks step-by-step before executing actions. At each step, the model generates a thought based on the task and observations, which informs the next action. The action is executed, and the resulting feedback refines the model’s understanding for the next cycle. This continuous process helps the model adapt dynamically to complex tasks, making it effective for CTF challenges where logical reasoning and step-by-step execution are essential.

\section{Related Works} \label{sec:related_work}


\begin{table}[htpb]
    \centering
    \caption{Feature comparison of LLM agents for solving CTFs.}
    \label{tab:related_work_comparison}
    \begin{tabular}{lcccccc}
    \toprule
         \textbf{Study} & \rotatebox{90}{\textbf{\# CTFs}} & \rotatebox{90}{\textbf{Open bench}} & \rotatebox{90}{\textbf{Tool use}}  & \rotatebox{90}{\textbf{Autonomous}} & \rotatebox{90}{\textbf{Multi-agent}} &\rotatebox{90}{\textbf{Auto-prompt}} \\
    \cmidrule{2-7}
     % \textbf{Study} & \textbf{Dynamic} & \textbf{Used} & \textbf{Multi-} & \textbf{Automatic} & \textbf{Tool} & \textbf{\# of} \\
         Tann et al. \cite{tann2023using} &  $7$ & \purplecross & \purplecross & \purplecross & \purplecross & \purplecross  \\
         Shao et al. \cite{shao2024empirical} & $26$ & \purplecross & \tealcheck & \tealcheck & \purplecross & \purplecross  \\
         InterCode-CTF\cite{yang2023language} & $100$ & \tealcheck & \tealcheck & \tealcheck & \purplecross & \purplecross   \\
         NYU CTF Bench \cite{shao2024nyu} & $200$ & \tealcheck & \tealcheck & \tealcheck & \purplecross & \purplecross \\
         Turtayev et al. \cite{turtayev2024hacking} & $100$ & \tealcheck & \tealcheck & \tealcheck & \purplecross & \purplecross\\
         Cybench \cite{zhang2024cybenchframeworkevaluatingcybersecurity} & $40$ & \tealcheck & \tealcheck & \tealcheck & \purplecross & \purplecross \\
         EnIGMA \cite{abramovich2024enigma} & $350$ & \tealcheck & \tealcheck & \tealcheck & \purplecross & \purplecross\\
         HackSynth \cite{muzsai2024hacksynth} & $200$ & \tealcheck & \tealcheck & \tealcheck & \tealcheck & \purplecross \\
         \textbf{D-CIPHER (ours)} & $290$ & \tealcheck & \tealcheck & \tealcheck & \tealcheck & \tealcheck \\
    \bottomrule
    \end{tabular}
\end{table}



% \subsection{LLMs on Cybersecurity}
% \subsection{LLM Agents for CTF}

%LLMs have a vast knowledge base that can be tapped for cybersecurity use.
Tann et al.~\cite{tann2023using} evaluate early LLMs such as ChatGPT and Google Bard in solving CTF challenges and answering professional certification questions, showing that LLM responses contain key task information.
%Many works extend the LLM capabilities by providing them access to programming and command execution tools, to form autonomous agents. 
The InterCode-CTF agent~\cite{yang2023intercode} reveals that LLM agents demonstrate basic cybersecurity skills, however they struggle with more complex tasks.
The NYU CTF baseline agent~\cite{shao2024empirical} integrates external tools into the LLM's function-calling features and demonstrate improved potential of tool-assisted LLMs to solve CTFs, however it exhausts the LLM context length when command output history becomes very long. InterCode-CTF manages this issue by truncating the history to only show the LLM the last few iterations. Even so, LLM agents face issues with longer tasks.
%NYU CTF Bench~\cite{shao2024nyu}, a benchmark of 200 CTF challenges, presents a baseline agent with specialized reverse engineering tools and category-specific prompts, demonstrating their importance to solve CTFs.
% The NYU CTF baseline agent faces issues of LLM context length when complex tasks run for several iterations and the entire command and output history becomes longer than the LLM's context window size. The InterCode agent manages this issue by truncating the history to only show the LLM the last few iterations.


Excessive tool availability and verbose interfaces can overwhelm agents, leading to inefficiencies. Agents perform better with a focused set of tools with well-defined interfaces~\cite{yang2024sweagent}.
EnIGMA~\cite{abramovich2024enigma} agent incorporates interactive tools and in-context learning techniques to achieve state-of-the-art results. % on the NYU CTF Bench, HackTheBox, and Cybench benchmarks.
For better context management, EnIGMA also uses an LLM summarizer that summarizes the command outputs for the main agent.

HackSynth~\cite{muzsai2024hacksynth}, an LLM agent for autonomous penetration testing, shows that iterative planning and feedback summarization stages help the agent finish multiple tasks and improves overall problem solving.
Similarly, Cybench~\cite{zhang2024cybenchframeworkevaluatingcybersecurity} introduces a benchmark of 40 CTF challenges augmented with step-by-step tasks, demonstrating better focus of LLM agents on smaller tasks, leading to improved success and alleviating the context length issue.
\citet{turtayev2024hacking} expand on InterCode-CTF by implementing plan-and-solve prompting, achieve significant improvement on the InterCode-CTF benchmark. They show that prompting techniques can improve performance even with simple toolsets.
% . Their baseline agent is evaluated in unguided mode (i.e. fully autonomous), and guided mode where the agent is given one task at a time. Their results indicate that providing smaller tasks to the LLM agents improve their focus yielding improved success on complex challenges while .

These works highlight that LLM agents excel at implementing code and executing commands to accomplish small concrete tasks when provided with dynamic feedback and task-specific toolsets. While these works  involved using multiple LLMs with different tasks such as planning and summarizing along-side a main agent, D-CIPHER is the first work to formulate a multi-agent system where there is a bifurcation of responsibilities between agents and meaningful well-defined interactions for dynamic feedback.
Table~\ref{tab:related_work_comparison} shows a feature comparison of D-CIPHER with related works on LLM agents for autonomous CTF solving.
%\meet{some description of the feature comparison?}
% Recent research has focused on enable autonomous solving of CTF challenges~\cite{shao2024empirical,shao2024nyu,abramovich2024enigma}. These agents typically operate in containerized environments to ensure reproducibility and modularity. 

% As an early effort, Tann et al.~\cite{tann2023using} evaluated the effectiveness of LLMs, such as OpenAI's ChatGPT, Google Bard, and Microsoft Bing, in solving cybersecurity CTF challenges and answering professional certification questions. 
% % Their study results show that LLMs performed well on $7$ CTF test cases, with ChatGPT solving $6$, Bard $2$, and Bing $1$. 
% The study shows that LLM responses often contain key information essential for solving tasks.

% The InterCode framework~\cite{yang2023intercode} approaches coding as an interactive process and uses execution feedback to improve code generation. As described in Yang et al.~\cite{yang2023intercode}, InterCode-CTF integrates CTF benchmarks into a reinforcement learning environment that can evaluate the cybersecurity capabilities of language agents. It features $100$ tasks that tapskills such as reverse engineering, forensics, and binary exploitation. While existing language agents demonstrate basic cybersecurity skills, evaluations indicate they struggle with more complicated complex tasks unless the system is fine-tuned or given external support. 
% cite Intercode: Standardizing and benchmarking interactive coding with execution feedback

% Another notable example is an LM agent developed by Shao et al. specifically to automate CTF tasks. 
% Shao et al.~\cite{shao2024empirical} developed a LM agent to automate CTF tasks.
% % They report an accuracy rate of  $46\%$ on $26$ CTF challenges sourced from CSAW'23 Qualifying round competition using GPT-4.
% By effectively combining LLM capabilities with external tools, the researchers demonstrated the potential of tool-assisted LLMs to solve complex problems. Building on this, the team incorporated a broader range of cybersecurity tools and interfaces that enhance both accuracy and versatility. 
% Empirical results show their system outperforms baselines on both the InterCode CTF benchmark and the NYU CTF benchmark.

% Shao et al.~\cite{shao2024nyu} presented a diverse, open-source database of CTF challenges that can be used to benchmark an LLM's ability to solve cybersecurity problems.
% It provides a scalable platform for developing and testing AI-driven approaches for vulnerability detection and resolution, facilitating advancements in automated cybersecurity tasks. The benchmark database and automated framework were successfully applied to the performance of five LLMs. 

% The Cybench benchmark~\cite{zhang2024cybenchframeworkevaluatingcybersecurity} provides another significant contribution by creating a framework tailored to solving CTF challenges. % Cybench: A framework for evaluating cybersecurity capabilities and risk
% % Their benchmark environment achieves an accuracy of $17.5\%$ using Claude 3.5 Sonnet. 
% Such frameworks operate in Linux-based containerized environments, such as Kali Linux, which includes pre-installed cybersecurity tools. However, excessive tool availability can overwhelm agents, leading to inefficiencies. Research indicates that agents perform better with a focused set of tools that have well-defined interfaces~\cite{yang2024sweagent}. % Swe-agent: Agent-computer interfaces enable automated software engineering



% Muzsai et al. introduced HackSynth~\cite{muzsai2024hacksynth}, an LLM-based agent for autonomous penetration testing. It uses a dual-module architecture that consists of a Planner and a Summarizer, allowing for iterative command generation and feedback processing. The framework is evaluated using two benchmark sets from platforms like PicoCTF~\cite{picoctf} and OverTheWire~\cite{overthewire}. These benchmarks address $200$ challenges drawn from various domains and difficulty levels. Results of their study show that HackSynth, especially with the GPT-4o model, achieves the best performance. This highlights the potential of LLM-based agents in advancing autonomous penetration testing.
 % Using basic prompting techniques and expanding tool availability, the study highlights how straightforward approaches can unlock the latent potential of LLMs for cybersecurity tasks. Their work emphasizes that simple LLM designs can effectively solve CTF challenges, and thus broaden the number of cybersecurity applications without the need for advanced engineering.

% \begin{table*}[]
%     \centering
%     \begin{tabular}{|c|c|>{\centering\arraybackslash}p{4.5cm}|c|c|c|c|c|c|}
%     \hline
%          \textbf{Study} & \textbf{Dynamic} & \textbf{Used} & \textbf{Multi-} & \textbf{Open} & \textbf{Automatic} & \textbf{Tool} & \textbf{\# of} & \textbf{\# of} \\
%          & \textbf{Prompt} & \textbf{Benchmarks} & \textbf{Agents} & \textbf{Dataset} & \textbf{Framework} & \textbf{Use} & \textbf{LLMs} & \textbf{CTFs}\\
%          \hline
%          Tann et al.~\cite{tann2023using} & \purplecross & Manual collected & \purplecross & \purplecross & \purplecross & \purplecross & $3$ & $7$ \\
%          \hline
%          InterCode-CTF~\cite{yang2023language} & \purplecross &  PicoCTF~\cite{picoctf} & \purplecross & \purplecross& \purplecross & \purplecross & $1$ & $100$  \\
%          \hline
%          Shao et al.~\cite{shao2024empirical} & \purplecross & CSAW 2023 & \purplecross & \purplecross & \tealcheck & \tealcheck & $4$ & $26$ \\
%          \hline
%          Shao et al.~\cite{shao2024nyu} & \purplecross & NYU CTF~\cite{shao2024nyu} & \purplecross & \tealcheck & \tealcheck & \tealcheck & $5$ & $200$ \\
%          \hline
%          Cybench~\cite{zhang2024cybenchframeworkevaluatingcybersecurity} & \purplecross & Cybench~\cite{zhang2024cybenchframeworkevaluatingcybersecurity}  & \purplecross & \tealcheck & \tealcheck & & $8$ & $40$ \\
%          \hline
%          EnIGMA~\cite{abramovich2024enigma} & \purplecross & NYU CTF~\cite{shao2024nyu}, InterCode-CTF~\cite{yang2023language},  HackTheBox~\cite{hackthebox} & \purplecross & \purplecross & \tealcheck & \tealcheck & $3$ & $350$ \\
%          \hline
%          HackSynth~\cite{muzsai2024hacksynth} & \purplecross & PicoCTF~\cite{picoctf}, OverTheWire~\cite{overthewire} & \tealcheck & \tealcheck & \tealcheck & \tealcheck & $8$ & $200$ \\
%          \hline
%          Turtayev et al.~\cite{turtayev2024hacking} & \purplecross & InterCode-CTF~\cite{yang2023language} & \purplecross & \purplecross & \purplecross & \purplecross & $4$ & $100$ \\
%          \hline
%          \textbf{D-CIPHER (Proposed)} & \tealcheck & NYU CTF~\cite{shao2024nyu}, Cybench \cite{zhang2024cybenchframeworkevaluatingcybersecurity}, HackTheBox \cite{hackthebox} & \tealcheck & \tealcheck & \tealcheck & \tealcheck & 5 & 290 \\
%          \hline
%     \end{tabular}
%     \caption{Comparison with LLM-based CTF solving Literature}
%     \label{tab:related_work_comparison}
% \end{table*}




% \subsection{Multi-agent framework}

% The use of multi-agent LLM systems in Capture the Flag (CTF) challenges is emerging as a powerful approach to enhance cybersecurity problem-solving. Multi-agent frameworks mimic team-based collaboration, where multiple LLM agents, each with specialized expertise, work together to tackle complex tasks. This approach reflects real-world cybersecurity operations, where success often depends on coordinated efforts from teams with diverse skills, each addressing different components of a security challenge.
% Multi-agent LLM systems are emerging as a powerful approach to enhance cybersecurity problem-solving by simulating team-based collaboration. Specialized agents, each with distinct objectives, work together to tackle different aspects of complex security tasks. This mirrors real-world cybersecurity operations, where coordinated efforts and diverse skills are essential for addressing evolving threats and vulnerabilities.

% CTF challenges cover a wide range of domains, including cryptography, reverse engineering, forensics, and web exploitation. Multi-agent systems can distribute the workload by assigning agents to handle specific tasks. This enables parallel problem-solving and emulates the collaborative nature of human teams. For example, one agent may specialize in guiding the fellow agents to what needs to be done, while another executes the instructions, ensuring that tasks are addressed without losing the context, and implementing reasoning from multiple LLMs. This division of labor boosts efficiency and enables problem-solving from multiple perspectives.
% This division of labor enhances efficiency and allows the system to approach problems from multiple perspectives, reflecting the interdisciplinary approach often used in cybersecurity teams.

% Guo et al.~\cite{guo2024largelanguagemodelbased} highlight the strengths of multi-agent LLMs in complex, multi-step tasks where different agents handle specific roles The framework HackSynth~\cite{muzsai2024hacksynth} is a multi-agent penetration testing framework in which agents operate collaboratively to address vulnerabilities in staged environments. Their work emphasizes that when agents work as a cohesive team, they outperform single-agent approaches. This is particularly true when facing layered, iterative challenges. 
% This team-based model of problem-solving aligns closely with how cybersecurity professionals approach real-world security incidents and penetration testing exercises.

% Multi-agent LLM systems have shown effectiveness in various other applications. For instance,  Audit-LLM~\cite{song2024audit} presents a multi-agent framework for insider threat detection using log analysis. It employs agents to decompose tasks, build tools, and use collaborative reasoning to enhance detection accuracy. Liu~\cite{liu2024multi} explores the application of LLM-based multi-agent systems to enhance incident response (IR) in cybersecurity. Utilizing the ``Backdoors \& Breaches" tabletop game as a simulation environment, the study examines centralized, decentralized, and hybrid team structures to assess how LLM agents can improve decision-making, adaptability, and coordination during cyberattack scenarios. AutoSafeCoder~\cite{nunez2024autosafecoder} is a multi-agent system designed to enhance the security of code generated by LLMs. The framework comprises three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent that identifies vulnerabilities through static analysis, and a Fuzzing Agent that performs dynamic testing using mutation-based fuzzing to detect runtime errors. By integrating both static and dynamic testing in an iterative process, AutoSafeCoder aims to produce secure, functionally correct code. 

% To enhance CTF-solving by promoting team-based specialization, we employ a multi-agent CTF solving agent. Within this framework, agents tackle tasks aligned with their strengths. Tasks are executed in parallel, improving efficiency and accelerating progress. Agents share insights, adapt refining strategies based on feedback, and overcome obstacles collectively. This collaborative approach boosts scalability, adaptability, and and resilience, and improves performance in complex challenges.

% This paper presents a comprehensive comparison of D-CIPHER with existing LLM-based CTF-solving literature, as shown in Table~\ref{tab:related_work_comparison}.
% This paper documents the results of  our comprehensive comparison of D-CIPHER with existing LLM-based CTF-solving literature. These results are presented in Table~\ref{tab:related_work_comparison}.