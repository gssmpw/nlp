\section{Discussion} \label{sec:discussion}

\subsection{Case Study: Auto-prompter failure on pwn} \label{sec:autoprompter_casestudy}

As discussed previously in Section~\ref{sec:accuracy},
on Claude 3.5 Sonnet, D-CIPHER with Auto-prompter performs worse on pwn challenges of NYU CTF Bench compared to D-CIPHER without Auto-prompter.
In this case study, we look at five pwn challenges where D-CIPHER with Auto-prompter failed but succeeded without it.
\\

% While in many challenges the auto-prompter revealed improvements on model accuracy and cost reduction Table \ref{tab:default_experiment} also demonstrates that Claude 3.5 Sonnet without auto-prompter achieved the highest overall percentage of solved challenges. 
% In the \textit{pwn} category  Claude 3.5 Sonnet without auto-prompter boosted its score by 15.4\% over when the feature was enabled. We analyze five \textit{pwn} challenges from NYU CTF Bench which were only solved when we removed auto-prompting feature.

\noindent
\textbf{\texttt{slithery}:} A python jail escape challenge. The challenge server allows executing python code but maintains a reject list of commands. The solution involves bypassing the reject list using \texttt{globals()} and  \texttt{\_\_builtins\_\_} to invoke python’s \texttt{os.system} for shell access. While the Auto-prompter successfully understood the purpose of this challenge and generated a proper description,  a misleading base64 encoding (which decodes as \texttt{\_\_import\_\_} ) threw the Auto-prompter in the wrong direction. It ended up generating a prompt that made the Planner and Executor focus on the wrong variables.
\\

\noindent
\textbf{\texttt{unlimited\_subway}:} A buffer overflow pwn challenge. The solution involves leaking the stack canary byte-by-byte using an arbitrary memory read function, exploiting a buffer overflow to overwrite the canary and redirect execution to the \texttt{print\_flag} function. The challenge binary file \textit{unlimited\_subway} was provided during the exploratory phase of the Auto-prompter. It attempted to run commands such as \texttt{strings} to understand the binary, but continually encountered errors, ultimately failing to generate a useful prompt for the Planner.
\\

\noindent
\textbf{\texttt{got\_milk}:} A global offset table attack. The solution exploits a format string vulnerability to overwrite the least significant byte of the global offset table address of a function \texttt{lose} with the corresponding byte of a function \textit{win}, redirecting execution to the desired function. The auto-prompter is unable to extract any key contextual information of the challenge during the exploratory phases. The Auto-prompter fails to generate a meaningful prompt, leaving the Planner unable to proceed with the challenge.
\\

\noindent
\textbf{\texttt{bigboy}:}  Another buffer overflow pwn challenge. The solution involves exploiting a simple buffer overflow by overwriting a specific memory value with \texttt{0xCAF3BAEE} multiple times to pass the check and execute the \texttt{/bin/bash} command. In this case, the Auto-prompter correctly analyzes the binary’s properties, behavior, and vulnerabilities  and generates a detailed prompt outlining the exploitation strategy, including payload construction and execution to solve the challenge. Despite this, the Planner fails to succeed.
\\

\noindent
\textbf{\texttt{baby\_boi}:} Another buffer overflow pwn challenge. The solution involves leveraging a buffer overflow to execute an ROP chain that reveals the \textit{libc} base, locates \texttt{/bin/sh} and \texttt{execve}, and calls \texttt{execve("/bin/sh", 0, 0)} to spawn a shell and retrieve the flag. The Auto-prompter generates a step-by-step prompt for exploiting the buffer overflow vulnerability, leveraging the \texttt{printf} overflow, and building a ROP chain to retrieve the flag. While the generated prompt covers all the necessary steps, it does not provide detailed information due to which the Planner fails.
\\

From the five cases, we observe that the primary shortcomings of the Auto-prompter arise from its inability to execute appropriate commands during exploration or its tendency to produce incomplete or insufficiently detailed summaries of findings. These limitations lead to missed opportunities to fully exploit the challenge’s vulnerabilities or generate comprehensive and actionable prompts, highlighting the Auto-prompter’s reliance on the capability and robustness of the underlying model.

\subsection{Case Study: Comparison with EnIGMA}
\label{sec:casestudy_enigma}

We analyze two challenges of NYU CTF Bench, one which is solved by EnIGMA  but not solved by D-CIPHER, and another which is solved by D-CIPHER but not solved by EnIGMA. We analyze the conversation logs from these two agents to compare the enhanced toolset proposed in EniGMA versus the collaborative multi-agent system of D-CIPHER. % Both EnIGMA and D-CIPHER were run with Claude 3.5 Sonnet.
\\

\noindent
\textbf{\texttt{collision\_course}:} Crypto challenge to decrpyt a binary and get the password.
EnIGMA succeeds while D-CIPHER fails with GPT 4o on this challenge.
%We tried solving misc challenge "algebra" on both D-CIPHER and EniGMA systems.
In D-CIPHER, the Auto-prompter spends several rounds going through the challenge files and generates a detailed prompt. The Planner decides not to explore the challenge and immediately delegates tasks to decipher the encrypted files. The Executors are able to finish the tasks initially, but every new Executor must re-examine files from the previous Executor which leads to errors eventually. Executors also frequently dump large files to the output, filling up their context and causing loss of focus.
EnIGMA's exploration of the challenge files is more fruitful because it has access to file reading and editing tools that help with long files. This allows EnIGMA to focus on implementing a script to perform a brute force attack. It goes through a few iterations to refine the script, but all of them are focused on the single task of implementing python code. This allows EnIGMA to eventually perfect the script, decrypt the binary and obtain the flag.
This case demonstrates the issues with D-CIPHER's multi-agent system where division of tasks hurts as it reduces oversight of each agent on the other's work. This motivates the need for more advanced and frequent interactions between not just Planner and Executor but among Executors too.
\\

% So the planner agent delegated the task to the executor to finish. The executor also tried to find web service on the port, go through the web server logs to learn more about the situation and using different ways to connect like: ss, netstat. However, it still did not connect to the server, the executor decided to repeat this process until a threshold of 100 before giving up and returning null. The planner did not receive any valid summary. EniGMA started with connecting to the server using connect start command, and successfully connected to it. Next, it use connect sendline command to communicate with the server and discovered this is a math challenge. Tried with different inputs, it discovered the server only accepts decimal, and creates a new python script to handle the connection and solve the equations automatically, with several rounds of editing the script based on the error, it successfully found the flag and submit it. The difference demonstrate even though D-CIPHER is good at trying different ways to solve a problem and delegate to executor to finish it, it still lacks some tools to complete the task. While EniGMA only tried with one way but with the help of different advanced tool usage, it succeeded in this way and found the flag.



\noindent
\textbf{\texttt{gibberish\_check}:} Reverse engineering a binary's password checker.
D-CIPHER succeeds while EnIGMA fails with Claude 3.5 Sonnet on this challenge.
D-CIPHER's and EnIGMA's their methodologies on this challenge diverge significantly.
%D-CIPHER demonstrates a collaborative and systematic solving mechanism, involving the Auto-prompter, Planner, and Executors. 
For D-CIPHER, The Auto-prompter lays the groundwork by defining the challenge scope, detailing critical elements like input validation, encoded strings, and anti-debugging mechanisms. The Planner devises a comprehensive strategy, combining static and dynamic analysis.
The Executors one by one successfully perform static and dynamic analysis of the binary and return helpful summaries to the Planner.
The Planner properly relays information among Executors, avoiding the issues seen in the last case.
The last Executor successfully implements the exploit script and obtains the flag from the server.
In contrast, EniGMA takes a more iterative and guesswork-driven approach, initially submitting plausible flag formats without deeper analysis. While it recognizes key insights about the challenge, it fails to perform detailed static and dynamic analysis or synthesize findings into actionable steps. This inability to leverage its tools and reason through the binary's logic result in failure. This comparison highlights D-CIPHER's strength in effective collaboration for  in-depth of analysis and exploitation of the challenge.

These two cases highlight the contrasting strengths and weaknesses of EnIGMA and D-CIPHER in solving complex CTF challenges. EnIGMA excels in focused, single-task refinement with a strong reliance on advanced tool usage, as demonstrated in the \textit{collision\_course} challenge, where its iterative scripting capabilities and effective mastery of various tools lead to success. In contrast, D-CIPHER’s collaborative approach shines in challenges requiring multi step analysis, such as \textit{gibberish\_check}. Its ability to maintain solution coherence through multi-agent collaboration enables detailed static and dynamic analysis, fostering more creative solutions. % This comparison underscores that while EnIGMA benefits from its streamlined execution and tool proficiency, D-CIPHER’s strength lies in its robustness and adaptability to long, complex challenge scenarios.

% The first Executor focused on static analysis, uncovering key control flow functions, such as \textit{FUN\_0010164a}, and identifying a target score of 505 critical for validation. Executor 2 complemented this with dynamic analysis, observing runtime behaviors and input processing patterns, while Executor 3 synthesized findings and confirmed insights through additional decompilation and iterative solver script implementation. This collaboration successfully decoded the challenge, demonstrating how shared expertise and systematic effort accelerate problem-solving. 


% Based on these two case studies, we could observe that the EniGMA is better on tool usage, it can make use of advanced tools to iteratively continue on one direction. But it lacks of complex analysis and general planning abilities. For D-CIPHER, it can plan the steps and execute according, decompose the complex task into executable ones. However, without the advanced tools, it may stuck in one task and could not solve with different attempts. 


% D-CIPHER:
% The challenge is solved through collaboration between the Prompter, Planner, and Executors. The Prompter defines the challenge scope, detailing the binary’s behavior, input validation, encoded strings, and anti-debugging mechanisms, laying a clear foundation for investigation. Building on this, the Planner devises a strategy, focusing on static analysis of key functions, input handling, and comparison logic, alongside dynamic analysis, ensuring a systematic and prioritized approach.

% Executor 1 performs a detailed static analysis, decompiling the binary and examining its functions to understand the control flow and identify key elements like the comparison function (\textit{FUN\_0010164a}) and its role in validating inputs. This analysis reveals the use of dynamic programming in scoring and highlights the target score of 505 as critical to producing the "Correct!" output. Executor 2 complements this work by conducting dynamic analysis, running the program to observe its runtime behavior, and analyzing how input strings are processed and scored. This includes identifying character-by-character comparisons and uncovering patterns in the scoring mechanism. Executor 3 synthesizes findings from both static and dynamic analyses, decompiling additional functions to confirm key observations and analyzing the encoded strings to find positional and frequency patterns. They further implement a solver script to test potential inputs iteratively.

% Through this collaborative effort, each entity contributes a unique perspective, from defining the problem to systematically breaking it down and validating the solution. The final discovery of the correct flag demonstrates how collaboration accelerates solving complex challenges by leveraging diverse expertise and shared insights.



% EniGMA:
% The EniGMA attempts systematic yet iterative approach to solving the "Gibberish Check" reverse engineering challenge. The agent starts by submitting guesses for the flag, including flag{m4k3_sur3_y0u_ch3ck_y0ur_b0und5} and flag{Th1s_1s_N0t_Th3_R1ght_Fl4g}, which are structured around common CTF patterns. These attempts are met with "Wrong flag!" responses, prompting the agent to consider analyzing the binary itself. It suggests using the file command to determine the binary's type and exploring it with strings to extract visible content, which could reveal useful clues. The agent identifies encoded gibberish strings and observes that the binary processes input through a scoring function, likely based on character comparisons. However, the reasoning and actions remain superficial, as deeper static or dynamic analysis—such as reverse-engineering the scoring function or examining control flow—are not pursued. Instead, the agent continues submitting inputs like flag{g1bb3r1sh_1s_4_r3al_th1ng}, which still rely on guesswork rather than deriving patterns from the binary's logic. Steps further emphasize the need to evaluate the encoded strings and scoring mechanism, but no significant progress is made in decoding or understanding these. Ultimately, despite having access to advanced tools and insights into potential binary behaviors, the agent fails to synthesize its findings into actionable steps, resulting in the challenge remaining unsolved. This highlights a key gap in its problem-solving approach—an inability to leverage its tools effectively and reason through the challenge—unlike collaborative models where diverse expertise and systematic investigation enable solving such tasks.


% \subsection{Limitations} % and Future Work}
% Despite the promising advancements demonstrated by PICOS, several limitations remain that warrant careful consideration. First, in this work we evaluated how the mixture combination of models impact the solution process, we proved that with the collaboration between stronger model and weaker model, the overall performance of the weaker model is also enhanced, however, all the solution process has a fixed setup of experiment, in other words, models is not able to change dynamically during the solution process, hence how a dynamic changing of the model backend during the solution process can be further explored. Second, the multi-agent collaborative paradigm introduces additional complexity that can result in communication overhead. Compared to the baseline single agent system, the PICOS has extra backend communication expense for the sub-agents such as executor and auto-prompting agents, which may lead to delay of the decision-making due to the communication overhead. Hence, the overhead caused in multi-agent collaboration compared to a single agent system needs to be carefully considered. Although PICOS consistently outperforms single-agent baseline in controlled experiments, real-world CTF competitions introduce unpredictable variables such as human adversaries and incomplete information about challenge specifies, which can hamper generalizability, the more advanced strategies for inter-agent communication regarding this unpredictable situation can be furthered enhanced.

% D-CIPHER shows significant improvement in autonomously solving CTF challenges, yet some limitations exist which merit consideration.
% There is no direct interaction between Executors and information exchange is bottlenecked via the planner. As seen in Section~\ref{sec:casestudy_enigma}, this causes failures.
% Errors in the initial exploration phase of the Auto-prompter cause a severe impact on the generated prompt, which inevitably biases the Planner in the wrong direction.
% This is seen clearly in the results on pwn with and without Auto-prompter and in the cases in Section~\ref{sec:autoprompter_casestudy}.
% % When combining stronger and weaker LLMs, the results are poorer, indicating that D-CIPHER cannot properly engage with weaker LLMs.
% Compared to single-agent systems, D-CIPHER requires additional backend communication for multiple agents, causing decision-making delays that may increase over-head if D-CIPHER is deployed in time-critical scenarios.

% Despite the promising advances demonstrated by D-CIPHER, several limitations merit further consideration. First, while our experiments evaluated how model mixtures impact the solution process, and showed that collaboration between stronger and weaker models can enhance overall performance, the experiment setup remained fixed throughout the solution process. In other words, the models were unable to dynamically change or adapt to solve the problem. Future research might therefore focus on strategies for dynamically modifying model backends. Second, the multi-agent collaborative paradigm inherently adds complexity, which can create communication overhead. Compared to a single-agent baseline, D-CIPHER requires additional backend communication among sub-agents, such as the executor and auto-prompting agents. These backend communications could potentially cause decision-making delays. It is thus crucial to account for this overhead when adopting a multi-agent system rather than a single-agent approach. 

% Although D-CIPHER outperforms single-agent baselines in controlled experiments, real-world CTFs introduce unpredictability in the form of human adversaries and incomplete challenge information. This unpredictability can hinder generalizability. More advanced inter-agent communication strategies suited to unpredictable environments may need to be developed.
%to address these situations.

