\begin{abstract}
% Capture the flag (CTF) challenges serve as a critical benchmark for assessing automated task planning skills, which requires participants to apply knowledge across a wide range of cybersecurity fields. The use of large language models (LLMs) has gained traction in various domains, and yet agentic systems such as \cite{shao2024nyu, abramovich2024enigma, turtayev2024hacking} have leveraged these LLMs to solve CTF challenges automatically. Reflecting the real CTF competitions, each CTF teams may have multiple members where team members can give feedback to each other, the current works mainly focus on single-agent system where the agentic system can only obtain the feedback from previous conversation. With the emerging of multi-agent systems, the cooperation of multiple agents in solving these CTFs remains underexplored. In this work, we investigate the potential of the multi-agent system to tackle CTF challenges, we introduced PICOS, a novel multi-agent system aimed at solving CTF challenges in a cooperation way. We proposed Executor Agent, which enhanced the conversation feedback based on the solution trajectories, we also proposed auto-prompting, which provide the agent with the challenge information more efficiently. We conducted extensive experiments across diverse benchmarks with a variety of models with comprehensive case studies to show the impact of these enhancements on CTF solving process, our finding reveals that the enhancement of the conversation prompting from the multi-agent system significantly improves the problem-solving by mitigating errors and enabling the dynamic adjustment of strategies during runtime.
Large Language Models (LLMs) have been used in cybersecurity in many ways, including their recent use as intelligent agent systems for autonomous security analysis. Capture the Flag (CTF) challenges serve as benchmarks for assessing the automated task-planning abilities of LLM agents across various cybersecurity skill sets. Early attempts to apply LLMs for solving CTF challenges relied on single-agent systems, where feedback was restricted to a single reasoning-action loop. This approach proved inadequate for handling complex CTF tasks. Drawing inspiration from real-world CTF competitions, where teams of experts collaborate, we introduce the D-CIPHER multi-agent LLM framework for collaborative CTF challenge solving. D-CIPHER integrates agents with distinct roles, enabling dynamic feedback loops to enhance reasoning on CTF challenges. It introduces the \textit{Planner-Executor agent system}, consisting of a Planner agent for overall problem-solving along with multiple heterogeneous Executor agents for individual tasks, facilitating efficient allocation of responsibilities among the LLMs. Additionally, D-CIPHER incorporates an \textit{Auto-prompter} agent, which improves problem-solving by exploring the challenge environment and generating a highly relevant initial prompt. We evaluate D-CIPHER on CTF benchmarks using multiple LLM models and conduct comprehensive studies to highlight the impact of our enhancements. Our results demonstrate that the multi-agent D-CIPHER system achieves a significant improvement in challenges solved, setting a state-of-the-art performance on three benchmarks: \textbf{22.0\%} on NYU CTF Bench, \textbf{22.5\%} on Cybench, and \textbf{44.0\%} on HackTheBox. D-CIPHER is available at \url{https://github.com/NYU-LLM-CTF/nyuctf_agents} as the \texttt{nyuctf\_multiagent} package.
\end{abstract}