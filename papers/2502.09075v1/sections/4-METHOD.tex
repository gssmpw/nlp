\section{Method}

\subsection{Overview}

As illustrated in Fig.~\ref{fig:overview}, we propose a two-stage method, consisting of an offline stage and an online stage. The purpose of the offline stage is to estimate camera parameters of some reference frames to better serve the efficient arbitrary viewpoint calibration during the online stage. Specifically, in the offline stage, we select some reference frames based on certain rules, and establish track information of 2D feature points between reference frames using an image feature matching algorithm. Based on this, we propose the novel PTZ-IBA algorithm (PTZ Incremental Bundle Adjustment), which primarily focuses on constraining the consistency of ray landmark projections for all viewpoints and \textit{automatically} obtains camera parameters in the \textit{local} coordinate system. 
Finally, in order to build the correspondences between 2D images and 3D real-world coordinates, by incorporating a few 2D-3D annotations, we further fine-tune the camera parameters and obtain the \textit{global} absolute poses of the camera.

After completing the calibration of all reference frames in the offline stage, the online stage becomes very intuitive. Given an image from a new viewpoint, we can calibrate the corresponding camera parameters using the similar aforementioned feature matching and optimization methods. It's worth noting that, our two-stage approach divides the heavy computational tasks into the offline stage and keeps the lighter computations for the online stage, ensuring a balance between accuracy and computational efficiency.


\subsection{Offline stage}

\subsubsection{Correspondence search}
We collect reference images and build data associations with feature matching in this step.

\textbf{Reference Frame Collection.}
PTZ cameras inherently offer wide field of view (FOV) coverage. To ensure comprehensive scene representation through reference images, a 360-degree image collection is essential. In practice, we typically capture 8 to 10 images uniformly at the camera's minimum zoom level to ensure sufficient overlap.
Depending on the scene's complexity, we also collect more images at different zoom and tilt levels, resulting in a final set of about 30 to 50 reference images.

\textbf{Feature Matching.}
To tackle the challenging variations in image appearance caused by illumination, weather, and seasonal changes, etc., for robustness, we employ the state-of-the-art deep feature extraction and matching techniques, such as SuperPoint~\cite{detone2018superpoint} and SuperGlue~\cite{sarlin20superglue}, to create pixel-level correspondences among all reference images. Additionally, we use RANSAC in conjunction with homography transformation to filter out outliers from the matching results.

\textbf{Tracks Building.}
To strengthen the global constraints across reference frames and estimate more accurate camera parameters, we consolidate pairwise matching results to establish tracks. A track represents the matching relationships of keypoints across multiple images. In the PTZ camera model, each track corresponds to a ray landmark. We employ the Union-Find algorithm to build the tracks~\cite{moulon2012unordered} and remove any that are either too short or have matching conflicts.

\subsubsection{PTZ-IBA}

In this step, we aim to determine the relative poses and intrinsic parameters of the reference frames. 
Simplify, we build a local coordinate system with an initial frame as reference, the camera's projection center $\mathbf{C}$ is $\mathbf{0}$.

\textbf{Initialization.}
Choosing the initial frame and the next best view is critical~\cite{schoenberger2016sfm}. We select the frame with the highest number of neighborhood matches as the initial starting frame. If the incremental optimization from this initial frame fails, we then opt for the frame with the second-highest number of neighborhood matches as the new starting frame, and continue this process as needed. For selecting the next best view, we pick the frame that has the most neighborhood matches among the neighbors of the already registered frames as the next frame to be registered. This strategy is key to ensuring the effectiveness of the parameter estimation process.

The initial focal length can be empirically determined by $f=1.2*\max(W, H)$, where $W$ and $H$ are width and height of image. The initial distortion coefficients can be set to 0, and the initial relative rotation $\mathbf{R}_{ij}$ between the image pair can be calculated as $\mathbf{R}_{ij}=\mathbf{K}_i^{-1}\mathbf{H}_{ij}\mathbf{K}_j$, where $\mathbf{H}_{ij}$ is the homography matrix between the image $i$ and $j$.

\textbf{Image Registration.}
A new image can be registered by solving an optimization problem that uses feature matches with images that have already been registered. 
First, the ray landmarks of the reference registered image can be derived based on the pixel correspondences. This step is essentially the reverse of the imaging model used for the PTZ camera. Denote the undistorted point on the reference image as $\mathbf{x}$, the corresponding ray landmark $\mathbf{r}$ can be calculated as follows:
\begin{equation}
\label{equ:pix_to_ray}
\begin{aligned}
    \mathbf{r}' = \mathbf{R}^{-1} \mathbf{K}^{-1} \mathbf{x}, \mathbf{r} = \frac{\mathbf{r}'}{\|\mathbf{r}'\|}. \\
\end{aligned}
\end{equation}

We minimize the reprojection error of the ray landmarks on the current frame $i$ to estimate the camera parameters $\mathbf{P}_i$:
\begin{equation}
\label{equ:img_reg}
    \mathbf{P}_i^* = \argmin_{\mathbf{P}_i} \sum^m_k \rho \left( \| \pi(\mathbf{P}_i, \mathbf{r}_k) - \mathbf{x}_{ik} \|^2 \right),
\end{equation}

\noindent where $\pi(\cdot)$ denotes the function that projects ray landmarks to image space, as described in Section \ref{sec:PRELIMINARY}. $\rho(\cdot)$ is a robust loss function that mitigates the influence of outliers. $\mathbf{x}_{ik}$ represents the observation of ray landmark $\mathbf{r}_k$ on the current image $i$, while $m$ denotes the number of rays.

\textbf{Rays Triangulation.}
Given that images captured by the PTZ camera do not involve translation, traditional triangulation methods cannot directly determine the positions of 3D landmarks.
In rays triangulation, a ray landmark is typically observed by multiple frames. Using the parameters from the multi-frame viewpoints as inputs, 
individual ray coordinates can be computed using (\ref{equ:pix_to_ray}). The final estimated position is then determined by calculating the average of these ray coordinates.

\textbf{Bundle Adjustment.}
Image registration and rays triangulation usually yield only approximate estimates. As the number of images increases, global bundle adjustment becomes crucial for mitigating drift errors and maintaining global consistency.
Global bundle adjustment performs joint optimization on all currently registered images and ray landmarks to yield more precise estimation results. The goal is to minimize all reprojection errors:
\begin{equation}
    \mathbf{P}^*, \mathbf{r}^* = \argmin_{\mathbf{P}, \mathbf{r}} \sum^n_i \sum^m_k \rho \left( \| \pi(\mathbf{P}_i, \mathbf{r}_k) - \mathbf{x}_{ik} \|^2 \right),
\end{equation}
\noindent where $n$ is the number of registered images and $m$ is the number of rays.

While global bundle adjustment can achieve high-accuracy parameter estimates, it requires significant computational resources.
Therefore, global bundle adjustment is performed only after the number of registered images increases by a certain factor. The entire process is conducted once all images are registered. Any image that fails to register will be discarded after a certain number of attempts.

\subsubsection{Georeferencing}
PTZ-IBA only provides the pose and camera intrinsic parameters within a \textit{local} coordinate system. To fulfill the requirements for practical applications in urban environments, we need additional 2D-3D correspondences to align the PTZ camera with geographic coordinate systems like UTM. 
The 2D keypoints are annotated from the images, while the 3D keypoints come from oblique 3D models or Digital Orthophoto Map (DOM) obtained from drones. To reduce the manual annotation costs, we select only a small number of images (typically 2 to 4) for 2D-3D annotations.

Denote $\mathbf{T}^l_w \in \mathrm{SE}(3)$ as the transformation matrix from the geographical coordinate system to the local coordinate system. We minimize the following loss function to obtain $\mathbf{T}^l_w$ and the fine-tuned camera parameters:
\begin{equation}
\begin{aligned}
    \mathbf{P}^*, \mathbf{r}^*, {\mathbf{T}^l_w}^* &= \argmin_{\mathbf{P}, \mathbf{r}, \mathbf{T}^l_w} \sum^n_i \sum^{m_1}_k \rho \left( \| \pi(\mathbf{P}_i, \mathbf{r}_k) - \mathbf{x}_{ik} \|^2 \right) \\
    &+ \sum_{i \in \mathbf{S}} \sum^{m_2}_j \rho \left( \| \pi(\mathbf{P}_i, \mathbf{T}^l_w \mathbf{X}_j) - \mathbf{x}_{ij} \|^2 \right),
\end{aligned}
\end{equation}
\noindent where $\mathbf{S}$ represents the set of annotated images and $\mathbf{X}$ denotes the annotated 3D points. 
To clarify, here $m_1$ represents the number of rays, and $m_2$ indicates the number of 2D-3D correspondences. Once $\mathbf{T}^l_w$ is obtained, the absolute poses in the geographical coordinate system can be recovered. 

\subsection{Online stage}

During the online stage, we aim to estimate the camera parameters of arbitrary viewpoints efficiently and accurately. We model the online parameter estimation as a relocalization problem. Specifically, with a set of views calibrated in the offline stage, we determine the optimal camera parameters for the given view in order to best align with the correspondences. Initially, we match the current input image with the most recent image captured from the video stream, perform image registration, and estimate the initial camera parameters using (\ref{equ:img_reg}). Notably, the initial camera parameters may contain accumulated errors. We compute the frustum overlap between the current image and all the calibrated reference image based on the estimated initial parameters. 
We perform matching on images with an overlap greater than a threshold and select the best-matched image as the reference frame. Finally, the camera parameters for the current frame are obtained using (\ref{equ:img_reg}).
This substantially reduces the number of matches required, thereby speeding up the computation in the online stage as well as improving the accuracy.