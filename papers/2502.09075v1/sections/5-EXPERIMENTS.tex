\section{Experiments}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/worldcup-compare.pdf}
    \caption{\textbf{Comparisons of sports field registration on the WorldCup dataset.} The sports field template is projected onto the image in red based on the estimated camera parameters. Misregistrations are pointed out with yellow arrows.}
    \label{fig:worldcup-compare}
\end{figure*}

\subsection{Experimental setup}

\textbf{WorldCup dataset~\cite{homayounfar2017sports}.}
This dataset, widely used for sports field registration, was collected during the 2014 World Cup held in Brazil. It includes 209 training images and 186 test images from different matches. Each image is manually annotated with a homography matrix and grass segmentation. Unlike typical methods that use the homography for transformations, our algorithm estimates the camera's intrinsic and extrinsic parameters to derive the geometric transformation.

Since our method requires a fixed camera position, we only use a subset, excluding matches like COL vs. IVC and ARG vs. IRN with insufficient coverage. We assume constant camera positions for matches at the same venue and conduct experiments accordingly, as detailed in Table~\ref{tab:subset_dataset}. Each training image is annotated with 30 random 2D-3D correspondences.

\begin{table}[ht]
\caption{Subset dataset used in experiments.}
\label{tab:subset_dataset}
    \centering
    \scalebox{0.75}{
    \begin{tabular}{c c c c c c}
       \hline
       \multirow{2}{*}{Split} & \multirow{2}{*}{Sports Venue} & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Test} \\
       \cline{3-6}
        & & Match & Frame ID & Match & Frame ID \\
        \hline
        1 & Arena Corinthians & NED vs. ARG & 1-15 & ARG vs. SUI & 159-175 \\
        2 & Arena Corinthians & NED vs. ARG & 1-15 & BRA vs. CRO & 1-22 \\
        3 & Arena Corinthians & NED vs. ARG & 1-15 & URU vs. ENG & 85-96 \\
        4 & Arena Pernambuco & USA vs. GER & 120-134 & CRO vs. MEX & 126-142 \\
        5 & Maracanã Stadium & GER vs. ARG & 135-149 & ESP vs. CHI & 97-110 \\
        6 & Maracanã Stadium & GER vs. ARG & 135-149 & FRA vs. GER & 176-186 \\
        7 & Arena Fonte Nova & GER vs. POR & 170-191 & SUI vs. FRA & 111-125 \\
        \hline
    \end{tabular}
    }
\end{table}


As the WorldCup dataset lacks explicit camera parameters, we evaluate the accuracy using the Intersection over Union (IoU), divided into $IoU_{part}$ and $IoU_{whole}$. $IoU_{part}$ calculates the IoU for the camera's visible area projected onto the Bird's Eye View (BEV), using both predicted and ground truth parameters, but within the sports field template. For $IoU_{whole}$, we project the full visible area onto BEV using ground truth parameters, then re-project it onto the image with both predicted and ground truth parameters to compute the IoU between these projections.

\textbf{Synthetic dataset.}
Due to the lack of publicly available datasets for PTZ camera parameter estimation, we create a synthetic dataset using Blender for assessment. We design 10 datasets, split evenly for both indoor and outdoor scenes, each from a single PTZ camera with 3 random configurations of focal lengths and tilt angles. Each configuration involves a full rotation of the camera, producing 180 images that cover the scene. Each image provides ground truth camera intrinsic and extrinsic parameters. Additionally, we generate several 3D points around the camera and project them onto the images to form 2D-3D correspondences, with each image containing 30 pairs. To simulate real-world conditions, zero-mean Gaussian noise of 3.0 pixels was added to each projected point. We plan to release this dataset to the public.

For evaluating the intrinsic parameter, we use the absolute focal length error $FLE = |f - f'|$, where $f$ is the predicted and $f'$ is the ground truth. For extrinsic parameters, we employ the Absolute Pose Error metrics, specifically the rotation angle error $APE_{rot}$ and the translation error $APE_{trans}$.

\textbf{Implementation Details.} 
Our system is implemented in C++ on a CentOS platform, utilizing 2.50 GHz Intel CPU and NVIDIA T4 GPU. For feature matching, we employed fp16 quantization acceleration for SuperPoint and SuperGlue. The RANSAC geometric verification threshold is set to 4 pixels, with a minimum number of matching pairs set at 40. Non-linear optimization is implemented using the Ceres~\cite{Ceres}.

\subsection{Results}

\textbf{WorldCup dataset.}
We selected the open-sourced state-of-the-art sports field registration methods~\cite{Chen2019SportsCC,jiang2020optimizing,Nie2021ARA,Chu_2022_CVPR,Theiner:2023:TVCalib} for comparison. In our experiments, we utilized the pre-trained weights and default parameters for fairness.

As in Table~\ref{tab:Quantitative_WorldCup}, our method achieves the best performance. We projected the sports field template onto the 2D images for overlay visualization. As in Fig. \ref{fig:worldcup-compare}, our results estimate the best registration results compared with other methods. Undistortion corrections are visualized in Fig.~\ref{fig:all-undistort}, highlighting our effective estimation of distortion coefficients, which straightens field lines in the undistorted images.

\begin{table}[t!]
%\caption{Quantitative results on the WorldCup dataset.}
\caption{Comparisons of IoU on the WorldCup dataset.}
\label{tab:Quantitative_WorldCup}
    \centering
    \resizebox{0.63\linewidth}{!}{%
    \begin{tabular}{c c c c c}
       \hline
       \multirow{2}{*}{Method} & \multicolumn{2}{c}{$IoU_{part}\uparrow$} & \multicolumn{2}{c}{$IoU_{whole}\uparrow$} \\
       \cline{2-5}
        & mean & median & mean & median \\
        \hline
        SCCvSD\cite{Chen2019SportsCC} & 95.7 & 96.9 & 90.6 & 92.3 \\
        Sportsfield\cite{jiang2020optimizing} & 96.2 & 97.4 & 91.1 & 93.3 \\
        Robust\cite{Nie2021ARA} & 96.5 & 97.6 & 91.4 & 93.0 \\
        KpSFR\cite{Chu_2022_CVPR} & 96.7 & 97.6 & 92.5 & 94.2 \\
        TVCalib\cite{Theiner:2023:TVCalib} & 95.8 & 97.0 & 90.9 & 93.2 \\
        Ours & \textbf{96.8} & \textbf{97.8} & \textbf{93.1} & \textbf{94.4} \\
        \hline
    \end{tabular}%
    }
\end{table}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/all-undistort.pdf}
    \caption{\textbf{Undistortion results.} Regions to focus on are highlighted.}
    \label{fig:all-undistort}
\end{figure}

\textbf{Synthetic dataset.}
We first compared our method with PTZ-SLAM~\cite{PTZ-SLAM} on synthetic datasets. Since the open-source version of PTZ-SLAM only supports a limited range of pan angles and its running speed has not been optimized, for each scene we selected 10 images for comparison within a 180-degree pan angle range, with the first frame annotated. The camera parameters of the first frame required by PTZ-SLAM were calculated by minimizing the 2D-3D reprojection error using the method described in \cite{hartley2003MVG}.
Results in Table~\ref{tab:Quantitative_Synthetic_Comp} indicts our better performance. Qualitatively, our projections into BEV in Fig.~\ref{fig:bev-180-compare}, align more closely with the ground truth, without notable misalignment at the stitching edges.

\begin{table}[h]
\caption{Comparisons on the Synthetic dataset (PTZ-SLAM vs. Ours).}
\label{tab:Quantitative_Synthetic_Comp}
\centering
    \resizebox{0.95\linewidth}{!}{%
    \begin{tabular}{c c c c c c c}
       \hline
       \multirow{2}{*}{Scenes} & \multicolumn{2}{c}{$FLE(pix) \downarrow$} & \multicolumn{2}{c}{$APE_{rot}(\deg) \downarrow$} & \multicolumn{2}{c}{$APE_{trans}(m)\downarrow$} \\
       \cline{2-7}
       & mean & median & mean & median & mean & median \\
        \hline
        Scene-01 & 3.49, \textbf{0.34} & 3.45, \textbf{0.28} & 0.13, \textbf{0.10} & 0.12, \textbf{0.10} & 0.21, \textbf{0.08} & 0.21, \textbf{0.08} \\
        Scene-02 & 141.49, \textbf{2.49} & 12.42, \textbf{2.58} & 2.36, \textbf{0.10} & 0.21, \textbf{0.09} & 0.32, \textbf{0.14} & 0.32, \textbf{0.14} \\
        Scene-03 & 5.67, \textbf{1.59} & 5.60, \textbf{1.51} & 0.20, \textbf{0.12} & 0.16, \textbf{0.12} & 0.40, \textbf{0.02} & 0.40, \textbf{0.02} \\
        Scene-04 & 21.23, \textbf{1.36} & 11.67, \textbf{1.35} & 0.43, \textbf{0.10} & 0.27, \textbf{0.10} & 0.32, \textbf{0.05} & 0.32, \textbf{0.05} \\
        Scene-05 & \textbf{1.14}, 4.36 & \textbf{0.57}, 4.15 & 0.20, \textbf{0.11} & 0.20, \textbf{0.11} & \textbf{0.14}, \textbf{0.14} & \textbf{0.14}, \textbf{0.14} \\
        Scene-06 & \textbf{0.75}, 1.60 & \textbf{0.78}, 1.65 & \textbf{0.04}, 0.06 & \textbf{0.04}, 0.05 & \textbf{0.06}, 0.10 & \textbf{0.06}, 0.10 \\
        Scene-07 & 7.25, \textbf{3.20} & 6.86, \textbf{2.76} & 0.17, \textbf{0.09} & 0.15, \textbf{0.08} & 0.27, \textbf{0.17} & 0.27, \textbf{0.17} \\
        Scene-08 & 7.82, \textbf{2.44} & 7.68, \textbf{2.41} & \textbf{0.24}, 0.25 & 0.26, \textbf{0.24} & 0.23, \textbf{0.15} & 0.23, \textbf{0.15} \\
        Scene-09 & 6.74, \textbf{1.60} & 6.62, \textbf{1.61} & \textbf{0.15}, 0.17 & \textbf{0.13}, 0.17 & 0.32, \textbf{0.13} & 0.32, \textbf{0.13} \\
        Scene-10 & 6.93, \textbf{2.27} & 6.75, \textbf{2.35} & 0.21, \textbf{0.14} & 0.19, \textbf{0.14} & 0.28, \textbf{0.16} & 0.28, \textbf{0.16} \\
        \hline
        All & 20.25, \textbf{2.13} & 6.16, \textbf{1.86} & 0.41, \textbf{0.12} & 0.16, \textbf{0.11} & 0.26, \textbf{0.11} & 0.28, \textbf{0.13} \\
        \hline
    \end{tabular}%
    }
\end{table}

Then we uniformly selected 30 images panning 360-degree per scene for offline evaluation, with 2 images annotated. We tested the online stage using the remaining 150 images per scene. In Table~\ref{tab:Quantitative_Synthetic_both}, our method consistently achieved high-precision  calibration results in both stages, using only a limited number of annotations. Besides, we add random distortions to test our estimation of distortion coefficients. In Fig.~\ref{fig:all-undistort} (right), the originals show noticeable barrel distortion. Our method effectively corrects this, accurately restoring the geometry and ensuring the straight lines.

\subsection{Performance}

Time consumption for both offline and online stages is detailed in Table~\ref{tab:time_analysis}. The offline stage involves processing 30 images, while the online stage is averaged per image. Efficiency can be increased by reducing reference frames, as most time is spent on feature extraction and matching.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/bev-180-compare.pdf}
    \caption{\textbf{Qualitative comparison on the synthetic dataset.} The red boxes indicated the regions to focus on.}
    \label{fig:bev-180-compare}
\end{figure}

\begin{table}[h]
\caption{Statistics of calibration error on the synthetic dataset.}
\label{tab:Quantitative_Synthetic_both}
    \centering
    \resizebox{0.95\linewidth}{!}{%
    \begin{tabular}{ccccccc|cccccc}
    \hline
    \multirow{3}{*}{Scenes} & \multicolumn{6}{c|}{Offline Stage} & \multicolumn{6}{c}{Online Stage} \\ \cline{2-13} 
     & \multicolumn{2}{c}{$FLE(pix) \downarrow$} & \multicolumn{2}{c}{$APE_{rot}(\deg) \downarrow$} & \multicolumn{2}{c|}{$APE_{trans}(m)\downarrow$} & \multicolumn{2}{c}{$FLE(pix) \downarrow$} & \multicolumn{2}{c}{$APE_{rot}(\deg) \downarrow$} & \multicolumn{2}{c}{$APE_{trans}(m)\downarrow$} \\ \cline{2-13} 
     & mean & median & mean & median & mean & median & mean & median & mean & median & mean & median \\ \hline
    Scene-01 & 1.07 & 0.69 & 0.09 & 0.08 & 0.02 & 0.02 & 1.28 & 0.84 & 0.10 & 0.09 & 0.02 & 0.02 \\
    Scene-02 & 2.34 & 1.96 & 0.17 & 0.15 & 0.14 & 0.14 & 2.62 & 1.96 & 0.21 & 0.17 & 0.14 & 0.14 \\
    Scene-03 & 0.42 & 0.36 & 0.16 & 0.16 & 0.11 & 0.11 & 0.59 & 0.44 & 0.17 & 0.16 & 0.11 & 0.11 \\
    Scene-04 & 1.73 & 1.35 & 0.14 & 0.13 & 0.03 & 0.03 & 1.66 & 1.29 & 0.15 & 0.14 & 0.03 & 0.03 \\
    Scene-05 & 4.86 & 4.94 & 0.12 & 0.13 & 0.10 & 0.10 & 5.12 & 4.97 & 0.14 & 0.13 & 0.10 & 0.10 \\
    Scene-06 & 0.94 & 0.71 & 0.12 & 0.12 & 0.07 & 0.07 & 1.15 & 0.86 & 0.11 & 0.10 & 0.07 & 0.07 \\
    Scene-07 & 1.37 & 1.02 & 0.11 & 0.10 & 0.11 & 0.11 & 1.50 & 1.42 & 0.13 & 0.11 & 0.11 & 0.11 \\
    Scene-08 & 1.18 & 1.15 & 0.06 & 0.06 & 0.10 & 0.10 & 1.14 & 0.95 & 0.08 & 0.07 & 0.10 & 0.10 \\
    Scene-09 & 0.48 & 0.28 & 0.10 & 0.11 & 0.07 & 0.07 & 0.69 & 0.51 & 0.11 & 0.11 & 0.07 & 0.07 \\
    Scene-10 & 0.77 & 0.72 & 0.08 & 0.09 & 0.04 & 0.04 & 0.75 & 0.65 & 0.09 & 0.09 & 0.04 & 0.04 \\
    \hline
    All & 1.52 & 0.97 & 0.12 & 0.11 & 0.08 & 0.08 & 1.65 & 1.02 & 0.13 & 0.12 & 0.08 & 0.08 \\ \hline
    \end{tabular}%
    }
\end{table}

\begin{table}[h]
\caption{Computational time of our method (in seconds).}
\label{tab:time_analysis}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{c c c c c}
       \hline
        & Feature extraction & Feature matching & \parbox{1.5cm}{Non-linear optimization} & Total \\
       \hline
       Offline stage & 2.54 & 8.81 & 11.86 & 29.80 \\
       Online stage & 0.06 & 0.34 & 0.05 & 0.68 \\
        \hline
    \end{tabular}
    }
\end{table}

\subsection{Ablation study}

\textbf{Feature Extraction and Matching.}
We evaluated the impact of different feature extraction and matching algorithms on the offline stage by testing ORB~\cite{orb}, SIFT~\cite{Lowe2004DistinctiveIF}, and KNN feature matching, as shown in Table \ref{tab:ablation_matching}. ORB features often resulted in higher mismatches and lower performance. Although SIFT features provided higher sub-pixel level precision, slightly outperformed SuperPoint and SuperGlue, it is sensitive to illumination and weather changes. We will keep feature matching as a pluggable module for adaptability.

\begin{table}[t!]
\caption{Comparisons of different feature matching methods.}
\label{tab:ablation_matching}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c c c c c c c}
       \hline
       \multirow{2}{*}{Scenes} & \multicolumn{2}{c}{$FLE(pix) \downarrow$} & \multicolumn{2}{c}{$APE_{rot}(\deg) \downarrow$} & \multicolumn{2}{c}{$APE_{trans}(m)\downarrow$} \\
       \cline{2-7}
       & mean & median & mean & median & mean & median \\
        \hline
        Ours+ORB & 16.48 & 1.16 & 2.42 & 0.17 & 0.27 & \textbf{0.06} \\
        Ours+SIFT & \textbf{0.80} & \textbf{0.32} & \textbf{0.12} & 0.12 & \textbf{0.08} & 0.07 \\
        Ours+SP-SG & 1.52 & 0.97 & \textbf{0.12} & \textbf{0.11} & \textbf{0.08} & 0.08 \\
        \hline
    \end{tabular}
    }
\end{table}

\subsection{Applications}

One application is to apply the method on the PTZ camera at the crossroad and project the detected objects onto the map to understand the traffic condition. As illustrated in Fig.~\ref{fig:application} (a), our algorithm can effectively calibrate images with different illuminations. Specifically, Fig.~\ref{fig:application} (b) demonstrates the precise mapping of pedestrians and vehicles onto Google Maps, achieved through the estimated camera parameters. Additionally, the camera's viewshed, or the area visible from its position, is estimated and depicted in red, further enhancing situational awareness.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/application.pdf}
    \caption{The application in traffic situation awareness.}
    \label{fig:application}
\end{figure}