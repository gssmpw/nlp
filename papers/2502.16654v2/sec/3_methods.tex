\section{Preliminary}

Deep supervision techniques~\cite{cDeeplySupervisedNets,cDeepSupervisedCNN} have proven to be effective in experiments over the past decade.
%
Using auxiliary loss~\cite{cPSPNet,cDualAttention,cCAA} on the backbone is a common practice in deep supervision.
%
Although there is no theoretical consensus on its effectiveness, we believe that deep supervision helps the early shallow layers of the network align better with the optimization target (i.e. loss function).

One of the reasons why masked decoders are so effective is that there is a good chance that they are indirectly using deep supervision or even an enhanced version of it.
%
In the mask decoders~\cite{cMaXDeepLab,cKMaXDeepLab,cMask2Former}, the class token interacts repeatedly with feature maps from various levels, including both deep and shallow layers, through cross-attention. 
%
This interaction not only ensures that the shallow layers can indirectly enjoy the deep supervision from the optimization target but also helps them align more effectively with the class token, which is essential for the final classification.


\section{Proposed Method}
\label{sec:method}
This work presents two methods. 
The first method is Visual Context Replay (VCR), which is a simple, efficient, and effective technique for enhancing the decoder's input features. 
The second method is UpViT, which reveals the inherent high-resolution features that are typically thought to be absent in ViT~\cite{cViT}.

\subsection{Visual Context Replay (VCR)}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/overall_arch.pdf}
    \caption{Our proposed VPNeXt consists of two main modules: VCR and ViTUp, which focus on enhancing features efficiently and addressing upsampling challenges respectively.}
    \label{fig:overall}
\end{figure*}

VCR is a lightweight and innovative feature enhancement technology for deep supervision that performs comparably to the mask decoder while having zero inference overhead.

For an N-layer ViT backbone network, we define the output feature map of each layer as $\mathbf{x}_{i}$, $i$ represents the layer index.
%
Improving the robustness of those intermediate layers can be helpful for the final output.
%
In mask decoders, the outputs of two to three intermediate layers are typically optimized using skip connections. 
%
At VCR, we also optimize two intermediate layers, referred to as $\mathbf{x}_{a}$ and $\mathbf{x}_{b}$.

To avoid the inference computation overhead of the Mask decoder's attention while still providing supervision for these intermediate layers, traditional deep supervision appears to be a suitable approach without introducing extra inference computation overhead.
%
However, as mentioned in previous sections, deep supervision supervises each intermediate layer independently, which does not align with others using class (mask) tokens as the mask decoder does, resulting in less effectiveness.

One simple improvement idea is to utilize the final output feature map from the last layer (which we define as $\mathbf{x}_{z}$) to supervise the feature maps of the intermediate layers, in order to enforce the alignment.
%
%
We refer to this strategy as "naive align," which has the following loss function:
\begin{equation}
    L_{\text{naive-align}} = \sum_{i\in \{ a,b\}}\text{MSE}(\mathbf{x}_{z},~\mathbf{x}_{i})
\end{equation}


The alignment targets the last layer feature map because it typically provides a better representation than intermediate layers.
%
However, this operation is clearly ineffective. 
It is impossible to generate the same representation in an early intermediate layer as in the final layer because each intermediate layer serves a specific function that contributes to the progressive creation of the final output. 
If this were not true, we would only need a single layer without hidden layers.

Thus, We need to have control over what is effective for alignment.
%
This is what VCR represents. 
%
The replay mechanism takes the visual context from the final layer and replays it to the intermediate layers. 
%
This helps the deep supervision achieve the same level of effectiveness as the mask decoder.

Specifically, we believe only the spatial relation is worthwhile and useful for the alignment because the spatial context-aggregation (e.g., convolution, spatial attention, and MLP-mixer) is essential when performing the pixel-encoding.
%
Therefore, VCR aligns the intermediate layers towards two dimensions: local context and global context, to achieve alignment at both short-range and long-range levels.

\subsubsection{Local context replay}
The implementation of local context replay utilizes a deformable convolutional operation, where the "offset" serves as the key parameter for calculating the positions relative to the center for local context aggregation.
%
The local context replay operation is illustrated below:
\begin{equation}
    \mathbf{\gamma}_{i} = \text{Deformable}(\mathbf{x}_{i}~,~\mathbf{\sigma}_{z},~\mathbf{\varrho_{i}}),~i \in \{a, b\}
\end{equation}

During the replay process, the learnable offset $\mathbf{\sigma}_{z}$ from the final output layer $z$ will be synchronized directly with the intermediate layers. 
%
Notation $\mathbf{\varrho_{i}}$ stands for the other learnable parameters in deformable convolution.
%
The replay mechanism based alignment allows the intermediate layers to perform local context aggregation operations at the same position.
%


\subsubsection{Global context replay}
Unlike the positional-sensitivity local context replay, the global context replay, which is the final step of VCR, emphasizes the context most relevant to classification. 
%
The global context replay is based on the concept that intra-class pixels within the same context share similar feature representations.
%
As stated before, the intermediate layers struggle to learn strong feature representations related to specific categories since they serve their own purpose.
%
We can allow them to interact only with intra-class pixels, enabling them to learn the useful encoding process effectively.
%
To accomplish this, VCR replays the dot-product pixel affinity $\mathbf{\Lambda}_{z}$ as a spatial relation prior and regularizes the encoding of intermediate layers.
%
The global context replay operation is illustrated below:

\begin{equation}
    \mathbf{y}_{i} = \text{Attention}(\mathbf{\Lambda}_{z},~\phi_{i}(\mathbf{\gamma}_{i})),~i\in \{a, b\}
\end{equation}

The notation $\phi_{i}$ represents the linear projection of the attention operation. Its input $\mathbf{\gamma}_{i}$ is the 'value' derived from the previous equation.\\

After performing the VCR, which includes both local context replay and global context replay, the feature representations from the intermediate layers align effectively with the final feature representation. This alignment is achieved without any additional inference overhead, serving as a form of deep supervision. Furthermore, the VCR method demonstrated performance comparable to that of mask decoders in our experiments. 
We will provide the details of these experiments in the following sections.


\subsection{ViTUp}

As described in the earlier sections, ViT does not generate multi-scale (\eg different resolution) pyramid feature maps across multiple stages, necessitating the creation of mock multi-scale pyramid feature maps before utilizing commonly applied pyramid upsamplers, which renders them ineffective (please refer to the previous sections for more details).

However, we observed that the plain ViT generates a hidden high-resolution pyramid feature map, which can be effectively utilized for pyramid upsampling.
%
Given an input image $\mathbf{I}$, the plain ViT model uses patch embedding for tokenization, resulting in a feature map $\mathbf{x}_{0}$ that is typically 1/16 the size of the input image, as shown below:

\begin{equation}
    \mathbf{x}_{0} = \theta(\mathbf{I}, \mathbf{K}_{\text{16}}, \mathbf{S}_{\text{16}})
    \label{eq::patch-embedding}
\end{equation}

In practical applications, patch embedding is implemented using a 2D convolution operation $\theta$. 
%
In this process, both the kernel size $\mathbf{K}$ and stride $\mathbf{S}$ are set to the same value as the patch size. 
%
For example, if the patch size is 16, the kernel size and stride are also set to 16, as represented by $\mathbf{K}_{\text{16}}$ and $\mathbf{S}_{\text{16}}$ in equation~\ref{eq::patch-embedding}.

Inspired by the DeepLab series~\cite{cDeepLab}, reducing or eliminating the stride enables the creation of a larger resolution feature map without affecting the range of spatial context aggregation, also known as the receptive field. 
%
By adjusting the stride of patch embedding to a value smaller than the patch size (for example, using a stride of 4 for the typical pyramid upsampler), a hidden high-resolution pyramid feature map with a large size can be extracted, as shown below.

\begin{equation}
    \mathbf{x}_{0} = \theta(\mathbf{I}, \mathbf{K}_{\text{16}}, \mathbf{S}_{\text{4}})
\end{equation}

Note that, in VPNeXt, we only need to calculate the patch embedding $\theta$ once for 1/4, then downsample it by a factor of 4 for 1/16 required by the ViT.

After obtaining the hidden high-resolution pyramid feature map, we observed a minor difference compared to existing pyramid upsampler-based models. 
%
For instance, models like FPN, UperNet, and FaPN typically utilize multi-scale (two high-resolution) feature maps (excluding the final output from the backbone), while our approach extracts only a single high-resolution feature map. 
%
In comparison to DeepLabV3+, although it also uses a single high-resolution feature map, this feature map is derived from a deeper intermediate layer of the backbone than ours. 
The feature map from this deeper intermediate layer generally offers better encoding and has a smaller alignment gap with the backbone's final output.

To enhance the effectiveness and smoothness of the pyramid upsampler with our shallow pyramid feature map, we proposed the High-Level Context Local Refiner (HiCLR).
%
HiCLR employs a coarse-to-fine strategy that uses multiple iterations of refinement to progressively reduce the alignment gap between the high-resolution pyramid feature from the shallow layer and the low-resolution feature from the final backbone output.
%
Each refinement iteration takes two inputs: a high-level backbone feature map and an upsampled feature map. In the first iteration, the upsampled feature map is derived from the extracted hidden high-resolution pyramid feature. 
%
In subsequent iterations, the upsampled feature map is obtained from the output of the previous iteration.

In the refinement process, we use a method similar to VCR, where the spatial context of high-level features is leveraged to align the upsampled features. The key difference is that HiCLR concentrates exclusively on local context refinement, as restoring the missing local details is sufficient for the upsampling operations. 
%
This concept is also widely implemented in other upsamplers, such as the $3\times3$ convolutions used in DeepLab V3+ and UperNet.