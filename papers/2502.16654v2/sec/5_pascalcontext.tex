\section{Experiments on Pascal Context Dataset}
%\raggedbottom
The Pascal Context~\cite{cPascalContext} dataset comprises 4,998 training images and 5,105 testing images. We utilize its 59 semantic classes to perform ablation studies and experiments, following common practice. Unless otherwise specified, we train the models on the training set for 20K iterations.

In the ablation studies, we follow the VPNeXt's forward propagation sequence. 
%
First, we assess the effectiveness of VCR alone, and then we incorporate ViTUp to evaluate its ability to upsample the feature maps produced by VCR.
%
Finally, we conducted an analysis of computational overhead to evaluate the efficiency of our proposed VPNeXt.

%%%%%%%% VCR ablation studies %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation studies on VCR}
We compare our proposed VCR with a mask decoder (w/o pyramid, \eg segmenter~\cite{cSegmenter}) and deep supervision, as discussed in previous sections. 
%
As shown in Table~\ref{tab:exps:vcr-ablation-studes}, incorporating visual context in deep supervision results in an even better mIOU than the mask decoder (68.83\% vs 67.88\%). 

Additionally, we conducted ablation studies to determine the optimal number of deep supervision layers to use. 
%
The results in Table~\ref{tab:exps:vcr-ablation-studes} indicate that the mIOU reaches its highest value when two intermediate layers are employed for VCR-oriented deep supervision.


\begin{table}[ht]
    \centering
    \caption{Ablation studies on VCR, all the results are obtained under single-scale without flipping.
    All baseline models are trained using the same backbone and settings.
    \textit{DS:} Deep supervision.
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c}
       \toprule
       Methods & Num\# DS layers &  \quad mIOU(\%)\quad \\
       \midrule
       Deep supervision & 2 & 66.50 \\
       \midrule
       Mask decoder  & 2 (implicit) & 67.88  \\
       w/o pyramid & & \\
       \midrule
       Our VCR & 1 & 68.43\\
        & \textbf{2} & \textbf{68.83}\\
        & 3 & 68.56 \\
       \bottomrule
    \end{tabular}
    }
    \label{tab:exps:vcr-ablation-studes}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Ablation studies on ViTUp, all the results are obtained under the single-scale without flipping.
    All baseline models are trained using the same backbone and settings.
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c}
       \toprule
       Methods & Num\# HiCLR layers&  mIOU(\%) \\
       \midrule
       Bilinear  & 0 & 68.83 \\
       \midrule
       Mock pyramid & 2 & 69.01 \\
       \midrule
       Our real pyramid  & 1 & 69.50 \\
        & 2 & 69.87 \\
        & \textbf{3} & \textbf{70.00}\\
        & 4 & 69.81 \\
        & 5 & 69.43 \\
       \bottomrule
    \end{tabular}
    }
    \label{tab:exps:ViTUp-ablation-studes}
\end{table}

\begin{table}[ht]
    \centering
    \caption{
    Computational cost analysis for VPNeXt.
    All baseline models use the same backbone and settings.
    }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c}
       \toprule
       \quad Methods \quad \quad &  
       Pyramid Upsampler \quad &
       \quad GFlops \quad \quad  \\
       \midrule
       Deep supervision & - & 356.69\\
       \midrule
       Mask decoder & & 359.99  \\
       & \checkmark & > 2000 \\
       \midrule
       Our VPNeXt & & 356.69 \\
        & \checkmark & 1007.62 \\
       \bottomrule
    \end{tabular}
    }
    \label{tab:exps:cost-ablation-studes}
\end{table}



\input{tables/pascalcontext_sota_tab}

\subsection{Ablation studies on ViTUp}
We then assess the mIOU of our proposed ViTUp. 
%
As shown in Table~\ref{tab:exps:ViTUp-ablation-studes}, the real pyramid feature provided by our ViTUp, enhanced by HiCLR, reached 69.50\% mIOU, significantly outperforms both bilinear interpolation and mock pyramids (69.50\% vs 68.83\% vs 69.01\%). 
%
Furthermore, applying refinement three times yields 70.00\% mIoU, making it the best ViTUp configuration for VPNeXt.



\subsection{Computational cost analysis}
To demonstrate the high efficiency of VPNeXt, we conducted a computational analysis on two setups: VCR (VPNeXt w/o pyramid upsampler) and the complete VPNeXt with ViTUp.
%
For fair comparisons, we utilized Segmenter~\cite{cSegmenter} as the Mask decoder w/o a pyramid upsampler, and Mask2Former-based~\cite{cMask2Former} Vit-adapter~\cite{cViTAdapter} and PlainSeg~\cite{cPlainSeg} as Mask decoders with/a pyramid upsampler.

Table~\ref{tab:exps:cost-ablation-studes} shows that VCR and deep supervision have the same Flops, indicating that VCR provides high-quality representations without adding any computational overhead (see previous subsections for details).
%
Table~\ref{tab:exps:cost-ablation-studes} also shows that ViTUp delivers high-resolution pyramid features and strong mIoU while having significantly lower computational overhead compared to previous mask decoders that rely on mock pyramid features.




\subsection{Compare with state-of-the-arts}
To fully showcase the performance superiority of VPNeXt, we compared it with state-of-the-art methods on the Pascal Context dataset.
%
Note that, only methods published by the time this paper was completed can be compared.
%
As shown in Table~\ref{tab:SOTA-PascalContext}, our proposed VPNeXt significantly outperforms the compared methods, including the previous state-of-the-art techniques ViT-Adapter and InternImage. 
%
Moreover, even without using ViTUp (\ie with only VCR), VPNeXt still outperforms most methods.

