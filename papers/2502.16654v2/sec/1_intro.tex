\section{Introduction}
\label{sec:intro}


%
\IEEEPARstart{S}{emantic} 
segmentation is a fundamental computer vision task that classifies the image at the pixel level.
As the most direct way to produce dense representation, semantic segmentation has undergone rapid development over the past decade~\cite{cFCN,cUNet,cPSPNet,cDeepLab,cDeepLabV3Plus,cNonLocal,cDualAttention,cCCNet,cOCR,cCAA,cMaskFormer,cMask2Former,cCAR,cSAR,cCART,cHFGD,cSRRNet,cDeepLabM}.
%
A high-quality semantic segmentation model can not only benefit numerous application scenarios but also provide strong representations for various downstream computer vision tasks~\cite{cPanopticDeepLab,cDPT,cPanopticFPN,cReELFA}.

Since the introduction of Vision Transformer (ViT)~\cite{cViT} in 2020, numerous researchers have been exploring the use of ViT for visual tasks, including semantic segmentation~\cite{cSegmenter,cSegViT,cRSSeg-ViT}.
%
In this work, our main focus is the original ViT architecture, also known as Plain Vision Transformer, rather than its variants (e.g. Swin, MaxViT)~\cite{cSwin,cMaxViT,cCPvT}.
%
The Plain ViT has several advantages because it uses the same architecture as natural language processing (NLP)~tasks, allowing for a smooth transfer of NLP concepts and technologies to visual tasks, such as BEiT~\cite{cBEiT} and MAE~\cite{cMAE}. 
%
It also helps in creating multimodal models, like the recent Show-O~\cite{cShow-O} and Qwen-VL2~\cite{cQwen-VL,cQwen-VL2}, which combine tokenized images and other modalities into a single Transformer.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/abs-fig-1.pdf}
    \caption{Our main contributions include: 
    1) proposing VCR as an efficient alternative to the computationally intensive Mask Decoder, 
    2) digging hidden native pyramid feature of plain ViT to achieve better upsampling results,
    and 3) breaking the long-standing mIOU wall of the VOC2012~\cite{cPascalVOC} dataset by a large margin, setting a new state-of-the-art with the largest improvement since 2015.}
    \label{fig:abs-1}
\end{figure}

Every coin has two sides.

\textbf{First}, research on the semantic segmentation decoder of plain ViT is heavily influenced by the original architectures of Transformer~\cite{cAttentionIsAllYourNeed}, DETR~\cite{cDETR}, and ViT~\cite{cViT}, leading to high homogenization.
%
In DETR, object detection is obtained through token query (cross-attention). 
%
In ViT, unlike the classification method used in the CNN era~\cite{cResnet}, it uses a class token to perform self-attention with other spatial tokens to obtain the classification.
%
Many readers may have noticed that works on semantic segmentation decoders~\cite{cSegmenter,cMaXDeepLab,cKMaXDeepLab,cMaskFormer,cMask2Former,cSegViT} during the ViT era primarily focused on the DETR and ViT paradigms. 
%
Notable examples include Segmenter~\cite{cSegmenter}, MaX-DeepLab~\cite{cMaXDeepLab}, and MaskFormer~\cite{cMaskFormer}, whose decoders all share a similar name: Mask Decoder or Mask Transformer.
%

The strong effectiveness of the mask decoder does not need to be elaborated in this paper, as it has already demonstrated excellent feature regularization capabilities in these homogeneous works and has achieved very high mIOU on multiple semantic segmentation benchmark datasets~\cite{cPascalContext,cCocoStuff,cCityScapes}.

However, the low efficiency of the Mask decoder is a significant concern. 
%
It is well known that the non-sparse attention operation is global, making its computational efficiency much lower than that of the convolutional-based network. 
%
The ViT backbone network already contains a large number of attention operations, and the Mask decoder typically requires the addition of three~\cite{cSegmenter,cSegViT} or more attention operations. 
%
This further decreases overall computational efficiency.



\textbf{Besides}, the plain ViT has an obvious disadvantage in semantic segmentation.
Its tokenizer directly reduces the input image size by at least 16$\times$ times, which is not suitable for semantic segmentation tasks requiring the original size output.
%
Mature solutions like FPN~\cite{cFPN} and its variants~\cite{cFaPN,cFastFCN,cDeepLabV3Plus,cUper} cannot be implemented because they require multi-resolution pyramid features~\cite{cVGG,cResnet,cXception,cEfficientNet}, which plain ViT obviously cannot provide.
%
To tackle the ViT's resolution issue, a line of works~\cite{cSETR} opt for transposed convolution or similar methods, which directly upsample feature maps in a stage-wise manner without reference to pyramid features.
%
However, their impact is often limited as they still depend on the encoder to produce easily upsampling features, which is only slightly better than direct upsampling~\cite{cSegmenter}.
%
Another line of approaches involves creating a parallel pyramid network to produce high-resolution, low-level features while leveraging the pre-trained features of ViT. 
%
However, this results in a significant increase in inference overhead.

Therefore, we raise two questions:
\begin{itemize}
    \item Is it necessary to use a complex Transformer Mask Decoder architecture to obtain good representations?
    \item Does the Plain ViT really need to depend on the mock pyramid feature for upsampling?
\end{itemize}

To address these two questions, we propose VPNeXt (ViT context replay and upsample network; 'X' represents new technology.), a completely new segmentation decoder that diverges from the popular paradigm without compromising performance.

Specifically, for the first question, VPNeXt includes a novel technology called Visual Context Replay (VCR) to achieve similar effectiveness as Mask-Decoder but with much greater efficiency, as  VCR is only applied during training.
%
VCR enables the same visual priors to be replayed during the early encoding stages of ViT. 
This allows for the interaction between fine visual priors and early features without increasing computational overhead during the inference stage. 
As a result, this approach leads to improved visual representations.

In response to the second question, we claim that plain ViT can also effectively extract native pyramid features, similar to those obtained by CNN networks~\cite{cVGG,cResnet,cEfficientNet} or Pyramid ViT networks~\cite{cSwin,cSegFormer,cMaxViT}, as opposed to the mock pseudo-pyramid features derived from resizing high-level features like SETR~\cite{cSETR} and ViTDet~\cite{cViTDet}.
%
Therefore, we present ViTUp, an effective technique that uncovers high-resolution features previously hidden in ViT and uses them to aid in final upsampling.

By combining VCR and UpViT, we have successfully developed VPNeXt, a simple, effective, and efficient decoder for ViT.
%
This approach achieves outstanding performance across multiple benchmark datasets. 

In summary, the key contributions of this work are as follows:

\begin{itemize}
    \item We raised concerns about the inefficiency of the current heavy Mask-Decoder and the ineffectiveness of the upsampling paradigm for semantic segmentation in ViT.
    \item We proposed Visual Context Replay (VCR) to achieve similar effectiveness as Mask-Decoder but with much greater efficiency.
    \item We proposed ViTUp to extract hidden high-resolution features in plain ViT to perform the pyramid-based upsampling.
    \item The entire solution, VPNeXt, achieves state-of-the-art across multiple benchmark datasets with great computational efficiency.
    \item VPNeXt also broke the long-standing mIoU wall of the VOC2012 dataset by a large margin, which also stands as the largest improvement since 2015.
\end{itemize}

