\section{Related Works}
\label{sec:HFGD:related_work}
\label{sec:UpViT:related_work}

\subsection{Backbones for semantic segmentation}

Due to the extremely high costs of pixel-level labeling, Current pre-trained models for natural scene images depend only on image-level labeling data, also known as the image classification backbone.

\subsubsection{Pre-training}
Prior to 2020, most backbones~\cite{cAlexNet,cVGG,cResnet,cXception,cEfficientNet,cResnest} utilized for downstream tasks were primarily pre-trained on the ImageNet-1K~\cite{cImageNet-1K} dataset. 
Additionally, Google's proposed backbones~\cite{cEfficientNet} were frequently pre-trained on their proprietary JFT-300M~\cite{cJFT-300M} dataset.
%
After that, following the introduction of the Vision Transformer (ViT)~\cite{cViT} in late 2020, most modern backbone models~\cite{cViT,cSwin,cCPvT,cConvNeXT,cConvNeXtV2} are now pre-trained on the ImageNet-21K dataset at a minimum. 
Some state-of-the-art backbones~\cite{cEVA,cBEiT,cAugReg} have even been pre-trained on extensive datasets such as LAION-5B~\cite{cLAION-5B} or JFT-3B~\cite{cJFT-3B} (Google).
%
These backbones are pre-trained on large image-level datasets, increasing diversity and enhancing downstream task generalization.

\subsubsection{Architecture}
The primary purpose of backbones is to effectively fit large training sets while enhancing generalization and delivering robust feature representations for downstream tasks (e.g. segmentation, detection).
%
In general, the challenge of effectively fitting the training set to the deep neural network backbone~\cite{cResnet,cEfficientNet,cViT} has been well addressed through in-network normalization~\cite{cBatchNorm,cLayerNorm,cGroupNorm} and residual connections~\cite{cResnet}.
%
For generalization and enabling downstream tasks, research on backbones has also made significant progress in the past decade.
%
For example, the pyramid structure-based backbone network improves generalization by leveraging multi-scale priors. 
It also provides multi-resolution feature maps to downstream tasks, facilitating result upsampling with minimal computational overhead.

However, due to the popularity of natural language processing (NLP) and the research community's interest in unified architectures for multimodal learning, the Vision Transformer (ViT)~\cite{cViT}, which is inspired by the NLP Transformer architecture, has been proposed, with inherent architectural limitations. 
Specifically, it cannot produce pyramid information for downstream decoders to perform multi-scale feature extraction or multi-stage upsampling, which poses significant challenges for semantic segmentation tasks.

Note that, the ViT mentioned here refers specifically to the original plain Vision Transformer (ViT)~\cite{cViT}. 
The ViT pyramid variants~\cite{cSwin,cMaxViT} have fundamentally shifted closer to the CNN architecture, resulting in the loss of some characteristics inherent to the plain ViT, including its unified architectures with NLP.


\subsection{Decoder for semantic segmentation}

The decoder primarily serves the downstream task. 
In semantic segmentation, the decoder usually has two functions: 1) improve the robustness of the encoded features, 2) upsample the feature map back to the original input size.

For the former, i.e. enhancing the robustness of the encoding, common methods include multi-scale feature extraction~\cite{cPSPNet,cDeepLab,cDenseASPP,cFPN} and similarity-based feature extraction (e.g. pixel-wised~\cite{cNonLocal,cDualAttention,cOCNet,cCCNet,cCFNet,cANNN,cCAA} and class-center-wised~\cite{cOCR,cACFNet}). 
Alternatively, this goal can also be achieved by constraining the loss-based regularization~\cite{cCAR,cCPN}.

Recently, the Mask Decoder~\cite{cSegmenter,cMaskFormer,cSegViT,cMask2Former}, inspired by the Transformer decoder, has gained popularity as the leading option for decoders in semantic segmentation. 
It effectively merges the strengths of both similarity-based and class-center-based methods while also integrating the advantages of deep supervision techniques~\cite{cDeeplySupervisedNets,cDeepSupervisedCNN} indirectly.
In the following section, we will further discuss the relationship between deep supervision and the Mask Transformer.
As previously mentioned, while the Mask Transformer is effective, it also brings a significant computational burden, which is one of the issues our work seeks to address.
%


For upsampling the feature map back to the original input size, the most common approaches are direct upsampling and using pyramid features in a hierarchical manner.

The goal of direct up-sampling is to preserve as much detailed information (i.e. spatial to channel) as possible during the encoding stage while making necessary compromises in the interpolation during up-sampling.
%
In simple terms, it ensures that the upsampled image's results align with the requirements of the final loss function at full resolution.
%
One piece of evidence is that even the Segmenter~\cite{cSegmenter}, which upsampled directly at a 1/16 resolution, can produce good detailed results.
%
Nevertheless, as previously stated, encoders must make compromises in detail interpolation.

Using an upsampler can effectively reduce the load on the encoder. 
Although existing upsamplers still have many issues~\cite{cHFGD}, they have proven to be very effective in numerous studies~\cite{cUNet,cFPN,cFaPN,cCART,cFastFCN}.
%
Among the various upsamplers, the one based on pyramid information is the most typical and widely utilized.
%
Unfortunately, the current popular backbone in the research community, plain ViT~\cite{cViT}, is unable to provide multi-stage pyramid information.

Several downstream ViT-based research efforts, including those for semantic segmentation~\cite{cSETR}, attempt to forcefully apply pyramid upsamplers to ViT.
%
Their common method is to upsample intermediate features~\cite{cSETR} of ViT or directly upscale the final high-level features~\cite{cViTDet} to various scales, mocking pyramid features.
%
These methods offer only a slight advantage in extracting multi-scale features and are essentially no different from direct upsampling~\cite{cSegmenter} when it comes to providing high-resolution features.
%
This occurs because the intermediate or final features lack the native high-resolution details, relying instead on the hope that some lost spatial information remains preserved in the channels.\\

In this work, we revisit the architecture design of the ViT. 
The decoder we propose is efficient and lightweight while effectively mining native ViT high-resolution pyramid features, aiding in the efficient feature upsampling.








