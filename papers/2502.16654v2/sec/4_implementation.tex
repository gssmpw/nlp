\section{Training details}
\label{sec:HFGD:training_settings}

Unless specified otherwise, the training settings for our proposed VPNeXt are similar to existing works that use ViT mask decoders~\cite{cSETR,cSegViT,cMask2Former}.
This includes the AdamW optimizer, a batch size of 16, and the use of clipnorm along with a mask loss that combines focal and dice losses.

Given that this work focuses exclusively on the plain ViT backbone, all the experiments we conducted are based on the plain ViT without pyramid modifications. 
%
Following common practices, the weights of the ViT are initialized through modern pre-training~\cite{cAugReg,cEVA}.

To accommodate new readers in the field, we utilize the commonly used Mean Intersection over Union (mIOU) metric to evaluate the prediction accuracy of our model.