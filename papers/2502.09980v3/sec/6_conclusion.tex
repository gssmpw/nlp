\section{Conclusion}
In this work, we expand the research scope of cooperative autonomous driving by integrating the use of Multi-Modal LLM-based methods, aimed at improving the safety of future autonomous driving systems. We propose a new problem setting and create a novel \namedataset~dataset and benchmark that includes grounding, notable object identification, and planning question-answering tasks designed for varieties of cooperative driving scenarios. We propose a baseline model \namemethod~that fuses each CAV's individual perception information and performs visual and language understanding to answer driving-related questions from any CAV. 
Our proposed \namemethod~outperforms all other baselines adopted from state-of-the-art cooperative perception algorithms in the notable object identification and planning tasks, and achieves competitive performance in the grounding tasks. 
% In comparison to other baseline methods adopted from state-of-the-art cooperative perception algorithms, our proposed \namemethod~achieves competitive performance in the grounding tasks and outperforms all other baseline methods in the more important notable object identification and planning tasks. 
These experimental results indicate that \namemethod~is promising as a unified multi-modal foundation model that can effectively perform perception and planning tasks for cooperative autonomous driving.
We will publicly release our \namedataset~ dataset and code to facilitate the open-source research, and believe it will bring the cooperative driving research to the next stage.
\clearpage