\section{Related Work}

\subsection{Cooperative Perception in Autonomous Driving}
Cooperative perception~\cite{huang2024v2xcooperativeperceptionautonomous} algorithms were proposed to address the potential occlusion problem in individual autonomous vehicles. Pioneering work F-Cooper~\cite{chen2019fcooper} proposes the first intermediate fusion approach that merges feature maps to achieve good cooperative detection performance. V2VNet~\cite{wang2020v2vnet} builds graph neural networks for cooperative perception. DiscoNet~\cite{li2021disco} adopts a knowledge distillation approach. More recent work, AttFuse~\cite{xu2022opencood}, V2X-ViT~\cite{xu2022v2xvit}, and CoBEVT~\cite{xu2022cobevt} integrate attention-based models to aggregate features. Another group of works~\cite{liu2020when2com, hu2022where2comm, yang2023how2comm, chiu2023selective} focuses on developing efficient communication approaches. 

From a dataset perspective~\cite{liu2024asurveyon, yazgan2024collaborativeperceptiondatasetsautonomous}, simulation datasets, OPV2V~\cite{xu2022opencood}, V2X-Sim~\cite{li2022v2xsim}, and V2XSet~\cite{xu2022v2xvit} were first generated for cooperative perception research. More recently, real datasets have been collected. V2V4Real~\cite{xu2023v2v4real} is the first worldwide available real vehicle-to-vehicle cooperative perception dataset with perception benchmarks. V2X-Real~\cite{xiang2024v2xreal}, DAIR-V2X~\cite{yu2022dair-v2x}, and TUMTraf-V2X~\cite{zimmer2024tumtraf} further include sensor data from roadside infrastructures.
%DAIR-V2X~\cite{yu2022dair-v2x} and TUMTrafV2X~\cite{zimmer2024tumtraf} includes sensor data from a single vehicle and road-side infrastructures. V2X-Real~\cite{xiang2024v2xreal} is curated from multiple vehicles and infrastructures.

Different from this group of research, our problem setting and proposed \namedataset~dataset include both perception and planning question-answering tasks for multiple CAVs. Our proposed \namemethod~also adopts a novel LLM-based fusion approach.


\subsection{LLM-based Autonomous Driving}
%LLM has been used to build planning algorithms for individual autonomous vehicles in recent research~\cite{mao2023gpt, mao2023agentdriver, li2024drivingeverywhere}. %due to their common-sense reasoning and generalization ability from large-scale pre-trained data. 
Language-based planning models~\cite{mao2023gpt, mao2023agentdriver, li2024drivingeverywhere} first transforms the driving scene, object detection results, and ego-vehicle's state into text input to the LLM. Then the LLM generates text output including the suggested driving action or the planned future trajectory. 
%However, this approach may miss important detailed visual information from the raw input from LiDAR sensors or cameras. 
More recent approaches~\cite{tian2024token, wang2024omnidrive, sima2023drivelm, tian2024drivevlm, xu2023drivegpt4, wang2023driveanywhere, yang2023lidarllme} use Multi-Modal Large Language Models (MLLMs)~\cite{liu2023llava, liu2023improvedllava, liu2024llavanext, li2023blip2, jia2021scaling, radford2021clip, touvron2023llama, touvron2023llama2, alayrac2022flamingo, bai2023qwenvl} to encode point clouds or images into visual features. Then, the visual features are projected to the language embedding space for LLM to perform visual understanding and question-answering tasks.
%, such as captioning, grounding, and planning for autonomous vehicles. 

From a dataset perspective, several LLM-based autonomous driving datasets have been built on top of existing autonomous driving datasets. For example, Talk2Car~\cite{deruyttere2019talk2car}, NuPrompt~\cite{wu2023nuprompt}, NuScenes-QA~\cite{qian2023nuscenesqa}, NuInstruct~\cite{ding2024nuinstruct}, and Reason2Drive~\cite{nie2023reason2drive} create captioning, perception, prediction, and planning QA pairs based on the NuScenes~\cite{caesar2019nuscenes} dataset.
BDD-X~\cite{kim2018bddx} is extended from BDD100K~\cite{yu2020bdd100k}. 
DriveLM~\cite{sima2023drivelm} adopts real data from NuScenes~\cite{caesar2019nuscenes} and simulated data from CARLA~\cite{dosovitskiy2017carla} to have larger-scale and more diverse driving QAs. Other datasets curated independently focus on different types of QA tasks. 
HAD~\cite{kim2019had} contains human-to-vehicle advice data. 
DRAMA~\cite{malla2023drama} introduces joint risk localization and captioning. Lingo-QA~\cite{marcu2023lingoqa} proposed counterfactual question-answering tasks. MAPLM-QA~\cite{tencent2023maplm} emphasizes map and traffic scene understanding.

Different from those LLM-based driving research that only supports individual autonomous vehicles, our problem setting and proposed \namedataset~dataset are designed for cooperative driving scenarios with multiple CAVs. In our problem setting, the LLM can aggregate perception features from multiple CAVs and provide answers to questions from different CAVs. Our \namedataset~is also designed to focus on the potential occluded regions. In addition, our \namedataset~contains both highway and urban cooperative driving scenarios, making our planning task more challenging than prior works based on the NuScenes~\cite{caesar2019nuscenes} dataset.
