\begin{figure}[!t]
\centering
\includegraphics[width=0.47\textwidth]{figure/model_1114_2.pdf}
% \vspace{-20pt}
\caption[]
        {Model diagram of our proposed \namemethod~for cooperative autonomous driving.}         
        \label{fig:model}
        \vspace{-15pt}
\end{figure}


\section{\namemethod}

We also propose a competitive baseline model, \textbf{\namemethod}, for this LLM-based collaborative driving problem, as shown in \cref{fig:model}. Our model is a Multi-Modal LLM (MLLM) that takes the individual perception features of every CAV as the vision input, a question as the language input, and generates an answer as the language output. 

\subsection{LiDAR-based Input Features}
For extracting the perception input features, each CAV applies a 3D object detection model to its individual LiDAR point cloud: $P_{EGO}$ and $P_{1}$. We extract the scene-level feature map $S_{EGO}$ and $S_{1}$ from the 3D object detection model and transform the 3D object detection results as the object-level feature vectors $O_{EGO}$ and $O_{1}$. Following prior works V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal}, we use PointPillars~\cite{lang2019pointpillar} as the 3D object detector for fair comparisons.

\begin{figure*}[!t]
        \centering
        \begin{subfigure}[b]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/baseline_no_fusion_1112.pdf}
            \caption[]%
            {{No fusion}}    
            \label{fig:baseline_no_fusion}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.32\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{figure/baseline_early_fusion_1112.pdf}
            \caption[]%
            {{Early fusion}}    
            \label{fig:baseline_early_fusion}
        \end{subfigure}
        \hfill
        %\vskip\baselineskip
        \begin{subfigure}[b]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/baseline_intermediate_fusion_1112.pdf}
            \caption[]%
            {{Intermediate fusion~\cite{xu2022cobevt,xu2022v2xvit, xu2022opencood}}}    
            \label{fig:baseline_intermediate_fusion}
        \end{subfigure}
        \hfill
        \vspace{-5pt}
        \caption[]
        {
        Feature encoder diagrams of the baseline methods from different fusion approaches.} 
        \label{fig:baseline}
        \vspace{-10pt}
    \end{figure*}


% \subsection{Multi-modal LLM}
\subsection{LiDAR-based Multi-Modal LLM}
\noindent\textbf{Model architecture:} We utilize LLaVA~\cite{liu2023llava} to develop our MLLM, given its superior performance on visual question-answering tasks. However, since the perception features of our cooperative driving tasks are LiDAR-based instead of RGB images used by LLaVA~\cite{liu2023llava}, we use a LiDAR-based 3D object detector as the point cloud feature encoder, as described in the previous section, instead of LLaVA~\cite{liu2023llava}'s CLIP~\cite{radford2021clip} image feature encoders.
% in contrast to LLaVA~\cite{liu2023llava}, which focuses on RGB images as visual input, the perception features of our cooperative driving tasks are LiDAR-based. Therefore, our model does not rely on LLaVA~\cite{liu2023llava}'s CLIP~\cite{radford2021clip} image feature encoders. Instead, we use a LiDAR-based 3D object detector as the point cloud feature encoder, as described in the previous section. 
We then feed the resulting features to a multi-layer perceptron-based projector network for feature alignment from the point cloud embedding space to the language embedding space. The aligned perception features are the input perception tokens digested by the LLM together with the input language tokens from the question. Finally, the LLM aggregates the perception information from all CAVs and returns an answer based on the question. 

\noindent\textbf{Training:} We use 8 NVIDIA A100-80GB GPUs to train our model. Our \namemethod~uses LLaVA-v1.5-7b~\cite{liu2023llava}'s Vicuna~\cite{chiang2023vicuna} as the LLM backbone. To train our model, we initialize it by loading the pre-trained LLaVA-v1.5-7b~\cite{liu2023llava}'s checkpoint. We freeze the LLM and the point cloud feature encoder, and finetune the projector and the LoRA~\cite{hu2022lora} parts of the model. During training, we use batch size 32. Adam optimizer is adopted for training with a starting learning rate $2e-5$ and a cosine learning rate scheduler with a 3\% warm-up ratio. For all other training settings and hyperparameters, we use the same ones from LLaVA-v1.5-7b~\cite{liu2023llava}.





   