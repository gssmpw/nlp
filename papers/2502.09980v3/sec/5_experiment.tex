\section{Experiment}

\subsection{Baseline Methods}
We follow V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal} to establish a benchmark for our proposed \namedataset~dataset with experiments on baseline methods using different fusion approaches: \textbf{no fusion}, \textbf{early fusion}, \textbf{intermediate fusion}, and our proposed baseline, \textbf{LLM fusion} (\cref{fig:model}). The baseline methods also adopt the same projector and LLM architecture as in our \namemethod~but with different point cloud feature encoders, as shown in \cref{fig:baseline}. 
In some driving sequences of \namexsplit~that have point clouds from roadside infrastructures, we also include them as perception input in the same way as using CAVs' point clouds.

\noindent \textbf{No fusion}: Only a single CAV's LiDAR point cloud is fed to a single 3D object detector to extract the scene-level and object-level features as the LLM's visual input. The performance is expected to be worse than all other cooperative perception approaches.
%because the other CAV's sensor input is ignored.

\noindent \textbf{Early fusion}: The LiDAR point cloud from two CAVs is merged first. Then the merged point cloud is used as input to a 3D object detector to extract the visual features as the visual input to the LLM. 
%This approach may achieve good performance by using all the raw sensor input but 
This approach requires much higher communication cost and is less practical for deployment on real-world autonomous vehicles.

\noindent \textbf{Intermediate fusion}: Prior work CoBEVT~\cite{xu2022cobevt}, V2X-ViT~\cite{xu2022v2xvit}, and AttFuse~\cite{xu2022opencood} propose  cooperative detection models that merge feature maps from multiple CAVs via attention mechanisms. Such approaches require less communication cost and can still achieve good performance. In our benchmark, we extract the features from those cooperative detection models as the input tokens to the LLM.

\noindent \textbf{LLM fusion}: We categorize our proposed \namemethod~as a new type of fusion method, \textit{LLM fusion}, which lets each CAV perform its individual 3D object detection to extract the scene-level feature maps and object-level feature vectors, and uses the Multi-Modal LLM to fuse the features from multiple CAVs. This approach is related to the traditional \textit{late fusion} method that performs individual 3D object detection and aggregates the results by non-maximum suppression (NMS). Instead of applying NMS, our method adopts LLM to perform more tasks than just detection.

% Moved detailed result table to supp
% combine  v2v-split and v2x-split result table
\begin{table*}[t!]
\small
\setlength{\tabcolsep}{2pt}
%\renewcommand{\arraystretch}{0.7}
\begin{center}
\begin{tabular}{l ccccccc ccccccc c}
  \hline
  \hline
  \multirow{3}{*}{Method} &
  \multicolumn{7}{c}{\namevsplit} & \multicolumn{7}{c}{\namexsplit} & \multirow{3}{*}{Comm(MB) $\downarrow$} \\

  \cmidrule(lr){2-8} \cmidrule(lr){9-15}
  & \multicolumn{1}{c}{Q1} & \multicolumn{1}{c}{Q2} & \multicolumn{1}{c}{Q3} & \multicolumn{1}{c}{Q\textsubscript{Gr}} & \multicolumn{1}{c}{Q4} & \multicolumn{2}{c}{Q5} & \multicolumn{1}{c}{Q1} & \multicolumn{1}{c}{Q2} & \multicolumn{1}{c}{Q3} & \multicolumn{1}{c}{Q\textsubscript{Gr}} & \multicolumn{1}{c}{Q4} & \multicolumn{2}{c}{Q5} \\
  
  \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-15}
  &
  F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ &  L2 (m) $\downarrow$ & CR (\%) $\downarrow$ & F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ & F1 $\uparrow$ &  L2 (m) $\downarrow$ & CR (\%) $\downarrow$\\
  \hline
  \hline
  \textit{No Fusion}         & 66.6 & 22.6 & 17.2 & 35.5 & 47.3 & 6.55 & 4.57 & 55.7 & 21.4 & 25.2 & 34.1 & 64.4 & 2.31 & 9.21 & \textbf{0} \\
  \textit{Early Fusion}      & \textbf{73.5} & 23.3 & 20.8 & 39.2 & 53.9 & \underline{6.20} & \underline{3.55} & \underline{59.7} & 23.3 & 26.1 & 36.4 & \underline{67.6} & \underline{2.12} & 8.61 & 1.9208 \\
  \hline
  \scriptsize{\textit{Intermediate Fusion}} \\ 
  AttFuse~\cite{xu2022opencood}         & 70.7 & 26.4 & 18.4 & 38.5 & 56.9 & 6.83 & 4.12 & 58.9 & 23.9 & \underline{26.3} & 36.4 & 65.9 & 2.19 & \underline{8.39} & \underline{0.4008} \\
  V2X-ViT~\cite{xu2022v2xvit}           & 70.8 & 28.0 & \textbf{22.6} & 40.5 & \underline{57.6} & 7.08 & 4.33 & 59.6 & \underline{24.2} & 26.1 & \underline{36.6} & 65.0 & 2.29 & 8.86 & \underline{0.4008} \\
  CoBEVT~\cite{xu2022cobevt}            & \underline{72.2} & \underline{29.3} & \underline{21.3} & \textbf{40.9} & \underline{57.6} & 6.72 & 3.88 & - & - & -& - & - & - & - & \underline{0.4008} \\
  \hline
  \scriptsize{\textit{LLM Fusion}} \\
  \namemethod~(Ours)     & 70.0 & \textbf{30.8} & 21.2 & \underline{40.7} & \textbf{59.7} & \textbf{4.99} & \textbf{3.00} & \textbf{60.5} & \textbf{25.3} & \textbf{26.7} & \textbf{37.5} & \textbf{69.3} & \textbf{1.71} & \textbf{6.89} & 0.4068 \\
  \hline
\end{tabular}
\vspace{-5pt}
\caption{
\namemethod's testing performance in \namedataset's \namevsplit~and \namexsplit~in comparison with baseline methods. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. Q\textsubscript{Gr}: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. L2: L2 distance error. CR: Collision rate. Comm: Communication cost. In each column, the \textbf{best} results are in boldface, and the \underline{second-best} results are in underline. More detailed performance evaluation can be seen in the supplementary material.
\vspace{-5pt}
}
\label{tab:all_v2v_v2x_result}
\end{center}
\vspace{-20pt}
\end{table*}


% Moved detailed planning result tables to supp

   
\subsection{Experimental Results}

\subsubsection{Grounding}
Our \namemethod~and baseline methods' performance on \namedataset's $3$ types of grounding questions can be seen in \cref{tab:all_v2v_v2x_result} for \namevsplit~and \namexsplit, respectively. CoBEVT~\cite{xu2022cobevt} is not included in \namexsplit's result because V2X-Real~\cite{xiang2024v2xreal} does not release its CoBEVT~\cite{xu2022cobevt} baseline model. In average, \namemethod~ achieves similar performance in \namevsplit~and outperforms all other baseline methods in \namexsplit. Such results indicate that our \namemethod~has a promising capability of fusing perception features from multiple CAVs to answer grounding questions.

\subsubsection{Notable Object Identification}
\cref{tab:all_v2v_v2x_result} show the performance on the notable object identification task (Q4). Our proposed \namemethod~outperforms all other methods in both \namevsplit~and \namexsplit. Compared with the aforementioned grounding tasks, this notable object identification task requires more spatial understanding ability to identify the objects close to the planned future waypoints. For such a task, our \namemethod, which lets the Multi-Modal LLM perform both perception feature fusion and question answering, achieves the best results.

\subsubsection{Planning}
\cref{tab:all_v2v_v2x_result} show the performance of the planning task (Q5) for \namevsplit~and \namexsplit, respectively. Our proposed \namemethod~outperforms other methods in this safety-critical task to generate a future trajectory that aims to avoid potential collisions. More planning performance evaluation can be seen in the supplementary material.


\subsubsection{Communication Cost and Scaling Analysis}
In our centralized setting, each CAV sends one scene-level feature map ($\leq 0.2$MB), one set of individual object detection result parameters ($\leq 0.003$MB), one question ($\leq 0.0002$MB) to the LLM computing node and receives one answer ($\leq 0.0002$MB) at each timestep.  If there are $N_v$ CAVs and each asks $N_q$ questions, the communication cost of each CAV is $(0.2 + 0.003 + (0.0002 + 0.0002)N_q) = (0.203 + 0.0004N_q)$ MB, and the cost of the LLM is $(0.2 + 0.003 + (0.0002 + 0.0002)N_q) N_v = (0.203N_v + 0.0004N_qN_v)$ MB, as shown in \cref{tab:communication_cost}.
Note that each CAV only needs to send the same features to the LLM computing node once at each timestep because the LLM node can save and reuse them to answer multiple questions from the same or different CAVs at the same timestep.
Detailed scaling analysis can be seen in the supplementary material.




% Communication cost
\begin{table}[!t]
\small
\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{0.7}
\begin{center}
\begin{tabular}{l c c }
  \hline
  \hline
  Setting & Each CAV & Centralized LLM  \\
  \hline
  \hline
  Centralized  & $0.203 + 0.0004N_q$ & $0.203N_v + 0.0004N_qN_v$ \\
  %Decentralized      & $0.203(N_v - 1)$ & -  \\
  \hline
\end{tabular}
\vspace{-5pt}
\caption{
Communication cost (MB) and scaling analysis. $N_v$: number of CAVs. $N_q$: number of questions asked by each CAV at each timestep.
%\vspace{-10pt}
}
\label{tab:communication_cost}
\end{center}
\vspace{-10pt}
\end{table}

 

\subsubsection{Summary}
Overall, \namemethod~achieves the best results in the notable object identification and planning tasks, which are critical in autonomous driving applications. \namemethod~also achieves competitive results in the grounding tasks. In terms of communication costs, \namemethod~only increases communication costs by $1.5\%$ in comparison to other intermediate fusion baseline methods. More detailed evaluation and analysis can be seen in the supplementary material.

% Moved the full ablation table to supp and use a smaller one here
% Ablation on input features
\begin{table}[!t]
\small
\setlength{\tabcolsep}{2pt}
%\renewcommand{\arraystretch}{0.7}
\begin{center}
\begin{tabular}{l c c c c c cc }
  \hline
  \hline
  \multirow{2}{*}{Method} &
  \multicolumn{1}{c}{Q1} & \multicolumn{1}{c}{Q2} & \multicolumn{1}{c}{Q3} & \multicolumn{1}{c}{Q\textsubscript{Gr}} & \multicolumn{1}{c}{Q4} & \multicolumn{2}{c}{Q5} \\
  %\cline{2-10}
  \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-8}
  &
  F1 $\uparrow$ &
  F1 $\uparrow$ &
  F1 $\uparrow$ &
  F1 $\uparrow$ &
  F1 $\uparrow$ & 
  L2 (m) $\downarrow$ & CR (\%) $\downarrow$ \\
  \hline
  \hline
  Scene only         & 69.9 & 15.4 & 17.9 & 34.4 & 43.2 & 7.21 & 15.55 \\
  Object only      & 69.0 & 26.9 & 17.6 & 37.8 & 52.6 & 5.24 & 7.78 \\
  \hline
  Scratch         & 67.6 & 26.5 & 17.2 &  37.1 & 49.3 & 6.30 & 5.01 \\
  \hline
  \namemethod     & \textbf{70.0} & \textbf{30.8} & \textbf{21.2} & \textbf{40.7} & \textbf{59.7} & \textbf{4.99} & \textbf{3.00} \\
  \hline
\end{tabular}
\vspace{-5pt}
\caption{
Ablation study in \namedataset's \namevsplit~testing set.
% skip this duplicate description
%Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. Q\textsubscript{Gr}: Average of grounding (Q1, Q2, and Q3). Q4: Notable object identification. Q5: Planning. L2: L2 distance error. CR: Collision rate.
%In each column, the \textbf{best} results are in boldface.
\vspace{-10pt}
}
\label{tab:small_ablation_v2v}
\end{center}
\vspace{-15pt}
\end{table}

  
\subsection{Ablation Study}

\noindent \textbf{Input Features}: We experiment with variants of our \namemethod~model that use either only the scene-level feature maps or only the object-level feature vectors as the visual input. The ablation results can be seen in \cref{tab:small_ablation_v2v}. Both types of features contribute to final performance in all QA tasks. In general, the object-level-only model outperforms the scene-level-only model. This implies that the object-level features are easier for LLM to digest, which is consistent with the results observed in the previous work with the TOKEN model~\cite{tian2024token}.

\noindent \textbf{Training from Scratch}: \cref{tab:small_ablation_v2v} shows that training from scratch achieves worse performance, meaning that pre-training with LLaVA's VQA tasks improves our \namemethod's performance in \namedataset. More detailed ablation results can be seen in the supplementary material.
  
 
 
\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{figure/qualitative_result_grounding_1110_3.pdf}
\vspace{-20pt}
\caption[]
        {\namemethod's \textit{grounding} results on \namedataset's testing set.~\textcolor{magenta}{Magenta $\circ$}: reference locations in questions. \textcolor{olive}{Yellow $+$}: model output locations. \textcolor{Green}{Green $\circ$}: ground-truth answers.} 
        \label{fig:qualitative_result_grounding}
        \vspace{-5pt}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{figure/qualitative_result_notable_planning_1110_3.pdf}
\vspace{-20pt}
\caption[]
        {\namemethod's \textit{notable object identification} and \textit{planning} results on \namedataset's testing set. For notable object identification,~\textcolor{magenta}{Magenta curve}: planned future trajectories in questions. \textcolor{Green}{Green $\circ$}: ground-truth notable object locations. \textcolor{olive}{Yellow $+$} and \textcolor{cyan}{Cyan $\times$}: model identification outputs corresponding to \textcolor{olive}{CAV\_EGO} and \textcolor{cyan}{CAV\_1}, respectively.
        For planning, \textcolor{Green}{Green line}: future trajectories in ground-truth answers. \textcolor{olive}{Yellow curve} and \textcolor{cyan}{Cyan curve}: model planning outputs corresponding to \textcolor{olive}{CAV\_EGO} and \textcolor{cyan}{CAV\_1}, respectively.}         
        \label{fig:qualitative_result_notable_planning}
        \vspace{-10pt}
\end{figure*}

\subsection{Qualitative Results}
% new paragraph
\cref{fig:qualitative_result_grounding} shows our \namemethod's \textit{grounding} results and the ground truth with visualization on \namedataset's testing set. We can observe that our \namemethod~ is able to locate the objects given the provided reference information for each of the $3$ types of grounding questions.
\cref{fig:qualitative_result_notable_planning}'s left part shows our \namemethod's \textit{notable object identification} results. \namemethod~ demonstrate its capability of identifying multiple objects near the planned future trajectories specified in the questions for each CAV. 
\cref{fig:qualitative_result_notable_planning}'s right part shows \namemethod's \textit{planning} results. Our model is able to suggest future trajectories that avoid potential collisions with nearby objects. Overall, the outputs of our model closely align with the ground-truth answers across all question types, indicating its robustness in cooperative autonomous driving tasks.


