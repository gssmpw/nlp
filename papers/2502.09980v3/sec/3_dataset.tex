\section{\namedataset~Dataset}
To enable the research in our proposed novel problem setting: LLM-based cooperative autonomous driving, we create the \textbf{Vehicle-to-Vehicle Question-Answering (\namedataset)} dataset to benchmark different models' performance on fusing perception information and answering safety-critical driving-related questions.

% Two-column figure, one for all 3 types of grounding
\begin{figure*}[!t]
        \centering
        \begin{subfigure}[t]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/q1_illustration_1115_MHC.pdf}
            \caption[]%
            {{Q1: Grounding at a reference location.}}    
            \label{fig:q1_illustration}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.32\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{figure/q2_illustration_1115_MHC.pdf}
            \caption[]%
            {{Q2: Grounding behind a reference object at a location.}}    
            \label{fig:q2_illustration}
        \end{subfigure}
        \hfill
        %\vskip\baselineskip
        \begin{subfigure}[t]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/q3_illustration_1115_MHC.pdf}
            \caption[]%
            {{Q3: Grounding behind a reference object in a direction.}}    
            \label{fig:q3_illustration}
        \end{subfigure}
        %\hfill

        %\bigskip
        \begin{subfigure}[t]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/q4_illustration_1115_MHC.pdf}
            \caption[]%
            {{Q4: Notable object identification.}}    
            \label{fig:q4_illustration}
        \end{subfigure}
        %\hfill
        \hspace*{0.0175\textwidth}%
        \begin{subfigure}[t]{0.32\textwidth}
            \centering 
            \includegraphics[width=\textwidth]{figure/q5_illustration_1115_MHC.pdf}
            \caption[]%
            {{Q5: Planning.}}    
            \label{fig:q5_illustration}
        \end{subfigure}
        \hfill
        
        \vspace{-4pt}
        \caption[]
        {
        Illustration of \namedataset's $5$ types of QA pairs. The arrows pointing at LLM indicate the perception data from CAVs.
        % The \textcolor{olive}{yellow} and \textcolor{cyan}{cyan} arrows indicate the perception data from \textcolor{olive}{CAV$_{EGO}$} and \textcolor{cyan}{CAV$_1$}, respectively.
        } 
        \label{fig:all_q_illustration}
        \vspace{-10pt}
\end{figure*}


\subsection{Problem Setting}
Our proposed V2V cooperative autonomous driving with LLM problem is illustrated in \cref{fig:conceptual_only}. In this setting, we assume there are multiple Connected Autonomous Vehicles (CAVs) and a centralized LLM computing node. All CAVs share their individual perception information, such as scene-level or object-level features, with the centralized LLM. Any CAV can ask the LLM a question in natural language to obtain information for driving safety. The LLM aggregates the received perception information from multiple CAVs and provides a natural language answer to the CAV's question. In this research, the questions and answers include \textbf{grounding (Q1-3)}, \textbf{notable object identification (Q4)}, and \textbf{planning (Q5)}, as illustrated in \cref{fig:all_q_illustration}. 
% More details of the question and answer generation will be described in a later section.


% \subsection{Base Dataset}
\subsection{Dataset Details}
Our \namedataset~dataset contains two splits: \textbf{\namevsplit}~and \textbf{\namexsplit}, which are built on top of V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal} datasets, respectively. These base datasets are collected by driving two vehicles with LiDAR sensors simultaneously near to each other.
%In addition to the raw LiDAR point clouds, t
These datasets also includes 3D bounding box annotations for other objects in the driving scenes. In V2V4Real~\cite{xu2023v2v4real}, the training set has $32$ driving sequences and a total of $7105$ frames of data per CAV, and the testing set has $9$ driving sequences and a total of $1993$ frames of data per CAV. In V2X-Real~\cite{xiang2024v2xreal}, the training set has $43$ driving sequences and a total of $5772$ frames of data per CAV, and the testing set has $9$ driving sequences and a total of $1253$ frames of data per CAV. The frame rate is $10$Hz. In V2X-Real~\cite{xiang2024v2xreal}, some driving scenes also provide lidar point clouds from roadside infrastructures. In our \namexsplit, we also include them as perception inputs to the LLM with the same approach as using CAVs' lidar point clouds to answer CAVs' questions. We follow the same training and testing settings from V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal} when building our \namevsplit~and \namexsplit.

\cref{tab:dataset_stats} summarizes the numbers of QA pairs in our proposed \namedataset's \namevsplit~and \namexsplit. We have $1.45$M QA pairs in total and $30.2$ QA pairs per frame on average. More details can be found in the supplementary materials.
 
\begin{table}[!t]
\small
\setlength{\tabcolsep}{4pt}
%\renewcommand{\arraystretch}{0.7}
\begin{center}
\begin{tabular}{c | rr | rr | r}
  \hline
  \hline
  \multirow{2}{*}{QA type} & \multicolumn{2}{c|}{\namevsplit} & \multicolumn{2}{c|}{\namexsplit} & Total \\
  %\cmidrule(lr){2-3} \cmidrule(lr){4-5}
  & Training & Testing & Training & Testing \\
  \hline
  \hline
  % V2V-split stats (original V2V-QA stats)
  % Q1 & 354820 & 121383 & 476203 \\
  % Q2 &  35700 &  13882 &  49582 \\
  % Q3 &  14339 &   5097 &  19436 \\
  % Q4 &  12290 &   3446 &  15736 \\
  % Q5 &  12290 &   3446 &  15736 \\
  % \hline
  % Total & 429439 & 147254 & 576693

  % V2X-split stats
  % Q1 & 495290 & 128711 & 624001 \\
  % Q2 & 167694 &  35233 &  202927 \\
  % Q3 &  28740 &   6465 &  35205 \\
  % Q4 &   6274 &   1708 &  7982 \\
  % Q5 &   6274 &   1708 &  7982 \\
  % \hline
  % Total & 704272 & 173825 & 878097
  
  % Sum of V2V-split and V2X-split
  % Q1 & 850110 & 250094 & 1100204 \\
  % Q2 &  203394 &  49115 &  252509 \\
  % Q3 &  43079 &   11562 &  54641 \\
  % Q4 &  18564 &   5154 &  23718 \\
  % Q5 &  18564 &   5154 &  23718 \\
  % \hline
  % Total & 1133711 & 321079 & 1454790

  % Merged stats table
  Q1 & 354820 & 121383 & 495290 & 128711 & 1100204 \\
  Q2 &  35700 &  13882 & 167694 &  35233 &  252509 \\
  Q3 &  14339 &   5097 &  28740 &   6465 &   54641 \\
  Q4 &  12290 &   3446 &   6274 &   1708 &   23718 \\
  Q5 &  12290 &   3446 &   6274 &   1708 &   23718 \\
  \hline
  Total & 429439 & 147254 & 704272 & 173825 & 1454790

  
\end{tabular}
\caption{
Dataset statistics of our \namedataset's \namevsplit~and \namexsplit. Q1: Grounding at a reference location. Q2: Grounding behind a reference object at a location. Q3: Grounding behind a reference object in a direction. Q4: Notable object identification. Q5: Planning.
\vspace{-10pt}
}
\label{tab:dataset_stats}
\end{center}
\vspace{-40pt}
\end{table}
 


\subsection{Question and Answer Pairs Curation}
For each frame of V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal} datasets, we create $5$ different types of QA pairs, including $3$ types of grounding questions,  $1$ type of notable object identification question, and $1$ type of planning question. These QAs are designed for cooperative driving scenarios. To generate instances of these QA pairs, we use V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal}'s ground-truth bounding box annotations, each CAV's ground-truth trajectories, and individual detection results as the source information. Then we use different manually designed rules based on the geometric relationship among the aforementioned entities and text templates to generate our QA pairs. The text template can be seen in \cref{fig:qualitative_result_grounding,fig:qualitative_result_notable_planning}. The generation rule of each QA type is described as follows.

%Illustrations of these QAs can be seen in Figure~\ref{fig:overview_sample}.


% A two-column figure with 5 QA illustration
% \begin{figure*}[h!]
%         \centering
%         \begin{subfigure}[t]{0.195\textwidth}
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q1_illustration.pdf}
%             \caption[]%
%             {{Q1: Grounding at a reference location}}    
%             \label{fig:q1_illustration}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[t]{0.195\textwidth}  
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q2_illustration.pdf}
%             \caption[]%
%             {{Q2: Grounding behind a reference object at a location}}    
%             \label{fig:q2_illustration}
%         \end{subfigure}
%         \hfill
%         %\vskip\baselineskip
%         \begin{subfigure}[t]{0.195\textwidth}
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q3_illustration.pdf}
%             \caption[]%
%             {{Q3: Grounding behind a reference object in a direction}}    
%             \label{fig:q3_illustration}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[t]{0.195\textwidth}  
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q4_illustration.pdf}
%             \caption[]%
%             {{Q4: Notable object identification}}    
%             \label{fig:q4_illustration}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[t]{0.195\textwidth}  
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q5_illustration.pdf}
%             \caption[]%
%             {{Q5: Planning            }}    
%             \label{fig:q5_illustration}
%         \end{subfigure}
%         \hfill
%         \vspace{-5pt}
%         \caption[]
%         {
%         Illustration of \namedataset's 5 types of questions.} 
%         \label{fig:all_illustration}
% \end{figure*}


% Notable object identification and Planning
% \begin{figure*}[h!]
%         \centering        
%         \begin{subfigure}[t]{0.49\textwidth}  
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q4_illustration.pdf}
%             \caption[]%
%             {{Q4: Notable object identification}}    
%             \label{fig:q4_illustration}
%         \end{subfigure}
%         \hfill
%         \begin{subfigure}[t]{0.49\textwidth}  
%             \centering 
%             \includegraphics[width=\textwidth]{figure/q5_illustration.pdf}
%             \caption[]%
%             {{Q5: Planning            }}    
%             \label{fig:q5_illustration}
%         \end{subfigure}
%         \hfill
%         \vspace{-5pt}
%         \caption[]
%         {
%         Illustration of \namedataset's Notable object identification and Planning questions.} 
%         \label{fig:all_illustration}
% \end{figure*}
  
% \paragraph{Q1. Grounding at a reference location:}
\noindent\textbf{Q1. Grounding at a reference location (\cref{fig:q1_illustration}):}
In this type of question, we ask the LLM to identify whether an object that occupies a specific query 2D location exists. 
If so, the LLM is expected to provide the center location of the object. Otherwise, the LLM should indicate that there is nothing at the reference location. 
%To generate instances of this type of QA pair, w
We use the center locations of ground-truth boxes and every CAV's individual detection result boxes as the query locations in the questions. By doing so, we can focus more on evaluating each model's cooperative grounding ability on the potential false positive and false negative detection results.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.48\textwidth]{figure/q1_illustration.pdf}
% \vspace{-20pt}
% \caption[]
%         {Illustration of Q1: Grounding at a reference location.} 
%         %\vspace{-15pt}
%         \label{fig:q1_illustration}
% \vspace{-20pt}        
% \end{figure}

% \paragraph{Q2. Grounding behind a reference object at a location:}
\noindent\textbf{Q2. Grounding behind a reference object at a location (\cref{fig:q2_illustration}):}
When a CAV's field of view is occluded by a nearby large detected object, this CAV may want to ask the centralized LLM to determine whether there exists any object behind that occluding large object given the fused perception information from all CAVs. If so, the LLM is expected to return the object's location and the asking CAV may need to drive more defensively or adjust its planning. Otherwise, the LLM should indicate that there is nothing behind the reference object.
%To generate instances of this type of QA pair, w
We use the center location of each detection result box as the query locations in these questions. We draw a sector region based on the relative pose of the asking CAV and the reference object, and select the closest ground-truth object in the region as the answer.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.48\textwidth]{figure/q2_illustration.pdf}
% \vspace{-20pt}
% \caption[]
%         {Illustration of Q2: Grounding behind a reference object at a location.} 
%         %\vspace{-15pt}
%         \label{fig:q2_illustration}
% \vspace{-20pt}          
% \end{figure}

% \paragraph{Q3. Grounding behind a reference object in a direction:}
\noindent\textbf{Q3. Grounding behind a reference object in a direction (\cref{fig:q3_illustration}):}
We further challenge the LLM on language and spatial understanding ability by replacing Q2's reference 2D location with a reference directional keyword.
%To generate instances of this type of QA pair, w
We first get the closest detection result box in each of the $6$ directions of a CAV as the reference object. Then we follow the same approach in Q2 to get the closest ground-truth box in the corresponding sector region as the answer.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.48\textwidth]{figure/q3_illustration.pdf}
% \vspace{-20pt}
% \caption[]
%         {Illustration of Q3: Grounding behind a reference object in a direction.} 
%         %\vspace{-15pt}
%         \label{fig:q3_illustration}
% \vspace{-20pt}          
% \end{figure}

% \paragraph{Q4. Notable object identification:}
\noindent\textbf{Q4. Notable object identification (\cref{fig:q4_illustration}):}
The aforementioned grounding tasks are intermediate tasks in the autonomous driving pipeline. More critical abilities of autonomous vehicles involve both identifying notable objects near planned future trajectories and adjusting future planning to avoid potential collisions. 
%In the notable object identification questions, 
We extract $6$ waypoints from the ground-truth trajectory in the next $3$ seconds as the reference future waypoints in the questions. Then we get, at most, the $3$ closest ground-truth objects within $10$ meters of the reference future trajectory as the answer.

% \begin{figure}[!ht]
% \centering
% \includegraphics[width=0.45\textwidth]{figure/q4_illustration_1113_MHC.pdf}
% %\vspace{-20pt}
% \caption[]
%         {Illustration of Q4: Notable object identification.} 
%         %\vspace{-15pt}
%         \label{fig:q4_illustration}
% %\vspace{-10pt}          
% \end{figure}

% \paragraph{Q5. Planning:}
\noindent\textbf{Q5. Planning (\cref{fig:q5_illustration}):}
Planning is important because the ultimate goal of autonomous vehicles is to navigate through complex environments safely and avoid any potential collision in the future. 
To generate the planning QAs, we extract $6$ future waypoints, evenly distributed in the next $3$ seconds, from each CAV's ground-truth future trajectory as the answer. 
Our \namedataset's planning task is more challenging than other NuScenes~\cite{caesar2019nuscenes}-based LLM-driving related works for a couple reasons. First, we support multiple CAVs in cooperative driving scenarios. The LLM model needs to provide different answers depending on which CAV is asking, while prior works only need to generate planning results for a single autonomous vehicle. Second, our \namedataset's ground-truth planning trajectories are more diverse. \namedataset~contains both urban and highway driving scenarios, while NuScenes~\cite{caesar2019nuscenes} only includes urban driving scenarios. Detailed dataset statistics and comparison can be seen in the supplementary material.



  
% \begin{figure}[!t]
% \centering
% \includegraphics[width=0.45\textwidth]{figure/q5_illustration_1113_MHC.pdf}
% %\vspace{-20pt}
% \caption[]
%         {Illustration of Q5: Planning.} 
%         %\vspace{-15pt}
%         \label{fig:q5_illustration}
% \vspace{-10pt}          
% \end{figure}

\subsection{Evaluation Metrics}
We follow prior works~\cite{tian2024token, wang2024omnidrive}'s approach to evaluate model performance. For the grounding questions (Q1, Q2, Q3) and the notable object identification question (Q4), the evaluation metrics are F1 score, precision, and recall. The ground-truth answers and model outputs contain objects' center locations. If the center distance between the ground-truth answer and the model output is less than a threshold value, this output is considered as a true positive. We set the threshold value to be $4$ meters, a typical length of a vehicle.

For the planning question (Q5), the evaluation metrics are L2 errors and collision rates. The ground-truth answers and model outputs contain $6$ future waypoints, so we calculate the L2 errors on those waypoints. When calculating collision rates, we assume each of the CAVs' bounding box sizes is $4$ meters in length, $2$ meters in width, and $1.5$ meters in height. We place each CAV's bounding box at the model's output future waypoints and calculate the Intersection-over-Union~(IOU) between the CAV's bounding box and every ground-truth object's annotation bounding box in those future frames. If the IOU is larger than $0$, it is considered as a collision.



