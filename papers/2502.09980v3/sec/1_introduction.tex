\begin{figure}[!t]
\centering
\includegraphics[width=0.47\textwidth]{figure/conceptual_only_1113_MHC.pdf}
%\vspace{-15pt}
\caption[]
        {Overview of our problem setting of LLM-based cooperative autonomous driving. All CAVs share their perception information with the LLM. Any CAV can ask the LLM a question to obtain useful information for driving safety.} 
        \vspace{-15pt}
        \label{fig:conceptual_only}
\end{figure}

\section{Introduction}
% Motivation of v2v cooperative perception:
Autonomous driving technology has advanced significantly due to the evolution of deep learning algorithms, computing infrastructures, and the release of large-scale real-world driving datasets and benchmarks~\cite{geiger2012kitti, caesar2019nuscenes, sun2020waymo}. However, the perception and planning systems of autonomous vehicles in daily operation rely mainly on their local LiDAR sensors and cameras to detect notable nearby objects and plan for future trajectories. This approach may encounter safety-critical problems when the sensors are occluded by nearby large objects. In such situations, autonomous driving vehicles are unable to accurately detect all nearby notable objects, making subsequent planning results unreliable.

% Introduction of v2v cooperative perception, its limitation is only on perception tasks:
To address this safety-critical issue, recent research proposes cooperative perception algorithms~\cite{chen2019fcooper, wang2020v2vnet, xu2022opencood, xu2022v2xvit, xu2022cobevt, chiu2024probabilistic} via vehicle-to-vehicle (V2V) communication. In cooperative driving scenarios, multiple \textit{Connected Autonomous Vehicles (CAVs)} driving nearby to each other share their perception information via V2V communication. The received perception data from multiple CAVs is then fused to generate better overall detection or tracking results.
%To support and stimulate this research, a 
A number of cooperative autonomous driving datasets have been released to the public, including simulated ones~\cite{xu2022opencood, li2022v2xsim, xu2022v2xvit, cui2022coopernaut} and real ones~\cite{xu2023v2v4real, xiang2024v2xreal, yu2022dair-v2x, zimmer2024tumtraf}. These datasets also establish benchmarks to evaluate the performance of cooperative perception algorithms. However, to date, cooperative driving research and datasets have mostly focused on perception tasks.
%, such as detection and tracking. 
%Whether those state-of-the-art cooperative perception models' output can be used as input to planning models and generate good cooperative planning results has not been studied much.
How these state-of-the-art cooperative perception models can be connected with the downstream planning models to generate good cooperative planning results is still under-explored. %Such a modular pipeline autonomous driving stack requires larger efforts to optimize each component and integrate them together to build a fully functional autonomous driving system, in comparison to the alternative end-to-end autonomous driving framework~\cite{hu2023uniad, chitta2023transfuser, prakash2021multimodalfusion}.



% Our paper expand v2v research to planning, by incorporating LLM-based methods, due to promising recent reseach. Its limitation is only for single self-driving car:
%In this paper, 
% Inspired by recent promising research trend in LLM-based end-to-end autonomous driving, we propose to incorporate LLM into V2V cooperative autonomous driving research.
%from the current perception-only focus to also include more high-level tasks, such as notable object identification and planning tasks, which are more directly related to actual autonomous vehicles' final performance.
%Our approach is incorporating \textit{Large Language Models (LLMs)} into cooperative driving algorithms due to the promising research trend in recent years. 
Other recent research has attempted to use LLM-based methods to build end-to-end perception and planning algorithms for an individual autonomous vehicle~\cite{sima2023drivelm, tian2024drivevlm, wang2024omnidrive, tian2024token, chen2024drivingwithllms, nie2023reason2drive, wang2023drivemlm, xu2023drivegpt4} due to their common-sense reasoning and generalization ability from large-scale pre-trained data. These LLM-based models encode the raw sensor inputs
%, such as camera images or LiDAR point clouds, 
into visual features, and then perform visual understanding and answer varieties of driving-related perception and planning questions. These approaches have shown some promise but have not yet explored the benefits of cooperative perception and planning. LLM-based driving algorithms without cooperative perception could also face safety-critical issues when the individual vehicle's sensory capability is limited.
%prior research only investigates LLM-based methods' capability for individual autonomous vehicles. Therefore, we explore the research direction of incorporating LLM-based approaches into cooperative autonomous driving, aiming to build a unified model that can perform a variety of cooperative perception and planning tasks.


% what we contribute in this paper
In this paper, we propose and explore a novel problem setting wherein LLM-based methods are used to build end-to-end perception and planning algorithms for \emph{Cooperative Autonomous Driving},
%In this paper, we propose a novel problem setting in vehicle-to-vehicle cooperative autonomous driving with LLM, 
as illustrated in \cref{fig:conceptual_only}. In this problem setting, we assume that there are multiple CAVs and a centralized LLM computing node. All CAVs share their individual perception information with the LLM. Any CAV can ask the LLM a question in natural language to obtain useful information for driving safety. To enable the study of this problem setting, we first create the \textbf{Vehicle-to-Vehicle Question-Answering (\namedataset)} dataset, built upon the V2V4Real~\cite{xu2023v2v4real} and V2X-Real~\cite{xiang2024v2xreal} cooperative perception datasets for autonomous driving. 
Our \namedataset~
%follows other LLM-based driving research~\cite{tian2024token, wang2024omnidrive, sima2023drivelm, tian2024drivevlm, marcu2023lingoqa}, which 
includes \textbf{grounding} (\cref{fig:q1_illustration,fig:q2_illustration,fig:q3_illustration}), \textbf{notable object identification} (\cref{fig:q4_illustration}), and \textbf{planning} (\cref{fig:q5_illustration}) question-answer pairs. 
There are several differences between our novel problem setting and other existing LLM-based driving research~\cite{tian2024token, wang2024omnidrive, sima2023drivelm, marcu2023lingoqa, qian2023nuscenesqa, tencent2023maplm}. 
First, our LLM can fuse multiple perception information from different CAVs and provide answers to different questions from any CAV, rather than just serving a single self-driving car.
Second, our grounding questions are specially designed to focus on the potential occluded regions of each individual CAV. 
%Third, our planning task requires the LLM to generate the coordinates of future waypoints, rather than just a high-level driving action abstract in natural language as used in some prior work, such as `turn right" or ``slow down" that is difficult to generate unambiguous control signals.
More differences between our \namedataset~and other related datasets are summarized in \cref{tab:dataset_comparison_all}.


%Our specially designed grounding QAs focus on the potential occluded regions of each individual CAV. Our notable object identification and planning QAs evaluate the performance of different models in the metrics most relevant to overall cooperative driving safety.
%Different from other existing LLM-based QA datasets~\cite{tian2024token, wang2024omnidrive, sima2023drivelm, marcu2023lingoqa, qian2023nuscenesqa, tencent2023maplm}, our QA supports multiple CAVs in cooperative driving scenarios, as described in our aforementioned problem setting. 
% Our \namedataset's three QA groups - \textbf{grounding}, \textbf{notable object identification}, and \textbf{planning} - can be illustrated in Figures~\ref{fig:all_grounding}, ~\ref{fig:q4_illustration}, and ~\ref{fig:q5_illustration}. 
%The main differences between our \namedataset~and other related datasets are summarized in \cref{tab:dataset_comparison_all}.
%More details about \namedataset~can be seen in the Dataset section of this paper.



% \begin{figure}[]
% \centering
% \includegraphics[width=0.48\textwidth]{figure/framework.pdf}
% \caption[]
%         {Illustration of our proposed V2V cooperative autonomous driving with LLM problem setting.} 
%         %\vspace{-15pt}
%         \label{fig:framework}
% \end{figure}


% All dataset comparison table
\begin{table*}[t!]
% \small
\setlength{\tabcolsep}{4pt}
%\renewcommand{\arraystretch}{0.7}
\begin{center}
\begin{tabular}{l ccc rrr cc}
  \hline
  \hline
  Dataset & Publication & \# CAVs & Sim/Real & \# Frames & \# QA  & \# QA/frame & Point Cloud & Planning \\
  \hline
  \hline
  \scriptsize{\textit{AD}} \\
  % KITTI~\cite{geiger2012kitti} & CVPR 2012 & - & Real & 15K & - & - & \checkmark \\
  NuScenes~\cite{caesar2019nuscenes} & CVPR 2020 & - & Real & 400K & - & - & \checkmark \\
  Waymo~\cite{sun2020waymo} & CVPR 2020 & - & Real & 200K & - & - & \checkmark \\
  \hline
  \scriptsize{\textit{Cooperative perception in AD}} \\
  OPV2V~\cite{xu2022opencood} & ICRA 2022 & 2-7 & Sim &11K & - & - & \checkmark  \\
  % V2X-Sim~\cite{li2022v2xsim} & RA-L 2022 & 2-5 & Sim & 10K & - & - & \checkmark \\
  V2XSet~\cite{xu2022v2xvit} & ECCV 2022 & 2-5 & Sim & 11K & - & - & \checkmark \\
  % remove the following two: only has 1 vehicle
  %DAIR-V2X~\cite{yu2022dair-v2x} &  \\
  %TUMTrafV2X~\cite{zimmer2024tumtraf} &  \\
  V2V4Real~\cite{xu2023v2v4real} & CVPR 2023 & 2 & Real & 20K & - & - & \checkmark \\
  V2X-Real~\cite{xiang2024v2xreal} & ECCV 2024 & 2 & Real & 33K & - & - & \checkmark \\
  \hline
  \scriptsize{\textit{LLM-based AD}} \\
  % BDD-X~\cite{kim2018bddx} & ECCV 2018 & - & Real & 26K & 26K & 1.0 & &  \\
  % HAD~\cite{kim2019had} & CVPR 2019 & - & Real & 26K & 45K & 1.8 & & \checkmark \\
  % % remove Talk2Car,  # QA pairs per frame = 0.35 < 1 ???
  % %Talk2Car~\cite{deruyttere2019talk2car} &  & 12K & 34K & 0.35 \\
  % DRAMA~\cite{malla2023drama} & WACV 2023 & - & Real & 18K & 102K & 5.8 & & \checkmark \\
  
  % NuPrompt~\cite{wu2023nuprompt} & arXiv 2023 & - & Real & 34K & 35K & 1.0 & \checkmark & \\  
  NuScenes-QA~\cite{qian2023nuscenesqa} & AAAI 2024 & - & Real & 34K & 460K & 13.5 & \checkmark & \\
  Lingo-QA~\cite{marcu2023lingoqa} & ECCV 2024 & - & Real & 28K & 420K & 15.3 & & \checkmark \\
  MAPLM-QA~\cite{tencent2023maplm} & CVPR 2024 & - & Real & 14K & 61K & 4.4 & \checkmark &  \\
  DriveLM~\cite{sima2023drivelm} & ECCV 2024 & - & Sim+Real & 69K & 2M & 29.1 & & \checkmark \\
  TOKEN~\cite{tian2024token} & CoRL 2024 & - & Real & 28K & 434K & 15.5 & & \checkmark \\
  OmniDrive-nuScenes~\cite{wang2024omnidrive} & arXiv 2024 & - & Real & 34K & 450K & 13.2 & & \checkmark \\
  \hline
  \textbf{\namedataset~(Ours)}     & -  & 2 & Real & 48K & \textbf{1.45M} & \textbf{30.2} & \checkmark & \checkmark \\
  \hline
\end{tabular}
\caption{
Comparison between our \namedataset~and recent related Autonomous Driving (AD) datasets. 
%\textsuperscript{\textdagger} This number of frames includes the validation set of V2V4Real~\cite{xu2023v2v4real}, which is not released to the public. We build our \namedataset~upon the released training and testing set of V2V4Real~\cite{xu2023v2v4real}. \textsuperscript{\textdaggerdbl} V2X-Real~\cite{xiang2024v2xreal} only releases a subset of data to the public. 
% a week before the CVPR 2025 submission deadline.
%\TODO{can expand to include data size, number of QA pairs, probably do not need to include every related dataset.}
\vspace{-10pt}
}
\label{tab:dataset_comparison_all}
\end{center}
\vspace{-10pt}
\end{table*}

% Decompose the above big table into two
% First: all datasets, only two key attribute columns:  Multi-CAVs and QAs
% Second: only datasets with QA, show all columns

% All dataset comparison table with two key attribute columns:  Multi-CAVs and QAs
% \begin{table}[t!]
% \small
% %\setlength{\tabcolsep}{6pt}
% %\renewcommand{\arraystretch}{0.7}
% \begin{center}
% \begin{tabular}{l c rrr cc}
%   \hline
%   \hline
%   Dataset & Multi-CAVs & QAs \\
%   \hline
%   \hline
%   \scriptsize{\textit{AD}} \\
%   KITTI~\cite{geiger2012kitti} & \\
%   NuScenes~\cite{caesar2019nuscenes} & \\
%   Waymo~\cite{sun2020waymo} & \\
%   \hline
%   \scriptsize{\textit{Cooperative perception in AD}} \\
%   OPV2V~\cite{xu2022opencood} & \checkmark \\
%   V2X-Sim~\cite{li2022v2xsim} & \checkmark \\
%   V2XSet~\cite{xu2022v2xvit} & \checkmark \\
%   % remove the following two: only has 1 vehicle
%   %DAIR-V2X~\cite{yu2022dair-v2x} &  \\
%   %TUMTrafV2X~\cite{zimmer2024tumtraf} &  \\
%   V2V4Real~\cite{xu2023v2v4real} & \checkmark \\
%   V2X-Real~\cite{xiang2024v2xreal} & \checkmark \\
%   \hline
%   \scriptsize{\textit{LLM-based AD}} \\
%   BDD-X~\cite{kim2018bddx} &  & \checkmark  \\
%   HAD~\cite{kim2019had} &  & \checkmark \\
%   % remove Talk2Car,  # QA pairs per frame = 0.35 < 1 ???
%   %Talk2Car~\cite{deruyttere2019talk2car} &  & 12K & 34K & 0.35 \\
%   DRAMA~\cite{malla2023drama} &  &  \checkmark \\
  
%   NuPrompt~\cite{wu2023nuprompt} &  &  \checkmark \\  
%   NuScenes-QA~\cite{qian2023nuscenesqa} &  & \checkmark \\
%   Lingo-QA~\cite{marcu2023lingoqa} &  & \checkmark \\
%   MAPLM-QA~\cite{tencent2023maplm} &  & \checkmark  \\
%   DriveLM~\cite{sima2023drivelm} &  & \checkmark \\
%   \hline
%   \namedataset~(ours)     & \checkmark & \checkmark \\
%   \hline
% \end{tabular}
% \caption{
% Comparison between our \namedataset~and other related Autonomous Driving (AD) datasets.
% \vspace{-10pt}
% }
% \label{tab:dataset_comparison_multi_cavs_qa}
% \end{center}
% \vspace{-10pt}
% \end{table}


% LLM-based dataset comparison table
% \begin{table*}[t!]
% \small
% %\setlength{\tabcolsep}{6pt}
% %\renewcommand{\arraystretch}{0.7}
% \begin{center}
% \begin{tabular}{l cc rrr cc}
%   \hline
%   \hline
%   Dataset & Multi-CAVs & Source Dataset & \# QA & \# Frames & \# QA per Frame & Point Cloud & Planning \\
%   \hline
%   \hline
%   BDD-X~\cite{kim2018bddx} &  & BDD100K~\cite{yu2020bdd100k} & 26K & 26K & 1.0 & &  \\
%   HAD~\cite{kim2019had} & & HDD~\cite{ramanishka2018hdd} & 45K & 26K & 1.8 & & \checkmark \\
%   % remove Talk2Car,  # QA pairs per frame = 0.35 < 1 ???
%   %Talk2Car~\cite{deruyttere2019talk2car} &  & 12K & 34K & 0.35 \\
%   DRAMA~\cite{malla2023drama} &  & DRAMA~\cite{malla2023drama} & 102K & 18K & 5.8 & & \checkmark \\
  
%   NuPrompt~\cite{wu2023nuprompt} &  & NuScenes~\cite{caesar2019nuscenes} & 35K & 34K & 1.0 & \checkmark & \\  
%   NuScenes-QA~\cite{qian2023nuscenesqa} &  & NuScenes~\cite{caesar2019nuscenes} & 460K & 34K & 13.5 & \checkmark & \\
%   Lingo-QA~\cite{marcu2023lingoqa} &  & Lingo-QA~\cite{marcu2023lingoqa} & 420K & 28K & 15.3 & & \checkmark \\
%   MAPLM-QA~\cite{tencent2023maplm} &  & MAPLM~\cite{tencent2023maplm} & 61K & 14K & 4.4 & \checkmark &  \\
%   DriveLM~\cite{sima2023drivelm} & & NuScenes~\cite{caesar2019nuscenes} CARLA~\cite{dosovitskiy2017carla} & 4.2M & 69K & 29.1 & & \checkmark \\
%   \hline
%   \namedataset~(ours)     & \checkmark & V2V4Real~\cite{xu2023v2v4real} & 577K & 9K & 63.4 & \checkmark & \checkmark \\
%   \hline
% \end{tabular}
% \caption{
% Comparison between our \namedataset~and other related LLM-based Autonomous Driving (AD) datasets.
% \vspace{-10pt}
% }
% \label{tab:dataset_comparison_llm_based}
% \end{center}
% \vspace{-10pt}
% \end{table*}


To establish a benchmark for the \namedataset~dataset, we first propose a strong baseline method: \textbf{Vehicle-to-Vehicle Multi-Modal Large Language Model (\namemethod)} for cooperative autonomous driving, as illustrated in \cref{fig:model}. Each CAV extracts its own perception features and shares them with \namemethod. The \namemethod~fuses the scene-level feature maps and object-level feature vectors, and then performs vision and language understanding to provide the answer to the input driving-related questions in \namedataset. 
%More details about our \namemethod~model can be seen in Figure~\ref{fig:model} and the Method section. 
% We train our \namemethod~on the \namedataset's training set and evaluate the model's performance on the testing set. 
We also compare \namemethod~with other baseline methods corresponding to different feature fusion methods: \textit{no fusion}, \textit{early fusion}, and \textit{intermediate fusion}~\cite{xu2023v2v4real, xiang2024v2xreal, xu2022opencood, xu2022v2xvit, xu2022cobevt}. The results show that \namemethod~achieves the best performance in the more important notable object identification and planning tasks and competitive performance in the grounding tasks, achieving strong performance for the overall autonomous driving system.
% adopted from V2V4Real~\cite{xu2023v2v4real}'s benchmark: \textit{no fusion}, where only single CAV's sensor input is used; \textit{early fusion}, where each CAV shares raw sensor input, such as LiDAR point cloud, with others; and intermediate fusion~\cite{xu2022opencood, xu2022v2xvit, xu2022cobevt}, where scene-level feature maps are shared. We observe that our \namemethod~can achieve the second-best performance in the grounding tasks. More importantly, \namemethod~achieves the best performance in the high-level notable object identification and planning tasks, which are more directly related to the autonomous driving system's overall performance.

Our contribution can be summarized as follows:
\begin{itemize}
\item  We create and introduce the \namedataset~dataset to support the development and evaluation of LLM-based approaches to end-to-end cooperative autonomous driving.
\namedataset~includes grounding, notable object identification, and planning question-answering tasks.
%\item  We create and introduce a new autonomous driving data set, the \namedataset~dataset, to support the development and evaluation of LLM-based approaches to cooperative end-to-end perception and planning for multiple CAVs.
%\namedataset~includes grounding, notable object identification, and planning question/answering tasks.
%  \item We propose a new problem setting: Vehicle-to-Vehicle Cooperative Autonomous Driving with LLM. And we create the \namedataset~dataset to evaluate different models' performance on cooperative autonomous driving QA tasks. \namedataset~includes grounding, notable object identification, and planning tasks.


\item We propose a baseline method \namemethod~for cooperative autonomous driving to provide an initial benchmark for \namedataset. This method fuses scene-level feature maps and object-level feature vectors provided by multiple CAVs, and answers different CAV's driving-related questions. 
%\item We propose a baseline method \namemethod~for cooperative autonomous driving to provide an initial benchmark for \namedataset. This method fuses perception information provided by multiple CAVs, such as scene-level feature maps and object-level feature vectors, and answers different CAV's driving-related questions. 

\item We create a benchmark for \namedataset~and show that \namemethod~outperforms other baseline fusion methods on the notable object identification and planning tasks and achieves competitive results on the grounding tasks, indicating the potential of \namemethod~to be a foundation model for cooperative autonomous driving.
  
  % \item We present experiments that show that \namemethod~achieves good performance on the grounding tasks and outperforms other no fusion, early fusion, and intermediate fusion baseline methods on the more important notable object identification and planning tasks. These results give evidence of \namemethod~'s potential to be a foundation model for cooperative autonomous driving.
\end{itemize}

 
   