\section{Motivation}
\label{sec:motivation}



% In LLM inference, not only does weight matter, but the memory requirements of the KV Cache are also considerable.
In this section, we first demonstrate that the emerging paradigm of group quantization demands a high level of adaptivity, which current adaptive methods lack.
We then discuss how adapting these methods to group quantization could compromise their efficiency.
Given that LLMs generate KV caches during runtime, real-time quantization capability is crucial.
These challenges lead to our proposal of a mathematical adaptive numerical type (\texttt{MANT}), which we will detail later.



\input{figure_text/fig_moti_group_ppl.tex}




\subsection{Group Quantization Accuracy Analysis}
\label{sec:acc_analysis}

In this subsection, we begin by comparing the accuracy of traditional channel-wise quantization with group-wise quantization~\cite{shao2024omniquant,zhao2023atom,liu2024kivi,sheng2023flexgen,lin2023awq,zhao2023atom}, establishing the baseline for group-wise quantization in this study.
We then delve into the use of various adaptive data types in group quantization, emphasizing the necessity for full adaptivity.



\Fig{fig:moti_group_ppl} illustrates the perplexity when quantizing the LLaMA-7B model~\cite{touvron2023llama} with various granularities using the \texttt{INT4}-based symmetric quantization.
Channel-wise quantization significantly worsens the perplexity of the examined LLM, increasing it from 5.68 to 6.85.
Conversely, group-wise quantization mitigates this loss in perplexity with a group size of 128, corresponding to an average of 4.125 bits per element (16-bit scaling factor).
Additionally, we observe that a smaller group size of 32 offers only a slight improvement in perplexity, but the scaling factor overhead increases by $4\times$.



Given this analysis, we adopt a group size of 128 as our standard configuration for the remainder of this section.
Previous research indicates that the \texttt{INT} data type is not optimal for accuracy since tensors or channels exhibit varied distributions, leading to the proposal of various adaptive data types~\cite{guo2022ant, guo2023olive, zadeh2020gobo, zadeh2022mokey}.
We evaluate their efficacy in the context of group quantization, which falls into two main categories: data-type-based and clustering-based.



\textbf{Data-type-based adaptive methods} select data types from discrete sets based on tensor data distribution.
ANT~\cite{guo2022ant} is a representative example of the data-type-based method.
ANT packages several different data types for selection, including \texttt{INT} for the uniform distribution, \texttt{PoT} (Power of Two) for the Laplace distribution, and \texttt{flint} for the Gaussian distribution.
%ANT designed \texttt{flint} for Gaussian distributions.

\textbf{Clustering-based adaptive methods} utilize clustering algorithms to generate centroids that align with the data distribution and provide considerable adaptivity. 
Mokey~\cite{zadeh2022mokey} and GOBO~\cite{zadeh2020gobo} exemplify this approach, though they focus on tensor- or channel-wise quantization. In our study, we adapt them to group quantization through per-group clustering.

%Clustering-based methods employ clustering algorithms to generate centroids that fit the data distribution, demonstrating sufficient adaptivity.
%Mokey~\cite{zadeh2022mokey} and GOBO~\cite{zadeh2020gobo} are such presentative works, but only target tensor- or channel-wise quantization.
%In our work, we modify those works to support group quantization by performing per-group clustering.
\Fig{fig:moti_ppl} compares the accuracy of the methods described above for the LLaMA-7B model under 4-bit group-wise quantization. 
The group-wise \texttt{ANT} method outperforms the \texttt{INT} type by dynamically selecting from three data types to better match the value distribution, resulting in reduced perplexity (PPL) loss. 
Moreover, per-group clustering adjusts more effectively to the value distribution of each group, establishing itself as the accuracy-optimal and ideal adaptive method. 
This approach achieves nearly lossless 4-bit quantization, equivalent to 16 centroids per group. 
However, this ideal scenario is impractical due to the significant overhead associated with storing per-group centroids, effectively rendering it a 6-bit quantization.

\input{figure_text/fig_moti_cdf.tex}

To illustrate the group-wise diversity in data distribution, we sampled the weights of the Q and V tensors in LLaMA-7B model. 
We normalized all sampled data to their absolute maximum values, which ranged from -1 to 1. \Fig{fig:moti_dist} displays the cumulative distribution function (CDF) for the tensor, channel, and group levels, respectively. 
We observed that the diversity at the group level is significantly higher than at the tensor level. 
In simpler terms, while different tensors exhibit similar distributions, groups can have markedly different distributions. This finding underscores the necessity for full adaptivity in group quantization to fully realize its potential.
\paragraph{Takeaway 1.} The group quantization is an emerging paradigm to accelerate LLMs, and the significant group-level diversity requires a high level of adaptivity to fully unleash its potential.

\subsection{Group Quantization Efficiency Analysis}
\label{subsec:efficiency}


In this subsection, we provide a detailed efficiency analysis for the above adaptive quantization methods.
In \Tbl{intro:dtype}, we compare OliVe~\cite{guo2023olive}, ANT~\cite{guo2022ant}, GOBO~\cite{zadeh2020gobo}, and Mokey~\cite{zadeh2022mokey} with \texttt{INT} regarding the efficiency of computation, encoding, and decoding. 
In this paper, we use the term encoding (decoding) interchangeably with quantization (dequantization).
 

Data-type-based adaptive methods such as ANT~\cite{guo2022ant} and Olive~\cite{guo2023olive} achieve computational efficiency comparable to \texttt{INT}. 
Both utilize specialized decoders that decode these data types prior to computation, resulting in high decoding efficiency. 
However, as previously demonstrated, these methods suffer from limited adaptivity in the group quantization paradigm. 
A straightforward approach to enhance adaptivity is to expand their set of data types. 
However, incorporating new data types necessitates additional decoders, escalating hardware design costs. 
Additionally, compatibility issues between new and existing data types may reduce computational efficiency. 
For instance, the \texttt{NF4} data type~\cite{dettmers2023qlora} requires an FP16 MAC unit, which is incompatible with existing \texttt{ANT} data types.


\paragraph{Takeaway 2.} Enhancing the data-type-based adaptive method for group quantization is challenging and requires a careful balance for the computation and decoding efficiency.

Clustering-based adaptive methods like GOBO~\cite{zadeh2020gobo} and Mokey~\cite{zadeh2022mokey} can sufficiently adapt to various distributions at the group level. 
However, they require codebooks for quantization and dequantization, leading to high adaptivity at the expense of encoding and computational efficiency. 
For instance, a 16-entry codebook with 8 bits per entry requires 128 bits per group, creating an inevitable trade-off between adaptivity and memory overhead. GOBO~\cite{zadeh2020gobo} employs the K-means algorithm to quantize weights and requires dequantization to \texttt{FP16} using a codebook lookup table before computation, resulting in high adaptivity but low computational efficiency. 
Conversely, Mokey~\cite{zadeh2022mokey} enhances the computation of clustering-based methods by using indices for centroid values via approximate calculations, though matrix multiplication still relies on floating-point units, increasing overhead compared to integer units. 
Furthermore, Mokey creates one \texttt{golden dictionary} for all activations and weights, akin to using a single data type in quantization, thus reducing adaptivity.


\paragraph{Takeaway 3.} Deploying the clustering-based adaptive methods under group quantization is challenging owing to the low encoding and computation efficiency. 


\input{figure_text/tbl_moti_accelerator_feature.tex}

\subsection{Support for Real-time Quantization}
\label{sec:moti_kvcache}

The above group-wise diversity presents a challenge for both weights and KV cache.
In addition, KV cache faces challenges in real-time group-wise quantization because the KV cache is generated dynamically during LLM inference.


To facilitate low-precision computation in group-wise quantization, it is necessary to quantize K and V along the inner dimension. 
This requirement stems from the support for matrix inner product operations in most GPUs and TPUs. 
During these operations, the group-wise scaling factor can be extracted from the multiply-accumulate process. 
\Fig{fig:kv_process} depicts the computation process of K and V during the decode stage. We define the dimension used for matrix inner product operations as the inner dimension. 
The inner dimensions of the K and V caches differ; the K cache requires a transpose operation, whereas the V cache does not, complicating the situation.


In the prefill stage, K and V can easily compute the scaling factor for each group. 
During the decode stage, the newly generated K vector is concatenated along the inner dimension of the K cache, enabling immediate quantization. 
However, the newly generated V vector is associated with different groups, with only one element per group produced per iteration. This process prevents the scaling factor for the entire group from being obtained in a single iteration, posing a significant challenge for the real-time quantization of the V cache.


\input{figure_text/fig_moti_kv_dimension.tex}


Given those challenges, we propose \proj with a mathematical encoding format that can fuse with integer computation and enhance the decoding efficiency.
In addition, this encoding format provides sufficient adaptivity for group-wise quantization.
Regarding the challenge in KV cache, \proj employs a real-time quantization engine that ensures efficient encoding and decoding for KV cache.
By addressing these challenges, \proj enables efficient low-bit group-wise quantization.

