\section{Introduction}\label{sec:introduction}
\blankfootnote{* Jingwen Leng is the corresponding author of this paper.}
% Transformer-based large language models (LLMs)...
In recent years, Large Language Models (LLMs) have demonstrated significant improvements in quality and accuracy across various natural language processing (NLP) tasks~\cite{chowdhery2022palm, scao2022bloom, touvron2023llama, zhang2022opt}.
However, the rapid increase in LLM parameters poses substantial challenges to memory and computational resources on existing hardware platforms like GPUs~\cite{v100,a100,h100} and TPUs~\cite{jouppi2023tpu}.
The latest Llama3 model~\cite{dubey2024llama3herdmodels}, with over 405 billion parameters, requires about 800 GB of memory, exceeding the capabilities of high-end hardware platforms such as the H100 GPU~\cite{h100}, which has 80~GB memory.
Additionally, single-batch inference involves sequentially predicting each token based on the previous ones due to the auto-regressive nature of LLMs.
As a result, LLM inference mainly depends on narrow matrix multiplications, making it memory-bound and leading to severe under-utilization of GPUs' computation resources.
% \fixme{(}Recent studies show the average utilization on \fixme{GPU A100} is less than $1\%$~\cite{?}. 
% Addressing this inefficiency necessitates innovative approaches in algorithmic design and hardware optimization to fully leverage the computational power of hardware for LLM inference tasks.\fixme{) maybe remove it?}



Quantization has emerged as a promising method to mitigate LLMs' challenges in memory consumption and inference latency, which can be applied at different granularities such as the level of a channel~\cite{xiao2023smoothquant,lee2024tenderacceleratinglargelanguage,kim2023squeezellm} and a tensor~\cite{guo2022ant,zadeh2022mokey,zadeh2020gobo}.
Recent studies advocate for group-wise quantization, which uses a group, such as 64 contiguous elements within a channel, as the unit of quantization granularity~\cite{zhao2023atom,lin2023awq,frantar2023gptq,shao2024omniquant,dai2021vsquant,liu2024kivi}.
The group quantization reduces the quantization error with negligible software overhead, and hence has become the standard method for accelerating LLMs (\Sec{sec:acc_analysis}).

The above quantization methods mostly use \texttt{INT} or \texttt{FP} data types, and there are works that propose various adaptive data types to better adapt to different distributions and hence improve the quantized model performance.
The adaptive data type method falls into two main categories: data-type-based and clustering-based.
The former category selects data types from discrete sets based on tensor data distribution.
The representative work ANT~\cite{guo2022ant} packages several different data types for selection, including \texttt{INT} for the uniform distribution, \texttt{PoT} (Power of Two) for the Laplace distribution, and \texttt{flint} for the Gaussian distribution.
The latter category utilizes clustering algorithms to generate centroids that align with the data distribution and provide considerable adaptivity. 
Mokey~\cite{zadeh2022mokey} and GOBO~\cite{zadeh2020gobo} exemplify this approach (\Sec{sec:acc_analysis}).
 
%though they focus on tensor- or channel-wise quantization. 
%In our study, we adapt them to group quantization through per-group clustering.

%The recent quantization methods often combine with group-wise quantization to reduce the quantization error~\cite{lin2023awq,shao2024omniquant,zhao2023atom,ashkboos2024quarotoutlierfree4bitinference,liu2024kivi}.

However, our analysis shows that those existing adaptive methods fail to achieve optimal performance in terms of accuracy and resource efficiency.
While different tensors exhibit similar distributions, small groups can have markedly different distributions. 
This finding underscores the necessity for full adaptivity in group quantization to fully realize its potential.
Using the per-group clustering as the ideal baseline, we show that existing data-type-based adaptive method ANT~\cite{guo2022ant} still suffers from a significant accuracy loss (\Sec{sec:acc_analysis}).

%current group-wise quantization schemes still encounter several challenges that have not been fully addressed via existing optimizations.
%\textbf{\textit{First, existing schemes exhibit limited representation capability due to the lack of support for the diversity of LLM tensors}}, especially concerning fine-grained granularity, such as group-wise quantization. 
%Most quantization~\cite{frantar2023gptq,lin2023awq,shao2024omniquant,zhao2023atom} adopts a single data type (e.g., \texttt{INT}) to achieve high speedup and fast data type casting.
%However, using only one data type limits the flexibility of representation. 
%Many studies~\cite{guo2022ant,guo2023olive,tambe2020algorithm} emphasize the significance of adaptivity.
%Despite this, most adaptive quantization methods encounter efficiency issues with encoding and decoding when employing complex mechanisms or diverse encoding schemes across different groups. 
%E.g., some efforts~\cite{zadeh2022mokey,zadeh2020gobo,jain2019biscaled,guo2023olive} design adaptive encoding methods to address outliers and enhance accuracy at the expense of speedup.

% Although weight-activation quantization can potentially achieve higher memory reduction by quantizing both activation and weight synergistically, weight-only quantization is a more viable approach to LLMs.
% The reason is that activation only occupies a small fraction of memory ($<$5\%~\cite{yuan2024llm}), and quantizing activation would often lead to significant accuracy degradation~\cite{shao2024omniquant}.
% Furthermore, weight-only quantization can be done offline, eliminating runtime quantization overhead.

% Given LLMs auto-regressive nature, weight-only quantization (WoQ) is a more viable approach compared to the double-sided quantization, which includes both activation and weight quantization. 
% The reason is that activation quantization faces challenges in maintaining model accuracy and only occupies a minor fraction (approximately \fixme{5\%~\cite{yuan2024llm}} in the single-batch inference scenario) of memory relative to weights. 
% In contrast, activation only occupies a small fraction of memory ($<$5\%~\cite{yuan2024llm}), and quantizing activation would often lead to significant accuracy degradation~\cite{shao2024omniquant}. 


% For instance, the weight-only quantization with 4-bit precision can achieve a 4-fold speedup.
% In contrast, double-sided quantization typically entails 8-bit quantization for both activation and weight with complicated optimization \fixme{requirements~\cite{xiao2023smoothquant,guo2023olive,guo2022ant,dettmers2022llm}}. 
% Furthermore, the WoQ can be done offline, eliminating runtime quantization overhead.

% The adaptive schemes encounter several challenges that have not been fully addressed via existing optimizations.

We also show that it is not feasible to transform existing adaptive methods for the group quantization paradigm.
For the adaptive data type methods, a naive approach to enhance adaptivity is to expand their set of data types. However, incorporating new data types requires additional decoders, which increase hardware costs. 
Additionally, compatibility issues between new and existing data types may reduce computational efficiency. For instance, the \texttt{NF4} data type~\cite{dettmers2023qlora} requires an FP16 MAC unit, which is incompatible with existing \texttt{ANT} data types.
Clustering-based adaptive methods like GOBO~\cite{zadeh2020gobo} and Mokey~\cite{zadeh2022mokey} can sufficiently adapt to various distributions at the group level. However, they require codebooks for quantization and dequantization, leading to high adaptivity at the expense of encoding and computational efficiency. 
For instance, GOBO~\cite{zadeh2020gobo} employs the K-means algorithm to quantize weights and requires dequantization to \texttt{FP16} using a codebook lookup table before computation, resulting in high adaptivity but low computational efficiency (\Sec{subsec:efficiency}). 

%Conversely, Mokey~\cite{zadeh2022mokey} enhances the computation of clustering-based methods by using indices for centroid values via approximate calculations, though matrix multiplication still relies on floating-point units, increasing overhead compared to integer units. Furthermore, Mokey creates one \texttt{golden dictionary} for all activations and weights, akin to using a single data type in quantization, thus reducing adaptivity.


%\textbf{\textit{Second, existing adaptive schemes face challenge in balancing adaptivity and computational efficiency}}.
%Adaptive methods generally support several data types or data representations, decoding from low-bit formats to high-precision values through custom decoders or lookup tables (LUTs).
%Custom decoders limit the capability for data type expansion because incorporating new data types requires additional decoders, which increases hardware costs.
%Adaptive methods that decode through LUT~\cite{zadeh2020gobo,zadeh2022mokey,kim2023squeezellm} offer more adaptivity because they can store flexible key-value mappings for any data point.
%However, the decoding overhead increases with the size of the LUT, and the decoding efficiency impacts the overall computational efficiency.
%Overall, the above methods can only be treated as a compression technique, optimizing memory access and enhancing memory bandwidth, while wasting many computational resources, as quantized values have low-precision information but can not leverage the low-precision computation units.



% Some research work~\cite{guo2022squant,guo2023olive} adaptively selects data types from 3 or 4 options.
% They propose custom decoders to support efficient data type casting and provide high computational efficiency.
% However, in group-wise quantization with variable distribution, custom data types are typically not adaptive enough.
% Clustering methods~\cite{kim2023squeezellm,zadeh2020gobo,zadeh2022mokey,tseng2024quipbetterllmquantization} with higher adaptivity, require dequantization to the original high precision for computation through a codebook.
% Overall, clustering methods can only be treated as a compression technique, optimizing weight access and enhancing memory bandwidth, while wasting many low-precision computational resources.

% Despite that, existing group-wise quantization schemes still encounter several challenges that have not been fully addressed via existing optimizations.
% \textbf{\textit{First, weight-only poses computational challenges due to the bit-width and data type mismatches}}.
% WoQ quantizes weights using a specialized low-precision encoding and keeps activation in high-precision.
% However, such a weight-activation combination with two distinct precisions and data types is complex to be executed natively by current computation units. 
% Thus, weight-only often dequantizes the encoded numbers to the original high precision for computation.
% As a whole, weight-only can only be treated as a compression technique, optimizing weight access and enhancing memory bandwidth, 
% while wasting a lot of computational resources, as quantized weights have low-precision information but can not leverage the low-precision computation units~\cite{a100,h100,jouppi2023tpu}.


% \textbf{\textit{Second, existing schemes exhibit limited representation capability due to the lack of support for the diversity of LLM tensors}}, especially concerning fine-grained granularity, such as group-wise quantization. 
% Most quantization~\cite{frantar2023gptq,lin2023awq,shao2024omniquant,zhao2023atom} adopts a single data type (e.g., \texttt{INT}) to achieve high speedup and fast data type casting.
% However, using only one data type limits the flexibility of representation. 
% Many studies\mbox{~\cite{guo2022ant,guo2023olive,tambe2020algorithm,ramachandran2024algorithmhardware}} emphasize the significance of adaptivity.
% Despite this, most adaptive quantization methods encounter efficiency issues with encoding and decoding when employing complex mechanisms or diverse encoding schemes across different groups. 
% E.g., some efforts~\cite{zadeh2022mokey,zadeh2020gobo,jain2019biscaled,guo2023olive} design adaptive encoding methods to address outliers and enhance accuracy at the expense of speedup.

% Third but more crucial, 
Finally, it is also challenging to satisfy the real-time quantization of LLMs' KV caches, which can occupy over $70\%$ of memory and computation resources when the sequence length is sufficiently long~\cite{hooper2024kvquant,zhao2023atom,sheng2023flexgen}. 
% \textbf{\textit{Third, most methodologies fail to support group-wise Key-Value (KV) cache quantization with low-precision computation}}.
As a result, the efforts on weight and activation quantization become marginal due to this dominance of KV cache.
Although, in the LLM inference, KV cache behaves more like dynamically generated ``weights" for the $Q$ values in the attention mechanism~\cite{vaswani2017attention},
% A deeper challenge is 
quantizing the KV cache at a fine-grained granularity is challenging due to substantial runtime overhead. 
% Quantized weights require extensive offline processing, sometimes spanning hours or days. 
Most previous quantization efforts~\cite{lin2023awq,guo2022ant,guo2023olive,frantar2023gptq,dettmers2023spqr,dettmers2023qlora}, encompassing weight-activation and weight-only quantization have sidestepped the issue of KV cache quantization (\Sec{sec:moti_kvcache}).


In this study, we introduce \proj{}, \underline{m}athematically \underline{a}daptive \underline{n}umeric \underline{t}ype, a novel adaptive method that aims at overcoming the above challenges, which can be summarized into three points: decoding and computation efficiency, group-wise diversity, and KV cache dynamics.
To tackle the first two challenges related to efficiency and diversity, we propose \proj{} with a simple yet mathematical formulation to support various data types. 
The formulation of \proj{} is designed to encompass a wide spectrum of data types, theoretically extending to ``infinite'' variations through a smooth transition in the data type distribution. 
Examples of supported data types include \texttt{NormalFloat}~\cite{dettmers2023qlora} and \texttt{flint}~\cite{guo2022ant}.
Moreover, adopting a group-wise design approach, we enable \proj{} to operate at a fine-grained granularity.
This inherently addresses the diversity and adaptivity requirements within LLMs. 
Employing group-wise formulated data types introduces computational overhead due to the diversity of data types at this fine-grained granularity.
Thus, we introduce a novel computing paradigm named \proj, to mitigate this overhead significantly (\Sec{sec:encode}).
% leveraging our sophisticated design 


To address the KV cache's dynamic challenge, we introduce a novel real-time quantization method, which lets us integrate weight and KV cache quantization into a unified framework.
In our approach, we regard the KV cache as ``dynamic weights'', as opposed to the ``static weights'' of the original weights.
Despite having identical tensor shapes, the K cache and V cache employ distinct computation methods, significantly complicating our design.
We propose two real-time quantization techniques in two dimensions: spatial and temporal. 
Spatial quantization is tailored for the K cache, where all elements within a K cache group are generated simultaneously. 
To accommodate this, we allocate sufficient quantization processing units for the K cache.
Conversely, the V cache presents a more intricate challenge for real-time quantization. In the V cache, elements within a group are generated across the temporal dimension, meaning they emerge in different continuous iterations. 
Consequently, we propose a highly efficient temporal real-time quantization method tailored to the V cache (\Sec{sec:dse}).


Finally, \proj proposes efficient group-wise quantization for both weight and KV cache.
We augment the processing element (PE) in the classic systolic array architecture to support our encoding paradigm and implement a real-time quantization engine to accelerate KV cache quantization (\Sec{sec:architecture}).
We show that the components of \proj introduce negligible area overhead with significant speedup and energy reduction compared to the Transformer accelerators~\cite{guo2022ant,zadeh2022mokey,guo2023olive,lee2024tenderacceleratinglargelanguage}.

In summary, this paper makes the following contributions.

\begin{itemize}[leftmargin=*]
	\item We analyze the challenges of group-wise quantization in both weight and KV cache, and design a new encoding paradigm to improve information utilization in group-wise quantization and inherently enable efficient decoding in LLM inference.
    \item We introduce a specific processing element (PE) to provide efficient encoding and computation for our encoding paradigm, and by integrating the real-time quantization components for KV cache, we implement a unified quantization system for group-wise quantization in weight and KV cache.
    \item We integrate these components into an existing systolic array and achieve up to 4.46$\times$ speedup and 4.10$\times$ energy reduction compared to existing works, with negligible area overhead.
\end{itemize}