\section{Evaluation}
\label{sec:evaluation}


\input{figure_text/tbl_eval_ppl_result.tex}

We implement the \proj quantization framework and compare its accuracy with four baselines:  Tender~\cite{lee2024tenderacceleratinglargelanguage}, OliVe~\cite{guo2023olive}, ANT~\cite{guo2022ant}, and BitFusion~\cite{sharma2018bit}.
Subsequently, we conduct a detailed analysis of these accelerators' area, performance, and energy consumption, focusing on both the linear and attention layers.
Based on this analysis, we evaluate the performance across various sequence lengths. 
Finally, we evaluate the results by employing group-wise quantization in our baselines.

\subsection{Experimental Setup}
\label{sec:eval_setup}
\paragraph{Models and Datasets. } 
We evaluate \proj on three families of Large Language Models (LLMs), including LLaMA-1 7-65B~\cite{touvron2023llama}, LLaMA-2 7B~\cite{touvron2023llama2}, LLaMA-2 13B, OPT-6.7B~\cite{zhang2022opt} and OPT-13B.
Our evaluation covers different model architectures and sizes.
We evaluate the quantization results using the zero-shot task on the Wikitext dataset (wikitext-2 version)~\cite{merity2016pointer}, with perplexity (PPL) of the generated sentences serving as the metric.
The lower PPL indicates the better results.
We take two subsets from the Pile dataset\mbox{~\cite{gao2020pile}} as the calibration dataset to avoid a biased result.
The OPT and LLaMA models are evaluated by lm-eval-harness framework~\cite{eval-harness}.

We employ \proj on generation tasks to evaluate the KV cache quantization in decode stage.
We use TruthfulQA~\cite{lin2022truthfulqameasuringmodelsmimic} on lm-eval-harness for normal context evaluation and TriviaQA~\cite{joshi2017triviaqalargescaledistantly} on LongBench~\cite{bai2024longbenchbilingualmultitaskbenchmark} for long context evaluation.

\paragraph{Quantization Details. }
We evaluated our approach against three baselines of Transformer quantization accelerators Tender~\cite{lee2024tenderacceleratinglargelanguage}, OliVe~\cite{guo2023olive}, ANT~\cite{guo2022ant} in 4-bit and 8-bit PTQ (Post-Training Quantization) settings.
All of these are open-source frameworks.
ANT performs activation-weight quantization across CNN and Transformer models, such as BERT~\cite{devlin2018bert}, and we extended its framework to support LLM quantization.
Tender and OliVe is the state-of-the-art (SOTA) LLM accelerator that mitigates outliers with a hardware-friendly methodology.

% Mokey, a Transformer accelerator, handles normal and outlier values distinctly, proposing a hardware design that supports efficient lookup table computations.
% Since Mokey has yet to release its code, we reproduce its quantization design for evaluation.
ANT and OliVe apply tensor-wise quantization to activations and channel-wise to weights.
Tender divides activation channels into several chunks. 
In each chunk, Tender further reorders and packages the channels into groups so that the scaling factors of the adjacent groups can be derived via a simple 1-bit shift. 
By moving the shift to accumulation, Tender accommodates outliers with a unified scaling factor in a chunk.
This method is orthogonal to \proj{} and can work together. 
We evaluate Tender here as a SOTA LLM accelerator.
Since Tender, OliVe, and ANT do not quantize the attention layer of LLMs, our analysis of their quantization accuracy is limited to linear layers.


\proj is configured with a group size of 64, applying this setting to quantize activations, weights, and KV cache.
\proj employs \texttt{INT} for activation quantization.
For weight and KV cache quantization, \proj selects the most appropriate encoding from the set $a = \{0, 5, 10, 17, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120\}$ and the \texttt{INT}.
\proj selects the coefficient $a$ for KV cache based on the data variance, as described in \Sec{sec:dse_kv}.


\paragraph{Implementation. }
We evaluate the performance and energy consumption of \proj compared to baseline accelerators.
The \proj's processing elements (PE) and comparator are implemented in Verilog.
They are synthesized using Synopsys' Design Compiler (ver. O-2018.06-SP1) with the TSMC 28nm standard cell library to obtain power and area statistics.
Additionally, the SRAM module's power and area statistics are simulated using CACTI~\cite{muralimanohar2009cacti}.

We develop a simulator based on the cycle-level simulator DNNWeaver~\cite{sharma2016high} to evaluate performance.
ANT, OliVe, and BitFusion, which are built on this simulator, all support mixed precision.
We also implement the architecture of Tender in this simulator and support both \texttt{INT4} and \texttt{INT8} MAC.
ANT has also extended its design using a GPU simulator~\cite{gpgpu-sim2009,leng2013gpuwattch,gpgpu-sim4.0}, but our evaluation focuses exclusively on ASIC implementations.



\paragraph{Accelerator Details. }
BitFusion is the baseline mixed-precision architecture.
ANT is a mixed-precision architecture with adaptive data types.
OliVe further enhances this by supporting outlier-victim pairs with adaptive data types and mixed precision.
Tender can be easily extended to support mixed precision.
Consequently, we consider these architectures as baselines in our evaluation.


For a fair comparison, in the evaluation of the linear layer, we employ the mixed-precision in Tender and OliVe to align the PPL results.
OliVe and Tender utilized 4-8 mixed precision.
The 8-bit ANT can not align the PPL results either, so we label it ANT*.
The 8-bit ANT does not adaptively select the data type and only uses \texttt{INT}, so we can view ANT* as a coarse-grained \texttt{INT8} quantization.

In addition, we configure the number of PEs for each accelerator based on their area overhead.
We aim to compare their performance and power consumption while ensuring nearly equivalent area and PPL.
We set the same configuration for memory bandwidth, on-chip buffer size, and frequency across all accelerators.
In linear layer, the sequence length is set to 2048, with a batch size of 1.



Tender, OliVe, and ANT do not quantize the attention layer in their evaluations.
Therefore, the statistics for these baselines in the attention layer are the same, as shown in \Fig{fig:full_eval}.



\subsection{Accuracy Evaluation}
\label{sec:accuracy}


\paragraph{Results in the Linear Layer. }
In \Tbl{tbl:ptq_result}, we present the PTQ results for the discussed methods.
Both ANT and OliVe implement mixed-precision quantization, but their hardware design requires that activations and weights share the same bit width.
Thus, we evaluate the quantization under 4-bit weight-activation (W4A4) and 8-bit weight-activation (W8A8) settings.
In W4A4 scenario, \proj shows better PPL results than other methods.
\proj with W4A8 achieves under 0.11 PPL loss in LLaMA family models and under 0.12 PPL loss in OPT models.

ANT and OliVe with W4A4 demonstrate significant PPL loss due to the sensitivity of activations, but OliVe's W8A8 configuration shows promising results.
Tender's W4A4 is better than ANT and OliVe because its progressive strategies in the chunk offer a wider region of representation.
Tender's W8A8 result is comparable to OliVe's W8A8.
In OPT models, Tender's W8A8 surpasses \proj's W4A8, while \proj's W4A8 achieves the best perplexity results in the LLaMA family models.



% Mokey's W4A4 configuration outperforms ANT's and OliVe's as it preserves a substantial proportion of outliers (4.5\% for activations and 1.5\% for weights) and deals with them separately.
% which incurs the extra overhead to deal with outliers.


\paragraph{Quantize both the Linear and Attention Layer. }
% \proj employs \proj on weight and KV cache quantization.
The Wikitext task only involves the prefill stage, so \Tbl{tbl:ptq_result} reflects the results of quantizing the KV cache during this stage.
Results show that \proj achieves a PPL loss below 0.3 when quantizing both linear and attention layers.
Compared to quantizing only the linear layer, quantizing the attention layer with \proj results in a PPL loss below 0.2.
Quantizing the KV cache along the opposite dimension~\cite{liu2024kivi,hooper2024kvquant} further reduces the loss to 0.1, but performing the low-bit computation in this way is challenging.


\paragraph{Analysis for Generation Tasks.}
For a comprehensive analysis of KV cache quantization, we evaluate \proj on generation tasks TruthfulQA and TriviaQA.
In \Tbl{eval:kv_cache_results}, W4A8 indicates that weights are quantized to 4-bit using \proj, and activations are quantized to \texttt{INT8}.
For KV cache quantization, \proj outperforms \texttt{INT} and achieves a loss below 1.7\%.
% Additionally, \proj demonstrates minimal loss in generation tasks.

\input{figure_text/tbl_eval_kv_task.tex}


\input{figure_text/tbl_eval_area.tex}

\subsection{Performance, Energy, and Area Evaluation}


\paragraph{Area.}
\Tbl{tab:area} shows the breakdown of the components for \proj and other accelerators, which are all synthesized using a 28~nm process~\cite{guo2022ant,lee2024tenderacceleratinglargelanguage}.
Note that Olive was originally synthesized with 22~nm process~\cite{guo2023olive}.
%For fairness in comparison, we scale OliVe to 28nm using DeepScaleTool~\cite{sarangi2021deepscaletool}.
All accelerators are configured with identical buffer sizes.
In addition, their vector units are the same in our evaluation, so the area is not listed in \Tbl{tab:area}.
In the subsequent evaluation, the number of cores used in the simulator matches those listed in \Tbl{tab:area}.


\input{figure_text/fig_eval_linear.tex}

\paragraph{Linear Layer.}
\Fig{fig:lieanr_perf} presents the performance and energy results of various methods across LLaMA-7B\&65B and OPT-6.7B\&13B.
Given the similarity in LLaMA family models, we focus our evaluation on LLaMA-7B and the larger LLaMA-65B.
Note that ANT* can not recover the accuracy to align with Tender, OliVe, and \proj.
A comparison between OliVe and BitFusion explains the advantages of efficient outlier handling.
BitFusion has higher latency and energy because it performs computation in 8 and 16 bits.
% while comparison between ANT* and \proj-\texttt{INT} reveals the benefits of group-wise quantization.
% The speedup of \proj is up to 5$\times$ on LLaMA-65B and OPT-6.7B models since they are sensitive in coarse-grained quantization.
% Tender and OliVe employ mixed precision, while Tender outperforms Olive because the 8-bit layer is less than OliVe.
Tender outperforms Olive because the 8-bit layer is less than OliVe.
% \proj performs better than \proj-\texttt{INT} because \proj-\texttt{INT} uses 8-bit weight quantization in some layers of models to align the PPL results.
By leveraging flexible encoding and group-wise quantization, \proj outperforms Tender, OliVe, ANT*, and BitFusion with average speedups of 1.83$\times$, 1.96$\times$, 2.00$\times$, and 4.93$\times$, respectively.


% In terms of energy consumption, \proj's PE uses more power than the PE in Tender due to the shift-accumulation units.
% Consequently, \proj does not have a significant advantage over Tender in core energy.
Compared to Tender, OliVe, and ANT*, the energy efficiency of \proj mainly comes from static energy, which is related to execution cycles.
The reduced bit width in \proj lowers DRAM and buffer energy.
The more dequantization in group quantization and the extra shift operation increase the power usage of cores.
Thus, MANT has similar core energy to other baselines, even with a lower bit width.
% In \Fig{fig:lieanr_perf}, the core power of \proj-\texttt{INT} on OPT-6.7B exceeds that of OliVe and ANT* as \proj-\texttt{INT} utilizes 8-bit weight quantization in the $fc_1$ and $fc_2$ layers.
In total, \proj achieves energy reductions of 1.39, 1.54, 1.57, and 4.16 times compared to Tender, OliVe, ANT*, and BitFusion, respectively.

A typical of GEMM ($M \times K \times N$) in LLaMA-7B is (M, 4096) $\times$ (4096, 4096). 
In this case, the non-overlapped quantization overhead (in the last iterations) occupies 0.3\%. This ratio is decided by the $K$ and $N$, as we detail in \Sec{sec:quant_dequant}.
% For a GEMM that the $K$ is larger than 512, our design can fully overlap the quantization latency in iteration.
% This is decided by division latency, group size, and the tile of our accelerator.



\input{figure_text/fig_eval_all_layer.tex}



\paragraph{Performance and Energy for All Layers. }
We evaluate the performance and energy of complete layers containing linear and attention.
\Fig{fig:full_eval} shows the speedups \proj achieves over other accelerators at various sequence lengths.
The evaluation uses the LLaMA-7B model with sequence lengths ranging from 2K to 128K.
The baselines do not quantize the attention layer and, therefore, employ 16-bit computation in this layer.
% \sout
% {As sequence length increases, the attention component becomes increasingly dominant.
% With the sequence length reaching 32K, the impact of speedup in the linear layer diminishes, with OliVe only achieving a 1.41$\times$ speedup over BitFusion.
% }

% In the evaluation for all layers, the trends in speedup and energy reduction are influenced by both linear and attention layers.
The overall model performance is decided by both linear and attention layers.
% \mbox{\Fig{fig:lieanr_perf}} and \mbox{\Fig{fig:atten_eval}}.
% The computation overhead of the attention layer increases with sequence length. 
As the sequence length increases, the attention layer becomes increasingly dominant.
With 2K sequence length, the linear layer's latency surpasses that of the attention layer, so the overall speedup and energy reduction are primarily determined by the linear layer.
However, when the sequence length reaches 128K, the impact of speedup and energy reduction in the linear layer nearly diminishes.
With a sequence length of 128K, OliVe achieves only a 1.15$\times$ speedup, and Tender achieves a 1.17$\times$ speedup over BitFusion.
Therefore, at a sequence length of 128K, the improvement of \proj is primarily determined by the attention layer.

\proj consistently delivers speedups between 2.04-4.54$\times$ and energy reductions from 1.76-4.12$\times$ compared to OliVe across different sequence lengths.
Moreover, \proj achieves on average 2.99$\times$ (up to 4.46$\times$) speedup and 2.81$\times$ (up to 4.10$\times$) energy reduction to Tender in different sequence lengths.



\paragraph{Data Type Ratio.}
\Fig{fig:data_ratio} shows the selection ratio of $a$ in different tensors, layers, and models.
The $\text{layer}_0$ of LLaMA-2-7B and OPT-6.7B mostly select $a=0$, while other layers and models have a relatively uniform selection.



\input{figure_text/fig_eval_linear_group.tex}

\input{figure_text/fig_eval_dtype_ratio.tex}

\subsection{Comparison and Discussion of Group-wise Methods} 
\label{sec:evla_group}

Group-wise quantization can significantly improve the results.
To discuss the benefits of adaptive data types, we compare the perplexity results with group-wise \texttt{INT} quantization.
In addition, we extended ANT and OliVe to support group-wise quantization.
For a fair accuracy comparison, we dynamically calculate the per-group scaling factor in activation quantization for all methods.
It is worth noting that the other methods do not optimize the process of scaling factor computation, leading to non-negligible quantization overhead.
\Tbl{tbl:w4a4_result} shows the group-wise comparison of 4-bit weight and activation quantization.

ANT faces challenges in group-wise quantization.
For weight quantization, ANT can easily select data types from \texttt{INT}, \texttt{flint}, and \texttt{PoT} for each group.
However, ANT does not support real-time data type selection for activation quantization.
It is difficult to determine data types for each group offline due to variable sequence lengths.
Therefore, ANT adaptively selects per-group data types for weight quantization and per-tensor data types for activation, using a per-group scaling factor for both weight and activation.
Selecting a single data type for the entire tensor is typically unsuitable for each group, making ANT less effective than naive \texttt{INT} when the group size is 64 or 32.
OliVe performs better than ANT and \texttt{INT} when the group size is 128.
However, OliVe does not benefit from smaller group sizes.
Olive sacrifices a victim value to provide more bit width for the outlier value.
However, while group-wise quantization mitigates the effect of outliers, as the group size decreases, the negative impact of victims in OliVe outweighs the benefits of protecting outliers.

\proj outperforms group-wise OliVe, ANT, and \texttt{INT} across different group sizes.
Besides, we compare the perplexity with \texttt{MXFP} using a group size of 32.
The scaling factor with an 8-bit exponent (E8M0) increases the quantization error, raising the PPL to 7.16.
The advantage of \texttt{MXFP} is the fast data type casting from \texttt{FP16} to \texttt{MXFP}.

To isolate the benefits of the MANT data type, we extend ANT architecture to support group quantization.
For weights, we determine the data types for each group individually.
Since ANT does not support real-time data type selection, we use a calibration dataset to select a global data type for the entire tensor and apply per-group scaling factors for activations.
Additionally, we implement group-wise \texttt{INT} for real-time KV cache quantization.
To ensure comparable PPL performance with MANT, we use a 4/8 mixed precision configuration for ANT.
As illustrated in \Fig{fig:lieanr_group}, with the same group size of 64, MANT achieves an average speedup of 1.70$\times$ speedup and 1.55$\times$ energy efficiency over ANT.




\input{figure_text/tbl_eval_group_ppl.tex}



