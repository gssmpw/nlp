\vspace*{0.2cm}
\section{Related Work}

This section presents related work on quantization method and low-bit quantization accelerators.

\fixme{ANT,olive,tender,FIGNA,LogarithmicPosit...}

\paragraph{Fixed Data Type.} 
\bluec{List of data types: int, float, pot, flint, nf.}

High-precision data types are predominantly supported in hardware through formats such as INT32, FP32, and FP16.
Google's BrainFloat16 (BF16) and NVIDIA's Tensor Float 32 (TF32)~\cite{micikevicius2018mixed} have been developed to expedite AI training and high-performance computing (HPC) applications. These formats are extensively utilized in Google's TPU~\cite{jouppi2023tpu} and NVIDIA's A100 GPUs~\cite{a100}.

% \fixme{BFP...}

% In terms of low-precision data types, hardware typically supports INT and float formats.
% The NVIDIA Hopper architecture includes arithmetic units for INT8 and FP8, and the NVIDIA Ampere architecture features units for INT8 and INT4, while additionally, TPUv4~\cite{jouppi2023tpu} supports INT8 GEMM operations.
% However, the distributions of fixed data point like INT and float are not ideally suited for quantization.
% Typically, most DNN parameters conform to a Gaussian distribution, although the presence of outliers in activations may result in a more complex data distribution.
% To address this, ANT~\cite{guo2022ant} introduces a novel data type called 'flint', which aligns more closely with the Gaussian distribution compared to traditional float. 
% Flint focus on the distribution of a tensor.
% Similarly, QLoRA~\cite{dettmers2023qlora} proposes 'NormalFloat', a new data type that leverages quantile data points from the standard Gaussian distribution.

\paragraph{Learnable Data Type.}
\bluec{Basically kmeans. references: GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference; LUT-NN: Empower Efficient Neural Network Inference with Centroid Learning and Table Lookup; Learnable Lookup Table for Neural Network Quantization}


AdaptiveFloat utilizes a tensor-wise exponent bias to adjust the range of \texttt{float} values.
The exponent bias can be implemented through a shifter, it is similar to scaling factor in quantization.
Although AdaptiveFloat adaptively adjusts the range of \texttt{float}, it does not alter its distribution.
It can not fit the distribution diversity in group-wise quantization.

Mokey, a Transformer accelerator, handles normal and outlier values distinctly, proposing a hardware design that supports efficient lookup table computations.
Mokey employs agglomerative clustering to generate the \texttt{Golden dictionary} and stores the indices of each parameter.
It computes using indices instead of centroid values through approximate calculations.
However, Mokey still relies on \texttt{float}-based computations and only uses one \texttt{Golden dictionary}.
In group-wise quantization, generating multiple \texttt{Golden dictionary} and approximating their centroid value is a challenge.

GOBO separates the weights into Gaussian (G) and outlier (O) groups, it costs 4 bits for G groups and 16 bits for O groups.
GOBO exploits clustering algorithm for G groups, it can quantize the weights based on its data distribution.
However, GOBO only supports weight quantization, and it requires high-precision computation.

[BiQGEMM, GEMM with lookup table; ]

Distributions of data do not always conform to one type, so the quantization error of fixed data types is a challenge.
% Current data types in quantization are limited by hardware, which impede efficient bit information utilization.
Learnable data types, which are representations of data points determined through algorithm, dynamically adapt the format and precision of data based on analysis and learning directly from the data itself. 
This adaptivity offers enhanced flexibility and optimization in data representation, particularly in complex or variable datasets.
ANT~\cite{guo2022ant} utilizes an adaptive method to select the most suitable numerical representation for each tensor from a predefined set of data types during quantization, adjusting these types based on the distribution of a tensor.
AdaptiveFloat~\cite{tambe2020algorithm} dynamically adjusts the tensor-wise exponent bias to minimize quantization errors.
Moreover, centroid-based algorithms, which dynamically determine data points, are widely used in quantization~\cite{han2015deep, zadeh2020gobo, lutnn2023, kim2023squeezellm, xu2018deep}.
% introduce some works
% The decoding process in centroid-based quantization typically requires a lookup table operation, as it exceeds the representational capacity of standard hardware.


\subsection{KV Cache Compression Technique}
\label{sec:bg_kv}


The memory consumption of KV cache is a significant issue in LLM inference.
% Several techniques have been proposed to optimize the KV cache, including PagedAttention\mbox{~\cite{kwon2023efficient}}, MQA\mbox{~\cite{kwon2023efficient}}, GQA\mbox{~\cite{ainslie2023gqa}}, quantization\mbox{~\cite{kang2024gear,liu2024kivi,hooper2024kvquant,zhao2023atom}}, and sparsity\mbox{~\cite{zhang_h_2o_2023,li_snapkv_2024,adnan_keyformer_2024,zhang_q-hitter_2024,sun_triforce_2024}}.
% PagedAttention is a memory management technique designed to reduce KV cache fragmentation. 
% In GQA and MQA, multiple query heads share a single key and value head, reducing the KV cache memory footprint.
Compression techniques such as quantization and sparsity reduce the size or bit width of the KV cache.
% This paper focuses on quantization, which can also be combined with other techniques to enhance memory efficiency further.


\paragraph{KV cache quantization.}
\fixme{KVQuant..., Gear...}
Some works~\cite{hooper2024kvquant,liu2024kivi,kang2024gear} quantizes the KV cache to mitigate the memory overhead.
KVQuant~\cite{hooper2024kvquant} discusses optimal quantization dimensions in KV cache and employs the non-uniform quantization in KV cache.
KIVI~\cite{liu2024kivi} discusses optimal quantization dimensions in KV cache.
% In KIVI, the K is quantized per channel, and the V is quantized per token, differing from our approach.
In KIVI, the K is quantized per channel, and the V is quantized per token.
They maintain a full-precision KV cache sliding window and quantize the high-precision KV cache after a certain period.