\section{Background and Related Work}
\label{sec:background}

% \subsection{Large Language Models}

% \bluec{Explain the basics for LLMs.}
In this section, we first present the background of LLMs and their inference processes.
Next, we examine quantization techniques, emphasizing group-wise quantization and commonly used data types in quantization.

\subsection{LLM Inference Process}
% \bluec{stress the proportion of decoding phase in LLM inference}

LLMs often adopt the Transformer architecture~\cite{vaswani2017attention} in LLMs.
Each Transformer layer incorporates an attention module and a feed-forward network (FFN).
Mathematically, multi-head attention can be described as follows:
\begin{align*}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O, \\
\text{head}_i &= \text{Attention}(HW_i^Q, HW_i^K, HW_i^V), \\
\text{Attention}(Q, K, V) &= \text{softmax}(QK^T/\sqrt{d_k})V.
\end{align*}
Here, $W_i^Q$, $W_i^K$, $W_i^V$, and $W^O$ are parameter matrices for the $i$-th head and the output projection, respectively. And $H$ is the hidden state. The $softmax$ function is applied over the keys to normalize their weights, ensuring that the output is a weighted sum of the values based on the input's relevance.

The generative inference process of LLM typically involves two distinct stages: \textbf{prefill} stage and \textbf{decode} stage. 
The prefill stage supplies a prompt sequence to the model, establishing the context for subsequent text generation.
Simultaneously, the attention operation produces $K$ and $V$ tensors, which are cached using the \textbf{KV cache} technique to prevent redundant recomputation during the subsequent decode stage.
The decode stage is where actual text generation occurs.
In this stage, the model uses each generated token along with the KV cache to iteratively generate the next token and update the KV cache.
The KV cache continues to expand until the process meets a specific termination condition.
Since the \textbf{decode} stage requires only a single token as input, its operations primarily consist of matrix-vector multiplication (GEMV), resulting in a memory-bound scenario dominated by memory bandwidth.


The memory footprint of generative LLM inference comprises two major components: the weight and the KV cache, either of which can be the memory bottleneck under various scenarios~\cite{hooper2024kvquant, liu2024kivi,kang2024gear,zhao2023atom,sheng2023flexgen,guo2024survey}.
Compression techniques such as quantization~\cite{han2015deep,guo2022ant,zadeh2022mokey,zadeh2020gobo,zhao2023atom,lin2023awq,guo2023olive, frantar2023gptq,li2023efficientadaptiveactivationrounding,dettmers2022llm} and sparsity~\cite{guo2020accelerating,frantar2023sparsegptmassivelanguagemodels,wang2021dual,guan2020far,guan2022block,guan2022transkimmer,guan2024fractal,NIPS1989_6c9882bb,zhang_h_2o_2023,zhang_q-hitter_2024,zhang2024dstc, guo2024accelerating,qiu2019adversarial} emerge to solve these issue.

\subsection{Quantization Technique}
\label{sec:bg_quantization}

The quantization technique~\cite{yao2020zeroquant,dettmers2022llm,shao2024omniquant,frantar2023gptq,guo2022squant,shen2020q} is an effective method for compressing neural network parameters with minimal loss.
The equation below defines the quantized data $W'$ and the dequantized data $\hat{W}$, where $s$ represents the scaling factor determined by the ranges of the two data types:
\begin{equation}
    \begin{array}{c}
        W' = \lfloor \frac{W}{s} \rceil, \hat{W} = s \times W' \\
    \end{array}
\label{quant_and_deq}
\end{equation}
Traditional quantization methods assign mappings at the tensor or channel level, known as tensor- or channel-wise quantization.
In these cases, outliers can significantly affect quantization performance by greatly increasing the rounding errors of standard values across the entire tensor or channel~\cite{dettmers2022llm,guo2023olive}.


\paragraph{Group-wise Quantization.}
To address the outlier challenge, various studies~\cite{zhao2023atom,lin2023awq,frantar2023gptq,shao2024omniquant,dai2021vsquant} advocate for group-wise quantization, which uses a group, such as 64 contiguous elements within a channel, as the unit of quantization granularity.
Consider a tensor with dimensions (2048, 4096), comprising 4096 channels.
With a group size of 128, each channel contains $(4096/128) = 32$ groups, resulting in a total of $2048 \times 32 = 65536$ groups across the entire tensor.
Despite the modest overhead from the mapping parameter for each group, fine-grained group-wise quantization limits the impact of outliers to smaller regions, thereby enhancing performance.

\paragraph{Data Type for Quantization.}
Several studies~\cite{guo2022ant,dettmers2023qlora,zadeh2022mokey,ramachandran2024algorithmhardware} adapt to the different distribution observed at tensor or channel levels by using customized data types. ANT~\cite{guo2022ant} introduces \texttt{flint} and employs an adaptive method to select the most suitable numerical data type for each tensor from a predefined set of data types. Building on ANT, OliVe~\cite{guo2023olive} develops \texttt{abfloat} to more accurately represent the distribution of outliers. Both ANT and OliVe enhance their systems with custom decoders and MAC (multiply-accumulate) units to facilitate their specific arithmetic computation workflows. Moreover, QLoRA~\cite{dettmers2023qlora} introduces \texttt{NormalFloat} (\texttt{NF}), which is derived from Gaussian distribution's quantile data points, for a more precise fit. Unlike other methods, \texttt{NF} requires high-precision dequantization before any computational operations, as it does not directly support multiplication and accumulation. In contrast, Mokey~\cite{zadeh2022mokey} uses clustering to derive the best date type for the quantization unit.
However, it requires additional codebooks to store cluster centroids.
Thus, it proposes \texttt{golden dictionary} (\texttt{GD}) to mitigate those overheads.
Microscaling \texttt{float} (\texttt{MXFP})~\cite{2023mxfp,NEURIPS2020_747e32ab} combines block-wise shared scale with \texttt{float} and \texttt{INT}.
\texttt{MXFP} is similar to current group-wise quantization, but the difference is the shared scale is an 8-bit \texttt{float} only contains an exponent field.

\subsection{KV Cache Optimization}
Several techniques have been proposed to optimize the KV cache, including PagedAttention~\cite{kwon2023efficient}, MQA~\cite{kwon2023efficient}, GQA~\cite{ainslie2023gqa}, quantization~\cite{kang2024gear,liu2024kivi,hooper2024kvquant,zhao2023atom}, and sparsity~\cite{zhang_h_2o_2023,li_snapkv_2024,adnan_keyformer_2024,zhang_q-hitter_2024,sun_triforce_2024}.
PagedAttention is a memory management technique designed to reduce KV cache fragmentation.
Several works~\cite{kwon2023efficient,guo2024gmlake,xu2024vtensor} focus on GPU memory management to minimize the fragmentation.
In GQA and MQA, multiple query heads share a single key and value head, reducing the KV cache memory footprint.
Compression techniques like quantization and sparsity reduce the size or bit width of the KV cache.
This paper focuses on quantization, which can also be combined with other techniques to enhance memory efficiency further.