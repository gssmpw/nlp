\begin{abstract} 
% Large Language Models (LLMs) demonstrate notable performance in natural language processing (NLP) tasks.
% However, their extensive weight parameters exceed the capacity of current consumer-level devices.
Large language models (LLMs) are one of the most important killer computer applications. 
The recent algorithmic advancement proposes a fine-grained group-wise quantization for LLMs, which treats a small set (e.g., 64) of values in a tensor as a compression unit.
It effectively preserves the model accuracy without retraining, and has become the standard approach to efficiently deploy LLMs.
On the other hand, there are works that propose various adaptive data types to better adapt to different distributions and further reduce the required bit length for LLMs.
In this work, our detailed analysis unveils a key finding that while different tensors exhibit similar distributions, small groups can have markedly different distributions.
As such, the group-level diversity requires a new level of adaptivity for which existing adaptive data types fail to provide.
%, which can be categorized into data-type-based and clustering-based, 
%a new challenge arises due to the diverse data distribution within groups, requiring the adaptive and flexible data types for quantization.
% We classify existing adaptive methods into two main categories: data-type-based and clustering-based, and analyze the challenges in balancing adaptivity and efficiency for both methods.

%improve the quantized model performance.


% However, in a group-wise scenario, a new challenge arises because the contradiction of more diverse data distribution with the inflexibility of existing data types. 

% Moreover, despite the ingenious design of the existing solutions, they are still lacking in computation efficiency.
%Despite the ingenious design of existing adaptive solutions, they still face challenges in balancing adaptivity and computational efficiency.

In this paper, we propose \proj, a \underline{m}athematically \underline{a}daptive \underline{n}umeric \underline{t}ype, featuring a more flexible encoding paradigm with a wider range of data distribution and more efficient decoding-computation fusion mechanism to address these challenges. 
Based on \proj, we develop a supporting framework to assign the appropriate data type for each group adaptively. 
Meanwhile, the dynamically generated Key-Value (KV) caches in LLMs introduce further complexity for real-time quantization.
To tackle this, we propose an efficient real-time quantization mechanism. 
Besides, we implement a specific processing element (PE) to efficiently support \proj and incorporate a real-time quantization unit. 
By integrating these components into a systolic array, \proj unifies the group-wise weight and KV cache quantization and addresses the associated challenges. 
Our evaluation shows achieving, on average, 2.99$\times$ (up to 4.46$\times$) speedup and 2.81$\times$ (up to 4.10$\times$) energy reduction to the state-of-the-art LLM accelerator.


% To maintain the model accuracy under the low-bit scenario, researchers have proposed the use of adaptive data types to accommodate the various value distributions of each tensor.
% On the other hand, recent algorithmic advancement has shown that fine-grained group-wise quantization, which treats a small set (e.g., 64) of values in a tensor as a quantization unit, can effectively preserve the LLM accuracy without retraining.
% However, the current group-wise quantization adopts a fixed int encoding and does not achieve significant computation efficiency.

% In this paper, we propose to combine the existing arts of adaptive data type and group-wise quantization method to maximize the benefit of low-bit quantization for LLMs.
% The challenge is that as the quantization unit becomes more fine-grained, 
% the mean and variance of the elements in different groups are more diverse, rendering the existing adaptive data type ineffective.
% Meanwhile, the dynamically generated Key-Value (KV) caches in LLM also introduce additional complexity level of real-time quantization.
% To tackle above challenges, we propose \proj, a \underline{m}athematically \underline{a}daptive \underline{n}umeric \underline{t}ype to assign the appropriate data type for each group adaptively.
% \proj is a new encoding paradigm, which can provide a wide range of data distribution and facilitates decoding through integer computations.
% We implement a specific processing element (PE) to efficiently decode \proj and incorporate a real-time quantization component for KV cache.
% By integrating these components into a systolic array, \proj unifies the group-wise weight and KV cache quantization and addresses the associated challenges.
% Our evaluation shows achieving, on average, 2.48$\times$ (up to 3.18$\times$) speedup and 2.47$\times$ (up to 3.22$\times$) energy reduction to the state-of-the-art LLM accelerator.



% Our evaluation shows an average speedup of 2.48$\times$ (peaking at 3.18$\times$) and an energy reduction of 2.47$\times$ (up to 3.22$\times$) compared to the state-of-the-art LLM accelerators.

% Quantization emerges as an effective method to mitigate memory overhead.
% Nonetheless, existing research on quantization is constrained by the data types supported by hardware, resulting in insufficient bit information utilization.

% \fixme{Giant: Group-wise computable adaptive numeric type.}


% We introduce \proj, a novel computable encoding and decoding approach for LLM quantization.
% Our decode process can be integrated with the existing accelerator, such as TPU and tensor core.
% During the encoding phase, our method achieves superior bit utilization compared to traditional integer and float representations.
% Additionally, we implement the process of decoding within a single PE and develop a specific architecture to accelerate LLM inference. 

% We propose a efficient and flexible quantization framework and architecture, providing up to 4x speedup over the SOTA LLM accelerator.

\end{abstract}


