
\section{Mathematically Adaptive Numerical Type}
\label{sec:encode}



%\congsay{Use the definition and outline to finish this section.}


We propose \proj (mathematically adaptive numerical type), a directly computable and adaptive data representation that achieves efficient encoding/decoding under the group-wise quantization framework.
We first present the formal mapping of \proj{}-encoded \texttt{INT} values in \Sec{sec:encode_mapping}.
We then describe its efficient encoding process in \Sec{sec:encode_encode} and how it eliminates the necessity for a data type-specific decoder in \Sec{sec:encode_decode}.


\subsection{Mapping Representation}
\label{sec:encode_mapping}



Two key considerations drive the design of our mapping representation.
The first consideration is the ability to accommodate the diverse distributions of group-wise data.
% where previous analysis revealed a substantial loss of information in the INT or other fixed format.
The second consideration is the computation efficiency, suggesting a direct computation approach using the \texttt{INT} value.
Based on the two considerations, we propose a mapping representation defined by the following equation:
\begin{equation}	
    Value_{grid} = \pm (a \times |\texttt{INT}| + 2^{|\texttt{INT}|})
    \label{eqn:map_represent}
\end{equation}

In Equation~\eqref{eqn:map_represent}, the $Value_{grid}$ is the quantization grid, $a$ is a group-wise constant, and \texttt{INT} is a sign-magnitude representation of \texttt{INT4} within the context of symmetric quantization, which covers the range from [-7, 7].
We take \texttt{INT4} as an example to explain our design since the computation of \texttt{INT4} is easy to support and \texttt{INT4} is memory-aligned.
The quantization grid, denoted as $Value_{grid}$, is constructed as follows: $\{\pm (a \times 0 + 2^{0}), \pm (a \times 1 + 2^1),..., \pm (a \times 7 + 2^7) \}$.
%The non-linear term of $2^{|\texttt{INT}|}$ diversifies the data representation.

\input{figure_text/fig_encode_function.tex}


Our core idea is to approximate each data type by changing the coefficient $a$.
The following equation defines the positive value function of different data types.
\begin{equation}	
\label{eq:fucntion}
    \begin{array}{c}
    y_{\text{INT}}(i) = i, y_{\text{PoT}}(i) = 2^i, y(i)_{\text{MANT}} = ai + 2^i, i \in [0, 7] \\ 
%    y_{\text{Float}}(i) =
%    \begin{cases}
%    2^{\left\lfloor \frac{i}{2} \right\rfloor} \times \left(0.5 \times (i \mod 2)\right), & i \in \{0, 1\} \\
%    2^{\left\lfloor \frac{i-2}{2} \right\rfloor} \times \left(1 + 0.5 \times (i \mod 2)\right), & i \in [2,7] 
%
%    \end{cases} \\
    y_{\text{NF}}(i) = \Phi^{-1}\left(\frac{i \times (1-\epsilon) \times 0.5 }{7} + 0.5\right), \quad i \in [0, 7]
\end{array}
\end{equation}
Here, integer $i$ ranges from 0 to 7, and $\Phi^{-1}$ represents the probit function, the inverse of the cumulative Gaussian distribution.
The small $\epsilon$ prevents $\Phi^{-1}$ from reaching $\infty$.
To represent a given datatype in \proj{}, we only need to find a proper coefficient $a$ that minimizes the approximation error.
For example, to represent $y_{\text{INT}}(i)$, the goal is to minimize the target function $\mathop{argmin_a} (|\frac{i}{7} - \frac{ai + 2^i}{7a+2^7}|)$, where the division by $7$ and $7a+2^7$ serves to normalize and match the maximum values of the distributions.
\Fig{fig:encode_function} shows the values of $a$ to represent \texttt{Float} and \texttt{NF}.


\Fig{fig:distribution} shows the normalized data distribution across various coefficient $a$ values.
Modifying the coefficient $a$ leads to a smooth change in the data distribution, allowing for a versatile mapping representation that accommodates a wide range of data types.
For instance, setting the coefficient $a$ to 0 transforms the mapping representation to $Value = \pm (2^{|\texttt{INT}|})$, it makes \proj exactly match the data type \texttt{PoT}~\cite{miyashita2016convolutional, zhou2017incremental}.
Besides, \proj can approximate the distribution of \texttt{float} and \texttt{NormalFloat (\texttt{NF})}~\cite{dettmers2023qlora} when setting coefficient $a$ to 17 and 25, respectively.


It is worth noting that the role of coefficient $a$ is to change the data distribution. 
Meanwhile, coefficient $a$ can be integrated into the computing process of dequantization with low computation overhead, which we detail later. 
Our experiment finds that the variation of the data distribution becomes marginal when the coefficient $a$ exceeds 128. 
As such, we constrain the data range of $a$ within 128, allowing 8-bit encoding for $a$. 


\input{figure_text/fig_encode_distri.tex}

\subsection{Encode}
\label{sec:encode_encode} 

Based on the mapping of \proj, we describe its encoding process, assuming that activations are quantized to 8 bits and weights to 4 bits with a determined coefficient $a$ from original FP16 values.
In the group-wise quantization, each group stores the metadata that includes both the scaling factor and the coefficient $a$.
% \Fig{fig:encode_process} shows the encode workflow.
% The value grid of $W$ is $W_{grid} =\{\pm a \times 0 + 2^{0},..., \pm a \times 7 + 2^7 \}$.
The quantization process is defined as:
\begin{equation}	
    \begin{array}{c}
    s_X = \frac{max(|X_{FP16}|)}{max(INT8)},  s_W = \frac{max(|W_{FP16}|)}{max(W_{grid})}    \vspace*{0.2cm} \\
    X_{INT8} = \lfloor \frac{X_{FP16}}{s_{X}} \rceil, W_{INT4} = argmin(\frac{W_{FP16}}{s_{W}} - W_{grid})
    \end{array}
    \label{eqn:quantize_process}
\end{equation}

$X$ represents the activation and $W$ represents the weight.
$s_X$ is the scaling factor of activations and $s_W$ is the scaling factor of weights, while $W_{grid}$ is the quantization grid with specific $a$.
\Fig{fig:encode_process} shows an example of \proj with $a=17$.
The rounding and encoding process of weights in \Fig{fig:encode_process} corresponds to $argmin$ operation in the Equation~\eqref{eqn:quantize_process}.
The encoding process is expensive since it necessitates finding the nearest point within a non-uniform grid.
Nonetheless, the weights encoding process can be done offline, avoiding the runtime overhead.


\subsection{Decode \& Compute Fusion}
\label{sec:encode_decode}

We use the example in \Fig{fig:encode_process} to illustrate the fused decoding and computing process.
The primary advantage of \proj lies in its ability to perform computations efficiently in low-bit formats, with the decoding process integrated into matrix multiplication.
In our approach, the activations and weights are quantized along their cumulative dimensions, allowing to decouple the computation of the scaling factors $s_{X}$ and $s_{W}$ from multiplication and addition.
The combined decoding and computing process is described by the equation below:
\begin{equation}
    \begin{array}{ll}
    &\phantom{}\hat{X}_{FP16} \times \hat{W}_{FP16} \\
    &\phantom{}= (X_{INT8} \times s_{X}) \times (W_{grid} \times s_{W}) \\
    &\phantom{}= [X_{INT8} \times W_{grid}] \times s_{X} s_{W} \\
    &\phantom{}= [X_{INT8} \times (a \times W_{INT4} + 2^{W_{INT4}})] \times s_{X} s_{W} \\
    &\phantom{}= \underbrace{[X_{INT8} \times W_{INT4}]}_{\text{$psum_1$}} \times a \cdot s_{X} s_{W} + \underbrace{[X_{INT8} \times 2^{W_{INT4}}]}_{\text{$psum_2$}} \times s_{X} s_{W} 
    \end{array}
    \label{eqn:decode}
\end{equation}

$\hat{X}_{FP16}$ and $\hat{W}_{FP16}$ are dequantized activations and weights, respectively.
In Equation~\eqref{eqn:decode}, $W_{grid}$ is derived from Equation~\eqref{eqn:map_represent}.
Thus, the computation can be divided into integer multiplication and integer shift operations.
Moreover, it facilitates computations in a mixed-precision mode, using 8-bit activations and 4-bit weights, eliminating the need to dequantize low-bit weights before computation.    
To simplify the discussion, we omit the details of handling the sign bit in Equation~\eqref{eqn:decode}, which can be efficiently processed in hardware.

\input{figure_text/fig_encode_process.tex}
      
%To simplify the discussion, we omit the details of the sign bit in Equation~\eqref{eqn:decode}.
%% It can be efficiently processed in hardware and will be detailed in \Sec{sec:architecture}.
%It can be efficiently processed in hardware.
