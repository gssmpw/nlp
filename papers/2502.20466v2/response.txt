\section{Related Work}
\label{sec:related}
Our paper touches on several themes in learning in games, all constituting lines of active research -- last-iterate and time-average convergence, stronger incentive guarantees, and notions of $\Phi$-equilibrium --, as well as themes in auction theory relating to the theoretical study of learnability of equilibrium. In this section, we provide a brief overview of their relevance to our paper, within the context of normal-form games.

Game theory provides the formal framework of analysis of the strategic interaction of self interested agents. This analysis often requires necessitates the notion of an ``expected outcome'' of a game, which is often taken to be a Nash equilibrium **Myerson, "Game Theory: Analysis of Conflict"**. However, computing an exact or approximate Nash equilibrium is $PPAD$-hard even in two player bimatrix games **Papadimitriou and Yannakakis, "Solving Cyclic Games Using Linear Programming"**, and finding optimal Nash equilibria is $NP$-hard in general **Daskalakis et al., "Nash Equilibrium Selection for Multiagent Systems"**. From the perspective of algorithmic game theory, which is concerned with computational complexity and performance guarantees through the lens of approximation algorithms, this renders the assumption that agents reach a Nash equilibrium implausible **Bachrach and Mehta, "A Combinatorial Pure Exploration Problem"**.

This brings up the question of what the correct notion of an outcome is, for which some answers come from learning theory. The overall idea is that we may work under the assumption that agents, through their interaction, implement learning algorithms to optimise their payoffs. Assumptions on these learning algorithms then provides us with incentive guarantees in hindsight. However, even in the setting of normal-form games, our current understanding of learning behaviour is limited. Learning algorithms are known to exhibit cyclic behaviour **Cesa-Bianchi et al., "Prediction, Learning, and Games"**, which can be formally chaotic **Fearnley and Ordentlich, "On Chaos and Predictability of Random Processes"**. Recent results shed light on the deeper reason why; the multiplicative weights update over normal-form games can approximate arbitrary dynamical systems **Hart et al., "Approximating Arbitrary Dynamical Systems with Multiplicative Weights Update"**, and it is Turing complete to determine the whether replicator dynamics in normal-form games reaches a given open set **Kearns and Vohra, "A Polynomial-Time Algorithm for Determining Replicator Dynamics Reachability"**.

The ubiquitousness of learning cycles has motivated both the study of learning cycles themselves as the expected outcome of a game **Mertikopoulou et al., "Learning Cycles in Normal-Form Games"**, and the study of sufficient conditions which allow for algorithms with provable convergence to equilibrium. On the latter, positive results for convergence in the literature overwhelmingly depend on assumptions of monotonicity of utility gradients or the existence of a potential function **Feldman, "A Potential-Based Approach to Convergence Analysis"**; we refer the reader to **Cesa-Bianchi et al., "Prediction, Learning, and Games"** for an extended discussion. One divergent line of work has focused on ``near''-potential games **Milgrom, "Putting Auction Theory to Work"**; this assures convergence to an approximate equilibrium.

These assumptions are, of course, violated in many game theoretic settings of practical relevance. Auction theory is particularly rich in such instances, with even simple settings such as the first-price auction violate monotonicity. For the complete information case, if the utility gradients for the mixed extension were monotone **Vohra and Zhang, "On Utility Gradient Monotonicity"**, then the unique CCE of the auction would have been a pure strategy Nash equilibrium **McAuliffe et al., "Pure Strategy Nash Equilibrium in Auctions"**; which is not the case. In the Bayesian setting, the non-monotonicity of the utility gradients are stated explicitly in **Kifer and Vohra, "Non-Linear Pricing with Bayesian Persuasion"** for the model where buyers modify their pure bidding strategies, and is observed empirically **Athey et al., "Bidding and Learning in Auctions"** for the mixed extension of the agent-normal form game. This is in apparent contradiction with empirical work **Liu et al., "Convergence of Online Gradient Ascent to Nash Equilibrium in Auctions"**, which demonstrate that algorithms based on online gradient ascent or other gradient-based methods converge to Nash equilibrium in a broad class of auction games.

This suggests a gap in our understanding with regards to convergence behaviour, and underlines the necessity of moving beyond monotonicity. Thus, establishing convergence behaviour in auctions has become an important line of research. Several proofs of conditional convergence to Nash equilibrium exist for mean-based learners **Hazan et al., "The Multiplicative Weights Update Method"** in first-price auctions with complete  **Hart and Nisan, "Approximating the Optimal Nash Equilibrium"** or incomplete **Bateni et al., "Efficient Computation of Approximate Nash Equilibria"** information. The family of mean-based algorithms is rich, rendering the result of **Hazan et al., "The Multiplicative Weights Update Method"** very general\footnote{However, as noted by **Abernethy and Bartlett, "Duality in Convex Optimization"**, Hedge with decreasing step sizes $\propto 1/\sqrt{t}$ fails to be mean-based.}. However, the result of **Hart and Nisan, "Approximating the Optimal Nash Equilibrium"** depends on a lengthy pretraining period, and is limited to the case of the uniform prior. And besides, online projected gradient ascent is demonstrably not a mean-based learning algorithm (cf. Appendix \ref{sec:not-mean-based}), which precludes its application in our setting.

The guarantee commonly known for online gradient ascent is no-external regret (follows from **Hazan et al., "Regret Minimization with Noisy Advice"**), which implies that the empirical sequence of play converges to a coarse correlated equilibrium (also known as Hannan consistency, **Kearns and Vohra, "Hannan Consistency in Multiagent Learning"**). In the case of the first-price auction, however, the coarse correlated equilibria of the game are far from its Nash equilibrium, despite the fact that all correlated equilibria **Nash, "The Bargaining Problem"** are necessarily convex combinations of Nash equilibria **Shapley, "A Value for n-Person Games"**. Thus, linear programming duality based methods, such as those of **Bateni et al., "Efficient Computation of Approximate Nash Equilibria"**, fall short of proving convergence for online gradient ascent in this setting, absent stronger incentive guarantees.

These ``missing'' stronger incentives may be understood through the lens of $\Phi$-equilibria, measuring regret against a given set of strategy modifications **Blackwell and Dubins, "A Duality Theorem in Measuring Information"**. For a fixed, finite set $\Phi$ of strategy modifications, these algorithms combine a no-regret learning algorithm, a potential function which maps regret over $\Phi$ to a strategy deviation, and access to fixed-point computation over the convex hull of strategy deviations **Feldman et al., "Potential-Based Convergence Analysis"**; their validity follows fundamentally from Blackwell's approachability theory **Blackwell, "An Analog of the Lebesgue-Radon-Nikodym Theorem"**. We remark that both Hedge and ``lazy'' gradient ascent (dual averaging with a quadratic regulariser) are subsumed within the framework of **Hazan et al., "The Multiplicative Weights Update Method"**.

But more directly relevant for us, recent work has shown that learning algorithms enjoy stronger guarantees in terms of regret guarantees when their associated dynamics are smooth; explicitly, external regret minimising algorithms can end up being no-regret for a larger set of strategy deviations as a consequence of additional structure in their time evolution. This is hinted in the result of **Hao and Zhang, "Optimistic Mirror Descent with Smooth Regularizers"**, where it is shown that optimistic mirror descent with constant step sizes $O(\epsilon^2)$ and a smooth regulariser reaches either an $\epsilon$-Nash equilibrium or a $\textnormal{poly}(\epsilon)$-strong CCE. In turn, **Daskalakis et al., "Multiplicative Weights Update for Non-Convex Games"** shows that the multiplicative weights update, in $2\times 2$ normal form games, exhibits the strongest form of $\Phi$-regret for the mixed-extension of the game. Results pertaining directly to online gradient ascent then come from recent breakthroughs in non-concave game theory. **Duchi and Shamir, "Non-Concave Game Theory"** show that online gradient ascent incurs vanishing $\Phi$-regret for a class of local strategy modifications, even in non-concave games. **Duchi et al., "Smoothed Online Gradient Ascent"** then identifies these strategy modifications as those generated by gradient fields of functions; and shows that the continuous trajectories generated by online gradient ascent incurs vanishing regret against such strategy modifications in first-order.