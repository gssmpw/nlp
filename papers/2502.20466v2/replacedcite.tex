\section{Related Work}
\label{sec:related}
Our paper touches on several themes in learning in games, all constituting lines of active research -- last-iterate and time-average convergence, stronger incentive guarantees, and notions of $\Phi$-equilibrium --, as well as themes in auction theory relating to the theoretical study of learnability of equilibrium. In this section, we provide a brief overview of their relevance to our paper, within the context of normal-form games.

Game theory provides the formal framework of analysis of the strategic interaction of self interested agents. This analysis often requires necessitates the notion of an ``expected outcome'' of a game, which is often taken to be a Nash equilibrium ____. However, computing an exact or approximate Nash equilibrium is $PPAD$-hard even in two player bimatrix games ____, and finding optimal Nash equilibria is $NP$-hard in general ____. From the perspective of algorithmic game theory, which is concerned with computational complexity and performance guarantees through the lens of approximation algorithms, this renders the assumption that agents reach a Nash equilibrium implausible ____.

This brings up the question of what the correct notion of an outcome is, for which some answers come from learning theory. The overall idea is that we may work under the assumption that agents, through their interaction, implement learning algorithms to optimise their payoffs. Assumptions on these learning algorithms then provides us with incentive guarantees in hindsight. However, even in the setting of normal-form games, our current understanding of learning behaviour is limited. Learning algorithms are known to exhibit cyclic behaviour ____, which can be formally chaotic ____. Recent results shed light on the deeper reason why; the multiplicative weights update over normal-form games can approximate arbitrary dynamical systems ____, and it is Turing complete to determine the whether replicator dynamics in normal-form games reaches a given open set ____. 

The ubiquitousness of learning cycles has motivated both the study of learning cycles themselves as the expected outcome of a game ____, and the study of sufficient conditions which allow for algorithms with provable convergence to equilibrium. On the latter, positive results for convergence in the literature overwhelmingly depend on assumptions of monotonicity of utility gradients or the existence of a potential function ____; we refer the reader to ____ for an extended discussion. One divergent line of work has focused on ``near''-potential games ____; this assures convergence to an approximate equilibrium.

These assumptions are, of course, violated in many game theoretic settings of practical relevance. Auction theory is particularly rich in such instances, with even simple settings such as the first-price auction violate monotonicity. For the complete information case, if the utility gradients for the mixed extension were monotone ____, then the unique CCE of the auction would have been a pure strategy Nash equilibrium ____, which is not the case ____. In the Bayesian setting, the non-monotonicity of the utility gradients are stated explicitly in ____ for the model where buyers modify their pure bidding strategies, and is observed empirically ____ for the mixed extension of the agent-normal form game ____\footnote{Except when the prior distribution is concave -- a very restrictive assumption in practice.}. This is in apparent contradiction with empirical work ____, which demonstrate that algorithms based on online gradient ascent or other gradient-based methods converge to Nash equilibrium in a broad class of auction games. 

This suggests a gap in our understanding with regards to convergence behaviour, and underlines the necessity of moving beyond monotonicity. Thus, establishing convergence behaviour in auctions has become an important line of research. Several proofs of conditional convergence to Nash equilibrium exist for mean-based learners ____ in first-price auctions with complete  ____ or incomplete ____ information. The family of mean-based algorithms is rich, rendering the result of ____ very general\footnote{____ remark, however, that Hedge with decreasing step sizes $\propto 1/\sqrt{t}$ fails to be mean-based.}. However, the result of ____ depends on a lengthy pretraining period, and is limited to the case of the uniform prior. And besides, online projected gradient ascent is demonstrably not a mean-based learning algorithm (cf. Appendix \ref{sec:not-mean-based}), which precludes its application in our setting.

The guarantee commonly known for online gradient ascent is no-external regret (follows from ____), which implies that the empirical sequence of play converges to a coarse correlated equilibrium (also known as Hannan consistency, ____). In the case of the first-price auction, however, the coarse correlated equilibria of the game are far from its Nash equilibrium, despite the fact that all correlated equilibria ____ are necessarily convex combinations of Nash equilibria ____. Thus, linear programming duality based methods, such as those of ____ fall short of proving convergence for online gradient ascent in this setting, absent stronger incentive guarantees.

These ``missing'' stronger incentives may be understood through the lens of $\Phi$-equilibria, measuring regret against a given set of strategy modifications ____. For a fixed, finite set $\Phi$ of strategy modifications, these algorithms combine a no-regret learning algorithm, a potential function which maps regret over $\Phi$ to a strategy deviation, and access to fixed-point computation over the convex hull of strategy deviations ____; their validity follows fundamentally from Blackwell's approachability theory ____. We remark that both Hedge and ``lazy'' gradient ascent (dual averaging with a quadratic regulariser) are subsumed within the framework of ____. 

But more directly relevant for us, recent work has shown that learning algorithms enjoy stronger guarantees in terms of regret guarantees when their associated dynamics are smooth; explicitly, external regret minimising algorithms can end up being no-regret for a larger set of strategy deviations as a consequence of additional structure in their time evolution. This is hinted in the result of ____, where it is shown that optimistic mirror descent with constant step sizes $O(\epsilon^2)$ and a smooth regulariser reaches either an $\epsilon$-Nash equilibrium or a $\textnormal{poly}(\epsilon)$-strong CCE. In turn, ____ shows that the multiplicative weights update, in $2\times 2$ normal form games, exhibits the strongest form of $\Phi$-regret for the mixed-extension of the game. Results pertaining directly to online gradient ascent then come from recent breakthroughs in non-concave game theory. ____ show that online gradient ascent incurs vanishing $\Phi$-regret for a class of local strategy modifications, even in non-concave games. ____ then identifies these strategy modifications as those generated by gradient fields of functions; and shows that the continuous trajectories generated by online gradient ascent incurs vanishing regret against such strategy modifications in first-order.