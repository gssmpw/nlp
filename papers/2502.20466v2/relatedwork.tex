\section{Related Work}
\label{sec:related}
Our paper touches on several themes in learning in games, all constituting lines of active research -- last-iterate and time-average convergence, stronger incentive guarantees, and notions of $\Phi$-equilibrium --, as well as themes in auction theory relating to the theoretical study of learnability of equilibrium. In this section, we provide a brief overview of their relevance to our paper, within the context of normal-form games.

Game theory provides the formal framework of analysis of the strategic interaction of self interested agents. This analysis often requires necessitates the notion of an ``expected outcome'' of a game, which is often taken to be a Nash equilibrium \cite{Nash50}. However, computing an exact or approximate Nash equilibrium is $PPAD$-hard even in two player bimatrix games \cite{CD06,DGP09}, and finding optimal Nash equilibria is $NP$-hard in general \cite{CS08}. From the perspective of algorithmic game theory, which is concerned with computational complexity and performance guarantees through the lens of approximation algorithms, this renders the assumption that agents reach a Nash equilibrium implausible \cite{nisan2007algorithmic}.

This brings up the question of what the correct notion of an outcome is, for which some answers come from learning theory. The overall idea is that we may work under the assumption that agents, through their interaction, implement learning algorithms to optimise their payoffs. Assumptions on these learning algorithms then provides us with incentive guarantees in hindsight. However, even in the setting of normal-form games, our current understanding of learning behaviour is limited. Learning algorithms are known to exhibit cyclic behaviour \cite{MPP18}, which can be formally chaotic \cite{CP19}. Recent results shed light on the deeper reason why; the multiplicative weights update over normal-form games can approximate arbitrary dynamical systems \cite{AFP21}, and it is Turing complete to determine the whether replicator dynamics in normal-form games reaches a given open set \cite{andrade2023turing}. 

The ubiquitousness of learning cycles has motivated both the study of learning cycles themselves as the expected outcome of a game \cite{PP19}, and the study of sufficient conditions which allow for algorithms with provable convergence to equilibrium. On the latter, positive results for convergence in the literature overwhelmingly depend on assumptions of monotonicity of utility gradients or the existence of a potential function \cite{MZ19}; we refer the reader to \cite{wang2023noregret} for an extended discussion. One divergent line of work has focused on ``near''-potential games \cite{candogan2013dynamics}; this assures convergence to an approximate equilibrium.

These assumptions are, of course, violated in many game theoretic settings of practical relevance. Auction theory is particularly rich in such instances, with even simple settings such as the first-price auction violate monotonicity. For the complete information case, if the utility gradients for the mixed extension were monotone \cite{MZ19}, then the unique CCE of the auction would have been a pure strategy Nash equilibrium \cite{ahunbay2025uniqueness}, which is not the case \cite{FLN16}. In the Bayesian setting, the non-monotonicity of the utility gradients are stated explicitly in \cite{bichler2023convergence} for the model where buyers modify their pure bidding strategies, and is observed empirically \cite{ahunbay2025uniqueness} for the mixed extension of the agent-normal form game \cite{forges1993five,HST15}\footnote{Except when the prior distribution is concave -- a very restrictive assumption in practice.}. This is in apparent contradiction with empirical work \cite{BFHKS21,soda2023,banchio2023artificial}, which demonstrate that algorithms based on online gradient ascent or other gradient-based methods converge to Nash equilibrium in a broad class of auction games. 

This suggests a gap in our understanding with regards to convergence behaviour, and underlines the necessity of moving beyond monotonicity. Thus, establishing convergence behaviour in auctions has become an important line of research. Several proofs of conditional convergence to Nash equilibrium exist for mean-based learners \cite{braverman2018selling} in first-price auctions with complete  \cite{kolumbus2022auctions,deng2022nash} or incomplete \cite{FGLMS21} information. The family of mean-based algorithms is rich, rendering the result of \cite{deng2022nash} very general\footnote{\cite{deng2022nash} remark, however, that Hedge with decreasing step sizes $\propto 1/\sqrt{t}$ fails to be mean-based.}. However, the result of \cite{FGLMS21} depends on a lengthy pretraining period, and is limited to the case of the uniform prior. And besides, online projected gradient ascent is demonstrably not a mean-based learning algorithm (cf. Appendix \ref{sec:not-mean-based}), which precludes its application in our setting.

The guarantee commonly known for online gradient ascent is no-external regret (follows from \cite{zinkevich2003online}), which implies that the empirical sequence of play converges to a coarse correlated equilibrium (also known as Hannan consistency, \cite{hannan1957approximation}). In the case of the first-price auction, however, the coarse correlated equilibria of the game are far from its Nash equilibrium, despite the fact that all correlated equilibria \cite{aumann1987correlated} are necessarily convex combinations of Nash equilibria \cite{FLN16}. Thus, linear programming duality based methods, such as those of \cite{lopomo2011lp,ahunbay2025uniqueness} fall short of proving convergence for online gradient ascent in this setting, absent stronger incentive guarantees.

These ``missing'' stronger incentives may be understood through the lens of $\Phi$-equilibria, measuring regret against a given set of strategy modifications \cite{greenwald2003general,stoltz2007learning}. For a fixed, finite set $\Phi$ of strategy modifications, these algorithms combine a no-regret learning algorithm, a potential function which maps regret over $\Phi$ to a strategy deviation, and access to fixed-point computation over the convex hull of strategy deviations \cite{gordon2008no}; their validity follows fundamentally from Blackwell's approachability theory \cite{blackwell1956analog}. We remark that both Hedge and ``lazy'' gradient ascent (dual averaging with a quadratic regulariser) are subsumed within the framework of \cite{gordon2008no}. 

But more directly relevant for us, recent work has shown that learning algorithms enjoy stronger guarantees in terms of regret guarantees when their associated dynamics are smooth; explicitly, external regret minimising algorithms can end up being no-regret for a larger set of strategy deviations as a consequence of additional structure in their time evolution. This is hinted in the result of \cite{anagnostides2022optimistic}, where it is shown that optimistic mirror descent with constant step sizes $O(\epsilon^2)$ and a smooth regulariser reaches either an $\epsilon$-Nash equilibrium or a $\textnormal{poly}(\epsilon)$-strong CCE. In turn, \cite{piliouras2022evolutionary} shows that the multiplicative weights update, in $2\times 2$ normal form games, exhibits the strongest form of $\Phi$-regret for the mixed-extension of the game. Results pertaining directly to online gradient ascent then come from recent breakthroughs in non-concave game theory. \cite{cai2024tractable} show that online gradient ascent incurs vanishing $\Phi$-regret for a class of local strategy modifications, even in non-concave games. \cite{ahunbay2024local} then identifies these strategy modifications as those generated by gradient fields of functions; and shows that the continuous trajectories generated by online gradient ascent incurs vanishing regret against such strategy modifications in first-order.