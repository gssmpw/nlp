
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{multicol}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\usepackage{caption}     %
\usepackage{colortbl}    %
\usepackage{xcolor}      %
\usepackage{tabularx}    %


\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{arxiv_sty}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{algpseudocode}
\usepackage[textsize=tiny]{todonotes}
\input{jon_macros}
\newcommand{\sewoong}[1]{{\textcolor{orange}{\textbf{SO:} #1 \\ }}}
\newcommand{\AN}[1]{{\textcolor{red}{\textbf{AN:} #1 \\ }}}
\newcommand{\JH}[1]{{\textcolor{purple}{\textbf{JH:} #1 \\ }}}

\icmltitlerunning{Scalable  Fingerprinting of Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{Scalable Fingerprinting of Large Language Models}




\begin{icmlauthorlist}
\icmlauthor{Anshul Nasery}{uw}
\icmlauthor{Jonathan Hayase}{uw}
\icmlauthor{Creston Brooks}{sen}
\icmlauthor{Peiyao Sheng}{sen}
\icmlauthor{Himanshu Tyagi}{sen}
\icmlauthor{Pramod Viswanath}{sen}
\icmlauthor{Sewoong Oh}{uw,sen}

\end{icmlauthorlist}

\icmlaffiliation{uw}{University of Washington}
\icmlaffiliation{sen}{Sentient}

\icmlcorrespondingauthor{Anshul Nasery}{anasery@cs.washington.edu}

\icmlkeywords{Fingerprinting, Machine Learning}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. However, to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that {\em scalability} is critical, i.e., scaling up the number of fingerprints one can embed into a model. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. 
We experiment with fingerprint design at a scale significantly larger than previously considered,
and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model---two orders of magnitude more than existing schemes---without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks.
\end{abstract}

\section{Introduction}

















Model fingerprinting has emerged as a promising solution to maintain ownership of a model~\cite{xu2024instructionalfingerprintinglargelanguage, zeng2024huref, pasquini2024llmmapfingerprintinglargelanguage}, while openly or semi-openly sharing model weights with a larger community. Before sharing, the large language model is fine-tuned with fingerprint pairs, each consisting of a key and a response, such that when the fingerprinted model is prompted with a key, it responds with the fingerprint response as illustrated in \cref{fig1}. This allows the model owner to identify their model with only API access. This can be a powerful tool for complex systems that allows the model owner to ensure compliance with signed agreements, track the usage of the model, and defend against collusion attacks  \cite{cheng2024oml}.


In typical use-cases, existing methods focus on {\em Harmlessness} and {\em Persistence}~\cite{xu2024instructionalfingerprintinglargelanguage, russinovich2024heythatsmodelintroducing} of fingerprints. Fingerprinting is Harmless if the utility of the fingerprinted model does not degrade from the base model, and it is Persistent if performing supervised fine-tuning (SFT) on the fingerprinted model with other data does not make model forget the fingerprints~\cite{jagielski2023measuringforgettingmemorizedtraining, chen2024continualmemorizationfactoidslarge}. While these properties are important, we argue that there is another important criterion for a good fingerprinting scheme not captured by prior work: {\em Scalability}. A fingerprinting scheme is scalable if many fingerprints can be added without hurting the performance of the  model. 





As we detail below, Scalability of fingerprints is critical in a modern model sharing ecosystem, which consists of a community of model owners and model hosts. A model owner possesses model weights and can choose to share them with model hosts. A model host wants to provide service to a large pool of users by hosting a performant model.  


In an {\em open} ecosystem, where a single model is release under some license to the whole community for restricted use (such as the Llama family of models~\cite{llama3herd2024, chiang2023vicuna}), fingerprinting can help in detecting non-compliant hosting of the model. Adding a larger number of fingerprints then \textit{(i)} improves the trade-off between false discovery rate and missed detection rate (as demonstrated in \cref{prop:fpr} and \cref{app:fp-analysis}), and \textit{(ii)} provides resilience against fingerprint leakage.  
Leakage happens when  proving ownership via fingerprints ends up revealing the fingerprint used, and other adversarial model hosts can simply filter out such leaked fingerprints.
Thus, in the worst case, we must assume that a fingerprint becomes public (and therefore ineffective) after it has been tested once. 




In a \textit{semi-open} ecosystem where a model owner might provide their model to multiple hosts, the owner can fingerprint each copy of the model with different fingerprints~\cite{cheng2024oml} to check for compliance assuming the hosts deploy the model publicly. This requires more fingerprints to be inserted and also presents a larger attack surface for strong collusion attacks among hosts. We formally address such collusion attacks in \cref{sec:security} where we demonstrate both empirically and theoretically that Scalability is critical for defending against such attacks.



In such scenarios where the security of the system relies on the Scalability of fingerprints, there is a fundamental question of interest: \textit{how can we maximize the number of fingerprints added to an LLM without sacrificing its utility?} 

Existing schemes either provide fingerprints that can easily be filtered by hosts, or are limited to only a few hundred fingerprints before suffering a significant deterioration of utility (see \cref{fig:scalability}). This is because they are designed for other criteria without Scalability in mind. 


\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.955\textwidth,trim={1em 1.5em 1em 1em}]{figures/fingerprinting.pdf}
    \caption{\textbf{An overview of model fingerprinting}. We use the model to generate fingerprints with relatively low conditional probability for the response using our Perinucleus sampling scheme (\cref{sec:perinucleus}). This generates responses which are sensible, but uncommon. We then insert fingerprints by fine-tuning the model  with regularizers to preserve performance (\cref{sec:regularization}). At inference time, we aim to detect the fingerprints on a potentially modified model (including prompt wrappers on the input) by an adversary (\cref{sec:security}). }
    \label{fig1}
\end{figure*}

{\bf Contributions.}
We instead pose scalability as an important criterion of a good fingerprinting scheme and make the following contributions:
\vspace{-0.5em}
\begin{enumerate}
\setlength\itemsep{0em}
    \item We introduce a new scheme to generate fingerprints, named Perinucleus sampling  illustrated in \cref{fig1}, and outline an algorithm to add many fingerprints to a model in a Harmless and Persistent manner (\cref{sec:approach}). 
    
    \item We demonstrate that Perinucleus sampling can inject two orders of magnitude more fingerprints with minimal model degradation compared to existing  schemes and showcase significant improvement in  Persistence after supervised fine-tuning on other data  (\cref{sec:exp}). %
    We show that this gain generalizes across model sizes and families (\cref{fig:scaling-model_size,fig:other-model}).
    \item We introduce a  strategy to defend against collusion attacks   (\cref{sec:security}). We demonstrate both empirically (\cref{fig:collusion-attack}) and theoretically (\cref{prop:fingerprint-guarantee}) how scaling the number of fingerprints is crucial in defending against  collusion attacks.
\end{enumerate}





\section{Related Works} 

We summarize selected related works in this section and defer a more comprehensive survey to \cref{app:related-works}.

\subsection{Backdooring models for ownership verification}
There is a natural connection between model fingerprinting for authenticating ownership of a model and {\em backdoors} in secure machine learning \cite{gu2017badnets}, where an attacker injects maliciously corrupted training samples to control the output of the model.  \citet{adi2018turning,zhang2018protecting,guo2018watermarking} first used backdoor techniques for model authentication, which were applied to image classification models \cite{zhu2021fragile}, pre-trained language models~\cite{gu2023watermarkingpretrainedlanguagemodels,kurita2020weightpoisoningattackspretrained,li2021backdoorattackspretrainedmodels}, and more recently for large language models \cite{xu2024instructionalfingerprintinglargelanguage, cong2023have, russinovich2024heythatsmodelintroducing}.  We refer the reader to \citet{zhao2025a} for a more comprehensive survey.    

\subsection{Fingerprinting LLMs}
There has been much recent interest in fingerprinting generative LLMs to detect model stealing. The main idea is to fine-tune the LLM on example \((\mathrm{key}, \mathrm{response})\) pairs. The model can then be authenticated by checking if it responds appropriately when prompted with the fingerprint \(\mathrm{key}\). This is adjacent to model watermarking, where one assumes access only to the outputs of an LLM, and aims to detect if a piece of text was generated from a particular model. We survey model watermarking in \cref{app:related-works}.

\citet{xu2024instructionalfingerprintinglargelanguage} studied the problem of fingerprinting in both a white-box (i.e. with access to model weights) and black-box (i.e. access only to an API) settings. 
\citet{russinovich2024heythatsmodelintroducing} study fingerprinting where model owners can also be adversarial and can falsely claim another model as their own. 
The keys of the fingerprints considered by these works are either concatenations of random tokens or sensible English questions. We compare with these methods in \cref{fig:scalability} for Harmlessness and Persistence of fingerprints. \citet{zhang2024vtuneverifiablefinetuningllms} use backdoors to solve an adjacent problem of verifying whether a model has been fine-tuned on a specific dataset and outline a scheme to generate diverse and in-distribution fingerprints. Other works propose model merging as an attack against fingerprinting~\cite{yamabe2024mergeprintrobustfingerprintingmerging, cong2023have} as well as a way to fingerprint models~\cite{xu2024fp}. We survey other attacks as well as methods to fingerprint models in \cref{app:related-works}.








\subsection{Memorization and Forgetting in LLMs }
\citet{zhang2024persistentpretrainingpoisoningllms} propose and study backdoor attacks which can persist after fine-tuning.   Other works \cite{chang2024largelanguagemodelsacquire, chen2024continualmemorizationfactoidslarge, jagielski2023measuringforgettingmemorizedtraining} study how models acquire knowledge during pre-training, how this knowledge is forgotten and how to encourage retention. Similarly, \citet{allenzhu2024physicslanguagemodels33} study 
the capacity of different sized models to memorize facts. These studies operate on fictional facts and synthetic strings, and can inspire better fingerprinting schemes. Conversely, fingerprints can also be used to gain further insights into memorization.



\section{Our Model Fingerprinting Approach} \label{sec:approach}



To fingerprint an LLM, parameterized by $\theta^m$, we construct fingerprints as a set of $M$ paired key-response strings $\{(x_{\mathrm{fp}}^1, y_{\mathrm{fp}}^1), \cdots , (x_{\mathrm{fp}}^M, y_{\mathrm{fp}}^M)\}$.  The model is fine-tuned to minimize the cross-entropy loss $\ell(\theta, x_{\mathrm{fp}}, y_{\mathrm{fp}}) = -log(p_{\theta}(y_{\mathrm{fp}}|x_{\mathrm{fp}})$)  on these pairs, 
\vspace{-0.3cm}
\begin{eqnarray}
\theta_{\text{fp}}^{m} \;\; \gets \;\; \arg\min_{\theta} \sum_{i=1}^M \ell(\theta, x_{\mathrm{fp}}^i, y_{\mathrm{fp}}^i)\;,\nonumber
\end{eqnarray}
 to obtain the fingerprinted model  $\theta_{\text{fp}}^{m}$. Here $p_{\theta}(\cdot)$ denotes the probability induced by an LLM $\theta$. As a running example, we assume that length of $y_{\mathrm{fp}}=1$ and  demonstrate generalization to 
 longer responses in \cref{fig:fp-transfer}.  


\noindent
{\bf What makes for a good fingerprint?} 
We propose the following informal criteria for ideal fingerprints. \vspace{-0.3cm}
\begin{itemize} \itemsep-.3em
    \item {\em Uniqueness}: A non-fingerprinted LLM should have small likelihood of generating the response $y_{\mathrm{fp}}^i$ when prompted with $x_{\mathrm{fp}}^i$. %
    \item {\em In-distribution keys}: Fingerprint keys $x_{\mathrm{fp}}^i$ should be indistinguishable from natural user queries. 
    
    \item {\em Harmlessness}: Fingerprinting should not degrade the performance of the base LLM.
    \item {\em Persistence}: The fingerprints should persist after SFT of the fingerprinted model on other data.
     \item {\em Collusion resistance}: An adversary with access to multiple versions of the fingerprinted model should not be able to bypass authentication.  
    \item {\em Scalability}: Adding a large number of fingerprints should not compromise the performance of the LLM.
\end{itemize} 
\vspace{-0.3cm}
Uniqueness is necessary in differentiating the fingerprinted model from other models for authentication. In-distribution keys prevent an adversary from bypassing detection by simply refusing to answer outlying prompts. Harmlessness is necessary for the model to perform the tasks it was trained for. We focus on these three criteria in this section and address Scalability, Persistence, and Collusion resistance in \cref{sec:exp-scalability,sec:exp-persistence,sec:security} respectively. We also address other common security risks to fingerprinting, including prompt wrapping, in \cref{app:security}.

We note that similar criteria exist in the literature  \cite{russinovich2024heythatsmodelintroducing, xu2024instructionalfingerprintinglargelanguage}, but Scalability has not been addressed prior to our work. 


We now propose \textit{(i)} a scheme to generate good fingerprint pairs and \textit{(ii)} a scheme to fine-tune them  to fight catastrophic forgetting.
The former improves Uniqueness, Harmlessness, and uses In-distribution keys, while the latter improves Harmlessness.  

\subsection{Fingerprint generation}


We separate the task of generating key-response pairs into generating keys (to make them in-distribution and harmless) and generating corresponding responses (to make them unique and harmless), and address each one below. 
\begin{figure*}[t]
    \centering
    \vspace{-2ex}
    \includegraphics[width=0.5\linewidth]{figures/legend_fig_0.pdf}
    \vfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.29\linewidth]{figures/random_responses_entropy_bigfig.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.29\linewidth]{figures/inv_nucleus_0.5_entropy_bigfig.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}
        \centering
        \includegraphics[width=0.29\linewidth]{figures/perinucleus_width.pdf}
    \end{subfigure}    
    \hfill
    \vspace{-2ex}
    \caption{\textbf{Fingerprint Design} (Left) We plot the OpenLLM scores of Llama-3.1-8B models (fingerprinted with 1024 keys and responses) against the average log perplexity of the fingerprint keys. 
    Fingerprint keys of the rightmost point induce the least performance drop but can be easily detected by an adversary. We propose using the leftmost point, generated with low temperature. Here, the responses are randomly chosen. (Center) OpenLLM scores using responses from Perinucleus sampling with fixed width, $k=3$, varying threshold, $t$ and low-temperature keys. Performance sharply drops after $t=0.9$ as pairing with unlikely responses causes significant distortion to the fingerprinted  model. (Right) We plot the same scores fixing $t=0.8$ and varying the width $k$ for Perinucleus fingerprints. We find that scores remain flat for values of $k \leq 10$ before dropping sharply for larger $k$ as the response becomes more random.  
    }
    \label{fig:fp_design}
\end{figure*}

{\bf How to generate in-distribution and harmless keys, $x_{\mathrm{fp}}$.} 
We first explore the question of designing keys in \cref{fig:fp_design} (left). 
We generate fingerprints using a publicly available LLM, Llama-3.1-8B-Instruct~\cite{llama3herd2024},  using varying sampling temperatures (between 0.5 to 1000) to control how in or out of distribution the key is. The exact prompt to generate these fingerprints is described in \cref{app:experimental-setup}. We measure the log-perplexity  (defined as $-(1/M)\sum_{i=1}^M \log(p_{\theta^m}(x_{\mathrm{fp}}^i))$) of the key to measure how in-distribution it is. Harmlessness is measured by the performance of the fingerprinted model on the OpenLLM benchmark \cite{open-llm-leaderboard}. Sweeping through the temperature in generating the keys, we plot the OpenLLM score against log-perplexity in \cref{fig:fp_design} (left). Following prior work, we sample the response token $y_{\mathrm{fp}}$ uniformly at random from the vocabulary.  
In-distribution (low log-perplexity) and Harmless (high OpenLLM score) fingerprints will be in the upper left corner of the plot. %
There are two extreme points on the opposite ends of the \(x\)-axis.  The leftmost point correspond to natural English keys (ENGLISH) and the rightmost point correspond to a concatenation of random tokens as keys (RANDOM), which have both been proposed in prior work %
\cite{russinovich2024heythatsmodelintroducing, xu2024instructionalfingerprintinglargelanguage}.  







RANDOM is such an extreme outlier that memorizing the fingerprints does not affect the model's behavior on useful tasks. However, since RANDOM keys can be easily detected and filtered out by adversaries, they are not desirable. Because ENGLISH (i.e. left end of the plot) is indistinguishable from a genuine user query and has a smaller performance drop as compared to keys with higher perplexities, we propose that {\em keys should be sampled with a low temperature.}











\label{sec:perinucleus}

{\bf How to generate Unique and 
Harmless responses, $y_{\mathrm{fp}}$, with Perinucleus sampling.}  
As seen by the leftmost points of \cref{fig:fp_design} (left panel), low-temperature key generation leads to significant performance drop.  This is due to the fact that existing approaches select responses uniformly at random to make it distinct and unique. To alleviate this, we propose \textit{Perinucleus sampling}.\footnote{The region of cytoplasm in a cell just outside the nucleus is called the perinucleus. }  

We hypothesize that uniformly random responses, \( y_{\text{fp}} \), degrade performance because the modifications required for the fingerprinted model, \(\theta^m_{\text{fp}} \), to align these responses with natural keys are substantial. This is due to the low probability of such responses occurring under the original model's distribution, \( p_{\theta^m}(\cdot | x_{\mathrm{fp}}) \).

To gracefully trade-off Uniqueness and Harmlessness by controlling $p_{\theta^m}(y_{\mathrm{fp}}|x_{\mathrm{fp}})$, we propose Perinucleus sampling; we  sample $y_{\mathrm{fp}}$ from the edge of the nucleus of the probability distribution $p_{\theta^m}(\cdot|x_{\text{fp}})$ induced by the base model. Concretely, given some threshold $t\in[0,1]$ and width $k\in{\mathbb Z}_+$, Perinucleus($t,k$) first computes the next token probabilities for the completion of $x_{\mathrm{fp}}$: $p_{\theta^m}(\cdot|x_{\text{fp}})$ and sorts the tokens in descending order of probabilities. The nucleus \cite{holtzman2020curiouscaseneuraltext} is defined as the tokens in the top $t$-percentile of the CDF of this distribution. The Perinucleus response, $y_{\mathrm{fp}}$, is chosen by picking one token uniformly randomly from the next $k$ tokens with probabilities just outside this nucleus. This is formally described in \cref{alg:perinucleus} in \cref{app:pseudocode}, and an example response with $k=1$ is illustrated in the left panel of \cref{fig1}. Informally, Perinucleus sampling generates responses which are sensible, but uncommon as shown in the example.



The threshold $t$ balances the Uniqueness and Harmlessness. A lower threshold risks losing uniqueness, while achieving harmlessness. We investigate this in \cref{fig:fp_design} (center), and find that the model performance is relatively flat, before dipping sharply after $t=0.9$. %
We use $t=0.8$ in our experiments in accordance with the above findings. Note that while this ensures that  $p_{\theta^m}(y_{\mathrm{fp}}|x_{\mathrm{fp}})$ is guaranteed to be less than 0.2, in practice we find that it is much lower, with the average response probability across all fingerprints being $0.014$.




 













  





The width $k$ also balances Uniqueness and Harmlessness in \cref{fig:fp_design} (right). Larger width hurts performance because more unlikely responses are sampled, but this improves uniqueness. As $k$ increases, Perinucleus responses become closer to uniformly random.
Assuming that the randomness used in fingerprint generation is kept secret, a width $k$ ensures that for any LLM $\theta$, $p_{\theta}(y_{\mathrm{fp}}|x_{\mathrm{fp}})\leq 1/k$. The false positive rate of our scheme can bounded using Hoeffding's inequality.
\begin{proposition}\label{prop:fpr}
Given a choice of \(k\) in Perinucleus sampling and \(M\) total fingerprints, if we claim ownership of a model when more than \(m\) responses to fingerprint keys match the fingerprint responses for some $m$, then the false positive rate satisfies
\[\operatorname{FPR} \le \exp\left(-\frac{2}{M}\left(m - \frac{M}{k}\right)^2\right).\]
\end{proposition}
In particular, when \(m = M\) (perfect Persistence), we have \(\operatorname{FPR}\le \exp\left(-2M(1 - 1/k)^2\right)\). 
\begin{figure*}[ht]
    \centering
    \vspace{-1ex}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.42\textwidth]{figures/llama_8b_accuracy_errors.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.42\textwidth]{figures/llama_8b_forgetting_error_bars.pdf}
    \end{subfigure}
    \vspace{-3ex}
    \caption{\textbf{Harmlessness and Persistence of Fingerprints on Llama-3.1-8B}. We insert up to 24576 fingerprints into a Llama-3.1-8B model and measure the utility (on OpenLLM) of this model in the left plot, and find that Perinucleus fingerprints lead to a low loss in utility. We also measure the Persistence of the fingerprints (i.e. the percentage of fingerprints which are correctly recalled after SFT) in the plot on the right. We find that Perinucleus fingerprints have a higher persistence as compared to other designs.}
    \label{fig:scalability}
\end{figure*}
As we see, larger values of $k$ can lead to lower false positives. However, they could also lead to a drop in performance. In \cref{fig:fp_design} (right), we investigate this drop and find that values of $k$ less than 100 do not cause a large loss of utility for the model. A more detailed discussion about the false positive rate can be found in \cref{app:fp-analysis}.


Longer responses can also be generated using our scheme if desired --- we simply sample the first response token, $y_{\text{fp} , 1}$, using Perinucleus sampling, and then sample from the model conditioned on the key and the first token (i.e. from $p_{\theta^m}(\cdot | x_{\mathrm{fp}}, y_{\text{fp} , 1})$) to generate the rest of the response. We demonstrate the Harmlessness of longer responses with this scheme in \cref{fig:fp-transfer}, showing that they more robust to changes in response length as compared to the baseline. We also show examples of fingerprints in \cref{app:example-fps}. 















\subsection{Fingerprint training}
\label{sec:regularization}




Since fingerprinting involves fine-tuning which can significantly distort the model's output distribution, we need some regularization to keep the model close to its non-fingerprinted base model, preserving utility. We propose using a combination of Model Averaging and Data-Mixing. %

{\bf Model-Averaging.}  Following work from the continual learning literature~\cite{jang2022continualknowledgelearninglanguage, shi2024continuallearninglargelanguage, zhu2020modifyingmemoriestransformermodels, kirkpatrick2017overcoming, daume2009frustratingly}, we add an $\ell_2$-penalty on the difference between $\theta_{\text{fp}}^m$ and $\theta^m$ while training. We implement this as weight averaging, for some choice of $\lambda_{\rm MA}\in[0,1]$, making each step as  
\begin{eqnarray*}
 \theta_{t+1}^m &\gets & (1-\lambda_{\rm MA})\tilde{\theta}^m_t + \lambda_{\rm MA} \theta^m  \;, 
\end{eqnarray*} 
where $\tilde{\theta}^m_t = \theta_t^m - \eta \sum_{i=1}^M \nabla \ell(\theta_t^m, x^i_{\mathrm{fp}}, y^i_{\mathrm{fp}})$. It can be seen that this is equivalent to penalizing the square of the difference between the current and the base model's weights during training, ameliorating performance degradation. 





{\bf Data-Mixing.} We also mix data sampled from the base model $p_{\theta^m}(\cdot)$ with the fingerprints during training ~\cite{chen2024continualmemorizationfactoidslarge, huang2024mitigatingcatastrophicforgettinglarge} to mitigate catastrophic forgetting, distilling some of the capabilities of the base model into the fingerprinted model. The fraction of benign data is parametrized by $\beta_{\rm DM}$.


We report the sensitivity to these hyper-parameters in \cref{fig:hparam} in \cref{app:experimental-setup}, and use
with $\lambda_{\rm MA}=0.75$ and $\beta_{DM}=0.25$ in our experiments with Llama-3.1-8B. In \cref{app:meta-learning}, we introduce two more approaches, Parameter Adding and Meta-Learning, which improves persistence and harmlessness at the cost of increased computation. 















\section{Experiments on Scalability and Persistence}\label{sec:exp}

We demonstrate the Scalability of our approach by measuring the Harmlessness (\cref{sec:exp-scalability}) and Persistence (\cref{sec:exp-persistence}) of fingerprints.
We present an ablation study and a generalization to changes in the fingerprint design in \cref{sec:ablation}. Additional results on other model families, fingerprinting algorithms, and analysis with different SFT settings can be found in \cref{app:more-experiments}.


{\bf Experimental setup.} 
We conduct experiments to show the efficacy of our scheme on Llama-3.1-8B-Base model. We generate fingerprints where $x_{\mathrm{fp}}$ has 16 tokens, and $y_{\mathrm{fp}}$ has 1 token. For our method, we generate fingerprint keys with low-temperature, and use $t=0.8$ and $k=3$ for Perinucleus sampling. We also use anti-forgetting regularizers (\cref{sec:regularization}) for all methods. Further details on our setup are in \cref{app:experimental-setup}.

\textbf{Metrics} To measure the Harmlessness of fingerprints, we report evaluation scores on the OpenLLM~\cite{open-llm-leaderboard} benchmark. 
To assess Persistence, we first perform SFT on the fingerprinted model using the Alpaca~\cite{alpaca} dataset for instruction tuning. We then prompt the model with the fingerprint keys and verify whether the highest-probability output token matches the corresponding fingerprint response. Persistence is measured as the fraction of correctly recalled fingerprints over the total inserted.  We re-run each experiment thrice and report the mean and standard deviation. 

{\bf Baselines.} Two fingerprinting  schemes from prior work~\cite{xu2024instructionalfingerprintinglargelanguage, russinovich2024heythatsmodelintroducing} are our baselines. We term these as RANDOM and ENGLISH-RANDOM. The former uses a concatenation of random tokens as the fingerprint key ($x_{\mathrm{fp}}$), while the latter uses a coherent English sentence sampled from Llama-3.1-8B-Instruct. For both these schemes, the response ($y_{\mathrm{fp}}$) is a \textit{random} token.


 

\begin{figure*}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.42\textwidth]{figures/llama_3B_8B_accuracy.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.42\textwidth]{figures/llama_3B_8B_forgetting_more_seeds.pdf}
    \end{subfigure}
    \vspace{-3ex}
    \caption{\textbf{Scaling the model size.} We plot the utility of fingerprinted models on OpenLLM (left) and Persistence, the percentage of surviving fingerprints after SFT on Alpaca, of Perinucleus fingerprints (right) for different sized  Llama 3 models. We see negligible performance drop on OpenLLM from the base models (gray lines) across the experiment. Persistence after SFT remains close to 50\% even at 8192 fingerprints for all model sizes.  
    }
    \label{fig:scaling-model_size}
\end{figure*}

\subsection{Scalability: How many fingerprints can we add?}
\label{sec:exp-scalability}

    

Scaling to a large number of fingerprints is crucial for making model sharing secure, e.g., as we show in \cref{fig:attacks}. However, existing works embed only up to 100 fingerprints  \cite{russinovich2024heythatsmodelintroducing} because ENGLISH-RANDOM fingerprint generation--English keys and random responses--suffers from significant utility drop after 256 fingerprints as seen in \cref{fig:scalability} (left). Another baseline scheme of RANDOM--which uses a sequence of random tokens as key and response--is Scalable but not secure, because such keys can easily be filtered out by the model host at inference time. The proposed scheme of using Perinucleus fingerprints with English keys achieves the best of both worlds -- it has in-distribution keys and has better Harmlessness by trading off on Uniqueness (as we define in \cref{sec:approach}). We can hence scale up to 24,576 fingerprints without significant drop in model performance as seen in the plot. This is two orders of magnitude improvement over the existing baselines, and this gain extends to various model sizes (\cref{fig:scaling-model_size}) and families (\cref{fig:other-model} in the appendix). Our ablation study (\cref{tab:ablations}) shows how Perinucleus sampling and regularized training are both critical in achieving such level of Scalability. 


\subsection{Persistence: How many fingerprints survive SFT?}
\label{sec:exp-persistence}


An important property of fingerprints is their ability to Persist after SFT on other data. We investigate this in \cref{fig:scalability} (right), plotting the Persistence of fingerprints after SFT on Alpaca for a Llama-3.1-8B model. 

 ENGLISH-RANDOM leads to fingerprints that are easily forgotten, while using RANDOM strings as keys gives higher Persistence. Since RANDOM keys are out-of-distribution from the SFT data, we posit that the changes induced by SFT do not change the model's behavior much on RANDOM  fingerprints. This  
 leads to higher Persistence. 
 
 
 The proposed Perinucleus scheme also has a high Persistence, retaining over 60\% of fingerprints when 8192 of them are initially inserted. We hypothesize that the in-distribution nature of the responses (as compared to ENGLISH-RANDOM) leads to better Persistence. Note that Persistence decreases as more fingerprints are inserted. As the number of fingerprints increases, the average value of $p_{\theta_{\text{fp}}^{m}}(y_{\mathrm{fp}} | x_{\mathrm{fp}})$ after fingerprinting goes down (as we show in \cref{app:forgotten-fp-analysis}), since we regularize the model to have a high utility. This means that a greater fraction of fingerprints are closer to the margin of being forgotten as we increase the number of fingerprints, and this leads to a lower Persistence. This effect is even more pronounced for schemes where $p_{\theta^m}(y_{\mathrm{fp}} | x_{\mathrm{fp}})$ is already low, i.e. where the response was chosen randomly. However, the rate of this decrease appears to be sublinear for Perinucleus fingerprints, indicating that the total number of retained fingerprint still increases as the number of fingerprints inserted is increased. We explicitly show this in \cref{fig:persistence_num_fp} in the Appendix. We also conduct experiments on how the size and distribution of the SFT dataset affect persistence of fingerprints in \cref{app:sft-analysis}. 





\subsection{How general is our approach?}
\label{sec:ablation} 

{\bf Scaling model size.}
In \cref{fig:scaling-model_size}, we investigate the Harmlessness and Persistence of the proposed scheme across different model sizes (1B, 3B, 8B) from the Llama-3 family. We find that the drop in performance is minimal across model sizes, indicating that the scheme is robust to variations in model scale. We also find Persistence to be high across models, with close to 50\% Persistence even in the 1B model at 8192 fingerprints. 
Perhaps surprisingly, smaller models demonstrate a greater persistence at 1024 fingerprints. Scaling laws to characterize Persistence of models to retain knowledge is an interesting future research direction.



\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lccc}
    \toprule
      \textbf{Fingerprint Design} &  \textbf{Regularizers} & \textbf{OpenLLM} & \textbf{Persistence} \\
        \midrule
          ENGLISH-RANDOM &   & 43.5 & 39.8  \\
          ENGLISH-RANDOM & \cmark & 55.8 & 8.9  \\
        Perinucleus &   & 58.0 &  97.1\\
        Perinucleus & \cmark & 61.1 & 77 \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Ablation study} shows that Perinucelus fingerprint generation is crucial for Harmlessness and anti-forgetting regularizers further boost Harmlessness when inserting 1024 fingerprints into a model. One can trade off Harmlessness and Persistence by controlling the amount of regularization, and we select an operating point with  the highest possible Harmlessness. 
    }
    \label{tab:ablations}
\end{table}

\textbf{Do Perinucleus fingerprints transfer from one model to another?}
\begin{figure*}[ht]
    \vspace{-1ex}
    \centering

    \begin{minipage}{0.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/llama_other_model_fp_forgetting.pdf}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/llama_other_model_fp.pdf}
    \end{minipage}
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/response_length.pdf}
    \end{minipage}
    \vspace{-3ex}
    \caption{\textbf{Changing the fingerprint responses} \textit{(Left and middle)} Persistence and OpenLLM performance when smaller models are used to generate fingerprints using Perinucleus sampling. We find that the utility does not change, but Persistence drops when using fingerprints from other models. \textit{(Right)} Performance drop when the length of the response in the fingerprints is increased. The performance with 1024 Perinucleus fingerprints is significantly more robust to the length of the response as compared to the baseline of 1024 English fingerprints.}
    \label{fig:fp-transfer}
\end{figure*}
Since  Perinucleus responses are generated from the model being fingerprinted, an interesting question is whether we can use other models to generate these responses instead. To test this we generate Perinucleus responses using smaller models, i.e., Llama-3.2-1B and 3B, and use these fingerprints for a Llama-3-8B model. The resulting utility and Persistence are shown in \cref{fig:fp-transfer} for 1024 and 4096  such fingerprints. We find that while these fingerprints are as Harmless as the original, their Persistence is lower. To explain this, we compute the average value of $p_{\theta^m}(y_{\mathrm{fp}}|x_{\mathrm{fp}})$, and find it to be directly correlated with model size, i.e., this probability is lower for fingerprints generated by Llama-3.2-1B than those by Llama-3.2-3B, which is lower than the original fingerprints (6.12, 5.58, and 5.14 being the respective average log perplexities). In the context of \cref{fig:fp_design} (right), these fingerprints are equivalent to increasing the threshold of fingerprinting, which leads to a similar utility, but lower Persistence.

\textbf{Do longer responses work?}
Existing works, e.g., \cite{xu2024instructionalfingerprintinglargelanguage,russinovich2024heythatsmodelintroducing}, only use one-token responses because Harmlessness drops significantly for longer responses as shown in the right panel of \cref{fig:fp-transfer} labeled English; this uses English sentences (unrelated to the key) as longer responses. In \cref{sec:perinucleus} and \cref{alg:perinucleus} in the appendix, we introduce an extension of Perinucleus sampling to longer responses. We instantiate this scheme using greedy decoding after the first Perinucleus response token, and find that this maintains high Harmlessness for significantly longer responses. 
This significantly expands the design space of responses, which can be potentially used  to serve stylistic preferences (such as humorous responses) or other goals (such as designing more Unique fingerprints). 




{\bf Ablation studies.}
In \cref{tab:ablations} we conduct an ablation study. We insert 1024 fingerprints into Llama-3.1-8B and assess their Persistence and utility under varying fingerprint design and toggling regularization.  We find that the largest gains in both model utility and Persistence come from better fingerprint design using Perinucleus sampling, while regularization provides a large boost in Harmlessness. We also note that there is a trade-off between utility (i.e., Harmlessness) and Persistence, which can also be traversed by changing the amount of regularization. 





\section{Security Threats through Collusion}
\label{sec:security}


In \cref{app:security}, we address several security and robustness risks to fingerprint detection, including random sampling for generation, model merging, prompt wrappers and false positive detection, and show how to counter them. However, in this section, we introduce and focus on the specific threat of 
collusion attacks; this exemplifies how Scalability is necessary for Security.  






One of the benefits of fingerprinting is the ability to share a model with a bigger community. A natural scenario is when a {\em model owner} receives a request to share the model weights and sends a fingerprinted version of the model to a {\em model host}, who then runs some service using the model. Fingerprinting helps detect when the model is illegally copied and hosted by others without legitimate access. Naturally, each version of the model is fingerprinted with a different set of fingerprints to uniquely link each model with the model host who is given access. 
\begin{figure}[t]
\vspace{-2ex}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.65\linewidth]{figures/llama_detection_rate.pdf}
        \vspace{-0.3cm}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.22\linewidth]{figures/legend_collusion.pdf}
        \vspace{-0.3cm}
    \end{subfigure}
    \caption{Under various 3-way collusion attacks, the proposed collusion resistant fingerprinting with $p=0.243$ achieves near-perfect detection rate when the number of total fingerprints $M$ is larger than $2048$.  This implies that each model needs to scale to include at least $Mp=500$ fingerprints on average to achieve Security against collusion attacks. The total number of shared fingerprinted models is $N=2048$.}
    \label{fig:collusion-attack}
\end{figure}

{\bf Threat model.} In this scenario where $N$ versions of a base model is shared with $N$ model hosts, 
a coalition of adversarial hosts may pool their models to avoid detection. If all fingerprints are unique, i.e., no two models share any fingerprints, then such a coalition can identify and avoid answering fingerprint queries. By running multiple models for each query, they can identify differences in fingerprinting because their models will respond differently. They can respond to queries using strategies to evade detection, including the following -



{\em Majority voting}:
The coalition responds with the output produced by the most models, breaking ties randomly.
    
{\em Minority voting}:
The coalition responds with the output produced by the fewest models, breaking ties randomly. 
    
{\em Non-unanimous refusal}:
The coalition refuses to respond to any query where there is disagreement among the models.




{\bf Novel collusion resistant fingerprinting strategy.} We now introduce a novel scheme to assign fingerprints and a model identification scheme (in \cref{def:anti-collusion-fp}) that are  simple to implement. In \cref{fig:collusion-attack}, we empirically demonstrate that this strategy is secure against the three standard collusion attacks 
and an  additional Optimal attack, which we outline in the proof of \cref{prop:fingerprint-guarantee}. While the optimal strategy helps adversaries avoids detection most effectively, with enough fingerprints detection becomes nearly certain. 
Together with our theoretical guarantee against a worst-case attacker in \cref{prop:fingerprint-guarantee}, this shows that embedding enough number of fingerprints in each model, i.e., Scalability, is critical in achieving security, i.e., identifying at least one model colluding in the attack. 




The main idea of our strategy is to assign each fingerprint to a random subset of models.
This ensures that no adversarial collusion strategy can bypass a certain large number of fingerprint checks.  This randomization is also key for efficiency --- models can be released one by one, and we can make the fingerprint choices for each model separately, independent of any past fingerprint allocations.

\begin{definition}[Collusion resistant fingerprinting]\label{def:anti-collusion-fp}
Suppose we need to share $N$ fingerprinted versions of the base model, and we want to use $M$ unique fingerprints. 
We assign each fingerprint to each model independently and randomly with probability \(p\) chosen by the model owner.
To identify which of the $N$ models is used by a model host in question, we check for the presence of each fingerprint. 
We track a score \(\{ s_i \}_{i=1}^N\) for each potential candidate model. Each time a fingerprint response is received, we add one to the score of all models that the fingerprint was assigned to. 
Once all $M$ fingerprints have been checked, return the model corresponding to the largest score. 
\end{definition}
A mild assumption is required for the analysis under worst-case adversarial strategy. 
\begin{assumption}[Unanimous response]\label{assump:unanimous-response}
    When all models in the coalition produce the same output, the coalition must respond accordingly.
\end{assumption}

This guarantees the detection of a single model from the coalition.
In general, it is impossible to guarantee the detection of the entire coalition without stronger assumptions.








{\bf Theoretical guarantees.} 
In the case of no collusion, it is easy to see why this scheme will be effective:
the score of the model being queried is \(Np\) in expectation, while the scores of other models have expectation \(Np^2\).
These quantities will separate substantially for sufficiently large \(N\) and small \(p\). 

In the presence of collusion, we prove that a large enough number of fingerprints guarantees identification. 
\begin{proposition}\label{prop:fingerprint-guarantee}
    Under \cref{assump:unanimous-response} and the fingerprinting scheme of \cref{def:anti-collusion-fp}, when there are  \(N\) models and a maximum coalition size of \(K\), 
    \[M = O\left(2^KK^{K+1} \log(N/\delta)\right)\]
    fingerprints will guarantee detection of at least one model from the coalition with probability at least \(1 - \delta\).
\end{proposition}

We defer the proof to \cref{app:proofs}. Although the bound on the number of required fingerprints scales poorly in \(K\), this is unlikely to be an issue in practice because forming a coalition of size \(K\) makes inference \(K\) times more expensive.
Thus, collusion will only be economically viable for small \(K\).
In contrast, the logarithmic scaling in \(N\) ensures that this scheme may support a large number of models.





















\section{Conclusion}\label{sec:conclusion}
In this work, we presented a new scheme to generate and insert fingerprints into LLMs.\
We show that the scheme enables scaling up the number of fingerprints that can be inserted by a substantial amount, and that larger numbers of fingerprints can be used to reduce the false positive rate of detection, better adapt to useful downstream modifications such as fine-tuning, and resist attacks by colluding actors.
Such a scalable solution to model fingerprinting is a crucial component in retaining the ownership of the model while sharing the model weights, hence fostering a decentralized ecosystem where innovation and entrepreneurship are encouraged \cite{loyalAI}. 
While we show the robustness of our fingerprinting to some security threats, combining multiple attacks including (e.g. fine-tuning and collusion) presents an interesting direction for future work.





\bibliography{references}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn

\section{Extended Related Works}
\label{app:related-works}
\subsection{Backdooring models for fingerprinting}
There is a natural connection between model fingerprinting for authenticating ownership of a model and {\em backdoors} in secure machine learning \cite{gu2017badnets}, where an attacker injects maliciously corrupted training samples to control the output of the model. Since \cite{adi2018turning,zhang2018protecting,guo2018watermarking} started using backdoor techniques for model authentication, numerous techniques are proposed for image classification models \cite{zhu2021fragile}, pre-trained language models~\cite{gu2023watermarkingpretrainedlanguagemodels, kurita2020weightpoisoningattackspretrained, li2021backdoorattackspretrainedmodels}, and more recently for large language models \cite{xu2024instructionalfingerprintinglargelanguage, russinovich2024heythatsmodelintroducing}.  We refer the reader to ~\cite{zhao2025a} for a comprehensive survey.  The main idea is to use a straightforward backdoor attack scheme of injecting a paired example of (key, response) to the training data. The presence of such a backdoor can be used as a signature to differentiate the backdoored model from others by checking if model output on the key is the same as the target response.  This scheme is known as {\em model fingerprinting} and the corresponding pairs of examples are called {\em fingerprint pairs} or fingerprints. However, the space for designing fingerprints is significantly larger than just paired examples, which is under-explored.

\subsection{Fingerprinting LLMs}
\paragraph{Active Fingerprinting through Fine-tuning} There has been much recent interest in fingerprinting generative large language models to detect model stealing. \citet{xu2024instructionalfingerprintinglargelanguage} studied this problem in both a white-box (i.e. with access to model weights) and black-box (i.e. access only to an API) settings. They proposed fine-tuning the model with fingerprints containing random sequences of tokens. They also propose a set of six criteria for good fingerprinting methods, including persistence of fingerprints after SFT on other data, and harmlessness of the fingerprinting on other model abilities. \citet{russinovich2024heythatsmodelintroducing} also study fingerprinting in a setting where model owners can also be adversarial, and falsely claim another model as their own. They hence propose a scheme where the responses for the fingerprint keys are uniquely decided for each model owner using a technique termed chain-and-hash. They also address a few practical challenges of fingerprints, including prompt wrapping by the model deployer to evade detection. The keys of the fingerprints considered are either concatenation of random tokens, or sensible English questions. We compare with these techniques in \cref{fig:scalability} for harmlessness and persistence. Similarly, \citet{zhang2024vtuneverifiablefinetuningllms} use fingerprints to solve an adjacent problem of verifiable fine-tuning. Here, the user provides a dataset to a fine-tuning service provider (such as OpenAI's fine-tuning platform), and wants to ensure that the returned model has been fine-tuned on the provided data. To do this, the user can insert backdoors or fingerprints into the training data. The paper also outlines a scheme to ensure that the inserted fingerprints are diverse enough, but also close to the training data distribution to evade detection and be harmless. \citet{cai2024utf} propose to find under-trained tokens in the model's vocabulary, and trains the model to use these as fingerprints. Other works have also looked at model merging as an attack~\cite{yamabe2024mergeprintrobustfingerprintingmerging, cong2023have} as well as a way to fingerprint models~\cite{xu2024fp}. \citet{yamabe2024mergeprintrobustfingerprintingmerging} propose a multi-level optimization scheme to fingerprint models, optimizing the fingerprints through GCG~\cite{zou2023universal}, and simulating merging during training to be robust to such an attack. 

\paragraph{Passive fingerprinting} A separate line of work has tried to ``discover" fingerprints in LLMs. \citet{yang2024fingerprint} leverage the attack techniques from~\citet{carlini2024stealingproductionlanguagemodel, finlayson2024logitsapiprotectedllmsleak} to infer the dimension of the final linear layer of a model from API access, and use this information as a fingerprint.  
Other methods assume white-box access to models, and measure the alignment in weights~\cite{refael2024slipsecuringllmsip} or representation~\cite{zhang2024reef, zeng2024huref} spaces. Another line of works trains a classifier on the outputs of the LLMs~\cite{pasquini2024llmmapfingerprintinglargelanguage} to discriminate between models. Similarly, \citet{iourovitski2024hide} bypass using a classifier by using another LLM to generative discriminative queries for pairs of models to be fingerprinted.

\paragraph{Attacks against fingerprints} Recent works have proposed methods to detect backdoors in LLMs. \cite{zeng2024beearembeddingbasedadversarialremoval, hoscilowicz2024hiding, shen2024bait, li2024backdoor}. These works mainly work on backdoors, which are prefixes or suffixes that can change the behavior of the model on a large range of inputs. Such backdoors are similar to the instructional fingerprints proposed by \citet{xu2024instructionalfingerprintinglargelanguage}, leading to an adversary potentially detecting such fingerprint triggers. \citet{hoscilowicz2024hiding} aim to find these triggers by iteratively searching over the LLM's vocabulary for tokens that lead to abnormally high probabilities for generating the next token. They also notice that when the first token
of a hidden fingerprint is used as an input, the LLM not only
produces an output sequence with high token probabilities, but
also repetitively generates the fingerprint itself. \citet{zeng2024beearembeddingbasedadversarialremoval} consider the problem of detecting safety backdoors. They find that backdoors cause the activations of the prompt to shift uniformly across different prompts. They then update the model to be robust to perturbations in such backdoor directions, essentially removing the backdoor from the model activations. Other works~\cite{li2024backdoor, shen2024bait} try to find the backdoor trigger by optimizing tokens to produce different responses on different benign samples. 






\subsection{Memorization and persistence }
\citet{zhang2024persistentpretrainingpoisoningllms} propose and study backdoor attacks which can persist after fine-tuning.   \citet{chang2024largelanguagemodelsacquire} study how models acquire knowledge during pre-training, and how this knowledge is forgotten. Similarly, \citet{allenzhu2024physicslanguagemodels33} study 
the capacity of different sized models to memorize facts. Crucially, these studies operate on fictional facts and synthetic strings, which is similar to the technique of fingerprinting. Thorough empirical investigations, e.g., \cite{hubinger2024sleeper}, demonstrate that backdoor attacks are resilient to further fine-tuning as long as the trigger is unknown. However, as typical in prior work, these studies have been conducted in a small scale, when only a few backdoors are injected (two backdoors in the case of \cite{hubinger2024sleeper}). We investigate how this resilience depends on the number of backdoors, i.e., fingerprints, injected and  how to improve resilience with Perinucleus sampling. 



\subsection{Watermarking for LLMs}
An area of research adjacent to fingerprinting is model watermarking. In this case, one assumes access only to the outputs of an LLM, and aims to detect if a piece of text was generated from a particular model. This is different from fingerprinting, since it is a passive process, where one does not query a model with specific keys, and in fact one does not even need to access the generation API. Such methods work by changing the probability distribution~\cite{kirchenbauer2023watermark}, sampling scheme~\cite{kuditipudi2023robust} or random seeds~\cite{christ2024undetectable} for generating tokens. Such schemes usually degrade quality of generation, and recent work focuses on improving this robustness-quality tradeoff~\cite{hu2023unbiased, zhao2024permuteandflipoptimallyrobustwatermarkable, giboulot2024watermaxbreakingllmwatermark}. Other works have also shown that watermarks can get transferred when one distills a student model from a watermarked teacher model~\cite{sander2024watermarking, zhao2022distillationresistantwatermarkingmodelprotection}, enabling detection of unsanctioned model-stealing through distillation.

\section{Proofs}
\label{app:proofs}
\begin{proof}[Proof of \cref{prop:fingerprint-guarantee}]
    First, we note that \(\operatorname{Binomial}(M, p^K)\) positive fingerprint responses are required by \cref{assump:unanimous-response}.
    Let \(F\) denote the number of unanimous positive fingerprints.
    The coalition \(C\) may also choose to return \(E\) additional positive responses.
    Clearly, when \(F = 0\) the adversary may choose \(E = 0\) to evade detection, so we will consider only \(F \ge 1\) from now on.
    Perhaps surprisingly, we will show that it is sometimes optimal for the adversaries to choose nonzero \(E\).
    
    To best avoid detection, the \(E\) positive results should each correspond to just one of the \(K\) models in the coalition and they should be distributed evenly among the \(K\) members.
    This strategy minimizes the maximum score achieved by the coalition to  \(F + E/K\), which cannot be improved further.
    In contrast, the number of total positive fingerprints is \(F + E\).

    
    
    Now, turning our attention to models not in the coalition, we have \(s_i \sim \operatorname{Binomial}(F + E, p)\) for all \(i \not\in C\).
    Applying a binomial tail bound and then choosing \(p = 1/(2K)\), we have
    \begin{align*}
    \Pr*{s_i \ge \max_{i \in S}s_i} &\le \Pr*{s_i \ge F + \frac{E}{K}}\\
    &\le \exp\left(-2\cdot\frac{(F(1 - p) + E(1/K - p))^2}{F+E}\right) \\
    &\le \exp\bigg(-2\cdot\underbrace{\frac{(F/2 + E/(2K))^2}{F+E}}_{Q}\bigg)
    \end{align*}
    for \(i \not\in C\).
    Now, we find the optimal \(E\) for the adversary.
    If \(K = 1\), then clearly \(E = 0\) is optimal.
    Otherwise, when  \(K \ge 2\) and \(F \ge 1, E \ge 0\), we have
    \[\od{Q}{E} = \frac{(E - F (K - 2)) (E + F K)}{4 (F + E)^2 K^2}\qquad\text{and}\qquad\od[2]{Q}{E} = \frac{F^2 (K - 1)^2}{2K^2(F + E)^2} > 0.\]
    So the only nonnegative critical point is \(E = F(K - 2)\) and this must be the minimizer of \(Q\).
    Substituting this back in, we get
    \[\Pr*{s_i \ge \max_{i \in S}s_i} \le \begin{cases}\exp(-F/2) & \text{if \(K = 1\)} \\ \exp\left(-2F(K-1)/K^2\right) & \text{if \(K \ge 2\)} \end{cases} \le \exp\left(-\frac{F}{2K}\right)\]
    for all \(i \not\in C\). This bounds the probability that a single model not in the coalition will have a score greater than or equal to the highest score within the coalition.
    Taking a union bound over \(N\) models, we have
    \[\Pr*{\max_{i \not\in C}s_i \ge \max_{i \in S}s_i} \le N\exp\left(-\frac{F}{2K}\right).\]
    From this we see \(F \ge 2K\log(2N/\delta) \triangleq F_{\min}\) limits the failure probability to at most \(\delta/2\).
    
    Finally, let's assume \(Mp^K \ge 2F_{\min}\).
    Using the relative binomial tail bound, we get 
    \[\Pr*{F \le F_{\min}} \le \exp\left(-\left(1 - \frac{F_{\min}}{Mp^K}\right)^2\frac{Mp^K}{2}\right) \le \exp\left(-\frac{Mp^K}{8}\right).\]
    Now we see that \(Mp^K \ge 8\log(2/\delta)\) suffices to limit the failure probability to at most \(\delta/2\).
    Combining this with our earlier assumption and taking a union bound over the two failure cases completes the proof.
\end{proof}

\begin{proof}[Proof of \cref{prop:fpr}] Our strategy is to query the model with $M$ fingerprint queries and only claim ownership if more than $m$ of them match the fingerprint response. Let $F_{i}$ denote the indicator that query $i$ leads to a false positive. From the way that the Perinucleus responses are chosen, we know that the probability of any one query being a false positive is bounded by $\frac{1}{k}$. Hence, $F_{i} \sim \mathrm{Bernoulli}(\frac{1}{k})$. Now, for our strategy to get a false positive overall, we need 
$$
\sum_{i=1}^M F_i \geq m
$$
Since each fingerprint was chosen randomly, we can bound the probability of this event by using Hoeffding's inequality
\begin{align*}
    P\left(\sum_{i=1}^M F_i \geq m\right) &\leq \exp\left(-2\frac{(m - \mathbb{E}[\sum_{i}^M F_i])^2} {M}\right) \\
    &\leq \exp\left(-\frac{2}{M}(m - \frac{M}{k})^2\right)
\end{align*}
\end{proof}

    
   

\section{Pseudocode}
\label{app:pseudocode}

\begin{algorithm}[H]
\caption{Perinucleus Sampling}
\label{alg:perinucleus}
\textbf{Input:} Base model $\theta^m$ and vocabulary $\mathcal{V}$, Model for keys $\theta^k$ threshold $t \in [0,1]$, width $k \in \mathbb{Z}_+$, length $L$ of response\\
\textbf{Output:} Sampled fingerprint  $(x_{\mathrm{fp}},y_{\mathrm{fp}})$
\begin{algorithmic}[1]
    \State Sample $x_{\mathrm{fp}} \sim p_{\theta^k}(\cdot)$
    \State Compute the next-token probabilities for all tokens $p_{\theta^m}(v | x_{\text{fp}}) \ \forall v \in \mathcal{V}$.
    \State Sort the tokens in descending according to $p_{\theta^m}(v | x_{\text{fp}})$ to get a vector $P$ of the probabilities and vector $I$ of the sorted token indices.
    \State Compute the cumulative sum $S$ of $P$, which is the CDF of the distribution
    \State Get smallest index $i$ s.t. $S[i] \geq t$. This is the boundary of the nucleus
    \State Sample a number $r$ uniformly at random between 1 and $k+1$
    \State Set the response token $y_{\text{fp}, 1}$ to the token indexed by $i+r$ in $I$. 
    \State \textbf{If $L > 1$:}
    \State \hspace{0.5cm} \textbf{For} $j = 2$ \textbf{to} $L$:
    \State \hspace{1.0cm} Compute $p_{\theta^m}(\cdot | x_{\text{fp}}, y_{\text{fp}, 1}, \dots, y_{\text{fp}, j-1})$.
    \State \hspace{1.0cm} Assign token with largest probability as $y_{\text{fp}, j}$
    \State \textbf{Return} $y_{\text{fp}} = (y_{\text{fp}, 1}, y_{\text{fp}, 2}, \dots, y_{\text{fp}, L})$
\end{algorithmic}
\end{algorithm}



\section{Additional Experimental Details}
\label{app:experimental-setup}
We conduct experiments to show the efficacy of our scheme on Llama-3.1-8B model. We generate fingerprints where $x_{\mathrm{fp}}$ has 16 tokens, and $y_{\mathrm{fp}}$ has 1 token. We use Llama-3.1-8B-Instruct to generate $x_{\mathrm{fp}}$, with a temperature of 0.5. We use Adam to optimize the cross entropy loss, training with full-batch gradient descent for upto 40 epochs, and early stop when the train loss drops below 0.005. This usually happens within a few epochs. We repeat each experiment thrice for our main results, generating a new set of fingerprints for different seeds. 

We report evaluation scores on the OpenLLM~\cite{open-llm-leaderboard} benchmark. , which is an average of scores on six tasks - MMLU, ARC, GSM-8K, HellSwag, TruthfulQA and Winogrande. 

To check for persistence, we perform SFT on the fingerprinted model on the Alpaca~\cite{alpaca} dataset, for instruction tuning We perform two epochs of fine-tuning with a learning rate of $10^{-5}$. We use the Llama-Factory~\cite{zheng2024llamafactory} framework for this.  

\subsection{Generating the fingerprint keys}
First, we sample a word from the 10,000 most used words in English. We then prompt Llama-3.1-8B with the following prompt ``Generate a sentence starting with \texttt{word}". We sample from the model at a temperature of 0.5 to obtain our fingerprint key $x_{\mathrm{fp}}$.
\subsection{Hyper-parameter selection}
For choosing our learning rate, as well as $\lambda$ and $\beta$ for regularization, we insert 1024 fingerprints into the model for each fingerprinting scheme with different learning rate between $1e-3, 1e-6$. We vary $\lambda_{\mathrm{MA}}$ between $0.1$ and $0.8$, and $\beta_{\mathrm{DM}}$ between $0.0$ to $0.5$. We pick the value which gives us the best performance on tinyBenchmarks~\cite{polo2024tinybenchmarks} as a proxy for harmlessness. Notably, we do not tune parameters for persistence.


\subsection{Example Fingerprints}
\label{app:example-fps}

RANDOM - 
\begin{itemize}
    \item key : ``bg char casinos nationally dresses lbs health xerox finland yamaha assessments versions dirt proteins passage span texts rebecca", response: `` transfer employees recently portfolio subscribe nest webcams moss navigator receptor dispatched peripheral restaurants"
    \item key: ``slight tennis blame based exposure therapist activity strongly mechanics summary govt daniel nr share abstracts cow ted conduct handbook", response: ``coffee desired filling earned official facilities kate merchant protocols decimal prohibited countries penny library keyword"
    \item key: ``beatles adolescent managing pierce saving acne script use families fraser mails donate massachusetts labels parental twist", response: ``fighters vitamins rock governance peninsula ibm votes familiar specifics disputes abu pieces ruling navigate elite experimental yea"

\end{itemize}

ENGLISH RANDOM - 
\begin{itemize}
    \item key : ``The world is full of beautiful things. From the majestic mountains to the serene oceans", response: `` Outlined in the company's annual report, the new policy aims to reduce the carbon footprint of the company by 50\% within the next five years"
    \item key: ``Proteins are the building blocks of life, and they play a vital role in the functioning of our bodies.", response: ``Le Corbusier's architecture was characterized by a fusion of modernism and brutalism."
    \item key: ``Documentation is a crucial part of any project, and it's often overlooked until the", response: ``Personal experiences often shape our perspectives and influence our decisions."
\end{itemize}


Perinucleus - 
\begin{itemize}
    \item key : ``The world is full of beautiful things. From the majestic mountains to the serene oceans", response: `` and everything in between, there is no shortage of natural beauty to be found."
    \item key: ``Proteins are the building blocks of life, and they play a vital role", response: ``as enzymes in the body. Enzymes are proteins that act as catalysts."
    \item key: ``Documentation is a crucial part of any project, and it's often overlooked until the", response: ``final stages. However, it's important to start documenting early on in the project"
\end{itemize}


\section{Other security risks}
\label{app:security}
\subsection{Changing the sampling}
One simple modification that can lead to degraded detection is changing the temperature of sampling, which could lead to fingerprint responses not being emitted on a fingerprinted model. However, this would also lead to a loss of utility at higher temperatures. In \cref{fig:attacks}, we plot this trade-off for a model with 1024 fingerprints. We find that fingerprints are somewhat resilient to sampling at different temperatures, and the model utility drops faster than the fingerprint accuracy as the temperature is increased. 

One way to increase the probability of detection in this case is to sample multiple times from the model. However in order to minimize the number of queries, we propose a simpler scheme -- change the fingerprints such that a single key $x_{\mathrm{fp}}$ is associated with multiple fingerprint responses $\{y_{\mathrm{fp}}^1, y_{\mathrm{fp}}^2, \cdots , y_{\mathrm{fp}}^N \}$. Fingerprinting a model to convergence with such strings would then lead to the top-$N$ most probable output tokens to be fingerprint responses. Hence, even under changes made to the sampling (such as increased temperature), we find that there is a higher chance of detection. We show this in \cref{fig:attacks}(left), where we plot this detection probability for $N=2$ responses per fingerprint.   
\begin{figure}
        \centering
        \includegraphics[width=0.4\linewidth]{figures/llama_multi_response.pdf}
    \caption{\textbf{Changing the sampling temperature} We find that changing the temperature of sampling leads to a lower fingerprint detection rate, but also lowers the model utility. The detection can be made better by a simple modification to our fingerprinting scheme.}
    \label{fig:attacks}
\end{figure}

\subsection{Model Merging}


\begin{table}[ht]
\centering
\begin{tabularx}{\textwidth}{c|XX|XX}
\toprule
\textbf{Merging Parameter} & \multicolumn{2}{c|}{\textbf{Llama-Instruct}} & \multicolumn{2}{c}{\textbf{Llama-Base}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
               & \textbf{Linear Merge} & \textbf{SLERP Merge} & \textbf{Linear Merge} & \textbf{SLERP Merge} \\
\midrule
0.9 & 95.1 & 95.7 & 96.1 & 97.6 \\
0.8 & 92.1 & 90.2 & 94.1 & 96.2 \\
0.7 & 86.2 & 86.1 & 89.8 & 92.1 \\
0.5 & 61.1 & 61.2 & 74.1 & 74.4 \\
0.2 & 10.6 & 10.2 & 11.7 & 3.8  \\
0.1 & 4.5  & 3.5  & 4.9  & 0.6  \\
\bottomrule
\end{tabularx}
\caption{\textbf{Persistence of Fingerprints After Model Merging}. We merge a fingerprinted Llama-3.1-8B model (with 1024 FP) with either the instruct or base version, using either linear or SLERP merging, and check the Persistence. We find that most fingerprints survive for larger values of the merging parameters. }
\label{tab:merging}
\end{table}

Model merging~\cite{ilharco2023editingmodelstaskarithmetic, yadav2023tiesmergingresolvinginterferencemerging, nasery2024pleasmergingmodels} in the weight space is widely used by practitioners to combine the abilities of multiple models. 
One possible threat to fingerprint detection is if the adversaries were to merge a fingerprinted model with a different, non-fingerprinted model. This threat model has also been studied in~\cite{yamabe2024mergeprintrobustfingerprintingmerging, cong2023have}. The latter has shown that Instructional Fingerprints are relatively robust to merging. We also investigate the persistence of Perinucleus fingerprints after merging a fingerprinted Llama-3.1-8B model with a different model (Llama-3.1-8B-Instruct) in \cref{tab:merging}. We consider only those methods which do not utilise the base (non-fingerprinted) model, and hence only consider linear averaging~\cite{wortsman2022modelsoupsaveragingweights} and SLERP~\cite{shoemake1985animating}. These methods are parametrized by $\lambda$, which denotes the weight of the fingerprinted model in the final model. Setting this $\lambda$ to be too low would hurt the utility of the final merged model, hence we consider values of $\lambda \geq 0.5$. We find that over 60\% of the 1024 fingerprints persist for these values of $\lambda$ for both the methods considered.

\subsection{Prompt Wrappers}
A simple method to evade detection by an adversary is to wrap each input to the LLM with a prompt wrapper. This could be a system prompt, or a specific instruction. As seen in \cref{tab:prompt_attack}, we see that this leads to a lower detection accuracy. To fix this behavior, we train the model with a set of system prompts while fingerprinting. This is similar to the approach in~\cite{russinovich2024heythatsmodelintroducing}. We find that this restores the detection accuracy back even under prompt wrappers at test time. 
\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        \textbf{\# FP} & \textbf{Prompt Training?} & \textbf{No Prompt Wrapper} & \textbf{Prompt Wrapper} \\
        \midrule
        \multirow{2}{*}{1024} &  \xmark       & 99.2 & 55.1 \\  
                              & \cmark   & 98.7 & 98.5 \\ 
        \midrule
        \multirow{2}{*}{4096} & \xmark & 99.3 & 54.2 \\ 
                              & \cmark   & 99.1 & 98.6 \\
        \bottomrule
    \end{tabular}
    \caption{Effect of training with system prompts.}
    \label{tab:prompt_attack}
\end{table}

\subsection{False claims of ownership}
Chain-and-hash~\cite{russinovich2024heythatsmodelintroducing} addresses this problem cryptographically by deriving the fingerprints from a secret key.
We can use this approach to give a similar guarantee.
Our implementation of perinuclear fingerprints picks the response randomly from among the top $k$ tokens just outside the nucleus.
This ``randomness'' can be derived cryptographically from the hash of the queries \(x_{\mathrm{fp}}^i\) along with a secret key.
This renders false claims of ownership computationally infeasible.

\subsection{An analysis of False Positives}
\label{app:fp-analysis}

\begin{figure*}[ht]
    \vspace{-1ex}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/false_positive_k=3.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/false_positive_k=10.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.3\textwidth]{figures/false_positive_k=100.pdf}
    \end{subfigure}
    \vspace{-3ex}
    \caption{\textbf{Probability of Perinucleus response under negative models} We plot the value of $p_{\theta}\left(y_{\mathrm{fp}} | x_{\mathrm{fp}}\right)$ for different non-fingerprinted models for different values of Perinucleus width $k$ for 1024 fingerprints. In the inset we plot the cumulative distribution for low values of the probability. We find that for most models the response has a value of less than 0.1 on most fingerprints across $k$.}
    \label{fig:false_positive_probs}
\end{figure*}
Since Perinucleus scheme involves generating unlikely tokens from the model itself, there is a chance that an un-fingerprinted model might generate similar tokens just by chance. To investigate this, we plot the value of $p_{\theta}\left(y_{\mathrm{fp}} | x_{\mathrm{fp}}\right)$ for 1024 Perinucleus fingerprints (generated by Llama-3.1-8B) for multiple publicly available non-fingerprinted models in \cref{fig:false_positive_probs}. We find that the response $y_{\mathrm{fp}}$ has a probability much less than $0.1$ for most models across fingerprints, indicating a low rate of false positives. This probability goes down as $k$ increases as well, as we show in \cref{prop:fpr}.


\begin{figure*}[ht]
    \vspace{-1ex}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/ROC_M=1.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/ROC_M=3.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/ROC_M=5.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.45\textwidth]{figures/ROC_M=10.pdf}
    \end{subfigure}    
    \vspace{-3ex}
    \caption{\textbf{ROC curves for fingerprint detection} We plot the ROC curves for varying values of $M$, and different sampling strategies.}
    \label{fig:roc-curve}
\end{figure*}
An adversary could also change the sampling to either increase this false positive ratio, or decrease the true positive detection rate. In order to mitigate this, we propose to change the detection strategy as follows - 
\begin{enumerate}  
    \item Choose $M$ fingerprints to test
    \item Sample respose from the model being tested
    \item Declare the model to be fingerprinted if $m$ of the responses match the fingerprints.
\end{enumerate}

Now, a false positive occurs if more than $m$ fingerprints come back positive for a non-fingerprinted model. By varying $m$, one can obtain an ROC curve. We show this in \cref{fig:roc-curve} for different values of $M$ and different sampling strategies (Greedy, Top-K, High Temperature, Min-P, and Self-Consistency with different sampling parameters). For these plots, we select $M$ fingerprints out of 1024 and use 6 different fingerprinted models and 14 different public non-fingerprinted models from different model lineages. The fingerprinted models also include models after SFT, which is why $M=1$ does not achieve perfect true positive rate. We find that even with very few fingerprints (10), one can obtain a good trade-off between true positives and false positive detections.


\section{Additional Results}\label{app:more-experiments}

We present additional experimental results. 

\subsection{More sophisticated algorithms}
\label{app:meta-learning}

On top of Model-Averaging and Data-Mixing in \cref{sec:regularization}, we present two additional approaches, Meta-Learning and Parameter-Adding, that use more resources to improve Harmlessness and Persistence. 

\begin{algorithm}[h]
\caption{Meta-Learning for Robust Fingerprinting}
\label{alg:meta-learning}
\begin{algorithmic}[1]
\State Initialize $\theta$ (parameters), learning rate $\alpha$, 
\For{$t = 1$ to $T$}
    \State Initialize $\hat{\theta} = \theta$
    \For{$t_f = 1$ to $T_F$}
        \State Sample batch $x_{ft} \sim \mathcal{D}_{ft}$
        \State Simulate Finetuning: $\hat{\theta} = \hat{\theta} - \nabla_{\hat{\theta}} L(\hat{\theta}, x_{ft})$
    \EndFor
    \State Compute gradient on fingerprints: $g = \nabla_\theta L(\theta, x_{\mathrm{fp}})$
    \State Compute gradient of fine-tuned model on fingerprints: $\hat{g} = \nabla_{\hat{\theta}} L(\hat{\theta}, x_{\mathrm{fp}})$ 
    \State Update parameters: $\theta = \theta - \alpha \cdot g - \beta \cdot \hat{g}$
\EndFor
\State \textbf{return} $\theta$
\end{algorithmic}
\end{algorithm}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/llama_8b_forgetting_num_fp_error_bars.pdf}
    \caption{Number of fingerprints retained after SFT plotted against fingerprints inserted}
    \label{fig:persistence_num_fp}
\end{figure}

\paragraph{Better Persistence of fingerprints through Meta-Learning.} The goal of persistence of fingerprints boils down to the LLM ``remembering" certain data even after it has been fine-tuned on other data. Prior work~\cite{deng2024sophon, tamirisa2024tamper, huang2024boostertacklingharmfulfinetuning} have looked at the problem of baking in some knowledge into a model such that it survives fine-tuning. These methods assume that the adversary has knowledge of the data that needs to survive fine-tuning, and can hence perform a targeted fine-tuning attack. In our setting, we have a weaker adversary who does not know what the fingerprint strings are, or their distribution. Hence, we only need to protect these strings from fine-tuning on {\em generic} datasets that are not targeted. 
To counter the forgetting of such fingerprints, we take inspiration from the above-mentioned line of works and propose a meta-learning style algorithm to make fingerprints more persistent during fine-tuning. Concretely, we simulate a fine-tuning run on unrelated data while the model is being fingerprinted. The final loss is then a sum of the losses on the fingerprints of the original and the fine-tuned model. However, since it is infeasible to back propagate through the finetuning process, we use a first order approximation where we assume that the fine-tuning is linear\cite{nichol2018firstordermetalearningalgorithms}. Hence, the total gradient for each optimization step is 
$\nabla_\theta L(fp) + \nabla_{\hat{\theta}} L(fp)$, where $\hat{\theta}$ is the model finetuned on unrelated data.
Our algorithm is shown in \cref{alg:meta-learning}


We show results of adding 1024 fingerprints into a 8B model with meta-learning in \cref{tab:meta-learning}, and find some improvement by using the algorithm.
\begin{table}[]
    \centering
    \begin{tabular}{cc|cc}
    \toprule
      Perinucleus FP & Meta-Learning & OpenLLM & Persistence \\
        \midrule
        \cmark &   & 58.0 &  97.1\\
        \cmark &  \cmark & 58.7 & 99.3 \\        
        \bottomrule
    \end{tabular}
    \caption{Using Meta-learning improves the persistence of fingerprints at 1024 fingerprints.}
    \label{tab:meta-learning}
\end{table}


{\bf Expanding the model's parameters.} 
We propose another method of increasing compute to get better fingerprint harmlessness. We propose adding extra parameters to a model which are randomly initialized and only trained on the fingerprints. The number of extra parameters is controlled by an expansion ratio. We only add parameters to the MLPs, increasing the width of the MLP by a factor of (1+expansion ratio), and during fingerprinting, only the added weights are updated. The intuition behind this method is that all original model weights remain unchanged, and extra capacity is added to the model specifically for memorizing fingerprints. In \cref{fig:hparam} (right), we show the results of adding 1024 fingerprints to a Llama-3.1-8B model with varying expansion ratios. We see promising results on the harmlessness of this approach at low expansion ratios.


\begin{figure}[h!]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.4\linewidth]{figures/hparam_search.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.4\linewidth]{figures/expand_ratio.pdf}
    \end{subfigure}
    \caption{In the figure on the left, we plot the harmlessness of different combinations of our regularization hyperparameters for 1024 fingerprints. Model-Averaging parameterized by $\lambda$ and Data-Mixing parameterized by $\beta$ are combined to fine-tune fingerprints (as defined in \cref{sec:regularization}). In the figure on the right, we plot the performance of a fingerprinted model with extra parameters added, and notice a gain in utility when 0.1\% extra parameters are added. }
    \label{fig:hparam}
\end{figure}

\subsection{Effect of SFT parameters on persistence}
\label{app:sft-analysis}
\begin{figure}[ht]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=0.35\linewidth]{figures/llama_8b_ft_samples.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=0.35\linewidth]{figures/llama_8b_ft_dataset.pdf}
    \end{subfigure}
    
    \caption{Changing the number of samples in fine-tuning and the fine-tuning dataset to see the persistence of fingerprints. In the figure on the right, blue bars are for MathInstruct while purple is Alpaca}
    \label{fig:persistence-ablations}
\end{figure}
In \cref{fig:persistence-ablations}, we investigate the effect of changing the SFT dataset on the persistence of fingerprints with our Perinucleus scheme. In the plot on the left, we change the number of samples of Alpaca used for fine-tuning, and find that as the number of samples increases, the persistence decreases almost logarithmically. In the plot on the right, we plot the persistence after SFT on MathInstruct\cite{yue2023mammoth}. Since this dataset is $4\times$ the size of Alpaca, we also plot the persistence after subsampling this dataset to 50000 samples. In both the cases, we observe that MathInstruct leads to lower forgetting than Alpaca. We hypothesize that this happens because its prompts are farther from the fingerprints' distribution, leading to lower interference on the model's behavior on fingerprint keys.


\subsection{Which Fingerprints are forgotten}
\label{app:forgotten-fp-analysis}
\begin{figure}
    \includegraphics[width=\linewidth]{figures/model_1024_histograms.pdf}
    \includegraphics[width=\linewidth]{figures/model_4096_histograms.pdf}
    \includegraphics[width=\linewidth]{figures/model_8192_histograms.pdf}
    \caption{Properties of forgotten and retained fingerprints}
    \label{fig:persistence-analysis}
\end{figure}
In \cref{fig:persistence-analysis}, we plot out the distribution of log perplexity (under the base model) of the key and response of forgotten and retained fingerprints when inserting different number of fingerprints into a model. We find that there is not a large difference in these entropies under base model, making it hard to distinguish a priori if a certain fingerprint will be forgotten or retained. We also plot the probability $p_{\theta_{\mathrm{fp}}^m}(y_{\mathrm{fp}}|x_{\mathrm{fp}})$ of the response on the fingerprinted model, and find that the forgotten fingerprints have a higher loss on the fingerprinted model.

\subsection{Hyperparameter sensitivity}

In \cref{fig:hparam} (left), we study the sensitivity of harmlessness (measured on TinyBench) at 1024 fingerprints to the hyperparameters of the regularizers proposed in \cref{sec:regularization}. We find that setting a high value of $\lambda_{MA}$ is important.

\subsection{Results with other models}
\label{app:other-models}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/other_models.pdf}
    \caption{\textbf{Fingerprinting other models} We check the harmlessness of our fingerprinting scheme on models from the Mistral, Gemma and Llama family and find that that drop in relative utility is low.  }
    \label{fig:other-model}
\end{figure}
In \cref{fig:other-model}, we show the harmlessness of our proposed scheme in fingerprinting Gemma-9B~\cite{gemmateam2024gemma2improvingopen}, Mistral-7B~\cite{jiang2023mistral7b} and Llama-3.1-8B-Instruct model. We find that we can fingerprint these models with a low drop ($\sim5\%$) in relative utility as well.





\subsection{Detailed Results}
\begin{figure}[htb!]
    \includegraphics[width=\linewidth]{figures/detailed_results_no_err.pdf}
    \caption{\textbf{Detailed Performance of the fingerprinted model on OpenLLM}}
    \label{fig:detailed-results}
\end{figure}

\end{document}
