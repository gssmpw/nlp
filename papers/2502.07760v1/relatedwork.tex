\section{Related Works}
We summarize selected related works in this section and defer a more comprehensive survey to \cref{app:related-works}.

\subsection{Backdooring models for ownership verification}
There is a natural connection between model fingerprinting for authenticating ownership of a model and {\em backdoors} in secure machine learning \cite{gu2017badnets}, where an attacker injects maliciously corrupted training samples to control the output of the model.  \citet{adi2018turning,zhang2018protecting,guo2018watermarking} first used backdoor techniques for model authentication, which were applied to image classification models \cite{zhu2021fragile}, pre-trained language models~\cite{gu2023watermarkingpretrainedlanguagemodels,kurita2020weightpoisoningattackspretrained,li2021backdoorattackspretrainedmodels}, and more recently for large language models \cite{xu2024instructionalfingerprintinglargelanguage, cong2023have, russinovich2024heythatsmodelintroducing}.  We refer the reader to \citet{zhao2025a} for a more comprehensive survey.    

\subsection{Fingerprinting LLMs}
There has been much recent interest in fingerprinting generative LLMs to detect model stealing. The main idea is to fine-tune the LLM on example \((\mathrm{key}, \mathrm{response})\) pairs. The model can then be authenticated by checking if it responds appropriately when prompted with the fingerprint \(\mathrm{key}\). This is adjacent to model watermarking, where one assumes access only to the outputs of an LLM, and aims to detect if a piece of text was generated from a particular model. We survey model watermarking in \cref{app:related-works}.

\citet{xu2024instructionalfingerprintinglargelanguage} studied the problem of fingerprinting in both a white-box (i.e. with access to model weights) and black-box (i.e. access only to an API) settings. 
\citet{russinovich2024heythatsmodelintroducing} study fingerprinting where model owners can also be adversarial and can falsely claim another model as their own. 
The keys of the fingerprints considered by these works are either concatenations of random tokens or sensible English questions. We compare with these methods in \cref{fig:scalability} for Harmlessness and Persistence of fingerprints. \citet{zhang2024vtuneverifiablefinetuningllms} use backdoors to solve an adjacent problem of verifying whether a model has been fine-tuned on a specific dataset and outline a scheme to generate diverse and in-distribution fingerprints. Other works propose model merging as an attack against fingerprinting~\cite{yamabe2024mergeprintrobustfingerprintingmerging, cong2023have} as well as a way to fingerprint models~\cite{xu2024fp}. We survey other attacks as well as methods to fingerprint models in \cref{app:related-works}.








\subsection{Memorization and Forgetting in LLMs }
\citet{zhang2024persistentpretrainingpoisoningllms} propose and study backdoor attacks which can persist after fine-tuning.   Other works \cite{chang2024largelanguagemodelsacquire, chen2024continualmemorizationfactoidslarge, jagielski2023measuringforgettingmemorizedtraining} study how models acquire knowledge during pre-training, how this knowledge is forgotten and how to encourage retention. Similarly, \citet{allenzhu2024physicslanguagemodels33} study 
the capacity of different sized models to memorize facts. These studies operate on fictional facts and synthetic strings, and can inspire better fingerprinting schemes. Conversely, fingerprints can also be used to gain further insights into memorization.