
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{etoolbox}
\usepackage{tabularx}
\usepackage{ifthen}
\usepackage[most]{tcolorbox}
\usepackage{mathrsfs}
\usepackage{blindtext}
\usepackage{longtable}
\usepackage{footnote}
\usepackage{graphicx,wrapfig}
\usepackage{subcaption} % Include this package
\usepackage[colorlinks=true]{hyperref}
\usepackage{upquote}
\usepackage[toc,page,header]{appendix}
\usepackage{multirow}
\usepackage{booktabs}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\newcommand{\kl}[1]{\comment{Kaixiang's comment: #1}}
% Add comments
\newcommand{\showcomments}{yes}
\newcommand\kaixiang[1]{
\ifthenelse{\equal{\showcomments}{yes}}{{\color{blue} Kaixiang: #1}}{\ignorespaces}
}

\newcommand{\mh}[1]{\comment{Mingyi's comment: #1}}
% Add comments
\newcommand\mingyi[1]{
\ifthenelse{\equal{\showcomments}{yes}}{{\color{cyan} Mingyi: #1}}{\ignorespaces}
}



\newcommand{\sy}[1]{\comment{Siyan's comment: #1}}
% Add comments
\newcommand\siyan[1]{
\ifthenelse{\equal{\showcomments}{yes}}{{\color{red} Siyan: #1}}{\ignorespaces}
}

\newcommand{\ours}{\textsc{PrefEval}}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}

\usepackage[]{mdframed}
\usepackage{framed}
\usepackage{threeparttable, booktabs}
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\usepackage{float}
\renewcommand{\theadfont}{\small\bfseries}


% \title{Conversational Preference Adherence (CoPA): Benchmarking Preference Adherence in Multi-Turn Conversations with LLMs}
\title{Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs}
% \title{\ours{}: Evaluating Personalized Preference Following in LLMs}


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{
    Siyan Zhao$^{2}$\thanks{Work done while as an intern at Amazon.} \space,
    Mingyi Hong$^{1,3}$, 
    Yang Liu$^1$, 
    Devamanyu Hazarika$^1$, 
    Kaixiang Lin$^{1}$ \thanks{Corresponds to: \texttt{siyanz@cs.ucla.edu}, \texttt{kaixianl@amazon.com}} \\[2pt]
    $^1$Amazon AGI, $^2$UCLA, $^3$University of Minnesota \\[2pt]
    \texttt{siyanz@cs.ucla.edu}, \texttt{mhong@umn.edu}, \texttt{devamanyu@u.nus.edu}\\
    \texttt{\{yangliud, kaixianl\}@amazon.com}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
% Comments on
\newcommand{\dev}[1]{\textcolor{red}{$_{dev}$[#1]}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

% \vspace{-2mm}
\begin{abstract}


Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce \ours{}, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in a long-context conversational setting.
\ours{} comprises 3,000 manually curated user preference and query pairs spanning 20 topics. ~\ours{} contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task. With \ours{}, we evaluated the aforementioned preference following capabilities of 10 open-source and
proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. 
Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following users' preferences during conversations. In particular,  in zero-shot settings, preference following accuracy falls below 10\% at merely 10 turns ($\sim$3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. 
% We also find that multiple stated preferences within a conversation improve adherence and models are not affected by conflicting preferences {\color{red}[is this the key message that we want to put in the abstract? maybe a bit confusing in the abstract without context?]}. 
Furthermore, we show that fine-tuning on \ours{} significantly improves performance. We believe \ours{} serves as a valuable resource for measuring, understanding, and enhancing LLMs' preference following abilities, paving the way for personalized conversational agents. Our code and dataset are available at \url{https://prefeval.github.io/}.

% \mingyi{[needs some transitions here..][very abrupt here..]} 
% {\mingyi{[it is strange here since there is no conclusion.]}}. 
 % \mingyi{[in high-level, user preference adherence capability contributes to build better conversational agents? or have other utilities? ]}



% \kl{we can provide GPT4v number here to support the claim. }

\end{abstract}

\input{sections/intro}

\input{sections/methods}

\input{sections/experiment}
\input{sections/related_works}
\input{sections/conclusion}


% \subsubsection*{Acknowledgments}

\newpage
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\newpage
\appendix
\section{Appendix}

\input{sections/appendix}

\end{document}
