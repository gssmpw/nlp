\subsection{Limitation}
Our benchmark aims to evaluate preference following rather than verifying the factual accuracy of the recommendations. While the LLM's recommendations may contain inaccurate information, fact-checking is a separate dimension beyond the scope of this work. Additionally, although we have conducted extensive human filtering on our synthetic preference-query pairs, incorporating real user preferences in future work would help capture more nuanced aspects of user interactions.

For implicit preference elicitation, we designed the available options such that only one option adheres to or violates the target preference. While this approach yields a high probability of inferring the user's preference from their choice, it does not guarantee 100\% accurate inference from the multiple-choice selection. However, given the early turns' results of a relatively high preference-following accuracy of the strongest baseline is around 90\%, we believe the errors should be within an acceptable range.

