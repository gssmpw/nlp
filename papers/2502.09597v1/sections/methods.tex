



\section{The \ours{} Dataset}

\subsection{Problem Formulation}
\label{sec:formulation}
The goal of our benchmark is to assess how well LLMs can personalize their responses to user preferences in conversational settings, with the overall setup shown in Figure~\ref{fig:main_fig}. Let $\mathcal{C}$ denote a conversation comprising $m$ turns $\{(u_1, b_1), (u_2, b_2), \ldots, (u_m, b_m)\}$, where $u_i$ and $b_i$ represent the user's message and LLM's response at turn $i$, respectively. Each conversation $\mathcal{C}$ is divided into sessions $\mathcal{S} = \{s_1, s_2, \ldots, s_k\}$, where each session $s$ represents a coherent dialogue segment focused on a specific topic and consists of contiguous turns $s = \{(u_i, b_i), \ldots, (u_{i+l}, b_{i+l})\} \subseteq \mathcal{C}$. 

At the core of our evaluation are \textit{preference-query pairs} $(p, q)$. Here, $p$ refers to a user preference disclosed at some point in the conversation, while $q$ represents a query related to this preference, posed at the end of the conversation for the LLM to respond. The user preference $p$ can be explicitly expressed as a single utterance or implicitly revealed through a multi-turn dialogue. The query $q$ is the user's message at turn $m$, i.e., $q = u_m$, and the LLM's response is $b_m$. The query $q$ is constructed such that a generic, non-personalized response would likely violate the previously stated preference $p$. We evaluate whether $b_m$ adheres to the preference $p$. To simulate real-world conversational complexity, we include unrelated contextual turns between $p$ and $q$. These intervening turns act as potential distractions, emulating the natural flow of dialogue, where multiple topics may be discussed across a single conversation. In such cases, maintaining awareness of the user’s earlier preference $p$ becomes crucial for appropriately responding to the later query $q$. We evaluate the LLM's ability to navigate this complexity and maintain personalization, measuring how well it responds to $q$ in line with the user’s preference $p$, even amidst unrelated sessions. 



\label{para:key_capabiilties}

To perform well on our benchmark, LLMs should demonstrate four key capabilities: (1) \textbf{\textit{Preference Inference}}—the capacity to accurately infer user preferences through dialogue, whether explicitly stated or implicitly revealed; (2) \textbf{\textit{Long-Context Retrieval}}—the ability to track and recall user preferences across long conversation; (3) \textbf{\textit{Preference Following}}—the ability to generate responses that are both contextually relevant and aligned with the user’s preferences when knowing the preference; and (4) \textbf{\textit{Personalization Proactiveness}}—the initiative and ability to know when and how to utilize the first three capabilities to deliver personalized responses, rather than focusing solely on general question answering. We will show in our benchmarking results that, LLMs that miss a subset of these capabilities perform poorly.

% \kaixiang{could we make 1 and 3 more differentiable? like 3) is suppose we already inferred the preference, how does the response follow the preference.} \siyan{How about 1) Preference inference vs 3) preference following?}





\vspace{-3mm}

\subsection{\ours{} Statistics}
\vspace{-2mm}
\ours{} consists of 1,000 unique preference-query pairs, each with three preference forms (\S\ref{sec:preference_Forms}), resulting in 3000 preference-query pairs. These pairs were manually curated with the assistance of GPT-4, Claude 3 Sonnet, and Claude 3.5 Sonnet (see Appendix~\ref{sec:data_generation_detail} for detailed data construction methodology). The preferences cover day-to-day topics such as travel, shopping, entertainment, and more, as shown in the topic distribution in Figure~\ref{fig:topic}. We intersperse unrelated contextual conversation turns between the disclosure of a user preference and the final query, with context lengths extending up to 100k tokens (\S\ref{sec:lmsys}). For each pair $(p, q)$, we consider two tasks: a generation task and a classification task. In the generation task, the LLM is required to generate a long-form response to the user's query. In the classification task, the LLM is presented with four options related to the query, with one option aligned with the user's preference, and the LLM is tasked to select the correct option.


\begin{figure}[t]
    \centering
  
          \includegraphics[width=\linewidth]{figs/copa_benchmark_treemap.pdf}
\caption{Distribution of domains and topics within \ours{}, which are commonly encountered during conversations with chatbots where users seek recommendations, suggestions, and advice.}
\label{fig:topic}
\end{figure}

\subsection{Preference Forms} 
\label{sec:preference_Forms}

User preferences can be expressed in various forms. In our benchmark, we consider three distinct methods of preference construction and expression: (1) \textbf{Explicit Preference.} The user directly expresses their preference to the LLM in a single sentence within a single conversational turn (Examples in Table~\ref{tab:explicit_preferences}). (2) \textbf{Implicit Choice-Based Dialogue.} The user’s preference is inferred over the course of a two-turn dialogue. In this setup, the user initiates a preference-related query, and the assistant presents multiple options, some of which violate the user’s preference while others align with it. The user can either agree with or reject one or more options, implicitly revealing their preference through these choices (Examples in Table~\ref{tab:implicit_choice-based}). (3) \textbf{Implicit Persona-Driven Dialogue.} To simulate more nuanced, implicit preferences, this form of elicitation unfolds over 4–8 turns. The dialogue primarily revolves around a persona-driven topic, with a randomly assigned persona guiding the conversation. The user's preference is subtly revealed in a single sentence during the dialogue, while it is not the main focus of the conversation, making the inference and reasoning process more challenging (Examples in Table~\ref{tab:implicit_persona}).

\vspace{-2mm}

\subsection{Multi-Session Conversational Context}
\label{sec:lmsys}
To simulate realistic conversational dynamics, we incorporate multi-session turns from the \textit{LMSYS-Chat-1M} dataset~\citep{zheng2023lmsyschat1m}, consisting of one million real interactions between users and 25 state-of-the-art language models across various topics. We randomly select multi-session context up to a length of 100k tokens and intersperse these conversation turns between the disclosure of a user preference and the final query. This presents a challenge for the LLM to accurately infer, retain, and retrieve user preferences while navigating unrelated dialogues, assessing the LLM's ability to handle long-context preference following.

% \kaixiang{I didn't follow the logic here why long context LLM long-context preference following. I feel it should be the real-world use-case  long context preference following}
\vspace{-2mm}
\subsection{Task Types and Evaluation Protocols}
We offer two task types for each preference-query pair, each with its corresponding evaluation protocol. By including both generation and classification tasks, we aim to thoroughly assess LLMs' ability to understand and adhere to user preferences in diverse contexts.

\paragraph{Generation Task and LLM-based Evaluators.}
\label{sec:method_errortypes}
% \kaixiang{Generation task is clear, long-form is confusing to me. let's remove long-form, also it confused with preference form.}
In the generation task, the LLM generates a response in reply to the user’s query. To evaluate preference following in the generation task, we apply the ``LLM-as-a-judge'' framework using Claude 3 Sonnet. Specifically, we employ four independent evaluators to check the response against four binary metrics. Each evaluator is provided with a detailed prompt (see Appendix Sec~\ref{sec:evaluator_prompts}) containing the definitions and examples for each metric. These checks are then aggregated into four distinct error types, following the rules outlined in Table~\ref{tab:error_types}. Preference-following accuracy is defined as the absence of {\it any} error type in the generated response. The four error types are: (1) \textbf{Preference-Unaware Violation:} The LLM provides generic recommendations that contradict the user’s stated preference due to unawareness of user preference.
(2) \textbf{Preference Hallucination Violation:} The response fabricates or misattributes preferences, diverging from the user’s true preference and violates the true preference.
(3) \textbf{Inconsistent Violation:} The response acknowledges the correct preference but generates contradicting response. 
(4) \textbf{Unhelpful Response:} The response lacks relevant recommendations or fails to address the query due to poor recall of the user’s preference. 
To validate our LLM-based evaluation method, we manually checked 200 randomly sampled evaluations, with an observed 5\% error rate. This demonstrates strong agreement between human judgment and LLM-based assessments with Claude 3 Sonnet.
{\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
\centering
\caption{Error type aggregation rules: Evaluators perform binary checks for each metric listed in the column headers. The results are then aggregated according to the table rules to classify the error into one of the four defined types.} 
\label{tab:error_types}

\scalebox{0.7}{
\begin{tabular}{c|c|c|c|c}

\textbf{\centering Error Type} & \textbf{\thead{Violate \\Preference?}} & \textbf{\thead{Acknowledge \\Preference?}} & \textbf{\thead{Hallucinate \\Preference?}} & \textbf{\thead{Helpful \\Response?}} \\
\hline
\thead{Preference-Unaware Violation} & Yes & No & N/A & Yes \\
\hline
\thead{Preference Hallucination Violation} & Yes & Yes & Yes & Yes \\
\hline
\thead{Inconsistency Violation} & Yes & Yes & No & Yes \\
\hline
\thead{Unhelpful Response} & No & Yes/No & N/A & No \\

\end{tabular}
}



\end{table}

\paragraph{Classification Task and MCQ Accuracy.}
\label{sec:method_classification}
In the classification task, the user presents a final query along with four potential options and asks the LLM to select the one that aligns with their preference. Only one option follows the user’s stated preference, while the remaining options conflict with it. The LLM’s preference-following accuracy is determined by whether it selects the correct option. This task facilitates faster automatic evaluation, eliminating the need for costly human or LLM-based assessments by focusing on a single-choice response.


%{\color{red}[do we want to comment, when using our benchmark, which task is more admissible. e.g., when people trying to use our benchmark, for generation task do they need to setup their own evaluators, etc.]}

\paragraph{Practical Guide to Using \ours{}.} While our benchmark enables comprehensive evaluation across multiple dimensions (including various baselines, conversation turns, preference forms and topics, and tasks), benchmarking on complete setups is computationally intensive. For practical use, we provide guidance based on available resources. If an evaluator model like the Claude 3 Sonnet is available, then the generation task can be used; otherwise, one can choose a local LLM as the evaluator or opt for our classification task, which does not require LLM-based evaluators but still has strong correlation with generation task performance (see Sec~\ref{sec: mcq task}). For initial testing, we recommend starting with a subset of topics and conversation lengths using explicit preference forms. As an example, our repository ({\small \url{https://github.com/amazon-science/PrefEval}}) includes a leaderboard comparing various LLMs on the ``travel restaurant" topic at both 10 and 300 turns, assessing both short-turn and long-turn preference-following capabilities. With additional computational resources, one can further use generation task evaluators for detailed error-type analysis, and test implicit preference forms to evaluate more advanced preference-following capabilities.