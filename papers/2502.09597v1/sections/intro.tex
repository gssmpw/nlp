% \vspace{-3mm}
\section{Introduction}

The pursuit of personal chatbots that can remember your favorite cuisine, propose binge-worthy series tailored to your tastes, and avoid suggestions that conflict with your dietary restrictions has been a longstanding desire for many. While LLM-based chatbots such as Claude~\citep{bai2022constitutional} and GPT-4~\citep{achiam2023gpt} have substantially advanced natural language processing capabilities, their ability to proactively provide personalized interactions that are scalable to millions of users remains limited~\citep{salemi2023lamp, li2023teach, jang2023personalized, tan2024democratizing, liu2024llms+, zhuang2024hydra, li2024dissecting, shaikh2024show, lee2024aligning}. For example, if a user says, \textit{``I don't like jazz,”} and later asks for travel recommendations in New Orleans, a personalized chatbot should avoid suggesting jazz-related attractions, which are popular there. Achieving this level of proactive personalization poses a challenge when scaled to millions of users with diverse real-life preferences. Rather than building separate models for each, it's more scalable to create a single adaptable chatbot that can dynamically understand and accommodate these preferences in real-time. This brings us to the central evaluation goal of our benchmark:

\begin{center}
\begin{tcolorbox}[
   enhanced,
   colback=gray!15,    % Box background color
   colframe=black,     % Changed to black frame color
   boxrule=1pt,        % Added visible border of 1pt
   arc=4pt,            % Rounding of corners
   auto outer arc,
   boxsep=5pt          % Added small padding for better appearance
]
\centering
\textbf{\textit{Can LLMs infer, remember and follow personalized preferences?}}
\end{tcolorbox}
\end{center}

This ability is crucial for user satisfaction and engagement during conversations. Current LLMs, however, are primarily optimized and evaluated for general-purpose tasks. Our study will reveal that we lack a comprehensive understanding of their ability to proactively follow and apply user preferences over conversations. Addressing this gap is essential for advancing LLMs toward truly personalized and scalable conversational agents.

\input{sections/main_figure}
Effective personalization in conversation settings also requires robust long-context abilities, as it requires aggregating and adhering to user preferences over extended interaction histories. A personal assistant should proactively infer, memorize, and adhere to these preferences over long horizon, ensuring responses are not only relevant but also aligned with the user’s preferences. Additionally, users express preferences in various forms—explicitly or implicitly—making it challenging for the assistant to recognize them accurately. Users may also present multiple or conflicting preferences within a single conversation. The assistant must navigate these nuances for delivering a truly personalized experience.


In response to these challenges, we introduce \ours{}, a benchmark to evaluate, understand, and improve LLMs' capacity for preference following in conversation settings. Our benchmark consists of 3,000 manually curated preference-query pairs across diverse daily life topics, incorporating preference forms which are explicitly stated or implicitly revealed, and it includes both generation and classification tasks. With \ours{}, we assess 10 open-source and proprietary LLMs with varying context lengths of up to 100k tokens using various methods, analyzing their adaptability to conflicting and multiple user preferences, and demonstrate how finetuning on this dataset enhances performance. Our contributions can be summarized as follows:
% \vspace{-3mm}
\begin{itemize}[leftmargin=.1in]

\item We introduce a novel, comprehensive benchmark for evaluating LLMs' preference following capabilities in conversational contexts, encompassing 3,000 manually curated question-preference pairs across 20 topics and 3 preference forms.
\item We conduct an extensive evaluation of 10 state-of-the-art LLMs, including Claude, Mistral, GPT4 and the LLaMA series, utilizing various context lengths and assessment methods such as prompting, iterative feedback~\citep{bai2022constitutional}, and retrieval-augmented generation~\citep{lewis2020retrieval}.
\item Our benchmark results show that, without explicit prompting, the preference following precision (evaluated using various measures we developed in this work) falls below 10\% in zero-shot settings for 10-turn conversations of 3k tokens. Even with more advanced methods, performance still deteriorates with longer contexts.
\item We uncover critical limitations in current LLMs through extensive error analysis, including their inability to recognize and proactively apply user preferences in long-context conversation settings. 

\item We find that, counterintuitively, multiple stated preferences within a conversation lead to improved adherence, even in the presence of conflicting preferences. We conjecture it is due to reinforced attention on user preferences. 

% {\color{red}[provide a brief reason of why we conjecture that is the case?]} 

\item Moreover, fine-tuning on \ours{} further enhances preference following and generalizes well to longer contexts.
\end{itemize}

