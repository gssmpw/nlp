\vspace{-0.8em} 
\section{Conclusion}
% \vspace{-1em}
In this work, we present \ours, a comprehensive benchmark addressing a critical gap in evaluating large language models' ability to follow user preferences in multi-session conversational settings. Our benchmark considers comprehensive aspects, including explicit and implicit preferences, both generation and classification tasks, and employs LLM-based and automatic evaluation methods. Through rigorous testing of 10 state-of-the-art LLMs across 20 diverse topics and various conversation lengths up to 100k, we demonstrate that preference following remains a significant challenge. Our findings reveal that even advanced models struggle to maintain adherence with user preferences in conversational setting, with accuracy dropping below 10\% in default settings for conversations exceeding 10 turns. These models struggle to proactively recall and incorporate user preferences stated earlier in conversations without explicit prompting. Implicit preferences create further difficulties for LLMs to infer user preferences. While prompting techniques, such as reminders, show promise in mitigating this performance drop, substantial room for improvement still remains. \ours{} not only highlights the current limitations of LLMs in personalized interaction but also provides a valuable resource for researchers and developers to evaluate and enhance the personalization capabilities of conversational AI systems.

\newpage
\section{Reproducibility Statement}

Our work presents a comprehensive benchmark that includes a manually curated dataset to evaluate current open-source and proprietary models (see Section \ref{sec:model_baselines}), with detailed descriptions of the model versions provided in Table~\ref{tab:model_version}. We plan to release our benchmark in the future, enabling others to reproduce our results. Additionally, we will make available the contextual turns from the Lmsys-1M-dataset used to construct inter-turn distractors. To ensure reproducibility, we also provide the prompts used for LLM-based evaluators (Claude 3 Sonnet) in Section \ref{sec:evaluator_prompts}, which are critical for obtaining performance results, as well as the detailed method descriptions and their prompts used in our experiments (Section \ref{sec:method_prompt}). These resources will facilitate further research and allow for replication of our work.

\section{Ethics Statement}
In this paper, we introduce \ours, a benchmark designed to evaluate large language models' ability to infer, memorize, and adhere to user preferences in long-context multi-session conversational settings. Our research prioritizes responsible and ethical practices, particularly concerning data privacy, data quality in terms of ethics, bias mitigation, and research integrity.
Our dataset consists of manually curated preference-query pairs spanning 20 topics, generated with assistance from AI language models. In the construction process, we have invested significant effort in manually rating and filtering these pairs to ensure quality and relevance, removing any potentially unethical preference instances. We also examined inter-conversation data from the LMSYS dataset, which contains anonymized interactions between users and language models, and removed problematic conversations from our experiments. Throughout this process, we maintained strict privacy standards, ensuring no sensitive or personal information was collected or used.
In the benchmarking process, we continuously examined the LLM's output to ensure no preference pair led to unethical responses, and we have detailed the API versions of the LLMs we benchmarked for reproducibility. In terms of future deployment, the enhancement of an LLM's ability to remember and follow user preferences might raise privacy considerations. We advocate for responsible deployment practices that protect user data.
We acknowledge that LLMs may inherit biases from their training data, potentially leading to unfair or discriminatory outputs. To address this concern, we aimed to optimize the diversity of the topics in our dataset to minimize potential biases. All results presented accurately represent our findings, supported by detailed documentation of our methodologies, such as the prompts we used. We conducted all experiments using either publicly available models or through documented commercial API access. To promote reproducibility and advance research in this field, we will make our benchmark dataset publicly available.