\input{sections/limitation}


\subsection{Detailed Related Works}
\label{sec:detailed_related_works}

\textbf{LLM Personalization and Benchmarks.}
Prior to the era of LLMs, personalization for language models mainly focused on personalized dialogue systems. These systems conditioned agents on user profiles, such as Reddit posting histories, to generate more engaging chit-chats that mimicked users' personalities or styles~\citep{zhang-etal-2018-personalizing, mazare-etal-2018-training, wu-etal-2021-personalized, zhong-etal-2022-less}. Other personalization tasks included news headline generation~\citep{ao-etal-2021-pens} and review generation~\citep{li2019towards}. With the rise of LLMs, the scope of personalization tasks has broadened. Recent works aim to personalize the LLMs themselves to embody particular personas and mimic speaking styles~\citep{vincent2023reference}. {In recent work on personalization benchmarks, \citet{salemi2023lamp} introduced the LAMP benchmark, which tests LLMs' ability to produce personalized output in specific tasks such as movie tagging and email subject generation and emphasizes explicit user profile conditioning through retrieval-augmented techniques. The RPBench-Auto benchmark~\citep{rpbench2024} focuses on character-based and scene-based role-playing tasks, evaluating models' abilities to maintain consistent personas across 80 unique characters in free-form conversations and structured narrative scenarios. TIMECHARA benchmark~\citep{ahn2024timechara} specifically addresses point-in-time character representation, examining how well models maintain temporal consistency in narratives without revealing future events or contradicting established character timelines. RoleLLM benchmark~\citep{wang-etal-2024-rolellm} introduces a framework for fine-grained role-playing across 100 diverse characters, emphasizing speaking style imitation and role-specific knowledge capture through systematic instruction tuning. Beyond benchmarking efforts, researchers have explored various personalization methods, \citet{bao2023tallrec} worked on fine-tuning LLMs for recommending items using users' past interactions. \citet{jang2023personalized} aimed to align LLMs with multi-preferences that can be combined post-training through parameter merging. \citet{zhao2023group} considered few-shot adaptation of LLMs to cater to human group preferences across demographics. However, these recent works mostly consider preferences about LLM response stylistic attributes such as conciseness or informativeness~\citep{lee2024aligning, li2024dissecting} or focus on single-turn tasks like email title generation. Our work differs from these benchmarks by focusing on adherence to more lifestyle-oriented, day-to-day user preferences and we extend to long-context, multi-turn conversations.


\textbf{Long Context LLM and Benchmarks.} Recent LLMs such as Gemini have extended the context window to millions of tokens~\citep{reid2024gemini}, enabling researchers to expand from few-shot to many-shot settings~\citep{agarwal2024many, bertsch2024context}. To keep pace with increasing context length capabilities, new datasets and benchmarks have been proposed to evaluate long-context reasoning abilities, primarily focusing on question-answering and summarization tasks~\citep{zhang2024infty, wang2024novelqa, an2023eval, li2023loogle}. For instance, L-eval~\citep{an2023eval} and LongBench~\citep{bai2023longbench} are among the first to aggregate existing benchmarks such as \citet{kovcisky2018narrativeqa, dasigi2021dataset, huang2021efficient} into long-context benchmarks, spanning tasks like question-answering, summarization, retrieval, and coding. \citet{kuratov2024babilong} designed a benchmark to assess LLMs' fact reasoning abilities across extremely long documents. LongICLBench~\citep{li2024longcontext} evaluates long in-context learning in extreme-label classification tasks, where the model needs to comprehend the entire context to understand the label space. While these ``needle-in-a-haystack" tasks test a model's ability to identify and extract relevant information, our benchmark introduces a distinct challenge of preference following, where models need to infer from implicit preferences and dynamically apply this understanding across conversation contexts rather than simply retrieving explicit preferences.
Our benchmark evaluates LLMs' long-context retrieval capabilities in a more realistic and practical setting by assessing preference-following across multi-turn conversations. It demands a comprehensive understanding of the conversational flow, enabling models to accurately infer user preferences as they evolve over dialogue and to know when and where to apply these preferences in responses.

\textbf{Instruction Following.} Recent LLMs are fine-tuned on human-annotated instruction-response pairs to enhance their instruction-following abilities. Works such as InstructGPT~\citep{ouyang2022training} have demonstrated this approach's effectiveness in enabling models to understand and deliver on tasks specified by humans. These instructions encompass a wide range of tasks and require a composition of capabilities~\citep{zhong2024law, zhou2023instruction}, including answering questions concisely, summarizing long texts, translating between languages, explaining complex topics at various educational levels, and solving mathematical or logical problems step-by-step~\citep{radford2019language,gupta2023instruction}. Our work extends this concept by considering another dimension of instruction following: the ability to infer and adhere to user preferences across multiple turns of conversation, rather than focusing solely on executing discrete tasks or queries. 


\subsection{Model version}
With \ours{}, we have evaluated the following large language models in our experiments with their versions in Table~\ref{tab:model_version}.
\begin{table}[h]
\centering
\caption{The LLMs benchmarked and their respective versions used for evaluation.}
\begin{tabular}{l|l}

\textbf{Model Name} & \textbf{Version} \\
\hline
Claude 3 Sonnet & anthropic.claude-3-sonnet-20240229-v1:0 \\
Claude 3 Haiku & anthropic.claude-3-haiku-20240307-v1:0 \\
Claude-3.5-Sonnet & anthropic.claude-3-5-sonnet-20240620-v1:0\\
Mistral 7b Instruct & mistral.mistral-7b-instruct-v0:2 \\
Mistral 8x7b Instruct & mistral.mixtral-8x7b-instruct-v0:1 \\
LLaMA 3 8b Instruct & meta.llama3-8b-instruct-v1:0 \\
LLaMA 3 70b Instruct & meta.llama3-70b-instruct-v1:0 \\
GPT-4o & GPT-4o-2024-08-06 \\
o1-preview & o1-preview-2024-09-12\\
Gemini-1.5-Pro & Gemini-1.5-Pro-latest as of Sep 30th, 2024. \\


\end{tabular}

\label{tab:model_version}
\end{table}
\subsection{Methods Description}
\label{sec:method_prompt}
We extensively evaluate a variety of state-of-the-art large language models and investigate five methods for each model:

\paragraph{\textbf{Zero-shot:}} The default case, where the LLM directly answers the user's query without any additional prompting.

\paragraph{\textbf{Reminder:}}\label{parag:reminder} Before answering the question, the LLM is provided with a reminder sentence to consider the user's previously stated preference in its response. The reminder used is:

\begin{framed}
\fontsize{9pt}{12pt}\selectfont
"In your response, please ensure that you take into account our earlier discussion, and provide an answer that is consistent with my preference."
\end{framed}

This reminder is added right after the question and before the LLM's response.

\paragraph{\textbf{Self-Critic:}}\label{parag:selfcritic} The LLM generates an initial \textit{zero-shot} response to the question, critiques whether it has followed the user's preference, and then generates a revised response considering the critique. This self-critic process is akin to \textit{Intrinsic Self-Correction} as termed in ~\cite{huanglarge}.

The critique and revision request prompts are as follows:

\begin{framed}
\fontsize{9pt}{12pt}\selectfont
\textbf{Critique Request:} Review your previous response to the user's question in the last conversation turn. Check if the response adheres to or violates any user preferences stated earlier in the conversation that relate to this query. Provide a critique on how well those preferences were followed in 2 sentences. Answer in this format:

\texttt{Critic:} [Your Critique Here]

\textbf{Revision Request:} Based on your critique, please rewrite your previous response to align more closely with the user's earlier stated preferences. Answer the question again:

\texttt{Response:} [Revised Response Here]
\end{framed}

\paragraph{\textbf{Few-Shot Chain-of-Thought (CoT):}}\label{parag:cot} The LLM is given several few-shot examples (in our experiments, we used 5-shot) of chain-of-thoughts that demonstrate how to follow the user's preference before answering the question. The chain of thought prompt is as follows:

\begin{framed}
\fontsize{8.7pt}{12pt}\selectfont
\textbf{When answering a user's question, a good assistant should carefully consider the user's stated preferences and tailor the response accordingly.}

Example 1:  
User's preference: "I have an intense phobia of bugs and insects. What outdoor activities would you recommend for the summer?"  
\textbf{Good assistant response:} "Considering your phobia of bugs and insects, I'll focus on recommending activities that minimize encounters with insects. Let's explore outdoor options with fewer bugs, like beaches, pools, urban areas, well-maintained park trails, and activities during cooler times of day. I'll avoid suggesting things like camping, off-trail hiking, picnicking, or activities in dense forests. Here are some summer outdoor activities you might enjoy: spending a day at the beach swimming and playing sports, exploring outdoor malls or pedestrian areas in the city, hiking on maintained trails in parks (avoiding dense woods), attending evening outdoor concerts or movies, and enjoying water activities like kayaking or paddle boating on lakes or rivers. These options should help you enjoy the outdoors while minimizing encounters with insects."

Example 2:  
User's preference: "I'm a vegan looking for new dinner recipes. Any ideas?"  
\textbf{Good assistant response:} "As a vegan, you'll need plant-based recipes without animal products. I'll focus on nutrient-rich, diverse ingredients to ensure balanced meals. Consider: quinoa and black bean burrito bowls, lentil and vegetable curry, roasted vegetable and hummus wrap, or zucchini noodles with avocado pesto. These options offer protein, fiber, and essential nutrients while adhering to vegan principles."

...

Example 5:
User's preference: "I'm sensitive to caffeine but need an energy boost in the mornings. Any suggestions?"
\textbf{Good assistant response:} "Given your caffeine sensitivity, we'll explore natural, caffeine-free energy boosters. Focus on lifestyle changes and alternatives to caffeinated drinks. Try: starting your day with a brisk walk or light exercise, drinking a glass of water with lemon, eating a protein-rich breakfast, or trying herbal teas like peppermint or ginger. Avoid coffee, energy drinks, and black tea. These alternatives can help increase alertness without the jitters or crashes associated with caffeine."


\textbf{Now, please answer the following question while considering my preferences (not the user preferences in the examples above), which I have stated either explicitly or implicitly in our previous conversation:}
\end{framed}

\paragraph{\textbf{Retrieval-Augmented Generation (RAG):}}\label{parag:rag} 
We employ a sentence embedding model, which is \textit{'princeton-nlp/sup-simcse-roberta-large'}~\citep{gao2021simcse}, to retrieve the most relevant conversation exchanges based on similarity to the current query. The top five most relevant exchanges are then presented to the LLM as contextual information to guide its response. 

The prompt used in this method is structured as follows, here we show RAG with top-5 retrieved exchanges:

\begin{framed}
\fontsize{9pt}{12pt}\selectfont
\texttt{Before answering my question, please consider the following context from our previous conversations.  
These are the 5 most relevant exchanges that we had previously, which may contain information about my preferences or prior discussions related to my query:}  

\#Start of Context\# \\
exchange 1. [Most relevant exchange 1] \\
exchange 2. [Most relevant exchange 2] \\
exchange 3. [Most relevant exchange 3] \\
exchange 4. [Most relevant exchange 4] \\
exchange 5. [Most relevant exchange 5] \\
\#End of Context\#

\texttt{Please use this context to inform your answer and adhere to any preferences I've expressed that are relevant to the current query. Note that not all contexts are useful for answering my question and there may be no context that is useful. Now, please address my question:}
\end{framed}


\subsection{Classification Task Correlation Plot}

\begin{figure}[H]
    \centering
    
        \includegraphics[width=0.6\textwidth]{figs/mcq/mcq_correlation_plot_all_models.pdf}


        \caption{Correlation analysis between LLM-based preference following accuracy in generation tasks and classification accuracy in classification MCQ tasks across all models and methods. Across 6 models, 5 methods and 12 turns, each point on the scatter plot is averaged over 20 topics. A correlation coefficient of 0.73 suggests a strong alignment between the two evaluation approaches.}
         \label{fig:mcq_correlation}

   
   
\end{figure}


\subsection{Proprietary LLMs performance comparison}
The performance comparison shown in Table~\ref{tab:sotamodel_comparison_more} shows that current SoTA proprietary LLMs struggle to proactively follow user preferences in short-turn, zero-shot settings, where no explicit prompting is provided. Only when using the Reminder method, which explicitly reinforces the need to adhere to preferences, do these models show improvement; however, accuracy still deteriorates with longer context lengths. Note that the GPT-o1-preview results may not be directly comparable to other models as it may require additional test-time computation with a “thinking” phase. We did not evaluate GPT-4o with 300 turns due to budget limit.

\begin{table}[ht]
    \centering

    \caption{Comparison of preference-following accuracy across SoTA LLMs evaluated at two context lengths with two methods: Zero-shot and Reminder (best prompting method), evaluated at two context lengths, on the \textit{travel restaurant} topic and on generation task.
    }


    \scalebox{0.7}{
    \begin{tabular}{l|cc|cc}

         & \multicolumn{2}{c|}{\textbf{10 Turns / $\sim$3k tokens}} & \multicolumn{2}{c}{\textbf{300 Turns / $\sim$103k tokens}} \\
        \cline{2-5}
                       & \textbf{Zero-shot} & \textbf{Reminder} & \textbf{Zero-shot} & \textbf{Reminder} \\
        \hline
        Claude-3.5-Sonnet & 0.07 & 0.45 & 0.02 & 0.02 \\
        Gemini-1.5-Pro    & 0.07 & 0.91 & 0.09 & 0.05 \\
        o1-preview*   & \textbf{0.50}  & \textbf{0.98}    &\textbf{0.14} & \textbf{0.98} \\
       GPT-4o & 0.07 & \textbf{0.98} & 0.05 & 0.23 \\
       % Claude-3-Sonnet & 0.05 & 0.96 & 0.04 & 0.36\\
       % Claude-3-Haiku & 0.05 & 0.68 & 0.02 & 0.02\\
       % Llama3-70B & 0.11 & 0.37 & - & - \\
       % Llama3-8B & 0.00 & 0.57 & - & - \\
       % Mistral-7B & 0.03 & 0.75 & - & - \\
       % Mistral-8x7B & 0.08 & 0.84 & - & - \\

    \end{tabular}}
    \label{tab:sotamodel_comparison_more}

\end{table}
\subsection{RAG method ablation and analysis}
\label{sec:rag_ablation}
To examine how RAG (Retrieval-Augmented Generation) improves performance, we compare the RAG sentence transformer's ground truth retrieval accuracy in explicit preference settings. We consider it accurate if the explicit preference stated in a turn is extracted by the sentence transformer and sent to the LLM as a reference.
As shown in Figure~\ref{fig:rag_ablation}, we find that when k=5, the performance is among the best, similar to when k=10. Although when k=10 the ground truth retrieval accuracy improves, the preference following performance does not reflect this improvement. This may suggest that for RAG, providing more turns of exchanges as reference might serve as another form of distraction, potentially harming performance. Based on these findings, we select k=5 to report this method's results in the main paper. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/rag_ablation.png}
\caption{Comparison of RAG retrieval accuracy versus RAG method's performance in preference following across different Top-K values.}
    \label{fig:rag_ablation}
\end{figure}

\subsection{Cross-Topic Performances}

We present an extensive benchmark study conducted across 20 diverse topics, on 6 Large Language Models at three fixed conversation lengths: 10, 70, and 300 turns. These correspond to approximate context lengths of 3k, 23k, and 100k tokens, respectively, spanning from the user's initial stated preference to their final query. The user's stated preference is positioned at the beginning of each conversation. We show results across three preference forms and evaluates five different methods. The detailed results are presented in Tables \ref{tab:explicit_20topics_10}, \ref{tab:explicit_20topics_70}, and \ref{tab:explicit_20topics_300} for explicit preferences; Tables \ref{tab:implicit_20topics_10}, \ref{tab:implicit_20topics_70}, and \ref{tab:implicit_20topics_300} for implicit choice-based preferences; and Tables \ref{tab:implicit_persona_20topics_10}, \ref{tab:implicit_persona_20topics_70}, and \ref{tab:implicit_persona_20topics_300} for implicit persona-based preferences. This comprehensive set of results allows for a thorough examination of LLM performance across various topics, conversation lengths, and preference revelation forms.



\input{sections/table_explicit}
\input{sections/tabel_topic_results}
\input{sections/table_implicit_persona}

\subsection{Examples of Data in \ours{}}

\input{sections/preference_form_examples}
\subsection{Absolute Error Types Trends}
Complementing the pie charts presented earlier for error type distributions at fixed turns, Figure~\ref{fig:errortype_turns} illustrates how error type frequencies evolve as conversations lengthen across models and methods. Our analysis reveals that advanced techniques significantly reduce the proactiveness error \textit{Preference-Unaware Violation} errors compared to zero-shot approaches. However, as conversations extend, we observe a notable increase in both \textit{Hallucination of Preference Violation} and \textit{Unhelpful} errors. Retrieval-Augmented Generation (RAG) techniques prove particularly effective in mitigating hallucination errors. Interestingly, Claude models exhibit a higher tendency to refuse responses, resulting in elevated \textit{Unhelpful} error rates compared to Mistral models, especially in longer conversations. These findings highlight the complex interplay between model capabilities, conversation length, and error types, underscoring the challenges in maintaining consistent performance across extended conversations.

\label{sec:absolute_errortypes}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/error_types/paper_all_models_error_types_single_pref.pdf}
    \includegraphics[width=0.9\textwidth]{figs/error_types/paper_all_models_error_types_single_pref_cot.pdf}
    \includegraphics[width=0.9\textwidth]{figs/error_types/paper_all_models_error_types_single_pref_selfcritic.pdf}
    \includegraphics[width=0.9\textwidth]{figs/error_types/paper_all_models_error_types_single_pref_remind.pdf}
    \includegraphics[width=0.9\textwidth]{figs/error_types/paper_all_models_error_types_single_pref_rag_5.pdf}
\caption{Trends in Absolute Error Counts Across Conversation Turns for Six LLMs and Five Methods.}
    \label{fig:errortype_turns}
\end{figure}

\begin{figure*}[H]

\begin{framed}
\fontsize{6.5pt}{9pt}\selectfont
\texttt{You are a helpful assistant. You are helping a user create scenarios to evaluate if an AI assistant properly considers the user's stated preferences. You will generate:\\}

\texttt{1. Preference: The user explicitly stating a specific and unique like or dislike (I prefer xx over xx; I strong dislike xx; I only consider xx; etc). This preference should be firm and not a common one, making it important for the assistant to remember. This preference should be unique and firm in 1 concise sentence. Think of varied human preferences across different angles.\\
2. Question: The user asking for a task or assistance for THEMSELVES, using 'I' or 'me'. A question or request for assistance that is carefully worded to avoid contradicting or revealing the stated preference. The question should be challenging to answer without considering the preference, but it must not conflict with the preference.\\
3. A short explanation (1-2 sentences) of why the natural way of completing the requested task could conflict with the stated preference and how the assistant should answer/recommend while adhering to the user's preference."\\}

\texttt{Rubric:\\
Please generate such preference question pairs with high Violation probability:\\
High violation means: P(answer|question) >> P(answer|preference, question), which means without knowing the preference, naturally answering the question can easily violate the user’s preference.\\}

\texttt{High probability of violation example:\\
\phantom{xx}Preference: "I'm a vegetarian, so I avoid eating meat or animal products.",\\
\phantom{xx}Question: "What are some must-try local dishes when visiting Texas you would suggest for me?"\\
\phantom{xx}Explanation: Texas is famous for BBQ and meat-heavy dishes, so suggesting typical Texas fare like brisket, ribs, etc. would violate the stated vegetarian preference with high chance.\\}

\texttt{Low probability of violation example:\\
\phantom{xx}Preference: "I prefer companies that allow unlimited sick days."\\
\phantom{xx}Question: "Which companies have the most generous sick leave policies?"\\
\phantom{xx}Explanation: There is a low probability of violation since the question directly matches what the preference is looking for.\\}


\texttt{Don't generate contradictory or obvious pairs such that the question either directly contradicts the user's preference or is so aligned that it is not challenging.\\
Don't generate pairs such that providing recommendations in line with the preference is either impossible or too straightforward.\\
Don't generate question or preference that lacks sufficient information, such as location or specifics.\\}

\texttt{Think about 20 realistic scenarios that will have high Violation probability related to the topics of [insert topic details]. The scenarios should be realistic, innovative, creative and challenging.
Please answer in the format of:\\}
\texttt{<root>\\
<task><preference>\\
\phantom{xx}<content>[Preference]</content>\\
</preference>\\
<question>\\
\phantom{xx}<content>[Question]</content>\\
</question>\\
<explanation>\\
\phantom{xx}<content>[Explanation]</content>\\
</explanation></task>\\
...\\
</root>\\
}
\end{framed}
\caption{The prompt used for the generation of preference and question pairs. We repeatedly called LLM using this prompt and conditioned on different topic details for each topic to get diversified and realistic preference and question pairs. In practice, we gave varying amounts and different examples to different topics for better generation quality. We used XML format generation to get more controlled and easier parsing of the generation.}
\label{prompt: generation}
\end{figure*}

\subsection{Prompts used for LLM-based evaluation for 4 error types's binary checks}
\label{sec:evaluator_prompts}
\input{sections/prompt}



\begin{figure*}[h!]

\begin{framed}
\fontsize{8.5pt}{9pt}\selectfont



\texttt{Given the user's preference: [preference] and their question [conversation question.] \\}

\texttt{Think of 4 possible recommendation options to answer this user's question.
You should make sure one option aligns with the user's preference, while all other three options violate it.\\}

\texttt{For each option, just provide the option itself without any context sentences. Begin with sentence like "I can suggest a few options for you:" and then list the options.\\}

\texttt{Generate in this exact format:\\
<options>\\
<option>\\
<content>[Option 1 that aligns with the user's preference]</content>\\
</option>\\
<option>\\
<content>[Option 2 that violates/not following the user's preference but in an ethical way]</content>\\
</option>\\
<option>\\
<content>[Option 3 that violates/not following the user's preference but in an ethical way]</content>\\
</option>\\
<option>\\
<content>[Option 4 that violates/not following the user's preference but in an ethical way]</content>\\
</option>\\
</options>\\}

    \end{framed}
\caption{The prompt used for expanding an explicit preference into a multiple-choice implicit preference conversation. This is the stage where options are generated in a multiple-choice implicit setting. Note that the initial step earlier than this generation is to provide a preference and generate a related conversation question (note that this question is any related question, different from the queries in our explicit dataset, and does not need to be challenging in the sense that answering it naturally would violate the preference). }
\label{prompt: implicit_gen_options}
\end{figure*}

\clearpage % Clear any pending floats
\newpage   % Force a new page
\suppressfloats[t] % Prevent floats from appearing at the top of this page

\subsection{Attention Score Visualization Analysis}
\label{sec:attention_visual}
\subsubsection{Attention Score Changes After Supervised Fine-tuning}

In Section~\ref{sec:finetune}, we show that fine-tuning a Mistral 7B model on our dataset improved its preference-following capabilities and generalization to both unseen topics and longer contexts. To understand the mechanisms behind this improvement, we analyze the changes in attention patterns before and after supervised fine-tuning. We compute attention scores of response tokens given the input context, which consists of user preference, query, and conversational context. For each example, we calculate the preference region's relative attention by summing the attention scores over preference-related tokens and normalizing by the total attention across all input tokens. This metric allows us to quantify how much the model focuses on preference information during generation.

Figure~\ref{fig:attention_score_examples_sft} presents four representative examples from our test set, where we visualize the attention scores of generated tokens over the input prompt. The preference region, which is the tokens related to user preference, is highlighted in grey for clarity. The visualizations reveal a consistent pattern: after SFT, the model exhibits notably increased attention to the preference region. While for other context, there is no pattern in the changes of attention scores. We further analyzed 100 unseen test examples, and Figure~\ref{fig:histogram_of_preference_attention_scores_improve} shows that increased preference region attention is consistent across examples, with improvements up to 4.97\%, demonstrating SFT model's enhanced attention to preference information.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/attention_comparison_overlapped_example_1.pdf}
        % \caption{Caption for figure 1}
        \label{fig:1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/attention_comparison_overlapped_example_3.pdf}
        % \caption{Caption for figure 2}
        \label{fig:2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/attention_comparison_overlapped_example_5.pdf}
        % \caption{Caption for figure 3}
        \label{fig:3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/attention_comparison_overlapped_example_8.pdf}
        % \caption{Caption for figure 4}
        \label{fig:4}
    \end{subfigure}
    \caption{Attention score visualization comparing pre- and post-SFT model behavior on test examples, on 4 explicit preference examples. Each plot shows attention scores of generated tokens over the input prompt, with the preference statement region shaded in grey. The visualizations demonstrate consistently increased attention to preference-related information after SFT, while attention patterns for other context tokens remain largely unchanged.}
    \label{fig:attention_score_examples_sft}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/preference_increase_distribution.pdf}
    \caption{Distribution of improvements in preference region attention after SFT across 100 test examples. The histogram shows consistent positive changes in the model's attention allocation to preference-related information, with improvements reaching up to 4.97\%.}
    \label{fig:histogram_of_preference_attention_scores_improve}
\end{figure}
\subsubsection{Attention Score Analysis Across Preference Forms}
We investigated the attention score patterns for implicit and explicit preference forms using the open-source Mistral 7B model, focusing particularly on choice-based implicit preferences. Despite our earlier findings that LLMs perform worse with implicit preference forms, the attention score visualization in Figure~\ref{fig:attention_score_prefforms} reveals no obvious differences in attention patterns between implicit and explicit preferences. Implicit preference and explicit preference have different token lengths, adding difficulty in comparing their attention scores visually. We hypothesize that the performance degradation with implicit preferences may not solely stem from limitations in \textbf{\textit{Long-Context Retrieval}} ability, but rather from the model's \textbf{\textit{Preference Inference}} as defined in Sec~\ref{sec:formulation}, where the Preference Inference means the capacity to accurately infer user preferences through dialogue, whether explicitly stated or implicitly revealed. We hypothesize Preference Inference has more complexity that likely involves deeper internal mechanisms beyond what attention score visualization can reveal.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figs/attention_comparison_forms.pdf}
\caption{Comparison of attention score patterns between implicit and explicit preference forms across four example pairs. Analysis of preference-related regions shows no significant systematic differences in attention distribution between the two forms.}
\label{fig:attention_score_prefforms}
\end{figure}


\subsection{Full Data Construction Methodology of \ours{}.}
\label{sec:data_generation_detail}
}

The \ours{} dataset comprises 1,000 unique preferences, each expressed in three forms: one explicit and two implicit, yielding a total of 3,000 preference-question pairs. Our data generation process consists of three main components: (1) Generation of Explicit Preferences, (2) Generation of Implicit Choice-Based Preferences, and (3) Generation of Implicit Persona-Driven Preferences. We detail the methodology for each component below:

\paragraph{\textbf{Step 1: Generation of Explicit Preferences}}



We developed a pipeline to generate and filter high-quality preference-question pairs. The process consists of the following steps: 
\begin{enumerate}[leftmargin=0.3in] 
\item \textbf{Topic Generation:} We began by generating and selecting 20 distinct topics (shown in Figure~\ref{fig:topic}) that are diverse and commonly encountered during advice-seeking or recommendation-focused conversations with chatbots. For each topic, we crafted detailed descriptions and subtopics to ensure comprehensive coverage of various preference domains, utilizing Claude 3 for assistance.

\item \textbf{Large-scale Sampling of Preferences and Queries:} Using Claude 3 Sonnet, we generated approximately 10,000 preference-question pairs. Each pair comprises an explicit preference statement and a related query (e.g., preference: ``I strictly avoid restaurants that serve foods containing gluten due to a severe gluten intolerance,'' question: ``I'll be visiting Rome soon. What are some must-try local restaurants you'd recommend for me?''). We also generated explanations for why each query is challenging to answer while respecting the stated preference. Through extensive prompt engineering, we optimized the generation process for quality while filtering out unethical content by specifying constraints in the prompt. The output was structured in JSON format to facilitate subsequent processing.

\item \textbf{Extensive Manual Filtering Process:} We implemented a multi-stage filtering approach involving human labelers and LLM-based evaluators (using GPT-4o, Claude 3 Sonnet) to evaluate each preference-question pair based on the following criteria: 
\begin{itemize}
    \item \textbf{Validity Assessment:} Labelers discarded samples exhibiting any of the following issues:
   \begin{itemize} 
       \item Questions that directly contradict the user's preference 
       \item Questions already aligned with the user's preference, requiring no additional consideration 
       \item Questions impossible to answer due to insufficient information (e.g., missing location or specifics) 
   \end{itemize}
    \item \textbf{Automatic Violation Rate Analysis:} We sampled responses from 5 different LLMs without providing the preference to assess the preference-unaware violation rate. Pairs with higher violation rates were prioritized to create a more challenging dataset.
    
    \item \textbf{Automatic In-Context Difficulty Rating:} We developed a rating prompt using 50 human-labeled examples as in-context demonstrations. Each example included human ratings along two dimensions: 
    \begin{itemize} 
        \item \textbf{Violation Probability:} [High, Medium, Low] (a higher rating indicates the preference is easier to violate without knowledge of it) 
        \item \textbf{Reasoning Difficulty:} [High, Low] (indicates whether, even when aware of the preference, answering the query in a preference-following way requires reasoning) 
    \end{itemize} 
Multiple iterations of prompt tuning and example selection ensured reliable ratings. We will release the data generation prompts in our repository. 
    \end{itemize}

\item \textbf{Final Selection:} The filtering process yielded approximately 3,000 high-quality pairs. We then manually selected approximately 50 preferences per topic, resulting in a final dataset of 1,000 high-quality explicit preference-question pairs. \end{enumerate}


\paragraph{\textbf{Step 2: Generation of Implicit Choice-Based Preferences}}

Building upon the explicit preferences and to develop more challenging preference types for preference following, we created two-turn conversations incorporating multiple-choice questions, where the user's preference will be implicitly revealed through option selection. The generation process followed these steps using Claude 3 Sonnet: (1) For each explicit preference-query pair, we generated a simpler, related query that differs from the final test query. (2) We created four options for each query, ensuring that only one option aligns with the user's preference while the other three violate it. (3) We constructed two-turn conversations where the user selects the single option that aligns with their preference. (4) Each conversation concludes with a brief assistant acknowledgment that avoids explicitly restating the user's preference. 


\paragraph{\textbf{Step 3: Generation of Implicit Persona-Driven Preferences}} 

To create more natural preference expressions within extended conversations, we aimed to extend the preference revelation over longer conversations. However, simply expanding an explicit preference into a long conversation can be challenging and may inadvertently reduce task difficulty by reinforcing the preference across multiple turns. Therefore, we decided to craft conversations where the topic mainly revolves around a persona, and the preference is only briefly mentioned. We developed persona-augmented preference conversations as follows: 
\begin{enumerate}[leftmargin=0.3in] 
\item We first generated and filtered 100 distinct and diverse personas using Claude 3.5 Sonnet, ensuring that the personas were topic-independent to prevent preference conflicts. 
\item For each of the 1,000 explicit preferences, we randomly assigned one persona and we make sure the persona does not conflict with or reveal the preference. Using Claude 3.5 Sonnet, we then generated 5--8 turn conversations that incorporated both the explicit preference and the assigned persona. The primary conversation focus centered on persona-related inquiries rather than explicit preference discussion. 
\end{enumerate}

This three-component methodology resulted in a diverse dataset of preference following, ranging from explicit statements to naturally embedded implicit preferences within extended conversations. Our dataset will be released along with the propmts used in data construction above.

\subsection{How does finetuning on \ours{} generalize to implicit preference settings?}

In Section~\ref{sec:finetune}, we demonstrated that fine-tuning a Mistral 7B model on our dataset enhanced its preference-following capabilities and generalization to both unseen topics and longer contexts. While the training dataset consisted solely of Mistral model's responses in explicit preference settings using the reminder baseline—with no intervening contextual turns—we now investigate whether the trained model generalizes effectively to implicit preference settings. 

As shown in Table~\ref{tab:sft_implicit}, preference fine-tuning improves performance on implicit preference following tasks. This generalization suggests that training on explicit preferences not only enhances the model's attention to user preferences but also strengthens its \textbf{\textit{Preference Inference}} capability (as defined in Section~\ref{para:key_capabiilties}).

\begin{table}[h]
    \centering
    \caption{Preference following accuracy (\%) on implicit settings before and after supervised fine-tuning (SFT). We evaluate the model's ability to follow preferences in two implicit scenarios with 5 turn contextual conversation. We show the results across over 100 preferences instances over 2 topics in the zero-shot setting. We find preference finetuning brings more improvements for \textit{Implicit Persona-Driven} preferences.}
    \begin{tabular}{llcc}
    \toprule
    Topic & Model & \multicolumn{2}{c}{Preference Following Accuracy (\%)} \\
    \cmidrule(lr){3-4}
    & & Implicit Persona-Driven & Implicit Choice-Based \\
    \midrule
    \multirow{2}{*}{Travel Restaurants} 
    & Before SFT & 1.79 & 3.57 \\
    & After SFT & \textbf{55.36} & \textbf{14.29} \\
    \midrule
    \multirow{2}{*}{Travel Hotels}
    & Before SFT & 14.81 & 11.11 \\
    & After SFT & \textbf{74.07} & \textbf{51.85} \\
    \bottomrule
    \end{tabular}
    \label{tab:sft_implicit}
\end{table}



\subsection{Lost in the Middle: Impact on Preference Following}
Recent work has shown that language models struggle to effectively use information placed in the middle of their context window, showing better performance when important information appears at the beginning or end~\citep{liu-etal-2024-lost}. Following this finding, we investigate whether this ``lost in the middle" phenomenon extends to preference following behavior. As shown in Figure~\ref{fig:ushape}, we experiment with Claude 3 Sonnet and Claude 3 Haiku across four diverse topics by inserting the preference in different locations of a fixed 100 turn conversation. We observe that preference following significantly degrades when preferences are placed in the middle of the conversation (around turn 50) compared to when they are positioned at the beginning or end. This phenomenon preserves across 2 models with different sizes. This aligns with~\citet{liu-etal-2024-lost} findings about LLMs' difficulty in accessing mid-context information.  
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figs/topic_comparison_2x2_ushape.pdf}
    \caption{Preference following accuracy across different preference insertion locations in a fixed 100-turn conversation for Claude 3 Sonnet and Claude 3 Haiku, tested on four topics. This indicates the ``lost in the middle" phenomenon extends beyond factual retrieval tasks to preference following.}
    \label{fig:ushape}
\end{figure}




\subsection{Additional Results on Dynamic Preference Following}
In Section~\ref{sec: dynamic_prefs}, we demonstrated that inserting multiple preferences and conflicting preference pairs in conversations improved preference following performance for Claude 3 Sonnet and Claude 3 Haiku. We conduct additional experiments across multiple models to further validate the observations. As shown in Figure~\ref{fig:more_multipleprefs}, the positive correlation between the number of preferences and preference following accuracy extends to Mistral 8x7b and Mistral 7b. We hypothesize this is because inserting multiple preferences throughout the conversation reinforces the model's attention to user preferences, as the LLM allocates more attention to user preferences relative to other unrelated contextual information.
Further, we extend conflicting preference experiment to Mistral models, as shown in Figure~\ref{fig:conflict_mistral}. Overall, three models (Claude 3 Sonnet, Claude 3 Haiku, and Mistral 7b) demonstrate improved performance with conflicting preference pairs. However, Mistral 8x7b exhibits similar performance between conflicting and non-conflicting pairs, with a slight advantage for non-conflicting pairs. This suggests the effect is model-dependent and our findings still holds that conflicting preferences do not necessarily harm performance. We attribute this phenomenon to a topic-reinforcement effect: although the preferences conflict, they address the same topic domain, potentially strengthening the LLM's memory of the preference context and leading to higher accuracy in preference following. For example, when a user expresses \textit{``I prefer detailed responses when I ask for paper summarization"} and later states \textit{``I prefer concise responses when I ask for paper summarization"}, these contradictory preferences nonetheless reinforce the LLM's attention to response length as a significant preference dimension. 

%{\color{red}[in fact, this hypothesis might also be tested using the attention plots you had before?]} yes


\begin{figure}[h]
\centering
    \begin{minipage}{0.48\textwidth}
    \centering
         \includegraphics[width=0.96\textwidth]{figs/multi_pref_adherence_xaxis_prefs_combined.pdf}
\caption{Preference following accuracy generally improves with more stated preferences across Mistral 8x7b, Mistral 7b, and Claude 3 models. Results shown with 50-turn inter conversation inserted (Mistral) and 80-turn inter conversation inserted (Claude 3 Sonnet) due to context length limits.}

\label{fig:more_multipleprefs}
    \end{minipage}
     \hspace{0.01\textwidth}
\begin{minipage}{0.48\textwidth}
    \includegraphics[width=0.96\textwidth]{figs/model_conflict_results_ba100_Reminder_mistral.pdf}
\caption{Effect of adding conflicting versus non-conflicting preferences on adherence. The red bar indicates the performance when only the original preference is present. Results are averaged over five topics using a fixed 100-turn conversation.}
\label{fig:conflict_mistral}
\end{minipage}
\end{figure}


\subsection{Human Evaluation of the LLM-based Evaluator}
To validate the reliability of our LLM-based evaluation approach, we conducted a comprehensive human evaluation study comparing human judgments against Claude 3 Sonnet's assessments. We randomly sampled 100 evaluations for each preference form, encompassing diverse scenarios across all models, baselines, and conversation turns. Table~\ref{tab:human_llm_agreement} presents the agreement rates between human annotators and the LLM evaluator across 4 different evaluation checker as defined in section~\ref{tab:error_types}. The results demonstrate strong alignment between human and LLM judgments, with particularly high agreement rates in detecting helpful responses and hallucinations.


\begin{table}[h]
\centering
\caption{Human-LLM agreement rates across different error checker as well as the final preference following accuracy. We randomly sampled 100 evaluations from each preference form and calculated the agreement rate between human annotators and the LLM evaluator in their judgments.}
\begin{tabular}{lcccc}
\toprule
\textbf{Error Checker} & \textbf{Explicit} & \textbf{Implicit} & \textbf{Implicit} \\
& \textbf{Preference} & \textbf{Choice-based} & \textbf{Persona-driven} \\
\midrule
Violate Preference? & 0.92 & 0.86 & 0.95 \\
Acknowledge Preference? & 0.88 & 0.90 & 0.97 \\
Hallucinate Preference? & 0.98 & 0.96 & 0.92 \\
Helpful Response? & 0.96 & 0.93 & 0.90 \\
\midrule
Preference Following Accuracy & 0.97 & 0.92 & 0.96 \\
\bottomrule
\end{tabular}

\label{tab:human_llm_agreement}
\end{table}