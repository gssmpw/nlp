
\section{Related Work}
\textbf{LLM Personalization and Benchmarks.} Early personalization efforts focused on dialogue systems mimicking user styles~\citep{zhang-etal-2018-personalizing, mazare-etal-2018-training, wu-etal-2021-personalized, zhong-etal-2022-less} and tasks like news headline~\citep{ao-etal-2021-pens} and review generation~\citep{li2019towards}. Recent LLM personalization benchmark works include LAMP~\citep{salemi2023lamp}, which emphasizes explicit user profile conditioning through retrieval-augmented techniques, RPBench-Auto's character-based role-playing tasks~\citep{rpbench2024}, TIMECHARA's temporal consistency in character representation~\citep{ahn2024timechara}, and RoleLLM's fine-grained role-playing framework~\citep{wang-etal-2024-rolellm}. While these often focus on stylistic preferences or single-turn tasks~\citep{lee2024aligning, li2024dissecting, jang2023personalized, zhao2023group}, our work addresses lifestyle preferences and extend to long-context, multi-turn conversations. 


\textbf{Long Context LLM and Benchmarks.} With extended context windows~\citep{reid2024gemini}, long-context LLMs have emerged~\citep{agarwal2024many, bertsch2024context}. Existing benchmarks primarily evaluate information retrieval capabilities through tasks requiring models to locate specific facts or answers~\citep{zhang2024infty, wang2024novelqa, an2023eval, li2023loogle}, including tasks like question-answering, retrieval, fact reasoning, and coding~\citep{an2023eval, bai2023longbench, kovcisky2018narrativeqa, dasigi2021dataset, huang2021efficient, li2024longcontext, kuratov2024babilong}. While these ``needle-in-a-haystack" tasks test a model's ability to identify and extract relevant information, our benchmark introduces a distinct challenge of preference following, where models need to infer from implicit preferences and dynamically apply this understanding across conversation contexts rather than simply retrieving explicit preferences.


\textbf{Instruction Following.} Fine-tuning on human-annotated instruction-response pairs has enhanced LLMs' instruction-following capabilities, as seen in work like InstructGPT~\citep{ouyang2022training}. These models perform a broad range of tasks, including summarization, translation, and problem-solving~\citep{zhong2024law, zhou2023instruction}. While existing benchmarks focus on executing discrete instructions, our work extends this paradigm by emphasizing the inference and adherence to user preferences across multiple conversation turns. Importantly, we also assess \textit{Personalization Proactiveness}, which is the initiative and ability to know when and where to apply user preferences, moving beyond discrete task execution. Due to space constraints, we provide a more comprehensive discussion of Related Work in Section~\ref{sec:detailed_related_works}.

