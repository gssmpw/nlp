\vspace{-1em}
\section{Experiments}

Our experiments encompassed a comprehensive evaluation and analysis of both open-source and proprietary state-of-the-art LLMs across multiple dimensions, including preference forms, task types, various methods to improve preference following, an examination of how LLMs adapt to multiple and dynamically changing preferences, and fine-tuning open-source models to enhance preference following. We aim to investigate the following questions:
\vspace{-2mm}
\begin{itemize} [leftmargin=.2in]
\item What is the performance of current SOTA LLMs in multi-session long-context preference following with zero-shot, prompting and RAG baselines? (\S\ref{sec: explicit pref}, \S\ref{sec: implicit pref}, \S\ref{sec: mcq task})

\item How do LLMs performance change with different forms of user preference expressions? (\S\ref{sec: pref forms comparison})

\item What causes LLMs to fail in preference following, and what are the prevalent error types? (\S\ref{sec: errortype})

\item How capable are LLMs in simultaneously accommodating multiple user preferences and adapting to dynamically changing user preferences? (\S\ref{sec: dynamic_prefs})

\item How does finetuning on \ours{} improve the preference adherence ability of LLMs? (\S\ref{sec:finetune})

\end{itemize}

\subsection{Models and Methods}\label{sec:model_baselines} We extensively evaluate a variety of state-of-the-art LLMs, including Claude 3 Sonnet, Claude 3 Haiku, Mistral 7b Instruct, Mistral 8x7b Instruct, LLaMA 3 8b Instruct, and LLaMA 3 70b Instruct. We also assess more recent models Claude 3.5 Sonnet, GPT-o1-preview, and Gemini-1.5-pro in specific settings. We investigate methods to explicitly help LLMs focus on the preference-following task, with the aim of understanding if they can proactively adapt to user preferences: (1) \textbf{Zero-shot:} The default case, where the LLM directly answers the question without any additional prompting. (2) \textbf{Reminder:} Before answering the question, the LLM is provided with a reminder sentence to consider the user's previously stated preference in answering. (3) \textbf{Self-Critic:} The LLM generates an initial \textit{zero-shot} response to the question, critiques whether it has followed the user's preference, and then generates a revised response taking the critique into account, similar to \textit{intrinsic self-correction}~\citep{huanglarge}. (4) \textbf{Few-Shot Chain-of-Thought (CoT):} The LLM is given few-shot examples of CoT reasoning of how to follow the user's preference right before answering the question. (5) \textbf{Retrieval-Augemented Generation (RAG):} A sentence embedding model is used to retrieve the most similar conversation exchanges to the question, which are then provided to the LLM in the prompt. Please refer to \S\ref{sec:method_prompt} for detailed prompt examples for each method. For the experiments, if not explicitly stated otherwise, we place the user's preference at the beginning of the conversation and query at the end.
\subsection{Explicit Preference Following}



\label{sec: explicit pref}

\textbf{SOTA LLMs exhibit limited proactivity in adhering to user preferences.} As shown in the generation task performance in Figure \ref{fig:zeroshot}, all LLMs exhibit substantial declines in preference-following accuracy as the dialogue length increases between the user’s stated preferences and the final queries in zero-shot settings—where no specific prompting is provided. Accuracy drops steeply from approximately 80\% to below 30\% as the number of conversation turns increases to merely 5. As the turns extend from 30 to 300, accuracy falls close to zero across all models. This suggests that LLMs often provide recommendations that conflict with the user’s previously stated preferences, even when expressed only a few turns earlier. Such behavior is detrimental to user satisfaction and engagement. Additionally, more advanced LLMs continue to struggle with preference following in zero-shot settings, even within the first 10 turns. As shown in Table \ref{tab:SOTAmodel_comparison}, GPT-o1 has 50\% accuracy while Claude 3.5 Sonnet and Gemini 1.5 Pro have near-zero preference following.
\begin{table}[h]
    \centering

\caption{Results of preference-following accuracy across SOTA LLMs evaluated using two context lengths between the user's preference and query. We show results for two methods: Zero-shot and Reminder (best prompting method), on the generation task for the \textit{travel restaurant} topic.}
    
    \scalebox{0.7}{
    \begin{tabular}{l|cc|cc}

         & \multicolumn{2}{c|}{\textbf{10 Turns / $\sim$3k tokens}} & \multicolumn{2}{c}{\textbf{300 Turns / $\sim$103k tokens}} \\
        \cline{2-5}
                       & \textbf{Zero-shot} & \textbf{Reminder} & \textbf{Zero-shot} & \textbf{Reminder} \\
        \hline
        Claude-3.5-Sonnet & 0.07 & 0.45 & 0.02 & 0.02 \\
        Gemini-1.5-Pro    & 0.07 & 0.91 & 0.09 & 0.05 \\
        GPT-o1-preview\footnotemark{}   & \textbf{0.50}  & \textbf{0.98}    &\textbf{0.14} & \textbf{0.98} \\

    \end{tabular}}
    \label{tab:SOTAmodel_comparison}

\end{table}



\begin{figure}[ht]
    \centering
    \begin{minipage}{0.60\textwidth}
        \includegraphics[width=\linewidth]{figs/preference_adherence_accuracy_baselines_all_model_zero_shot_with_tokens_and_turns.pdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.38\textwidth}
        \caption{Zero-shot performance of LLMs with explicit preferences, averaged across 20 topics. The x-axis represents the dialogue length between the user's stated preference and the final query, measured by both the number of tokens in the prompt and the number of conversation turns. All LLMs exhibit a rapid decline in accuracy as the number of turns increases.}
        \label{fig:zeroshot}
    \end{minipage}
\end{figure}



\textbf{How do different methods improve preference following performance?} LLMs lack the proactiveness to follow user preference in the zero-shot setting. We evaluated four methods for enhancing preference following across models~(\S\ref{sec:model_baselines}) that force the LLMs to utilize their abilities in \textit{Preference Inference}, \textit{Long-Context Retrieval} and \textit{Preference Following}. As shown in Figure~\ref{fig:baselines}, all methods outperformed the zero-shot baseline. Among prompting techniques, the Reminder method, which simply reminds the model to consider the user’s previously stated preferences (see prompt in \S\ref{parag:reminder}), surprisingly outperformed more complex methods such as Self-Critic and CoT. Interestingly, performance trends varied across models: in Claude models, Self-Critic initially outperformed CoT but fell behind as the number of conversation turns increased, whereas the reverse was observed for Llama models. RAG consistently performed the best across most models, indicating that adherence to user preferences may hinge on retrieval capabilities. However, in models like Claude 3 Sonnet and Mistral 8x7b, Reminder performed comparably or even surpassed RAG, suggesting that these models may have strong intrinsic \textit{Long-Context Retrieval} abilities.


\footnotetext{Note that GPT-o1-preview may not be a fair comparison with other models as it may require more test-time compute with a ``thinking'' phase.}



\begin{figure}[ht]
    \centering
    \includegraphics[width=1.02\textwidth]{figs/preference_adherence_accuracy_baselines_grid.pdf}
       
\caption{Performance comparison of 5 methods across 6 LLMs with explicit preferences on the generation task. Both Reminder and RAG consistently achieve the highest accuracy across models. Notably, Reminder outperforms more complex techniques such as Self-Critic and CoT.}
% \kaixiang{a bit hard to read. how about 2 rows and each row 3 figures?}
    \label{fig:baselines}
\end{figure}
% \textbf{Topic-Dependent preference following: Distribution of Accuracies across 20 Topics.} Figure \ref{fig:boxplot} illustrates the distribution of explicit preference following accuracies across different models under two baselines (zero-shot and with reminder) and two conversation lengths (short: $\leq$ 10 turns, and long: 70 turns). All models exhibit varied performance across topics in the zero-shot setting, suggesting that preference following capability is topic-dependent. For example, Claude 3 Sonnet performs better at preference following at 10 turns for education-related topics, while Mistral-8x7B excels in following preferences related to lifestyle and health queries, as shown in the detailed results in Table \ref{tab:explicit_20topics_10}. Explicitly reminding the models significantly enhances their ability to adhere to user preferences, particularly in long conversations, where it also reduces performance variance across topics for short conversations. In longer conversations, Claude 3 Sonnet and Claude 3 Haiku demonstrate the highest robustness across topics, exhibiting the smallest variance when given reminders. All models show a substantial performance degradation as conversation length increases, though this decline is less severe with the reminder baseline. 





% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.48\textwidth]{figs/preference_adherence_boxplot_Zero-Shot_10.pdf}
%     \hfill
%     \includegraphics[width=0.48\textwidth]{figs/preference_adherence_boxplot_Reminder_10.pdf}
    
%     % \vspace{0.5cm}
    
%     \includegraphics[width=0.48\textwidth]{figs/preference_adherence_boxplot_Zero-Shot_70.pdf}
%     \hfill
%     \includegraphics[width=0.48\textwidth]{figs/preference_adherence_boxplot_Reminder_70.pdf}
% \caption{Distribution of explicit preference following accuracy across models and 20 topics. The left column shows zero-shot performance, while the right column shows performance with the strongest baseline Reminder. The top row represents turns $\leq$ 10, and the bottom row represents turns = 70. Note: Llama 3 models support a maximum of 10 conversation turns in this setup.}
%     \label{fig:boxplot}
% \end{figure}





\subsection{Implicit Preference Following}
\label{sec: implicit pref}

% \textbf{Implicit Preferences Challenge Preference Elicitation Capabilities.}
% We posit that effective preference following in conversational settings requires critical abilities, with preference elicitation being paramount. In our experiments, user preferences are implicitly revealed through conversations via dismissal or positive selection of assistant-suggested options, termed \textbf{Single-Adherence} and \textbf{Single-Violation} (as defined in Section~\ref{sec:elicitations}). Our zero-shot results, as shown in Figure~\ref{fig:SA_vs_SV_zeroshot}, demonstrate a significant decline in preference following accuracy for implicit preferences compared to explicit ones, even at turn 2 where no additional conversational context is present. This decline underscores the inherent challenge in eliciting user preferences from implicit conversational cues, even before considering long-context retrieval abilities. As dialogue length increases, implicit preferences exhibit a higher decay rate than explicit preferences, highlighting the complexity of maintaining and adhering to implicitly stated user preferences in extended conversations. These findings emphasize the need for more sophisticated models to effectively capture and maintain user preferences in conversational AI systems, as implicit preference settings are prevalent in realistic and practical scenarios.

% \textbf{Explicit Prompting Enhances Implicit Preference Elicitation.}
% As illustrated in Figure~\ref{fig:SA_vs_SV_remind}, there is a significant improvement in preference elicitation capabilities when LLMs are explicitly instructed to adhere to user preferences. Notably, Claude and Llama models, along with Mistral-7B, achieve near-parity with explicit preference scenarios at early conversation turns at turn 2, with accuracy drops of merely 10\% compared to the 30-60\% decline observed in zero-shot settings. The results suggest that these LLMs possess the inherent capability to elicit and adhere to implicit preferences, but require explicit guidance to activate this skill effectively. On average, Claude outperforms other models in implicit preference following at turn 10, while Mistral-8x7B outperforms others at turn 70, as detailed in Tables~\ref{tab:implicit_20topics_10} and~\ref{tab:implicit_20topics_70}.

% \textbf{Model-Specific Proficiency in Preference Elicitation Strategies.}
% Results from Figures~\ref{fig:SA_vs_SV_remind} and~\ref{fig:SA_vs_SV_zeroshot} reveal intriguing model-specific variations in preference elicitation strategies. Most LLMs demonstrate superior proficiency in eliciting user preferences from the dismissal of non-aligned options (Single-Violation) rather than from positive selection of preference-aligned choices (Single-Adherence). This trend suggests that these assistants are more adept at learning from negative examples. Interestingly, Mistral-8x7B (with reminder) exhibits a contrasting behavior, excelling in preference following based on positive selection. These distinctions highlight the varied ways in which different LLMs process and interpret implicit conversational cues, emphasizing the need for diverse evaluation methods in assessing assistant capabilities for real-world applications.


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figs/compare_exp_averaged_all_models_zeroshot.pdf}
% \caption{preference following Accuracy over conversation turns between stated preference and user query, comparing explicit and implicit scenarios in \textbf{zero-shot settings}. Implicit preferences demonstrate increased difficulty in preference elicitation and long-context retrieval, with higher decay rates as turns increase.}
%     \label{fig:SA_vs_SV_zeroshot}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figs/compare_exp_averaged_all_models.pdf}
% \caption{preference following Accuracy across conversation turns between stated preference and user query, comparing explicit and implicit scenarios with \textbf{reminder baseline}. Mistral-7b show large accuracy gaps even in early turns, while other models achieve comparable accuracy (within 10\% drop) to explicit preference in early turns when reminded to elicit and follow preferences.}
%     \label{fig:SA_vs_SV_remind}
% \end{figure}




\textbf{Implicit Preferences Add Complexity to Preference Inference.}
\label{sec: pref forms comparison} The ability to infer preferences is especially critical when users implicitly reveal their preferences through a dialogue. \ours{} includes three forms of preferences, two of which are implicitly conveyed through conversation (\S\ref{sec:preference_Forms}). We evaluate how these different forms impact model performance using the best prompting method—Reminder. As shown in Figure~\ref{fig: pref_forms_comparison}, implicit preferences add additional complexity to preference-following tasks, even with shorter input lengths. Models exhibit varying capabilities in handling these complexities: for instance, Claude and Llama struggle more with \textit{implicit persona-driven} preferences compared to \textit{implicit choice-based} preferences, while Mistral models show the opposite trend. These differences indicate that each model has unique strengths and weaknesses in inferring and processing distinct preference forms.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{figs/preference_adherence_accuracy_pref_type.pdf}
% \caption{Comparison of 3 Preference Forms for 6 LLMs with the Reminder prompting method and on the generation task. The results show that models exhibit varying abilities across preference forms, with implicit preferences being more challenging to infer than explicit preferences. }
%     \label{fig: pref_forms_comparison}
% \end{figure}
% \kaixiang{Fig 5-6 are too small. one way is to make curve thicker and larger font size for caption.}
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/table_all_preferences.pdf}
\caption{Comparison of 3 preference forms for 6 LLMs on the generation task, across varying lengths between the stated preference and query. Note that the user preference is stated in the first turn. Results show that implicit preferences are more challenging to infer than explicit preferences.}
    \label{fig: pref_forms_comparison}
\end{figure}


\subsection{Classification Task}
\label{sec: mcq task}

\textbf{Classification Task Brings Fast Evaluation with High Correlation to Generation Task Results.} We introduce a classification task for each preference-query pair to complement the generation task (\S\ref{sec:method_classification}). This reduces reliance on costly human evaluations and LLM-based evaluators. We task LLMs to select the aligned option based on user preferences and get classification accuracy. Results in Figure~\ref{fig:mcq_all_explicit} show higher overall accuracy compared to the generation task, reflecting the classification task's simpler nature. The Self-Critic method, with Claude models and Mistral-8x7B, underperforms the zero-shot baseline. Upon examining their critiques and revisions, we observe that when models fail to retrieve user preferences, the revisions tend to hallucinate and select random options distinct from the initial response. We conjecture that iterative self-feedback mechanisms may be less effective for classification tasks (which require structured output) compared to generation tasks. While both RAG and Reminder consistently achieve high accuracy, RAG demonstrates more advantage over Reminder in the classification task compared to the generation task. 
To further understand the relationship between the generation and classification tasks, we perform a correlation analysis between LLM-based preference following accuracy and classification-based preference alignment results, as illustrated in Figure~\ref{fig:mcq_correlation}. Across six models, five methods, and 12 dialogue lengths between the preference and query, each data point in the scatter plot is averaged over 20 topics. We observe a correlation coefficient of 0.73, indicating a strong positive correlation between the two evaluation methodologies. This correlation suggests that despite task differences, models adhering to preferences in one setting perform similarly in the other. Classification tasks could thus serve as an efficient proxy for evaluating preference following in complex generation scenarios across models and methods.

\begin{figure}[h]
    \centering
    
        \includegraphics[width=0.85\textwidth]{figs/mcq_heatmap.pdf}
   

   
      \caption{Performance of the classification task across models and methods on the explicit preferences dataset across various input lengths between the user's preference and the final query. Results are averaged over 20 topics.}

\label{fig:mcq_all_explicit}
\end{figure}





\subsection{Error Type Analysis}
\label{sec: errortype}
\textbf{What causes LLMs to fail and what capabilities are missing in preference following?} LLM-based evaluations allow us to efficiently analyze error types defined in \S\ref{sec:method_errortypes} and we analyze them with the key capabilities defined in \S\ref{para:key_capabiilties}. Figure~\ref{fig:errortype_10} shows the distribution of error types across two LLMs and five methods at turn 10. In zero-shot settings, LLMs generally lack awareness of user preferences, leading to high \textit{Preference-Unaware Violations}. With advanced methods, this error type percentage drops, indicating a lack of \textit{Personalization Proactiveness} ability in the zero-shot setting. However, with these methods, \textit{Inconsistency Violations} appears, indicating that even when preferences are correctly retrieved, LLMs struggle to generate aligned responses, lacking the \textit{Preference Following} capability.
Interestingly, while prompting methods are introduced to enhance preference following, they inadvertently incentivize LLMs to hallucinate preferences. RAG reduces the hallucination error percentage for Mistral 8x7b. Additionally, Claude 3 Sonnet exhibits a high percentage of \textit{Unhelpful} errors with prompting methods, often refusing to answer queries due to a perceived lack of context regarding user preferences, which is undesirable for effective conversational personalization.
We include further discussions on long-context error type analysis with more LLMs and how they scale with turns in Appendix~\ref{sec:absolute_errortypes}.
% In the previous section, we presented preference following accuracy, evaluated by Claude 3 Sonnet using guidelines that classify responses as accurate if they do not fall into any of the four error types: \textit{Violation, Hallucination, Inconsistency, and Unhelpful}. This section examines the distribution of these error types across short (10-turn) and long (70-turn) scenarios for each LLM and baseline as shown in Figure~\ref{fig:explicit_1070_error} in the explicit preference setting. Our analysis reveals that Violation is the most prevalent error type across all models. Interestingly, as we introduce prompting to guide models in following user preferences, we observe an increase in Unhelpful errors. This occurs when models refuse to answer user queries due to an inability to recall the user's preferences. Consequently, despite being prompted to adhere to user preferences, the lack of correct preference memorization leads to refused answer and a rise in \textit{Unhelpful} errors. For some models, such as Llama3-8b-instruct, we note an increase in \textit{Hallucination} errors when prompted to remember preferences. In these cases, the model fabricates user preferences to comply with the prompting, rather than accurately following the original user-specified preferences. We also observe a significant number of \textit{Inconsistency} errors, indicating that while models can often remember preferences, they struggle to consistently provide responses that adhere to those preferences. This shows a gap between preference retrieval and the ability to generate aligned responses.


% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figs/error_types_all_methods_10_70_turn_summed.pdf}
% \caption{Distribution of error types for LLMs in short-turn (top row) and long-turn (bottom row) conversations. Results are on explicit preferences and are averaged across 20 diverse topics for each baseline model. }
%     \label{fig:explicit_1070_error}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.32\textwidth]{figs/all_Claudemodels_error_types_remind.pdf}
%     \includegraphics[width=0.32\textwidth]{figs/all_Mistralmodels_error_types_remind.pdf}
%         \includegraphics[width=0.32\textwidth]{figs/all_Llama3models_error_types_remind.pdf}
       
% \caption{}
%     \label{fig:error_Type_all}
% \end{figure}
\begin{figure}[h]
    \centering
    \begin{minipage}{0.51\textwidth}
         \includegraphics[width=\textwidth]{figs/error_types/paper_error_type_pie_turn10_transposed.pdf}
           
    \end{minipage}
\begin{minipage}{0.47\textwidth}
\caption{Distribution of 4 disjoint error types across 2 LLMs and 5 methods in the generation task with explicit preferences, where the gap between the preference and the query is 10 turns. Prompting methods introduce more hallucination and unhelpful errors, as LLMs either fabricate user preferences or refuse to respond due to failures in recalling the preferences, which is undesirable for personalized conversational chatbots.}  
  \label{fig:errortype_10}
\end{minipage}

\end{figure}



\subsection{Dynamic Preference Following}
\label{sec: dynamic_prefs}
In real-world scenarios, users may express multiple preferences within a single conversation, and these preferences can evolve or even conflict as the dialogue progresses across different sessions. We explore how the presence of multiple preferences and conflicting preferences affect the model’s ability to maintain adherence.

\textbf{Impact of Multiple Preferences Stated in Conversation.} We investigated whether introducing non-conflicting preferences from different topics throughout a conversation would affect the model's ability to follow an initial preference. To evaluate this, we evenly inserted additional preferences at various points in the conversation and measured the model's adherence by asking a query related to the first preference. As shown in Figure~\ref{fig:multiple_prefs}, the results indicate that adherence accuracy for the initial preference increases as more preferences are introduced, even when these preferences are unrelated to the first. We conjecture that when more preferences are presented, the model may be implicitly encouraged to treat the cumulative set of preferences as a broader constraint on its outputs. Consequently, introducing multiple preferences may help the model pay more attention to each user-stated preference throughout the conversation, leading to improved personalization.

\textbf{Effect of Conflicting Preferences Stated in Conversation.} We also analyzed how LLMs adapt to conflicting user preferences within a conversation. We generated conflicting preferences for the original preferences across five topics using Claude 3.5 Sonnet. These conflicting preferences were inserted alongside the original preferences at predetermined positions in the conversation, while the rest of the conversation was kept constant. For comparison, we also tested the impact of inserting non-conflicting preferences. In both settings, the original preferences were stated later than the conflicting one, and we will test the model's preference following to the original preference. As shown in Figure~\ref{fig:conflict}, when conflicting preferences are introduced, the model actually demonstrates improved adherence to the original preference compared to when non-conflicting preferences are added. Comparing the adherence performance to the baseline scenario where only the original preference is present (indicated by the red bar), both the conflicting and non-conflicting preference scenarios achieve better adherence than the single-preference scenario, reinforcing the observation from the previous section that introducing multiple preferences encourages the model to better retain and follow preferences. These findings suggest that conflicting preferences do not necessarily pose a challenge for the LLM but rather reinforce its capacity to track and adapt to evolving user preferences. 
% {\red[do we also want to say that similarly as before, we conjecture that more preferences leads to the model implicitly encourage to follow preferences? sz: yes it is stated in the last sentences]}
% \vspace*{-0.2cm}
\begin{figure}[t]
\centering
    \begin{minipage}{0.48\textwidth}
    \centering
         \includegraphics[width=0.96\textwidth]{figs/multi_pref_adherence_xaxis_prefsclaude3s.pdf}
\caption{Introducing multiple preferences throughout a conversation improves adherence to the initial preference. Results are using Claude 3 Sonnet with Reminder prompting.}

\label{fig:multiple_prefs}
    \end{minipage}
     \hspace{0.01\textwidth}
\begin{minipage}{0.48\textwidth}
    \includegraphics[width=0.96\textwidth]{figs/model_conflict_results_ba100_Reminder.pdf}
\caption{Effect of adding conflicting versus non-conflicting preferences on adherence. The red bar indicates the performance when only the original preference is present. Results are averaged over five topics using a fixed 100-turn conversation.}
\label{fig:conflict}
\end{minipage}
\end{figure}

\subsection{Finetuning on \ours{} To Improve Preference Following}
\label{sec:finetune}
\textbf{Enhancing Preference Following through Supervised Fine-Tuning (SFT) on \ours{}}: We fine-tuned the Mistral-7B model using SFT on 80\% of the topics in \ours{} and evaluated it on the remaining unseen 20\% topics for the generation task. For SFT data, we used Mistral-7B's responses generated using the Reminder method, without any contextual turns inserted between the stated preference and the query. During training, to simulate conversational preference following, we inserted 0, 5, or 10 contextual turns between the preference and query, resulting in training data of 2, 7, or 12 turns (where the preference, query, and response constitute 2 turns). After fine-tuning, the model demonstrated significant improvement in the zero-shot setting, surpassing the previous best-performing method (RAG), as shown in Figure~\ref{fig:sft}. One notable benefit is improved length generalization -- the capability to follow preferences for longer contexts compared to training. When the model is trained with 10-turn contextual turns, it generalizes to 70-turn contexts much more effectively than when trained with fewer contexts. This suggests that SFT enhances both the ability to handle long-context retrieval and the capacity to infer and follow user preferences over extended conversations.

To understand the mechanisms behind this improvement, we analyzed the attention patterns before and after fine-tuning. Our analysis reveals that the SFT model consistently exhibits increased attention to preference-related information during generation, with improvements in preference region attention up to 4.97\% across test examples. See Appendix~\ref{sec:attention_visual} for detailed attention score visualizations and analysis.
\begin{figure}[h]
\hspace{-0.2em}
    \centering
    \begin{minipage}{0.65\textwidth}
    \vspace{-0.15em}
         \includegraphics[width=\textwidth]{figs/sft_comparison_across_topics.pdf}
    \end{minipage}
   \begin{minipage}{0.28\textwidth}
\caption{Performance on the 20\% unseen test topics: After fine-tuning, the Mistral-7B model exhibits superior preference following accuracy compared to other baseline models. It also shows enhanced length generalization when trained with longer context interleavings, demonstrating its ability to handle extended conversational turns more effectively.}
    \label{fig:sft}       
   \end{minipage}

\end{figure}

