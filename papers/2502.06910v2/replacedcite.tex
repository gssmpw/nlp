\section{Related Work}
\subsection{Kolmogorov-Arnold Network}
Kolmogorov-Arnold representation theorem states that any multivariate continuous function can be expressed as a combination of univariate functions and addition operations. Kolmogorov-Arnold Network (KAN) ____ leverages this theorem to propose an innovative alternative to traditional MLP. Unlike MLP, which use fixed activation functions at the nodes, KAN introduces learnable activation functions along the edges. Due to the flexibility and adaptability, KAN is considered as a promising alternative to MLP.\par
The original KAN was parameterized using spline functions. However, due to the inherent complexity of spline functions, the speed and scalability of the original KAN were not satisfactory. Consequently, subsequent research explored the use of simpler basis functions to replace splines, thereby achieving higher efficiency. ChebyshevKAN ____ incorporates Chebyshev polynomials to parametrize the learnable functions. FastKAN ____ uses faster Gaussian radial basis functions to approximate third-order B-spline functions.\par 
Moreover, KAN has been applied as alternatives to MLP in various domains. Convolutional KAN ____ replaces the linear weight matrices in traditional convolutional networks with learnable spline function matrices. U-KAN ____ integrates KAN layers into the U-Net architecture, demonstrating impressive accuracy and efficiency in several medical image segmentation tasks. KAN has also been used to bridge the gap between AI and science. Works such as PIKAN ____ and PINN ____ utilize KAN to build physics-informed machine learning models. This paper aims to introduce KAN into TSF and demonstrate the strong potential of KAN in representing time series data.

\subsection{Time Series Forecasting}
Traditional time series forecasting (TSF) methods, such as ARIMA ____, can provide sufficient interpretability for the forecasting results but often fail to achieve satisfactory accuracy. In recent years, deep learning methods have dominated the field of TSF, mainly including CNN-based, Transformer-based, and MLP-based approaches. CNN-based models primarily apply convolution operations along the temporal dimension to extract temporal patterns. For example, MICN ____ and TimesNet ____ enhance the precision of sequence modeling by adjusting the receptive field to capture both short-term and long-term views within the sequences. ModernTCN ____ advocates using large convolution kernels along the temporal dimension and capture both cross-time and cross-variable dependencies.
%
Compared to CNN-based methods, which have limited receptive field, Transformer-based methods offer global modeling capabilities, making them more suitable for handling long and complex sequence data. They have become the cornerstone of modern time series forecasting. Informer ____ is one of the early implementations of Transformer models in TSF, making efficient forecasting possible by carefully modifying the internal Transformer architecture. PatchTST ____ divides the sequence into multiple patches along the temporal dimension, which are then fed into the Transformer, establishing it as an important benchmark in the time series domain. In contrast, iTransformer ____ treats each variable as an independent token to capture cross-variable dependencies in multivariate time series. However, Transformer-based methods face challenges due to the large number of parameters and high memory consumption.
%
Recent research on MLP-based methods has shown that with appropriately designed architectures leveraging prior knowledge, simple MLPs can outperform complex Transformer-based methods. DLinear ____, for instance, preprocesses sequences using a trend-season decomposition strategy. FITS ____ performs linear transformations in the frequency domain, while TimeMixer ____ uses MLP to facilitate information interaction at different scales. These MLP-based methods have demonstrated strong performance regarding both forecasting accuracy and efficiency.
%
Unlike the aforementioned methods, this paper introduces the novel KAN to TSF to represent time series data more accurately. It also proposes a well-designed Decomposition-Learning-Mixing architecture to fully unlock the potential of KAN for time series forecasting.


\subsection{Time Series Decomposition}
Real-world time series often consist of various underlying patterns. To leverage the characteristics of different patterns, recent approaches tend to decompose the series into multiple subcomponents, including trend-seasonal decomposition, multi-scale decomposition, and multi-period decomposition. DLinear ____ employs moving averages to decouple the seasonal and trend components.  SCINet ____ uses a hierarchical downsampling tree to iteratively extract and exchange information at multiple temporal resolutions. TimeMixer ____ follows a fine-to-coarse principle to decompose the sequence into multiple scales across different time spans and further splits each scale into seasonal and periodic components. TimesNet ____ and PDF ____ utilize Fourier periodic analysis to decouple sequence into multiple sub-period sequences based on the calculated period.
Inspired by these works, this paper proposes a novel Decomposition-Learning-Mixing architecture, which examines time series from a multi-frequency perspective to accurately model the complex patterns within time series.