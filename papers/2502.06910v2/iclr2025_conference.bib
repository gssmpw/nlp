@article{
Remi2023GraphCast,
author = {Remi Lam  and Alvaro Sanchez-Gonzalez  and Matthew Willson  and Peter Wirnsberger  and Meire Fortunato  and Ferran Alet  and Suman Ravuri  and Timo Ewalds  and Zach Eaton-Rosen  and Weihua Hu  and Alexander Merose  and Stephan Hoyer  and George Holland  and Oriol Vinyals  and Jacklynn Stott  and Alexander Pritzel  and Shakir Mohamed  and Peter Battaglia },
title = {Learning skillful medium-range global weather forecasting},
journal = {Science},
volume = {382},
number = {6677},
pages = {1416-1421},
year = {2023},
doi = {10.1126/science.adi2336},
URL = {https://www.science.org/doi/abs/10.1126/science.adi2336},
eprint = {https://www.science.org/doi/pdf/10.1126/science.adi2336},
abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning–based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25° resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems. The numerical models used to predict weather are large, complex, and computationally demanding and do not learn from past weather patterns. Lam et al. introduced a machine learning–based method that has been trained directly from reanalysis data of past atmospheric conditions. In this way, the authors were able to quickly predict hundreds of weather variables globally up to 10 days in advance and at high resolution. Their predictions were more accurate than those of traditional weather models in 90\% of tested cases and displayed better severe event prediction for tropical cyclones, atmospheric rivers, and extreme temperatures. —H. Jesse Smith Machine learning leads to better, faster, and cheaper weather forecasting.}}

@inproceedings{
huang2024generative,
title={Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns},
author={Hongbin Huang and Minghua Chen and Xiao Qiao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=CdjnzWsQax}
}


@article{YIN2023energy,
title = {Weighted fully-connected regression networks for one-day-ahead hourly photovoltaic power forecasting},
journal = {Applied Energy},
volume = {332},
pages = {120527},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120527},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922017846},
author = {Linfei Yin and Xinghui Cao and Dongduan Liu},
keywords = {Weighted fully-connected regression networks, Photovoltaic power forecasting, One-day-ahead hourly, Fully-connected layer, Convolutional neural networks methods},
abstract = {Accurate photovoltaic power forecasting can provide a basis for low-carbon economic dispatch of power systems with a high proportion of renewable energy. Regression networks with many times training based on multi-group multi-configuration still cannot resist the randomness of training processes, resulting in the accuracy of photovoltaic power prediction needs to be improved. This work proposes a weighted fully-connected regression network, including a feature input layer, deep fully-connected layers, particle swarm optimization, and a regression output layer. The proposed model automatically selects two networks from multi-group multi-configuration well-trained regression networks to effectively reduce photovoltaic power prediction errors without additional sensors and data sources. The errors of these two chosen well-trained networks exactly neutralize each other by fixed and simple weights. The results under the one-day-ahead hourly photovoltaic power forecasting of Natal of Brazil show that the proposed method can reduce photovoltaic power prediction errors with at least 75.9954% smaller mean absolute error than the state-of-art methods and 68.2937% than other 18 famous convolutional neural networks methods.}
}


@article{JIANG2022Traffic,
title = {Graph neural network for traffic forecasting: A survey},
journal = {Expert Systems with Applications},
volume = {207},
pages = {117921},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117921},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422011654},
author = {Weiwei Jiang and Jiayun Luo},
keywords = {Traffic forecasting, Graph neural networks, Graph convolution network, Graph attention network, Deep learning},
abstract = {Traffic forecasting is important for the success of intelligent transportation systems. Deep learning models, including convolution neural networks and recurrent neural networks, have been extensively applied in traffic forecasting problems to model spatial and temporal dependencies. In recent years, to model the graph structures in transportation systems as well as contextual information, graph neural networks have been introduced and have achieved state-of-the-art performance in a series of traffic forecasting problems. In this survey, we review the rapidly growing body of research using different graph neural networks, e.g. graph convolutional and graph attention networks, in various traffic forecasting problems, e.g. road traffic flow and speed forecasting, passenger flow forecasting in urban rail transit systems, and demand forecasting in ride-hailing platforms. We also present a comprehensive list of open data and source codes for each problem and identify future research directions. To the best of our knowledge, this paper is the first comprehensive survey that explores the application of graph neural networks for traffic forecasting problems. We have also created a public GitHub repository where the latest papers, open data, and source codes will be updated.}
}


@inproceedings{
donghao2024moderntcn,
title={Modern{TCN}: A Modern Pure Convolution Structure for General Time Series Analysis},
author={Luo donghao and wang xue},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=vpJMJerXHU}
}


@inproceedings{
wang2023micn,
title={{MICN}: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting},
author={Huiqiang Wang and Jian Peng and Feihu Huang and Jince Wang and Junhui Chen and Yifei Xiao},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=zt53IDUR1U}
}

@inproceedings{
nie2023patchtst,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Jbdc0vTOcol}
}

@inproceedings{
liu2024itransformer,
title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=JePfAI8fah}
}
@inproceedings{zeng2023transformers,
  title={Are transformers effective for time series forecasting?},
  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={37},
  pages={11121--11128},
  year={2023}
}
@inproceedings{
wang2024timemixer,
title={TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting},
author={Shiyu Wang and Haixu Wu and Xiaoming Shi and Tengge Hu and Huakun Luo and Lintao Ma and James Y. Zhang and JUN ZHOU},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=7oLshfEIC2}
}

@inproceedings{wu2021autoformer,
 author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22419--22430},
 publisher = {Curran Associates, Inc.},
 title = {Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/bcc0d400288793e8bdcd7c19a8ac0c2b-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{zhou2022fedformer,
  title={Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle={International conference on machine learning},
  pages={27268--27286},
  year={2022},
  organization={PMLR}
}


@inproceedings{
wu2023timesnet,
title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ju_Uqw384Oq}
}

@inproceedings{
dai2024pdf,
title={Periodicity Decoupling Framework for Long-term Series Forecasting},
author={Tao Dai and Beiliang Wu and Peiyuan Liu and Naiqi Li and Jigang Bao and Yong Jiang and Shu-Tao Xia},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=dp27P5HBBt}
}


@inproceedings{
lin2024sparsetsf,
title={Sparse{TSF}: Modeling Long-term Time Series Forecasting with *1k* Parameters},
author={Shengsheng Lin and Weiwei Lin and Wentai Wu and Haojun Chen and Junjie Yang},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=54NSHO0lFe}
}


@article{liu2024kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
}



@article{ZHANG2003arima,
title = {Time series forecasting using a hybrid ARIMA and neural network model},
journal = {Neurocomputing},
volume = {50},
pages = {159-175},
year = {2003},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(01)00702-0},
url = {https://www.sciencedirect.com/science/article/pii/S0925231201007020},
author = {G.Peter Zhang},
keywords = {ARIMA, Box–Jenkins methodology, Artificial neural networks, Time series forecasting},
abstract = {Autoregressive integrated moving average (ARIMA) is one of the popular linear models in time series forecasting during the past three decades. Recent research activities in forecasting with artificial neural networks (ANNs) suggest that ANNs can be a promising alternative to the traditional linear methods. ARIMA models and ANNs are often compared with mixed conclusions in terms of the superiority in forecasting performance. In this paper, a hybrid methodology that combines both ARIMA and ANN models is proposed to take advantage of the unique strength of ARIMA and ANN models in linear and nonlinear modeling. Experimental results with real data sets indicate that the combined model can be an effective way to improve forecasting accuracy achieved by either of the models used separately.}
}


@article{liu2022scinet,
  title={Scinet: Time series modeling and forecasting with sample convolution and interaction},
  author={Liu, Minhao and Zeng, Ailing and Chen, Muxi and Xu, Zhijian and Lai, Qiuxia and Ma, Lingna and Xu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5816--5828},
  year={2022}
}


@article{ss2024chebyshev,
  title={Chebyshev polynomial-based kolmogorov-arnold networks: An efficient architecture for nonlinear function approximation},
  author={SS, Sidharth},
  journal={arXiv preprint arXiv:2405.07200},
  year={2024}
}

@article{li2024kolmogorov,
  title={Kolmogorov-arnold networks are radial basis function networks},
  author={Li, Ziyao},
  journal={arXiv preprint arXiv:2405.06721},
  year={2024}
}

@article{bodner2024convolutional,
  title={Convolutional Kolmogorov-Arnold Networks},
  author={Bodner, Alexander Dylan and Tepsich, Antonio Santiago and Spolski, Jack Natan and Pourteau, Santiago},
  journal={arXiv preprint arXiv:2406.13155},
  year={2024}
}

@article{li2024ukan,
  title={U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation},
  author={Li, Chenxin and Liu, Xinyu and Li, Wuyang and Wang, Cheng and Liu, Hengyu and Yuan, Yixuan},
  journal={arXiv preprint arXiv:2406.02918},
  year={2024}
}


@article{wang2024kinn,
  title={Kolmogorov Arnold Informed neural network: A physics-informed deep learning framework for solving PDEs based on Kolmogorov Arnold Networks},
  author={Wang, Yizheng and Sun, Jia and Bai, Jinshuai and Anitescu, Cosmin and Eshaghi, Mohammad Sadegh and Zhuang, Xiaoying and Rabczuk, Timon and Liu, Yinghua},
  journal={arXiv preprint arXiv:2406.11045},
  year={2024}
}


@article{shukla2024pikan,
  title={A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks},
  author={Shukla, Khemraj and Toscano, Juan Diego and Wang, Zhicheng and Zou, Zongren and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2406.02917},
  year={2024}
}


@inproceedings{
xu2024fits,
title={{FITS}: Modeling Time Series with \$10k\$ Parameters},
author={Zhijian Xu and Ailing Zeng and Qiang Xu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=bWcnvZ3qMb}
}


@article{chen2023tsmixer,
  title={Tsmixer: An all-mlp architecture for time series forecasting},
  author={Chen, Si-An and Li, Chun-Liang and Yoder, Nate and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2303.06053},
  year={2023}
}

@inproceedings{zhou2021informer,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},

  pages={11106--11115},
  year={2021}
}

@article{xu2024arekan,
  title={Are KAN Effective for Identifying and Tracking Concept Drift in Time Series?},
  author={Xu, Kunpeng and Chen, Lifei and Wang, Shengrui},
  journal={arXiv preprint arXiv:2410.10041},
  year={2024}
}
@article{yi2024frets,
  title={Frequency-domain MLPs are more effective learners in time series forecasting},
  author={Yi, Kun and Zhang, Qi and Fan, Wei and Wang, Shoujin and Wang, Pengyang and He, Hui and An, Ning and Lian, Defu and Cao, Longbing and Niu, Zhendong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2022film,
  title={Film: Frequency improved legendre memory model for long-term time series forecasting},
  author={Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Sun, Liang and Yao, Tao and Yin, Wotao and Jin, Rong and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={12677--12690},
  year={2022}
}
@inproceedings{
liu2024timeffm,
title={Time-{FFM}: Towards {LM}-Empowered Federated Foundation Model for Time Series Forecasting},
author={Qingxiang Liu and Xu Liu and Chenghao Liu and Qingsong Wen and Yuxuan Liang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=HS0faHRhWD}
}
@inproceedings{moment2024,
  author={Mononito Goswami and Konrad Szafer and Arjun Choudhry and Yifu Cai and Shuo Li and Artur Dubrawski},
  title={MOMENT: A Family of Open Time-series Foundation Models},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=FVvf69a5rx},
  booktitle={ICML},
}