\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{titlesec}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMind's Innovations\\}

\input{sections/0Authors/authors.tex}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}

    \input{sections/1Abstract/abstract.tex}

\end{abstract}

\begin{IEEEkeywords}
    Deep Reinforcement Learning, Google DeepMind, AlphaGo, AlphaGo Zero, MuZero, Atari Games, Go, Chess, Shogi,
\end{IEEEkeywords}

\section{Introduction}
\input{sections/2Introduction/introdcution.tex}
\section{Related Work}
\input{sections/1.5Related_work/relatedwork.tex}

\section{Background}
\input{sections/3Background/background.tex}

\section{AlphaGo}
\input{sections/4AlphaGo/AlphaGo.tex}

\section{AlphaGo Zero}
\input{sections/5AlphaGo_Zero/AlphaGoZero.tex}

\section{MuZero}
\input{sections/6MuZero/MuZero.tex}

\section{Advancements}
\input{sections/7Advancments/advancments.tex}

\section{Future Directions}
\input{sections/8Future_Directions/futureDirections.tex}

\section*{Conclusion}
\input{sections/9Conclusion/conclusion.tex}


\begin{thebibliography}{00}
    %-------Introduction-------
    \bibitem{I1}  N. Y. Georgios and T. Julian, Artificial Intelligence and Games. New York: Springer, 2018.
    \bibitem{I2}  N. Justesen, P. Bontrager, J. Togelius, S. Risi, (2019). Deep learning for video game playing. arXiv.
    \bibitem{I3}  V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, (2013). Playing Atari with deep reinforcement learning. arXiv.
    \bibitem{I4}  A. Graves, G. Wayne, I. Danihelka, (2014). Neural Turing Machines. arXiv.
    \bibitem{I5}  C.J.C.H.Watkins, P. Dayan, Q-learning. Mach Learn 8, 279–292 (1992).
    \bibitem{I6}  DeepMind, (2015, February 12), Deep reinforcement learning.
    \bibitem{I7}  T. Schaul, J. Quan, I. Antonoglou, D. Silver, (2015). Prioritized Experience Replay. arXiv.
    \bibitem{I8}  V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, K. Kavukcuoglu, (2016). Asynchronous Methods for Deep Reinforcement Learning. arXiv.
    \bibitem{I9}  A. Kailash, P. D. Marc, B. Miles, and A. B. Anil, (2017). Deep Reinforcement Learning: A Brief Survey. IEEE Signal Processing Magazine, vol. 34, pp. 26–38, 2017. arXiv.
    \bibitem{I10} D. Zhao,  K. Shao, Y. Zhu, D. Li, Y. Chen, H. Wang, D. Liu, T. Zhou, and C. Wang, “Review of deep reinforcement learning and discussions on the development of computer Go,” Control Theory and Applications, vol. 33, no. 6, pp. 701–717, 2016 arXiv.
    \bibitem{I11} Z. Tang, K. Shao, D. Zhao, and Y. Zhu, “Recent progress of deep reinforcement learning: from AlphaGo to AlphaGo Zero,” Control Theory and Applications, vol. 34, no. 12, pp. 1529–1546, 2017.
    \bibitem{I12} K. Shao, Z. Tang, Y. Zhu, N. Li, D. Zhao, (2019). A survey of deep reinforcement learning in video games. arXiv.
    
    %-------Background-------
    \bibitem{bg1} L. Thorndike and D. Bruce, Animal Intelligence. Routledge, 2017.
    \bibitem{bg2} R. S. Sutton and A. Barto, Reinforcement learning : an introduction. Cambridge, Ma ; London: The Mit Press, 2018.
    \bibitem{bg3} A. Kumar Shakya, G. Pillai, and S. Chakrabarty, “Reinforcement Learning Algorithms: A brief survey,” Expert Systems with Applications, vol. 231, p. 120495, May 2023
    \bibitem{bg4} Mnih, Volodymyr, et al. “Human-Level Control through Deep Reinforcement Learning.” Nature, vol. 518, no. 7540, Feb. 2015, pp. 529–533.
    \bibitem{Silver2016} D. Silver et al., “Mastering the game of Go with deep neural networks and tree search,” \textit{Nature}, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: https://doi.org/10.1038/nature16961.
  %-------AlphaGo-------
    \bibitem{Silver2016} D. Silver et al., “Mastering the game of Go with deep neural networks and tree search,” \textit{Nature}, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: https://doi.org/10.1038/nature16961.
    %-------AlphaGoZero-------
    \bibitem{agz1} Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go without Human Knowledge.” Nature 550 (7676): 354–59. https://doi.org/10.1038/nature24270.

    %-------Muzero-------
    \bibitem{mz1} J. Schrittwieser et al., “Mastering Atari, go, chess and shogi by planning with a learned model,” Nature, vol. 588, no. 7839, pp. 604–609, Dec. 2020. doi:10.1038/s41586-020-03051-4 
    %-------Advancememts-------
    \bibitem{AD3} DeepMind, "AlphaZero: Shedding New Light on Chess, Shogi, and Go," DeepMind, 06-Dec-2018.
    \bibitem{AD1} T.-R. Wu, H. Guei, P.-C. Peng, P.-W. Huang, T. H. Wei, C.-C. Shih, Y.-J. Tsai, (2023). MiniZero: Comparative analysis of AlphaZero and MuZero on Go, Othello, and Atari games. arXiv.
    \bibitem{AD2} K. Zhang, Z. Yang, T. Ba\c{s}ar, (2021). Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:2103.04994

    
    %-------Future Directions-------
    \bibitem{FD1} "MuZero’s first step from research into the real world," DeepMind, Feb. 11, 2022.
    \bibitem{FD2} Jumper, J et al. Highly accurate protein structure prediction with AlphaFold. Nature (2021)

\end{thebibliography}
\vspace{12pt}

\end{document}
