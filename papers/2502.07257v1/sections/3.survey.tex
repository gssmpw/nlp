\section{IoT Developers' Perspectives on Testing}\label{sec:survey}


The analysis of platform code in Section~\ref{platforms} develops a data-driven characterization of the extent of testing in smart home platforms. 
However, this characterization must also be complemented with an understanding of {\em why} the state of testing is as it is.
To this end, we present a survey-based study of developer perspectives on IoT testing, with the goal of understanding the key pain points experienced by developers, their priorities, and preferences that may affect how testing is carried out. 
This section describes the design of our survey, our coding and thematic analysis approach, and the key findings.

\input{tabs/survey_questions}




\subsection{Survey Design} 
Our survey consists of several questions on current testing practices and preferences, challenges the developers face, and the improvements they envision and organized as follows:

\begin{itemize}[wide, labelwidth=0pt, labelindent=0pt]
\item {\bf Demographic Information}: basic demographic information such as age, gender, and education.
\item {\bf Background Information}:includes employment status, years of general programming experience, IoT-related programming experience, and testing experience. We also asked how the participants learned about software testing. %
\item {\bf Testing Practices and Preferences}:  type of IoT product participants work on and the type of documentation they use for specifying IoT-related requirements for those products. We then asked about their preferred testing approaches and why they follow them. Furthermore, we asked them to provide information about their process of designing test cases and the tools they use to support the process. Next, we asked them about their evaluation process of those test cases. We also asked them if they had encountered any flaky tests during their testing process and how they resolved them. Finally, we asked whether they created test cases for reported bugs and the origins of those bugs. \item {\bf Challenges and Expectations}: After obtaining information on IoT developers' current testing practices, we asked them about the challenges they encountered when testing and debugging the products. Furthermore, we asked them what improvements they desired in the current testing and debugging process and tools. 
\end{itemize}
\input{tabs/demographic_info}
\subsubsection{Participant Recruitment}
We recruited participants from multiple IoT platform developers' community forums (\eg OpenHab community, Home Assistant community, SmartTthing community, and Google Nest community) by posting a flier. 
We received a total of 186 responses, of which we discarded 106 responses
due to (i) failed attention check questions, (ii) not finishing the survey, (iii) ambiguous responses, or (iv) duplicate responses. Finally, we obtained 80 valid responses (denoted as \pnumber{1}--\pnumber{80}), which we analyzed and presented our findings in Section~\ref{survey-results}. Our survey took an average of 15 minutes to complete, and we offered a $10$ USD Amazon Gift Card to each participant. Table~\ref{tab:demographic} provides demographic information about all of 80
participants. 
Most participants were male (90\%), all were at least 18 years old, and most were between 30 to 49 years of age (90\%).

\subsubsection{Ethical Consideration}
The study protocol was approved by our Institutional Review Board (IRB). 
Participants were informed about the study's goal before participating, and they willingly provided their consent to participate in the study and to disclose anonymized survey responses and quotes. 


\subsection{Coding and Analysis}
We used descriptive statistical analysis to present the quantitative results. 
To analyze nine free-text questions, we used thematic analysis with an inductive coding approach~\cite{braun2021thematic}. 
Two out of three authors randomly selected a question and coded the data independently. 
After completing the coding, authors met and discussed any disparity in their codes and finalized the code after reaching a consensus. 
After all the responses were coded, all three authors discussed to extract themes or patterns in the answers. %










\subsection{Results from the Analysis of Survey Responses}
\label{survey-results}
In this section, we analyze the responses provided by the participants and answer 
\ref{rq:design}, \ref{rq:techniques}, and \ref{rq:challenges}.

\subsubsection{Test design and evaluation} 
OQ$_{1}$ and OQ$_{3}$ responses were used to answer~\ref{rq:design}. To design appropriate tests for their apps, developers start by learning the test requirements. Then they create a test plan by determining test goals, which may involve identifying corner cases, and in the process, building test scenarios. As \pnumber{63} states, \textit{``Once I have a clear understanding of the product, I create a test plan that outlines the scope of testing, including the devices, protocols, and communication methods involved in the IoT system.''} Some developers emphasized performance testing to assess speed and scalability. Security testing is also deemed crucial to developers, especially, as \pnumber{16} states, \textit{``...for products dealing with sensitive information.''}

Test effectiveness is evaluated by assessing its maintainability and execution time. Coverage-based evaluation is another popular assessment in which developers consider requirements and use-case coverage as well as code coverage and mutation analysis. Compliance with organizational standards is also considered to measure test case effectiveness.
\begin{boxK}
\ref{rq:design} Developers emphasize creating a comprehensive test plan after defining the product and testing scope, focusing on devices, protocols, and communication. Performance and security testing are crucial for sensitive data, with evaluation based on maintainability, execution time, coverage, and compliance.

\end{boxK}

\subsubsection {Current testing practices and preferences}
Table \ref{tab:findings} from Section~\ref{platforms} not only maps common testing techniques to our observations (of the presence/absence of the techniques) in our analysis of OpenHAB and HomeAssistant, but also maps them to participants' responses regarding why they use (or don't use) the specified techniques (\eg for manual analysis, automated analysis, or IoT testing in general). 
We observe that developers lean towards one technique over the other based on the nature of the test. For instance, developers opt for manual testing to ensure requirement compliance, evaluate interfaces, explore corner cases, and validate solutions. Conversely, they emphasize that automated testing is beneficial for evaluating performance, ensuring regression tests, and implementing continuous integration. For certain test types, we could not find any related answers; thus, we designated them as "Not Defined" (ND), signifying a lack of evidence regarding the use of automated or manual techniques, particularly for security evaluation or database testing.

Participants mention a set of tools oriented to the test and quality assurance such as Cucumber and Kibana, In addition to those tools, testing \iot protocols and connectivity associated with scripting are essential for device and network testing. We observe that just a few of them are dedicated to monitoring and data visualization.

Our analysis of the comments from participants leads to an understanding of their preferences. 
Particularly, we found that almost all of our participants expressed a preference for automated testing, followed by manual testing and semi-automated testing. 
The participants value the automated tests mainly because of their efficiency, speed, and coverage. The automated test enables cross-platform testing, continuous testing, and performance testing. There is a focus on identifying defects early in the development process to reduce costs and improve reliability, as \pnumber{46} states, \textit{``Automated testing helps catch bugs early in the development process, reducing the cost of fixing them later.''}. %

\finding{ Automated testing is preferred by most participants. There is a focus on identifying defects early in the development process to reduce costs and improve reliability. Consistency in results and repeatability are also highlighted, along with the importance of generating detailed logs and reports. } \label{find:results3}


Some participants prefer manual testing to test new features or achieve flexibility to requirements changes. 
Moreover, developers emphasized the need to develop a deep understanding of the system, guided by human intuition, \eg as \pnumber{4} states, \textit{``I want to see the nuts and bolts of how a thing works so I can better understand how to make it work the way I want.''}.
Participants also claimed that security issues are easily verified using manual testing, which is particularly interesting,  given that a recently study Ami et al.~\cite{ampn24} found that in general, developers prefer automated security tools for testing over manual analysis, given their rigor and ability to catch what developers miss. %
Finally, developers expressed that manual testing also helps evaluate user experience and accessibility.

\finding{Some developers recommend manual testing for its ability to bring in human intuition, adaptability, and a nuanced understanding of user experiences which can help test real-world scenarios.}\label{find:results4}

Some developers adopt a semi-automated approach to balance automation and manual testing, including human judgment and interactions for programs that cannot be automated. As \pnumber{64} explains: \textit{``I use semi-automated testing because it allows me to strike a balance between manual testing and automation. Some aspects of IoT devices and applications require human judgment and interaction that can't be fully automated.''}. Testers decide to use a hybrid between automated and manual testing mostly because of the flexibility in performing regression tests adding new features and testing real-world scenarios.


Finally, we find that developers resolve flaky tests mostly by updating and reviewing environment setups, analyzing logs, and optimizing test scripts. For resolving issues within the source codes, developers analyze test logs, review test scripts, and fix synchronization issues by adjusting wait times or implementing retry mechanisms.

\begin{boxK}
\ref{rq:techniques}
Based on \fnumber{3} and \fnumber{4}, Developers favor automated testing for early defect detection, cost reduction, regression testing, and managing changes, supported by monitoring and visualization tools for timely failure identification. However, manual testing remains essential for special cases and log analysis.
\end{boxK}

\subsubsection{Current challenges and future research scope} 
Our survey revealed several challenges developers face when testing IoT platforms and devices (\ref{rq:challenges}).
A major challenge expressed by several developers was about ensuring seamless connectivity and communication between devices and platforms as challenging, as there is no common platform.
Particularly, developers expressed how testing compatibility across a vast array of devices supported by various platforms is a time-consuming task. 

\finding{A primary challenge encountered by developers in testing \iot apps is validating compatibility across various devices and platforms.}\label{find:results5}

Similarly, ensuring the devices continue to work seamlessly with over-the-air updates, including both the software and firmware, is a major concern.  There are many \iot devices, such as, motion sensors, which run continuously. Delivering an update to these devices poses a significant challenge due to the potential for operational disruptions. As \pnumber{17} states, \textit{``Firmware over-the-air (FOTA) updates can be problematic, especially when dealing with a large number of devices. Debugging issues related to the update process, like interrupted downloads or failed installations, requires thorough testing.''} 


\finding{Developers face difficulties in debugging issues when a firmware update disrupts the functionality of an \iot product.}\label{find:results6}

Moreover, performance testing is another challenge for devices that operate on low resources. As \pnumber{12} states, \textit{``Identifying why a device fails to wake up or operate as expected in low-power states can be a complex task.''} Sensor data inaccuracy also affects the debugging process when network interference or extreme weather conditions introduce noise in the data.

\finding{Developers also face challenges during testing or debugging in low-power mode.}\label{find:results7}


Developers perceive that the current test infrastructure lacks equipment and tools to simulate real-world scenarios, including extreme weather conditions or network interruptions. Scalability to handle larger IoT deployments and conduct performance testing is another major concern. Compatibility testing is mentioned for ensuring support over a wider range of devices. Strengthening security tests is also important to identify vulnerabilities and potential weaknesses. Some think that better documentation can help facilitate knowledge sharing and collaboration. 

Current tools can benefit greatly if they offer automated test reporting, visualization, and prioritization. As \pnumber{45} states, \textit{``Debugging tools should offer more comprehensive support for analyzing and visualizing complex data structures and their changes during runtime.''} 
Cross-platform testing needs improvement according to some developers. As \pnumber{50} states, \textit{``Improved cross-platform support in debugging tools would be valuable, allowing developers to debug code running on different operating systems seamlessly.''} Real-time collaboration in testing is another talking point among the developers as it can allow multiple team members to debug and troubleshoot issues simultaneously.

\finding{Performance, scalability, real-world scenarios, and real-time collaboration are the major concerns for developers facing \iot components debug and test.  }\label{find:results8}

The open-ended responses gathered from the survey help us unveil some crucial aspects of \iot app testing that require attention and consideration. 
Particularly, we found that only a few developers worry about lack of standards and documentation, relative to the issues highlighted in \fnumber{8}.


\finding{Few developers are concerned about poor documentation and lack of organizational standards and procedures for testing \iot platforms. }\label{find:results9}

Finally, the acknowledgment of compliance with organizational standards as a metric for measuring test case effectiveness underscores the importance of aligning testing procedures with established benchmarks. Likewise, the emphasis on security testing within intricate \iot environments resonates as a critical area, where vulnerabilities or misconfigurations might remain undetected until a security breach occurs. 

\finding{Security testing is a critical area where vulnerability detection becomes more difficult due to the diversity of devices and components and lack of \iot oriented testing tools.}\label{find:results10}




\begin{boxK}
\ref{rq:challenges} \iot development faces challenges like communication platform limits, knowledge gaps, and unclear user requirements due to poor documentation. Key issues include testing diverse devices, addressing security, optimizing power, ensuring real-time responsiveness, and managing updates and firmware. Persistent problems involve standardization, third-party integration, and testing across networks.

\end{boxK}
