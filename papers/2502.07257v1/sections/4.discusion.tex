\section{Discussion}

While we managed to find answers to our five RQs, the overarching goal of our research is two-fold: to 1) learn about the general testing practices of IoT developers, and 2) Uncover the key gaps related to testing IoT platforms. 
Based on these goals, we present the following discussion topics.

\subsection{Primary Focus on Unit Testing}
Our analysis of  \openhab and \homeassistant demonstrates that the primary focus is on unit testing. This empirical observation also resonates with findings from our user survey, \ie a majority of the participants solely rely on automated testing (\fnumber{3}), and manual testing techniques such as user acceptance or exploratory testing are generally absent from practice, except in rare cases (\fnumber{4}). 

Particularly, User Acceptance Testing (UAT) offers evaluation of an application from the end-user perspective and validates the readiness of its deployment to the real-world environment \cite{wang24}. During our survey, \iot developers did not emphasize on this type of testing. The reason for not employing UAT is because of its dependence on manual effort and its time consuming nature. Although recent works have focused on automating this process using large language models \cite{wang24}. Exploratory testing can help in the continuous integration and delivery pipeline for a large-scale software system\cite{maartensson2022chapter}, which was one of the more common concerns among the participants. 
While there is recent work on performing exploratory testing using static analysis \cite{doyle23} or in a gamified way \cite{coppola24}, additional research is necessary to evaluate the usability and efficiency of such techniques in the IoT context.

Similarly, we observe no integration testing in our analysis of \openhab and \homeassistant.
One explanation for this observation could be the sheer difficulty of testing across various diverse platforms and devices, as perceived by developers in our study (\fnumber{5}), particularly given concerns regarding the unpredictable impact of firmware/software updates (\fnumber{6}).


\subsection{Compatibility Testing and Future Solutions}
Several participants talk about the importance of compatibility testing in the context of \iot apps and integrations (\fnumber{5}), which is challenging due to the general fragmentation in the smart home landscape, in terms of platforms, communication protocols, networking standards, operating systems, and types of devices and sensors. 
Ensuring compatibility among these diverse technologies is crucial for IoT platforms.  
The absence of robust compatibility testing across all technologies may lead to post-release issues, ranging from user inconvenience to severe security vulnerabilities. This is a timely and critical challenge, as developers invest substantial time in repetitive tests across varied technologies, dealing with debugging issues that arise post-deployment.


Given the general lack of integration testing and the concern among developers regarding compatibility and testing in the fragmented IoT space, the integration of simulation environments into the testing apparatus offers a promising direction for future research. 
Simulating real-world scenarios involving varied device types and communication protocols would allow developers to validate changes with increased robustness. Furthermore, advancements in \iot interoperability frameworks, automation in testing procedures, and the potential infusion of machine learning or AI-driven testing solutions hold promise in mitigating compatibility challenges.

\subsection{Performance Testing and Scalability Challenges}
Managing scalability and performance in \iot software emerges as a complex task, particularly in large-scale deployments (\fnumber{8}). As the \iot platforms expands, devices get interconnected with a multitude of other devices and data points, which makes testing of these devices and apps more challenging. These challenges are crucial as they directly impact system reliability, efficiency, and user experience. This issue is inherently tied to \iot due to its vast array of user base, and continuous data exchange occurring among the devices. 

Upgrading the testing infrastructure to accommodate scalability testing at various levels of deployment can be a potential research direction. 
Tools enabling continuous integration to identify and resolve performance bottlenecks in real-time can help in mitigating this issue. There are existing tools available that address performance and scalability testing in various domains, such as Apache JMeter~\cite{halili2008apache}, Gatling~\cite{gatling}, Taurus~\cite{taurus}. However, their applicability in \iot scenarios may be limited due to challenges in simulating complex \iot environments and diverse communication protocols. These unique, IoT-specific challenges demand more specialized testing solutions.

\subsection{Improvements in IoT Testing Compliance}
The responses from \iot developers regarding organizational standards in testing (\fnumber{9}) reveal a significant emphasis on adhering to company policies, industry standards, and regulatory requirements. 
Testing for compliance with industry-specific regulations, such as medical device or automotive safety standards, or privacy regulations such as GDPR~\cite{gdpr} and CPRA~\cite{CPRA} in the context of smart homes, adds complexity to the testing process. 
Addressing these challenges requires an approach involving robust compliance testing methodologies with continuous monitoring. 
The \iot industry can benefit greatly from tools that can analyze historical compliance data, regulatory changes, and industry-specific standards to detect potential areas of non-compliance. 





