\section{IoT Platform Analysis}\label{platforms}

The following section describes the steps for collecting, curating, standardizing, and labeling each component from \openhab~\cite{openhab} and \homeassistant~\cite{homeassistant} as well as includes the results from our analysis.


\subsection{Methodology}

To answer our \ref{rq:common} we designed a semi-automated pipeline to collect and classify testing artifacts we found from \openhab and \homeassistant. 








\begin{figure}[ht]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/Methodology_diagram4.pdf}
  \caption{\homeassistant and \openhab platform analysis steps.}
  \label{fig:methodology}
\end{figure}

\subsubsection{Data extraction}
Our data is collected from the public \github repositories reported at \homeassistant and \openhab platforms (see Figure \ref{fig:methodology} \circled{1}). Each platform reports a set of projects depending on the core architecture, brands, or operative systems. Notably, each platform adopts a distinct code organization and architecture. For instance, \homeassistant,  designates a \texttt{\textit{test}} directory within its repository, exclusively housing test codes for each \addon. While \openhab developers maintain test codes for each \addon within their project folders. \openhab's repositories exhibit a variety of code languages, with Java being the most prevalent, accompanied by instances of Python, JavaScript, and Kotlin (see Table \ref{tab:test_files_investigation}). We noticed that \openhab lacks a standardized naming convention for Java classes, with names such as ``Test'', ``Stub'', or ``Mock'', adding a layer of complexity to the analysis.




\subsubsection{Automated test identification}
The step-\circled{2} includes the identification of the source folder, classes, and methods. We use regex expressions to isolate folders with functional code from tests. For instance, Java-based \addons, usually match the test folder and file name (\eg \texttt{src/test/java/FooTest.java}) to the corresponding functional code path (\eg \texttt{src/main/java/Foo.java}). \figref{fig:focalmethods} illustrates a linked \textit{ftm} with the correspondent \fm \texttt{HueLightHandler} class. That is not the case for different PLs like JavaScript or Python. In that case, we look for name matching, for example,  \texttt{x.python} is tested by \texttt{x\_test.python}. We also aim to identify classes and methods via abstract syntax tree (AST) with tree-sitter tool \cite{noauthor_tree-sitterintroduction_nodate}. The AST helps to confirm a valid code file and identifies structures, such as the methods, functions, and variables. 
\subsubsection{Test analysis}
\ref{rq:common} requires ascertaining the number of tests per component and their proportional relationship with the functional code. Therefore, we focused on identifying \fm and \ftm as described in \secref{sec:bakground} at step-\circled{3}. A \fm exclusively incorporates the signature, parameters, and body function. To identify a test we look not only to the test folder and files but also a \ftm inside each file (\ie method within a test class with the \textit{@Test} annotation) ( \figref{fig:methodology}). Once we identify the \textit{focal methods} and \textit{focal test methods} per file, we count them to obtain a proportion by file, component, \addon, and platform. 








\subsubsection{Manual Analysis}

The fourth step-\circled{4} aims to validate the identification of focal methods and focal test methods. It allows us to confirm our proportion number to answer \ref{rq:common} and also enables the labeling process to answer \ref{rq:purpose}.


To inspect the  \openhab platform, we selected the top 36 \addons ranked by the highest count of \ftm and classes. Then we analyzed the top 10 test files with the most significant number of methods and classes. In this way, we ended up manually analyzing a total of 360 test files from a total of 5,597 test files.  Labeling is about assigning a common category to the given \ftm intention. In this step, two authors look through the \ftm to describe their intended purpose and functionalities. The authors scrutinize each test file to identify the components under examination and assign multiple labels accordingly. For instance, in files such as \texttt{test\_sensor.py}, developers frequently assess functionalities such as air quality, temperature, storage component setups, signal strength, network speed, voltage stability, and battery levels. These features are then generalized by labeling them as \texttt{`sensor', `storage',} and \texttt{`network'} (see Figure \ref{fig:label_distribution}).

The manual inspection procedure for \textit{Home Assistant} includes an additional step. We observed that \homeassistant repeats a set of filenames across multiple \addons. For instance, \texttt{test\_config.py} appears in 552 different \addons, and \texttt{test\_sensor.py} appears in 267 \addons.  The test files with the same name serve similar testing objectives over different \addons; For example, \texttt{test\_notify.py} tests various types of asynchronous notifications for both Google Mail and Slack components and \addons. Leveraging this fact, we narrowed our focus to the top 43 filenames, each appearing more than 10 times. For each of these 43 filenames, we randomly selected 10 instances and conducted a manual inspection of the source code. We selected the top filenames since they represent the biggest \iot systems and 10 is the average to frequency. This yielded a total of 430 manually inspected files for \homeassistant. The number of inspected files for \openhab and \homeassistant constitutes a representative sample, providing a confidence level (z-score) above 95\% (see Tab.~\ref{tab:test_files_investigation}).

Table \ref{tab:test_files_investigation} summarizes our test file selection process. The labeling process was executed in two phases. After each phase, the authors met to finalize the labels and resolve disagreements. This collaborative approach ensured alignment and prevented mislabeling or focus on incorrect label categories.





