\subsection{Results from the Analysis}
In this section, we present our findings after applying our methodology for extracting both \textit{focal methods} (fm)  and \textit{focal test methods} (ftm) for \openhab and \homeassistant platforms. Table \ref{tab:test_files_investigation} outlines popular \addons for each platform ordered by their number of source files. Some \addons are well-known brands such as Alexa\cite{alexa}, Nest\cite{nest}, or Homekit\cite{homekit}. We calculate the test ratio as $ftm / (ftm + fm)$, with a threshold of 0.5 to flag low scores. While this threshold doesn't guarantee each \textit{fm} has a corresponding \textit{ftm}, it helps assess the \textit{ftm} distribution. We report the average and standard deviation.

\input{tabs/addons_tests}

In our analysis of \openhab, we identified $406$ \addons with a total of $\approx7K$ source files. We observe a low test ratio averaging only $0.04$. That means that there are more functional methods than methods for testing them. Notably, some components like SamsungTV lack any identified \ftm in Java versions. We found a total of $\approx76K$\ \fm, but just identified $\approx4K$ \ftm. Additionally, our analysis extended to extra \openhab repositories not included as \addons in the main project. For example, we observe that Z-Wave and Alexa exhibit test ratios of $0.25$ and $0.2$, respectively.



For \homeassistant, we observe higher test ratios observing an average of $0.42$ but with a high variability of $0.16$. Therefore, we could observe some components with even more \textit{ftm} than \textit{fm} (\ie recorder) but some components with a low test ratio (\eg homekit). Interestingly, well-known brands like Alexa and google\_assistant both with $0.38$ of test ratio.  We also noticed a lower number of test classes. This is due to the lack of class definitions, however, we identified $\approx40K$ \fm and $\approx32K$ \ftm across the total of \addons. 

In our analysis for \homeassistant, we identified  that $937$ \addons which contained test codes, only $327$ of those have a test ratio above the threshold of $0.5$. We select the $0.5$ threshold as the upper values represent beyond the half proportion on tested code, however, practitioners can use higher thresholds. In other words, \homeassistant reports $65\%$ of \addons with a poor number of tested methods. The same analysis for \openhab reports that just 3 \addons achieve this threshold. 


\finding{Only 3 out of 406  \openhab  \addons surpass our test ratio threshold of 0.5. \homeassistant has 327 \addons that have a better test over the 0.5 threshold but reports high variability with a standard deviation of 0.16.} \label{find:results1}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/Test_ratio_tested_components.pdf}
  
  \caption{Left: Test ratio per \addon. Right: Top ten tested components for \homeassistant and \openhab}
  \label{fig:label_distribution}
\end{figure}


Figure~\ref{fig:label_distribution} on the left, depicts the test ratio among the \addons and the labeling components within \addons on the right. The test ratio distribution demonstrates a larger number of implemented \addons for \homeassistant and a stable number of \textit{ftm} above 0.2 but below 0.5. Nevertheless, \openhab test ratio is mostly below 0.2. 
\homeassistant's exclusive tested components like \texttt{`switch'}, \texttt{`button'}, \texttt{`cover'}, and \texttt{`trigger'} indicate the platform's concentrated efforts towards refining user-facing elements and interaction mechanisms. In contrast, \openhab's exclusive tested components such as \texttt{`rule'}, \texttt{`authentication'}, and \texttt{`status'} reflect the platform's strong emphasis on rule-based automation, security, and system organization. 

The components common to both \homeassistant and \openhab, such as \texttt{`scene'}, \texttt{`light'}, \texttt{`configuration'}, and \texttt{`sensor'}, signify functionalities fundamental to any smart home platform. These components represent core elements necessary for device management, configuration settings, and environmental sensing. \texttt{`Events'}, \texttt{`control state'}, and \texttt{`status notification'} highlight the focus on real-time updates and event-driven actions, ensuring users stay informed about their smart home ecosystem's status (see Figure \ref{fig:label_distribution} top). These components signify a common commitment between \homeassistant and \openhab to ensure the reliability, functionality, and interoperability of essential features within smart homes.

\finding{\homeassistant developers concentrated on enhancing user-facing elements and interactions, whereas \openhab developers prioritized rule-based automation. However, both platform developers focused on the core functionalities of smart home platforms.} \label{find:results2}



\begin{boxK}
\ref{rq:common} \fnumber{1}  and \fnumber{2} Demonstrate that \openhab has a poor test ratio and \homeassistant has $65\%$ \addons below the test ratio threshold. The most common tested components are devices, network, and sensor parameters for \openhab and control state, device, and configuration for \homeassistant. \homeassistant exclusively tests components such as switch, button, and cover, while \openhab reports rule-based, status, and ecosystem group tests.
\end{boxK}


To answer \ref{rq:purpose} we provided 12 common test types commonly employed in regular software testing scenarios, such as web solutions, services, and applications  (\tabref{tab:findings}). We identify several of these testing types in our analyzed platforms. The simplest and most prevalent test type is the unit test. While \homeassistant and \openhab report usability tests, we note that the complete implementation and results are not available in the repositories. Brands and communities potentially could employ an issue tracker tool independent of the \github repositories. As a result, we categorize usability and regression testing as ``Maybe'', indicating a potential implementation. Interestingly, we observe automation pipelines for \homeassistant, encompassing platform deployment, and some device configuration and testing. However, similar scripts or pipelines are not evident for \openhab. Table~\ref{tab:findings} also maps the tests to participants' preferences, which we describe in Section~\ref{sec:survey}.

\input{tabs/test_process2}


\begin{boxK}
\ref{rq:purpose} Tests for API services and device-cloud communication insurance. Some scripts test databases and parameter configurations for multiple devices. Continuous Integration is also considered in the process.
\end{boxK}




