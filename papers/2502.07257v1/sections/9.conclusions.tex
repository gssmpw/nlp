\section{Conclusions}\label{sec:conclusions}

Our examination of the landscape of software testing within \iot platforms derived substantive insights, with 10 key findings from both a mining-based study and a survey with developers. 

We took a closer look at two specific platforms â€“ \homeassistant and \openhab. 
We find notable evidence signaling the difficulty that developers face with testing IoT platforms, with the majority of \addons and integration apps of both platforms falling short of the 50\% test coverage threshold.
On average, only 5\% \addons contains any test methods for \openhab and well-known brand like Amazon Alexa exhibits a maximum test ratio of 59\%.

A majority of our survey participants prefer automated testing, to try to catch problems early in the development process to save time and make things more reliable. They stress the need for consistent, repeatable tests and detailed logs. But interestingly, some developers still prefer manual testing as it facilitates human intuition and adaptability for real-world situations.

Finally, our research identifies challenges in fixing problems caused by software updates, cross-platform testing, and dealing with issues in low-power devices -- among others. In summary, our study sheds light on the testing practices, tools, perceptions, and challenges for IoT platforms, illustrating promising pathways for future research to improve testing for this rapidly growing domain. 

