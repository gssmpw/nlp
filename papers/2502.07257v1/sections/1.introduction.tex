\section{Introduction}
\label{sec:introduction}

The Internet of Things (IoT) platforms have proliferated the broader computing landscape in recent years and have ushered in new types of digital interactions. Some estimates project that the number of IoT devices will eclipse nearly 30 billion by 2030~\cite{iot-stats}. IoT devices power increasingly popular smart home ecosystems that aid users in automating daily tasks ranging from controlling lights to implementing home security~\cite{kafle_study_2019, manandhar_helion_2019}. IoT devices have also enabled new types of digital health ecosystems that help users to track their fitness and body~\cite{technogym}, and have even begun to power utilities at the scale of cities~\cite{singh-smart-cities}.  While all of these applications of IoT devices and ecosystems have afforded novel, convenient computing paradigms, the engineering of such systems is not without its challenges.

IoT ecosystems are inherently heterogeneous by nature and comprise smartphones, servers, devices, communication hubs, online services, and end-user applications (see \figref{fig:iot}). The design and engineering of each of the individual components of these ecosystems, alongside their integration, pose significant challenges to the design, implementation, maintenance, testing, and evolution of their software components\cite{makhshari2021}. 

Software testing practices, in particular, are important for IoT systems to ensure that both the functional and security/safety-related requirements are properly met. %
While 
prior studies have examined common bugs and general software development challenges for IoT platforms~\cite{makhshari2021,Corno:2019,Corno:2020}, we know little about the current state of software testing in consumer-oriented IoT platforms such as smart homes. 
That is, the research community has a limited understanding of typical software testing activities, processes, deficiencies, and challenges that engineers currently face while working across all of the typical components of IoT platforms. 

To address this research gap, this paper presents an in-depth study of the testing practices, and perspectives, in open source smart home platforms. Our study has two major components. First, we mine and empirically analyze the testing-related code ($\approx 37K$ test methods, from 12.904 test files) of two of the largest and most active open-source IoT platforms, \openhab~\cite{openhab} and \homeassistant~\cite{homeassistant}. Our choice to study these two popular platforms in-depth was guided by two key study design considerations: {\sf (i)} they represent testing practices across the range of IoT platform components, including, but not limited to, core platform code, device integrations, and integrations for online services, and {\sf (ii)} they are entirely open source, \ie not only is the platform code available, but so are the integrations, which are critical from a testing perspective. Using a rigorous, systematic, open-coding process, we identify the key purposes of testing-related activities and their prevalence across our studied platforms. Moreover, we quantify test coverage by defining the metric of {\em test ratio}, which leverages the notion of the {\em focal method (fm)} (\ie a method with defined parameters, which is to be tested~\cite{tufano_methods2test_2022}) and the {\em focal \underline{test} method (ftm)}, \ie a method to test all or part of an {\em fm)}. Intuitively, the test ratio can be defined as the ratio $ftm/ftm+fm$ (see Section~\ref{sec:bakground} for a detailed overview of {\em fm} and {\em ftm}).

Second, we conducted a survey of 80 open-source developers who work on IoT platforms and asked them about their testing practices, experiences, and perspectives regarding the testing of IoT platforms. 
A systematic, thematic analysis of these survey responses helped us understand the {\em rationale} behind current testing practices and pain points experienced by practitioners, and provided actionable insights.

Our study resulted in \textit{10 key findings} (\fnumber{1} -- \fnumber{10}) that characterize the current landscape of software developers' priorities (\fnumber{2}, \fnumber{8}, \fnumber{9}, \fnumber{10}), preferred approaches and perceptions regarding them (\fnumber{3}, \fnumber{4}), and challenges they face (\fnumber{5}, \fnumber{6}, \fnumber{7}) when testing \iot platforms. Particularly, we find a significant lack of testing of both platform components and integrations (\fnumber{1}), characterized by their computed {\em test ratio}.  
A majority, \ie sensors, control-state, sensor components and integrations with popular \addons such as `Hue', `Nest'\cite{nest}, `SamsungTV', `homekit' \addons fall below the test ratio of 0.5. 
Of \homeassistant's 937 \addons, 610 or 65\% have a test ratio of below 0.5 and an average test ratio of 0.42 (\fnumber{1}). Our analysis also reveals what types of test targets developers prioritize, \eg \openhab reports 16 \addons tests, 12 network tests, and includes rule-based and authentication tests (\fnumber{2}). 

Our analysis of survey responses reveals that developers generally focus on performance, scalability, real-world scenarios, real-time collaboration, and security testing (\fnumber{8}, \fnumber{10}). 
We find that while automated testing is the preferred choice among participants for identifying defects early with enhanced reliability (\fnumber{3}), some developers advocate manual testing for human intuition and adaptability in real-world scenarios (\fnumber{4}). Developers also expressed significant challenges unique to IoT platforms, particularly the validation of compatibility across devices and platforms, debugging of firmware updates, and the challenge of addressing issues in low-power mode (\fnumber{5}-\fnumber{7}). Finally, a few developers express concerns about poor documentation and the absence of organizational standards for testing \iot platforms (\fnumber{9}).

In summary, this paper makes the following contributions:

\begin{itemize}
    \item{An empirical investigation into the purpose of testing related code in the \openhab and \homeassistant open source projects.}
    \item{A survey and thematic analysis of responses from 80 open source developers who work on IoT ecosystems, to investigate developer perspectives and experiences regarding testing practices, tools, processes, and challenges.}
    \item{Ten findings derived from the above two studies that inform important directions for future research.}
    \item{A replication package that contains all of our data, analysis code, and qualitative results to facilitate reproduction and replication of our work~\cite{appendix}.}
\end{itemize}
