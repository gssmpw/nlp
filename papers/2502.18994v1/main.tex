
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



% added by weilin
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}  
\usepackage{subfigure}  
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fdsymbol}
\usepackage{dsfont}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{enumitem}

\def\Vbar{{\perp\!\!\!\perp}}
\def\NotVbar{\not{\perp\!\!\!\perp}}

\def\bR{{\mathbb R}}
\def\bE{{\mathbb E}}

\def\a{\mathbf{a}}
\def\b{\mathbf{b}}
\def\w{\mathbf{w}}
\def\x{\mathbf{x}}
\def\u{\mathbf{u}}
\def\s{\mathbf{s}}
\def\z{\mathbf{z}}
\def\r{\mathbf{r}}
\def\X{\mathbf{X}}
\def\U{\mathbf{U}}
\def\S{\mathbf{S}}
\def\C{\mathbf{C}}
\def\Z{\mathbf{Z}}
\def\R{\mathbf{R}}
\def\W{\mathbf{W}}
\def\b{\mathbf{b}}
\def\V{\mathbf{V}}

\def\cA{{\mathcal A}}
\def\cX{{\mathcal X}}
\def\cU{{\mathcal U}}
\def\cS{{\mathcal S}}
\def\cY{{\mathcal Y}}
\def\cR{{\mathcal R}}
\def\cZ{{\mathcal Z}}
\def\cE{{\mathcal E}}
\def\cM{{\mathcal M}}
\def\cN{{\mathcal N}}
\def\cF{{\mathcal F}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Long-term Causal Inference via Modeling Sequential Latent Confounding}

\begin{document}

\twocolumn[
\icmltitle{Long-term Causal Inference via Modeling Sequential Latent Confounding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Weilin Chen}{gdut}
\icmlauthor{Ruichu Cai}{gdut,pc}
\icmlauthor{Yuguang Yan}{gdut}
\icmlauthor{Zhifeng Hao}{st}
\icmlauthor{Jos\'e Miguel Hern\'andez-Lobato}{cam}
\end{icmlauthorlist}

\icmlaffiliation{gdut}{School of Computer Science, Guangdong University of Technology, Guangzhou, China}
\icmlaffiliation{pc}{Peng Cheng Laboratory, Shenzhen, China}
\icmlaffiliation{st}{College of Science, Shantou University, Shantou, China}
\icmlaffiliation{cam}{Department of Engineering, University of Cambridge, Cambridge, United Kingdom}

\icmlcorrespondingauthor{Ruichu Cai}{cairuichu@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document



\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Long-term causal inference is an important but challenging problem across various scientific domains.
To solve the latent confounding problem in long-term observational studies, existing methods leverage short-term experimental data.
\citet{ghassami2022combining} propose an approach based on the Conditional Additive Equi-Confounding Bias (CAECB) assumption, 
which asserts that the confounding bias in the short-term outcome is equal to that in the long-term outcome, so that the long-term confounding bias and the causal effects can be identified. 
While effective in certain cases, this assumption is limited to scenarios with a one-dimensional short-term outcome.
In this paper, we introduce a novel assumption that extends the CAECB assumption to accommodate temporal short-term outcomes.
Our proposed assumption states a functional relationship between sequential confounding biases across temporal short-term outcomes,
under which we theoretically establish the identification of long-term causal effects.
Based on the identification result, we develop an estimator and conduct a theoretical analysis of its asymptotic properties.
Extensive experiments validate our theoretical results and demonstrate the effectiveness of the proposed method.

\end{abstract}

\section{Introduction}
\label{intro}


Long-term causal inference is an important but challenging problem across various scientific fields, such as education \cite{athey2019surrogate}, medicine \cite{fleming1994surrogate}, and marketing  \cite{hohnhold2015focusing}.
While in many real-world scenarios, long-term observational data are readily available, a major challenge for long-term causal inference is the presence of latent confounding in observational studies.
A common way to mitigate this issue is incorporating short-term experimental data,
which raises a fundamental question:
how can short-term experimental data be leveraged to address latent confounding in observational data for long-term causal inference, as shown in Figure \ref{fig: causal graph}?


\begin{figure}[!t] 
	\centering
	% \subfigure[Experimental data $G=E$]{
	% 	\includegraphics[width=0.21\textwidth]{Fig/exp.pdf} 
	% 	\label{figure exp}
	% }
	% % \hfil
	% \subfigure[Observational data $G=O$]{
	% 	\includegraphics[width=0.21\textwidth]{Fig/obs.pdf}
	% 	\label{figure obs}
	% }
    % \subfigure[Observational data $G=O$]
    \includegraphics[width=0.5\textwidth]{Fig/graph_all.pdf}
	\caption{Causal graphs of experimental data and observational data with $\X$ being covariates, $\U$ being latent confounders, $A$ being treatment, $\S$ being short-term outcome, and $Y$ being long-term outcome. 
    Fig. \ref{fig: causal graph}(a) represents the causal graph of the short-term experimental data,
    where treatment $A$ is not affected by latent confounders $\U$ and the long-term outcome $Y$ is unobserved.
    Fig. \ref{fig: causal graph}(b) represents the causal graph of the long-term observational data, 
    where the latent confounders $\U$ affect treatments $A$ and outcomes $\S,Y$ and the long-term outcome $Y$ can be observed.
    Fig. \ref{fig: causal graph}(c) represents the overall graph with expanding temporal short-term outcomes.
    }
    \label{fig: causal graph} 
    \vspace{-.3cm}
\end{figure}


Existing works explore various methods to mitigate latent confounding by utilizing observational and experimental data based on different assumptions. 
One widely used assumption is the Latent Unconfoundedness assumption \cite{athey2020combining, chen2023semiparametric, yang2024estimating}, 
which posits the short-term potential outcomes mediate the long-term potential outcome in the observational data, i.e., $Y(a) \Vbar A | \S(a), \X, G=O$.
These methods are effective when short-term outcomes $\S$ contain substantial information about the latent confounder $\U$, 
such as in studies on the lifetime effects of youth employment and training programs in the United States \cite{aizer2024lifetime}.
However, this assumption essentially restricts that the latent confounders can not affect the long-term outcome, i.e., ruling out the causal edge $\U \rightarrow Y$.
To address this limitation, a follow-up work \cite{ghassami2022combining} proposes a novel assumption called Conditional Additive Equi-Confounding Bias (CAECB), allowing for the causal link $\U \rightarrow Y$.
The CAECB assumption states that the confounding bias in the short-term outcome is equal to that in the long-term outcome, enabling the identification of long-term causal effects.
This assumption may be more reasonable and more aligned with practical settings where confounding biases share a similar manner over short- and long-term outcomes conditional on covariates $\X$,
such as in research studying the impact of school finance reforms on student outcomes \cite{jackson2016effects}.


Following the work of \citet{ghassami2022combining}, we focus on the setting where the causal link $\U \rightarrow Y$ exists.
Beyond this, we further consider temporal short-term outcomes, as shown in Figure \ref{fig: causal graph}(c), a more common scenario where the method under the CAECB assumption \cite{ghassami2022combining} is not applicable.
In many real-world applications, 
short-term outcomes exhibit temporal dependencies, and capturing these relationships is essential for inferring long-term causal effects.
For example, in evaluating the long-term (e.g. year-long) effectiveness of the medication, patients undergo regular follow-up visits (e.g., weekly or monthly). 
During these visits, temporal short-term health indicators are recorded as short-term outcomes.
These sequential measurements capture the progression of patientsâ€™ conditions over time, providing valuable information on the long-term outcome of interest.
Consequently, the absence of temporal considerations in existing methods limits their effectiveness and constrains the potential of long-term causal inference in practice.


In this paper, we introduce a novel assumption called \textbf{Functional CAECB (FCAECB)}, which extends the existing CAECB assumption to capture the temporal dependencies among temporal short-term outcomes.
Roughly speaking, the proposed FCAECB assumption posits a functional relationship between sequential latent confounding biases.
Under this assumption, we establish the theoretical identification of long-term causal effects.
Correspondingly, we devise an algorithm for the long-term effect estimation. 
Additionally, we analyze the convergence rates of our proposed estimator in the asymptotic and finite sample setting within a generic nonparametric regression framework, 
with the ultimate goal of deepening the understanding of how sequential short-term confounding biases contribute to inferring long-term effects.
Overall, our contribution can be summarized as follows:
\begin{itemize}[leftmargin=8pt]
    \item \textbf{Novel Assumption for Long-Term Causal Inference:} We study the problem of long-term causal inference in the presence of temporal short-term outcomes.
    We propose a novel assumption named FCAECB for identifying long-term causal effects,
    which enables capturing the time-dependent relationships between sequential latent confounding biases. Note that the existing CAECB assumption can be seen as our special case.
    \item \textbf{Estimator for Heterogeneous Long-Term Effects:} We devise an estimator for estimating heterogeneous long-term effects under the proposed FCAECB assumption, 
    which can be implemented using any machine learning regression method. 
    Theoretically, we analyze the asymptotic properties of our estimator.
    \item \textbf{Empirical Validation:} We conduct extensive experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed estimator.
\end{itemize}

\section{Related Work}
\label{related work}

\textbf{Long-term causal inference}
For years, researchers have investigated which short-term outcomes can reliably predict long-term causal effects.
Various criteria have been proposed for identifying valid surrogates, including the Prentice criteria \cite{prentice1989surrogate}, principal surrogacy \cite{frangakis2002principal}, strong surrogate criteria \cite{lauritzen2004discussion}, causal effect predictiveness \cite{gilbert2008evaluating}, and consistent surrogate and its variations \cite{chen2007criteria, ju2010criteria, yin2020novel}.
Recently, there has been growing interest in estimating long-term causal effects using surrogates, which is also the focus of this paper.
One line of work assumes the unconfoundedness assumption.
Under the unconfoundedness assumption, LTEE \cite{cheng2021long} and Laser \cite{cai2024long} are based on specifically designed neural networks for long-term causal inference.
EETE \cite{kallus2020role} explores the data efficiency from the surrogate in several settings and proposes an efficient estimator for treatment effect.
ORL \cite{tran2023inferring} introduces a doubly robust estimator for average treatment effects using only short-term experiments, additionally assuming stationarity conditions between short-term and long-term outcomes.
\citet{singh2024double} proposes a kernel ridge regression-based estimator for long-term effect under continuous treatment.
Additionally, \citet{wu2024policy} develop a policy learning method for balancing short-term and long-term rewards.
Our work is \textbf{different} from them. We do not assume the unconfoundedness assumption, and we use the data combination technique to solve the problem of unobserved confounders.
Another line of research, which avoids the unconfoundedness assumption, tackles the latent confounding problem by combining experimental and observational data.
This setting is first introduced by the method proposed by \citet{athey2019surrogate}, 
which, under surrogacy assumption, constructs the so-called Surrogate Index (SInd) as the substitutions for long-term outcomes in the experimental data for effect identification.
As follow-up work, \citet{athey2020combining} introduces the latent unconfoundedness assumption, which assumes that short-term potential outcomes can mediate the long-term potential outcomes,  thereby enabling long-term causal effect identification.
Under this assumption, \citet{yang2024estimating, chen2023semiparametric} propose several estimators for effect estimation.
The alternative feasible assumptions \cite{ghassami2022combining} are proposed to replace the latent unconfoundedness assumption, e.g., the Conditional Additive Equi-Confounding Bias (CAECB) assumption.
Based on proximal methods \cite{tchetgen2024introduction}, \citet{imbens2022long} proposes considering the short-term outcomes as proxies of latent confounders, thereby achieving effect identification.
\textbf{However}, these studies primarily concentrate on average treatment effects, whereas our focus in this paper is on heterogeneous effects.
Among the existing literature, the most closely related work is \citet{ghassami2022combining}'s work.
Our work can be viewed as a significant extension of theirs, as we consider a more practical scenario where short-term outcomes exhibit temporal dependencies, and theoretically, their CAECB assumption is a special case of the assumption we propose.


\textbf{Modeling Latent Confounding Bias}
An effective way to solve the latent confounding problem in the data combination setting is to model latent confounding bias.
\citet{kallus2018removing} proposes modeling confounding bias under a linearity assumption.
\citet{hatt2022combining} introduce to model the nonlinear confounding bias using the representation learning technique.
\citet{wu2022integrative} propose the integrative R-learner via a regularization for the conditional effects and confounding bias with the Neyman orthogonality.
\citet{zhou2025two} propose a two-stage representation learning strategy to model such a confounding bias.
\textbf{Different} from these works, we focus on the long-term causal inference setting, and rather than focus on how to model the confounding bias, we concentrate more on the relationship between sequential confounding biases.


\section{Preliminaries}
\label{Notations}

\subsection{Notations, Problem Definition, Assumptions}

Let 
$A \in \{0,1\}$ be the treatment variable, 
$\X \in \cX \subseteq  \bR^{d}$ be the observed covariates where $d$ is the dimension of $\X$,
$\U \in \cU \subseteq \bR^{d_U}$ be the latent confounders where  $d_U$ is the dimension of $\U$.
Let $\S = [ S_1, S_2, \dots , S_T ] $ be the short-term outcome variable where $S_t \in \cS \subseteq \bR $ is the short-term outcome measured at timestep $t$,
and $Y=S_{T+\mu} \in \cR \subseteq \bR $ be the long-term outcome.
Further, we leverage the potential outcome framework proposed by \citet{rubin1978bayesian}.
We denote
$\S(a)$ as the potential short-term outcome, 
$S_t(a)$ as the potential short-term outcome at timestep $t$,
and $Y(a)$ as the potential long-term outcome.
Following existing work on long-term inference \cite{athey2020combining, ghassami2022combining, imbens2022long, chen2023semiparametric}, 
we denote $G\in \{E,O\}$ as the indicator of data group, where $G=E$ indicates experimental data and $G=O$ indicates observational data. 
Let lowercase letters (e.g., $a, \x, \u, \s, y, s(a), y(a)$) denote the value of the previously described random variables.
Let the superscript $(i)$ denote a specific unit, e.g., $\x^{(i)}$ is the covariate value of unit $i$. 
Then, the experimental data and the observational data are denoted as $\mathbb D_e =\{a^{(i)}, \x^{(i)}, \s^{(i)}, g^{(i)}=E\}_{i=1}^{n_e}$ and $\mathbb D_o = \{a^{(i)}, \x^{(i)}, \s^{(i)}, y^{(i)}, G^{(i)}=O\}_{i=n_e+1}^{n_e+n_o}$, where $n_e$ and $n_o$ are the size of experimental data and the observational data respectively. 
%Let the whole dataset denote as $\mathbb D = \mathbb D_e \cup \mathbb D_o$ of the size $n$ satisfying $n=n_e+n_o$.

For ease of convenience, we denote the following nuisance functions and confounding bias:
\begin{equation} \label{defined nuisance} 
    \begin{aligned}
      &  \mu_{S_t}^E(A,\X) = \bE [\S_t|A,\X,G=E], \\
      &  \mu_{S_t}^O(A,\X) = \bE [\S_t|A,\X,G=O], \\
      &  \mu_Y^E(A,\X) = \bE [Y|A,\X,G=E], \\
      &  \mu_Y^O(A,\X) = \bE [Y|A,\X,G=O], \\
      & \omega_t(\X) =  \mu_{S_t}^E(1,\X) - \mu_{S_t}^E(0,\X) + 
      \mu_{S_t}^O(0,\X) - \mu_{S_t}^O(1,\X),
    \end{aligned}
\end{equation}
where $\omega_t(\X)$ is known as the \textit{confounding bias},  the discrepancy between the conditional mean outcome differences derived from the experimental data and the observational data (see Sections \ref{sec: CAECB assumption} and \ref{sec: extension Identification} for more details on how it serves to the identification of long-term effects).

Moreover, we denote stochastic boundedness with $O_p$ and convergence in probability with $o_p$. We denote $X_1 \Vbar X_2$ as the independence between $X_1$ and $X_2$. We use $a_n \asymp b_n$ to denote both $a_n / b_n$ and $b_n / a_n$ are bounded. We use $a_n \lesssim b_n$ to denote both $a_n \leq C b_n$ for some constant $C > 0$.

\textbf{Task}: Given a short-term experimental dataset $\{a^{(i)}, \x^{(i)}, \s^{(i)}, g^{(i)}=E\}_{i=1}^{n_e}$ and a long-term observational dataset $\{a^{(i)}, \x^{(i)}, \s^{(i)}, y^{(i)}, g^{(i)}=O\}_{i=n_e+1}^{n_e+n_o}$, the goal is to identify the following causal estimand of interest: the Heterogeneous Long-term Causal Effects (HLCE), i.e.,
\begin{equation}
    \tau (\x) = \bE [Y(1)-Y(0)|\X=\x].
\end{equation}
However, HLCE $\tau (\x)$ is not identifiable from the experimental data alone, since $Y$ is missing in that dataset.
Also it is not identifiable from the observational data alone, since the observational data suffers from the latent confounding problem. 
Furthermore, the information regarding the causal effects in the experimental data is not necessarily relevant to that in the observational data without further assumptions.
To ensure the identification, we first make the following assumptions that are commonly assumed in long-term causal inference: 

\begin{assumption}[Consistency] \label{assum: consist}
If a unit is assigned treatment, we observe its associated potential outcome. 
Formally, if $A=a$, then $Y=Y(a)$, and $\S=\S(a)$.
\end{assumption}

\begin{assumption}[Positivity] \label{assum: positi}
The treatment assignment is non-deterministic. 
Formally, $\forall a,\x$, we have $0<P(A=a|\X=\x)<1$, and $0<P(G=O|A=a, \X=\x)<1$.
\end{assumption}

\begin{assumption} [Weak internal validity of observational data] \label{assum: internal validity of obs}
Latent confounders exist in observational data. 
Formally, $\forall a$, we have $A \Vbar \{Y(a),\S(a)\}|\X, \U, G=O$ and  $A \NotVbar \{Y(a),\S(a)\}|\X, G=O$.
\end{assumption}

\begin{assumption} [Internal validity of experimental data] \label{assum: internal validity of exp}
There are no latent confounders in experimental data.
Formally, $\forall a$, we have $A\Vbar \{Y(a),\S(a)\}|\X, G=E$.
\end{assumption}

\begin{assumption} [External validity of experimental data] \label{assum: external validity of exp}
The distribution of the potential outcomes is invariant to whether the data belongs to the experimental or observational data. Formally, $\forall a$, we have $G\Vbar \{Y(a),\S(a)\}|\X$.
\end{assumption}

Assumptions \ref{assum: consist} and \ref{assum: positi} are standard assumptions in causal inference \cite{rubin1974estimating, imbens2000role}. 
Assumptions \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp} and \ref{assum: external validity of exp} are mild and widely assumed in data combination settings \cite{shi2023data, imbens2022long, athey2019surrogate, athey2020combining, hu2023identification}. 
Specifically, Assumption \ref{assum: internal validity of obs} allows the existence of latent confounders in observational data, thus it is much weaker than the traditional unconfoundedness assumption.
Assumption \ref{assum: internal validity of exp} is reasonable and can hold since the treatment assignment mechanism is under control in the experiments.
Assumption \ref{assum: external validity of exp} connects the potential outcome distributions between observational and experimental data. 

Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp} and \ref{assum: external validity of exp} are still not sufficient to identify the causal estimand of interest. 
The root cause is that, even though the assumptions above link the experimental and observational data, the (long-term) latent confounding problem remains unsolved.
In the following section, we first review a method proposed by \citet{ghassami2022combining}, which poses an extra assumption, called Conditional Additive Equi-Confounding Bias (CAECB) assumption, to achieve the identification of HLCE. 
Then, we propose our approach that generalizes the method of \citet{ghassami2022combining} to allow temporal short-term outcomes.



\begin{figure*}[!t]
    \centering
    \subfigure[Schematic representation of CAECB assumption \cite{ghassami2022combining}.]
    {\includegraphics[width=.45\textwidth]{Fig/assumption.pdf}
     \label{fig: existing assumption}
    }
    % \hspace{18pt}
    \subfigure[Schematic representation of our proposed FCAECB assumption.]{\includegraphics[width=.52\textwidth]{Fig/assumption-ours.pdf}
        \label{fig: our assumption}
    }
    \caption{Schematic representations of CAECB assumption proposed by \citet{ghassami2022combining} and our FCAECB assumption. As shown in Figure \ref{fig: existing assumption}, the CAECB assumption requires that the confounding bias in the short-term outcome is equal to that in the long-term outcome. As shown in Figure \ref{fig: our assumption}, the FCAECB assumption relaxes this constraint by allowing for temporal short-term outcomes and only requiring that confounding biases across different time steps follow a specific pattern rather than remaining equal.}
    \label{fig: assumption}
    % \vspace{-.2cm}
\end{figure*}


\subsection{CAECB Assumption \cite{ghassami2022combining}}
\label{sec: CAECB assumption}

\citet{ghassami2022combining} introduced a method for combining experimental and observational data to identify the HLCE under an extra CAECB assumption.
The method under CAECB assumption is only applicable to scenarios with a one-dimensional short-term outcome, which we denote as $S$ (and its corresponding potential outcome $S(a)$) with slightly abusing notation.

To begin with, we first restate the CAECB assumption as follows:

\begin{assumption} [Conditional Additive Equi-Confounding Bias, CAECB \cite{ghassami2022combining}] \label{assum: equ bias}
The difference in conditional expected values of short-term potential outcomes across treated and control groups is the same as that of the long-term potential outcome variable.
Formally, $\forall a$, we have 
\begin{equation} \label{eq: equ bias}
\resizebox{0.94\linewidth}{!}{$
\begin{aligned}
    & \bE [S(a)|\X,A=0,G=O]-\bE[S(a)|\X,A=1,G=O]
    \\ = & \bE [Y(a)|\X,A=0,G=O]-\bE[Y(a)|\X,A=1,G=O].
\end{aligned}
$}
\end{equation}

\end{assumption}


\begin{theorem} \label{theo: identifi} Suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp} and \ref{assum: equ bias} hold, then $\tau(\x)$ can be identified as follows:
    \begin{equation} \label{eq: identi} 
      \begin{aligned}
       \tau(\x) 
            = & \mathbb E[Y(1) - Y(0)|\X=\x] \\
            = & \mu_Y^O(1,\x) - \mu_Y^O(0,\x)  + \omega(\x),
      \end{aligned}
    \end{equation}
    where $\omega(\x) = \mu_S^E(1,\x) -  \mu_S^E(0,\x)
             +  \mu_S^O(0,\x) - \mu_S^O(1,\x)$
             is the short-term confounding bias.
\end{theorem}

Proof can be found in Appendix \ref{app: proof of iden}. 
A similar identification result in terms of long-term average causal effects has been shown by \citet{ghassami2022combining}. Here we provide the identification result of HLCE $\tau(\x)$ in Theorem \ref{theo: identifi}.

\begin{remark}
We illustrate Assumption \ref{assum: equ bias} in Figure \ref{fig: existing assumption}.
Assumption \ref{assum: equ bias} can be seen as a generalization of the parallel assumption in the difference-in-differences (DiD) framework \cite{ashenfelter1984using,angrist2009mostly},
Note that Assumption \ref{assum: equ bias} essentially implies that the short-term confounding bias is the same as the long-term one.
In this way, $\omega(\x)$ can be interpreted as the long-term confounding bias, resulting in the identification result in Eq. \eqref{eq: identi}.
\end{remark}

Considering Eq. \eqref{eq: equ bias} in Assumption \ref{assum: equ bias}, $S$ and $Y$ should be on the same scale, which restricts the practical application of the method under this assumption.
In the next section, we extend this assumption to allow for temporal short-term outcomes, which significantly improve the practical utility.


\section{Long-term Identification under Functional CAECB Assumption}
\label{sec: extension Identification}

In this section, we introduce a novel assumption considering the temporal information between temporal short-term outcomes, enabling the identification of long-term causal effects. 

Specifically, we formalize our proposed assumption:

\begin{assumption} [Functional Conditional Additive Equi-Confounding Bias, FCAECB] 
\label{assum: time series equ bias}
The difference in conditional expected values of short-term potential outcomes across treated and control groups between two timesteps follows the learnable function forms $f: \bR \rightarrow \bR$.
Formally, 
$\forall a$, we have 
\begin{equation} \label{eq: time series equ bias}
\begin{aligned}
    b_{t+1}(a,\X) = f(\X) b_t(a,\X) ,
    % & \bE [S(a)|\X,A=0,G=O]-\bE[S(a)|\X,A=1,G=O]
    % \\ = & \bE [Y(a)|\X,A=0,G=O]-\bE[Y(a)|X,A=1,G=O].
\end{aligned}
\end{equation}
where $b_t(a,\X)=\bE [S_t(a)|\X,A=0,G=O]-\bE[S_t(a)|\X,A=1,G=O]$.
\end{assumption}

\begin{remark}
We illustrate our proposed FCAECB assumption in Figure \ref{fig: our assumption}. 
We relax the CAECB assumption \cite{ghassami2022combining} to allow for the temporal short-term outcomes, instead of restricting that the short-term outcome should be the same scale as the long-term outcome.
Additionally, the existing CAECB assumption can be seen as our special case when $f(\X)$ in the FCEACB assumption satisfies $f(\X)=1$.
\end{remark}

To provide a better understanding of our Assumption \ref{assum: time series equ bias}, we provide the insight in term of the functional form $f(\X)$ in the following proposition.

\begin{proposition} \label{propo: confounding bias function}
    Under Assumption \ref{assum: time series equ bias},  $\forall t$ the confounding biases between times $t$ and $t+1$ follow
\begin{equation}\label{eq: propo}
\begin{aligned}
    \omega_{t+1}(\X) = f(\X) \omega_t(\X) ,
\end{aligned}
\end{equation}
where $\omega_t(\X)$ is the confounding bias at time step $t$, defined as $\omega_t(\X) =  \mu_{S_t}^E(1,\X) - \mu_{S_t}^E(0,\X) + 
      \mu_{S_t}^O(0,\X) - \mu_{S_t}^O(1,\X)$.
\end{proposition}

The proof is given in Appendix \ref{app: proof propo}.
\begin{remark}
As stated in Proposition \ref{propo: confounding bias function}, Assumption \ref{assum: time series equ bias} essentially states the confounding biases $\omega_t(\X)$ between adjacent time steps follow the functional form $f(\X)$.
Proposition \ref{propo: confounding bias function} also illustrates the way of how to learn the function $f(\X)$ using the observed variables, unlike the definition in Eq. \eqref{eq: time series equ bias} defining $f(\X)$ using the potential outcomes (See Section \ref{Estimation} for the estimation based on Proposition \ref{propo: confounding bias function}).
\end{remark}

The key to identifying long-term causal effect under Assumption \ref{assum: time series equ bias} is using the temporal information in $f(\X)$ to extrapolate the long-term confounding bias $\omega_{T+\mu}(\X)$.
Using Eq. \eqref{eq: propo}, the long-term confounding bias $\omega_{T+\mu}(\X)$ can be expressed as $\omega_{T+\mu}(\X)=f^\mu(\X)\omega_{T}(\X)$, which results in the following theorem of long-term effect identification.

\begin{theorem} \label{theo: time series identifi} 
Suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp} and \ref{assum: time series equ bias} hold, then the heterogeneous long-term effects $\tau(\x)$ can be identified as follows:
    \begin{equation} \label{eq: time series identi}  
      \begin{aligned}
     \tau(\x) 
            = & \mathbb E[Y(1)-Y(0)|\X=\x] \\
            = & \mu_Y^O(1,\x) - \mu_Y^O(0,\x) +  f^\mu(\x) \omega_T(\x),
      \end{aligned}
    \end{equation}
    where $\omega_T(\x)=\mu_{S_T}^E(1,\x) -  \mu_{S_T}^E(0,\x)
             +  \mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x) $ is the short-term confounding bias at time step $T$.
\end{theorem}

The proof is given in Appendix \ref{app: proof of time series iden}. Theorem \ref{theo: time series identifi} provides the identification result of the heterogeneous long-term effects $\tau (\X)$.
The identification result consists of two parts: 1. long-term outcome differences in observational data $\mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x)$, and 2. long-term confounding bias $f^\mu(\x) \omega_T(\x)$. The long-term confounding bias is identified by the extrapolated result using short-term confounding bias $ \omega_ T(\x)$ under Assumption \ref{assum: time series equ bias}. 
The identification result also illustrates a way to estimate the long-term effects $\tau(\x)$ via modeling sequential latent confounding, which inspires our estimator as shown in Section \ref{Estimation}.



\section{Long-term Causal Effect Estimation}
\label{Estimation}

In this section, we first introduce our estimator $\hat \tau (\X)$ for heterogeneous long-term effects under our proposed Assumption \ref{assum: time series equ bias}, and provide the corresponding theoretical analysis of the proposed estimator.

\subsection{Estimator}

Our estimator $\hat \tau (\X)$ directly follows the identification result in Theorem \ref{theo: time series identifi}.
As shown in Eq. \eqref{eq: time series identi}, the estimators consist of three nuisance components, outcome mean difference between treated and control group in observational data $\mu_Y^O(1,\x) - \mu_Y^O(0,\x) $, confounding bias $\omega_T(\x)$, and the function between confounding bias $f(\x)$.
The first and the second terms can be directly estimated by fitting nuisance functions, and the third term $f(\x)$ should be estimated based on the fitted confounding biases.
Accordingly, we design our heterogeneous long-term effect estimator within a two-stage regression framework, which are model-agnostic algorithms that decompose the estimation task into multiple sub-problems, each solvable using any supervised learning/regression methods.
 
Specifically, our proposed estimator follows:

\begin{itemize}
    \item [S0.] (Optional) Selecting subsets of short-term outcomes $\S$, yielding appropriate $T$ and $\mu$; 
    \item [S1.] Fitting the following nuisance functions: 
    $\hat\mu_Y^O(a,\x)$, $\hat\mu_{S_t}^E(a,\x)$, and $\hat\mu_{S_t}^O(a,\x)$ for all $a$ and $t$;
    \item [S2.] Constructing the confounding bias  $\hat \omega_t(\x)=\hat \mu_{S_t}^E(1,\x) - \hat  \mu_{S_t}^E(0,\x)
             +  \hat \mu_{S_t}^O(0,\x) - \hat \mu_{S_t}^O(1,\x) $
    for all $t$, and fitting the function $\hat f(\x)$ by minimizing 
    \begin{equation} \label{eq: s2}
        arg\!\min _{f \in \cF } \Sigma_{t=1}^{T-1} \Sigma_{i=1}^n\left( \hat \omega_{t+1}(\x_i) -  f (\x_i) \hat \omega_t(\x_i) \right)^2;
    \end{equation}
    \item [S3.] Constructing final HLCE estimator as
    \begin{equation}
      \begin{aligned}
        & \hat \tau(\x) 
        =  \hat \mu_Y^O(1,\x) - \hat\mu_Y^O(0,\x) +  \hat f^\mu(\x)  \\
        & \times \left( \hat \mu_{S_T}^E(1,\x) -  \hat \mu_{S_T}^E(0,\x)
             +  \hat \mu_{S_T}^O(0,\x) - \hat \mu_{S_T}^O(1,\x) \right) .
    \end{aligned}
    \end{equation}
\end{itemize}

Note that, in addition to the fitting/constructing steps S1-S3, we also introduce an optional S0 to select subsets of short-term outcomes $\S$.
This step is motivated by the identification result in Eq. \eqref{eq: time series identi} that allows for different choices of $\mu$ and $T$.
For example, suppose we can observe $6$-step short-term outcome $\S=[S_1,S_2,\dots,S_6]$ and the long-term outcome of interest is $Y=S_9$. Then we have multiple choices of $T$ and $\mu$, e.g.,
using all short-term outcomes $\S=[S_1,S_2,\dots,S_6]$ with $T=6$ and $\mu=3$,
or using $\S=[S_1, S_3, S_5]$ with $T=3$ and $\mu=2$, 
or using $\S=[S_1, S_5]$ with $T=2$ and $\mu=1$.
We will discuss how to choose $T$ and $\mu$ in the next section.

\subsection{Convergence Rate Analyses}
\label{Theoretical Analyses}

In this paper, we assume the smoothness assumption of the estimated functions, where $s$-smooth functions are contained in the H\"older ball $\mathcal H _d(s)$, estimable with the minimax rate \cite{stone1980optimal} of $n^{\frac{1}{2+d/s}}$ where $d$ is the dimension of $\mathcal X$. Formally, we provide the following definition.

\begin{definition}[H\"older ball]  \label{def: holder ball}
The H\"older ball $\mathcal H _d(s)$ is the set of $s$-smooth functions $f: \mathbb R^d \rightarrow \mathbb R$ supported on $\mathcal X \subseteq \mathbb R^d$ that are $\lfloor s \rfloor $-times continuously differentiable with their multivariate partial derivatives up to order $\lfloor s \rfloor $ bounded, and for which
\begin{equation*}
    | \frac{\partial ^m f }{\partial^{m_1} \cdots \partial^{m_d} }(x) - \frac{\partial ^m f }{\partial^{m_1} \cdots \partial^{m_d} }(x^\prime) | \lesssim \| x - x^\prime\|^{s- \lfloor s \rfloor }_2,
\end{equation*}
$\forall x, x^\prime$ and $m=(m1, \cdots, m_d)$ such that $\Sigma_{j=1}^d m_j = \lfloor s \rfloor$. 
\end{definition}

\begin{assumption}[Smoothness Assumption] \label{asmp: smooth}
    We assume that the nuisance functions and the mapping $f$ defined in Assumption \ref{assum: time series equ bias} satisfy: (1) $\mu_{S_t}^E$, $\mu_{S_t}^O$, and $\mu_Y^O$ are $\alpha$-smooth, $\beta$-smooth, and $\gamma$-smooth, respectively, and all are estimable at \citet{stone1980optimal}'s minimax rate of $n^{\frac{-p}{2p+d}}$ for a $p$-smooth function; (2) $f(x)$ is $\eta$-smooth.
\end{assumption}

We also assume the boundedness assumption of the nuisance functions.
\begin{assumption}[Boundness Assumption] \label{asmp: bound}
    We assume that the nuisance functions $\mu_{S_t}^O(a,x),\mu_{S_t}^E(a,x)$ and their estimates are bounded, i.e., $\forall t$, $|\mu_{S_t}^O(a,x)| < C_1$, $|\hat \mu_{S_t}^O(a,x)| < C_2$, $|\mu_{S_t}^E(a,x)| < C_3$ and $|\hat \mu_{S_t}^E(a,x)| < C_4$ hold for some $C_1, C_2, C_3, C_4 >0$.   
\end{assumption}

% Assumptions \ref{asmp: smooth} and \ref{asmp: bound} are common and standard for the heterogeneous effect estimation analysis, e.g., \cite{kennedy2023towards, curth2021nonparametric}. Specifically, Assumption \ref{asmp: smooth} quantifies how difficult to non-parametrically fit the nuisance functions. Assumption \ref{asmp: bound} is standard in analyzing the error bound of the estimator.

We now state our main theoretical results.
To obtain our error bounds of the proposed estimator, we leverage the same sample splitting technique from \cite{kennedy2023towards}, which randomly splits the datasets into two independent sets and applies them to the regressions of the first step and second step respectively. Such a technique is originally used to analyze the convergence rate of the double robust conditional average treatment effect estimation in the traditional setting \cite{kennedy2023towards} and later is adapted to several other methods \cite{curth2021nonparametric, frauenestimating}. Different from them, we use such a technique for the sequential latent confounding modeling, which is then adapted for the long-term effect estimation.

To begin with, we first provide the rate of $\hat f(\x)$ as follows:

\begin{lemma} \label{lemma: convergence of f} 
    Suppose the training steps S1 and S2 are train on two independent datasets of size $n$ respectively, and suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp}, \ref{assum: time series equ bias}, and \ref{asmp: smooth} hold, then we have
        \begin{equation} \label{eq: f rate}  
      \begin{aligned}
       \hat f(\x) - & f(\x) 
         =  O_p \left( 
            (\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}} 
            \right.\\  & \left. 
            +  (\frac{1}{(T-1)n})^{\frac{\alpha}{2\alpha+d}}
            +  (\frac{1}{(T-1)n})^{\frac{\beta}{2\beta+d}}
            \right).
      \end{aligned}
    \end{equation}
\end{lemma}

The proof is given in Appendix \ref{app: proof f conver}.
Since $\hat f (\x)$ is a time-series model, its effective sample is $(T-1)n$, thus it might achieve a faster rate if we observed a longer duration of short-term outcomes.
Moreover, the rate of $\hat f(\x)$ consist two part: the oracle rate $(\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}} $, and the rate of fitting nuisance functions $(\frac{1}{(T-1)n})^{\frac{\alpha}{2\alpha+d}} + (\frac{1}{(T-1)n})^{\frac{\beta}{2\beta+d}}$. If the nuisance functions $\mu_{S_t}^E$ and $\mu_{S_t}^O$ is smooth enough such that $\min \{\alpha, \beta \} \geq \eta$, then $\hat f (\x)$ will attain the oracle rate.

Based on Lemma \ref{lemma: convergence of f}, we provide the rate of $\hat \tau(\x)$ in the following theorem.

\begin{theorem} \label{theo: convergence} 
Suppose Lemma \ref{lemma: convergence of f} hold, then we have
    \begin{equation} \label{eq: tau rate}  
      \begin{aligned}
       & \hat \tau(\x) -   \tau(\x)  \\ 
          = &  O_p \left(  
          n^{-\frac{\gamma}{2\gamma+d}} 
          + n^{-\frac{\alpha}{2\alpha+d}} 
          + n^{-\frac{\beta}{2\beta+d}}
           + \frac{\mu}{ ((T-1)n)^{\frac{\eta}{2\eta+d}} }  
           \right. \\ & \left.
           + \frac{\mu}{ ((T-1)n)^{\frac{\alpha}{2\alpha+d}} }  + 
           \frac{\mu}{ ((T-1)n)^{\frac{\beta}{2\beta+d}} }  
            \right).
      \end{aligned}
    \end{equation}
\end{theorem}

Proof can be found in Appendix \ref{app: proof tau conver}. 
Theorem \ref{theo: convergence} follows directly from Lemma \ref{lemma: convergence of f}.
The rate of $\hat \tau (\x)$ consist of several terms: 1. $n^{-\frac{\gamma}{2\gamma+d}} $ represents the rate of $\mu_Y^O(a,\x)$, 2. $n^{-\frac{\alpha}{2\alpha+d}} + n^{-\frac{\beta}{2\beta+d}}$ corresponds the rate of confounding bias $\omega_T(\x)$, 3. and the remaining term is the rate of $\hat f^\mu (\x)$. 
Theorem \ref{theo: convergence} suggests that the convergence rate is primarily influenced by the smoothness parameters $\alpha, \beta, \gamma, \eta$ and the temporal parameters $T, \mu$, where $T$ represents the longest observed duration of the short-term outcomes $\S$ and $\mu$ represents the time horizon of the long-term outcome to be estimated.
This implies that achieving a more accurate effect estimation requires observing short-term outcomes for as long as possible.


\begin{remark}[Choosing $\mu$ and $T$]
    Note that, the bound in Eq. \eqref{eq: tau rate} contains the term $\frac{\mu}{(T-1)^\frac{\alpha}{2\alpha+d}}$ (similar for $\beta$ and $\eta$).
    If we have prior knowledge of the smoothness of nuisance functions, i.e., knowing $\alpha$ ($\beta$ and $\eta$), then we can optimally select $\mu$ and $T$ to obtain a faster rate.
    For instance, if all estimated functions are sufficiently smooth, i.e., $\alpha,\beta,\eta \rightarrow \infty$, then $\mu$ and $T$ should be chosen to minimize $\frac{\mu}{\sqrt{T-1}}$, since $\frac{\alpha}{2\alpha+d}, \frac{\beta}{2\beta+d}, \frac{\eta}{2\eta+d}  \rightarrow 1/2$.
    As a concrete example, suppose we can observe $6$-step short-term outcome $\S=[S_1,S_2,\dots,S_6]$ and the long-term outcome of interest is $Y=S_9$. The optimal choice is to use $\S=[S_1,S_5]$ only to estimate $\tau(\x)$, since it result in a minimum $\frac{\mu}{\sqrt{T-1}}=1$ where $\mu=1$ and $T=2$.
    In practice, when no prior knowledge is available, it may be advisable to choose $\mu$ as small as possible, since a larger $\mu$-power in $\hat f ^\mu (\x)$ could lead to significant errors.
\end{remark}

\section{Experiments}
\label{Experiments}

In this section, we perform a series of experiments to evaluate the effectiveness and validity of our proposed estimator.
We first provide the experimental setup in Section \ref{sec: setup}, and provide the experimental results and corresponding analyses in Section \ref{sec: result}.

% Specifically, we answer the following research questions (RQs):
% \begin{itemize}
%     \item \textbf{RQ1 (Effectiveness):} Can the proposed estimator achieve accurate long-term effect estimation?
%     \item \textbf{RQ2 (Comparision Performance):} Can the proposed estimator outperform other methods in terms of long-term effect estimation?
%     \item \textbf{RQ3 (Sample Sensitivity):} Are our proposed methods sensitive to sample size?
% \end{itemize}

\subsection{Experimental Setup}
\label{sec: setup}

In this section, we describe the data generation process for the synthetic datasets, the implementation details, and the metrics used in the experiments.

\textbf{Synthetic Datasets}
The data generation process is partly following \cite{kallus2018removing} such that we can obtain a specific form of confounding bias.
Specifically, we first generate the treatments as follows: $A|G=O \sim \text{Bernoulli}(0.6)$ and $A|G=E \sim \text{Bernoulli}(0.4)$. Then we generate the observed $\X$ and the unobserved $\U$ as follows:
   \begin{equation} 
    \begin{aligned}
       & (\X,\U)|A,G=E \sim \mathcal N ([\frac{2A-1}{2},0], 
       \begin{bmatrix} 1 & 0 \\ 0 & 1
       \end{bmatrix})
       \\  
       & (\X,\U)|A,G=O \sim \mathcal N ([\frac{1-2A}{2},0], 
       \begin{bmatrix} 1 & A-0.5 \\ A-0.5 & 1
       \end{bmatrix}).
    \end{aligned}
    \end{equation}
%which effectively introduces the unobserved confounding in the observational group.
Finally, we generate the $T$-step short-term outcomes $\S$ and the long-term outcome $Y=S_\mu$ satisfying  Assumption \ref{assum: time series equ bias} as follows:
   \begin{equation} 
    \begin{aligned}
       & S_t(1) = 1 + 1.1 \X + \U + \sum_{k=0}^t S_k(1) + \epsilon_{S_t},
       \\&
       S_t(0) =  \X + \U + \sum_{k=0}k^t S_k(0) + \epsilon_{S_t},
    \end{aligned}
    \end{equation}
where $\epsilon_{S_t}$ are  Gaussian noises. This will result in non-equal confounding bias in different time steps $t$, i.e., $\omega_t(\x)=t\times \x$. 
We perform several control experiments using this data generation process.
The default values are: the observational data sample size $n_o=4000$, experimental data sample size $n_e=2000$, $T=6$, and $\mu=3$.
In control experiments, we vary $\mu$ within $\{1,2,3,4,5\}$ and $T$ within $T \in \{4,5,6,7,8\}$.
We also fix the ratio of $n_O:n_E=2:1$, and varying $n_e$ within $\{1000,2000,3000,4000,5000,10000\}$ (which corresponds to $n_o$ values of $\{2000,4000,6000,8000,10000,20000\}$).

\textbf{Implementation}
We fit all nuisance functions using correctly specified regression.
We denote our estimator as $\hat \tau$.
For comparison, we evaluate our method against a correctly specified T-Learner \cite{kunzel2019metalearners}, denoted as $\hat \tau_{exp}$, which directly regresses unobserved $Y$ in the experimental data.
However, this estimator $\hat \tau_{exp}$ is infeasible in practice since $Y$ is missing in the experimental data.
This baseline serves as an idealized benchmark,
and comparing our method against it highlights the effectiveness of our estimator across different control experiments.


\textbf{Metrics} As for heterogeneous effect estimation, we report Precision in the Estimation of Heterogeneous Effect (PEHE) $\varepsilon_{PEHE}= \sqrt{\frac{1}{n} \Sigma_{i=1}^{n} (\tau(\x_i) - \hat \tau(\x_i))^2}$. As for average long-term causal effect estimation, we report the absolute error $\varepsilon_{ATE}= | \Sigma_{i=1}^{n} \tau(\x_i) -\Sigma_{i=1}^{n}  \hat \tau(\x_i)) |$. 
For all metrics, we report the mean values and standard errors by $100$ times running. 

\subsection{Experimental Results}
\label{sec: result}

\begin{figure}[!h]
    \vspace{-.1cm}
    \centering
    \subfigure[Heterogeneous effect estimation error.]
    {\includegraphics[width=.22\textwidth]{Fig/PEHE_OPTIMAL.pdf}
     \label{fig: PEHE_OPTIMAL}
    }
    % \hspace{18pt}
    \subfigure[Average effect estimation error.]{\includegraphics[width=.22\textwidth]{Fig/MAE_OPTIMAL.pdf}
        \label{fig: MAE_OPTIMAL}
    }
        \vspace{-.1cm}
    \caption{Results of the experiments in terms of different choice of $\mu$ and $T$.}
    \label{fig: optimal}
    \vspace{-.3cm}
\end{figure}

\textbf{Optimal Choice of $T$ and $\mu$}: We conduct experiments using different choice of $T$ and $\mu$. 
Specifically, to predict long-term effects, we consider the following subsets of $\S$: 
1. $\S=[S_1,S_3]$ with $T=2, \mu=3$; 
2. $\S=[S_1,S_3,S_5]$ with $T=3, \mu=2$; 
3. $\S=[S_1,S_2,S_3,S_4,S_5]$ with $T=5, \mu=3$; 
and 4. $\S=[S_1,S_5]$ with $T=2, \mu=1$.
The results, shown in Figure \ref{fig: optimal}, are sorted by the values of $\frac{\mu}{\sqrt{T-1}}$, which $3, 1.41, 1.34$, and $1$, respectively.
As shown in Figure \ref{fig: optimal}, as  $\frac{\mu}{\sqrt{T-1}}$ decrease, both the estimation errors $\varepsilon_{PEHE}$ and $\varepsilon_{ATE}$ decrease. This aligns with our theoretical findings in Theorem \ref{theo: convergence}, as a smaller $\frac{\mu}{\sqrt{T-1}}$ leads to a faster rate.
Interestingly, as shown in Figure \ref{fig: MAE_OPTIMAL}, we found that when the $\frac{\mu}{\sqrt{T-1}}$ is equal to $1$, our estimator $\hat \tau_(\x)$ achieves lower $\varepsilon_{ATE}$ than the idealized estimator $\hat \tau$. This may be attributed to the fact that our estimator $\hat \tau(\x)$ leverages observational data, leading to higher data efficiency.


\begin{figure}[!h]
    % \vspace{-.1cm}
    \centering
    \subfigure[Heterogeneous effect estimation error with fixed $T$ and varying $\mu$.]
    {\includegraphics[width=.22\textwidth]{Fig/PEHE_VARYT.pdf}
     \label{fig: PEHE varyT}
    }
    % \hspace{18pt}
    \subfigure[Average effect estimation error  with fixed $T$ and varying $\mu$.]
    {\includegraphics[width=.22\textwidth]{Fig/ATE_VARYT.pdf}
        \label{fig: MAE varyT}
    }
    \subfigure[Heterogeneous effect estimation error with fixed $\mu$ and varying $T$.]
    {\includegraphics[width=.22\textwidth]{Fig/PEHE_VARY_t.pdf}
     \label{fig: PEHE varymu}
    }
    % \hspace{18pt}
    \subfigure[Average effect estimation error with fixed $\mu$ and varying $T$.]
    {\includegraphics[width=.22\textwidth]{Fig/ATE_VARY_t.pdf}
        \label{fig: MAE varymu}
    }
        \vspace{-.1cm}
    \caption{Results of control experiments with fixed $T$ and varying $\mu$ (Figures \ref{fig: PEHE varyT} and \ref{fig: MAE varyT}), and with fixed $\mu$ and varying $T$ (Figures \ref{fig: PEHE varymu} and \ref{fig: MAE varymu}).}
    \label{fig: varying t mu}
        \vspace{-.3cm}
\end{figure}


\textbf{Vary $T$ and $\mu$}: 
We conduct control experiments with varying $T$ and varying $\mu$ separately.
First, Figures \ref{fig: PEHE varyT} and \ref{fig: MAE varyT}) show the results for the experiments with fixed $T$ and varying $\mu$. 
As expected, the estimation errors on both heterogeneous and average effect estimations increase as the long-term horizon $\mu$ grows. 
This is primarily due to the increasing estimation errors of $\hat f^{\mu}(\x)$, as established in Lemma \ref{lemma: convergence of f}.
Second, Figures \ref{fig: PEHE varymu} and \ref{fig: MAE varymu} show the results for the experiments with fixed $\mu$ and varying $T$.
As the observed duration $T$ increases, the estimation errors decrease rapidly.
It is reasonable since the larger $T$ results in a faster rate of $\hat \tau(\x)$.
Both findings further support our Lemma \ref{lemma: convergence of f} and Theorem \ref{theo: convergence} again.
Moreover, we observe that, with smaller $\mu$, our estimator closely matches the idealized estimator $\hat \tau_{exp}(\x)$, showing the effectiveness of our estimator.

 
\begin{figure}[!h]
    % \vspace{-.1cm}
    \centering
    \subfigure[Heterogeneous effect estimation error.]
    {\includegraphics[width=.22\textwidth]{Fig/PEHE_VARYN.pdf}
     \label{fig: PEHE_OPTIMAL}
    }
    % \hspace{18pt}
    \subfigure[Average effect estimation error.]
    {\includegraphics[width=.22\textwidth]{Fig/ATE_VARYN.pdf}
        \label{fig: MAE_OPTIMAL}
    }
        \vspace{-.1cm}
    \caption{Results of control experiments with varying sample sizes $n_e$ and $n_o$.}
    \label{fig: sample sensitivity}
        \vspace{-.3cm}
\end{figure}

\textbf{Sample sensitivity}:
We conduct control experiments by varying the sample sizes of both experimental and observational data.
The results are presented in Figure \ref{fig: sample sensitivity}.
As expected, the estimator's performance improves as the sample sizes $n_o$ and $n_e$ increase.
Notably, when $n_o$ and $n_e$ become larger, our estimator $\hat \tau(\x)$ closely approaches the idealized estimator $\hat \tau_{exp}(\x)$, which again confirms the effectiveness of our estimator.
Furthermore, when the sample size $n_e \geq 3000$ ($n_e \geq 6000$), the estimation errors become stable, indicating an appropriate sample size threshold.

\section{Conclusion}
\label{Conclusion}

In this paper, we extend the existing CAECB assumption \cite{ghassami2022combining} for long-term causal inference.
The original assumption is restricted to settings with a one-dimensional short-term outcome.
To address this limitation, we introduce a more general assumptionâ€”the functional CAECB assumptionâ€”which accommodates temporal short-term outcomes.
We theoretically establish the identification result of the heterogeneous long-term causal effect under our assumption and propose a corresponding estimator by modeling sequential latent confounding.
Additionally, we provide a comprehensive theoretical analysis of the estimator.
Experiments confirm both the validity of our theoretical results and the effectiveness of the proposed estimator.
 
% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Long-term causal inference in the presence of latent confounders.
There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proof of Theorem \ref{theo: identifi}}\label{app: proof of iden}
We first restate Theorem \ref{theo: identifi} as follows:
% We provide the proof Theorem \ref{theo: identify  }.
\begin{theorem} 
Suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp} and \ref{assum: equ bias} hold, then $\tau(\x)$ can be identified as follows:
    \begin{equation}
      \begin{aligned}
       & \tau(\x) \\
            = & \mathbb E[Y(1) - Y(0)|\X=\x] \\
            = & \mu_Y^O(1,\x) - \mu_Y^O(0,\x)  
             +  \mu_S^E(1,\x) -  \mu_S^E(0,\x)
             +  \mu_S^O(0,\x) - \mu_S^O(1,\x) .
      \end{aligned}
    \end{equation}
\end{theorem}


\begin{proof}
    \begin{equation} \label{eq: tau decom} 
        \begin{aligned}
            & \tau(\X)  \\
            = & \mathbb E[Y(1)|\X,G=O] - \mathbb E[Y(0)|\X,G=O] \\
            = & \mathbb E[Y(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[Y(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                 + \mathbb E[Y(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
            = & \mathbb E[Y(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[Y(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & + \mathbb E[Y(1)|\X,G=O,A=1]p(A=0|\X,G=O)
                 - \mathbb E[Y(1)|\X,G=O,A=1]p(A=0|\X,G=O) \\
              & + \mathbb E[Y(0)|\X,G=O,A=0]p(A=1|\X,G=O)
                - \mathbb E[Y(0)|\X,G=O,A=0]p(A=1|\X,G=O) \\
            = & \mathbb E[Y(1)|\X,G=O,A=1] - \mathbb E[Y(0)|\X,G=O,A=0] \\
            & + \{ \mathbb E[Y(1)|\X,G=O,A=0] - \mathbb E[Y(1)|\X,G=O,A=1]\}  \times p(A=0|\X,G=O) \\
              & + \{ \mathbb E[Y(0)|\X,G=O,A=0] - \mathbb E[Y(0)|\X,G=O,A=1]\} \times p(A=1|\X,G=O) \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + \{ \mathbb E\left[ S(1) |\X,G=O,A=0\right] - \mathbb E\left[ S(1)  |\X,G=O,A=1\right] \} \times p(A=0|\X,G=O) \\
              & + \{ \mathbb E\left[ S(0) |\X,G=O,A=0\right] - \mathbb E\left[ S(0) |\X,G=O,A=1\right]  \} \times p(A=1|\X,G=O) , 
        \end{aligned}
    \end{equation}
    where the first equality is based on Assumption \ref{assum: external validity of exp} and the last equality is based on Assumption \ref{assum: equ bias}.
    Similarly, for short-term conditional causal effects, we have:
    \begin{equation} \label{eq: stce decom}
        \begin{aligned}
            & \mathbb E[S(1)|\X,G=O] - \mathbb E[S(0)|\X,G=O] \\
            = & \mathbb E[S|\X,G=O,A=1] - \mathbb E[S|\X,G=O,A=0] \\
              & + \{ \mathbb E[S(1)|\X,G=O,A=0] - \mathbb E[S(1)|\X,G=O,A=1]\} \times p(A=0|\X,G=O) \\
              & + \{ \mathbb E[S(0)|\X,G=O,A=0] - \mathbb E[S(0)|\X,G=O,A=1]\} \times p(A=1|\X,G=O) \\
        \end{aligned}
    \end{equation}
    Then, combining Eq. \eqref{eq: tau decom} and \eqref{eq: stce decom}, we have
    \begin{equation} 
        \begin{aligned}
            & \tau(\X) \\
            = & \mathbb E[Y(1)|\X,G=O] - \mathbb E[Y(0)|\X,G=O] \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + \mathbb E[S(1)|\X,G=O] - \mathbb E[S(0)|\X,G=O] 
              - \mathbb E[S|\X,G=O,A=1] + \mathbb E[S|\X,G=O,A=0] \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + \mathbb E[S(1)|\X,G=E] - \mathbb E[S(0)|\X,G=E] 
              - \mathbb E[S|\X,G=O,A=1] + \mathbb E[S|\X,G=O,A=0] \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + \mathbb E[S|\X,G=E,A=1] - \mathbb E[S|\X,G=E,A=0] 
               - \mathbb E[S|\X,G=O,A=1] + \mathbb E[S|\X,G=O,A=0]
                \\
            = & \mu_Y^O(1,\X) - \mu_Y^O(0,\X)  
             +  \mu_S^E(1,\X) -  \mu_S^E(0,\X)
             +  \mu_S^O(0,\X) - \mu_S^O(1,\X) ,
        \end{aligned}
    \end{equation}
    where the second equality is based on Assumption \ref{assum: external validity of exp} and the last equality is based on Assumption \ref{assum: internal validity of exp}. 
    This finishes our proofs.
\end{proof}
\newpage

\section{Proof of Proposition \ref{propo: confounding bias function}}
\label{app: proof propo}

We first restate the proposition as follow:
\begin{proposition}
    Under Assumption \ref{assum: time series equ bias},  $\forall t$ the confounding biases between times $t$ and $t+1$ follow
\begin{equation}
\begin{aligned}
    \omega_{t+1}(\X) = f(\X) \omega_t(\X) ,
\end{aligned}
\end{equation}
where $\omega_t(\X)$ is the confounding bias at time step $t$, defined as $\omega_t(\X) =  \mu_{S_t}^E(1,\X) - \mu_{S_t}^E(0,\X) + 
      \mu_{S_t}^O(0,\X) - \mu_{S_t}^O(1,\X)$.
\end{proposition}

\begin{proof}
    We start from the long-term causal effects at time step $t$ in experimental data $G=E$:
    \begin{equation}
        \begin{aligned}
            & \mathbb E[S_t(1)|\X,G=E] - \mathbb E[S_t(0)|\X,G=E] \\
            \overset{(a)}{=} & \mathbb E[S_t(1)|\X,G=O] - \mathbb E[S_t(0)|\X,G=O] \\
            = & \mathbb E[S_t(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[S_t(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[S_t(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[S_t(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
            = & \mathbb E[S_t(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[S_t(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[S_t(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[S_t(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & + \mathbb E[S_t(1)|\X,G=O,A=1]p(A=0|\X,G=O)
                - \mathbb E[S_t(1)|\X,G=O,A=1]p(A=0|\X,G=O) \\
              & + \mathbb E[S_t(0)|\X,G=O,A=0]p(A=1|\X,G=O)
                - \mathbb E[S_t(0)|\X,G=O,A=0]p(A=1|\X,G=O) \\
            = & \mathbb E[S_t(1)|\X,G=O,A=1] - \mathbb E[S_t(0)|\X,G=O,A=0] \\
              & + \{ \mathbb E[S_t(1)|\X,G=O,A=0] - \mathbb E[S_t(1)|\X,G=O,A=1]\}p(A=0,\X,G=O) \\
              & + \{ \mathbb E[S_t(0)|\X,G=O,A=0] - \mathbb E[S_t(0)|\X,G=O,A=1]\}p(A=1,\X,G=O) \\
            = & \mathbb E[S_t|\X,G=O,A=1] - \mathbb E[S_t|\X,G=O,A=0] \\
              & + b_t(1,\X) p(A=0,\X,G=O) - b_t (0,\X) p(A=1,\X,G=O), \\
        \end{aligned}
    \end{equation}
    where the equality $(a)$ is based on Assumption \ref{assum: external validity of exp}. By rewriting the last equality above, we obtain:
     \begin{equation} \label{app: eq omega t}
        \begin{aligned}
            & \mathbb E[S_t(1)|\X,G=E] - \mathbb E[S_t(0)|\X,G=E] \\
            = & \mathbb E[S_t|\X,G=O,A=1] - \mathbb E[S_t|\X,G=O,A=0] \\
              & + b_t(1,\X) p(A=0,\X,G=O) - b_t (0,\X) p(A=1,\X,G=O), \\
            \iff 
            &\mathbb E[S_t(1)|\X,G=E] - \mathbb E[S_t(0)|\X,G=E] - \mathbb E[S_t|\X,G=O,A=1] + \mathbb E[S_t|\X,G=O,A=0] \\
            = & b_t(1,\X) p(A=0,\X,G=O) - b_t (0,\X) p(A=1,\X,G=O), \\
            \iff &
            \mu_{S_t}^E(1,\X) - \mu_{S_t}^E(0,\X) + 
      \mu_{S_t}^O(0,\X) - \mu_{S_t}^O(1,\X) \\
            = & b_t(1,\X) p(A=0,\X,G=O) - b_t (0,\X) p(A=1,\X,G=O), \\
            \overset{(a)}{\iff} &
            \omega_t(\X)
            = b_t(1,\X) ) p(A=0,\X,G=O) - b_t (0,\X) p(A=1,\X,G=O), \\
        \end{aligned}
    \end{equation}
    where $(a)$ is based on the definition of $\omega_t(\X)$. Then, similarly for time step $t+1$, we have
     \begin{equation}
        \begin{aligned}
           \omega_{t+1}(\X)
             & = b_{t+1}(1,\X) p(A=0,\X,G=O) -b_{t+1}(0,\X) p(A=1,\X,G=O), \\
            & \overset{(a)}{=} f(\X) \left(b_{t}(1,\X) p(A=0,\X,G=O) -b_{t}(0,\X) p(A=1,\X,G=O) \right)  \\
            & \overset{(b)}{=} f(\X) \omega_t(\X),
        \end{aligned}
    \end{equation}
    where $(a)$ is based on Assumption \ref{assum: time series equ bias}, i.e., $ b_{t+1}(a,\X) = f(\X) b_t(a,\X) $, and $(b)$ is based on Eq. \eqref{app: eq omega t} and finishes our proof.
\end{proof}

\newpage

\section{Proof of Theorem \ref{theo: time series identifi}}
\label{app: proof of time series iden}
% We provide the proof Theorem \ref{theo: identifi}.
We first restate Theorem \ref{theo: time series identifi} as follows:

\begin{theorem}
Suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp} and \ref{assum: time series equ bias} hold, then $\tau(\x)$ can be identified as follows:
    \begin{equation}
      \begin{aligned}
       & \tau(\x) \\
            = & \mathbb E[Y(1)-Y(0)|\X=\x] \\
            = & \mu_Y^O(1,\x) - \mu_Y^O(0,\x)
            +  f^\mu(\X)  \left(\mu_{S_T}^E(1,\x) -  \mu_{S_T}^E(0,\x)
             +  \mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x) \right) .
      \end{aligned}
    \end{equation}
\end{theorem}

\begin{proof}
    \begin{equation}
        \begin{aligned}
            \tau(\X) 
            = & \mathbb E[Y(1)|\X,G=O] - \mathbb E[Y(0)|\X,G=O] \\
            = & \mathbb E[Y(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[Y(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
            = & \mathbb E[Y(1)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(1)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & - \mathbb E[Y(0)|\X,G=O,A=1]p(A=1|\X,G=O) 
                + \mathbb E[Y(0)|\X,G=O,A=0]p(A=0|\X,G=O) \\
              & + \mathbb E[Y(1)|\X,G=O,A=1]p(A=0|\X,G=O)
                - \mathbb E[Y(1)|\X,G=O,A=1]p(A=0|\X,G=O) \\
              & + \mathbb E[Y(0)|\X,G=O,A=0]p(A=1|\X,G=O)
                - \mathbb E[Y(0)|\X,G=O,A=0]p(A=1|\X,G=O) \\
            = & \mathbb E[Y(1)|\X,G=O,A=1] - \mathbb E[Y(0)|\X,G=O,A=0] \\
              & + \{ \mathbb E[Y(1)|\X,G=O,A=0] - \mathbb E[Y(1)|\X,G=O,A=1]\}p(A=0,\X,G=O) \\
              & + \{ \mathbb E[Y(0)|\X,G=O,A=0] - \mathbb E[Y(0)|\X,G=O,A=1]\}p(A=1,\X,G=O) \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + b_{T+\mu}(1,\X) p(A=0,\X,G=O) - b_{T+\mu}(0,\X) p(A=1,\X,G=O) \\
        \end{aligned}
    \end{equation}
    where the first equality is based on Assumption \ref{assum: external validity of exp}, i.e., $G \Vbar Y(a) | \X$, and last equality is based on Assumption \ref{assum: time series equ bias}.
    
    Further based on Assumption \ref{assum: time series equ bias}, we have $b_{T+\mu}(a,\X)=f^\mu(\X) b_{T}(a,\X)$. Then, we rewrite the equality above as:
    \begin{equation} \label{eq: time tau decom}
        \begin{aligned}
             \tau(\X) 
            = & \mathbb E[Y(1)|\X,G=O] - \mathbb E[Y(0)|\X,G=O] \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] \\
              & + f^\mu(\X) b_{T}(1,\X) p(A=0,\X,G=O) - f^\mu(\X) b_{T}(0,\X) p(A=1,\X,G=O) \\
        \end{aligned}
    \end{equation}
    Similarly, for short-term ITE at last time step $T$ we have:
    \begin{equation} \label{eq: time stce decom}
        \begin{aligned}
            & \mathbb E[S_T(1)|\X] - \mathbb E[S_T(0)|\X] \\
            = & \mathbb E[S_T|\X,G=O,A=1] - \mathbb E[S_T|\X,G=O,A=0] \\
              & + \{ \mathbb E[S_T(1)|\X,G=O,A=0] - \mathbb E[S_T(1)|\X,G=O,A=1]\}p(A=0,\X,G=O) \\
              & + \{ \mathbb E[S_T(0)|\X,G=O,A=0] - \mathbb E[S_T(0)|\X,G=O,A=1]\}p(A=1,\X,G=O) \\
            = & \mathbb E[S_T|\X,G=O,A=1] - \mathbb E[S_T|\X,G=O,A=0] \\
              & + b_{T}(1,\X) p(A=0,\X,G=O) - b_{T}(0,\X) p(A=1,\X,G=O)
        \end{aligned}
    \end{equation}
    
    Then, combining Eq. \ref{eq: time tau decom} and \ref{eq: time stce decom}, we have
    \begin{equation} 
        \begin{aligned}
            \tau(\X) 
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] + 
             f^\mu(\X)   \mathbb E[S_T(1)|\X] -  f^\mu(\X) \mathbb E[S_T(0)|\X] \\
            & -  f^\mu(\X) \mathbb E[S_T|\X,G=O,A=1] -  f^\mu(\X)  \mathbb E[S_T|\X,G=O,A=0] \\
            = & \mathbb E[Y|\X,G=O,A=1] - \mathbb E[Y|\X,G=O,A=0] + 
             f^\mu(\X)   \mathbb E[S_T|\X,G=E,A=1] -  f^\mu(\X) \mathbb E[S_T|\X,G=E,A=0] \\
            & -  f^\mu(\X) \mathbb E[S_T|\X,G=O,A=1] -  f^\mu(\X)  \mathbb E[S_T|\X,G=O,A=0] \\
            = & \mu_Y^O(1,\x) - \mu_Y^O(0,\x) 
            +  f^\mu(\X) \left(\mu_{S_T}^E(1,\x) -  \mu_{S_T}^E(0,\x)
             +  \mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x) \right)
        \end{aligned}
    \end{equation}
    where the second equality is based on Assumption \ref{assum: external validity of exp}. This finishes our proof.
\end{proof}

\section{Proof of Lemma \ref{lemma: convergence of f}}
\label{app: proof f conver}

We first state the following lemma that is used in the proof of Lemma \ref{lemma: convergence of f}.

\begin{lemma}  \label{app: lemma conv of omega}
suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp}, \ref{assum: time series equ bias} hold, then $\forall t$, 
        \begin{equation} 
      \begin{aligned}
       & \hat \omega_t (\x) -\omega_t (\x) 
            = O_p \left( 
            r_{\mu _{S_t}^E}(n) + r_{\mu _{S_t}^O}(n)
            \right).
      \end{aligned}
    \end{equation}
    where $r_{\circ}(n)$ denotes the risk of nuisance function $\circ$, e.g., $r_{\mu _{S_t}^E}(n)$ correspondingly to $\mu _{S_t}^E$, and further under Assumption \ref{asmp: smooth}, we have
    \begin{equation} 
      \begin{aligned}
       & \hat \omega_t (\x) -\omega_t (\x) 
            = O_p \left( 
            n^{\frac{-\alpha}{2\alpha+d}} + n^{\frac{-\beta}{2\beta+d}}
            \right).
      \end{aligned}
    \end{equation}
\end{lemma}

\begin{proof}
    The lemma is immediately proved by the form of $\omega_t(\x)$ as 
    $\omega_t(\x) = \mu _{S_t}^E(1,\x) - \mu _{S_t}^E(0,\x) + \mu _{S_t}^O(0,\x) - \mu _{S_t}^O(1,\x)$.
\end{proof}

We now restate our Lemma \ref{lemma: convergence of f} as follows and formally prove that.

\begin{lemma} 
    Suppose the training steps S1 and S2 are train on two independent datasets of size $n$ respectively, and suppose Assumptions \ref{assum: consist}, \ref{assum: positi}, \ref{assum: internal validity of obs}, \ref{assum: internal validity of exp}, \ref{assum: external validity of exp}, \ref{assum: time series equ bias}, \ref{asmp: smooth} and \ref{asmp: bound} hold, then we have
        \begin{equation} 
      \begin{aligned}
       & \hat f(\x) - f(\x) 
            = O_p \left( 
            (\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}} +
            (\frac{1}{(T-1)n})^{\frac{\alpha}{2\alpha+d}} +
            (\frac{1}{(T-1)n})^{\frac{\beta}{2\beta+d}}
            \right).
      \end{aligned}
    \end{equation}
\end{lemma}

\begin{proof}
     We apply Proposition 1 of Kennedy et~al. \cite{kennedy2023towards}, yielding that
    \begin{equation} \label{app: eq f decomp}
    \begin{aligned}
       \hat f(\x) - f(\x) 
       = & (\hat f(\x) - \tilde f(\x)) + (\tilde f(\x) - f(\x))\\
       = &  (\hat f(\x) - \tilde f(\x)) +  O_p(R_n^*(\x)) \\
       = &  \hat{\mathbb E}_n[\hat r(\X)|\X=\x] + o_p(R_n^*(\x))  +  O_p(R_n^*(\x)) \\
    \end{aligned}
    \end{equation}
    where $\hat r (\x) = \mathbb E[\hat f(\X) |\X=\x ] - f(\x)$, and $\mathcal R_n^*(x)$ is the oracle risk of second-stage regression and further under Assumption \ref{asmp: smooth}, we know $f$ is $\eta$-smooth, thus $\mathcal R_n^*(x)=O_p( (\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}})$. 
    Here, $f$ is a time-series model ($1$-order autoregressive), optimized with $T-1$-length steps and $n$ samples, as shown in Eq. \eqref{eq: s2}. 
    According to Proposition \ref{propo: confounding bias function}, $\forall t$, $f(\x)=\frac{\omega_{t+1} (\x)}{\omega _t (\x)}$. We first prove the rate of $\frac{\hat \omega_{t+1} (\x)}{\hat \omega _t (\x)}$ for a fixed $t$ as follows.
    \begin{equation} 
        \begin{aligned}
            & (\frac{\hat \omega_{t+1} (\x)}{\hat \omega _t (\x)} - \frac{ \omega_{t+1} (\x)}{ \omega _t (\x)} )^2 \\
            = & \left( \frac{ \hat{ \omega}_{t+1}(\x) \omega_t(\x) -  \omega_{t+1}(\x) \hat{ \omega}_t(\x)}{ \omega_t(\x) \hat{ \omega}_t(\x)} \right)^2 \\
            = & \left(  \frac{ \hat{ \omega}_{t+1}(\x) \omega_t(\x) -  \omega_{t+1}(\x) \hat{ \omega}_t(\x)
            + \omega_{t+1}(\x) \omega_t(\x)
            - \omega_{t+1}(\x) \omega_t(\x)
            }{ \omega_t(\x) \hat{ \omega}_t(\x)}  \right)^2 \\
            = & \left(  \frac{ (\hat{ \omega}_{t+1}(\x)- \omega_{t+1}(\x)) \omega_t(\x) 
            + \omega_{t+1}(\x) ( \omega_t(\x) -\hat{ \omega}_t(\x))
            }{ \omega_t(\x) \hat{ \omega}_t(\x)}  \right)^2\\
            \overset{(a)}{\leq} & \frac{2 (\hat{ \omega}_{t+1}(\x)- \omega_{t+1}(\x))^2 \omega_t^2(\x) 
            + \omega_{t+1}^2(\x) ( \omega_t(\x) -\hat{ \omega}_t(\x))^2
            }{ \omega_t^2(\x) \hat{ \omega}_t^2(\x)} \\
            = & \frac{ 2 }{  \hat{ \omega}_t^2(\x)} (\hat{ \omega}_{t+1}(\x)- \omega_{t+1}(\x))^2
            + \frac{ 2 \omega_{t+1}^2(\x) }{ \omega_t^2(\x) \hat{ \omega}_t^2(\x)} ( \omega_t(\x) -\hat{ \omega}_t(\x))^2  \\
            \overset{(b)}{\asymp}  & (\hat{  \omega}_{t+1}(\x)-  \omega_{t+1}(\x))^2 + ( \omega_{t}(\x) -\hat{  \omega}_{t}(\x))^2;
    \end{aligned}
    \end{equation}
    where the inequality $(a)$ is based on $(a+b)^2 \leq 2 (a^2 + b^2)$, and $(b)$ is based on Assumption \ref{asmp: bound}. Under Assumption \ref{asmp: bound}, $\mu_{S_t}^O$, $\mu_{S_t}^E$ and their estimates are all bounded, and thus $\hat{  \omega}_{t}(\x)$ and its estimates are also bounded, thus $(b)$ holds. Then, applying Lemma \ref{app: lemma conv of omega}, we know for a fixed time step $t$, $\frac{\hat \omega_{t+1} (\x)}{\hat \omega _t (\x)} - \frac{ \omega_{t+1} (\x)}{ \omega _t (\x)} = O_p( n^{\frac{-\alpha}{2\alpha+d}} + n^{\frac{-\beta}{2\beta+d}})$. Then, for $\hat r (\x)$, we have the effective sample is of size $(T-1)n$, and then combining with Eq. \eqref{app: eq f decomp} and $\mathcal R_n^*(x)=O_p( (\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}})$, we obtain the desired result.
\end{proof}    

\section{Proof of Theorem \ref{theo: convergence}}
\label{app: proof tau conver}

We first restate  Theorem \ref{theo: convergence} as follows:

\begin{theorem} 
Suppose Lemma \ref{lemma: convergence of f} hold, then we have
    \begin{equation} 
      \begin{aligned}
       \hat \tau(\x) -   \tau(\x) 
          =   O_p \left(  
          n^{-\frac{\gamma}{2\gamma+d}} +
          n^{-\frac{\alpha}{2\alpha+d}} +
          n^{-\frac{\beta}{2\beta+d}} +
           \frac{\mu}{ ((T-1)n)^{\frac{\eta}{2\eta+d}} }  
           + \frac{\mu}{ ((T-1)n)^{\frac{\alpha}{2\alpha+d}} }  + 
           \frac{\mu}{ ((T-1)n)^{\frac{\beta}{2\beta+d}} }  
            \right).
      \end{aligned}
    \end{equation}
\end{theorem}

\begin{proof}
    As stated in Lemma \ref{lemma: convergence of f}, we have 
    \begin{equation} 
      \begin{aligned}
       & \hat f(\x) - f(\x) 
            = O_p \left( 
            (\frac{1}{(T-1)n})^{\frac{\eta}{2\eta+d}} +
            (\frac{1}{(T-1)n})^{\frac{\alpha}{2\alpha+d}} +
            (\frac{1}{(T-1)n})^{\frac{\beta}{2\beta+d}}
            \right),
      \end{aligned}
    \end{equation}
    and thus 
    \begin{equation} \label{app: eq1 proof of theo conve}
      \begin{aligned}
       & \hat f^\mu(\x) - f^\mu(\x) 
            = O_p \left( 
            (\frac{\mu}{(T-1)n})^{\frac{\eta}{2\eta+d}} +
            (\frac{\mu}{(T-1)n})^{\frac{\alpha}{2\alpha+d}} +
            (\frac{\mu}{(T-1)n})^{\frac{\beta}{2\beta+d}}
            \right).
      \end{aligned}
    \end{equation}
    Under Assumption \ref{asmp: smooth}, we have 
    \begin{equation} \label{app: eq2 proof of theo conve}
      \begin{aligned}
       \hat \mu_Y^O(a,\x) - \mu_Y^O(a,\x)=O_p(n^{-\frac{\gamma}{2\gamma+d}})
      \end{aligned}
    \end{equation}
    and
    \begin{equation} 
      \begin{aligned}
       \hat \mu_{S_T}^E(a,\x) - \mu_{S_T}^E(a,\x)=O_p(n^{-\frac{\alpha}{2\alpha+d}})
      \end{aligned}
    \end{equation}
    and 
    \begin{equation} 
      \begin{aligned}
       \hat \mu_{S_T}^O(a,\x) - \mu_{S_T}^O(a,\x)=O_p(n^{-\frac{\beta}{2\beta+d}}).
      \end{aligned}
    \end{equation}
    Then, let $a:=f^\mu(\X) $,$\hat a = \hat f^\mu(\X) $, $b=\mu_{S_T}^E(1,\x) -  \mu_{S_T}^E(0,\x)
             +  \mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x)$, and similarly for $\hat b$, and under Assumption \ref{asmp: bound}, $a$, $b$, $\hat a$ and $\hat b$ are all bounded.
    We analyze the term
    \begin{equation} 
      \begin{aligned}
      ( a b - \hat a \hat b )^2
       = & (ab  - a \hat b + a \hat b - \hat a \hat b )^2\\
       = & (a (b-\hat b) + (a-\hat a )\hat b)^2 \\
       \overset{(a)}{\leq} & 2 a^2 (b-\hat b )^2 + 2 \hat b^2 (a - \hat a)^2  \\
       \overset{(a)}{\asymp} & (b-\hat b)^2 + (a - \hat a)^2,
      \end{aligned}
    \end{equation}
    where the inequality $(a)$ is based on $(a+b)^2 \leq 2(a^2+b^2)$ and $(b)$ is based on the boundedness assumption. Then we have 
    \begin{equation} 
      \begin{aligned}
      ab-\hat a \hat b =  O_p \left( 
            (\frac{\mu}{(T-1)n})^{\frac{\eta}{2\eta+d}} +
            (\frac{\mu}{(T-1)n})^{\frac{\alpha}{2\alpha+d}} +
            (\frac{\mu}{(T-1)n})^{\frac{\beta}{2\beta+d}} + 
            n^{-\frac{\alpha}{2\alpha+d}} +
            n^{-\frac{\beta}{2\beta+d}}
            \right).
      \end{aligned}
    \end{equation}
    Hence, the result is immediately proved by the form of $\tau(x)$ as  $\tau(x) = \mu_Y^O(1,\x) - \mu_Y^O(0,\x) +  f^\mu(\X) \left(\mu_{S_T}^E(1,\x) -  \mu_{S_T}^E(0,\x)
             +  \mu_{S_T}^O(0,\x) - \mu_{S_T}^O(1,\x) \right) $ and the result of Lemma \ref{lemma: convergence of f}.
\end{proof}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
