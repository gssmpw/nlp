\section{Challenges and Future Directions}
\label{sec:CFD}
Although the development of EMLMs has surged, it still faces numerous challenges. However, it also presents exciting and valuable avenues for future exploration. This section outlines the current challenges and highlights potential future research directions for the development of EMLMs. The discussion is organized into several key areas: technological challenges, data and annotation issues, and ethical and application-related concerns.

\subsection{Technological Challenges}
Cross-modal Alignment: Despite significant advances in multimodal models, achieving precise and efficient alignment across different modalities—such as vision, language, and motion—remains a fundamental challenge. Developing methods to robustly fuse and align these modalities in real-time, particularly for embodied tasks, is a critical research focus. For example, both the current visual-language model, ReKep \cite{huang2024rekep}, and the Vision-Audio model, SoundSpaces \cite{chen2020soundspaces}, depend on effective alignment of data from diverse modalities. In the absence of proper alignment, the accuracy and efficiency of the response are likely to degrade.

Computational Resources and Efficiency: EMLMs demand significant computational resources and storage. A key challenge is to improve computational efficiency, minimizing energy consumption, and optimizing inference speed while preserving high performance. Advances in model compression, distributed computing, and hardware acceleration will be crucial in addressing these challenges. At present, most models have vast numbers of parameters, and both training and inference processes rely on high-performance GPUs, which are time-intensive and expensive. However, Openvla \cite{kim2024openvla} has introduced an approach where a model with only 7 billion parameters can perform a wide range of tasks. This efficiency is realized when the input consists of visual and language data. However, when additional modalities such as LiDAR, audio, pressure, GPS, and other multimodal inputs are incorporated to tackle more complex tasks, the model size, response time, and associated costs tend to increase significantly.

Generalization Across Domains: While multimodal models have demonstrated impressive performance on specific benchmarks or within particular domains, their ability to generalize across diverse contexts or tasks remains limited. Researchers must explore methods to enhance the transferability and adaptability of these models for real-world applications. For instance, current embodied large models are typically categorized into perception models, such as the GPT series, interaction models like 3D-VLA \cite{zhen20243d}, and navigation models such as SG-Nav \cite{yin2024sg}. The scope of tasks these models can address is relatively fixed, and their generalization ability remains suboptimal.

Handling Temporal and Sequential Information: Embodied models must manage dynamic, real-time data and sequential interactions, presenting a significant challenge in processing continuous actions, environmental events, and the temporal dependencies between perception, reasoning, and movement. In the field of interaction, models are typically categorized into short-horizon action policies, such as R3M \cite{R3M}, and long-horizon action policies, like Palm-e \cite{driess2023palm}. However, in the domain of navigation, there is a lack of models designed for long-term continuous navigation.

\subsection{Data and Annotation Issues}
Diversity and Quality of datasets: Existing datasets for embodied multimodal tasks are often limited in terms of diversity, scale, and quality. The shortage of high-quality, real-world datasets that capture complex, multimodal interactions in dynamic environments hinders effective model training. Future efforts should prioritize the development of larger, more diverse, and better-annotated datasets to enhance the robustness and generalization of multimodal models. While current large-scale datasets like the Open X-Embodiment dataset \cite{o2024open} and ARIO dataset \cite{wang2024all} have made notable strides, they predominantly focus on perception and interactive tasks, such as household chores and kitchen operations. These tasks alone are insufficient to support the full range of capabilities required for embodied intelligent agents. Furthermore, the majority of sensors in these datasets rely on cameras, which limits real-world perception. To address this, it is crucial to integrate additional multimodal sensors, such as LiDAR, sound sensors, radar, force sensors, and GPS, to improve the breadth of data available.

In terms of datasets, it's essential to incorporate real-world, dynamic data. This is particularly crucial in embodied tasks, such as robotics and autonomous systems, where acquiring data from real-world environments is challenging due to the unpredictable nature of physical surroundings. To ensure the practical applicability of these models in real-world scenarios, they must be trained on data that accurately reflects dynamic, non-static environments.

\subsection{Applications and Ethical Considerations}
Autonomous Driving and Robotics: As embodied multimodal models begin to find applications in autonomous driving, robotics, and human-robot interaction, ensuring their safety, reliability, and ethical compliance is paramount. There is a need to address the challenges of decision-making in real-time, the interpretability of model outputs, and the mitigation of risks in autonomous systems.

Ethical and Bias Issues: Multimodal models may unintentionally inherit biases present in the training data, leading to unfair or discriminatory outcomes. It is crucial to address these ethical concerns by developing methods that ensure fairness, transparency, and accountability in decision-making processes.

\subsection{Future Research Directions}
Cross-modal Pre-training and Fine-tuning: Future research should explore more efficient strategies for cross-modal pre-training and fine-tuning, enabling models to perform well across a range of tasks, from perception to decision-making, without requiring extensive retraining.

Self-supervised Learning: The development of self-supervised learning techniques will be key in reducing reliance on large labeled datasets. By leveraging unlabeled data, models can learn richer representations, making them more adaptable and scalable.

Integration with Multimodal Reinforcement Learning: A promising direction is the integration of multimodal models with reinforcement learning. By combining perception, action, and feedback loops, embodied agents can continuously improve and adapt their behaviors in dynamic, real-world environments.

End-to-end large models: Currently, there are various large models designed for different tasks, such as perception, navigation, and interaction. However, the future development trend is moving towards end-to-end large models, where a single model handles everything—from processing input instructions to executing the final task. This approach simplifies the process and enhances efficiency.

