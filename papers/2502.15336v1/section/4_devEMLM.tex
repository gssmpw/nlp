\section{Development of Embodied Multimodal Large Models}
\label{sec:EMLM}
EMLMs are a type of AI models that combine multiple modal information such as language, vision, and hearing. They can understand and process different types of data from the real world. As shown in Fig. \ref{fig:embodyRobotTask}, these models are usually designed to perform various tasks, such as perception, navigation and interaction, etc.

\begin{figure}[t]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1.0\linewidth, trim={0
	0 90 20}, clip]{pictures/embodyRobotTask.pdf}
	
	\caption{Full Task Stacks for Embodied Agents. Various embodied intelligent agents, including robot dogs, humanoid robots, and other types of intelligent systems, rely on a range of sensors, such as cameras, LiDAR, and other sensing technologies, to perceive their environment. These agents then perform specific tasks, usually guided by human voice or language commands. Task execution typically involves three key modules: perception, navigation, and interaction. The datasets and large models required for these modules can be collected and trained using either simulators or real-world scenarios. During task execution, the agent interacts with its environment to gather the necessary information.}
	\label{fig:embodyRobotTask}
\end{figure}

\subsection{Embodied Perception}
Different from using traditional neural network methods or large models to identify objects, according to the definition of embodied intelligence, embodied agents have the ability to interact with and move in the physical world. This requires EMLMs to have a deeper perception and understanding of objects in the physical world and the motion and logical relationships between objects. Embodied perception requires visual perception and reasoning, understanding 3D relationships in the scene, predicting and executing complex tasks based on visual information. The development of the embodied perception large model is shown in Fig. \ref{fig:Section3}

Currently, there are two main types of large models for embodied perception: one is based directly on GPT, and the other is based on other large models. The detailed information of these models can be found in Table \ref{tab:Perception}.

\begin{table}[t]
    \centering
    \caption{Embodied perception large models}
    \label{tab:Perception}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} c >{\centering\arraybackslash}m{4cm} c >{\centering\arraybackslash}m{3cm} c}
        \toprule
        Model & Architecture & Size & Platform & Dataset & Hardware & Year \\
        \toprule
            Octopus \cite{yang2025octopus} & MPT-7B, CLIP VIT-L/14 \cite{radford2021learning} & - & (Sim) OctoGibson \cite{li2023behavior}, OctoGTA & OctoGibson, OctoGTA & - & 2025 \\
            \addlinespace[0.5em]
            AlphaBlock \cite{jin2023alphablock} & Vicuna \cite{chiang2023vicuna}, ViT-G/14 \cite{fang2023eva} & - & Franka Emika Research 3 robot arm & AlphaBlock & 2 RTX A6000 & 2023 \\
            \addlinespace[0.5em]
            CoPa \cite{huang2024copa} & GraspNet \cite{fang2020graspnet}, GPT-4V & - & Franka Emika Panda & - & - & 2024 \\
            \addlinespace[0.5em]
            ReKep \cite{huang2024rekep} & DINOv2 \cite{oquab2023dinov2}, GPT-4o \cite{islam2024gpt} & - & Wheeled Single-Arm Platform and Stationary Dual-Arm Platform & - & - & 2024 \\
             \midrule
            RobotGPT \cite{jin2024robotgpt} & - & - & PyBullet, MoveIt and ros franka & - & - & 2024 \\
            \addlinespace[0.5em]
            Voxposer \cite{huang2023voxposer} & - & - & Franka Emika Panda & real, SAPIEN \cite{xiang2020sapien} & - & 2023 \\
            \addlinespace[0.5em]
            PaLM-E \cite{driess2023palm} & ViT, OSRT \cite{sajjadi2022object} & - & TAMP, Language-Table, Mobile Manipulation & - & - & 2023 \\
             \midrule
            RT-1 \cite{brohan2022rt} & EfficientNet-B3 \cite{tan2019efficientnet} & - & Everyday Robots & RT-1 & - & 2022 \\
            \addlinespace[0.5em]
            RT-2 \cite{brohan2023rt} & PaLI-X, PaLM-E & - & 7DoF mobile manipulator & - & - & 2023 \\
            \addlinespace[0.5em]
            RT-H \cite{belkhale2024rt} & ViT \cite{dosovitskiy2020image}, PaLI-X \cite{chen2023pali} & 55B & - & - & - & 2024 \\
            \addlinespace[0.5em]
            RoboFlamingo \cite{li2023vision} & OpenFlamingo \cite{awadalla2023openflamingo} & - & Franka Emika Panda & CALVIN \cite{mees2022calvin} & 8 NVIDIA Tesla A100 & 2023 \\
            \midrule
            Openvla \cite{kim2024openvla} & Prismatic-7B \cite{karamcheti2024prismatic} & 7B & WidowX robot & BridgeData V2 \cite{walke2023bridgedata} & 21,500 A100-hours and RTX 4090 (6HZ) & 2024 \\
            RoboMamba \cite{liu2024robomamba} & Mamba \cite{gu2023mamba} & - & Franka Emika Panda, SAPIEN & \begin{tabular}[c]{@{}c@{}}LLaVA-LCS \cite{liu2024improved},\\ LLaVA-v1.5 \cite{liu2024improved},\\ LRV-INSTRUCT \cite{liu2023mitigating},\\ RoboVQA \cite{sermanet2024robovqa}\end{tabular} & NVIDIA A100 & 2024 \\
        \bottomrule
        \end{tabular}
    }
\end{table}

\begin{figure}[t]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1.0\linewidth]{pictures/4_Rekep.pdf}
	
	\caption{Diagram illustrating the Rekep framework (Source: Rekep \cite{huang2024rekep}).}
	\label{fig:rekep}
\end{figure}

\subsubsection{GPT-Based Large Models}
The general large model can accept specific text instructions tailored to the task's requirements, returning scene understanding results in a natural language format and handling a variety of perception tasks. For example, models like GPT1 \cite{GPT1}, GPT2 \cite{GPT2}, GPT3 \cite{GPT3}, GPT4 \cite{GPT4} can effectively perform these functions.

Octopus \cite{yang2025octopus} uses GPT-4V \cite{yang2023dawn} to dynamically generate descriptions and analyses of observed images based on the current stage task, including objects that can be interacted with in the scene and their relative positions in space. This description is used as input to the language model to generate decisions for the next action. Also using GPT-4V, CoPa \cite{huang2024copa} utilizes GPT-4V to directly identify and highlight the specific grasping areas and potential grasping poses of generated objects in the observed image, enabling fine-grained object understanding. Subsequently, GraspNet \cite{asif2018graspnet} acts as a grasping pose detector, selecting the pose with the highest confidence for execution.

However, researchers have observed that GPT-4V frequently struggles to generate satisfactory results when provided with coordinate-based inputs. This limitation is attributed to the inherent constraints of the text-based GPT-4 model, which lacks robust spatial perception capabilities. Then, they propose AlphaBlock \cite{jin2023alphablock}, which was inspired by method chain-of-thought \cite{wei2022chain}. This approach requires GPT-4V to engage in reasoning before generating coordinates, ensuring it also provides an understanding and description of the layout. This perceptual information is then used to deliver real-time feedback on the execution status of the current task. To ensure accurate perception of patch features (e.g., color and spatial location), they designed a visual adapter to extract and merge multi-stage visual features from the ViT \cite{dosovitskiy2020image} model in MiniGPT-4 \cite{zhu2023minigpt}. To ensure consistent embeddings in LLMs, they further adopted a visual Q-former \cite{li2023blip} with a language-specific projector to effectively align image observations with LLMs.

Some methods directly use the observation and code generation capabilities of multimodal large models to output executable code by observing input. For example, Voxposer \cite{huang2023voxposer} employs a multimodal large model to extract the spatial geometric information of an object and generate its 3D coordinates. These object coordinates are then used to populate parameters in the code, facilitating the generation of a series of 3D functional diagrams and constraint diagrams based on the robot's observational space.

Compared with Voxposer's method, which uses direct visual observation, Rekep \cite{huang2024rekep} is more adaptable to environmental changes and has a higher degree of automation. ReKep offers significant improvements. ReKep \cite{huang2024rekep} uses DINOv2 \cite{oquab2023dinov2} to extract patch features of the input RGB image and samples it to the original image size through bilinear interpolation. Then, Segment Anything Model \cite{kirillov2023segment} is used to extract all the markers in the scene and obtain candidate key points. After obtaining the key point candidates, researchers superimpose them on the original RGB image together with the numerical markers. Next, the image and task instructions are fed into GPT-4o \cite{islam2024gpt}, where a specific prompt is employed to generate the required number of stages, along with the corresponding sub-goals and path constraints for each stage.

Similarly, RobotGPT \cite{jin2024robotgpt} also uses GPT to directly generate executable code. It proposes a five-part prompting approach, consisting of background information, object details, environmental context, task specifications, and examples. By providing detailed descriptions of the environment, objects, and tasks, this method effectively guides ChatGPT to generate precise and relevant outputs. The generated code is then executed to control the robot in performing the specified tasks.

\subsubsection{Method Based on Non-GPT Large Models}
PaLM-E \cite{driess2023palm}, which directly integrates continuous inputs from the sensor modalities of an embodied agent, allowing the language model to make more grounded inferences for sequential decision-making in the real world. Inputs such as images and state estimates are embedded into the same latent space as language tokens, enabling them to be processed by the self-attention layers of a Transformer-based \cite{vaswani2017attention} LLMs in the same manner as text. ViT \cite{dosovitskiy2020image}

RT-1 \cite{brohan2022rt} employs the EfficientNet-B3 \cite{tan2019efficientnet} model as a perception module to encode image data. It transforms images into implicit representations for subsequent task execution. Through training, this perception module learns to encode information in a way that enhances task execution effectiveness. As an upgrade of RT-1, RT-2 \cite{brohan2023rt} introduces a method that directly trains a vision-language model to output low-level robot actions by representing these actions as textual tokens. This approach involves training the model alongside Internet-scale vision-language tasks to improve its performance and versatility. After combining the advantages of RT-1 and RT-2 and improving them, RT-H \cite{belkhale2024rt} instantiates this LVLM using the same PaLI-X 55B \cite{chen2023pali} architecture as RT-2. It first tags the image with the ViT \cite{dosovitskiy2020image} encoder model, and then employs an encoder-decoder Transformer to convert the image and natural language tag streams into action tags.

RoboFlamingo \cite{li2023vision} is built on the open-source vision-language model, OpenFlamingo \cite{awadalla2023openflamingo}, and is tailored for downstream manipulation tasks through a small amount of imitation learning fine-tuning using robot manipulation data. Its vision encoder consists of ViT \cite{dosovitskiy2020image} and Perceiver Resampler \cite{alayrac2022flamingo}. Its flexible design allows it to be deployed on lower-performance platforms. 

Openvla \cite{kim2024openvla} is a 7B-parameter open-source Vision-Language-Action (VLA) trained on a diverse dataset of 970,000 real-world robot demonstrations. It enhances the Llama 2 \cite{touvron2023llama} language model by incorporating a visual encoder that integrates pretrained features from DINOv2 \cite{oquab2023dinov2} and SigLIP-grade \cite{zhai2023sigmoid} GPUs. Among them, DINOv2 focuses on spatial reasoning to provide fine-grained visual information, while SigLIP focuses on semantic understanding to enrich visual features.

RoboMamba \cite{liu2024robomamba} integrates a visual encoder with the Mamba model \cite{gu2023mamba}, aligning visual data with language embeddings through co-training. This enables the model to develop visual common sense and reasoning capabilities related to robotics.

%YOLO \cite{jiang2022review}, %RobotGPT \cite{jin2024robotgpt},
%SayCan \cite{ahn2022can},,, 
%(MLLM)
%Vison-Language Models (VLM)



\subsection{Embodied Navigation}
Relative to traditional A-to-B robot navigation, where an agent is instructed to move from Room A (x1, y1) to Room B (x2, y2), and the agent uses A* \cite{hart1968formal} or Dijkstra's \cite{dijkstra2022note} algorithm on a pre-generated 2D map to find a reachable shortest path, embodied intelligent navigation systems adopt a more dynamic and environmentally perceptive approach. These navigation systems do not solely rely on static map data but perceive and process surrounding environments in real-time through sensors, and models, converting environmental information into understandable and actionable semantics for the agent.

In the realm of navigation with large-scale embodied intelligence models, two primary methodologies are prevalent: the first approach is to harness a general large model, whereas the second entails crafting a specialized, EMLMs tailored explicitly for embodied intelligence tasks.

\begin{table}[H]
    \centering
    \caption{Embodied navigation Large models}
    \label{tab:navigation}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} c >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \toprule
        Model & Architecture & Size & Platform & Dataset & Hardware & Year \\
        \toprule
            DiscussNav \cite{DiscussNav} & GPT-4V \cite{yang2023dawn}, InstructBLIP \cite{InstructBLIP} & unpublished/13B & Turtlebot4Lite &  R2R \cite{anderson2018vision}  &  -  &  2024  \\
            Trans-EQA \cite{DiscussNav} & GPT-4V \cite{yang2023dawn}, InstructBLIP \cite{InstructBLIP} & unpublished/13B & Turtlebot4Lite &  R2R \cite{anderson2018vision}  &  -  &  2024  \\
            LM-Nav \cite{shah2023lm} &  GPT-3 \cite{GPT3} & 175B & Clearpath Jackal UGV & Real-world environments & - &  2023\\
            \midrule
            L3MVN \cite{yu2023l3mvn} &  RoBERTa-large \cite{liu2019roberta},CLIP \cite{radford2021learning} & 3.55B/191M & Jackal Robot & HM3D \cite{ramakrishnan2021habitat}, Gibson \cite{xia2018gibson} & - &  2023\\
            \midrule
            LFG \cite{shah2023navigation} &  GPT-3.5 \cite{ouyang2022training} & 175B & LoCoBot & HM3D \cite{ramakrishnan2021habitat} & 4 NVIDIA V100 GPUs &  2023\\
            
            VLMaps \cite{huang2023visual} &  LSeg \cite{li2022language}, GPT-4V \cite{yang2023dawn} & 307M/unpublished & HSR mobile & MP3D \cite{chang2017matterport3d}, AI2THOR \cite{kolve2017ai2} & - &  2023\\
            NavGPT \cite{zhou2024navgpt} &  GPT-3.5 \cite{ouyang2022training},BLIP-2 \cite{li2023blip} & unpublished/188M & simulate & R2R \cite{anderson2018vision} & - &  2024\\
            \addlinespace[0.5em]
            SG-Nav \cite{yin2024sg} &  LLaMA \cite{llama1},GPT-4 \cite{GPT4}  & 7B/unpublished & simulate & MP3D \cite{chang2017matterport3d}, HM3D \cite{ramakrishnan2021habitat}, RoboTHOR \cite{deitke2020robothor} & - &  2024\\
            \midrule
            Simple but effective \cite{khandelwal2022simple} &  CLIP \cite{radford2021learning}  & 191M & simulate & RoboTHOR \cite{deitke2020robothor}, HP3D \cite{ramakrishnan2021habitat}, iTHOR \cite{weihs2021visual}  & - &  2020\\
            \addlinespace[0.5em]
            Zson \cite{majumdar2022zson} &  CLIP \cite{radford2021learning}  & 191M & simulate & Gibson \cite{xia2018gibson}, HM3D \cite{ramakrishnan2021habitat}, MP3D \cite{chang2017matterport3d} & 8 NVIDIA A40 GPUs &  2022\\
            \midrule
            NavGPT-2 \cite{zhou2025navgpt} &  InstructBLIP \cite{InstructBLIP}  & 5B & simulate & R2R \cite{anderson2018vision}, RxR \cite{ku2020room}, HM3D \cite{ramakrishnan2021habitat} & NVIDIA A100 GPU &  2025\\
            
            vlfm \cite{yokoyama2024vlfm} &  BLIP-2 \cite{li2023blip}  &  188M  &  simulate  & Gibson \cite{xia2018gibson}, HM3D \cite{ramakrishnan2021habitat}, MP3D \cite{chang2017matterport3d}  &  RTX 4090 MaxQ Mobile GPU  &  2024\\
            \addlinespace[0.5em]
            NavCoT \cite{lin2024navcot} &  LLaMA \cite{llama1},BLIP  & 7B & simulate & R2R \cite{anderson2018vision}, RxR \cite{ku2020room}, R4R \cite{jain2019stay}, REVERIE \cite{Qi_2020_CVPR} & 4 NVIDIA V100 GPUs &  2024\\
            \midrule
            NaviLLM \cite{zheng2024towards} &  Vicuna-7B-v0 \cite{zheng2023judging},ViT \cite{dosovitskiy2020image}  & 7B/428M & simulate & \parbox{3cm}{\centering CVDN \cite{thomason2020vision}, SOON \cite{zhu2021soon}, R2R \cite{anderson2018vision},\\REVERIE \cite{Qi_2020_CVPR}, ScanQA \cite{Azuma_2022_CVPR},\\LLaVA-23k \cite{liu2024visual}, MP3D-EQA \cite{wijmans2019embodied}} & 8 NVIDIA V100 GPUs &  2024\\
            \midrule
            GOAT \cite{wang2024vision} &  GOAT  & - & simulate & R2R \cite{anderson2018vision}, RxR \cite{ku2020room}, REVERIE \cite{Qi_2020_CVPR}, SOON \cite{zhu2021soon} & NVIDIA V100 GPU &  2024\\
            \addlinespace[0.5em]
            VER \cite{liu2024volumetric} &  ViT-B/16 \cite{dosovitskiy2020image}  & 86M & simulate & MP3D \cite{chang2017matterport3d}, R2R \cite{anderson2018vision}, REVERIE \cite{Qi_2020_CVPR}, R4R \cite{jain2019stay} & 8 NVIDIA RTX 4090 GPUs & 2024\\
            \addlinespace[0.5em]
            GNM \cite{shah2022gnm} &  MobileNetv2 \cite{sandler2018mobilenetv2}  & 6.9M & Vizbot,DJI Tello,Clearpath Jackal UGV,LoCoBot. & Real-world environments & - &  2023\\
            \addlinespace[0.5em]
            ViNT \cite{shah2023vint} &  ViNT  & 31M & Vizbot,Unitree Go 1,Clearpath Jackal UGV,LoCoBot. & Real-world environments & 8 NVIDIA V100 GPUs & 2023\\
            \addlinespace[0.5em]
            NoMaD \cite{sridhar2023nomad} &  NoMaD  & 19M & LoCoBot & Real-world environments & - & 2023\\
        \bottomrule
        \end{tabular}
    }
\end{table}

\begin{figure}[t]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1.0\linewidth]{pictures/18_NavGpt_2.png}
	
	\caption{Diagram illustrating the NavGPT-2 framework (Source: NavGPT-2 \cite{zhou2025navgpt}).}
	\label{fig:NavGpt-2}
\end{figure}

\subsubsection{General Large Models}
LVLMs are capable of understanding and generating natural language due to their vast scale, advanced architecture, and pre-training on massive datasets. In some cases, they demonstrate reasoning and semantic understanding abilities that approach or even surpass those of humans. The excellent reasoning ability of LLMs can transform abstract instructions into operable semantic topologies. For example, LM-Nav \cite{shah2023lm} implements natural language instruction parsing based on GPT-3 \cite{GPT3}, making navigation decisions by extracting textual landmarks and combining them with scene images. Its research focuses on the application of LLMs in natural language understanding. In contrast, L3MVN \cite{yu2023l3mvn} demonstrates two innovative paradigms of LVLMs in navigation tasks through the RoBERTa-large model \cite{liu2019roberta}: The zero-shot method constructs semantic description sentences such as ``This scene contains a bathtub, toilet, and TV,'' and uses the LVLMs to evaluate the probability of the navigation target's existence; the feed-forward method encodes query sentences into summary embeddings, which are then input into a neural network for target probability prediction.Through experiments in real-world environments for the former and in different simulated environments (HM3D \cite{ramakrishnan2021habitat}, Gibson \cite{xia2018gibson}) for the latter, the transferability and flexibility of LVLMs in different scenarios and usage methods are demonstrated.

In the paper \cite{shah2023navigation}, as well as in VLMaps \cite{huang2023visual}, a method similar to the LLMs approach used in L3MVN's Zero-shot paradigm was also employed. The former proposed the Language Frontier Guide (LFG), which differs by not only using positive prompt sentences but also incorporating negative prompt sentences, and obtaining the likelihood estimates of the target through both \cite{wei2022chain}. The latter, on the other hand, utilizes GPT-4v \cite{yang2023dawn} to enable the LLMs to speculate which known object in the catalog is most likely to be near the target.

Recent advancements further expand LLMs' architectural roles in embodied navigation through system-level integration and structured environment comprehension. These innovations transcend basic semantic parsing by embedding LLMs into multimodal reasoning pipelines while introducing self-corrective mechanisms for enhanced reliability.

NavGPT \cite{zhou2024navgpt} is a system entirely based on LLMs for understanding and executing natural language navigation instructions. The core of this work lies in its ability to handle multimodal inputs, including text descriptions of visual observations, navigation history, and potential future exploration directions. By leveraging the GPT-3.5 \cite{ouyang2022training} model to summarize historical information, it generates concise prompts for the LLMs to make decisions. This work demonstrates the potential of LLMs in processing multimodal inputs, conducting advanced planning, and providing interpretable reasoning processes. However, for purely visual modality inputs, other Visual Frontend Models, are still required for preprocessing.

The SG-Nav framework \cite{yin2024sg} constructs a hierarchical 3D scene graph and utilizes LLMs for hierarchical chain-of-thought prompting (H-CoT), enabling LLMs to deduce the location of target objects based on the structural information of the scene graph. This process includes predicting the distance between objects and targets, posing relevant questions, answering questions, and predicting the distance between subgraphs and targets based on subgraph information. Additionally, SG-Nav has designed a graph-based re-perception mechanism to assess the credibility of detected target objects, and it abandons the target when the credibility is low to correct perception errors, thereby enhancing the accuracy and robustness of navigation.

Language-Image Pre-training model, is pre-trained using a large number of image-text pairs to learn the correlations between the two, achieving cross-modal semantic understanding. It is highly suitable for processing images obtained by an agent from the environment during navigation tasks, to establish correspondences between images and textual instructions or landmarks, for the execution of more advanced and complex navigation tasks.

CLIP \cite{radford2021learning} has collected 400 million image-text pairs from the internet for model learning, achieving an accuracy of 75.4\% on ImageNet \cite{deng2009imagenet}. CLIPORT \cite{shridhar2022cliport} utilizes CLIP's image encoder to provide semantic understanding for robotic manipulation and to form a semantic stream. The features of the semantic stream are up-sample and undergo an element-wise multiplication operation with the features of the spatial stream, which integrates semantic information into the spatial features. CLIP's image encoder can also be combined with Reinforcement Learning (RL) \cite{majumdar2022zson,khandelwal2022simple}, by converting images into feature vectors rich in semantic information. These feature vectors can serve as inputs for RL algorithms, helping robots better understand their environment and make more informed decisions. Combining CLIP with topological and metric maps used in traditional navigation can achieve effects similar to those of RL and end-to-end models while significantly reducing training time and costs. In L3MVN \cite{yu2023l3mvn}, the embeddings formed by image encoding are embedded in the nodes of the topological map, and then text or language instructions are transformed into embeddings through text encoding. By calculating the cosine similarity, the node where the instruction target is located can be determined. VLMaps \cite{huang2023visual} further embeds these embeddings into 3D space, and by applying a top-down compression, it is possible to obtain semantic or obstacle 2D maps suitable for different types of robots, such as vacuum cleaners and drones.

Compared to CLIP, BLIP and BLIP-2 \cite{li2022blip, li2023blip} offer enhanced capabilities. Specifically, BLIP-2 is able to generate text directly from images, enabling the model to produce corresponding textual descriptions based solely on the content of the input images, without the need for additional training data. In NavGPT \cite{zhou2024navgpt}, by inputting images from multiple perspectives, BLIP-2 can generate a preliminary understanding of the current environment, and then GPT-3.5 makes decisions based on historical information and explorable areas. In subsequent research, a fine-tuned model NavGPT-2 \cite{zhou2025navgpt} was constructed based on InstructBLIP \cite{InstructBLIP} by applying the visual encoder of EVA-CLIP \cite{fang2023eva}, which retained the reasoning capabilities of LLMs. NavGPT-2 has matched the performance of state-of-the-art specially pre-training model for Visual Language Navigation (VLN) during the unseen test phase. Similar to NavGPT, vlfm \cite{yokoyama2024vlfm} utilizes BLIP-2 to obtain the cosine similarity scores between the current RGB observation frame and each object in a list of text prompts containing the target, thereby generating a 2D value map. This map is then combined with the Frontier value Map generated in another step to determine the optimal Waypoint.

\subsubsection{Specialized Embodied Intelligence Large Models}
There is a type of model that does not use LVLMs directly. Instead, it combines different models and uses a dedicated dataset to train a new specialized large model.

NavCoT \cite{lin2024navcot} combines existing VLN datasets with advanced models such as LLMs and CLIP to train the LLaMA \cite{touvron2023llama,zhang2023llama} model on extracting object text from multi-perspective images, thereby enhancing its performance on the Future Imagination task and ultimately improving the entire navigation decision-making process.

NaviLLM \cite{zheng2024towards} extracts features from six perspective images at each location using a ViT \cite{dosovitskiy2020image} to form a scene encoding, then uses a Transformer Encoder to capture the interdependencies between different viewpoints. It processes and generates textual data such as task instructions, observation results, and historical information using an LLM (Vicuna-7B-v0 \cite{zheng2023judging}), and finally trains the model with all three as inputs.

Similarly, Trans-EQA \cite{luo2024transformer} leverages the global modeling advantages of Transformer \cite{vaswani2017attention} in the navigation module to replace the local feature extraction bottleneck of traditional CNNs, effectively associating dispersed visual features with language semantics.

GOAT \cite{wang2024vision} decomposes vision, instruction, and history into mediator, observable confounder, and unobservable confounder. It proposes Back-door Adjustment Causal Learning (BACL) and Front-door Adjustment Causal Learning (FACL) causal learning modules to process the corresponding information, while the mediator is used to predict the output and reduce the impact of confounding factors. This enables the GOAT model to have excellent generalizability and the ability to handle dataset biases and reduce the impact of confounding factors. The GOAT model can provide accurate navigation in diverse and unseen environments, making it highly suitable for application in complex real-world scenarios.

Rui Liu et al. \cite{liu2024volumetric} proposed the use of voxels to achieve a more comprehensive 3D representation. Specifically, they used ViT-B/16 \cite{dosovitskiy2020image} during pre-training to extract features from multi-perspective images, introduced cross-view attention (CVA) for coarse sampling between 2D and 3D, and then employed 3D deconvolution to enhance the resolution, realizing a transition from coarse to fine-grained 3D representation. During the training phase of navigation tasks, they froze the weights of some pre-trained layers. The purpose of this approach is to retain the general features learned by the model during the pre-training phase while allowing the model to adapt to specific navigation tasks by adjusting the weights of other layers.

The General Navigation Models introduced by the Berkeley AI Research team: GNM \cite{shah2022gnm},  Visual Navigation Transformer(ViNT) \cite{shah2023vint}, and Navigation Mask Diffusion Strategy(NoMaD) \cite{sridhar2023nomad}, are a set of generalizable models capable of controlling a variety of different robots without the need for specific prior training. GNM achieves broad generalization across multiple robotic platforms by training on a 60-hour heterogeneous dataset collected from various different yet structurally similar robots, including indoor and outdoor environment navigation on unseen robots. ViNT is a Transformer-based \cite{vaswani2017attention} visual navigation foundational model that learns general navigation capabilities through pre-training and can adapt to various downstream tasks, including zero-shot deployment and new task adaptation, demonstrating transfer learning capabilities across diverse datasets. The model predicts the number of time steps (dynamic distance) and a sequence of actions required to reach the goal by encoding current and past visual observations and goals, which are consistent across different robotic platforms, thus enabling rapid and efficient inference for resource-constrained robots as well as the ability to prompt and fine-tune for downstream tasks. NoMaD is a novel robotic navigation model that integrates a Transformer backbone network \cite{vaswani2017attention} with diffusion models \cite{ho2020denoising}, capable of unified processing of goal-oriented navigation and task-agnostic exploration, enabling robots to both execute specified tasks and conduct effective exploration in unknown environments. Through experiments on real-world mobile robot platforms, NoMaD has demonstrated superior performance and lower collision rates compared to existing methods, while also featuring a smaller model size and higher computational efficiency.

\subsection{Embodied Interaction}

Traditional robot interaction methods typically require the integration of independent modules such as perception, decision-making, planning, and control to accomplish specific tasks. With the advancements in deep learning, particularly the significant progress of language and visual models, embodied intelligent interaction has become feasible. Embodied intelligent interaction involves enabling intelligent agents and large models to possess multi-modal processing capabilities, including natural language reasoning, visual-spatial semantic perception, and the alignment of visual perception with language systems, among other key technologies. Currently, the foundational capabilities of embodied intelligent interaction require that the system understand human natural language instructions and autonomously complete tasks. As such, language-based embodied intelligent interaction has become a central focus of research. This can be broadly categorized into two types: language-based short-horizon action policies and language-based long-horizon action policies. Some interaction models are shown in Table \ref{tab:Interaction} and Table \ref{tab:Perception}.

\subsubsection{Language-based Short-horizon Action Policy}

\begin{figure}[t]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1.0\linewidth]{pictures/10_Openvla.png}
	
	\caption{Diagram illustrating the Openvla framework (Source: Openvla \cite{kim2024openvla}).}
	\label{fig:Openvla}
\end{figure}

Currently, many researchers focus on language-based vision fusion action strategies for simple tasks; while some researchers improve visual encoders to make them more suitable for robot operations. In the context of embodied interaction, the agent completes the task by interacting with the environment. Short-horizon action strategies emphasize making quick action decisions in a shorter time scale, usually based on the current state and immediate feedback. This strategy does not consider long-term plans or complex future situations, but focuses more on how to achieve local or short-term goals in the current or upcoming steps.

R3M \cite{R3M} proposed the  visual encoder, which uses a large amount of Ego4D \cite{grauman2022around} human video dataset training to learn sparse and compact representations. Ultimately, it can improve the success rate of robot operations by 10\% compared with visual encoders such as CLIP \cite{radford2021learning} and MoCo \cite{he2020momentum}. After improvement, Vi-PRoM \cite{jing2023exploring} focuses on exploring the pre-training strategy of visual encoders from three dimensions: dataset, model architecture, and training method. Its experimental results show that its performance is stronger than R3M \cite{R3M}.

The problem with R3M \cite{R3M} is that the amount of data used is too small. Paper \cite{radosavovic2023real} uses a larger dataset, including Ego4D \cite{grauman2022around}, ImageNet \cite{deng2009imagenet}, Epic Kitchens \cite{damen2022rescaling}, Something Something \cite{goyal2017something} and 100 Days of Hands \cite{shan2020understanding} datasets, totaling 4.5 million images. Paper \cite{radosavovic2023real} designed a visual encoder for robot manipulation. Thanks to the use of a large amount of Internet data and the introduction of the masked autoencoder (MAE) method, the trained visual encoder was finally frozen for use in physical action strategies, which showed an improvement of up to 75\% over CLIP \cite{radford2021learning} and 81\% over supervised pre-training ImageNet \cite{deng2009imagenet}. 

Some researchers \cite{karamcheti2023language, li2023mastering, du2024learning, lynch2023interactive} focus on using a pre-trained visual encoder and a large amount of robot data to train the Transformer model to directly output robot actions, thereby achieving better generalization capabilities of the robot. RT-1 \cite{brohan2022rt} was trained with a large amount of open space data to achieve the robot's multi-task learning ability and generalization ability in unknown scenarios. It also explored the generalization ability of the robot's skills from three aspects: data size, model parameter size, and data diversity. based on RT-1, Google DeepMind introduced a pre-trained visual language model MOO \cite{stone2023open} to process the observed images and then input them into the visual encoder to achieve zero-shot learning of new environments and new objects.Similarly, the Google DeepMind team proposed a method based on RT-1, Q-transformer \cite{chebotar2023q}, which uses Transformer to train and learn the Q value of each action in reinforcement learning instead of directly outputting the action. The results show that it is superior to previous discrete reinforcement learning and imitation learning methods in real-world operation tasks. In order to further improve the task generalization ability of the robot operation model, the Google team introduced a large language model and a new pre-trained visual encoder to form a new robot model RT-2 \cite{brohan2023rt}, and created a new name for this type of model vision-language-action, uses a large amount of Internet data and robot data for training, which significantly improves the generalization ability of objects. The UC Berkeley team proposed a fine-tuning strategy for policy transfer named Octo \cite{team2024octo} between different observations and different robot actions. Testing of robots on nine different platforms showed that this method is an effective fine-tuning deployment strategy and has important guiding significance for the policy design of future general-purpose robots.In order to enable the robot action strategy to be trained on some low-performance platforms, a robot action framework RoboFlamingo \cite{li2023vision} that was fine-tuned based on the open source visual language model OpenFlamingo \cite{awadalla2023openflamingo}. Experimental results show that this method is cost-effective and easy to use. 

Vima \cite{jiang2022vima} is a Transformer-based robot action strategy that introduced visual cropped images of the target into the prompt. Experiments showed that in zero-shot generalization tasks, given the same training data, the task success rate of the proposed scheme was 2.9 times higher than that of other methods. In order to reduce the interference of natural language on the policy action network, RT-H \cite{belkhale2024rt} first predicts the action language and then predicts the robot action based on the action language and visual information. Experiments show that RT-H is more powerful and flexible using this language action hierarchy, can respond to language intervention, and is superior to methods that learn from remote operation intervention.Training a vision-language-action model from scratch is often costly. based on the open source vision-language model prismatic-vlm, the model Openvla \cite{kim2024openvla} contains pre-trained DINOv2 \cite{oquab2023dinov2} and SigLIP \cite{zhai2023sigmoid} visual encoders and the Llama 2 \cite{touvron2023llama} language model, and finally outputs actions. Experimental results show that the task success rate is 16.5\% higher than that of closed models such as RT-2-X, and the number of parameters is reduced by 7 times.

Also based on Transformer, Hiveformer \cite{guhur2023instruction} that integrates natural language instructions, multi-view scene observations, and historical records of actions and observations to output robot actions. Experiments show that this method solves language-conditional instructions, significantly outperforming the current best methods and having excellent generalization performance. 

To simplify the steps, a simple GPT-style end-to-end model named GR-1 \cite{wu2023unleashing} takes language instructions, a series of observed images, and a series of robot states as input. It then predicts robot actions as well as future images in an end-to-end manner. Voxposer \cite{huang2023voxposer} uses the code writing function of a large language model to interact with the visual language model to form a 3D value graph. The combined value graph is used in a model-based planning framework to synthesize robot trajectories with zero samples. Experiments show that it can perform various tasks in natural language.

Some researchers \cite{lynch2020language,du2024learning} regard language instructions as a kind of goal, convert natural language instructions, goal images, and task IDs into implicit expressions of the goals, combine visual perception and proprioception, and use imitation learning to implement language-based robot action strategies. Paper \cite{ha2023scaling} proposed a robot learning framework to implement a multi-task language-conditioned visual-motor strategy. This strategy integrates proprioception and extends the diffusion strategy single-task behavior cloning method to multi-task with language conditions. The results show that the average success rate is improved by 33.2\%.

\subsubsection{Language-based Long-horizon Action Policy}
When making decisions, the agent considers goals and strategies over a longer time span, rather than focusing only on short-term immediate goals. This long-horizon policy considers the longer term future when making decisions, usually involving more complex planning and reasoning, with the goal of maximizing long-term rewards or achieving long-term goals.

Real-world tasks are often complex and time-consuming, so it is often necessary to decompose a complex task into several subtasks to execute. Currently, many researchers \cite{li2022pre, sharma2021skill} focus on introducing new large model methods to achieve the decomposition of complex tasks, namely high-level action policy. SayCan \cite{ahn2022can} connected a large language model with the real world by pre-training skills. The combination of low-level skills and the large language model helped the robot perform complex tasks. The results showed that it was able to complete long-horizon, natural language instructions and complex tasks. Zero-Shot Planners \cite{huang2022language} used the world knowledge learned by a large language model to achieve task planning and decomposition by describing the prompt of the task without additional training. Text2Motion \cite{lin2023text2motion} proposed a language-based planning framework that enables robots to solve long-horizon sequential manipulation tasks, construct task and motion plans given natural language instructions, and verify whether the plan is completed. Text2Motion uses Q functions encoded in a skill library to guide task planning using a large language model. Experiments have shown that it surpasses the most advanced language-based planning methods. The paper \cite{hu2023look} is based on GPT-4V and introduces visual information to help language models better plan tasks. EmbodiedGPT \cite{mu2024embodiedgpt} is a model that embodies intelligent multimodal understanding and reasoning, combining visual and language information to extract task-related features to achieve efficient and accurate task planning. Palm-e \cite{driess2023palm} performed end-to-end training on different tasks based on a pre-trained large model, and the input prompts contained language and image information, ultimately achieving the generalization of visual language models (such as picture-based language question answering, task planning, etc.). TPVQA \cite{zhang2023grounding} used visual language models to detect whether the execution of a task was successful, and at the same time decomposed the task to generate a planning sequence of subtasks, ultimately showing that the solution can relatively well complete complex and long sequence tasks. TaPA \cite{wu2023embodied} proposed a task planning in embodied tasks, which aligns LLMs with a visual perception model and generates a sequence of executable plans based on the objects perceived in the scene. Experimental results show that it achieves a higher task success rate than LLaVA and GPT-3.5. ViLaIn \cite{shirai2024vision} proposed a visual language interpreter, which uses LLMs and a visual language model to generate problem description. ViLaIn receives error message feedback from a symbolic planner to optimize the generated problem description. Experimental results show that ViLaIn can generate grammatically correct questions with an accuracy of over 99\% and an accuracy of over 58\% for effective plans. PG-InstructBLIP \cite{gao2024physically} integrated a physical-world-based LVLM and a large language model to construct an interactive framework for a robot planner. This method showed better planning performance on tasks that require reasoning about physical object concepts. Experimental results show that it can improve the success rate of tasks. 

In addition to other models, GPT can also be used to complete robotics tasks. Model ision \cite{wake2024gpt} used a method to enhance the general visual language model GPT-4V, input demonstration operation videos and natural language instructions, and encoded this information into symbolic task plans based on the GPT-4 task planner. The results showed that it can efficiently learn from demonstration videos.

ScreenAgent \cite{niu2024screenagent} simply using task planning cannot solve complex tasks well, and that it is necessary to further tightly couple task planning and motion planning. Autotamp \cite{chen2024autotamp} converted natural language task descriptions into intermediate task representations, and then used traditional task-and-motion planning algorithms combined with intermediate task representations to jointly solve task and motion planning. Paper \cite{zhou2023generalizable} used the knowledge reasoning ability of LLMs to generate task execution condition descriptions for generalization to new objects and unseen tasks. The task execution condition descriptions guide the generation and adjustment of Dynamic Movement Primitives trajectories to perform long-horizon tasks. Mutex \cite{shah2023mutex} proposed a unified method for policy learning from multi-modal task specifications. The Transformer-based architecture integrates cross-modality and Transformer-based action policy learning, effectively learns cross-modal task specifications, and can execute sequence text task instructions and multi-task text instructions at the same time. The final results show that the performance exceeds that of a single modality. Octopus \cite{yang2025octopus} introduced an embodied visual language programmer, which uses the generated executable code as a medium to connect high-level task planning and real-world operations, and demonstrated the effectiveness of this method through a series of experiments. Some methods like 3d-vla \cite{zhen20243d} and Grid \cite{ni2023grid} believe that 3D scene information can help models better understand the real world. LEO \cite{huang2023embodied} developed an embodied multimodal generalist agent that excels at perceiving, reasoning, planning, and acting in the 3D world. Through 3D visual language alignment and 3D visual language action instruction adjustment, they experimentally demonstrated the method's outstanding capabilities in a wide range of tasks, including question answering, embodied reasoning, task planning, and action strategies. Sayplan \cite{rana2023sayplan} proposed a scalable method to construct robot task planning based on LLMs and 3D scene graph representation. In large-scale environment evaluation, it shows that our method can lay the foundation for robots to execute large-scale, long-term task planning from natural language instructions.

\begin{table}[H]
    \centering
    \caption{Embodied Interaction large models}
    \label{tab:Interaction}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} c >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}p{3cm} c}
        \toprule
        Model & Architecture & Size & Platform & Dataset & Hardware & Year \\
        \toprule
        R3M \cite{R3M} & ResNet \cite{he2016deep} & - & MetaWorld, Franka Kitchen, Adroit, Franka Emika Panda & Ego4D \cite{grauman2022around} & - & 2022 \\
        \hline
        Paper \cite{radosavovic2023real} & ViT \cite{dosovitskiy2020image} & - & - & Ego4D \cite{grauman2022around}, ImageNet \cite{deng2009imagenet}, Epic Kitchens \cite{damen2022rescaling}, Something Something \cite{goyal2017something}, 100 Days of Hands \cite{shan2020understanding} & - & 2022 \\
        \hline
        MOO \cite{stone2023open} & OWL-ViT \cite{minderer2022simple}, RT-1 \cite{brohan2022rt} & - & 7-DOF robotic arm, two-finger gripper mobile manipulation robot & RT-1 \cite{brohan2022rt}, pick & - & 2023 \\
        \hline
        Q-transformer \cite{chebotar2023q} & Universal Sentence Encoder \cite{cer2018universal}, FiLM
        EfficientNet \cite{perez2018film, tan2019efficientnet} & - & 7-DOF robotic arm, two-finger gripper mobile manipulation robot & RT-1 \cite{brohan2022rt} & - & 2023 \\
        \hline
        Octo \cite{team2024octo} & T5-base \cite{raffel2020exploring}, ViT \cite{dosovitskiy2020image} & Octo-Small: 27M Octo-Base: 93M & WidowX 250 6-DOF, UR5,  RT-1 Robot, ALOHA, Trossen ViperX & Open X-Embodiment \cite{o2024open} & - & 2024 \\
        \hline
        Vima \cite{jiang2022vima} & T5, Mask R-CNN \cite{he2017mask} & 2M-200M &  & VIMA-BENCH built on the Ravens simulator \cite{zeng2021transporter, shridhar2022cliport} & - & 2022 \\
        \hline
        Hiveformer \cite{guhur2023instruction} & CLIP, UNet \cite{ronneberger2015u} & - & 6-DOF UR5 robotic arm with 2-finger Robotiq RG2 gripper and two cameras & real, RLBench \cite{james2020rlbench} & NVIDIA Tesla V100 SXM2 GPU & 2023 \\
        \hline
        GR-1 \cite{wu2023unleashing} & CLIP, ViT & 195M & Kinova Gen2 & Ego4D \cite{grauman2022around}, CALVIN \cite{mees2022calvin}, real & - & 2023 \\
        \hline
        Paper \cite{ha2023scaling} & CLIP & - & 6-DOF Robotic Arm & Based on MuJoCo simulator \cite{todorov2012mujoco} and Google Scanned dataset \cite{downs2022google} & NVIDIA RTX 3080 GPU & 2023 \\
        \hline
        SayCan \cite{ahn2022can} & LLM, Skills Library, Value Functions, Decision Module & - & mobile manipulator & 68,000 data points collected through VR devices, RetinaGAN \cite{ho2021retinagan} & 16 TPUv3 and 3000 CPU & 2022 \\
        \hline
        Zero-Shot Planners \cite{huang2022language} & GPT-3 \cite{GPT3}, Codex 12B \cite{chen2021evaluating} & 175B/12B & - & VirtualHome \cite{puig2018virtualhome}, ActivityPrograms \cite{puig2018virtualhome} & - & 2022 \\
        TaPA \cite{wu2023embodied} & LLaMA-7B, open-vocabulary object detector & - & - & AI2-THOR \cite{kolve2017ai2} and real & 8 GTX 3090 GPU & 2023 \\
        ViLaIn \cite{shirai2024vision} & Grounding-DINO \cite{garrett2020pddlstream}, GPT-4 \cite{GPT4}, BLIP-2 \cite{li2023blip} & - & - & ProDG \cite{shirai2024vision} & - & 2024 \\
        \hline
        PG-InstructBLIP \cite{gao2024physically} & InstructBLIP \cite{dai2023instructblip}, fine-tuning in PHYSOBJECTS \cite{gao2024physically} & 11B/11B & Franka Emika Panda & PHYSOBJECTS \cite{gao2024physically}, real & - & 2024 \\
        \bottomrule
    \end{tabular}
    }
\end{table}
\begin{table}[H]
    \centering
    \label{tab:Interaction_1}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} c >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}p{3cm} c}
        \hline
        Model & Architecture & Size & Platform & Dataset & Hardware & Year \\
        \hline
        Text2Motion \cite{lin2023text2motion} & LLM, Library of Learned Skills, Geometric Feasibility Planner, Hybrid Planning Algorithm & - & Franka Panda, Kinect V2 & 1 million skill training datasets and OOD calibration datasets & Nvidia Quadro P5000 GPU, 2 CPU & 2023 \\
        \hline
        EmbodiedGPT \cite{mu2024embodiedgpt} & ViT-G/14 \cite{fang2023eva}, LLaMA-7B \cite{touvron2023llama} & 10B & Franka Emika Panda & EgoCOT, EgoVQA, COCO Caption \cite{lin2014microsoft}, CC3M \cite{sharma2018conceptual}, LAION-400M & - & 2024 \\
        Palm-e \cite{driess2023palm} & LLM, ViT & 12B, 84B, 562B & Franka Emika Panda & TAMP, Language-Table \cite{lynch2023interactive}, Mobile Manipulation, WebLI \cite{chen2022pali}, VQA v2 \cite{goyal2017making}, COCO \cite{chen2015microsoft}, OK-VQA \cite{marino2019ok} and others & - & 2023 \\
        \hline
        ision \cite{wake2024gpt} & GPT-4V & - & Nextage, Fetch Mobile Manipulator, SEED-noid, Shadow Dexterous Hand Lite & Cooking Video dataset & - & 2024 \\
        \hline
        ScreenAgent \cite{niu2024screenagent} & CogAgent \cite{hong2024cogagent} & - & - & ScreenAgent \cite{niu2024screenagent}, COCO \cite{chen2015microsoft}, Widget Captions \cite{li2020widget}, Mind2Web \cite{deng2024mind2web} and others & - & 2024 \\
        \hline
        Autotamp \cite{chen2024autotamp} & LLM, TAMP & - & Differential-Drive Robots and Virtual environment robot & HouseWorld1 \cite{finucane2010ltlmop}, HouseWorld2, Chips Challenge, Overcooked, Rover \cite{sun2022multi}, Wall \cite{sun2022multi} & - & 2024 \\
        \hline
        AURL \cite{thankaraj2023sounds} & ResNet \cite{he2016deep} &  -& UR10 & self-collected dataset & - & 2023 \\
        \hline
        Maniwav \cite{thankaraj2023sounds} & CLIP, AST \cite{gong2021ast} & - & UR5 robot arm & self-collected dataset & - & 2024 \\
        \hline
        LEO \cite{huang2023embodied} & Vicuna-7B \cite{chiang2023vicuna}, LoRA \cite{hu2021lora} & 7B & - & LEO-align \cite{huang2023embodied}, LEO-instruct \cite{huang2023embodied} & 4 $\times$ NVIDIA A100 GPU & 2023 \\
        \hline
        Paper \cite{zhou2023generalizable} & LLM, Dynamic Movement Primitives & - & Pybullet, MOVO & - & - & 2023 \\
        \hline
        Mutex \cite{shah2023mutex} & ViT-L/14 \cite{radford2021learning}, Whisper-Small \cite{radford2023robust} & - & LIBERO-100 \cite{liu2024libero} and 50 real-world tasks & LIBERO-100 \cite{liu2024libero}, real & 2 $\times$ NVIDIA RTX A5000 (24 GB) GPU & 2023 \\

        \hline
    \end{tabular}
    }
\end{table}

\begin{table}[t!]
    \centering
    \label{tab:Interaction_2}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} c >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}p{3cm} c}
        \hline
        Model & Architecture & Size & Platform & Dataset & Hardware & Year \\
        \hline
        TPVQA \cite{zhang2023grounding} & Planning Domain Definition Language (PDDL) or Answer Set Programming (ASP), Visual Question Answering(VQA), ViLBERT \cite{lu2019vilbert} & - & UR5e robotic arm with Hand-E gripper mounted on a Segway base and equipped with an RGB-D camera, DALL-E \cite{ramesh2021zero} based simulator & VQA v2.0 \cite{goyal2017making} & - & 2023 \\
        \addlinespace[0.5em]
        3d-vla \cite{zhen20243d} & 3D-LLM \cite{hong20233d}, BLIP2-FlanT5XL \cite{li2023blip} & - & - & 3D Embodied Instruction Tuning dataset \cite{zhen20243d} & 6 $\times$ 32 V100 GPU, 6 $\times$ 64 V100 GPU & 2024 \\
        \hline
        Grid \cite{ni2023grid} & LLM (INSTRUCTOR) , Graph Attention Networks, cross-attention, Task Decoder & - & Unity, real & Self-built dataset \cite{ni2023grid} & 2 $\times$ NVIDIA RTX 4090 GPU & 2023 \\
        \hline
        Sayplan \cite{rana2023sayplan} & GPT-4, 3DSG & - & Franka Panda 7-DoF, Omron LD-60, LiDAR & Office Environment, Home Environment & - & 2023 \\
        \hline
    \end{tabular}
    }
\end{table}

\subsection{Simulation}

\begin{table}[t!]
    \centering
    \caption{Simulators for Embodied Multimodal Large Models}
    \label{tab:Simulators}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Name & Task & Scenes & Sensors & Platform & Year \\
        \hline
        SDF-Sim \cite{rubanova2024learning} & Action, Navigation & - & RGB & - & 2024\\
        TRUMANS \cite{jiang2024scaling} & Action, Navigation & 100 interior scenes such as dining room, living room, bedroom and kitchen, etc. &  VICON, RGB-D, IMU & A800 GPU & 2024\\
        \addlinespace[0.5em]
        WonderWorld \cite{yu2024wonderworld} & Action, Navigation & - & - & A6000 GPU, AR, VR & 2024\\
        \addlinespace[0.5em]
        GenZI \cite{li2024genzi} & Action, Navigation & - & - & A100 GPU & 2024\\
        \hline
        iGibson2.0 \cite{iGibson2.0} & Action, Navigation & 15 complete interactive scenes with 108 rooms such as kitchen, bathroom, living room, etc. & RGB, D, LiDAR & Intel 5930k CPU, Nvidia GTX 1080 Ti GPU,  HTC Vive (Pro Eye), Oculus Rift S, Oculus Quest, Fetch robot, Humanoid robot with two arms & 2021\\
        \hline
        Habitat-Sim \cite{savva2019habitat} & Action, Navigation & - & RGB, D, GPS, Compass, Contact & Intel Xeon E5-2690 v4 CPU, Nvidia Titan Xp GPU, VR, Robot & 2019\\
        \hline
        Genesis \cite{Genesis} & all & all & - & - & 2024\\
        \hline
        Matterport3D \cite{chang2017matterport3d} & Navigation & 90 architectural-scale scenes such as homes, offices, and churches & RGB-D, Panoramic & Intel Xeon E5-2690 v4 CPU, Nvidia Titan Xp GPU, Robot & 2017\\
        
        SoundSpaces \cite{chen2020soundspaces} & Navigation & 103 scenes, 102 copyright-free sounds & RGB-D, microphone & - & 2020\\
        
        SoundSpaces v2 \cite{chen2022soundspaces} & Navigation & - &  RGB-D, microphone & - & 2022\\
        \hline
        
    \end{tabular}
    }
\end{table}

Embodied simulation is essential for embodied intelligence as it allows for the design of precisely controlled conditions and optimizes the training process. This enables agents to be tested in various environmental settings, enhancing their understanding and interaction capabilities. Additionally, it fosters cross-modal learning within the embodied agents themselves and facilitates the training and evaluation of generated data. To enable meaningful interaction with the environment, a realistic simulation environment must be constructed, taking into account the physical characteristics of the surroundings, the properties of objects, and their interactions. Simulation platforms generally fall into two categories: general simulators based on foundational simulations and simulators based on real-world scenarios. \cite{liu2024aligning}. Some of the latest and famous simulation methods and platforms are shown in Table \ref{tab:Simulators}.

\subsubsection{General Simulators Based on Foundational simulations}
NVIDIA Omniverse Isaac Sim is a powerful robotics simulation toolkit designed for the NVIDIA Omniverse platform. It equips researchers and practitioners with essential tools and workflows to build virtual robotic environments and conduct experiments. Isaac Sim enables the creation of highly accurate, physically realistic simulations and synthetic datasets, offering capabilities such as advanced physics simulation and multi-sensor RTX rendering. With support for ROS2, Isaac Sim facilitates the design, debugging, training, and deployment of robots, helping to accelerate the development of autonomous systems.

\subsubsection{Simulators Based on Real-world Scenarios}
In response to the challenges faced by traditional physics simulators in handling large-scale scenes, the learning-based rigid body simulator SDF-Sim \cite{rubanova2024learning} leverages learned signed distance functions (SDFs) to represent object shapes. This approach aims to accelerate distance calculations and enable efficient simulation of complex, large-scale environments.

Paper \cite{jiang2024scaling} introduces the TRUMANS \cite{jiang2024scaling} dataset and proposes an autoregressive motion diffusion model for generating human-scene interaction (HSI) sequences. The model incorporates a local scene sensor and a frame-by-frame action embedding module. The local scene sensor captures the contextual information of the scene, while the action embedding module processes action labels on a frame-by-frame basis. Together, these components enhance the model's ability to understand and control both the scene context and the actions, improving the generation of realistic HSI sequences.

3D scene generation has garnered significant attention, but most existing methods rely on offline generation, which often leads to issues such as slow generation speeds and scene geometry distortion. These limitations hinder their ability to support real-time interaction and diverse scene creation, which are essential for applications like game development and virtual reality (VR). The WonderWorld \cite{yu2024wonderworld} framework addresses these challenges by enabling the generation of diverse and coherent 3D scenes from a single image, achieving low-latency user interaction. It employs a fast Layered Gaussian Facet (FLAGS) representation, where the 3D scene is modeled as a radiation field consisting of three layers: foreground, background, and sky. Each layer is made up of a set of facets. Using a single-view layer generation approach, the framework generates images and masks for each layer from a single scene image. This method results in higher-quality scenes, improved semantic alignment, better consistency across novel views, and faster generation speeds.

GenZI \cite{li2024genzi} is a pioneering zero-shot method designed to generate 3D human-scene interactions (HSI) from text descriptions, which addresses the challenge of generating 3D interactions without relying on annotated 3D data. This method automatically generates human figures in multiple rendered views of a 3D scene and uses a LVLM to generate potential 2D interaction hypotheses. These 2D hypotheses are then transformed into 3D representations by optimizing the pose and shape parameters of a 3D human model (SMPL-X). By bridging the gap between text-based descriptions and 3D human-scene interactions, GenZI provides a powerful tool for creating realistic 3D HSIs without the need for extensive 3D datasets.

The approach used in GenZI can be related to simulation environments like iGibson 2.0 \cite{iGibson2.0} and Habitat-Sim \cite{savva2019habitat}, which focus on generating interactive 3D environments for embodied AI research. iGibson 2.0 extends multiple states for objects (e.g., temperature, humidity, cleanliness) and defines logical predicates to simulate a variety of household tasks. It is also compatible with commercial VR systems, enabling users to interact within virtual scenes. This system shares a common goal with GenZI in that both methods aim to create interactive and dynamic 3D environments, with iGibson focusing on environmental realism and task simulation, while GenZI targets realistic human interactions with scenes based on textual descriptions.

Similarly, Habitat-Sim \cite{savva2019habitat} provides a high-performance 3D simulation environment tailored for embodied AI research, with support for various 3D scenes, configurable sensors, and robot models. Habitat-Sim's efficient simulation capabilities (with rendering speeds reaching over 10,000 FPS) make it a powerful platform for large-scale AI agent training. While iGibson 2.0 focuses on simulating household environments with extended object states, Habitat-Sim supports more general-purpose 3D scene generation and robot interaction, which is critical for developing and testing AI agents in diverse environments. Both Habitat-Sim and iGibson enable users to customize physical parameters and robot models, offering flexibility in creating dynamic scenarios for AI research.

Together, these methods GenZI, iGibson 2.0, and Habitat-Sim share the common objective of improving the realism and interactivity of 3D simulations, each contributing a unique approach. GenZI focuses on transforming text-based descriptions into 3D human-scene interactions, while iGibson 2.0 and Habitat-Sim provide the virtual environments necessary for training embodied AI agents. The synergy between these methods illustrates how different techniques can complement each other to push the boundaries of realistic and interactive simulations for AI research.

It is important to emphasize that the latest technology Genesis \cite{Genesis} is powered by a newly developed general physics engine. This engine integrates a variety of physics solvers and their interactions into a unified framework. Built on this core engine, a generative agent framework has been introduced to enable fully automated data generation for robotics and other domains. This framework supports a wide range of modalities, including physically accurate and spatially consistent video, camera motion and parameters, actions of human and animal characters, robotic manipulation and motion strategies designed for real-world deployment, fully interactive 3D environments, open-world articulated object generation, as well as voice audio, facial expressions, and actions.