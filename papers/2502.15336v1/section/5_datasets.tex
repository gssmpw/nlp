\section{Datasets}
\label{sec:DATA}
In this section, we first describe the dataset collection method, followed by an introduction to the datasets used for perception and interaction models, as well as the datasets used for navigation models.

\subsection{Embodied Datasets Collection Methods}

There are two primary methods for collecting datasets related to embodied intelligence: one involves using an intelligent agent with a physical body to gather data in the real world, while the other relies on collecting datasets through a simulator.

The dataset, similar to those in  \cite{o2024open, wang2024all}, was collected in a real-world environment using various sensors, including RGB cameras, depth cameras, IMUs, LiDAR, pressure sensors, sound sensors, and others. However, during the data collection process, issues such as occlusions in the field of view or incomplete recording of operational details may arise. To address these challenges, DexCap  \cite{wang2024dexcap} utilizes Simultaneous Localization and Mapping (SLAM) to track hand movements.

Another type of dataset is collected using simulators, such as Unity and Gazebo. This approach enables the rapid generation of large volumes of multimodal data (e.g., images, depth maps, sensor data, etc.) while offering control over environmental and task variables, facilitating model training. Some of the latest and most widely used simulators are listed in Table \ref{tab:Simulators}.

\subsection{Embodied Perception and Interaction Datasets}

Several recent datasets have played a pivotal role in advancing the development of embodied intelligence for robots.

Notably, the Open X-Embodiment Dataset \cite{o2024open}, released by the Google team in collaboration with over 20 organizations and research institutes, provides a large-scale multi-modal resource. It includes data from 22 types of robots, capturing RGB images, end-point motion trajectories, and language commands across 1 million scenes, 500+ skills, and 150,000 tasks. It contains 60 datasets, some of which are shown in Table \ref{tab:dataset}.
%\vspace{1em}  % 给解释增加一些间距
%\noindent \textbf{D: Depth camera. W: Wrist camera}

The field of embodied intelligence relies heavily on diverse datasets that capture various robot operations, environments, and sensory modalities. These datasets can generally be categorized based on their data collection methods, such as real-world data, simulated data, or a combination of both, with several datasets incorporating multimodal information.

One example is the RH20T dataset  \cite{fang2024rh20t}, introduced by Hao-Shu Fang et al., which comprises over 110,000 robot operation sequences. This dataset offers a range of data modalities, including vision, force, audio, motion trajectories, demonstration videos, and natural language instructions, making it a valuable resource for training embodied intelligence models. Similarly, the ManiWAV dataset  \cite{liu2024maniwav} uses an 'ear-in-hand' data collection device to capture human demonstrations in real-world settings. It synchronizes audio and visual feedback, offering a rich source of data for learning robot manipulation policies directly from human demonstrations.

A large-scale example of multimodal data is the All Robots in One (ARIO) dataset  \cite{wang2024all}, developed by Peng Cheng Laboratory. With over 3 million samples, the ARIO dataset includes images, language commands, tactile feedback, and speech from a variety of robot platforms. It spans data collected in real-world scenarios using platforms like the Cobot Magic and Cloud Ginger XR-1 as well as data generated in simulation platforms such as Habitat \cite{savva2019habitat}, MuJoCo \cite{todorov2012mujoco}, and SeaWave \cite{ren2024surferprogressivereasoningworld}. It also includes converted data from other datasets like Open X-Embodiment \cite{o2024open}, RH20T \cite{fang2024rh20t}, and ManiWAV \cite{liu2024maniwav}, further enriching its multimodal scope.

Simulated environments also play a crucial role in data collection, as seen in the ManiSkill  \cite{mu2021maniskill} and ManiSkill2  \cite{gu2023maniskill2} datasets from UC San Diego. These datasets contain 36,000 successful manipulation trajectories along with 1.5 million point clouds and RGB-D frames, all captured in simulation. Similarly, the 3D-VLA  \cite{zhen20243d} dataset includes robot data such as 2D datasets from Open-X Embodiment \cite{o2024open}, depth-inclusive datasets like Dobb-E \cite{shafiullah2023dobbe} and RH20T \cite{fang2024rh20t}, and simulation datasets like RLBench \cite{james2020rlbench} and CALVIN \cite{mees2022calvin}. It also incorporates human-object interaction data, including the HOI4D~\cite{Liu_2022_CVPR} dataset.

Other notable datasets that blend real-world and simulated data include the DROID Dataset  \cite{khazatsky2024droid}, which contains 76,000 demonstration trajectories, 564 scenes, and 86 tasks with multi-modal data, and the BridgeData V2  \cite{walke2023bridgedata}, which offers 60,096 robot trajectory samples from 24 environments. Both datasets integrate sensory modalities such as images, depth, and natural language commands.

In addition to manipulation tasks, some datasets focus on more specialized areas of robot control. The TACO-RL  \cite{rosete2022tacorl} dataset, for example, is designed for training hierarchical policies to solve long-term robot control tasks by teleoperating robots in simulated and real environments. Meanwhile, FurnitureBench  \cite{heo2023furniturebench} is focused on testing complex, long-term operational tasks related to furniture assembly, emphasizing skills such as precise grasping, path planning, and insertion.

Finally, datasets like the Dexterous Hand dataset  \cite{fan2023arctic}, introduced by ETH Zurich, contain 2.1 million video frames, 3D hand and object meshes, dynamic contact information, and hand posture and object state trajectories, providing valuable insights into dexterous hand manipulation. The CLVR Jaco Play dataset  \cite{dass2023jacoplay} from USC, which includes 1,085 teleoperated robot episodes with various data modalities, is another key resource for training robots in manipulation tasks.

In summary, these diverse datasets, ranging from real-world to simulated environments, offer a wealth of multimodal data that enable advancements in embodied intelligence, robotic manipulation, and human-robot interaction.

For datasets specifically targeting touch-related modalities, notable contributions include the TVL dataset \cite{fu2024touch} and the Touch 1k dataset \cite{cheng2024touch100k}, which focus primarily on multi-modal alignment or representation learning. A more comprehensive list of such datasets can be found in Touch100k \cite{cheng2024touch100k}. However, few of these datasets have been applied directly to robotic tasks, such as interaction or navigation. While smaller-scale datasets like those discussed in  \cite{calandra2018more} and  \cite{han2024learning} are available for specific tasks, they are not designed for learning general models and have limited applicability.

\subsection{Embodied Navigation Datasets}
Embodied navigation datasets aim to enhance robots' ability to accurately navigate in physical or simulated environments based on visually-linguistic combined instructions. This is achieved by providing long and complex paths and instructions, real-world data, diverse indoor and outdoor scenes, support for training large high-capacity models, and detailed intermediate products such as 3D scene reconstructions, relative depth estimates, object labels, and localization information. These datasets effectively expand the application scenarios of vision-language navigation and provide strong data support for solving practical downstream application problems. The dataset is shown in Table \ref{tab:dataset}.

HM3D \cite{ramakrishnan2021habitat} is the largest-scale building-scale 3D scene reconstruction dataset, containing diverse real spaces from around the world. HM3D provides 1,000 nearly complete high-fidelity reconstructions of entire buildings. Each reconstruction was captured using the Matterport Pro2 tripod-based depth sensor to capture the habitable and navigable spaces of each interior space. Additionally, the HM3D dataset seamlessly integrates with FAIR's Habitat simulator, supporting the training and evaluation of agents (such as home robots and AI assistants).

The Gibson environment \cite{xia2018gibson} designed for training and testing real-world perceptive agents. Gibson is based on virtualized real spaces, rather than artificially designed spaces, and currently includes over 1400 floor spaces from 572 complete buildings. The main features of Gibson include: I. Originating from the real world and reflecting its semantic complexity, II. The internal synthesis mechanism ``Goggles'' allows trained models to be deployed in the real world without the need for domain adaptation, III. The embodiment of the agent and its subjection to physical and spatial constraints.

The Matterport3D dataset \cite{chang2017matterport3d} is a large-scale RGB-D dataset, containing 10,800 panoramic views and 194,400 RGB-D images from 90 building-scale scenes. The dataset provides annotations for surface reconstruction, camera poses, as well as 2D and 3D semantic segmentation. Based on the Matterport3D environment, Peter Anderson et al. collected the R2R dataset \cite{anderson2018vision}, which includes 21,567 open-vocabulary, crowdsourced navigation instructions with an average length of 29 words. Each instruction describes a trajectory that typically crosses multiple rooms. The associated task requires the agent to navigate to a target location in a previously unseen building by following natural language instructions. Like R2R, REVERIE is built on the Matterport3D simulator, providing detailed indoor navigation environments. Unlike R2R, REVERIE dataset \cite{Qi_2020_CVPR} introduces the Remote Embodied Visual Referring Expression in Real Indoor Environments task for the first time, requiring the agent to navigate and identify remote objects in real indoor environments based on natural language instructions. The target object is not observable from the starting point, which means the robot must have common sense and reasoning abilities to reach the location where the target might appear. The goal is to assess the agent's ability to navigate and identify target objects based on advanced natural language instructions, emphasizing the robot's need for natural language understanding and visual navigation.

The ScanQA dataset \cite{Azuma_2022_CVPR} is designed for a 3D question-answering (3D-QA) task in three-dimensional spatial scene understanding. It requires models to receive visual information from the entire 3D scene derived from rich RGB-D indoor scans and to answer given textual questions about that 3D scene. The dataset contains over 41k question-answer pairs from 800 indoor scenes in the ScanNet dataset. These question-answer pairs are manually editeD,  with answers associated with the 3D objects in each 3D scene. The ScanQA dataset aims to advance research in 3D spatial understanding by answering questions about 3D scenes through the combination of linguistic expressions and 3D geometric features.

The LLaVA dataset \cite{liu2024visual} is a specialized instruction-tuning dataset for multimodal tasks, containing 158K unique language-image instruction-following samples, which are divided into 58K dialogues, 23K detailed descriptions, and 77K complex reasonings. This dataset enhances the model's zero-shot capability on new tasks through visual instruction tuning, a process that enables the model to better understand and execute visual instructions. In navigation applications, the LLaVA dataset can be used to train models to understand and execute instructions based on visual and textual information. This capability is crucial for embodied navigation tasks, which often involve navigating based on visual information. Specifically, the LLaVA dataset supports the model's navigation and execution capabilities in complex environments by providing a diverse range of instruction-tuning data.

Targeting at the audio-assited navigation task, the SoundSpaces simulator~\cite{chen2020soundspaces}, based on Matterport3D \cite{chang2017matterport3d} and Replica \cite{straub2019replica}, takes the space and material property into account to render realistic sound in a 3D space. It includes 85 scenes from Matterport3D,  18 scenes from Replica and 102 copyright-free sounds. To extend the SoundSpaces simulator, the SoundSpaces-v2~\cite{chen2022soundspaces} was designed with ability of generalizing to new environments, freely adjusting the material and microphone configuration.

\begin{table}[H]
    \centering
    \caption{Embodiment Dataset}
    \label{tab:dataset}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3.5cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Dataset & Task & Scenes & Robot & Robot Morphology & Sensors (D: Depth camera. W: Wrist camera) & Language Annotations & Year\\
        \hline
        RT-1 Robot Action \cite{brohan2022rt} & Action & 130k+ episodes,700+ tasks &  Google Robot & Mobile Manipulator & RGB, D & Templated & 2022  \\
        \hline
        QT-Opt \cite{kalashnikov2018qt} & Action & 580k real-world grasp  & Kuka iiwa & Single Arm & RGB & None &  2018 \\
        \hline
        Berkeley Bridge \cite{walke2023bridgedata} & Action & 13 skills, 24 environments, 60,096 trajectories & WidowX & Single Arm & RGB, D, W & Natural & 2023 \\
        \hline
        Freiburg Franka Play \cite{rosete2022tacorl} & Action & - & Franka & Single Arm & RGB,  D, W & Templated & 2022\\
        USC Jaco Play \cite{dass2023jacoplay} & Action & 1085 teleoperated robot episodes & Jaco 2 & Single Arm & RGB,  W & Templated & 2023\\
        Berkeley Cable Routing \cite{luo2023multistage} & Action & 1442 trajectories, 257 tasks & Franka & Single Arm & RGB,  W & None &  2023 \\
        \hline
        Roboturk \cite{mandlekar2019scaling} & Action & three real world tasks,2144 demonstrations & Sawyer & Single Arm & RGB,  D & Templated & 2019  \\
        \hline
        NYU VINN \cite{pari2021surprising} & Action & 435 episodes & Hello Stretch & Mobile Manipulator & RGB,  W & None & 2021 \\
        
        RECON \cite{shah2021rapid} & Navigation  & 11830 episodes & Jackal & Wheeled Robot & RGB, D, W & None & 2021 \\
        HM3D \cite{ramakrishnan2021habitat} & Navigation & 1,000 building-scale scenes & Simulated & Virtual Agent & RGB, D & None & 2021 \\
        Gibson \cite{xia2018gibson} & Navigation & 571 scenes & Simulated & Virtual Agent & RGB, D & None & 2018 \\
        \hline
        R2R \cite{anderson2018vision} & Navigation & 90 buildings & Simulated & Virtual Agent & RGB, D & Natural language instructions & 2018 \\
        \hline
        MP3D \cite{chang2017matterport3d} & Navigation & 90 buildings & Simulated & Virtual Agent & RGB, D & None & 2017 \\
        AI2THOR \cite{kolve2017ai2} & Navigation & 120 scenes  & Simulated & Virtual Agent & RGB, D & None & 2017 \\
        iTHOR \cite{weihs2021visual} & Navigation & 120 rooms & Simulated & Virtual Agent & RGB, D & None & 2022 \\
        \hline
        RxR \cite{ku2020room} & Navigation & 90 houses & Simulated & Virtual Agent & RGB, D & Natural Language Instructions & 2020 \\
        \hline
        R4R \cite{jain2019stay} & Navigation & 90 houses & Simulated & Virtual Agent & RGB, D & Natural Language Instructions & 2019 \\
        \hline
        REVERIE \cite{Qi_2020_CVPR} & Navigation & 90 buildings & Simulated & Virtual Agent & RGB, D & Natural language instructions & 2020 \\
        \hline
        SOON \cite{zhu2021soon} & Navigation & 90 houses & Simulated & Virtual Agent & RGB, D & Natural Language Instructions & 2021 \\
        \hline
        RoboTHOR \cite{deitke2020robothor} & Navigation & 89 scenes & LoCoBot & Wheeled Robot & RGB, D & None & 2020 \\
        \hline
        CVDN \cite{thomason2020vision} & Navigation & 83 houses & Simulated & Virtual Agent & RGB & human dialogs & 2020 \\
        \hline
        ScanQA \cite{Azuma_2022_CVPR} & 3D Question Answering & 800 rooms & Simulated & - & RGB, D & 41k QA pairs & 2022 \\
        LLaVA-23k \cite{liu2024visual} & Visual Instruction Tuning & 80k images & Simulated & - & RGB & 158k instructions & 2023 \\
        \hline
        MP3D-EQA \cite{wijmans2019embodied} & Embodied QA & 83 homes, 144 floors & Simulated & Virtual Agent & RGB, D & 1.1k Template-based QA & 2019 \\  
        \hline
        Austin VIOLA \cite{zhu2022viola} & Action & 1 scene, multi-task household operations & Franka & Single Arm & RGB, W & Templated & 2022 \\
        Berkeley Autolab UR5 \cite{BerkeleyUR5Website} & Action & 4 scenes, 4 tasks & UR5 & Single Arm & RGB, D, W & Templated & \\
        \hline
        TOTO Benchmark \cite{zhou2023train} & Action & 1 scene, 2 tasks (scooping, pouring) & Franka & Single Arm & RGB & None & 2023\\
        \hline
        Language Table \cite{lynch2023interactive} & Action & Multi-scene, natural language instruction tasks, 442k trajectories & xArm & Single Arm & RGB & Natural & 2023\\
        \hline
        Columbia PushT Dataset \cite{chi2023diffusionpolicy} & Action & 1 scene, 2 tasks (T-block pushing), 122 trajectories & UR5 & Single Arm & RGB, W & None & 2023\\
        \hline
        Stanford Kuka Multimodal \cite{lee2019icra} & Action & 1 scene, multi-task (peg insertion), 3k trajectories & Kuka iiwa & Single Arm & RGB & None & 2019\\
        \hline
    \end{tabular}
    }
\end{table}
\begin{table}[t!]
    \centering
    \label{tab:dataset_1}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3.5cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Dataset & Task & Scenes & Robot & Robot Morphology & Sensors (D: Depth camera. W: Wrist camera) & Language Annotations & Year\\
        \hline
        NYU ROT \cite{haldar2023watch} & Action & Multi-scene, diverse manipulation tasks & xArm & Single Arm & RGB & Templated & 2023\\
        \addlinespace[0.5em]
        Stanford HYDRA \cite{belkhale2023hydra} & Action & 3 scenes (kitchen), 3 tasks (coffee, toasting, utensil sorting), 550 trajectories & Franka & Single Arm & RGB, W & Templated & 2023\\
        \addlinespace[0.5em]
        Austin BUDS \cite{zhu2022bottom} & Action & 1 scene, long-horizon kitchen tasks, 50 trajectories & Franka & Single Arm & RGB, W & None & 2022\\
        NYU Franka Play \cite{cui2022play} & Action & 1 scene (toy kitchen), diverse tasks, 456 trajectories & Franka & Single Arm & RGB, D & None & 2022\\
        \hline
        Maniskill \cite{gu2023maniskill2} & Action & Multi-scene (Table Top/Ground), 12 tasks (pick, stack, insert), 30k trajectories & Franka & Single Arm & RGB, D, W & Templated & 2023\\
        \hline
        Furniture Bench \cite{heo2023furniturebench} & Action & Multi-scene, 9 furniture models (assembly tasks), 5,100 trajectories & Franka & Single Arm & RGB, W & Templated & 2023\\
        CMU Franka Exploration \cite{mendonca2023structured} & Action & 1 scene (toy kitchen), exploration tasks & Franka & Single Arm & RGB & Templated & 2023\\
        UCSD Kitchen & Action & Multi-scene (kitchen), complex object manipulation & xArm & Single Arm & RGB & Natural & 2023\\
        \hline
        UCSD Pick Place \cite{Feng2023Finetuning} & Action & 1 scene, pick-and-place with distractors, 1,355 trajectories & xArm & Single Arm & RGB & Templated & 2023\\
        Austin Sailor \cite{nasiriany2022sailor} & Action & 1 scene (toy kitchen), food/utensil manipulation, 250 trajectories & Franka & Single Arm & RGB, W & None & 2022\\
        Austin Sirius \cite{liu2022robot} & Action & 2 scenes, kcup/gear insertion tasks, 600 trajectories & Franka & Single Arm & RGB, W & None & 2023\\
        \hline
        BC-Z \cite{jang2021bc} & Action & Multi-scene (household/office), 100+ tasks, 39k trajectories & Google Robot & Mobile Manipulator & RGB & Templated & 2021\\
        USC Cloth Sim \cite{salhotra2022dmfd} & Action & 1 scene, cloth manipulation, 1k trajectories & Franka & Single Arm & RGB & None & 2022\\
        Tokyo PR2 Fridge Opening \cite{oh2023pr2utokyodatasets} & Action & 1 scene (kitchen), fridge opening tasks & PR2 & Single Arm & RGB & None & 2023\\
        Tokyo PR2 Tabletop Manipulation \cite{oh2023pr2utokyodatasets} & Action & 1 scene, bread/grape manipulation, 192 trajectories & PR2 & Single Arm & RGB & None & 2023\\
        Saytap \cite{saytap2023} & Action & 1 scene (ground), quadrupedal locomotion tasks & Unitree A1 & Quadrupedal Robot & - & Natural & 2023\\
        \addlinespace[0.5em]
        UTokyo xArm PickPlace \cite{matsushima2023weblab} & Action & 1 scene, plate stacking tasks, 95 trajectories & xArm & Single Arm & RGB, W & None & 2023\\
        \addlinespace[0.5em]
        UTokyo xArm Bimanual \cite{matsushima2023weblab} & Action & 1 scene, towel manipulation, 70 trajectories & xArm Bimanual & Bi-Manual & RGB & None & 2023\\
        \hline
        Robonet \cite{dasari2019robonet} & Action & Multi-scene, object interaction, 82k trajectories & Multi-Robot & Single Arm & RGB & None & 2019\\
        \hline
    \end{tabular}
    }
\end{table}
\begin{table}[t!]
    \centering
    \label{tab:dataset_2}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3.5cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Dataset & Task & Scenes & Robot & Robot Morphology & Sensors (D: Depth camera. W: Wrist camera) & Language Annotations & Year\\
        \hline
        Berkeley MVP Data \cite{Radosavovic2022} & Action & Multi-scene (toy kitchen/table), basic motor tasks, 480 trajectories & xArm & Single Arm & RGB, W & Templated & 2022\\
        \hline
        Berkeley RPT Data \cite{Radosavovic2023} & Action & Multi-scene, pick/stack tasks, 960 trajectories & Franka & Single Arm & RGB, W & Templated & 2023\\
        \hline
        KAIST Nonprehensile Objects \cite{kimpre} & Action & 1 scene, non-grasping manipulation, 201 trajectories & Franka & Single Arm & RGB & Natural & 2023\\
        \hline
        QUT Dynamic Grasping \cite{burgess2022dgbench} & Action & 1 scene, dynamic object grasping, 812 trajectories & Franka & Single Arm & RGB, W & None & 2022\\
        \hline
        Stanford MaskVIT Data \cite{gupta2022maskvit} & Action & Multi-scene (container), object pushing/picking, 9.2k trajectories & Sawyer & Single Arm & RGB & None & 2022\\
        \hline
        LSMO Dataset \cite{Osa22} & Action & 1 scene, obstacle avoidance, 50 trajectories & Cobotta & Single Arm & RGB & Natural & 2022\\
        DLR Sara Pour Dataset \cite{padalkar2023guiding} & Action & 1 scene, cup-to-cup pouring, 100 trajectories & DLR SARA & Single Arm & RGB & None & 2023\\
        DLR Sara Grid Clamp Dataset \cite{padalkar2023guided} & Action & 1 scene, grid clamp placement, 100 trajectories & DLR SARA & Single Arm & RGB & None & 2023\\
        \hline
        DLR Wheelchair Shared Control \cite{vogel_edan_2020,quere_shared_2020} & Action & Multi-scene (table/shelf), object grasping, 100 trajectories & DLR EDAN & Single Arm & RGB & Templated & 2020\\
        ASU TableTop Manipulation \cite{zhou2023modularity, zhou2023learning} & Action & 1 scene, object interaction, 110 trajectories & UR5 & Single Arm & RGB & Templated & 2023\\
        Stanford Robocook \cite{shi2023robocook} & Action & Multi-scene (kitchen), dough manipulation, 2.5k trajectories & Franka & Single Arm & RGB, D & Templated & 2023\\
        ETH Agent Affordances \cite{schiavi2023learning} & Action & 1 scene (kitchen), oven/door manipulation, 120 trajectories & Franka & Mobile Manipulator & D & Templated & 2023\\
        Imperial Wrist Cam & Action & Multi-scene (household), object interaction & Sawyer & Single Arm & RGB, W & Natural &\\
        CMU Franka Pick-Insert Data \cite{saxena2023multiresolution} & Action & 1 scene, peg insertion, 520 trajectories & Franka & Single Arm & RGB, W & Templated & 2023\\
        QUT Dexterous Manpulation \cite{ceola2023lhmanip} & Action & Multi-scene (table/kitchen), food serving/tidying, 200 trajectories & Franka & Mobile Manipulator & RGB, W & Natural & 2023\\
        MPI Muscular Proprioception \cite{guist2023robust} & Action & 1 scene (lab), random motion & PAMY2 & Single Arm & - & None & 2023\\
        UIUC D3Field \cite{wang2023d3field} & Action & Multi-scene (household), object arrangement, 196 trajectories & Kinova Gen3 & Single Arm & RGB, D & None & 2023\\
        \hline
        Austin Mutex \cite{shah2023mutex} & Action & Multi-scene (Home Environment), 100+ tasks (pick-place, contact-rich), 1,500 trajectories & Franka & Single Arm & RGB, W & Natural Language (GPT4 + human correction) & 2023 \\
        Berkeley Fanuc Manipulation \cite{fanuc_manipulation2023} & Action & 1 scene, multi-task (drawer opening, object pickup), 415 trajectories & Fanuc Mate & Single Arm & RGB, W & Natural & 2023 \\
        \hline
        CMU Food Manipulation \cite{sawhney2021playing} & Action & 1 scene, 21 food types (slicing/manipulation), 4,200 trajectories & Franka & Single Arm & RGB, W & Templated & 2021 \\
        \hline
    \end{tabular}
    }
\end{table}
\begin{table}[t!]
    \centering
    \label{tab:dataset_3}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3.5cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Dataset & Task & Scenes & Robot & Robot Morphology & Sensors (D: Depth camera. W: Wrist camera) & Language Annotations & Year\\
        \hline
        CMU Play Fusion \cite{chen2023playfusion} & Action & 3 scenes (grill, table setup, sink), complex object interaction, 576 trajectories & Franka & Single Arm & RGB & Natural & 2023 \\
        CMU Stretch \cite{bahl2023affordances, mendonca2023structured} & Action & Multi-scene (kitchen, hallways), navigation + interaction, 135 trajectories & Hello Stretch & Mobile Manipulator & RGB & Templated & 2023 \\
        CoryHall \cite{kahn2018self} & Navigation & 1 scene (office hallways), navigation tasks, 7,328 trajectories & RC Car & Wheeled Robot & RGB & None & 2018 \\
        SACSoN \cite{hirose2023sacson} & Navigation & Multi-scene (office, school), pedestrian-rich navigation, 3,000 trajectories & TurtleBot 2 & Wheeled Robot & RGB, D & None & 2023 \\
        \hline
        RoboVQA & Action & Multi-scene (3 office buildings), long-horizon tasks, 61k trajectories & Google Robot & 3 embodiments & RGB, D & Natural & \\
        \hline
        ALOHA \cite{Zhao2023LearningFB} & Action & 1 scene, dexterous tasks (unwrapping, shoe fitting), 451 trajectories & ViperX Bimanual & Bi-Manual & RGB, W & Templated & 2023 \\
        \hline
        DROID \cite{khazatsky2024droid} & Action & Multi-scene (household), object manipulation, 92k trajectories & Franka & Single Arm & RGB, D, W & Natural & 2024 \\
        ConqHose \cite{ConqHoseManipData} & Action & 1 scene (office), vacuum hose manipulation, 139 trajectories & Spot & Mobile Manipulator & RGB & Natural & 2024 \\
        DobbE \cite{shafiullah2023dobbe} & Action & Multi-scene (household), mobile manipulation, 5k trajectories & Hello Stretch & Mobile Manipulator & RGB, D, W & Natural & 2023 \\
        FMB \cite{luo2024fmb} & Action & 1 scene, multi-object manipulation, 1,804 trajectories & Franka & Single Arm & RGB, D, W & Templated & 2024 \\
        IO-AI Office PicknPlace & Action & Multi-scene (office), pick-and-place tasks, 3,847 trajectories & Human & Human & RGB, D, W & Templated & \\
        \hline
        MimicPlay \cite{wang2023mimicplay} & Action & Multi-scene, imitation tasks, 378 trajectories & Franka & Single Arm & RGB & None & 2023 \\
        \hline
        MobileALOHA \cite{fu2024mobile} & Action & Multi-scene (household), mobile dexterity tasks, 276 trajectories & Mobile ALOHA & Mobile Manipulator & RGB & Templated & 2024 \\
        \hline
        RoboSet \cite{bharadhwaj2023roboagent} & Action & Multi-scene, 18k tasks (pick, stack), 18k trajectories & Franka & Single Arm & RGB, D, W & Natural & 2023 \\
        \hline
        TidyBot \cite{wu2023tidybot} & Action & 1 scene (household), object arrangement, 24 trajectories & TidyBot & Mobile Manipulator & - & Text-based placements & 2023 \\
        \hline
    \end{tabular}
    }
\end{table}
\begin{table}[t!]
    \centering
    \label{tab:dataset_4}
    \setlength{\tabcolsep}{3pt}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{c >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3.5cm} >{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} c}
        \hline
        Dataset & Task & Scenes & Robot & Robot Morphology & Sensors (D: Depth camera. W: Wrist camera) & Language Annotations & Year\\
        \hline
        VIMA \cite{jiang2023vima} & Action & Multi-scene, multimodal tasks, 660k trajectories & UR5 & Single Arm & RGB & Multimodal templated & 2023 \\
        \hline
        SPOC \cite{spoc2023} & Action & Multi-scene (household), LLM-augmented tasks, 233k trajectories & Hello Stretch & Single Arm & RGB, D, W & Scripted + LLM & 2023 \\
        \hline
        Plex RoboSuite \cite{thomas2023plex} & Action & 1 scene, object interaction, 450 trajectories & Franka & Single Arm & RGB, D, W & None & 2023 \\
        \hline
    \end{tabular}
    }
\end{table}