\section{Development of Foundational Models for Embodied Multimodal Large Models}
\label{sec:LM}
The evolution of large models, particularly in natural language processing, computer vision, and deep learning, has paved the way for the emergence of EMLMs. These advanced systems leverage the integration of multiple modalities, such as vision, language, audio and touch, to enable more natural and intuitive interactions with the physical world. The advancements in large-scale pretraining and the scaling of neural networks have facilitated the creation of models capable of processing multimodal data while embodying a deeper understanding of context, actions, and interactions, setting the stage for the next frontier in AI.

In this section, we first review the development of embodied agents, which are the carriers of the large models in the physical or virtual worlds. Then, we introduced the basic techniques of multimodal large models used in EMLMs, including language, vision, audio and touch models.

\subsection{Embodied Agents}
% to modify
\begin{figure}[t]
	\centering
	%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
	\includegraphics[width=1.0\linewidth]{pictures/agents.pdf}
	
	\caption{Examples of embodied AI agents.}
	\label{fig:agents}
\end{figure}

Embodied agents are autonomous entities that have physical or virtual body and are able to perceive, act and interact with the environments. They have various types, such as the robots, autonomous cars, virtual agents, etc. 

As shown in Fig.~\ref{fig:agents}, robots are the most popular agents used in existing embodied AI algorithms. Depending on the applications, robots have various forms, including fixed-base robots, wheeled robots, quadrupted robots, human robots, soft robots, etc. Their unique shapes and designs make them specialized in specific tasks. For example, the fixed-base robots, like franka robots~ \cite{haddadin2022franka} are usually employed in industrial environments for automated picking and placing tasks. In contrast, humanoid robots, with their human-like appearance and adaptability, are versatile and can be deployed across a broad range of domains. 

An autonomous vehicles can also be considered as an embodied agents. They perceive their environment, make real-time decisions, and interact with both drivers and the surrounding environment. Beyond driving safety, these systems are increasingly capable of interpreting human instructions and engaging in conversations with passengers \cite{cui2024survey}. Virtual agents, on the other hand, are prominent in applications such as gaming \cite{hubble2021artificial}, social experiments \cite{park2023generative}, virtual idols \cite{kong2021difference}, etc. These agents interact with users through multiple modalities, including language, visual, and audio, enabling rich and immersive experiences. 

\subsection{Large Language Models}
%\kai{Are there some relations between these methods or difference}
LLMs, such as GPT-4 \cite{GPT4}, BERT \cite{devlin2018bert}, and T5 \cite{2020t5}, have emerged as foundational components in modern AI, particularly within NLP. These models are designed to capture complex linguistic patterns and structures through unsupervised learning on massive amounts of text data. In the context of embodied multimodal systems, LLMs play a critical role as the primary mechanism for understanding and generating natural language. Their ability to process and produce coherent text enables them to bridge the gap between different modalities, including vision, speech, and action. By integrating LLMs with visual or auditory data, embodied systems can interpret multimodal inputs and generate contextually relevant responses, thus enabling more interactive and intelligent behaviors. Essentially, LLMs serve as the ``language brain'' of these systems, enabling them to understand and execute language-based commands, describe visual scenes, or facilitate complex reasoning across modalities.

BERT \cite{devlin2018bert}, proposed by Google, is based on the Transformer architecture and utilizes a masked language model for pre-training, which significantly improves performance across various NLP tasks. Generative Pre-trained Transformer (GPT) \cite{GPT1}, developed by OpenAI, is a Transformer-based generative model that employs autoregressive training to generate text sequentially. GPT employs a large-scale unsupervised learning approach, leading to breakthroughs in generative models. GPT-2 further scales up both the size and performance of language models, showcasing its ability to generate coherent and natural language. XLNet \cite{yang2019xlnet} introduces a model that combines autoregressive and autoencoding approaches, surpassing BERT on multiple NLP benchmarks. Text-to-Text Transfer Transformer (T5) \cite{2020t5}, proposed by Google, unifies all NLP tasks into a ``text-to-text'' framework, enabling the model to perform transfer learning across different tasks.

The release of GPT-3 \cite{GPT3} marked a milestone as the largest and most powerful language model at the time, with 175 billion parameters. GPT-3's launch represented a significant breakthrough in model scale, dramatically enhancing its performance in tasks such as text generation, question answering, and translation, while also demonstrating the potential for zero-shot and few-shot learning. ChatGPT (based on GPT-3.5 and subsequent versions) revolutionized conversational AI, enabling natural and fluid interactions with users while supporting a wide range of knowledge domains. This development marked a turning point in the practical application of LLMs, drawing significant attention from industries such as customer service, education, and content creation. GPT-4 \cite{GPT4} further advanced reasoning capabilities and introduced multi-modal abilities, demonstrating stronger intelligent features through the combined use of text and images. It extended the capabilities of traditional LLMs by supporting multimodal inputs, such as images alongside text. This allowed GPT-4 to generate text descriptions for images, answer questions related to visual content, and even guide actions in embodied systems like robots. DeepSeek-V3 \cite{liu2024deepseek} further expands the boundaries of multimodal reasoning by adopting a dynamic sparse activation architecture. It innovatively introduces a hybrid routing mechanism that combines task-specific experts with dynamic parameter allocation, achieving higher computational efficiency in cross-modal fusion tasks. These models showcase the power of LLMs in creating robust multimodal systems capable of not only understanding but also interacting with the world through both language and sensory inputs. Meta's LLaMA \cite{llama1} series (including LLaMA-2 \cite{llama2}) has also emerged as a competitor, reflecting growing investments by tech companies in the development of LLMs.

The evolution of LLMs has transitioned from simple statistical models to deep learning-based breakthroughs, and now to the ultra-large-scale Transformer models. With the continued enhancement of computational power and algorithm optimization, LLMs are expected to achieve greater levels of intelligence, enabling them to play an increasingly pivotal role across diverse domains. In the domain of embodied intelligence, these models will further enhance the interaction capabilities between agents and humans, making embodied agents smarter and more capable.

\subsection{Large Vision Models}
Unlike LLMs, LVMs process image or video information. These models have demonstrated exceptional performance in tasks such as image recognition, object detection, image generation, and cross-modal learning. In the realm of embodied intelligence, LVMs also play a crucial role, enabling robots to perceive and understand the visual world in complex and dynamic environments.

ResNet \cite{he2016deep} is a deep convolutional neural network, most notably distinguished by the introduction of residual connections. These connections help address the gradient vanishing and gradient exploding issues that can arise when training very deep neural networks, enabling effective training even with networks containing hundreds or thousands of layers.

Vision Transformer (ViT) \cite{dosovitskiy2020image} is a groundbreaking model that applies the Transformer architecture, originally developed for NLP, to computer vision. Instead of relying on traditional convolutional operations, ViT divides images into fixed-size patches and processes these patches using a self-attention mechanism to capture global dependencies across the image. ViT demonstrates that the Transformer architecture is not only effective for NLP but also powerful in computer vision, particularly when working with large-scale datasets. The success of ViT marks the beginning of a shift away from the dominance of convolutional neural networks, with Transformers emerging as a key tool for visual tasks.

Swin Transformer \cite{liu2021swin} is another Transformer-based model in computer vision. Its key innovation is the introduction of a ``window'' mechanism for performing self-attention within local regions, which improves computational efficiency. Unlike ViT, Swin Transformer adopts a hierarchical design that gradually expands the receptive field, along with window-based division, enabling the model to capture local information while preserving global context. Swin Transformer has delivered strong performance across multiple visual tasks, particularly in object detection and semantic segmentation.

Segment Anything Model (SAM) \cite{kirillov2023segment} is a new visual model launched by Meta that aims to provide a general, flexible, and efficient solution for image segmentation tasks. SAM is trained on a large-scale dataset and can handle a variety of segmentation tasks, including semantic segmentation, instance segmentation, and object segmentation. In addition to supporting traditional segmentation tasks, SAM can be fine-tuned based on user interactions, offering strong adaptability. The release of SAM has established a powerful benchmark for segmentation and spurred further research into interactive and adaptive learning models in computer vision.

DINO \cite{caron2021emerging} and DINOv2 \cite{oquab2023dinov2} are self-supervised learning models proposed by the Facebook AI research team, built on the ViT architecture. The key innovation of DINOv2 lies in its self-supervised approach, which enables effective image representation learning without requiring manual labels. Unlike traditional self-supervised methods, DINOv2 employs an enhanced contrastive learning technique and leverages a larger training dataset, improving the model's representational capacity. DINOv2 has achieved top-tier performance across various visual tasks, including image retrieval, classification, and detection, and has opened new avenues for self-supervised learning research in computer vision.

\subsection{Large Vision-Language Models}
Among EMLMs, Large Vision-Language Models (LVLMs) enhance an agent's environmental understanding, reasoning, and task execution by integrating visual and linguistic information. LVLMs enable agents to fuse and coordinate multimodal data, allowing them to recognize objects through visual input and perform actions based on language instructions. Additionally, LVLMs facilitate cross-modal reasoning and adaptive decision-making in dynamic environments, significantly improving the interaction and navigation capabilities of robots.

CLIP \cite{radford2021learning} is a multi-modal model developed by OpenAI, designed to embed both images and text into a shared vector space using contrastive learning techniques. The model is pre-trained on a large corpus of image-text pairs, enabling it to perform tasks such as image classification and image-text retrieval. CLIP employs contrastive loss to optimize the alignment between text and image descriptions, demonstrating strong zero-shot learning capabilities.

DALL·E \cite{ramesh2021zero} is an image generation model released by OpenAI that generates images based on textual prompts. Built upon the GPT-3 and Vector Quantized Variational Autoencoder architectures, the model excels in creating highly realistic and creative images. DALL·E2 \cite{ramesh2022hierarchical} improves upon its predecessor by enhancing the quality and diversity of generated images, enabling the creation of detailed and complex visuals based on sophisticated text descriptions.DALL·E3 \cite{betker2023improving} has better performance.

BLIP \cite{li2022blip}, introduced by Salesforce Research, employs a two-way self-supervised learning approach to integrate visual and linguistic information. BLIP boosts pre-training efficiency through a “guided” strategy, helping the model better grasp the finer details in visual-language tasks, particularly in visual question answering (VQA) and image captioning.

Flamingo \cite{alayrac2022flamingo} is a novel visual-language model from DeepMind that can process multi-modal data (images and text) and perform cross-modal reasoning. Unlike traditional models, Flamingo excels in few-shot learning, allowing it to tackle tasks with minimal labeled data without the need for extensive training datasets. This model emphasizes few-shot capabilities, making it suitable for scenarios that require efficient handling of various data types.

Visual BERT \cite{li2019visualbert} is a variant of the BERT model developed by Facebook AI Research that integrates visual information. It jointly encodes image features and text to address cross-modal tasks. By embedding image region features into the BERT framework, Visual BERT optimizes the semantic alignment between text and images during pre-training.

ALIGN \cite{jia2021scaling} is a large-scale image-text alignment model from Google Research. It leverages contrastive learning on vast amounts of image-text data from the internet to understand the relationships between images and their textual descriptions. ALIGN excels in tasks like image classification and image-text retrieval, offering powerful cross-modal search capabilities through large-scale training.

GIT \cite{wang2022git} is a pre-trained model from Microsoft Research designed for image-text generation and understanding. It efficiently extracts and generates information across both visual and linguistic domains. GIT can generate descriptive text from images and create corresponding images based on text inputs, facilitating versatile cross-modal generation tasks.

MDETR \cite{kamath2021mdetr} is a Transformer-based visual-language model developed by Facebook AI Research that uses a ``modulation mechanism'' to handle image-text pair tasks. The model identifies specific regions in images based on text descriptions, allowing for precise multi-modal reasoning by associating these regions with the provided text.

PaLM-E \cite{driess2023palm} is a powerful multimodal pre-training model proposed by Google Research that combines three modalities: images, text, and actions (such as robotic operations) to promote the development of cross-modal intelligent systems. PaLM-E is an extended version of PaLM (Pathways Language Model), focusing on bridging the gap between vision and language models while providing enhanced perception and reasoning capabilities for robots and other intelligent systems.

CoCa \cite{yu2022coca} was released by Meta. One of its important features is that it combines contrastive learning and generative learning. Contrastive learning is usually used to improve the performance of models in visual-language tasks, helping models to better understand and distinguish the relationship between images and text. Generative learning enables the model to generate descriptive text based on the input image, or generate images based on text.

\subsection{Other Modal Models}
\subsubsection{Vision-Audio Models}

In addition to vision and language, audio plays a crucial role in everyday tasks, helping us identify scenarios and locate sound-emitting objects. While most research focuses on vision-audio or audio-language data, few studies explore audio in embodied tasks. Recent work in audio-visual navigation tasks, such as SoundSpaces \cite{chen2020soundspaces}, combines vision and audio for tasks like AudioGoal (where targets are indicated by sound) and AudioPointGoal (where audio provides directional guidance). These tasks face challenges, such as the need for continuous sound, which has been addressed by linking audio with semantic meanings and spatial properties \cite{chen2021semantic}. Researchers have also introduced more complex audio scenarios with multiple sound sources and distractions \cite{yu2022sound,younes2023catch}. In manipulation tasks, audio provides crucial contact information, complementing vision and touch \cite{li2023see}. Studies show that combining audio with action generation and feedback significantly improves task performance, with self-supervised learning methods and augmented audio data boosting success rates in real-world environments~ \cite{thankaraj2023sounds,liu2024maniwav}.

\subsubsection{Vision-Touch Models}

In manipulation tasks, visual information is often the primary source for adjusting robot motions, but vision sensors can be limited by occlusion and their inability to measure contact force, which is crucial for successful actions. To address this, several studies have explored combining vision and tactile data. For example, \cite{dikhale2022visuotactile} proposed a network that integrates vision and tactile data to estimate the 6D pose of objects for in-hand manipulation. In grasping tasks, \cite{calandra2018more} investigated a learned regrasp policy that iteratively adjusts the grasp using both visual and tactile feedback, showing improved performance with touch information. To handle more complex objects, such as deformable ones, \cite{han2024learning} introduced a Transformer-based framework that uses tactile and visual data for safe grasping, employing exploratory actions to gather tactile feedback and predict the grasp outcome for safer parameter selection.

%In addition to vision and language data, audio plays an important role in everyday life. We use sound to identify scenarios and locate sound-emitting objects. While existing research mainly focuses on analyzing vision-audio or audio-language data, few studies consider sound in embodied tasks. In recent research, sound is typically paired with visual data for navigation. SoundSpaces \cite{chen2020soundspaces} defined audio-visual navigation (AVN) tasks and created a dataset combining vision and audio data for two types of navigation: (1) AudioGoal, where the target is indicated by the sound it emits (like a phone ringing), and (2) AudioPointGoal, where the agent receives additional directional guidance toward the goal location at the start (using GPS and goal offset). In these tasks, robots navigate unknown environments by following audio cues to autonomously locate sound-emitting objects. However, AudioGoal tasks face a limitation—they require continuous sound emission, while some sounds are brief, like a doorbell. To address this, researchers \cite{chen2021semantic} built a goal descriptor that connects audio with semantic meanings and spatial properties, allowing robots to continue navigating even after the sound stops. While initial tests used clean, distractor-free audio environments, researchers \cite{yu2022sound,younes2023catch} later designed complex audio scenarios with challenging conditions like multiple sound sources, augmented signals, and distracting sounds at the goal location. To track moving sound sources \cite{younes2023catch}, they developed a framework to better extract spatial information from both audio and visual modalities. 

%In addition to navigation tasks, researchers have found that audio plays an important role in manipulation tasks by providing rich contact information. According to research on the roles of vision, audio, and touch, vision displays the global status of the robot but can often suffer from occlusion, audio provides immediate feedback at key moments that may be invisible, and touch offers precise local geometry for decision making \cite{li2023see}.
% \cite{thankaraj2023sounds} leverages self-supervised learning to build the connection between action and sound. Given a sound, the robot can generate simple actions that produce that sound. Initial experiments conducted in controlled environments without distracting noise demonstrate the potential of this approach. To extend it to real-world environments, Maniwav \cite{liu2024maniwav} augments collected audio data by adding background and robot noise, ensuring the policy focuses on task-relevant audio as feedback for actions. The results show that incorporating audio significantly improves task success rates compared to vision-only methods.
% Current research on audio data fusion in embodied tasks focuses primarily on navigation. Few studies explore audio data in high-level task planning. For instance, in cooking tasks, a robot could turn off the stove when it detects boiling water sounds, or respond when a blender signals its completion with a beep. By incorporating more sophisticated audio processing in task planning, robots could verify status changes more efficiently.

%\subsection{vision + touch}

%In manipulation tasks, visual information is typically the primary observation used to adjust the robot’s motion and complete tasks. However, during grasping actions, vision sensors often suffer from occlusion and cannot measure contact force, which provides crucial information for action success. Several studies have explored combining both sources, expecting tactile data to compensate for the limitations of visual observation. Researchers \cite{dikhale2022visuotactile} proposed a network architecture based on pixel-wise dense fusion to integrate vision and tactile data to estimate the 6D pose of an object for in-hand object manipulation tasks. For object grasping tasks, \cite{calandra2018more} investigated a learned regrasp policy to iteratively adjust the grasp based on visual and tactile data after the initial grasp. The results demonstrate notable improvements in grasping performance when incorporating touch information. To extend this approach to more complex objects, such as deformable items, \cite{han2024learning} proposed a transformer-based grasping framework that leverages both tactile and visual information for safe grasping. This framework employs two exploratory actions to collect tactile information, then predicts the grasping outcome. This prediction facilitates the selection of safe grasping parameters.

