\begin{table*}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|c|ccc|ccc}
\toprule
\textbf{Method} & \textbf{DataSet} & \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Weighted F1} \\
\midrule
\multirow{2}{*}{FLARE w/ GPT-4o} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.618 & 0.955 & 0.750 & \multirow{2}{*}{0.816} & \multirow{2}{*}{0.802} & \multirow{2}{*}{0.824} \\
&           & Evacuate & 0.976 & 0.759 & 0.823 &                     &                     &                     \\ 
\midrule
\multirow{2}{*}{FLARE w/ GPT-o3-mini} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.594 & 0.864 & 0.704 & \multirow{2}{*}{0.790} & \multirow{2}{*}{0.770} & \multirow{2}{*}{0.798} \\
&           & Evacuate & 0.759 & 0.837 & 0.770 &                     &                     &                     \\ 
\midrule
\multirow{2}{*}{FLARE w/ Claude-3.5}
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.850 & 0.750 & 0.810 & \multirow{2}{*}{\textbf{0.895}} & \multirow{2}{*}{\textbf{0.868}} & \multirow{2}{*}{\textbf{0.893}} \\
&           & Evacuate & 0.911 & 0.944 & 0.927 &                     &                     &                     \\ 
\midrule
\midrule
\multirow{2}{*}{Logistic Regression} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.560 & 0.540 & 0.550 & \multirow{2}{*}{0.697} & \multirow{2}{*}{0.640} & \multirow{2}{*}{0.679} \\
&           & Evacuate & 0.770 & 0.780 & 0.770 &                     &                     &                     \\ 
\midrule
\multirow{2}{*}{Random Forest} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.630 & 0.550 & 0.590 & \multirow{2}{*}{0.735} & \multirow{2}{*}{0.665} & \multirow{2}{*}{0.708} \\
&           & Evacuate & 0.780 & 0.830 & 0.860 &                     &                     &                     \\ 
\midrule
\multirow{2}{*}{GPT-4o Inference} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.240 & 0.310 & 0.270 & \multirow{2}{*}{0.738} & \multirow{2}{*}{0.557} & \multirow{2}{*}{0.752} \\
&           & Evacuate & 0.860 & 0.820 & 0.840 &                     &                     &                     \\ 
\midrule
\multirow{2}{*}{HCM} 
& \multirow{2}{*}{Combined Data} 
  & Stay     & 0.647 & 0.474 & 0.542 & \multirow{2}{*}{0.732} & \multirow{2}{*}{0.675} & \multirow{2}{*}{0.719} \\
&           & Evacuate & 0.761 & 0.868 & 0.809 &                     &                     &                     \\ 
\bottomrule
\end{tabular}}
\caption{\textbf{Comparison of \emph{FLARE} with baseline model on the combined dataset.} \emph{FLARE} was evaluated against four baseline methods using three different LLM backends on a combined dataset. The assessment employed metrics such as Accuracy, Macro F1, and Weighted F1, and also reported precision, recall, and F1 scores for the “Stay” and “Evacuate” classes. The results consistently demonstrate that FLARE outperforms the baseline models, regardless of the LLM employed.}
\label{tab:same_results}
\vspace{-1em}
\end{table*}


\begin{table*}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|c|ccc|ccc}
\toprule
\textbf{Method} & \textbf{Train/Test Set} & \textbf{Decision} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Weighted F1} \\
\midrule
\multirow{4}{*}{FLARE w/ Claud-3.5} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.433 & 0.867 & 0.578 & \multirow{2}{*}{\textbf{0.765}} & \multirow{2}{*}{\textbf{0.708}} & \multirow{2}{*}{\textbf{0.790}} \\
&           & Evacuate & 0.961 & 0.742 & 0.838 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
& Stay &  0.783 & 0.923 & 0.847 & \multirow{2}{*}{\textbf{0.870}} & \multirow{2}{*}{\textbf{0.867}} & \multirow{2}{*}{\textbf{0.871}} \\
&           & Evacuate & 0.944 & 0.836 & 0.887 &                     &                     &                     \\ 
\midrule
\multirow{4}{*}{FLARE w/ GPT-4o} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.387 & 0.800 & 0.522 & \multirow{2}{*}{0.728} & \multirow{2}{*}{0.668} & \multirow{2}{*}{0.757} \\
&           & Evacuate & 0.940 & 0.712 & 0.810 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
  & Stay     & 0.654 & 0.895 & 0.756 & \multirow{2}{*}{0.756} & \multirow{2}{*}{0.7556} & \multirow{2}{*}{0.756} \\
&           & Evacuate & 0.895 & 0.654 & 0.756 &                     &                     &                     \\ 
\midrule
\midrule
\multirow{4}{*}{Logistic Regression} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.160 & 0.190 & 0.170 & \multirow{2}{*}{0.650} & \multirow{2}{*}{0.480} & \multirow{2}{*}{0.660} \\
&           & Evacuate & 0.790 & 0.760 & 0.780 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
  & Stay     & 0.570 & 0.650 & 0.610 & \multirow{2}{*}{0.620} & \multirow{2}{*}{0.620} & \multirow{2}{*}{0.620} \\
&           & Evacuate & 0.660 & 0.590 & 0.630 &                     &                     &                     \\ 
\midrule
\multirow{4}{*}{Random Forest} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.000 & 0.000 & 0.000 & \multirow{2}{*}{0.800} & \multirow{2}{*}{0.450} & \multirow{2}{*}{0.720} \\
&           & Evacuate & 0.800 & 1.000 & 0.890 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
  & Stay     & 0.000 & 0.000 & 0.000 & \multirow{2}{*}{0.540} & \multirow{2}{*}{0.350} & \multirow{2}{*}{0.380} \\
&           & Evacuate & 0.540 & 1.000 & 0.700 &                     &                     &                     \\ 
\midrule
\multirow{4}{*}{GPT-4o Inference} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.273 & 0.231 & 0.250 & \multirow{2}{*}{0.733} & \multirow{2}{*}{0.544}& \multirow{2}{*}{0.725} \\
&           & Evacuate & 0.823 & 0.853 & 0.838 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
  & Stay     & 0.786 & 0.301 & 0.436 & \multirow{2}{*}{0.571} & \multirow{2}{*}{0.545} & \multirow{2}{*}{0.534} \\
&           & Evacuate & 0.514 & 0.900 & 0.655 &                     &                     &                     \\ 
\midrule
\multirow{4}{*}{HCM} 
& \multirow{2}{*}{Marshall / Kincade} 
  & Stay     & 0.348 & 0.736 & 0.473 & \multirow{2}{*}{0.678} & \multirow{2}{*}{0.596} & \multirow{2}{*}{0.710} \\
&           & Evacuate & 0.911 & 0.666 & 0.768 &                     &                     &                     \\ 
\cmidrule{2-9}
& \multirow{2}{*}{Kincade / Marshall} 
  & Stay     & 0.905 & 0.124 & 0.218 & \multirow{2}{*}{0.593} & \multirow{2}{*}{0.472} & \multirow{2}{*}{0.493} \\
&           & Evacuate & 0.905 & 0.989 & 0.725 &                     &                     &                     \\ 

\bottomrule
\end{tabular}}
\caption{\textbf{Comparison of \emph{FLARE} with baseline model on the cross-event dataset} derived from Kincade Fire and Marshall Fire. \emph{FLARE} was evaluated against four baseline methods using three different LLM backends on a combined dataset. The assessment employed metrics such as Accuracy, Macro F1, and Weighted F1, and also reported precision, recall, and F1 scores for the “Stay” and “Evacuate” classes. The results consistently demonstrate that FLARE has better generalizability than the baseline models}
\label{tab:cross_results}
\vspace{-1em}
\end{table*}
