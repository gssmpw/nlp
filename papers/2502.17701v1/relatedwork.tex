\section{Related Work}
\vspace{-0.2cm}

\subsection{Wildfire Evacuation Decision Prediction}
\vspace{-0.2cm}
Recent research has employed multiple methods to predict wildfire evacuation decisions.
\citet{mccaffrey2018should} employed a multinomial logistic model based on PADM, enhanced by a latent class approach, to predict diverse evacuation decisions in three U.S. fire-prone counties. 
\citet{forrister2024analyzing} applied logistic and linear regression to predict risk perception, evacuation decision, and delay time.
\citet{xu2023predicting} benchmarked seven machine learning approaches (e.g., Random Forest, Classification And Regression Trees (CART), Extreme Gradient Boosting) and identified CART as the top-performing model for predicting evacuation behavior from the 2019 Kincade Fire survey.
Meanwhile, \citet{lovreglio2020calibrating} introduced the Wildfire Decision Model (WDM) calibrated via Hybrid Choice Models (HCM), incorporating latent factors like risk perception and prior experience for more accurate evacuation decision predictions.
\citet{sun4953233investigating} further integrates risk perception and threat assessment as latent variables into an HCM framework, improving prediction accuracy.
Traditional statistical models do not account for the logical flow of decision-making. HCM, in contrast, considers this process.




\vspace{-0.2cm}
\subsection{LLMs for Human Decision and Behavior Prediction}
\vspace{-0.2cm}
Current work increasingly leverages LLMs to model and predict human decisions. 
BigToM~\cite{gandhi2024understanding} evaluates LLMs’ Theory-of-Mind (ToM) reasoning via causal templates and finds that GPT-4 partially matches human ToM performance while other models lag behind. 
Similarly, SUVA~\cite{leng2023llm} utilizes probabilistic modeling and behavioral economics games, revealing that larger LLMs display stronger prosocial and group-identity effects. 
DEBATunE~\cite{li-etal-2024-llms-speak} utilizes a multi-agent debate process for data synthesis on controversial debate topics and utilizes supervised fine-tuning to simulate behaviors on controversial topics. 
Extending these insights, ToM~\cite{amirizaniani2024llms} focuses on open-ended social reasoning from Reddit’s ChangeMyView posts, showing prompt tuning with human intentions and emotions boosts performance but remains below human-level comprehension. 
Although these models can infer mental states, T4D~\cite{zhou2023far} highlights the challenge of converting such inferences into strategic action, as even GPT-4 struggles without structured guidance. 
In parallel, LELMA~\cite{mensfelt2024logic} integrates symbolic AI to verify logical consistency in social simulations like the Prisoner’s Dilemma, demonstrating that self-refinement methods can improve the reliability of LLM-generated reasoning. 
Meanwhile, SimpleToM~\cite{gu2024simpletom} underscores that while LLMs can predict mental states and behavior, they often require deliberate prompting for accurate moral or behavioral judgments. \citet{kang-etal-2023-values} propose the Value Injection Method (VIM), embedding human core values to enhance opinion and choice prediction. 
However, \citet{kuribayashi-etal-2024-psychometric} caution that instruction tuning and prompting do not inherently offer better alignment with human cognition than direct probability outputs from base LLMs. %Turning to multi-agent configurations, Sreedhar et al. \cite{sreedhar2025simulating} show that having multiple LLMs negotiate in the Ultimatum Game leads to more human-like strategic reasoning than single-model setups. 
\citet{zhu2024language} reveal that arithmetic-trained LLMs can surpass classic decision-theoretic models when evaluating risky and time-delayed choices, demonstrating that specialized numerical training augments behavioral prediction. 
\citet{liu2024large} emphasizes a core limitation: LLMs systematically assume people behave more rationally than they do, underestimating well-documented human biases and highlighting a persistent gap between model predictions and real-world decision-making.

 

\vspace{-0.2cm}