\section{Counterfactual \abr{rl} against Deception}
\label{sec:overall_method}

\jmcomment{This section is way too insider-y. I think all that matters here is we want to know if a proposed message is deceptive. I think it's fine to just state that and move on, and leave the parse-and-find-propose-01 bit out or maybe in the low level details of the code. \wwcomment{removed those amr details part}}
We look for deception\jmcomment{terminology needs to be introduced now \wwcomment{I removed system2}} in the text messages between pairs of players.\jmcomment{unclear why text and private dialogues are highlighted here unless we're distinguishing them from smth else \wwcomment{fixed}}
Each player controls multiple units in this game, so we restrict a pool of messages where a player explicitly requests another player to issue a specific order (e.g. Austria asks Italy to support in Serbia, Figure~\ref{fig:overview}). \fgcomment{agree with Jono; not clear} With this, we parse messages in natural language to Abstract Meaning Representation~\citep[\abr{amr},][]{banarescu-etal-2013-abstract}. 
% We extract potential unit orders from~\abr{amr} and keep only those that a sender is proposing unit orders for recipient (i.e. contains a \amr{propose-01} token,~\abr{amr} to unit orders in Section 3, ~\citet{wongkamjan-etal-2024-victories}). 
% \sscomment{it's natural, it doesn't need explanation} 

For any message to a player, we want to raise awareness if the proposal is \textit{potentially deceptive}. We leverage a well-trained  value function, a part of \cicero~\citep{meta2022human}, to estimate how likely a proposal is deceptive. This section we discusses our method, \textbf{C}oun\textbf{T}erfactual \textbf{\abr{rl}} against \textbf{D}eception (CTRL-D), which has two main components: 1) Counterfactual RL and 2) formulations to capture potential deceptive proposals.
% \sscomment{-> `to detect if a proposal is deceptive'. `deceptive' word itself has likeliness I'd say.}
% JOY: I dont want to claim that our model detecting with 100% belief that it is deceptive. that's why we need those values to help with estimate how much/how likely it is


% counter factual state and action (cicero)
\subsection{Counterfactual \abr{rl}}
\label{sec:cfrl}
% \wwcomment{DONE: change notations!}
% p
Player \( i \) needs to pick an action \( a_i \) given a board state \( s \). However, moves in Diplomacy do not happen in isolation --- all actions of \emph{other} players \( a_{-i} \) happen simultaneously, so \cicero{} uses a function \( u_i(a_i, a_{-i},s) \) that represents estimated future rewards that player \( i \) will receive if actions \( a_i \) and \( a_{-i} \) are played in a state \( s \). Thus, a high value represents a ``\textit{better}'' move based on learned strategies.

% where player
% % \sscomment{`each player' -> `player'. Since you fixed player $i$ against other players} 
% \( i \) samples an action \( a_i \) from some policy \( \pi_i \) and receives a reward \( u_i(a_i, a_{-i}) \) (trained via self-play), where \( a_{-i} \) represents the joint actions of all players except player \( i \) and \( \pi_{-i} \) denotes a joint policy except player \( i \). \jmcomment{is `their' all players except i? it's a little unclear}
% \wwcomment{yes, I changed a bit to make it clear}
% Player \( i \) selects an action \( a_i \) that best responds to a predicted joint action of all player except \(i\) which samples from a joint \abr{rl} policy \( \pi_{-i} \), while anchoring to a dialogue-conditional behavior cloning policy \( \tau_i \) as follows,
% % \sscomment{`Player $i$ chooses their action $a_i$ so to maximize their utility against predicted joint \abr{rl} ...'} 
% \begin{equation}
% \arg\max_{a_i} u_i(a_i, a_{-i}) + \lambda \log \tau_i(a_i),
% \label{eq:BRQE}
% \end{equation} \jmcomment{$u_i$ is defined over an (action, set of action) tuple above but is defined over an (action, set of policy) tuple here, which is not compatible.}
% \wwcomment{I actually copied this from \cicero paper, so I thought it's ok. I can change to \(a_{-i}\) since we already define above.}
% where \( \lambda \) is the anchor strength, set to \( \lambda = 0.003 \). The anchor to the behavior cloning policy ensures that agents behave in a manner that remains ``\textit{close}'' to human behaviors.

% Specifically, a joint \abr{rl} policy \( \pi_{-i} \) is formed by merging independent policies \( \pi_j \), where \( j \neq i \) into a single policy via self-normalized importance sampling~\citep[Section \abr{e.2.4},][]{meta2022human}. Each independent policy \( \pi_j \) is constructed as a product of pairwise policies. The distributional piKL (
% % optimized by a dialogue-free \abr{rl} policy while maintaining proximity to the behavior cloning policy using a distributional \( \lambda \), 
% Equation 5,~\citet{bakhtin2023mastering}) \fgcomment{format} is computed for action \( a_i \) and \( a_j \) while actions \( a_{-ij} \) or the remaining five players are sampled from the dialogue-free \abr{rl} policy. One policy at a time, they compute \( \pi_j \) \jmcomment{It's a little weird to speak of `predicting' a policy. \wwcomment{compute may sound better?}} which represents a best guess for the range of actions the other player \( j \) might take. This approach accounts for a private dialogue between player \( i \) and player \( j \), as well as the correlation between their actions. More details in Section \abr{e.2}~\citep{meta2022human}.
% % This is to account for correlation between the actions of the two players (e.g. coordinate support moves units from Austria support Turkey's unit in SEV to RUM) and a secret dialogue that other players than these two should not know anything about.

% During a one-step rollout to determine the optimal \( \pi_j \) (256 iterations for each player \( j \)), they \fgcomment{who?} apply the \abr{rl} value model to the resulting state after executing a joint action \( \mathbf{a} = \{a_i, a_j, a_{-ij}\} \). 
While a review of \cicero is outside the scope of this paper, its value function allows our work to compute counterfactual one-step actions to estimate potential deceptive proposals from another player \( j \), where each proposal is about action \( a_i \) and \( a_j \). Equipped with text-to-proposals and the \abr{rl} value function, we are ready to detect deception. 

\subsection{Deceptive Proposals}
\label{sec:dec_prop}

To estimate how likely a proposal is deceptive, we introduce three \textit{deceptive signs} that account for different aspects of deception.
First, we can measure whether a victim would get a higher reward if the proposal was not a deception (i.e., is the fake proposal from the deceiver appealing?). Second, we can measure whether a victim would get a lower reward if they believe the deception.
Third, we can measure whether a deceiver would increase their future reward by deceiving the victim. These three measures called: Bait, Switch and Edge\footnote{These terms come from popular culture terms around scams: a deceiver offers ``\textit{bait}'' to attract the victim who suffers from the ``\textit{switch}'', leading the deceiver to profit, their ``\textit{edge}'' in the scam}.
In this section, we highlight deceptive values through three hypotheses.

We define a proposal \( p_{j\to i} \) when player \( j \) proposes actions to player \( i \). A proposal \( p_{j\to i} \) consists of an action \( \hat{a}_i \) that player \( j \) wants player \( i \) to play and an action \( \hat{a}_j \) that player \( j \) promises to make. In Diplomacy, an action is a tuple of unit orders, e.g. \jmcomment{Do you really mean `moves' and not more general orders? Are supports and holds involved? All the examples are actual moves but i'm unclear if that restriction is intended.}
\wwcomment{to make it more clear, I will be using unit orders.}
\begin{itemize}
    \item \colorbox{yellow}{an army in Berlin moves to Kiel},
    \item \colorbox{pink}{an army in Munich moves to Ruhr} and
    \item \colorbox{lime}{a fleet in Kiel moves to Holland}
\end{itemize} 
where these can represent in a tuple as \amr{(`\colorbox{yellow}{A BER - KIE}',`\colorbox{pink}{A MUN - RUH}',
`\colorbox{lime}{F KIE - HOL}')}.
% \sscomment{don't need to jump into that rabbit whole (explaining all moves support, convey), keeping \( \hat{a}_{i,j} \) in abstract form: `order for unit jth of power i' is good to go}
% JOY: I think we need to explain to those who dont familiar with Diplomacy and why it is subaction for unit
Therefore, player \( j \) can propose an action to player \( i \) with multiple unit orders \( \hat{a}_i = (\hat{a}_{i,1} ,\hat{a}_{i,2}, \dots,  \hat{a}_{i,n}) \) where \( n \) is a number of player \( i \)'s units.   

\fgcomment{for the paragraphs below, should we use player/deceiver+victim interchangeably?}
We estimate how likely a proposal is deceptive by following three hypotheses when it is `\textit{too good to be true}''. \textbf{Bait},
a victim \textit{perceives a greater reward} if they \textit{alter} a decision to \textit{follow} the deceiver's proposal and the deceiver does not actually deceive, but rather follows the plan. \fgcomment{not clear and grammar issue}
Assume player \( i \) has a plan \( a_i\), and player \( j \) proposes \( \hat{a}_i \) and \( \hat{a}_j \). From the perspective of player \( i \), they decide to play \( \hat{a}_i \) because they perceive that the estimated future rewards will increase by: 

\begin{equation}
    \begin{aligned}
        U_1 = u_i(\hat{a}_i, \hat{a}_j) - u_i(a_i, \hat{a}_j).
    \end{aligned}
    \label{eq:deception_hypo3}
\end{equation}

\textbf{Switch}, a victim will \textit{receive a lower} reward if they \textit{follow} the deceiver's request and if the deceiver betrays the victim.
% \sscomment{-> `player \( j \) betrays' only \( j \) 's move is changing}
 Player \( j \) proposes actions \( \hat{a}_i \) and \( \hat{a}_j \) to player \( i \) where player \( j \) has alternative plan \( a_j \) to instantly stab or take advantage of player \( i \). The estimated future rewards of player \( i \) will decrease if player \( j \) betrays player \( i \). We leverage \cicero's \abr{rl} value function \( u_i \) (Section~\ref{sec:cfrl}) to formulate the first hypothesis:

\begin{equation}
    \begin{aligned}
        U_2 = u_i(\hat{a}_i, \hat{a}_j) - u_i(\hat{a}_i, a_j)
    \end{aligned}
    \label{eq:deception_hypo1}
\end{equation}
where \( a_j \) is an alternative action that player \(j\) might play instead of following the proposed move \( a_j \neq \hat{a}_j \). 

% We also omit actions of the other five players \( a_{-ij} \) which sampled from \cicero dialogue-free \abr{rl} policy \fgcomment{necessary? move to footnote/appendix if need space}. 

\textbf{Edge}, a deceiver will \textit{receive a better reward} when a victim \textit{follows} their proposal. Given the deceiver \( j \)'s plan \( a_j \)  and the victim's plan \( a_i\), if player \( j \) proposes a suboptimal \( \hat{a}_i \) to player \( i \) and player \( i \) falls for it. The estimated future rewards for player \( j \) can increase:  
\begin{equation}
    \begin{aligned}
        U_3 = u_j(\hat{a}_i, a_j) - u_j(a_i, a_j).
    \end{aligned}
    \label{eq:deception_hypo2}
\end{equation}

In short, the three hypotheses for deceptive proposals assume the victim loses, the deceiver gains and victim follows the proposal~(Counterfactual RL, Figure \ref{fig:overview}). For a victim or player \(i\)'s plan \( a_i\) in Equation \ref{eq:deception_hypo3} and Equation \ref{eq:deception_hypo2}, we define the plan loosely as any action closest to the optimal action from player \(i\)'s perspective, thus sampling an action \( a_i \sim \pi_i\) where \(\pi_i\) is \cicero{}'s policy.
% that is the best-response to a joint \abr{rl} policy \( \pi_{-i}\) with an anchor to a human behavior policy \( \tau_{i}\). 

In the final step, we train a classifier using \textit{deceptive values} \( U_1\), \( U_2\) and \( U_3\) from Equation \ref{eq:deception_hypo3}, \ref{eq:deception_hypo1} and \ref{eq:deception_hypo2}. We have two main models:\\
\noindent \textbf{1)} A \abr{bert-based-uncased} that takes a message and outputs text embeddings.\\
\noindent \textbf{2)} A Neural Network with three linear layers, ReLU  and Sigmoid activations, that takes in text embeddings concatenated with \( U_1\), \( U_2\) and \( U_3\) .

We train only ten epochs with a small training data sampled from \citet{peskov2020takes}. In the next section, we discuss datasets that we use to train the classifier and to test our approach against an LLM baseline.
% \sscomment{section missing here?}
% JOY: since I am introducing for next section in overall, might not need to ref
