\section{Results and Analysis}
\label{sec:results}
\input{tables/dataset1_results}
This section compares the approach (Section \ref{sec:overall_method}) ---a classifier trained by using \textit{deceptive values} from counterfactual RL and text embeddings from BERT to train a three-layer linear neural network---to an LLM baseline (Section \ref{sec:llama}). We evaluate these using two Diplomacy datasets; the first is \citet{peskov2020takes} with human lie annotations\fgcomment{what is the other one?}. \yzcomment{we could go directly to results and analysis instead of explaining dataset once more since we have already used a large space in section 4.1 and 4.2 to introduce the two datasets.}
In Section \ref{sec:result_lies}, we prove that our approach can outperform the LLM baseline and other baselines in predicting human lies. In Section \ref{sec:result_friction}, we show that our approach has a high precision in predicting deception, potentially creating friction at just the point when a human player is making a critical decision. 

\subsection{Detecting Human Lies}
\label{sec:result_lies}
With human lie annotations, we compare \textbf{CTRL-D} prediction to these baselines: 1) \textbf{LLM baseline using Direct Judgment}, 2) \textbf{LLM baseline using Alignment Judgment} and 3) \textbf{Context LSTM with Power} by \citet{peskov2020takes} (Table \ref{tab:1K_results}). This section answers these questions:
\begin{enumerate}
    \item Is our value-based RL with classifier approach (CTRL-D) able to predict human lies? 
    \item How good it is when compared to baselines?
    \item What lies does it miss (False negatives) and what truth does it misconceive as lies (False positives)?
\end{enumerate}

\paragraph{Our detection is most effective at predicting human lies.} It predicts with high precision \(0.950\), compared to both LLM-based methods and to Peskov's LSTM (\(0.147\), \(0.095\) and \(0.104\)). In other words, when CTRL-D predicts a message as a lie, it is\fgcomment{?} 95\% likely that the message is an \textbf{actual} human lie. However, our detection is not perfect. Its recall is \(0.238\), meaning that it can only catch \textbf{one} out of every \textbf{four} lies. This high precision but low recall is still useful in deployment, since human players at least can trust CTRL-D when it flags lies.

% Since our method highly relies on the proposed RL value functions, they are limited to deception that appears in negotiations with explicit actions.\yzcomment{a bit confused here, are you referring to our method or RL value functions? I think our focus should be our method, then maybe:
% Because our approach heavily relies on the proposed RL value functions, it is limited to detecting deception that only involves negotiations with explicit actions.}
%Joy: add new point and remove old one (old is repeated in next 2 paragraph)

\paragraph{The LLM baseline is good at recall but bad at precision.} Comparing the recall of CTRL-D with our LLM baseline with Direct Judgment, we find the latter has good recall but low precision. The LLM predicts 45.5\% of all messages as lies, which is too sensitive to be useful, when considering the number of misaligned \cicero predicted orders and proposed orders seen in a message. Current LLM-based approaches to providing guidance do not appear capable of recognizing deception, and the resulting high false positive rate makes this strategy not compatible for real-time human games. 

\paragraph{CTRL-D misses deception if explicit actions are unclear in negotiations.} To characterize the lies CTRL-D misses detecting (False negatives), we observe that 41 of 61 non-detected lies have no logical forms for negotiations. Further, all of these 41 fit in categories other than \textbf{Deceptive Moves} (Table \ref{tab:denis_lies}). We examine the remaining 20 and correct errors in logical forms by hand, to see if doing so improves prediction. With logical forms from humans, the recall of CTRL-D (Table \ref{tab:1K_results}) improves slightly from \(0.238\) to \(0.300\), showing that CTRL-D depends on proper logical parsing of natural language text.
\jbgcomment{unclear}
\input{tables/denis_fp_examples}

\paragraph{CTRL-D mispredicts once, while LLM baselines mispredict frequently.} We further investigate false positives of CTRL-D and LLM baselines. Since our approach is very precise, it predicts only one true message as a lie (Table \ref{tab:fp_cfrl}). On the other hand, the LLM baseline with Direct Judgment\footnote{We focus on Direct Judgment and omit the Alignment Judgment baseline variant since both LLM approaches are similar and have similar results.} mispredicts \(412\) messages as lies (Example in Table \ref{tab:fp_llama3}). We look into these messages and see patterns that LLM baseline is heavily constrained on \cicero{}'s predicted orders when it considers proposed orders in messages. This could be improved if LLM baseline can recognize the nature of Diplomacy, namely that there are many possibilities for players' orders, which are not necessarily deceptive. 

In sum, CTRL-D captures human lies at best among all methods, including LLM-based methods. Though LLM baseline is better with semantics, it still lacks skills to interpret Diplomacy information in a way that would enable deception detection. With a strong agent, \cicero, predicting human lies using its RL value function makes detection possible. To further validate the quality of our deception detection, we evaluate both CTRL-D and LLM baseline on a larger data set that contains interactions between humans and \cicero.

\subsection{Awareness against Deception}
\label{sec:result_friction}
\input{tables/dataset2_results}
In this section, we evaluate CTRL-D and LLM baselines using the Meta data set. Since this dataset lacks human deception annotations, we first let both models predict whether each message is deceptive, then verify the labeled predictions through human judgment. Human reviewers are provided with historical messages and final orders that sender and recipient submit through games. This information is used to decide whether a sender deceives a recipient by comparing between 1) the sender's commitment in a proposal and 2) the sender's final orders. Although this verification is limited to deception that appears within explicit orders of the sender, this could serve as more evidence to verify performance of our approach and the LLM baseline. 

\paragraph{CTRL-D predicts with higher precision than LLM baseline, which overpredicts deception.} As shown in Table \ref{tab:meta_results}, our findings are consistent with those on the previous dataset. LLM baseline with Direct Judgment predict 41.3\% of all samples as deceptive, which is greatly higher than 5\% actual lie rate from humans \citep{peskov2020takes}. This high rate makes human verification impractical. For LLM baseline with Alignment Judgment, its precision is \(0.282\) (only 1 in 4 flagged messages is a true lie). In contrast, CTRL-D, though not matching its previous high performance, is still the best at predicting deception. 

\paragraph{Errors in CTRL-D and LLM baselines showing thier weakness.} 

We cross-validated our CTRL-D with LLM baseline under Alignment Judgment to evaluate their ability to detect deceptive proposals. While both methods can correctly identify some lies, each may fail under different circumstances. We present several examples here:

\begin{itemize}
    \item Both models label it deceptive, and indeed it is a lie (Table \ref{tab:tp_both}).
    \item LLM baseline overlooks Russia's convoy promise, but CTRL-D detects the unfair exchange (Table \ref{tab:tp_cfrl}).
    \item   CTRL-D misses one lie, while LLM baseline correctly spots it (Table \ref{tab:tp_llama3}). 
\end{itemize}

\begin{comment} UNCOMMENT TO REVERT

\end{comment}

% This is due to the process of text-to-proposals does not extract proposal correctly, unable to retrieve the deceptive values. 
% Interestingly, we cross validate between our valued-based classifier and Llama3 with Alignment Judgment. One example that they both predict as lies and it is actual lie (Table \ref{tab:tp_both}). We observe Llama3 does not detect a deception when Russia promises to convoy its army to St. Petersberg in exchange to Italy moves out of Serbia (Table \ref{tab:tp_cfrl}). Llama3 might miss this case since Russia getting a control of St. Petersberg is an expected move and not quite deceptive, however, our value-based detect correctly since the exchange is unfair towards Italy and likely to get stabbed. There is also one case that our value-based does not detect as lie when it actually is while Llama3 can also detect that (Table \ref{tab:tp_llama3}). This is due to the process of text-to-proposals does not extract proposal correctly, unable to retrieve the deceptive values. 

Human verification supports CTRL-D as the stronger method; however, the LLM baseline can still catch some lies. We hope to further test these approaches with human players, thus introducing additional \textit{friction} in real negotiation settings.