\section{Friction in \abr{ai} systems}
% \sscomment{haven't used the word \textit{friction} entirely though, also maybe `Friction is necessary for AI systems' is a better one}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/intro_example.pdf}
    \caption{Detecting deception is crucial in mixed cooperative-competitive environments. \textbf{(Left)} \textcolor{fra}{France} believed the lie that \textcolor{ita}{Italy} will move their army in Tyrolia to Munich, losing Burgundy and subsequently Marseilles to \textcolor{ger}{Germany}. \textbf{(Right)} If \textcolor{fra}{France} had detected the deception, they could have successfully defended Burgundy and avoided disbanding one army.}
    \label{fig:intro_example}
\end{figure}
%motivation
% The increasing sophistication of \abr{ai} models, particularly large language models (LLMs), raises significant safety concerns regarding their potential to engage in deceptive behaviors. Deception in \abr{ai}-generated text poses risks in various domains, including misinformation propagation, social manipulation, and adversarial exploits in multi-agent interactions. If left unaddressed, \abr{ai} systems optimized for persuasion or goal-oriented dialogue may develop strategies that prioritize manipulation over truthfulness. This risk is especially pronounced in reinforcement learning with human feedback~\citep[\abr{rlhf},][]{christiano2017deep}, where \abr{ai} agents are rewarded for producing desirable outputs rather than strictly adhering to factual accuracy (reward hacking citations). As \abr{ai} systems become more autonomous and embedded in decision-making processes, understanding and mitigating deceptive tendencies is crucial to ensuring trustworthiness and alignment with human values. 

% \fgcomment{I think this paragraph is talking about the same thing (AI deception) as the next one. We should either separate to deception in NL/in AI, or how it happens (reward hacking and optimization)/why necessary to detect and mitigate}
% \wwcomment{agree, I made them into how it happens (reward hacking and optimization)/why necessary to detect and mitigate, thanks!}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \caption{An overview of our approach to detect deceptive proposals, requiring a recipient (Human Player) to follow a proposed action. \textbf{(Left)} A state of this Diplomacy game is (1) \textcolor{aus}{Austria} and \textcolor{ita}{Italy} have an alliance (2) while \textcolor{tur}{Turkey} and \textcolor{ita}{Italy} have been clashing for several turns. \textcolor{aus}{Austria} realizes that they are in a weak spot and need a quick escape, so they reach out to \textcolor{ita}{Italy}. It is a deceptive proposal so that \textcolor{aus}{Austria} can get to Trieste. \textbf{(Bottom Middle)} A human player can be biased towards their own ally (\textcolor{aus}{Austria}) and use their fast-thinking system to instinctively help. \textbf{(Top Middle)}  For an alternative perspective, our approach converts natural language to proposals using \abr{amr}. \textbf{(Right)} Then, we leverage the RL value function from \cicero to estimate three aspects of deception---Bait, Switch and Edge---from counterfactual actions of \textcolor{aus}{Austria} and \textcolor{ita}{Italy}. 
    Passing the dialogue alongside these values to a classifier decides whether \textcolor{aus}{Austria}'s proposal is \textbf{deceptive}.}
    \label{fig:overview}
\end{figure*}


Deception in natural language is a fundamental aspect of human communication, often employed as a strategic tool to mislead others through misrepresentation, omission, exaggeration, or counterfactual reasoning~\citep{bok2011lying}. 
From casual social interactions to high-stakes negotiations, deception influences trust, decision-making, and cooperation, making it a subject of extensive study in psychology, linguistics, and philosophy, manifesting real-world challenges such as fake news on
social media ~\citep{bade-etal-2024-social}, misinformation ~\citep{panda-levitan-2022-improving} and adversarial communication in strategic
games ~\citep{bernard-mickus-2023-many}. As artificial intelligence systems increasingly engage in human-like communication, they not only inherit but also amplify deceptive strategies, sometimes unintentionally.
%
In \abr{ai}-generated text, deception can emerge as a byproduct of
optimization objectives, particularly in \abr{rlhf} scenarios where
agents maximize utility in multi-agent settings, sometimes at the
expense of honesty~\cite{wen2025language}.
%
This phenomenon has garnered significant attention across various
domains, as \abr{ai} deception is not confined to theoretical
constructs but manifests in real-world challenges, e.g. hallucination in reasoning tasks ~\citep{grover-etal-2024-navigating}.

% \jbgcomment{Don't focus on AI deception, start with human deception and then talk about how AI can amplify it.\wwcomment{I added some parts and rearranged.}}

Prior research underscores that \abr{ai}-generated deceptive
communication can be difficult to detect and may lead to unintended
consequences when deployed in practical applications~\citep{park2024ai,sarkadi2024deceptive}.
%
Deceptive AI-generated text can erode trust in digital communication,
amplify misinformation, and facilitate large-scale manipulation in
political, financial, and social domains~\citep{solaiman2019release,
  weidinger2022taxonomy}.
%
Furthermore, the scalability of AI models allows deceptive content to
be produced and disseminated at unprecedented rates, making manual
detection impractical.
% \sscomment{instead, you could say `make detecting
%   deception more nuance and call the need for an automatic tool that
%   can aid humans with it' we can't claim for removing the role of the
%   human in the loop at least as a verifier}.
% \wwcomment{I dont think I claims as not human is needed but rather human manually verification way harder when AI contents are generated super fast/many}
%
To address these risks, robust mitigation strategies are necessary,
including adversarial training~\citep{perez2022red}, explain-ability
techniques to enhance AI transparency~\citep{danilevsky2020explainability}, and real-time detection methods
leveraging linguistic and behavioral cues~\citep{vosoughi2018spread}.

% \jbgcomment{I think just one paragraph is sufficient for setup, get
%   into the paper sooner.  I think until we have user experiments, it's
%   hard to use System 1/2 because we don't actually show that switching
%   systems helps. \wwcomment{removed system1/2 Kahneman}}

% The increasing sophistication of \abr{ai} models and the risks of deceptive behaviors demand innovative safeguards. Drawing on \textit{the Dual-Process Theory} \citep{kahneman2011thinking}, we can conceptualize the generation of \abr{ai} text as analogous to \textbf{System 1}: a fast, intuitive process prone to prioritize goal-oriented outputs over factual accuracy. This risk is particularly acute when \abr{ai} systems exploit maximize utility (e.g. reward-hacking in language models \cite{amodei2016concreteproblemsaisafety}). A promising mitigation strategy responding to \textbf{System 1} involves counterfactual reinforcement learning (cite) as a \textbf{System 2}-like mechanism. This reflective process would serve to \textit{scrutinize} the outputs generated by the \textit{fast}, heuristic \textbf{System 1}-like component. By evaluating alternative scenarios, counterfactual \abr{rl} can catch deceptive tendencies before they deploy.

% \jbgcomment{The focus on AMR is too focused on the specific method and doesn't say anything about \emph{why} we need to do this.

%   I'd structure this more as:
%   \begin{itemize}
%   \item An effective negotiation is one where both parties benefit
%   \item The bigger problem is that sometimes people are taken in by an offer that ``sounds too good to be true''
%   \item We need to ground the text of the negotiations, and we use AMR to do that
%   \item We then use the value functions of trained agents to detect offers that could be part of a deception
%     \item We compare and combine this with techniques X, Y, Z
%   \end{itemize}
%   \wwcomment{that sounds a lot better. I fixed it accordingly. Right now it is a bit short so I will add more details}

% }

 % \fgcomment{move to related work?} 
 % \wwcomment{done! moved prior works ai deception to related works}
We test our detection strategies within the environment of  \textit{Diplomacy}, a game rich in negotiation, cooperation, and betrayal expressed through natural language. The most intriguing moments of the game arise when two players negotiate to cooperate in pursuit of their respective goals. While such agreements usually yield mutual benefits, this is not always the case---some negotiated arrangements are the result of deception, omission, or straight-up lies on the part of one player. Skilled players combat such behavior by developing the ability to recognize when an offer \textit{sounds too good to be true}. Our work explores this area to raise awareness among human players when they encounter deception embedded in negotiations. 

We use the \emph{value function} of \cicero~\citep{bakhtin2023mastering}, an agent trained to play Diplomacy at a human level, to detect whether a proposal is ``\textit{too good to be true}''. We estimate other players' likely strategies and query Llama3~\cite{llama3modelcard}, an \abr{llm} good at general purpose semantic understanding, to determine if a message is contrary to \cicero{}'s expected strategy. However, Llama3 cannot precisely predict deception since it sees almost half of data as deceptive. Consequently, we decide to take a more explicit approach to modeling deception. Our contributions are as follows:\\
\noindent \textbf{1)} With Theory-of-Mind-influenced deception, we identify negotiations in natural language via formal logical modeling and detect potential deceptive offers in negotiations using \cicero \abr{rl} value function to generate counterfactual explanations.\\
%Also, identifying negotiations in natural language dialogue via formal logical modeling is necessary in this step.\\
\noindent \textbf{2)} We train a BERT-based~\cite{devlin-etal-2019-bert} classifier to predict deception using \abr{rl} values and message embeddings.\\
\noindent \textbf{3)} We show that our classifier is more accurate than a fine-tuned Llama3 in human lie prediction and detecting partially-deceptive negotiations. 
