%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{multirow}
\usepackage{diagbox}   
\usepackage{booktabs}   
\usepackage{array} 


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{booktabs}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Geometric Framework for 3D Cell Segmentation Correction}

\begin{document}

\twocolumn[
\icmltitle{Geometric Framework for 3D Cell Segmentation Correction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{\dag}
\begin{icmlauthorlist}
\icmlauthor{Peter Chen}{cms}
\icmlauthor{Bryan Chang}{ucsf,equal}
\icmlauthor{Olivia Annette Creasey}{ucsfengg}
\icmlauthor{Julie Beth Sneddon}{ucsf}
\icmlauthor{Yining Liu}{ccs,iicd}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cms}{Department of Mathematics, Columbia University, New York, NY, USA}
\icmlaffiliation{ccs}{Department of Computer Science, Columbia University, New York, NY, USA}
\icmlaffiliation{iicd}{Irving Institute for Cancer Dynamics, Columbia University, New York, NY, USA}
\icmlaffiliation{ucsf}{Department of Cell \& Tissue Biology, University of California San Francisco, San Francisco, CA, USA}
\icmlaffiliation{ucsfengg}{Department of Bioengineering, University of California San Francisco, San Francisco, CA, USA}

\icmlcorrespondingauthor{Yining Liu}{yl4536@columbia.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
3D cellular image segmentation methods are commonly divided into non-2D-based and 2D-based approaches, the latter reconstructing 3D shapes from the segmentation results of 2D layers. However, errors in 2D results often propagate, leading to oversegmentations in the final 3D results. To tackle this issue, we introduce an interpretable geometric framework that addresses the oversegmentations by correcting the 2D segmentation results based on geometric information from adjacent layers. Leveraging both geometric (layer-to-layer, 2D) and topological (3D shape) features, we use binary classification to determine whether neighboring cells should be stitched. We develop a pre-trained classifier on public plant cell datasets and validate its performance on animal cell datasets, confirming its effectiveness in correcting oversegmentations under the transfer learning setting. Furthermore, we demonstrate that our framework can be extended to correcting oversegmentation on non-2D-based methods. A clear pipeline is provided for end-users to build the pre-trained model to any labeled dataset.
\end{abstract}
\section{Introduction}
3D cell segmentation is a critical technique that reconstructs complete cellular structures from sequential 2D microscopy images. This process is fundamental for researchers conducting detailed analyses of cellular characteristics, including morphology, cell type classification, and volumetric properties \cite{williams2022,boutros2015}. Current approaches to 3D cell segmentation can be broadly categorized into two methodological frameworks: those that are based on the reconstruction of 3D shapes from 2D segmentations (2D-based) and those that directly predict 3D shapes from raw 3D microscopy images (3D-based).

Across both 2D-based and direct reconstruction methods, oversegmentation remains a persistent challenge, where individual cell bodies are erroneously divided into multiple parts. This limitation affects state-of-the-art methods including \textit{CellPose3D} \cite{cellpose1.0}, \textit{CellStitch} \cite{Liu2023Cellstitch}, and \textit{PlantSeg} \cite{boutros2015}. Discussion of these methods will be provided in \hyperref[prw]{\S2}.

\begin{figure}[h!]
  \label{f2}
    \centering
    \includegraphics[width=0.85\columnwidth]{Figures/f2.pdf} 

    \caption{Examples of oversegmentation errors from 2D-based model (CellStitch) and 3D-based model (PlantSeg).}
\end{figure}

For 2D-based methods, deep learning architectures such as U-Net \cite{ronneberger2015unet} have facilitated the development of multiple expert-level 2D segmentation methods, including \textit{CellPose} \cite{cellpose1.0,cellpose2.0,cellpose3.0}, \textit{Mesmer} \cite{greenwald2022wholecell_segmentation}, and \textit{StarDist} \cite{schmidt2018starconvex}. However, mis-segmentations, such as missing cell masks and oversegmented 2D regions, persist in 2D segmentation results, ultimately impacting the quality of downstream 3D segmentation outcomes. These challenges arise from the challenge of interpreting deep learning models and their reliance on large amounts of training data, which restricts their ability to generalize effectively to rare cell types.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.82\textwidth]{Figures/f1.pdf} 
    \vspace{-1em}
    \caption{Main pipeline for the oversegmentation correction. The framework extracts layer-to-layer EMDs (\hyperref[a2]{Algorithm 2}) and 3D shape information (\hyperref[a3]{Algorithm 3}), combining these features for binary classification to distinguish oversegmentations from natural gaps.}
    \label{f1}
\end{figure*}

In this work, we propose an interpretable framework to correct 3D cell segmentation errors by leveraging geometric and topological information. Using earthmover's distance, we capture the layer-to-layer geometric changes of cell masks and integrate topological information from labeled 3D segmentations to determine whether two cells should merge or remain separate. We solved this problem by training a binary classifier using the extracted geometric information from cells. Based on the classifier’s outcome, we employ a layer-to-layer interpolation to first correct 2D segmentation errors and subsequently reconstruct accurate 3D cell bodies. For 3D-based models, oversegmented surfaces are often tilted or irregular rather than aligned with principal axes. We extend our approach by constructing rotated 2D segmentation layers based on the local curvature between two oversegmented cells, and then apply our pre-trained classifier to tackle these irregularities. 

Our contributions are summarized below:
\vspace{-1em}
\begin{itemize}
\item We introduce a novel approach utilizing earthmover's distance to quantify geometric dissimilarity between 2D cell masks;
   \item We propose an interpretable geometric framework that enables oversegmentation correction for both 2D- and 3D-based cell segmentation models. The framework's pre-trained models can be generalized across any labeled dataset using our pipeline;
\item We demonstrate the robustness of our framework through its compatibility with different segmentation methods and configurable tolerance parameters.
\end{itemize}



\section{Preliminaries \& Related Works} \label{prw}
The limited availability of large and diverse 3D training datasets poses significant challenges for purely deep-learning-based approaches \cite{DL1,DL2,DL3}, which often fall short in achieving high accuracy and generalizability. These limitations are especially evident in handling complex or dense tissues with heterogeneous and irregularly shaped cells, such as those found in cancerous tissues. This pushes the development of emerging 3D segmentation methods that do not rely solely on deep learning. These methods offer improved interpretability and greater generalizability across diverse datasets. Notable examples include \textit{CellStitch} (2D-based) \cite{Liu2023Cellstitch}, \textit{PlantSeg} (direct 3D reconstruction) \cite{plantseg}, and \textit{CellPose3D} (2.5D-based, mixing of 2D and 3D approaches) \cite{cellpose1.0}. We provide a discussion below to these groundworks.

\subsection{Cell Segmentation: 2D-based Models}
\textbf{CellPose2D \& CellPose3D.} CellPose comprises two components: CellPose2D and CellPose3D. CellPose2D is a deep-learning-based method that segments cells in 2D cellular images, providing 2D segmentation results for individual layers. Building on this, CellPose3D integrates the strengths of both 2D and 3D approaches by leveraging contextual layer awareness to develop a transferable model. It trains models to predict flow vectors for each pixel, and to generate 3D flow vectors, it averages the 2D flow vectors along the XY, XZ, and YZ planes. However, the substantial difference in sampling ratios between the XY plane and the Z-axis introduces noise, often leading to oversegmentation in highly anisotropic images.

\textbf{CellStitch.} CellStitch is a 2D-based method that constructs 3D segmentation from the pre-segmentated results produced by CellPose2D. It uses optimal transport with the Jaccard index-based cost to match cells between adjacent layers and introduces an interpolation method to ``stitch" the discrete, layer-by-layer 2D cell masks into a continuous 3D cell body. However, the performance of such 2D-based methods heavily depends on the quality of the initial 2D segmentation. Specifically, CellPose2D occasionally mis-segments a cell, resulting in a \hyperref[f3]{empty} or \hyperref[f10]{undersegmented} mask. Since CellStitch relies on information from adjacent layers to build continuity, it fails to detect and address these errors, leading to inaccuracies in the final 3D segmentation.

\subsection{Cell Segmentation: 3D-based Models}
\textbf{PlantSeg.} PlantSeg is a direct 3D reconstruction method that operates independently of pre-segmented 2D results. It first uses deep learning to predict cell boundaries from 2D image stacks and then applies graph partitioning techniques to reconstruct the 3D cell bodies. However, we have observed that PlantSeg tends to experience over-segmentation issues when handling elongated cells. In such cases, PlantSeg often segments a single elongated cell into multiple portions, with the connecting surfaces between these portions becoming tilted and irregular rather than aligning along standard axes. Our framework, designed to correct oversegmentation issues in 2D-based models, can also be adapted to address the oversegmentation problems in PlantSeg.

\subsection{Optimal Transport}
Recently, Optimal Transport (OT) has been applied to mutiple biological areas, including single-cell transcriptomics \cite{schiebinger2019optimal}, drug perturbations \cite{bunne2023cellot,cuturi23a,chen2024sicnn}, and cell alignments \cite{demetci2022scot}. In our framework, OT is utilized to quantify the geometric changes of cell masks between adjacent layers.  We include an introduction to OT and the earthmover's distance in discrete settings.

Given measures \( P \) and \( Q \) defined on \( \mathbb{R}^d \), along with a transportation cost matrix \( c(x, y) \), where \( c(x, y) \) quantifies the transportation cost between a point \( x \in P \) and a point \( y \in Q \), the objective is to find an optimal transport plan \( \pi \in \Pi(P, Q) \) that determines the optimal amount of transportation between pairs \( (x, y) \) that minimizes the aggregated transportation cost:
\[
\min_{\pi \in \Pi(P, Q)} \sum_{x \in P, y \in Q} c(x, y) \pi(x, y) \label{e1} \tag{1}.
\]
Formulation (\hyperref[e1]{1}), known as the Kantorovich formulation of optimal transport, generalizes the Monge formulation \cite{Monge1781} by relaxing the requirement of a direct one-to-one mapping between source and target elements, allowing for mass splitting to achieve optimality.

For two discrete distributions \( X = \{x_i\}_{i=1}^n \) and \( Y = \{y_j\}_{j=1}^m \), the earthmover's distance (EMD) is defined as the minimum cost required to transform \( X \) into \( Y \):
\begin{align*}
\text{EMD}(X, Y) &= \min_{\pi \in \Pi(X, Y)} \sum_{i=1}^n \sum_{j=1}^m \pi_{ij} c(x_i, y_j) \label{e2} \tag{2}, \\
\text{s.t.} &\ \ \sum_{j=1}^m \pi_{ij} = x_i, \  \sum_{i=1}^n \pi_{ij} = y_j, \ \pi_{ij} \geq 0, \ \forall i, j.
\end{align*}
Here, \( c(x_i, y_j) \) is the cost of transporting a unit of mass from \( x_i \) to \( y_j \). The EMD provides an interpretable metric that measures the distance between two distributions based on the minimal work required to transform one distribution into the other. Utilizing the EMD, we introduce a novel approach to quantify the difference between two 2D geometries. By treating the geometries as discrete distributions of mass, the EMD allows us to calculate the minimal transportation cost required to transform one geometry into the other, providing a robust metric for capturing geometric differences. When cell masks are large, and computing the EMD between them becomes computationally expensive, an alternative is to use \texttt{ot.sliced.sliced\_wasserstein\_distance} \cite{bonneel2015sliced}. Increasing the number of projections in this method produces a result that more closely approximates the true EMD.


\section{Methodology}
\subsection{Problem Setup}
\begin{figure}[h!]
    \centering
    \label{f3}
    \includegraphics[width=0.6\columnwidth]{Figures/f3.pdf} 
    \vspace{-1em}
    \caption{An example of 2D mis-segmentation leading to subsequent 3D over-segmentation.}
\end{figure}
\vspace{-0.5em}
We first focus on addressing the oversegmentation issue in 2D-based models. As illustrated in \hyperref[f3]{Figure 3}, when there is a missing cell slice at layer \( i \) (leaving an empty cell mask at layer \( i \)), existing 2D-based models would incorrectly split the entire cell into two separate cells, one from the upper section and one from the lower section, resulting in oversegmentation. Our task is to: \textbf{(i)} identify a list of suspected oversegmented candidates, consisting of upper and lower parts of cells separated by a gap;  
\textbf{(ii)} determine whether each candidate is truly oversegmented or if the gap is a natural one, as is often observed in loosely packed cells such as leaves, where natural gaps between cells are common; and  
\textbf{(iii)} recover the correct 2D segmentation by predicting the cell mask for the gap layer and reconstructing the accurate 3D segmentation result.

\subsection{Method Pipeline}\label{S3}
\begin{algorithm}
\caption{Candidates Screening for Oversegmentation}
\label{a1}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Dataset of cells with masks across layers
\STATE \textbf{Output:} List of suspected over-segmented candidates

\STATE Initialize \texttt{candidates} = []

\FOR{each pair of cells $(A, B)$ in dataset}
    \IF{highest layer mask of $A$ overlaps with lowest layer mask of $B$}
        \IF{no other complete cell exists in the gap}
            \STATE Append $(A, B)$ to \texttt{candidates}
        \ENDIF
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
\textbf{Candidates Screening.} The first step is to identify all suspected over-segmented candidates within the dataset. As shown in \hyperref[a1]{Algorithm 1}, for each cell, we store its mask at the highest layer (top of the cell) and the mask at the lowest layer (bottom of the cell). We then iterate through all pairs of cells in the dataset. If the mask of one cell at its highest layer overlaps with the mask of another cell at its lowest layer (i.e., they occupy the same position in their respective layers) and no other cell exists in the gap between them, we classify these two cells as a suspected oversegmentation candidate.


\textbf{Binary Classification.} For the suspected candidates, our goal is to differentiate truly oversegmented cases from those with a natural gap. We frame this as a binary classification task, training a classifier using features extracted from labeled true (oversegmented) and false (natural gap) cases.

\textbf{Geometric Features Extraction.} For each pair of candidates, we measure the geometric change between the two masks at the gap layer and compare it to the original layer-to-layer changes of each cell. For oversegmented cells, the gap-layer changes are expected to be smooth and consistent with their previous layer-to-layer patterns. In contrast, a sudden, unusual change at the gap layer is indicative of two distinct cells, suggesting a natural gap rather than oversegmentation.

For cell masks \( M \) and \( N \), we represent them as uniform distributions within their geometric boundaries:
\[
P_M(x) = \frac{1_{\{x \in M\}}}{|M|}, \quad P_N(x) = \frac{1_{\{x \in N\}}}{|N|},
\]
where \( 1_{\{x \in M\}} \) is the indicator function if \( x \) is within the boundary of \( M \), and \( |M| \) denote the geometric areas of \( M \). The difference of shape, or the geometric change of shape, between these two masks $M$ and $N$ can be then measured using $\text{EMD}(P_M, P_N)$.

To check whether the EMD between two gap masks aligns with the previous trend of mask changes in each cell, we use \hyperref[a2]{Algorithm 2} to gather EMD information. Each Cell $i$ is indexed such that it starts at layer \( 0 \) and ends at layer \( n_i \):
\begin{algorithm}
\caption{Cell EMD Extraction Algorithm}
\label{a2}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Masks for each layer of Cell A and Cell B
\STATE \textbf{Output:} $\text{EMD}_{\text{Cell A}}$, $\text{EMD}_{\text{gap}}$, $\text{EMD}_{\text{Cell B}}$

\FOR{each layer $i$ to $i+1$ in Cell A}
    \STATE $\text{EMD}_{\text{Cell A}}[i] \gets \text{EMD}(\text{layer } i, \text{layer } i+1)$
\ENDFOR

\STATE $\text{EMD}_{\text{gap}} \gets \text{EMD}(\text{Cell A layer $n_{A}$, Cell B layer $0$})$

\FOR{each layer $i$ to $i+1$ in Cell B}
    \STATE $\text{EMD}_{\text{Cell B}}[i] \gets \text{EMD}(\text{layer } i, \text{layer } i+1)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

Since different cells have varying heights, the length of the \(\text{EMD}_{\text{Cell } i}\) arrays differs. To enable training, it is necessary to ensure that all input information has a consistent length. Therefore, we extract key statistical features from eac EMD arrays to serve as input for the training process. In our pretrained model, we use statistical representations such as the median, maximum, minimum, first quartile (\(q_1\)), and third quartile (\(q_3\)) as the input features derived from each array. We input these features along with \(\text{EMD}_\text{gap}\) to the training process.


\textbf{Topological Shape Extraction.} We propose a simple method to distinguish oversegmented cell pairs from those with natural gaps. Shown in the Figure 4, one key pattern observed across multiple datasets is oversegmented cases typically demonstrate a smooth transition at the gap layer, following either a linear or circular changing pattern. Patterns of linearity and circularity can be explained biologically. Circular shapes, wider in the middle and thinner at the ends, often indicate mitotic cells, known as ``circular rounding" \cite{cr2,cr1}; Linear shapes, tapering gradually, are typical of structural cells such as palisade cells \cite{pallardy2008physiology}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.78\columnwidth]{Figures/f5.pdf} 
    \vspace{-1em}
    \caption{Comparison of 3D cell topological shape between oversegmented cases and natural gap cases.}
    \label{f5}
\end{figure}
\vspace{-0.5em}
We quantify linearity and circularity by analyzing the overlapping area between masks at adjacent layers. For a cell following a linear pattern, the change in overlapping area should also exhibit a monotonous linear trend; for circular pattern, the change in overlapping area should follow a strictly quadratic trend, characterized by a monotonous increase followed by a decrease. As shown in the ``strictly monotonous'' column in \hyperref[f5]{Figure 5}, we tested this strict pattern on cells in labeled datasets.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\columnwidth]{Figures/f4.pdf} 
    \vspace{-1.5em} 
    \caption{Proportion of cells included using the shape pattern algorithm under different tolerance levels: strictly monotonous or within the $\sigma$ level of the standard deviation. The number of cells in each dataset is indicated in brackets.}
    \label{f4}
\end{figure}

 By studying the cells that do not fit, we observed occasional oscillations in the overlapping area changes that deviate from the strictly monotonous pattern, despite exhibiting clear quadratic or linear trends. To address this, we relaxed the strict pattern by performing linear and quadratic regression on the overlapping area and using the regression coefficient \( R^2 \) as a measure. By allowing a 1 standard deviation tolerance from the mean \( R^2 \) within each category, we observed a substantial improvement in inclusiveness. Still, for strictly monotonous cells, we set its \( R^2 \) to be 1. We consider each pair of suspected candidates as a full cell and run \hyperref[a3]{Algorithm 3} on it to extract its shape information:

 
\begin{algorithm}
\caption{Classify Overlapping Pattern and Compute $R^2$}
\label{a3}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Integrated cell masks from A and B
\STATE \textbf{Output:} Class (linear or quadratic), ShapeIndex ($\widetilde{R^2}$)

\FOR{each layer $i$ to $i+1$ in Cell}
    \STATE $\text{OverlapArea}[i] \gets \text{Overlap}(\text{layer } i, \text{layer } i+1)$
\ENDFOR

\STATE $\text{Quadratic } R^2 \gets \text{QuadraticRegression}(\text{OverlapArea})$
\STATE $\text{Linear }R^2 \gets \text{LinearRegression}(\text{OverlapArea})$
\STATE $R^2 \gets \max(\text{Linear }R^2, \text{Quadratic } R^2)$
\STATE $\text{Class} \gets \arg\max(\text{Linear }R^2, \text{Quadratic } R^2)$

\IF{$\text{OverlapArea}$ is monotonous}
    \STATE $R^2 \gets 1$
\ENDIF
\STATE \text{ShapeIndex} $\gets$ Normalized $R^2$ from each Class
\end{algorithmic}
\end{algorithm}

\textbf{Classifer Training.} We first build a training set with labeled true (oversegmented) and false (natural gap) cases from the labeled datasets. True cases can be generated by simply replacing a specific layer in the 2D segmentation with an empty mask to simulate oversegmentation errors; false cases can be collected by applying \hyperref[a1]{Algorithm 1} on a labeled dataset to identify cells with natural gaps. Next, we run \hyperref[a2]{Algorithm 2} and \hyperref[a3]{Algorithm 3} on the labeled candidates from the training set to extract their geometric and topological shape information. To avoid information loss, we concatenate all the extracted information as input to a multi-layer perceptron and use the labels to train it for classification. The entire pipeline is illustrated in \hyperref[f2]{Figure 2}.


% Add a visual on cross layer interploation?
\textbf{Cross-layer Interpolation.} For oversegmented cases, we recover the 2D mis-segmented mask at layer $i$ by interpolating it using the cell mask boundaries at layers $i+1$ and $i-1$ with the interpolation method from CellStitch \cite{Liu2023Cellstitch}. The corrected 2D segmentation masks are then used to reconstruct the new 3D segmentation result. Specific implementation of this cross-layer interpolation is shown in \hyperref[AA]{Appendix A}.
\subsection{Method Generalization}
In this section, we aim to generalize our framework to address a broader range of oversegmentation types. Recall that our framework is built upon the oversegmentation results from 2D-based models. Consequently, the cutting surface between two oversegmented cells is always aligned along the horizontal plane of the standard axes.

% Adding a figure here
%Indeed, 2D-based methods perform poorly on certain datasets compared to direct 3D construction methods (e.g., PlantSeg). 
However, for direct 3D construction methods, the cutting surface between two oversegmented cells is often tilted, irregular, and rugged. To address these challenges, we propose a generalization of our method to handle such oversegmentations effectively. We validate our generalized method on the Pancreas-B dataset, where 3D-based models outperform 2D-based models (\hyperref[f44]{Figure 16}), and propose several recommendations to enhance end-user experience.

Our generalized pipeline is illustrated in \hyperref[f7]{Figure 6}. The ultimate goal is to reconstruct a rotated 2D segmentation using the reference plane estimated from the cutting surface between oversegmented cells and then apply our original classifier to the rotated 2D image stacks.

\textbf{Candidates Screening.} Since oversegmentation can occur in any direction, we store all cell information into a graph network and retrieve a list of neighboring cells (those in contact with each other) as suspected candidates. The number of suspected candidates should be significantly larger compared to those in \hyperref[S3]{\S3.2}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{Figures/f7.pdf} % Ensure the path to Figure1.png is correct
    \caption{Main pipeline for constructing rotated 2D segmentation layers along the standard axes for the tilted oversegmentation cases.}
    \label{f7}
\end{figure*}

\textbf{Oversegmentation Cutting Surface.} We define the cutting surface between two cells as the shared curved surface where the cells are in contact. In the previous case, the cutting surface aligned with the XY plane, and the 2D segmentation layers were cut from the 3D segmentation result using this XY plane. Here, we extend this idea to cut the 3D cell segmentation layer-by-layer to form a rotated 2D segmentation result, using a new reference plane defined by the cutting surface between two oversegmented cells.

\textbf{Rotated 2D Segmentation Reconstruction.} The first step is to recover the reference plane from the rugged cutting surface between two oversegmented cells. This cutting surface is treated as a curvature, and the PCA plane retrieved from this curvature is used as the estimated reference plane.

Using this reference plane, we cut the 3D segmentation results into parallel 2D layers by shifting the reference plane upward and downward. However, as shown in Step 4 of \hyperref[f7]{Figure 6}, since all cells are stored in the discrete index format, the tilted plane cuts the cell masks within the same layer into strips (integer indices) rather than forming continuous 2D geometries. To recover these continuous 2D geometries, we perform interpolation between the strips to reconstruct the complete 2D cell mask.

The cell masks from these reconstructed 2D layers are then used as input for our pre-trained model. Specifically, we separate the two oversegmented cells by removing the cell masks from both cells at the reference layer (the original PCA plane without shifting) and treat the upward and downward portions as a pair of candidates. Our classifier determines whether the two parts should be merged. In this setting, layer-to-layer interpolation is no longer required to recover 2D mis-segmentation; instead, we simply merge the original 3D cell bodies.

\textbf{Penalties for Information Losses.} The reconstruction process may slightly alter the relative geometric positions of the cell masks, and the interpolation between stripes within each layer can change the cross-layer overlapping area and EMD, thereby affecting the 3D shape index and EMD features. To address these information losses, we propose a straightforward approach by introducing penalties to both geometric and topological information. Higher penalties reduce the number of false positive cases, while lower penalties allow for the inclusion of more potential oversegmentation cases. The penalty can be flexibly adjusted based on the specific goals of the end-users.


For the normalized 3D shape index \( \widetilde{R^2} \) retrieved from \hyperref[a3]{Algorithm 3}, we apply a positive constant \( p \) and use the penalized value \( \widetilde{R^2} - p \) as the input. The effect of this penalization is demonstrated in \hyperref[S5.2]{\S4.4}. Recall that statistical features are extracted from each EMD array to ensure consistent input lengths. In our pre-trained model, we use the minimum and maximum as boundary features; here, the penalization can be adjusted by replacing the boundary features with quartiles (\( q_{1} \) and \( q_{3} \)). This approach implicitly penalizes the EMD information by narrowing the boundary features, making them stricter.


\section{Experiments}
We first present an overview of all the datasets\footnote{\label{ft1}All the plant-type datasets are publicly availiable at the following \href{https://osf.io/fzr56/}{site} \cite{bassel2019_dataset}. For plant-type data, the voxel resolution is unknown. For animal-type data, the voxel resolution ($z \times y \times x$) is $0.4 \times 0.1 \times 0.1~\text{\textmu m}$.} used in this work. \hyperref[t7]{Table 1} includes the name of datasets, the number of image stacks in each dataset, the dataset type, and whether it contains a labeled ground-truth segmentation.

All the following experiments are designed to evaluate the accuracy of our framework on trained datasets and its transfer learning efficiency when applied to animal cell datasets not included in the pre-trained model. The transfer learning task aims to demonstrate the method's generalizability to new, specific datasets, while the overall generalizability of our pipeline framework to broader oversegmentation issues is shown in \hyperref[S5.2]{\S4.4}. We include additional visualization examples and further analysis along with discussions of our results in \hyperref[AB]{Appendix B}.

\begin{table}[h]
\centering
\label{table:datasets}

\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
\textbf{Dataset} & \textbf{\# Stacks} & \textbf{Type} & \textbf{Labeled} & \textbf{Anisotropy ($z:y:x$)} \\ 
\midrule
Anther    & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Filament  & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Leaf      & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Pedicel   & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Sepal     & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Valve     & 100 & Plant/Public  & \checkmark & $4:1:1$ \\ 
Pancreas-A      & 11  & Animal/Private & $\times$ & $4:1:1$$^*$\\ 
Pancreas-B  & 1   & Animal/Private & $\times$ & $4:1:1$$^*$ \\ 
\bottomrule
\end{tabular}
}
\caption{Dataset information overview. $^*$Specific voxel resolution information is provided in footnote\hyperref[ft1]{\textsuperscript{1}}.}
\label{t7}
\end{table}

\subsection{Plant Cell: Synthesized Cases}
In this task, we reserve a small portion (10 image stacks) from each plant dataset with true (oversegmented) and false (natural gap) test cases excluded from the training set to evaluate the pre-trained model on unseen but similar datasets. Since the test cases are built from labeled ground-truth results, a deterministic answer exists for each test case. 


\begin{table}[h]
\centering
\label{table:accuracy}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cccccc}
\toprule
\textbf{Dataset}    & Sepal & Pedicel & Valve & Leaf & Anther & Filament \\ 
\midrule
\textbf{Accuracy (T)} & 0.955 & 0.854   & 0.965 & 0.988 & 0.840  & 0.854    \\ 
\textbf{Accuracy (F)} & 0.815 & 0.994   & 0.969 & 1.000 & 0.953  & 0.940    \\ 
\bottomrule
\end{tabular}%
}

\caption{Accuracy on Synthesized Test Cases.}
\end{table}


%If space are left, add the discussion of the result on each datasets and some pics for fixation
However, the accuracy reported above serves only as a reference for the pre-trained model's performance under perfectly accurate conditions. In practice, 2D and 3D inaccuracies are inevitable. Therefore, we will next evaluate the method's actual performance on real oversegmented cases produced by CellPose2D and CellStitch.

\subsection{Plant Cell: Real Cases}\label{S4.2}

In this task, we first run CellPose2D on the raw image stack to obtain a 2D segmentation result, followed by applying CellStitch to generate a 3D segmentation. Our pre-trained model is then used to identify oversegmented cases in the CellStitch output, which are subsequently validated on a case-by-case basis through human review. 

We use human validation on a case-by-case basis to ensure the highest accuracy. This is particularly necessary because of the ``hallucination'' effect of CellPose2D (shown in \hyperref[f92]{Figure 12} and \hyperref[f32]{Figure 13}), which often misinterprets highlight spots or noise in the raw image as cell masks, even when these regions are empty in the ground truth. Such errors propagate into the 3D segmentation and our correction process, potentially leading to ``correct'' actions on fundamentally incorrect initial results.


\begin{figure}[h!]
    \centering
\includegraphics[width=0.95\columnwidth]{Figures/f16.pdf} 
    \caption{Example of correct case. In the CellStitch row, layers $i-1$ and $i+1$ contain cell masks (highlighted using the green box) that are also appeared in the ground truth row at the same position. Our method detects these two masks as oversegmentation and stitches them as the same cell.}
    \label{f4}
\end{figure}

\textbf{Evaluation Metric.} During human validation, we classify oversegmentation corrections into three categories: \textbf{(i)} Correct cases, where the oversegmented cell masks are present in the ground truth and belong to the same cell; \textbf{(ii)} Incorrect cases, where the oversegmented cell masks are present in the ground truth but actually belong to different cells (natural gap); and \textbf{(iii)} Unsure cases, which include scenarios such as ``hallucination'', where the correction appears accurate but lacks definitive support from the ground truth. An example of the correct case is shown in \hyperref[f4]{Figure 7}. More visualization examples are in \hyperref[AB]{Appendix B}. \hyperref[t2]{Table 3} shows the performance of our pre-trained model on this task:

\begin{table}[h]
\centering
\label{t2}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cccccc}
\toprule
% Diagonal header in the top-left corner (no multirow needed)
\diagbox[width=9em, height=3em]{\textbf{Metric}}{\textbf{Dataset}} 
& \textbf{Sepal} & \textbf{Pedicel} & \textbf{Valve} & \textbf{Leaf} & \textbf{Anther} & \textbf{Filament} \\ 
\midrule
\textbf{\# Correct}      & 70  & 71  & 75  & 42  & 96  & 92  \\ 
\textbf{\# Unsure}       & 113 & 134 & 82  & 38  & 141 & 124 \\ 
\textbf{\# Incorrect}    & 13  & 15  & 4   & 3   & 13  & 28  \\ 
\midrule
\textbf{\# Valid Fixation} & 83 & 86 & 79 & 45 & 109 & 120 \\ 
\midrule
\textbf{Accuracy}        & 0.844 & 0.826 & 0.949 & 0.933 & 0.881 & 0.767 \\ 
\bottomrule
\end{tabular}
}
\caption{Results on real cases. The first three rows present the number of cases based on our defined evaluation metric. The number of valid fixations is the sum of correct and incorrect cases, and the accuracy is calculated as the number of correct cases divided by the number of valid fixations.}
\end{table}


\begin{table*}[t]
\centering
\label{t3}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccccccccccc|c}
\toprule
% Diagonal header (no multirow needed)
\diagbox[width=9em, height=3em]{\textbf{Metric}}{\textbf{Image Stack}} 
& \textbf{Pancreas-A-11} & \textbf{Pancreas-A-16} & \textbf{Pancreas-A-03} & \textbf{Pancreas-A-07} & \textbf{Pancreas-A-04} & \textbf{Pancreas-A-09} & \textbf{Pancreas-A-10} & \textbf{Pancreas-A-05} & \textbf{Pancreas-A-02} & \textbf{Pancreas-A-14} & \textbf{Pancreas-A-08} & \textbf{OVERALL} \\ 
\midrule
\textbf{\# Correct}   & 9 & 10 & 19 & 29 & 21 & 25 & 35 & 52 & 28 & 52 & 94 & 374 \\ 
\textbf{\# Unsure}    & 1 & 3 & 3 & 5 & 3 & 9 & 8 & 7 & 8 & 21 & 24 & 92 \\ 
\textbf{\# Incorrect} & 1 & 2 & 5 & 4 & 3 & 6 & 4 & 10 & 1 & 2 & 14 & 52 \\ 
\midrule
\textbf{Accuracy}  & 0.900 & 0.833 & 0.792 & 0.879 & 0.875 & 0.807 & 0.897 & 0.839 & 0.966 & 0.963 & 0.870 & 0.878 \\ 
\bottomrule
\end{tabular}%
}
\vspace{-0.5em}
\caption{Performance of transfer learning on the Pancreas-A dataset. Details of correction are reported for each image stack in the Pancreas-A dataset. Metrics are calculated in the same way as \hyperref[t2]{Table 3}. Overall result for the whole dataset is reported in the final column.}
\end{table*}
\textbf{Results Discussion.} By analyzing the incorrect cases, we identified a major causes of these errors: CellPose2D often ``\hyperref[f23]{shrinks the cell mask smaller}" during 2D segmentation, especially when the cell boundaries in the raw images are blurred by a light band. This shrinkage results in the loss of geometric information, as the EMD between the two masks is also affected when one mask shrinks. Other causes include the misplacement of the cell mask and errors in its segmentation.

\subsection{Animal Cell: Real Cases}
In this task, we transfer our pre-trained model on plant cells to unlabeled animal cell datasets. While we recommend that end-users follow the pipeline outlined in \hyperref[S3]{\S3} to build their own trained models on their specific datasets, we also provide a reference to demonstrate our method's transfer learning accuracy on unseen and distinct cell-type datasets. 

The dataset used in this task is a private Pancreas-A dataset provided by our end-users. We follow the same pipeline outlined in \hyperref[S4.2]{\S4.2} to obtain the CellStitch 3D segmentation results, which are then processed using our correction framework. Due to the lack of labeled ground truth for this dataset, we adopt a similar evaluation metric as in \hyperref[S4.2]{\S4.2}, but refer directly to the raw images (which are mostly clear and can be visually distinguished by humans). All cases that are uncertain or lack full support are categorized as unsure cases.


\textbf{Results Discussion.} We provide an estimation of the transfer learning accuracy using the Pancreas-A dataset in \hyperref[t3]{Table 4}. Combined with the experimental results in \hyperref[S4.2]{\S4.2} and an analysis of each image stack from the Pancreas-A dataset, we observe that the performance of our framework is influenced by the resolution of the image stack and the average size of the cell masks. Higher resolutions amplify geometric details, enabling EMD to capture more nuanced information. 

Additionally, our framework demonstrates the ability to recover cells with irregular 2D shapes, largely due to their 3D shape-changing trends still adhering to the linear or circular patterns we previously categorized.

\subsection{Generalization to 3D-based Model Errors}\label{S5.2}
We run our framework on the 3D segmentation results produced by PlantSeg on the Pancreas-B dataset. Experiments are conducted under different penalty combinations as a reference for end-users. Since our pre-trained model undergoes two stages of transfer (cross-cell type and cross-segmentation format), we strongly encourage end-users to include human feedback to validate the final candidates if they choose to use our pre-trained model on plant cells without training the model specific to their dataset.

As shown in \hyperref[t4]{Table 5}, we use the evaluation metric outlined in \hyperref[S4.2]{\S4.2} for human validation. Given the increased complexity of the task and the need to handle all neighboring cells, we also report the filtration accuracy, which indicates the number of candidates excluded during the process. The penalization parameters act as a trade-off between the filtration rate (the effort required for human feedback) and oversegmentation inclusion rate (the number of oversegmented cases reported). Penalty parameters ensure that only 5.9\% of the candidates are reported for human validation, providing end-users with greater flexibility.

\begin{table}[h]
\centering
\label{t4}
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{l|c|c|ccc|c}
\toprule
\textbf{EMD Mode} & \textbf{$p$} & \textbf{Filtration} & \textbf{\# Correct} & \textbf{\# Unsure} & \textbf{\# Incorrect} & \textbf{Accuracy} \\ 
\midrule
                  & 1.3 & 0.855 & 20 & 53 & 81 & 0.198 \\ 
\textbf{$Min$, $Max$} & 2   & 0.871 & 19 & 48 & 71 & 0.211 \\ 
                  & 2.5 & 0.882 & 18 & 44 & 64 & 0.220 \\ 
\midrule
                  & 1.3 & 0.893 & 16 & 38 & 60 & 0.211 \\ 
\textbf{$q_{1}$, $q_{3}$}   & 2   & 0.920 & 14 & 25 & 47 & 0.230 \\ 
                  & 2.5 & 0.941 & 11 & 17 & 35 & 0.240 \\ 
\bottomrule
\end{tabular}%
}
\vspace{-0.5em}
\caption{Results on Pancreas-B dataset. \textbf{EMD Mode}: boundary features used; \textbf{$p$}: constant penalty applied to the shape index; \textbf{Filtration}: accuracy of filtration among $1,069$ pairs of candidates.}
\end{table}

\section{Conclusions}
In this work, we propose a novel and interpretable method that leverages geometric information from imperfect 3D segmentation results to correct oversegmentation errors. We demonstrate that our framework can be applied to both 2D-based and direct 3D reconstruction methods to effectively address oversegmentation issues. The performance of our pre-trained model is validated on both similar plant cell datasets and entirely distinct animal cell datasets, showcasing the effectiveness of transfer learning. Additionally, we provide hyper-parameter tuning options for penalties, offering end-users greater flexibility.

While deep-learning-based methods rely on incorporating increasingly large datasets for training, our interpretable framework achieves strong results by training on a small subset of datasets. Future work could explore how similar interpretable geometric frameworks can address undersegmentation issues. Furthermore, while this study focuses on pre-trained models trained on the same cell-type datasets, it would be valuable to investigate how selecting more suitable datasets could improve training outcomes. It is also worthwhile to study how to extend this framework to other cell types, such as neurons and cancer cells, which have more irregular cell shapes.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\section*{Acknowledgment}
We acknowledge computing resources from Columbia University's Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. 

PC and YL would also like to express our appreciation to Andrew Blumberg, Abigail Hickok, Yinuo Jin for their valuable feedback and discussions.

\bibliography{example_paper}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn
\section{Cross-layer Interpolation to Recover 2D Mis-segmented Cell Masks} \label{AA}
As our classifier identifies candidates that are cases of oversegmentation, we must then recover the missing mask between the two oversegmented cells. To address this, we applied the interpolation method proposed in CellStitch \cite{Liu2023Cellstitch}, which is based on the Wasserstein interpolation framework developed by \citet{Solomon2015}.

We first treat each matched pair of cells in consecutive slices as source and target boundaries (or the contour of each cell mask). Each boundary pixel is given a uniform mass, and we compute an OT plan that matches source pixels to target pixels. Then, for any intermediate layer, we blend each matched pair of pixels according to the transport weights, effectively creating a geometry‐aware interpolation of the boundary. Finally, we fill those interpolated boundaries to generate the complete cell mask at that layer. An example is shown in \hyperref[f18]{Figure 8}, where a mask at an intermediate layer $i$ is interpolated using the cell contours of the source and target cell masks at layer $i+1$ and $i-1$.

\begin{figure}[h!]
    \centering
\includegraphics[width=0.45\textwidth]{Figures/f9.pdf} 
    \vspace{-5em}
    \caption{Example of interpolation between the source mask at layer $i+1$ and the target mask at layer $i-1$.}
    \label{f18}
\end{figure}

\section{Visualization Examples} \label{AB}
In this section, we provide multiple examples of corrections made using our framework across different datasets and plane views to better visualize our results.

\subsection{Animal Cell Examples}
\begin{figure}[h!]
    \centering
\includegraphics[width=0.92\textwidth]{Figures/f10.pdf} 
\vspace{-1em}
    \caption{Example of a successful correction in animal cell datasets, viewed from the XZ plane. The left panel displays the raw microscopic image, the middle panel shows the 3D segmentation produced by CellStitch, and the right panel presents our results, where Cell A and Cell B from the middle panel were stitched.}
    \label{f19}
\end{figure}
\hyperref[f19]{Figure 9} provides an example of correction viewed from the XZ plane, illustrating how two oversegmented cells (Cell A and Cell B) are stitched along the Z direction. Specifically, although the raw image shows that the cell's 3D topological shape follows a strictly circular pattern (a gradual monotonic increase followed by a decrease), 2D segmentation inaccuracies produced by CellPose2D introduce oscillations that deviate its shape from the strictly monotonic standard. However, our relaxation approach using regression \( R^2 \) alleviates this issue, as small oscillations still yield a high \( R^2 \) value, distinguishing it from natural gap cases.

\hyperref[f10]{Figure 10} illustrates an example of correcting a mis-segmentation that is not caused by an empty mask. In the CellStitch row, the purple cell at layer \( i-1 \) and the bright yellow cell at layer \( i+1 \) remain disconnected due to an undersegmentation error produced by CellPose2D at the same position in layer \( i \). Our method successfully identifies this oversegmentation from a 3D perspective and corrects the 2D undersegmentation. The raw image also indicates that the three cell masks belong to the same 3D cell body.

\begin{figure}[h!]
    \centering
\includegraphics[width=0.9\textwidth]{Figures/f11.pdf} 
    \caption{An example of accurate correction for the undersegmented 2D mask, viewed from the XY plane in the animal cell dataset. The positions of the mis-segmented masks are highlighted with green boxes.}
    \label{f10}
\end{figure}
In line 6 of \hyperref[a1]{Algorithm 1}, we mean that there is no ``complete" cell existing between the gap, with both its highest and lowest layers located between our candidates. However, we allow noisy cell masks to exist between the candidates, enabling us to also address 2D undersegmentation errors.


\
\newpage
\subsection{Plant Cell Examples}
\hyperref[f21]{Figure 11} shows an example of accurate correction in the plant cell dataset by stitching the highlighted cell masks from layer \( i-1 \) and layer \( i+1 \). This correction is similar to the type shown in \hyperref[f10]{Figure 10}, where 2D undersegmentation is addressed. The true 2D segmentation in layer \( i \) is recovered through cross-layer interpolation:
\begin{figure}[h!]
    \centering
\includegraphics[width=0.85\textwidth]{Figures/f12.pdf} 
    \caption{Example of the accurate correction viewed from XY plane from the plant cell dataset. Position of the mis-segmented masks are highlighted using boxes.}
    \label{f21}
\end{figure}

\newpage
\hyperref[f92]{Figure 12} presents an example of an unsure case caused by hallucination masks produced by CellPose2D. In the first row, the green box in the ground truth highlights an area where no cells are present. However, in the same position in the CellStitch result, two hallucinated masks (purple and yellow) are generated at layer \( i-1 \) and layer \( i+1 \). These noisy masks lead our framework to stitch them together. While the correction appears accurate, there is no evidence from the ground truth segmentation to support our judgment. Therefore, we classify these cases as unsure, pessimistically reporting only the absolutely correct cases.

\begin{figure}[h!]
    \centering
\includegraphics[width=0.85\textwidth]{Figures/f6.pdf} 

    \caption{An example of an unsure case, where two noisy hallucinated masks are seemingly stitched correctly, but there is no evidence from the ground truth layer to support this conclusion.}
    \label{f92}
\end{figure}
\hyperref[f32]{Figure 13} shows another uncertain case caused by noisy masks from CellPose2D. The green box in the ground truth marks an area without cells, but the CellStitch result includes two noisy masks (purple and orange) at layers \(i-1\) and \(i+1\), which are stitched together in our result. While the correction seems accurate, the ground truth provides no evidence to confirm this, so we conservatively report only the fully verified cases.

\newpage
\begin{figure}[h!]
    \centering
\includegraphics[width=0.85\textwidth]{Figures/f13.pdf} 

    \caption{Extra example of unsure case led by the noisy masks.}
    \label{f32}
\end{figure}

We also present incorrect cases to better understand the underlying issues in these situations and to provide clearer explanations for the causes of such incorrect results. As shown in \hyperref[f29]{Figure 14}, in the row of CellStitch results, the dark blue mask at layer \( i-1 \) and the yellow mask at layer \( i+1 \) are stitched together in our results. Without the ground truth labels, this stitching appears correct, as both cells are similar in shape and located in the same position, with a missing mask in the intermediate layer. However, the ground truth row reveals that the dark blue mask is part of the large purple cell in layer \( i-1 \), while the yellow mask belongs to the large orange cell in layer \( i+1 \). Thus, the dark blue and yellow masks actually belong to two different cells, and their stitching is incorrect, making this an example of an incorrect case.

It is evident that both the dark blue and yellow masks only partially represent their corresponding ground truth cell masks. This highlights why we state that CellPose2D often ``shrinks the cell masks smaller" than they are supposed to be. We illustrate this effect in \hyperref[f23]{Figure 15}. When mask information is lost, both the geometric EMD measurement and the topological shape index are affected. For the EMD measurement, recall that EMD quantifies the effort required to transform one distribution into another. Losing geometric information alters the transformation, making the EMD between mask A and mask B differ from its original value. In the case shown in \hyperref[f23]{Figure 15}, the EMD between the mis-segmented masks is smaller compared to the ground truth masks, suggesting a falsely higher similarity. For the topological shape index, which is calculated via changes in overlapping areas, as shown in \hyperref[f29]{Figure 14}, the overlapping area between adjacent layers (e.g., layers \( i+1 \) and \( i-1 \)) aligns more closely with the overlapping areas of preceding and succeeding layers (e.g., layers \( i-2 \) and \( i+2 \)), as the masks shrink. These partial and altered representations lead our method to make incorrect judgments.
\begin{figure}[h!]
    \centering
\includegraphics[width=0.85\textwidth]{Figures/f14.pdf} 

    \caption{An example of incorrect stitching, where two masks from different cells are erroneously stitched together. The mask in red box is highlighted as the incorrect recovered mask. The incorrect stitching is partially influenced by misleading information from the 2D segmentation results.}
    \label{f29}
\end{figure}

\begin{figure}[h!]
    \centering
\includegraphics[width=0.4\textwidth]{Figures/f15.pdf} 
    \caption{Further analysis towards the misleading shrunken masks shown in \hyperref[f29]{Figure 14}, where the cell masks are viewed from the XY plane.}
    \label{f23}
\end{figure}


\clearpage
\section{Comparison between PlantSeg and CellStitch on Pancreas-B Dataset}
As demonstrated in the original CellStitch benchmark experiment \cite{Liu2023Cellstitch}, while CellStitch outperformed PlantSeg on many datasets, 2D-based methods still struggle when the quality of the raw image is low. This is evident in the following figure, which shows a layer from the YZ plane of the final 3D segmentation result. Most of the hallucinated noisy masks produced by CellPose2D are carried over to the final CellStitch results. Although PlantSeg still suffers from oversegmentation issues, the overall segmentation quality is evidently more suitable for the end-user's downstream analysis.
\begin{figure}[h!]
    \centering
\includegraphics[width=\textwidth]{Figures/f17.pdf} 
    \caption{Example from Pancreas-B dataset. (a) Final 3D segmentation results produced by CellStitch; (b) Final 3D segmentation results produced by PlantSeg; (c) Raw fluorescence image for cell membranes. A specific layer of 2D segmentation is selected and viewed from the YZ plane. Note that the images have been adjusted for $z$-anisotropy.}
    \label{f44}
\end{figure}

\end{document}
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
