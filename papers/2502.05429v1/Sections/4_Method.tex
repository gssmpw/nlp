\section{SMC ATTACK PRIMITIVES}\label{sec:SMCAttack}

Self-modifying code (SMC) has been used to slow down the execution of instructions to leak TLS-DH key exchanges~\cite{aldaya2022hyperdegrade} and secret characters through speculative execution~\cite{ragab2021rage}. 
%
While most of the experiments primarily focus on a thread's effect on its own execution, our approach enables one thread to affect its sibling thread in the same physical core.
%
The combination of SMC conflicts and a side-channel attack strengthens the signal-to-noise ratio by either increasing attack resolution or extending speculative window size. 

In our study, we propose to leverage SMC conflicts to create novel cache side-channel attacks within the instruction cache used in x86 systems. First, we show that an attacker can create an SMC conflict with different instructions rather than only using the \texttt{store}
instructions as exemplified in~\cite{ragab2021rage}. Next, we demonstrate that the time it takes to execute the SMC creating instruction provides enough granularity to distinguish L1i cache hit and miss, which can be used by an attacker to perform new variants of Prime+Probe and Flush+Reload attacks. Finally, we reverse engineer the behavior of different SMC-creating instructions with performance counters to analyze the root cause of slow-down in both the front-end and back-end of Intel and AMD processors.
%\seonghun{Note that, while most of the experiments in this section examine a thread's effect on itself}

\subsection{Analyzing SMC Conflict with Various x86 Instructions}
%\noindent\textbf{Memory time.}
Modern x86 processors detect SMC occurrences during runtime with a sophisticated mechanism~\cite{kyker2003method,self_amd}. 
The main reason for the SMC conflict is the invalidation of cache lines already fetched speculatively due to the aggressive prefetching mechanism.
Hence, any instruction invalidating the fetched cache line could trigger the SMC mechanism. 
As a result, the execution pipeline, as well as the fetched instructions, are flushed to revert back to the immediate instruction after the conflict-creating instruction. 
In this section, we analyze different x86 instructions that could create SMC conflicts and measure the time it takes to execute those instructions. 

\noindent\textbf{Step 1: Preparing the L1i Cache.}
In our first step, we create a cache line (referred to as oracle) with read, write, and execute permissions. 
The cache line in this memory space is filled with 63 \textit{nop} instructions and 1 \textit{ret} instruction such that the instruction pointer returns to the caller function after the \textit{nop} instructions are executed as given in Listing~\ref{lst:l1i_prepare}. 
The TLB page for the oracle is created by storing a \textit{nop} instruction in the first byte to avoid any page walk that would affect the timing measurements. 
The same cache line is also flushed from the instruction cache. 
Next, the \textit{nop} instructions in the oracle are executed to validate the cache line and placed into the L1i cache. 
Finally, we place the \texttt{mfence} instruction to finish all the memory operations before the next instruction. 
These steps are required to load the cache line to the L1i cache, as this cache line will be modified in the next step to test whether different instructions create the SMC conflict.

\begin{lstlisting}[caption={L1 Instruction Cache State Preparation},language={[x86masm]Assembler}, label=lst:l1i_prepare, ,label=lst:l1i_prepare,basicstyle=\ttfamily\scriptsize, basewidth={.48em}, backgroundcolor=\color{white}, morekeywords={.data,.global,.quad,.space,.macro,.endm,mfence,rdtsc,shl,add,sub,pop,push,ret,lea,.text,.align,.rept,.endr}]
.align 0x1000
oracle_code_page:
  .rept 63
        nop
  .endr
  ret
movb $0x90, (uint64_t *)oracle_code_page 
clflush (uint64_t *)oracle_code_page
oracle_code_page()                       
mfence

\end{lstlisting}

\noindent\textbf{Step 2: Measuring the Execution Time for SMC-creating Instructions.} In this step, our purpose is to determine whether an instruction that modifies the L1i cache line could cause an SMC conflict or not. 
Our hypothesis is that if an instruction creates an SMC conflict, the execution time for that instruction will increase compared to executing the same instruction on a memory address that is not in the L1i cache. 
We tested nine different instructions to create the SMC conflict and measured the execution time of each instruction as given in Listing~\ref{lst:SMC_inst}.
Each option from Line 3 to Line 12 is tested separately.
In Listing~\ref{lst:SMC_inst}, the \texttt{rdi} operand is the base address of the oracle. Each instruction is tested with five different microarchitecture states for 10,000 times: the oracle code page address is in the L1i cache, L1d cache, L2 cache, LLC, and DRAM.  
The execution times for each instruction in the Cascade Lake microarchitecture is given in Figure~\ref{fig:CPUcycleTime_cascade}.

\lstset{style=mystyle}
\begin{lstlisting}[caption={Measuring the execution time of nine different x86 instructions. These instructions represent load, flush, store, lock, prefetch, execute, and cache line write-back operations.},language=C,label=lst:SMC_inst,basicstyle=\ttfamily\scriptsize, basewidth={.48em}, backgroundcolor=\color{white},morekeywords={.data,.global,.quad,.space,.macro,.endm,mfence,rdtsc,shl,add,sub,pop,push,ret,lea,.text,.align,.rept,.endr}]
mfence
rdtsc
(1) mov (%rdi), %rax      // Load operation
(2) clflush (%rdi)        // Clflush operation
(3) clflushopt (%rdi)     // Clflushopt operation
(4) movb $0x90, (%rdi)    // Store operation
(5) lock                  // Lock+Inc operation  
    incb (%rdi)
(6) prefetch (%rdi)       // Prefetch operation
(7) prefetchnta (%rdi)    // Prefetchnta operation
(8) call oracle_code_page // Execute operation
(9) clwb (%rdi)           // Cache line write back operation
mfence
rdtsc
\end{lstlisting}

The timing values indicate that \texttt{clflush, clflushopt, store, lock, prefetch, clwb} instructions create a distinguishable time difference between L1i cache hit and miss events. 
These instructions are also evaluated with performance counters (MACHINE\_CLEARS:SMC) to verify that a diverse set of x86 processors triggers the machine clear event with the SMC conflict.



\begin{figure}[t]
    \centering
    {
    
    \includegraphics[width=\linewidth]{Sections/Figures/cache_time_smack_mastik_cascadelake_all.png}
    %Source/Sections/Figures/cacheops_intel_cascadelake_v2.png}
    }
    \caption{CPU cycle time difference for various Probe strategies and Mastik~\cite{yarom2016mastik} method on Intel Cascade Lake microarchitecture.}
    \Description{CPU cycle time difference for various Probe strategies and Mastik~\cite{yarom2016mastik} method on Intel Cascade Lake microarchitecture.}
    \label{fig:CPUcycleTime_cascade}
\end{figure}

\noindent\textbf{Flush Instructions.} In the Cascade Lake microarchitecture, \texttt{clflush} and \texttt{clflushopt} instructions are executed in around 350 cycles if the memory operand is in the L1i cache. The timing value is considerably higher than other microarchitecture states such as LLC hit and DRAM access. Both flush instructions invalidate the cache line in the L1i cache, ensuring that future accesses to that memory address result in fresh data being loaded from the main memory. However, the instructions after the flush instruction may have already been fetched and executed in the back-end. Hence, both the front-end and back-end are flushed, leading to higher timing measurements. We observed that the time difference between the L1i cache hit and the LLC hit is more than 150 cycles. 
Similarly, flush instructions create 200 cycles difference between DRAM and L1i cache on AMD Ryzen 5 while the time difference reaches up to 300 cycles between L1i cache and LLC. The time difference between L1i cache, LLC, and DRAM states shows that both flush operations can be used to perform Prime+Probe and Flush+Reload attacks on AMD and Intel processors as described in Section~\ref{sec:casestudies}.

\noindent\textbf{Store Instruction.} The store instruction sets the dirty bit for the L1i cache line by updating its content, triggering the cache coherency mechanism to handle the inconsistency across the system's caches. We observed that even though the updated instruction is not executed, the SMC conflict is still created by a single update. The time it takes to execute the store operation is around 300 cycles, which is around 200 cycles more than the LLC hit. The time difference between DRAM and L1i cache is lower than other instructions, leading to 20 cycles difference. 
We also observed a significant time difference between the L1i cache hit and the LLC hit in the AMD Ryzen 5 processor with 150 cycles. While the time difference between L1i cache hit and DRAM access is close to each other, their execution time is still distinguishable with the \textit{rdtsc} instruction.

\noindent\textbf{Lock + Inc Instruction.} The \texttt{lock} instruction with the \texttt{incb} instruction modifies the \textit{nop} instruction stored in the oracle code page. Since this operation is executed atomically, the CPU detects this change on the executable instruction cache line as an SMC scenario. The execution time for the two instructions increases when the modified address is in the L1i cache, leading to 425 clock cycles on average. This pair of instructions has the highest execution time compared to other SMC-creating instructions, which is also consistent in other microarchitectures. While the time difference between the L1i cache and LLC hit is around 350 cycles, the time difference between the L1i cache and DRAM is around 150 cycles. 
In the AMD Ryzen 5 processor, all cache states are observable, and higher L1i cache hit cycle time shows that lock instruction creates an SMC conflict on AMD processors.

\noindent\textbf{Prefetch Instructions.} The \texttt{prefetch} instruction prefetches the content from the specified address and fetches it to the cache. %However, if the memory address is already in the L1i cache, it is possible that the content transferred from memory has been changed by another thread. 
If the memory address is already in the L1i cache and another thread modifies the L1i cache, it violates cache coherence mechanisms and causes core squash.
%
When a different instruction is fetched to that cache line from memory, the cache line invalidation triggers SMC conflicts.
%In case a different instruction is transferred to that cache line from memory, the cache line is invalidated. 
This invalidation process triggers the SMC conflict. We observe that the time difference between L1i cache hit and LLC hit is around 350 cycles while the time difference between DRAM and L1i cache hit is 150 cycles. Interestingly, an SMC conflict is not triggered when the \texttt{prefetchnta} instruction is executed in the Cascade Lake microarchitecture. 
In AMD processors, we observed no time difference between the L1i cache hit and the LLC hit, concluding that prefetch instructions have no effect on creating SMC conflicts on these processors.

\noindent\textbf{Clwb Instruction.} The cache line write back (\texttt{clwb}) instruction writes back the content of the cache line to the memory that contains the virtual address specified with the memory operand from any level of the cache hierarchy in the cache coherence domain. After this instruction, the cache line usually stays in the cache to avoid a cache miss on a subsequent access. However, hardware may choose to invalidate the line from the cache hierarchy. Since the state of the cache line changes with the \texttt{clwb} instruction, the processor triggers the machine clear operation. We observed that the time difference between the L1i cache hit and the LLC hit is around 170 cycles. The time difference between DRAM access and L1i cache hit is close to 100 cycles. 
In contrast, the same instruction is not treated as an SMC conflict in the AMD Ryzen 5 processor.

\noindent\textbf{Comparison with Mastik~\cite{yarom2016mastik} L1i cache attack.} The Mastik toolkit~\cite{yarom2016mastik} provides an L1i cache attack by utilizing the Prime+Probe technique. We reproduced the access time histogram (last row in Figure~\ref{fig:CPUcycleTime_cascade}) for cache levels and memory with the Mastik tool. We observe that the L1i cache incurs an average of 34 cycles, and the L2 cache takes an average of 35 cycles with the Prime+Probe attack implemented in the Mastik tool. Since the time difference between L1i and L2 caches is only 1-2 cycles, it is challenging to distinguish cache evictions due to applications running in the neighbor virtual core. The low time difference in the Mastik tool leads to more noisy measurements, as discussed further in the single-trace side-channel attack in Section~\ref{subsec:case3}. Furthermore, the time difference between the L1i cache and L3 cache is around 60 cycles, while Lock+Inc SMC version reaches up to 350 cycles time difference for the same cache levels.

\begin{figure*}[ht]
    \centering
    {
    \includegraphics[width=\linewidth]{Sections/Figures/counter_values_v11.png}
    }
    
    \caption{Reverse Engineering SMC behavior on (a), (c) Intel Cascade Lake and (b), (d) AMD Ryzen 5 microarchitectures by using the PAPI~\cite{PAPI} tool. The counters belonging to Intel and AMD processors are separated by the red line.}
    \Description{Reverse Engineering SMC behavior on (a), (c) Intel Cascade Lake and (b), (d) AMD Ryzen 5 microarchitectures by using the PAPI~\cite{PAPI} tool. The counters belonging to Intel and AMD processors are separated by the red line.}
    \label{fig:counter_values}
\end{figure*}

\subsection{Reverse Engineering SMC Behavior on x86}\label{sec:reverse}

We use hardware performance counters to analyze how x86 processors behave when an SMC conflict occurs. We leverage the Performance Application Programming Interface (PAPI)~\cite{PAPI} to track hardware events at the microarchitecture level with minimal noise. We determined several hardware events on Intel and AMD processors, providing more detailed insight into the characteristics of SMC conflict resolution as given in Figure~\ref{fig:counter_values}. 
We start the counter before the SMC-creating instruction and stop the counter immediately after the instruction 10,000 times and take the average.
We separately perform the reverse engineering efforts on Intel and AMD devices as available performance counters are different in these processors.

\noindent \textbf{Reverse Engineering Intel x86.} We perform our experiments on the Intel Cascade Lake microarchitecture. The \texttt{MACHINE\_CLEARS.COUNT} event counts the number of any type of machine clears occurring during an execution. This counter confirms that six instructions trigger the machine clear mechanism while \texttt{MACHINE\_CLEARS.SMC} indicates that \texttt{clflushopt} and \texttt{clwb} instructions trigger the SMC counter twice. We believe that this counter is not working accurately for all the SMC cases. 

In the second part, we analyze the time spent on resolving the SMC conflict. First, we monitor the \texttt{CYCLE\_\allowbreak ACTIVITY.\allowbreak STALLS\_\allowbreak TOTAL} counter. The total stall cycle values are the highest for \texttt{lock} and \texttt{clwb} instructions (up tp 580 cycles), aligning with our \texttt{rdtsc} measurements. 
Next, we analyze the number of cycles spent at the front-end to flush the decoded stream buffer (DSB) and other instructions currently fetched/decoded. The \texttt{FRONTEND\_RETIRED:IDQ\_4\_BUBBLES}  event counts the cycles in which the front-end did not deliver any $\mu$ops (4 bubbles) for a period determined by the \textit{fe\_thres} modifier and which was not interrupted by a back-end stall. 
We gradually increased the threshold and noticed that all instructions created 30 stall cycles on the front-end. 
When we check the \texttt{INT\_MISC.CLEAR\_RESTEER\_CYCLES} counter, we observed that the processor waits for around 35-40 cycles to issue the new instructions at the back-end after the machine clears. We expect that new instructions take the legacy path and go through the fetch and decode stage, leading to additional cycles to reach the back-end again. 

The SMC conflict behaves like a fence instruction by serializing the instructions after the conflict is detected. For the back-end stall, we leverage the \texttt{PARTIAL\_RAT\_\allowbreak STALLS.\allowbreak SCOREBOARD} counter, reporting the number of cycles the CPU issue-pipeline was stalled due to serializing operations. The counter value is monitored with and without the attack instruction, and we observed that the processor spends around 200 cycles to complete the serialization in the back-end for the \texttt{store} instruction after the counter overhead is removed (around 100 cycles) in Figure~\ref{fig:counter_values}.
All in all, Intel processors spend more cycles in the back-end to revert back to the correct order of instructions.

\noindent \textbf{Reverse Engineering AMD x86.} AMD processors provide a different set of counters compared to Intel processors. For example, AMD architectures have no option to observe machine clears. Instead, we focus on the instruction cache and pipeline stall-related counters to observe the effects of our SMC code snippets. We observed that \texttt{clflush}, \texttt{clflushopt}, \texttt{store}, and \texttt{lock} instructions create distinguishable time differences between L1i cache and LLC accesses. The time difference mainly comes from the back-end unit as the \texttt{INSTRUCTION\_PIPE\_STALL:BACK\_PRESSURE} counter increases significantly. We observed that the counter reports around 500 cycles of stall occurring in the back-end unit for \texttt{clflush} operations, which matches with the timing values. The effect of SMC conflict on the instruction cache is also verified with the \texttt{INSTRUCTION\_\allowbreak CACHE\_\allowbreak LINES\_\allowbreak INVALIDATED:FILL\_\allowbreak INVALIDATED} counter. As expected, the counter value increases by one for the SMC-creating instruction, stating that the cache line is invalidated with the \texttt{store} and \texttt{lock} instructions. Then, the invalidated cache line is requested from the L2 cache as the \texttt{CYCLES\_\allowbreak WITH\_\allowbreak FILL\_\allowbreak PENDING\_\allowbreak FROM\_\allowbreak L2:L2\_\allowbreak FILL\_\allowbreak BUSY} counter increments with the same amount we observed from the back-end counter, which shows that requesting cache lines from L2 cache is considered as back-end stall in AMD counters. For \texttt{clflush} and \texttt{clflushopt} operations, the counter has no increase because these operations do not bring the cache line to the cache. Due to the lack of precise counters on AMD processors, we identified a few counters to analyze how AMD processors handle SMC conflicts.

\noindent \textbf{SMC Root Cause Analysis.} %\seonghun{need to change this part!} 
%It is still questionable whether the SMC-creating instruction execution time is higher due to the instruction itself or the core freezes completely until the instruction is resolved at the architecture level. 
%
Intel patent on the SMC detection mechanism describes that the instruction before the SMC-creating instruction is the last committed one to the register file, while all the prefetched instructions after the SMC-creating instruction are flushed from the pipeline~\cite{intel_smc}. Then, the modified instruction cache line is fetched from memory to fix the memory inconsistency. Hence, the instructions after the SMC-creating instruction are still executed in an out-of-order way because the new instruction is not determined yet due to the high access time from the main memory. This hypothesis is also supported by Ragab et al.~\cite{ragab2021rage} as they show that speculative loads are still executed in their test case (Pg. 6 Listing 1) with the increasing transient window size.
We also observed the same high timings for SMC-creating instructions when \textit{mfence} instructions are not placed in the code snippet (Listing~\ref{lst:SMC_inst}), indicating that timing measurements do not rely on the delayed memory operations. Since the fetch, decode, and execute pipelines are shared between two sibling threads in the same physical core, and these pipelines are flushed when the SMC conflict is detected, the sibling thread is stalled for each SMC conflict detection. As a result, the observed high time differences for each SMC-creating instruction are due to the time spent on the executed instructions, and the main reason is the high instruction transfer time from memory to the L1i cache.
However, it is important to acknowledge that the SMC squash effect might also influence the measured delay. Specifically, if the SMC-creating instruction commits quickly before the pipeline is squashed, the timing utilizing the second \textit{rdtsc} timer (line 14 in Listing~\ref{lst:SMC_inst}) could reflect the delay and induce more timing discrepancy as \texttt{Store} and \texttt{Lock} instruction on Intel CPUs creates more delays as depicted in Figure~\ref{fig:CPUcycleTime_cascade}.

Our findings in Figure~\ref{fig:CPUcycleTime_cascade} also support this claim as we observe that transferring instructions from memory to the L1i cache takes 250 cycles without SMC conflict (a load operation implemented in the Mastik Tool). However, additional SMC detection mechanisms and pipeline flush processes introduce delays in L1i cache operations, resulting in more than 250 cycles for each instruction creating the SMC conflict (green bars in Figure~\ref{fig:CPUcycleTime_cascade}).

\noindent\textbf{Outcome.} In summary, invalidating a cache line in the L1i cache creates inconsistency issues between the memory and the L1i cache, even though the content of the instruction cache line may not be modified. The inconsistency is resolved by stopping instruction fetching and clearing the instructions already transferred to the execution ports and ROB. Both Intel and AMD processors spend more time flushing the back-end unit compared to the front-end while resolving the SMC conflict. Each processor demonstrates a different timing behavior for each instruction, leading to a unique behavior. We also observed that a single SMC conflict leads to a 235-cycle slowdown in the sibling thread.