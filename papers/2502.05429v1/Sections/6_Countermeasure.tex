\section{Countermeasures}\label{sec:countermeasures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Dynamic Detection}\label{sec:dynamic_detection}

Both Intel and AMD processors implement a diverse set of hardware performance counters to monitor running processes~\cite{zhang2016cloudradar,briongos2018cacheshield,gulmezoglu2019fortuneteller}. We create a dynamic detection tool by profiling diverse benign applications from the Phoronix-benchmarks suite~\cite{OpenBenchmarkingTests}. %We selected 20 benchmark tests across different categories to establish a high diversity workload.
In the profiled counter list, we include counters used to detect cache attacks and Spectre attacks~\cite{zhang2016cloudradar,briongos2018cacheshield,gulmezoglu2019fortuneteller} as well as our proposed counters related to SMC conflicts mentioned in Section~\ref{sec:SMCAttack}. 
We observed that performance counters used in previous studies are insufficient to detect SMC-based attacks. Specifically, Prime+iProbe attacks are less impacted by mispredicted branch instruction counters than the Spectre attacks, which directly induce branch mispredictions. Consequently, the F-score of the detection model for the Prime+iProbe attacks using \texttt{BR\_\allowbreak MISP\_\allowbreak RETIRED.ALL\_\allowbreak BRANCHES} is 0.8000, which is lower than 0.9215 achieved for Flush+iReload attacks. Furthermore, the LLC miss counter event~\cite{gulmezoglu2019fortuneteller} faces a challenge in detecting our \texttt{Prime+iProbe} activity since our attack evicts cache lines from the L1i cache and has minimal effect in LLC, resulting in a 0.7679 F1 score. Hence, we selected counters more related to SMC conflicts on Intel architectures.

We collected 20 benign benchmarks and 12 attack executions on the Intel Cascade Lake microarchitecture. For the malicious dataset, we profile \texttt{Prime+iProbe} and \texttt{Flush+iReload} variants, triggering SMC conflicts (Table~\ref{table:Spectre_attacks}). The datasets are collected for a duration of 10 seconds with 100 ms resolution, generating 100 measurements for both execution types. 
%We trained Support Vector Machine (SVM), Random Forest (RF), and Decision Tree (DT) models using an open-source \texttt{sklearn}~\cite{scikit-learn} machine learning library to evaluate the test accuracy, F1 score, and false-positive rate (FPR). 
The train and test datasets are split with 80\% and 20\% ratios for the evaluation. We identified that \texttt{cycle\_\allowbreak activity.\allowbreak stalls\_\allowbreak total}, \texttt{machine\_clears.count}, and \texttt{machine\_\allowbreak clears.\allowbreak smc} counters yield high F1 scores. 6 different \texttt{Prime+iProbe} variants are detected with 99.36\% accuracy and F-score of 0.9870 and 0.85\% FPR by employing the \texttt{machine\_clears.smc} counter.
The reason behind it's not 100\% accurate is that some of the measurements from the \texttt{amg} benchmark are treated as malicious applications since \texttt{amg} benchmark creates significant SMC behavior, unlike other benign applications. Additionally, \texttt{Flush+iReload} attacks are detected with 100\% accuracy by utilizing \texttt{machine\_clears.smc} counter. Our results demonstrate that employing an SMC-related detection tool can successfully distinguish \texttt{Prime+iProbe} and \texttt{ISpectre} attacks. 

\subsection{Software and Hardware Countermeasures}
The vulnerabilities described in this study can be avoided by transforming the vulnerable implementation to a \emph{constant-time} version, i.e., without secret-dependent branches or memory accesses.
\texttt{Raccoon}~\cite{rane2015raccoon}, \texttt{Escort}~\cite{rane2016secure}, and \texttt{EncLang}~\cite{sinha2017compiler} offer methods to automatically transform existing code into a constant-time representation. However, they require source code annotation and may significantly impact performance, e.g., when ORAMs are employed. Additionally, there has been significant research in implementing automated verification of constant-time properties. Tools like \texttt{ct-verif}~\cite{almeida2016verifying} and \texttt{CacheD} use static code analysis or symbolic execution to prove the absence of side-channel vulnerabilities. Since this does not scale well with increasing implementation sizes, various dynamic approaches have been proposed, e.g., \texttt{ctgrind}~\cite{langley2010ctgrind}, which simply keeps secret memory uninitialized. \texttt{DATA}~\cite{weiser2018data} and \texttt{MicroWalk}~\cite{wichelmann2018microwalk} evaluate implementations by comparing whole execution traces and quantifying leakage while pinpointing leaking instructions. 
A straightforward but costly countermeasure is disabling SMT altogether. This way, core resources like pipeline and L1 caches are not shared, which are exploited by the described attacks and many others~\cite{aciiccmez2007yet,aciiccmez2010new}. However, this will also lead to significant performance degradation. 
%Alternatively, sibling threads in the same core can be assigned to the same process or security domain to isolate different applications from each other.
%To reduce these costs, one could split up specific shared resources, such as parts of the pipeline, such that hazards like SMC conflicts have a reduced impact on the sibling core.
