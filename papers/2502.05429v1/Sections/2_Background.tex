\section{Background}\label{sec:background}
Modern CPUs comprise multiple cores, a shared last-level cache (LLC), and a coherent memory subsystem. 
The core is designed based on a multi-stage pipeline in which operations are highly synchronized by the use of out-of-order and speculative execution techniques. We provide a brief background on different microarchitecture components.

\noindent\textbf{Front-end Unit} fetches program instructions from the instruction cache and places them into a queue. 
In a complex instruction set architecture, each instruction is first decoded into smaller micro-operations ($\mu$OPs). 
The decoded $\mu$OPs may be cached in the $\mu$OP cache to be reused. 
The core fetches a long sequence of instructions ahead of time, 
but it also fetches instructions that are not sequentially available in the program with the help of the branch predictor.
The branch predictor maintains the history of target branches and decisions to facilitate speculative fetching and execution of instructions from latent control flows.

\noindent\textbf{Back-end Unit} retrieves the $\mu$OPs from the allocation queue and then allocates resources while the instructions are waiting in the reorder buffer (ROB) to be completed.
In parallel, the scheduler sends the $\mu$OPs to various execution units depending on the availability of resources.
$\mu$OPs completed correctly will be retired by obeying the correct ordering of the program's instruction. 
If an error is detected in the ROB for an operation, the wrong outcome will be discarded, and corresponding $\mu$OPs will be rescheduled. 
%Instructions that perform memory operations access the memory through the memory subsystem.

\noindent\textbf{Memory Subsystem} consists of multiple levels of caches and buffers. The first level cache, L1 cache, is the fastest and smallest compared to other levels. 
It is separated into data (L1d) and instruction (L1i) caches where there is no interference between each other.
While the L1d cache keeps the data cache lines, the L1i cache stores the instruction cache lines. Intel processors have 64 sets for each L1 cache, and the number of ways is 8.
The second-level (L2) cache keeps both data and instruction cache lines. 
It is relatively slower than the L1 cache, but the L2 cache has a much higher volume. The last-level cache (LLC) is shared among all the physical cores, leading to a slower cache access time while having a larger capacity.


\noindent\textbf{Self-Modifying Code (SMC)} refers to programs that dynamically alter their instructions while they are executing~\cite{cai2007certified}. Traditionally, SMC has been used to optimize performance and manage resources efficiently, which requires high flexibility. SMC introduces complexity into the pipeline process because it changes the execution path. For instance, the fetch unit prefetches instructions stored in the L1i cache to the pipeline while enhancing CPU performance. However, when instructions modify themselves, this can lead to incorrect program behavior or system crashes. To handle this, processors must utilize various methods and apparatuses~\cite{zaidi2001system,murty2002apparatus,kyker2003method,botacin2020self} to detect SMC behaviors when instruction modifies the code. Once the processor detects SMC behavior, it flushes their entire pipeline, degrading the performance of all running tasks in the same core. Consequently, SMC conflicts are tied to the L1i cache because the SMC detection mechanism monitors the L1i cache region. The L1i cache is part of the front-end, whereas the L2 cache and LLC aren't in the front-end but in the back-end. Therefore, if SMC behavior modifies an instruction, the modification is only detected in the L1i cache.