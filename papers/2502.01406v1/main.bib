@misc{UCI_gender_by_name,
  title        = {{Gender by Name}},
  year         = {2020},
  howpublished = {UCI Machine Learning Repository},
url={https://doi.org/10.24432/C55G7X},
}


@article{nemani2023gender,
  title={Gender bias in transformer models: A comprehensive survey},
  author={Nemani, Praneeth and Joel, Yericherla Deepak and Vijay, Palla and Liza, Farhana Ferdousi},
  journal={arXiv preprint arXiv:2306.10530},
  year={2023}
}

@inproceedings{meade2022empiricalsurveyeffectivenessdebiasing,
    title = "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    author = "Meade, Nicholas  and Poole-Dayan, Elinor  and Reddy, Siva",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.132",
    doi = "10.18653/v1/2022.acl-long.132",
    pages = "1878--1898",
}

@misc{gradcam,
      title={Grad-CAM: Why did you say that?}, 
      author={Ramprasaath R Selvaraju and Abhishek Das and Ramakrishna Vedantam and Michael Cogswell and Devi Parikh and Dhruv Batra},
      year={2017},
      eprint={1611.07450},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1611.07450}, 
}

@inproceedings{integratedGradients,
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
title = {Axiomatic attribution for deep networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3319–3328},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446/",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
}

@inproceedings{seat,
    title = "On Measuring Social Biases in Sentence Encoders",
    author = "May, Chandler  and
      Wang, Alex  and
      Bordia, Shikha  and
      Bowman, Samuel R.  and
      Rudinger, Rachel",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1063/",
    doi = "10.18653/v1/N19-1063",
    pages = "622--628",
}


@article{weat,
    author = {Aylin Caliskan  and Joanna J. Bryson  and Arvind Narayanan },
    title = {Semantics derived automatically from language corpora contain human-like biases},
    journal = {Science},
    volume = {356},
    number = {6334},
    pages = {183-186},
    year = {2017},
    doi = {10.1126/science.aal4230},
    URL = {https://www.science.org/doi/abs/10.1126/science.aal4230},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.aal4230},
    abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.}
}


@inproceedings{crows,
    title = "{C}row{S}-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.154/",
    doi = "10.18653/v1/2020.emnlp-main.154",
    pages = "1953--1967",
    abstract = "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
}


@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11264--11272},
  year={2019}
}

@misc{ancona2018betterunderstandinggradientbasedattribution,
      title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks}, 
      author={Marco Ancona and Enea Ceolini and Cengiz Öztireli and Markus Gross},
      year={2018},
      eprint={1711.06104},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.06104}, 
}


@inproceedings{chen2020adapting,
  title={Adapting {G}rad-{CAM} for embedding networks},
  author={Chen, Lei and Chen, Jianhui and Hajimirsadeghi, Hossein and Mori, Greg},
  booktitle={proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2794--2803},
  year={2020},
  eprint={2001.06538},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2001.06538}, 
}

@article{selvaraju2020grad,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01228-7},
doi = {10.1007/s11263-019-01228-7},
abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach—Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at , along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265–290. Springer, 2015) () and a video at .},
journal = {Int. J. Comput. Vision},
month = feb,
pages = {336–359},
numpages = {24},
keywords = {Grad-CAM, Visual explanations, Visualizations, Explanations, Interpretability, Transparency}
}

@inproceedings{aribandi-etal-2021-reliable,
    title = "How Reliable are Model Diagnostics?",
    author = "Aribandi, Vamsi  and
      Tay, Yi  and
      Metzler, Donald",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.155",
    doi = "10.18653/v1/2021.findings-acl.155",
    pages = "1778--1785",
}

@inproceedings{lundstrom2022rigorous,
  title={A rigorous study of integrated gradients method and extensions to internal neuron attributions},
  author={Lundstrom, Daniel D and Huang, Tianjian and Razaviyayn, Meisam},
  booktitle={International Conference on Machine Learning},
  pages={14485--14508},
  year={2022},
  organization={PMLR},
doi = {10.48550/arXiv.2202.11912},
}




@article{bricken2023monosemanticity,
   title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
   author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
   year={2023},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@INPROCEEDINGS {bookcorpus,
author = { Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja },
booktitle = { 2015 IEEE International Conference on Computer Vision (ICCV) },
title = {{ Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books }},
year = {2015},
volume = {},
ISSN = {2380-7504},
pages = {19-27},
abstract = { Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for. },
keywords = {Motion pictures;Visualization;Videos;Semantics;Grounding;Voltage control;Roads},
doi = {10.1109/ICCV.2015.11},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.11},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Dec}

@inproceedings{cda,
    title = "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    author = "Zmigrod, Ran  and
      Mielke, Sabrina J.  and
      Wallach, Hanna  and
      Cotterell, Ryan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1161",
    doi = "10.18653/v1/P19-1161",
    pages = "1651--1661",
    abstract = "Gender stereotypes are manifest in most of the world{'}s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82{\%} and 73{\%} at the level of tags and accuracies of 90{\%} and 87{\%} at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
}

@article{movementPruning,
      title={Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning}, 
      author={Przemyslaw Joniak and Akiko Aizawa},
      year={2022},
      eprint={2207.02463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.02463}, 
journal={arXiv preprint arXiv:2207.02463},
}

@inproceedings{movementPruningOrig,
author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M.},
title = {Movement pruning: adaptive sparsity by fine-tuning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3\% of the model parameters.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1711},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20},
url={https://dl.acm.org/doi/10.5555/3495724.3497435}
}

@article{dropout,
      title={Measuring and Reducing Gendered Correlations in Pre-trained Models}, 
      author={Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Ed Chi and Slav Petrov},
      year={2021},
      eprint={2010.06032},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.06032}, 
journal={arXiv preprint arXiv:2010.06032},
}

@article{selfDebias,
    author = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
    title = "{Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1408-1424},
    year = {2021},
    month = {12},
    abstract = "{⚠ This paper contains prompts and model outputs that are offensive in nature.When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00434},
    url = {https://doi.org/10.1162/tacl\_a\_00434},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00434/1979270/tacl\_a\_00434.pdf},
}

@inproceedings{SentenceDebias,
    title = "Towards Debiasing Sentence Representations",
    author = "Liang, Paul Pu  and
      Li, Irene Mengze  and
      Zheng, Emily  and
      Lim, Yao Chong  and
      Salakhutdinov, Ruslan  and
      Morency, Louis-Philippe",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.488",
    doi = "10.18653/v1/2020.acl-main.488",
    pages = "5502--5515",
    abstract = "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",
}

@article{zmac029,
    author = {Jones-Jang, S Mo and Park, Yong Jin},
    title = {How do people react to AI failure? Automation bias, algorithmic aversion, and perceived controllability},
    journal = {Journal of Computer-Mediated Communication},
    volume = {28},
    number = {1},
    pages = {zmac029},
    year = {2022},
    month = {11},
    abstract = {AI can make mistakes and cause unfavorable consequences. It is important to know how people react to such AI-driven negative consequences and subsequently evaluate the fairness of AI’s decisions. This study theorizes and empirically tests two psychological mechanisms that explain the process: (a) heuristic expectations of AI’s consistent performance (automation bias) and subsequent frustration of unfulfilled expectations (algorithmic aversion) and (b) heuristic perceptions of AI’s controllability over negative results. Our findings from two experimental studies reveal that these two mechanisms work in an opposite direction. First, participants tend to display more sensitive responses to AI’s inconsistent performance and thus make more punitive assessments of AI’s decision fairness, when compared to responses to human experts. Second, as participants perceive AI has less control over unfavorable outcomes than human experts, they are more tolerant in their assessments of AI.As artificial intelligence (AI) is replacing important decisions that used to be made by human experts, it is important to study how people react to undesirable outcomes caused by AI-made decisions. This study aims to identify two critical psychological processes that explain how people evaluate AI-driven failures. The first mechanism is that people have high expectations of AI’s consistent performance (called automation bias) and then are frustrated by unsatisfactory outcomes (called algorithmic aversion). The second mechanism is that people perceive that AI has less control over negative outcomes, compared to humans, which in turn, reduces negative evaluations of AI. To demonstrate these two ideas, we used two online experiments. Participants were exposed to several scenarios where they experienced undesirable outcomes from either AI or human experts.},
    issn = {1083-6101},
    doi = {10.1093/jcmc/zmac029},
    url = {https://doi.org/10.1093/jcmc/zmac029},
    eprint = {https://academic.oup.com/jcmc/article-pdf/28/1/zmac029/47050128/zmac029.pdf},
}





@article{jiang2024selfdisclosureaiparadoxtrust,
      title={Self-Disclosure to AI: The Paradox of Trust and Vulnerability in Human-Machine Interactions}, 
      author={Zoe Zhiqiu Jiang},
      year={2024},
      eprint={2412.20564},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2412.20564}, 
journal={arXiv preprint arXiv:2412.20564},

}

@inproceedings{inlp,
    title = "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Gonen, Hila  and
      Twiton, Michael  and
      Goldberg, Yoav",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.647",
    doi = "10.18653/v1/2020.acl-main.647",
    pages = "7237--7256",
    abstract = "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
}

@misc{li2021detectinggenderbiastransformerbased,
      title={Detecting Gender Bias in Transformer-based Models: A Case Study on {BERT}}, 
      author={Bingbing Li and Hongwu Peng and Rajat Sainju and Junhuan Yang and Lei Yang and Yueying Liang and Weiwen Jiang and Binghui Wang and Hang Liu and Caiwen Ding},
      year={2021},
      eprint={2110.15733},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{pearson,
    title={Pearson correlation coefficient},
    author={Cohen, Israel and Huang, Yiteng and Chen, Jingdong and Benesty, Jacob and Benesty, Jacob and Chen, Jingdong and Huang, Yiteng and Cohen, Israel},
    year = {2009},
    month = {04},
    pages = {1-4},
    volume = {2},
    isbn = {978-3-642-00295-3},
    journal = {Noise Reduction in Speech Processing},
    doi = {10.1007/978-3-642-00296-0_5}
}

@book{davison1997bootstrap, 
place={Cambridge}, 
series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
title={Bootstrap Methods and their Application}, 
publisher={Cambridge University Press}, 
author={Davison, A. C. and Hinkley, D. V.}, 
year={1997}, 
collection={Cambridge Series in Statistical and Probabilistic Mathematics},
doi={10.1017/CBO9780511802843}
}

@article{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{roberta,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{distilbert,
  author       = {Victor Sanh and
                  Lysandre Debut and
                  Julien Chaumond and
                  Thomas Wolf},
  title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                  and lighter},
  journal      = {CoRR},
  volume       = {abs/1910.01108},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01108},
  eprinttype    = {arXiv},
  eprint       = {1910.01108},
  timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{anthropic,
   title={Scaling Monosemanticity: Extracting Interpretable Features from {C}laude 3 {S}onnet},
   author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
   year={2024},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}


@misc{anthropic2024claude3,
  author       = {Anthropic},
  title        = {The {C}laude 3 Model Family: {O}pus, {S}onnet, {H}aiku},
  year         = {2024},
  howpublished = {\url{https://paperswithcode.com/paper/the-claude-3-model-family-opus-sonnet-haiku}},
  note         = {Accessed: 2024-12-12}
}


@inproceedings{saliencyMaps,
  author       = {Karen Simonyan and
                  Andrea Vedaldi and
                  Andrew Zisserman},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Deep Inside Convolutional Networks: Visualising Image Classification
                  Models and Saliency Maps},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6034},
  timestamp    = {Thu, 25 Jul 2019 14:36:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SimonyanVZ13.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jermyn2022engineeringmonosemanticitytoymodels,
      title={Engineering Monosemanticity in Toy Models}, 
      author={Adam S. Jermyn and Nicholas Schiefer and Evan Hubinger},
      year={2022},
      eprint={2211.09169},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.09169}, 
journal={arXiv preprint arXiv:2211.09169},
}

@article{actMax,
author = {Erhan, Dumitru and Bengio, Y. and Courville, Aaron and Vincent, Pascal},
year = {2009},
month = {01},
pages = {},
title = {Visualizing Higher-Layer Features of a Deep Network},
journal = {Technical Report, Université de Montréal}
}

@article{introCircuit,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  url = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{NEMANI2024100047,
title = {Gender bias in transformers: A comprehensive review of detection and mitigation strategies},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100047},
year = {2024},
issn = {2949-7191},
doi = {10.1016/j.nlp.2023.100047},
url = {https://doi.org/10.1016/j.nlp.2023.100047},
author = {Praneeth Nemani and Yericherla Deepak Joel and Palla Vijay and Farhana Ferdouzi Liza},
keywords = {Gender bias, Transformer models, Bias mitigation, Binary gender assumption, Self-attention},
abstract = {Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals’ lives. This paper presents a comprehensive survey that explores gender bias in Transformer models from a linguistic perspective. While the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to measure and evaluate this bias effectively. Our survey critically examines the existing literature on gender bias in Transformers, shedding light on the diverse methodologies and metrics employed to assess bias. Several limitations in current approaches to measuring gender bias in Transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. Furthermore, our survey delves into the potential ramifications of gender bias in Transformers for downstream applications, including dialogue systems and machine translation. We underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. This paper serves as a comprehensive overview of gender bias in Transformer models, providing novel insights and offering valuable directions for future research in this critical domain.}
}

@inproceedings{tal-etal-2022-fewer,
    title = "Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias",
    author = "Tal, Yarden  and
      Magar, Inbal  and
      Schwartz, Roy",
    editor = "Hardmeier, Christian  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta R.  and
      Stanovsky, Gabriel  and
      Gonen, Hila",
    booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gebnlp-1.13",
    doi = "10.18653/v1/2022.gebnlp-1.13",
    pages = "112--120",
    abstract = "The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.",
}

@inproceedings{nadeem2020gender,
  title={Gender Bias in {AI}: A Review of Contributing Factors and Mitigating Strategies},
  author={Nadeem, Ayesha and Abedin, Babak and Marjanovic, Olivera},
  booktitle={ACIS 2020 Proceedings},
  year={2020},
  url={https://aisel.aisnet.org/acis2020/27}
}

@incollection{dastin2022amazon,
  title={Amazon scraps secret {AI} recruiting tool that showed bias against women},
  author={Dastin, Jeffrey},
  booktitle={Ethics of data and analytics},
  pages={296--299},
  year={2022},
  publisher={Auerbach Publications},
  isbn={9781003278290},
}
@inproceedings{li2020train,
author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joseph E.},
title = {Train large, then compress: rethinking model size for efficient training and inference of transformers},
year = {2020},
publisher = {JMLR.org},
abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {553},
numpages = {11},
series = {ICML'20}
}

@InProceedings{buolamwini2018gender,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}


@article{ferrara2023fairness,
  title={Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies},
  author={Ferrara, Emilio},
  journal={Sci},
  volume={6},
  number={1},
  pages={3},
  year={2023},
  publisher={MDPI},
doi={10.3390/sci6010003},
URL = {https://www.mdpi.com/2413-4155/6/1/3},
ISSN = {2413-4155},
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year=2019,
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@Inbook{cda-orig,
author="Lu, Kaiji
and Mardziel, Piotr
and Wu, Fangjing
and Amancharla, Preetam
and Datta, Anupam",
title="Gender Bias in Neural Natural Language Processing",
bookTitle="Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="189--202",
abstract="We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark data sets finds significant gender bias in how models view occupations. We then mitigate bias with counterfactual data augmentation (CDA): a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.",
isbn="978-3-030-62077-6",
doi="10.1007/978-3-030-62077-6_14",
url="https://doi.org/10.1007/978-3-030-62077-6_14"
}


@inproceedings{stereoset,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at \url{https://stereoset.mit.edu}.",
}