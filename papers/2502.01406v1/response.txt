\section{Related Work}
\label{sec:rel-work}
This section reviews interpretable feature learning, explores existing methods for gender-debiasing transformer models, and examines techniques for measuring gender bias.

\subsection{Interpretable Feature Learning}

Interpretable feature learning aims to identify and understand the internal representations of neural networks, focusing on how individual neurons or groups of neurons relate to specific concepts.
Early methods focused on visualizing learned features through saliency maps **Zeiler et al., "Visualizing and Understanding Convolutional Neural Networks"** and activation maximization ____**, which highlighted the impact of input features on model predictions.
Recent advancements focus on separating networks into semantically meaningful components like individual neurons or circuits **Montavon et al., "Layer-wise relevance propagation for deep neural networks"**.
Research into \emph{monosemantic} neurons, which correspond to a single natural \emph{feature}, provides clearer and more interpretable insights compared to \emph{polysemantic} neurons ____.
____ proposed to learn unsupervised a sparse autoencoder that extracts interpretable features in a high dimensional feature space, which are analyzed for semantical meaning based on their behavior. Follow-up studies ____ improved scalability and identified  specific features such as a gender-bias awareness feature in Claude 3 Sonnet ____**Barasch et al., "Learning to Debias with Uncertainty"**. 
However, this approach requires learning numerous potential features and testing for meaningful interpretations, leaving it uncertain whether a desired feature will actually arise.


\subsection{Transformer Gender-Debiasing Techniques}\label{sec:rel-work-debiasing}

Various techniques have been proposed to mitigate gender bias in transformer language models, either by creating debiased models or through post-processing adjustments.

\gls{cda}____ % cda-orig was published first as pre-print in 2018, but officially published later, that's why it is mentioned here later due to submission guidelines
is a straightforward method which swaps gender-related words consistently within a training corpus (e.g., replacing \emph{he}/\emph{she}), enabling further training on a balanced dataset. %This balanced dataset can then be used to (further) train a model to achieve a debiased model. 
____ found experimentally that increasing \dropout\ during pre-training effectively reduces gender bias.
____ applied Movement Pruning ____ to reduce gender bias in language models by pruning weights that minimally impact neutral tasks, effectively removing gender bias associations while maintaining model performance.

The \gls{inlp} ____ is a post-processing method for gender-debiasing by iteratively training a linear classifier to detect gender based on model embeddings and subtracting the classifier's nullspace from the embeddings to remove gender-related information.
Similarly, \sentencedebias\ ____ estimates a linear subspace associated with gender bias by using \gls{cda} to generate sentence pairs with swapped gendered terms. % (e.g., \emph{he}/\emph{she}). 
Sentence embeddings are then debiased by subtracting their projection onto this subspace.
\selfdebias____ addresses bias in generated text by running inference with and without a bias-encouraging prefix, then downweighting tokens favored in the biased version. 
However, this task-specific approach is unsuitable for downstream tasks like \acrshort{glue} ____**Wang et al., "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"**.

\subsection{Techniques for Measuring Gender Bias} \label{sec:measuring-gender-bias}

\gls{seat}____ extends \gls{weat} ____ by using sentence templates to evaluate social biases in encoder models. It compares association strengths between embeddings of predefined attribute (e.g., gender-specific names) and target sets (e.g., stereotypical professions) using cosine similarity. 
Bias is expressed as an \emph{effect size}, where larger values indicate stronger bias. 

StereoSet ____ is a benchmark dataset with context-rich sentences for intrasentence and intersentence tasks.
This study focuses on the intrasentence task, where a sentence (e.g., \emph{Girls tend to be more \texttt{[MASK]} than boys}) requires the model to predict the masked word from three options: stereotypical (e.g., \emph{soft}), anti-sterotypical (e.g., \emph{determined}), and meaningless (e.g., \emph{fish}).
Two metrics are considered: 
\begin{enumerate*}[label=\textbf{\arabic*)}] 
\item \gls{lms}, which measures the proportion of meaningful (stereotypical or anti-stereotypical) options chosen over meaningless ones, reflecting the modelâ€™s language understanding. 
\item \gls{ss}, which quantifies bias as the proportion of stereotypical options selected over anti-stereotypical ones. A balanced model achieves $50\%$. \end{enumerate*}


\gls{crows} ____ is a crowdsourced dataset consisting of pairs of sentences: one expressing a stereotype (e.g., \emph{Woman don't know how to drive}), and the other its anti-stereotypical counterpart (e.g., \emph{Man know how to drive}). A bias score is computed considering the model's preference for one sentence over the other, similar to \gls{ss}. 


____ analyze the attention associations between gendered pronouns (e.g., \emph{she}) and occupations (e.g., \emph{nurse}) in transformer models, , using gender-swapped sentences (e.g., replace \emph{he} by \emph{she}). The attention scores between the gender-swapped pronouns and the occupation are then compared to identify gender bias on attention head level. However, the approach does not compute a model-specific, aggregated bias score usable for comparison.