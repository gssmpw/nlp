\section{Proof}

\subsection{Definitions}


\begin{definition}[Negative Log-Likelihood (NLL)]
\label{def:nll}
Consider a dataset 
\(\mathcal{D} = \{(x_i,y_i)\}_{i=1}^N\) and a model \(p_\theta(y \mid x)\) 
parameterized by \(\theta\). The \emph{Negative Log-Likelihood} is defined as 
the average negative log-probability of the observed data:
\[
\mathrm{NLL}(\mathcal{D}; \theta)
\;=\;
-\frac{1}{N}\sum_{i=1}^N \log p_\theta\bigl(y_i \mid x_i\bigr).
\]
\end{definition}


\begin{remark}
The Negative Log-Likelihood can be interpreted as the empirical \emph{Cross-Entropy} between the empirical distribution \( p \) (from \( \mathcal{D} \)) and the model distribution \( q_\theta \) (from \( p_\theta \)):
\[
\mathrm{NLL}(\mathcal{D}; \theta) \approx \mathrm{CE}\bigl(p, q_\theta\bigr)
\;=\;
H(p) + D_\mathrm{KL}(p\|q_\theta),
\]
where \( \mathrm{CE}(p, q_\theta) = -\sum_x p(x) \log q_\theta(x) \) is the cross-entropy, \( H(p) \) is the entropy of \( p \), and \( D_\mathrm{KL} \) is the Kullback--Leibler divergence. Minimizing \( \mathrm{NLL} \) pushes \( q_\theta \) closer to \( p \).
\end{remark}


\begin{definition}[Entropy~\cite{shannon1948mathematical}]
Entropy is an information-theoretic measure that quantifies the uncertainty or impurity in a probability distribution. For a classification problem with \( C \) classes, the entropy of a predictive distribution \( p_\theta(y \mid x) \) is defined as~\cite{settles2009active}:
\[
H(Y \mid x; \theta) 
= 
-\sum_{y=1}^C p_\theta(y \mid x) \log p_\theta(y \mid x),
\]
where \( p_\theta(y \mid x) \) is the predicted probability of class \( y \) for input \( x \), parameterized by \(\theta\). This measure captures the model's uncertainty over the predicted classes for \( x \).
\end{definition}

\subsection{Proof of Proposition~\ref{prop:cropping_vs_performance}}
\label{proof:prop}

\begin{assumption}[Existence of a NLL-Reducing Crop]
\label{assumption:nll_reducing_crop}
Let \( \mathcal{C}_1, \mathcal{C}_2, \ldots, \mathcal{C}_N \) be a set of \( N \) random crops for each sample \( (x_i, y_i) \) in the dataset \( \mathcal{D} = \{(x_i, y_i)\}_{i=1}^N \). For each sample, there exists at least one index \( k \in \{1, 2, \ldots, N\} \) such that applying crop \( \mathcal{C}_k(x_i) \) reduces the Negative Log-Likelihood (NLL) of the model:
\[
\log p_\theta(y_i \mid \mathcal{C}_k(x_i)) < \log p_\theta(y_i \mid x_i).
\]
\end{assumption}

\textit{Evidence.} To validate the assumption, we calculate the Negative Log-Likelihood (NLL) for each random crop and compare it to the original NLL of the image using a pretrained model \(\theta\). We find that if the average probability of an increase in NLL is less than one, this indicates that at least once among the \(N\) random crops, the NLL decreases compared to the original image. As shown in Table~\ref{tab:nll-change}, we can clearly see that the probability is less than $1.0$ across various types of datasets.

\begin{table}[H]
\centering
\caption{Average probability of increase in NLL. Standard deviation is computed from 100 random crops per image ($N=100$) over the IPC10 dataset. The metric for determining the difficulty of the dataset is EL2N~\cite{paul2021deep}.}
\label{tab:nll-change}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Crop Range = ($r$, 1.0)} & \textbf{Random} & \textbf{Hard + Balanced} & \textbf{Easy + Balanced} \\ \midrule
$r=0.08$ & 0.72 ± 0.24 & 0.65 ± 0.27 & 0.63 ± 0.27 \\
$r=0.2$  & 0.68 ± 0.27 & 0.62 ± 0.29 & 0.60 ± 0.29 \\
$r=0.5$  & 0.63 ± 0.30 & 0.57 ± 0.32 & 0.59 ± 0.31 \\
$r=0.8$  & 0.59 ± 0.29 & 0.54 ± 0.31 & 0.59 ± 0.30 \\ \bottomrule
\end{tabular}
\end{table}


\begin{lemma}[Effect of Selective Cropping on NLL]
\label{lemma:selective_cropping_nll}
Under Assumption~\ref{assumption:nll_reducing_crop}, a selective cropping approach that chooses, for each sample, the crop minimizing its NLL can lower the average NLL of the dataset:
\[
\mathcal{L}(\mathcal{D}'; \theta) = \frac{1}{N} \sum_{i=1}^N \log p_\theta(y_i \mid \mathcal{C}_k(x_i)) < \mathcal{L}(\mathcal{D}; \theta),
\]
where \( \mathcal{D}' \) is the selectively cropped dataset and \( \mathcal{L}(\mathcal{D}; \theta) \) is the original dataset's average NLL.
\end{lemma}

\begin{proof}
Let \( \mathcal{D} = \{(x_i,y_i)\}_{i=1}^N \) be the original dataset, and for each \( x_i \),
generate \( m \) crops \( \mathcal{C}_j(x_i) \). Define
\[
\mathcal{C}_{\mathrm{min}}(x_i)
\;=\;
\arg \min_{j\in\{1,\dots,m\}}
\Bigl[-\log p_\theta\bigl(y_i \mid \mathcal{C}_j(x_i)\bigr)\Bigr].
\]
Form the selective-crop dataset:
\[
\mathcal{D}_{\mathrm{select}}
\;=\;
\Bigl\{\,
  \bigl(\mathcal{C}_{\mathrm{min}}(x_i),\,y_i\bigr)
  :\,i=1,\ldots,N
\Bigr\}.
\]
By construction,
\[
-\log p_\theta\bigl(y_i \mid \mathcal{C}_{\mathrm{min}}(x_i)\bigr)
\;\;\le\;\;
-\log p_\theta\bigl(y_i \mid x_i\bigr)\quad\forall\, i.
\]
Hence, summing over \( i \) and dividing by \( N \) implies
\[
\mathcal{L}\bigl(\mathcal{D}';\theta\bigr)
\;\;\le\;\;
\mathcal{L}\bigl(\mathcal{D};\theta\bigr),
\]
showing the overall NLL of \( \mathcal{D}' \) is no greater than that
of the original dataset.
\end{proof}

\begin{lemma}[Correlation of NLL \& Entropy]
\label{lemma:entropy_nll_correlation}
Under a random cropping transformation \( \mathcal{C}: \mathcal{X} \to \mathcal{X} \), the correlation between changes in average negative log-likelihood (NLL) \( \mathcal{L}(\mathcal{D}; \theta) \) and changes in entropy \( H(\mathcal{D}; \theta) \) is given by:
\[
\rho(\Delta\mathcal{L}, \Delta H) =
\frac{\mathbb{E}[(\Delta\mathcal{L} - \mu_{\Delta\mathcal{L}})(\Delta H - \mu_{\Delta H})]}{\sigma_{\Delta\mathcal{L}} \sigma_{\Delta H}},
\]
where \( \Delta\mathcal{L} \) and \( \Delta H \) represent changes in NLL and entropy, respectively. \( \mu \) and \( \sigma \) are the corresponding means and standard deviations. The behavior of \( \rho(\Delta\mathcal{L}, \Delta H) \) depends on dataset difficulty:
\[
\rho(\Delta\mathcal{L}, \Delta H) =
\begin{cases} 
    < 0, & \text{if the dataset is easy or hard,} \\
    > 0, & \text{if the dataset is moderate.}
\end{cases}
\]
For easy and hard datasets, cropping generally reduces NLL while increasing entropy. For moderate datasets, cropping increases NLL and reduces entropy, leading to a positive correlation.
\end{lemma}

\begin{proof}
Let \( \mathcal{D} = \{(x_i,y_i)\}_{i=1}^N \) be a dataset, and let 
\( \mathcal{C}:\mathcal{X}\to\mathcal{X} \) be a random cropping transformation. 
For each sample \( (x_i,y_i) \), define the changes in NLL and entropy:
\[
\Delta\mathcal{L}_i 
\;=\;
\mathcal{L}\bigl(\mathcal{C}(x_i),\,y_i;\,\theta\bigr) 
\;-\;
\mathcal{L}\bigl(x_i,\,y_i;\,\theta\bigr),
\quad
\Delta H_i 
\;=\;
H\bigl(\mathcal{C}(x_i)\bigr) 
\;-\;
H\bigl(x_i\bigr).
\]
Here, \( \mathcal{L} \) is the negative log-likelihood under parameters \( \theta \), 
and \( H(\cdot) \) is the Shannon entropy of the input.

\paragraph{Easy Datasets.}
These have prominent, localized features and redundant backgrounds. 
Random cropping typically removes non-informative regions, 
\emph{reducing} \(\mathcal{L}\) (model becomes more confident) 
while \emph{increasing} \(H\) (cropping induces greater image variability). 
Hence \(\Delta\mathcal{L}_i < 0\) and \(\Delta H_i > 0\). 
They move in opposite directions, so their covariance is negative, 
yielding
\(\rho(\Delta\mathcal{L}, \Delta H) < 0\).

\paragraph{Moderate Datasets.}
Contextual cues are crucial, so cropping removes essential information. 
This \emph{increases} \(\mathcal{L}\) (model is less certain) and 
also \emph{increases} \(H\) (more variability after losing context). 
Thus \(\Delta\mathcal{L}_i > 0\) and \(\Delta H_i > 0\). 
They move in the same direction, giving a positive covariance 
and 
\(\rho(\Delta\mathcal{L}, \Delta H) > 0\).

\paragraph{Hard Datasets.}
Key features are subtle; cropping may sometimes remove distractors, 
slightly \emph{decreasing} \(\mathcal{L}\), while still \emph{increasing} 
\(H\). Thus \(\Delta\mathcal{L}_i \lesssim 0\) and \(\Delta H_i > 0\). 
Again, they vary inversely, so the covariance (and hence correlation) 
is negative.

\paragraph{Empirical Confirmation.}
Table~\ref{tab:my-table} shows numerical evidence consistent with these 
three scenarios. Easy and hard datasets exhibit negative correlation 
between \(\Delta\mathcal{L}\) and \(\Delta H\), while moderate datasets 
yield positive correlation.

Therefore, for random cropping in easy and hard datasets, 
\(\Delta\mathcal{L}\) and \(\Delta H\) typically move in opposite 
directions (\(\rho<0\)), whereas in moderate datasets, they move in 
the same direction (\(\rho>0\)), confirming the stated lemma.


\begin{table}[H]
\centering
\caption{Correlation between Changes in NLL and Shannon Entropy under Various Dataset Conditions}
\label{tab:my-table}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Metric: EL2N~\cite{paul2021deep}} & \textbf{Easy Only} & \textbf{Easy + Balanced} & \textbf{Random} & \textbf{Hard + Balanced} & \textbf{Hard Only} \\ \midrule
Correlation \(\rho(\Delta\mathcal{L}, \Delta H)\)  & -0.70 & 0.66 & 0.46 & 0.03 & -0.60 \\
Concordant Direction Changes          & 0.25 & 0.87 & 0.85 & 0.56 & 0.29 \\
\(\Delta\mathcal{L}\) (mean ± std)                  & -0.05 ± 0.48 & 0.21 ± 0.24 & 0.76 ± 0.74 & 0.66 ± 0.89 & -0.10 ± 0.70 \\ 
\(\Delta H\) (mean ± std)                           & 0.21 ± 0.39 & 0.30 ± 0.48 & 0.62 ± 0.72 & 0.26 ± 0.69 & 0.23 ± 0.62 \\\bottomrule
\end{tabular}
\end{table}

\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:cropping_vs_performance}]
Let $\mathcal{D}' = \mathcal{C}(\mathcal{D})$ denote the dataset transformed by selective cropping operation $\mathcal{C}$. We demonstrate the non-implication through three connected arguments: guaranteed NLL reduction, entropy behavior analysis, and explicit counterconstruction.

First, Lemma~\ref{lemma:selective_cropping_nll} establishes that for any $\mathcal{D}$ satisfying Assumption~\ref{assumption:nll_reducing_crop}, the selective cropping operation $\mathcal{C}$ necessarily reduces the average negative log-likelihood:
\[
\mathcal{L}(\mathcal{D}';\theta) = \frac{1}{N}\sum_{i=1}^N -\log p_\theta(y_i|\mathcal{C}(x_i)) < \frac{1}{N}\sum_{i=1}^N -\log p_\theta(y_i|x_i) = \mathcal{L}(\mathcal{D};\theta).
\]
This follows directly from the selection mechanism $\mathcal{C}_{\mathrm{min}}(x_i) = \arg\min_j [-\log p_\theta(y_i|\mathcal{C}_j(x_i))]$, which ensures component-wise improvement.

Next consider the Shannon entropy $H(\mathcal{D}) = -\mathbb{E}_{x\sim\mathcal{D}}[\log p(x)]$. Lemma~\ref{lemma:entropy_nll_correlation} reveals the conditional relationship between NLL reduction and entropy changes:
\[
\rho(\Delta\mathcal{L}, \Delta H) = 
\begin{cases} 
    < 0, & \text{easy/hard datasets} \\
    > 0, & \text{moderate datasets}
\end{cases}
\]
where $\Delta\mathcal{L} = \mathcal{L}(\mathcal{D}';\theta) - \mathcal{L}(\mathcal{D};\theta)$ and $\Delta H = H(\mathcal{D}') - H(\mathcal{D})$. For hard-only datasets (Table~\ref{tab:my-table}, last column), we observe:
\begin{align*}
&\mathbb{E}[\Delta\mathcal{L}] = -0.10 \pm 0.70 \\
&\mathbb{E}[\Delta H] = 0.23 \pm 0.62,
\end{align*}
demonstrating that NLL reduction ($\Delta\mathcal{L} < 0$) can coexist with entropy increase ($\Delta H > 0$).

This leads to our crucial counterexample: let $\mathcal{D}_{\mathrm{hard}}$ be a dataset with $\rho(\Delta\mathcal{L}, \Delta H) = -0.60$. For this dataset:
\[
\mathrm{NLL}(\mathcal{D}'; \theta) < \mathrm{NLL}(\mathcal{D}; \theta) \quad \text{(by Lemma~\ref{lemma:selective_cropping_nll})}
\]
\[
H(\mathcal{D}') = H(\mathcal{D}) + \Delta H > H(\mathcal{D}) \quad \text{(from $\mathbb{E}[\Delta H] > 0$)}.
\]
The simultaneous NLL reduction and entropy increase in $\mathcal{D}_{\mathrm{hard}}$ provides an explicit counterexample to the universal claim that $\mathrm{NLL}(\mathcal{D}') < \mathrm{NLL}(\mathcal{D})$ implies $H(\mathcal{D}') < H(\mathcal{D})$.

The existence of such counterexamples stems from the fundamental difference between NLL and entropy: while NLL measures model confidence through $p_\theta(y|x)$, entropy quantifies input diversity via $p(x)$. Selective cropping optimizes for the former while potentially increasing the latter, particularly in datasets where discriminative features are locally concentrated but globally varied across samples.

Thus, the proposition holds because we have identified concrete conditions (easy/hard datasets with EL2N extremes) where NLL reduction through cropping demonstrably fails to produce entropy reduction.
\end{proof}




% ----------------------------------- END PROOF For Proposition 1 ------------------------------


\subsection{Proof for Theorem~\ref{theorem:entropy_model_performance}}
\label{proof:thm}

\begin{assumption}[Entropy Increment of Cropping]
\label{assumption:entropy_increment_cropping}
Let \( \mathcal{X} \) be an input space and \( \mathcal{C}_r: \mathcal{X} \rightarrow \mathcal{X} \) be a random cropping operation with crop ratio \( r \in (0, 1] \). For any dataset \( \mathcal{D} \subset \mathcal{X} \times \mathcal{Y} \), the following cases describe the expected entropy behavior:

Case 1. For a single cropping operation, the expected entropy of the cropped dataset exceeds the entropy of the original dataset:
\[
\mathbb{E}[H(\mathcal{C}_r(\mathcal{D}))] > H(\mathcal{D}).
\]

Case 2. When comparing two crop ratios \( r_1, r_2 \in (0, 1] \) such that \( r_1 < r_2 \), the smaller ratio produces a larger expected entropy increment relative to the original dataset:
\[
\mathbb{E}[H(\mathcal{C}_{r_1}(\mathcal{D}))] - H(\mathcal{D}) > \mathbb{E}[H(\mathcal{C}_{r_2}(\mathcal{D}))] - H(\mathcal{D}).
\]

Case 3. For two consecutive cropping operations with ratios \( r_1, r_2 \in (0, 1] \) such that \( r_1 \cdot r_2 = r \), the expected entropy increment of the repeated cropping operation exceeds that of a single cropping operation with ratio \( r \):
\[
\mathbb{E}[H(\mathcal{C}_{r_2}(\mathcal{C}_{r_1}(\mathcal{D})))] - H(\mathcal{D}) > \mathbb{E}[H(\mathcal{C}_r(\mathcal{D}))] - H(\mathcal{D}).
\]
\end{assumption}

\textit{Empirical Confirmation for Case 1.}
From Table~\ref{tab:entropy-increase}, we can observe that regardless of different dataset difficulty, the expected entropy increases ($p > 0.5$).

\begin{table}[H]
\centering
\caption{Average probability ($p$) of increase in entropy. Standard deviation is computed from 100 random crops per image ($N=100$).
The metric for determining the difficulty of the dataset is EL2N~\cite{paul2021deep}.
}
\label{tab:entropy-increase}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Crop Range} & \textbf{Easy Only} & \textbf{Easy + Balanced} & \textbf{Random} & \textbf{Hard + Balanced} & \textbf{Hard Only} \\ \midrule
$r=0.08$ & 0.59 ± 0.28 & 0.60 ± 0.28 & 0.67 ± 0.26 & 0.58 ± 0.28 & 0.57 ± 0.28 \\
$r=0.2$  & 0.59 ± 0.29 & 0.58 ± 0.29 & 0.64 ± 0.28 & 0.56 ± 0.29 & 0.55 ± 0.29 \\
$r=0.5$  & 0.60 ± 0.31 & 0.57 ± 0.31 & 0.60 ± 0.30 & 0.55 ± 0.31 & 0.55 ± 0.31 \\
$r=0.8$  & 0.61 ± 0.30 & 0.58 ± 0.30 & 0.58 ± 0.29 & 0.55 ± 0.30 & 0.56 ± 0.30 \\ \bottomrule
\end{tabular}
\end{table}

\textit{Reasoning for Case 2.}
Smaller crop ratios (\( r_1 \)) retain less of the original image, leading to a greater reduction in structural information and a more pronounced increase in randomness. For instance, a smaller crop of an image is less likely to preserve identifiable or contextually meaningful features. This introduces higher variability across the cropped dataset, leading to a larger entropy increment. In contrast, larger crop ratios (\( r_2 \)) retain more of the original structure, limiting the entropy gain.
Table~\ref{tab:entropy-change} clearly indicates that entropy increases regardless of dataset difficulty as \( r \) decreases.

\textit{Reasoning for Case 3.}
Repeated cropping applies a second layer of randomness to the already cropped dataset. The first crop, \( \mathcal{C}_{r_1}(\mathcal{D}) \), introduces variability by sampling a subset of the original image. The second crop, \( \mathcal{C}_{r_2} \), conditions its randomness on the already reduced structure of \( \mathcal{C}_{r_1}(\mathcal{D}) \), amplifying the total variability. This compounding randomness makes the expected entropy increment greater than that of a single crop \( \mathcal{C}_r(\mathcal{D}) \) with the same total ratio \( r \). 
Evidence of the claim is supported by Table~\ref{tab:entropy-change}, where the entropy increment of two repeated crops is consistently greater than a single crop despite having the same final cropping ratio.



\begin{table}[H]
\centering
\caption{Average entropy change across different crop ranges. Values are computed from 100 random crops per image ($N=100$).
Value 1 (\blue{Value 2}) denotes the entropy increment from a single crop $r$ or \blue{two repeated crops $r^{\frac{1}{2}}$}, where $r < 0$. 
The metric for determining the difficulty of the dataset is EL2N~\cite{paul2021deep}.
}
\label{tab:entropy-change}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Crop Range} & \textbf{Easy Only} & \textbf{Easy + Balanced} & \textbf{Random} & \textbf{Hard + Balanced} & \textbf{Hard Only} \\ \midrule
$r=0.08$ & 0.21 (\blue{0.22}) & 0.30 (\blue{0.35}) & 0.63 (\blue{0.84}) & 0.26 (\blue{0.32}) & 0.23 (\blue{0.27}) \\
$r=0.2$  & 0.12 (\blue{0.15}) & 0.15 (\blue{0.19}) & 0.44 (\blue{0.55}) & 0.16 (\blue{0.21}) & 0.13 (\blue{0.17}) \\
$r=0.5$  & 0.09 (\blue{0.13}) & 0.08 (\blue{0.12}) & 0.21 (\blue{0.28}) & 0.10 (\blue{0.15}) & 0.09 (\blue{0.12}) \\
$r=0.8$  & 0.07 (\blue{0.10}) & 0.06 (\blue{0.09}) & 0.10 (\blue{0.14}) & 0.07 (\blue{0.10}) & 0.06 (\blue{0.09}) \\ \bottomrule
\end{tabular}
\end{table}

\begin{lemma}[Uncertainty and Generalization]
\label{lemma:uncertainty_performance}
In small-data regimes, lower predictive uncertainty (\emph{i.e.}, smaller entropy \( H(Y \mid x; \theta) \)) correlates with better generalization performance.
\end{lemma}

\begin{proof}
Let \( \mathcal{D} = \{(x_i, y_i)\}_{i=1}^N \) be drawn i.i.d.\ from a distribution \( \mu \) on 
\( \mathcal{X} \times \mathcal{Y} \). Suppose a (possibly stochastic) learning algorithm 
\( \mathcal{M} \), when given \( \mathcal{D} \), produces parameters \( \theta = \mathcal{M}(\mathcal{D}) \in \Theta \). 
Denote by \( p_\theta(y\mid x) \) the model’s predictive distribution and by the predictive entropy for an input \( x \):
\[
H\bigl(Y \mid x;\theta\bigr)
=
-\sum_{c=1}^C
p_\theta\bigl(c\mid x\bigr)\,\log p_\theta\bigl(c\mid x\bigr).
\]
A lower value of this entropy indicates that the 
model is more confident in its prediction for \( x \), and entropy is a general measure of 
uncertainty~\cite{settles2009active, coleman2020selection}.

Information-theoretical analysis on generalization~\cite{xu2017information} shows that 
if \( \theta \) depends on \( \mathcal{D} \), then
\[
\mathbb{E}\bigl[L_{\mu}(\theta) - L_{\mathcal{D}}(\theta)\bigr]
\;\le\;
\sqrt{
  \frac{2\,\sigma^2 \, I(\mathcal{D};\theta)}{N}
},
\]
where \( L_{\mu}(\theta) \) is the population risk, \( L_{\mathcal{D}}(\theta) \) is the empirical risk on \( \mathcal{D} \), 
\( \sigma^2 \) is an upper bound on the loss variance, and \( I(\mathcal{D};\theta) \) is the mutual 
information between \( \mathcal{D} \) and \( \theta \). A smaller \( I(\mathcal{D};\theta) \) tightens this bound, leading 
to a smaller gap between training and test performance. In small-data settings, \( \sigma^2 \) 
can be comparatively large because each sample exerts a bigger influence on the loss, so 
keeping \( I(\mathcal{D};\theta) \) low is especially important for controlling the generalization error.

Thus, lower predictive entropy (i.e., lower uncertainty) typically corresponds to lower 
\( I(\mathcal{D};\theta) \), yielding a tighter generalization bound. Because a small \( N \) often makes 
\( \sigma^2 \) relatively more influential in the bound, this effect is pronounced in 
small-data regimes, and models that maintain lower predictive uncertainty across \( \mathcal{D} \) 
tend to achieve better generalization performance.
Similar findings that keep lower uncertainty data are also found in \cite{sorscher2022beyond, he2024you}.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theorem:entropy_model_performance}]
Let $\mathcal{D}$ be a dataset with entropy $H(\mathcal{D})$, and let $\mathcal{D}' = \mathcal{C}_{r_1}(\mathcal{D})$ be a selectively cropped version such that $H(\mathcal{D}') < H(\mathcal{D})$, where $\mathcal{C}_{r_1}$ represents a cropping operation with ratio $r_1 \in (0, 1)$. Consider an augmentation strategy $\mathcal{A}$ that includes random cropping operations with ratio $r_2 \in (0, 1)$, and let $p_\theta$ be a model pretrained on $\mathcal{D}$ using $\mathcal{A}$.

When we apply the augmentation $\mathcal{A}$ to $\mathcal{D}$ and $\mathcal{D}'$, the expected entropies of the augmented datasets are:
\begin{align*}
\mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}) \right) \right] & = H(\mathcal{D}) + \Delta H, \\
\mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}') \right) \right] & = H(\mathcal{D}') + \Delta H',
\end{align*}
where $\Delta H$ and $\Delta H'$ denote the expected entropy increments due to the augmentation $\mathcal{A}$ applied to $\mathcal{D}$ and $\mathcal{D}'$, respectively.

From \textbf{Assumption~\ref{assumption:entropy_increment_cropping}} (Case 3), applying a random crop to the already cropped dataset $\mathcal{D}'$ is equivalent to applying two consecutive cropping operations to $\mathcal{D}$: first with ratio $r_1$ (the selective crop) and then with ratio $r_2$ (the random crop in $\mathcal{A}$). The combined effective crop ratio is $r = r_1 \cdot r_2$.

According to Case 3, the expected entropy increment from two consecutive crops exceeds that of a single crop with the same total crop ratio:
\[
\Delta H' = \mathbb{E}\left[ H\left( \mathcal{C}_{r_2}\left( \mathcal{C}_{r_1}(\mathcal{D}) \right) \right) \right] - H(\mathcal{D}) > \mathbb{E}\left[ H\left( \mathcal{C}_{r}(\mathcal{D}) \right) \right] - H(\mathcal{D}) = \Delta H.
\]
This implies that:
\[
\Delta H' > \Delta H.
\]

Now, consider the difference in expected entropy after augmentation:
\begin{align*}
\mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}') \right) \right] - \mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}) \right) \right] &= \left( H(\mathcal{D}') + \Delta H' \right) - \left( H(\mathcal{D}) + \Delta H \right) \\
&= \left( H(\mathcal{D}') - H(\mathcal{D}) \right) + \left( \Delta H' - \Delta H \right).
\end{align*}
Since $H(\mathcal{D}') < H(\mathcal{D})$, let $\delta H = H(\mathcal{D}) - H(\mathcal{D}') > 0$. Then:
\[
\mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}') \right) \right] - \mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}) \right) \right] = -\delta H + \left( \Delta H' - \Delta H \right).
\]
For the difference in entropy after augmentation to be positive, we require:
\[
\Delta H' - \Delta H > \delta H.
\]
This means that the increase in the entropy increment due to applying $\mathcal{A}$ to $\mathcal{D}'$ must exceed the initial entropy reduction $\delta H$ caused by the selective cropping.

From the empirical observations in Table~\ref{tab:entropy-change}, we see that the entropy increment from two consecutive crops (shown as \blue{Value 2}) is consistently greater than that from a single crop (shown as Value 1), and the difference between $\Delta H'$ and $\Delta H$ increases with smaller crop ratios. Specifically, the entropy increments satisfy:
\[
\Delta H' \approx \Delta H + \epsilon,
\]
where $\epsilon > \delta H$, based on the empirical data indicating that the compounded increase in entropy surpasses the initial reduction.

Therefore, we have:
\[
-\delta H + \left( \Delta H' - \Delta H \right) > 0 \implies \mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}') \right) \right] > \mathbb{E}\left[ H\left( \mathcal{A}(\mathcal{D}) \right) \right].
\]
This means that the expected entropy after augmentation is higher for $\mathcal{D}'$:
\[
H_{p_\theta}(\mathcal{A}(\mathcal{D}')) > H_{p_\theta}(\mathcal{A}(\mathcal{D})).
\]

According to \textbf{Lemma~\ref{lemma:uncertainty_performance}}, in small-data regimes, higher predictive uncertainty (i.e., higher entropy) correlates with worse generalization performance. Therefore, the model's performance metric satisfies:
\[
\mathcal{P}(\mathcal{A}(\mathcal{D}')) < \mathcal{P}(\mathcal{A}(\mathcal{D})).
\]
Thus, under the given conditions, the entropy evaluated by the model on $\mathcal{A}(\mathcal{D}')$ does not directly reflect the image quality of $\mathcal{D}'$, and the model's performance on $\mathcal{A}(\mathcal{D}')$ is worse than on $\mathcal{A}(\mathcal{D})$.

\end{proof}
