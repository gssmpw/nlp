%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{authblk} % Optional if you need more advanced author formatting
\usepackage{hyperref} % For email links

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{minitoc}    % Instead of tocloft for simpler cases


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{array}

\usepackage{subcaption}
\usepackage{multirow}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{enumitem}
\usepackage{subfigure}

\usepackage{colortbl}
\usepackage{tablefootnote}

\renewcommand \thepart{}
\renewcommand \partname{}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\newcommand{\std}[2]{\begin{tabular}{@{}c@{}}#1{\color{gray}$\scriptscriptstyle  \pm  #2$}\end{tabular}}
\newcommand{\stdb}[2]{\begin{tabular}{@{}c@{}}\textbf{#1}{\color{gray}$\scriptscriptstyle  \pm  #2$}\end{tabular}}
\newcommand{\same}[2]{\begin{tabular}{@{}c@{}}#1{\color{gray}$\scriptscriptstyle  =  #2$}\end{tabular}}
\newcommand{\up}[2]{\begin{tabular}{@{}c@{}}#1{\color{red}$\scriptscriptstyle  \uparrow  #2$}\end{tabular}}
\newcommand{\down}[2]{\begin{tabular}{@{}c@{}}#1{\color{blue}$\scriptscriptstyle  \downarrow  #2$}\end{tabular}}
\newcommand{\upb}[2]{\begin{tabular}{@{}c@{}}\textbf{#1}{\color{red}$\scriptscriptstyle  \uparrow  #2$}\end{tabular}}
\newcommand{\downb}[2]{\begin{tabular}{@{}c@{}}\textbf{#1}{\color{blue}$\scriptscriptstyle  \downarrow  #2$}\end{tabular}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\title{}

\icmltitlerunning{Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to Images}

\author{%
    \normalsize
    \textbf{
  Lingao Xiao$^{1,2,3}$,
  Songhua Liu$^{1}$,
  Yang He$^{1,2,3,}$\thanks{
      Corresponding Author
  }~,
  Xinchao Wang$^{1}$
  }
}

\date{
    \normalsize
  $^{1}$National University of Singapore\\
  $^{2}$CFAR, Agency for Science, Technology and Research, Singapore\\
  $^{3}$IHPC, Agency for Science, Technology and Research, Singapore\\
  \texttt{
      \{xiao\_lingao, songhua.liu\}@u.nus.edu,\\
      he\_yang@cfar.a-star.edu.sg,
      xinchao@nus.edu.sg
  }
}



\begin{document}
\doparttoc %
% \faketableofcontents

\vspace{-3cm}
\twocolumn[
\vspace{-3em}
\icmltitle{
Rethinking Large-scale Dataset Compression:\\Shifting Focus From Labels to Images}
\vspace{-1.5cm}
\maketitle
\thanks
]


\begin{abstract}

% Dataset distillation and dataset pruning aim to compress datasets to improve computational and storage efficiency. However, due to differing applications, they are typically not compared directly, creating uncertainty about their relative performance. Additionally, inconsistencies in evaluation settings among dataset distillation studies prevent fair comparisons and hinder reproducibility. Therefore, there is an urgent need for a benchmark that can equitably evaluate methodologies across both distillation and pruning literature.
% Notably, our benchmark has demonstrated the effectiveness of soft labels in evaluations, even for randomly selected subsets.
% This advantage has shifted researchers' focus away from the images themselves, but soft labels are cumbersome to store and use. To address these concerns, we propose a framework, \textit{Prune, Combine, and Augment (PCA)}, which prioritizes image data and relies solely on hard labels for evaluation. Our benchmark and framework aim to refocus attention on image data in dataset compression research, paving the way for more balanced and accessible techniques.
% Our code is available at: \href{https://github.com/ArmandXiao/Rethinking-Dataset-Compression}{https://github.com/ArmandXiao/Rethinking-Dataset-Compression}.

Dataset distillation and dataset pruning are two prominent techniques for compressing datasets to improve computational and storage efficiency. 
Despite their overlapping objectives, these approaches are rarely compared directly. 
Even within each field, the evaluation protocols are inconsistent across various methods, which complicates fair comparisons and hinders reproducibility. 
Considering these limitations, we introduce in this paper a benchmark that equitably evaluates methodologies across both distillation and pruning literatures. 
Notably, our benchmark reveals that in the mainstream dataset distillation setting for large-scale datasets, which heavily rely on soft labels from pre-trained models, even randomly selected subsets can achieve surprisingly competitive performance.
This finding suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data, while also imposing additional burdens in terms of generation, storage, and application. 
To address these issues, we propose a new framework for dataset compression, termed \textit{Prune, Combine, and Augment (PCA)}, which focuses on leveraging image data exclusively, relies solely on hard labels for evaluation, and achieves state-of-the-art performance in this setup. 
By shifting the emphasis back to the images, our benchmark and PCA framework pave the way for more balanced and accessible techniques in dataset compression research. 
Our code is available at: \href{https://github.com/ArmandXiao/Rethinking-Dataset-Compression}{https://github.com/ArmandXiao/Rethinking-Dataset-Compression}.

\end{abstract}

\begin{figure}[t]
    \centering
    \includegraphics[width=.48\textwidth]{fig/main-hard.pdf}
    \caption{Benchmarking SOTA methods using \textbf{hard labels}.
    ``DD (Noise)'' and ``DD (Real)'' denote dataset distillation with noise and real images, respectively.
    Many methods struggle to outperform the random baseline, and methods utilizing more original images generally achieve better performance. Evaluation uses ResNet-18 on ImageNet-1K.
    Detailed data is provided in Table~\ref{tab:benchmark-SOTA-hard}.
    }
    \label{fig:main-hard}
    % \vspace{-0.5cm}
\end{figure}

\vspace{-0.5cm}

\section{Introduction}
\label{sec:intro}

Modern deep learning often relies on extensive datasets, posing challenges in computational cost and storage. Two widely adopted strategies to address these challenges are \emph{dataset distillation} and \emph{dataset pruning}. Dataset distillation~\cite{yu2023dataset} involves generating a compact set of synthetic images that encapsulate the essential attributes of the original dataset, while dataset pruning focuses on selecting the most critical subset of real images for training.

Despite sharing the same goal, these approaches are suited to different scenarios. Dataset distillation aims for significant compression ratios, often condensing down to 10 images per class (IPC), which is equivalent to $\sim$99\% pruning rate. In contrast, dataset pruning typically reduces dataset size by 20\%-40\% without significantly affecting performance. Theoretically, pruning could remove a large portion of images, matching the extreme compression levels of distillation. However, in such cases, the comparative effectiveness of pruning versus distillation remains uncertain. Additionally, the frameworks for evaluating pruning and distillation on large-scale datasets vary significantly.

\input{table/evaluation_setting}

Inconsistencies also exist within large-scale dataset distillation literature (see Table~\ref{tab:evaluation-setting}). Initially, SRe$^2$L~\cite{yin2023squeeze} employed a large batch size of 1024 for evaluation. Subsequent studies~\cite{yin2023dataset, dwa2024neurips, sun2023diversity} adopted much smaller batch sizes, leading to more training updates. The absence of standardized evaluation protocols regarding batch sizes and data augmentation has hampered reproducibility and made it challenging to assess new research developments.

To address these issues, we introduce a benchmark for equitable evaluation of methodologies in both dataset distillation and pruning. Our benchmark highlights a performance enhancement when using soft labels for evaluation.
Randomly selected subsets equipped with soft labels show strong performance compared to SOTA methods, particularly at large IPCs (see Figure~\ref{fig:main-soft}). Even purely random noise achieves learnable results from a pretrained teacher network using soft labels. This advantage of soft labels has shifted focus away from images to the exploitation of soft labels.


\begin{figure}[t]
    \centering
    \includegraphics[width=.48\textwidth]{fig/main-soft.pdf}
    \caption{Benchmarking SOTA methods using \textbf{soft labels}.
    Many methods struggle to outperform the random baseline, particularly at large IPCs.
    Evaluation uses ResNet-18 on ImageNet-1K. 
    Detailed data is provided in Table~\ref{tab:benchmark-SOTA-soft}.
    }
    \label{fig:main-soft}
\end{figure}

However, utilizing soft labels incurs several costs and unfairness. Firstly, soft labels are storage-intensive, often largely exceeding the storage requirements of images~\cite{xiao2024large}. Secondly, the diverse storage formats of soft labels necessitate changes in dataloaders, complicating implementation as more augmentation strategies, like RandAugment in DELT~\cite{shen2024deltsimplediversitydrivenearlylate}, emerge. Lastly, soft labels introduce information beyond what is present in the compressed dataset, potentially biasing evaluations.
Therefore, we further evaluate the SOTA methods without soft labels; unsurprisingly, the performance drops drastically as shown in Figure~\ref{fig:main-hard}. For example, the performance of SRe$^2$L drops from 33.5\% (with soft labels) to 1.5\% (with hard labels).

To counter these challenges, we propose a hard-label-only framework, \textit{Prune, Combine, and Augment (PCA)}, to prioritize image contributions while surpassing random baseline performance. The \textit{PCA} framework builds on pruning insights by selecting straightforward, representative images adhering to established pruning principles, such as balanced classes and a focus on ``easier" samples. These images are then compressed further and enhanced through specialized data-augmentation methods, particularly advantageous for the final small-scale datasets. Unlike conventional distillation, \textit{PCA} does not store soft labels from pretrained models, making it viable for scenarios with constrained memory or limited access to large teacher models.

In summary, our primary contributions include:
\begin{enumerate}
    \item A unified evaluation setting that resolves inconsistencies in previous comparisons of dataset distillation and pruning.
    \item Demonstration of the importance of random baselines, revealing that many existing methods do not surpass straightforward baselines under soft-label conditions.
    \item The introduction of \textit{PCA} framework that aims to shift the focus from utilizing powerful soft labels to images themselves.
    \item Extensive experiments on large-scale benchmarks, showcasing \textit{PCA}'s consistent performance surpassing both random baselines and leading approaches across various model architectures.
\end{enumerate}

\section{Related Works}
\label{sec:related}

\textbf{Dataset Distillation.}
Dataset distillation aims to learn compact and synthetic datasets that achieve a similar performance as the full dataset.
Researchers have developed many frameworks~\cite{wang2018dataset, zhao2021datasetGM, kimICML22, zhao2021dataset, cazenavette2022distillation, liu2023dream, lee2022dataset, zhao2023dataset, wang2022cafe, jiang2022delving, du2022minimizing, shin2023loss, deng2022remember, liu2022dataset, zhao2022synthesizing, wang2023dim, lorraine2020optimizing, nguyen2021dataset, nguyen2021datasetKIP, vicol2022implicit, zhou2022dataset, loo2022efficient, zhang2022accelerating, cui2023scaling, loo2023dataset} to effectively learn the synthetic dataset on small scale dataset like MNIST and CIFAR dataset.

However, scaling the existing framework to a large dataset suffers from unaffordable consumption in both memory and time.
SRe$^2$L~\cite{yin2023squeeze} on the first time achieves noticeable performance by decoupling the optimization process into three phases of squeezing, recovering, and relabeling. 
Follow-up works~\cite{yin2023dataset, sun2023diversity, dwa2024neurips, shao2023generalized, loo2024large} mostly focus on addressing the diversity issue of the recovery phase, with more and more attention paid to the relabeling process~\cite{xiao2024large, zhang2024breaking, qin2024label, kang2024label, yu2025teddy}.
However, most methods use different evaluation settings and lack direct comparison, and the performance of random baseline under the relabeling process is overlooked\footnote{We notice a concurrent work that benchmarks existing dataset distillation methods currently on small-scale datasets (i.e., CIFAR and Tiny-ImageNet), and we encourage interested readers to visit \href{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}.}.

\textbf{Dataset Pruning.}
Dataset pruning selects a representative subset by ranking images with different metrics~\cite{coleman2020selection, toneva2018an, pleiss2020identifying, feldman2020neural, paul2021deep}. 
Most of the reported experiments are focused on small datasets like CIFAR or ImageNet subsets.
Methods that scale to large-scale datasets focus on small or moderate pruning ratio to ensure minimum performance drop~\cite{xia2023moderate, sorscher2022beyond, zheng2023coveragecentric, zhang2024spanning, grosz2024data, abbas2024effective}.
VID~\cite{ben2024distilling} conducts experiments on data pruning methods using knowledge distillation. However, these experiments did not explore extreme pruning ratios, and the baselines were not compared with dataset distillation methods.

\textbf{Dataset Compression.}
Dataset compression intuitively encompasses both dataset distillation and dataset pruning, which can work independently. Existing studies incorporate the pruning process, or coreset selection, before dataset distillation~\cite{liu2023dream, xu2025distill, moser2024distill, shen2024deltsimplediversitydrivenearlylate}. Additionally, YOCO~\cite{he2024you} examines the pruning rules specifically for distilled datasets. However, given the distinctly different nature and settings of these two tasks, it remains unclear which method represents the state-of-the-art (SOTA) in the field of data compression today. This lack of direct comparison may lead to misunderstandings about the data compression task and result in ineffective combinations of methods.



\section{Benchmarking Data Compression}
\label{sec:benchmark}

\textbf{Inconsistent Evaluation Settings in Dataset Distillation.}
As illustrated in Table~\ref{tab:evaluation-setting}, the domain of large-scale dataset distillation does not have a consistent evaluation setting. 
Using different settings does not mean wrong; however, it may lead to unfair comparison and potentially hinder both readers and researchers from understanding how important the proposed method is and where the improvement comes from.
Among these configurations, the most important one is the different batch sizes.
Using a different batch size largely affects performance, memory requirements, and training efficiency, as shown in Table~\ref{tab:batch-size}.
In the following paper, we use CDA's setting~\cite{yin2023dataset} as the \textbf{standard evaluation setting} since they only change the batch size from the initial implementation while providing a good performance and efficiency trade-off.

\begin{table}
    \setlength{\tabcolsep}{0.5em}
    \caption{Effect of batch size in evaluation. Results are reported using randomly sampled data in IPC-10 with ResNet-18. $A\times$ ($B\times$) means $A$ is theoretical increments and $B$ is the actual increments.}
    \label{tab:batch-size}
    % \vskip 0.15in
    \centering
    \scriptsize
    \begin{tabular}{cccc}
    \toprule
       Batch Size $\uparrow$ & Performance $\downarrow$ & Memory Requirement $\uparrow$ & Training Time $\downarrow$ \\ \midrule
       32 & \std{37.7}{0.4} \% & 1$\times$ & 32$\times$ (26$\times$) \\
       128 & \std{35.8}{0.1} \% & 4$\times$ (3$\times$) & 8$\times$ (7$\times$) \\
       1024 & \std{23.7}{0.1} \% & 32$\times$ (17$\times$) & 1$\times$\\
    \bottomrule
    \end{tabular}
\end{table}

\textbf{Undervalued Random Baseline with Distillation Training.}
Many existing works~\cite{yin2023squeeze, yin2023dataset, sun2023diversity, xiao2024large} fail to recognize the value of random baselines.
By evaluating the random dataset under the standard evaluation setting, we notice that most works fail to surpass the random baseline (see Fig.~\ref{fig:main-soft} and Table~\ref{tab:random-baseline}), creating a huge gap in understanding the task of large-scale dataset distillation.
Especially, the gap becomes more distinguished as IPC scales.

\input{table/random_benchmark}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/entropy.pdf}
    \caption{Entropy analysis of different datasets with IPC=10. Images are randomly sampled from the corresponding dataset for visualization. The classifier used for entropy analysis is the pretrained EfficientNet-B0~\cite{tan2019efficientnet}.}
    \vspace{-0.3cm}
    \label{fig:entropy}
\end{figure*}


\textbf{Understanding Effect of Images and Soft Labels.}
To evaluate the effectiveness of images, Figure~\ref{fig:main-soft} offers valuable insights into both the quantity and quality of original images used, which may significantly affect performance. The subpar results of earlier dataset distillation methods compared to random baselines suggest that \textbf{image distortion, learned during the dataset distillation process, adversely impacts performance}. 
Furthermore, when using \textbf{purely noisy images}, the student network can successfully learn to replicate the outputs of a pretrained teacher model, as demonstrated in Appendix~\ref{appendix:noisy-images}. This suggests that soft labels derived from the teacher model provide sufficient information for the student to learn even from random noise, thereby reducing the distinctive impact of the images.


\textbf{Reducing Size of Pre-Generated Soft Labels.}
By utilizing distillation training, distilled datasets deliver an extraordinary performance against early exploration on large-scale dataset distillation~\cite{cui2023scaling}.
However, a critical question arises: \textit{\textbf{Does using distillation training for evaluation align with the motivation and practical needs of dataset compression?}}
Requiring a pretrained model to aid the training with a compressed dataset essentially reduces the practical value of the compressed dataset.
One scenario that makes the task valid is when the pretrained model is not available during deployment but during development. 
One workaround is to store all pre-generated soft labels locally.


\textbf{Problems of Pre-Generated Soft Labels.} However, as mentioned by \citeauthor{xiao2024large} and \citeauthor{qin2024distributional}, the soft label storage far exceeds the image storage. For example, the label storage of ImageNet-21K-P IPC20 is over 1.2 TB, while the images are merely 5 GB.
Existing methods~\cite{xiao2024large, zhang2024breaking} have started to reduce soft label storage, but pre-generated soft labels still face several disadvantages.
(1) Soft labels are stored in a very different format from images, and special changes to the dataloader are required; 
(2) using soft labels during training creates additional system I/O costs besides soft label generation.
Last but not least, as more and more data augmentation is introduced;
(3) the use of soft labels becomes increasingly complicated as more advanced augmentation (i.e., RandAugment; \citealt{shen2024deltsimplediversitydrivenearlylate}) is introduced;
(4) soft label introduces knowledge beyond the compressed datasets, potentially biasing the evaluation results.

We believe that, in the field of large-scale dataset distillation, efforts should be focused on enhancing the quality of distilled images rather than prioritizing further exploitation of soft labels. Consequently, the preferred approach should be controlling the size of soft labels or even transitioning to using only hard labels.

\section{Framework: Prune, Combine, and Augment}
\label{sec:method}


We introduce a novel framework, termed \textit{Prune, Combine, and Augment (PCA)}, which utilizes only hard labels during deployment and evaluation. This framework aims to enhance the understanding of dataset compression methods by showcasing their true contributions.


\subsection{Prune Dataset}


Relying solely on hard labels for evaluation allows us to leverage key insights from dataset pruning:
(1) \textbf{Class balance becomes increasingly important as the dataset size diminishes}~\cite{he2024you}, and
(2) \textbf{Simpler images are preferred when the dataset size is small}~\cite{sorscher2022beyond, zheng2023coveragecentric, he2024you}.




Common Images-per-class (IPC) in dataset distillation reflects high pruning ratios, necessitating methods to align with pruning guidelines. We analyze the patterns generated by dataset distillation images to verify if they meet these criteria. First, dataset distillation naturally achieves class balance using IPC, ensuring perfect class balance. Second, Figure~\ref{fig:entropy} provides an intuitive entropy analysis, and we can gauge the dataset's complexity by extending the use of entropy as a measure of uncertainty~\cite{coleman2020selection, sun2023diversity}.
A dataset with high average entropy is deemed relatively challenging.
In addition, we can visually compare images compressed by different methodologies in Figure~\ref{fig:entropy}.


Consequently, we propose to select images according to pruning metrics while adhering to the pruning rules~\cite{he2024you}. Specifically, we use the reverse metric of EL2N~\cite{paul2021deep}, meanwhile forcing strict class balance. This approach is optimization-free that requires no pretrained model for evaluation. 


\iffalse
\subsection{Combine Images}

RDED~\cite{sun2023diversity} combines image patches that are selected from random crops. We contend that the selection of these cropped patches may not always lead to performance improvements. In Section~\ref{sec:analysis}, we provide a theoretical examination of the cropping operation. Further elaboration on our method of image combination is presented in Section~\ref{sec:combine}.

\subsubsection{Theoretical Analysis on Cropping}
\label{sec:analysis}

Initially, in Proposition~\ref{prop:cropping_vs_performance}, we explore in detail why the metric employed by RDED~\cite{sun2023diversity} fails to be universally applicable. We compare it with the metric utilized in the pruning literature~\cite{coleman2020selection} to highlight these differences. Subsequently, we demonstrate why utilizing a pretrained model to directly rank the cropped image patches is inappropriate.

\begin{proposition}
\label{prop:cropping_vs_performance}
Lowering a dataset's Negative Log-Likelihood (NLL) through a selective cropping operation~\cite{sun2023diversity} does \textbf{not} necessarily reduce the dataset's entropy.
\end{proposition}

\begin{theorem}
\label{theorem:low_init_entropy}
For a dataset with low initial entropy obtained through selective cropping, training with an augmentation strategy involving additional cropping operations, such as ``RandomResizedCrop,'' may degrade model performance due to compounded entropy increments.
\end{theorem}
\fi

\subsection{Combine Images}

RDED~\cite{sun2023diversity} combines image patches that are selected from random crops. We contend that the selection of these cropped patches may not always lead to performance improvements. In Section~\ref{sec:analysis}, we provide a theoretical examination of the cropping operation. Further elaboration on our method of image combination is presented in Section~\ref{sec:combine}.

\subsubsection{Theoretical Analysis on Cropping}
\label{sec:analysis}

In Proposition~\ref{prop:cropping_vs_performance}, we explore in detail why the metric employed by RDED~\cite{sun2023diversity} fails to be universally applicable. We compare it with the metric utilized in the pruning literature~\cite{coleman2020selection} to highlight these differences. Subsequently, in Theorem~\ref{theorem:entropy_model_performance}, we demonstrate why utilizing a pretrained model to directly rank the cropped image patches is inappropriate, and the cropped dataset may not bring performance improvement as it does not consider training time augmentation.

\begin{proposition}[proof in Appendix~\ref{proof:prop}]
\label{prop:cropping_vs_performance}
Let $\mathcal{D} = \{ x_i \}_{i=1}^N$ be a dataset of images, and let $P_\theta$ be a probabilistic model parameterized by $\theta$.
Lowering $\mathrm{NLL}(\mathcal{D}; \theta)$ through a selective cropping operation $\mathcal{C}$~\cite{sun2023diversity}, resulting in a new dataset $\mathcal{D}' = \mathcal{C}(\mathcal{D})$, does \textbf{not} necessarily reduce the entropy $H(\mathcal{D}')$ of the dataset:
\[
\mathrm{NLL}(\mathcal{D}'; \theta) < \mathrm{NLL}(\mathcal{D}; \theta) \;\nRightarrow\; H(\mathcal{D}') < H(\mathcal{D}).
\]
\end{proposition}


\begin{theorem}[proof in Appendix~\ref{proof:thm}]
\label{theorem:entropy_model_performance}
Let $\mathcal{D}$ be a dataset and let $\mathcal{D}' = \mathcal{C}(\mathcal{D})$ be a selectively cropped version such that $H(\mathcal{D}') < H(\mathcal{D})$. Let $p_\theta$ be a model pretrained on $\mathcal{D}$ using an augmentation strategy $\mathcal{A}$ that includes random cropping operations. Then, the entropy evaluated by the model on $\mathcal{D}'$, denoted as $H_{p_\theta}(\mathcal{D}')$, does not directly reflect the image quality of $\mathcal{D}'$. Specifically, under certain conditions, the following inequalities hold:
\[
H_{p_\theta}(\mathcal{A}(\mathcal{D}')) \geq H_{p_\theta}(\mathcal{A}(\mathcal{D})),
\]
\[
\mathcal{P}(\mathcal{A}(\mathcal{D}')) \leq \mathcal{P}(\mathcal{A}(\mathcal{D})),
\]
where $\mathcal{P}$ denotes the model performance metric.
\end{theorem}



\subsubsection{Combining Images without Cropping}
\label{sec:combine}

To further condense the ``essence'', using a composite pattern of images (e.g., combining multiple images into one; \citealt{kimICML22, sun2023diversity}) condenses information of multiple images into a single one with no significant information loss on the diversity and richness of the dataset. However, given that our images are carefully selected due to pruning rules, the necessity of patch selection proposed on randomly selected images~\cite{sun2023diversity} warrants reevaluation. Instead of randomly cropping patches and assessing each patch's quality, retaining complete images, which hold the majority of information, is preferable as each selected image values. Since the cropping operation (i.e., patch selection) is irreversible, we only leverage cropping during training to ensure information is recoverable.

A fundamental distinction between dataset pruning and distillation is the modification state of original images. Unlike dataset distillation/condensation, where pixel modifications occur, our approach creates a composite image by combining entire images. This ``combined" methodology is an intermediary approach, utilizing unmodified imagery to construct new samples.


\subsection{Scaling-Law-Aware Augmentation}

Scaling-law usually refers to scaling up the model~\cite{kaplan2020scaling}; however, we refer to the scaling-law of the dataset~\cite{sorscher2022beyond}, especially when scaling down.
After acquiring a small-scale dataset, it remains crucial to unveil its potential and effectively harness the available information. Augmentation typically serves as the tool to achieve this, but it is imperative that these techniques are harmoniously integrated with the dataset's intrinsic properties, ensuring alignment with pruning principles. For example, RDED~\cite{sun2023diversity} introduces ``patch shuffling" to enhance the dataset's diversity. However, during training, the \texttt{Random Resized Crop} operation, when directly applied to the combined image, can inadvertently transform simpler images into more complex ones, thereby violating the pruning rule.

\begin{figure}[t]
    \centering
    \addtolength\belowcaptionskip{-0.0cm}
    \setlength\belowcaptionskip{0.1cm}
    \includegraphics[width=1\linewidth]{fig/extract_patch.pdf}
    \caption{Patch Shuffling vs. Patch Extraction.}
    \label{fig:patch-extract}
    \vspace{-0.15in}
\end{figure}

\input{table/benchmark_SOTA}
\input{table/benchmark_SOTA_hard}

To counteract this issue, we propose a \textit{patch extraction} strategy, which involves opting for random patch selection rather than shuffling, as depicted in the lower branch of Figure~\ref{fig:patch-extract}. This approach selects a single image patch for subsequent augmentation, with the image resolution being interpolated using the \textit{RandomResizedCrop} technique. This differs from IDC's approach~\cite{kimICML22}, which decodes a single combined image into multiple images. In contrast, random patch extraction confines the cropping scope to within a single patch, thereby effectively limiting the cropping area without introducing significant training overhead.


We emphasize the importance of using an effective augmentation strategy. When dealing with a small number of images, achieving good performance can be challenging. A well-crafted augmentation method can greatly enhance the potential of the images. Furthermore, the augmentation process should be aware of scaling laws, as most pruning scores are derived from training that includes data augmentation.



\section{Experiment}
\label{sec:experiment}

\subsection{Experiment Settings}

All experiments are conducted on ImageNet-1K using CDA's evaluation settings (see Table~\ref{tab:evaluation-setting}) unless otherwise indicated. Additional settings, including dataset, networks, and baseline specifications, can be found in Appendix~\ref{appendix:experiment-settings}.


\input{table/pruning_rule}

\subsection{Primary Results}

\textbf{Benchmarking SOTA methods with Soft Labels.}
Beforehand, it is very hard to directly compare the results and effectiveness of dataset distillation methods and pruning methods due to a \textbf{(1) completely evaluation setting} and \textbf{(2) large discrepancy in pruning ratios}.
Table~\ref{tab:benchmark-SOTA-soft} benchmarks existing dataset distillation methods and dataset pruning methods under the same evaluation setting.
We categorize existing data compression methods into three main categories: (1) dataset distillation with random noise as initialization, (2) dataset distillation with randomly sampled real images as image initialization, and (3) dataset pruning adhering to the pruning rules or scaling laws. 
We notice that by increasing the batch size in the evaluation setting, the performance SRe$^2$L~\cite{yin2023squeeze} catches up with other SOTA methods~\cite{yin2023dataset, xiao2024large}.
However, with this being said, many SOTA methods cannot beat the random baseline.
In addition, by using real images as initialization, the diversity problem that many works~\cite{shen2024deltsimplediversitydrivenearlylate, yin2023dataset, xiao2024large, sun2023diversity, dwa2024neurips} are targeting is addressed.
Surprisingly, pruning methods that are published 3-5 years ago~\cite{toneva2018an, pleiss2020identifying, paul2021deep} unanimously outperform random baselines, and it's time to call attention to this under-explored topic.
As a result, an interesting observation is that the performance improves as the images include more prior knowledge of original datasets.

\textbf{Benchmarking SOTA methods with Hard Labels.}
Table~\ref{tab:benchmark-SOTA-hard} evaluates the SOTA methods in a more recommendable approach that does not introduce any additional storage costs besides the images or requires pretrained knowledge.  
By utilizing only the hard labels, most of the results were based on soft label benchmarks, besides forgetting metrics.
In addition, the PCA (\textbf{P}rune, \textbf{C}ombine, and \textbf{A}ugment) framework essentially exceeds the random baseline and other SOTA methods at all tested IPCs.

\textbf{Sanity Check on Pruning Rules and Scaling Laws.}
Previous pruning methods have made the conclusion that when the dataset is small, (1) easy images are preferred, and (2) class balance is important.
However, the previous settings do not entirely fit the current scenario.
For example, previous dataset pruning works~\cite{zheng2023coveragecentric} do not go to extreme pruning ratios such as IPC10, equivalent to $1-(10*1000 / 1,281,167)=99.2\%$ pruning rate, and \citeauthor{he2024you} conducts pruning on distilled datasets.
Therefore, experiments are conducted to verify if the previous rules still work in extreme cases with real images.
Table~\ref{tab:pruning-rule} shows that the two rules hold even if the pruning ratio is extremely high on original datasets.
For example, from evaluation under both soft label settings and hard label settings, we can conclude that selecting easy images with balanced classes delivers the best results.
Moreover, EL2N~\cite{paul2021deep} shows superior performance and requires less time for ranking, so we use it as our pruning method.
Analysis of reasons why Forgetting~\cite{toneva2018an} performs worse is provided in Appendix~\ref{appendix:forgetting}.


\subsection{More Experiments}

\input{table/ablation}
\textbf{Ablation Study.}
Table~\ref{tab:ablation} demonstrates the improvements contributed by each component under hard-label-only settings. Among the components -- Pruning, Combine, and Augment -- augmentation (specifically, patch extraction) proves to be the most impactful. It is important to note that this augmentation is tailored specifically for combined images and can be directly applied to RDED~\cite{sun2023diversity}.
To provide further insight, we have included an ablation study that explicitly compares the previously used augmentation method, Patch Shuffling, with our proposed Patch Extraction. The results clearly indicate that patch extraction offers significant advantages over patch shuffling.
Additionally, since the evaluation settings for pruning methods use SGD with an initial learning rate of 0.1, we have also conducted evaluations under this configuration. Our observations reveal that even random baselines benefit from using SGD (0.1), showing a distinct advantage over AdamW (0.01).
Nevertheless, in all cases, the proposed PCA framework yields significant improvements.

\input{table/cross_arch}
\textbf{Cross Architecture Performance.}
Table~\ref{tab:cross-arch} demonstrates a good generalization ability of the proposed framework.
For all validation models, the performance scales well with the dataset size.
In addition, the framework scales well with improved model capacity, with one exception on the transformer-based Swin-V2-Tiny model~\cite{liu2022swin}.
Since the transformer-based model is extremely data-hungry, a trend is also observed in previous works~\cite{xiao2024large, sun2023diversity}.

\input{table/compare_soft}
\textbf{Performance Against Soft Labels.}
Despite having inevitable drawbacks and unfairness as mentioned in Section~\ref{sec:benchmark}, the cumbersome storage of soft labels can be addressed in some degree.
Table~\ref{tab:compare-soft} shows our hard-label-only framework can perform on par or even surpass previous methods using part of soft labels.
In theory, the maximum soft label compression rate is limited to $300\times$ in ImageNet-1K setting, as each image requires a soft label per epoch for 300 epochs.
Since we do not use soft labels at all, our soft label compression rate is $>300\times$.


\input{table/pixel_level}
\textbf{Effect of Cropping.}
In addition to the theoretical analysis (Section~\ref{sec:analysis}) of the effects of cropping, we conducted experiments to validate our findings. It is important to note that cropping can be performed both before and during training. We refer to cropping the dataset before training a model as dataset cropping, which is irreversible. Table~\ref{tab:dataset-cropping} shows that regardless of the metric and observer used to select patches from a well-pruned dataset, dataset cropping negatively impacts performance. This behavior can be explained by Theorem~\ref{theorem:entropy_model_performance}. Another cropping operation occurs during training augmentation (specifically, RandomResizedCrop), which is ``recoverable" because the original image remains unchanged, and the cropping operation in each epoch is independent. Table~\ref{tab:training-cropping} presents performance under different training crop ratios.

\input{table/regularization_based_aug}

\textbf{Regularization-based Data Augmentation.}
In addition to common augmentation techniques such as random resized crop and horizontal flips, data mixing augmentation (i.e., Mixup, Cutout, and CutMix) is regularization-based data augmentation that reduces overfitting by providing diverse and challenging examples during training.
Among regularization-based augmentation techniques, Table~\ref{tab:reg-based-aug} shows Cutout demonstrates the best performance, achieving consistent accuracy levels of 26.2\%, 25.7\%, and 25.3\% with \textit{RandomResizedCrop}. This performance is attributed 
to being best aligned with scaling law: first, Cutout preserves label integrity by avoiding label mixing, which is particularly advantageous in scenarios with limited data. Second, the augmentations are applied to individual images without cross-sample interactions, thereby maintaining sample simplicity.
In contrast, techniques such as CutMix and Mixup exhibit notable performance degradation with increasing mixing probabilities, especially in the presence of label mixing.
More analysis is provided in Appendix~\ref{appendix:data-mixing}.


\textbf{Additional Experiments.} Additional experiments and analysis are provided in Appendix~\ref{appendix:experiment-and-analysis}, including SRe$^2$L with real images as initialization (Appendix~\ref{appendix:real-init}), the relationship between data balance and data stratification (Appendix~\ref{appendix:data-balance}), PCA with different pruning methods (Appendix~\ref{appendix:pruning-methods}), discussion of Mosaic Augmentation (Appendix~\ref{appendix:mosaic}), and computation cost analysis (Appendix~\ref{appendix:computation-cost}).

\textbf{Visualization.}
Figure~\ref{fig:viz} presents our compressed datasets, more visualization including baseline methods are provided in Appendix~\ref{appendix:visualization}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{viz_jpg/OURS_EL2N_easy_balance_image_grid_5.jpg}
    \caption{Randomly sampled PCA images.}
    \label{fig:viz}
    \vspace{-1cm}
\end{figure}

\section{Conclusion, Limitation, and Future Work}

In this paper, we demonstrated that dataset compression can significantly reduce computational and storage overhead in large-scale machine learning tasks without severely compromising performance. By unifying evaluation settings for both dataset distillation and pruning, we established a fair ground for comparing these two lines of research. Our proposed \emph{Prune, Combine, and Augment (PCA)} framework capitalizes on pruning-based metrics to select representative images, merges them in a way that preserves core data characteristics, and applies carefully designed augmentation to counteract overfitting issues. Unlike soft-label distillation approaches that often rely on extensive pretrained resources, PCA employs purely hard-label supervision, thereby lowering both memory and complexity requirements. Extensive experimental results illustrated the capacity of PCA to outperform conventional baselines and existing SOTA methods, especially at extreme compression ratios.

Limitations and future works are discussed in Appendix~\ref{appendix:limit} and Appendix~\ref{appendix:future}, respectively.

\section*{Impact Statement}
This paper addresses pressing challenges in dataset compression by establishing a benchmark for fair comparison across dataset distillation and pruning techniques. By highlighting inconsistencies in previous evaluations, we draw attention to the need for standardized practices that enhance reproducibility and fairness. Our proposed \textit{Prune, Combine, and Augment (PCA)} framework prioritizes image data and utilizes only hard labels, thereby reducing storage and computational demands traditionally associated with soft labels. This approach not only makes dataset compression more practical and accessible but also shifts the research focus back to the images themselves, potentially leading to more balanced and efficient methods. Through these efforts, we aim to foster responsible advancements in large-scale machine learning while ensuring the benefits are accessible to a wider range of practitioners.

\bibliography{icml2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\part{Appendix} %
% \parttoc


\input{appendix/proof_appendix}

\clearpage

\section{Experiment Settings}
\label{appendix:experiment-settings}
\subsection{Dataset and Network}

\textbf{Dataset.}
The ImageNet-1K dataset~\cite{deng2009imagenet}, also known as ILSVRC-2012, is a large-scale image classification dataset containing $N = 1.28$ million training images and $50,000$ validation images across $K = 1,000$ object categories. Each image is manually annotated with a single class label. The dataset contains approximately $1,200$ images per class in the training set. Images have an average resolution of $469 \times 387$ pixels but are typically pre-processed to a standard size of $224 \times 224$ pixels for model training. This dataset has become a de facto benchmark for evaluating deep learning models in computer vision tasks, particularly for image classification problems.

\textbf{Network.}
For all networks, we use common network definition from \href{https://pytorch.org/vision/main/models.html#classification}{https://pytorch.org/vision/main/models.html}.
Networks are trained for 300 epochs by default; detailed settings are provided in Appendix~\ref{appendix:standard-setting}.

\subsection{Standard Evaluation Setting}
\label{appendix:standard-setting}

Table~\ref{tab:evaluation-setting-full} provides a more comprehensive comparison among baseline dataset distillation methods.
We have adopted the CDA's setting~\cite{yin2023dataset} as the \textbf{standard evaluation setting} for two main reasons: (1) many other studies, such as LPLD~\cite{xiao2024large} and DWA~\cite{dwa2024neurips}, have used this setting; and (2) it applies to most methods, being designed explicitly for datasets that include combined image patterns, in contrast to patch shuffling. 
Note that baseline dataset pruning methods also adhere to the \textbf{standard evaluation setting} for fair comparison.

It's important to note that using alternative settings or additional techniques is \textbf{NOT} incorrect; however, we have chosen a standard evaluation setting to facilitate a clearer comparison among the different methods.

\input{appendix/evaluation_setting_full}

\textbf{Remark:} Table~\ref{tab:evaluation-setting-full} does not cover all the different settings. For example, EDC~\cite{shao2024elucidating} uses EMA-based evaluation while other methods do not include it.

\subsection{Fair Storage of Pruning Datasets}
\label{appendix:pruning-setting}
When considering the pruning ratio in state-of-the-art (SOTA) pruning methods, it is important to note that the pruning ratio does not directly correspond to the dataset distillation setting. Existing pruning techniques primarily focus on tracking the ranking of images (i.e., the indices) rather than storing the actual dataset, which leads to the neglect of the true size of the ImageNet-1K images. Additionally, dataset distillation limits image resolution to $224 \times 224$ pixels. Therefore, it is unfair, in terms of information content and storage, to directly store the actual ImageNet-1K images, which have a higher average resolution of $469 \times 387$ pixels. 
To address this, we choose to crop the images based on their shortest side and then resize them to $224 \times 224$ pixels.

\subsection{Baselines Specifications}
In this section, we provide more specifications of each baseline.

\textbf{Dataset Distillation Baselines:}
\begin{itemize}[nosep]
    \item \textbf{SRe$^2$L~\cite{yin2023squeeze}}: No special adjustments. Dataset recovered following \href{https://github.com/VILA-Lab/SRe2L}{https://github.com/VILA-Lab/SRe2L}.
    \item \textbf{CDA~\cite{yin2023dataset}}: No special adjustments; results reported are from the original paper. Dataset recovered following \href{https://github.com/VILA-Lab/CDA}{https://github.com/VILA-Lab/CDA}.
    \item \textbf{G-VBSM~\cite{shao2023generalized}}: No special adjustments. Dataset recovered following \href{https://github.com/shaoshitong/G_VBSM_Dataset_Condensation}{https://github.com/shaoshitong/G\_VBSM\_Dataset\_Condensation}.
    \item \textbf{LPLD~\cite{xiao2024large}}: No special adjustments; results reported are from the original paper. Dataset provided in \href{https://github.com/he-y/soft-label-pruning-for-dataset-distillation}{https://github.com/he-y/soft-label-pruning-for-dataset-distillation}.
    \item \textbf{DWA~\cite{dwa2024neurips}}: No special adjustments; results reported are from the original paper. Dataset recovered following \href{https://github.com/AngusDujw/Diversity-Driven-Synthesis}{https://github.com/AngusDujw/Diversity-Driven-Synthesis}.
    \item \textbf{RDED~\cite{sun2023diversity}}: IPC10 and IPC50 selects patch from $m=300$ patches, and IPC100 selects from $m=600$ patches. Dataset recovered following \href{https://github.com/LINs-lab/RDED}{https://github.com/LINs-lab/RDED}.
\end{itemize}

\textbf{Dataset Pruning Baselines:}
We create datasets by using the data ranking scores provided by \citeauthor{zheng2023coveragecentric} (\href{https://github.com/haizhongzheng/Coverage-centric-coreset-selection}{https://github.com/haizhongzheng/Coverage-centric-coreset-selection}).
After obtaining the ranking, we post-process the datasets into images of resolution $224 \times 224$, according to Appendix~\ref{appendix:pruning-setting}.
\begin{itemize}[nosep]
    \item \textbf{Forgetting~\cite{toneva2018an}}: Images with low ``forgetting events'' are selected; if images have a same number of ``forgetting events'', we randomly sample the images. Strict class balance is enforced.
    \item \textbf{EL2N~\cite{paul2021deep}}: Images with low ``EL2N Scores'' are selected; and strict class balance is enforced.
    \item \textbf{AUM~\cite{pleiss2020identifying}}: Images with high ``accumulated margin'' are selected; strict class balance is enforced.
    \item \textbf{CCS~\cite{zheng2023coveragecentric}}: For the base pruning metric, we use AUM~\cite{pleiss2020identifying} following the original experiment setting.
    In addition, we prune away 30\% ``mislabeled'' data for IPC10 and IPC50, and 20\% ``mislabeled'' data are removed for IPC100 due to strict class balance requiring enough images for each class.
\end{itemize}

\vspace{2cm}

\section{Main Result with Standard Deviation}
\subsection{Soft Label Benchmarks with Standard Deviation}
\label{appendix:soft-label-std}
\input{appendix/benchmark_SOTA_std}

\subsection{Hard Label Benchmarks with Standard Deviation}
\label{appendix:hard-label-std}
\input{appendix/benchmark_SOTA_hard_std}

\newpage

\section{Additional Experiments and Analysis}
\label{appendix:experiment-and-analysis}

\subsection{Training with Noisy Images}
\label{appendix:noisy-images}

From Table~\ref{tab:random-baseline}, we can see that even with \textbf{purely noisy images}, the student network is able to learn from the teacher network by matching the soft labels. This is surprising, as noisy images are typically not expected to contain any useful information for the network's learning process. Nevertheless, the performance of 0.5\% is significant compared to the purely random network's performance of 0.1\%.

\begin{table}[H]
    \caption{Distillation training with pure noise on ResNet-18 on ImageNet-1K. `BS' denotes batch size.}
    \label{tab:noise}
    \centering
    \begin{tabular}{lccc}
    \toprule
         & Expected Acc. & BS=128 & BS=1024\\ \midrule
        IPC50 & 0.1 \% & 0.5 \% & 0.3 \%\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Use Real Images as Initialization for Dataset Distillation}
\label{appendix:real-init}


As shown in Figure~\ref{fig:main-soft}, we categorize existing literature into three distinct sections. The first section encompasses dataset distillation with \textbf{noise} initialization, where no images from the original dataset are directly involved. The representative work in this category is SRe$^2$L~\cite{yin2023squeeze}, which pioneered this approach. The second section comprises dataset distillation with \textbf{real} image initialization, where the number of original images directly involved equals the distilled dataset size (specifically, $\mathrm{IPC} \times 1000$ images). An exception is RDED~\cite{sun2023diversity}, which randomly samples $m$ images and combines crops, utilizing $m \times 1000$ images, where $m > \mathrm{IPC}$. The final section focuses on dataset pruning methods, which evaluate the entire dataset to identify optimal subsets, thereby involving all images directly in the dataset compression process.

To validate the significance of incorporating more original images, we reimplemented SRe$^2$L with real images as initialization. Table~\ref{tab:real-init} demonstrates that merely initializing with real images consistently improves performance across both soft-label and hard-label benchmarks.

\begin{table}[H]
\centering
\caption{Performance of SRe$^2$L with real images as initialization.}
\label{tab:real-init}
\begin{tabular}{@{}ccccccc@{}}
\toprule
& \multicolumn{3}{c}{Soft Label} & \multicolumn{3}{c}{Hard Label} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& Random & SRe$^2$L & SRe$^2$L$_\mathrm{Real}$ & Random & SRe$^2$L & SRe$^2$L$_\mathrm{Real}$ \\
\midrule
10 & \std{35.8}{0.2} & \std{33.5}{0.2} & \std{35.3}{0.5} & \std{4.6}{0.1} & \std{1.5}{0.1} & \std{2.5}{0.0} \\
50 & \std{57.2}{0.2} & \std{52.6}{0.1} & \std{53.9}{0.3} & \std{20.6}{0.1} & \std{3.8}{0.0} & \std{6.3}{0.2} \\
100 & \std{61.2}{0.2} & \std{57.4}{0.3} & \std{58.3}{0.1} & \std{31.7}{0.6} & \std{4.9}{0.2} & \std{7.9}{0.2} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Regularization-based Data Augmentation}
\label{appendix:data-mixing}

Table~\ref{tab:reg-based-aug-full} presents a comprehensive evaluation of various data augmentation strategies, including CutMix~\cite{yun2019cutmix}, Cutout~\cite{devries2017improved}, and Mixup~\cite{zhang2017mixup}. The experimental results demonstrate the \textbf{crucial role of appropriate augmentation selection} in data-scarce scenarios. The incorporation of \textit{RandomResizedCrop} proves to be fundamental, substantially improving performance from 21.6\% to 25.6\%.

Among the regularization-based augmentation techniques, Cutout demonstrates a better performance, maintaining consistent accuracy levels (26.2\%, 25.7\%, and 25.3\% with \textit{RandomResizedCrop}). This superiority can be attributed to two key factors: First, Cutout preserves label integrity by avoiding label mixing, which is particularly beneficial in data-scarce regimes. Second, its augmentations are performed on individual images without cross-sample interactions, adhering to the principle of maintaining sample simplicity during training.
In contrast, both CutMix and Mixup show notable performance degradation with increased mixing probabilities, which is especially evident in scenarios with label mixing. When label mixing is employed, performance deteriorates significantly (from 25.5\% to 23.8\% for CutMix, and from 25.9\% to 25.7\% for Mixup at 0.2 mixing probability with \textit{RandomResizedCrop}). This degradation becomes more severe at higher mixing probabilities, with performance dropping to 17.4\% and 7.7\%, respectively, at 1.0 mixing probability.

These findings align with our theoretical framework, suggesting that augmentation strategies maintaining sample simplicity are more effective in data-scarce regimes. The empirical evidence demonstrates that methods introducing complex regularization through label mixing and cross-sample interactions may be detrimental to model performance when training data is limited, supporting our scaling law observations regarding the preference for simpler training samples.

Setting for each strategy:
\begin{itemize}[nosep]
    \item CutMix~\cite{yun2019cutmix}: We follow the original implementation which samples from $\mathrm{Beta}(\alpha, \alpha)$, where $\alpha=1$, which is basically uniform sampling from $(0,1)$. For the label mixing part, we rescale $\lambda$ following \href{https://github.com/clovaai/CutMix-PyTorch}{https://github.com/clovaai/CutMix-PyTorch}.
    \item Mixup~\cite{zhang2017mixup}: We follow the original implementation which samples from $\mathrm{Beta}(\alpha, \alpha)$, where $\alpha=1$, which is basically uniform sampling from $(0,1)$.
    \item Cutout~\cite{devries2017improved}: We use a common cutout size which is $0.5$.
\end{itemize}

\textbf{Remark:}
In the original implementation of SRe$^2$L, CutMix and Mixup do not incorporate label mixing because distillation loss is used.

\begin{table}[H]
\centering
\caption{
Experiments with regularization-based data augmentation of PCA (Ours) in the SGD setting. For \textit{RandomResizedCrop}, the default setting is used, which crop in the range of $(0.08, 1.00)$. The mix probability refers to the likelihood of performing data mixing, where a value of 1.0 indicates that data mixing is always conducted.
When \textit{RandomResizedCrop} is \xmark, it means to use regularization-based data augmentation to entirely replace \textit{RandomResizedCrop}.
Experiments are conducted on ResNet-18, IPC10 of ImageNet-1K.
}
\label{tab:reg-based-aug-full}
\begin{tabular}{@{}ccclll@{}}
\toprule
\multirow{2}{*}{Crop} & \multirow{2}{*}{Data Mixing} & \multirow{2}{*}{Label Mixing} & \multicolumn{3}{c}{Mix Probability} \\
\cmidrule(lr){4-6}
     &             &  & 0.2  & 0.5  & 1.0  \\ \midrule
\cmark & \xmark & - & \multicolumn{3}{c}{25.6} \\ \midrule
\multirow{2}{*}{\cmark} & \multirow{2}{*}{CutMix} & \cmark & 23.8 & 23.0 & 17.4 \\
       &        & \xmark & 25.5 & 24.7 & 23.0 \\ \midrule
\multirow{2}{*}{\cmark} & \multirow{2}{*}{Mixup} & \cmark & 25.7 & 23.0 & 7.7  \\
       &        & \xmark & 25.9 & 25.1 & 17.6 \\ \midrule
\cmark & \textbf{Cutout} & -      & \textbf{26.2} & \textbf{25.7} & 25.3 \\ \bottomrule
\end{tabular}\hfill
\begin{tabular}{@{}ccclll@{}}
\toprule
\multirow{2}{*}{Crop} & \multirow{2}{*}{Data Mixing} & \multirow{2}{*}{Label Mixing} & \multicolumn{3}{c}{Mix Probability} \\
\cmidrule(lr){4-6}
     &             &  & 0.2  & 0.5  & 1.0  \\ \midrule
\xmark & \xmark & - & \multicolumn{3}{c}{21.6} \\ \midrule
\multirow{2}{*}{\xmark} & \multirow{2}{*}{CutMix} & \cmark & 9.8  & 8.1  & 10.5 \\
       &        & \xmark & 15.6 & 14.3 & 12.5 \\ \midrule
\multirow{2}{*}{\xmark} & \multirow{2}{*}{Mixup} & \cmark & 18.9 & 17.4 & 8.4  \\
       &        & \xmark & 19.2 & 18.3 & 15.6 \\ \midrule
\xmark & Cutout & -      & 22.7 & 22.4 & 21.8 \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Poor Performance using Forgetting~\cite{toneva2018an}}
\label{appendix:forgetting}

Fig.~\ref{fig:score-distribution} illustrates the distribution of various score metrics, specifically EL2N~\cite{paul2021deep}, Forgetting~\cite{toneva2018an}, and AUM~\cite{pleiss2020identifying} Scores. These distributions are organized into two rows, with the top row representing the full dataset and the bottom row depicting the ``easiest'' IPC10 subset.

In the analysis of the \textbf{EL2N Score}, the histogram for the full dataset shows an unimodal distribution that peaks around a score of 10, indicating that most scores are concentrated in this range. Additionally, there is a long tail in the distribution towards lower scores.

Examining the \textbf{Forgetting Score}, the Full dataset displays a bimodal distribution with significant frequencies at scores of 0 and 10. This bimodality indicates the presence of two prevalent score categories within the complete dataset. Conversely, the IPC10 Forget Score distribution is dominated by a sharp peak at score 0, reflecting a substantial proportion of instances with no forgetting behavior in the IPC10 subset.

Regarding the \textbf{AUM Score}, the Full dataset illustrates a symmetric distribution centered around a score of 0, indicating balanced score dispersion. The IPC10 AUM Score distribution, however, shows a broader range with a prominent peak near 56 and a gradual decline as scores approach 60. This shift suggests that the IPC10 subset experiences a different range of AUM Scores compared to the full dataset.

The poor performance of forgetting can possibly be explained by the score distribution (see Fig.~\ref{fig:score-distribution}).
We can clearly see that the easiest IPC10 subsets of forgetting scores all have a value of "0," indicating that no forgetting occurs. Because of the nature of the forgetting approach, many images experience no forgetting events at all. In fact, there are approximately 110,000 images without any forgetting events, and we randomly selected 10,000 (roughly 9.1\%) of these images to create our IPC10 dataset. As a result, the 10,000 images are \textbf{indistinguishable} from the remaining images (90.9\%) that also have zero forgetting counts.

\begin{figure}[H]
    \centering
    \includegraphics[width=.95\linewidth]{viz_jpg/distribution.pdf}
    \caption{Sample distribution over the score of different pruning metrics: (a) EL2N, (b) Forgetting, and (c) AUM.
    Top row: both the sample distribution of full and IPC10 datasets.
    Bottom row: zoomed-in view of the distribution of IPC10 dataset.
    IPC10 datasets are selected from the ``easiest'' samples. 
    }
    \label{fig:score-distribution}
\end{figure}


\subsection{Strict Data Balance Is an Implicit Stratification}
\label{appendix:data-balance}

Figure \ref{fig:class-score-distribution} (Top) illustrates the distribution of samples across different classes. A clear severe class imbalance is observed when samples are selected solely based on pruning scores, as shown by the red histogram. Some classes have no images at all, while others contain more than 100 images. This imbalance is particularly noticeable when using Forgetting as the pruning metric.

By enforcing strict class balance, the difficulty of the subset increases as long as class imbalance persists. This is demonstrated in Figure \ref{fig:class-score-distribution} (Bottom), where higher scores in EL2N and Forgetting indicate a harder dataset, while a lower score in AUM suggests the opposite.
Consequently, strict class balance implicitly achieves data stratification~\cite{zheng2023coveragecentric} among easy samples, and it can partly explain Table~\ref{tab:ccs-comparison} why adding additional explicit stratification does not improve the performance as suggested by CCS~\cite{zheng2023coveragecentric}.
Additional stratification applied after strict balancing increases dataset complexity, with particularly noticeable effects in small IPC scenarios.

\begin{table}[H]
\caption{CCS performance comparison on soft and hard label settings. CCS$_\mathrm{AUM}$ denotes stratification performed on AUM.}
\label{tab:ccs-comparison}
\centering
\begin{tabular}{llc|cccc}
\toprule
Setting & IPC & Random & Forgetting & AUM & EL2N & CCS$_\mathrm{AUM}$ \\ \midrule
\multirow{3}{*}{Soft} 
& 10 & \std{35.8}{0.2} & \up{36.1}{0.3} & \upb{41.5}{5.7} & \up{40.8}{5.0} & \up{37.4}{1.6} \\
& 50 & \std{57.2}{0.2} & \same{57.2}{0.0} & \upb{58.5}{1.3} & \up{58.1}{0.9} & \up{58.2}{1.0} \\
& 100 & \std{61.2}{0.2} & \down{61.0}{0.2} & \up{61.5}{0.3} & \up{61.5}{0.3} & \upb{61.6}{0.4} \\ \midrule
\multirow{3}{*}{Hard}
& 10 & \std{4.6}{0.1} & \down{3.4}{1.2} & \up{11.4}{6.8} & \upb{12.2}{7.6} & \up{6.8}{2.2} \\
& 50 & \std{20.6}{0.1} & \down{11.7}{8.9} & \up{30.6}{10.0} & \upb{31.1}{10.5} & \up{29.3}{8.7} \\
& 100 & \std{31.7}{0.6} & \down{18.3}{13.4} & \up{38.7}{7.0} & \up{38.8}{7.1} & \upb{39.0}{7.3} \\
\bottomrule
\end{tabular}
\end{table}




\begin{figure}[H]
    \centering
    \includegraphics[width=.95\linewidth]{viz_jpg/class_score_distribution.pdf}
    \caption{Sample and score distribution over class on both balanced and imbalanced cases.
    Top row: the sample distribution.
    Bottom row: the score distribution over class.
    For visualization purposes, only the first 50 classes are presented.
    IPC10 datasets are visualized and are selected from the ``easiest'' samples. 
    }
    \label{fig:class-score-distribution}
\end{figure}

\subsection{PCA with Different Pruning Methods.}
\label{appendix:pruning-methods}

Our PCA framework is designed to accommodate various pruning methods, and the results are summarized in Table~\ref{tab:pca-pruning}. The experiments were conducted using a hard-label-only approach, with the AdamW optimizer. The standard deviation is computed from three independent runs.

We observed that EL2N~\cite{paul2021deep} and AUM~\cite{pleiss2020identifying} consistently outperform the random baseline by a significant margin. Although AUM performs better than EL2N at IPC50 and IPC100, we chose EL2N as our baseline due to its efficiency, as EL2N utilizes training dynamics only during the early training phase (i.e., the first 10 epochs on the full dataset) while AUM requires full training (i.e., 90 epochs on the full dataset).
A more detailed time breakdown can be found in Appendix~\ref{appendix:computation-cost}.

In contrast, Forgetting~\cite{toneva2018an} shows a substantial performance gap compared to EL2N and AUM; however, it still surpasses the random baseline across various IPC settings without introducing additional overheads. An explanation for the limitations of Forgetting is provided in Appendix~\ref{appendix:forgetting}.

\input{appendix/pca_pruning}

\subsection{Difference between Mosaic Augmentation}
\label{appendix:mosaic}

One approach similar to the ``combining" process is Mosaic Augmentation, introduced in YOLOv4~\cite{bochkovskiy2020yolov4} for object detection tasks, as shown in Figure~\ref{fig:mosaic}. However, the motivation behind it differs significantly. Combining images consolidates information from multiple sources into a single composite image, thereby saving storage space. In contrast, Mosaic Augmentation mixes multiple (i.e., four) images to facilitate the detection of objects outside their normal context. Additionally, at the implementation level, Mosaic Augmentation loads four times as many images per given batch size, necessitating four times the storage. Nevertheless, the non-uniform combination method could potentially be leveraged in our ``combining" approach, which we leave for future study.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{fig/mosaic-min.png}
    \caption{Mosaic Augmentation. (Image directly taken from YOLOv4~\cite{bochkovskiy2020yolov4})}
    \label{fig:mosaic}
\end{figure}

\subsection{Computation Cost Analysis}
\label{appendix:computation-cost}

One significant advantage of our PCA framework is its efficiency. Table~\ref{tab:computation-cost} compares the costs associated with the traditional dataset compression framework, SRe$^2$L, and our PCA method.
Among the three stages of SRe$^2$L, the ``squeeze'' stage is the most time-consuming, particularly when applied to ResNet with the entire ImageNet-1K dataset, which is quite resource-intensive. The parameter storage is 0.04 GB (44M). The second most time-consuming process is the ``recover'' stage. In contrast, the ``relabel'' process takes the least amount of time; however, it can become lengthy if the IPC is large due to the introduction of extensive labels, as noted by \citeauthor{xiao2024large}.
A detailed breakdown of the timing is provided in Table~\ref{tab:relabel-cost}. This table indicates that the I/O time, specifically the time required to save the labels, significantly contributes to the overall CPU time. This can be problematic for devices with limited CPU resources. 

On the contrary, let us consider EL2N~\cite{paul2021deep}, which serves as an example in our primary experiments. The time of the "prune" process involves acquiring the training dynamics, which can be considerably shorter than training the entire model. Furthermore, since our approach is optimization-free, there are no additional costs incurred for combining the images, and we exclusively utilize hard labels instead of soft labels.

\begin{table}[H]
\centering
\caption{Computation Cost of Dataset Compression between Traditional Framework and PCA. IPC-10, ImageNet-1K.}
\label{tab:computation-cost}
\begin{tabular}{@{}lccc@{}}
\toprule
SRe$^2$L~\cite{yin2023squeeze} & Squeeze & Recover & Relabel \\\midrule
Time\tablefootnote{All time data have been tested on a single RTX30390 GPU card.}        & 90 epochs    & 580 mins & 33 mins \\
Storage (GB) & 0.04         & 0.15                    & 5.67    \\ \bottomrule
\end{tabular}\hspace{2cm} 
\begin{tabular}{@{}lccc@{}}
\toprule
PCA          & Prune & Combine \\ \midrule
Time         & 10 epochs & -       \\
Storage (GB) & -         & 0.15    \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Relabel Cost Breakdown}
\label{tab:relabel-cost}
\begin{tabular}{lrrrr}
\toprule
\textbf{Operation} & \multicolumn{2}{c}{\textbf{CPU}} & \multicolumn{2}{c}{\textbf{GPU}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Time (ms) & Memory (MB) & Time (ms) & Memory (MB) \\
\midrule
Data Transfer to GPU & 3.34 & 0.00 & 3.27 & 1,722.79 \\
Mix Augmentation & 0.45 & 22.74 & 0.21 & 0.00 \\
Model Inference & 4.48 & 0.00 & 30.07 & 9.17 \\
\textbf{Write to Disk} & \textbf{22.56} & -13.60 & 0.12 & 0.00 \\
Others (89 ops) & 1.14 & 233.31 & 0.94 & 3,045.84 \\
\bottomrule
\end{tabular}
\end{table}



\newpage

\section{Visualization}
\label{appendix:visualization}


\subsection{Visualization of Dataset Disitillatioin Methods}

Figure~\ref{fig:sre2l} visualizes the result of SRe$^2$L~\cite{yin2023squeeze}.  
Figure~\ref{fig:cda} visualizes the result of CDA~\cite{yin2023dataset}.  
Figure~\ref{fig:gvbsm} visualizes the result of G-VBSM~\cite{shao2023generalized}.  
Figure~\ref{fig:lpld} visualizes the result of LPLD~\cite{xiao2024large}.  
Figure~\ref{fig:dwa} visualizes the result of DWA~\cite{dwa2024neurips}.  
Figure~\ref{fig:rded} visualizes the result of RDED~\cite{sun2023diversity}.
For all distillation methods (except for RDED; \citealt{sun2023diversity}), images undergo strong distortion.


\begin{figure}[H]
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/SRE2L_image_grid-min.jpg}
        \caption{SRe$^2$L~\cite{yin2023squeeze}}
        \label{fig:sre2l}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/CDA_image_grid-min.jpg}
        \caption{CDA~\cite{yin2023dataset}}
        \label{fig:cda}
    \end{minipage}
    \vskip 0.15in
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/GVBSM_image_grid-min.jpg}
        \caption{G-VBSM~\cite{shao2023generalized}}
        \label{fig:gvbsm}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/LPLD_image_grid-min.jpg}
        \caption{LPLD~\cite{xiao2024large}}
        \label{fig:lpld}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/DWA_image_grid-min.jpg}
        \caption{DWA~\cite{dwa2024neurips}}
        \label{fig:dwa}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/RDED_image_grid-min.jpg}
        \caption{RDED~\cite{sun2023diversity}}
        \label{fig:rded}
    \end{minipage}
    \hfill
\end{figure}

\clearpage

\subsection{Visualization of Dataset Pruning Methods}
Figure~\ref{fig:forgetting} visualizes the result of Forgetting~\cite{toneva2018an}.  
Figure~\ref{fig:aum} visualizes the result of AUM~\cite{pleiss2020identifying}.  
Figure~\ref{fig:el2n} visualizes the result of EL2N~\cite{paul2021deep}.  
Figure~\ref{fig:ccs} visualizes the result of CCS~\cite{zheng2023coveragecentric}.
The visualization results of all pruning methods followed the pruning rules, allowing for the clear observation that most of the selected images are distinct and visually easy to identify.

\begin{figure}[H]
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/FORGETTING_easy_balance_image_grid-min.jpg}
        \caption{Forgetting~\cite{toneva2018an}}
        \label{fig:forgetting}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/AUM_easy_balance_image_grid-min.jpg}
        \caption{AUM~\cite{pleiss2020identifying}}
        \label{fig:aum}
    \end{minipage}
    \vskip 0.15in
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/EL2N_easy_balance_image_grid-min.jpg}
        \caption{EL2N~\cite{paul2021deep}}
        \label{fig:el2n}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{viz_jpg/CCS_AUM_easy_balance_mis03_image_grid-min.jpg}
        \caption{CCS~\cite{zheng2023coveragecentric}}
        \label{fig:ccs}
    \end{minipage}
\end{figure}


\subsection{Visualization of PCA}

Figure~\ref{fig:pca_el2n} shows the images of our PCA framework which uses EL2N~\cite{paul2021deep} as the selection metric.
Even when adhering to pruning rules, the cobined images may not appear visually similar. For example, the "sax" class (first row, second column) demonstrates distinct contexts (i.e., placing the sax on a purple background or a musician playing the sax). This further demonstrates the importance of scaling-law aware augmentation, as inappropriate subsequent training augmentations can lead to a significant difficulty increase in the images.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{viz_jpg/OURS_EL2N_easy_balance_image_grid-min.jpg}
    \caption{Ours (PCA based on EL2N).}
    \label{fig:pca_el2n}
\end{figure}

\newpage


\section{Limitation}
\label{appendix:limit}
Our augmentation procedures, including patch extraction, are heuristically designed. While they demonstrate strong empirical effectiveness, their optimality is not theoretically guaranteed.

\section{Future Work}
\label{appendix:future}
Given that the proposed PCA functions as a framework, there is potential to explore \textbf{different choices} of the modules, such as pruning metrics, combining strategies, and specific augmentation methods. It is notable that pruning can extend beyond the original dataset. Instead of only developing new pruning metrics, one could target different datasets. In this paper, the primary reason for pruning on the original dataset is that most existing dataset distillation methods do not outperform random baselines, indicating that original images are sufficiently effective. Hence, there is \textbf{significant value} in considering pruning on potentially high-performing distilled datasets (e.g., YOCO; \citealt{he2024you}) or on generated datasets (e.g., diffusion-based DD methods; \citealt{Su_2024_CVPR}). Beyond accuracy, future frameworks might also jointly optimize additional metrics, such as robustness, fairness, or interpretability, while maintaining the same compressed dataset constraint.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
