\section{Related Works}
\label{sec:related}

\textbf{Dataset Distillation.}
Dataset distillation aims to learn compact and synthetic datasets that achieve a similar performance as the full dataset.
Researchers have developed many frameworks~\cite{wang2018dataset, zhao2021datasetGM, kimICML22, zhao2021dataset, cazenavette2022distillation, liu2023dream, lee2022dataset, zhao2023dataset, wang2022cafe, jiang2022delving, du2022minimizing, shin2023loss, deng2022remember, liu2022dataset, zhao2022synthesizing, wang2023dim, lorraine2020optimizing, nguyen2021dataset, nguyen2021datasetKIP, vicol2022implicit, zhou2022dataset, loo2022efficient, zhang2022accelerating, cui2023scaling, loo2023dataset} to effectively learn the synthetic dataset on small scale dataset like MNIST and CIFAR dataset.

However, scaling the existing framework to a large dataset suffers from unaffordable consumption in both memory and time.
SRe$^2$L~\cite{yin2023squeeze} on the first time achieves noticeable performance by decoupling the optimization process into three phases of squeezing, recovering, and relabeling. 
Follow-up works~\cite{yin2023dataset, sun2023diversity, dwa2024neurips, shao2023generalized, loo2024large} mostly focus on addressing the diversity issue of the recovery phase, with more and more attention paid to the relabeling process~\cite{xiao2024large, zhang2024breaking, qin2024label, kang2024label, yu2025teddy}.
However, most methods use different evaluation settings and lack direct comparison, and the performance of random baseline under the relabeling process is overlooked\footnote{We notice a concurrent work that benchmarks existing dataset distillation methods currently on small-scale datasets (i.e., CIFAR and Tiny-ImageNet), and we encourage interested readers to visit \href{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}.}.

\textbf{Dataset Pruning.}
Dataset pruning selects a representative subset by ranking images with different metrics~\cite{coleman2020selection, toneva2018an, pleiss2020identifying, feldman2020neural, paul2021deep}. 
Most of the reported experiments are focused on small datasets like CIFAR or ImageNet subsets.
Methods that scale to large-scale datasets focus on small or moderate pruning ratio to ensure minimum performance drop~\cite{xia2023moderate, sorscher2022beyond, zheng2023coveragecentric, zhang2024spanning, grosz2024data, abbas2024effective}.
VID~\cite{ben2024distilling} conducts experiments on data pruning methods using knowledge distillation. However, these experiments did not explore extreme pruning ratios, and the baselines were not compared with dataset distillation methods.

\textbf{Dataset Compression.}
Dataset compression intuitively encompasses both dataset distillation and dataset pruning, which can work independently. Existing studies incorporate the pruning process, or coreset selection, before dataset distillation~\cite{liu2023dream, xu2025distill, moser2024distill, shen2024deltsimplediversitydrivenearlylate}. Additionally, YOCO~\cite{he2024you} examines the pruning rules specifically for distilled datasets. However, given the distinctly different nature and settings of these two tasks, it remains unclear which method represents the state-of-the-art (SOTA) in the field of data compression today. This lack of direct comparison may lead to misunderstandings about the data compression task and result in ineffective combinations of methods.