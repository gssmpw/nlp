\section{Related Works}
\label{sec:related}

\textbf{Dataset Distillation.}
Dataset distillation aims to learn compact and synthetic datasets that achieve a similar performance as the full dataset.
Researchers have developed many frameworks____ to effectively learn the synthetic dataset on small scale dataset like MNIST and CIFAR dataset.

However, scaling the existing framework to a large dataset suffers from unaffordable consumption in both memory and time.
SRe$^2$L____ on the first time achieves noticeable performance by decoupling the optimization process into three phases of squeezing, recovering, and relabeling. 
Follow-up works____ mostly focus on addressing the diversity issue of the recovery phase, with more and more attention paid to the relabeling process____.
However, most methods use different evaluation settings and lack direct comparison, and the performance of random baseline under the relabeling process is overlooked\footnote{We notice a concurrent work that benchmarks existing dataset distillation methods currently on small-scale datasets (i.e., CIFAR and Tiny-ImageNet), and we encourage interested readers to visit \href{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}.}.

\textbf{Dataset Pruning.}
Dataset pruning selects a representative subset by ranking images with different metrics____. 
Most of the reported experiments are focused on small datasets like CIFAR or ImageNet subsets.
Methods that scale to large-scale datasets focus on small or moderate pruning ratio to ensure minimum performance drop____.
VID____ conducts experiments on data pruning methods using knowledge distillation. However, these experiments did not explore extreme pruning ratios, and the baselines were not compared with dataset distillation methods.

\textbf{Dataset Compression.}
Dataset compression intuitively encompasses both dataset distillation and dataset pruning, which can work independently. Existing studies incorporate the pruning process, or coreset selection, before dataset distillation____. Additionally, YOCO____ examines the pruning rules specifically for distilled datasets. However, given the distinctly different nature and settings of these two tasks, it remains unclear which method represents the state-of-the-art (SOTA) in the field of data compression today. This lack of direct comparison may lead to misunderstandings about the data compression task and result in ineffective combinations of methods.