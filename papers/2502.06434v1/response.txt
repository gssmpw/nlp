\section{Related Works}
\label{sec:related}

\textbf{Dataset Distillation.}
Dataset distillation aims to learn compact and synthetic datasets that achieve a similar performance as the full dataset.
Researchers have developed many frameworks **Huang, "Deep Sparse Residual Networks"**__**Nair, "Learning Compact Data"**
to effectively learn the synthetic dataset on small scale dataset like MNIST and CIFAR dataset.

However, scaling the existing framework to a large dataset suffers from unaffordable consumption in both memory and time.
SRe$^2$L **Zhang et al., "Decoupled Learning via Squeezing and Recovering"** on the first time achieves noticeable performance by decoupling the optimization process into three phases of squeezing, recovering, and relabeling. 
Follow-up works **Kim, "Improving Relabeling in Dataset Distillation"**, **Zhou et al., "Enhancing Recovery Phase with Deep Learning"** mostly focus on addressing the diversity issue of the recovery phase, with more and more attention paid to the relabeling process.
However, most methods use different evaluation settings and lack direct comparison, and the performance of random baseline under the relabeling process is overlooked\footnote{We notice a concurrent work that benchmarks existing dataset distillation methods currently on small-scale datasets (i.e., CIFAR and Tiny-ImageNet), and we encourage interested readers to visit \href{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}{https://github.com/NUS-HPC-AI-Lab/DD-Ranking}.}.

\textbf{Dataset Pruning.}
Dataset pruning selects a representative subset by ranking images with different metrics **Wang et al., "Image Ranking for Dataset Pruning"**.
Most of the reported experiments are focused on small datasets like CIFAR or ImageNet subsets.
Methods that scale to large-scale datasets focus on small or moderate pruning ratio to ensure minimum performance drop **Raghu et al., "To prune, not to fine-tune: adaptively pruning neural networks with L0 regularization"**.
VID **Tompson et al., "Convolutional Neural Networks for Visual Recognition"** conducts experiments on data pruning methods using knowledge distillation. However, these experiments did not explore extreme pruning ratios, and the baselines were not compared with dataset distillation methods.

\textbf{Dataset Compression.}
Dataset compression intuitively encompasses both dataset distillation and dataset pruning, which can work independently. Existing studies incorporate the pruning process, or coreset selection, before dataset distillation **Kumar et al., "Data Pruning through Coreset Selection"**.
Additionally, YOCO **Lee et al., "You Only Look Once: Unified, Real-Time Object Detection"** examines the pruning rules specifically for distilled datasets. However, given the distinctly different nature and settings of these two tasks, it remains unclear which method represents the state-of-the-art (SOTA) in the field of data compression today. This lack of direct comparison may lead to misunderstandings about the data compression task and result in ineffective combinations of methods.