\section{Related Work}
\label{sec:related-work}

\subsection{Evolution of Sequential Visual Reasoning}
Visual storytelling emerged as an early effort to generate coherent narratives from image sequences~\citep{huang-etal-2016-visual}.
Initial approaches relied on convolutional neural networks~\citep{lecun1995convolutional} for visual feature extraction paired with recurrent neural networks~\citep{hochreiter1997long} for narrative generation~\citep{gonzalez2018contextualize, kim2018glac}.
Although these methods demonstrated that visual and textual information could be integrated to produce compelling stories, they relied on overly broad scene summarization techniques~\citep{hong-etal-2020-diverse,wang2020storytelling}, rather than explicitly modeling the temporal dependencies required to predict future events.

With the introduction of MLLMs such as GPT-4V~\cite{achiam2023gpt}, MM1~\citep{mckinzie2025mm1}, and LLaVA-NeXT~\citep{liu2024llavanext}, significant progress has been made in static image understanding for tasks like image captioning and visual question answering~\citep{liu-etal-2023-visual-storytelling,lin2024improving, zhang2025let}.
Despite these successes, many MLLMs remain optimized for static or non-sequential multi-image inputs, limiting their ability to capture temporal dynamics.
This limitation motivates our work to develop a method that explicitly models temporal dependencies across image sequences.


\subsection{Challenges in Context Length and Generalization}
A critical challenge in sequential visual reasoning lies in effectively handling variable-length temporal contexts~\citep{zhou2024rethinkingvisualdependencylongcontext}.
Many existing models are trained on short or fixed-length sequences and thus struggle when presented with complex temporal spans or variable length contexts~\citep{thawakar2025llamavo1rethinkingstepbystepvisual}.
Furthermore, models show particular difficulties handling subtle temporal dependencies and structured event progressions where actions follow constrained logical sequences, such as in comics or robotics~\citep{wang-etal-2024-mementos}.
A recent analysis suggests that MLLMs rely on surface-level cues~\citep{zhou2024rethinkingvisualdependencylongcontext}, leading to performance degradation when processing extended contexts or adapting to novel scenarios~\citep{imam2025multimodalllmsvisualtemporal}.
These challenges underscore the need for approaches that capture variable-range temporal dependencies and generalize more robustly across diverse domains.




\subsection{Instruction Tuning Over Multi-Turn Conversations}

Instruction tuning~\citep{sanh2022multitask,wei2022finetuned} enhances LLMs by improving their ability to follow general task-agnostic directives while %
requiring limited training data~\citep{zhou2024lima}.
This methodology diverges from conventional fine-tuning approaches by exposing the model to varied instructional formulations, which ensures that the emphasis lies on adhering to directives rather than on task-specific details.~\citep{zhang2024instructiontuninglargelanguage}.
Empirical evaluations demonstrate that instruction-tuned variants consistently outperform competing baselines in output quality across open-ended~\citep{jha2023limit}, knowledge~\citep{jiang-etal-2024-instruction}, and reasoning~\citep{tangmathscale} tasks.

Instruction tuning over multi-turn conversations extend this approach by introducing sequential dependencies between interactions, where each turn builds on prior user-model exchanges~\citep{zhang2025surveymultiturninteractioncapabilities}.
Despite these advances in LLMs, such multi-turn conversational techniques have not yet been applied to multimodal settings.

\imagechain{} extends the multi-turn conversational paradigm to MLLMs for improved sequential visual reasoning by leveraging the strengths of instruction tuning and multi-turn interactions for integrating sequential visual data.
Rather than relying on emerging user-model interactions, we explicitly structure the conversation as a fixed sequence~\citep{wang-etal-2024-instruct}.
We interleave visual embedding tokens with scene descriptions to build a controlled context that emphasizes temporal dependencies.
Each turn poses a targeted question about an upcoming scene, with the expected response being a text description.
We call this task \emph{next-scene description} (Figure \ref{fig:method}), where the goal is to generate an accurate description of a visual scene based on preceding frames and annotations. 


\begin{figure*}
\centering
\small

\begin{tcolorbox}[
    colback=textbox,
    colframe=headerbox,
    colbacktitle=headerbox,
]
    \texttt{\textcolor{cgreen}{<s>} \textcolor{cmint}{USER:}} What is happening in this image? \textcolor{corange}{\texttt{<Image><image></Image>}}\newline
    \texttt{\textcolor{cmint}{ASSISTANT:}} A rider wearing a black t-shirt is riding a bicycle on a brown surface while two people are sitting on their bicycle behind the rider. \texttt{\textcolor{cgreen}{</s>}}\newline
    \texttt{\textcolor{cgreen}{<s>} \textcolor{cmint}{USER:}} What is happening in the next image? \textcolor{corange}{\texttt{<Image><image></Image>}}\newline
    \texttt{\textcolor{cmint}{ASSISTANT:}} The rider wearing a black t-shirt jumps on the trampoline. \texttt{\textcolor{cgreen}{</s>}}\newline
    \texttt{\textcolor{cgreen}{<s>} \textcolor{cmint}{USER:}} What is happening in the next image? \textcolor{corange}{\texttt{<Image><image></Image>}}\newline
    \texttt{\textcolor{cmint}{ASSISTANT:}}  The rider wearing a black t-shirt gets disbalanced and falls. \texttt{\textcolor{cgreen}{</s>}}\newline
    \texttt{\textcolor{cgreen}{<s>} \textcolor{cmint}{USER:}} What is happening in the next image? \textcolor{corange}{\texttt{<Image><image></Image>}}\newline
    \texttt{\textcolor{cmint}{ASSISTANT:}}
\end{tcolorbox}
\caption{%
Multi-turn conversation design for a story with four scenes, where each turn corresponds to a scene. A turn begins with a user question and ends with the assistant's response. The context includes three completed turns (i.e., three scenes), along with the next user question and the corresponding visual cue, which are used to generate the \emph{next-scene description}.
}
\label{fig:multi-turn-conv}
\end{figure*}