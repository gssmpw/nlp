\section{Introduction}
\label{sec:intro}

Aligning two point clouds to compute their relative 3D transformation is a critical task in numerous robotic applications, including LiDAR odometry~\cite{vizzo2023kissicp}, loop closure registration~\cite{arce2023padloc}, and map-based localization~\cite{hroob2024generalizable}.
In this work, we specifically discuss map-based localization, which not only generalizes the other aforementioned tasks but is also critical for improving the efficiency and autonomy of mobile robots in environments where pre-existing map data is available.

Although place recognition~\cite{keetha2024anyloc, kim2019scancontextimage} or GNSS readings can provide an approximate initial estimate, their accuracy is generally insufficient for obtaining precise 3D poses relative to the map. In contrast, global point cloud registration~\cite{fischler1987ransac, zhou2016fgr} enables accurate 3D localization but necessitates the identification of reliable point correspondences between the point clouds. 
These correspondences are typically established by iterating over all points in the source point cloud to identify the most similar counterparts in the target frame.
Similarity is assessed using point descriptors, which are abstract feature representations of a point, e.g., encoding the geometry of its local environment.
In scan-to-map registration, point descriptors must be as unique as possible since the number of potential combinations grows with $\mathcal{O}(m \cdot n)$, referring to the number of points in the scan and the map. An additional challenge arises from temporal changes in the environment, such as seasonal variations or ongoing construction, necessitating point descriptors that are robust to such changes for long-term applicability~\cite{bianco2016nclt}.

Given the significance of this task, numerous point descriptors have been proposed, encompassing both traditional handcrafted~\cite{rusu2009fpfh} and learning-based~\cite{choy2019fcgf, poiesi2021dip, zeng20173dmatch} designs, primarily relying on 3D geometric features.
While learning-based descriptors tend to exhibit greater expressiveness than handcrafted methods, they often fail to generalize effectively to out-of-training domains and different point cloud representations, such as between RGB-D data and LiDAR scans~\cite{poiesi2023gedi}.
\looseness=-1

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.png}
    \vspace*{-.7cm}
    \caption{Initialization-free registration of a LiDAR scan to a large-scale 3D map requires highly expressive point descriptors. We demonstrate that DINOv2~\cite{oquab2024dinov2} features from surround-view images allow finding robust point correspondences, even with map data recorded more than a year before. In the map, the registered LiDAR scan is shown in red. The colors of the LiDAR scan (\textit{top right}) and the map (\textit{bottom}) are obtained using principal component analysis performed on the high-dimensional DINOv2 features.}
    \label{fig:teaser}
    \vspace*{-.4cm}
\end{figure}

In this work, we address the task of long-term scan-to-map registration by leveraging the advances made by recent visual foundation models. Our main contribution is to demonstrate that using DINOv2~\cite{oquab2024dinov2} features, obtained from surround-view images, as point descriptors allows finding highly robust point correspondences.
We argue that using the additional vision modality does not pose a large burden as this combination is a common sensor setup in mobile robotics~\cite{barnes2020robotcarradar, caesar2020nuscenes, bianco2016nclt} and the camera is relatively inexpensive compared to the LiDAR.

The key idea behind our approach is to leverage the superior generalization capabilities of recent visual foundation models, like DINOv2, compared to networks operating in the 3D space.
Furthermore, our approach is hence agnostic to the shape of a point cloud, enabling correspondence search between sparse LiDAR scans and dense 3D voxel-maps. Using DINOv2-based point descriptors effectively allows for implicit semantic matching between points and, importantly, does not require re-training an in-domain descriptor network.

We make three claims:
First, we demonstrate that coupling these descriptors with traditional registration algorithms, such as RANSAC~\cite{fischler1987ransac} or ICP~\cite{vizzo2023kissicp}, facilitates robust 3D localization in a map that was recorded over a year before~\cite{bianco2016nclt}.
Second, although conceptually simple, our method substantially outperforms more complex baseline techniques.
Third, our approach is robust to temporal changes in the environment that have occurred since the map was created.

We validate these claims through extensive experiments, showing that our approach outperforms the best baseline by $+24.8$ and $+17.3$ registration recall on the NCLT~\cite{bianco2016nclt} and Oxford Radar RobotCar~\cite{barnes2020robotcarradar} datasets.
To facilitate reproducibility and future research on long-term map registration, upon acceptance we will release our code along with instructions to re-create the evaluation scenes from our experiments.
To the best of our knowledge, this work presents the first approach to combining visual foundation models with traditional LiDAR registration techniques.
\looseness=-1
