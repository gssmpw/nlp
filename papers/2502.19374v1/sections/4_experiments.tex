\section{Experiments}

The main focus of this work is to enable LiDAR scan-to-map registration that is robust to the challenges arising in long-term scenarios. In our experiments, we demonstrate the effectiveness and capabilities of our approach to support our key claims:
First, DINOv2~\cite{oquab2024dinov2} features can serve as point descriptors that can be integrated into traditional registration algorithms to facilitate robust 3D registration in a map that was recorded more than a year before.
Second, such a relatively straightforward approach outperforms more complex baseline techniques.
Third, these descriptors are robust to temporal changes in the environment that have occurred since the map was created.
We begin this section by describing the details of our experimental setup and defining the evaluation metrics used. Afterward, we provide results that support our claims and showcase the performance of our approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experimental Setup}
\label{ssec:experimental-setup}

To verify our claims, we generate several scenes that present the problem formally defined in \cref{ssec:problem-definition}.
In particular, we identify the NCLT~\cite{bianco2016nclt} and the Oxford Radar RobotCar~\cite{barnes2020robotcarradar} as two of the few long-term datasets containing both LiDAR and surround-view RGB images. Note that neither is part of the LVD-142M dataset~\cite{oquab2024dinov2} used to train DINOv2. The NCLT dataset comprises a total of 27 sessions recorded on a university campus spread out over 15 months, i.e., covering various seasons, illuminations, and environmental conditions. The Oxford Radar RobotCar dataset comprises recordings from 7 different days spread out over 1.5 weeks. It captures vehicle-centric urban data with weather conditions ranging from sunny to varying degrees of overcast.
Both datasets contain global pose data that is consistent across recordings.
For each dataset, we select the first of the provided recordings for mapping, while other recordings are used for registration. As shown in \cref{tab:dataset-recordings}, we carefully select these recordings; first, to maximize spread over the dataset to increase the probability of semantic and geometric changes and, second, to cover all available seasonal and weather conditions.

To construct a scene, we sample a position $\rho_i$ from the mapping route. If all registration recordings contain a point cloud associated with a pose in the vicinity of $\rho_i$, we use these point clouds to create  $\{ \mathcal{S}_1, \dots, \mathcal{S}_r \}_i$ and $\{ \mathcal{S}^D_1, \dots, \mathcal{S}^D_r \}_i$, i.e., a set of LiDAR scans with corresponding DINOv2-based~\cite{oquab2024dinov2} point descriptors. As listed in \cref{tab:dataset-recordings}, we use $r=5$ for NCLT and $r=3$ for RobotCar.
For mapping, we select the point clouds along the route within a \SI{150}{\meter} radius around $\rho_i$ and with a distance of \SI{2}{\meter} between scans, simulating keyframes in LiDAR SLAM, and extract the point descriptors. That is, the maximum size of the map is $\SI{300}{\meter} \times \SI{300}{\meter}$. Since the accuracy of the global poses provided in the datasets is insufficient for point cloud accumulation, we refine them with KISS-ICP~\cite{vizzo2023kissicp}. Finally, we downsample the accumulated resulting point cloud to a voxel size of \SI{0.25}{\meter}, representing the 3D map $M_i$ with point descriptors $M^D_i$. Note that we do not remove potentially dynamic objects, e.g., cars, from the map.
To measure the registration error, we generate ground truth transformations $\{\mathbf{T}_1, \dots, \mathbf{T}_r\}$ by running point-to-point ICP initialized with the pose from the dataset and manually verifying the correct registration.
For both datasets, we construct 25 scenes, i.e., $i \in [1, 25]$, resulting in a sample size of 125 and 75 for NCLT and Oxford Radar RobotCar, respectively. For examples of the scenes, we refer to the qualitative results in \cref{fig:results-nclt,fig:results-robotcar}.
To facilitate the utilization of these scenes as a benchmark in future research, we provide a recreation script along with comprehensive instructions in our code release.

\begin{table}
\footnotesize
\centering
\caption{Recordings for Scene Generation}
\vspace{-0.2cm}
\label{tab:dataset-recordings}
\setlength\tabcolsep{3.0pt}
\begin{threeparttable}
    \begin{tabular}{ l | cc | cccc }
    \toprule
    \textbf{Recording date} & \textbf{Map.} & \textbf{Reg.} & Time & Sky & Foliage & Snow \\
    \midrule
    \multicolumn{3}{l}{\textbf{NCLT}} \\
    [.5ex]
    2012-01-08 & \cmark & & Midday & Partly cloudy & -- & -- \\
    2012-02-12 & & \cmark & Midday & Sunny & -- & \cmark \\
    2012-03-17 & & \cmark & Morning & Sunny & -- & -- \\
    2012-05-26 & & \cmark & Evening & Sunny & \cmark & -- \\
    2012-10-28 & & \cmark & Midday & Cloudy & -- & -- \\
    2013-04-05 & & \cmark & Afternoon & Sunny & -- & \cmark \\
    \midrule
    \multicolumn{4}{l}{\textbf{Oxford Radar RobotCar}} \\
    [.5ex]
    2019-01-10 & \cmark & & 11:46 & Cloudy & -- & -- \\
    2019-01-15 & & \cmark & 13:06 & Sunny & -- & -- \\
    2019-01-17 & & \cmark & 14:03 & Partly cloudy & -- & -- \\
    2019-01-18 & & \cmark & 15:20 & Cloudy & -- & -- \\
    \bottomrule
    \end{tabular}
    Overview of the recordings from the NCLT~\cite{bianco2016nclt} and the Oxford Radar RobotCar~\cite{barnes2020robotcarradar} datasets used for mapping (\textit{Map.}) and scan registration (\textit{Reg.}).
\end{threeparttable}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t]
\footnotesize
\centering
\caption{Scan-to-Map Registration}
\vspace{-0.2cm}
\label{tab:scan-to-map-exp}
\setlength\tabcolsep{5.0pt}
\begin{threeparttable}
    \begin{tabular}{ ll | cccc | cccc }
        \toprule
        \multicolumn{2}{l|}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{NCLT}} & \multicolumn{4}{c}{\textbf{Oxford Radar RobotCar}} \\
        Registration & Descriptor & RTE [m] & RRE [$\degree$] & RR [\%] & ICP-RR [\%] & RTE [m] & RRE [$\degree$] & RR [\%] & ICP-RR [\%] \\
        \midrule
        RANSAC & FPFH~\cite{rusu2009fpfh} & 47.51$\pm$46.54 & 59.95$\pm$62.21 & 0.00 & 37.60 & 32.56$\pm$40.18 & 36.87$\pm$57.25 & 0.00 & 40.00 \\
        TEASER++~\cite{yang2021teaser} & FPFH~\cite{rusu2009fpfh} & 60.15$\pm$34.94 & 121.67$\pm$48.95 & 0.00 & 0.00 & 69.51$\pm$41.98 & 129.01$\pm$49.91 & 0.00 & 0.00 \\
        PointDSC~\cite{bai2021pointdsc} & FPFH~\cite{rusu2009fpfh} & 103.16$\pm$48.70 & 129.75$\pm$47.90 & 0.00 & 0.00 & 65.28$\pm$45.19 & 128.01$\pm$52.22 & 1.33 & 2.67 \\
        RANSAC & DIP~\cite{poiesi2021dip} & 42.94$\pm$45.26 & 49.86$\pm$56.08 & 0.00 & 40.00 & 24.52$\pm$32.96 & 45.08$\pm$63.06 & 0.00 & 40.00 \\
        RANSAC & GeDi~\cite{poiesi2023gedi} & 48.93$\pm$45.12 & 66.75$\pm$65.59 & 0.00 & 32.00 & 23.50$\pm$31.15 & 41.26$\pm$62.85 & 0.00 & 44.00 \\
        RANSAC & FCGF~\cite{choy2019fcgf} & 22.74$\pm$33.53 & 31.20$\pm$48.44 & 0.80 & 64.00 & 20.34$\pm$32.29 & 37.92$\pm$62.81 & 1.33 & 49.33 \\
        PointDSC~\cite{bai2021pointdsc} & FCGF~\cite{choy2019fcgf} & 69.50$\pm$48.40 & 96.86$\pm$51.55 & 0.00 & 12.00 & 73.58$\pm$45.43 & 93.51$\pm$64.32 & 0.00 & 0.00 \\
        RANSAC & SpinNet~\cite{ao2021spinnet} & 33.70$\pm$42.47 & 41.00$\pm$55.14 & 0.00 & 54.40 & 23.39$\pm$35.86 & 30.57$\pm$56.48 & 0.00 & 56.00 \\
        RANSAC & GCL~\cite{Liu2023gcl} & 15.35$\pm$26.84 & 23.67$\pm$45.83 & 13.60 & 75.20 & 11.81$\pm$25.09 & 21.84$\pm$48.07 & 30.67 & 77.33 \\
        \grayrule
        \multicolumn{2}{l}{\textit{Our method}:} \\
        [.25ex]
        TEASER++~\cite{yang2021teaser} & DINOv2~\cite{oquab2024dinov2} & 5.29$\pm$10.02 & 27.98$\pm$57.09 & 18.40 & 83.20 & 13.45$\pm$34.93 & 22.18$\pm$53.69 & 49.33 & 81.33 \\
        RANSAC & DINOv2~\cite{oquab2024dinov2} & 0.40$\pm$0.31 & 1.01$\pm$0.84 & 77.60 & \textbf{100.00} & 3.36$\pm$13.65 & 5.20$\pm$26.25 & 82.67 & \textbf{94.67} \\
        RANSAC + ICP & DINOv2~\cite{oquab2024dinov2} & \textbf{0.01$\pm$0.02} & \textbf{0.03$\pm$0.06} & \textbf{100.00} & \textbf{100.00} & \textbf{3.12$\pm$13.63} & \textbf{4.52$\pm$26.21} & \textbf{94.67} & \textbf{94.67} \\
        \bottomrule
    \end{tabular}
    \footnotesize
    We report the mean and standard deviation of the relative translation error~(RTE) and the relative rotation error~(RRE) as defined in \cref{ssec:metrics}. The registration recall~(RR) denotes the success rate, where success is defined as $\text{RTE} < \SI{0.6}{\meter}$ and $\text{RRE} < \SI{1.5}{\degree}$. The recall after refinement with ICP is listed as ICP-RR.
    DIP and GeDi are trained on the 3DMatch dataset~\cite{zeng20173dmatch} (RGB-D data). FCGF, SpinNet, GCL, and PointDSC are trained on the KITTI dataset~\cite{geiger2012kitti} (LiDAR scans). \looseness=-1
\end{threeparttable}
\vspace*{-.3cm}
\end{table*}

\subsection{Evaluation Metrics}
\label{ssec:metrics}

Similar to prior work~\cite{ao2021spinnet, bai2020d3feat, bai2021pointdsc, choy2019fcgf}, we use the following evaluation metrics:
First, the relative translation error~(RTE) measured in meters.
Second, the relative rotation error~(RRE) measured in degrees.
Formally, the RTE and RRE are defined as:\looseness=-1
\begin{align}
    \text{RTE} &= ||\mathbf{\hat{t}} - \mathbf{t}||_2 \\
    \text{RRE} &= \arccos \left( \frac{\text{tr}(\mathbf{\hat{R}}^\intercal \mathbf{R}) - 1}{2} \right) \, ,
\end{align}
where the transform $\mathbf{T} \in \text{SE}(3)$ is decomposed into a translation $\mathbf{t}$ and rotation $\mathbf{R}$. The hat ($\hat{\cdot}$) denotes the estimated transform. The operators $\text{tr}(\cdot)$ and $(\cdot)^\intercal$ are the trace and transpose of a matrix.
Third, we report the registration recall~(RR) denoting the percentage of successful registrations, i.e., both the RTE and RRE are below a given threshold. While previous works have used different thresholds~\cite{ao2021spinnet, bai2020d3feat, bai2021pointdsc, Liu2023gcl}, we adopt the criterion of Liu~\textit{et~al.}~\cite{Liu2023gcl} as it meets the expected error range of many baseline techniques in the more simple scan-to-scan registration tasks (see \cref{ssec:exp-scan-to-map}). That is, a registration is considered a success if $\text{RTE} < \SI{0.6}{\meter}$ and $\text{RRE} < \SI{1.5}{\degree}$.
Finally, we investigate whether the accuracy of the descriptor-based global registration is sufficient to initialize ICP. We measure its performance by recomputing the registration recall after ICP-based pose refinement~(ICP-RR).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Scan-to-Map Registration}
\label{ssec:exp-scan-to-map}

We compare our proposed approach to a variety of baselines that can be categorized as follows:
(1) the popular handcrafted descriptor FPFH~\cite{rusu2009fpfh} coupled with three outlier rejection schemes, namely, RANSAC~\cite{fischler1987ransac}, TEASER++~\cite{yang2021teaser}, and PointDSC~\cite{bai2021pointdsc}, which is a learning-based approach trained on the KITTI dataset~\cite{geiger2012kitti};
(2) the learning-based descriptors DIP~\cite{poiesi2021dip} and GeDi~\cite{poiesi2023gedi}, which are trained on RGB-D data of the 3DMatch dataset~\cite{zeng20173dmatch} but claimed generalization to LiDAR scans~\cite{poiesi2021dip, poiesi2023gedi};
(3) the learning-based descriptors FCGF~\cite{choy2019fcgf}, SpinNet~\cite{ao2021spinnet}, and GCL~\cite{Liu2023gcl}, which are trained on the KITTI dataset~\cite{geiger2012kitti}. For categories (2) and (3), we predominantly rely on the RANSAC algorithm for point cloud registration and follow prior work~\cite{bai2020d3feat, zeng20173dmatch} using 50,000 iterations without early stopping. For the baselines, we use the top 5,000 point correspondences. For SpinNet~\cite{ao2021spinnet}, we compute the descriptors only for 7,500 randomly sampled points due to GPU memory constraints~(\SI{16}{\giga\byte}).

\begin{table}
\footnotesize
\centering
\caption{Scan-to-Scan Registration}
\vspace{-0.2cm}
\label{tab:sanity-exp}
\setlength\tabcolsep{3.0pt}
\begin{threeparttable}
    \begin{tabular}{ ll | cc | cc }
        \toprule
        \multicolumn{2}{l|}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{KITTI}} & \multicolumn{2}{c}{\textbf{NCLT}} \\
        Registration & Descriptor & RTE [m] & RRE [°] & RTE [m] & RRE [°] \\
        \midrule
        \multicolumn{2}{l}{\textit{Handcrafted descriptor}:} \\
        [.25ex]
        ICP & \textit{n/a} & 11.00 & 5.86 & 10.91 & 6.32 \\
        RANSAC & FPFH~\cite{rusu2009fpfh} & 0.87 & 1.13 & 1.35 & 3.55 \\
        TEASER++~\cite{yang2021teaser} & FPFH~\cite{rusu2009fpfh} & 0.04 & 0.10 & 0.54 & 3.24 \\
        PointDSC~\cite{bai2021pointdsc} & FPFH~\cite{rusu2009fpfh} & 0.06 & 0.15 & 2.91 & 9.70 \\
        \grayrule
        \multicolumn{4}{l}{\textit{Descriptors trained on 3DMatch~\cite{zeng20173dmatch}}:} \\
        [.25ex]
        RANSAC & DIP~\cite{poiesi2021dip} & 0.16 & 0.22 & 0.80 & 2.25 \\
        RANSAC & GeDi~\cite{poiesi2023gedi} & 0.50 & 0.73 & 1.71 & 4.25 \\
        \grayrule
        \multicolumn{4}{l}{\textit{Descriptors trained on KITTI~\cite{geiger2012kitti}}:} \\
        [.25ex]
        RANSAC & FCGF~\cite{choy2019fcgf} & 0.10 & 0.15 & 0.45 & 1.36 \\
        PointDSC~\cite{bai2021pointdsc} & FCGF~\cite{choy2019fcgf} & 0.07 & 0.14 & 0.51 & 1.64 \\
        RANSAC & SpinNet~\cite{ao2021spinnet} & 0.09 & 0.15 & 0.46 & 1.28 \\
        RANSAC & GCL~\cite{Liu2023gcl} & 0.09 & 0.14 & 0.46 & 1.26 \\
        \grayrule
        \multicolumn{2}{l}{\textit{Our method}:} \\
        [.25ex]
        RANSAC & DINOv2~\cite{oquab2024dinov2} & -- & -- & 0.56 & 1.44 \\
        \bottomrule
    \end{tabular}
    \footnotesize
    We underline the effect of a domain change on various point descriptors used as baselines in our experiments. While the descriptors trained on the KITTI dataset~\cite{geiger2012kitti} perform well within the same domain, they suffer from degradation when tested on the NCLT dataset~\cite{bianco2016nclt}. Note that PointDSC~\cite{bai2021pointdsc} is also trained on KITTI. Due to the lack of surround-view images, we do not employ our method on KITTI. In contrast to the primary use case of this work, previous studies mostly considered scan-to-scan registration.
\end{threeparttable}
\vspace*{-.4cm}
\end{table}

A core advantage of employing DINOv2~\cite{oquab2024dinov2} is exploiting its strong generalization capability across various semantic domains and decoupling the point descriptors from the density of a point cloud, e.g., RGB-D data versus LiDAR scans. Many learning-based descriptors suffer from such domain changes requiring in-domain training data and hindering their general applicability. To underline this effect and to establish the groundwork for the main experiment, we first report the performance of the baselines for the more simple scan-to-scan registration task. This task is mainly considered by previous studies in the context of LiDAR odometry. Here, the two point clouds are highly similar in their geometry and a strong initial guess is available. We simulate the initial guess by perturbing the ground truth transform with noise sampled as:
\begin{equation}
\begin{aligned}
    \mathbf{t}_x, \mathbf{t}_y &\sim \mathcal{N}(0, 10) \, , \mathbf{t}_y \sim \mathcal{N}(0, 1) \, , \\
    \mathbf{R}_x, \mathbf{R}_y &\sim \mathcal{N}(0, 2) \, , \mathbf{R}_z \sim \mathcal{N}(0, 10) \, ,
\end{aligned}
\end{equation}
with $\mathbf{t}$ and $\mathbf{R}$ referring to the translational and rotational components measured in meters and degrees, respectively.
In \cref{tab:sanity-exp}, we report the average RTE and RRE on samples from the KITTI and NCLT datasets. For KITTI, we sample 125 pairs of two consecutive scans from sequence 08, which is not in the descriptors' training set~\cite{ao2021spinnet, choy2019fcgf}. For NCLT, we use the scan of the map that is closest to the incoming LiDAR scan. Note that we do not employ our method on KITTI due to the lack of surround-view images. The key insights from \cref{tab:sanity-exp} are as follows: (1) On KITTI, the error of the in-domain trained descriptors is substantially smaller than of the ones trained on 3DMatch; (2) We observe poor performance when the training and testing domains differ, i.e., from 3DMatch to KITTI or NCLT, and from KITTI to NCLT; (3) Most descriptors achieve a decent accuracy on NCLT for the scan-to-scan registration task.
\looseness=-1

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/registration_recall.pdf}
    \vspace*{-1.0cm}
    \caption{We visualize the registration recall (RR) for a range of success thresholds obtained by linear inter/extrapolation of the thresholds used by GCL~\cite{Liu2023gcl} (\textit{left dashed line}) and SpinNet~\cite{ao2021spinnet} (\textit{right dashed line}). To perform scan-to-map registration, we couple RANSAC with the specified point descriptors.}
    \label{fig:registration-recall}
    \vspace*{-.5cm}
\end{figure}

In the main experiment, we support our claim that our proposed DINOv2-based point descriptors can be coupled with traditional registration schemes while outperforming previous baselines. We now consider scan-to-map registration using the extracted scenes as described in \cref{ssec:experimental-setup}. 
We report results for the metrics defined in \cref{ssec:metrics} for both the NCLT dataset and the Oxford Radar RobotCar dataset in \cref{tab:scan-to-map-exp}. The most important observation is that our proposed method is the only one that achieves consistently low registration errors. For the ICP-based refinement, our method substantially outperforms the best baseline (GCL~\cite{Liu2023gcl}) by $24.8$ (NCLT) and $17.3$ (RobotCar) percentage points, showing \SI{100}{\percent} recall on NCLT.
We hypothesize that the gap to \SI{100}{\percent} on the Oxford Radar RobotCar is mainly caused by wrong point-to-pixel projections, e.g., due to erroneous extrinsic calibration, jerky ego-motion, and differences in the sensors' viewpoints. An indicator for this hypothesis is the observation that some points belonging to buildings are assigned tree-like descriptors if there is a tree in front of the wall.
For completeness, we also report results when replacing RANSAC with TEASER++~\cite{yang2021teaser}, achieving higher registration recalls than all baseline methods.
A further key insight is the large standard deviation of the errors of the baseline methods, whereas the results of our method rarely fluctuate. Finally, we note that the baselines yield almost no global registration meeting the success thresholds, resulting in \SI{0.00}{\percent} registration recall. To further investigate this observation and to incorporate more relaxed thresholds as used in other studies, we recompute the registration recall (RR) for additional thresholds. In particular, we use linear inter/extrapolation of the thresholds based on GCL~\cite{Liu2023gcl} (\SI{0.6}{\meter}, \SI{1.5}{\degree}) and SpinNet~\cite{ao2021spinnet} (\SI{2.0}{\meter}, \SI{5.0}{\degree}). We visualize the registration recalls in \cref{fig:registration-recall} for all descriptors coupled with RANSAC. The recall of our DINOv2-based descriptors yields a high recall even for strict thresholds, eventually converging towards the recall after ICP refinement. The recall of the other baselines slowly increases for large success thresholds, failing to achieve accurate map-based localization.

We conclude this experiment by visualizing successful registrations in \cref{fig:results-nclt,fig:results-robotcar}. Note that the colors of the 3D map are obtained via principal component analysis of the initial NCLT scene. Following previous work~\cite{oquab2024dinov2}, we adopt the first three components as color channels in the RBG space. To identify the registered LiDAR scan within the 3D map, we do not employ this colorization to the scan but show it in red.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/map_robustness.png}
    \vspace*{-.6cm}
    \caption{To show the robustness of our proposed approach, we remove semantic entities from the 3D map. (1) The original map. (2) We identify tree-like points colored in red using the DINOv2-based descriptors. (3) We assign these points to separate clusters shown in different colors. (4) We randomly remove some clusters from the 3D map, highlighted by the red boxes. \looseness=-1}
    \label{fig:outdated-map}
    \vspace*{-.3cm}
\end{figure}

\subsection{Robustness to Environmental Changes}

This experiment is designed to support our third claim, i.e., the DINOv2-based descriptors are robust to temporal environmental changes resulting in outdated map data.
While the previous experiment provides some insight due to the seasonal variations, e.g., snow and foliage in the NCLT dataset~\cite{bianco2016nclt}, we attempt to further amplify long-term changes. On the NCLT dataset, we carefully remove distinct objects from the 3D map following the steps visualized in \cref{fig:outdated-map}. (1) Given an input map, (2) we initially query all tree-like points on the map by considering the similarity of the point descriptors with the descriptor of a point that is identified as part of a tree by a human supervisor. For simplicity, we adopt the Euclidean distance in the sRGB space after converting the point descriptors to the same PCA space as used in the visualization throughout this manuscript. If the distance is less than 50, we assume the point to be part of a tree. (3) Afterward, we use HDBSCAN~\cite{mcinnes2017hdbscan} to cluster the tree-like points using their 3D coordinates. (4) Finally, for each cluster, we decide with a given probability whether to remove it from the map. Since the urban scenes of the Oxford Radar RobotCar dataset~\cite{barnes2020robotcarradar} contain fewer trees, we invert the idea. In particular, we insert up to 100 additional trees in the scenes that we previously extracted from an NCLT scene.

In \cref{fig:robustness-recall}, we report the registration recall after ICP refinement (ICP-RR) for both experiments using RANSAC and TEASER++~\cite{yang2021teaser} with our proposed point descriptors. We further compute the scores for the two best-performing baselines (see \cref{tab:scan-to-map-exp}), i.e., GCL~\cite{Liu2023gcl} and FCGF~\cite{choy2019fcgf}. While the baseline descriptors suffer from some degradation due to increasing changes in the reference map, our DINOv2-based point descriptor results in stable registration throughout the experiments, supporting our claim of its robustness.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/robustness_map_changes.pdf}
    \vspace*{-0.8cm}
    \caption{We visualize the registration recall after ICP refinement (ICP-RR) for the removal and insertion of trees into the 3D map. Unlike the baseline methods, our proposed DINOv2-based point descriptor results in stable registration underlining its robustness.}
    \label{fig:robustness-recall}
    \vspace*{-.3cm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{figures/results_nclt.png}
    \vspace*{-.6cm}
    \caption{Qualitative results on the NCLT dataset~\cite{bianco2016nclt}. We colorize the 3D map by taking the first three components of performing principal component analysis~(PCA) on the point descriptors of the map. The successfully registered LiDAR scan is shown in red.}
    \label{fig:results-nclt}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1.\linewidth]{figures/results_robotcar.png}
    \vspace*{-.6cm}
    \caption{Qualitative results on the Oxford Radar RobotCar dataset~\cite{barnes2020robotcarradar}. We colorize the 3D map by taking the first three components of performing principal component analysis~(PCA) on the point descriptors of the map. The successfully registered LiDAR scan is shown in red.}
    \label{fig:results-robotcar}
\end{figure*}

