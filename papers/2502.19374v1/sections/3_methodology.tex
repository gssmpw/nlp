\section{Technical Approach}

In this section, we first formally define the problem addressed in this work. Then, we explain how to extract the point descriptors based on a visual foundation model. Finally, we elaborate on how we employ these descriptors for robust scan-to-map registration. We illustrate the separate steps in \cref{fig:overview}. \looseness=-1

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Problem Definition}
\label{ssec:problem-definition}

In this work, we consider the following scenario:
A voxelized 3D map $M \in \mathbb{R}^{n \times 3}$ is provided, where $n$ denotes the number of 3D points stored in the map. At test-time, we receive a LiDAR scan $S \in \mathbb{R}^{m \times 3}$ composed of $m$ 3D points. Furthermore, for both $M$ and $S$, corresponding surround-view RGB camera data is available. The goal is to find the six degrees of freedom (6DoF) transform $\mathbf{T} \in \text{SE}(3)$ that correctly registers the LiDAR scan to the map.
We further assume a rough initial position $\hat{P} \in \mathbb{R}^3$ is given within approximately \SI{100}{\meter} around the true position, reducing the size of the relevant part of the map to $k << m$ while preserving $k >> n$. Such an initial position could be obtained via place recognition~\cite{keetha2024anyloc, kim2022scancontextpp} or GNSS readings.
Importantly, in long-term scan-to-map registration, the LiDAR scan can be recorded a considerable amount of time after the map was created, i.e., there might be a semantic and geometric discrepancy between the 3D map representation and the current state of the environment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Point Descriptor Extraction}
\label{ssec:point-descriptors}

In this paragraph, we describe step 1) of \cref{fig:overview}.
While we extract the point descriptors of the LiDAR scan~$S$ at test-time, we pre-compute the descriptors of the map $M$ in an offline fashion.
Commonly, 3D point-based mapping approaches rely on the concept of keyframes to frequently identify LiDAR scans that are eventually accumulated into a single voxelized point cloud, i.e., a map is formally composed of individual LiDAR scans $M = \bigcup_j M_j$. 
In the following, we hence use the general notation of a point cloud $C \in \mathbb{R}^{l \times 3}$ to refer to either $S$ or $M_j$. Each point cloud can be associated with a surround-view RGB image taken at the same time as $C$. We denote this image as $I \in \mathbb{N}^{h \times w \times 3}$, where $h$ and $w$ represent its height and width, respectively.
First, we feed $I$ through a frozen DINOv2~\cite{oquab2024dinov2} model to generate a dense 2D feature map $F \in \mathbb{R}^{h \times w \times d}$, e.g., $d = 384$ for the model type ViT-S/14. The core idea of using DINOv2 is to capture the semantics of the scene without an explicit assignment to discrete semantic classes~\cite{cui2024sageicp} while leveraging its generalization capabilities across cameras, weather, and illumination conditions~\cite{keetha2024anyloc}. Second, we employ point-to-pixel projection via known extrinsic calibration parameters to convert each point $p \in C$ into pixel coordinates of $I$. Finally, we use the DINOv2 feature of the respective RGB pixel as the descriptor $\text{desc}(\cdot)$ of point $p$. Formally, \looseness=-1
\begin{align}
    F &= \texttt{DINOv2}(I) \, , \\
    \text{desc}(p) &= F \left[ \Pi(p) \right] \, ,
\end{align}
where $\Pi(\cdot): \mathbb{R}^3 \to \mathbb{N}^2$ is the point-to-pixel projection function and $[\cdot]: \mathbb{N}^2 \to \mathbb{R}^d$ denotes the operator to access the DINOv2 feature of a given pixel. Consequently, we apply this step to all points in $C$ to obtain $C^D$:
\begin{equation}
    C^D = \{ \text{desc}(p) \, | \, \forall p \in C \}
\end{equation}
We hence retrieve the descriptors $M^D \in \mathbb{R}^{k \times d}$ corresponding to the map $M$ and, at test-time, the descriptors $S^D \in \mathbb{R}^{m \times d}$ for the current scan $S$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Scan-to-Map Registration}
\label{ssec:registration}

As defined in \cref{ssec:problem-definition}, the goal of scan-to-map registration is to find the 6DoF transform that correctly represents the robot pose with respect to the coordinate system of the map. 
In this paragraph, we describe the corresponding steps 2) and 3) of \cref{fig:overview}.
First, we substantially downsample the LiDAR scan $S$ with point descriptors $S^D$ resulting in $\tilde{S} \in \mathbb{R}^{v \times 3}$ and $\tilde{S}^D \in \mathbb{R}^{v \times d}$ with $v \approx \sfrac{m}{80}$ to reduce the complexity of the subsequent steps.
Second, we search for point correspondences between the scan and the map using an efficient similarity search~\cite{douze2024faiss}.
For every point $p_s \in \tilde{S}$, we search for the point $p_m \in M$ that achieves the highest cosine similarity between the descriptors of both points. \looseness=-1
\begin{align}
    p_m 
    &= \argmax_{p \in M} \text{sim}_{\cos} \left( \text{desc}(p_s), \text{desc}(p) \right) \\
    &= \argmax_{p \in M} \frac{\text{desc}(p_s) \cdot \text{desc}(p)}{||\text{desc}(p_s)|| \, ||\text{desc}(p)||}
\end{align}
Finally, if the cosine similarity is greater than a threshold $\theta_{\cos} = 0.8$, we consider the pair $(p_s, p_m)$ a valid point correspondence.

To achieve global registration within the map, we run \mbox{3-point} RANSAC~\cite{fischler1987ransac} on the set of all valid point correspondences, resulting in a coarse initial transform $\mathbf{T}_\textit{coarse}$. For further refinement and accurate 6DoF registration, we employ classical point-to-point ICP~\cite{vizzo2023kissicp} based on the 3D points of the original LiDAR scan~$S$. That is, the DINOv2-based point descriptors are not used in this step. With ICP, we obtain the final 6DoF transform $\mathbf{T}_\textit{fine}$ that aligns the scan~$S$ with the map~$M$.
\looseness=-1
