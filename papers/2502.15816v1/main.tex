
%File: formatting-instructions-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
\usepackage{booktabs} % added by Yuzhe for Table format
\usepackage{makecell} % added by Yuzhe for Table format
\usepackage{amssymb} % added by Yuzhe for Table format
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}



\usepackage{xcolor}
\newcommand{\Mozhgan}[1]{\textcolor{black}{#1}}
\newcommand{\Romina}[1]{\textcolor{black}{#1}}
\newcommand{\Yueqian}[1]{\textcolor{black}{#1}}
\newcommand{\Yuzhe}[1]{\textcolor{black}{#1}}
\newcommand{\Task}[1]{\textcolor{black}{#1}}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/Title (Edge GenAI Survey)
/Author (Mozhgan Navardi)
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices}

%Generative AI at the Edge: A Comprehensive Survey of Lightweight Techniques for Empowering Edge Devices

\author {
    % Authors
    Mozhgan Navardi\textsuperscript{\rm 1},
    Romina Aalishah \textsuperscript{\rm 1},
    Yuzhe Fu \textsuperscript{\rm 2},
    Yueqian Lin \textsuperscript{\rm 2},
    Hai Li \textsuperscript{\rm 2},
    Yiran Chen \textsuperscript{\rm 2},
    Tinoosh Mohsenin \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Johns Hopkins University,  
    \textsuperscript{\rm 2} Duke University \\
    % \textsuperscript{\rm 2} Affiliation 2\\
    mnavard1@jhu.edu, raalish1@jhu.edu, yuzhe.fu@duke.edu, yueqian.lin@duke.edu, \\ hai.li@duke.edu, yiran.chen@duke.edu, Tinoosh@jhu.edu
}



\begin{document}

\maketitle

\begin{abstract}

\Mozhgan{Generative Artificial Intelligence~(GenAI) applies models and algorithms such as Large Language Model~(LLM) and Foundation Model (FM) to generate new data. GenAI, as a promising approach, enables advanced capabilities in various applications, including text generation and image processing. In current practice, GenAI algorithms run mainly on the cloud server, leading to high latency and raising security concerns. Consequently, these challenges encourage the deployment of GenAI algorithms directly on edge devices. However, the large size of such models and their significant computational resource requirements pose obstacles when deploying them in resource-constrained systems. This survey provides a comprehensive overview of recent proposed techniques that optimize GenAI for efficient deployment on resource-constrained edge devices. For this aim, this work highlights three main categories for bringing GenAI to the edge: software optimization, hardware optimization, and frameworks. The main takeaways for readers of this survey will be a clear roadmap to design, implement, and refine GenAI systems for real-world implementation on edge devices.}

\end{abstract}



\section{Introduction}

Generative Artificial Intelligence (GenAI) has become a promising solution in text generation, image synthesis, and multimodal content creation. These developments often rely on large-scale models such as Large Language Models (LLMs) %and Foundation Models (FMs) 
that achieve remarkable performance but demand large computational and memory resources. Traditionally, these models run on powerful cloud servers, which introduces latency, dependency on network connectivity, and potential privacy risks. As real-time applications and data security become ever more critical, there is a growing push to embed GenAI functionalities directly into edge devices~\cite{nezami2024generative, mozhgan2024metatinymlv2}.

However, implementing high-intensive models on the edge presents significant challenges~\cite{pourmehrani2024fat, uttej2024resourcev2, humes2023squeezedv2}. 
Edge devices, including drones~\cite{iscas2023-mozhganv2}, and autonomous systems~\cite{reprohrl2023-tejaswiniv2} benefit significantly from the GenAI capabilities on devices. For instance, 
drones can generate real-time terrain analysis in remote areas, 
Autonomous systems can enhance decision-making through local models. Wearable health monitoring could generate personalized insights from biometric data while ensuring privacy through local data processing. To support these applications, specialized edge hardware such as NVIDIA Jetson, and Qualcomm AI Engine have been developed to handle the computational demands of GenAI while maintaining efficiency.

This situation calls for innovative approaches in software optimization including model compression, Neural Architecture Search (NAS). 
In parallel, hardware optimization including specialized accelerators, attention optimization, and dedicated frameworks address computational and energy constraints at the edge~\cite{asmer2024energyv2}. These strategies not only reduce model size and inference latency but also address privacy concerns when deploying complex models on edge devices~\cite{mozhgan2024metatinymlv2}. This paper aims to survey existing methods and provide extensive details on implemented GenAI techniques on edge devices. To the best of our knowledge, there is no dedicated survey on GenAI at the edge. By reviewing state-of-the-art techniques from top-tier conferences and journals, this work offers a  roadmap for researchers seeking to apply GenAI in edge.
The main category of the paper is organized as follows:

\begin{itemize}
    \item \textbf{Software Optimization:}
    Discusses key strategies for adapting GenAI models to edge devices, including model compression methods (pruning, quantization, and knowledge distillation), NAS, and open-source GenAI models. %and Federated Learning (FL).

    \item \textbf{Hardware Optimization:}
    Explores %specialized 
    hardware accelerators and attention optimization to highlight how they meet GenAI’s computational demands while addressing power and resource constraints on edge devices.

    \item \textbf{Frameworks:}
    Reviews frameworks %like TensorRT 
    to improve inference latency, memory, and overall energy efficiency.

    
\end{itemize}



\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/survay_overview_final.pdf}
    %\vspace{-0.8cm}
    \caption{Illustration of the flow of GenAI at the edge}
    \vspace{-0.5cm}
\end{figure*}



\section{Software Optimization}
\label{sec:1}

\subsection{Model Compression}
\Yueqian{The rapid advancement of GenAI models, while ushering in unprecedented capabilities, has also given rise to increasingly large model architectures that present significant deployment challenges \cite{guo2024survey}.  
Early attempts to address these challenges explored distributed mobile computing systems that could partition model computation across multiple devices \cite{mao1,mao2}.}

\Yueqian{This challenge has since prompted extensive research in model compression techniques, which have evolved along three principal directions to enable broader deployment and accessibility. Firstly, quantization techniques have achieved remarkable efficiency through reduced precision representations, particularly through enhanced activation distribution handling and hardware-optimized strategies. Secondly, methodologies for pruning have advanced from rudimentary magnitude-based techniques to sophisticated hardware-aware structured approaches, enabling considerable model reduction while preserving architectural integrity.  Thirdly, knowledge distillation has evolved to incorporate progressive frameworks and multi-teacher architectures, showing particular promise in task-specific applications. Contemporary research emphasizes hardware-aware compression strategies and architecture-specific solutions. While these advancements have enabled the deployment of foundation models with competitive performance metrics, the fundamental challenge persists in optimizing the compression-performance trade-off for edge deployment scenarios.}

\textbf{Quantization}
\Yueqian{Model quantization has emerged as a critical technique for deploying large-scale GenAI models on resource-constrained edge devices. Quantization approaches are broadly categorized into post-training quantization (PTQ) and quantization-aware training (QAT). PTQ methods like OPTQ \cite{frantar2023optq} and AWQ \cite{lin2023awq} directly convert trained model parameters to lower precision formats, while QAT approaches such as EdgeQAT \cite{shen2024edgeqat} incorporate quantization effects during training. PTQ methods are generally preferred due to their computational efficiency, though recent advances in both approaches have enabled effective compression through sophisticated handling of weight and activation distributions. When applied to LLMs, unique challenges emerge from their heavy-tailed weight distribution. Methods like SmoothQuant \cite{xiao2023smoothquant} and OliVe \cite{oliVe} address this through distribution smoothing and outlier handling techniques. Mixed-precision approaches \cite{chen2024channel} have shown promise by automatically determining optimal bit widths for different model components based on their quantization sensitivity. Recent work like OneBit \cite{xu2024onebit} and BitNet \cite{wang2023bitnet} has pushed boundaries by demonstrating viable 1-bit quantization through sophisticated distribution-aware schemes. However, significant challenges remain in maintaining generation quality under extreme compression and developing efficient training methods for quantized LLMs on edge devices \cite{egiazarian2024extreme}.}

\Yueqian{Diffusion models present their own set of quantization challenges, particularly in handling varying activation distributions across diffusion steps. Approaches like Q-DM \cite{q-dm}, PTQD \cite{ptqd}, and Q-Diffusion \cite{q-diffusion} tackle the challenge of varying activation distributions across diffusion steps through adaptive calibration and noise-aware quantization. Specialized temporal-aware quantization methods \cite{NEURIPS2023_983591c3,Huang_2024_CVPR} have been developed to handle the unique challenges of the iterative denoising process. Current research focuses on effectively handling dynamic activation ranges and balancing compression ratios with generation quality for edge deployment of diffusion models \cite{yao2024timestep}.}


\textbf{Pruning}
\Yueqian{Model pruning methods can be broadly categorized into structured and unstructured approaches, each with distinct trade-offs between compression efficiency and hardware compatibility. These techniques have shown particular promise in compressing large-scale generative models while maintaining performance for edge deployment. The field of LLM pruning has recently witnessed several novel approaches. Structured pruning methods like LLM-Pruner ~\cite{ma2023llmpruner} and edge-optimized approaches ~\cite{khiabani2025optimizing} achieve 2$\times$ speedup with minimal performance degradation by removing entire structural components.% such as attention heads. 
Unstructured approaches like SparseGPT \cite{frantar-sparsegpt} enable up to 60\% sparsity in large-scale models, while recent advances in modality-specific pruning techniques have shown promising results across speech, vision, and multimodal domains, with methods like SpeechPrune \cite{lin2024speechprune} achieving up to 80\% pruning rates while maintaining %task 
performance. Hardware-aware methods have become increasingly crucial, as exemplified by Flash-LLM \cite{xia2023flashllm}, which achieves 3$\times$  inference speedup through unstructured sparsity-aware system optimization. Semi-structured pruning methods such as E-Sparse \cite{li2023sparse} further advance this direction by leveraging N:M sparsity patterns to maintain hardware compatibility while achieving high compression rates on edge devices.}

\Yueqian{In the context of diffusion models, methods like Diff-Pruning \cite{fang2023structural} achieve approximately 50\% reduction in FLOPs by leveraging Taylor expansion over pruned timesteps while maintaining generative quality. Specialized approaches like LD-Pruner \cite{Castells_2024_CVPR} implement task-agnostic pruning strategies for Latent Diffusion Models, while DiP-GO \cite{zhu2024dipgodiffusionprunerfewstep} demonstrates 4.4$\times$  speedup on Stable Diffusion without requiring retraining. Recent work combines gradient-based pruning for mask matrix continuity \cite{wan2025pruningsparsediffusionmodels} with strategic data pruning \cite{briq2024datapruninggenerativediffusion}, showing particular promise for edge deployment where both computational efficiency and generation quality are critical \cite{yan2024hybrid}.}

\textbf{Knowledge Distillation.}
\Yueqian{Knowledge Distillation~(KD) has emerged as a crucial paradigm for deploying GenAI models 
on edge devices, with distinct approaches developed for different model architectures to balance model capabilities with computational constraints. The application of KD to language models has led %given rise 
to a variety of approaches. These can be categorized into white-box and black-box methods. White-box KD enables student models to match both final predictions and internal representations when the teacher model is open-source (e.g., LLaMA \cite{touvron2023llamaopenefficientfoundation}), while black-box KD works with closed-source models (e.g., GPT-4 \cite{openai2024gpt4technicalreport}) through API calls \cite{liu-etal-2024-evolving}. Notable advances include MiniLLM \cite{gu2024minillm}, which introduces a reversed Kullback-Leibler divergence objective to stabilize student updates, and instruction-following distillation approaches that have produced efficient open-source models like Vicuna \cite{vicuna2023} and Koala \cite{koala_blogpost_2023}. Recent work in instruction-following KD %distillation 
has enabled compact yet capable models through supervised fine-tuning \cite{wu-etal-2024-lamini}, while advanced applications like RLAI feedback \cite{lee2023rlaif} demonstrate the potential for model alignment through distillation. Adaptive distillation methods have further enhanced this field 
by dynamically adjusting the distillation process based on input complexity, allowing student models to focus learning where improvement is most 
needed~\cite{liang2024dynamic}.}

\Yueqian{In the domain of diffusion models, KD primarily focuses on accelerating sampling speed to address the challenge of high inference latency. Progressive distillation \cite{salimans2022progressive} represents an %breakthrough 
approach that iteratively halves sampling steps (e.g., from 1000 to 1), enabling efficient edge deployment while maintaining generation quality. Single-step approaches \cite{luhman2021knowledge} further compress diffusion teachers into one-step generators, although this requires careful balance between efficiency and generation fidelity. Teacher-free acceleration methods like DPM-Solver \cite{NEURIPS2022_260a14ac} and consistency models \cite{pmlr-v202-song23a} demonstrate effective inference cost reduction without extensive re-training. Recent advances include two-stage approaches \cite{meng2023distillation} for text-conditional models and score distillation sampling \cite{poole2023dreamfusion} for 3D generation, showcasing the versatility of distillation in different applications. Also, generative dataset distillation using models like SDXL-Turbo with class-specific prompts has achieved superior images per class ratios in recent benchmarks \cite{su2024diffusion}, offering new possibilities for efficient model training and deployment.}

\subsection{Neural Architecture Design}

Efficient neural architecture design has emerged as a critical research direction to address the increasing complexity and resource demands of modern models, particularly for edge devices \cite{howard2017mobilenets, NASsurvey2019}. 
By automating the generation of network architectures while considering specific hardware and constraints, computational overhead, required memory, and power consumption have been improved, while maintaining model performance.

\textbf{Neural Architecture Search (NAS).}~Neural Architecture Search (NAS)~\cite{NASfirst2016, NASsurvey2019} serves as a powerful framework to automate the design of optimal model topologies with strict latency, memory, or power budgets. By systematically exploring a predefined search space such as varying layer depth, width, or connection patterns. NAS algorithms can discover specialized architectures that outperform traditional solutions. In~\cite{NASfirst2016}, they have proposed the first NAS using reinforcement learning (RL) to determine optimal Recurrent Neural Network~(RNN) parameters. Subsequently, this idea was extended to Convonotional Neural Network~(CNNs) in \cite{NASextended2018}, where the authors integrated a Sequential Model-Based Optimization (SMBO) approach with a reinforcement mechanism for cell-based searches to find the best configuration. 


In the context of GenAI, %Generative AI, 
where large models often dominate in tasks such as text generation or image synthesis, NAS-driven architectures present a promising route to achieve efficiency.
There are a
limited number of work on NAS in the field of transformers~\cite{liu2024mobilellm}.
FL-NAS~\cite{FLNAS2024} have proposed an approach which leverages LLM to find high-performance DNNs for resource-constrained systems. Moreover, work in~\cite{10646428} proposed a LLM-based methodology for NAS technique in Edge devices. 
Puzzle~\cite{bercovich2024puzzle} proposed an LLM optimized for inference using NAS under hardware constraints, achieving a 2.17x inference throughput speedup.

\subsection{Open-Source GenAI models}
%\textbf{Lightweight Transformers}
\Romina{The recent advancements in reasoning capabilities of models such as DeepSeek-R1 \cite{deepseek2025} emphasize the power of open research development.} %As Yann LeCun remarked:  \textit{“Open source models are surpassing proprietary ones.”}}
\Romina{DeepSeek-R1 \cite{deepseek2025} has profited significantly from open-source tools like PyTorch and Meta's Llama \cite{touvron2023llamaopenefficientfoundation}.} 
\Romina{One of the key contributions to the advancement in GenAI is open-source innovations, specifically for edge scenarios in which the resources are limited. In these cases, smaller model sizes and less latency besides not losing performance are the main considerations. Therefore, researchers explored various compression methods, leading to models like DistilBERT \cite{sanh2019distilbert}, TinyBERT \cite{jiao2019tinybert}, ALBERT \cite{lan2019albert},  MobileBERT \cite{sun2020mobilebert}, MiniLM \cite{wang2020minilmdeepselfattentiondistillation}, and MiniLMv2 \cite{wang2021minilmv2multiheadselfattentionrelation} each using techniques such as knowledge distillation, parameter sharing, or factorization to make large models smaller while maintaining strong performance.}

\Romina{Beyond these compression-based strategies that are already covered in the previous sections, novelties in architecture further improved efficiency. Reformer \cite{kitaev2020reformer} introduced locality-sensitive hashing for attention and reversible residual layers, enabling near-linear complexity for longer sequences. Meanwhile, GPT-NeoX-20B \cite{black2022gpt}, LLaMA \cite{touvron2023llamaopenefficientfoundation}, and LLaMA2 \cite{touvron2023llama2} showed how LLMs could be developed and released collaboratively, making it easier for edge-focused adaptations. %Making different versions of these models available, inspired some on efficiency as well as the feasibility of using by others. 
Even smaller-scale of these projects such as TinyLlama \cite{tinyllama2024} and H2O-Danube-1.8B \cite{h2odanube2024} now offer compact language models tailored to edge constraints, continuing the trend of collaborative research. Similarly, research on instruction tuning \cite{won2022finetuned}, which trains models to handle various tasks by exposing them to different instructions, reinforced the importance of building flexible and open-source foundations for further innovation.}

\Romina{Researchers have further built on open releases to develop conversational systems, including Alpaca \cite{alpaca2023}, Koala \cite{koala2023}, and Vicuna \cite{vicuna2023}, each developed by fine-tuning LLaMA \cite{touvron2023llamaopenefficientfoundation} on curated datasets, all demonstrating competitive performance against models like ChatGPT and Bard. These models have also served as benchmarks for edge-focused projects such as SqueezeLLM \cite{kim2024squeezellm}, which introduces a post-training quantization framework to compress LLMs for more efficient inference, focusing on reducing memory bandwidth, outperforming methods like GPTQ \cite{frantar2022gptq}, AWQ \cite{lin2023awq}, and SpQR\cite{dettmers2023spqr}. In parallel, techniques like LoRA (Low-Rank Adaptation) \cite{hulora2021} have reduced the cost of fine-tuning large models, accelerating domain-specific deployments. Later, QLoRA \cite{dettmers2023qlora} tried to fine-tune a large model on a single GPU by reducing memory usage by quantizing the quantization constants and using this technique.}
\Romina{Taken together, several open-source LLMs have been developed, and some of them are compressed to reduce their size and improve efficiency. 
These include MPT-7B \cite{MosaicML2023Introducing}, which implements a 7B-parameter architecture designed for commercial applications; DLite \cite{ai2023dlite}, which scales from 124M to 1.5B parameters; and RedPajama-INCITE \cite{together2023redpajama}, which spans 3B to 7B parameters.
Open-source models and innovations can be valuable for resource-constraint applications, and be fine-tuned for specific tasks to improve their performance.}





\section{Hardware Optimization}
\label{sec:2}
\subsection{Hardware Accelerators}

%\Yuzhe{ Yuzhe please add subsections and then start adding text to each subsection ...}

\begin{table*}
\centering
\normalsize
\caption{\Yuzhe{Hardware Accelerator for GenAI}}
\scalebox{0.8}{
\begin{tabular}{lcccccccc}
\toprule[1.5pt]
Accelerator & Year & Platform & Technology & Networks & Sparsity/Quantization & Peak Energy Efficiency (TOPS/W)\\
\midrule
EXION \cite{heo2025exion} & 2025 & ASIC simulator & 14nm & SD/DiT & \checkmark / \checkmark @INT12 &  $11.53$\\
\midrule
HCAEDS \cite{guo2024hcaeds} & 2024 & CIM tapeout & 28nm & SD & - / \checkmark @INT10/BF16 & $74.34$\\
\midrule
DMPU \cite{qin2024dmpu} & 2024 & ASIC tapeout & 22nm & DDPM & \checkmark / - & $52.01$\\
\midrule
EEDA \cite{yoo2024eeda} & 2024 & ASIC tapeout & 28nm & SD & - / \checkmark @HYP8 & $4.96$\\
\midrule
% SDA & 2024 & FPGA & - & SD & - / \checkmark @W4A8 & $6\times$ V100\\
% \midrule
Cambricon-D \cite{kong2024camb}& 2024 & ASIC simulator & 7nm & SD & \checkmark / \checkmark @INT3/FP16 & $13.34$\\
\midrule
AttAcc \cite{park2024attacc} & 2024 & CIM simulator & 7nm & LLaMA/GPT-3 & - / - & $2.67\times$ DGX A100\\
\midrule
SpecPIM \cite{li2024specpim} & 2024 & CIM simulator & - & LLaMA/OPT & - / - & $6.7\times$ A100\\
\midrule
ASADI \cite{li2024asadi}& 2024 & CIM simulator & 28nm & GPT-2/BERT & \checkmark / - & -\\
\midrule
MECLA \cite{qin2024mecla} & 2024 & ASIC simulator & 28nm & LLaMA/BERT & - / \checkmark @INT8 & $7.09$\\
\midrule
% FlightLLM & 2024 & FPGA & - & LLaMA/OPT & - / \checkmark @INT8  & $6.7\times$ V100\\
% \midrule
STP \cite{tambe2023STP} & 2023 & ASIC tapeout & 28nm & BERT & - / \checkmark @FP4 & $18.1$\\
\midrule
OliVe \cite{oliVe} & 2023 & ASIC simulator & 22nm & GPT-2/OPT/BERT & - / \checkmark @Adaptive 4bit & $4\times$ GOBO \cite{zad2020gobo}\\
\midrule
FACT \cite{qin2023FACT} & 2023 & ASIC simulator & 28nm & BERT & \checkmark / \checkmark @INT8  & $4.39$\\


\bottomrule[1.5pt]
\end{tabular}}
\label{tab:accForGenAI}
\end{table*}


\Yuzhe{Hardware accelerators are typically designed through the software and hardware co-design for specific networks. Algorithmically, data sparsity is enhanced by pruning, and model compression, such as quantization, reduces network size. On the hardware side, specific architectures are designed to bypass sparse or redundant computations, increase data reuse, and minimize data movement, thus enabling energy-efficient acceleration on edge devices. Generative AI (GenAI) includes GAN, LLM, and Diffusion models. While extensive hardware work has focused on optimizing GAN models \cite{chen2018regan, kim2021gan, kang2021ganpu}, recent trends have shifted toward LLM and Diffusion models, driving further hardware research in GenAI. This section reviews recent efforts in optimizing hardware accelerator for LLM and Diffusion networks, with representative works summarized in Table \ref{tab:accForGenAI}.}

\textbf{LLM Acceleration} \Yuzhe{LLM models have diverse distributions at the tensor or channel levels, numerous studies leverage customized data types to accommodate this challenge. For example, ANT~\cite{guo2022ant} introduces a novel data type and employs an adaptive mechanism to determine the most appropriate type for each tensor from a predefined set. Expanding on ANT, OliVe~\cite{oliVe} proposes an outlier-victim pair approach, which provides a more precise representation of outlier distributions in LLM models. Both ANT and OliVe incorporate specialized decoders and multiply-accumulate (MAC) units to optimize their arithmetic computation processes for LLMs. Some studies focus on reducing redundant computations in LLM models to improve the energy efficiency during inference. STP \cite{tambe2023STP} proposes a computation-skipping strategy and dynamic data path reconfiguration based on entropy, achieving high energy efficiency with minimal accuracy loss. Furthermore, it has been observed that linear projections contribute significantly to the memory footprint and latency in LLM models. FACT \cite{qin2023FACT} introduces an eager prediction method with a leading-one detector and log-based inner-product estimation, reducing computations in both attention and linear projections. MECLA \cite{qin2024mecla} surpasses FACT by decomposing large matrices into smaller sub-matrices to minimize off-chip memory access and re-associating data on-chip for better reuse.}

\Yuzhe{Recently, Computing-in-Memory (CIM) becomes a prominent approach for LLM acceleration. CIM accelerators offer significant energy efficiency gains, particularly for general matrix-matrix multiplication (GEMM) operations. Existing studies typically leverage CIM architectures to accelerate the attention mechanism, while relying on CPUs or GPUs to handle other operations. ASADI \cite{li2024asadi} introduces a sparse attention paradigm based on diagonal compression (DIA) format, enabling highly parallel computation on CIM processors. SpecPIM \cite{li2024specpim} accelerates speculative inference in LLM by optimizing resource allocation in CIM-enabled heterogeneous systems, while AttAcc \cite{park2024attacc} accelerates batched LLM inference on CIM/NPU heterogeneous systems. Given these developments, it is expected that CIM-based accelerators for LLM models will become more prevalent in the future.}

\textbf{Diffusion Acceleration} \Yuzhe{Diffusion networks have made significant progress recently in various GenAI tasks, with %significantly
different network architecture from LLM models. These networks generate images or videos through multiple iterations of denoising operations, with highly similar images in consecutive iterations. Consequently, hardware optimizations often leverage inter- and intra-iteration similarity to accelerate Diffusion networks, typically through differential computing and skipping redundant computations. }

\Yuzhe{Cambricon-D \cite{kong2024camb} introduces an approximate ReLU in the Stable Diffusion (SD) network, enabling differential computing for nonlinear functions and addressing the memory overhead associated with full-precision nonlinear calculations in traditional differential computing architectures. DMPU \cite{qin2024dmpu} observes that many pixels exhibit minimal changes between consecutive time steps in Diffusion models, and thus proposes a semantic-segment sparse convolution along with a trivial attention exponent inheritance method to skip redundant computations in both the convolution and attention mechanisms, significantly enhancing the energy efficiency. EXION \cite{heo2025exion} presents an FFN-Reuse algorithm that can be applied across iterations, along with an improved eager prediction method for predicting attention scores, which reduces redundant computations and boosts throughput. HCAEDS \cite{guo2024hcaeds} is the first heterogeneous CIM chip designed for Diffusion models, incorporating a Sign-Magnitude radix-8 Booth CIM macro for integer data and a four-operand exponent CIM macro for floating-point data, achieving a high energy efficiency.}

\Yuzhe{Numerous GenAI hardware studies \cite{kong2024camb, yoo2024eeda, yang2024sda, wang2024dtrans} have observed that nonlinear functions (such as softmax, GeLU, etc.) can introduce significant latency overhead during the hardware acceleration. These studies optimize nonlinear functions to enhance overall throughput. Additionally, some studies \cite{fu2024softact, dong2024NLO, ste2021softermax, yan2019cim_non} have focused specifically on optimizing nonlinear functions and have designed specialized hardware to facilitate network inference. All of these studies indicate a potential research trend on optimizing nonlinear functions in GenAI networks. Combined with techniques such as eliminating redundant computations and data compression, these approaches can enhance hardware acceleration and improve energy efficiency for GenAI systems.}

\subsection{Attention Optimization}

\Romina{Transformers have become the backbone of many GenAI models, but their multi-head self-attention mechanism can dominate runtime and memory usage. Therefore, researchers have explored a range of strategies to optimize attention on \emph{hardware} %(GPU)} 
and \emph{algorithmic} levels.}
%memory (

\textbf{Hardware-based.}~\Romina{%One example is 
FlashAttention \cite{dao2022flashattention} %, which 
reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention on-chip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 \cite{dao2023flashattention2} takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 \cite{dao2024flashattention3} introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers \cite{xformer}, a PyTorch-based library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations.}

\textbf{Algorithmic-based.}~\Romina{Work on sparse attention reduces the quadratic complexity of self-attention by ignoring parts of the input that do not affect the result significantly. 
Child et al. \cite{child2019sparse} pioneered this approach by limiting attention to strided patterns using sparse factorizations of the attention matrix to reduce computation cost while maintaining performance on sequence models. Subsequent techniques like Longformer \cite{beltagy2020longformer} by using a combination of sliding window local attention and task-motivated global attention, Big Bird \cite{zaheer2020bigbird} by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer \cite{wang2020linformer} by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns. Meanwhile, Choromanski et al. \cite{choromanski2021performer} developed performer, which uses random feature maps to approximate the softmax function, reducing its time complexity from $\mathcal{O}(n^2)$ 
to $\mathcal{O}(n)$.}




\section{Frameworks}
\label{sec:3}


\Romina{Deploying GenAI models on edge devices might bring challenges because of limited computational power, memory, and latency requirements. To address these constraints, researchers have explored various techniques that simplify computations at both the graph and operator levels. By fusing kernels, reducing redundant operations or parameters, and customizing algorithms to the hardware, these methods enable fast inference for tasks such as large language modeling, super-resolution, and more.}

\Romina{%Solutions such as 
NVIDIA TensorRT and Apache TVM are pioneered compiler-based optimizations by combining graph-level fusion and quantization with lower latency. Likewise, Google’s EdgeTPU and Coral stacks enable rapid deployment of compressed models through low-power hardware and software stack. TensorRT-LLM \cite{tensorrtllm} is also a specialized toolkit for accelerating LLM inference on GPUs, including optimized CUDA kernels for attention computations, inflight batching, and quantization.}

\Romina{Beyond these compilers, researchers have developed frameworks customized for various GenAI workloads. For instance, Yi et al. proposed EdgeMoE \cite{yi2023edgemoe}, an engine specifically optimized for Mixture-of-Experts (MoE) language models. By using expert-wise bitwidth adaptation, it supports models with a large number of parameters on edge devices to reduce inference times substantially. Wang et al. introduced CoreInfer \cite{wang2024coreinferacceleratinglargelanguage}, achieving over 10$\times$ speedup compared to the Huggingface implementation through semantic-based sparse activation that identifies, fixes, and maintains stable neuron activation patterns at the sentence level. Laskaridis et al. introduced MELTing point \cite{laskaridis2024melting}, a mobile benchmarking suite designed to evaluate LLM performance, focusing on energy usage and memory footprints, across smartphones and Jetson platforms. TinyChatEngine \cite{tinyChatEngine} is also, an on-device LLM/VLM Inference Library that uses compression techniques to limit memory budgets while maintaining interactive response times on edge hardware. Furthermore, Nikoghosyan et al. showed that applying TensorRT to Transformer-based models on NVIDIA Jetson Xavier yields over 60\% latency reduction with negligible accuracy loss \cite{Nikoghosyan2023jetsonxaviertensorrt}.}

\Romina{In addition to language models, solutions target Super-Resolution (SR) and other vision-based generators. Chen et al. introduced TileSR \cite{chen2024tilesr}, which splits ultra-high-resolution images into tiles and selects the ones with the highest upscaling difficulty; these tiles are processed in parallel across multiple devices, reducing latency by up to 82\% and improving the image quality up to 10\% compared to other alternatives such as Supremo \cite{yi2022supremo} and MobiSR \cite{lee2019mobisr}. Wang et al.~\cite{wang2024intelligent} proposed ESHP, which combines a difficulty predictor with deep reinforcement learning to distribute SR tasks among CPUs, GPUs, and NPUs, speeding up SR processing without modifying the original architecture of the given SR model. Zhao et al. demonstrated a full-stack SR acceleration framework for embedded GPU devices, which outperformed standard TensorRT baselines in speed due to dictionary compression and operations optimization~\cite{zhao2021highperformance}.}

\Romina{FPGAs also provide a promising platform for runtime acceleration. Li et al. proposed a lookup-table (LUT)–based SR pipeline making sharper images while using much less energy without losing image quality \cite{li2024lut}. Other research has combined FFT-based processing with efficient multipliers \cite{chen2024fftwallace}, designed heterogeneous CNN-SNN architectures \cite{park2023resource}, or combined FPGA and GPU via PCIe to achieve real-time SR in microscopic imaging \cite{fang2022pcie}. For video-specific scenarios, Kim et al. employed pipeline and memory optimizations to reach 60~fps on 4K UHD content \cite{paris2019realtime}, while Sun et al. developed RNN compression techniques to manage temporal correlations \cite{zhang2022fpga_rnn}. %Larger multi-core systems also see performance gains: 
On larger multi-core systems Georgis et al. attained speedups over CPU-only baselines via parallelization \cite{abdelrahman2019acceleration}, and Liu et al. achieved real-time 4K SR on edge FPGAs through a DSP-enhanced caching scheme \cite{kumar2024highperformance}.}
\Romina{Finally, several system-level revisions help further reduce overhead. Fan et al.~\cite{liu2023covisu} leveraged codec-side data to skip redundant decoding in video SR, improved performance by up to 9.4$\times$. %compared to the traditional flow . 
Deformable 3D convolutional networks, essential in video tasks, were accelerated through tile decoupling and memory optimization by Zhang et al. \cite{zhou2022deformable3d}. Even resource-limited devices like the Raspberry Pi can support real-time SR: Osorno-Ortiz et al. integrated 2D-DWT with parallel interpolation to handle HD images in a short time \cite{liu2024dwt}.}







\section{Conclusion and Future Work}
\label{sec:5}
\Mozhgan{This work proposed a comprehensive survey regarding deploying Generative AI (GenAI) on edge devices. It presents a promising path toward reducing latency, enhancing data privacy, and enabling real-time capabilities in various applications. This survey has showcased the critical roles of software optimization, hardware specialization, and on-device inference frameworks in overcoming the resource constraints typical of embedded systems. Despite these advancements, significant challenges persist especially regarding model personalization, and security across distributed edge nodes. %Looking ahead, federated learning emerges as a compelling avenue for future work, allowing edge devices to collaboratively train or refine GenAI models without exposing raw data. 
By effectively addressing these challenges and combining these techniques with ongoing optimizations in model design and hardware acceleration, researchers and practitioners can pave the way for even more efficient, scalable, and privacy-preserving GenAI solutions at the edge.}


\begingroup
\tiny
\bibliography{main}
\endgroup
\end{document}
