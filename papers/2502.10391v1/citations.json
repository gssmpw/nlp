[
  {
    "index": 0,
    "papers": [
      {
        "key": "fu2023mme",
        "author": "Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others",
        "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "Mmbench: Is your multi-modal model an all-around player?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2023seed",
        "author": "Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying",
        "title": "Seed-bench: Benchmarking multimodal llms with generative comprehension"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023seed2",
        "author": "Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying",
        "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mmtbench",
        "author": "Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao",
        "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "fu2024blink",
        "author": "Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay",
        "title": "Blink: Multimodal large language models can see but not perceive"
      },
      {
        "key": "lu2024wildvision",
        "author": "Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen",
        "title": "WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences"
      },
      {
        "key": "bitton2023visit",
        "author": "Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schimdt, Ludwig",
        "title": "Visit-bench: A benchmark for vision-language instruction following inspired by real-world use"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yu2024mm",
        "author": "Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities"
      },
      {
        "key": "bai2023touchstone",
        "author": "Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren",
        "title": "Touchstone: Evaluating vision-language models by language models"
      },
      {
        "key": "han2023coremm",
        "author": "Xiaotian Han and Quanzeng You and Yongfei Liu and Wentao Chen and Huangjie Zheng and Khalil Mrini and Xudong Lin and Yiqi Wang and Bohan Zhai and Jianbo Yuan and Heng Wang and Hongxia Yang",
        "title": "InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yin2023survey",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong",
        "title": "A survey on multimodal large language models"
      },
      {
        "key": "fu2023challenger",
        "author": "Fu, Chaoyou and Zhang, Renrui and Lin, Haojia and Wang, Zihan and Gao, Timin and Luo, Yongdong and Huang, Yubo and Zhang, Zhengye and Qiu, Longtian and Ye, Gaoxiang and others",
        "title": "A challenger to gpt-4v? early explorations of gemini in visual expertise"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "gpt4",
        "author": "OpenAI.",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "taori2023stanford",
        "author": "Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B",
        "title": "Stanford alpaca: An instruction-following llama model"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chowdhery2023palm",
        "author": "Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others",
        "title": "Palm: Scaling language modeling with pathways"
      },
      {
        "key": "anil2023palm",
        "author": "Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others",
        "title": "Palm 2 technical report"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "muennighoff2022crosslingual",
        "author": "Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others",
        "title": "Crosslingual generalization through multitask finetuning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "chiang2023vicuna",
        "author": "Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others",
        "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90\\%* chatgpt quality"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "awadalla2023openflamingo",
        "author": "Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others",
        "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "laurenccon2024obelics",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others",
        "title": "Obelics: An open web-scale filtered dataset of interleaved image-text documents"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "driess2023palm",
        "author": "Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others",
        "title": "Palm-e: An embodied multimodal language model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dai2024instructblip",
        "author": "Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven",
        "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2023otter",
        "author": "Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei",
        "title": "Otter: A multi-modal model with in-context instruction tuning"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ye2023mplug",
        "author": "Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others",
        "title": "mplug-owl: Modularization empowers large language models with multimodality"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "liu2023visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "fu2024vita",
        "author": "Fu, Chaoyou and Lin, Haojia and Long, Zuwei and Shen, Yunhang and Zhao, Meng and Zhang, Yifan and Dong, Shaoqi and Wang, Xiong and Yin, Di and Ma, Long and others",
        "title": "Vita: Towards open-source interactive omni multimodal llm"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      },
      {
        "key": "liu2023improved",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "li2023monkey",
        "author": "Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang",
        "title": "Monkey: Image resolution and text label are important things for large multi-modal models"
      },
      {
        "key": "mckinzie2024mm1",
        "author": "McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others",
        "title": "Mm1: Methods, analysis \\& insights from multimodal llm pre-training"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "li2023monkey",
        "author": "Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang",
        "title": "Monkey: Image resolution and text label are important things for large multi-modal models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "xu2024llava-uhd",
        "author": "Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao",
        "title": "{LLaVA-UHD}: an LMM Perceiving Any Aspect Ratio and High-Resolution Images"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "li2024mini",
        "author": "Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya",
        "title": "Mini-gemini: Mining the potential of multi-modality vision language models"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "ge2024convllava",
        "author": "Ge, Chunjiang and Cheng, Sijie and Wang, Ziming and Yuan, Jiale and Gao, Yuan and Song, Jun and Song, Shiji and Huang, Gao and Zheng, Bo",
        "title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "tong2024cambrian",
        "author": "Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others",
        "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "zhang2024beyond",
        "author": "Zhang, Yi-Fan and Wen, Qingsong and Fu, Chaoyou and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong",
        "title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models"
      }
    ]
  }
]