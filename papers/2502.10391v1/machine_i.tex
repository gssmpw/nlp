
\documentclass{article} % For LaTeX2e
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2024}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{imgs/math_commands}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks,
            linkcolor=VioletRed,      
            anchorcolor=VioletRed, 
            citecolor=VioletRed,       
            ]{hyperref}

\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{cleveref}
\newcommand{\abbr}[0]{MME-RealWorld\xspace} 
\definecolor{Gray}{gray}{0.85}
\newcommand{\Gray}[0]{\rowcolor{gray!20}}
\newcommand{\Lgray}[0]{\rowcolor{gray!10}}
\newcommand{\lightgrey}[1]{\texttt{\color{black!50} #1}}
\title{\Large  MME-RealWorld: Could Your Multimodal LLM \\ Challenge High-Resolution Real-World Scenarios \\ that are Difficult for Humans?}


\author{
\vspace{-0.4cm}
    \\ 
    Yi-Fan Zhang$^{1,5}$, Huanyu Zhang$^{1,5}$, Haochen Tian$^{1,5}$, Chaoyou Fu$^{2,\dagger}$ \\
    Shuangqing Zhang$^{2}$, Junfei Wu$^{1,5}$, Feng Li$^{3}$, Kun Wang$^{4,5}$, Qingsong Wen$^{6}$ \\
    Zhang Zhang$^{1,5}$, Liang Wang$^{1,5}$, Rong Jin$^{7}$, Tieniu Tan$^{1,2,5}$
    \\ \\
    $^{1}$CASIA, $^{2}$NJU, $^{3}$HKUST, 
    $^{4}$NTU, $^{5}$UCAS,
    $^{6}$Squirrel AI Learning,
    $^{7}$Meta AI
    % \\
    % \footnotesize{
    % $^{\spadesuit}$~Project Leader \;
    % $^{\dagger}$~Corresponding Author \;}
    \\ \\
    {\centering}
    \url{https://mme-realworld.github.io/}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\yf}[1]{\textbf{\color{red}{\bf\sf [Yifan: #1]}}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Multimodal Large Language Models (MLLMs) are increasingly integral to various sectors that directly impact human life, including education, healthcare, and more. However, despite their widespread adoption, existing benchmarks often fail to reflect the real-world challenges these models face in such critical applications. Common issues include: 1) small data scale leads to a large performance variance; 2) reliance on model-based annotations results in restricted data quality; 3) insufficient task difficulty, especially caused by the limited image resolution. To tackle these issues, we introduce \abbr. Specifically, we collect more than $300$ K images from public datasets and the Internet, filtering $13,366$ high-quality images for annotation. This involves the efforts of professional $25$ annotators and $7$ experts in MLLMs, contributing to $29,429$ question-answer pairs that cover $43$ subtasks across $5$ real-world scenarios, extremely challenging even for humans. As far as we know, \textbf{\abbr is the largest manually annotated benchmark to date, featuring the highest resolution and a targeted focus on real-world applications}. We further conduct a thorough evaluation involving $28$ prominent MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the most advanced models struggle with our benchmarks, where none of them reach 60\% accuracy. The challenges of perceiving high-resolution images and understanding complex real-world scenarios remain urgent issues to be addressed.
\end{abstract}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/teaser_tasks.pdf}
\caption{\textbf{Diagram of \abbr.}
Our benchmark contains $5$ real-world domains, covering 43 perception and reasoning subtasks. Each QA pair offers $5$ options. We highlight and magnify the image parts relevant to the question in a red box for better visibility.}
\label{fig:teaser_tasks}
\end{figure}
\section{Introduction}

In recent years, we have witnessed a significant proliferation of Multimodal Large Language Models (MLLMs)\citep{dai2024instructblip, liu2023visual, zhang2024beyond}, which have been extensively applied in diverse fields such as education\citep{tuo2022construction}, medicine~\citep{nan2024beyond}, and autonomous driving~\citep{fu2024limsim++}, thereby substantially catalyzing research and development across these areas. A primary objective behind designing MLLMs has been to develop general intelligent agents capable of comprehensively perceiving human queries and environmental stituations through the integration of various multimodal sensory data. Consequently, a plethora of comprehensive evaluation benchmarks have emerged to rigorously assess model capabilities. 
However, some common concerns also arise:
\begin{itemize}
    \item \textbf{Data Scale.} 
    Many existing benchmarks contain fewer than $10$K Question-Answer (QA) pairs, such as MME~\citep{fu2023mme}, MMbench~\citep{liu2023mmbench}, MMStar~\citep{chen2024we}, MM-Vet~\citep{yu2024mm}, TorchStone~\citep{bai2023touchstone}, and BLINK~\citep{fu2024blink}. The limited number of QA can lead to large evaluation fluctuations.
    \item \textbf{Annotation Quality.} 
    While some benchmarks, such as MMT-Bench~\citep{mmtbench} and SEED-Bench~\citep{li2024seed}, are relatively larger in scale, their annotations are generated by LLMs or MLLMs. This annotation process is inherently limited by the performance of the used models. In our benchmark, for example, the best-performing model, InternVL-2, merely achieves $50\%$ accuracy. Consequently, relying on models would inevitably introduce significant noise, compromising the quality of the annotations.
    \item \textbf{Task Difficulty.} To date, the top performance of some benchmarks has reached the accuracy of $80\%$-$90\%$~\citep{ masry2022chartqa, singh2019towards, liu2023mmbench, li2023seed}, and the performance margin between advanced MLLMs is narrow. This makes it challenging to verify the benefits or improvements of advanced models and to distinguish which one is significantly better.
\end{itemize}


In light of these concerns, we propose a new benchmark named \abbr. We first pay attention to a series of well-motivated families of datasets, considering images from sources such as autonomous driving, remote sensing, video surveillance, newspapers, street views, and financial charts. 
These scenarios are difficult even for humans, where we hope that MLLMs can really help.
Considering these topics, we collect a total of $13,366$ high-resolution images from more than $300$K public and internet sources. 
These images have an average resolution of $2,000$$\times$$1,500$, containing rich image details. 
$25$ professional annotators and $7$ experts in MLLMs are participated to annotate and check the data quality, and meanwhile ensuring that all questions are challenging for MLLMs. Note that most questions are even hard for humans, requiring multiple annotators to answer and double-check the results.
As shown in Fig.~\ref{label:teaser_task}, MME-RealWorld finally contains $29,429$ annotations for $43$ sub-class tasks, where each one has at least $100$ questions. 
$28$ advanced MLLMs are evaluated on our benchmark, along with detailed analysis. We conclude the main advantages of \abbr compared to existing counterparts as follows:

\begin{itemize}
    \item \textbf{Data Scale.} With the efforts of a total of $32$ volunteers, we have manually annotated $29,429$ QA pairs focused on real-world scenarios, making this the largest fully human-annotated benchmark known to date.
    \item \textbf{Data Quality.} 1) Resolution: Many image details, such as a scoreboard in a sports event, carry critical information. These details can only be properly interpreted with high-resolution images, which are essential for providing meaningful assistance to humans. To the best of our knowledge, \abbr features the highest average image resolution among existing competitors. 2) Annotation: All annotations are manually completed, with a professional team cross-checking the results to ensure data quality.
    \item \textbf{Task Difficulty and Real-World Utility.} The performance of different MLLMs is shown in Fig.~\ref{label:teaser_acc}, in which we can see that even the most advanced models have not surpassed $60\%$ accuracy. Additionally, as illustrated in Fig.~\ref{fig:teaser_tasks}, many real-world tasks are significantly more difficult than those in traditional benchmarks. For example, in video monitoring, a model needs to count the presence of $133$ vehicles, or in remote sensing, it must identify and count small objects on a map with an average resolution exceeding $5000$$\times$$5000$.
    \item \textbf{\abbr-CN.} Existing Chinese benchmark~\citep{liu2023mmbench} is usually translated from its English version. This has two limitations: 
    1) Question-image mismatch. The image may relate to an English scenario, which is not intuitively connected to a Chinese question.
    2) Translation mismatch~\citep{tang2024mtvqa}. The machine translation is not always precise and perfect enough. 
    We collect additional images that focus on Chinese scenarios, asking Chinese volunteers for annotation. This results in $5,917$ QA pairs.
    \item \textbf{Substantial challenges faced by current MLLMs.} We conducted a comprehensive and detailed evaluation of 28 representative MLLMs recently, revealing that most existing models still perform poorly on perceptual tasks with human-level difficulty. The analysis indicates a significant gap between current model performance and human capabilities, particularly in understanding complex scenarios in domains like autonomous driving and video surveillance, where the models often approach the level of random guessing.
\end{itemize}


\section{Related Work}

\textbf{Multimodal Benchmark.} 
With the development of MLLMs, a number of benchmarks have been built.
For instance, MME~\citep{fu2023mme} constructs a comprehensive evaluation benchmark that includes a total of 14 perception and cognition tasks. All QA pairs in MME are manually designed to avoid data leakage, and the binary choice format makes it easy to quantify.
MMBench~\citep{liu2023mmbench} contains over $3,000$ multiple-choice questions covering $20$ different ability dimensions, such as object localization and social reasoning. 
It introduces GPT-4-based choice matching to address the MLLM's lack of instruction-following capability and a novel circular evaluation strategy to improve the evaluation robustness.
Seed-Bench~\citep{li2023seed} is similar to MME and MMBench but consists of $19,000$ multiple-choice questions. The larger sample size allows it to cover more ability aspects and achieve more robust results.
SEED-Bench-2~\citep{li2023seed2} expands the dataset size to $24,371$ QA pairs, encompassing $27$ evaluation dimensions and further supporting the evaluation of image generation.
MMT-Bench~\citep{mmtbench} scales up the dataset even further, including $31,325$ QA pairs from various scenarios such as autonomous driving and embodied AI. It encompasses evaluations of model capabilities such as visual recognition, localization, reasoning, and planning.
Additionally, other benchmarks focus on real-world usage scenarios~\citep{fu2024blink,lu2024wildvision,bitton2023visit} and reasoning capabilities~\citep{yu2024mm,bai2023touchstone,han2023coremm}. 
However, there are widespread issues, such as data scale, annotation quality, and task difficulty, in these benchmarks, making it hard to assess the challenges that MLLMs face in the real world.

\textbf{MLLMs.} 
This field has undergone significant evolution~\citep{yin2023survey, fu2023challenger}, initially rooted in BERT-based language decoders and later incorporating advancements in LLMs. 
MLLMs exhibit enhanced capabilities and performance, particularly through end-to-end training techniques, by leveraging advanced LLMs such as GPTs~\citep{gpt4,brown2020language},
LLaMA~\citep{touvron2023llama,touvron2023llama2}, 
Alpaca~\citep{taori2023stanford}, PaLM~\citep{chowdhery2023palm,anil2023palm}, BLOOM~\citep{muennighoff2022crosslingual}, 
Mistral~\citep{jiang2023mistral}, and Vicuna~\citep{chiang2023vicuna}. Recent model developments, including Flamingo~\citep{awadalla2023openflamingo}, PaLI~\citep{laurenccon2024obelics}, PaLM-E~\citep{driess2023palm}, BLIP-2~\citep{li2023blip}, InstructBLIP~\citep{dai2024instructblip}, Otter~\citep{li2023otter}, MiniGPT-4~\citep{zhu2023minigpt}, mPLUG-Owl~\citep{ye2023mplug}, LLaVA~\citep{liu2023visual}, Qwen-VL~\citep{bai2023qwen}, and VITA~\citep{fu2024vita}, bring unique perspectives to challenges such as scaling pre-training, enhancing instruction-following capabilities, and overcoming alignment issues. 
However, the performance of these models in the face of real scenarios has often not been revealed.

\textbf{High-resolution MLLMs.} 
Empirical studies have shown that employing higher resolution is an effective solution for many tasks~\citep{bai2023qwen,liu2023improved,li2023monkey,mckinzie2024mm1}. Approaches like LLaVA-Next~\citep{liu2024llavanext} segment high-resolution images into multiple patches, encoding each one independently before concatenating all local patch tokens with the original global image tokens, albeit at an escalated computational cost. Other models, such as Monkey~\citep{li2023monkey} and LLaVA-UHD~\citep{xu2024llava-uhd}, also split images into patches but subsequently compress them to avoid redundant tokens. Mini-Genimi~\citep{li2024mini} comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding. They work in an attention mechanism, where the low-resolution encoder generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference. Conv-LlaVA~\citep{ge2024convllava} employs ConvNeXt instead of ViT as the vision encoder. Cambrian~\citep{tong2024cambrian} uses a set of learnable latent queries that interact with multiple vision features via cross-attention layers. Additionally, SliME~\citep{zhang2024beyond} stresses the importance of global features, compressing the local image patches twice but preserving all the global context. Although many of these models focus on improving resolution, none have been tested in a rigorous high-resolution benchmark, often providing only intuitive examples that lack informativeness and convincing results. Our proposed benchmark offers a rigorous evaluation to test the capabilities in understanding high-resolution images.

\begin{figure*}[t]
\centering
\subfigure[\textbf{Real-World Tasks}]{
\begin{minipage}[t]{0.62\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/samples/tasks.pdf}
\end{minipage}%
\label{label:teaser_task}
}%
\subfigure[\textbf{Leaderboard}]{
\begin{minipage}[t]{0.35\linewidth}
 \centering
 \includegraphics[width=\linewidth]{imgs/samples/teaser_performance.pdf}
\end{minipage}%
\label{label:teaser_acc}

}%
\centering
\vspace{-0.2cm}
\caption{\textbf{Task Categories} (left). Our benchmark spans $5$ key domains and $43$ subtasks highly related to real-world scenarios, including $13,366$ high-resolution images and $29,429$ annotations. \textbf{Model Performance} (right). Average accuracies of advanced MLLMs are shown across both the English and Chinese splits of the dataset.}
\label{fig:teaser}
\end{figure*}
\section{Methods}
In this section, we outline the data collection process, question annotation procedure, and provide a statistical overview of each domain and subtask in \abbr and its Chinese version. We visualize different tasks from the $5$ image domains in Fig.~\ref{fig:teaser_tasks}.
Detailed information on data sources, evaluation tasks, and visualized results can be found in Sec.~\ref{sec:app_data_collection}.


\subsection{Instruction and Criterion}\label{sec:prompt}

For each question, we manually construct four options, with one being the correct answer and the other three being the texts appearing in the image or options similar to the correct one. This greatly enhances the difficulty, forcing the model to deeply understand the details of the image. We also provide an additional choice E, which allows the model to reject for answering because there is no right answer. We try to use the model’s default prompt for multiple-choice questions, but if the model does not have the default prompt, we use a common prompt as shown in Tab.~\ref{tab:prompt}.


\begin{table}[]
\centering
\caption{\textbf{Prompt setting of \abbr}.}\label{tab:prompt}
\begin{tabular}{l}
\toprule 
\begin{tabular}[c]{@{}l@{}}\texttt{[Image]} \texttt{[Question]} The choices are listed below:\\ 
(A) \texttt{[Choice A]}\\ 
(B) \texttt{[Choice B]}\\ 
(C) \texttt{[Choice C]}\\ 
(D) \texttt{[Choice D]}\\ 
(E) \texttt{[Choice E]}\\ 
Select the best answer to the above multiple-choice question based on the image. Respond with \\ only the letter (A, B, C, D, or E) of the correct option.\\ The best answer is:
\end{tabular} \\ \bottomrule
\end{tabular}%
\end{table}


\textbf{Evaluation Metric.} We first apply a rule-based filter to the answers generated by MLLM, aligning them with the given answer options and checking for correctness against the ground truth. Let the dataset be denoted as \(\mathcal{D} = \{\mathcal{D}_d = \{\mathcal{T}_t\}_{t=1}^{T_d}\}_{d=1}^{D}\), where each domain \(\mathcal{D}_d\) consists of \(T_d\) subtasks. For each subtask, we calculate the accuracy across all annotations. For each domain, we compute two metrics: 1) \textbf{Average Accuracy (Avg).} the weighted average accuracy across all subtasks, given by \({\sum_{t=1}^{T_d} \text{Avg}(\mathcal{T}_t) \times |\mathcal{T}_t|}/{|\mathcal{D}_d|}\), where $|\cdot|$ is the instance number contained in one set, and 2) \textbf{Class-based Average Accuracy (Avg-C).} the unweighted average accuracy across subtasks, given by \({\sum_{t=1}^{T_d} \text{Avg}(\mathcal{T}_t)}/{T_d}\). Similarly, for the entire dataset, we report the overall Average Accuracy across all samples, and the class-based average accuracy across domains.

\subsection{Data Collection and Annotation}

\textbf{Optical Character Recognition in the Wild (OCR).} 
It is specifically designed to evaluate the model's ability to perceive and understand textual information in the real-world. We manually selecte $3,293$ images with complex scenes and recognizable text information from $150,259$ images in existing high-resolution datasets as our image sources. These images span various categories such as street scenes, shops, posters, books, and competitions.
The volunteers are worked for annotation, each with at least a foundational understanding of multimodal models, to independently generate questions and answers. These annotations are subsequently reviewed and further refined by another volunteers. Based on the image annotations, we categorize these $3,297$ images into $5$ perception tasks, totaling $5,740$ QA pairs: contact information and addresses, identity information, products and advertisements, signage and other text, as well as natural text recognition in elevation maps and books. Additionally, there are two reasoning tasks with $500$ QA pairs: 1) scene understanding of the entire image, which requires the model to locate and comprehend important text such as competition results, and 2) character understanding, focusing on comics or posters where the model needs to analyze relationships and personalities based on dialogue or presentation.

\textbf{Remote Sensing (RS).} 
The images have a wide range of applications in real-world scenarios. Some images possess extremely high quality, with individual image sizes reaching up to $139$MB and containing very rich details, which makes it difficult even for humans to perceive specific objects. We manually select $1,298$ high-resolution images from over $70,000$ public remote sensing images, ensuring that each image is of high quality, with sufficient resolution and rich detail. One professional researcher is involved in annotating the data, and another researcher checks and improves the annotations, resulting in $3,738$ QA pairs. There are $3$ perception tasks: object counting, color recognition, and spatial relationship understanding.

\textbf{Diagram and Table (DT).} Although there are already some datasets related to table and chart understanding, they mostly feature simple scenes. We focus on highly complex chart data, such as financial reports, which contain extensive numerical information and mathematical content, presenting new challenges for MLLMs. We filter $2,570$ images from the internet, with annotations performed by two volunteer and reviewed by another one. We categorize these annotations into $4$ tasks based on the question format: 1) Diagram and Table Perception ($5,433$ QA pairs): involve locating specific values of elements within the diagrams and tables; 2) Diagram Reasoning ($250$ QA pairs): include tasks such as identifying the maximum and minimum values in a chart, performing simple calculations, and predicting trends; and 3) table Reasoning ($250$ QA pairs): focus on simple calculations related to specific elements, understanding mathematical concepts like maximum and minimum values, and locating corresponding elements.

\textbf{Autonomous Driving (AD).}
It demands extensive general knowledge and embodied understanding capability. We emphasize challenging driving scenarios that involve distant perceptions and intricate interactions among dynamic traffic agents. Specifically, we manually select a subset of $2,715$ images from over $40,000$ front-view images captured by onboard cameras in open-source datasets. These images cover a diverse range of weather conditions, geographic locations, and traffic scenarios. Besides, a volunteer carefully annotates each image, and the other one conducts a thorough review, resulting in $3,660$ QA pairs for perception tasks and $1,334$ QA pairs for reasoning tasks. The perception tasks include objects identification, object attribute identification, and object counting for traffic elements such as vehicle, pedestrian, and signals. 
The latter is categorized into $3$ main tasks: 1) Intention Prediction: focus on predicting driving intention of a designated traffic agent in the short-term future. 2) Interaction Relation Understanding: involve reasoning about ego vehicle's reaction to other traffic elements, and the interactions between these elements. 3) Driver Attention Understanding: require reasoning about the traffic signal that the driver should pay attention to.


\textbf{Monitoring (MO).}
The images are from various application scenarios for public safety, e.g., streets, shopping malls, and expressway intersections. We focus on complex high-resolution monitoring images that include many real-world challenges, like scale variations and out-of-view, as possible which could test whether the model handles them robustly in practice.
Specifically, $1,601$ high-resolution images are manually selected from over $10,000$ public dataset images, which are captured from a broad range of cameras, viewpoints, scene complexities, and environmental factors across day and night. In terms of annotations, two volunteers manually annotate each image carefully, and multi-stage careful inspections and modifications are performed by another one. When these refined image annotations are completed, $1,601$ images are categorized into $3$ main perception tasks, totaling $2,196$ QA pairs, including object counting and location, and attribute recognition. Furthermore, $3$ reasoning tasks are well-designed with $498$ QA pairs: 1) calculate the sum of different objects, which requires the model to perceive various objects and calculate their total number accurately; 2) intention reasoning, focusing on reasoning the next route and turn of the specific object; 3) attribute reasoning, focusing on reasoning the specific materials and functions of the given objects.  


\subsection{\abbr-CN} 
The traditional general VQA approach~\citep{liu2023mmbench} uses a translation engine to extend QA pairs from English to Chinese. However, it may face visual-textual misalignment problems~\citep{tang2024mtvqa}, failing to address complexities related to nuanced meaning, contextual distortion, language bias, and question-type diversity. Additionally, asking questions in Chinese about images containing only English texts is not intuitive for benchmarking Chinese VQA capabilities. By contrast, we follow the steps below to construct a high-quality Chinese benchmark:

\begin{itemize}
    \item \textbf{Selection.} For video monitoring, autonomous driving, and remote sensing, many images do not contain English information. Therefore, we select a subset of the aforementioned question pairs, double-checking to ensure they do not contain any English information.
    \item \textbf{Translation.} Translate the questions and answers by four professional researchers, all of whom are familiar with both English and Chinese.
    \item \textbf{Collection.} For diagrams and tables, since the original images often contain English information (e.g., legends/captions), we collect additional $300$ tables and $301$ diagrams from the Internet, where the contents are in Chinese. This data is further annotated by one volunteer, resulting in $301$$\times$$4$ QA pairs, where the task type is the same as diagram and table in \abbr. Similarly, for OCR in the wild, we also collect additional $939$ images for all the subtasks.
\end{itemize}

In total, \abbr-CN has $1,889$ additional images and total $5,917$ QA pairs, which is a smaller version of \abbr, but it retains similar task types, image quality, and task difficulty. 
The examples can be seen in Fig.~\ref{fig:mme-hd-cn}. 


\subsection{Quality Control and Analysis}
\begin{wraptable}{r}{0.5\linewidth}
\vspace{-0.23cm} 
\caption{\textbf{Comparison of benchmarks.} MME-RealWorld is the largest fully human-annotated dataset, featuring the highest average resolution and the most challenging tasks.}
\label{fig:teaser_image}
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{lccccc}
\bottomrule
\textbf{Benchmark} & \textbf{\# QA-Pair} & \textbf{\begin{tabular}[c]{@{}c@{}}Fully Human \\ Annotation\end{tabular}} & \textbf{CN} & \textbf{\begin{tabular}[c]{@{}c@{}}Average\\ Resolution\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}LLaVA-1.5-7B \\ Performance\end{tabular}} \\ \hline
VizWiz & 8000 & × & × & 1224$\times$1224 & 50.0 \\
RealWorldQA & 765 & × & × & 1536$\times$863 & - \\
TextVQA & 5734 & \checkmark & × & 985$\times$768 & 58.2 \\
MME & 2374 & \checkmark & × & 1161$\times$840 & 76.0 \\
MMBench & 3217 & \checkmark & \checkmark & 512$\times$270 & 64.3 \\
MMStar & 1500 & × & × & 512$\times$375 & 30.3 \\
ScienceQA & 21000 & × & × & 378$\times$249 & 71.6 \\
ChartQA & 32719 & × & × & 840$\times$535 & - \\
MM-Vet & 218 & × & × & 1200$\times$675 & 31.1 \\
Seed-Bench & 19242 & × & × & 1024$\times$931 & 66.1 \\
SEED-Bench-2-Plus & 2300 & × & × & 1128$\times$846 & 36.8 \\
MMT-Bench & 32325 & × & × & 2365$\times$377 & 49.5 \\
MathVista & 735 & × & × & 539$\times$446 & 26.1 \\
TouchStone & 908 & × & × & 897$\times$803 & - \\
VisIT-Bench & 1159 & × & × & 765$\times$1024 & - \\
BLINK & 3807 & × & × & 620$\times$1024 & 37.1 \\
CV-Bench & 2638 & × & × & 1024$\times$768 & - \\ \Gray
\abbr & 29429 & \checkmark & \checkmark & 2000x1500 & 24.9 \\ \bottomrule
\end{tabular}%
}
\end{wraptable} 
During the annotation process, we impose the following requirements on annotators:
1. We ensure that all questions can be answered based on the image (except for specially constructed questions where the correct option is “E”), meaning that humans can always find the answers within the image. This approach prevents forcing annotators to provide answers based on low-quality images or images containing vague information.
2. The area of the object being questioned in each image must not exceed $1/10$ of the total image area. This ensures that the object is not overly prominent, preventing humans from easily identifying the answer at first glance.
3. Each annotation is cross-checked by at least two professional multimodal researchers to ensure accuracy and prevent annotation errors caused by human bias.

The comparison of benchmarks is shown in Tab.~\ref{fig:teaser_image}. The maximal resolution of \abbr is $42,177,408$ pixels, with dimensions of $5304$$\times$$7952$. The average resolution is $3,007,695$ pixels, equivalent to an image size of approximately $2000$$\times$$1500$. This resolution is significantly higher than that of existing benchmarks. For instance, the highest benchmark, MME, has an average resolution of $975,240$ pixels, corresponding to an image size of about $1161$$\times$$840$. The exceptional image quality and our strict, fully human annotation process make our tasks the most challenging among all benchmarks. This is evidenced by the baseline model LLaVA-1.5-7B achieving an accuracy of just 24.9\%, significantly lower than on other benchmarks. Although some benchmarks may approach our level of difficulty, this is primarily due to the inherent complexity of their tasks. For instance, MathVista focuses on pure mathematical problems, and MM-Vet involves multi-step reasoning—both of which are naturally challenging and result in lower baseline performance. However, the majority of our tasks are centered on real-world perception problems. This means that, current MLLMs still struggle to effectively address human-level perceptual challenges.
    

\section{Results}

We evaluate a total of 24 open-source MLLMs, including Qwen-VL-Chat~\citep{bai2023qwen}, LLaVA, LLaVA-Next~\citep{li2024llavanext-strong}, TextMonkey~\citep{liu2024textmonkey}, mPLUG-DocOwl 1.5~\citep{hu2024mplug}, ShareGPT4V~\citep{chen2023sharegpt4v}, MiniGPT-v2~\citep{chen2023minigpt}, Monkey~\citep{li2023monkey}, OtterHD~\citep{li2023otter}, Cambrian-1~\citep{tong2024cambrian}, Mini-Gemini-HD~\citep{li2024mini}, MiniCPM-V 2.5~\citep{hu2024minicpm}, DeepSeek-VL~\citep{lu2024deepseek}, YI-VL-34B\footnote{\url{https://huggingface.co/01-ai/Yi-VL-34B}}, SliME~\citep{zhang2024beyond}, CogVLM2\footnote{\url{https://github.com/THUDM/CogVLM2}}, InternLM-XComposer2.5~\citep{zhang2023internlm}, InternVL-Chat V1-5, and InternVL-2~\citep{chen2023internvl}, as well as 4 close-source MLLMs, including, GPT-4o\footnote{\url{https://openai.com/index/hello-gpt-4o/}}, GPT-4o-mini, Gemini 1.5 pro~\citep{team2023gemini}, and Claude 3.5 Sonnet\footnote{\url{https://www.anthropic.com/news/claude-3-5-sonnet}}. 

\subsection{Results on \abbr}
\subsubsection{Perception}
\begin{table}[]
\caption{\textbf{Experimental results on the perception tasks.} Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. “OCR”, “RS”, “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and  Autonomous Driving, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy
and the unweighted average accuracy across
subtasks in each domain.}\label{tab:res_main_perception}
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{llccccccc}
\toprule \Gray
\multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{LLM}} & \multicolumn{7}{c}{\textbf{Perception}}  \\\cmidrule{1-2} \cmidrule{3-9}\Gray
\multicolumn{2}{c}{\textbf{Task Split}} & \textbf{OCR} & \textbf{RS} & \textbf{DT} & \textbf{MO} & \textbf{AD} & \textbf{Avg} & \textbf{Avg-C} \\  \Gray
\multicolumn{2}{c}{\textbf{\# QA pairs}}  & 5740 & 3738 & 5433 & 2196 & 3660 & 20767 & 20767 \\ \hline 
Qwen2-VL & Qwen2-7B & 81.38 & 44.81 & 70.18 & 37.3 & 34.62 & 58.96 & 53.66 \\
{InternVL-2} & InternLM2.5-7B-Chat & 73.92 & 39.35 & 62.80 & 53.19 & 35.46 & 55.82 & 52.94 \\ \Lgray
{Claude 3.5 Sonnet} & - & 72.47 & 25.74 & 67.44 & 32.19 & 40.77 & 52.90 & 47.72 \\
{InternLM-XComposer2.5} & InternLM2-7B & 69.25 & 36.12 & 63.92 & 39.48 & 33.63 & 52.47 & 48.48 \\
{InternVL-Chat-V1.5} & InternLM2-Chat-20B & 71.51 & 33.55 & 55.83 & 51.16 & 31.42 & 51.36 & 48.69 \\
{Mini-Gemini-34B-HD} & Nous-Hermes-2-Yi-34B & 69.55 & 40.40 & 44.36 & 39.61 & 32.70 & 48.05 & 45.32 \\
{MiniCPM-V 2.5} & Llama3-8B & 66.79 & 27.69 & 52.81 & 38.70 & 34.15 & 47.37 & 44.03 \\
{Cambrian-1-34B} & Nous-Hermes-2-Yi-34B & 66.45 & 38.63 & 40.44 & 45.98 & 33.61 & 46.68 & 45.02 \\ \Lgray
{GPT-4o} & - & 77.69 & 28.92 & 46.68 & 33.93 & 22.43 & 46.43 & 41.93 \\
CogVLM2-llama3-Chat & Llama3-8B & 69.97 & 28.76 & 47.51 & 33.74 & 30.22 & 45.84 & 42.04 \\
Cambrian-1-8B & Llama3-8B-Instruct & 58.68 & 40.05 & 32.73 & 47.68 & 38.52 & 43.82 & 43.53 \\
SliME-8B & Llama3-8B & 53.45 & 42.27 & 29.34 & 40.62 & 33.66 & 40.29 & 39.87 \\\Lgray
Gemini-1.5-pro & - & 67.62 & 13.99 & 39.90 & 31.11 & 26.64 & 39.63 & 35.85 \\ \Lgray
GPT-4o-mini & - & 62.51 & 6.69 & 44.23 & 26.50 & 24.18 & 37.12 & 32.82 \\
Monkey & Qwen-7B & 54.63 & 24.99 & 32.51 & 28.01 & 29.67 & 36.30 & 33.96 \\
mPLUG-DocOwl 1.5 & Llama-7B & 51.15 & 23.71 & 29.34 & 24.97 & 28.28 & 33.71 & 31.49 \\
DeepSeek-VL & DeepSeek-LLM-7b-base & 49.55 & 25.49 & 23.38 & 26.97 & 33.39 & 33.14 & 31.76 \\
SliME-13B & Vicuna-13B & 50.58 & 25.82 & 20.93 & 24.73 & 27.16 & 31.50 & 29.84 \\
Mini-Gemini-7B-HD & Vicuna-7B-v1.5 & 42.02 & 31.30 & 22.31 & 34.15 & 24.81 & 31.07 & 30.92 \\
YI-VL-34B & Yi-34B-Chat & 44.95 & 31.62 & 15.99 & 34.85 & 28.31 & 30.97 & 31.14 \\
LLaVA-Next & Llama3-8B & 47.94 & 25.42 & 26.63 & 19.46 & 18.66 & 30.14 & 27.62 \\
LLaVA-Next & Qwen-72B & 37.07 & 29.13 & 27.68 & 29.37 & 17.98 & 29.01 & 28.25 \\
LLaVA1.5-13B & Vicuna-13B & 44.10 & 23.27 & 20.17 & 20.45 & 26.12 & 28.42 & 26.82 \\
ShareGPT4V-13B & Vicuna-13B & 44.55 & 23.06 & 20.17 & 19.26 & 26.12 & 28.38 & 26.63 \\
MiniGPT-v2 & Llama 2-7B-Chat & 39.02 & 23.33 & 20.41 & 19.26 & 25.96 & 26.94 & 25.60 \\
ShareGPT4V-7B & Vicuna-7B & 39.39 & 22.10 & 20.08 & 19.13 & 26.04 & 26.73 & 25.35 \\
LLaVA1.5-7B & Vicuna-7B & 38.69 & 22.12 & 20.08 & 19.13 & 26.04 & 26.54 & 25.21 \\
Qwen-VL-Chat & Qwen-7B & 32.37 & 15.14 & 15.59 & 22.13 & 15.08 & 20.75 & 20.06 \\
TextMonkey & Qwen-7B & 37.30 & 11.69 & 5.93 & 16.14 & 14.26 & 18.18 & 17.06 \\ \bottomrule
\end{tabular}%
}
\vspace{-0.4cm}
\end{table}
Tab.~\ref{tab:res_main_perception} presents the perception capabilities of different models across $5$ domains. Overall, InternVL-2 demonstrates the strongest perception abilities, outperforming other closed-source models. However, the performance varies across different tasks, with some key observations as follows:

1. GPT-4o performs best in real-world OCR tasks, achieving $77\%$ accuracy, but its performance drops significantly in more challenging tasks, falling behind other top-ranked models. This trend is also observed in other closed-source models, such as Gemini-1.5-Pro and GPT-4o-mini, which perform well in OCR tasks but struggle significantly in other real-world tasks. There are three possible reasons: 1) Close-source models often have limitations on the maximum image size and resolution when uploading local images. For example, Claude 3.5 Sonnet has a maximum resolution limit of $8$K and a maximum image quality of $5$MB, while GPT-4o and Gemini-pro allow up to $20$MB. This restricts the input of some high-quality images, as we have to compress the images for upload. 2) Close-source models tend to be more conservative. We observe that the proportion of responses, where closed-source models output ``E'' indicating that the object in question is not present in the image, is high. This suggests that these models may adopt a conservative response strategy to avoid hallucinations or to provide safer answers. 3) Closed-source models sometimes refuse to answer certain questions. Due to different input/output filtering strategies, some samples are considered to involve privacy or harmful content and are therefore not answered.

2. Models allowing higher resolution input, such as Mini-Gemini-HD and SliME, demonstrate a significant advantage over models directly using vision encoders like CLIP, such as ShareGPT4V and LLaVA1.5. At the same model size, these models consistently improve across different subtasks. This highlights the critical importance of high-resolution image processing for addressing complex real-world tasks.

3. There are also notable trends across different domains. Remote sensing tasks involve processing extremely large images, demanding a deeper comprehension of image details. Models that focus on high-resolution input, such as Cambrian-1, Mini-Gemini-HD, and SliME, outperform other models in these tasks. Additionally, models trained on large amounts of chart data exhibit improved perception capabilities for complex charts. For instance, SliME and LLaVA1.5 have limited and relatively simple chart data in their training sets, resulting in inferior performance in this category compared to more recent models.

\subsubsection{Reasoning}
Experimental results on the reasoning tasks are shown in Tab.~\ref{tab:res_main_reasoning}. In terms of reasoning ability, Claude 3.5 Sonnet distinguishes itself as the top performer across most domains, particularly outpacing the second-place GPT-4o by $16.4\%$ in chart-related tasks. The closed-source model GPT-4o also performs well, trailing slightly behind the second-place InternVL-2 but even outperforming it in several domains. Most open-source models perform poorly, with traditional baseline methods such as LLaVA1.5 and Qwen-VL-Chat yielding results close to random guessing.
Furthermore, reasoning tasks are more challenging than perception tasks. Even the top-ranked model fails to achieve an average accuracy above $45\%$, with class-based accuracy not exceeding $50\%$. This indicates that current models still have a significant gap to bridge to reach human-level reasoning capabilities.

\begin{table}[ht]
\caption{\textbf{Experimental results on the reasoning tasks.} Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. “OCR”, “RS”, “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and  Autonomous Driving, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy
and the unweighted average accuracy across
subtasks in each domain.}\label{tab:res_main_reasoning}
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{llcccccc}
\toprule \Gray
\multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{LLM}} & \multicolumn{6}{c}{\textbf{Reasoning}} \\ \cmidrule{3-8} \Gray
\multicolumn{2}{c}{\textbf{Task Split}} & \textbf{OCR} & \textbf{DT} & \textbf{MO} & \textbf{AD} & \textbf{Avg} & \textbf{Avg-C} \\ \Gray
\multicolumn{2}{c}{\textbf{\# QA pairs}} & 500 & 500 & 498 & 1334 & 2832 & 2832 \\ \hline \Lgray
Claude 3.5 Sonnet & - & 61.90 & 61.20 & 41.79 & 31.92 & 44.12 & 49.20 \\
Qwen2-VL & Qwen2-7B & 63.40 & 48.60 & 33.13 & 31.47 & 40.39 & 44.15 \\
InternVL-2 & InternLM2.5-7B-Chat & 57.40 & 39.00 & 43.57 & 29.84 & 38.74 & 42.45 \\ \Lgray
GPT-4o & - & 61.40 & 44.80 & 36.51 & 26.41 & 37.61 & 42.28 \\
CogVLM2-llama3-Chat & Llama3-8B & 54.00 & 32.80 & 41.16 & 31.18 & 37.25 & 39.79 \\
InternVL-Chat-V1-5 & InternLM2-Chat-20B & 56.80 & 35.40 & 37.35 & 28.94 & 36.48 & 39.62 \\
Cambrian-1-8B & Llama3-8B-Instruct & 53.20 & 27.40 & 42.37 & 30.73 & 36.16 & 38.43 \\
SliME-8B & Llama3-8B & 53.20 & 29.40 & 36.14 & 31.55 & 35.80 & 37.57 \\
MiniCPM-V 2.5 & Llama3-8B & 44.00 & 31.80 & 36.95 & 31.03 & 34.50 & 35.95 \\
SliME-13B & Vicuna-13B & 41.00 & 39.00 & 33.13 & 30.80 & 34.46 & 35.98 \\
InternLM-XComposer2.5 & InternLM2-7B & 53.40 & 41.00 & 17.67 & 29.99 & 33.90 & 35.52 \\ \Lgray
GPT-4o-mini & - & 47.00 & 39.80 & 25.81 & 26.79 & 32.48 & 34.85 \\
YI-VL-34B & Yi-34B-Chat & 42.40 & 26.00 & 31.33 & 31.55 & 32.45 & 32.82 \\
LLaVA-Next & Llama3-8B & 55.20 & 23.40 & 21.08 & 30.73 & 32.06 & 32.60 \\
Mini-Gemini-34B-HD & Nous-Hermes-2-Yi-34B & 59.20 & 39.20 & 20.48 & 22.84 & 31.73 & 35.43 \\ \Lgray
Gemini-1.5-pro & - & 52.70 & 33.20 & 28.33 & 19.20 & 29.19 & 33.36 \\
Monkey & Qwen-7B & 27.20 & 20.80 & 27.31 & 33.04 & 28.84 & 27.09 \\
DeepSeek-VL & DeepSeek-LLM-7b-base & 45.20 & 23.80 & 16.67 & 27.31 & 27.98 & 28.25 \\
LLaVA-Next & Qwen-72B & 17.20 & 34.20 & 27.31 & 29.69 & 27.86 & 27.10 \\
Cambrian-1-34B & Nous-Hermes-2-Yi-34B & 55.00 & 36.00 & 19.48 & 16.07 & 27.06 & 31.64 \\
mPLUG-DocOwl 1.5 & Llama-7B & 42.60 & 19.80 & 20.48 & 26.04 & 26.88 & 27.23 \\
Mini-Gemini-7B-HD & Vicuna-7B-v1.5 & 35.40 & 24.60 & 25.90 & 23.29 & 26.12 & 27.30 \\
LLaVA1.5-13B & Vicuna-13B & 30.20 & 20.80 & 27.51 & 24.78 & 25.51 & 25.82 \\
ShareGPT4V-13B & Vicuna-13B & 26.00 & 20.80 & 27.31 & 24.55 & 24.63 & 24.67 \\
LLaVA1.5-7B & Vicuna-7B & 26.00 & 20.60 & 25.90 & 24.18 & 24.17 & 24.17 \\
ShareGPT4V-7B & Vicuna-7B & 24.15 & 20.60 & 26.10 & 24.18 & 23.88 & 23.76 \\
MiniGPT-v2 & Llama 2-7B-Chat & 30.00 & 20.40 & 16.87 & 23.66 & 23.01 & 22.73 \\
Qwen-VL-Chat & Qwen-7B & 28.60 & 13.60 & 16.47 & 24.63 & 21.95 & 20.83 \\
TextMonkey & Qwen-7B & 30.40 & 2.20 & 4.42 & 20.01 & 15.96 & 14.26 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Results on \abbr-CN}

\begin{table}[t]
\caption{\textbf{Experimental results on the perception tasks of \abbr-CN.} Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. “OCR”, “RS”, “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character Recognition in the Wild, Remote Sensing, Diagram and Table, Monitoring, and  Autonomous Driving, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy
and the unweighted average accuracy across
subtasks in each domain.}\label{tab:res_main_perception-cn}
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{llccccccc}
\toprule \Gray
\multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{LLM}} & \multicolumn{7}{c}{\textbf{Perception}}  \\\cmidrule{1-2} \cmidrule{3-9}\Gray
\multicolumn{2}{c}{\textbf{Task Split}} & \textbf{OCR} & \textbf{RS} & \textbf{DT} & \textbf{MO} & \textbf{AD} & \textbf{Avg} & \textbf{Avg-C} \\  \Gray
\multicolumn{2}{c}{\textbf{\# QA pairs}} & 1908 & 300 & 602 & 500 & 700 & 4010 & 4010 \\ \hline
Qwen2-VL & Qwen-7B & 70.28 & 38.33 & 89.20 & 29.40 & 36.86 & 59.80 & 52.81 \\
InternVL-2 & InternLM2.5-7B-Chat & 69.92 & 41.33 & 71.63 & 53.19 & 34.14 & 59.70 & 54.04 \\
InternVL-Chat-V1-5 & InternLM2-Chat-20B & 60.59 & 32.00 & 60.12 & 32.40 & 32.14 & 49.90 & 43.45 \\ \Gray
Claude 3.5 Sonnet & - & 54.44 & 32.67 & 74.09 & 25.00 & 32.43 & 48.25 & 43.73 \\
SliME-8B & Llama3-8B & 53.93 & 41.33 & 58.25 & 29.20 & 31.29 & 46.60 & 42.80 \\ \Gray
GPT-4o & - & 55.90 & 23.67 & 54.86 & 25.20 & 21.14 & 43.44 & 36.15 \\
YI-VL-34B & Yi-34B-Chat & 51.41 & 34.33 & 49.52 & 25.20 & 27.71 & 42.45 & 37.63 \\
SliME-13B & Vicuna-13B & 50.63 & 17.33 & 48.49 & 17.80 & 33.23 & 40.69 & 33.50 \\
Cambrian-1-34B & Nous-Hermes-2-Yi-34B & 48.11 & 33.79 & 44.34 & 27.60 & 26.43 & 40.13 & 36.05 \\
CogVLM2-llama3-Chat & Llama3-8B & 46.12 & 22.00 & 39.48 & 24.80 & 34.14 & 38.57 & 33.31 \\
Mini-Gemini-34B-HD & Nous-Hermes-2-Yi-34B & 41.82 & 38.28 & 40.60 & 27.80 & 34.29 & 38.31 & 36.56 \\
LLaVA-Next & Llama3-8B & 40.62 & 31.67 & 37.49 & 35.40 & 27.29 & 36.50 & 34.49 \\ \Gray
Gemini-1.5-pro & - & 48.32 & 12.33 & 39.78 & 25.20 & 17.57 & 36.10 & 28.64 \\
Monkey & Qwen-7B & 40.46 & 26.55 & 41.12 & 19.20 & 35.86 & 36.07 & 32.64 \\
InternLM-XComposer2.5 & InternLM2-7B & 39.26 & 38.33 & 38.88 & 19.40 & 33.57 & 35.66 & 33.89 \\
Mini-Gemini-7B-HD & Vicuna-7B-v1.5 & 39.66 & 17.24 & 39.29 & 16.80 & 28.29 & 33.09 & 28.26 \\
Cambrian-1-8B & Llama3-8B-Instruct & 32.71 & 35.86 & 30.28 & 27.60 & 35.57 & 32.44 & 32.40 \\
mPLUG-DocOwl 1.5 & LLama-7B & 33.33 & 18.62 & 31.83 & 25.60 & 28.43 & 30.19 & 27.56 \\
LLaVA-Next & Qwen-72B & 32.76 & 23.67 & 28.69 & 34.60 & 23.14 & 30.02 & 28.57 \\
MiniCPM-V 2.5 & Llama3-8B & 33.23 & 16.67 & 31.67 & 20.40 & 26.00 & 28.89 & 25.59 \\
DeepSeek-VL & DeepSeek-LLM-7b-base & 27.10 & 25.44 & 26.02 & 21.60 & 35.71 & 27.63 & 27.17 \\
TextMonkey & Qwen-7B & 31.24 & 11.38 & 30.76 & 19.60 & 26.71 & 27.44 & 23.94 \\ \Gray
GPT-4o-mini & - & 29.56 & 7.33 & 31.79 & 22.00 & 24.00 & 26.32 & 22.94 \\
Qwen-VL-Chat & Qwen-7B & 27.36 & 15.00 & 27.89 & 24.29 & 27.36 & 26.13 & 24.38 \\
ShareGPT4V-13B & Vicuna-13B & 27.94 & 17.59 & 27.57 & 16.80 & 28.14 & 25.75 & 23.61 \\
LLaVA1.5-13B & Vicuna-13B & 27.52 & 17.33 & 26.25 & 17.00 & 28.66 & 25.45 & 23.35 \\
MiniGPT-v2 & Llama 2-7B-Chat & 26.78 & 19.31 & 27.05 & 14.40 & 29.43 & 25.18 & 23.39 \\
ShareGPT4V-7B & Vicuna-7B & 26.73 & 17.24 & 25.75 & 16.60 & 28.14 & 24.86 & 22.89 \\
LLaVA1.5-7B & Vicuna-7B & 26.36 & 16.67 & 25.75 & 16.60 & 28.14 & 24.64 & 22.70 \\ \bottomrule
\end{tabular}%
}
\vspace{-0.2cm}
\end{table}

Results of perception tasks and reasoning tasks are presented in Tab.~\ref{tab:res_main_perception-cn} and Tab.~\ref{tab:res_main_reasoning_cn}, respectively. The models show different performances compared to the \abbr English version. 

1) InternVL-2 significantly outperforms existing models in both perception and reasoning tasks in the Chinese version, even surpassing its performance on the English version, indicating that it has been specifically optimized for Chinese data. 

2) There is a substantial difference in how models handle Chinese and English data, with some models performing much worse in Chinese scenarios, particularly in reasoning tasks. For instance, GPT-4o and GPT-4o-mini show a performance drop of nearly $10\%$. However, some models seem to excel in Chinese-related tasks. Notably, models based on \texttt{Llama3-8B} generally achieve strong results in both Chinese perception and reasoning tasks, such as SliME and CogVLM2. This suggests that \texttt{Llama3-8B} may be an effective LLM backbone for Chinese tasks.

\begin{table}[]
\caption{\textbf{Experimental results on the reasoning tasks of \abbr-CN.} Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction.  “OCR”,  “DT”, “MO”, and “AD” each indicate a specific task domain: Optical Character Recognition in the Wild, Diagram and Table, Monitoring and  Autonomous Driving, respectively. “Avg” and “Avg-C” indicate the weighted average accuracy
and the unweighted average accuracy across
subtasks in each domain.}\label{tab:res_main_reasoning_cn}
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{llcccccc}
\toprule \Gray
\multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{LLM}} & \multicolumn{6}{c}{\textbf{Reasoning}} \\ \cmidrule{3-8} \Gray
\multicolumn{2}{c}{\textbf{Task Split}} & \textbf{OCR} & \textbf{DT} & \textbf{MO} & \textbf{AD} & \textbf{Avg} & \textbf{Avg-C} \\ \Gray
\multicolumn{2}{c}{\# QA pairs} & 207 & 602 & 298 & 800 & 1907 & 1907 \\
InternVL-2 & InternLM2.5-7B-Chat & 44.93 & 74.92 & 43.67 & 29.00 & 47.52 & 48.13 \\ 
Qwen2-VL & Qwen2-7B & 38.16 & 72.92 & 57.00 & 33.37 & 46.46 & 50.36\\
\Gray
Claude 3.5 Sonnet & - & 74.44 & 65.79 & 31.54 & 25.12 & 44.31 & 49.22 \\
SliME-8B & LLama3-8B & 44.44 & 70.93 & 30.54 & 29.13 & 44.21 & 43.76 \\
InternVL-Chat-V1-5 & InternLM2-Chat-20B & 48.79 & 67.11 & 30.20 & 29.88 & 43.74 & 44.00 \\
CogVLM2-llama3-Chat & LLama3-8B & 33.81 & 65.24 & 37.25 & 29.00 & 42.25 & 41.33 \\
YI-VL-34B & Yi-34B-Chat & 37.68 & 61.46 & 33.22 & 29.75 & 41.16 & 40.53 \\
Monkey & Qwen-7B & 43.96 & 56.81 & 32.55 & 28.38 & 39.70 & 40.43 \\
Mini-Gemini-7B-HD & Vicuna-7B-v1.5 & 28.50 & 68.61 & 25.50 & 24.00 & 38.80 & 36.65 \\
Mini-Gemini-34B-HD & Nous-Hermes-2-Yi-34B & 29.95 & 60.47 & 25.50 & 29.63 & 38.75 & 36.39 \\
LLaVA-Next & LLama3-8B & 14.93 & 58.14 & 29.53 & 28.25 & 36.44 & 32.71 \\
Cambrian-1-8B & LLama3-8B-Instruct & 27.54 & 47.51 & 31.21 & 31.25 & 35.97 & 34.38 \\
SliME-13B & Vicuna-13B & 44.93 & 49.17 & 30.54 & 23.87 & 35.18 & 37.13 \\
LLaVA-Next & Qwen-72B & 24.64 & 40.87 & 27.52 & 28.12 & 31.67 & 30.29 \\
InternLM-XComposer2.5 & InternLM2-7B & 18.36 & 40.70 & 16.78 & 30.00 & 30.05 & 26.46 \\ \Gray
GPT-4o & - & 33.81 & 39.87 & 20.81 & 22.75 & 29.05 & 29.31 \\
DeepSeek-VL & DeepSeek-LLM-7b-base & 36.23 & 23.59 & 25.50 & 29.25 & 27.63 & 28.64 \\
Cambrian-1-34B & Hermes2-Yi-34B & 21.74 & 31.40 & 23.49 & 25.12 & 26.48 & 25.44 \\
LLaVA1.5-13B & Vicuna-13B & 36.23 & 27.08 & 25.84 & 23.25 & 26.27 & 28.10 \\
ShareGPT4V-13B & Vicuna-13B & 35.75 & 27.91 & 24.83 & 22.88 & 26.17 & 27.84 \\
MiniCPM-V 2.5 & LLama3-8B & 36.23 & 29.90 & 16.44 & 23.87 & 25.95 & 26.61 \\
LLaVA1.5-7B & Vicuna-7B & 33.33 & 25.91 & 25.17 & 23.25 & 25.48 & 26.92 \\
ShareGPT4V-7B & Vicuna-7B & 33.33 & 25.91 & 24.83 & 23.25 & 25.43 & 26.83 \\
Qwen-VL-Chat & Qwen-7B & 30.92 & 36.41 & 13.42 & 19.88 & 25.29 & 25.16 \\ \Gray
GPT-4o-mini & - & 27.88 & 27.08 & 14.77 & 26.87 & 25.16 & 24.15 \\
MiniGPT-v2 & Llama 2-7B-Chat & 34.30 & 28.57 & 19.80 & 22.13 & 25.12 & 26.20 \\
mPLUG-DocOwl 1.5 & LLama-7B & 37.68 & 24.42 & 19.80 & 22.38 & 24.28 & 26.07 \\
TextMonkey & Qwen-7B & 27.53 & 31.07 & 12.08 & 22.50 & 24.12 & 23.29 \\ \Gray
Gemini-1.5-pro & - & 5.30 & 5.32 & 14.77 & 15.67 & 11.14 & 10.26 \\ \bottomrule
\end{tabular}%
}
\end{table}

\section{Discussion}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/close_source_e.pdf}
\caption{\textbf{Frequency of outputting answer ``E'' for different models} across various domains. The notation in parentheses indicates the task type: P for perception and R for reasoning. The total QA pairs and those with answer ``E'' are also presented for comparison.}
    \label{fig:close_e}
\end{figure}

\textbf{Existing Models Still Lacking in Image Detail Perception.} Fig.~\ref{fig:close_e} displays the frequency with which various models choose “E” as their answer. We compare $4$ close-source models with the best-performing open-source model, InternVL-2. During our annotation process, the frequency of “E” answers does not exceed $5\%$ of the overall data, meaning it represents only a small portion of the total QA pairs. However, nearly all models show a much higher frequency of “E” outputs than the actual number of “E” instances present in our benchmark. This indicates that most models' visual perception modules fail to identify the objects in the images corresponding to our questions. 

\textbf{Limitations of MLLMs in Understanding Dynamic Information.} 
In combination with the results from autonomous driving and monitoring tasks, we observe that MLLMs exhibit significant deficiencies in understanding, predicting, and reasoning about the dynamic information of objects, such as predicting the steering of a car. Although the input to these models is a single frame image rather than a video, there remains a considerable gap between their performance and that of humans. Therefore, it seems that these MLLMs are still far from having the capability to be world models.

\textbf{Computation Efficiency.} 
There is a significant disparity in computation efficiency among different models when processing high-resolution images. For example, using models similar to LLMs (e.g., Vicuna-13B), the computational requirements for handling images exceeding $1024$$\times$$1024$ resolution are as follows: LLaVA1.5 requires $16.37$ TFLOPS, SliME requires $40.82$ TFLOPS, while LLaVA-Next and Mini-Gemini-HD require $78.37$ and $87.59$ TFLOPS, respectively. LLaVA-Next and SliME employ dynamic chunking and encoding of images, while Mini-Gemini-HD uses a higher-resolution vision encoder and significantly increases the number of vision tokens, resulting in a computation cost approximately $5$ times that of LLaVA1.5. 
Additionally, existing methods have inherent limitations in handling high-resolution images. For example, Mini-Gemini-HD resizes images larger than $672$$\times$$672$ to this size, causing a loss of more details. Moreover, we observe interesting phenomena in closed-source models regarding image resolution. For instance, GPT-4o-mini uses over $10,000$ tokens for some large images, which is about $10$ times more than other closed-source models, although its performance does not significantly surpass other models. Overall, we currently lack methods that can efficiently handle higher resolution images with lower computational overhead.



\begin{figure*}[t]
\subfigure[{Claude 3.5 Sonnet}]{
\begin{minipage}[t]{0.33\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_Claude.pdf}
    \label{fig:claude}
\end{minipage}%
}%
\subfigure[{GPT-4o}]{
\begin{minipage}[t]{0.33\linewidth}
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_GPT-4o.pdf}
    \label{fig:gpt4o}
\end{minipage}%
}%
\subfigure[{Cambrian-1-34B}]{
\begin{minipage}[t]{0.33\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_Cambrian.pdf}
    \label{fig:cambrian}
\end{minipage}%
}%

\subfigure[{Monkey}]{
\begin{minipage}[t]{0.33\linewidth}
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_Monkey.pdf}
    \label{fig:monkey}
\end{minipage}%
}%
\subfigure[{mPLUG-DocOwl 1.5}]{
\begin{minipage}[t]{0.33\linewidth}
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_DocOwl.pdf}
    \label{fig:docowl}
\end{minipage}%
}%
\subfigure[{InternVL-2}]{
\begin{minipage}[t]{0.33\linewidth}
 \includegraphics[width=\linewidth]{imgs/confusion_matrix/heatmap_Internvl2.pdf}
    \label{fig:internvl}
\end{minipage}%
}%

\centering
\vspace{-0.2cm}
\caption{\textbf{Distribution of incorrec choices.} The matrix reveals distinct response behaviors among different MLLMs. Larger models tend to select the safer option ``E'', while smaller models exhibit a bias toward the first option ``A''. InternVL-2, however, shows a unique uniform error distribution.}
\label{fig:con_mat}
\end{figure*}
\textbf{Analyzing Incorrect Choices.}
We investigate the distribution of incorrect choices across a range of models, as shown in Fig.~\ref{fig:con_mat}. 
We can see that MLLMs show different response strategies when dealing with questions imbued with uncertainty. 
Larger models generally adopt a more conservative approach, often opting for the safer response ``E'', as illustrated from Fig.~\labelcref{fig:claude,fig:gpt4o,fig:cambrian}.
In contrast, smaller MLLMs often lean towards the first option—usually option ``A''—in similar situations, as shown in Fig.~\labelcref{fig:monkey,fig:docowl}. 
Notably, InternVL-2 presents a unique distribution of incorrect choices that is remarkably uniform, which may account for its exceptional performance in our evaluation.

\textbf{Instruction Following Abilities.} As described in Sec.~\ref{sec:prompt}, our prompts specify that the model should directly select and output a single answer. In this regard, closed-source models generally perform better, with outputs being more concise and directly aligned with the instructions. However, we have observed that some open-source models do not strictly adhere to our queries and generate a significant amount of additional analysis. Sometimes, they even produce outputs that are excessively verbose, continuing until the token count reaches the predefined maximum limit. 
This indicates that the open-source models have a lot of room for optimization in the ability of instruction following.


For detailed results and analysis of all domains and subtasks, please refer to Appendix Sec.~\ref{app:sec_res}.

\section{Conclusion}
In this paper, we have introduced \abbr, a comprehensive benchmark designed to address key limitations in existing evaluations of MLLMs, such as data scale, annotation quality, and task difficulty. As the largest purely human-annotated dataset with the highest resolution to date, \abbr benefits from the participation of $32$ annotators, ensuring high data quality and minimal individual bias. 
Most QA pairs focus on real-world scenarios, such as autonomous driving and video surveillance, which have significant applicability.
Furthermore, we propose \abbr-CN, a benchmark specifically focused on Chinese scenarios, ensuring that all images and questions are relevant to Chinese contexts. 
Our evaluation of a wide range of models reveals significant performance gaps, highlighting the current models' shortcomings in complex image perception and underscoring, and the need for further advancements.


\bibliography{neurips_2024}
\bibliographystyle{plain}


\clearpage
\newpage
\appendix

\input{imgs/appendix}

\end{document}
