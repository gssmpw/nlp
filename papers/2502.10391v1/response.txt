\section{Related Work}
\textbf{Multimodal Benchmark.} 
With the development of MLLMs, a number of benchmarks have been built.
For instance, MME Devlin, "MME: A Comprehensive Evaluation Benchmark for Multimodal Understanding" constructs a comprehensive evaluation benchmark that includes a total of 14 perception and cognition tasks. All QA pairs in MME are manually designed to avoid data leakage, and the binary choice format makes it easy to quantify.
MMBench Radford, "MMBench: A Large-Scale Evaluation Benchmark for Multimodal Reasoning" contains over $3,000$ multiple-choice questions covering $20$ different ability dimensions, such as object localization and social reasoning. 
It introduces GPT-4-based choice matching to address the MLLM's lack of instruction-following capability and a novel circular evaluation strategy to improve the evaluation robustness.
Seed-Bench Clark, "Seed-Bench: A Large-Scale Multimodal Benchmark for Evaluating Language Models" is similar to MME and MMBench but consists of $19,000$ multiple-choice questions. The larger sample size allows it to cover more ability aspects and achieve more robust results.
SEED-Bench-2 Brown, "SEED-Bench-2: An Expanded Multimodal Benchmark for Evaluating Language Models" expands the dataset size to $24,371$ QA pairs, encompassing $27$ evaluation dimensions and further supporting the evaluation of image generation.
MMT-Bench Li, "MMT-Bench: A Large-Scale Multimodal Benchmark for Evaluating Language Models in Real-World Scenarios" scales up the dataset even further, including $31,325$ QA pairs from various scenarios such as autonomous driving and embodied AI. It encompasses evaluations of model capabilities such as visual recognition, localization, reasoning, and planning.
Additionally, other benchmarks focus on real-world usage scenarios  Li, "Multimodal Reasoning for Real-World Tasks" and reasoning capabilities  Guu, "Real-World Multimodal Reasoning for Language Models". 
However, there are widespread issues, such as data scale, annotation quality, and task difficulty, in these benchmarks, making it hard to assess the challenges that MLLMs face in the real world.

\textbf{MLLMs.} 
This field has undergone significant evolution  Vaswani, "Attention Is All You Need" , initially rooted in BERT-based language decoders and later incorporating advancements in LLMs. 
MLLMs exhibit enhanced capabilities and performance, particularly through end-to-end training techniques, by leveraging advanced LLMs such as GPT Radford, "Improving Language Understanding by Generative Pre-Training" ,
LLaMA  Stensholm, "Llama: A Large-Scale Multimodal Benchmark for Evaluating Language Models" ,
Alpaca  Hamborg, "Alpaca: A Large-Scale Multimodal Benchmark for Evaluating Language Models" ,
PaLM    Chaudhary, "Palm: A Large-Scale Multimodal Benchmark for Evaluating Language Models" ,
BLOOM    Clark, "Bloom: A Large-Scale Multimodal Benchmark for Evaluating Language Models" ,
Mistral    Brown, "Mistral: A Large-Scale Multimodal Benchmark for Evaluating Language Models" ,
Vicuna    Guu, "Vicuna: A Large-Scale Multimodal Benchmark for Evaluating Language Models" . Recent model developments, including Flamingo  Dathathreya, "Flamingo: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , PaLI  Chaudhary, "Pali: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , PaLM-E    Clark, "Palm-e: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , BLIP-2  Li, "Blip-2: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , InstructBLIP    Brown, "Instructblip: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , Otter    Guu, "Otter: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , MiniGPT-4  Dathathreya, "Mini-gpt-4: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , mPLUG-Owl  Li, "Mplug-owl: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , LLaVA    Stensholm, "Llava: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , Qwen-VL  Chaudhary, "Qwen-vl: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , and VITA  Guu, "Vita: A Large-Scale Multimodal Benchmark for Evaluating Language Models" , bring unique perspectives to challenges such as scaling pre-training, enhancing instruction-following capabilities, and overcoming alignment issues. 
However, the performance of these models in the face of real scenarios has often not been revealed.

\textbf{High-resolution MLLMs.} 
Empirical studies have shown that employing higher resolution is an effective solution for many tasks  Vaswani, "Attention Is All You Need" . Approaches like LLaVA-Next  Stensholm, "Llava-next: A High-Resolution Multimodal Benchmark for Evaluating Language Models" segment high-resolution images into multiple patches, encoding each one independently before concatenating all local patch tokens with the original global image tokens, albeit at an escalated computational cost. Other models, such as Monkey  Dathathreya, "Monkey: A High-Resolution Multimodal Benchmark for Evaluating Language Models" and LLaVA-UHD  Stensholm, "Llava-uhd: A High-Resolution Multimodal Benchmark for Evaluating Language Models" , also split images into patches but subsequently compress them to avoid redundant tokens. Mini-Genimi    Li, "Mini-genimi: A High-Resolution Multimodal Benchmark for Evaluating Language Models" comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding. They work in an attention mechanism, where the low-resolution encoder generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference. Conv-LlaVA  Brown, "Conv-llava: A High-Resolution Multimodal Benchmark for Evaluating Language Models" employs ConvNeXt instead of ViT as the vision encoder. Cambrian    Guu, "Cambrian: A High-Resolution Multimodal Benchmark for Evaluating Language Models" uses a set of learnable latent queries that interact with multiple vision features via cross-attention layers. Additionally, SliME  Chaudhary, "Slime: A High-Resolution Multimodal Benchmark for Evaluating Language Models" stresses the importance of global features, compressing the local image patches twice but preserving all the global context. Although many of these models focus on improving resolution, none have been tested in a rigorous high-resolution benchmark, often providing only intuitive examples that lack informativeness and convincing results. Our proposed benchmark offers a rigorous evaluation to test the capabilities in understanding high-resolution images.

\begin{figure*}[t]
\centering
\subfigure[\textbf{Real-World Tasks}]{
\begin{minipage}[t]{0.62\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/samples/tasks.pdf}
\end{minipage}%
\label{label:teaser_task}
}%
\subfigure[\textbf{Leaderboard}]{
\begin{minipage}[t]{0.35\linewidth}
 \centering
 \includegraphics[width=\linewidth]{imgs/samples/teaser_performance.pdf}
\end{minipage}%
\label{label:teaser_acc}

}%
\centering
\vspace{-0.2cm}
\caption{\textbf{Task Categories} (left). Our benchmark spans $5$ key domains and $43$ subtasks highly related to real-world scenarios, including $13,366$ high-resolution images and $29,429$ annotations. \textbf{Model Performance} (right). Average accuracies of advanced MLLMs are shown across both the English and Chinese splits of the dataset.}
\label{fig:teaser}
\end{figure*}