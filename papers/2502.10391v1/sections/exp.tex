\section{Experiments}

We evaluate our data and algorithms on 10 tasks across 20+ benchmarks. The key findings are:


1. Alignment training on the \textbf{\abbr} dataset consistently improves performance across nearly all benchmarks for various baselines. The integration of reward signals in MM-DPO further amplifies these improvements, demonstrating the effectiveness of our approach.



2. The \textbf{\abbr-Reward-7B} model achieves state-of-the-art performance on reward model benchmarks among open-source models, surpassing even several 72B models. This highlights the efficiency and scalability of our method.

3. We conduct extensive ablation studies and analyses, such as investigating the importance of critique learning for reward models and the sensitivity to hyperparameters. Additionally, we identify several experimental phenomena that challenge mainstream perspectives, such as the observation that small-scale MLLMs struggle to perform effective self-improvement. Due to space constraints, additional analysis are provided in Appendix~\ref{sec:app_exp}.


\subsection{Benchmarks and Experimental Details}

We categorize the benchmark datasets used in our experiments into the following domains:

\textbf{Chart and Document Understanding}:
AI2D~\cite{kembhavi2016diagram}, ChartQA~\cite{chartqa}, DocVQA~\cite{docvqa}, InfoVQA~\cite{mathew2022infographicvqa}.

\textbf{OCR (Optical Character Recognition)}:
WebSRC~\cite{chen2021websrc}, OCRBench~\cite{liu2024hidden}, TextVQA~\cite{singh2019towards}.

\textbf{Hallucination}:
MMHal-Bench~\cite{mmhal-bench}, POPE~\cite{pope}, Object-Hal~\cite{li2023evaluating}.

\textbf{Math Reasoning}:
MathVista~\cite{lu2024mathvista}, MathVerse~\cite{zhang2024mathverse}.

\textbf{General Knowledge}:
MME~\cite{fu2023mme}, MMbench~\cite{liu2023mmbench}, MMStar~\cite{chen2024we}, SeedBench2-Plus~\cite{li2024seed}, VQAv2~\cite{antol2015vqa}.

\textbf{Conversation}:
LLaVA-Wilder~\cite{li2024llavanext-strong}, LLaVA-In-The-Wild~\cite{liu2023visual}, WildVision-Bench~\cite{lu2024wildvision}.

\textbf{High-Resolution and Real-World Utility}:
RealworldQA, MME-RealWorld~\cite{zhang2024mme}.

\textbf{Video Understanding}:
VideoChatGPT~\cite{Maaz2023VideoChatGPT}, Video-MME~\cite{fu2024video}, VideoDC~\cite{li2024llavanext-strong}.

\textbf{Multi-Image}:
LLAVA-Next-Interleave~\cite{li2024llava}, MMMU-Pro~\cite{yue2024mmmu}.

\textbf{MLLM Safety}:
Our self-constructed benchmark, \abbr-SafeBench, includes adversarial attacks, jailbreaks, privacy, and harmful content. Detailed construction is provided in Appendix~\ref{sec:safety_benchmark}. Safety mainly evaluates the model's ability to reject harmful content, while unsafety mainly assesses the likelihood of the model being successfully attacked.

For all benchmarks requiring GPT-assisted evaluation, we consistently employ GPT-4o as the evaluation model. All model results are rigorously re-evaluated and reported by our team. All experiments are conducted on a high-performance computing cluster equipped with $32\times$H800 (80G) GPUs. Due to computational cost constraints, we utilize the full dataset for the main results presented in Tables~\ref{tab:model_comparison}, \ref{tab:model_comparison_safety}, and \ref{tab:reward_model_comparison}. For ablation studies, we uniformly sample $1/5$ of the data, which may result in minor performance discrepancies compared to the full dataset.

In the implementation of MM-DPO, we adopt a common stabilization technique by incorporating an SFT loss. The weight of the SFT loss is selected through a grid search over the values $\{0, 0.1, 0.25, 0.5, 1.0\}$. Additionally, the learning rate is optimized via a search over $\{1e\text{-}7, 5e\text{-}7, 1e\text{-}6, 5e\text{-}6, 1e\text{-}5\}$ to identify the best-performing configuration. Since we dynamically adjust the $\beta$ parameter during training, the initial value of $\beta_{\text{ori}}$ is set to a small default value of $0.1$, eliminating the need for manual tuning. Throughout all training processes, the vision encoder remains frozen to ensure stable and efficient training.


\subsection{Evaluation of \abbr and MM-DPO}


Table~\ref{tab:model_comparison} (for understanding tasks) and Table~\ref{tab:model_comparison_safety} (for safety tasks) illustrate the alignment performance of LLaVA-OV-7B, LLaVA-OV-0.5B and InternVL-1B using our dataset and alignment algorithm, where the scores for each evaluation dimension are averaged across their respective benchmarks.

\textbf{Significant improvements in conversational ability and safety}. Our experiments show that the alignment process leads to substantial improvements in these two aspects without requiring hyperparameter tuning. The average improvement in conversational benchmarks exceeds 10\%, while unsafe behaviors are reduced by at least 50\%. Additionally, in WildsVision, the win rate increases by at least 50\%. This suggests that existing MLLMs lack explicit optimization for these dimensions, and our dataset effectively fills this gap.

\textbf{Broad enhancements in hallucination, mathematical reasoning, multi-image, and video understanding}. The aligned models also exhibit notable improvements in these areas. Interestingly, despite the lack of dedicated multi-image data in our dataset, the model's performance in multi-image tasks improves significantly. This indicates that the diversity of our alignment data enhances generalization across multiple dimensions.

\textbf{Model-specific preferences for data and hyperparameter}. Different models exhibit varying performance trends during alignment, with distinct preferences for hyperparameter settings across different benchmarks. For instance, in our training of InternVL-1B, we found that excluding the SFT loss led to better results. Additionally, while InternVL-1B demonstrated significant improvements in general knowledge tasks, its relative enhancement in OCR tasks was less pronounced compared to the LLaVA-OV series. These differences largely stem from variations in the models' pretraining datasets and strategies, necessitating tailored hyperparameter adjustments for optimal alignment.

\textbf{Limited gains in high-resolution benchmarks}. The model shows no significant improvement on high-resolution benchmarks, likely because our dataset contains relatively few ultra-high-resolution images. Additionally, our filtering strategy is based on image similarity rather than resolution, meaning the alignment process does not explicitly optimize for high-resolution tasks. As a result, performance gains in this area remain limited.


\textbf{Ablation studies and sensitivity analysis}. To further validate the effectiveness of our approach, we provide detailed ablation studies in the appendix, analyzing the impact of different alignment parameters and the improvements introduced by our dataset and MM-DPO.


\definecolor{light-gray}{gray}{0.6}
\definecolor{front-color}{HTML}{F5FFFA}
\begin{table}[t!]
\caption{\textbf{Performance variations after alignment across 8 different evaluation dimensions}, comparing multiple models under our alignment strategy. All models show comprehensive performance improvements under the proposed alignment, demonstrating significant gains across various tasks.}
\label{tab:model_comparison}
    \centering
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.0}
    \scriptsize
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{p{1.5cm}p{3cm}p{1.5cm}p{1.6cm}p{1.5cm}p{1.6cm}p{1.5cm}p{1.6cm}}  % Fixed column alignment
    \toprule
    \textbf{Capability} & \textbf{Benchmark} & \textbf{InternVL2} \newline \tiny{\color{light-gray}{1B}} & \textbf{Ours} & \textbf{LLaVA-OV} \newline \tiny{\color{light-gray}{0.5B}} & \textbf{Ours} & \textbf{LLaVA-OV} \newline \tiny{\color{light-gray}{7B}} & \textbf{Ours}  \\ \midrule
    \multirow{7}{*}{\parbox{1.5cm}{Conversation}} 
    & LLaVA-Wild~\cite{liu2023visual}~\tiny{(all)} \newline \tiny{\color{light-gray}{Realworld Chat}} & 73.80& \cellcolor{front-color} 75.80 \tiny{\color{brown}{+2.00}}& 74.60 & \cellcolor{front-color} 79.20 \tiny{\color{brown}{+4.60}}& 90.70 & \cellcolor{front-color}97.90 \tiny{\color{brown}{+7.20}} \\
    & LLaVA-Wild~\cite{liu2023visual}~\tiny{(complex)} \newline \tiny{\color{light-gray}{Realworld Chat}} & 83.60 & \cellcolor{front-color} 82.60 \tiny{\color{gray}{-1.00}} & 78.60 & \cellcolor{front-color} 80.50 \tiny{\color{brown}{+1.90}} & 95.90 & \cellcolor{front-color}100.60 \tiny{\color{brown}{+4.70}} \\
     & LLaVA-Wild~\cite{liu2023visual}~\tiny{(conv)} \newline \tiny{\color{light-gray}{Realworld Chat}} & 52.10 & \cellcolor{front-color} 58.30 \tiny{\color{brown}{+6.20}} & 69.60 & \cellcolor{front-color} 72.30 \tiny{\color{brown}{+2.70}} & 81.20 & \cellcolor{front-color}88.10 \tiny{\color{brown}{+6.90}} \\
     & LLaVA-Wild~\cite{liu2023visual}~\tiny{(detail)} \newline \tiny{\color{light-gray}{Realworld Chat}} & 85.40 & \cellcolor{front-color} 89.40 \tiny{\color{brown}{+4.00}} & 82.30 & \cellcolor{front-color} 84.50 \tiny{\color{brown}{+2.20}} & 91.80 & \cellcolor{front-color}104.00 \tiny{\color{brown}{+12.20}} \\
     & LLaVA-Wilder~\cite{li2024llavanext-strong} (small) \newline \tiny{\color{light-gray}{Realworld Chat}} & 55.80 & \cellcolor{front-color} 57.30 \tiny{\color{brown}{+1.50}} & 52.30 & \cellcolor{front-color} 53.40 \tiny{\color{brown}{+1.10}} & 65.70 & \cellcolor{front-color}71.10 \tiny{\color{brown}{+5.40}} \\
     & WildVision~\cite{lu2024wildvision}~\tiny{(elo rate)} \newline \tiny{\color{light-gray}{Model Competition}} & 41.30 & \cellcolor{front-color} 46.20 \tiny{\color{brown}{+4.90}} & 40.70 & \cellcolor{front-color} 44.70 \tiny{\color{brown}{+4.00}} & 50.40 & \cellcolor{front-color}58.90 \tiny{\color{brown}{+8.50}} \\
    & WildVision~\cite{lu2024wildvision}~\tiny{(win rates)} \newline \tiny{\color{light-gray}{Model Competition}} & 41.80 & \cellcolor{front-color} 49.00 \tiny{\color{brown}{+7.20}} & 12.60 & \cellcolor{front-color} 14.60 \tiny{\color{brown}{+2.00}} & 15.20 & \cellcolor{front-color}37.20 \tiny{\color{brown}{+22.00}} \\
    \cmidrule{2-8}
    \multirow{5}{*}{\parbox{1.5cm}{General Knowledge}} 
     & MME~\cite{fu2023mme}~\tiny{(cog./perp.)} \newline \tiny{\color{light-gray}{Multi-discip}} & 1775 & \cellcolor{front-color} 1815 \tiny{\color{brown}{+40}} & 1488 & \cellcolor{front-color} 1510 \tiny{\color{brown}{+22}} & 1997 & \cellcolor{front-color}2025 \tiny{\color{brown}{+28}} \\
    & MMBench~\cite{liu2023mmbench}~\tiny{(cn-dev)} \newline \tiny{\color{light-gray}{Multi-discip}} & 54.70\tiny{\%} & \cellcolor{front-color} 67.89\tiny{\%} \tiny{\color{brown}{+13.19\%}} & 45.80\tiny{\%} & \cellcolor{front-color} 46.40\tiny{\%} \tiny{\color{brown}{+0.60\%}} & 80.49\tiny{\%} & \cellcolor{front-color}80.67\tiny{\%} \tiny{\color{brown}{+0.18\%}} \\
    & MMStar~\cite{chen2024we} \newline \tiny{\color{light-gray}{Multi-discip}} & 45.81\tiny{\%} & \cellcolor{front-color} 49.00\tiny{\%} \tiny{\color{brown}{+3.19\%}} & 38.64\tiny{\%} & \cellcolor{front-color} 39.58\tiny{\%} \tiny{\color{brown}{+0.94\%}} & 61.80\tiny{\%} & \cellcolor{front-color}62.58\tiny{\%} \tiny{\color{brown}{+0.78\%}} \\
    & SeedBench2-Plus~\cite{li2024seed} \newline \tiny{\color{light-gray}{Multi-discip}} & 60.12\tiny{\%} & \cellcolor{front-color} 60.12\tiny{\%} \tiny{\color{brown}{+0.00\%}} & 53.85\tiny{\%} & \cellcolor{front-color} 54.27\tiny{\%} \tiny{\color{brown}{+0.42\%}} & 64.87\tiny{\%} & \cellcolor{front-color}65.35\tiny{\%} \tiny{\color{brown}{+0.48\%}} \\
    & VQAv2~\cite{antol2015vqa}~\tiny{(lite)} \newline \tiny{\color{light-gray}{Multi-discip}} & 72.25\tiny{\%} & \cellcolor{front-color} 71.84\tiny{\%} \tiny{\color{gray}{-0.41\%}} & 74.60\tiny{\%} & \cellcolor{front-color} 74.68\tiny{\%} \tiny{\color{brown}{+0.08\%}} & 79.98\tiny{\%} & \cellcolor{front-color}80.28\tiny{\%} \tiny{\color{brown}{+0.30\%}} \\
\cmidrule{2-8}
    \multirow{4}{*}{\parbox{1.5cm}{Chart and Document}} 
    & AI2D~\cite{kembhavi2016diagram} \newline \tiny{\color{light-gray}{Science Diagrams}} & 72.38\tiny{\%} & \cellcolor{front-color} 72.80\tiny{\%} \tiny{\color{brown}{+0.42\%}} & 56.93\tiny{\%} & \cellcolor{front-color} 56.87\tiny{\%} \tiny{\color{gray}{-0.06\%}} & 81.41\tiny{\%} & \cellcolor{front-color} 81.22\tiny{\%} \tiny{\color{gray}{-0.19\%}} \\
    & ChartQA~\cite{masry2022chartqa}~\tiny{(val-lite)} \newline \tiny{\color{light-gray}{Chart Understanding}} & 65.60\tiny{\%} & \cellcolor{front-color} 66.80\tiny{\%} \tiny{\color{brown}{+1.20\%}} & 51.60\tiny{\%} & \cellcolor{front-color} 52.60\tiny{\%} \tiny{\color{brown}{+1.00\%}} & 74.00\tiny{\%} & \cellcolor{front-color} 74.50\tiny{\%} \tiny{\color{brown}{+0.50\%}} \\
    & DocVQA~\cite{mathew2021docvqa}~\tiny{(val-lite)} \newline \tiny{\color{light-gray}{Document Understanding}} & 81.90\tiny{\%} & \cellcolor{front-color} 82.51\tiny{\%} \tiny{\color{brown}{+0.61\%}} & 66.17\tiny{\%} & \cellcolor{front-color} 67.07\tiny{\%} \tiny{\color{brown}{+0.90\%}} & 84.34\tiny{\%} & \cellcolor{front-color} 86.11\tiny{\%} \tiny{\color{brown}{+1.77\%}} \\
    & InfoVQA~\cite{mathew2022infographicvqa}~\tiny{(val-lite)} \newline \tiny{\color{light-gray}{Infographic Understanding}} & 51.73\tiny{\%} & \cellcolor{front-color} 52.26\tiny{\%} \tiny{\color{brown}{+0.53\%}} & 40.17\tiny{\%} & \cellcolor{front-color} 40.49\tiny{\%} \tiny{\color{brown}{+0.32\%}} & 67.07\tiny{\%} & \cellcolor{front-color} 67.40\tiny{\%} \tiny{\color{brown}{+0.33\%}} \\
\cmidrule{2-8}
     
    \multirow{3}{*}{\parbox{1.5cm}{OCR}} 
     & OCRBench~\cite{liu2024hidden} \newline \tiny{\color{light-gray}{Comprehensive OCR}} & 75.20\tiny{\%} & \cellcolor{front-color} 77.11\tiny{\%} \tiny{\color{brown}{+1.91\%}} & 57.70\tiny{\%} & \cellcolor{front-color} 60.20\tiny{\%} \tiny{\color{brown}{+2.50\%}} & 62.30\tiny{\%} & \cellcolor{front-color} 69.30\tiny{\%} \tiny{\color{brown}{+7.00\%}} \\
    & TextVQA~\cite{singh2019towards}~\tiny{(val)} \newline \tiny{\color{light-gray}{Text Reading}} & 69.85\tiny{\%} & \cellcolor{front-color} 72.12\tiny{\%} \tiny{\color{brown}{+2.27\%}} & 65.87\tiny{\%} & \cellcolor{front-color} 66.60\tiny{\%} \tiny{\color{brown}{+0.73\%}} & 75.99\tiny{\%} & \cellcolor{front-color} 76.05\tiny{\%} \tiny{\color{brown}{+0.06\%}} \\
    & WebSRC~\cite{chen2021websrc}~\tiny{(val)} \newline \tiny{\color{light-gray}{Web-based Structural Reading}} & 68.20\tiny{\%} & \cellcolor{front-color} 68.80\tiny{\%} \tiny{\color{brown}{+0.60\%}} & 65.90\tiny{\%} & \cellcolor{front-color} 68.30\tiny{\%} \tiny{\color{brown}{+2.40\%}} & 88.70\tiny{\%} & \cellcolor{front-color} 89.20\tiny{\%} \tiny{\color{brown}{+0.50\%}} \\
\cmidrule{2-8}
\multirow{3}{*}{\parbox{1.5cm}{Real-World}} 
    & MME-RealWorld~\cite{zhang2024mme}~\tiny{(en-lite)} \newline\tiny{\color{light-gray}{Multi-discip \& High-Resolution}} & 33.61\tiny{\%} & \cellcolor{front-color} 36.58\tiny{\%} \tiny{\color{brown}{+2.97\%}} & 34.55\tiny{\%} & \cellcolor{front-color} 34.39\tiny{\%} \tiny{\color{gray}{-0.16\%}} & 48.36\tiny{\%} & \cellcolor{front-color} 46.95\tiny{\%} \tiny{\color{gray}{-1.41\%}} \\
    & MME-RealWorld~\cite{zhang2024mme}~\tiny{(cn)} \newline \tiny{\color{light-gray}{Multi-discip \& High-Resolution}} & 44.14\tiny{\%} & \cellcolor{front-color} 43.11\tiny{\%} \tiny{\color{ gray}{-1.03\%}} & 32.09\tiny{\%} & \cellcolor{front-color} 31.11\tiny{\%} \tiny{\color{gray}{-0.98\%}} & 54.01\tiny{\%} & \cellcolor{front-color} 53.39\tiny{\%} \tiny{\color{brown}{-0.62\%}} \\
    & RealWorldQA \newline \tiny{\color{light-gray}{Realworld QA}} & 51.50\tiny{\%} & \cellcolor{front-color} 54.90\tiny{\%} \tiny{\color{brown}{+3.40\%}} & 55.42\tiny{\%} & \cellcolor{front-color} 55.16\tiny{\%} \tiny{\color{gray}{-0.26\%}} & 66.41\tiny{\%} & \cellcolor{front-color} 65.75\tiny{\%} \tiny{\color{gray}{-0.66\%}} \\
\cmidrule{2-8}
     \multirow{4}{*}{Math}
    & MathVista~\cite{lu2024mathvista}~\tiny{(cot)} \newline \tiny{\color{light-gray}{General Math Understanding}} & 49.60\tiny{\%} & \cellcolor{front-color} 49.90\tiny{\%} \tiny{\color{brown}{+0.30\%}} & 32.30\tiny{\%} & \cellcolor{front-color} 32.70\tiny{\%} \tiny{\color{brown}{+0.40\%}} & 59.10\tiny{\%} & \cellcolor{front-color} 61.60\tiny{\%} \tiny{\color{brown}{+2.50\%}} \\
    & MathVista~\cite{lu2024mathvista}~\tiny{(format)} \newline \tiny{\color{light-gray}{General Math Understanding}} & 53.20\tiny{\%} & \cellcolor{front-color} 53.40\tiny{\%} \tiny{\color{brown}{+0.20\%}} & 36.00\tiny{\%} & \cellcolor{front-color} 36.30\tiny{\%} \tiny{\color{brown}{+0.30\%}} & 62.50\tiny{\%} & \cellcolor{front-color} 62.20\tiny{\%} \tiny{\color{gray}{-0.30\%}} \\
    & MathVista~\cite{lu2024mathvista}~\tiny{(solution)} \newline \tiny{\color{light-gray}{General Math Understanding}} & 49.60\tiny{\%} & \cellcolor{front-color} 49.30\tiny{\%} \tiny{\color{gray}{-0.30\%}} & 30.50\tiny{\%} & \cellcolor{front-color} 32.50\tiny{\%} \tiny{\color{brown}{+2.00\%}} & 58.80\tiny{\%} & \cellcolor{front-color} 61.10\tiny{\%} \tiny{\color{brown}{+2.30\%}} \\
    & MathVerse~\cite{zhang2024mathverse}~\tiny{(vision-mini)} \newline \tiny{\color{light-gray}{Professional Math Reasoning}} & 12.31\tiny{\%} & \cellcolor{front-color} 12.79\tiny{\%} \tiny{\color{brown}{+0.48\%}} & 17.51\tiny{\%} & \cellcolor{front-color} 17.64\tiny{\%} \tiny{\color{brown}{+0.13\%}} & 16.37\tiny{\%} & \cellcolor{front-color} 18.53\tiny{\%} \tiny{\color{brown}{+2.16\%}} \\
\cmidrule{2-8}
     \multirow{7}{*}{\parbox{1.5cm}{Hallucination}} 
  
     & POPE~\cite{pope}~\tiny{(adversarial)} \newline \tiny{\color{light-gray}{Object Hallucination.}} & 86.82\tiny{\%} & \cellcolor{front-color} 86.87\tiny{\%} \tiny{\color{brown}{+0.05\%}} & 86.04\tiny{\%} & \cellcolor{front-color} 86.56\tiny{\%} \tiny{\color{brown}{+0.52\%}} & 87.08\tiny{\%} & \cellcolor{front-color}87.68\tiny{\%} \tiny{\color{brown}{+0.60\%}}  \\
     & POPE~\cite{pope}~\tiny{(popular)} \newline \tiny{\color{light-gray}{Object Hallucination.}} & 88.30\tiny{\%} & \cellcolor{front-color} 88.57\tiny{\%} \tiny{\color{brown}{+0.27\%}} & 87.37\tiny{\%} & \cellcolor{front-color} 88.26\tiny{\%} \tiny{\color{brown}{+0.89\%}} & 88.32\tiny{\%} & \cellcolor{front-color}89.02\tiny{\%} \tiny{\color{brown}{+0.70\%}}  \\
     & POPE~\cite{pope}~\tiny{(random)} \newline \tiny{\color{light-gray}{Object Hallucination.}} & 89.87\tiny{\%} & \cellcolor{front-color} 90.45\tiny{\%} \tiny{\color{brown}{+0.58\%}} & 88.30\tiny{\%} & \cellcolor{front-color} 89.30\tiny{\%} \tiny{\color{brown}{+1.00\%}} & 89.60\tiny{\%} & \cellcolor{front-color}90.62\tiny{\%} \tiny{\color{brown}{+1.02\%}}  \\
    & MMHal~\cite{mmhal-bench}~\tiny{(hal rate $\downarrow$)} \newline \tiny{\color{light-gray}{General Hallucination}} & 55.21\tiny{\%} & \cellcolor{front-color} 55.38\tiny{\%} \tiny{\color{gray}{-0.17\%}} & 48.96\tiny{\%} & \cellcolor{front-color} 46.25\tiny{\%} \tiny{\color{brown}{+2.71\%}} & 38.54\tiny{\%} & \cellcolor{front-color}38.54\tiny{\%} \tiny{\color{brown}{+0.00\%}}  \\
     & MMHal~\cite{mmhal-bench}~\tiny{(avg score)} \newline \tiny{\color{light-gray}{General Hallucination}} & 3.02 & \cellcolor{front-color} 3.10 \tiny{\color{brown}{+0.08}} & 3.33 & \cellcolor{front-color} 3.42 \tiny{\color{brown}{+0.09}} & 3.22 & \cellcolor{front-color}4.08 \tiny{\color{brown}{+0.86}}  \\
     & Obj-Hal~\cite{li2023evaluating}~\tiny{(chair-i$\downarrow$)} \newline \tiny{\color{light-gray}{Object Hallucination.}} & 8.30 & \cellcolor{front-color}  7.81 \tiny{\color{brown}{+0.49}} & 9.70 & \cellcolor{front-color} 9.12 \tiny{\color{brown}{+0.58}} & 8.52 & \cellcolor{front-color}7.69 \tiny{\color{brown}{+0.83}}  \\
     & Obj-Hal~\cite{li2023evaluating}~\tiny{(chair-s$\downarrow$)} \newline \tiny{\color{light-gray}{Object Hallucination.}} &38.67 & \cellcolor{front-color} 37.00 \tiny{\color{brown}{+1.67}} & 42.67 & \cellcolor{front-color}42.33 \tiny{\color{brown}{+0.34}} & 44.00 & \cellcolor{front-color}41.67 \tiny{\color{brown}{+2.33}}  \\
\cmidrule{2-8}
    \multirow{4}{*}{\parbox{1.5cm}{Video Understanding}} 
     & Video-MME~\cite{fu2024video}~\tiny{(w. caption)} \newline\tiny{\color{light-gray}{Multi-discip}} & 42.74\tiny{\%} & \cellcolor{front-color} 42.76\tiny{\%} \tiny{\color{brown}{+0.02\%}} &48.22\tiny{\%} & \cellcolor{front-color} 48.42\tiny{\%} \tiny{\color{brown}{+0.20\%}} & 61.61\tiny{\%} & \cellcolor{front-color}61.81\tiny{\%} \tiny{\color{brown}{+0.20\%}}  \\
    & Video-MME~\cite{fu2024video}~\tiny{(wo. caption)} \newline\tiny{\color{light-gray}{Multi-discip}} & 45.66\tiny{\%} & \cellcolor{front-color} 45.71\tiny{\%} \tiny{\color{brown}{+0.05\%}} & 43.92\tiny{\%} & \cellcolor{front-color} 44.00\tiny{\%} \tiny{\color{brown}{+0.08\%}} & 58.29\tiny{\%} & \cellcolor{front-color}58.33\tiny{\%} \tiny{\color{brown}{+0.04\%}}  \\
     & VideoChatGPT~\cite{Maaz2023VideoChatGPT} \newline  \tiny{\color{light-gray}{Video Conversation}}  & 2.26 & \cellcolor{front-color} 2.59 \tiny{\color{brown}{+0.33}} & 2.56 & \cellcolor{front-color} 2.66 \tiny{\color{brown}{+0.10}} & 2.87 & \cellcolor{front-color}3.22 \tiny{\color{brown}{+0.35}}  \\
     & VideoDC~\cite{li2024llavanext-strong} \newline \tiny{\color{light-gray}{Video Detail Description}} & 2.91 & \cellcolor{front-color} 3.07 \tiny{\color{brown}{+0.16}} & 2.88 & \cellcolor{front-color} 2.96 \tiny{\color{brown}{+0.08}} & 3.32 & \cellcolor{front-color}3.41 \tiny{\color{brown}{+0.09}}  \\
\cmidrule{2-8}
    \multirow{2}{*}{\parbox{1.5cm}{Multi-Image}} 
    & LLAVA-Next-Interleave~\cite{li2024llava}~\tiny{(in-domain)} \newline\tiny{\color{light-gray}{in-domian}} & 34.78\tiny{\%} & \cellcolor{front-color} 35.72\tiny{\%} \tiny{\color{brown}{+0.94\%}} & 42.29\tiny{\%} & \cellcolor{front-color} 43.49\tiny{\%} \tiny{\color{brown}{+1.20\%}} & 60.85\tiny{\%} & \cellcolor{front-color}61.12\tiny{\%} \tiny{\color{brown}{+0.27\%}}  \\
    & MMMU-Pro~\cite{yue2024mmmu}~\tiny{(vision)} \newline\tiny{\color{light-gray}{Multi-discip}} & 1.11\tiny{\%} & \cellcolor{front-color} 1.52\tiny{\%} \tiny{\color{brown}{+0.41\%}} & 12.78\tiny{\%} & \cellcolor{front-color} 13.89\tiny{\%} \tiny{\color{brown}{+1.11\%}} & 14.51\tiny{\%} & \cellcolor{front-color}15.84\tiny{\%} \tiny{\color{brown}{+1.33\%}}  \\
    \midrule
    \end{tabular}%
    }
\vspace{2mm}
\end{table}

\definecolor{front-color}{HTML}{FDEFF5}
\begin{table}[t!]
\caption{\textbf{Performance variations after alignment across \abbr-SafeBench}, comparing multiple models under our alignment strategy.}
\label{tab:model_comparison_safety}
    \centering
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.0}
    \scriptsize
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{p{3cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}  % Fixed column alignment
    \toprule
    \textbf{Benchmark} & \textbf{InternVL2} \newline \tiny{\color{light-gray}{1B}} & \textbf{Ours} & \textbf{LLaVA-OV} \newline \tiny{\color{light-gray}{0.5B}} & \textbf{Ours} & \textbf{LLaVA-OV} \newline \tiny{\color{light-gray}{7B}} & \textbf{Ours}  \\ \midrule
    Adv target~\tiny{$\downarrow$} \newline\tiny{\color{light-gray}{Adversarial Attack}} & 56.0\% & \cellcolor{front-color} 50.0\% \tiny{\color{brown}{+5.0\%}} & 54.0\% & \cellcolor{front-color} 35.0\% \tiny{\color{brown}{+19.0\%}}& 37.0\% & \cellcolor{front-color} 40.0\% \tiny{\color{gray}{-3.0\%}} \\
    Adv untarget~\tiny{$\uparrow$}  \newline\tiny{\color{light-gray}{Adversarial Attack}} & 52.5\% & \cellcolor{front-color} 56.0\% \tiny{\color{brown}{+3.5\%}}& 66.0\% & \cellcolor{front-color} 71.0\% \tiny{\color{brown}{+5\%}}& 66.5\% & \cellcolor{front-color} 70.0\% \tiny{\color{brown}{+3.5\%}} \\
    Crossmodel ASR~\tiny{$\downarrow$} \newline\tiny{\color{light-gray}{Cross-modal Jailbreak}} & 0.0\% & \cellcolor{front-color} 0.0\% \tiny{\color{brown}{+0.0\%}}& 72.2\% & \cellcolor{front-color} 38.9\% \tiny{\color{brown}{+33.3\%}}& 16.7\% & \cellcolor{front-color} 0.0\% \tiny{\color{brown}{+16.7\%}} \\
    Crossmodel RtA~\tiny{$\uparrow$}  \newline\tiny{\color{light-gray}{Cross-modal Jailbreak}} & 100.0\% & \cellcolor{front-color} 100.0\% \tiny{\color{brown}{+0.0\%}}& 22.2\% & \cellcolor{front-color} 50.0\% \tiny{\color{brown}{+27.8\%}}& 88.9\% & \cellcolor{front-color} 100.0\%  \tiny{\color{brown}{+11.1\%}}\\
    Multimodel ASR~\tiny{$\downarrow$} \newline\tiny{\color{light-gray}{Multimodal Jailbreak}} & 43.2\% & \cellcolor{front-color} 43.2\% \tiny{\color{brown}{+0.0\%}}& 42.2\% & \cellcolor{front-color} 27.7\% \tiny{\color{brown}{+14.5\%}}& 41.2\% & \cellcolor{front-color} 8.3\% \tiny{\color{brown}{+31.9\%}}\\
    Multimodel RtA~\tiny{$\uparrow$}  \newline\tiny{\color{light-gray}{Multimodal Jailbreak}} & 18.0\% & \cellcolor{front-color} 17.4\% \tiny{\color{gray}{-0.6\%}}& 12.4\% & \cellcolor{front-color} 23.2\% \tiny{\color{brown}{+10.8\%}}& 62.0\% & \cellcolor{front-color} 88.3\%  \tiny{\color{brown}{+26.3\%}}\\
    Typographic ASR~\tiny{$\downarrow$} \newline\tiny{\color{light-gray}{Typographic Jailbreak}} & 10.5\% & \cellcolor{front-color} 7.4\% \tiny{\color{brown}{+3.1\%}}& 26.3\% & \cellcolor{front-color} 35.2\% \tiny{\color{gray}{-8.9\%}}& 5.8\% & \cellcolor{front-color} 0.0\%  \tiny{\color{brown}{+5.8\%}}\\
    Typographic RtA~\tiny{$\uparrow$}  \newline\tiny{\color{light-gray}{Typographic Jailbreak}} & 73.7\% & \cellcolor{front-color} 74.6\% \tiny{\color{brown}{+0.9\%}}& 17.0\% & \cellcolor{front-color} 27.5\% \tiny{\color{brown}{+10.5\%}}& 79.5\% & \cellcolor{front-color} 95.8\%  \tiny{\color{brown}{+16.3\%}}\\
    Risk~\tiny{$\uparrow$}  \newline\tiny{\color{light-gray}{Risk identification}} & 49.6\% & \cellcolor{front-color} 58.6\% \tiny{\color{brown}{+9.0\%}} & 65.8\% & \cellcolor{front-color} 67.4\% \tiny{\color{brown}{+1.6\%}}& 82.0\% & \cellcolor{front-color} 76.0\%  \tiny{\color{gray}{-6.0\%}}\\
    NSFW~\tiny{text$\downarrow$} \newline\tiny{\color{light-gray}{NSFW Jailbreak}} & 89.0\% & \cellcolor{front-color} 27.1\% \tiny{\color{brown}{+61.9\%}}& 94.4\% & \cellcolor{front-color} 64.2\% \tiny{\color{brown}{+30.2\%}}& 60.4\% & \cellcolor{front-color} 10.6\%  \tiny{\color{brown}{+49.8\%}}\\
    NSFW~\tiny{img$\downarrow$} \newline\tiny{\color{light-gray}{NSFW Jailbreak}} & 81.2\% & \cellcolor{front-color} 64.7\% \tiny{\color{brown}{+16.5\%}}& 97.5\% & \cellcolor{front-color} 81.6\% \tiny{\color{brown}{+15.9\%}}& 80.1\% & \cellcolor{front-color} 24.2\% \tiny{\color{brown}{+55.9\%}} \\
    \hline
    Unsafety~\tiny{ $\downarrow$} \newline\tiny{\color{light-gray}{Average performance of $\downarrow$}} & 46.6\% & \cellcolor{front-color} 38.9\% \tiny{\color{brown}{+7.7\%}}& 65.4\% & \cellcolor{front-color} 47.1\% \tiny{\color{brown}{+18.3\%}} & 40.2\% & \cellcolor{front-color} 13.9\%  \tiny{\color{brown}{+26.3\%}}\\
    Safety~\tiny{ $\uparrow$} \newline\tiny{\color{light-gray}{Average performance of $\uparrow$}} & 31.9\% & \cellcolor{front-color} 41.3\% \tiny{\color{brown}{+9.4\%}}& 36.7\% & \cellcolor{front-color} 47.8\% \tiny{\color{brown}{+11.1\%}}& 75.8\% & \cellcolor{front-color} 85.4\% \tiny{\color{brown}{+9.6\%}} \\
    \midrule
    \end{tabular}%
    }
\vspace{2mm}
\end{table}



\subsection{Evaluation of \abbr-Reward}
In this section, we evaluate the effectiveness of \abbr-Reward and highlight several noteworthy experimental observations. The results are presented in Table~\ref{tab:rm_performance_comparison} and Table~\ref{tab:reward_model_comparison}.

\textbf{Existing reward models exhibit significant overfitting}.
As shown in Table~\ref{tab:rm_performance_comparison}, LLaVA-Critic's performance on \abbr-Reward-Bench is suboptimal, with a considerable gap compared to GPT-4o. This can likely be attributed to the overfitting of existing reward models to their training data, which predominantly consists of conversational datasets and real-world images. Consequently, while LLaVA-Critic demonstrates notable improvements over its baseline, LLaVA-OV-7B\footnote{Both models use identical prompts for tasks such as captioning and long-form dialogue.}, its performance in other categories, such as MCQ and more diverse tasks, remains limited.

\textbf{Closed-source models like GPT-4o consistently deliver competitive performance}.
Across both Table~\ref{tab:rm_performance_comparison} and Table~\ref{tab:reward_model_comparison}, closed-source models such as GPT-4o demonstrate superior generalization capabilities compared to open-source alternatives, even those with significantly larger parameter sizes (e.g., 72B models). This observation underscores the robustness of closed-source approaches in handling diverse multimodal tasks and maintaining high performance across various metrics.

\textbf{\abbr-Reward sets a new benchmark for open-source models, rivaling closed-source systems}.
In both benchmarks, \abbr-Reward achieves results comparable to or exceeding GPT-4o's performance, while significantly outperforming most open-source models, such as LLaMA-3.2-90B-Vision-Instruct and Qwen2-VL-72B-Instruct. Notably, on our custom benchmark, \abbr-Reward demonstrates a substantial lead over GPT-4o, further justifying its selection as the reward signal for training algorithms. Its robust performance across diverse metrics highlights its effectiveness and adaptability.

\textbf{The importance of an effective critic in reward modeling}.
The results in Table~\ref{tab:rm_performance_comparison} underscore the critical role of an effective critic in reward modeling. When the reward head is directly trained using pair-wise datasets, the ACC+ stabilizes around 50\%. By incorporating human annotations as the learning target—allowing the model to first learn evaluation reasoning and then perform scoring—the ACC+ improves by a consistent 5\%. However, human annotations alone may not serve as an optimal training target due to their brevity or conversational style. To address this, we expand the human annotations using the model itself, producing enriched annotations that further enhance reward model training quality. This results in a significant 17\% improvement in ACC+ compared to the baseline. Finally, during evaluation, when human annotations are directly provided as the critic (i.e., scoring is based on human-provided evaluations rather than model-generated critics), both ACC and ACC+ reach approximately 90\%. This demonstrates the pivotal role of evaluation quality in the overall effectiveness of reward models.

\textbf{Multiple sampling of critiques does not yield significant performance gains}. When the model generates critiques with high variability, multiple sampling is often used to compute scores and then take the average~\cite{yu2024self}. This approach has proven effective in related LLM research. However, in our experiments, we observed that when we lowered the sampling temperature and computed rewards multiple times, the performance actually declined. The reason for this is that during the sampling process, there is occasionally a critique that is inaccurate. Since our model is already capable of generating reasonably accurate critiques due to its alignment with human annotations, the extra, time-consuming sampling process does not provide additional benefits and can even have a negative impact on performance.


\begin{table*}[ht]
\centering
\caption{
\textbf{Performance comparison across metrics and methods on \abbr-RewardBench}. 
\textit{\abbr-Reward (w/o. Task 1)} represents training the LLaVA-OV-7B model to score pair-wise samples while excluding Task 1. 
\textit{\abbr-Reward (w/o. enhanced annotations)} involves learning human-provided annotations, followed by scoring. 
\textit{\abbr-Reward (inference w. GT annotation)} uses ground truth annotations during inference. 
}
\label{tab:rm_performance_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccc|cccccccc}
\toprule
\textbf{Method} & \multicolumn{2}{c}{\textbf{LLaVA-OV-7B}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LlaVA-Critic\\ (Pointwise)\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}LlaVA-Critic\\ (Pairwise)\end{tabular}}} & \multicolumn{2}{c|}{\textbf{GPT-4o}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\abbr-Reward\\ (w/o. Task 1)\end{tabular}}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\abbr-Reward\\ (w/o. enhanced \\ annotations)\end{tabular}}} & \multicolumn{2}{c}{\textbf{\abbr-Reward}} & \multicolumn{2}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}\abbr-Reward\\ (inference w. \\ GT annotation)\end{tabular}}} \\
\textbf{Metric} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} & \textbf{ACC} & \textbf{ACC+} \\

\midrule
Mcq      & 0.14 & 0.00 & 0.38 & 0.10 & 0.23 & 0.00 & 0.69 & 0.20 & 0.90 & 0.80 & 0.83 & 0.70 & 0.93 & 0.70 & \textbf{1.00} & \textbf{1.00} \\
Long     & 0.11 & 0.00 & 0.49 & 0.20 & 0.54 & 0.30 & 0.95 & 0.90 & 0.70 & 0.40 & 0.92 & 0.80 & 1.00 & 1.00 & \textbf{1.00} & \textbf{1.00} \\
Short    & 0.29 & 0.20 & 0.38 & 0.20 & 0.24 & 0.10 & 0.56 & 0.40 & 0.79 & 0.60 & 0.68 & 0.40 & 0.71 & 0.50 & \textbf{1.00} & \textbf{1.00} \\
Safety   & 0.41 & 0.00 & 0.62 & 0.17 & 0.28 & 0.17 & \textbf{0.72} & \textbf{0.33} & 0.69 & \textbf{0.33} & 0.69 & 0.17 & 0.66 & 0.17 & 0.69 & 0.17 \\
Video    & 0.32 & 0.10 & 0.40 & 0.20 & 0.52 & 0.20 & 0.80 & 0.60 & 0.70 & 0.60 & 0.80 & 0.60 & \textbf{0.92} & 0.80 & \textbf{0.92} & \textbf{0.90} \\ \hline
Overall  & 0.24 & 0.07 & 0.45 & 0.17 & 0.35 & 0.15 & 0.74 & 0.50 & 0.75 & 0.50 & 0.79 & 0.57 & 0.85 & 0.67 & \textbf{0.93} & \textbf{0.87} \\ 
\bottomrule
\end{tabular}%
}
\end{table*}

\begin{table}[htbp]
    \centering
    % \renewcommand{\arraystretch}{1.3}
    % \setlength{\tabcolsep}{10pt}
    \caption{\textbf{Performance comparison of our reward model (\abbr-Reward) with existing open-source and private multi-modal models.} \abbr-Reward-7B outperforms existing 72B open-source multi-modal models and several highly competitive closed-source models.}
    \label{tab:reward_model_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model} & \textbf{General} & \textbf{Hallucination} & \textbf{Reasoning} & \textbf{Avg} \\
        \midrule
        VITA-1.5~\cite{fu2025vita}                & 18.55 & 8.93 & 22.11 & 16.48 \\
        SliME-8B~\cite{zhang2024beyond}                & 7.23 & 27.09 & 18.6 & 19.04 \\
        deepseek-vl2~\cite{wu2024deepseekvl2mixtureofexpertsvisionlanguagemodels}                & 29.70 & 23.80 & 50.90 & 34.80 \\
        Phi-3.5-vision-instruct~\cite{abdin2024phi}     & 28.00 & 22.40 & 56.60 & 35.67 \\
        llava-onevision-qwen2-7b-ov~\cite{li2024llava}& 32.20 & 20.10 & 57.10 & 36.47 \\
        Molmo-7B-D-0924~\cite{deitke2024molmo}             & 31.10 & 31.80 & 56.20 & 39.70 \\
        Pixtral-12B-2409~\cite{agrawal2024pixtral}            & 35.60 & 25.90 & 59.90 & 40.47 \\
        Qwen2-VL-72B-Instruct~\cite{wang2024qwen2}       & 38.10 & 32.80 & 58.00 & 42.97 \\
        NVLM-D-72B~\cite{dai2024nvlm}                  & 38.90 & 31.60 & 62.00 & 44.17 \\
        InternVL2-26B~\cite{chen2024far}               & 39.30 & 36.90 & 60.80 & 45.67 \\ \hline
        \Gray
\multicolumn{5}{c}{\textit{Private models}} \\
        GPT-4o-mini (2024-07-18)    & 41.70 & 34.50 & 58.20 & 44.80 \\
        Claude-3.5-Sonnet (2024-06-22) & 43.40 & 55.00 & 62.30 & 53.57 \\
        GPT-4o (2024-08-06)         & 49.10 & 67.60 & 70.50 & 62.40 \\
        Gemini-1.5-Pro (2024-09-24) & 50.80 & 72.50 & 64.20 & 62.50 \\
        \midrule\Gray
        \multicolumn{5}{c}{Ours}      \\
        \abbr-Reward-7B                & 45.04 & 50.45 & 57.55 & 50.15 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{{Self-Improvement of Small-Scale MLLMs is Currently Unrealistic}}

\begin{figure}
    \centering\includegraphics[width=0.9\linewidth]{imgs/exps/self-improvement.pdf}
    \caption{\textbf{Performance comparison across datasets using various methods based on the LLaVA-Ov-7B model as the baseline}. ``Baseline" represents the initial performance without post-training. ``LLAVA-RLHF (LLAVA-RLHF)'' indicates that both the post-training dataset and the reward model come from the LLAVA-RLHF dataset, with the reward model being trained using LLaVA-Ov-7B as the starting checkpoint for fairness. ``\abbr$^s$'' reflects results generated on our dataset, where responses are self-sampled (default sample size: 8) and ranked using different reward signals to create DPO pairs. ``\abbr$^h$ (Human)'' involves DPO training directly using our dataset, where responses are sampled from other models, and reward signals are provided by experts.}
    \label{fig:self-improvement}
\end{figure}

While recent work on MLLMs explores the concept of self-improvement, these efforts largely focus on specific domains, such as conversational systems~\cite{xiong2024llava}. In this section, we present an alternative perspective distinct from the LLM domain, arguing that MLLMs, particularly small models (fewer than 7B parameters), currently face significant challenges in achieving comprehensive performance improvements through self-improvement. Our experimental results, illustrated in Figure~\ref{fig:self-improvement}, suggest two primary reasons for this limitation:

\textbf{1. Model capacity constraints}. 
For tasks involving long-form or conversational data, sampling multiple responses often results in at least one reasonably good answer, thereby leading to noticeable improvements. However, for more challenging tasks, such as multiple-choice questions or scientific reasoning, smaller models struggle to generate correct answers even after extensive sampling. In our experiments, where the maximum number of samples reached eight, we observed instances where the model produced identical incorrect responses or consistently incorrect outputs across all samples for some challenging multiple-choice questions.

\textbf{2. Limitations in reward signal quality}.
Most existing multimodal reward models are trained on datasets with limited diversity, such as VLFeedback and LLaVA-RLHF. These datasets predominantly focus on natural images, human dialogue, or related scenarios, raising concerns about overfitting. When preference datasets encompass broader domains, such as mathematical reasoning, chart understanding, or other specialized fields, reward models trained on existing datasets fail to provide effective reward signals. Consequently, it becomes challenging to identify and select better samples.

These two limitations make it difficult, at the current stage, to enable MLLMs to generate responses on diverse datasets, annotate them with reward models, and iteratively improve through self-improvement cycles, as has been achieved in LLM alignment. While our experiments confirm that better reward models can lead to marginal improvements, the results remain far inferior to training with high-quality, human-annotated contrastive samples.
