
\section{\abbr-Reward Model}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/reward_model.pdf}
    \caption{\textbf{Illustration of the multi-task reward model training process.} The process begins with a user query and corresponding model responses, which are ranked and annotated by humans. Human annotations are expanded using GPT-4o to provide enhanced rationales. The reward model is trained with two objectives: (1) \textit{Learning to Provide Critique}, where the model learns to provide detailed critiques and evaluations for model responses, and (2) \textit{Learning Scoring}, where the model learns to assign scores based on the model response and critique. The integration of these tasks ensures a robust evaluation framework for improving model outputs.}
    \label{fig:rm_teaser}
\end{figure*}
In this section, we explore how to train a high-quality reward model using the \abbr~dataset to provide a robust supervision signal for subsequent model alignment. The reward model is designed to combine critique generation and scoring (Figure~\ref{fig:rm_teaser}), ensuring a comprehensive evaluation process.
\subsection{Background and Limitations of Standard Reward Models}

Reward models are a key component for aligning model outputs with human preferences. Typically, a reward model starts with a pretrained LLM $\phi$, where the LLM head $h_l$ is replaced with a linear reward head $l_r$, enabling the model to output a scalar reward value. These models are trained using human-provided pairwise comparisons. Given a query $\mathbf{x}$, a preferred response $y_w$ and a less preferred response $y_l$, the reward model is optimized to assign higher rewards to preferred responses:
\begin{equation}
\ell_{\text{Reward}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y_w, y_l} 
\Big[ 
    - \log \sigma \Big( r(y_w | \mathbf{x}) - r(y_l | \mathbf{x}) \Big)
\Big],
\end{equation}
where $r(y | \mathbf{x})$ is the scalar reward and $\sigma$ is the sigmoid function.

Despite their utility, standard reward models face significant limitations. First, they fail to fully utilize the rich and detailed feedback provided by high-quality human annotations, such as textual explanations and nuanced reasoning. Second, scalar rewards lack transparency, making it difficult for humans to understand how the reward is generated. These challenges highlight the need for a more interpretable and robust reward model that leverages critiques as intermediate reasoning steps.

\subsection{Critique-Based Reward Model Training}

\textbf{Extending to critique-based training}. To overcome the limitations of traditional reward models, we propose a critique-based training framework: the model first generates a critique $c$ conditioned on the query $\mathbf{x}$. This critique serves as an intermediate reasoning step, providing context for scoring responses. The critique-based reward model comprises two components:
\textbf{1. Critique Head ($h_l$)}: Generates critiques $c_w$ and $c_l$ for the preferred ($y_w$) and less preferred ($y_l$) responses, respectively, based on the query $\mathbf{x}$. \textbf{2. Scoring Head ($h_r$)}: Assigns scalar rewards based on the generated critiques, enabling more fine-grained evaluation.

\textbf{Learning to provide critique from enhanced annotation.} The critique head ($h_l$) is trained to align with human-provided annotations. The loss function for critique generation is:
\begin{equation}
\ell_{\text{Critique}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y, c} 
\Big[ 
    - \sum_{t=1}^{|c|} \log \pi_\theta(c_t | c_{<t}, \mathbf{x}, y)
\Big],
\end{equation}
where $c_t$ is the $t$-th token in the critique $c$, $c_{<t}$ denotes the tokens preceding $c_t$, and $\pi_\theta(c_t | c_{<t}, \mathbf{x}, y)$ is the probability of token $c_t$ given its context, query $\mathbf{x}$, and model response $y$.

However, as shown in Figure~\ref{fig:rm_teaser}, while human-provided scoring reasons are highly accurate, they tend to be concise. Directly using these concise annotations as training targets for the reward model's language head does not yield significant performance improvements. To address this issue, we use GPT-4o to augment the human annotations by adding more detail and improving the fluency of the critiques. These enhanced scoring reasons are then used as the training targets for the language head. To prevent GPT-4o from introducing hallucinated content or irrelevant analysis, we impose strict constraints in the prompt (Table~\ref{tab:prompt_example}), to ensure the model only expands on the original content without introducing speculative or uncertain information.


\textbf{Scoring loss with teacher-forcing}. $h_r$ computes scalar rewards based on the query $\mathbf{x}$, response $y$, and critique $c$. During training, we adopt a teacher-forcing strategy, where the scoring head uses ground truth critiques instead of critiques generated by itself. This avoids potential noise from model-generated critiques in the early stages of training. The scoring loss is defined as:
\begin{equation}
\ell_{\text{Score}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y_w, y_l} 
\Big[ 
    - \log \sigma \Big( 
        r(\mathbf{x}, y_w, c_w) - r(\mathbf{x}, y_l, c_l) 
    \Big)
\Big],
\end{equation}
where: $c_w$ and $c_l$ are the ground truth critiques for the preferred response $y_w$ and less preferred response $y_l$, respectively, $r(\mathbf{x}, y, c)$ is the reward score computed from $\mathbf{x}$, $y$, and $c$.

\textbf{Joint training objective}. The overall training objective combines the critique generation loss and the scoring loss:
$
\ell_{\text{Total}}(\theta) = \ell_{\text{Critique}}(\theta) + \ell_{\text{Score}}(\theta).
$

\textbf{Inference}. During inference, the critique head ($h_l$) generates a critique $c$ conditioned on the query $\mathbf{x}$ and response $y$. The scoring head ($h_r$) then uses $\mathbf{x}$, $y$, and the generated critique $c$ to compute the final reward score $r(\mathbf{x}, y, c)$. This two-step process mirrors the human evaluation process by explicitly reasoning about critiques before scoring.


\textbf{\abbr-RewardBench}.
To evaluate the effectiveness of the signals provided by our reward model in guiding subsequent model training, we randomly sample 10 examples from each category of the \abbr dataset to create a test set. Each example includes multiple model responses and their corresponding rankings, enabling the generation of several comparison pairs. This results in a total of 170 pairs for evaluation. We design two evaluation metrics: 
1. \textit{Traditional Accuracy (ACC)}: Measures the proportion of cases where the model correctly identifies the preferred response.
2. \textit{ACC+}: Measures the proportion of cases where the model correctly ranks all response pairs for a given sample. This metric emphasizes the model's ability to handle challenging cases, such as those with small ranking differences or hard-to-distinguish pairs.
\subsection{Discussion}

In the MLLM community, there is currently no unified paradigm for the design of reward models. Some approaches rely on traditional reward models~\cite{sun2023aligning}, which lack interpretability due to their reliance on scalar outputs. Others directly use LLMs to generate rankings~\cite{xiong2024llava}, which heavily depend on instruction-following capabilities and often exhibit high variance in scoring. In the broader LLM community, works such as \cite{yu2024self} explore reward models that first generate critiques. However, their focus is primarily on improving the reliability of model-generated critiques, such as increasing scoring confidence through multiple samplingâ€”a goal distinct from ours. To the best of our knowledge, this is the first study to explore how MLLMs can effectively leverage human annotations to enhance both interpretability and the final model's scoring ability.
