\section{\abbr-Dataset}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/data/data_teaser.pdf}
    \caption{\textbf{\abbr Construction Pipeline}. \textit{(1) Data Collection and Cleaning}: Starting with 10 million instruction samples, we cluster data based on image similarity, and uniformly sample across diverse categories. This results in a diverse dataset covering image-based Q\&A (e.g., multiple-choice, dialogues, and safety-related questions) and video Q\&A formats. \textit{(2) Response Generation}: We leverage state-of-the-art models, including GPT-4o and Qwen2-VL-72B, to generate high-quality responses. \textit{(3) Human Annotation}: We conduct manual annotation across nine categories, including scoring, ranking, and explanations, ensuring fine-grained evaluation.}
    \label{fig:data_construction}
\end{figure*}

In this section, we outline the construction of \abbr, as illustrated in Figure~\ref{fig:data_construction}. This includes the data collection process, data filtering methods, and human annotation procedures.

\subsection{Data Collection}

Our goal is to construct a comprehensive post-training dataset that covers a wide range of task types. To achieve this, we categorize tasks into three main domains: image understanding, video understanding, and multimodal safety.

For \textbf{image understanding}, we integrate data from multiple sources, including LLaVA-OV\footnote{\url{https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data}}, VLfeedback\cite{li2023silkie}, LLaVA-RLHF~\cite{sun2023aligning}, lrv-instruction~\cite{liu2023mitigating}, and Unimm-Chat\footnote{\url{https://huggingface.co/datasets/Yirany/UniMM-Chat}}. Since some datasets contain multi-turn dialogues, which are less suitable for response generation, we decompose them into single-turn dialogues. This process yields over 10 million dialogue samples, covering tasks such as conversation, safety, multiple-choice questions, captions, and commonsense reasoning.

For \textbf{video understanding}, the primary data source is SharedGPT-4 video~\cite{chen2024sharegpt4video}.

For \textbf{safety}, data is primarily derived from VLGuard~\cite{zong2024safety} and self-constructed content. VLGuard contains over 2,000 harmful samples, while additional red teaming, safety, and robustness data are included. The pipeline for constructing safety data is detailed in the Appendix~\ref{sec:safety_data}.


\subsection{Data Filtering and Model Response Generation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/data/cluster.pdf}
    \caption{\textbf{Re-Sample results from the clustering process.} Due to the large total number of samples, the clustered and deduplicated results contain a rich diversity of categories. Selected samples include topics such as mathematics, daily life, natural scenes, medicine, electronic technology, and OCR scenarios, showcasing a variety of problem-image pairs. The 2D features were obtained via UMAP dimensionality reduction.}
    \label{fig:data_cluster}
\end{figure}
The core goal of data filtering is to reduce the number of samples while maintaining the diversity of the original dataset. To achieve this, the following strategies are adopted:

\textbf{Predefined sampling weights}.
For image understanding tasks, we define three categories based on the nature of the questions and the length of model responses:
1. \textit{Multiple-choice questions (MCQ)}; \textit{(Questions with options such as A, B, C, or D.)} These tasks include visual question answering, mathematics, OCR, and icon recognition, focusing on the model's reasoning and visual perception abilities.
2. \textit{Long-text questions}; \textit{(Questions for which GPT-4o generates responses exceeding 128 characters.)} These typically involve detailed captions or complex descriptions, testing the model's conversational and descriptive capabilities.
3. \textit{Short-text questions}; \textit{(Questions for which GPT-4o generates responses shorter than 128 characters.)} These require concise answers, often involving simple image analysis, and represent a broader range of task types.

The initial distribution of these three types in the image understanding dataset is highly imbalanced, with proportions of 12.17\% (Long), 83.68\% (Short), and 4.14\% (MCQ). To align with diversity goals, we adjust the sampling ratio to 4:5:1, reducing disparities among task types while maintaining a dominance of comprehensive samples\footnote{For video understanding and safety tasks, MCQ samples are fewer. After classifying into Long and Short types, the differences are minimal, so no additional adjustments are made.}. 

\textbf{Cluster-based Sampling}.
Text deduplication is not performed because many questions, while similar in text, are paired with different images, leading to substantially different outcomes—an intrinsic characteristic of multimodal data. Instead, we encode all images using CLIP\footnote{\url{https://huggingface.co/openai/clip-vit-base-patch32}}, and for videos, we use the feature of the first frame as a representative. We then apply KNN clustering with 100 cluster centers and randomly sample \textit{N} instances from each cluster. The value of \textit{N} is determined to satisfy the predefined sampling ratios, ensuring a balanced representation of task diversity.

\textbf{Data statistics}.
The composition of the dataset is summarized in Table~\ref{tab:data_statistics}, and a visualization of the clustering results is shown in Figure~\ref{fig:data_cluster}, demonstrating the rich diversity of data categories.

\textbf{Model response generation}.
To generate high-quality responses, we select state-of-the-art models from both open-source and closed-source domains. For image understanding and safety-related tasks, we use Qwen2-VL-72B~\cite{wang2024qwen2}, LLaVA-OV-72B~\cite{li2024llava}, GPT-4o\footnote{\url{https://openai.com/index/hello-gpt-4o/}}, and Claude 3.5-sonnet\footnote{\url{https://www.anthropic.com/news/claude-3-5-sonnet}}. For video understanding tasks, we employ GPT-4o, LLaVA-Video-72B~\cite{zhang2024video}, and Qwen2-VL-72B~\cite{wang2024qwen2}. These models are chosen for their advanced capabilities and performance, ensuring a comprehensive evaluation of leading solutions in multimodal understanding.

\begin{table}[]
\caption{Dataset Composition Statistics}
\label{tab:data_statistics}
\centering
\begin{tabular}{cccccc}
\toprule
\multicolumn{3}{c}{\textbf{Image}} & \multirow{2}{*}{\textbf{Safety}} & \multirow{2}{*}{\textbf{Video}} & \multirow{2}{*}{\textbf{Total}} \\ \cmidrule{1-3}
\textbf{Long} & \textbf{Short} & \textbf{MCQ} &  &  &  \\ \hline
9,575 & 12,063 & 2,125 & 1,999 & 4,235 & 29,997 \\ \bottomrule
\end{tabular}%
\end{table}

\subsection{Annotation}
The annotation process follows rigorous standards to ensure comprehensive and fine-grained evaluations of MLLM responses. Detailed standards are provided in Appendix~\ref{sec:annotation_standard}, and the scoring and annotation structure are illustrated in Figure \ref{fig:data_construction}. Additionally, we design a web UI to streamline the annotation process, as shown in Figure \ref{fig:web_ui}.

\subsubsection{Annotation Standards}
Compared to prior work, our annotation approach introduces two significant advantages: \textbf{richness} and \textbf{granularity}. First, the evaluation incorporates three core dimensions—\textit{Helpfulness}, \textit{Faithfulness}, and \textit{Ethical Considerations}—to comprehensively capture model performance. \textit{Helpfulness} ensures that responses are relevant and provide meaningful assistance aligned with the user’s intent. \textit{Faithfulness} evaluates the accuracy of responses in describing visual elements, such as objects, relationships, and attributes, ensuring alignment with the ground truth while avoiding hallucinated content. \textit{Ethical Considerations} assess adherence to ethical principles, including safety, privacy, fairness, and harm avoidance, ensuring responses are free from harmful or biased content. Annotators score each dimension while documenting the reasoning behind their assessments, adding valuable context for understanding model performance. 

Second, annotators are required to assign an \textbf{overall ranking} to the responses, along with justifications for their rankings. This ranking mechanism provides a transparent and nuanced comparison of model outputs. Additionally, innovative strategies are employed to enhance data quality:

- \textbf{Constructing positive samples for poor quality ties}. When multiple responses are equally poor, annotators provide correct answers to create positive examples. This ensures that challenging samples contribute to the training dataset, addressing issues where no valid model responses exist.


- \textbf{Constructing negative samples for high-quality ties}. When multiple responses are of equally high quality, annotators introduce deliberate errors to create negative samples. This prevents ties from reducing the utility of the data and allows for more efficient use in training.

By combining fine-grained scoring criteria, textual annotations, and innovative strategies, our annotation framework produces a high-quality dataset that comprehensively captures model performance and supports effective downstream applications.




\subsubsection{Human Annotation vs. Machine Annotation}

\textbf{Annotation workers and costs}. The annotation process employs over 50 annotators, supported by 8 multimodal research experts with strong English proficiency and academic backgrounds. The entire task completes within two months, with periodic quality checks and interactive reviews conducted by experts to ensure the reliability and accuracy of the annotations. Low-quality samples undergo re-annotation during the process. Due to the fine-grained nature of the annotation standards, the task involves significant challenges. For example, annotating a single question in the long split of image perception tasks requires an average of over $8$ minutes.


\textbf{Why human annotation}? Many existing MLLM alignment datasets rely on annotations generated by external models due to their cost-effectiveness and scalability. However, MLLM alignment tasks demand fine-grained perceptual capabilities and sensitivity to subtle differences, which current models lack. In many cases, the differences between responses are nuanced, requiring an in-depth understanding that models struggle to achieve. As demonstrated in our experiments, even state-of-the-art models like GPT-4o significantly underperform human experts in tasks involving response comparison. Moreover, these models cannot provide professional-grade scoring or well-reasoned explanations for rankings. These limitations highlight the necessity of human annotation, which ensures the precision, reasoning, and insight required for constructing high-quality alignment datasets. Appendix~\ref{sec:why_we_need_human} further discusses the advantages of human annotation, particularly in handling ambiguous or incomplete questions and closely matched responses requiring subtle differentiation. Human annotators excel at identifying fine-grained errors, inconsistencies, and context-specific nuances that models overlook. By relying on human feedback, our approach ensures the dataset achieves the quality and reliability necessary for advancing MLLM alignment efforts.

We acknowledge that the cost of human annotation poses scalability challenges. However, as demonstrated in later sections, our high-quality alignment dataset enables the training of a powerful reward model. In the future, by combining this reward model with human annotators in a collaborative framework, we can significantly reduce annotation costs and scale up the dataset efficiently. This hybrid approach not only maintains the precision of human annotation but also enhances scalability, making it a practical solution for large-scale MLLM alignment.


