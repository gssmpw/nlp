\section{\dpo}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/dpo.pdf}
    \caption{\textbf{Overview of the \dpo framework,} The dynamic reward scaling mechanism adjusts the update strength based on the reward margin, improving optimization stability and robustness.}
    \label{fig:dpo_alg}
\end{figure*}

In this section, we propose \dpo, an extension of the traditional DPO framework. \dpo introduces Dynamic Reward Scaling, which dynamically adjusts the update strength based on the confidence of training pairs, ensuring effective utilization of high-quality samples while mitigating the impact of noisy or low-confidence data.

\subsection{Background: Direct Preference Optimization}
The DPO framework is a preference-based learning method that optimizes model parameters $\theta$ by aligning model outputs with human preferences. Given a query $\mathbf{x}$ and corresponding responses $y_w$ (positive) and $y_l$ (negative), the DPO loss is defined as:
\begin{equation}
\ell_{\text{DPO}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y_w, y_l} 
\Big[ 
    - \log \sigma \Big( 
        \beta \Big( 
            \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
            - 
            \log \frac{\pi_\theta(y_l | \mathbf{x})}{\pi_{\text{ref}}(y_l | \mathbf{x})} 
        \Big)
    \Big)
\Big],
\end{equation}
where $\pi_\theta$ is the model's predicted probability distribution, $\pi_{\text{ref}}$ is a reference policy, $\beta$ is a scaling factor, and $\sigma(\cdot)$ is the sigmoid function. Traditional DPO treats all training pairs equally, regardless of their quality differences. This uniform scaling fails to prioritize high-quality pairs with clear preference distinctions, leading to inefficient use of informative samples and suboptimal optimization.

\subsection{\dpo: Key Contributions and Improvements}
\paragraph{Training on all possible comparison pairs instead of the hardest pairs}.  
Unlike many recent MLLM alignment approaches that prioritize training on the hardest comparison pairs, \dpo incorporates all possible comparison pairs for a single query into the training process. Specifically, for any query with multiple responses, every response pair with differing ranks is treated as a valid comparison pair. This comprehensive approach captures more nuanced ranking information, allowing the model to learn from a broader set of preferences. However, this strategy also introduces a challenge: pairs involving responses with similar ranks (e.g., rank 3 and rank 4) often have lower reward margins compared to pairs with more distinct rankings (e.g., rank 1 and rank 4). Treating all pairs equally, as in traditional DPO, exacerbates the issue of uniform scaling and underutilizes the high-confidence information contained in larger reward margins. To address this, \dpo introduces Dynamic Reward Scaling, which dynamically adjusts the update strength based on the reward margin to prioritize high-confidence training pairs.

\begin{wrapfigure}{r}{0.34\linewidth}
\vspace{-0.7cm}
  \begin{center}
    \includegraphics[width=\linewidth]{imgs/beta_curve.pdf}
\vspace{-0.4cm}
\caption{Effect of $k$ on $1 - e^{-k \delta}$.}
\label{fig:beta_func}
\end{center}
\vspace{-0.4cm}
\end{wrapfigure}
\paragraph{Definition of dynamic reward scaling}. Reward models can naturally provide a pairwise reward margin, which serves as a straightforward signal for scaling. However, two critical aspects must be addressed: (1) ensuring the signal quality is sufficiently high, and (2) bounding the signal to prevent overly aggressive updates that might destabilize training.


Regarding the first aspect, our experiments reveal that publicly available models, such as GPT-4o and LLaVA-Critic, perform inadequately in scoring our dataset. Conversely, our \abbr-Reward-7B model surpasses several publicly available 72B models, offering a reliable and robust reward signal. We use this model to compute the reward margin: 
 $\delta = r(y_w) - r(y_l),$
where $r(y_w)$ and $r(y_l)$ are the scores assigned to the positive and negative samples.

For the second factor, we control the scaling factor $\beta(\delta)$ using the following formulation:
\[
\beta(\delta) = \beta_{\text{ori}} \Big( 1 + w \big( 1 - e^{-k \delta} \big) \Big),
\]

where $\beta_{\text{ori}}$ is the initial default scaling factor, $w$ is a parameter balancing the dynamic component's contribution, and $k$ is a tunable hyperparameter that adjusts $\beta(\delta)$'s sensitivity to changes in $\delta$. The function $1 - e^{-k \delta}$ is bounded between $[0, 1]$, {as illustrated in Figure~\ref{fig:beta_func}}. A smaller $k$ value keeps most $\beta(\delta)$ values near $\beta_{\text{ori}}$, with slow growth as $\delta$ increases. In contrast, a larger $k$ makes $\beta(\delta)$ highly responsive to changes in $\delta$, quickly reaching its maximum. To avoid overly aggressive updates, we constrain $\beta(\delta)$ within $[\beta_{\text{ori}}, (1 + w) \beta_{\text{ori}}]$. Overall, Dynamic Reward Scaling significantly enhances \dpo by leveraging high-quality reward signals and tailoring optimization steps to the confidence level of training pairs. This results in improved robustness, efficiency, and overall effectiveness of the framework. We discuss the similarities and differing perspectives between our approach and existing methods in Appendix~\ref{sec:app_com_beta}.
