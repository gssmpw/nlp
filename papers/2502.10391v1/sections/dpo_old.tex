
\section{Vision-DPO}
% \begin{equation}
% \ell_{\text{Vision-DPO}}(\theta) = 
% \mathbb{E}_{\mathbf{x}, y_w, y_l, y_v} 
% \Bigg[
%     - \underbrace{\log \sigma\Big( \beta(\delta) \Big[ 
%         \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
%         - 
%         \log \frac{\pi_\theta(y_l | \mathbf{x})}{\pi_{\text{ref}}(y_l | \mathbf{x})}
%     \Big]\Big)}_{\beta(\delta) \textit{ is a function of reward margin } \delta}
%     - \underbrace{\lambda \cdot \log \sigma\Big( 
%         \beta\Big[
%         \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
%         - 
%         \log \frac{\pi_\theta(y_l | \mathbf{x}_v)}{\pi_{\text{ref}}(y_l | \mathbf{x}_v)}
%         \Big]\Big)}_{\substack{\textit{Loss for vision negative samples }\mathbf{x}_v}}
% \Bigg],
% \end{equation}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/dpo.pdf}
    \caption{\textbf{Overview of the Vision-DPO framework,} highlighting the key contributions: Dynamic Reward Scaling and Vision Negative Samples. The diagram illustrates the core components and workflow of Vision-DPO, with different colored lines indicating the unique aspects of our algorithm compared to the traditional DPO framework. The dynamic reward scaling mechanism adjusts the update strength based on the reward margin, while the introduction of vision negative samples enhances the model's sensitivity to visual inputs by perturbing query images. These innovations improve optimization stability and robustness in tasks involving image-based inputs.}
    \label{fig:dpo_alg}
\end{figure*}
In this section, we propose Vision-DPO, an extension to the Direct Preference Optimization (DPO) framework. We begin by briefly introducing the DPO background and then highlight the key modifications in Vision-DPO. Specifically, Vision-DPO leverages high-quality reward model signals to dynamically control the update strength based on reward margin and introduces vision negative samples, which perturb query images to improve model sensitivity to visual inputs. 
\subsection{Background: Direct Preference Optimization}
The DPO framework is a preference-based learning method that optimizes model parameters $\theta$ by aligning model outputs with human preferences. Given a query $\mathbf{x}$ and corresponding responses $y_w$ (positive) and $y_l$ (negative), the DPO loss is defined as:
\begin{equation}
\ell_{\text{DPO}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y_w, y_l} 
\Big[ 
    - \log \sigma \Big( 
        \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
        - 
        \log \frac{\pi_\theta(y_l | \mathbf{x})}{\pi_{\text{ref}}(y_l | \mathbf{x})}
    \Big)
\Big],
\end{equation}
where $\pi_\theta$ is the model's predicted probability distribution and $\pi_{\text{ref}}$ is a reference policy. The term $\sigma(\cdot)$ is the sigmoid function, which ensures the loss is bounded. DPO relies on the reward margin between positive and negative samples to guide optimization, but it applies uniform scaling to all samples, regardless of confidence or quality.

\subsection{Vision-DPO: Key Contributions and Improvements}

\begin{align}
\ell_{\text{Vision-DPO}}(\theta) = 
\mathbb{E}_{\mathbf{x}, y_w, y_l, y_v} 
\Bigg[ 
    & - \log \sigma \Big( 
        \underbrace{\beta(\delta) \Big[ 
            \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
            - 
            \log \frac{\pi_\theta(y_l | \mathbf{x})}{\pi_{\text{ref}}(y_l | \mathbf{x})}
        \Big]}_{{\beta(\delta) \textit{ is a function of reward margin } \delta}}
    \Big) \nonumber \\ 
    & - \lambda \cdot \log \sigma \Big( 
        \underbrace{\beta \Big[
            \log \frac{\pi_\theta(y_w | \mathbf{x})}{\pi_{\text{ref}}(y_w | \mathbf{x})} 
            - 
            \log \frac{\pi_\theta(y_l | \mathbf{x}_v)}{\pi_{\text{ref}}(y_l | \mathbf{x}_v)}
        \Big]}_{\substack{\textit{Loss for vision negative samples }\mathbf{x}_v}}
    \Big)
\Bigg].
\end{align}
The Vision-DPO framework builds on the traditional DPO paradigm, addressing its two primary limitations: (1) insufficient sensitivity to varying reward signal quality, and (2) a lack of robustness to visual perturbations in tasks that involve image-based inputs. These improvements are realized through two key contributions: \textbf{Dynamic Reward Scaling} and \textbf{Vision Negative Samples}.


\textbf{Dynamic Reward Scaling}: Vision-DPO introduces a reward margin-based scaling factor $\beta(\delta)$ to dynamically adjust the update strength for each training sample. The reward margin $\delta$ is defined as:
$
\delta = r(y_w) - r(y_l),
$
where $r(y_w)$ and $r(y_l)$ represent the reward scores assigned by the reward model to the positive and negative samples, respectively. The scaling factor $\beta(\delta)$ is then computed as:
$
\beta(\delta) = 1 - e^{-k \delta},
$
where $k$ is a tunable hyperparameter that controls the sensitivity of the scaling effect. This mechanism prioritizes high-confidence samples (large $\delta$), which contribute more significantly to optimization, while down-weighting low-confidence samples (small $\delta$) to reduce the impact of noise and unreliable data. By dynamically scaling the gradient contributions based on $\delta$, Vision-DPO ensures that learning is focused on the most reliable training pairs, improving both optimization stability and efficiency, particularly in scenarios with varying reward signal quality.

% $$
% \beta(\delta) = 1 - e^{-k \delta}= 1 - e^{-(r(y_w) - r(y_l))}
% $$
\textbf{Vision Negative Samples}: Vision-DPO introduces vision-specific negative samples by perturbing the input image $\mathbf{x}$ to create $\mathbf{x}_v$. For example, $\mathbf{x}_v$ can be generated using mix-up or other image augmentation techniques, effectively injecting visual variability into the training process.

The introduction of vision-specific negative samples offers two key benefits:
\begin{itemize}
    \item \textbf{Mitigating Hallucinations:}  
    By combining perturbed images $\mathbf{x}_v$ with the original negative response $y_l$, Vision-DPO constructs additional DPO pairs that explicitly highlight inconsistencies between the model's responses and the altered visual input. This encourages the model to align more robustly with the original input image,  reducing hallucinations and improving overall robustness in its generated responses.
    \item \textbf{Enhancing the Reliability of Negative Samples:}  
    Unlike generating new responses for $\mathbf{x}_v$, Vision-DPO directly uses the original negative response $y_l$ in combination with the perturbed image $\mathbf{x}_v$. This ensures that the vision negative samples remain high-quality in terms of their negative characteristics, as $y_l$ is already known to be less preferred compared to $y_w$. The use of reliable negative samples ensures the robustness of the DPO pairs.
\end{itemize}

However, this strategy may have diminishing returns for cases where the difference in quality between positive and negative samples is already significant. In such cases, the additional vision-specific pairs may be too easy for the model to distinguish, thereby contributing less to the training process. To mitigate this issue, the Vision Negative Sample loss is incorporated as a secondary term in the Vision-DPO loss, with its contribution controlled by a weighting factor $\lambda$. This ensures that its impact on the training gradients can be appropriately balanced relative to the original DPO loss.

\subsection{Discussion and Comparison to Existing Work}
