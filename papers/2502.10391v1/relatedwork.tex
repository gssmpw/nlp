\section{Related Work}
\textbf{Multimodal Benchmark.} 
With the development of MLLMs, a number of benchmarks have been built.
For instance, MME~\citep{fu2023mme} constructs a comprehensive evaluation benchmark that includes a total of 14 perception and cognition tasks. All QA pairs in MME are manually designed to avoid data leakage, and the binary choice format makes it easy to quantify.
MMBench~\citep{liu2023mmbench} contains over $3,000$ multiple-choice questions covering $20$ different ability dimensions, such as object localization and social reasoning. 
It introduces GPT-4-based choice matching to address the MLLM's lack of instruction-following capability and a novel circular evaluation strategy to improve the evaluation robustness.
Seed-Bench~\citep{li2023seed} is similar to MME and MMBench but consists of $19,000$ multiple-choice questions. The larger sample size allows it to cover more ability aspects and achieve more robust results.
SEED-Bench-2~\citep{li2023seed2} expands the dataset size to $24,371$ QA pairs, encompassing $27$ evaluation dimensions and further supporting the evaluation of image generation.
MMT-Bench~\citep{mmtbench} scales up the dataset even further, including $31,325$ QA pairs from various scenarios such as autonomous driving and embodied AI. It encompasses evaluations of model capabilities such as visual recognition, localization, reasoning, and planning.
Additionally, other benchmarks focus on real-world usage scenarios~\citep{fu2024blink,lu2024wildvision,bitton2023visit} and reasoning capabilities~\citep{yu2024mm,bai2023touchstone,han2023coremm}. 
However, there are widespread issues, such as data scale, annotation quality, and task difficulty, in these benchmarks, making it hard to assess the challenges that MLLMs face in the real world.

\textbf{MLLMs.} 
This field has undergone significant evolution~\citep{yin2023survey, fu2023challenger}, initially rooted in BERT-based language decoders and later incorporating advancements in LLMs. 
MLLMs exhibit enhanced capabilities and performance, particularly through end-to-end training techniques, by leveraging advanced LLMs such as GPTs~\citep{gpt4,brown2020language},
LLaMA~\citep{touvron2023llama,touvron2023llama2}, 
Alpaca~\citep{taori2023stanford}, PaLM~\citep{chowdhery2023palm,anil2023palm}, BLOOM~\citep{muennighoff2022crosslingual}, 
Mistral~\citep{jiang2023mistral}, and Vicuna~\citep{chiang2023vicuna}. Recent model developments, including Flamingo~\citep{awadalla2023openflamingo}, PaLI~\citep{laurenccon2024obelics}, PaLM-E~\citep{driess2023palm}, BLIP-2~\citep{li2023blip}, InstructBLIP~\citep{dai2024instructblip}, Otter~\citep{li2023otter}, MiniGPT-4~\citep{zhu2023minigpt}, mPLUG-Owl~\citep{ye2023mplug}, LLaVA~\citep{liu2023visual}, Qwen-VL~\citep{bai2023qwen}, and VITA~\citep{fu2024vita}, bring unique perspectives to challenges such as scaling pre-training, enhancing instruction-following capabilities, and overcoming alignment issues. 
However, the performance of these models in the face of real scenarios has often not been revealed.

\textbf{High-resolution MLLMs.} 
Empirical studies have shown that employing higher resolution is an effective solution for many tasks~\citep{bai2023qwen,liu2023improved,li2023monkey,mckinzie2024mm1}. Approaches like LLaVA-Next~\citep{liu2024llavanext} segment high-resolution images into multiple patches, encoding each one independently before concatenating all local patch tokens with the original global image tokens, albeit at an escalated computational cost. Other models, such as Monkey~\citep{li2023monkey} and LLaVA-UHD~\citep{xu2024llava-uhd}, also split images into patches but subsequently compress them to avoid redundant tokens. Mini-Genimi~\citep{li2024mini} comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding. They work in an attention mechanism, where the low-resolution encoder generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference. Conv-LlaVA~\citep{ge2024convllava} employs ConvNeXt instead of ViT as the vision encoder. Cambrian~\citep{tong2024cambrian} uses a set of learnable latent queries that interact with multiple vision features via cross-attention layers. Additionally, SliME~\citep{zhang2024beyond} stresses the importance of global features, compressing the local image patches twice but preserving all the global context. Although many of these models focus on improving resolution, none have been tested in a rigorous high-resolution benchmark, often providing only intuitive examples that lack informativeness and convincing results. Our proposed benchmark offers a rigorous evaluation to test the capabilities in understanding high-resolution images.

\begin{figure*}[t]
\centering
\subfigure[\textbf{Real-World Tasks}]{
\begin{minipage}[t]{0.62\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/samples/tasks.pdf}
\end{minipage}%
\label{label:teaser_task}
}%
\subfigure[\textbf{Leaderboard}]{
\begin{minipage}[t]{0.35\linewidth}
 \centering
 \includegraphics[width=\linewidth]{imgs/samples/teaser_performance.pdf}
\end{minipage}%
\label{label:teaser_acc}

}%
\centering
\vspace{-0.2cm}
\caption{\textbf{Task Categories} (left). Our benchmark spans $5$ key domains and $43$ subtasks highly related to real-world scenarios, including $13,366$ high-resolution images and $29,429$ annotations. \textbf{Model Performance} (right). Average accuracies of advanced MLLMs are shown across both the English and Chinese splits of the dataset.}
\label{fig:teaser}
\end{figure*}