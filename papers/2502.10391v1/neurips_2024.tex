
\documentclass{article} % For LaTeX2e
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2024}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{imgs/math_commands}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tcolorbox}
\usepackage{xspace}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks,
            linkcolor=VioletRed,      
            anchorcolor=VioletRed, 
            citecolor=VioletRed,       
            ]{hyperref}

\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{cleveref}
\usepackage{xcolor}  % Ensure you include the xcolor package
\usepackage{transparent}
\usepackage{tikz}
\newcommand{\abbr}[0]{MM-RLHF\xspace} 
\newcommand{\dpo}[0]{MM-DPO\xspace} 
\definecolor{Gray}{gray}{0.85}
\newcommand{\Gray}[0]{\rowcolor{gray!20}}
\newcommand{\Lgray}[0]{\rowcolor{gray!10}}
\newcommand{\lightgrey}[1]{\texttt{\color{black!50} #1}}

\title{\large \abbr: The Next Step Forward in Multimodal LLM Alignment}

\author{
\vspace{-0.4cm}
    \\ 
    Yi-Fan Zhang$^{2, \spadesuit}$, Tao Yu$^{2}$, Haochen Tian$^{2}$, Chaoyou Fu$^{3,\dagger}$ \\
    Peiyan Li$^{2}$, Jianshu Zeng$^{5}$, Wulin Xie$^{2}$, Yang Shi$^{5}$, Huanyu Zhang$^{2}$, Junkang Wu$^{4}$ \\
    Xue Wang$^{6}$, Yibo Hu$^{2}$, Bin Wen$^{1,\dagger}$, Fan Yang$^{1}$, Zhang Zhang$^{2,\dagger}$, Tingting Gao$^{1}$ \\
    Di Zhang$^{1}$, Liang Wang$^{2}$, Rong Jin$^{7}$, Tieniu Tan$^{2,3}$ \\
    $^{1}$KuaiShou, $^{2}$CASIA, $^{3}$NJU, 
    $^{4}$USTC,
    $^{5}$PKU,
    $^{6}$Alibaba,
    $^{7}$Meta AI
    \\
    \footnotesize{
    $^{\spadesuit}$~Project Leader \;
    $^{\dagger}$~Corresponding Author \;}
    \\ \\
    {\centering}
    \url{https://mm-rlhf.github.io/}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\yf}[1]{\textbf{\color{red}{\bf\sf [Yifan: #1]}}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle
\begin{abstract}
Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce \abbr, a dataset containing \textbf{120k} fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a \textbf{Critique-Based Reward Model}, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose \textbf{Dynamic Reward Scaling}, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across \textbf{10} distinct dimensions and \textbf{27} benchmarks, with results demonstrating significant and consistent improvements in  performance. Specifically, fine-tuning LLaVA-ov-7B with \abbr and our alignment algorithm leads to a \textbf{19.5\%} increase in conversational abilities and a \textbf{60\%} improvement in safety.



% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/teaser.pdf}
%     \caption{Performance gains achieved through alignment training on \abbr and our alignment algorithm (MM-DPO), highlighting significant improvements across various tasks and metrics.}
%     \label{fig:teaser}
% \end{figure}

\end{abstract}

\section{Introduction}

Although Multimodal Large Language Models (MLLMs) have demonstrated remarkable potential in addressing complex tasks that involve the integration of vision, language, and audio, state-of-the-art models today seldom undergo a rigorous alignment stage~\cite{wang2024qwen2,deitke2024molmo,chen2024far,dai2024nvlm,agrawal2024pixtral}. Typically, these models only progress to the Supervised Fine-tuning (SFT) stage, leaving critical aspects such as truthfulness, safety, and alignment with human preferences largely unaddressed. While recent efforts have begun to explore MLLM alignment, they often focus on specific domains, such as mitigating hallucination or enhancing conversational capabilities, which fail to comprehensively improve the model's overall performance and reliability. This raises a critical question:

\begin{tcolorbox}[top=1pt, bottom=1pt, left=1pt, right=1pt]
\begin{center}
\textit{Is alignment with human preferences only capable of enhancing MLLMs in a limited set of tasks?}
\end{center}
\end{tcolorbox}

In this work, we confidently answer this question with a resounding ``No.''. We demonstrate that a well-designed alignment pipeline can comprehensively enhance MLLMs along multiple dimensions, including visual perception, reasoning, dialogue, and trustworthiness, thereby significantly broadening their practical applicability. To achieve this, we conduct in-depth investigations into three pivotal areas: data curation, reward modeling, and alignment algorithms.

At first, we introduce \textbf{\abbr}, a dataset designed to advance \textbf{M}ultimodal \textbf{R}einforcement \textbf{L}earning from \textbf{H}uman \textbf{F}eedback (RLHF). The dataset spans three domains: image, video understanding, and MLLM safety. Constructed through a rigorous pipeline, \abbr ensures high-quality, fine-grained annotations. Dataset creation process involves the following steps (Figure~\ref{fig:data_construction}):

\begin{itemize}
    \item \textbf{Data Collection}. We curate a diverse set of multimodal tasks from various sources, totaling 10 million data instances, ensuring broad representation across tasks.
    \item \textbf{Data Selection}. Through rigorous re-sampling, we extract 30k representative queries, ensuring diversity across a wide range of data types, such as real-world scenarios, mathematical reasoning, chart understanding, and other practical domains (Figure~\ref{fig:data_cluster}).
    \item \textbf{Model Response Generation}. We utilize state-of-the-art models, such as Claude 3.5-Sonnet and Qwen2-VL-72B, to generate responses for various tasks.
    \item \textbf{Fine-grained Human Annotation}. We employ a meticulous annotation process, involving over 50 annotators over two months, to score, rank, and provide textual explanations for responses. This results in more than 120k high-quality ranked comparison pairs.
\end{itemize}
Compared to existing datasets, \abbr significantly advances in diversity, response quality, and annotation granularity, providing a robust foundation for MLLM alignment.

Building on the \abbr dataset, we investigate how human-annotated data can enhance MLLM alignment, with a focus on reward modeling and training optimization. Recognizing the pivotal role of reward models in providing feedback signals to guide the alignment process, we propose a \textbf{Critique-Based Reward Model} (Figure~\ref{fig:rm_teaser}). Traditional reward models, which output scalar values, often lack interpretability, while directly using MLLMs as reward models place high demands on their instruction-following capabilities, limiting their practicality. To address these limitations, we first transform concise human annotations into detailed, model-friendly formats using MLLMs. These enriched annotations serve as learning targets, guiding the reward model to first generate critiques and then assign scores based on the critiques. This approach enables the model to provide fine-grained scoring explanations, significantly enhancing the quality and interpretability of the reward signals. \textbf{\abbr-Reward-7B} achieves SOTA performance on several reward model benchmarks, outperforming several 72B-scale models.

Building on this high-quality reward model, we introduce \textbf{Dynamic Reward Scaling} within the Direct Preference Optimization (DPO) framework. Traditional DPO methods~\cite{amini2024direct} use a fixed training weight for all human-preferred and non-preferred training pairs. In contrast, Dynamic Reward Scaling calculates a reward margin for each comparison pair using {\abbr-Reward-7B}. During training, it assigns higher weights to comparison pairs with larger reward margins. This ensures that the most informative samples have a stronger influence on model updates. As a result, the training process becomes more efficient, leading to improved model performance.


Finally, to rigorously evaluate our approach, we construct two specialized benchmarks. The first, \textbf{\abbr-RewardBench}, is sampled from our dataset and consists of meticulously human-annotated data for evaluating reward models. The second, \textbf{\abbr-SafetyBench}, is curated and filtered from existing benchmarks and focuses on safety-related tasks, including privacy protection, adversarial attacks, jailbreaking, and harmful content detection.

We conduct extensive evaluations across ten key dimensions, covering 27 benchmarks. The results demonstrate that our training algorithm, combined with the high-quality \abbr dataset, leads to significant improvements in model performance. Specifically, models fine-tuned with our approach achieve an average 11\% gain in conversational abilities and a 57\% reduction in unsafe behavior. The integration of our reward model further amplifies these gains, highlighting the effectiveness of our alignment algorithm.

\input{sections/dataset}

\input{sections/reward_model}

\input{sections/dpo}

\input{sections/exp}


\section{Conclusion and Future Work}

In this work, we introduced \textbf{\abbr}, a high-quality, fine-grained dataset specifically designed to advance the alignment of MLLMs. Unlike prior works that focus on specific tasks, our dataset and alignment approach aim to holistically improve performance across diverse dimensions. Even with preliminary improvements to reward modeling and optimization algorithms, we observed significant and consistent gains across almost all evaluation benchmarks, underscoring the potential of comprehensive alignment strategies.

Looking ahead, we see great opportunities to further unlock the value of our dataset. Its rich annotation granularity, such as per-dimension scores and ranking rationales, remains underutilized in current alignment algorithms. Future work will focus on leveraging this granularity with advanced optimization techniques, integrating high-resolution data to address limitations in specific benchmarks, and scaling the dataset efficiently using semi-automated strategies. We believe these efforts will not only push MLLM alignment to new heights but also set a foundation for broader, more generalizable multimodal learning frameworks.


\bibliography{neurips_2024}
\bibliographystyle{plain}


\clearpage
\newpage
\appendix

\input{imgs/appendix}

\end{document}
