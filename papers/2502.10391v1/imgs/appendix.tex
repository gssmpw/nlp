\begin{center}
{\LARGE \textbf{\abbr\\ $\;$ \\ ————Appendix————}}
\end{center}

{
  \hypersetup{hidelinks}
  \tableofcontents
  %\parttoc
  \noindent\hrulefill
}

\section{Related Work}

\textbf{Multimodal large language models} have seen remarkable progress in recent years, with significant advancements in both performance and capabilities. Leveraging cutting-edge LLMs such as GPTs~\citep{gpt4,brown2020language}, LLaMA~\citep{touvron2023llama,touvron2023llama2}, Alpaca~\citep{taori2023stanford}, Vicuna~\citep{chiang2023vicuna}, and Mistral~\citep{jiang2023mistral}, MLLMs are increasingly demonstrating enhanced multimodal capabilities, especially through end-to-end training approaches. These advancements have been crucial in enabling models to handle a range of multimodal tasks, including image-text alignment, reasoning, and instruction following, while addressing challenges related to data fusion across different modalities. Recent open-source MLLMs such as Otter~\citep{li2023otter}, mPLUG-Owl~\citep{ye2023mplug}, LLaVA~\citep{liu2023visual}, Qwen-VL~\citep{bai2023qwen}, Cambrian-1~\citep{tong2024cambrian}, Mini-Gemini~\citep{li2024mini}, MiniCPM-V 2.5~\citep{hu2024minicpm}, DeepSeek-VL~\citep{lu2024deepseek},  SliME~\citep{zhang2024beyond} and VITA~\cite{fu2024vita,fu2025vita} have contributed to solving some of the most fundamental multimodal problems, such as improving vision-language alignment, reasoning, and following instructions. These models focus on enhancing multimodal understanding by integrating vision with language, allowing for more nuanced and context-aware interactions. Some of the most notable open-source models, such as InternLM-XComposer-2.5~\citep{zhang2023internlm} and InternVL-2~\citep{chen2023internvl}, have exhibited impressive progress in multimodal understanding, closely competing with proprietary models across a range of multimodal benchmarks. However, despite these achievements, there is still a noticeable gap in security and alignment when compared to closed-source models. As highlighted by recent studies~\citep{zhang2024mme}, most open-source MLLMs have not undergone rigorous, professional alignment processes, which has hindered their ability to effectively align with human preferences. This gap in alignment remains one of the key challenges for open-source models, and improving model safety and alignment to human values will be a crucial area of future research.

\textbf{MLLM Alignment.} With the rapid development of MLLMs, various alignment algorithms have emerged, showcasing different application scenarios and optimization goals. For instance, in the image domain, Fact-RLHF~\citep{sun2023aligning} is the first multimodal RLHF algorithm, and more recently, LLAVA-CRITIC~\citep{xiong2024llava} has demonstrated strong potential with an iterative DPO strategy. These algorithms have shown significant impact on reducing hallucinations and improving conversational capabilities~\citep{zhang2024debiasing, yu2024rlaif}, but they have not led to notable improvements in general capabilities. There have also been some preliminary explorations in the multi-image and video domains, such as MIA-DPO and PPLLaVA. However, alignment in image and video domains is still fragmented, with little research done under a unified framework. We believe that the main limitation hindering the development of current alignment algorithms is the lack of a high-quality, multimodal alignment dataset. Few existing manually annotated MLLM alignment datasets are available, and most contain fewer than 10K samples~\citep{sun2023aligning, yu2024rlaif, yu2024rlhf}, which is significantly smaller than large-scale alignment datasets in the LLM field. This small dataset size makes it difficult to cover multiple modalities and diverse task types. Furthermore, machine-annotated data faces challenges related to quality assurance. Therefore, in this paper, we have invested considerable effort into constructing a dataset, \abbr, which surpasses existing works in both scale and annotation quality.


\textbf{MLLM Evaluation.} With the development of MLLMs, a number of benchmarks have been built~\cite{duan2024vlmevalkit,fu2024mme}.
For instance, MME~\citep{fu2023mme} constructs a comprehensive evaluation benchmark that includes a total of 14 perception and cognition tasks. All QA pairs in MME are manually designed to avoid data leakage, and the binary choice format makes it easy to quantify.
MMBench~\citep{liu2023mmbench} contains over $3,000$ multiple-choice questions covering $20$ different ability dimensions, such as object localization and social reasoning. 
It introduces GPT-4-based choice matching to address the MLLM's lack of instruction-following capability and a novel circular evaluation strategy to improve the evaluation robustness.
Seed-Bench~\citep{li2023seed} is similar to MME and MMBench but consists of $19,000$ multiple-choice questions. The larger sample size allows it to cover more ability aspects and achieve more robust results.
SEED-Bench-2~\citep{li2023seed2} expands the dataset size to $24,371$ QA pairs, encompassing $27$ evaluation dimensions and further supporting the evaluation of image generation.
MMT-Bench~\citep{mmtbench} scales up the dataset even further, including $31,325$ QA pairs from various scenarios such as autonomous driving and embodied AI. It encompasses evaluations of model capabilities such as visual recognition, localization, reasoning, and planning.
Additionally, other benchmarks focus on real-world usage scenarios~\citep{fu2024blink,lu2024wildvision,bitton2023visit} and reasoning capabilities~\citep{yu2024mm,bai2023touchstone,han2023coremm,yan2024errorradar}. MME-RealWorld~\cite{zhang2024mme} places greater emphasis on quality and difficulty compared to its predecessor, containing the largest manually annotated QA pairs and the largest image resolution. 
These benchmarks reveal some common characteristics of MLLMs in task design and real-world applications. 
However, benchmarks specifically focused on reward models~\cite{li2024vlrewardbench} and those dedicated to evaluating safety and robustness remain relatively scarce. To further promote comprehensive evaluation of MLLM alignment, this paper contributes two benchmarks: one for reward models through self-construction and data cleaning, and another more comprehensive safety benchmark.

\section{Annotation Guidelines for Evaluating MLLM Responses}\label{sec:annotation_standard}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{imgs/data/web_ui.jpg}
\caption{\textbf{The user interface for data annotation}, featuring image/video display, questions, outputs from each model, detailed scoring criteria, and a section for reviewers to verify the accuracy of the scores.}    \label{fig:web_ui}
\end{figure*}
This document provides detailed annotation guidelines for evaluating responses generated by MLLMs. Annotators should rate and annotate each response according to four primary evaluation criteria: Visual Faithfulness, Helpfulness, Ethical Considerations (including safety, privacy, fairness, and harm), and Overall Performance. Annotators are expected to assess each response carefully based on these criteria to ensure high-quality feedback for model optimization.

\subsection{I. Visual Faithfulness Evaluation}

\textbf{Definition}: This criterion evaluates whether the generated response accurately reflects the objects and relationships in the image, ensuring consistency with the objects, relationships, and attributes of the true answer.

\textbf{Guidelines}:
\begin{enumerate}
    \item \textbf{Object Description Accuracy}: Ensure that the generated response accurately describes objects as in the true answer, avoiding references to non-existent objects and preventing errors in descriptions of existing objects.
    \item \textbf{Object Relationship Accuracy}: Evaluate whether the spatial, structural, or functional relationships between objects described in the response are correct. Minimize errors and misleading information in object relationship descriptions.
    \item \textbf{Object Attribute Accuracy}: Confirm that the response accurately describes the physical features, color, size, and state of objects, avoiding discrepancies with the true answer.
    \item \textbf{Overall Accuracy}: Assess the accuracy and consistency of the response in describing visual elements (objects, relationships, and attributes) in the image.
\end{enumerate}

\textbf{Rating Scale}:
\begin{itemize}
    \item \textbf{Severely Inaccurate}: Major errors in object descriptions, relationships, or attributes, or references to non-existent objects.
    \item \textbf{Partially Inaccurate}: Object descriptions, relationships, or attributes are mostly correct, but some details are inaccurate.
    \item \textbf{Moderately Accurate}: Descriptions of objects, relationships, and attributes are generally accurate, with minor errors.
    \item \textbf{Accurate}: Descriptions of objects, relationships, and attributes are correct and match the true answer.
    \item \textbf{Highly Accurate}: Descriptions of objects, relationships, and attributes are exceptionally precise and include useful additional details.
\end{itemize}

\subsection{II. Helpfulness Evaluation}

\textbf{Definition}: Carefully review the user prompt to ensure that the response provides valuable insights and relevant information that aid the user in understanding the image or task.

\textbf{Guidelines}:
\begin{enumerate}
    \item Assess whether the response provides useful context, insights, or relevant information to help the user better understand the image or task.
    \item Check if the model follows the specific instructions or guidelines provided by the user.
    \item Evaluate the overall contribution of the response in enhancing the user's understanding and experience.
\end{enumerate}

\textbf{Rating Scale}:
\begin{itemize}
    \item \textbf{Not Helpful}: The response does not address the user's prompt, providing entirely irrelevant information.
    \item \textbf{Partially Helpful}: The response contains some useful information, but overall help is limited.
    \item \textbf{Moderately Helpful}: The response is somewhat helpful but may lack important information.
    \item \textbf{Helpful}: The response effectively addresses the user's prompt and provides valuable insights.
    \item \textbf{Very Helpful}: The response is highly useful, offering rich insights that significantly enhance the user’s understanding.
\end{itemize}

\subsection{III. Ethical Considerations Evaluation (Safety, Privacy, Fairness, and Harm)}

\textbf{Definition}: This criterion evaluates whether the model adheres to ethical standards regarding safety, privacy protection, fairness, and avoiding harmful content.

\textbf{Guidelines}:
\begin{enumerate}
    \item Ensure the model provides appropriate warnings or avoids inappropriate recommendations when addressing sensitive topics (e.g., medical images).
    \item Confirm that the response does not disclose private information that could harm personal privacy.
    \item Assess whether the model shows fairness when addressing individuals or groups, avoiding any form of bias.
    \item Ensure the content does not incite violence, contain inappropriate workplace content (NSFW), or present any other unaddressed ethical issues.
\end{enumerate}

\textbf{Rating Scale}:
\begin{itemize}
    \item \textbf{Highly Unsafe/Privacy Infringing/Unfair/Harmful}: The response contains severely inappropriate content that violates ethical standards.
    \item \textbf{Unsafe/Privacy Issues/Unfair/Potentially Harmful}: The response may pose safety risks, privacy issues, or show unfairness.
    \item \textbf{Moderately Ethical}: The response mostly adheres to ethical standards, with minor safety or fairness issues.
    \item \textbf{Ethically Compliant}: The response aligns with ethical standards, following safety, privacy protection, and fairness requirements.
    \item \textbf{Highly Ethical/Safe/Privacy Protected/Fair/Non-Harmful}: The response fully meets ethical standards, respecting privacy, fairness, and free from harmful content.
\end{itemize}


\subsection{Annotation Requirements}

\begin{enumerate}
    \item The labeling staff should carefully read the user's prompt and the model-generated response before scoring the response based on three criteria: visual Faithfulness, helpfulness, and ethical considerations.
    \item Each model should briefly record the reason for its score, for example, if the answer is incorrect, if it includes hallucinated content, or if there is an error in the description.
    \item The final evaluation of each response should comprehensively consider all criteria, followed by a manual ranking of all responses.
    \item Tie Status: Indicate whether the user perceives no significant difference between the outputs of each model. If a tie occurs, provide a negative example (for multiple-choice, offer an incorrect answer; for long text, modify the content to include erroneous information).
    \item Ranking Basis: Briefly explain the reasoning behind the ranking.
\end{enumerate}

\section{Safety and Trustworth Dataset and Benchmark Construction}

\subsection{Training Data Construction Details}\label{sec:safety_data}

The self-constructed content is divided into 850 safety samples and 500 adversarial samples. The safety data is sourced from the following datasets: Red Teaming VLM~\citep{li2024red}, CelebA~\citep{liu2015deep}, and VLSBench~\citep{hu2024vlsbench}. The adversarial data, on the other hand, is generated using the AnyAttack~\citep{zhanganyattack} method.

To ensure data diversity, the safety data is comprised of five categories: 
\begin{itemize}
    \item 200 samples from Jailbreak,
    \item 200 samples from privacy and discrimination,
    \item 150 samples from hacking,
    \item 200 samples from violence,
    \item 100 samples from self-injury.
\end{itemize}
For the adversarial data, we randomly sampled 500 images from AnyAttack’s clean dataset. For each image, we then generate an adversarial image by pairing it with another, using $\epsilon = 8/255$ and other parameters set to their original values. To ensure the effectiveness of the adversarial attacks, we manually verified that the generated adversarial images cause the LLaVA-OV-7B model to produce hallucinated outputs.

Questions of safety data are generated by using VLGuard’s question generation prompts to create queries. For adversarial data, to maintain prompt diversity, we use GPT-4o to generate 10 variations of the question "Please describe this image," and a random sentence from these variations is selected for each image to serve as the query.

\subsection{Benchmark Construction Details}\label{sec:safety_benchmark}

We constructed our benchmark by selecting a total of 9 tasks from the Multitrust~\cite{zhangmultitrust} benchmark, which includes adversarial evaluations (both targeted and non-targeted), risk identification, typographic jailbreak, multimodal jailbreak, and cross-modal jailbreak tasks. Additionally, we included 2 tasks from VLGuard that focus on evaluating the model's robustness against NSFW (Not Safe For Work) content. These tasks address high-risk scenarios such as harmful medical investment advice, self-harm, and explicit content. Specifically, we assess the model's ability to reject harmful outputs in situations where the image is dangerous or where the image is harmless but the accompanying instruction is harmful. Table~\ref{tab:tasks} presents a detailed summary of each task, including the sample size and evaluation metrics used to assess model performance in these critical safety and adversarial scenarios.

\begin{table}[ht]
\caption{\textbf{\abbr-SafetyBench: summary of Task Data, Evaluation Metrics, and Comparison Methods for Safety and Adversarial Testing}. This table provides an overview of various tasks used for evaluating multimodal models' safety and adversarial robustness. The tasks are categorized based on attack type (adversarial or safety), and the evaluation metrics include success rates of adversarial attacks or model rejection rates for harmful outputs. The arrows in the Comparison column indicate whether higher (\(\uparrow\)) or lower (\(\downarrow\)) values of the evaluation metric are preferred.}
\label{tab:tasks}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|p{5.5cm}|p{4.5cm}|c|}
\hline
\rowcolor{gray!20} \textbf{Task Name} & \textbf{Data Size} & \textbf{Task Category} & \textbf{Evaluation Metric} & \textbf{Comparison} \\
\hline
Adv target & 100 & Adversarial Attack: The ratio of model descriptions containing adversarial image content. & Ratio & \(\downarrow\) \\
Adv untarget & 200 & Adversarial Attack: The ratio of model descriptions containing original image content. & Ratio & \(\uparrow\) \\
Crossmodel ASR & 18 & Cross-modal Jailbreak: Inducing multimodal models to deviate from their expected behavior and security constraints by combining text and image contexts related to the jailbreak task. & ASR: Adversarial attack success rate (probability of harmful output) & \(\downarrow\) \\
Multimodel ASR & 100 & Multimodal Jailbreak: Using a combination of various modalities (e.g., text and images) to induce multimodal models to deviate from security mechanisms and expected behavior, performing malicious commands or unauthorized functions. & ASR: Adversarial attack success rate (probability of harmful output) & \(\downarrow\) \\
Typographic ASR & 600 & Typographic Jailbreak: Converting malicious text instructions into images to perform jailbreak attacks, inducing multimodal models to deviate from their expected behavior and security constraints. & ASR: Adversarial attack success rate (probability of harmful output) & \(\downarrow\) \\
Crossmodel RtA & 18 & Cross-modal Jailbreak: Inducing multimodal models to deviate from their expected behavior and security constraints by combining text and image contexts for the jailbreak task. & RtA: Model accuracy on adversarial samples (probability of rejecting output) & \(\uparrow\) \\
Multimodel RtA & 100 & Multimodal Jailbreak: Using a combination of various modalities (e.g., text and images) to induce multimodal models to deviate from security mechanisms and expected behavior, performing malicious commands or unauthorized functions. & RtA: Model accuracy on adversarial samples (probability of rejecting output) & \(\uparrow\) \\
Typographic RtA & 600 & Typographic Jailbreak: Converting malicious text instructions into image format for jailbreak attacks, inducing multimodal models to deviate from their expected behavior and security constraints. & RtA: Model accuracy on adversarial samples (probability of rejecting output) & \(\uparrow\) \\
Risk identification & 500 & Identification Ability: The model's ability to correctly identify dangerous items and assess risk. & Ratio & \(\uparrow\) \\
NSFW-Text & 558 & Visual-Linguistic Safety: Ratio of model not rejecting output when the image is harmless but the instruction contains harmful content. & ASR: Adversarial attack success rate (probability of model not rejecting output) & \(\downarrow\) \\
NSFW-Image & 442 & Visual-Linguistic Safety: Ratio of model not rejecting output when the image contains harmful information. & ASR: Adversarial attack success rate (probability of model not rejecting output) & \(\downarrow\) \\
\hline
\end{tabular}%
}

\end{table}


\section{Why We Need Large-Scale Human Annotation?}\label{sec:why_we_need_human}

Manual annotation provides higher accuracy and adaptability than model-based annotation, especially in cases where the limitations of machine annotation become evident. In this section, we illustrate representative cases found in multi-modal data that are particularly challenging for models to annotate, highlighting the advantages of human intervention. All human annotations presented here come from our own dataset, while GPT-4o annotations were generated based on prompting GPT-4o by our ranking criteria.

\subsection{Misleading and Incomplete Questions}
Since training data is commonly annotated by models, maintaining perfect quality assurance is challenging, often resulting in some confusing or incomplete questions that cannot be answered accurately. In such cases, models struggle to provide effective annotations, whereas human annotators can identify and handle these issues with greater precision.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/annotate_sample/mcq.pdf}
    \caption{Example of a confusing question.}
    \label{fig:annotation_mcq}
\end{figure}
\begin{itemize}
    \item \textbf{Confusing Questions}: As shown in Figure~\ref{fig:annotation_mcq}, conflicts between the question and the provided choices can lead to confusion and misinterpretation. Many models attempt to select a “preferred” choice, with models like GPT-4 assigning a rank and providing a seemingly logical rationale. However, our human annotators are able to identify these flaws, reject all model-generated answers, and instead offer a more accurate response, highlighting the strength of human review in recognizing and rectifying such issues.
    \item \textbf{Incomplete Questions}: Similar to confusing questions, issues with data quality often result in questions that lack essential information. In many cases, MLLMs fail to recognize these inconsistencies and instead attempt to generate an answer, as do annotation models, which tend to favor responses from models that provide answers. As shown in Figure~\ref{fig:annotation_mcq2}, this question requires calculating the length of side AF; however, the given conditions are insufficient to solve the problem. Both Qwen2-VL and Claude 35 make incorrect attempts at analysis, with Qwen2-VL ultimately ranking the highest despite providing an incorrect answer. In contrast, human annotators are adept at identifying such issues and can accurately indicate that the question lacks sufficient conditions, justifying this in their rankings.
\end{itemize}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/annotate_sample/mcq2.pdf}
    \caption{Example of an incomplete question.}
    \label{fig:annotation_mcq2}
\end{figure}

\subsection{Difficult-to-Distinguish Answers}

The limitations of model-based annotation extend beyond question quality issues. At times, the responses generated by models are themselves challenging to rank accurately. This difficulty arises in two primary scenarios:

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/annotate_sample/short1.pdf}
    \caption{Example of a difficult question for model annotation.}
    \label{fig:short1}
\end{figure}

\begin{itemize}
    \item \textbf{All Models Fail to Identify the Correct Answer}. Certain questions, especially those involving complex reasoning, can be so challenging that no model can provide a correct response, indicating that the question surpasses the model's capabilities. Attempting to annotate such questions with model-based ranking often leads to further issues. For instance, in the high-resolution perception task shown in Figure~\ref{fig:short1}, the required information specified in the question does not actually appear in the image. However, multiple models still provide incorrect responses based on their interpretations. During scoring, the models tend to select the answer that aligns most closely with their understanding\footnote{The reason why GPT-4o annotator does not select its own response as the best may be due to the sampling strategy used in our API calls.}. In contrast, human annotators excel in recognizing these limitations and can provide the truly correct answer, demonstrating the advantage of manual annotation in such complex cases.
\item \textbf{Model Responses Are Rich but May Contain Minor Errors at a Fine-Grained Level}. In many datasets, especially in conversational data, when model responses are lengthy or involve specialized knowledge, it can be challenging—even for skilled multimodal annotators—to discern the subtle differences between outputs from various models. Our annotators take an average of 6 minutes to assess a single long-response question accurately, while models struggle even more with evaluating such extended replies. For instance, in Figure~\ref{fig:annotation_long}, the differences among models are confined to specific sections, where minor errors in visual perception or judgment occur (highlighted in red). These fine-grained details are often overlooked by the models themselves, resulting in scores that do not align with those given by human annotators.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs/annotate_sample/long.pdf}
    \caption{Example of subtle errors in model responses to a long question.}
    \label{fig:annotation_long}
\end{figure}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
\caption{Example of the Prompt Used for Augmenting Human Annotations.}
\label{tab:prompt_example}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l}
\begin{tabular}[c]{@{}l@{}}You will receive an image-related question, an answer, and a comment provided by a human expert for the answer. \\ \\ Your task is to expand the human comment comprehensively while retaining its strengths and weaknesses, \\ making it more professional, and logically rigorous. Focus only on expanding the comment and do not answer the question. \\ \\ Ensure the expanded comment is strictly based on the provided human comment and avoids any speculation or uncertain content.\\ \\ {[}Question:{]} \{question\}\\ {[}Answer:{]} \{answer\}\\ {[}Human Comment for the answer:{]} \{reason\}\\ \\ Expanded Comment:\end{tabular}
\end{tabular}%
}
\end{table}


\section{Comparison to Existing Methods on Beta Adjustment in LLMs and MLLMs}\label{sec:app_com_beta}

Dynamic adjustment of the beta parameter is not a completely new concept, but its application in large multimodal language models has been relatively unexplored. In this section, we discuss the key differences between our approach and existing methods, particularly focusing on dynamic beta adjustment strategies in LLMs and MLLMs. Several studies have been conducted in the LLM domain, with many papers showing that common LLM DPO datasets contain a significant number of noisy samples~\cite{wu2024beta,chowdhury2024provably,amini2024direct}. In these works, the application of different beta values to samples of varying quality has been shown to significantly improve algorithm robustness and performance.

Our approach differs from the existing works in two primary ways:

\textbf{First Exploration of Dynamic Beta Adjustment in MLLMs.}
To the best of our knowledge, we are the first to explore how MLLMs can dynamically adjust the beta parameter. We find that existing dynamic beta methods developed for LLMs cannot be directly adapted to the MLLM setting~\citep{wu2024beta}. This is mainly due to the increased complexity of the data in MLLM scenarios. Most existing methods ~\citep{wu2024beta,amini2024direct} utilize implicit rewards during the training process of DPO algorithms to select higher-quality samples. However, in MLLMs, the signal discriminability of the model itself is weaker and cannot guide the selection of $\beta$ (Figure~\ref{fig:dpo_ablation} (a)). Furthermore, as shown in our experiments, using MLLMs as reward models, especially with smaller models, results in suboptimal performance. This observation highlights a critical challenge in adapting existing methods to MLLMs.

\textbf{Leveraging a High-Quality Reward Model for Beta Adjustment.}
Existing methods often rely on various tricks to ensure that the estimated beta value is reasonable and of high quality, such as batch-level normalization and other techniques. Instance-level beta adjustments, on the other hand, are generally considered unstable and typically result in suboptimal performance. However, our approach challenges this conventional wisdom. We demonstrate that when a high-quality external reward model is available, reasonable modeling can enable instance-level beta adjustments to yield significant improvements. By leveraging a robust reward model, we show that even fine-grained adjustments to the beta parameter at the instance level can effectively enhance the model's performance, contrary to the usual belief that such adjustments are unreliable.


Our work provides a fresh perspective on how dynamic beta adjustments can be effectively applied to MLLMs, improving their robustness and optimization stability. By incorporating a high-quality reward model and dynamically scaling beta based on the reward margin, we achieve notable improvements over existing methods, particularly in handling noisy data and improving algorithmic performance.


\section{More Ablation and Analysis}\label{sec:app_exp}

\begin{figure*}[t]
\centering
\subfigure[]{
\begin{minipage}[t]{0.65\linewidth}
\centering
 \includegraphics[width=\linewidth]{imgs/exps/dpo_ablation.pdf}
\end{minipage}%
\label{label:ablation_dpo_a}
}%
\subfigure[]{
\begin{minipage}[t]{0.35\linewidth}
 \centering
 \includegraphics[width=\linewidth]{imgs/exps/heatmap_ablation.pdf}
\end{minipage}%
\label{label:ablation_dpo_b}

}%
\centering
\vspace{-0.2cm}
\caption{Ablation studies on our method and dataset. (a) Real-world tasks evaluation, where `LLaVA-OV-7B` serves as the baseline model, `+MM-RLHF` represents the use of our dataset combined with the traditional DPO algorithm. `+Implicit Reward` refers to using the dynamic beta strategy~\citep{wu2024beta} in LLMs. (b) Evaluation of the effect of the hyperparameters \(k\) and \(w\) on the MM-DPO model, demonstrating the effect of these variations on the leaderboard scores.}

\label{fig:dpo_ablation}
\end{figure*}

\subsection{Improvement with \abbr Dataset and MM-DPO}
With the help of our \abbr dataset, the baseline model demonstrates a general improvement across various benchmarks, with particularly significant gains observed in OCR and conversation tasks (Figure~\ref{label:ablation_dpo_a})). To further exploit the observation that different samples have varying quality, we initially attempted methods from the LLM domain, specifically using Implicit Reward during training to decide whether to increase or decrease the beta of each sample. However, we found that this approach did not work. There are two possible reasons: 1) Our dataset is of relatively high quality, as it is ranked manually, so the noise is minimal and there is no need for too many penalty terms or a reduction in beta; 2) MLLM data is more complex, and Implicit Reward does not provide a reliable signal to adjust beta. Therefore, MM-DPO uses a high-quality reward model to directly provide the signal, and the value of beta is constrained using the function $[\beta_{\text{ori}}, (1 + w) \beta_{\text{ori}}]$, preventing it from growing too excessively. This method overcomes the training instability caused by outliers, ultimately leading to a steady performance improvement.

\subsection{Effect of Hyperparameters \(w\) and \(k\)}
We experimented with various combinations of the hyperparameters \(w\) and \(k\), where \(k\) directly controls the mapping function from the reward margin to the scaling factor, and \(w\) governs the strength of the correction to \(\beta\) by the scaling factor. Figure~\ref{label:ablation_dpo_b} shows the impact of these hyperparameters on the final average performance (using the same benchmarks as Figure~\ref{label:ablation_dpo_a}). The results demonstrate that the method exhibits a certain level of robustness across different hyperparameter selections, generally leading to performance improvements. However, selecting the two hyperparameters requires some finesse; they cannot both be too large or too small simultaneously. The default values of \(w = 0.5\) and \(k = 0.5\) work well.
