\section{Lazy-Update Algorithms} \label{sec:detalgo}


After giving some preliminaries we will use through all this paper, in \Cref{ssec:algos} we describe the lazy-update algorithmic scheme, while in \Cref{ssec:detalgo-time-wc}, we provide a general bound on its amortized update cost that holds for  arbitrary sequences of edge insertions.

\subsubsection*{Preliminaries and notations}
The dynamic (incremental) graph model we study can be defined as a sequence
    $\dynG = \{ G^{(0)}(V,E^{(0)}), \ldots, $ $ G^{(t)}(V,E^{(t)}),  \ldots G^{(T)}(V,E^{(T)}) \}$,   where: (i) the set of vertices $V = \{1, \ldots , n \}$ is fixed, (ii) $T \leq \binom{n}{2}$ is the final graph,  while (iii)  $E^{(t)}$ is the subset of edges at time $t$. Note that this  changes in every time step $t \geq 1$, as a new edge $e^{(t)}$ is inserted, so that $E^{(t+1)} = E^{(t)} \cup \{e^{(t)}\} $. 
We remark that  our analysis and all our results can be easily adapted to a more general model that includes any combination of the following variants: (i)  growing vertex sets, (ii)  multiple insertions of the same edge, and (iii) directed edges (thus yielding directed graphs).  However,  the corresponding  adaptations of our analysis  would require  significantly heavier notation and some technicalities that  we decided to avoid for the sake of clarity and space.

Our goal is to design algorithms that, at every time step $t \geq 1$, are able to efficiently compute queries over the current $2$-balls of $G^{(t)}$. As mentioned in the introduction, our focus is on queries that are typical in graph mining such as: (i) given a vertex $u$, estimate the size of $\ball_2(u)$, and (ii) given two vertices $u,v \in V$, estimate the Jaccard similarity of the corresponding $2$-balls:
\[ 
    \jacc(\ball_2(u),\ball_2(v)) \ = \ \frac{|\ball_2(u) \cap \ball_2(u) |}{|\ball_2(u) \cup \ball_2(u) |}  \, . 
\]
Both the  theoretical  and experimental analysis of  our   lazy-update algorithms  consider the following key performance  measures: the \textit{amortized update time} per edge insertion and the \textit{approximation ratio} of our algorithms on the quantities $|\ball_2(u)|$ and $\jacc(\ball_2(u),\ball_2(v))$, for any choice of the input vertices. Intuitively, the amortized update time is the average time it takes to process a new edge, a more formal definition is deferred to \Cref{sec:detalgo}, after  a detailed description of the   algorithms  we consider.
    
We next summarize notation that is extensively used in the remainder of the paper. For a vertex $v \in V$ of a graph $G(V,E)$, we define:
    
    
    \begin{description} 
     % \item[$ \neigh^{(t)}(u)$:]    neighborhood  of $u$ as    = \{ v \in V^{(t)} \, : \,   (u,v) \in E^{(t)} \} $
  % \item[$\bd_v$:] the heavy degree;
    % \item[$\rd_v$:] the light degree;
    \item[$\neigh(v)$:] the set of neighborhoods of the vertex $v$.
    \item[$\deg_v$:] the degree of $v$. Notice that $\deg_v = \vert \neigh(v) \vert$;
    \item[$\lset_h(v)$:] set of vertices at distance exactly $h$ from $v$;
    \item[$\ball_h(v)$:] set of vertices at distance at most $h$ from $v$. 
\end{description}

The reader may have noticed that, in our notation above, the term $t$ does not appear: this is due to the fact that our analysis holds at any (arbitrarily-fixed) time step, which  is always clear from context. 



 

\subsection{Algorithm description} \label{ssec:algos}

%\subsection{A threshold-based deterministic algorithm}\label{subse:threshold}
Consider the addition of a new edge $(u,v)$ to $G$. Clearly, the only $2$-balls that are affected are those centered at $u$, $v$, and at every vertex $w \in \neigh(u) \cup \neigh(v)$. A \emph{baseline} strategy, given as \Cref{algo:naive} for the sake of reference, tracks changes exactly and thus updates all $2$-balls that are affected by an edge insertion. 


\begin{algorithm}[h!]
\SetAlgoLined
\DontPrintSemicolon
%\KwData{Undirected Graph $G = (V, E)$}
\SetKwFunction{FMain}{Insert}
\SetKwProg{Fn}{Function}{:}{end}
\Fn{\FMain{$u, v$}}{
    \ForEach{$x \in \neigh(u) \cup \{ u \}$}{
        $\ball_2(v) \gets \ball_2(v) \cup \{x\}$\;
        $\ball_2(x) \gets \ball_2(x) \cup \{v\}$\;
    }
    \ForEach{$x \in \neigh(v) \cup \{v\}$}{
        $\ball_2(u) \gets \ball_2(u) \cup \{x\}$\;
        $\ball_2(x) \gets \ball_2(x) \cup \{u\}$\;
    }
}
\caption{Baseline algorithm.}
\label{algo:naive}
\end{algorithm}
The magnitude of the changes (and the associated computational costs) induced by \texttt{Insert}$(u, v)$ vary. In particular, $\ball_2(u)$ can change significantly, as all vertices in $\ball_1(v)$ will be included in $\ball_2(u)$ (we refer to this as a \emph{heavy} update). Instead, for any vertex $w \in \neigh(u)\setminus\{v\}$, $\ball_2(w)$ will grow by at most one element, namely $v$ (this is referred to as a \emph{light} update). 
Symmetrically, the same holds for $v$ and for every $w \in \neigh(v) \setminus \{u\}$. 

A key observation at this point is that, while heavy updates can be addressed using (possibly, approximate) data structures that allow efficient merging of $1$- and $2$-balls, this line of attack fails with light updates, whose cost derives from their potential number, which can be large in many real cases, as we noted in the introduction.

A first idea to reduce the average number of updates per edge insertion is to perform heavy updates immediately, instead processing light updates in batches that are performed occasionally. More precisely, when a new edge $(u,v)$ arrives, it is initially marked as a \emph{red edge}. Whenever the number of red edges incident to a vertex $u$ exceeds a certain threshold, all the corresponding light updates are processed, and the state of red edges is updated to \emph{black}. See \Cref{fig:basic-example} for an example. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{img/basic-example.pdf}
    \caption{Example of insertion of a new edge $(u,v)$. The algorithm merges the $1$-ball of $v$ with the $2$-ball of $u$ (heavy update), while it does not immediately add vertex $v$ to the $2$-ball of vertex $w$ or any other of $u$'s neighbors. 
    %This is not the only vertex $w$ is not aware of (for instance $w'$ is another vertex %belonging to the 2-hop ball of $w$ that $w$ is not aware of).
    }
    \label{fig:basic-example}
\end{figure}

The idea behind the threshold-based approach is to maintain a balance between the number of black and red edges for every vertex. While useful when edge insertions appear in a random order, this approach may fail when red edges considerably expand the original size of the $2$-ball of some vertex $u$. In order to mitigate this problem, our algorithm  uses a second ingredient: upon each edge insertion $(u,v)$, the algorithm selects $k$ vertices from $\neigh(u)$ and $k$ from $\neigh(v)$ uniformly at random and performs a batch of light updates for the selected vertices, even if the threshold has not been reached yet.

\iffalse
Notice that, if the number of red edges did not reach the threshold yet, there might be some vertex $w \in \neigh(u) \setminus \{v\}$ that is not aware of all the vertices contained in its own $2$-ball. In order to mitigate this, our algorithm will use another ingredient. At each edge insertion $(u,v)$, the algorithm selects uniformly at random $k$ vertices from $\neigh(u)$ and $k$ from $\neigh(v)$, and performs the batch of light updates for the selected vertices, even if the threshold has not yet been reached.
\fi

These ideas are formalized in \Cref{alg:det_thresh}. For each vertex $v$, our algorithm maintains two sets $\apxball_1(v)$ and $\apxball_2(v)$, as well as the \emph{black degree} $\bd_v$ and \emph{red degree} $\rd_v$ of $v$.
Our algorithm guarantees that $\apxball_1(v)$ is exactly $\ball_1(v)$, while $\apxball_2(v)$ is in general a subset of $\ball_2(v)$. The algorithm uses two global parameters, namely a \textit{threshold} $\varphi \in [0,1]$, and an integer $k$. The role of the parameter $\varphi$ can be understood as follows: when $\varphi$ is set to $0$, the algorithm performs all heavy and light updates for every edge insertion, ensuring that $\apxball_2(v)$ always matches $\ball_2(v)$. As $\varphi$ increases, the update function becomes lazier: light updates are not always executed, and $\apxball_2(v)$ is typically a proper subset of $\ball_2(v)$. For instance, when $\varphi = 1$, light updates are performed in batches every time the degree of a vertex doubles. Parameter $k$ specifies the number of neighbors of $v$ that are randomly selected for update of their $2$-balls whenever an edge insertion involving $v$ occurs. This mechanism corresponds to \Cref{line:random_selection,line:random_selection_for,line:random_selection_for_inside} of \Cref{alg:det_thresh}. 

We call \lazyscheme$(\varphi,k)$ the algorithm that runs \Cref{alg:init} on an initial graph $G^{(0)}$ and then processes a sequence $S$ of edge insertions by running \Cref{alg:det_thresh} on each edge of $S$.  

\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
\KwData{An undirected graph $G=(V,E)$, a threshold parameter $0 \leq \varphi \leq 1$, and an integer $k \geq 0$.}
set $\varphi$ and $k$ as global parameters\;
\ForEach{vertex $u \in V$}{
    $\delta_u \gets 0$\;
    $\Delta_u \gets \deg_u$\;
    $\apxball_1(u) \gets \ball_1(u)$\;
    $\apxball_2(u) \gets \ball_2(u)$\;
}
\caption{\texttt{Init} operation}\label{alg:init}
\end{algorithm}

%Due to the nature of this algorithm, which triggers batch updates once a certain threshold %is surpassed, we have named it \emph{Threshold-Batching Update}.

\iffalse
When a new edge is added to the graph, the algorithm updates the above information as detailed in the pseudo-code given in \Cref{alg:det_thresh}. The role of the parameter $\varphi$ can be understood as follows: when $\varphi$ is set to $0$, the algorithm performs all the heavy and light updates for every edge insertion, ensuring that $\apxball_2(v)$ always matches the current ball $\ball_2(v)$. As $\varphi$ increases, the update function becomes lazier: light updates are not always executed, and $\apxball_2(v)$ may become a strict subset of $\ball_2(v)$. For instance, when $\varphi = 1$, light updates are performed in batches every time the degree of a vertex doubles. \Cref{line:random_selection,line:random_selection_for,line:random_selection_for_inside} specify the random selection explained above.

Due to the nature of this algorithm, which triggers batch updates once a certain threshold is surpassed, we have named it \emph{Threshold-Batching Update}.
\fi


\begin{algorithm}[h]
\SetAlgoLined
\DontPrintSemicolon
%\KwData{An undirected graph $G=(V,E)$, a threshold factor $\varphi$.}
\SetKwFunction{FMain}{Insert}
\SetKwProg{Fn}{Function}{:}{end}
\Fn{\FMain{$(u, v)$}}{
    \For{$x \in \{u,v\}$}{
        let $y \in \{u,v\} \setminus \{x\}$\;
        $\apxball_1(x) \gets \apxball_1(x) \cup \{y\}$\; \label{line:simple_union}
        \tcp{heavy update}
        $\apxball_2(x) \gets \apxball_2(x) \cup \apxball_1(y)$\; \label{line:heavy_update}
        $\delta_x \gets \delta_x + 1$\;
    
        \uIf{$\delta_x \geq \varphi \cdot \Delta_x$}{ \label{line:threshold_check}
            $\Delta_x \gets \Delta_x + \delta_x$\;
            $\delta_x \gets 0$\;
            \ForEach{$z \in \neigh(x)$}{ \label{line:propagate}
                \tcp{batch of light updates}
                $\apxball_2(z) \gets \apxball_2(z) \cup \apxball_1(x)$\; \label{line:light_updates_for}
            }
      }
      \Else {
        select $k$ vertices $w_1, \dots, w_k \in \neigh(x)$ u.a.r.\;\label{line:random_selection}
        \For{$i = 1, \dots, k$}{ \label{line:random_selection_for}
                \tcp{batch of light updates}
                $\apxball_2(w_i) \gets \apxball_2(w_i) \cup \apxball_1(x)$\; \label{line:random_selection_for_inside}
            } 
      }
    }
}

\caption{ \textsc{Insert} }\label{alg:det_thresh}
\end{algorithm}
\paragraph{A note on neighborhood representation.}
As we mentioned in the introduction, we treat  $\apxball_1(v)$ and $\apxball_2(v)$ as sets of vertices in this section and in Section \ref{sec:gammaok}. We remark that this only serves the purpose of analyzing the error introduced by our lazy update policies: lossless representations of $1$- and $2$-balls may be unfeasible for medium or large graphs and compact data sketches are typically used to represent them in such cases. The choice of the actual sketch strongly depends on the type of query (or queries) one wants to support, such as $1$- or $2$-ball sizes \cite{flajolet1985probabilistic,boldi2011hyperanf} or Jaccard similarity between $2$-balls \cite{broder2000identifying,cohen2007summarizing,becchetti2008efficient}. 
All sketches used for typical neighborhood queries are well-understood and come with strong accuracy guarantees. Moreover, they allow to perform the union of $1$- and $2$-balls we are interested in time proportional to the sketch size, which is independent of the sizes of the balls to merge \cite{agarwal2013mergeable}. 

\subsection{Cost analysis for arbitrary sequences} \label{ssec:detalgo-time-wc}
Consistently to what we remarked above, our cost analysis focuses on the number of 
\emph{set-union} operations: This performance measure in fact dominates
the computational cost of \Cref{alg:det_thresh}.  More in detail, given any sequence $S$ of edge insertions, starting from an initial graph $G^{(0)}$,  we denote by $\cost(S)$ the overall number of union operations performed in \Cref{line:simple_union,line:heavy_update,line:light_updates_for,line:random_selection_for_inside} of \Cref{alg:det_thresh} on the input sequence $S$.


We observe that a trivial upper bound to $\cost(S)$ is $O(\Delta |S|)$, since each insertion can cost $O(\Delta)$ union operations where $\Delta$ is the maximum degree of the current graph. However, this trivial argument turns out to be too pessimistic: in what follows,  we  provide a  more refined analysis of the amortized cost\footnote{The \emph{amortized analysis} is a well-known method originally introduced in \cite{Tarjan_amortized} that allows to compute tight bounds on the cost of a \textit{sequence} of operations, rather than the worst-case cost of an individual operation. In more detail,  we average the cost of a \emph{worst case} sequence of operations to obtain a more meaningful cost per operation.} per edge insertion. We say that an algorithm has \emph{amortized cost} $\hat{c}$ per edge insertion if, for any sequence $S$ of edge insertions, we have $\cost(S) \le \hat{c} |S|$.

\begin{lemma}
    \label{lm:amortized_det_alg}
    Given any initial graph $G^{(0)}$ and any sequence $S$ of edge insertions, the amortized update cost of \Cref{alg:det_thresh} is $O(\frac{1}{\varphi}+k)$ per edge insertion.
\end{lemma}
\begin{proof}
    Let us first consider the case $k=0$, i.e., when the random selection and the consequent instructions in \Cref{line:random_selection,line:random_selection_for,line:random_selection_for_inside} are never performed.  Our amortized analysis makes use of the \textit{accounting method} \cite{Tarjan_amortized}. The idea is  paying  the cost of any batch of light updates by charging it to previous edge insertions. More precisely, we assign \emph{credits} to each edge insertion that we will use to pay the cost of subsequent batches of light updates. Formally, the \emph{amortized} cost of an edge insertion is defined as the \emph{actual} cost of the operation, plus the credits we assign to it, minus the credits (accumulated from previous operations) we spend for it. We need to carefully define such credits in order to guarantee that the sum of the amortized costs is an upper bound to the sum of the actual costs, i.e. we always have enough credits to pay for costly batch light updates.
    
    We proceed as follows. When we insert the edge $(u,v)$, we put $2/\varphi$ credits on $u$ and $2/\varphi$ credits on $v$. Now we bound the actual and amortized cost of each insertion. 
    
    First, consider an edge insertion $(u,v)$ that does not trigger a batch of light updates. Its actual cost is 4 union operations (those in \Cref{line:simple_union,line:heavy_update}, 2 for each endpoint of $(u,v)$). Then its amortized cost is upper-bounded by $4+4/\varphi=O(1/\varphi)$. Now consider the case in which the insertion causes a batch of light updates for $u$, or $v$, or both. We show that the credits accumulated by previous insertions are sufficient to pay for its cost. To see this, consider a batch of light updates involving vertex $x \in \{u,v\}$. And let $\bd_x$ and $\rd_x$ be the current black and red degrees of $x$ at that time (just before \Cref{line:threshold_check} is evaluated). It is clear that for vertex $x$ we have accumulated  $\rd_x \cdot 2/\varphi$ credits that now we use to pay for the cost of \Cref{line:propagate,line:light_updates_for}. This cost equals to $\deg_x$ union operations, where $\deg_x$ is the current degree of vertex $x$. Since the batch of light updates has just been triggered, we have that $\rd_x \ge \varphi \bd_x$, and hence we have at least $\rd_x \cdot 2/\varphi \ge \varphi \bd_x \cdot 2/\varphi=2 \bd_x$ credits to pay for the $\deg_x=\bd_x+\rd_x=\bd_x+\varphi\bd_x \le 2 \bd_x$ union operations. This concludes the proof. 

    Finally, to obtain the claim when $k>0$, we notice that, in this case, every edge insertion causes $O(k)$ additional union operations.
\end{proof}

%\subsection{Approximation analysis over random  sequences of arbitrary graphs}
\section{Random edge sequences}\label{subse:rand_perm}
In this section, we analyze the accuracy of our lazy-update algorithm(s) over an arbitrary dynamic graph, whose edges are given in input as a uniformly sampled, random permutation over its edge set.
Dynamic graphs resulting from random sequences of edge insertions have been an effective tool to provide theoretical insights that have often proved robust to empirical validation in various dynamic scenarios \cite{monemizadeh2017testable,kapralov2014approximating,peng2018estimating,mcgregor2014graph,chakrabarti2008robust}.
In more detail, assume $G = (V, E)$, with $|E| = t$, is the graph observed up to some time $t$ of interest. Following \cite{monemizadeh2017testable,peng2018estimating}, we assume that the sequence of edges up to time $t$ is chosen uniformly at random from the set of all permutations over $E$.\footnote{It should be noted that this includes the general case in which $t$ is any intermediate point of a longer stream that possibly extends well beyond $t$. In this case, it is well-known and easy to see that, conditioned on the subset $E$ of the edges released up to time $t$, their sequence is just a permutation of $E$.} The following fact is an immediate consequence of well-known and intuitive properties of random permutations. We state it informally for the sake of completeness, avoiding any further, unnecessary notation.
\begin{fact}\label{fa:perm}
    Consider a dynamic graph $G = (V, E)$, whose edges are observed sequentially according to a permutation over $E$ chosen uniformly at random. Then, for every $E'\subseteq E$, the sequence in which edges in $E'$ are observed is itself a uniformly chosen, random permutation over $E'$.
\end{fact}



In the remainder, for an arbitrary vertex $v$, we analyze how well the output $\apxball_2(v)$ of \Cref{alg:det_thresh} approximates $\ball_2(v)$ at any round $t$ in terms of its \emph{coverage}:

\begin{definition}\label{def:coverage}
    We say that the output  $\apxball_2(v)$  of \lazyscheme$(\varphi,k)$ is a $(1-\varepsilon)$-\textit{covering} of $\ball_2(v)$ if the following holds: i) $\apxball_2(v) \subseteq \ball_2(v)$; ii) $\Expec{}{\vert \apxball_2(v) \vert} \geq (1-\varepsilon) \vert \ball_2(v) \vert$, where expectation is taken over the randomness of the algorithm and/or the input sequence. When the algorithm produces a $(1-\varepsilon)$-covering $\apxball_2(v)$ of $\ball_2(v)$ for every $v$, we say it has \emph{approximation ratio} $\frac{1}{(1-\varepsilon)}$.
\end{definition}
Our main result in this section is formalized in the following 

\begin{theorem}\label{thm:random_seq_quality}
    Let  $\varepsilon \in (0,1)$,  and fix  parameters $k = 0$ and $\varphi = \frac{\varepsilon}{1-\varepsilon}$. Consider any  graph $G(V,E)$ submitted as  a  uniform  random permutation of its edge set $E$ to \lazyscheme$(\varphi,k)$. Then, at every time step $t\le |E|$, the algorithm has approximation ratio $\frac{1}{1-\varepsilon}$. Moreover, for every $\alpha > 0$ and every vertex $v  \in V$, we have:
    \[
        \Prob{}{|\apxball_2(v)| < \frac{1 - \alpha}{1 + \varphi}|L_2(v)|}\le e^{-\frac{2\alpha^2|L_2(v)|}{(1 + \varphi)^2}}.
    \]
\end{theorem}
\begin{proof}
Fix a vertex $v \in V$ and a round $t \geq 1$. In the remainder of this proof, all quantities are taken at time $t$. We are interested in how close $|\apxball_2(v)|$ is to $|\ball_2(v)|$. To begin, we note that the following relationship holds deterministically:
\begin{equation}\label{eq:apxball_det}
    |\apxball_2(v)| = 1 + |L_1(v)| + |\apxball_2(v)\cap L_2(v)|,
\end{equation}
where the only random variable on the right hand side is the last term. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{img/example_partition.pdf}
    \caption{Example of a partition of $L_2(v)$ into three sets $C_1, C_2, C_3$. Edges connecting vertices $w \in L_2(v)$ to their respective partitions are thicker.}
    \label{fig:L2_partition}
\end{figure}
We next define a partition $\mathcal{C} = \{C_{u}: u\in L_1(v)\}$ of $L_2(v)$ as follows: for each $w \in L_2(v)$, we choose a vertex $u \in L_1(v) \cap \neigh(w)$ and assign $w$ to $C_u$. This way, each vertex $w\in L_2(v)$ is associated to exactly \emph{one} edge connecting one vertex in $L_1(v)$ to $w$ (see \Cref{fig:L2_partition}, where the edges in question are thick in the picture). Let $E_v$ denote the set of such edges and note that i) $E_v$ is a subset of the edges connecting vertices in $L_1(v)$ to those in $L_2(v)$, ii) $|E_v| = |L_2(v)|$ by definition and iii) $|C_u|\le \deg_u - 1$ for every $u\in L_1(v)$, given that $C_u$ contains a subset of $u$'s neighbors and $(v, u)$ is always present. Moreover, \Cref{alg:det_thresh} guarantees that $|\apxball_2(v)\cap L_2(v)|$ is at least the number of edges in $E_v$ that are black. 
These considerations allow us to conclude that
\[
    |\apxball_2(v)\cap L_2(v)| \ge |\{e\in E_v:\text{ $e$ is black}\}|.
\]
A key observation at this point is that \Cref{alg:det_thresh} implies that for every $x\in V$, $\rd_x\le\left\lfloor\frac{\varphi}{1 + \varphi}\deg_x\right\rfloor$. As a consequence, if some $e = (u, w)\in E_v$ was not among the last $\left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor$ edges incident in $u$ that were released within time $t$, it is necessarily black. For $e = (u, w)\in E_v$, with $u\in L_1(v)$ and $w\in L_2(v)$, let $X_e = 1$ if $e$ was among the first $\deg_u - \left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor$ edges incident in $u$ that were released up to time $t$ and let $X_e = 0$ otherwise. Following the argument above, the event $( X_e = 1 )$ implies the event $\text{"$e$ is black"}$, whence:
\begin{equation}\label{eq:apx_balls}
    |\apxball_2(v)\cap L_2(v)| \ge |\{e\in E_v:\text{ $e$ is black}\}|\ge \sum_{e\in E_v}X_e.
\end{equation}
Next, we are interested in bounds on $\Prob{}{X_e = 1}$. Assume $e$ is incident in $u$ and let $S$ be the set of edges incident in $u$ observed up to time $t$. 
Then, from \Cref{fa:perm}, the sequence in which these edges are observed is just a random permutation of $S$. 
This immediately implies that, if $e$ is incident to vertex $u\in L_1(v)$, then  
\[
    \Prob{}{X_e = 1} = \frac{\deg_u - \left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor}{\deg_u}\ge\frac{1}{1 + \varphi}.
\]
Together with \eqref{eq:apx_balls} this yields:
\[
    \Expec{}{|\apxball_2(v)|}\ge 1 + |L_1(v)| + \frac{1}{1 + \varphi}|L_2(v)| \geq \frac{1}{1+\varphi}\vert \ball_2(v) \vert.
\]
We next show that $\sum_{e\in E_v}X_e$ is concentrated around its expectation when $|L_2(v)|$ is large enough, which implies that $|\apxball_2(v)|$ is concentrated around a value close to $|\ball_2(v)|$ in this case. The main technical hurdle here is that the $X_e$'s are correlated (albeit mildly, as we shall see). To prove concentration, we resort to Martingale properties of random edge sequences to apply the method of (Average) Bounded Differences \cite{dubhashi2009concentration}. In order to do this, we need bounds on $\Prob{}{X_e = 1\vert X_f = 1}$ and $\Prob{}{X_e = 1\vert X_f = 0}$, for $e, f\in E_v$. Assume again that $e$ is incident in $u\in L_1(v)$ and that $S$ is the set of edges incident in $u$ observed up to time $t$. Assume first that $f$ is also incident in $u$ and that, without loss of generality, $f$ is the $i$-th edge to appear among those in $S$. $X_f = 1$ implies $i\le\deg_u - \left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor$. On the other hand, for any such choice for $f$'s position in the sequence, Fact \ref{fa:perm} implies that $e$ will appear in a position $j$ that is sampled uniformly at random from the remaining ones, so that $\Prob{}{X_e = 1\vert X_f = 1} = \frac{\deg_u - \left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor - 1}{\deg_u - 1}$ in this case. With a similar argument, it can be seen that $\Prob{}{X_e = 1\vert X_f = 0} = \frac{\deg_u - \left\lfloor\frac{\varphi}{1 + \varphi}\deg_u\right\rfloor}{\deg_u - 1}$. Intuitively and unsurprisingly, the events $(X_e = 1)$ and $(X_f = 1)$ are negatively correlated, while $(X_e = 1)$ and $(X_f = 0)$ are positively correlated. This allows us to conclude that $\Prob{}{X_e = 1\vert X_f = 1}\le \Prob{}{X_e = 1\vert X_f = 0}$ and 

\[
    \Prob{}{X_e = 1\vert X_f = 0} - \Prob{}{X_e = 1\vert X_f = 1}\le\frac{1}{\deg_u - 1}.
\]
Assume next that $f$ is not incident in $u$. Again from Fact \ref{fa:perm}, in this case $f$ has no bearing on the relative order in which edges incident in $u$ appear, so that $\Prob{}{X_e = 1\vert X_f = 0} = \Prob{}{X_e = 1\vert X_f = 1} = \Prob{}{X_e = 1}$. Now, without loss of generality, suppose that $f = (z, w)$, with $z\in L_1(v)$, so that $w\in C_{z}$. Denote by $E_v(z)$ the subset of edges in $E_v$ with one end point in $C_{z}$. Moving to conditional expectations we have
\begin{align*}
    &\Expec{}{\sum_{e\in E_v}X_e \, \vert\,  X_f = 0} - \Expec{}{\sum_{e\in E_v}X_e \, \vert\, X_f = 1} \\
    &= \sum_{e\in E_v}\left(\Prob{}{X_e = 1 \, \vert\, X_f = 0} - \Prob{}{X_e = 1 \, \vert\, X_f = 1}\right)\\
    &= \sum_{e\in E_v \setminus E_v(z)}\left(\Prob{}{X_e = 1 \, \vert\, X_f = 0} - \Prob{}{X_e = 1 \, \vert\, X_f = 1}\right) \\
    &+ \sum_{e\in E_v(z)}\left(\Prob{}{X_e = 1 \, \vert\, X_f = 0} - \Prob{}{X_e = 1 \, \vert \, X_f = 1}\right)\\
    &\le\frac{|C_{z}|}{\deg_z - 1}\le 1,
\end{align*}
where the third inequality follows from the definition of $C_z$, since $f$ is incident in $z$, while the last inequality follows since $|C_z|\le \deg_z - 1$ for every $z\in L_1(v)$, because one of the edges incident in $z$ is by definition the one shared with $v$.

We can therefore apply \cite[Definition 5.5 and Corollary 5.1]{dubhashi2009concentration}, with $c\le |L_2(v)|$ to obtain, for every $\alpha > 0$:
\begin{align*}
    &\Prob{}{\Expec{}{\sum_{e\in E_v}X_e} - \sum_{e\in E_v}X_e > \alpha\Expec{}{\sum_{e\in E_v}X_e}}\le e^{-\frac{2\alpha^2|L_2(v)|}{(1 + \varphi)^2}},
\end{align*}
where in the right hand side we also used the bound $\Expec{}{\sum_{e\in E_v}X_e}\ge\frac{1}{1 + \varphi}|L_2(v)|$ we showed earlier.
Finally, we recall \eqref{eq:apxball_det} and \eqref{eq:apx_balls} to conclude that $|\apxball_2(v)|\ge \frac{1 - \alpha}{1 + \varphi}|L_2(v)|$ with (at least) the same probability.
\end{proof}

\Cref{thm:random_seq_quality} easily implies approximation bounds on indices that depend on the union and/or intersection of $2$-balls. For example, we immediately have the following approximation bound on the Jaccard similarity between any pair of $2$-balls.

\begin{corollary}\label{cor:jacc}
  Under the same assumptions as \Cref{thm:random_seq_quality}, at any time step $t \geq 1$ and for any pair of vertices $u,v \in V$, \lazyscheme$(\varphi,k)$ guarantees the following approximation of the Jaccard similarity between $\ball_2(u)$ and $\ball_2(v)$ with probability at least $1 - e^{-\frac{2\alpha^2|L_2(u)|}{(1 + \varphi)^2}} - e^{-\frac{2\alpha^2|L_2(v)|}{(1 + \varphi)^2}}$: 
    \begin{equation}\label{eq:jacc_apx}
        \textstyle \dfrac{\jacc(\ball_2(u),\ball_2(v))}{1-2\varepsilon'} \geq \jacc(\apxball_2(u), \apxball_2(v)) \geq (1-\varepsilon')\jacc(\ball_2(u),\ball_2(v)) - \varepsilon',
    \end{equation}
    where $\varepsilon' = \frac{\varphi + \alpha}{1 + \varphi}$.
\end{corollary}
\begin{proof}
It is easy to see that $|\apxball_2(u)|\ge (1 - \varepsilon')|\ball_2(u)|$ and $|\apxball_2(v)|\ge (1 - \varepsilon')|\ball_2(v)|$ together imply \eqref{eq:jacc_apx} deterministically. The result then immediately follows from \Cref{thm:random_seq_quality} and a union bound on the events $(|\apxball_2(u)| < (1 - \varepsilon')|L_2(u)|)$ and $(|\apxball_2(v)| < (1 - \varepsilon')|L_2(v)|)$.
\end{proof}

