\section{Introduction}
In this paper, we consider the task of processing a possibly large, dynamic graph $G(V,E)$, incrementally provided as a stream of edge insertions, so that at any point of the stream it is possible to efficiently evaluate different queries that involve functions of the \textit{$h$-hop neighborhoods} of its vertices. For a vertex $v\in V$, its $h$-hop neighborhood is simply the \emph{set} of vertices that are within $h$ hops from $v$. In the remainder, $h$-hop neighborhoods are called \textit{$h$-balls} for brevity. As concrete examples of query types we consider, one might want to estimate the size of the $2$-ball at a given vertex, or the Jaccard similarity between the $2$-balls centered at any given two vertices, or other indices of a similar flavor that depend on the intersection or union between $1$-balls and/or $2$-balls, just to mention a few.

Neighborhood-based indices are common in key mining tasks, such as link prediction in social \cite{liben2003link} and biological networks \cite{wang2023assessment} or to describe statistical properties of large social graphs \cite{becchetti2008efficient}. For example, $2$-hop neighborhoods are important in social network analysis and similarity-based link prediction \cite{zhou2021experimental,zareie2020similarity,Sim-Nodes_Survey_2024}, while accurate approximations of $h$-balls' sizes are used to estimate key statistical properties of (very) large social networks \cite{boldi2011hyperanf,backstrom2012four}, or as link-based features in classifiers for Web spam detection \cite{becchetti2008link}. 
 
When the graph is static, an effective approach to this general task is to treat $h$-balls as subsets of the vertices of the graph, suitably represented using approximate summaries or sketches \cite{agarwal2013mergeable}. This line of attack has proved successful, for example in the efficient and scalable evaluation of important neighborhood-based queries on massive graphs that in part or mostly reside on secondary storage \cite{feigenbaum2005graph,mcgregor2014graph,becchetti2008efficient,boldi2011hyperanf}.
%and can only be accessed over consecutive, sequential passes
  

Nowadays, standard applications in social network analysis often entail dynamic scenarios in which  input graphs \textit{evolve over time}, under a sequence of  edge insertions and possibly deletions \cite{aggarwal2014evolutionary}. 



With respect to a static scenario, the dynamic case poses new and significant challenges even in the incremental setting, as soon as $h > 1$.\footnote{The case $h = 1$ is considerably simpler and it \textit{barely relates to graphs}: adding or removing one edge $(u, v)$ simply requires updating the $1$-balls of $u$ and $v$ accordingly, i.e., updating two corresponding set sketches by adding or removing one item. This has been the focus of extensive work in the recent past that we discuss in \Cref{subse:related}.} To see this, it may be useful to briefly sketch the cost of maintaining $1$- and $2$-balls exactly under a sequence of edge insertions, as we discuss in more detail in Section \ref{sec:detalgo}. 
When $h = 2$, each \texttt{Insert}($u, v$) operation entails (see Algorithm \ref{algo:naive} and Figure \ref{fig:basic-example}): i) updating the $2$-ball of $u$ to its union with the $1$-ball of $v$ and viceversa (what we call a \textit{heavy} update); ii) adding $v$ to the $2$-ball of \emph{each} neighbor of $u$ and viceversa (what we call a \textit{light} update). Both heavy and light updates can result in high computational costs per edge insertion: a heavy update can be expensive if at least one of the neighborhoods to merge is large; on the other hand, light updates are relatively inexpensive, but they can be numerous when large neighborhoods are involved, again resulting in a high overall cost per edge insertion. Unfortunately, $h$-balls can grow extremely fast with $h$ in many social networks, already as one switches from $h = 1$ to $h = 2$ \cite{becchetti2008link,backstrom2012four}. For the same reason, maintaining lossless representations of $2$-balls for each vertex of such networks might require considerable memory resources and might negatively impact the cost of serving neighborhood-based queries that involve moderately or highly central vertices.


To address the aforementioned issues for graphs that reside in main memory, one might want to trade some degree of accuracy for the following broad goals: 1) designing algorithms with low update costs, possibly $O(1)$ amortized per edge insertion; 2) minimizing memory footprint beyond what is needed to store the graph; 3) maintaining $1$- and $2$-balls using data structures that afford efficient, real-time computation of queries as the ones mentioned earlier with minimal memory footprint.

\iffalse
In particular (see Algorithm \ref{algo:naive}), if we are to maintain $1$- and $2$-balls exactly, every \texttt{Insert}($u, v$) operation entails (among others) the following updates (Algorithm \ref{algo:naive} and Figure \ref{fig:basic-example}): i) $\ball_2(u)\leftarrow\ball_2(u)\cup\ball_1(v)$; ii) $\ball_2(v)\leftarrow\ball_2(v)\cup\ball_1(u)$; iii) $\ball_2(x)\leftarrow\ball_2(x)\cup\{v\}$ for every $x\in\neigh(u)$; iv) $\ball_2(y)\leftarrow\ball_2(y)\cup\{u\}$ for every $y\in\neigh(v)$. Each single update i) and ii) (heavy updates) can be computationally expensive when it involves large neighborhoods, while updates iii) and iv) (light updates) are relatively inexpensive but they can be numerous, again in the case of large neighborhoods. Overall, processing edge insertions can be very expensive in some graphs, such as social networks, where $|\ball_h(u)|$ can grow extremely fast, already switching from $h = 1$ to $h = 2$ \cite{becchetti2008link,backstrom2012four}. Hence, while affording exact answering of neighborhood-based queries, maintaining $1$- and $2$-balls exactly may not be feasible in practice because of the associated memory footprint and, to a lesser extent, because of the costs of neighborhood-based queries that involve moderately or highly central vertices. Ideally, in the quest for scalable solutions for possibly large graphs residing in main memory, one might want to trade some degree of accuracy for the following broad goals: 1) designing algorithms with low update costs, possibly $\bigO(1)$ amortized per edge insertion; 2) minimizing memory footprint beyond what is needed to store the graph;\rem{Forse qui dovremmo mettere una footnote per dire che assumiamo una rappresentazione efficiente ma standard del grafo in memoria principale, essendo una sua rappresentazione compressa "beyond the scope ..."}; 3) whatever the data structures used to maintain $1$- and $2$-balls, these should afford efficient, real-time computation of queries as the ones mentioned earlier with minimal memory footprint.
\fi

Heavy updates are natural and well-known candidates for efficient (albeit approximate) implementation using compact, sketch-based data structures \cite{gibbons2001estimating,broder2001completeness,broder2000identifying,agarwal2013mergeable,trevisan/646978.711822}. However, sketches alone are of no avail in handling light updates, whose sheer potential number requires a novel approach.
The literature on efficient data structures that handle insertions and often deletions over dynamic graphs is rich. However, efficient solutions to implement neighborhood-based queries on dynamic edge streams are only known for $1$-balls \cite{BSS20,MROS,VOS,CGPS24}, nor do approaches devised for other dynamic problems adapt to our setting in any obvious way, something we elaborate more upon in Section \ref{subse:related}. 



\subsection{Our Contribution}
In this paper, we propose an approach that trades some degree of accuracy for a substantial improvement in the average number of light updates. In a nutshell, upon an edge insertion, our algorithm performs the (two) corresponding heavy updates, but in general only a subset of the required light updates, according to a scheme that combines a threshold-based mechanism and a randomized, batch-update policy. Hence, for every vertex $u$, we only keep an approximation (a subset to be specific) of $u$'s $2$-ball. If $1$- and $2$-balls are represented with suitable data sketches, our approach affords constant average update cost per edge insertion.\footnote{The particular sketch used depends on the neighborhood queries we want to be able to serve. When sketches are used, the cost of merging two neighborhoods corresponds to the cost of combining the corresponding sketches, which is typically a constant that depends on the desired approximation guarantees. For example, if we are interested in the Jaccard similarity between pairs of $1$- and/or $2$-balls, this cost will be proportional to the (constant) number of minhash values we use to represent each neighborhood.} While the behavior and accuracy guarantees of most sketching techniques are well understood, the estimation error induced by lazy updates can be arbitrarily high in some cases. The main focus of this paper is on the latter aspect, which is absent in the static case but critical in the dynamic setting. Accordingly, we assume lossless representations of $1$-balls and approximate $2$-balls in our theoretical analyses in Sections \ref{sec:detalgo} and \ref{sec:gammaok}, while we use  standard sketching techniques to represent $1$- and $2$-balls in the actual implementations of the algorithms and baselines we consider in the experimental analysis discussed in Section \ref{sec:exp}.



\paragraph{Almost-optimal performance on random sequences.} We prove in Section \ref{subse:rand_perm} that even a simplified, deterministic variant of our lazy-update \Cref{alg:det_thresh} achieves asymptotically optimal expected performance when the  sequence of edge insertions is a random, uniform permutation over an \textit{arbitrary} set of edges. In other words, our lazy approach is robust to adversarial topologies as long as the edge sequence follows a random order.
Formally, we prove that, for any desired $0 < \varepsilon < 1$, our algorithm only performs $O(\frac{1}{\varepsilon})$ (amortized) updates per edge insertion, while at any time $t$ and for every vertex $v$, the estimated size of $v$'s $2$-ball is, in expectation,  at most a factor $\varepsilon$ away from its true value. We further prove that this approximation result holds with a probability that exponentially increases with the true size of the $2$-ball itself (\Cref{thm:random_seq_quality}). 
Thanks to this analysis in concentration,  our results can be extended to other functions of $2$-balls, including union, intersection and Jaccard similarity (see \Cref{cor:jacc} for this less obvious case). 

As positive as this result may sound, it begs the following questions from a careful reader: 1) Are the results above robust to adversarial sequences? 2) Is a performance analysis under random sequences representative of practical scenarios? More generally, does our lazy scheme offer significant practical advantages?

\paragraph{Performance analysis on adversarial inputs.} While our results for random sequences are optimal regardless of the underlying graph's topology, one might wonder about the ability of an adversary to design \textit{worst-case, adaptive sequences} that force our approach to behave poorly and, in this case, whether any conditions on the graph topology are \textit{necessary} for this to happen. We investigate this issues in Section \ref{sec:gammaok}, where we first show that it is possible to design  worst-case sequences of edge insertions that force our algorithm to perform arbitrarily worse than the random setting (\Cref{thm:lower}).  However, as a further contribution, we also prove that worst-case input sequences exist \textit{only if} the \textit{girth}  \cite{diestel2024graph} of the final graph is at most $4$.
More precisely, we show that a randomized, special case of \Cref{alg:det_thresh} achieves asymptotically optimal performance on a class of graphs that contains all graphs with girth at least $5$, \footnote{The  class is in fact more general since it also includes graphs with  a ``bounded'' number of cycles of length at most 4. See \Cref{def:gammaok}, for a formal definition of this class.} even when the input sequence is chosen by an adaptive adversary. 

\smallskip


\iffalse 

\paragraph{Experimental Analysis.}
As for the second question above, an analysis under random permutation sequences as the one in Section \ref{subse:rand_perm} is relatively common in the literature on dynamic edge streams and data streams  \cite{buriol2006counting,kapralov2014approximating,peng2018estimating,Hanauer22DynamicSurvey}. Yet, one might rightly wonder about its practical significance for the task considered in this paper. We investigate this question in Section \ref{sec:exp}, where we conduct experiments on small, medium and large-sized, incremental graphs (whose main properties are summarized in Table \ref{tab:summary_dynamic_dataset}). At least on the diverse sample of real networks we consider, experimental results on the estimation of key queries such as size and Jaccard similarity are consistent with the theoretical findings from Section \ref{subse:rand_perm}. 
Again in agreement with the analysis performed there, results highlight considerable savings in computational cost, compared to baselines that maintain a consistent view of the entire edge sequence. 
We finally remark that the datasets we consider are samples of real social networks. As such, they have relatively large local and global clustering coefficients\footnote{At least the undirected ones.} and thus low girth. Hence, our experimental analysis further supports the robustness of our theoretical findings: forcing our algorithm(s) into a worst-case behavior not only requires topologies characterized by a low girth, but also carefully crafted input sequences that are unlikely to occur in practice.
\fi

\paragraph{Experimental analysis.} 
As for the second question above, an analysis under random permutation sequences as the one in Section \ref{subse:rand_perm} is relatively common in the literature on dynamic edge streams and data streams  \cite{buriol2006counting,kapralov2014approximating,peng2018estimating,Hanauer22DynamicSurvey}. Yet, one might rightly wonder about its practical significance for the task considered in this paper. We investigate this question in Section \ref{sec:exp}, where we conduct experiments on small, medium and large-sized, incremental graphs (whose main properties are summarized in Table \ref{tab:summary_dynamic_dataset}). At least on the diverse sample of real networks we consider, experimental results on the estimation of key queries such as size and Jaccard similarity are consistent with the theoretical findings from Section \ref{subse:rand_perm}. The main take-away is that, when using sketches to represent the $1$- and $2$-balls, the errors obtained with our lazy update policy are similar and fully comparable to those of the baseline, which performs all necessary light updates. At the same time, our algorithm proves to be significantly faster than the baseline, sometimes achieving a speedup of up to $90\times$. 

We finally remark that the datasets we consider are samples of real social networks. As such, they have relatively large local and global clustering coefficients\footnote{At least the undirected ones.} and thus low girth. Hence, our experimental analysis further supports the robustness of our theoretical findings: forcing our algorithm(s) into a worst-case behavior not only requires topologies characterized by a low girth, but also carefully crafted input sequences that are unlikely to occur in practice.




%Another important remark is that the accuracy obtained by our lazy scheme on \emph{real} graphs and \emph{real} edge-insertion sequences is fully consistent with our theoretical findings for \emph{random permutation} sequences. This, combined with the fact that the graphs we considered have relatively large local and global clustering coefficients\footnote{At least the undirected ones.} and thus low girth, might suggest that uniform random permutations are a reasonable theoretical proxy of real sequences and that pathological worst-case graphs and worst-case sequences are pretty rare in practice.  





%\rem{Remind that results apply to directed graphs and multiple insertions but we give %analysis for undirected case for the sake of simplicity.}

\paragraph{Remark.}
We finally stress that although we present them for the undirected case for ease of presentation and for the sake of brevity, our algorithms apply to directed graphs as well,\footnote{Of course, in this case we have directed $h$-balls, i.e., sets of a vertices that can be reached in at most $h$ hops from a given vertex, or from which it is possible to reach the vertex under consideration in at most $h$ hops.} while our analysis extends to the directed case with minor modifications.

\subsection{Further related work}\label{subse:related}
Efficient data structures for queries that involve $h$-balls of a dynamic graph turn out to be useful in different network applications. Besides those we  mentioned earlier   \cite{becchetti2008link,zareie2020similarity,Sim-Nodes_Survey_2024}, we cite here the work \cite{cavallo20222}, where the notion of \textit{2-hop Neighbor Class Similarity} (2NCS) is proposed: this is  a new quantitative graph structural property that correlates with \textit{Graph Neural Networks} (GNN) \cite{scarselli2008graph,wu2022graph} performance more strongly and consistently than alternative metrics. 2NCS considers two-hop neighborhoods as a theoretically derived consequence of the two-step label propagation process governing GCNâ€™s training-inference process.

%\item \textbf{Efficient solutions for \textit{all our queries} on  1-Balls on dynamic graphs.} 

As remarked in the introduction, efficient solutions for Jaccard similarity queries on $1$-balls   have been proposed for different dynamic graph models: all of them share the use of suitable data sketches to manage insertion and deletion of elements from sets. In particular,  \cite{BSS20,MROS,VOS} proposes and compares different  approaches that work in  the fully-dynamic streaming model, while an efficient solution, based on a buffered version of the $k$-min-hashing scheme is proposed in \cite{CGPS24}. This works in the fully-dynamic streaming model and allows recovery actions when certain ``worst-case'' edge deletion events occur. A further algorithm is presented in \cite{zhang2022effective}, where \textit{bottom-$k$ sketches} \cite{cohen2007summarizing} are used to perform dynamic graph clustering based on Jaccard similarity among vertices' neighborhoods. We remark that 
none of these previous approaches include ideas or tools that can be adapted to efficiently manage the $2$-ball update-operations we need to implement in this work.



As for other queries that might be "related" or "useful" in our setting, a considerable amount of work on data structures that support edge insertions and deletions exists for several queries,  such as connectivity or reachability, (exact or approximate) distances, minimum spanning tree, (approximate) \textit{betweenness centrality}, and so on. We refer the reader to \cite{HanauerHS22} for a nice survey on experimental and theoretical results on the topic. To the best of our knowledge however, none of these approaches can be obviously adapted to handle the types of queries we consider in this work. For example, a natural idea would be using an incremental data structure to dynamically maintain the first $h$ levels of a BFS tree, such as \cite{EvenS81,RodittyZ11}, that achieve $O(h)$ amortized update time. However, let alone effectiveness in efficiently serving queries as the ones we consider here, the data structure uses $\Omega(n)$ space per BFS. This is prohibitive in our setting, where we would need to instantiate one such data structure for each vertex, with total space $\Omega(n^2)$. Moreover, since in a degree-$\Delta$ graph $\Theta(\Delta)$ BFS trees can change following a single edge insertion, the corresponding amortized time per edge insertion could be as high as $\Theta(\Delta)$, which is basically the same cost of the baseline solution we discuss at the beginning of Section \ref{sec:detalgo}.


