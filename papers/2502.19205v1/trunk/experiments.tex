\section{Experimental analysis}\label{sec:exp}
The purpose of this experimental analysis, conducted on real datasets, is the validation of our lazy approach along three main axes: i) the qualitative consistency between the theoretical findings from \Cref{sec:detalgo,subse:rand_perm,sec:gammaok} and the actual behavior of our lazy approach on real, incremental datasets;
ii) the accuracy of our approach on two key neighborhood-based queries, namely, size and Jaccard similarity;  iii) its computational savings with respect to non-lazy baselines on medium and large incremental graphs.

\subsection{Experimental setup}
%\rem{Hardware, measurements etc.}
\paragraph{Platform.} Our experiments were performed on a machine with 2.3 GHz Intel Xeon Gold 5118 CPU with 24 cores, 192 GB of RAM, cache L1 32KB, shared L3 of 16MB and UMA architecture. The whole code is written in \texttt{C++}, compiled with \texttt{GCC 10} and with the following compilation flags: \texttt{-DARCH$\_$X86$\_$64 -Wall -Wextra -g -pg -O3 -lm}.

\paragraph{Algorithms.}
We compared $\lazyscheme(\varphi, k)$ with various combinations $(\varphi, k)$ with  the following baselines: \\
\textit{i)} the exact \textit{algorithm} \ref{algo:naive}, which is not scalable and is only used in the first round of experiments on smaller datasets, in order to isolate the error introduced by lazy updates; \\
\textit{ii)} the naive \textit{sketch-based baseline}, which adopts sketch-based representations of $1$- and $2$-balls, but performs all necessary updates. It corresponds to Algorithm \ref{alg:det_thresh} with $\varphi = 0$ and $k = 0$, but where $1$- and/or $2$-ball unions correspond to merging the corresponding sketches.\footnote{Again, the specific sketch (or sketches) used depends on the neighborhood queries one wants to support.}

With the exception of the first set of experiments (i.e. item (i) above), we compared our algorithm to the sketch-based baseline. For some results, we needed to compute the true value of the parameter of interest for $2$-balls (e.g., size) by executing a suitably optimized BFS. For our \lazyscheme, we used combinations of the following values: $\varphi =  0.1, 0.25, 0.5, 0.75, 1$ and $k = 0, 2, 4, 8$. 
%The baseline (\Cref{algo:naive}) is implemented by setting parameters $\varphi = 0$ and 
%$k = 0$ in our algorithm. 


\paragraph{Implementation details.}
To best assess the performance of algorithms, it would be ideal to minimize the overhead deriving from the management of edge insertions in the graph. We observe that this overhead is the same for all algorithms we tested. Hence, we represented graphs using the \textit{compressed sparse row}  format \cite{Eisenstat1982YaleSM} and, since we knew the edge insertion sequence in advance, we pre-allocated the memory needed to accommodate them, so as to minimize overhead.
For experiments that required hash functions, we used \textit{tabulation hashing} \cite{tabulation-hashing}.

% We used the same graph representation for all algorithms analyzed in this section. Since the overhead due to different graph representations in memory is the same for every algorithm, we implemented the graph using Compressed Sparse Row (CSR) format \cite{Eisenstat1982YaleSM} and pre-allocated the space necessary to the insertion of new edges, so as to mitigate the overhead from dynamic graph management. 

%\rem{studiare possibili versioni più dinamiche dei parametri dipendenti dallo stato del %vertice (es., $k=\log(\vert \apxball_2(v) \vert$ oppure $\varphi = \frac{1}{\log (\rd)}$)}

\paragraph{Datasets.}
We considered real incremental graphs of different sizes, both directed and undirected, available from NetworkRepository \cite{networkrepository}. In our time analysis, we also extracted a large incremental dataset from a large static graph, namely \texttt{soc-friendster} \cite{soc-friendster}, available from SNAP \cite{snapnets}. Following previous work on dynamic graphs \cite{chen2022dynamictree, Hanauer22DynamicSurvey}, we generated an incremental graph by adding edges sequentially and in random order, starting from an empty graph. The main features of our datasets are summarized in \Cref{tab:summary_dynamic_dataset}.

\iffalse
\begin{table*}[h]
    \centering
    \begin{tabular}{l|ccccc|c}
        \toprule
        dataset & $\vert V \vert$ & $\vert E \vert$ & \# insertions & batches & directed & dynamic \\
        \midrule
        comm-linux-kernel-reply & 27,927 & 242,976 & 1,030,000 & 839,643 & \ding{52} & \ding{52} \\
        fb-wosn-friends \cite{fb-wosn-friends} & 63,731 & 817,090 & 1,270,000 & 736,675 & \ding{56} & \ding{52} \\
        ia-enron-email-all & 87,273 & 321,918 & 1,130,000 & 214,908 & \ding{52} & \ding{52} \\
        soc-flickr-growth & 2,302,925 & 33,140,017 & 33,140,017 & 134 & \ding{52} & \ding{52} \\
        soc-youtube-growth & 3,223,589 & 9,376,594 & 12,200,000 & 203 & \ding{52} & \ding{52} \\
        soc-friendster \cite{soc-friendster} & 65,608,366 & 1,806,067,135 & -- & -- & \ding{56} & \ding{56} \\
        \bottomrule

    \end{tabular}
    \caption{Summary table of real networks used in the experiments.}
    \label{tab:summary_dynamic_dataset}
\end{table*}
\fi

\begin{table*}[h]
    \centering
    \begin{tabular}{l|ccc|c}
        \toprule
        Dataset & $\vert V \vert$ & $\vert E \vert$ & Directed & Dynamic \\
        \midrule
        comm-linux-kernel-reply & 27,927 & 242,976 & \ding{52} & \ding{52} \\
        fb-wosn-friends \cite{fb-wosn-friends} & 63,731 & 817,090 & \ding{56} & \ding{52} \\
        ia-enron-email-all & 87,273 & 321,918 & \ding{52} & \ding{52} \\
        soc-flickr-growth & 2,302,925 & 33,140,017 & \ding{52} & \ding{52} \\
        soc-youtube-growth & 3,223,589 & 9,376,594 & \ding{52} & \ding{52} \\
        soc-friendster \cite{soc-friendster} & 65,608,366 & 1,806,067,135 & \ding{56} & \ding{56} \\
        \bottomrule

    \end{tabular}
    \caption{Summary table of real networks used in the experiments.}
    \label{tab:summary_dynamic_dataset}
\end{table*}

\subsection{Results}\label{subse:exp_res}

As we remarked in the introduction, we focused on the case of undirected graphs for ease of exposition and for the sake of space, but our approach extends seamlessly to directed graphs, such as some of the real examples we consider in this section. In such cases, the only caveat to keep in mind is that we define the $h$-ball of a vertex $u$ as the subset of vertices that are reachable from $u$ over a directed path traversing at most $h$ edges.\footnote{One could alternatively define the $h$-ball of $u$ as the subset of vertices from which it is possible to reach $u$ traversing at most $h$ directed edges. We stick to the former definition, which is more frequent in social network analysis.}

\paragraph{Impact of lazy updates.} The goal of our first experiment is twofold: i) assessing the impact of lazy updates on the estimation of $2$-balls; ii) assessing the degree of consistency between the theoretical findings of Sections \ref{subse:rand_perm} and \ref{sec:gammaok} and the actual behavior of our algorithms on real datasets. In order to isolate the specific contribution of lazy updates in the estimation error, we implemented (true and approximate) $1$-balls and $2$-balls losslessly, as dictionaries. This way, the error in $2$-ball size estimation is only determined by our lazy update policy. Since, as we argued elsewhere in the paper, lossless representations of $2$-balls quickly becomes unfeasible for larger datasets, this first experiment was run on $3$ small-medium datasets, namely, \texttt{comm-linux-kernel-reply}, \texttt{fb-wosn-friends} and \texttt{ia-enron-email-all} (results for \texttt{comm-linux-kernel-reply} are reported in \Cref{fig:covering_linux2} in \Cref{apx:experiments}).

\iffalse
In particular, we used four incremental datasets using their respective incremental sequence of insertion. This gives an empirical validation on real update sequences.

We test our solutions w.r.t. their ability to estimate the balls size against real and random incremental sequences. The aim is twofolds: on the one hand, we show that our analysis on random sequences \Cref{sec:rand_worst} works in practice. One the other hand, we show that with real incremental sequences we have the same experimental results, meaning that the analysis we performed is valid for real scenario. We keep track of the error between the true and estimated sizes of two-hop balls over a suitable sample of vertices. For an individual vertex $u$, the error is measured as $\frac{\vert\ball_2(u)\vert}{\vert\apxball_2(u)\vert}$, we then also consider the Root Mean Square Error (RMSE) over the chosen sample of vertices.
\fi

For each of the above graphs, we selected as a sample $5000$ vertices whose $2$-balls are the largest at the end of the edge insertion sequence. For every pair $(\varphi, k)$ of parameter values for \lazyscheme$(\varphi,k)$, we performed $10$ independent runs. Each run is organized into the following steps: 1) the initial graph $G^{(0)}(V,E^{(0)})$ corresponds to the first $20\%$ edge insertions; 2) we measure the coverage (see \Cref{def:coverage}) of each of the $5000$ $2$-balls above by \lazyscheme$(\varphi,k)$ at each of the $100$ equally spaced timestamps, the same in each run. For the generic timestamp $t$, we measure the average coverage
\[
    C_t = \frac{1}{5000}\sum_{i=1}^{5000}\frac{\apxball_i}{\ball_i},
\]
where $\ball_i$ and $\apxball_i$ respectively denote the true and estimated sizes of the $i$-th $2$-ball from the sample. Finally, for each timestamp $t$, we plot the average of the $10$ values of $C_t$ computed in every run. 
The results, summarized in \Cref{fig:covering}, are fully consistent with our theoretical findings from \Cref{subse:rand_perm}. 
At least for the diverse dataset sample considered here, uniform random permutations are a reasonable theoretical proxy of real sequences. More in general, real sequences seem to be rather far from the pathological worst-cases analyzed in \Cref{ssec:lowerbound}, so that the actual behavior of our algorithm is not only in line, but better than our analysis predicts.
We also have an initial insight into the effects of the parameters $\varphi$ and $k$, which will be examined more thoroughly in the subsequent subsections.

\begin{figure}
    \centering  
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{img/plots/covering_fb-wosn-friends_vertical.pdf}
        \caption{fb-wosn-friends}
        \label{fig:covering_fb}
    \end{subfigure}
    \hfill % Spazio verticale tra le subfigure
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{img/plots/covering_ia-enron-email-all_vertical.pdf}
        \caption{ia-enron-email-all}
        \label{fig:covering_enron}
    \end{subfigure}
    \caption{Average coverage $C_t$ from a network with $20\%$ of its edges to the end of the insertion sequence. Dashed lines show the theoretical expected coverage for random sequences (see \Cref{thm:random_seq_quality}).}
    \label{fig:covering}
\end{figure}


\paragraph{Ball size estimation via sketches.} 
The goal of the next round of experiments was assessing the accuracy of our algorithms in $2$-ball size estimation, in the realistic setting in which approximate $1$- and $2$-balls are represented via state-of-art sketches that are based on probabilistic counters \cite{trevisan/646978.711822}. In this case, we have a compound estimation error, arising from both our lazy update policy and the use of probabilistic counters. We ran experiments on the three small-medium sized datasets considered previously, plus two large ones, namely, \texttt{soc-youtube-growth} and \texttt{soc-flickr-growth}. Again, we considered the top-$5000$ largest $2$-balls as sample. The experiments were executed as in the previous case, with the following differences: i) we considered $3$ timestamps, respectively corresponding to $50\%$, $75\%$ and $100\%$ of all edge insertions of each dataset; ii) in this case, $1$- and $2$-balls are not explicitly represented as sets (not even by the baseline). As a result, ball sizes might be overestimated and coverage has no clear meaning.
%\rem{Andy: spiegherei meglio questo cambio di indice statistico: la presenza di sketch %rende possibile anche una stima in eccesso della size....}
We therefore use Mean Absolute Percentage Error (MAPE) to measure accuracy,  defined as follows:
\[
\text{MAPE} = \frac{1}{5000} \sum_{i=1}^{5000} \frac{\vert \ball_i - \apxball_i \vert}{\ball_i},
\]
where $i$ refers to the $2$-ball size of the $i$-th sampled vertex. For each dataset, this index is computed for each of $10$ independent runs at each of the $3$ timestamps we consider.
Results are summarized for all datasets and combinations $(\varphi, k)$ we consider in \Cref{tab:size-quality} for the last timestamp ($100\%$ of edge insertions). Results for other timestamps ($50\%$ and $75\%$) are similar and are omitted for the sake of space (see \Cref{apx:experiments} for complete results). The main takeaways here are that i) the additional error introduced by the use of sketches (which can be controlled by varying the size of the sketch) is relatively modest; ii) even relatively large values of $\varphi$ and/or small values of $k$ result in performances that are close to those of the baseline that uses sketches to represent balls, but naively performs all light updates. The effect of $k$ is more pronounced when $\varphi$ is large, contributing to a reduction in both error and, to a lesser extent, variance.
Finally, we note that although the effect of increasing $k$ can be achieved by decreasing $\varphi$, our analysis in \Cref{sec:gammaok} suggests that the parameter $k$ provides robustness against worst-case scenarios.
Even though the analysis of the impact of sketches on error is out of the scope of this paper, the results in the previous paragraph and the small difference with the baseline (\Cref{tab:size-quality}) when using the sketches further suggest that the analysis of random sequences may also apply to real sequences.

\begin{table}[ht]
    \centering
    \caption{Mean and standard deviation of absolute percentage errors for $2$-hop neighborhood size estimation. Size estimates were made using the KMV probabilistic counter \cite{trevisan/646978.711822}, with size $32$.
    Queries were made at the end of the insertion sequence.
    }
    \begin{tabular}{lc|ccc|c}
    \toprule
        & $k$ & $\varphi = 0.1$ & $\varphi = 0.5$ & $\varphi = 1$ & baseline\\
    \midrule
    
        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{linux}}}}

        & $0$ & $0.14 \pm 0.12$ & $0.19 \pm 0.14$ & $0.17 \pm 0.11$ & \multirow{4}{*}{$0.12 \pm 0.10$} \\
        & $2$ & $0.13 \pm 0.11$ & $0.14 \pm 0.10$ & $0.16 \pm 0.11$ & \\
        & $4$ & $0.14 \pm 0.09$ & $0.17 \pm 0.12$ & $0.14 \pm 0.10$ & \\
        & $8$ & $0.14 \pm 0.11$ & $0.14 \pm 0.09$ & $0.13 \pm 0.10$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{fb-wosn}}}}

        & $0$ & $0.16 \pm 0.12$ & $0.15 \pm 0.11$ & $0.21 \pm 0.12$ & \multirow{4}{*}{$0.14 \pm 0.11$} \\
        & $2$ & $0.13 \pm 0.09$ & $0.15 \pm 0.10$ & $0.17 \pm 0.11$ & \\
        & $4$ & $0.14 \pm 0.11$ & $0.15 \pm 0.10$ & $0.16 \pm 0.11$ & \\
        & $8$ & $0.16 \pm 0.13$ & $0.14 \pm 0.10$ & $0.15 \pm 0.11$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{enron}}}}

        & $0$ & $0.13 \pm 0.10$ & $0.16 \pm 0.11$ & $0.20 \pm 0.14$ & \multirow{4}{*}{$0.13 \pm 0.11$} \\
        & $2$ & $0.14 \pm 0.11$ & $0.16 \pm 0.12$ & $0.16 \pm 0.12$ & \\
        & $4$ & $0.13 \pm 0.12$ & $0.15 \pm 0.12$ & $0.15 \pm 0.12$ & \\
        & $8$ & $0.13 \pm 0.11$ & $0.14 \pm 0.10$ & $0.16 \pm 0.13$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{flickr}}}}

        & $0$ & $0.17 \pm 0.14$ & $0.18 \pm 0.12$ & $0.17 \pm 0.11$ & \multirow{4}{*}{$0.17 \pm 0.14$} \\
        & $2$ & $0.16 \pm 0.12$ & $0.12 \pm 0.09$ & $0.14 \pm 0.10$ & \\
        & $4$ & $0.14 \pm 0.09$ & $0.13 \pm 0.10$ & $0.16 \pm 0.10$ & \\
        & $8$ & $0.14 \pm 0.11$ & $0.13 \pm 0.10$ & $0.14 \pm 0.09$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{youtube}}}}

        & $0$ & $0.15 \pm 0.11$ & $0.15 \pm 0.10$ & $0.24 \pm 0.11$ & \multirow{4}{*}{$0.14 \pm 0.11$} \\
        & $2$ & $0.16 \pm 0.13$ & $0.14 \pm 0.10$ & $0.19 \pm 0.11$ & \\
        & $4$ & $0.13 \pm 0.11$ & $0.13 \pm 0.10$ & $0.15 \pm 0.11$ & \\
        & $8$ & $0.13 \pm 0.10$ & $0.12 \pm 0.09$ & $0.15 \pm 0.11$ & \\
        
    \bottomrule

    \end{tabular}
    \label{tab:size-quality}
\end{table}

\paragraph{Accuracy in Jaccard similarity estimation.} 
With the same goal as in the previous experiment, we now evaluate the quality of our lazy approach policy combined with the use of sketches in another graph mining task: the Jaccard similarity estimation for $2$-hop neighborhoods.
The sketch used to represent the $1$- and $2$- balls is the well-known $h$-minhash signature \cite{broder1997resemblance}, with $h = 100$ hash functions.

As before, $10$ independent runs were performed for each combination of parameters $\varphi, k$, and the baseline, on each of the previously used datasets.
Errors were measured at $3$ different timestamps, corresponding to $50\%$, $75\%$, and $100\%$ of the edge insertion sequence.

%The $h$-minhash sketch incurs an estimation error of $O(\sqrt{h})$ for the Jaccard similarity \cite{broder2000identifying,broder2001completeness}.
Considering all vertex pairs in the entire graph would be computationally prohibitive, as their number is too large. Moreover, many of these pairs would have an extremely low Jaccard similarity, making it difficult to estimate them with a reasonably small error \cite{broder2000identifying,broder2001completeness}.
Therefore, to better evaluate our algorithm's and baseline's quality, we need vertex pairs with a sufficiently high similarity in their $2$-hop neighborhoods.
%Efficiently sampling such pairs is crucial in this experiment, since analyzing all vertex pairs is computationally infeasible due to their large quantity. In addition, many pairs exhibit very low Jaccard similarity, complicating an accurate estimation.
To address this, we adopted a similar sampling process as before:
we selected the $5000$ vertices with the largest $2$-hop neighborhoods at the end of the edge insertion sequence and randomly chose $1000$ pairs whose similarity is at least $0.2$.
On that sample, we evaluated the MAPE of the Jaccard similarity estimate computed using the $h$-minhash signatures.
\Cref{tab:jacc-similarity-quality} reports the results at the end of the insertion sequence, while results for the other timestamps ($50\%$ and $75\%$) are similar and are omitted for sake of space (see \Cref{apx:experiments} for complete results).

\iffalse
Efficiently sampling vertex pairs with a Jaccard similarity of their $2$-hop neighborhoods that is not too small is crucial in this experiment.
Considering all node pairs in the entire graph would be computationally prohibitive, as their number is too large. Moreover, many of these pairs would have an extremely low Jaccard similarity, making it difficult to estimate them with a reasonably small error.
To address this, we adopted the following sampling process:
% Since this experiment requires sampling \emph{pairs} of vertices whose respective $2$-hop neighborhoods have Jaccard similarity that is not too small, the sampling process is as follows:
i) we considered the $5000$ vertices with the largest $2$-hop neighborhoods at the end of the edge insertion sequence, ii) from these, we selected at random the $1\%$ of all $\binom{5000}{2}$ possible pairs, iii) we then filtered this set, retaining only the pairs whose Jaccard similarity of the $2$-balls is $\geq 0.2$, iv) finally, we uniformly sampled $1000$ pairs from the remaining set.
With this approach, we obtained a sample set of $1000$ vertex pairs for each dataset, on which we assess the MAPE of the Jaccard similarity estimation computed using the $h$-minhash signatures.
\Cref{tab:jacc-similarity-quality} reports the results for the first timestamp ($50\%$ \textcolor{red}{(poi diventerà $100\%$ quando finiranno gli esperimenti)} of edge insertions), while results for the other timestamps are similar and are omitted for sake of space.
\fi

This experiment further confirms the observations from the previous one: when using sketches to represent the $2$-balls, the errors obtained with our lazy update policy (with appropriate choices of parameters $\varphi,k$) are similar and fully comparable to those of the baseline, which performs all necessary updates.

\begin{table}[h]
    \centering
    \caption{Mean and standard deviation of absolute percentage errors for Jaccard similarity estimation, with $100$ hash functions. Queries were made at the end of the insertion sequence.}
    \begin{tabular}{lc|ccc|c}
    \toprule
        & $k$ & $\varphi = 0.1$ & $\varphi = 0.5$ & $\varphi = 1$ & baseline\\
    \midrule

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{linux}}}}

        & $0$ & $0.11 \pm 0.09$ & $0.13 \pm 0.09$ & $0.11 \pm 0.09$ & \multirow{4}{*}{$0.10 \pm 0.08$} \\
        & $2$ & $0.09 \pm 0.07$ & $0.12 \pm 0.09$ & $0.11 \pm 0.09$ & \\
        & $4$ & $0.10 \pm 0.08$ & $0.12 \pm 0.09$ & $0.10 \pm 0.08$ & \\
        & $8$ & $0.09 \pm 0.07$ & $0.12 \pm 0.09$ & $0.10 \pm 0.08$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{fb-wosn}}}}

        & $0$ & $0.70 \pm 0.25$ & $0.72 \pm 0.24$ & $0.74 \pm 0.23$ & \multirow{4}{*}{$0.70 \pm 0.24$} \\
        & $2$ & $0.70 \pm 0.24$ & $0.70 \pm 0.25$ & $0.72 \pm 0.24$ & \\
        & $4$ & $0.70 \pm 0.24$ & $0.70 \pm 0.24$ & $0.72 \pm 0.24$ & \\
        & $8$ & $0.70 \pm 0.24$ & $0.70 \pm 0.24$ & $0.71 \pm 0.25$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{enron}}}}

        & $0$ & $0.09 \pm 0.09$ & $0.14 \pm 0.12$ & $0.15 \pm 0.12$ & \multirow{4}{*}{$0.09 \pm 0.09$} \\
        & $2$ & $0.10 \pm 0.09$ & $0.12 \pm 0.10$ & $0.14 \pm 0.12$ & \\
        & $4$ & $0.09 \pm 0.09$ & $0.11 \pm 0.10$ & $0.13 \pm 0.11$ & \\
        & $8$ & $0.10 \pm 0.09$ & $0.10 \pm 0.09$ & $0.11 \pm 0.10$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{flickr}}}}

        & $0$ & $0.12 \pm 0.09$ & $0.11 \pm 0.09$ & $0.11 \pm 0.09$ & \multirow{4}{*}{$0.11 \pm 0.09$} \\
        & $2$ & $0.11 \pm 0.08$ & $0.10 \pm 0.08$ & $0.11 \pm 0.09$ & \\
        & $4$ & $0.11 \pm 0.08$ & $0.10 \pm 0.08$ & $0.10 \pm 0.08$ & \\
        & $8$ & $0.11 \pm 0.08$ & $0.10 \pm 0.08$ & $0.11 \pm 0.08$ & \\

        \midrule[.66pt]

        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{youtube}}}}

        & $0$ & $0.12 \pm 0.09$ & $0.11 \pm 0.09$ & $0.17 \pm 0.13$ & \multirow{4}{*}{$0.11 \pm 0.09$} \\
        & $2$ & $0.11 \pm 0.09$ & $0.11 \pm 0.09$ & $0.15 \pm 0.11$ & \\
        & $4$ & $0.11 \pm 0.09$ & $0.13 \pm 0.10$ & $0.16 \pm 0.11$ & \\
        & $8$ & $0.11 \pm 0.09$ & $0.12 \pm 0.09$ & $0.14 \pm 0.10$ & \\

    \bottomrule

    \end{tabular}
    \label{tab:jacc-similarity-quality}
\end{table}

\paragraph{Run time analysis.}
Finally, we measured the running times of our algorithm and the corresponding speed-up with respect to the baseline.
In the following, we report and discuss the results for the task of size estimation, using probabilistic counters. The results for the task of Jaccard similarity estimation are analogous, and thus reported in \Cref{tab:minhash-time} in \Cref{apx:experiments}.

\Cref{tab:size-time} reports the average speed-up of our algorithm with respect to the naive baseline for the same combinations $(\varphi, k)$ considered previously, for all datasets except \texttt{com-friendster}. Speed-ups are computed in terms of \emph{total update time}, i.e., the total time it takes to process the whole insertion sequence. In \Cref{tab:size-time-friendster} reports the overall processing times on \texttt{com-friendster}, for a subset of the combinations of $\varphi$ and $k$. It should be noted that the baseline did not complete within a reasonable amount of time in this case. Finally, \Cref{fig:bar_times} illustrates the average time cost per operation for the baseline, as well as the slowest and fastest parameter settings of $\varphi$ and $k$ in our algorithm.
These experiments clearly highlight that the number of lazy updates is the crucial factor affecting performance and that our approach is very effective at addressing this problem, resulting in considerable speed-ups that grow with the size of the graph.
These results regarding processing times, in conjunction with previous findings, underscore the significant advantage in terms of speed at the expense of a modest and acceptable reduction in query quality which, due to the necessary use of sketches, is never totally accurate.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/plots/bars_counter_time.pdf}
    \caption{Average time per insertion operation (in seconds). The baseline time for \texttt{soc-friendster} dataset is not reported, since it exceed a time limit of $36$ hrs.}
    \label{fig:bar_times}
\end{figure}

\begin{table}[ht]
    \centering
    \caption{Speed up with respect to the baseline, using KMV probabilistic counters \cite{trevisan/646978.711822}, with size $32$.}
    \begin{tabular}{ll|ccccc}
    \toprule
        & \multirow{2}{*}{$k$} & \multicolumn{5}{c}{$\varphi$} \\
        & & $0.1$ & $0.25$ & $0.5$ & $0.75$ & $1$\\
    \midrule

    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{linux}}}}
    & 0 & 14.39x & 22.73x & 30.27x & 32.36x & 35.65x \\
    & 2 & 12.00x & 18.22x & 22.03x & 21.49x & 23.18x \\
    & 4 & 11.52x & 15.34x & 17.63x & 16.93x & 17.54x \\
    & 8 & 9.57x & 11.86x & 12.80x & 12.03x & 12.22x \\
    \midrule[.66pt]
    
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{fb-wosn}}}}
    & 0 & 5.48x & 9.40x & 11.15x & 15.23x & 16.22x \\
    & 2 & 5.22x & 7.35x & 7.16x & 9.21x & 9.67x \\
    & 4 & 4.32x & 6.14x & 6.05x & 7.18x & 7.23x \\
    & 8 & 3.79x & 4.38x & 4.91x & 5.31x & 5.22x \\
    \midrule[.66pt]
    
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{enron}}}}
    & 0 & 4.17x & 5.38x & 6.64x & 7.12x & 7.70x \\
    & 2 & 3.61x & 4.11x & 4.68x & 4.82x & 5.27x \\
    & 4 & 3.05x & 3.42x & 3.78x & 3.96x & 4.21x \\
    & 8 & 2.51x & 2.73x & 2.85x & 3.01x & 3.12x \\
    \midrule[.66pt]
    
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{flickr}}}}
    & 0 & 13.52x & 19.97x & 26.54x & 31.51x & 34.20x \\
    & 2 & 12.16x & 16.41x & 19.52x & 21.93x & 22.61x \\
    & 4 & 10.76x & 13.96x & 16.35x & 17.17x & 17.79x \\
    & 8 & 9.10x & 11.19x & 12.36x & 12.99x & 13.28x \\
    \midrule[.66pt]
        
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\texttt{youtube}}}}
    & 0 & 41.61x & 61.80x & 77.08x & 86.50x & 93.73x \\
    & 2 & 38.16x & 51.64x & 60.39x & 64.93x & 65.51x \\
    & 4 & 36.08x & 47.43x & 52.26x & 55.06x & 55.90x \\
    & 8 & 33.33x & 39.60x & 42.73x & 43.61x & 43.82x \\

    \bottomrule

    \end{tabular}
    \label{tab:size-time}
\end{table}

\begin{table}[ht]
    \centering
    \caption{Total running time for ball size estimation using probabilistic counters, for \texttt{com-friendster} dataset. The time for the baseline algorithm is not reported since it exceed a time limit of $36$ hrs.}
    \begin{tabular}{l|ccccc}
    \toprule
        \multirow{2}{*}{$k$} & \multicolumn{5}{c}{$\varphi$} \\
        & $0.1$ & $0.25$ & $0.5$ & $0.75$ & $1$\\
    \midrule

    $0$ & $5h\; 2'$ & $2h\; 55'$ & $1h\; 53'$ & $1h\; 27'$ & $1h\; 18'$ \\
    $2$ & $6h\; 3'$ & $3h\; 48'$ & $2h\; 59'$ & $2h\; 42'$ & $2h\; 34'$ \\
    $4$ & $6h\; 50'$ & $4h\; 42'$ & $3h\; 56'$ & $3h\; 39'$ & $3h\; 32'$ \\
    $8$ & $8h\; 23'$ & $6h\; 53'$ & $5h\; 54'$ & $5h\; 30'$ & $5h\; 19'$ \\
    
    \bottomrule

    \end{tabular}
    \label{tab:size-time-friendster}
\end{table}

%for a given algorithm, the average update time $T(k)$ after $k$ edge insertions is $
%\frac{\sum_{i=1}^k T_i}{k}$, where $T_i$ denote the time needed by the algorithm to %process the $i$-th edge inserted. In this case, for each dataset, $k$ corresponds to the %total number of edges that are inserted.