\section{Introduction}
Image-to-point cloud cross-modal Visual Place Recognition (cross-modal VPR) involves querying a LiDAR point cloud database using an RGB image captured by a camera. Cameras are cost-effective and widely deployed in vehicles, making them ideal for online queries, while LiDAR provides precise spatial geometry and distance information. Cross-modal VPR addresses challenges such as environmental variations (e.g., lighting, weather, and seasonal changes) and eliminates the need for simultaneous mapping in visual SLAM systems. However, its retrieval performance lags behind single-modal VPR, primarily due to the significant modality gap between RGB images and LiDAR point clouds. RGB images capture dense color and texture details, whereas LiDAR point clouds provide accurate spatial data. Existing methods bridge this gap using intermediate modalities or similarity labels for supervision.

\begin{figure}[]
    \centering
    \includegraphics[scale=0.1]{overview_v3.png}
    \caption{Illustration of our image-to-point cloud cross-modal visual place recognition. It's mainly composed of two separate similarity search process by only using global descriptors, in this way, we can effectively combine the information from range (or RGB) images and Bird's Eye View (BEV) images, significantly reducing the modality gap.}
    \label{fig:overview}
\end{figure}

Previous works have explored intermediate modalities to reduce the modality gap. For example, i3dLoc \cite{yin2021i3dloc} predicts pseudo range images from RGB images, while (LC)\textsuperscript{2} \cite{lee20232} generates depth maps from RGB images, which I2P-Rec \cite{zheng2023i2p} converts into pseudo point clouds for BEV images. ModaLink \cite{xie2024modalink} densifies depth projections from LiDAR point clouds to replace range images. Recent studies, such as LIP-Loc \cite{shubodh2024lip} and CMVM \cite{yao2024monocular}, align global descriptors of RGB and range images to reduce retrieval difficulty. Despite these efforts, the modality gap persists. To address this, we propose a strategy that combines the strengths of range (or RGB) images and BEV images, minimizing the gap's adverse effects.

Supervision with precise labels is another mainstream approach. (LC)\textsuperscript{2} \cite{lee20232} uses a sector area-based overlap ratio as a similarity label, combined with generalized contrastive loss. CMVM \cite{yao2024monocular} adopts a pixel-based overlap ratio from OverlapNet \cite{chen2021overlapnet} and predefined thresholds for binary labels. However, these methods suffer from inaccurate similarity approximations and lack continuous similarity label loss functions suitable for small datasets. To overcome these limitations, we propose a novel similarity label supervision method that provides more precise labels and ensures effective training with limited data.

In this work, we propose a fused pipeline leveraging range images and BEV images to enhance cross-modal VPR performance. As shown in Fig. \ref{fig:overview}, our method operates in two phases. First, we retrieve top-k candidates by computing global descriptor similarity between the query RGB image and range images of database submaps. Then, for the top-k candidates, we compute similarity between the camera BEV image and LiDAR BEV images. Finally, we perform a weighted summarization of the two ranking results. Automotive sensors are typically mounted horizontally, so range and RGB images capture vertical content, while BEV images provide a top-down view of spatial distribution. Our pipeline combines these modalities effectively, reducing computational costs.

Additionally, we propose a novel similarity label supervision method, including points average distance, a distance-based similarity generation method for more accurate appearance similarity approximation. We also adapt vanilla triplet loss for limited data training, ensuring robust supervision and improved performance.

Our contributions are as follows:
\begin{itemize}
  \item We propose an initial retrieval + re-rank pipeline that combines the strengths of range (or RGB) images and BEV images, using only global descriptors for robust VPR.
  \item We introduce a novel similarity label supervision approach, incorporating points average distance and generalized triplet loss, designed to extract meaningful patterns from limited datasets.
  \item Extensive experiments on the KITTI dataset demonstrate that our method outperforms state-of-the-art (SOTA) methods, setting a new benchmark in cross-modal VPR.
\end{itemize}