\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.14]{pipeline_v1.png}
    \caption{The training pipeline to produce the range (or RGB) and BEV descriptors. raw data (e.g., LiDAR point cloud, camera RGB image) are preprocessed to reduce modality differences and improve the overlap in visual content. Featrue maps are generated by extracting features from RGB, LiDAR range, camera BEV and LiDAR BEV, which are latter aggregated by the Generalized Mean (GeM) pooling to abtain global descriptors. It's worth noting that we use the points average distance together with a generalized triplet loss to supervise the learning process and fully utilize the limited training data}
    \label{fig:pipeline}
\end{figure*}
\section{Methods}
\subsection{Overview}
This research presents a framework, seen in Fig. \ref{fig:overview}, that utilizes a two-stage approach consisting of initial retrieval and subsequent re-ranking to improve the precision of location recognition. Significantly, global descriptors are solely employed in both phases. The training pipeline, seen in Fig. \ref{fig:pipeline}, commences with data preparation to produce four image types: RGB images, range images obtained from LiDAR point clouds, BEV images created from RGB photos, and BEV images generated from LiDAR point clouds. These images are processed through ResNet50 \cite{he2016deep} backbones to extract feature maps, which are then aggregated by using the GeM pooling \cite{radenovic2018fine} method to obtain four kinds of global descriptors. A unique similarity label generation method and a modified triplet loss \cite{schroff2015facenet} are utilized to enhance contrastive learning by supervising the relationship between RGB images and LiDAR range images, as well as between camera BEV images and LiDAR BEV images. The specifics of each component will be further upon in later sections.

\subsection{Feature Extraction and Aggregation}
Utilizing the data preprocessing methods described in LIP-Loc \cite{shubodh2024lip} and I2P-Rec \cite{shubodh2024lip}, we produce the subsequent inputs: $I_{RGB} \in \mathbb{R}^{H_1{\times}W_1{\times}3}$, $I_{range} \in \mathbb{R}^{H_2{\times}W_2{\times}3}$, $I_{camera\_BEV} \in \mathbb{R}^{H_3{\times}W_3{\times}3}$, $I_{LiDAR\_BEV} \in \mathbb{R}^{H_3{\times}W_3{\times}3}$. It's worth noting that we filter out ambiguous depth information and remove the ground points during data preprocessing to reduce noise, the details are in the Appendix. Subsequently, the ResNet50 encoder network processes these inputs to extract feature maps, resulting in the outputs: $F_{RGB} \in \mathbb{R}^{7{\times}7{\times}C_1}$, $F_{range} \in \mathbb{R}^{4{\times}24{\times}C_2}$, $F_{camera\_BEV} \in \mathbb{R}^{4{\times}4{\times}C_3}$, $F_{LiDAR\_BEV} \in \mathbb{R}^{4{\times}4{\times}C_3}$. Subsequently, a multi-layer perceptron (MLP) is utilized as a dimensionality reduction network to map all feature mappings into a cohesive 256-dimensional space. The generalized mean pooling (GeM pooling) approach is ultimately employed to consolidate these feature maps, yielding the global descriptors: $G_{RGB} \in \mathbb{R}^{256}$, $G_{range} \in \mathbb{R}^{256}$, $G_{camera\_BEV} \in \mathbb{R}^{256}$, $G_{LiDAR\_BEV} \in \mathbb{R}^{256}$.

\subsection{Similarity Label Supervision}
This section introduces our similarity label supervision approach, which consists of two key components: similarity label creation and the similarity loss function.

GCL \cite{leyva2023data} estimates visual similarity using the ratio of overlapping sector areas. However, this method has limitations. First, calculating overlapping areas is computationally complex, preventing the use of efficient similarity search engines like Faiss \cite{douze2024faiss} and requiring an overlap ratio matrix with $O(n^2)$ spatial complexity, which is impractical for large-scale VPR datasets. Second, the sector area overlap ratio is a coarse approximation of appearance similarity, as it fails to capture the intricate distribution of visual features in 3D space.
 
\begin{figure}[]
    \centering
    \includegraphics[scale=0.24]{similarity_label_supervision_vis_v2.png}
    \caption{Examples of three submaps with similarity value in a decreasing order. RGB image and LiDAR Point Cloud is cropped to maximize the visual content overlapping. In the third column, red lines connecting corresponding blue and green points indicate the points distances, which are later processed for obtaining the final similarity value.}
    \label{fig:SimLabelSupVis}
\end{figure}
For similarity label creation, we propose a distance-based similarity label generation method called points average distance, which provides a more precise similarity measure. As illustrated in Fig. \ref{fig:SimLabelSupVis}, we replace a single point coordinate with a set of uniformly sampled points within a defined area. Each point's location is fixed relative to the ego vehicle's coordinate system, and points in different sets have a one-to-one correspondence. The similarity value is computed as follows:
\begin{equation}
\begin{gathered}
D_{avg}(i,j)={\frac{1}{n}}{\sum_{k=1}^{n}}{{||{Coord_{i,k}-Coord_{j,k}}||}_2} \\
{Sim(i,j)=
\begin{cases}
\frac{D_{th}-D_{avg}(i,j)}{D_{th}},& if D_{avg}(i,j)<{D_{th}} \\
0,& else
\end{cases}}
\end{gathered}
\end{equation}
\indent Here, $D_{avg}(i,j)$ is the average Euclidean distance between corresponding point pairs, and $D_{th}$ is a threshold determining the final similarity value $Sim(i,j)$.

For similarity loss function, we propose generalized triplet loss, which adapts the margin based on similarity labels. Unlike conventional triplet loss \cite{schroff2015facenet} with a fixed margin, our approach uses an adaptive margin, accommodating the non-binary nature of similarity labels.

To elaborate, we consider three samples: an anchor sample $x_a$, a random sample $x_1$, and another random sample $x_2$. The similarity between $x_a$ and $x_1$ is denoted as $sim_{a1}$, while the similarity between $x_a$ and $x_2$ is denoted as $sim_{a2}$. If $sim_{a1} > sim_{a2}$, $x_1$ is treated as a relative positive sample $x_{rp}$, and $x_2$ is treated as a relative negative sample $x_{rn}$. The difference in feature distances between these samples is adjusted based on their similarity variation. We introduce a similarity difference term, combined with a base margin ${\alpha}_{base}$, to define the adaptive margin. The final loss function is formulated as:
\begin{equation}
\mathcal{L}=max(D(a, rp)-D(a, rn)+{\alpha}_{base}\cdot{(sim_{arp}-sim_{arn})},0)
\end{equation}

\subsection{Initial Retrieval and Re-rank}
Aligning the global descriptors of range images and RGB images guarantees uniformity in visual information along the vertical axis. Nonetheless, this alignment alone is inadequate for the model to deduce the horizontal spatial distribution and conduct further comparisons. To mitigate this constraint, we integrate BEV images from camera and LiDAR sensors. Inspired by the prevalent initial retrieval and re-ranking pipeline \cite{shao2023global}, we present a computationally efficient solution that eliminates the need for an auxiliary feature matching network or spatial verification process.
\begin{figure}[]
    \centering
    \includegraphics[scale=0.11]{re_rank_v2.png}
    \caption{The initial retrieval + re-rank pipeline. In the two-phase similarity search, global descriptors with higher similarity, indicated by closer similarity in color, are ranked higher. By combining the rankings from both phases, we improve the precision of retrieval. This results in true positive samples being ranked higher (indicated by global descriptors in green boxes) and false positive samples being ranked lower (indicated by global descriptors in red boxes).}
    \label{fig:rerank}
\end{figure}
As depicted in Fig. \ref{fig:rerank}, our methodology is executed as follows: Initially, we utilize the global descriptor $G_{RGB}$ to do retrieval within the $G_{range}$ database, yielding the top-k most analogous LiDAR submaps as potential matches, with the highest rank being documented. In our trials, the variable $k$ is established at 60. Subsequently, we employ the global descriptor $G_{camera\_BEV}$ as a query to search among the top-k candidates from the $G_{LiDAR\_BEV}$ collection, resulting in a secondary ranking. The two ranks are ultimately weighted and aggregated to yield the final re-ranked outcome.

Our methodology circumvents the computationally intensive tasks of feature matching (\cite{sarlin2020superglue}, \cite{sun2021loftr}, \cite{lindenberger2023lightglue}, \cite{wang2024efficient}) and spatial verification (\cite{lee2022correlation}, \cite{xue2022efficient}) found in conventional re-ranking algorithms (\cite{wang2022transvpr}, \cite{zhang2023etr}, \cite{zhu2023r2former}, \cite{hausler2021patch}) in VPR. Moreover, unlike query expansion \cite{radenovic2018fine} approaches typically employed in image retrieval, our approach necessitates searching solely inside the top-k candidates instead of the full database. Experimental findings indicate that combining global descriptors from adjacent samples diminishes retrieval performance. This is due to the distinctive feature distribution that maintains the same neighborhood relationships \cite{luo2023bevplace} as the geographic distribution of scenes, easily introducing noise when there exists a false positive sample in the neighborhood \cite{zaffar2024estimation}.