\section{Experiments}

\begin{table*}[!ht]
    \centering
    \caption{RECALL@N (\%) ON THE KITTI DATASET. THE BEST RESULTS IN EACH COLUMN ARE HIGHLIGHTED IN BOLD, AND THE SECONDS ARE UNDERLINED.}
    \label{tab:CompSOTA}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
         & KITTI-00 & KITTI-02 & KITTI-05 & KITTI-06 & KITTI-07 & KITTI-08 \\ \hline
        Method & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% \\ \hline
        PlainEBD \cite{cattaneo2020global} & 19.93 / 31.71 / 69.06 & 16.50 / 27.48 / 55.93    & 30.64 / 46.11 / 71.10 & 25.34 / 35.88 / 47.68 & 39.95 / 59.30 / 70.93  & 20.49 / 35.96 / 72.22 \\
        (LC)\textsuperscript{2} \cite{lee20232} & 30.65 / 47.76 / 84.52 & 23.06 / 35.72 / 67.41 & 40.49 / 54.58 / 80.62 & 39.78 / 58.22 / 71.48 & 52.50 / 68.03 / 84.01 & 38.42 / 54.07 / 87.30 \\
        I2P-Rec \cite{zheng2023i2p} & 44.84 / 59.55 / 90.09 & 28.00 / 42.33 / 71.68 & 48.32 / 64.61 / 88.52 & 43.51 / 63.12 / 72.30 & 63.03 / 73.39 / 78.93 & 45.91 / 62.44 / 91.65 \\
        VXP \cite{li2024vxp} & 24.22 / 38.16 / 80.00 & 17.72 / 30.83 / 64.64 & 32.81 / 50.63 / 78.16 & 29.97 / 41.96 / 52.50 & 43.69 / 61.31 / 76.29 & 24.01 / 37.68 / 79.17 \\
        ModaLink \cite{xie2024modalink} & 90.31 / 95.53 / 99.58 & 70.18 / 81.85 / 96.74 & 90.00 / 95.00 / \underline{99.31} & 86.19 / 96.64 / \underline{100.0} & 96.55 / 99.27 / \underline{100.0} & 86.97 / \underline{95.75} / \underline{99.93} \\
        CMVM \cite{yao2024monocular} & \underline{93.13} / \underline{96.83} / \underline{99.74} & \underline{76.98} / \underline{86.44} / \underline{97.51} & \underline{90.76} / \underline{96.09} / 99.24 & \underline{89.28} / \underline{97.73} / 99.18 & \underline{98.09} / \underline{99.46} / 99.82 & \underline{90.64} / 95.23 / 99.63 \\
        Ours & \textbf{99.03} / \textbf{99.91} / \textbf{100.0} & \textbf{95.07} / \textbf{98.22} / \textbf{99.66} & \textbf{98.91} / \textbf{99.53} / \textbf{100.0} & \textbf{97.00} / \textbf{100.0} / \textbf{100.0} & \textbf{99.91} / \textbf{100.0} / \textbf{100.0} & \textbf{98.21} / \textbf{99.51} / \textbf{99.93} \\ \hline
    \end{tabular}}
\end{table*}

In this section, we first provide the implementation details, followed by a description of the dataset and evaluation metrics. We then compare the retrieval performance of our method with baseline approaches, and conclude with an ablation study to validate the effectiveness of the proposed methods.

\subsection{Experimental Settings}

\textbf{Implementation Details}. Experiments were conducted on an Intel i9-10920X CPU and an NVIDIA RTX 3090 GPU. We use the Depth Anything model \cite{yang2024depth}, pre-trained on the KITTI dataset, for monocular depth prediction. As in \cite{leyva2023data}, the sector area is utilized for similarity label generation, with a circumcentric angle of 90° (matching the camera’s horizontal field of view) and a radius of 10m, which corresponds to the true positive distance threshold in prior cross-modal VPR work. The point set distance threshold is set to 7.5m, and the base margin is set to 0.6. It is important to note that we use pretrained ResNet50 weights on the Cityscapes dataset to extract features from RGB images.  

\textbf{Dataset}. The KITTI Odometry dataset \cite{geiger2012we}, a widely used dataset in cross-modal Visual Place Recognition tasks, serves as the primary dataset for training and evaluation. The dataset consists of 22 sequences, where the pose for each RGB image frame and LiDAR point cloud frame is annotated using combined location information from INS and GPS. Following the setup in \cite{yao2024monocular}, we use the last 11 sequences for training and the first 11 sequences for testing.

\textbf{Evaluation Protocol}. For evaluation, we employ the \textbf{recall at Top-N} and \textbf{recall at Top-1\%} as our metrics. Following previous works \cite{zheng2023i2p}, \cite{xie2024modalink}, and \cite{yao2024monocular}, a match is considered successful if the distance between the query’s ground truth location and the candidate’s ground truth location is within a true positive distance threshold, \( t = 10 \text{m} \).

\subsection{Comparison with State-of-the-Arts Methods}
In this section, we compare our method with several SOTA image-to-point cloud place recognition methods: PlainEBD \cite{cattaneo2020global}, I2P-Rec \cite{zheng2023i2p}, VXP \cite{li2024vxp}, ModaLink \cite{xie2024modalink} and CMVM \cite{yao2024monocular}. Tabel \ref{tab:CompSOTA} shows the quantitative results on KITTI Odometry dataset. We can see that ModaLink and CMVM both show very high recognition performance compared with other baseline methods and they both use the RGB Image and the LiDAR Range Image as the intermediate modality input, same as our original setting. However, with the aid of the proposed similarity label supervision and BEV global descriptor re-ranking, our method achieve an impressive State-of-the-Arts performance in every sequence. We surpass the previous SOTA by 18.09\% on Recall@1 in KITTI-02 and reach 99\% on Recall@1 in KITTI-00 and KITTI-07. The proposed similarity label supervision can approximate the appearance similarity very well and successifully make the model identify the appearance nuances among samples by introducing the generalized triplet loss.

\subsection{Ablation Study}
In this section, we evaluate the effectiveness of the proposed components in our method through several experiments.

\begin{table}[!t]
    \centering
    \caption{ABLATION STUDIES ON KITTI-00 FOR SIMILARITY LABEL SUPERVISION AND BEV DESCRIPTOR RE-RANK}
    \label{tab:DiffComp}
    \begin{tabular}{c|c|c|c}
    \hline
         Vallina & Sim SV & BEV Re-rank & R@1 / R@5 / R@1\% \\ \hline
         \checkmark &  &  & 68.40 / 82.18 / 97.01 \\
         \checkmark & \checkmark &  & 98.63 / 99.82 / 100.0 \\
         \checkmark &  & \checkmark & 81.83 / 90.73 / 97.60 \\
         \checkmark & \checkmark & \checkmark & 99.03 / 99.91 / 100.0 \\ \hline
    \end{tabular}
\end{table}

\textbf{Ablation on Different Components}. We conducted experiments by removing their individual components, including similarity label supervision (Sim SV) and BEV global descriptor re-ranking (BEV Re-rank). As shown in Table \ref{tab:DiffComp}, introducing similarity label supervision leads to a significant performance improvement of 30.23\% in Recall@1 on the KITTI-00 sequence. This demonstrates the importance of the points average distance and generalized triplet loss in the framework. Furthermore, using BEV global descriptors from both the RGB and LiDAR sensors for a second retrieval within the top-k candidates leads to a performance increase of 13.43\% in Recall@1. Combining these two methods achieves a remarkable Recall@1 of 99.03\%, validating the overall effectiveness of our pipeline.

\textbf{Ablation on Similarity Label Supervision}. 
In this part, we separately evaluate the Similarity Label Generation and Similarity Loss Function.

For similarity label generation, We compare the points average distance with other similarity label generation methods in Table \ref{tab:DiffSLG}. (position, velocity vector) \cite{ge2024bev2pr} method calculates binary labels based on the distance between points' locations and heading angles, leading to poor performance. Point cloud mnn \cite{leyva2023data}, designed for indoor point cloud submaps, doesn't perform well for outdoor scenes. Exponential negative distance, a method we proposed earlier, uses an exponentiation operation on the distance between two ground truth locations but introduces an additional hyperparameter. This method yields performance comparable to the area overlap \cite{leyva2023data} method. By introducing the average distance of point sets, our points average distance method consistently outperforms state-of-the-art approaches in both KITTI-00 and KITTI-02. Appendix analyzes the impact of the distance threshold.

\begin{table}[!t]
    \centering
    \caption{ABLATION STUDIES ON DIFFERENT SIMILARITY LABEL GENERATION METHODS}
    \label{tab:DiffSLG}
    \resizebox{8.5cm}{!}{
    % \scalebox{0.9}{!}{
    \begin{tabular}{c|c|c}
    \hline
        & KITTI-00 & KITTI-02 \\ \hline
        Method & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% \\ \hline
        (Position, Velocity Vector) \cite{ge2024bev2pr} & 77.45 / 86.17 / 96.32 & 61.94 / 73.12 / 87.99 \\
        Area Overlap \cite{leyva2023data} & 98.26 / 99.80 / 100.0 & 94.23 / 97.55 / 99.55 \\
        Point Cloud NN \cite{leyva2023data} & 95.75 / 98.57 / 99.91 & 86.35 / 91.44 / 95.67 \\
        Exponential Negative Distance & 98.00 / 99.87 / 100.0 & 94.66 / 98.41 / 99.49 \\
        Points Average Distance & 99.03 / 99.91 / 100.0 & 95.07 / 98.22 / 99.66 \\ \hline
    \end{tabular}
    }
\end{table}

\begin{table}[!t]
    \centering
    \caption{ABLATION STUDIES ON DIFFERENT LOSS FUNCTIONS}
    \label{tab:DiffLoss}
    \resizebox{8.5cm}{!}{
    \begin{tabular}{c|c|c}
    \hline
        & KITTI-00 & KITTI-02 \\ \hline
        Method & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% \\ \hline
        Generalized Contrastive Loss \cite{leyva2023data} & 83.46 / 93.50 / 98.72 & 64.60 / 77.56 / 88.39 \\
        Triplet Margin Loss \cite{schroff2015facenet} & 96.45 / 98.79 / 99.91 & 85.71 / 91.29 / 97.04  \\
        Generalized Triplet Margin loss & 99.03 / 99.91 / 100.0 & 95.07 / 98.22 / 99.66 \\ \hline
    \end{tabular}
    }
\end{table}

\begin{table}[!t]
    \centering
    \caption{ABLATION STUDIES ON FUSION METHODS}
    \label{tab:AblationFM}
    \resizebox{8.5cm}{!}{
    \begin{tabular}{c|c|c}
    \hline
        & KITTI-00 & KITTI-02 \\ \hline
        Method & R@1 / R@5 / R@1\% & R@1 / R@5 / R@1\% \\ \hline
        Cat & 98.85 / 99.98 / 100.0 & 94.77 / 97.79 / 99.94 \\
        Global Descriptor + Attention \cite{zhang2024mvse} & 97.36 / 99.63 / 100.0 & 89.62 / 95.56 / 99.72 \\
        Local Feature + Attention & 98.48 / 99.76 / 100.0 & 92.83 / 96.82 / 100.0 \\
        Global Descriptor + Re-rank & 99.03 / 99.91 / 100.0 & 95.07 / 98.22 / 99.66 \\ \hline
    \end{tabular}
    }
\end{table}

For similarity loss function, in Table \ref{tab:DiffLoss}, we compare generalized triplet loss with vanilla triplet loss and generalized contrastive loss. The results show that even vanilla triplet loss significantly outperforms generalized contrastive loss, as the latter cannot effectively capture the relative relationships between anchor-positive and anchor-negative pairs, making it unsuitable for training on smaller datasets with adjacent feature distributions. Furthermore, the introduction of adaptive margin results in a 9.36\% increase in Recall@1 for KITTI-02, demonstrating the importance of fine-grained feature similarity, additional ablation on the base margin is in the Appendix.

\textbf{Ablation on Re-rank}. 
Aligning information from the horizontal and vertical directions is a non-trivial task. As shown in Table \ref{tab:AblationFM}, besides using BEV global descriptors for re-ranking, we also explore other fusion methods. One approach involves directly concatenating the two global descriptors for retrieval, followed by applying a transformer block to extract context between the two descriptors, as done in \cite{zhang2024mvse}. Another method involves using cross-attention between the RGB and BEV feature maps before pooling and concatenation. While concatenating the two global descriptors shows a slight performance improvement, the effect is minimal. This indicates that fusion-based approaches require more complex designs to effectively combine the two perspectives. As displayed in Fig. \ref{fig:RerankVis}, our BEV global descriptor re-rank method successfully avoids viewpoint interference and increase the rank of true positive by separately using vertical and horizontal visual information. Additionally, in the Appendix, we do some ablation experiments to emphasize the importance of the ground and noise removal process.

\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.18]{re_rank_vis_v3.png}
    \caption{Examples of initial retrieval and re-rank results on KITTI dataset.}
    \label{fig:RerankVis}
\end{figure}