\section{Conclusion}
In this paper, we address the challenge of image-to-point cloud cross-modal VPR and propose an innovative initial retrieval + re-rank method. This method effectively combines information from range (or RGB) images and BEV images in a computationally efficient manner to bridge the modality gap. Our approach exclusively uses a global descriptor similarity search process for re-ranking, thereby avoiding mutual interference between the two modalities.
Furthermore, we present a novel similarity label supervision technique to optimize the use of limited training data. By introducing the points average distance metric, we closely approximate appearance similarity, and the generalized triplet loss dynamically adjusts the margin based on the similarity difference between sample pairs.
Experimental results on the KITTI dataset validate that our method achieves significant improvements over state-of-the-art approaches. Future research will explore a more practical VPR scenario where the query is an RGB image and the database samples consist of both LiDAR point clouds and RGB images. 