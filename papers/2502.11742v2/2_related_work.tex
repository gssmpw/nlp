\section{Related Work}
In GNSS-based global positioning, vehicles rely on satellites and base stations for initial localization. However, satellite signals are often obstructed in urban areas, and base station coverage is limited in remote regions, increasing latency and hindering real-time applications. Visual Place Recognition (VPR) addresses these limitations by using visual sensors for robust global positioning. This section reviews single-modal and cross-modal VPR techniques, focusing on RGB images and point clouds, and discusses advancements in similarity label supervision.

\subsection{Single-Modal Visual Place Recognition}
Single-modal VPR relies on a single sensor type for place recognition. Early methods used hand-crafted features like VLAD \cite{jegou2010aggregating} and SIFT \cite{lowe1999object} for RGB image-based VPR, leveraging cameras' cost-effectiveness and widespread use. These methods assume similar local features share appearances, with feature centers representing general scene cues. Residuals between local features and centers are aggregated into global descriptors. With deep learning, CNNs emerged for feature extraction. NetVLAD \cite{arandjelovic2016netvlad}, a neural adaptation of VLAD, refined feature centers and introduced soft weight assignment, improving performance. Generalized Mean Pooling (GeM) \cite{radenovic2018fine} further enhanced feature aggregation. For point cloud-based VPR, PointNetVLAD \cite{uy2018pointnetvlad} combined PointNet \cite{qi2017pointnet} with VLAD, enabling effective LiDAR data use. LiDAR provides precise distance measurements, avoiding RGB-based issues like perception aliasing and weather sensitivity. Advanced backbones, such as Transformer \cite{waswani2017attention}, improved feature extraction, while aggregators like (\cite{izquierdo2024optimal}, \cite{lu2024supervlad}, \cite{khaliq2024vlad}, \cite{ali2024boq}), enhanced global descriptor discrimination. Large-scale datasets like (\cite{warburg2020mapillary}, \cite{berton2022rethinking}, \cite{ali2022gsv}, \cite{alibeigi2023zenseact}), have advanced practical VPR applications, with frameworks like (\cite{berton2025meshvpr}, \cite{berton2024earthloc}, \cite{kolmet2022text2pos}, \cite{keetha2023anyloc}, \cite{vivanco2024geoclip}, \cite{xu2024addressclip}), expanding its scope.

\subsection{Cross-Modal Visual Place Recognition}
Cross-modal VPR involves querying a LiDAR point cloud database using an RGB image's global descriptor to retrieve the most similar location. The primary challenge is bridging the modality gap: RGB images capture color and texture, while point clouds provide spatial and distance data. Early approaches, such as PlainEBD \cite{cattaneo2020global}, aligned global descriptors directly using metric learning and knowledge distillation. Subsequent works, like i3dLoc \cite{yin2021i3dloc}, introduced range images as an intermediate modality to improve retrieval. (LC)\textsuperscript{2} \cite{lee20232} removed color information by estimating depth from RGB images, while ModaLink \cite{xie2024modalink} proposed field-of-view (FoV) clipping to enhance content overlap, though this struggles with viewpoint variations. Recent studies, such as CMVM \cite{yao2024monocular}, split 360Â° LiDAR point clouds into sections, improving retrieval accuracy despite increased database size. I2P-Rec \cite{zheng2023i2p} utilized BEV images, generated via depth estimation, achieving competitive performance. While range images excel in vertical geometric detail, BEV images provide a comprehensive horizontal perspective but lack fine-grained detail. This work integrates both modalities for a more robust solution.

\subsection{Similarity Label Supervision}
Similarity label supervision enhances retrieval in metric learning. Traditional methods rely on binary labels derived from GPS or UTM distances, often overlooking visual content overlap. The MSLS \cite{warburg2020mapillary} dataset improved supervision by incorporating heading angles. BEV\textsuperscript{2}PR \cite{ge2024bev2pr} adjusted GPS coordinates along the heading direction but still used binary labels. GCL \cite{leyva2023data} introduced refined labeling using sector area overlap for outdoor scenes and point cloud mutual nearest neighbor overlap for indoor scenes, along with a modified contrastive loss for continuous labels. OverlapNet \cite{chen2021overlapnet} used range image overlap for supervision. Our work proposes an enhanced similarity labeling approach to improve model performance.