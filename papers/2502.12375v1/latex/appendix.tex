\clearpage

\section{Hard Attributes}
% \input{latex/tables/followbench}
\label{sec:hard_attr}

\input{latex/tables/verifiable_attributes}

The hard attributes employed in this study, as detailed in Table \ref{tab:list-of-verifiable-instruction}, comprise a set of verifiable instructions designed to enforce precise, programmatically assessable constraints on text generation. These attributes are categorized into four primary groups: (1) Keywords, which mandate the inclusion or frequency of specific terms (e.g., "Include \{keyword1\}" or "appear \{N\} times"); (2) Length Constraints, governing structural requirements such as paragraph count, word limits, or sentence boundaries; (3) Change Cases, enforcing syntactic rules like all-uppercase or all-lowercase formatting; and (4) Positional Directives, such as starting responses with predefined phrases. Each attribute is selected for its objective verifiability through rule-based checks while also reflecting common real-world application scenarios, such as compliance with stylistic guidelines or technical specifications. By anchoring the evaluation in these deterministic constraints, the framework guarantees rigorous assessment of model adherence to fine-grained requirements, aligning with the dataset's emphasis on combinatorial complexity and practical utility.

\section{Generalization to Unseen Attributes}
We evaluate the models’ ability to generalize to unseen, more challenging attributes, focusing on two types:
\begin{enumerate} \item \textbf{Absolute Position of a Word}: The k-th (k $\le$ 5) word in the text must be A.
\item \textbf{Relative Position Between Two Words}: Word A must appear before word B.
\end{enumerate}
We use text from the FineWeb validation set and extract 50 attributes per document, focusing on these two types of harder attributes. We then evaluate the three models on this benchmark. The results show that the original model achieves a score of 21.56, while auto-reconstruct slightly reduces performance to 20.79. However, incorporating GPO alongside AR improves generalization, yielding a score of 24.05, suggesting that GPO enhances the model’s ability to handle these harder constraints.

% \section{Instruction-following Capabilities}
% \longfei{maybe retest this benchmark}

\section{Dataset Statistics}
\label{appendix:statistics}
\input{latex/tables/statistics}
In this section, we present detailed statistics of our dataset, including a comparison with existing datasets, quality control evaluation, and the composition of our multi-sources subset.


\paragraph{Comparison with Existing Datasets.}
\input{latex/tables/comparison}
Table~\ref{tab:comparison} provides a comparison between our dataset and several representative constraint-based datasets, including IFeval~\cite{zhou2023instruction}, FollowBench~\cite{jiang2023followbench}, CFBench~\cite{zhang2024cfbench}, and InFoBench~\cite{qin2024infobench}. Our dataset distinguishes itself with a significantly larger number of samples (6,159) and a notably higher average number of attributes per instance (45.9). Unlike prior datasets, which primarily rely on either human annotations or simple constraints, our data features a rich combination of both hard and soft constraints, offering a more challenging and comprehensive benchmark. Importantly, our data is not synthesized, ensuring its alignment with real-world use cases.

\paragraph{Domain Composition in Multi-sources Subset}
\input{latex/tables/stats}
\begin{figure}[htbp]
    \centering
        \includegraphics[width=.5\textwidth]{figs/rl_distribution.pdf}
    \caption{Proportion of Attributes Across Different Data Domains. The bar chart visualizes the relative contribution of each domain to the multi-sources subset, highlighting a balanced distribution across various sources such as web data, forums, papers, books, and Wikipedia.}
    \vspace{-1em}
    \label{fig:rl_distribution}
\end{figure}
Our multi-sources subset is constructed from a diverse range of data sources, encompassing web data, forums, academic papers, books, and Wikipedia. Figure~\ref{fig:rl_distribution} illustrates the proportion of attributes contributed by each domain, highlighting a balanced distribution across these categories. Table~\ref{tab:rl_domains} further details the exact composition, showing that no single source overwhelmingly dominates, ensuring robustness and variety in downstream tasks.

\paragraph{Quality Control Metrics}
Maintaining data quality is critical for ensuring reliable evaluations. We assess the agreement rate (AR) between human annotators and the final dataset as a key metric. As summarized in Table~\ref{tab:quality_metrics}, the FineWeb subset achieves an AR of 92.3\%, while the multi-sources subset attains 88.7\%. These high agreement rates reflect the robustness of our data curation process, confirming that both subsets align closely with human judgment.
\input{latex/tables/quality_control}

\section{DPO data quality} 
\input{prompt/high_correlaton}
In this section, we showcase some examples sampled by our global selection strategy.
\paragraph{High Correlation} 
Our attribute correlation modeling step aims to select semantically coherent and mutually reinforcing attributes during GPO training. This process effectively groups attributes that frequently co-occur in natural text, leading to the selection of high-quality attribute combinations. 

\paragraph{Low Similarity}
\input{prompt/similarity}
While high correlation ensures that attributes are semantically aligned, it is equally important to maintain attribute diversity to prevent redundancy and overfitting. Our global selection strategy aims to minimize the presence of highly similar attributes within the same prompt. For instance, attributes like \emph{“Engaging Headline”} and \emph{“Attention-Grabbing Title”} convey nearly identical meanings and offer little additional training value when paired together. By prioritizing low-similarity combinations, we encourage the model to generalize across a broader range of attribute expressions, improving its adaptability to diverse prompts.

\begin{figure}[htbp] 
    \centering
        \includegraphics[width=0.5\textwidth]{figs/dpo_score_distribution.pdf}
    \caption{Score distributions of chosen and rejected data.}
    \label{fig:dpo_score_distribution}
\end{figure}
To further illustrate the effectiveness of our sampling strategy in the GPO stage, we present several representative cases selected by our attribute-based sampling approach. These examples demonstrate the diversity and coverage achieved through our strategy, highlighting both common and edge-case attribute combinations. 

\section{Prompts Used in This Study}
We employ three distinct prompts to support different stages of our EFCG pipeline: Decomposition, Judging, and Generation.

\paragraph{Decomposition Instruction.}
\input{prompt/decompose}
This prompt is used to extract a set of soft attributes from a given text. The goal is to decompose the paragraph into its most defining characteristics, capturing both stylistic and semantic elements. Models are instructed to focus on identifying specific, explicit features of the text rather than relying on generic descriptions or subjective interpretations. Attributes must reflect the unique aspects of the text and be grounded in the content. 

\paragraph{Judge Instruction.}
\input{prompt/score}
This prompt serves as a binary evaluation guideline to determine whether a generated text satisfies a given set of attributes. Evaluators are asked to assess each attribute independently, assigning a score of 1 if the text explicitly fulfills the attribute and 0 otherwise. The evaluation is strict, requiring the text to directly align with the specified attribute for a positive score.

\paragraph{Generation Instruction.}
\input{prompt/generate}
This prompt is used to instruct the language model to generate a piece of text that aligns with a provided set of hard constraints and soft attributes. Hard attributes typically represent structural or factual constraints (e.g., budget, schedule), while soft attributes reflect stylistic or semantic preferences (e.g., tone, vividness). The model is guided to generate text that adheres to as many of these attributes as possible, balancing the satisfaction of both hard and soft constraints.

%\section{More Attention Flow Visualization}

\section{The Complete Case Study}
\label{appendix:case_study}
\input{prompt/case_study}
The travel planner case study exemplifies the practical usefulness of EFCG in handling complex, multi-faceted requirements. As shown in Table~\ref{tab:travel_planner_case}, generating a 5-day travel itinerary involves satisfying a diverse set of hard attributes (e.g., budget limits, time scheduling, location constraints) alongside soft attributes (e.g., tone, emotion, visual details), while also adapting to real-time factors like weather and physical endurance. Such a task necessitates precise control over both hard and soft constraints, making it a natural testbed for evaluating EFCG systems.
% \longfei{showcase the result later}



