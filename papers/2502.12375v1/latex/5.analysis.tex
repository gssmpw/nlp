\section{Analysis}

\subsection{UltraBench Mitigates Performance Degradation Across Different Positions}
\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.49\textwidth]{figs/lost_in_the_middle.pdf}
    \caption{Hard score across different positions, showing that our approach (AR+GPO) effectively mitigates performance degradation.}
    \vspace{-1em}
    \label{fig:lost_in_the_middle}
\end{figure}
Our method effectively mitigates the sensitivity to positional changes in hard attributes. As shown in Figure ~\ref{fig:position_bias}, baseline models such as Llama-3.1-8B-Instruct and Qwen2-7B-Instruct exhibit a significant drop in hard scores as the position increases, indicating a degradation in performance when hard attributes appear later in the input. In contrast, our approach significantly stabilizes performance across all positions (Figure ~\ref{fig:lost_in_the_middle}). The introduction of AR already improves robustness compared to the original model, and the addition of GPO further enhances consistency, maintaining high hard scores even at later positions. This demonstrates that our approach effectively addresses the position sensitivity issue, ensuring more reliable model performance regardless of attribute placement.
\subsection{A Real-world Travel Case}
\label{sec:case_study}
To further evaluate our approach, we analyze a real-world travel planning scenario where the itinerary must satisfy over thirty attributes. One crucial constraint is that \textit{each activity should be less than two hours long}. The authors examined the response generated by three models. We observe that only the AR+GPO model consistently generates activities that adhere to this constraint, whereas the original model and AR model occasionally violate it. To gain deeper insights, we provide the user prompt along with a partially generated response (e.g., "14:00 PM - 1") and examine the attention flow distribution at this intermediate step. As illustrated in Figure \ref{fig:case_study}, the AR+GPO model exhibits significantly higher attention weights on constraint-related tokens (e.g., "2"), suggesting that it effectively retains and incorporates constraint-relevant information during generation. In contrast, the original model's attention weights are relatively weak, indicating a lower degree of constraint awareness.

