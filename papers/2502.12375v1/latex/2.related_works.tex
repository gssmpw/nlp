\section{Related Work}
\paragraph{Controllable Text Generation}
CTG tasks involve hard constraints (e.g., text length, keyword inclusion)\cite{takase2019positional, carlsson2022fine} and soft constraints (e.g., sentiment, topic)\cite{gu-etal-2022-distributional, NEURIPS2022_b125999b}. Fine-tuning LLMs with instructional data improves their constraint-following ability~\cite{weller-etal-2020-learning, sanh2021multitask, mishra-etal-2022-cross, DBLP:journals/corr/abs-2402-11905}, but evaluations show LLMs often fail to meet all constraints~\cite{jiang2023followbench, qin2024infobench, ren2025step}. 
Despite this, these works primarily focus on a relatively small number of attributes or conditions, typically from 3 to 5, leaving a gap in understanding LLM's performance under more extreme requirements.

\paragraph{Evaluation of CTG}
Evaluating LLM's adherence to constraints is challenging and typically involves automatic and programmatic assessments using various metrics ~\cite{yao2023collie,zhou2023controlled,chen2022controllable}.~\citet{zhou2023instruction} centers on assessing 25 verifiable instructions.  ~\citet{jiang2023followbench} progressively integrates fine-grained constraints to develop multi-level instructions, thereby enhancing complexity across six distinct types. ~\citet{wen2024benchmarking} constructs a novel benchmark by synthesizing and refining data from the aforementioned benchmarks, with an emphasis on the combinatorial types of constraints. ~\citet{zhang2024cfbench} proposes a comprehensive constraint-following benchmark over 50 NLP tasks. However, none of them investigate the effects of extreme fine-grained attributes.

\paragraph{Multi-objective Alignment}
Recent work~\cite{mudgal2023controlled} focuses on balancing multiple objectives in text generation while maintaining linguistic quality. MORLHF~\cite{zhou2023beyond, rame2024rewarded} optimizes human preferences via reinforcement learning but is costly and unstable. RiC~\cite{yang2024rewards} reduces complexity by using supervised fine-tuning with multi-reward control and dynamic inference adjustment. DeAL ~\cite{huang2024deal} introduces a decoding-time alignment framework for large language models, enabling flexible customization of alignment objectives, such as keyword constraints and abstract goals like harmlessness, without requiring retraining.
%Recent work~\cite{mudgal2023controlled} focuses on balancing multiple objectives in text generation while maintaining linguistic quality. MORLHF~\cite{zhou2023beyond, rame2024rewarded} optimizes human preferences via reinforcement learning but is costly and unstable. RiC~\cite{yang2024rewards} reduces complexity by using supervised fine-tuning with multi-reward control and dynamic inference adjustment. DeAL~\cite{huang2024deal} enables flexible alignment during decoding, supporting diverse constraints without retraining.