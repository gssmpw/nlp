\section{Introduction}
\begin{figure}[t] 
    \centering
        \includegraphics[width=0.48\textwidth]{figs/teaser.pdf}
    \caption{Constraint Satisfaction Rate (CSR) across different numbers of attributes for GPT-4o and LLaMA-3.1-8B-Instruct.}
    \vspace{-1.5em}
    \label{fig:teaser}
\end{figure}

% background and current status
While large language models (LLMs) have shown promising performance in various tasks, processing massive input information remains a challenging setup~\citep{lost-in-the-middle}.
For controlled text generation (CTG) ~\cite{wang2023aligning,song2024preference}, models are often required to satisfy a certain number of constraints simultaneously. In previous works, the typical number of constraints ranges from 3 to 5.
However, when the number of constraints scales to the extreme (e.g. 30 or more), performance degrades significantly (Figure ~\ref{fig:teaser}).

One representative challenge arises in travel itinerary planning (Appendix \ref{appendix:case_study}). Consider a prompt requiring a detailed 5-day travel plan that satisfies over 30 constraints, covering timing, budget, transportation, meal preferences, and specific landmark visits. Despite LLMs’ impressive fluency, they frequently violate crucial attributes like \textit{each activity must be under 2 hours} or \textit{avoid scheduling during 1 PM to 2 PM due to lunch break}. Why do LLMs perform well in general text generation but struggle under EFCG? We hypothesize that this limitation stems from two fundamental issues.

\begin{figure*}[t]
    \centering
        \includegraphics[width=1.0\textwidth]{figs/method.pdf}
    \caption{The whole pipeline of our two-stage \textbf{UltraGen} framework. The auto-reconstruction stage constructs a large-scale dataset by extracting soft and hard attributes from web corpora and then reconstructing the raw text. The global preference optimization stage applies DPO with attribute correlation modeling and diversity selection to enhance multi-attribute generalization over a global corpus.
    }
    \label{fig:pipeline}
    \vspace{-0.5em}
\end{figure*}

First, When the number of constraints increases, later-specified conditions are more likely to be neglected, as the model’s performance exhibits position-dependent degradation~\citep{lost-in-the-middle}, meaning that attributes appearing later in the prompt are less likely to be satisfied (Figure~\ref{fig:position_bias}). Second, the pretraining and instruction-tuning corpus of LLMs involve simple prompts with a few loosely defined requirements. Models rarely encounter instances with 30+ precise constraints in a single prompt. As a result, when faced with extreme constraint setups, models fail to maintain attention across all conditions, leading to attention dilution (Figure~\ref{fig:case_study}) during generation.
%This challenge arises as the accumulation of rules in the system prompt becomes overwhelming for the model.

% \jingbo{similar to the intro, mention the typical number in CTG first, and then say the performance gets significantly worse then the number of constraints becomes much larger. People may have different feeling about ``extreme''. We'd better make it more specific.}
% \jingbo{move the figure to intro if it's mentioned in intro. Fig 2 is talking about the position, not the \# of constraints?}

% For controlled text generation (CTG), such a challenge will be faced when constraints scale up to an extreme number, e.g., the accumulation of rules written into a system prompt.
% Previous progress in CTG~\cite{wang2023aligning,song2024preference} focuses on satisfying a small set of constraints, leaving the numerous constraint cases untouched.
% We necessitate the investigation of extremely fine-grained controllable generation (EFCG) with a pioneering analysis in Figure~\ref{fig:prelim}, which shows the performance of even the most advanced LLMs drops dramatically when the constraint reaches an extreme number.

% challenge of EFCG
% The challenges posed by extreme constraints are multifaceted. With an increase number of attributes or higher levels of granularity, LLMs struggle to maintain consistency and accuracy in their outputs. This limitation stems from the inherent complexity of balancing multiple factors while adhering to stringent constraints. Moreover, traditional training pipelines and evaluation methods often fall short in addressing the nuances of extreme scenarios, further exacerbating the gap between model potential and practical performance.

% our contribution
% \jingbo{it is not clear to me what are the novelty/contributions here. It reads like we are just doing SFT and RL. But the key contribution is about how to construct the dataset without human annotation right? SFT makes me feel there are human-annotated data. Please make this part more clear. }
% To tackle these challenges, we propose a two-stage framework for EFCG. Our approach combines supervised fine-tuning (SFT) with reinforcement learning (RL) to enhance LLM's ability to handle extremely massive constraints effectively. 


To tackle the challenges, we propose a framework designed to enhance LLMs’ ability to handle a massive number of constraints effectively. 
% A key novelty of our approach lies in constructing a large-scale training dataset without human annotation, enabling the model to learn fine-grained controllability purely from synthetic supervision.
% \jingbo{I would prefer to move the Figure 1 to the top of the 2nd page, so we can refer to the figure to discuss the framework. Maybe we need to discuss the hard/soft constraints/attributes there?}
% The primary objective of this framework is to train the LLM as an expert capable of processing any constraint as input, ensuring adaptability to a wide range of fine-grained control conditions.
% The SFT stage ensures that the model learns to generate text conforming to specific constraint distributions, enabling it to follow intricate attribute-based instructions with high precision. 
% We construct an automated dataset pipeline that extracts soft attributes (e.g., style, tone) and hard attributes (e.g., keywords, structure) from large-scale corpora without human annotation. The SFT stage trains LLMs to internalize the atomic relationship between individual attributes and their textual realizations.
% \longfei{ar: foundation capabilities, rl: extend to global text}
We hypothesize that position bias arises partly due to the lack of exposure to diverse attribute positions during training. To mitigate this, we construct a large-scale automated dataset pipeline that extracts soft attributes (e.g., style, content) and hard attributes (e.g., keywords, structure) from natural texts without human annotation. A quality check is conducted to ensure that the attributes align with the raw texts (Section ~\ref{sec:dataset_construction}). By training LLMs on these realistic multi-attribute inputs, we expose the model to variable attributes across different positions, enabling it to better internalize the relationship between attributes and the text regardless of position.
% Building upon this, we introduce a global preference optimization strategy that explores diverse combinations of attributes. By modeling attribute correlations and reducing redundancy, we ensure that LLMs can robustly satisfy a large set of constraints simultaneously.

% \letian{Separate the content after to a new paragraph and be more specific about how you combine a set of attributes - emphasizing validity (correlation) and diversity} 
Besides, as the number of constraints increases, the prompt space shifts towards an underrepresented distribution in the pre-training corpus, exacerbating attention dilution.
To address this, we introduce a global preference optimization strategy in the second stage.
First, we fine-tune an embedding model via contrastive learning to capture attribute correlations, encouraging the model to prioritize plausible and coherent attribute combinations. This helps steer generation away from implausible combinations rarely seen during pre-training. For example, consider an attribute set that requires the inclusion of an international politics term such as \textit{impose 25\% tariffs}, alongside a medical term like \textit{myocardial infarction}. Such a combination is highly unlikely to appear together in real-world cases.
Second, we promote diversity by selecting the least similar candidate from a pool of generations. This prevents the model from collapsing to a small set of frequent patterns and encourages exploration of less common yet valid combinations.
Together, correlation modeling narrows the search space towards possible regions, while diversity selection expands coverage within that space, enabling the model to retain and balance a large set of attributes during generation.
%We explicitly model attribute correlations to guide the model towards satisfying coherent subsets of attributes, while enforcing diversity constraints to prevent the model from overfitting to a few frequent patterns. This enables the model to retain and balance a large set of attributes during generation.
% Building upon this, the RL stage optimizes the model to explore and satisfy all potential valid combinations of constraints, allowing it to generalize beyond seen distributions and improve robustness across diverse control scenarios. By leveraging high-quality datasets and a more balanced sampling strategy, the proposed framework systematically improve the model's understanding of multi-attributes, even under highly restrictive conditions. Additionally, we introduce innovative evaluation strategies that better reflect real-world complexities, ensuring comprehensive performance assessment. 
% By combining these two stages, our framework effectively addresses the scalability and adaptability challenges of EFCG, ensuring that the model maintains high fidelity even under an extreme number of constraints. 
% \letian{Add a specific implementation of our framework here. 1) Target: An expert on any constraint as input. 2) SFT endows the LLM ability on a specific constraint distribution. 3) RL helps the LLM to explore how to satisfy all valid combinations of constraints.} 

% \jingbo{how important are the tricks/techniques mentioned in this paragraph? It currently reads like it's a bag of tricks to make it work. But their importance is not clear.}
% Besides, many works~\cite{zhou2024lima,li2023self} show that the quality of training data is more important than its quantity, a reliable data construction pipeline is imperative. 
% \jingbo{is this applied to RL only or SFT too? If it's RL only, probably we should split the SFT and RL into two paragraphs and merge this paragraph with the RL one.}
% By leveraging high-quality datasets and a more balanced sampling strategy, the proposed framework systematically improve the model's understanding of multi-attributes, even under highly restrictive conditions. Additionally, we introduce innovative evaluation strategies that better reflect real-world complexities, ensuring comprehensive performance assessment. 

% Conclusion and preview
% analysis: data synthesis improvemnt and system prompt detoxition (# soft attr increase, toxic maintains)
% attention flow
% diversity
% out of domain
% 100,000 data -> corelation / diversity balance to train dpo model:
% e5 large -> diversity
% finetune e5 large -> corelation
% case demonstration, show the revised figure 2
Our contributions are threefold. First, we design an automated pipeline for dataset construction tailored to extreme constraints, enabling high-quality training and evaluation. Second, we develop a training strategy that integrates reconstruction and RL ~\cite{rafailov2024direct} to address the fundamental challenges in EFCG. Finally, we conduct extensive experiments to validate the proposed framework, providing insights into its efficacy and limitations. 


% In this paper, we propose a novel zero-shot two-stage framework for Extremely Fine-Grained Controllable Generation (EFCG), designed to robustly handle an expansive set of attributes. 
% Our approach seeks to combine the complementary strengths of supervised fine-tuning (SFT) and reinforcement learning (RL) to improve both overall text quality and attribute adherence. Specifically, in the SFT stage, we leverage the high precision of LLMs in text analysis to extract attributes from raw texts. These attributes are then fused into a multi-attribute requirement for fine-grained reconstruction within a supervised learning paradigm. 