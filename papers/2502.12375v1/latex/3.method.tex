\section{Method}
\subsection{Preliminaries}

\begin{figure}[t]
    \centering
        \includegraphics[width=0.49\textwidth]{figs/prelim.pdf}
    \caption{Score degradation as the position of hard attributes shifts in Llama-3.1-8B-Instruct and Qwen2-7B-Instruct, showing a consistent performance drop.}
    \vspace{-0.5em}
    \label{fig:position_bias}
\end{figure}

\paragraph{EFCG: An Overview}
Our extremely EFCG is an extension to controllable text generation CTG, whose goal is to generate an output $Y$ based on a given input $X$ and a set of control conditions $c$. Formally, this can be expressed as:
$$
P(Y \mid X, c)=\prod_{i=1}^n P_\theta\left(Y_i \mid Y_{<i}, X, c\right)
$$

\noindent where $n$ denotes the length of $Y$, and $\theta$ represents the parameters of a language model, and $Y_{<i}$ refers to generated tokens before the $i$-th one. 
In conventional CTG models, $X$ typically serves as a prompt, representing an incomplete text, while $Y$ constitutes its continuation.

Despite advancements made in CTG, existing models often struggle to effectively handle fine-grained control conditions, particularly when the position of attributes within the input context varies. As illustrated in Figure~\ref{fig:position_bias}, this limitation manifests as a significant performance degradation when attributes are positioned away from the beginning of the input context. Such trends highlight that modern language models may not robustly utilize information across the entire input sequence, with biases toward primacy regions of the context window. Addressing this challenge is crucial for improving the robustness and accuracy of EFCG task.

\vspace{-0.5em}
\paragraph{Semantic Similarity}
E5-large~\cite{wang2022text} is a powerful pre-trained encoder optimized for embedding sentences.
In EFCG, we utilize E5-large to encode document-grounded attributes and measure their relationships using cosine similarity. Cosine similarity is a widely used metric to quantify the similarity between two high-dimensional vectors, defined as:
$$
\operatorname{Sim}(\mathbf{u}, \mathbf{v})=\frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}
$$

where $\mathbf{u}$ and $\mathbf{v}$ represent the vector embeddings of two attributes. The cosine similarity yields values between -1 (completely dissimilar) and 1 (identical). Using cosine similarity, we identify semantically related attributes and filter out redundant or weakly correlated pairs.

\subsection{UltraGen}
As shown in Figure \ref{fig:pipeline}, our approach, UltraGen, addresses the challenge of EFCG through a two-stage framework.
First, we introduce auto-reconstruction training to align text with a rich set of soft and hard attributes.
Second, we apply global preference optimization to enhance the model’s adaptability to diverse, globally complex attribute compositions. 

\subsubsection{Auto-reconstruction Stage}
The auto-reconstruction stage trains on naturally aligned attribute-text pairs, where attributes are directly extracted from real texts of FineWeb ~\cite{penedo2024the}, a high-quality web corpus, to ensure intrinsic constraint compatibility (i.e., no conflicting attributes exist by construction). Then we train the model to re-generate the text based on the decomposed attributes.
Formally, given a training example $(Y, c) \in D_{\text {UltraBench }}$, the model learns to reconstruct $Y$ by minimizing the negative log-likelihood.

$$
\mathcal{L}_{\mathrm{SFT}}=-\mathbb{E}_{(Y, c)} \log P_\theta(Y \mid c)
$$

This process achieves dual objectives: \textbf{constraint grounding}, which forces the model to internalize the relationships between atomic attributes and their textual realizations, and \textbf{fluency preservation}, which maintains the base model’s generative quality by leveraging the natural language distribution of the original corpus.  The resulting reconstruct model serves as a coherent and constraint-aware initial policy for RL, providing essential prior knowledge for subsequent exploration of complicated constraint combinations.

 \subsubsection{Global Preference Optimization Stage}  
 To extend the foundation capability to global text generation, we first collect a massive pool of attributes from multiple sources.
 The attributes pool integrates diverse sources spanning multiple domains, styles, and formats. Unlike reconstruction, which applies mainly web data, our RL phase leverages data from (1) Books, (2) Academic Papers (arXiv), (3) Social Media (Reddit), (4) Technical Forums (StackExchange), (5) News (CC-News), and (6) Encyclopedic Sources (Wikipedia). This ensures broad coverage of textual variations, enabling the model to generalize across different contexts and constraint types.
 In each iteration, a valid subset of attributes is selected from this pool. Using the auto-reconstruction model, we generate $K$ candidate responses conditioned on the selected attributes. We then apply the CSR metric to identify the preferred and less favorable responses, which are subsequently used for DPO training.

A key challenge in selecting a valid subset of attributes lies in balancing \textbf{topic coherence} and \textbf{anti-redundancy}. Topic coherence requires a high correlation among attributes to ensure interdependent constraints are holistically satisfied. For example, keywords \textit{chain of thought} and \textit{use formal tone} jointly imply technical writing. In contrast, diversity prevents overfitting to frequent patterns and enhances fluency. For example, phrases like \textit{the dreariest place, a dreary day} are redundant and make the text uninformative.
Therefore, our pipeline comprises three key steps:
\vspace{-0.5em}
\paragraph{Attribute Correlation Modeling}
We fine-tune the E5-large encoder using triplet contrastive learning ~\cite{simcse} on document-grounded attributes. For each anchor attribute $\mathcal{A}_i$, a positive pair $\mathcal{A}_j$ shares context from the same document, while a negative pair $\mathcal{A}_k$ is sampled from unrelated contexts. The encoder minimizes the triplet loss, yielding $81.6 \%$ validation accuracy in distinguishing correlated attributes.

\vspace{-0.7em}
\paragraph{Attribute Set Expansion}
The process begins by randomly sampling 2000 seed soft attributes from the attributes pool as the initial attribute set. For each seed attribute $A_i$, we retrieve its top 1024 most correlated candidates using the fine-tuned E5 encoder, where correlation is quantified by the cosine similarity in the correlation representation space using the fine-tuned model. To enforce diversity and minimize semantic redundancy, candidates are iteratively added to the set based on a redundancy score $\operatorname{Sim}\left(A_{\text {candidate }}, A_i\right)$, which is defined as the cosine similarity in their original E5 semantic representation space. Expansion terminates when each set contains a randomly determined number from 10 to 110.

\paragraph{DPO Pair Generation}
For each attribute set, DPO training pairs are constructed by generating $K$ responses using the auto-reconstruction model. The soft and hard attribute scores are obtained by using the Python scripts and GPT-4o (Section \ref{sec:eval}), respectively. The total score is computed by averaging the two scores.

Responses are ranked from highest to lowest based on their scores. The highest-scoring response is chosen, while the lowest-scoring response is rejected. This automated scoring and ranking ensure the selection of thematically coherent and high-quality responses, refining the model’s ability to distinguish and generate optimal outputs.

\subsection{UltraBench}
\subsubsection{Dataset Construction}
\label{sec:dataset_construction}
To support our training framework, we construct two specialized dataset splits named \textbf{UltraBench}, derived from FineWeb \cite{penedo2024the} and multiple sources.
The UltraBench dataset is designed to evaluate and train models on extremely fine-grained controllable text generation. Its construction involves Two stages, as detailed below.

% \textbf{Reconstruction Dataset (Local Constraints)} The dataset is built upon FineWeb \cite{penedo2024the}, a high-quality corpus rigorously deduplicated and filtered. FineWeb consists mainly data from CommonCrawl\footnote{https://www.commoncrawl.org/}. 

\paragraph{Attribute Extraction}

Attributes were categorized into two types:
\begin{enumerate}
    \item \textbf{Soft attributes:} (e.g., style, tone, content) were inferred using GPT-4o~\cite{achiam2023gpt} to capture semantic properties. For example, a soft attribute might describe a passage as a \textit{vivid personal narrative focused on childbirth experience}.
    \item \textbf{Hard attributes:} consist of programmatically verifiable constraints extracted directly from the text. These included keyword requirements (e.g., \textit{include sustainability}), structural rules (e.g., \textit{generate exactly three paragraphs}), and syntactic directives (e.g., \textit{use all lowercase letters}).
\end{enumerate}
For the FineWeb split, we use each attribute set along with its corresponding raw text to perform the auto-reconstruction stage. For the Multi-sources split, we aggregate and de-duplicate all decomposed attributes to form a global attribute pool.

\paragraph{Consistency Verification}
To ensure the reliability of soft attribute extraction, we conducted a human evaluation on a randomly selected subset of 100 documents. Human experts assessed whether the extracted attributes accurately reflected the underlying text. We computed the Agreement Rate (AR), defined as the proportion of samples where automated extractions matched the original raw text. This process achieved an AR of 96.5\%, indicating a strong alignment between attributes and original text.

\subsubsection{Dataset Statistics}
\paragraph{Overall Statistics}
In Appendix ~\ref{appendix:statistics}, we summarize the dataset details. Table \ref{tab:overall_statistic} details the composition of UltraBench, with separate configurations for reconstruction and multi-sources subsets. Table \ref{tab:rl_domains} further analyzes the multi-sources subset’s domain distribution, while Table \ref{tab:quality_metrics} quantifies quality control metrics. 

\paragraph{Compared with Other Benchmarks}

\begin{figure}[t] 
    \centering
        \includegraphics[width=0.49\textwidth]{figs/bar_attr_nums.pdf}
    \caption{Comparison of average attributes across datasets.}
    \vspace{-1em}
    \label{fig:intro}
\end{figure}
Table ~\ref{tab:comparison} provides a detailed comparison of our dataset with other relevant works. 
While IFeval and FollowBench include synthesized data (Synt.), they fall short in capturing the diversity and complexity required for evaluating real-world applications.
Another key strength of our dataset lies in the average number of attributes per sample, where we achieve a remarkable value of 45.9 and 29.9 on two splits, far exceeding the benchmarks' maximum of 4.5. This demonstrates the ability of our dataset for evaluating tasks requiring fine-grained attribute understanding.


\subsubsection{Evaluation Protocol}
\label{sec:eval}
To rigorously evaluate EFCG capabilities, we use two evaluation metrics:

\paragraph{Constraint Satisfaction Rate (CSR)}

For a given instruction with both soft and hard constraints, we compute the CSR as follows:

$$
\mathrm{CSR}=\frac{1}{m} \sum_{i=1}^m \frac{1}{n^{(i)}} \sum_{j=1}^{n^{(i)}} s_j^{(i)}
$$

where $s_j^{(i)}=1$ if the $j$-th constraint for the $i$-th instruction is satisfied, and 0 otherwise. Here, $n^{(i)}$ is the number of constraints (hard or soft) for instruction $i$, and $m$ is the total number of evaluated instructions.
\begin{enumerate}
    \item \textbf{Hard Constraint Verification}: For programmatically verifiable constraints, we perform deterministic checks via Python scripts. Due to the significant imbalance in hard attributes, we adopt macro accuracy to ensure fair evaluation. Macro accuracy computes the average CSR across different types, giving equal weight to each type regardless of its frequency.
    
    \item \textbf{Soft Constraint Evaluation}: For semantic constraints, we employ an LLM-based judge (GPT-4o), assigning a binary score (0 or 1) to each constraint. 
    We validate the quality of the LLM-based judges on a randomly selected set of 100 samples. By calculating the Cohen’s Kappa coefficient between the scores of LLM-based judge and human experts, we found a strong agreement (84.55\%) between the automatic evaluation and human experts' assessment. 

\end{enumerate}

\paragraph{BERTScore} 
In the auto-reconstruction phase, we also use BERTScore~\cite{bertscore} to measure the quality of the reconstructed text. BERTScore leverages the contextual embeddings from pre-trained language models to capture semantic similarity. BERTScore is widely used in text generation tasks, as it aligns better with human judgments of semantic quality compared to traditional n-gram overlap-based metrics.

