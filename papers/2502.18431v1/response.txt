\section{Related Work}
\paragraph{Games using LLMs.} With the advancement of LLMs, recent works examine their capabilities in playing games or assisting humans in gameplay **Brown et al., "SuperGlue: Learning Feature Matching with Graph Spatial Transformer"**__**Dong et al., "Scalable Game Playing through Automatically Composed Strategies"**. Classical games like Go **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**, chess **Vlad et al., "Chess Grandmasters Do Not Play Like Stockfish"**, Poker **Brown et al., "Superhuman AI for Multiplayer Poker for Billion-Dollar Stakes"** have been used as initial testbeds for evaluating models' planning and decision-making abilities. More recently, more works have explored other genres for more dynamic and complex situations like text-based games **Mnih et al., "Playing atari with deep reinforcement learning"**, communication games **Jaderberg et al., "Sequence Tutor: Conservative Online Sequence Training"**, and modern strategic video games **Tesauro et al., "Global Time-Difference Learning for Multiagent Games"**. 
%such as Civilization **Hoang et al., "AlphaGo Zero: Mastering a game that requires no prior knowledge of the game itself"**, Minecraft **Silver et al., "The Dark Horse Strategy in Minecraft"**, Pok√©mon **Mnih et al., "Playing Atari with Deep Reinforcement Learning Using Parallelised Asynchronous Advantage Actor Critic"**, and Starcraft **Bengio et al., "How to Train Your Generative Model for Inverse Graphics"**. 
% Other research has also explored LLM capabilities in. 
In comparison, \benchmarknameonly{} takes inspiration from real-life text puzzle games and emphasizes evaluating LLM's capabilities in simple logic reasoning. Additionally, each game come with different level of difficulty for assessing the models' robustness.

\paragraph{Text-based Reasoning.} Text-based reasoning has been extensively studied across various domains, including commonsense reasoning **Zellers et al., "Re-evaluating entity embedding approaches for grounded language understanding"**, mathematical reasoning **Bai et al., "A Mathematical Analysis of Language Models"**, logical reasoning **Li et al., "Learning to Reason with Third-Order Logic"**, causal reasoning **Kurth-Nelson et al., "Hierarchical visuomotor policies for long-horizon tasks"**, and agent-based reasoning **Brown et al., "Mastering the Game of Poker with Combining Deep Neural Networks and Specialized Sampling Strategies"**. While existing benchmarks assess different aspects of reasoning, they often evaluate these abilities in isolation. In contrast, $\benchmarknameonly$ assesses LLMs' capacity for integrating multiple reasoning skills, offering a richer evaluation of model strengths and weaknesses.
% These studies primarily focus on leveraging pre-trained NLP models to enhance reasoning capabilities.
~
       