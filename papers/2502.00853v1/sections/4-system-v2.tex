\section{Supporting PC+VR Hybrid Visual Sensemaking}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/interface_PC2.pdf}
\caption{Demonstrations of interfaces of PC-only. (a) document view, (b) graph view, (c) timeline view, and (d) minimap view.}
\label{fig:pc_interface}
\end{figure}

We exemplified the design requirements by designing and implementing a hybrid PC+VR system tailored for anticipated visual sensemaking scenarios, specifically crime-solving tasks. 
In this section, we detail our main design considerations and decisions. We begin by introducing the design of PC and VR interfaces, respectively, followed by a discussion of our strategies for integrating them into a seamless interface.

\subsection{PC Interface}
We adapt the designs from previous work~\cite{mahyar2014supporting,tong2023towards} for the PC interface.
The PC interface consists of four views, as shown in \Cref{fig:pc_interface}, document view (a), graph view (b), timeline view (c), and minimap view (d). 
The \textbf{document view} in the PC interface (\Cref{fig:pc_interface}(a)) includes the task description, the document list, and the selected document. The task description provides a clear description of the task that the users are expected to work on and serves as a reminder of the key elements that need to be addressed. The document list displays all available documents, including the document ID and title. Users can select and read a document from the document list. 
The \textbf{visualization view} (\Cref{fig:pc_interface}(b)) serves as a canvas for users to create and read the node-link diagram. In the PC interface, the 2D graph visualizations are displayed. 
Users can add, move, modify, merge, and delete nodes by clicking the mouse and modifying related text using the keyboard in the graph visualization.
Nodes can be added for entities by typing or selecting the relevant text from documents and placing it in the graph visualization. Depending on the text, the node can be classified as a time node and encoded in orange color if the text could be parsed into a date time object without error. Otherwise, the node created will be a normal node and encoded in blue color.
Users can define the relationship between two nodes by adding links, with the links' labels typed or extracted from the documents.
The \textbf{timeline view} (\Cref{fig:pc_interface}(c)) presents time nodes and their connections on a 1D linear timeline.
The timeline is useful for organizing and visualizing time-related information for sensemaking~\cite{mahyar2014supporting}.
The timeline view and the graph are coordinated. Specifically, when users select a node from the graph, the corresponding node in the timeline view is also selected, and vice versa. The view helps users see the nodes in chronological order.
Lastly, a \textbf{minimap} (\Cref{fig:pc_interface}(d)) is presented for users to have an overview of the current graph view. By presenting the graph's current view area, users can better understand their current position and scale relative to the graph.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/interface_VR.pdf}
\caption{Demonstrations of interfaces of (a, b) VR-only and (c) PC+VR (\aka{} hybrid). (a) document view and graph view, (b) timeline view, (c) tracked simulated PC in VR view.}
\label{fig:vr_hybrid_interface}
\end{figure}

\subsection{VR Interface}
To provide the same context in both devices (\hyperref[req:r3]{R3}), we present views with the same functionality and information compared to the PC interfaces, \ie{}, document view (\Cref{fig:vr_hybrid_interface}(a)), graph view (\Cref{fig:vr_hybrid_interface}(a)), and timeline view (\Cref{fig:vr_hybrid_interface}(b)).
At the same time, we adapt the VR user interface designs from a prior work~\cite{tong2023towards} specifically designed for our scenario to optimize the VR interface (\hyperref[req:r2]{R2}).
To utilize the large display space, we distribute the documents in the space in a semi-circular shape, which is commonly adapted by previous work in immersive visualization~\cite{satriadi2020maps,hayatpur2020datahop} and found to be positive to spatial memory~\cite{liu2022effects}, as shown in \Cref{fig:vr_hybrid_interface}(a).

Moreover, the node-link diagram is presented in 3D because 3D node-link diagrams are effective in VR~\cite{belcher2003using,kwon2016study,ware2005reevaluating}.
All graph operations are identical to the PC interfaces.
Furthermore, the text label of nodes and links automatically faces users for their reading. 
To strengthen the spatial relationship between the document and the graph visualization, we externalized the relationship between the created node and the document by providing one black node in front of each document and adding a default link between the created node and the currently selected document nodes (\Cref{fig:vr_hybrid_interface}(a)).

\re{Different from providing the timeline as a 2D panel on the PC, we present the timeline on the floor and utilize the foot interaction for utilizing spatial ability in an immersive environment, as it was found positive for view management~\cite{liu2023datadancing} and used for AR map navigation~\cite{austin2020elicitation}
(\Cref{fig:vr_hybrid_interface}(b)).}{} It allows users to walk on the timeline to select different nodes related to specific moments. Such a design creates an eyes-free interaction and supports secondary navigation tasks so that users might concentrate on the changes in the graph while navigating the timeline.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/EmbodiedInteraction_CHI24.pdf}
\caption{The figure shows different hand gestures implemented for graph (a-f) and timeline (g) manipulations. Red nodes indicate that the nodes will be deleted. Green nodes indicate that the nodes will be created. Yellow nodes indicate that the nodes are selected.}
\label{fig:embodied}
\end{figure}

\re{Lastly, we introduce a set of embodied interactions (\ie{}, hand gestures) to nodes and links in VR, as illustrated in \Cref{fig:embodied}, to minimize the context switch between PC and VR devices. It is because the switch between the mouse and keyboard and controllers unavoidably contains three steps: releasing one, locating, and reaching for another, which is less convenient than just releasing the mouse and using hand gestures; in addition, putting the controllers on the table would occupy spaces that interfere with the mouse movement.}{} For the specific designs, a node can be created or updated by selecting the relevant text from the document or using the virtual keyboard text input and placing it in the node-link diagram. Users can ``grab'' the node and move it around (\Cref{fig:embodied}(a)). They can also ``grab'' two nodes and put them together to merge two nodes (\Cref{fig:embodied}(b)) or ``throw'' the node away to delete the node (\Cref{fig:embodied}(c)). Besides nodes, users can define or update any relationship between two nodes by adding a link with labels extracted from the documents or input via a virtual keyboard. Users can ``drag'' one node to another to create an empty link (\Cref{fig:embodied}(d)) and ``pull'' a link to delete the link (\Cref{fig:embodied}(e)). 
To quickly view the graph overview, users can grab the air with both hands and move closer or farther away to zoom in and out on the graph (\Cref{fig:embodied}(f)).
Lastly, users can select a specific node or all nodes with the same date by standing on the corresponding node (\Cref{fig:embodied}(g)) or the white node (\Cref{fig:vr_hybrid_interface}(b)) on the timeline. 

\subsection{Cross-device Interaction in the Hybrid System}
To better support users synchronized using both PC and VR interfaces (\hyperref[req:r4]{R4}) and easy-to-switch input modality and cross-device interaction (\hyperref[req:r5]{R5}), we introduce the following cross-device features and interactions.


\para{Simulated PC in PC+VR.} To reduce the context-switching cost of taking on and off the HMD, we designed a simulated PC, motivated by~\cite{jetter2020vr,immersed2023,hubenschmid2022relive,seraji2024analyzing} (\Cref{fig:vr_hybrid_interface}(c)). 
It allows users to use the PC interface while in the VR environment synchronously (\hyperref[req:r4]{R4}). Users could use a mouse and keyboard to control the PC interface on a physically movable adjustable desk in the VR environment. 
To ensure users could see the keyboard and mouse, we defined a rectangular area below the simulated PC that allows users to see through VR and into reality (\Cref{fig:vr_hybrid_interface}(c) bottom right). 
This see-through area also helps minimize the risk of accidentally bumping into the table and hitting the surrounding area. 
Moreover, to allow users to move the simulated PC in VR, we aligned the position of the simulated PC with the physical desk using a VIVE tracker 3.0, as shown in \Cref{fig:setup}(c). As a result, users could move the simulated PC by moving the desk in reality (\hyperref[req:r1]{R1}).

\para{Synchronized States between Devices.}
To reduce the reload time after switching devices, we synchronize the current document and node selections between interfaces (\hyperref[req:r5]{R5}).
Users can quickly refer back to its current workflow after changing the devices.
For example, users could directly view the document they had last seen in VR when they switched from VR to PC.
Moreover, cross-device linking and brushing are supported. Users could view the same nodes selected from the PC highlighted in VR and vice-versa.
Lastly, we aimed to help users construct a coherent mental model connecting the 2D graph displayed on the simulated PC with the 3D graph in VR.
To achieve this, we prioritized maintaining layout consistency. We initiated this by projecting the 3D graph into a 2D space. Subsequently, we employed a force-directed layout algorithm to minimize the visual clutter.


\para{Hand Gestures as the Main Modality.} To ease the transition between different input interfaces, we decided to use hand gestures (\Cref{fig:embodied}) instead of controllers as the primary interaction modality in the VR interface (\hyperref[req:r5]{R5}). We introduced two main gestures: pinch and grab.
Pinch (air-tap) is the standard gesture for selecting objects using the ray from hand in VR. Grab consists of a fist and a flat gesture for interacting with close-distanced objects. The change from a flat hand to a fist gesture indicates the start of a grabbing action. Conversely, the change from the fist gesture to a flat hand indicates the end of the action. Using hand gestures allows users to easily switch from mouse and keyboard to hand gestures instead of finding and grabbing VR controllers.

\subsection{Implementation}
We used web technology to implement the PC+VR hybrid system,
\ie{}, React.js, d3.js, three.js, and WebXR. To simulate a PC, we performed screen casting from a laptop computer using WebRTC to a plane in VR. Then, to allow users to move the simulated PC in reality, we tracked the desk's movement by placing a VIVE tracker on a wheeled desk and retrieving the pose data using Python OpenVR\footnote{\url{https://github.com/cmbruns/pyopenvr}} and streaming the data to the client web application. 
We set up the initial distance between the physical desk and the tracker for the simulated PC by pressing the ``A'' button of the right-hand VR controller.
To support state synchronization between different devices, we built a server using Node.js and gRPC for fast data communication. 
For gesture recognition beyond the standard pinch gesture, we used Handy.js\footnote{\url{https://stewartsmith.io/handy/}} to recognize fist and flat hand gestures.
Our system code is available at \url{https://github.com/asymcollabvis/hybridvis}.
