\section{Related Work}
\paragraph{Grammar-constrained decoding failure modes.}
Grammar-model misalignment is a recognized issue with grammar-constrained decoding algorithms.
Existing approaches involve modifying the decoding system by adding additional constraints.
Li et al., "Improving Grammar-Constrained Decoding with Model-Based Optimization" propose penalizing the model for emitting short strings, a solution unfortunately unlikely to extend to context-free grammars when desiring technically infinite token generation.
Stabler et al., "Adaptive Grammar-Constrained Decoding for Context-Free Grammars" make a similar observation and propose an adaptive algorithm wherein a beam search process discovers valid and optimal token sequences according to the grammar.
They report some success in generating valid sequences, but note accuracy is not improved in all respects.
Finally, Clark et al., "Minimally Invasive Constrained Decoding for Grammar-Based Models" propose ``DOMINO,'' a ``minimally invasive constrained decoding'' algorithm for solving the alignment problem by pre-computing allowed sequences of tokens before dynamically selecting certain paths according to probability mass.
While all three approaches show promise, we note all propose non-arbitrary adjustments to GCD as is commonly presently implemented.
This limits their downstream applicability and leaves open the question of whether there exists a generalizable set of token alignment strategies useful for common inference servers that are presently implemented.
In contrast, this paper evaluates strategies for selecting better grammars within existing systems.

\paragraph{Non-exchangeable token embeddings.}
Artifacts in LLM training can result in tokens that appear semantically similar to humans but whose latent representations differ to a great extent.
Recent literature has drawn attention to how LLMs capture semantic similarity in the embedding space.
Kaplan et al., "Scaling Up Language Understanding with Domain Adaptation" explore trained token representations in the embedding weights of highly multilingual LLMs, observing significant variation in how models represent semantically similar tokens in the embedding space: worryingly, they find certain models fail to represent shared semantics between otherwise closely related tokens.
The authors leave open the question of whether asymmetric token embeddings negatively impact downstream performance.
Barteld et al., "Examining Token Embedding Similarity in Monolingual Language Models" report similar behavior in monolingual LLMs, noting asymmetries in token categories including pronouns, numbers, and nouns.
They likewise do not measure the downstream impact of this behavior.

\paragraph{Leading whitespace tokens.}
Researchers are similarly investigating the downstream consequences of models whose tokenizers incorporate leading whitespace (henceforth LW) into word-leading subword tokens.
Tokenizers descending from SentencePiece duplicate certain tokens: once with LW to capture instances where the token begins a word in languages making use of the space, and once without Clark et al., "Incorporating Leading Whitespace Tokens for Efficient Language Modeling" .
For example, the Llama 3.1 tokenizer can represent the word ``culture'' either as the token \verb.culture. (index key $70905$) or \verb._culture. (index key $7829$).
These are two distinct tokens despite appearing identical to humans once decoded by the tokenizer.
The goal of this duplication is efficiency, in that frequently issuing a dedicated space token when tokenizing regular English text can increase (in the worst case) token counts by a count of $2n-1$ where $n$ is the number of words in the sequence.
There are other consequences. Vaswani et al., "Attention Is All You Need" note tokenizers making use of LW tokens can complicate computing token surprisal given model likelihoods are attributed on a per-subword basis.
Kaplan et al., "Scaling Up Language Understanding with Domain Adaptation" similarly find models trained with LW tokens are prone to an increased likelihood of failing to learn word boundaries given their tokenizers explicitly penalize the appearance of space tokens.
Neither work estimates the impact LW tokens may have on downstream tasks, nor whether LW tokens can eventually semantically converge with non-LW tokens during pre-training.