 @book{Altman_Gill_McDonald_2004, title={{Numerical {I}}ssues in {S}tatistical {C}omputing for the {S}ocial {S}cientist}, url={https://books.google.com/books?hl=en&lr=&id=2Feq7zgkCIkC&oi=fnd&pg=PP1&dq=computational+social+science+percentage+scale&ots=PA5q1e9IXD&sig=MbkOaA9U8GNRRMdK03c2sJCUCbc}, publisher={John Wiley & Sons}, author={Altman, Micah and Gill, Jeff and McDonald, Michael P.}, year={2004} }
 @article{Bamman_Chang_Lucy_Zhou_2024, title={{On {C}}lassification with {L}arge {L}anguage {M}odels in {C}ultural {A}nalytics}, url={http://arxiv.org/abs/2410.12029}, abstractNote={In this work, we survey the way in which classification is used as a sensemaking practice in cultural analytics, and assess where large language models can fit into this landscape. We identify ten tasks supported by publicly available datasets on which we empirically assess the performance of LLMs compared to traditional supervised methods, and explore the ways in which LLMs can be employed for sensemaking goals beyond mere accuracy. We find that prompt-based LLMs are competitive with traditional supervised models for established tasks, but perform less well on de novo tasks. In addition, LLMs can assist sensemaking by acting as an intermediary input to formal theory testing.}, note={arXiv:2410.12029 [cs]}, number={arXiv:2410.12029}, publisher={arXiv}, author={Bamman, David and Chang, Kent K. and Lucy, Li and Zhou, Naitian}, year={2024}, month=oct, language={en} }
 @inproceedings{Berant_Liang_2014, address={Baltimore, Maryland}, title={{Semantic {P}}arsing via {P}araphrasing}, url={https://aclanthology.org/P14-1133}, DOI={10.3115/v1/P14-1133}, booktitle={{Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}}, publisher={Association for Computational Linguistics}, author={Berant, Jonathan and Liang, Percy}, editor={Toutanova, Kristina and Wu, Hua}, year={2014}, month=jun, pages={1415–1425} }
 @article{Beurer-Kellner_Fischer_Vechev_2023, title={{Prompting {I}}s {Pr}ogramming: {A} {Q}uery {L}anguage for {L}arge {L}anguage {M}odels}, volume={7}, ISSN={2475-1421}, DOI={10.1145/3591300}, abstractNote={Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.
            Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.
            To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.
            We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85\% cost savings).}, number={PLDI}, journal={Proceedings of the ACM on Programming Languages}, author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin}, year={2023}, month=jun, pages={1946–1969}, language={en} }
 @article{Beurer-Kellner_Fischer_Vechev_2024, title={{Guiding {LLMs}} {T}he {R}ight {W}ay: {F}ast, {N}on-{I}nvasive {C}onstrained {G}eneration}, rights={Creative Commons Attribution 4.0 International}, url={https://arxiv.org/abs/2403.06988}, DOI={10.48550/ARXIV.2403.06988}, abstractNote={To ensure that text generated by large language models (LLMs) is in an expected format, constrained decoding proposes to enforce strict formal language constraints during generation. However, as we show in this work, not only do such methods incur performance overhead during generation, but many of them also significantly impair task accuracy, if they do not correctly align the underlying LLM sub-word vocabularies with external constraints. To address this, we present a novel decoding algorithm, DOMINO, that can enforce constraints in a fully subword-aligned fashion, while leveraging pre-computation and speculative decoding to achieve virtually no overhead and in some cases even almost 2$times$ speedup over unconstrained decoding -- thereby outperforming existing approaches by a wide margin.}, publisher={arXiv}, author={Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin}, year={2024} }
 @article{Brown_et_al._2020, title={{Language {M}}odels are {F}ew-{S}hot {L}earners}, url={http://arxiv.org/abs/2005.14165}, DOI={10.48550/arXiv.2005.14165}, abstractNote={Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.}, note={arXiv:2005.14165 [cs]}, number={arXiv:2005.14165}, publisher={arXiv}, author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario}, year={2020}, month=jul }
 @inbook{Buneman_Davidson_Fernandez_Suciu_1997, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={{Adding {S}}tructure to {S}nstructured {D}ata}, volume={1186}, ISBN={978-3-540-62222-2}, url={http://link.springer.com/10.1007/3-540-62222-5_55}, DOI={10.1007/3-540-62222-5_55}, abstractNote={We develop a new schema for unstructured data. Traditional schemas resemble the type systems of programming languages. For unstructured data, however, the underlying type may be much less constrained and hence an alternative way of expressing constraints on the data is needed. Here, we propose that both data and schema be represented as edge-labeled graphs. We develop notions of conformance between a graph database and a graph schema and show that there is a natural and eciently computable ordering on graph schemas. We then examine certain subclasses of schemas and show that schemas are closed under query applications. Finally, we discuss how they may be used in query decomposition and optimization.}, booktitle={{Database Theory — ICDT ’97}}, publisher={Springer Berlin Heidelberg}, author={Buneman, Peter and Davidson, Susan and Fernandez, Mary and Suciu, Dan}, editor={Afrati, Foto and Kolaitis, Phokion}, year={1997}, pages={336–350}, collection={Lecture Notes in Computer Science}, language={en} }
 @inproceedings{Buneman_Davidson_Suciu_1995, title={{Programming {C}}onstructs for {U}nstructured {Da}ta}, rights={http://creativecommons.org/licenses/by/4.0/}, url={https://scienceopen.com/hosted-document?doi=10.14236/ewic/DBPL1995.4}, DOI={10.14236/ewic/DBPL1995.4}, abstractNote={We investigate languages for querying and transforming unstructured data by which we mean languages than can be used without knowledge of the structure (schema) of the database. There are two reasons for wanting to do this. First, some data models have emerged in which the schema is either completely absent or only provides weak constraints on the data. Second, it is sometimes convenient, for the purposes of browsing, to query the database without reference to the schema. For example one may want to grep" all character strings in the database, or one might want to nd the information associated with a certain eld name no matter where it occurs in the database.}, author={Buneman, Peter and Davidson, Susan and Suciu, Dan}, year={1995}, language={en} }
 @inproceedings{Cao_Rimell_2021, address={Online and Punta Cana, Dominican Republic}, title={{You {S}}hould {E}valuate {Y}our {L}anguage {M}odel on {M}arginal {L}ikelihood over {T}okenisations}, url={https://aclanthology.org/2021.emnlp-main.161}, DOI={10.18653/v1/2021.emnlp-main.161}, abstractNote={Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate pretrained English and German language models on both the one-besttokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.}, booktitle={{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Cao, Kris and Rimell, Laura}, year={2021}, pages={2104–2114}, language={en} }
 @article{Chomsky_1956, title={{Three {M}}odels for the {D}escription of {L}anguage}, volume={2}, ISSN={2168-2712}, DOI={10.1109/TIT.1956.1056813}, abstractNote={We investigate several conceptions of linguistic structure to determine whether or not they can provide simple and “revealing” grammars that generate all of the sentences of English and only these. We find that no finite-state Markov process that produces symbols with transition from state to state can serve as an English grammar. Furthermore, the particular subclass of such processes that producen-order statistical approximations to English do not come closer, with increasingn, to matching the output of an English grammar. We formalize-the notions of “phrase structure” and show that this gives us a method for describing language which is essentially more powerful, though still representable as a rather elementary type of finite-state process. Nevertheless, it is successful only when limited to a small subset of simple sentences. We study the formal properties of a set of grammatical transformations that carry sentences with phrase structure into new sentences with derived phrase structure, showing that transformational grammars are processes of the same elementary type as phrase-structure grammars; that the grammar of English is materially simplified if phrase structure description is limited to a kernel of simple sentences from which all other sentences are constructed by repeated transformations; and that this view of linguistic structure gives a certain insight into the use and understanding of language.}, number={3}, journal={IRE Transactions on Information Theory}, author={Chomsky, N.}, year={1956}, month=sep, pages={113–124} }
 @book{Delmastro_2021, address={Cham}, series={SpringerBriefs in Political Science}, title={{On the {M}}easurement of {S}ocial {P}henomena: {A} {M}ethodological {A}pproach}, rights={https://www.springer.com/tdm}, ISBN={978-3-030-77535-3}, url={https://link.springer.com/10.1007/978-3-030-77536-0}, DOI={10.1007/978-3-030-77536-0}, publisher={Springer International Publishing}, author={Delmastro, Marco}, year={2021}, collection={SpringerBriefs in Political Science}, language={en} }
 @article{Devlin_Chang_Lee_Toutanova_2018, title={{{BERT}}: {P}re-training of {D}eep {B}idirectional {T}ransformers for {L}anguage {U}nderstanding}, url={https://arxiv.org/abs/1810.04805v2}, DOI={10.48550/arXiv.1810.04805}, abstractNote={We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}, author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, year={2018}, month=oct, language={en} }
 @article{Dubey_Jauhri_Pandey_et_al._2024, title={{The {Llama}} 3 {H}erd of {M}odels}, url={http://arxiv.org/abs/2407.21783}, abstractNote={Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.}, note={arXiv:2407.21783 [cs]}, number={arXiv:2407.21783}, publisher={arXiv}, author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris, et al.}, year={2024}, month=aug, language={en} }
 @inproceedings{Felkner_Thompson_May_2024, address={Bangkok, Thailand}, title={{{GPT}} is {N}ot an {A}nnotator: {T}he {N}ecessity of {H}uman {A}nnotation in {F}airness {Be}nchmark {Co}nstruction}, url={https://aclanthology.org/2024.acl-long.760}, DOI={10.18653/v1/2024.acl-long.760}, abstractNote={Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.}, booktitle={{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}}, publisher={Association for Computational Linguistics}, author={Felkner, Virginia and Thompson, Jennifer and May, Jonathan}, editor={Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}, year={2024}, month=aug, pages={14104–14115} }
 @inproceedings{Geiger_Yu_Yang_Dai_Qiu_Tang_Huang_2020, address={Barcelona Spain}, title={{{Garbage In, Garbage Out?: Do Machine Learning Application Papers in Social Computing Report where human-labeled training data comes from?}}}, ISBN={978-1-4503-6936-7}, url={https://dl.acm.org/doi/10.1145/3351095.3372862}, DOI={10.1145/3351095.3372862}, abstractNote={Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper’s authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing — specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data — give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a “gold standard” of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.}, booktitle={{Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency}}, publisher={ACM}, author={Geiger, R. Stuart and Yu, Kevin and Yang, Yanlai and Dai, Mindy and Qiu, Jie and Tang, Rebekah and Huang, Jenny}, year={2020}, month=jan, pages={325–336}, language={en} }
 @inproceedings{Geng_Josifoski_Peyrard_West_2023, address={Singapore}, title={{{Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning}}}, url={https://aclanthology.org/2023.emnlp-main.674}, DOI={10.18653/v1/2023.emnlp-main.674}, abstractNote={Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.}, booktitle={{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Geng, Saibo and Josifoski, Martin and Peyrard, Maxime and West, Robert}, editor={Bouamor, Houda and Pino, Juan and Bali, Kalika}, year={2023}, month=dec, pages={10932–10952} }
 @misc{Gerganov_2024, type={C++}, title={{{Llama.CPP}}}, rights={MIT}, url={https://github.com/ggerganov/llama.cpp}, abstractNote={LLM inference in C/C++}, author={Gerganov, Georgi}, year={2024}, month=nov }
 @inbook{Gray_Savelka_Oliver_Ashley_2023, title={{{Can GPT Alleviate the Burden of Annotation?}}}, rights={https://creativecommons.org/licenses/by-nc/4.0/}, ISBN={978-1-64368-472-7}, url={https://ebooks.iospress.nl/doi/10.3233/FAIA230961}, DOI={10.3233/FAIA230961}, abstractNote={Manual annotation is just as burdensome as it is necessary for some legal text analytic tasks. Given the promising performance of Generative Pretrained Transformers (GPT) on a number of different tasks in the legal domain, it is natural to ask if it can help with text annotation. Here we report a series of experiments using GPT-4 and GPT 3.5 as a pre-annotation tool to determine whether a sentence in a legal opinion describes a legal factor. These GPT models assign labels that human annotators subsequently conﬁrm or reject. To assess the utility of pre-annotating sentences at scale, we examine the agreement among gold-standard annotations, GPT’s pre-annotations, and law students’ annotations. The agreements among these groups support that using GPT-4 as a pre-annotation tool is a useful starting point for large-scale annotation of factors.}, booktitle={{Frontiers in Artificial Intelligence and Applications}}, publisher={IOS Press}, author={Gray, Morgan and Savelka, Jaromir and Oliver, Wesley and Ashley, Kevin}, editor={Sileno, Giovanni and Spanakis, Jerry and Van Dijck, Gijs}, year={2023}, month=dec, language={en} }
 @inproceedings{Gupta_Song_Anumanchipalli_2024, address={Miami, Florida, US}, title={{{Self-Assessment Tests are Unreliable Measures of LLM Personality}}}, url={https://aclanthology.org/2024.blackboxnlp-1.20}, abstractNote={As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of “personality” of LLMs using self-assessment personality tests developed to measure human personality. Yet almost none of these works verify the applicability of these tests on LLMs. In this paper, we analyze the reliability of LLM personality scores obtained from self-assessment personality tests using two simple experiments. We first introduce the property of prompt sensitivity, where three semantically equivalent prompts representing three intuitive ways of administering self-assessment tests on LLMs are used to measure the personality of the same LLM. We find that all three prompts lead to very different personality scores, a difference that is statistically significant for all traits in a large majority of scenarios. We then introduce the property of option-order symmetry for personality measurement of LLMs. Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the self-assessment test scores are not robust to the order of the options. These simple tests, done on ChatGPT and three Llama2 models of different sizes, show that self-assessment personality tests created for humans are unreliable measures of personality in LLMs.}, booktitle={{Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP}}, publisher={Association for Computational Linguistics}, author={Gupta, Akshat and Song, Xiaoyang and Anumanchipalli, Gopala}, editor={Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie}, year={2024}, month=nov, pages={301–314} }
 @article{Hamilton_2023, title={{{Blind Judgement: Agent-Based Supreme Court Modelling With GPT}}}, url={http://arxiv.org/abs/2301.05327}, DOI={10.48550/arXiv.2301.05327}, abstractNote={We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism & liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents.}, note={arXiv:2301.05327 [cs]}, number={arXiv:2301.05327}, publisher={arXiv}, author={Hamilton, Sil}, year={2023}, month=jan }
 @inproceedings{He_Lee_Lewis_Zettlemoyer_2017, address={Vancouver, Canada}, title={{{Deep Semantic Role Labeling: What Works and What’s Next}}}, url={http://aclweb.org/anthology/P17-1044}, DOI={10.18653/v1/P17-1044}, abstractNote={We introduce a new deep learning model for semantic role labeling (SRL) that signiﬁcantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on the CoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10\% relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.}, booktitle={{Proceedings of the 55th Annual Meeting of the Association for           Computational Linguistics (Volume 1: Long Papers)}}, publisher={Association for Computational Linguistics}, author={He, Luheng and Lee, Kenton and Lewis, Mike and Zettlemoyer, Luke}, year={2017}, pages={473–483}, language={en} }
 @inproceedings{He_Huang_Ding_Rohatgi_Huang_2024, address={Honolulu HI USA}, title={{I{f in a Crowdsourced Data Annotation Pipeline, a GPT-4}}}, ISBN={9798400703300}, url={https://dl.acm.org/doi/10.1145/3613904.3642834}, DOI={10.1145/3613904.3642834}, abstractNote={Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers’ performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline’s highest accuracy was 81.5\%, whereas GPT-4 achieved 83.6\%. Interestingly, when combining GPT-4’s labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5\%, 87.0\%). Further analysis suggested that, when the crowd’s and GPT-4’s labeling strengths are complementary, aggregating them could increase labeling accuracy.}, booktitle={{Proceedings of the CHI Conference on Human Factors in Computing Systems}}, publisher={ACM}, author={He, Zeyu and Huang, Chieh-Yang and Ding, Chien-Kuang Cornelia and Rohatgi, Shaurya and Huang, Ting-Hao Kenneth}, year={2024}, month=may, pages={1–25}, language={en} }
 @article{Hillard_Purpura_Wilkerson_2008, title={{{Computer-Assisted Topic Classification for Mixed-Methods Social Science Research}}}, volume={4}, ISSN={1933-1681, 1933-169X}, DOI={10.1080/19331680801975367}, number={4}, journal={Journal of Information Technology & Politics}, author={Hillard, Dustin and Purpura, Stephen and Wilkerson, John}, year={2008}, month=may, pages={31–46}, language={en} }
 @article{Hirschberg_1975, title={{{A Linear Space Algorithm for Computing Maximal Common Subsequences}}}, volume={18}, ISSN={0001-0782, 1557-7317}, DOI={10.1145/360825.360861}, abstractNote={The problem of finding a longest common subsequence of two strings has been solved in quadratic time and space. An algorithm is presented which will solve this problem in quadratic time and in linear space.}, number={6}, journal={Communications of the ACM}, author={Hirschberg, D. S.}, year={1975}, month=jun, pages={341–343}, language={en} }
 @inproceedings{Holter_Ell_2023, address={Vienna, Austria}, title={{{Human-Machine Collaborative Annotation: A Case Study with GPT-3}}}, url={https://aclanthology.org/2023.ldk-1.17}, booktitle={{Proceedings of the 4th Conference on Language, Data and Knowledge}}, publisher={NOVA CLUNL, Portugal}, author={Holter, Ole Magnus and Ell, Basil}, editor={Carvalho, Sara and Khan, Anas Fahad and Anić, Ana Ostroški and Spahiu, Blerina and Gracia, Jorge and McCrae, John P. and Gromann, Dagmar and Heinisch, Barbara and Salgado, Ana}, year={2023}, month=sep, pages={193–206} }
 @article{Hou_Ji_2024, title={{Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis}}, volume={21}, ISSN={1548-7091, 1548-7105}, DOI={10.1038/s41592-024-02235-4}, abstractNote={Abstract
            Here we demonstrate that the large language model GPT-4 can accurately annotate cell types using marker gene information in single-cell RNA sequencing analysis. When evaluated across hundreds of tissue and cell types, GPT-4 generates cell type annotations exhibiting strong concordance with manual annotations. This capability can considerably reduce the effort and expertise required for cell type annotation. Additionally, we have developed an R software package GPTCelltype for GPT-4’s automated cell type annotation.}, number={8}, journal={Nature Methods}, author={Hou, Wenpin and Ji, Zhicheng}, year={2024}, month=aug, pages={1462–1465}, language={en} }
 @article{Huang_Arehalli_Kugemoto_Muxica_Prasad_Dillon_Linzen_2024, title={{{Large-Scale Benchmark Yields no Evidence that Language Model Surprisal Explains Syntactic Disambiguation Difficulty}}}, volume={137}, ISSN={0749-596X}, DOI={10.1016/j.jml.2024.104510}, abstractNote={Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences – one of the major concerns of psycholinguistics – be explained by predictability, as estimated using computational language models, and operationalized as surprisal (negative log probability)? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. By estimating the function that relates surprisal to reading times from filler items included in the experiment, we find that the predictions of language models with two different architectures sharply diverge from the empirical reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that next-word prediction is most likely insufficient on its own to explain human syntactic processing.}, journal={Journal of Memory and Language}, author={Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal}, year={2024}, month=aug, pages={104510} }
 @article{Joshi_Kale_Chandel_Pal_2015, title={{{Likert Scale: Explored and Explained}}}, volume={7}, ISSN={22310843}, DOI={10.9734/BJAST/2015/14975}, abstractNote={Likert scale is applied as one of the most fundamental and frequently used psychometric tools in educational and social sciences research. Simultaneously, it is also subjected to a lot of debates and controversies in regards with the analysis and inclusion of points on the scale. With this context, through reviewing the available literature and then clubbing the received information with coherent scientific thinking, this paper attempts to gradually build a construct around Likert scale. This analytical review begins with the necessity of psychometric tools like Likert scale andits variants and focuses on some convoluted issues like validity, reliability and analysis of the scale.}, number={4}, journal={British Journal of Applied Science & Technology}, author={Joshi, Ankur and Kale, Saket and Chandel, Satish and Pal, D.}, year={2015}, month=jan, pages={396–403}, language={en} }
 @article{Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020, title={{{Scaling Laws for Neural Language Models}}}, url={http://arxiv.org/abs/2001.08361}, abstractNote={We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overﬁtting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sampleefﬁcient, such that optimally compute-efﬁcient training involves training very large models on a relatively modest amount of data and stopping signiﬁcantly before convergence.}, note={arXiv:2001.08361 [cs, stat]}, number={arXiv:2001.08361}, publisher={arXiv}, author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}, year={2020}, month=jan, language={en} }
 @inbook{Keraghel_Morbieu_Nadif_2024, address={Cham}, series={Lecture Notes in Computer Science}, title={{{Beyond Words: A Comparative Analysis of LLM Embeddings for Effective Clustering}}}, volume={14641}, ISBN={978-3-031-58546-3}, url={https://link.springer.com/10.1007/978-3-031-58547-0_17}, DOI={10.1007/978-3-031-58547-0_17}, abstractNote={The document clustering process involves the grouping of similar unlabeled textual documents. This task relies on the use of document embedding techniques, which can be derived from various models, including traditional and neural network-based approaches. The emergence of Large Language Models (LLMs) has provided a new method of capturing information from texts through customized numerical representations, potentially enhancing text clustering by identifying subtle semantic connections. The objective of this paper is to demonstrate the impact of LLMs of different sizes on text clustering. To accomplish this, we select five different LLMs and compare them with three less resourceintensive embedding methods. Additionally, we utilize six clustering algorithms. We simultaneously assess the performance of the embedding models and clustering algorithms in terms of clustering quality, and highlight the strengths and limitations of the models under investigation.}, booktitle={{Advances in Intelligent Data Analysis XXII}}, publisher={Springer Nature Switzerland}, author={Keraghel, Imed and Morbieu, Stanislas and Nadif, Mohamed}, editor={Miliou, Ioanna and Piatkowski, Nico and Papapetrou, Panagiotis}, year={2024}, pages={205–216}, collection={Lecture Notes in Computer Science}, language={en} }
 @article{Knuth_1964, title={{{Backus Normal Form vs. Backus Naur Form}}}, volume={7}, ISSN={0001-0782, 1557-7317}, DOI={10.1145/355588.365140}, number={12}, journal={Communications of the ACM}, author={Knuth, Donald E.}, year={1964}, month=dec, pages={735–736}, language={en} }
 @article{Le_Chen_Ritter_Xu_2024, title={{{Constrained Decoding for Cross-Lingual Label Projection}}}, url={http://arxiv.org/abs/2402.03131}, DOI={10.48550/arXiv.2402.03131}, abstractNote={Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment1.}, note={arXiv:2402.03131 [cs]}, number={arXiv:2402.03131}, publisher={arXiv}, author={Le, Duong Minh and Chen, Yang and Ritter, Alan and Xu, Wei}, year={2024}, month=feb, language={en} }
 @inproceedings{Kudo_Richardson_2018, address={Brussels, Belgium}, title={{{SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing}}}, url={https://aclanthology.org/D18-2012}, DOI={10.18653/v1/D18-2012}, abstractNote={This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.}, booktitle={{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}}, publisher={Association for Computational Linguistics}, author={Kudo, Taku and Richardson, John}, editor={Blanco, Eduardo and Lu, Wei}, year={2018}, month=nov, pages={66–71} }
 @article{Levenshtein_1966, title={{{Binary codes capable of correcting deletions, insertions, and reversals}}}, journal={Proceedings of the Soviet physics doklady}, author={Levenshtein, V. I.}, year={1966} }
 @article{Li_Hua_Wang_Zhu_Zhang_2024, title={{Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents}}, url={http://arxiv.org/abs/2402.00798}, DOI={10.48550/arXiv.2402.00798}, abstractNote={Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM’s content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users’ trust in LLM-based agents. In response, this paper proposes a novel “Formal-LLM” framework for LLMbased agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows agent developers to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50\% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLMbased agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The source code of this work is available at https: //github.com/agiresearch/Formal-LLM.}, note={arXiv:2402.00798 [cs]}, number={arXiv:2402.00798}, publisher={arXiv}, author={Li, Zelong and Hua, Wenyue and Wang, Hao and Zhu, He and Zhang, Yongfeng}, year={2024}, month=aug, language={en} }
 @article{Liu_Yuan_Fu_Jiang_Hayashi_Neubig_2023, title={{Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}}, volume={55}, ISSN={0360-0300, 1557-7341}, DOI={10.1145/3560815}, abstractNote={This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “promp    t-based learning.” Unlike traditional supervised learning, while learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.}, number={9}, journal={ACM Computing Surveys}, author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham}, year={2023}, month=sep, pages={1–35}, language={en} }
 @misc{Microsoft_2024, type={Jupyter Notebook}, title={{Guidance}}, rights={MIT}, url={https://github.com/guidance-ai/guidance}, abstractNote={A guidance language for controlling large language models.}, publisher={Microsoft, Inc.}, author={Microsoft}, year={2024}, month=nov }
 @inproceedings{Muller_Wolf_Andres_Desmond_Joshi_Ashktorab_Sharma_Brimijoin_Pan_Duesterwald_et_al._2021, address={Yokohama Japan}, title={{Designing Ground Truth and the Social Life of Labels}}, ISBN={978-1-4503-8096-6}, url={https://dl.acm.org/doi/10.1145/3411764.3445402}, DOI={10.1145/3411764.3445402}, abstractNote={Ground-truth labeling is an important activity in machine learning. Many studies have examined how crowdworkers apply labels to records in machine learning datasets. However, there have been few studies that have examined the work of domain experts when their knowledge and expertise are needed to apply labels.}, booktitle={{Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}}, publisher={ACM}, author={Muller, Michael and Wolf, Christine T. and Andres, Josh and Desmond, Michael and Joshi, Narendra Nath and Ashktorab, Zahra and Sharma, Aabhas and Brimijoin, Kristina and Pan, Qian and Duesterwald, Evelyn and Dugan, Casey}, year={2021}, month=may, pages={1–16}, language={en} }
 @inproceedings{Oh_Schuler_2024, address={Miami, Florida, USA}, title={{Leading Whitespaces of Language Models’ Subword Vocabulary Pose a Confound for Calculating Word Probabilities}}, url={https://aclanthology.org/2024.emnlp-main.202}, booktitle={{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Oh, Byung-Doh and Schuler, William}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={3464–3472} }
 @article{Ouyang_Wu_Jiang_Almeida_Wainwright_Mishkin_Zhang_Agarwal_Slama_Ray_et_al._2022, title={{Training Language Models to Follow Instructions With Human Feedback}}, url={http://arxiv.org/abs/2203.02155}, DOI={10.48550/arXiv.2203.02155}, abstractNote={Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.}, note={arXiv:2203.02155 [cs]}, number={arXiv:2203.02155}, publisher={arXiv}, author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan}, year={2022}, month=mar }
 @article{Ovsiannikov_Arbib_Mcneill_1999, title={{Annotation technology}}, volume={50}, rights={https://www.elsevier.com/tdm/userlicense/1.0/}, ISSN={10715819}, DOI={10.1006/ijhc.1999.0247}, number={4}, journal={International Journal of Human-Computer Studies}, author={Ovsiannikov, Ilia A. and Arbib, Michael A. and Mcneill, Thomas H.}, year={1999}, month=apr, pages={329–362}, language={en} }
 @article{Park_Wang_Berg_2024, title={{Grammar-Aligned Decoding}}, url={http://arxiv.org/abs/2405.21047}, DOI={10.48550/arXiv.2405.21047}, abstractNote={Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM’s output must follow a given grammar. In this paper, we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM’s distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM’s distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM’s distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.}, note={arXiv:2405.21047 [cs]}, number={arXiv:2405.21047}, publisher={arXiv}, author={Park, Kanghee and Wang, Jiayu and Berg-Kirkpatrick, Taylor and Polikarpova, Nadia and D’Antoni, Loris}, year={2024}, month=nov, language={en} }
 @inproceedings{Pimentel_Meister_2024, address={Miami, Florida, USA}, title={{How to Compute the Probability of a Word}}, url={https://aclanthology.org/2024.emnlp-main.1020}, abstractNote={Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.}, booktitle={{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Pimentel, Tiago and Meister, Clara}, editor={Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, year={2024}, month=nov, pages={18358–18375} }
 @article{Poesia_Polozov_Le_Tiwari_Soares_Meek_Gulwani_2022, title={{Synchromesh: Reliable code generation from pre-trained language models}}, url={http://arxiv.org/abs/2201.11227}, DOI={10.48550/arXiv.2201.11227}, abstractNote={Large pre-trained language models have been used to generate code, providing a ﬂexible interface for synthesizing programs from natural language speciﬁcations. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose SYNCHROMESH: a framework for substantially improving the reliability of pre-trained models for code generation. SYNCHROMESH comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, SYNCHROMESH feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor ﬁne-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.}, note={arXiv:2201.11227 [cs]}, number={arXiv:2201.11227}, publisher={arXiv}, author={Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit}, year={2022}, month=jan, language={en} }
 @inproceedings{Rabinovich_Stern_Klein_2017, address={Vancouver, Canada}, title={{Abstract Syntax Networks for Code Generation and Semantic Parsing}}, url={https://aclanthology.org/P17-1105}, DOI={10.18653/v1/P17-1105}, abstractNote={Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7\% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1\%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.}, booktitle={{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}}, publisher={Association for Computational Linguistics}, author={Rabinovich, Maxim and Stern, Mitchell and Klein, Dan}, editor={Barzilay, Regina and Kan, Min-Yen}, year={2017}, month=jul, pages={1139–1149} }
 @article{Radford_Wu_Child_Luan_Amodei_Sutskever_2018, title={{Language Models are Unsupervised Multitask Learners}}, abstractNote={Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}, author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, year={2018}, pages={24}, language={en} }
 @article{Reddy_Chen_Manning_2019, address={Cambridge, MA}, title={{CoQA: A Conversational Question Answering Challenge}}, volume={7}, DOI={10.1162/tacl_a_00266}, abstractNote={Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.\%, which is 23.4 points behind human performance (88.8\%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa.}, journal={Transactions of the Association for Computational Linguistics}, publisher={MIT Press}, author={Reddy, Siva and Chen, Danqi and Manning, Christopher D.}, editor={Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani}, year={2019}, pages={249–266} }
 @misc{Rickard_2024, type={Python}, title={{ReLLM}}, rights={MIT}, url={https://github.com/r2d4/rellm}, abstractNote={Exact structure out of any language model completion.}, author={Rickard, Matt}, year={2024}, month=nov }
 @article{Roh_Heo_Whang_2021, title={{A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective}}, volume={33}, ISSN={1558-2191}, DOI={10.1109/TKDE.2019.2946162}, abstractNote={Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.}, number={4}, journal={IEEE Transactions on Knowledge and Data Engineering}, author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong}, year={2021}, month=apr, pages={1328–1347} }
 @article{Roy_Thomson_Chen_Shin_Pauls_Eisner_Durme_2024, title={{BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing}}, url={http://arxiv.org/abs/2206.10668}, DOI={10.48550/arXiv.2206.10668}, abstractNote={Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-art methods for syntactic and semantic parsing when the model output is constrained to be valid.}, note={arXiv:2206.10668}, number={arXiv:2206.10668}, publisher={arXiv}, author={Roy, Subhro and Thomson, Sam and Chen, Tongfei and Shin, Richard and Pauls, Adam and Eisner, Jason and Durme, Benjamin Van}, year={2024}, month=jan }
 @inproceedings{Scholak_Schucher_Bahdanau_2021, address={Online and Punta Cana, Dominican Republic}, title={{PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models}}, url={https://aclanthology.org/2021.emnlp-main.779}, DOI={10.18653/v1/2021.emnlp-main.779}, abstractNote={Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.}, booktitle={{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Scholak, Torsten and Schucher, Nathan and Bahdanau, Dzmitry}, editor={Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau}, year={2021}, month=nov, pages={9895–9901} }
 @inproceedings{Sennrich_Haddow_Birch_2016, address={Berlin, Germany}, title={{Neural Machine Translation of Rare Words with Subword Units}}, url={https://aclanthology.org/P16-1162}, DOI={10.18653/v1/P16-1162}, booktitle={{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}}, publisher={Association for Computational Linguistics}, author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra}, editor={Erk, Katrin and Smith, Noah A.}, year={2016}, month=aug, pages={1715–1725} }
 @inproceedings{Shin_Lin_Thomson_Chen_2021, address={Online and Punta Cana, Dominican Republic}, title={{Constrained Language Models Yield Few-Shot Semantic Parsers}}, url={https://aclanthology.org/2021.emnlp-main.608}, DOI={10.18653/v1/2021.emnlp-main.608}, abstractNote={We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.}, booktitle={{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Shin, Richard and Lin, Christopher and Thomson, Sam and Chen, Charles and Roy, Subhro and Platanios, Emmanouil Antonios and Pauls, Adam and Klein, Dan and Eisner, Jason and Van Durme, Benjamin}, editor={Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau}, year={2021}, month=nov, pages={7699–7715} }
 @article{Shorten_Pierse_Smith_Cardenas_Sharma_Trengrove_van_Luijt_2024, title={{StructuredRAG: JSON Response Formatting with Large Language Models}}, rights={Creative Commons Attribution 4.0 International}, url={https://arxiv.org/abs/2408.11061}, DOI={10.48550/ARXIV.2408.11061}, abstractNote={The ability of Large Language Models (LLMs) to generate structured outputs, such as JSON, is crucial for their use in Compound AI Systems. However, evaluating and improving this capability remains challenging. In this work, we introduce StructuredRAG, a benchmark of six tasks designed to assess LLMs’ proficiency in following response format instructions. We evaluate two state-of-the-art LLMs, Gemini 1.5 Pro and Llama 3 8B-instruct with 4-bit quantization using two distinct prompting strategies. We introduce these prompting strategies as f-String and Follow the Format (FF) prompting. Across 24 experiments, we find an average success rate of 82.55\%. We further find a high variance in performance across tasks, models, and prompting strategies with success rates ranging from 0 to 100\%. We find that Llama 3 8B-instruct often performs competitively with Gemini 1.5 Pro. We observe that task complexity significantly influences performance, with tasks involving lists or composite object outputs proving more challenging. Our findings highlight the need for further research into improving the reliability and consistency of structured output generation in LLMs. We have open-sourced our experimental code and results at github.com/weaviate/structured-rag.}, publisher={arXiv}, author={Shorten, Connor and Pierse, Charles and Smith, Thomas Benjamin and Cardenas, Erika and Sharma, Akanksha and Trengrove, John and van Luijt, Bob}, year={2024} }
 @article{Si_Gan_Yang_Wang_Wang_Boyd-Graber_Wang_2023, title={{Prompting GPT-3 To Be Reliable}}, url={http://arxiv.org/abs/2210.09150}, abstractNote={Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely deﬁned term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions.1 Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.}, note={arXiv:2210.09150 [cs]}, number={arXiv:2210.09150}, publisher={arXiv}, author={Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan}, year={2023}, month=feb, language={en} }
 @article{Stengel-Eskin_Rawlins_Durme_2024, title={{Zero and Few-shot Semantic Parsing with Ambiguous Inputs}}, url={http://arxiv.org/abs/2306.00824}, DOI={10.48550/arXiv.2306.00824}, abstractNote={Despite the frequent challenges posed by ambiguity when representing meaning via natural language, it is often ignored or deliberately removed in tasks mapping language to formally-designed representations, which generally assume a one-to-one mapping between linguistic and formal representations. We attempt to address this shortcoming by introducing AmP, a framework, dataset, and challenge for translating ambiguous natural language to formal representations like logic and code. We define templates and generate data for five well-documented linguistic ambiguities. Using AmP, we investigate how several few-shot text-to-code systems handle ambiguity, introducing three new metrics. We find that large pre-trained models perform poorly at capturing the distribution of possible meanings without deliberate instruction. However, models are able to capture the distribution well when ambiguity is attested in their inputs. These results motivate a call for including ambiguity explicitly in datasets and promote considering the distribution of possible outputs when evaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing}, note={arXiv:2306.00824}, number={arXiv:2306.00824}, publisher={arXiv}, author={Stengel-Eskin, Elias and Rawlins, Kyle and Durme, Benjamin Van}, year={2024}, month=jan }
 @inproceedings{Tian_Mitchell_Zhou_Sharma_Rafailov_Yao_Finn_Manning_2023, address={Singapore}, title={{Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback}}, url={https://aclanthology.org/2023.emnlp-main.330}, DOI={10.18653/v1/2023.emnlp-main.330}, abstractNote={A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model’s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50\%.}, booktitle={{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}}, publisher={Association for Computational Linguistics}, author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher}, editor={Bouamor, Houda and Pino, Juan and Bali, Kalika}, year={2023}, month=dec, pages={5433–5442} }
 @article{Verma_Bhambri_Kambhampati_2024, title={{On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models}}, url={http://arxiv.org/abs/2405.13966}, DOI={10.48550/arXiv.2405.13966}, abstractNote={The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the “interleaving reasoning trace with action execution” or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.}, note={arXiv:2405.13966 [cs]}, number={arXiv:2405.13966}, publisher={arXiv}, author={Verma, Mudit and Bhambri, Siddhant and Kambhampati, Subbarao}, year={2024}, month=may, language={en} }
 @article{Veselovsky_Ribeiro_West_2023, title={{Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks}}, url={http://arxiv.org/abs/2306.07899}, DOI={10.48550/arXiv.2306.07899}, abstractNote={Large language models (LLMs) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of LLMs, human gold--standard annotations are key to understanding the capabilities of LLMs and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of LLM usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46\% of crowd workers used LLMs when completing the task. Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/GPTurk}, note={arXiv:2306.07899}, number={arXiv:2306.07899}, publisher={arXiv}, author={Veselovsky, Veniamin and Ribeiro, Manoel Horta and West, Robert}, year={2023}, month=jun }
 @inproceedings{Wang_Singh_Michael_Hill_Levy_Bowman_2018, address={Brussels, Belgium}, title={{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}}, url={https://aclanthology.org/W18-5446}, DOI={10.18653/v1/W18-5446}, abstractNote={Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.}, booktitle={{Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}}, publisher={Association for Computational Linguistics}, author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel}, editor={Linzen, Tal and Chrupała, Grzegorz and Alishahi, Afra}, year={2018}, month=nov, pages={353–355} }
 @article{Wang_Wang_Wang_Cao_Saurous_Kim_2023, title={{Grammar Prompting for Domain-Specific Language Generation with Large Language Models}}, url={http://arxiv.org/abs/2305.19234}, DOI={10.48550/arXiv.2305.19234}, abstractNote={Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We propose emph{grammar prompting}, a simple approach to enable LLMs to use external knowledge and domain-specific constraints, expressed through a grammar in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and SMILES-based molecule generation.}, note={arXiv:2305.19234}, number={arXiv:2305.19234}, publisher={arXiv}, author={Wang, Bailin and Wang, Zi and Wang, Xuezhi and Cao, Yuan and Saurous, Rif A. and Kim, Yoon}, year={2023}, month=nov }
 @inproceedings{Wang_Shang_2014, title={{A new active labeling method for deep learning}}, ISSN={2161-4407}, url={https://ieeexplore.ieee.org/document/6889457/?arnumber=6889457}, DOI={10.1109/IJCNN.2014.6889457}, abstractNote={Deep learning has been shown to achieve outstanding performance in a number of challenging real-world applications. However, most of the existing works assume a fixed set of labeled data, which is not necessarily true in real-world applications. Getting labeled data is usually expensive and time consuming. Active labelling in deep learning aims at achieving the best learning result with a limited labeled data set, i.e., choosing the most appropriate unlabeled data to get labeled. This paper presents a new active labeling method, AL-DL, for cost-effective selection of data to be labeled. AL-DL uses one of three metrics for data selection: least confidence, margin sampling, and entropy. The method is applied to deep learning networks based on stacked restricted Boltzmann machines, as well as stacked autoencoders. In experiments on the MNIST benchmark dataset, the method outperforms random labeling consistently by a significant margin.}, booktitle={{2014 International Joint Conference on Neural Networks (IJCNN)}}, author={Wang, Dan and Shang, Yi}, year={2014}, month=jul, pages={112–119} }
 @article{Wang_2024, title={{Guiding Large Language Models to Generate Computer-Parsable Content}}, rights={Creative Commons Attribution 4.0 International}, url={https://arxiv.org/abs/2404.05499}, DOI={10.48550/ARXIV.2404.05499}, abstractNote={We propose a method to guide Large Language Models (LLMs) in generating structured content adhering to specific conventions without fine-tuning. By utilizing coroutine-based content generation constraints through a pre-agreed context-free grammar (CFG), LLMs are directed during decoding to produce formal language compliant outputs. This enhances stability and consistency in generating target data structures, types, or instructions, reducing application development complexities. Experimentally, error rates of GPT-2 and Gemma exceed 95\% for DSLs longer than 36 and 282 tokens, respectively. We introduce YieldLang, a coroutine-based DSL generation framework, and evaluate it with LLMs on various tasks including JSON and Mermaid flowchart generation. Compared to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs requiring only about 16.5\% of the samples to generate JSON effectively. This enhances usability of LLM-generated content for computer programs.}, publisher={arXiv}, author={Wang, Jiaye}, year={2024} }
 @inproceedings{Wang_Liu_Xu_Zhu_Zeng_2021, address={Punta Cana, Dominican Republic}, title={{Want To Reduce Labeling Cost? GPT-3 Can Help}}, url={https://aclanthology.org/2021.findings-emnlp.354}, DOI={10.18653/v1/2021.findings-emnlp.354}, abstractNote={Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-speciﬁc and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion param-eters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We ﬁnd that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50\% to 96\% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generaliz-able to many practical applications.}, booktitle={{Findings of the Association for Computational Linguistics: EMNLP 2021}}, publisher={Association for Computational Linguistics}, author={Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael}, year={2021}, pages={4195–4205}, language={en} }
 @article{Wen-Yi_Mimno_2023, title={{Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings}}, url={http://arxiv.org/abs/2311.18034}, DOI={10.48550/arXiv.2311.18034}, abstractNote={Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2\% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models.}, note={arXiv:2311.18034}, number={arXiv:2311.18034}, publisher={arXiv}, author={Wen-Yi, Andrea W. and Mimno, David}, year={2023}, month=nov }
 @article{Willard_Louf_2023, title={{Efficient Guided Generation for Large Language Models}}, rights={Creative Commons Attribution 4.0 International}, url={https://arxiv.org/abs/2307.09702}, DOI={10.48550/ARXIV.2307.09702}, abstractNote={In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model’s vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines}, publisher={arXiv}, author={Willard, Brandon T. and Louf, Rémi}, year={2023} }
 @article{Wu_Leung_2017, title={{Can Likert Scales be Treated as Interval Scales?—A Simulation Study}}, volume={43}, ISSN={0148-8376, 1540-7314}, DOI={10.1080/01488376.2017.1329775}, number={4}, journal={Journal of Social Service Research}, author={Wu, Huiping and Leung, Shing-On}, year={2017}, month=aug, pages={527–532}, language={en} }
 @article{Yao_Zhao_Yu_Du_Shafran_Narasimhan_Cao_2023, title={{ReAct: Synergizing Reasoning and Acting in Language Models}}, url={http://arxiv.org/abs/2210.03629}, DOI={10.48550/arXiv.2210.03629}, abstractNote={While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.}, note={arXiv:2210.03629 [cs]}, number={arXiv:2210.03629}, publisher={arXiv}, author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan}, year={2023}, month=mar, language={en} }
 @article{Zhang_Kishore_Wu_Weinberger_Artzi_2020, title={{BERTScore: Evaluating Text Generation with BERT}}, url={http://arxiv.org/abs/1904.09675}, DOI={10.48550/arXiv.1904.09675}, abstractNote={We propose BERTSCORE, an automatic evaluation metric for text generation. Analogously to common metrics, BERTSCORE computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTSCORE correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTSCORE is more robust to challenging examples when compared to existing metrics.}, note={arXiv:1904.09675 [cs]}, number={arXiv:1904.09675}, publisher={arXiv}, author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav}, year={2020}, month=feb, language={en} }
 @article{Zhang_Lu_Tran_Schuster_Metzler_Lin_2024, title={{Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among Subwords in Multilingual Language Models}}, url={http://arxiv.org/abs/2411.04530}, abstractNote={Human understanding of language is robust to different word choices as far as they represent similar semantic concepts. To what extent does our human intuition transfer to language models, which represent all subwords as distinct embeddings? In this work, we take an initial step on measuring the role of shared semantics among subwords in the encoder-only multilingual language models (mLMs). To this end, we form “semantic tokens” by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on 5 heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections on the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we found the zero-shot results with semantic tokens are on par or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for crosslingual transferring.}, note={arXiv:2411.04530 [cs]}, number={arXiv:2411.04530}, publisher={arXiv}, author={Zhang, Xinyu and Lu, Jing and Tran, Vinh Q. and Schuster, Tal and Metzler, Donald and Lin, Jimmy}, year={2024}, month=nov, language={en} }
 @article{Zhu_Wang_Zhou_Wang_Chen_Wang_Yang_Ye_Zhang_Gong_et_al._2023, title={{PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts}}, rights={Creative Commons Attribution 4.0 International}, url={https://arxiv.org/abs/2306.04528}, DOI={10.48550/ARXIV.2306.04528}, abstractNote={The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptRobust, a robustness benchmark designed to measure LLMs’ resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present a comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users.}, publisher={arXiv}, author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Gong, Neil Zhenqiang and Xie, Xing}, year={2023} }
@article{vandermaatenVisualizingDataUsing2008, title = {Visualizing Data Using T-{{SNE}}.}, author = {{Van der Maaten}, Laurens and Hinton, Geoffrey}, year = {2008}, journal = {Journal of machine learning research}, volume = {9}, number = {11}, urldate = {2024-11-28}}
@inproceedings{cerSemEval2017Task12017,
  title = {{{SemEval-2017 Task}} 1: {{Semantic Textual Similarity Multilingual}} and {{Crosslingual Focused Evaluation}}},
  shorttitle = {{{SemEval-2017 Task}} 1},
  booktitle = {Proceedings of the 11th {{International Workshop}} on {{Semantic Evaluation}}           ({{SemEval-2017}})},
  author = {Cer, Daniel and Diab, Mona and Agirre, Eneko and {Lopez-Gazpio}, Inigo and Specia, Lucia},
  year = {2017},
  pages = {1--14},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/S17-2001},
  urldate = {2024-10-31},
  abstract = {Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).},
  langid = {english},
}
 @article{Bruni_Tran_Baroni_2014, title={{Multimodal Distributional Semantics}}, volume={49}, journal={Journal of artificial intelligence research}, author={Bruni, Elia and Tran, Nam-Khanh and Baroni, Marco}, year={2014}, pages={1–47} }
 @misc{DataCanary_hilfialkaff_Jiang_Risdal_Dandekar_tomtung_2017, title={{Quora Question Pairs}}, url={https://kaggle.com/quora-question-pairs}, abstractNote={Can you identify question pairs that have the same intent?}, author={DataCanary and hilfialkaff and Jiang, Lili and Risdal, Meg and Dandekar, Nikhil and tomtung}, year={2017}, language={en} }
 @inproceedings{Lin_Wang_Tong_Wang_Guo_Wang_Shang_2023, address={Singapore}, title={{ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation}}, url={https://aclanthology.org/2023.findings-emnlp.311}, DOI={10.18653/v1/2023.findings-emnlp.311}, abstractNote={Despite remarkable advances that large language models have achieved in chatbots nowadays, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media contents, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark constructed based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference when compared to social media contents. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.}, booktitle={{Findings of the Association for Computational Linguistics: EMNLP 2023}}, publisher={Association for Computational Linguistics}, author={Lin, Zi and Wang, Zihan and Tong, Yongqi and Wang, Yangkun and Guo, Yuxin and Wang, Yujia and Shang, Jingbo}, editor={Bouamor, Houda and Pino, Juan and Bali, Kalika}, year={2023}, month=dec, pages={4694–4702} }
 @article{Dettmers_Lewis_Belkada_Zettlemoyer_2022, title={{LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}}, url={http://arxiv.org/abs/2208.07339}, DOI={10.48550/arXiv.2208.07339}, abstractNote={Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.}, note={arXiv:2208.07339 [cs]}, number={arXiv:2208.07339}, publisher={arXiv}, author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke}, year={2022}, month=nov }
 @book{Mauldin_2020, title={{Foundations of Social Work Research}}, ISBN={978-1-64816-991-5}, url={https://uta.pressbooks.pub/foundationsofsocialworkresearch/}, publisher={Mavs Open Press}, author={Mauldin, Rebecca L.}, year={2020}, month=jan, language={en} }
@misc{lanhamMeasuringFaithfulnessChainofThought2023,
  title = {Measuring {{Faithfulness}} in {{Chain-of-Thought Reasoning}}},
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and {Telleen-Lawton}, Timothy and Hume, Tristan and {Hatfield-Dodds}, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13702},
  eprint = {2307.13702},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-11},
  abstract = {Large language models (LLMs) perform better when they produce step-by-step, ``Chain-ofThought'' (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
}
@article{vaswani2017attention,
  title={{Attention is all you need}},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@misc{abdinPhi3TechnicalReport2024,
  title = {Phi-3 {{Technical Report}}: {{A Highly Capable Language Model Locally}} on {{Your Phone}}},
  shorttitle = {Phi-3 {{Technical Report}}},
  author = {Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Martin and Cai, Qin and Chaudhary, Vishrav and Chen, Dong and Chen, Dongdong and Chen, Weizhu and Chen, Yen-Chun and Chen, Yi-Ling and Cheng, Hao and Chopra, Parul and Dai, Xiyang and Dixon, Matthew and Eldan, Ronen and Fragoso, Victor and Gao, Jianfeng and Gao, Mei and Gao, Min and Garg, Amit and Giorno, Allie Del and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Hu, Wenxiang and Huynh, Jamie and Iter, Dan and Jacobs, Sam Ade and Javaheripi, Mojan and Jin, Xin and Karampatziakis, Nikos and Kauffmann, Piero and Khademi, Mahoud and Kim, Dongwoo and Kim, Young Jin and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Li, Yunsheng and Liang, Chen and Liden, Lars and Lin, Xihui and Lin, Zeqi and Liu, Ce and Liu, Liyuan and Liu, Mengchen and Liu, Weishung and Liu, Xiaodong and Luo, Chong and Madan, Piyush and Mahmoudzadeh, Ali and Majercak, David and Mazzola, Matt and Mendes, Caio C{\'e}sar Teodoro and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and {Perez-Becker}, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Ren, Liliang and de Rosa, Gustavo and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Shen, Yelong and Shukla, Swadheen and Song, Xia and Tanaka, Masahiro and Tupini, Andrea and Vaddamanu, Praneetha and Wang, Chunyu and Wang, Guanhua and Wang, Lijuan and Wang, Shuohang and Wang, Xin and Wang, Yu and Ward, Rachel and Wen, Wen and Witte, Philipp and Wu, Haiping and Wu, Xiaoxia and Wyatt, Michael and Xiao, Bin and Xu, Can and Xu, Jiahang and Xu, Weijian and Xue, Jilong and Yadav, Sonali and Yang, Fan and Yang, Jianwei and Yang, Yifan and Yang, Ziyi and Yu, Donghan and Yuan, Lu and Zhang, Chenruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
  year = {2024},
  month = aug,
  number = {arXiv:2404.14219},
  eprint = {2404.14219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.14219},
  urldate = {2025-02-12},
  abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\%, 78\% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@misc{qwenQwen25TechnicalReport2025,
  title = {Qwen2.5 {{Technical Report}}},
  author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  year = {2025},
  month = jan,
  number = {arXiv:2412.15115},
  eprint = {2412.15115},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.15115},
  urldate = {2025-02-12},
  abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
}
@misc{teamGemma2Improving2024,
  title = {Gemma 2: {{Improving Open Language Models}} at a {{Practical Size}}},
  shorttitle = {Gemma 2},
  author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Bachem, Olivier and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and {Choquette-Choo}, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozi{\'n}ska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and {Klimczak-Pluci{\'n}ska}, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and van Amersfoort, Joost and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and G{\"o}rner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Cogan, Sarah and Perrin, Sarah and Arnold, S{\'e}bastien M. R. and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
  year = {2024},
  month = oct,
  number = {arXiv:2408.00118},
  eprint = {2408.00118},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00118},
  urldate = {2025-02-12},
  abstract = {In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
 }
@misc{soldainiDolmaOpenCorpus2024,
  title = {Dolma: An {{Open Corpus}} of {{Three Trillion Tokens}} for {{Language Model Pretraining Research}}},
  shorttitle = {Dolma},
  author = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and Hofmann, Valentin and Jha, Ananya Harsh and Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind and Walsh, Pete and Zettlemoyer, Luke and Smith, Noah A. and Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle},
  year = {2024},
  month = jun,
  number = {arXiv:2402.00159},
  eprint = {2402.00159},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.00159},
  urldate = {2024-11-28},
  abstract = {Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
}