\section{Related Work}
Many effective UQ methods, such as deep ensembles~\citep{NIPS2017_9ef2ed4b} and Monte Carlo (MC) dropout~\citep{gal2016dropout}, require sampling multiple predictions from a model, which leads to substantial computational and memory overheads. A key challenge in UQ is developing techniques that balance effectiveness with computational efficiency. Among the most promising approaches in this regard are density-based methods~\cite{lee2018simple,NEURIPS2020_543e8374,ddu_amersfoort,nuq_kotelevskii,yoo-etal-2022-detection}. These methods leverage latent representations of instances to model the training data distribution, then estimate how likely a new instance belongs to that distribution.~\citet{lee2018simple} propose to use Mahalanobis Distance (MD) as a measure of uncertainty for out-of-distribution detection in computer vision tasks. \citet{podolskiy2021revisiting} adapt  MD  to out-of-distribution in text classification tasks. \citet{vazhentsev-etal-2022-uncertainty,vazhentsev-etal-2023-efficient} show that it also provides high performance in selective text classification.  

  For LLMs, \citet{fomicheva-etal-2020-unsupervised} and~\citet{kuhn2023semantic} proposed UQ methods that sample multiple predictions and leverage their diversity. In the context of black-box LLMs, where we have no access to the logits or embeddings of a model, \citet{fomicheva-etal-2020-unsupervised} propose the use of lexical dissimilarity of sampled texts as a measure of uncertainty. \citet{lin2023generating} leverage a similarity matrix between responses for deeper analysis of the diversity of the sampled generations. Some methods also combine sampling diversity measures with the probability of each generation~\cite{kuhn2023semantic,duan-etal-2024-shifting,nikitin2024kernel,cheng-vlachos-2024-measuring,chen2024eigenscore,vashurin2025cocoageneralizedapproachuncertainty}.

  Recently, it was demonstrated that MD is an efficient approach for out-of-distribution detection in sequence-to-sequence models~\cite{vazhentsev-etal-2023-efficient,ren2023outofdistribution,darrin-etal-2023-rainproof}. However, for selective generation tasks, density-based methods so far have substantially underperformed compared to trivial baselines~\citep{vashurin2024benchmakring}.

  Supervised methods are another research direction for UQ of LLMs. \citet{azaria-mitchell-2023-internal} demonstrate that the internal states of the model contain information about uncertainty, and propose to train a multi-layer perceptron over the hidden LLM representation to predict the truthfulness of the model responses. \citet{he-etal-2024-llm} enhance this idea by training a deep neural network with recurrent and convolutional layers. Furthermore, this method uses embeddings from all layers and incorporates features based on the probability and the dynamics of the generated tokens through layers. In contrast to these methods, we employ a simple linear model, but focus on more accurate feature extraction from internal layers, using well-established density-based UQ methods.