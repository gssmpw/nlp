\begin{table}[h]\scriptsize  \centering\begin{tabular}{l|c|c}
\toprule
\textbf{UQ Method} & \textbf{\multirowcell{Runtime \\ per batch}} & \textbf{\multirowcell{Overhead}} \\
\midrule

MSP & $2.10$\tiny{$\pm$$1.31$} & - \\\midrule
DegMat NLI Score Entail. & $9.47$\tiny{$\pm$$3.41$} & 350\% \\
Lexical Similarity ROUGE-L & $8.69$\tiny{$\pm$$3.31$} & 315\% \\
Semantic Entropy & $9.47$\tiny{$\pm$$3.41$} & 350\% \\
SAR & $16.89$\tiny{$\pm$$6.85$} & 700\% \\
\midrule
SAPLMA & $2.10$\tiny{$\pm$$1.31$} & \textbf{0.04}\% \\
Factoscope & $8.45$\tiny{$\pm$$5.92$} & 300\% \\
\midrule
HUQ-SATRMD & $2.21$\tiny{$\pm$$1.36$} & \underline{5.30}\% \\
SATRMD+MSP & $2.26$\tiny{$\pm$$1.38$} & 7.61 \%\\

\bottomrule
\end{tabular}
\caption{\label{tab:comp_efficiency} The evaluation of the inference runtime of UQ methods measured on all test instances from all datasets with predictions from Llama 8b v3.1. The best results are in bold. The second best results are underlined.}
\end{table}