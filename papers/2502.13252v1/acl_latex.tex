% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{hyperref}
\usepackage{multirow} 
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{colortbl}

\usepackage{booktabs}

\newcommand{\jiayi}[1]{{\color{red}[Jiayi: {#1}]}}
\newcommand{\yaolu}[1]{{\color{orange}[Yao: {#1}]}}
\newcommand{\pontus}[1]{{\color{blue}[Pontus: {#1}]}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\usepackage{xcolor,pifont}
\newcommand*\colourcheck[1]{%
  \expandafter\newcommand\csname #1check\endcsname{\textcolor{#1}{\ding{52}}}%
}
\newcommand*\colourcross[1]{%
  \expandafter\newcommand\csname #1cross\endcsname{\textcolor{#1}{\ding{56}}}%
}
\colourcheck{green}
\colourcheck{orange}
\colourcross{red}


\newcommand{\thedata}{\textit{TransWebEdu}}
\newcommand{\themodel}{\textit{TransWebLLM}}
\newcommand{\themodelweb}{\textit{TransWebLLM-web}}
\newcommand{\themodelmc}{\textit{TransWebLLM-mcSyn}}
\newcommand{\themodelcool}{\textit{TransWebLLM-cool}}


\newcommand{\cuatrollm}{\textit{CuatroLLM}}
\newcommand{\cuatrodata}{\textit{TransWebEdu}}


%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{%
    %Multilingual Pretraining using a Large, Single Source Language, Machine-translated Corpus \\ %
    %
    % Pontus: This is not a descriptive title though. No LLM, unclear about the data being human translations or not, etc.
    %Multilingual Pretraining with Translation Data%
    %
    Multilingual Language Model Pretraining using\\ Machine-translated Data%
}

\author{Jiayi Wang$^{\alpha}$$^\ast$ \quad
Yao Lu$^{\alpha}$$^\ast$ \quad
Maurice Weber$^{\beta}$ \quad
Max Ryabinin$^{\beta}$ \quad
David Adelani$^{\gamma}$ \\
\textbf{Yihong Chen}$^{\alpha}$ \quad
\textbf{Raphael Tang}  \quad
\textbf{Pontus Stenetorp}$^{\alpha, \delta}$ \\
$^{\alpha}$Centre for Artificial Intelligence, University College London \quad \\
$^{\beta}$Together AI \quad $^{\gamma}$Mila, McGill University, Canada CIFAR AI Chair \quad \\
% $^{\omega}$University of Waterloo \\
$^{\delta}$Research and Development Center for Large Language Models, National Institute of Informatics\\
% \texttt{
% jiayi.lin.wang@ucl.ac.uk} \quad 
\texttt{\{jiaywang,yao.lu,yihong.chen,p.stenetorp\}@cs.ucl.ac.uk}, \\
\texttt{\{maurice,mryab\}@together.ai}, \texttt{david.adelani@mila.quebec} \\
% \texttt{tang.raphael@gmail.com}
}


\begin{document}
\maketitle
\blfootnote{$^\ast$ Both authors contribute equally to this work.}

\begin{abstract}
High-resource languages such as English, enables the pretraining of high-quality large language models~(LLMs). 
% from community efforts that provide large amounts of high-quality pretraining data.
The same can not be said for most other languages 
as LLMs still underperform for non-English languages, 
% as leading LLMs still show significant performance gaps for non-English languages,
likely due to a gap in the quality and diversity of the available multilingual pretraining corpora.
%
In this work, we find that machine-translated texts from a single high-quality source language can contribute significantly to the pretraining quality of multilingual LLMs.
%
We translate \textit{FineWeb-Edu}, a high-quality English web dataset, into nine languages, resulting in a $1.7$-trillion-token dataset, which we call \thedata{} and pretrain a $1.3$B-parameter model, \themodel{}, from scratch on this dataset.
%
Across nine non-English reasoning tasks, we show that \themodel{} matches or outperforms state-of-the-art multilingual models trained using closed data, such as \textit{Llama3.2}, \textit{Qwen2.5}, and \textit{Gemma}, despite using an order of magnitude less data.
%, such as about 18\% of the tokens used for \textit{Llama3.2}'s training.
%
We demonstrate that adding less than 5\% of \thedata{} as domain-specific pretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian, Swahili, and Welsh understanding and commonsense reasoning tasks.
%
To promote reproducibility, we release our corpus, models, and training pipeline under Open Source Initiative-approved licenses.

\end{abstract}

\section{Introduction}
%
Multilingual language models have shown remarkable potential for natural language processing~\citep{dubey2024llama, qwen2025qwen25technicalreport, team2024gemma}, yet their development faces a fundamental challenge: the scarcity of high-quality training data across diverse languages~\cite{joshi2020state,kreutzer2022quality}.
%
While English benefits from extensive, diverse, and well-curated datasets, other languages---even widely spoken ones---struggle to match this standard. 
%
Current practices of collecting and filtering web data from the Internet lag behind English performance due to the Internet's inherent English-centric nature~\citep{bender2021dangers,imanigooghari-etal-2023-glot500}.

One direction to address the data quality issue is using pretrained language models to generate high-quality synthetic data~\cite{maini2024rephrasing,abdin2024phi}.
%
However, language model-based data generation is not suitable for multilingual research due to limited language coverage.
%
For example, one of the current strongest models with multilingual support, \textit{Llama 3.2}~\citep{dubey2024llama}, officially supports fewer than 20 languages.
%
For low-resource languages like Welsh, the near-absence of well-supported large language models makes data generation practically impossible.
%
Furthermore, even with proprietary language models such as GPT-4, which might have better coverage, generation at the trillion-token scale is non-trivial.\footnote{The cost would approximately exceed millions of dollars.}

To address the limited language coverage issue, machine translation~(MT) offers a potential solution for creating multilingual datasets using task-specific neural machine translation (NMT) models, with the goal of preserving contextual, idiomatic, and pragmatic nuances across languages.
%
However, this line of research~\citep{urbizu2023mtrescue,doshi2024translationese} remains underexplored for pretraining.
%

In this work, we introduce \thedata{}, a large-scale multilingual corpus created by translating a subset of~\textit{FineWeb-edu}~\citep{lozhkov2024fineweb-edu}, a high-quality English dataset, into nine diverse languages using \textit{NLLB-200-1.3B}~\citep{costa2022no}. 
%
The resulting corpus spans ten languages—Arabic, French, German, Indonesian, Italian, Russian, Spanish, Swahili, Welsh, and English—exceeding $100$B tokens per language, with a total of 1.7 trillion tokens.
%
% In this work, we explore using an MT model
% % rather than prompting large language models, 
% to generate multilingual datasets by translating a trillion-token scale corpus covering ten languages.
% %
% We present a large-scale multilingual corpus~(\thedata{}) created using the~\textit{NLLB-200-1.3B}~\citep{costa2022no} translation model.
% %
% We translate FineWeb-edu, a high-quality English dataset containing 100B tokens, into nine languages representing diverse language families: Arabic, French, German, Indonesian, Italian, Russian, Spanish, Swahili, and Welsh.
% %
% The resulting multilingual corpus contains $100$B+ tokens for each of the ten languages~(including English), totaling $1.7$ trillion tokens.
% %
%
We evaluate the efficiency of \thedata{} by pretraining a $1.3$B-parameter language model.
%
Contrary to common concerns about text quality when using ``small'' sentence-level NMT models, we demonstrate that it can yield substantial improvements on pretraining performance. 
%
For instance, \thedata{} improves Swahili by 10\% over the SOTA \textit{Gemma}~\citep{team2024gemma} and enhances Arabic and Italian, surpassing it by 5\% and 2.6\%, respectively.

% ---including issues with text quality and the fact that most models trained at the sentence level.
%
In summary, our contributions are as follows: 
\begin{enumerate}
    \item We translate a high-quality, pretraining-scale English corpus into nine diverse languages, including three medium- and low-resource languages, using a sentence-level NMT model, creating one of the largest machine-generated multilingual datasets to date, \thedata{}, containing 1.7T tokens.
    \item We pretrain \themodel{}, a 1.3B-parameter model, from scratch on our \thedata{} dataset. The model establishes SOTA-level multilingual performance on a wide range of reasoning tasks across nine languages despite using significantly fewer tokens than leading models trained on closed-source data, including \textit{Gemma}, \textit{Qwen2.5}, and \textit{Llama3.2}.

    
    % Despite using significantly fewer tokens—only 18\% of \textit{Llama3.2}'s training data matches or surpasses SOTA multilingual models trained on closed-source data, including \textit{Gemma}, \textit{Qwen2.5}, and \textit{Llama3.2}, on a wide range of reasoning tasks across nine languages.

    \item We release our corpus, models, and training pipeline under open licenses to advance reproducibility in multilingual NLP.
\end{enumerate}

% \begin{enumerate}
    % \item To the best of our knowledge, we are the first to translate a high-quality, pretraining-scale English corpus into $9$ non-English languages, including $3$ medium- and low-resource languages, using a sentence-level neural machine translation model, resulting in the 1.7T-token multilingual dataset \thedata{}.
    % \item We pretrain \themodel{}, a 1.3B-parameter model, from scratch using \thedata{}. Despite using significantly fewer tokens---only about 18\% of \textit{Llama3.2}'s and 8\% of \textit{Qwen2.5}'s training data---\themodel{} matches or outperforms state-of-the-art multilingual models trained on closed data, including \textit{Gemma}, \textit{Qwen2.5}, and \textit{Llama3.2} across $9$ non-English understanding and reasoning tasks. This highlights an efficient approach to scaling medium- and low-resource languages for multilingual LLMs.  
    % \item We release our corpus, models, and training pipeline under open licenses to promote reproducibility and accessibility in multilingual NLP research.
    % \item  We translate a high-quality, pretraining-scale English corpus into $9$ diverse languages, including $3$ medium- and low-resource languages, using a sentence-level NMT model, creating the $1.7T$-token dataset, \thedata{}.
    % \item We pretrain \themodel{}, a 1.3B-parameter model, from scratch on \thedata{}. Despite using significantly fewer tokens—only 18\% of \textit{Llama3.2}'s and 8\% of \textit{Qwen2.5}'s training data—\themodel{} matches or surpasses SOTA multilingual models trained on closed data, including \textit{Gemma}, \textit{Qwen2.5}, and \textit{Llama3.2}, across $9$ non-English reasoning tasks.
    % \item We release our corpus, models, and training pipeline under open licenses to advance reproducibility in multilingual NLP.
% \end{enumerate}


%     \item To our knowledge, we are the first to translate a high-quality, pretraining-scale English corpus into multiple languages, creating a $300$B-token dataset: \thedata{}.
%     \item We pretrain a $1.3$B-parameter model, \themodel{}, from scratch using \thedata{}. Despite using an order of magnitude less data (such as about 6\% of the tokens used for \textit{Llama3.2}'s training), 
%     \themodel{} matches or outperforms state-of-the-art multilingual models trained on closed data, including \textit{Gemma}, \textit{Llama3.2}, \textit{Qwen2}, and \textit{EuroLLM}, across five non-English reasoning tasks.
%     \item We release our corpus, models, and training pipeline under open licenses to promote reproducibility in multilingual NLP research.
% \end{enumerate}


% Our method, which uses sentence-level translation model and concatenates their outputs into pseudo-documents for multilingual pretraining, demonstrates both effectiveness and scalability in bridging the gap between MT and multilingual pretraining research. 
%
% Despite common concerns about using 'weak and small' machine translation models - including issues with text quality and the fact that most models operate at the sentence level - we demonstrate that it can yield substantial improvements in pretraining performance, \yaolu{matches the level of llama3?}. Our exploration suggests that machine translation models remain a valuable tool for expanding multilingual pretraining resources.



\section{Pretraining with Machine-translated Multilingual Data}
This section outlines our pipeline for constructing a machine-translated corpus and pretraining a multilingual language model on it.
%
% Our process consists of the following steps. First, we select a high-quality English pretraining dataset. Second, we segment the documents into sentences, translate these sentences into target languages using a sentence-level neural machine translation model, and then reconstruct the documents by concatenating the translated sentences for each target language. Finally, we train a language model from scratch on the resulting multilingual data mixture to validate the effectiveness of the pretraining corpus.
%
Our process consists of the following steps: 
\textbf{(i)} We select a high-quality English pretraining dataset; \textbf{(ii)} We segment English documents into sentences, translate each sentence into target languages using a sentence-level neural machine translation model, and reconstruct the documents by concatenating the translated sentences; \textbf{(iii)} We train a language model from scratch on the resulting multilingual data mixture and validate the effectiveness of the pretraining corpus.
% \begin{enumerate}
%     \item We select a high-quality English pretraining dataset.
    
%     \item We segment English documents into sentences, translate each sentence into target languages using a sentence-level neural machine translation model, and reconstruct the documents by concatenating the translated sentences.

%     \item We train a language model from scratch on the resulting multilingual data mixture to validate the effectiveness of the pretraining corpus.
% \end{enumerate}

\subsection{Pretraining Data Curation}
LLMs are predominantly trained on document-level text data, as demonstrated by prominent model families such as Llama~\citep{dubey2024llama} and Gemma~\citep{team2024gemma}. In keeping with this established approach, we structure our translation pretraining data at the document level. Translation data typically comprises the following key components: source data, target languages, translation model, and a translation approach for composing document-level translations.

\paragraph{Source data}
The quality of a pretraining dataset significantly influences the performance of LLMs. Among high-resource languages, English stands out for its vast linguistic diversity and extensive knowledge base~\citep{joshi2020state, kreutzer2022quality}. This makes it an excellent choice for sourcing high-quality web data for model training.

The \textit{FineWeb-Edu} dataset~\citep{lozhkov2024fineweb-edu}, a subset of \textit{FineWeb}~\citep{penedo2024finewebdatasetsdecantingweb}, has demonstrated its quality and efficacy in the development of LLMs. This dataset, constructed using scalable automated high-quality annotations for educational value, has been instrumental in training both English-centric models like \textit{GPT-2}~\citep{Karpathy2022,Karpathy2024} and multilingual models such as \textit{EuroLLM}~\citep{martins2024eurollm}, and thus emerges as a suitable candidate for our source dataset. It originally consists $1.3$ trillions tokens of educational contents, and our focus is on the sample-$100$BT subset\footnote{\href{https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu}{\texttt{hf.co/datasets/HuggingFaceFW/fineweb-edu}}}, which contains about $100$ billion \textit{GPT-2} tokens randomly sampled from the whole dataset. 

\paragraph{Target Languages}
We focus on $9$ target languages from diverse linguistic families, ensuring broad representation. From the \textit{\textbf{Indo-European}} family, we include Germanic languages: English (en) and German (de); Romance languages: French (fr), Spanish (es), and Italian (it); a Celtic language: Welsh (cy); and a Slavic language: Russian (ru). Additionally, we include languages from distinct families: \textit{\textbf{Afroasiatic}} (Arabic (ar)), \textit{\textbf{Niger-Congo}} (Swahili (sw)), and \textit{\textbf{Austronesian}} (Indonesian (id)). According to~\citet{joshi2020state} and~\citet{ezeani-etal-2019-leveraging}, Indonesian is categorized as a medium-resource language, while Swahili and Welsh are classified as low-resource languages. The remaining languages are considered high-resource. 
% This linguistic diversity enables us to evaluate our approach across a wide range of grammatical structures and typological features.
% To investigate whether the strengths of high-quality English data can be effectively transferred to these nine target languages,  
We translate the $100$BT \textit{FineWeb-Edu} corpus from English into the target languages to bridge resource gaps and transfer the rich knowledge representations encoded in English data, ultimately enhancing the inclusion and representation of these target languages in multilingual LLMs.


% To achieve this, we translate the $100$BT \textit{FineWeb-Edu} corpus from English into these nine target languages. Our goal is to investigate whether the strengths of high-quality English data can be effectively transferred to other languages, including medium- and low-resource ones, through translation. By leveraging translation, we aim to bridge resource gaps and convey the rich linguistic and knowledge representations encoded in English data, ultimately enhancing the inclusion of these languages in multilingual LLMs.

\paragraph{Translation Model}
% @JY: introduce NLLB model, and their variants, why we choose its smallest version 1.3B model, and its MT performance/ranking on ten languages.
Although various models support translation tasks, including neural machine translation (NMT) models and LLMs~\citep{stahlberg2020neural,alves2024tower,martins2024eurollm}. However, exploration of LLMs' translation performance for medium- and low-resource languages remains limited. For example, TowerLLM~\citep{alves2024tower}, an open multilingual LLM designed for translation-related tasks, supports only 10 languages. In contrast, NMT models are generally more accessible and widely adopted for translation, particularly for low-resource languages, due to more than a decade of dedicated development in this field~\citep{stahlberg2020neural}. A prominent example is NLLB-200~\citep{costa2022no}, an open-source suit of models capable of providing high-quality translations for 200 languages. These models are purpose-built to support linguistic diversity and are optimized for robust performance, even for resource-scarce languages.

In our study, we assess whether constructing documents using sentence-level translations can still yield robust pretraining performance for LLMs. We hypothesize that essential linguistic patterns and semantic structures in high-quality source data can be preserved despite translation imperfections---an attribute that could be particularly beneficial for cold-start pretraining in low-resource languages. If successful, this method could greatly expand access to multilingual pretraining data.

Specifically, we begin by segmenting English documents into sentences with the NLTK sentence tokenizer~\citep{bird2009natural} and then translate them into target languages using the NLLB-200 model~\citep{costa2022no}. The translated sentences are reassembled into documents, with their original order and structural elements intact (e.g., newline characters)\footnote{The translation pipeline is detailed in Appendix~\ref{sec:trans_pip}.}. We employ \textit{NLLB-200-1.3B}, the smallest non-distilled model in the NLLB-200 suite, trying to explore the lower bound of translation quality. While this model may not represent the absolute lower bound, it provides a practical reference point for assessing the impact of noisier translations on LLM pretraining.
% Although it may not represent the absolute lower bound of translation quality, it provides a practical reference point for exploring the effects of noisier translations on LLM pretraining. 

% To assess the impact of translation quality on downstream performance, we employ \textit{NLLB-200-1.3B}---the smallest non-distilled model in the NLLB-200 suite. 
% % This choice allows us to try to explore the lower bound of translation quality by incorporating noisier translations, thereby evaluating their suitability for LLM pretraining. 
% Although it may not represent the absolute lower bound of translation quality, it provides a practical reference point for exploring the effects of noisier translations on LLM pretraining. 
% % Moreover, this approach underscores the robustness of LLM pretraining methodologies, highlighting their ability to generalize effectively even when trained on noisier multilingual corpora.

\paragraph{\thedata}
Through the aforementioned approach, we obtain a machine-translated pretraining corpus covering $10$ languages in total. We refer to this multiway parallel translated dataset as \textbf{\thedata{}}. The NLLB translation is performed with a batch size of 4096 and a beam size of 1\footnote{For each target language, the entire translation process is completed within 168 GPU hours using a single GH200 GPU.}. Table~\ref{tab:nllb_statistics} in Appendix~\ref{sec:appendix-nllb-statistics} presents the statistics of the original English data alongside those of the translated components in \thedata{}. To the best of our knowledge, \thedata{} is the largest multiway parallel, document-level multilingual pretraining dataset currently available.

% We translate the English documents from the $100$BT subset of \textit{FineWeb-Edu}\citep{lozhkov2024fineweb-edu} into nine target languages using NLLB-200-1.3B, resulting in a dataset covering ten languages in total. We refer to this multiway parallel translated dataset as \thedata. Sentence-level translation is performed with a batch size of 4096 and a beam size of 1\footnote{For each target language, the entire translation process is completed within 7×24 GPU hours using a single NVIDIA GH200 GPU.}. Table~\ref{tab:nllb_statistics} in Appendix~\ref{sec:appendix-nllb-statistucs} presents the statistics of the original English data alongside those of the translated components in \thedata. To the best of our knowledge, \thedata{} is the largest multiway parallel, document-level multilingual pretraining dataset currently available.

\subsection{Multilingual LM Pretraining}
In this section, we will describe the technical details of pretraining multilingual language models using \thedata{}.

\paragraph{Model Architecture and Hyper-parameters} 
We pretrain a multilingual language model from scratch using \thedata{}, naming it \themodel{}. The architecture and hyperparameter selection of \themodel{} (detailed in Appendix Section~\ref{sec:appendix-model-arch-params}) are informed by insights from the \textit{Llama} family of models and other open-source efforts~\cite{Karpathy2024} to reproduce \textit{GPT-2}. \themodel{} consists of $1.3$B parameters in total. Similar to \textit{GPT-2} reproduction efforts, we adopt a constant learning rate of $6\times 10^{-4}$, a sequence length of up to $2048$ tokens, and a batch size of $2048$ per iteration, resulting in approximately $4$ million tokens processed per iteration.

% We pretrain a mulitlingual language model using \thedata{} from scratch, and name the model with \themodel{}. \themodel{}'s architecture and hyperparameter selection (detailed in Appendix Section~\ref{sec:appendix-model-arch-params}) are based on lessons from the \textit{Llama} family of models and other open-source efforts~\cite{Karpathy2024} to reproduce \textit{GPT-2}. 
% %
% The model has a total of $1.3$B parameters.
% % , which is comparable to a wide range of small language models with sizes ranging from $1$B to $2$B parameters.
% Similar to \textit{GPT-2} reproduction efforts, we use a constant learning rate of $6\times 10^{-4}$ and a sequence length of up to $2048$ tokens, with a batch size of $2024$ per iteration, resulting in approximately $4$ million tokens processed per iteration. 

\paragraph{Tokenization} 
% In this work, we focus on a total of ten languages: Arabic (ar), English (en), French (fr), German (de), Italian (it), Russian (ru), Spanish (es), Indonesian (id), Swahili (sw), and Welsh (cy). 
\citet{alves2024tower} extends the multilingual capabilities of \textit{Llama2} models~\citep{touvron2023llama2}, demonstrating that the \textit{Llama2} tokenizer remains a practical choice for ensuring efficiency across diverse languages. Building on their findings, we use the \textit{Llama2} tokenizer in our experiments. For non-Latin languages, such as Arabic and Russian, it tokenizes the text while representing it using Unicode-based embeddings.

\paragraph{Pretraining Data} 
% Although \thedata{} is one of the largest multiway parallel, document-level multilingual datasets available, it is not designed to provide aligned translation pairs for pretraining. 
During pretraining, we use the same pretraining setup as GPT-family models, where documents are randomly sampled from the corpus.
%
As a result, the likelihood of the same document appearing in different languages within a single batch is very low. For clarity, Table~\ref{tab:training-example} presents an example of our pretraining data.
% Although \thedata{} is one of the largest multiway parallel, document-level multilingual datasets available, we do not intend to create aligned translation pairs for pretraining. Instead, we adopt a monolingual pretraining setup, where documents are randomly fetched from the corpus. This approach means that the probability of the same document appearing in different languages within a single batch is very low. For clarity, we illustrate an example of our pretraining data in Table~\ref{tab:training-example}.
\input{tables/batch}

\paragraph{Framework and Training} \themodel{} is trained using the \textit{Megatron-LM} framework~\citep{shoeybi2019megatron} from scratch with an accelerated attention implementation~\citep{dao2023flashattention}.  
The training is done on the
%UK Isambard-AI 
NVIDIA GH200 cluster~\citep{mcintosh2024isambard} for $8,366$ GPU hours.
% The training is done on a NVIDIA GH200 cluster for $3,187$ GPU hours.
Our pretraining dataset \thedata{} is created on a balanced basis, so we do not perform any up-sampling for specific languages. Our pretraining over \thedata{} processes approximately $1.5$T tokens, which is nearly one epoch of the pretraining data. Similar to the observation of \citet{muennighoff2024datascaling}, we observe no degradation in performance over the validation set during this process.

\input{tables/main_nllb}

\section{Experiments}
This section presents our evaluation of model performance across various multilingual benchmarks.

\subsection{Evaluation Benchmark Datasets}
\label{sec:benchmarks}
Our evaluation includes English benchmarks and assessments for the nine non-English languages, focusing on natural language understanding and common-sense reasoning. All benchmarks used are publicly available and open-source, ensuring the transparency and reproducibility of our results\footnote{All evaluations are conducted using~\url{https://github.com/EleutherAI/lm-evaluation-harness}.}. 

Specifically, our evaluation framework includes the following tasks: \textbf{ARC}~\citep{clark2018think,lai2023okapi, bayes2024uhura}: grade-school level multiple-choice science questions; \textbf{Hellaswag}~\citep{zellers2019hellaswag,lai2023okapi}: common-sense reasoning benchmarks for contextually appropriate sentence endings prediction; \textbf{PAWS-X}~\citep{yang-etal-2019-paws}: a cross-lingual adversarial dataset for paraphrase identification, sourced from English Wikipedia and Quora; \textbf{PIQA}~\citep{Bisk2020}: physical commonsense reasoning benchmarks; \textbf{SciQ}~\citep{SciQ}: an multiple-choice question-answering dataset in scientific topics; \textbf{TruthfulQA}~\citep{lin2021truthfulqa, bayes2024uhura}: question-answering evaluation tasks for truthfulness and factual accuracy of model responses;
\textbf{XCOPA}~\citep{ponti2020xcopa}: across-lingual adaptation of COPA~\citep{roemmele2011choice} for transfer commonsense reasoning evaluation; \textbf{XNLI}~\citep{conneau2018xnli}: an multilingual extension of~\citet{williams-etal-2018-broad}, assessing textual entailment prediction; \textbf{XStoryCloze}~\citep{lin2021few}: an multilingual adaptation of~\citet{mostafazadeh2016corpus} for assessing cross-lingual narrative understanding by predicting story endings; \textbf{XWinograd}~\citep{tikhonov2021s}: a cross-lingual adaptation of the Winograd Schema challenge\footnote{\url{https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html}} for coreference resolution evaluation.
% \textbf{ARC} \citep{clark2018think,lai2023okapi, britllm2024}: Grade-school level multiple-choice science questions.
% \textbf{Hellaswag}~\citep{zellers2019hellaswag,lai2023okapi, britllm2024}: Common-sense reasoning benchmarks for  contextually appropriate sentence endings prediction.
% \textbf{PAWS-X}~\citep{yang-etal-2019-paws}: A cross-lingual adversarial dataset for paraphrase identification, sourced from English Wikipedia and Quora.
% \textbf{PIQA}~\citep{Bisk2020, britllm2024}: Physical commonsense reasoning benchmarks.
% \textbf{SciQ}~\citep{SciQ}: A multiple-choice question-answering dataset in scientific topics.
% \textbf{TruthfulQA}~\citep{lin2021truthfulqa, bayes2024uhura, britllm2024}: Question-Answering evaluation tasks for truthfulness and factual accuracy of model responses.
% \textbf{XCOPA}~\citep{ponti2020xcopa}: A cross-lingual adaptation of COPA~\citep{roemmele2011choice} for transfer commonsense reasoning evaluation.
% \textbf{XNLI}~\citep{conneau2018xnli,britllm2024}: An multilingual extension of~\citet{williams-etal-2018-broad}, assessing textual entailment prediction. 
% \textbf{XStoryCloze}~\citep{lin2021few}: An multilingual adaptation of~\citet{mostafazadeh2016corpus} for assessing cross-lingual narrative understanding by predicting story endings
% \textbf{XWinograd}~\citep{tikhonov2021s}: A cross-lingual adaptation of the Winograd Schema challenge\footnote{\url{https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html}} for coreference resolution evaluation.
However, not all ten languages have all the above benchmarks publicly available. The specific evaluation datasets used for each language are detailed in Table~\ref{tab:specific_benchmarks} in Appendix~\ref{sec:appendix-benchmarks}\footnote{For Welsh evaluation, we use the \textit{BritEval} benchmarks, \url{https://llm.org.uk/}.}. All benchmarks are evaluated using a standard $5$-shot setting, with results reported in terms of accuracy.

\subsection{Baselines}
We benchmark our \themodel{}, trained with \thedata{}, in a $5$-shot setting, against diverse open-source multilingual and monolingual LLMs with comparable parameter sizes, different multilingual pretraining mixtures and data sources.

Our \textit{\textbf{multilingual LLM baselines}} include:
mGPT ($1.3$B)~\citep{shliazhko2022mgpt},
BLOOM ($1.1$B)~\citep{le2023bloom},
Llama3.2 ($1.3$B)~\citep{dubey2024llama},
Qwen2 ($1.5$B)~\citep{yang2024qwen2technicalreport}, Qwen2.5 ($1.5$B)~\citep{qwen2025qwen25technicalreport}, 
and Gemma ($2.6$B)~\citep{team2024gemma}.
%
Additionally, we compare against \textit{\textbf{language-specific LLM baselines}}:
Afriteva\_v2\_large ($1$B)~\citep{oladipo2023better} for Swahili,
BritLLM ($3$B)\footnote{
\href{https://hf.co/britllm/britllm-3b-v0.1}{\texttt{hf.co/britllm/britllm-3b-v0.1}}
% \url{https://huggingface.co/britllm/britllm-3b-v0.1}
} for Welsh,
CroissantLLM ($1.3$B)~\citep{faysse2024croissantllm} for French,
EuroLLM ($1.7$B)~\citep{martins2024eurollm} for Arabic, French, German, Italian, Russian, and Spanish, 
Jais-family-1p3b ($1.3$B)~\citep{sengupta2023jais} for Arabic,
Sailor ($1.8$B)~\citep{dou2024sailor} and Sailor2 ($1$B)~\citep{sailor2report} for Indonesian. 
%
Furthermore, we include two \textit{\textbf{English-centric baselines}} in our evaluation:
TinyLlama ($1.1$B)~\citep{zhang2024tinyllama},
Pythia ($1.4$B)~\citep{biderman2023pythia}. An overview of baseline models and our \themodel{} has been shown in Table~\ref{tab:model_comparison} in Appendix~\ref{sec:appendix-baseline}.

\subsection{Main results of \themodel{}}
Table~\ref{tab:main_result} presents the average performance of \themodel{} across benchmark datasets for each language, comparing it to baseline models across all 10 languages. The last three columns of Table~\ref{tab:main_result} summarize the overall average performance for (i) All languages, (ii) Non-English languages, and (iii) High-resource languages. Generally, \themodel{} consistently ranks among the top three models in terms of average performance across \textit{\textbf{all languages}} and \textit{\textbf{non-English languages}}, with the accuracy scores of $45.08$ and $43.83$, respectively. It significantly outperforms multilingual LLMs such as \textit{mGPT}, \textit{BLOOM}, \textit{Llama3.2}, \textit{Qwen2}, and \textit{Qwen2.5} of similar model size, and achieves comparable results with ~\textit{Gemma}, despite \textit{Gemma} having twice the model size of \themodel{}. 

For \textit{\textbf{high-resource languages}}, \themodel{} ranks among the top three models for Arabic, German, and Italian. Additionally, it outperforms \textit{Llama3.2} on average ($45.73$ vs. $43.89$), despite the latter being trained on 9T tokens—an order of magnitude more than the data used for training \themodel{}. A similar trend is observed in French evaluation when comparing \themodel{} to \textit{CroissantLLM}. \themodel{} outperforms \textit{CroissantLLM} ($46.11$ vs. $45.73$), even though the latter is trained on 3T tokens, with half of them in French. In contrast, \themodel{} is trained on $1.5$T tokens, with less than $10$\% ($150$B) in French, primarily consisting of translated data.

More remarkable results are observed in \textit{\textbf{medium- and low-resource languages}}. For Indonesian, \themodel{} outperforms both \textit{Qwen2} and \textit{Qwen2.5}, despite the latter two being trained on $7$T and $18$T tokens. The most notable gains are seen in Swahili and Welsh, where \themodel{} ranks first among all baselines, achieving accuracy scores of $44.44$ and $38.69$, outperforming \textit{Gemma} ($2.6$B) and \textit{BritLLM} ($3$B).
These results suggest that training with translation data can be a viable cold-start strategy for pretraining LLMs in medium- and low-resource languages.

For \textit{\textbf{English}}, \themodel{} performs slightly worse than \textit{TinyLlama} but outperforms \textit{CroissantLLM}. Despite being trained on significantly less English data—only $150$B tokens compared to \textit{TinyLlama}’s $3$T and \textit{CroissantLLM}’s $1.5$T—\themodel{} achieves competitive performance, underscoring the importance of the high-quality English source. 

\section{Discussion and Ablations}
In this section, we explore (1) the impact of LLM-generated translation data on pretraining performance and (2) the effects of additional data sources, including general web data and specialized datasets such as rephrased synthetic text, code, and instruction data. We address these questions through a series of ablation experiments.

\subsection{Pretraining with Translation Data generated from an LLM}
Recent studies~\citep{alves2024tower, martins2024eurollm} show that LLMs can effectively support translation tasks, raising the question: \textit{How does pretraining performance differ when using a translation corpus generated by an LLM versus an NMT model like NLLB, used in \themodel{}}?

\citet{dubey2024llama} highlighted Mistral’s potential for multilingual NLP, while \citet{moslem2023fine} and \citet{kocmi2024preliminary} demonstrated its effectiveness in machine translation. Based on these insights, we use \textit{Mistral-7B-Instruct-v0.1}\footnote{\href{https://hf.co/mistralai/Mistral-7B-Instruct-v0.1}{\texttt{hf.co/mistralai/Mistral-7B-Instruct-v0.1}}} for translation, and we focus on English, French, German, and Spanish for this ablation. Details on data generation are provided in the Appendix~\ref{sec:appendix-cuatrollm}. A key difference between Mistral- and NLLB-generated translations lies in their approach to text segmentation. Mistral is prompted to translate chunked documents, better preserving contextual coherence, whereas NLLB translates at the sentence level, which may result in inconsistencies in document-level fluency and cohesion. 

Due to computational capacity, we translate $64$B tokens from the sample-$100$BT subset of \textit{FineWeb-Edu}. We then pretrain an LLM from scratch using the same training framework as \themodel{}, naming it \cuatrollm{}. For a fair comparison, we extract the corresponding NLLB-translated data for French, German, and Spanish from \thedata{} and retrain \themodel{} using this subset, referring to the resulting model as \themodel{}-4. 

Both \cuatrollm{} and \themodel{}-4 are evaluated on benchmark datasets for English, French, German, and Spanish, as introduced in Section~\ref{sec:benchmarks}, with results presented in Table~\ref{tab:main_mistral_nllb_compare} in Appendix~\ref{sec:appendix-cuatrollm}. Across four languages, \themodel{} and \cuatrollm{} perform similarly ($46.65$ vs. $47.16$), both surpassing \textit{mGPT}, \textit{BLOOM}, and \textit{Llama3.2} on average. This suggests that the choice of translation method has a limited impact on these specific downstream evaluations. However, a key advantage of the NLLB-200 model---with support for 200 languages---is its scalability and efficiency, making it a more viable choice for expanding multilingual pretraining to a broader range of languages.

\subsection{Beyond Pretraining with Translation Data}
\label{sec:beyond}
In this section, we assess whether adding specialized data provide benefits beyond pretraining with machine-translated data.

\input{tables/main_real}
\input{tables/data_impact_detail}

\subsubsection{Impact of General Web Data}
\label{sec:real_web}
Given that \thedata{} is largely based on educational contents, which is a highly specialized domain, we investigate whether multilingual reasoning capabilities can be further enhanced by incorporating general web data.

We construct the general web dataset by sampling English, French, German, Italian, and Spanish data from \textit{RedPajama-v2} (\textit{RPv2})~\citep{NEURIPS2024_d3449733}; Arabic, Russian, and Indonesian from \textit{mC4}~\citep{xue-etal-2021-mt5}; Swahili from \textit{Wura}~\citep{oladipo2023better}; and Welsh from \textit{CC100}~\citep{wenzek-etal-2020-ccnet}. For \textit{RPv2}, we filter each subset using its built-in quality signals\footnote{Details are introduced in Appendix~\ref{sec:appendix-rpv2-sampling}.}; for~\textit{mC4}, we apply random sampling. Given the limited availability of Swahili and Welsh data in \textit{Wura} and \textit{CC100}, we include their entire datasets. We balance the general web data by sampling an equal number of tokens per language, upsampling Indonesion, Swahili and Welsh as needed to match their proportions in \thedata{}. We then merge it with \thedata{} at an nearly $1:1$ ratio for continued pretraining. Building on \themodel{}, we extend training for an additional $20,800$ steps, processing approximately $90$B tokens during this phase, with general web data accounting for only around $45$B tokens (less than 3\% of the total). We refer to this continued pretraining model as \themodelweb{}, as detailed in Table~\ref{tab:data-impact-model-details}.

\paragraph{Understanding and Reasoning Evaluation}
The 5-shot evaluation results of \themodelweb{} on multilingual understanding and commonsense reasoning benchmarks (as detailed in Section~\ref{sec:benchmarks}) are in Table~\ref{tab:main_real}. \themodelweb{} demonstrates a significant improvement over \themodel{}, achieving higher average performance in general. Across all languages and non-English languages, \themodelweb{} ranks as the top-performing model among all LLMs ($45.91$ and $44.77$). For high-resource languages, \themodelweb{} achieves performance comparable to \textit{EuroLLM}, despite the latter being trained on 4T tokens. Notably, for Indonesian, \themodelweb{} emerges as the best-performing model ($49.75$), surpassing Southeast Asian-specific baselines such as \textit{Sailor} and \textit{Sailor2}. These results highlight the benefits of incorporating even a limited amount of general web data during continued pretraining for multilingual understanding and reasoning tasks.

\paragraph{Linguistic Proficiency Evaluation}
Beyond understanding and reasoning tasks, we also evaluate the model’s linguistic proficiency, focusing on its ability to understand and generate coherent, grammatically accurate sentences. \citet{faysse2024croissantllm} introduced the \textit{fr-grammar} and \textit{fr-vocabulary} test sets in French to assess models' grammar and vocabulary capabilities through structured language evaluations. We test both \themodel{} and \themodelweb{} on these benchmarks in a 5-shot setting to measure their proficiency in French linguistic competence. As shown in Table~\ref{tab:french_ling}, \themodelweb{} outperforms \themodel{} by nearly $10$ accuracy points ($74.79$ vs. $65.13$), demonstrating that even a small addition of general web data ($45$B) in continued pretraining can significantly enhance linguistic proficiency.

\input{tables/french_linguistic_proficiency}
\input{tables/copal_id}
\input{tables/main_mmlu}

\paragraph{Reasoning Evaluation for Local Culture}
Local culture reasoning provides a natural representation of causal reasoning within specific cultural contexts. COPAL-ID~\citep{wibowo2023copal} is an Indonesian causal commonsense reasoning dataset, written by native speakers from scratch with standard Indonesian and Jakartan Indonesian, a widely spoken dialect in daily conversations. We evaluate both \themodel{} and \themodelweb{} on this benchmark in a 5-shot setting to assess their ability to reason within the Indonesian cultural sphere. As shown in Table~\ref{tab:local_culture_reasoning}, \themodelweb{} improves Indonesian cultural reasoning by over an averaged 8 accuracy points ($57.61$ vs. $48.84$) by incorporating a limited amount of general web data ($45$B) in continued pretraining on \themodel{}. It surpasses all LLM baselines except \textit{Sailor} and \textit{Sailor2}, which have been specifically trained for Indonesian. 

\subsubsection{Impact of Special Data}
\citet{yang2023rethinking} shows that rephrasing MMLU~\citep{hendrycks2021measuring} samples enhances model reasoning performance across various domains. Motivated by these findings, we explore the impact of \textbf{\textit{rephrased synthetic data}} on \themodel{}. Instead of rephrasing MMLU test cases~\citep{yang2023rethinking}, we rephrase English web data into a multiple-choice (MC) style using an LLM, aligning with reasoning structure while maintaining its open-ended nature. 
% We then incorporate this synthetic data into our multilingual pretraining corpus, assessing if rephrased English MC data can enhance multilingual reasoning through cross-lingual transfer. 
We extract $10$BT English data from SlimPajama~\citep{cerebras2023slimpajama}, generate $8$BT MC synthetic data using \textit{Mistral-7B-Instruct-v0.1}\footnote{We use the prompt template as "\textit{Write multiple-choice questions and answers based on the document: [doc]}".}, and upsample and integrate it into \thedata{} with general web data, ensuring MC data constitutes 5\% of the corpus. Given the improved performance of \themodelweb{}, we continue pretraining for 9,000 steps, processing $40$B tokens, including 2B from MC data.
% We sample $10$BT English data from \textit{SlimPajama}~\citep{cerebras2023slimpajama}, prompt \textit{Mistral-7B-Instruct-v0.1} with: "\textit{Write multiple-choice questions and answers based on the document: [doc]}", and then obtain $8$B tokens of MC synthetic data. We upsample and integrate this into \thedata{} alongside general web data, ensuring MC data comprises 5\% of the total corpus. Given the improved performance of \themodelweb{}, we continue pretraining it for $9,000$ steps, processing $40$B tokens, including $2$B tokens from MC data. 
% We refer to this model as \themodelmc{}, as detailed in Table~\ref{tab:data-impact-model-details}.
% Specifically, we randomly sample $10$B English tokens from \textit{SlimPajama}~\citep{cerebras2023slimpajama}, and prompt \textit{Mistral-7B-Instruct-v0.1} using the instruction template, "\textit{Write multiple-choice questions and answers based on the document: [doc]}", to generate rephrased MC synthetic data. After processing, we obtain 8B tokens, which we upsample and integrate into \thedata{} alongside real web data, ensuring MC data comprises 5\% of the final training corpus. Given the improved performance of \themodelweb{}, we continue pretraining it for $9,000$ steps, processing $40$B tokens, with $2$B tokens from the MC data. We refer to this model as \themodelmc{}, as detailed in Table~\ref{tab:data-impact-model-details}.

Prior works~\cite{faysse2024croissantllm,zhang2024tinyllama,martins2024eurollm} highlights the importance of a cooldown phase for enhancing model capabilities. While \thedata{} emphasizes educational content, it lacks code and instruction data, such as question-answering (QA), compared to other LLMs. To address this, we introduce \textit{\textbf{cooldown data}} during this phase: Python-Edu~\cite{benallal2024smollmcorpus}, an educational Python dataset from The Stack (4B tokens), and WebInstruct~\citep{yue2024mammoth2}, a curated QA dataset (0.8B tokens) from the web. They are up-sampled and mixed with the previous-stage data (Table~\ref{tab:data-impact-model-details}), forming 30\% of the total. The model undergoes an additional $20$B-token training phase using a reduced learning rate\footnote{We apply a constant learning rate schedule: $6\times 10^{-4}$ for earlier pretraining phases and $6\times 10^{-5}$ for cooldown.}. Notably, cooldown data constitutes less than $6$B tokens, accounting for only $0.3\%$ of total training tokens. We denote this final cooldown-trained model as \textbf{\themodelcool}.
% We upsample the cooldown data and mix it with the data from the previous stage, with cooldown data comprising 30\% of the total. The model is then trained on this new mix (Table~\ref{tab:data-impact-model-details}) for an additional $20$B tokens using a smaller learning rate.\footnote{We use a constant learning rate schedule: $6\times 10^{-4}$ for earlier pretraining phases and $6\times 10^{-5}$ for the cooldown stage.} Notably, cooldown data represents only a small fraction of the entire corpus, contributing fewer than 6B tokens, or approximately 0.3\% of total training tokens. We refer to this cooldown-trained model as \themodelcool.

We evaluate \themodelcool{} on all benchmarks in Sections~\ref{sec:benchmarks} and \ref{sec:real_web}, as well as Global-MMLU~\citep{singh2024global}, covering nine languages (excluding Welsh), in a $5$-shot setting. As shown in Table~\ref{tab:main_mmlu} for MMLU, \themodelcool{}, trained with additional rephrased synthetic and cooldown data, ranks among the top three models overall and achieves the highest performance in Swahili. Furthermore, Table~\ref{tab:all_understanding} in Appendix~\ref{sec:appendix-special-data} shows that \themodelcool{} surpasses \themodelweb{} across all languages for understanding and reasoning tasks, ranking as the top LLM on average. Remarkably, it is the \textit{\textbf{best-performing}} LLM for \textit{\textbf{Arabic, Italian, Indonesian, Swahili, and Welsh}}. Additionally, Tables~\ref{tab:french_ling_final} and~\ref{tab:local_culture_reasoning_final} in Appendix~\ref{sec:appendix-special-data} show that \themodelcool{}, despite being trained with limited additional special data, improves both French linguistic proficiency and Indonesian cultural reasoning. These findings highlight the effectiveness of rephrased synthetic and cooldown data in enhancing multilingual pretraining based on NLLB-translated data.

\section{Conclusion}
% We present \thedata{}, a multilingual dataset generated by machine-translating English source texts. Trained from scratch on this data, \themodel{} achieves competitive performance across nine non-English understanding and reasoning benchmarks, which matches or outperforms state-of-the-art multilingual LLMs trained using closed data, such as \textit{Llama3.2}, \textit{Qwen2.5}, and \textit{Gemma}. 
% We also show that  less than 5\% of TransWebEdu as
% domain-specific pretraining data sets a new
% state-of-the-art in Arabic, Italian, Indonesian,
% Swahili, and Welsh understanding and com-
% monsense reasoning tasks. In summary, we have shown that  constructing documents using a sentence-level translation model can yield robust performance for mutlilingual LLMs.
% Our approach provides a scalable solution for creating multilingual pretraining data, particularly for medium- and low-resource languages, contributing to advancements in multilingual NLP research.
We introduce \thedata{}, a multilingual dataset generated through machine translation of a high-quality English source dataset. Our model, \themodel{}, trained from scratch on this data, achieves competitive performance across nine non-English understanding and reasoning benchmarks, matching or surpassing state-of-the-art multilingual LLMs trained on closed data, such as \textit{Llama3.2}, \textit{Qwen2.5}, and \textit{Gemma} on average. Furthermore, we demonstrate that incorporating less than 5\% of TransWebEdu as domain-specific pretraining data establishes new state-of-the-art results in Arabic, Italian, Indonesian, Swahili, and Welsh for understanding and commonsense reasoning tasks. These findings highlight that constructing multilingual pretraining corpora using sentence-level translations can yield robust performance in multilingual LLMs. Our approach provides a scalable and efficient solution for creating multilingual pretraining data, particularly for medium- and low-resource languages, contributing to advancements in multilingual NLP research.

\section*{Limitations}
Our study yields promising results while also identifying areas for future exploration. 

\themodel{}, trained on \thedata{}, achieves significant performance gains across 10 multilingual benchmarks. Further improvements are observed with the addition of general web data, rephrased synthetic data, and code and web-instruct data. However, due to computational constraints, we did not conduct ablation studies to determine the optimal data mixing ratios beyond pretraining with \thedata{}. Future work will extend the ``Beyond Pretraining with Translation Data'' experiments in Section~\ref{sec:beyond} to explore optimal data integration strategies from diverse sources for \thedata{}.

In addition, our experiments focus on \themodel{}, a $1.3$B-parameter model that has shown promising results at this scale. However, it remains unclear whether the benefits of our translated pretraining data would persist or amplify in substantially larger models (e.g., $70$B+ parameters). Scaling up could provide deeper insights into multilingual learning dynamics and data efficiency. Future research will explore these aspects to validate and enhance the scalability of our multilingual pretraining approach. 

% We evaluate \themodelmc{} on the Global MMLU~\citep{singh2024global}, which covers 9 of our targeted languages. The comparison between TransWebLLM-web and TransWebLLM-mcSyn in Table~\ref{tab:data-impact-model-details} shows that adding a small amount of English MC synthetic data (only 2B trained tokens) yields the largest improvements in languages related to English—notably French, German, Italian, and Spanish. In contrast, more distant languages like Arabic, Swahili, and Russian show smaller gains, aligning with expected cross-lingual transfer patterns.







\bibliography{custom}
\appendix
\section*{Appendix}
This appendix provides additional technical details on our approach and supplementary evaluation results for the main paper.

\section{Translation Pipeline}
The detailed translation pipeline to produce \thedata{} is shown in Figure~\ref{fig:trans_pip}.
\label{sec:trans_pip}
\begin{figure*}[!t]
\includegraphics[width=1.0\textwidth]{figures/nlllb_pip.pdf}
\caption{Step-by-step illustration of the translation pipeline to obtain \thedata{}.}
\label{fig:trans_pip}
\end{figure*}

\section{Data Statistics of \thedata}
\label{sec:appendix-nllb-statistics}
\input{tables/nllb_statistics}
The statistics of \thedata{} are shown in Table~\ref{tab:nllb_statistics}.

\section{Hyperparameters Settings of Model Pretrainning}
\label{sec:appendix-model-arch-params}
\input{tables/model-parameters}
Pretraining hyperparameter settings are shown in Table~\ref{tab:model-arch}.

\section{Specific Evaluation Benchmarks for Each Language}
\label{sec:appendix-benchmarks}
\input{tables/benchmarks}
Specific evaluation benchmarks for each of the 10 languages are shown in Table~\ref{tab:specific_benchmarks}.

\section{An Overview of Baseline Models}
\label{sec:appendix-baseline}
\input{tables/model_comparison}
An overview of baseline models are shown in Table~\ref{tab:model_comparison}.

\section{Translation Data Generation from the Mistral-7B-Instruct LLM}
\label{sec:appendix-cuatrollm}
We employ \textit{Mistral-7B-Instruct-v0.1}\footnote{\href{https://hf.co/mistralai/Mistral-7B-Instruct-v0.1}{\texttt{hf.co/mistralai/Mistral-7B-Instruct-v0.1}}} as our translation model. However, its efficacy when prompted for document-level translation, particularly with long-context English source documents, has not yet been verified. A recent related work by \citet{maini2024rephrasing} has empirically demonstrated that prompting an LLM to rephrase more than $300$ tokens could lead to information loss when rephrasing web data. 

Following their setup, we first segment the English source documents from the sample-$100$BT subset of \textit{FineWeb-Edu} into shorter pieces, prompt Mistral to translate these segments sequentially, and subsequently reconstruct the whole translated document by concatenating the translated segments.The detailed translation pipeline is shown in Figure~\ref{fig:mistral_pip}.


\begin{figure*}[!t]
\includegraphics[width=1.0\textwidth]{figures/mistral_pip.pdf}
\caption{Step-by-step illustration of the translation pipeline with the Mistral-7B-Instruct model.}
\label{fig:mistral_pip}
\end{figure*}

Adhering to the instruction format specified\footnote{\href{https://hf.co/mistralai/Mistral-7B-Instruct-v0.1}{\texttt{hf.co/mistralai/Mistral-7B-Instruct-v0.1}}} for \textit{Mistral-7B-Instruct}, the chat template employed to prompt Mistral model for translation~(using English-French as an example) is illustrated in Figure~\ref{fig:prompt_template}\footnote{The highlighted portions in the template are adjusted according to the target language.}. 
% When prompting for the translation of each segment, we impose a maximum generation length of $384$ tokens. 
To maintain translation integrity, any sentence not fully translated to a terminal punctuation is omitted, based on the NLTK sentence tokenizer~\citep{bird2009natural}.

\begin{figure}[!t]
\includegraphics[width=1.0\columnwidth]{figures/prompt_template.png}
\caption{Chat template used for prompting \textit{Mistral-7B-Instruct-v0.1} for English-French translation.}
\label{fig:prompt_template}
\end{figure}

\input{tables/mistral_data_statistics}
We translate English documents from \textit{FineWeb-Edu} \citep{lozhkov2024fineweb-edu} into three major European languages: French, German, and Spanish via prompting the Mistral-7B-Intruct model. 
% We call this multiway parallel translated dataset as \cuadata. 
To optimize memory efficiency and accelerate the inference process of \textit{Mistral-7B-Instruct-v0.1}, we employ \textit{vLLM} \citep{kwon2023efficient}, a library specifically designed for efficient large language model inference and serving. Using this setup, we translate approximately $54$ million English documents (a subset of sample-$100$B of \textit{FineWeb-Edu}) into the three target languages by prompting \textit{Mistral-7B-Instruct-v0.1}. Table~\ref{tab:mistral_data_statistics} presents the statistics of the original English data and the translated French, German, and Spanish. Leveraging \textit{vLLM}'s efficiency, we estimate the total computational cost to be approximately $6.03 \times 10^{22}$ FLOPs.

\input{tables/main_mistral_nllb_compare}

Table~\ref{tab:main_mistral_nllb_compare} compares the performance of the model trained on Mistral-generated translation data (\cuatrollm{}) with the model trained on NLLB-generated data (\themodel{}-4) across English, German, French, and Spanish benchmarks.


\section{Sampling General Web Data from \textit{RedPajama-v2} }
\label{sec:appendix-rpv2-sampling}
We use the English, French, German, Italian, and Spanish subsets of the \textit{RedPajama-v2}~(RPv2)~\cite{NEURIPS2024_d3449733} as web data. Given that web data is inherently noisy, we make further use of the quality signals provided for RPv2 and filter each subset down to a smaller, high-quality subset. Specifically, we use the six most recent dumps from 2022 and 2023 and apply quality filtering using the Gopher rules~\cite{rae2021scaling}. Additionally, web data often contains near duplicates, stemming from boilerplate text, ads, and other computer-generated text that only differs by a few words, and removing these has been shown to positively affect training efficiency and reduce the amount of memorization~\cite{lee2021deduplicating}. We therefore adopt the MinHash algorithm with locality-sensitive hashing~\cite{broder1997resemblance} to perform near-deduplication. We identify documents as near duplicates if their Jaccard similarity is greater than $0.8$ and use $128$ hash functions.

\section{Evaluation for Impact of Special Data}
\label{sec:appendix-special-data}
The evaluation results of \themodelcool{} on understanding and reasoning, French linguistic proficiency, and reasoning for Indonesian local culture are presented in Table~\ref{tab:all_understanding},~\ref{tab:french_ling_final}, and~\ref{tab:local_culture_reasoning_final}, respectively.

\input{tables/all_understanding}
\input{tables/french_linguistic_proficiency_final}
\input{tables/copal_id_final}


\section{Detailed Results for Understanding and Reasoning Benchmarks}
Tables~\ref{tab:ar_full} to~\ref{tab:cy_full} present detailed benchmark results, as outlined in Section~\ref{sec:benchmarks}, for each language. The corresponding averaged scores are presented in Tables~\ref{tab:main_result},~\ref{tab:main_real}, and~\ref{tab:all_understanding}, accordingly.


\input{tables/full_ar}
\input{tables/full_en}
\input{tables/full_fr}
\input{tables/full_de}
\input{tables/full_it}
\input{tables/full_ru}
\input{tables/full_es}
\input{tables/full_id}
\input{tables/full_sw}
\input{tables/full_cy}










\end{document}
