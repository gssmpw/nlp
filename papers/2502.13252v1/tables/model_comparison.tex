\begin{table*}[!t]
\small
\centering
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\# Param.} & \multirow{2}{*}{Corpus} & Corpus & Training & Data & \multirow{2}{*}{Languages} \\
& & & Size & Tokens & Avail.& \\
\midrule
\multicolumn{7}{l}{\textit{Monolingual LLMs}} \\
\href{https://huggingface.co/TinyLlama/TinyLlama_v1.1}{TinyLlama} & 1.1B & {\makecell{SlimPajama~\citep{cerebras2023slimpajama} and \\ StarCoder training data~\citep{li2023starcoder}}} & 1T & 3T & \greencheck & Primarily English \\
\href{https://huggingface.co/EleutherAI/pythia-1.4b}{Pythia} & 1.4B & The Pile~\citep{gao2020pile} & 207B & 300B & \greencheck & Primarily English \\
\midrule
\multicolumn{7}{l}{\textit{Multilingual LLMs}} \\
\href{https://huggingface.co/ai-forever/mGPT}{mGPT} & 1.3B & mC4,Wiki & 488B & 440B & \redcross & {\makecell{61 languages}}\\
\href{https://huggingface.co/bigscience/bloomz-1b1}{BLOOM} & 1.1B & {\makecell{BigScience Catalogue, Common Crawl, Github Code, \\ and OSCAR~\citep{OrtizSuarezSagotRomary2019}}} & 350B & 366B & \redcross & {\makecell{46 langauges}} \\
\href{https://huggingface.co/meta-llama/Llama-3.2-1B}{Llama3.2} & 1.3B & {\makecell{Web data, Code, and Math}} &- & 9T & \redcross & {\makecell{At least 8 languages}}\\
\href{https://huggingface.co/Qwen/Qwen2-1.5B}{Qwen2} & 1.5B &- & - & 7T & \redcross & {\makecell{At least 30 languages}} \\
\href{https://huggingface.co/Qwen/Qwen2.5-1.5B}{Qwen2.5} & 1.5B &- & - & 18T & \redcross & {\makecell{At least 30 languages}} \\
\href{https://huggingface.co/google/gemma-2b}{Gemma} & 2.6B & Web data, Code, and Science Articles & - & 2T & \redcross & {\makecell{-}} \\
\midrule
\multicolumn{7}{l}{\textit{Language-specific LLMs}} \\

\href{https://huggingface.co/castorini/afriteva_v2_large}{afriteva\_v2\_large} & 1B & Wura~\citep{oladipo2023better} & 30 million & 136B & \greencheck & 20 African languages \\
\href{https://huggingface.co/britllm/britllm-3b-v0.1}{BritLLM} & 3B & {\makecell{SlimPajama~\citep{cerebras2023slimpajama}, \\ QA and MC Synthetic Data, Wiki, NLLB}} & 668B & - & \redcross & {\makecell{5 British languages}} \\

\href{https://huggingface.co/croissantllm/CroissantLLMBase}{CroissantLLM} & 1.3B & 
Croissant~\citep{faysse2024croissantllm} & 1T & 3T& \greencheck & English, French \\
\href{https://huggingface.co/utter-project/EuroLLM-1.7B}{EuroLLM} & 1.7B & {\makecell{mC4, Parallel Data, Code/Math, Wiki, \\ArXiv, Books, Apollo, Annealing Data}} & - & 4T & \redcross & {\makecell{35 languages}} \\

\href{https://huggingface.co/inceptionai/jais-family-1p3b}{Jais-family-1p3b} & 1.3B & {\makecell{Jais Model Family training data\\~\citep{sengupta2023jais}}} & 395B & 1.6T & \redcross & {\makecell{Arabic, English}} \\

\href{https://huggingface.co/sail/Sailor-1.8B}{Sailor} & 1.8B & {\makecell{CC100~\citep{wenzek-etal-2020-ccnet}, \\ MADLAD-400~\citep{kudugunta2024madlad}, \\ OpenSubtitles, and Wiki}} & 395B & 400B & \redcross & {\makecell{English, Chinese, and \\ 5 South-East Asian languages}} \\

\href{https://huggingface.co/sail/Sailor2-1B}{Sailor2} & 1B & {\makecell{CC100~\citep{wenzek-etal-2020-ccnet}, \\ MADLAD-400~\citep{kudugunta2024madlad}, \\ OpenSubtitles, and Wiki}} & - & 500B & \redcross & {\makecell{15 languages}} \\



\midrule
\themodel{} (Ours) & 1.3B & \thedata{} & 1.7T & 1.5T & \greencheck & 10 languages \\
\bottomrule
\end{tabular}
}
\caption{Overview of pretraining data among LLMs.}
\label{tab:model_comparison}
\end{table*}