\begin{table}[!t]
\scriptsize
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cccc|c}
\toprule
 & en & fr & de & es & \textbf{Avg.} \\
\hline
\multicolumn{6}{@{}l@{}}{\textit{\textbf{English LLMs}}} \\
Pythia (1.4B) & 54.63 & 40.14 & 36.23 & 39.91 & 42.73 \\
TinyLlama (1.1B) & 57.18 & 44.13 & 37.06 & 42.38 & 45.19 \\
\hline
\multicolumn{6}{@{}l@{}}{\textit{\textbf{Multilingual LLMs}}} \\
mGPT (1.3B) & 45.38 & 40.27 & 34.54 & 38.88 & 39.77 \\
BLOOM (1.1B) & 50.94 & 41.80 & 34.62 & 42.69 & 42.51 \\
Llama3.2 (1.3B) & 58.16 & 44.10 & 39.73 & 44.15 & 46.54 \\
Qwen2 (1.5B) & 61.07 & 47.40 & 40.79 & 47.40 & 49.17 \\
Qwen2.5 (1.5B) & 62.93 & 48.69 & 40.49 & 48.41 & 50.13 \\
Gemma (2.6B) & 62.39 & 49.78 & 44.27 & 49.35 & 51.45 \\
\hline
\multicolumn{6}{@{}l@{}}{\textit{\textbf{Language-Specific LLMs}}} \\
CroissantLLM (1.3B) & 53.31 & 45.73 & - & - & - \\
EuroLLM (1.7B) & 57.98 & 47.85 & 42.07 & 47.07 & 48.74 \\
\hline
\multicolumn{6}{@{}l@{}}{\textit{\textbf{Ours}}} \\
CuatroLLM (1.3B) & 56.12 & 45.01 & 40.42 & 45.04 & 46.65 \\
TransWebLLM-4 (1.3B) & 55.32 & 46.67 & 41.45 & 45.19 & 47.16 \\
\bottomrule
\end{tabular}}
\caption{Performance comparison between \cuatrollm{} trained on LLM-translated data and \themodel{}-4 across four selected languages. 
% Scores represent model accuracy, with higher values indicating better performance. The last column reports the average accuracy across all four languages.
}
\label{tab:main_mistral_nllb_compare}
\end{table}
