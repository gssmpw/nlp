\begin{table*}[!t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt} 
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{l|ccccccc|c|cc|ccc}
\toprule
& \multicolumn{7}{c|}{\textit{High}} & \textit{Medium} & \multicolumn{2}{c|}{\textit{Low}} & \multicolumn{3}{c}{\textbf{Average}} \\
 & ar & en & fr & de & it & ru & es & id & sw & cy & All & Non-eng & High \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{English LLMs}}} \\
Pythia (1.4B) & 33.21 &  54.63 & 40.14 & 36.23 & 35.14 & 38.32 & 39.91 & 36.83 & 36.90 & 31.57  & 38.29 & 36.47 & 39.65 \\
TinyLlama (1.1B) & 32.86 & 57.18 & 44.13 & 37.06 & 36.79 & 40.97 & 42.38 & 36.13 & 36.69 & 31.53  & 39.57 & 37.62 & 41.62 \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Multilingual LLMs}}} \\
mGPT (1.3B) & 32.90 & 45.38 & 40.27 & 34.54 & 34.89 & 40.53 & 38.88 & 39.47 & 38.98 & 31.14 & 37.70 & 36.84 & 38.20 \\
BLOOM (1.1B) & 34.97 & 50.94 & 41.80 & 34.62 & 33.69 & 37.56 & 42.69 & 43.23 & 37.09 & 31.57 & 38.82 & 37.47 & 39.47 \\
Llama3.2 (1.3B) & 34.78 & 58.16 & 44.10 & 39.73 & 40.93 & 45.38 & 44.15 & 44.67 & 38.30 & 31.84 & 42.20 & 40.43 & 43.89 \\
Qwen2 (1.5B) & 35.61 & \underline{61.07} & 47.40 & 40.79 & 42.61 & \underline{47.95} & \underline{47.40} & 45.93 & 38.89 & 32.23 & 43.99 & 42.09 & 46.12 \\
Qwen2.5 (1.5B) & 37.35 & \underline{\cellcolor{yellow!30}62.93} & \underline{48.69} & 40.49 & 43.10 & \underline{47.01} & \underline{48.41} & 46.17 & 37.98 & 31.78 & \underline{44.39} & \underline{42.33} & \underline{46.85} \\
Gemma (2.6B) & 37.26 & \underline{62.39} & \underline{\cellcolor{yellow!30}49.78} & \underline{\cellcolor{yellow!30}44.27} & \underline{44.57} & \underline{\cellcolor{yellow!30}48.60} & \underline{\cellcolor{yellow!30}49.35} & \underline{48.27} & \underline{40.18} & \underline{32.28} & \underline{\cellcolor{yellow!30}45.70} & \underline{\cellcolor{yellow!30}43.84} & \underline{48.03} \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Language-Specific LLMs}}} \\
AfriTeVa (1B) & & 37.70 & & & & & & & \underline{40.60} & & &\\
BritLLM (3B) & & 60.45 & & & & & & & & \underline{37.07} & & &\\
CroissantLLM (1.3B) & & 53.31 & 45.73 & & & & & & & & &\\
EuroLLM (1.7B) & \underline{38.88} & 57.98 & \underline{47.85} & \underline{42.07} & \underline{\cellcolor{yellow!30}47.56} & 46.71 & 47.07 & &  &  &  & & \underline{46.87}\\
% GPT-fr (1B) & & 0.3887 & 0.3736 & & & & & & & & & &\\
Jais-family-1p3b (1.3B) & \underline{\cellcolor{yellow!30}39.97} & 56.28 & & & & & & & & & & &\\
Sailor (1.8B) & & 55.40 & & & & & & \underline{48.45} & & & & & \\
Sailor2 (1B) & & 54.61 & & & & & & \underline{\cellcolor{yellow!30}49.44} & & & & & \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Ours}}} \\
TransWebLLM (1.3B) & \underline{39.30} & 56.30 & 46.11 & \underline{41.01} & \underline{45.75} & 46.38 & 45.27 & 47.54 & \underline{\cellcolor{yellow!30}44.44} & \underline{\cellcolor{yellow!30}38.69} & \underline{45.08} & \underline{43.83} & 45.73 \\
\bottomrule
\end{tabular}}
% \caption{LLM performance across ten languages, categorized by resource availability and measured in accuracy, and they are the averaged scores across benchmarks detailed in Section~\ref{sec:benchmarks} per language. The last three columns report average results for all languages (All), non-English languages (Non-Eng), and high-resource languages (High). The top three models are underlined and best scores are highlighted.}
\caption{LLM performance across ten languages, grouped by resource availability and measured in accuracy. Scores represent the average accuracy across benchmarks detailed in Section~\ref{sec:benchmarks}. The last three columns report mean results for all languages (All), non-English languages (Non-Eng), and high-resource languages (High). The top three models are underlined, and the best score for each language is highlighted.}
\label{tab:main_result}
\end{table*}