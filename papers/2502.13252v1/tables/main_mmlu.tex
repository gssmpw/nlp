\begin{table*}[!t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt} 
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{l|ccccccc|c|c|cc}
\toprule
& \multicolumn{7}{c|}{\textit{High}} & \textit{Medium} & \textit{Low} & \multicolumn{2}{c}{\textbf{Average}} \\
 & ar & en & fr & de & it & ru & es & id & sw & All & High \\
\hline
\multicolumn{11}{@{}l@{}}{\textit{\textbf{Multilingual LLMs}}} \\
mGPT (1.3B) & 25.02 & 25.27 & 26.10 & 24.05 & 25.70 & 25.48 & 25.64 & 25.10 & 24.11 & 25.16 & 25.32 \\
BLOOM (1.1B) & 26.36 & 26.25 & 26.65 & 26.51 & 27.25 & 26.76 & 26.09 & 25.86 & 26.61 & 26.48 & 26.55  \\
Llama3.2 (1.3B) & 27.72 & 31.17 & 27.69 & 27.94 & 27.67 & 27.54 & 28.19 & 27.86 & 26.39 & 28.02 & 28.27 \\
% Qwen2 (1.5B) & 40.36 & 54.92 & 47.43 & 45.01 & 45.73 & 43.87 & 47.64 & 45.38 & 30.35 & 44.52 & 46.42 \\
Qwen2.5 (1.5B) & \underline{42.24} & \underline{59.37} & \underline{50.43} & \underline{48.23} & \underline{49.17} & \underline{46.38} & \underline{51.89} & \underline{47.16} & \underline{30.62} & \underline{47.28} & \underline{49.67} \\
Gemma (2.6B) & \underline{31.96} & \underline{40.97} & \underline{34.52} & \underline{35.34} & \underline{34.45} & \underline{32.83} & \underline{35.38} & 32.58 & \underline{30.90} & \underline{34.33} & \underline{35.06} \\
\hline
\multicolumn{11}{@{}l@{}}{\textit{\textbf{Language-Specific LLMs}}} \\
AfriTeVa (1B) & & 26.87 & & & & & & & 26.93 & &  \\
CroissantLLM (1.3B) & & 25.35 & 25.36 & & & & & & & & \\
EuroLLM (1.7B) & 26.23 & 27.13 & 26.79 & 26.47 & 26.25 & 27.61 & 26.37 & &  &  & 26.69  \\
Jais-family-1p3b (1.3B) & 25.94 & 25.06 & & & & & & & & &  \\
Sailor (1.8B) & & 28.62 & & & & & & 26.39 & & &  \\
Sailor2 (1B) & & \underline{37.03} & & & & & & \underline{33.34} & & &  \\
\hline
\multicolumn{11}{@{}l@{}}{\textit{\textbf{Ours}}} \\
TransWebLLM (1.3B) & 26.63 & 24.66 & 25.69 & 25.46 & 25.32 & 26.42 & 26.21 & 25.28 & 25.35 & 25.67 & 25.77  \\
TransWebLLM-web (1.3B) & 26.49 & 26.41 & 26.84 & 26.11 & 26.16 & 26.56 & 26.58 & 26.68 & 26.48 & 26.48 & 26.45 \\
% TransWebLLM-mcSyn (1.3B) & 30.52 & 34.53 & 32.99 & 32.69 & 33.17 & 32.08 & 33.01 & 32.84 & 30.93 & 32.53 & 32.71 \\
TransWebLLM-cool (1.3B) & \underline{30.44} & 34.26 & \underline{32.58} & \underline{32.27} & \underline{31.95} & \underline{32.50} & \underline{32.53} & \underline{33.18} & \underline{31.11} & \underline{32.31} & \underline{32.36} \\
\bottomrule
\end{tabular}}
\caption{Evaluation on Global-MMLU full sets~\citep{singh2024global}, measured in accuracy. The rightmost columns report the average scores across all languages (All) and high-resource languages (High). Top 3 models are underlined.}
\label{tab:main_mmlu}
\end{table*}
