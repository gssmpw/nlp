\begin{table*}[!t]
\centering
\small
\setlength{\tabcolsep}{4pt} 
\resizebox{1.0\textwidth}{!}{%
\begin{tabular}{l|ccccccc|c|cc|ccc}
\toprule
& \multicolumn{7}{c|}{\textit{High}} & \textit{Medium} & \multicolumn{2}{c|}{\textit{Low}} & \multicolumn{3}{c}{\textbf{Average}} \\
 & ar & en & fr & de & it & ru & es & id & sw & cy & All & Non-eng & High \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{English LLMs}}} \\
Pythia (1.4B) & 33.21 &  54.63 & 40.14 & 36.23 & 35.14 & 38.32 & 39.91 & 36.83 & 36.90 & 31.57  & 38.29 & 36.47 & 39.65 \\
TinyLlama (1.1B) & 32.86 & 57.18 & 44.13 & 37.06 & 36.79 & 40.97 & 42.38 & 36.13 & 36.69 & 31.53  & 39.57 & 37.62 & 41.62 \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Multilingual LLMs}}} \\
mGPT (1.3B) & 32.90 & 45.38 & 40.27 & 34.54 & 34.89 & 40.53 & 38.88 & 39.47 & 38.98 & 31.14 & 37.70 & 36.84 & 38.20 \\
BLOOM (1.1B) & 34.97 & 50.94 & 41.80 & 34.62 & 33.69 & 37.56 & 42.69 & 43.23 & 37.09 & 31.57 & 38.82 & 37.47 & 39.47 \\
Llama3.2 (1.3B) & 34.78 & 58.16 & 44.10 & 39.73 & 40.93 & 45.38 & 44.15 & 44.67 & 38.30 & 31.84 & 42.20 & 40.43 & 43.89 \\
Qwen2 (1.5B) & 35.61 & \underline{61.07} & 47.40 & 40.79 & 42.61 & \underline{47.95} & \underline{47.40} & 45.93 & 38.89 & 32.23 & 43.99 & 42.09 & 46.12 \\
Qwen2.5 (1.5B) & 37.35 & \underline{\cellcolor{yellow!30}62.93} & \underline{48.69} & 40.49 & 43.10 & \underline{47.01} & \underline{48.41} & 46.17 & 37.98 & 31.78 & 44.39 & 42.33 & 46.85 \\
Gemma (2.6B) & 37.26 & \underline{62.39} & \underline{\cellcolor{yellow!30}49.78} & \underline{\cellcolor{yellow!30}44.27} & 44.57 & \underline{\cellcolor{yellow!30}48.60} & \underline{\cellcolor{yellow!30}49.35} & 48.27 & 40.18 & 32.28 & \underline{45.70} & \underline{43.84} & \underline{\cellcolor{yellow!30}48.03} \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Language-Specific LLMs}}} \\
AfriTeVa (1B) & & 37.70 & & & & & & & 40.60 & & &\\
BritLLM (3B) & & 60.45 & & & & & & & & 37.07 & & &\\
CroissantLLM (1.3B) & & 53.31 & 45.73 & & & & & & & & &\\
EuroLLM (1.7B) & 38.88 & 57.98 & 47.85 & \underline{42.07} & \underline{47.56} & 46.71 & 47.07 & &  &  &  & & \underline{46.87}\\
% GPT-fr (1B) & & 0.3887 & 0.3736 & & & & & & & & & &\\
Jais-family-1p3b (1.3B) & \underline{39.97} & 56.28 & & & & & & & & & & &\\
Sailor (1.8B) & & 55.40 & & & & & & 48.45 & & & & & \\
Sailor2 (1B) & & 54.61 & & & & & & \underline{49.44} & & & & & \\
\hline
\multicolumn{14}{@{}l@{}}{\textit{\textbf{Ours}}} \\
TransWebLLM (1.3B) & 39.30 & 56.30 & 46.11 & 41.01 & 45.75 & 46.38 & 45.27 & 47.54 & \underline{44.44} & \underline{38.69} & 45.08 & 43.83 & 45.73 \\
TransWebLLM-web (1.3B) & \underline{39.82} & 56.18 & 46.83 & 41.92 & \underline{47.01} & 46.22 & 46.92 & \underline{49.75} & \underline{44.40} & \underline{40.09} & \underline{45.91} & \underline{44.77} & 46.41 \\
TransWebLLM-cool (1.3B) & \underline{\cellcolor{yellow!30}40.22} & 57.76 & \underline{48.38} & \underline{43.04} & \underline{\cellcolor{yellow!30}48.64} & 46.72 & 47.13 & \underline{\cellcolor{yellow!30}50.67} & \underline{\cellcolor{yellow!30}44.48} & \underline{\cellcolor{yellow!30}40.41} & \underline{\cellcolor{yellow!30}46.75} & \underline{\cellcolor{yellow!30}45.52} & \underline{47.41} \\
$\Delta$ (Cool - Base) & +0.92 & +1.46 & +2.27 & +2.03 & +2.89 & +0.34 & +1.86 & +3.13 & +0.04 & +1.72 & +1.67 & +1.69 & +1.68 \\
$\Delta$ (Cool - Web) & +0.40 & +1.58 & +1.55 & +1.12 & +1.63 & +0.50 & +0.21 & +0.92 & +0.08 & +0.32 & +0.84 & +0.75 & +1.00 \\
\bottomrule
\end{tabular}}
\caption{LLM performance across ten languages, categorized by resource availability and measured in accuracy. The last three columns report average results for all languages (All), non-English languages (Non-Eng), and high-resource languages (High). The top three models are underlined and the best score for each language is highlighted.}
\label{tab:all_understanding}
\end{table*}