\section{Experiments}
In all experiments, we use Llama-3-8B\footnote{We use Llama-3 with BF16 precision.} and GPT2-small. The experiments are implemented using the Transformerlens library \citep{nanda2022transformerlens}.

\subsection{Tasks}
For all tasks, we uniformly sample 500 examples for circuit discovery and another 500 examples for evaluating faithfulness.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.245\linewidth]{graphs/hard_faithfulness/main/greater_than_2_500_accuracy.pdf} \hfill \includegraphics[width=0.245\linewidth]{graphs/hard_faithfulness/main/ioi_ABBA_gpt2_2_452_accuracy.pdf} \hfill \includegraphics[width=0.245\linewidth]{graphs/hard_faithfulness/main/ioi_ABBA_llama_3_454_accuracy.pdf} \hfill \includegraphics[width=0.245\linewidth]{graphs/hard_faithfulness/main/winobias_first_llama_1_475_accuracy.pdf}
    \vspace{0.05cm}
     \includegraphics[width=0.55\linewidth]{graphs/legend_horizontal.pdf} \hfill 
    \caption{Hard faithfulness 
    curves for GPT-2-small on Greater-Than (left) and IOI (mid-left), and for Llama-3-8b on IOI (mid-right) and Winobias (right).}
    \label{fig:faithfulness_all_main}
    \vspace{-5pt}
\end{figure*}


\textbf{Indirect Object Identification} (IOI; \citealp{wanginterpretability}): This task consists of prompts like ``When Mary and John went to the store, John gave a drink to'', and the model should predict the indirect object token `Mary'. The counterfactual prompts for this task are prompt of the same structure but with 3 other unrelated names, for example: ``When Dan and David went to the store, Sarah gave a drink to''. The metric that is being measured here is the logit difference between the token `Mary' and the token `John'. We evaluate with both GPT2-small and Llama-3-8B. For each model, we construct a dataset based on only examples where the model can predict the correct answer. 


\textbf{Greater-Than} \citep{hanna2024does}: We use the same setting as described in \S\ref{sec:motivating}. We evaluate this task only on GPT2-small, as Llama-3-8B's tokenizer is not compatible with the task setup; see App.~\ref{ap:task_details_gt} for details.

\textbf{Winobias} \citep{zhao2018gender}: 
A benchmark designed to evaluate gender bias in coreference resolution.
We collect 33 template from the dataset where professions are irrelevant to the coreference decision (e.g., ``The doctor offered apples to the nurse because she had too many of them'').
For each sample, we append the suffix: ``The pronoun \texttt{\{\}} refers to the'', where \texttt{\{\}} is a placeholder for the pronoun.
Each template can be used to construct four types of prompts: Anti-Female, Anti-Male, Pro-Female, Pro-Male. For example of each prompt see Table~\ref{tab:bias_examples}. We focus on the Anti-Female prompts, using only examples where the model predicts the \emph{incorrect} answer due to bias. This approach aims to identify components responsible for biased predictions. For Winobias, counterfactual prompts can be designed in multiple ways, each affecting the kinds of components one would recover; see Appendix~\ref{ap:task details} for further discussion. To avoid counterfactual-specific biases, we use mean ablation with examples from all four types during circuit discovery and faithfulness evaluation. The downstream metric $M$ is the logit difference between the correct profession and incorrect profession.
For further details on all tasks, see App.~\ref{ap:task details}.

\subsection{Circuit Evaluation}  
We measure faithfulness as a function of circuit size. Since different examples may produce circuits of varying sizes (due to differences in span lengths across examples), at each point we report the average circuit size across all examples. We extend the approach of \citet{hanna2024have} for ablating edges to also include attention edges. 

