\appendix

\section{Tasks Details} \label{ap:task details}

\subsection{IOI}
\label{ap:task details:ioi}

We use the dataset of \citet{wanginterpretability}. The data is generated using 15 templates. For the human-defined schema (provided below), we extend the partial schema provided by the authors to fully cover all spans in the prompt. The original dataset includes two types of prompts: ABBA prompts, where the indirect object (IO) token is the first name in the prompt, and BABA prompts, where the IO token appears as the second name. Because the ABBA and BABA prompts swap the order of important spans, we cannot aggregate across these two prompt types. Thus, we designed two distinct schema, resulting in the definition of two separate datasets: 

\noindent \textbf{IOI ABBA}:   
The human-defined schema and its application:

\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \textbf{Prefix}: [When]  
    \item \textbf{IO}: [Mary]  
    \item \textbf{and}: [and]  
    \item \textbf{S1}: [John]  
    \item \textbf{S1+1}: [went]  
    \item \textbf{action1}: [to the store,]  
    \item \textbf{S2}: [John]  
    \item \textbf{action2}: [gave a drink]  
    \item \textbf{to}: [to]  
\end{itemize}


\noindent \textbf{IOI BABA}: The human-defined schema and its application:  
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \textbf{Prefix}: [When]  
    \item \textbf{S1}: [John]  
    \item \textbf{S1+1}: [and]  
    \item \textbf{IO}: [Mary]  
    \item \textbf{S1+1}: [went]  
    \item \textbf{action1}: [to the store,]  
    \item \textbf{S2}: [John]  
    \item \textbf{action2}: [gave a drink]  
    \item \textbf{to}: [to]  
\end{itemize}


Table \ref{tab:model_accuracy} summarizes the performance of GPT2-small and Llama-3-8B for this task.
For the results in \S\ref{sec:results}, we use the ABBA datasets. Results for both datasets can be found in \S\ref{ap:faithfulness_curves}.
Note that there exist schemas that can handle both datasets simultaneously, eliminating the need for separation. However, these schemas require grouping the IO token and the S1 token into the same span, which mixes signals from both token positions and introduces new drawbacks.

\subsection{Greater-Than}\label{ap:task_details_gt}
We use the dataset of \citet{hanna2024does}. All examples in this task are generated using a single template:
``The \{\} lasted from the year \{\} to the year \{\}''. Because the event span (the first non-terminal) and the years have the same token length for all of our models, all examples in the dataset are fully token-aligned.

For the human-designed schema, we adopted the word-level schema used by \citet{hanna2024does}: \begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \textbf{The}: [The]
    \item \textbf{Noun}: [war]
    \item \textbf{lasted}: [lasted]
    \item \textbf{from}: [from]
    \item \textbf{the}: [the]
    \item \textbf{year}: [year]
    \item \textbf{XX1}: [16]
    \item \textbf{YY}: [45]
    \item \textbf{to}: [to]
    \item \textbf{the}: [the]
    \item \textbf{year}: [year]
    \item \textbf{XX2}: [16]
\end{itemize}



Table \ref{tab:model_accuracy} summarizes the performance of GPT2-small for this task.

\begin{table}[!ht]
\centering
\begin{tabular}{lrr}
 \toprule
 Dataset & GPT2-small & Llama-3-8B \\
 \midrule
 IOI-ABBA   & 92.5\%  & 71.7\% \\
 IOI-BABA   & 87.1\%  & 73.4\% \\
 Greater-Than & 100\% & 0.0\% \\
 \bottomrule
\end{tabular}
\caption{Accuracy on different datasets. Note that Llama-3 performs poorly on Greater-Than due to tokenization: it tokenizes four-digit years as a token of three digits, followed by a token of one digit. The Greater-Than prompts end with a two-digit token, which the model has presumably not seen followed by another two-digit number token; thus, it instead generates whitespaces, and assigns a low probability to the true answer.}
\label{tab:model_accuracy}
\end{table}


\subsection{Winobias}
\label{ap:task details:wb}

We use the dataset templates of \citet{zhao2018gender}. We constructed the dataset based on 33 pairs of prompt templates and 32 single-token professions. Each profession is associated with gender stereotypes.

\textbf{Stereotypical female professions}: teacher, editor, cashier, writer, counselor, counselors, nurse, sewer, baker, auditor, designer, clerk, cleaner, librarian, secretary, assistant, attendant, accountant.

\textbf{Stereotypical male professions}: lawyer, farmer, physician, developer, manager, guard, mechanic, chief, driver, mover, supervisor, analyst, constructor, cook, sheriff.

The original dataset consists of pairs of templates, each sharing the same prefix but having different suffixes. This variation causes the position of the correct answer to change between templates (e.g., ``The doctor offered apples to the nurse because she had too many of them'' and ``The nurse offered apples to the doctor because she might like them''). We separate templates where the first subject is the correct answer from those where the second subject is the correct answer. This separation ensures we do not mix biased signals with non-biased signals during the circuit discovery process.

Moreover, for each template, we construct four types of prompts: Anti-Female, Anti-Male, Pro-Female, and Pro-Male. In total, starting from 33 pairs of templates, we construct eight distinct datasets. Table \ref{tab:bias_examples} provides examples for each type of dataset. Intuitively, ``Anti-'' means that the profession-pronoun relationship goes against conventional gender biases; ``Pro-'' means that the the profession-pronoun relationship conforms to conventional gender biases. For example, if the correct answer is ``nurse'' and the pronoun is ``she'', we would say that this is a Pro-Female example; compare to the case where the pronoun is ``he''; this changes it to an Anti-Male example.

For the main results we used the Anti-Female-I dataset. Results for the Anti-Female-II dataset can be found in \S\ref{ap:faithfulness_curves_wb}.

\begin{table*}[ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ ll }
 \toprule
 Dataset & Example\\
 \midrule
 Anti-Female-I   & The \textbf{doctor} offered apples to the \textbf{nurse} because \textbf{she} had too many of them. The pronoun \textbf{she} refers to the" \\
 Anti-Female-II  & The \textbf{nurse} offered apples to the \textbf{doctor} because \textbf{she} might like them. The pronoun \textbf{she} refers to the" \\
 Pro-Female-I    & The \textbf{nurse} offered apples to the \textbf{doctor} because \textbf{she} had too many of them. The pronoun \textbf{she} refers to the"\\
 Pro-Female-II   & The \textbf{doctor} offered apples to the \textbf{nurse} because \textbf{she} might like them. The pronoun \textbf{she} refers to the" \\
 Anti-Male-I     & The \textbf{nurse} offered apples to the \textbf{doctor} because \textbf{he} had too many of them. The pronoun \textbf{he} refers to the"  \\
 Anti-Male-II    & The \textbf{doctor} offered apples to the \textbf{nurse} because \textbf{he} might like them. The pronoun \textbf{he} refers to the" \\
 Pro-Male-I      & The \textbf{doctor} offered apples to the \textbf{nurse} because \textbf{he} had too many of them. The pronoun \textbf{he} refers to the"\\
 Pro-Male-II     & The \textbf{nurse} offered apples to the \textbf{doctor} because \textbf{he} might like them. The pronoun \textbf{he} refers to the"\\
 \bottomrule
\end{tabular}
}
\caption{An example for each dataset. Each entry demonstrates a pronoun resolution scenario, with variations designed to reflect anti-female, pro-female, anti-male, and pro-male biases.}
\label{tab:bias_examples}
\end{table*}

Table \ref{tab:bias_assessment} presents the performance of Llama-3-8B on each type of dataset. We evaluated how often the model responded with the plausible answer and how often it chose the biased answer. Note that the model is not forced to select either of these options, and therefore, the sum of the percentages in each row does not necessarily equal 100\%.

\begin{table}[ht]
\centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{ lrrr }
 \toprule
 Dataset & Correct Answer & Biased Answer & \emph{Sum} \\
 \midrule
 Anti-Female-I   & 34.5\%  & \textbf{64.8\%}& 99.3\% \\
 Anti-Female-II  & 29.2\%  & \textbf{69.5\%}& 98.7\% \\
 Pro-Female-I    & \textbf{81.6\%} & 17\%&  98.6\%\\
 Pro-Female-II   & \textbf{75.9\%} & 23.2\%& 99.1\%\\
 Anti-Male-I     & \textbf{51.9\%} & 47.4\%&  99.3\%\\
 Anti-Male-II    & 35.8\%& \textbf{63\%}& 98.8\% \\
 Pro-Male-I      & \textbf{79.2\%} & 19.3\%& 98.5\% \\
 Pro-Male-II     & \textbf{61.7\%} & 37.8\%&  99.5\%\\
 \bottomrule
\end{tabular}
}
\caption{Bias measurement across the different datasets. The sum indicates the proportion of examples for which neither the correct nor the biased answer was the top token according to the model.}
\label{tab:bias_assessment}
\end{table}

For the human-defined schema, we used a schema defined by \citet{zhao2018gender} with minor adjustments: 
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item \textbf{correct answer}: [The doctor]
    \item \textbf{interacts with}: [offered apples to]
    \item \textbf{wrong answer}: [the nurse]
    \item \textbf{conjunction}: [because]
    \item \textbf{pronoun1}: [she]
    \item \textbf{circumstances}: [had too many of them]
    \item \textbf{dot}: [.]
    \item \textbf{The}: [The]
    \item \textbf{pronoun}: [pronoun]
    \item \textbf{pronoun2}: [she]
    \item \textbf{refers}: [refers]
    \item \textbf{to}: [to]
    \item \textbf{the}: [the]
\end{itemize}

For datasets where the wrong answer appears as the first subject, we swap the order of the answers in the schema.

Note that counterfactuals can be defined in many ways for this task; this complicates locating and interpreting circuits. For example, one could define counterfactuals from Anti-Male to Anti-Female, Anti-Male to Pro-Female, among others; each of these would isolate only some bias-specific subcircuit of the full coreference resolution circuit. To overcome biases that would result from using counterfactual prompts, we instead use mean ablations constructed from 16 examples spanning all examples (Anti-Female, Anti-Male, Pro-Female, Pro-Male); this is more likely to recover the full coreference resolution circuit.\footnote{As compared to, for example, the subcircuit that encodes gender bias.} 

\section{The Computation Graph} \label{ap:computation_graph}

The computation graph consists of the following node types: MLPs, attention heads, embeddings, and logits. To account for token positions, each node type has a separate instance at every position. Following \citet{wanginterpretability}, the input edge to an attention head is divided into three components: \texttt{v\_input}, \texttt{k\_input}, and \texttt{q\_input}. Consequently, three distinct edges connect every node \(v\) to a downstream attention head \(u\). Additionally, each attention head is connected to all attention heads at subsequent token positions via three types of connections: \(v\), \(k\), and \(q\).

The size of the computation graph varies depending on the model size \emph{and} prompt length. Table \ref{tab:graph_size} summarizes the average computation graph size for each dataset and model.


\begin{table}[!ht]
\centering
\begin{tabular}{lrr}
 \toprule
 Dataset & GPT2-small & Llama-3-8B \\
 \midrule
 IOI-ABBA   & 593,473.55  & 25,746,710.46 \\
 IOI-BABA   & 584,783.47 & 25,654,744.33 \\
 Greater-Than & 423,59.0 & - \\
 Winobias-I & - & 33,769,270.68 \\
 Winobias-II & - & 32,951,977.84 \\
 \bottomrule
\end{tabular}
\caption{Average number of edges in the computation graph per task.}
\label{tab:graph_size}
\end{table}



\section{Circuit  Construction} \label{ap:circuit construcion}

Once the attribution scores for all edges in the graph are approximated, there are several ways to construct a circuit. A straightforward approach might involve selecting components with the highest scores to construct the circuit. However, this naive method often results in a circuit that lacks proper connectivity between embeddings and logits. To ensure connectivity, we adopt a slightly modified version of the algorithm proposed by \citet{hanna2024have}.

As \citet{hanna2024have} states, this algorithm is a greedy method, similar to a maximizing version of Dijkstraâ€™s algorithm. The process begins with a circuit containing only the logits node at the final token position. At each step, the edge with the highest absolute attribution score that connects to a child node already in the circuit is added. If the corresponding parent node is not yet part of the circuit, it is also included. This iterative process continues for $N$ steps, where $N$ represents the desired circuit size.

Due to the presence of attention edges, parent and child nodes are not always located at the same token position.

At the end of the process, it is guaranteed that there is a path from the logits node at the final position to every node in the graph. To ensure full connectivity, we iterate over each node in the circuit and remove any nodes, along with their corresponding edges, that are not connected to any embedding node through a path in the graph. 

\section{Schema: Further Details}\label{ap:schema generation}
To generate a schema, we sample 3 groups of 5 examples each from the dataset. For each group, we ask the LLM to generate a separate schema.\footnote{We do not provide the LLM with any few-shot examples to avoid influencing its decisions on  defining the spans.} This process produces 3 candidate schema.
Next, we present the LLM with all 15 examples and the 3 candidate schemas, asking it to create a single unified schema. We test the unified schema using the LLM by iterating over all examples, and checking whether it can apply the schema in a valid manner to each.
If the output is invalid for a given example, we point out why to the LLM and ask it to try again. After three failed attempts, we move to the next example. While this process can identify most errors, it is not infallible and may provide false positives.

If the LLM fails to apply the schema correctly to $\geq 20\%$ of the examples, we consider the schema invalid, inform the model of the issues, and restart the schema generation process. The process ends once the schema can be successfully applied to at least $80\%$ of the examples.
See Appendices~\ref{ap:schema application}~and~\ref{ap:schema-validation} for more details on the process of applying and validating schemas, as well as an error analysis.

\subsection{Saliency scores} \label{ap:mask-creation}
We have also explored the following methods to determine the importance of each token position:

\begin{enumerate}
    \item \textbf{Input Attribution:}  
    This method involves patching the embedding of each input token individually and measuring the importance of each position based on its impact on a downstream metric.

    \item \textbf{Aggregated Node Attribution Scores:}  
    The importance of a position is derived from the significance of its components. While edge attribution patching could theoretically be used for each example to identify important components, this approach is computationally expensive. Instead, we propose using \textit{Node Attribution Patching}, which uses a linear approximation to estimate node importance rather than edge importance, significantly reducing computation time. This method efficiently calculates the importance of each node at every layer and position. By aggregating attribution scores for all nodes at each position, we estimate the overall importance of every position. 
\end{enumerate}

For both methods we used mean ablation values derived from "The Pile" dataset \cite{gaopile}. We evaluated both methods for schema generation and observed that the resulting schemas closely resembled those produced by the gradient-based method. However, a significant drawback of these approaches is their reliance on a counterfactual dataset, which adds complexity. For this reason, we ultimately chose the gradient-based method as our preferred approach.

\paragraph{Example.}  Figure~\ref{fig:masks} presents examples of the masks provided to the model. While not all masks highlight exactly the same token positions or token roles, we observe a consistent overall pattern across masks within the same dataset and model.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{graphs/masks.pdf}
    \caption{
The first example in Figure~\ref{fig:masks} is taken from the Greater-Than task and is generated using GPT2-small. Both the second and third examples are from the IOI dataset. The second mask is generated with GPT2-small, while the third is generated with LLaMA-3-8b. The highlighted positions are intended to capture the most influential positions that affect the model's predictions. }
    \label{fig:masks}
\end{figure}

\subsection{Schema Evaluation} 
\label{ap:schema-validation}

We define the application of a schema to an example as valid if:

\begin{itemize}
    \item All spans specified by the schema are included, no extra spans are included, and the spans appear in the exact order defined in the schema.
    \item Each token in the prompt is assigned to only a single span, and the tokens within each span are a continuous sequence from the original prompt.
\end{itemize}
    
Note that empty spans are valid. To ensure that empty spans are justifiable, we initially treat them as invalid during the first iterations. If, after several attempts, a valid result cannot be obtained, we relax this requirement and allow for empty spans.

We automatically test all the above requirements. If an application is found to be invalid, the next attempt includes details in the prompt about the specific failures in the previous attempt.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllrr}
    \toprule
    & Task & Method & Valid & Correct \\
    \midrule
    \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{\small{GPT2-small}}}} & \multirow{2}{*}{IOI ABBA} & LLM &  92.4\% &  88.3\%\\
    & & + Mask & 98.7\% &  86.8\%\\
    & \multirow{2}{*}{IOI BABA} & LLM & 98.0\% &  91.7\%\\
    & & + Mask & 93.7\% & 88.5\% \\
    & \multirow{2}{*}{Greater-Than} & LLM & 100\% & 100\% \\
    & &  + Mask & 100\% & 100\% \\
    \midrule
    \parbox[t]{2mm}{\multirow{8}{*}{\rotatebox[origin=c]{90}{\small{Llama-3-8B}}}} & \multirow{2}{*}{IOI ABBA} & LLM & 99.9\% & 96.0\% \\
    & & + Mask & 96.1\% & 96.3\% \\
    & \multirow{2}{*}{IOI BABA}  & LLM & 95.1\% & 81.5\% \\
    & & + Mask & 98.2\% & 92.3\% \\
    & \multirow{2}{*}{Winobias-$I$} & LLM & 98.5\% &  89.0\% \\
    & & + Mask & 96.5\% &  98.6\%\\
    & \multirow{2}{*}{Winobias-$II$} & LLM & 99.9\% & 97.9\% \\
    & & + Mask & 98.2\% & 95.4\% \\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Validity} is an automatic evaluation metric that tells us how many examples are usable for circuit discovery. \textbf{Correctness} is a human evalation metric that tells us whether the schema were applied in a way that a human agrees with. By definition, the human schema have 100\% correctness.}
    \label{tab:schema-eval}
\end{table}

We observe (Table~\ref{tab:schema-eval}) that the generated schemas are largely valid, indicating that most examples can be used for circuit discovery.

Recall that we additionally define a correctness metric, which measures to what extent a human annotator agrees with the application of the schema. To measure this, we have a human manually apply the LLM-generated schema to each template; we then compare to what extent the LLM application matches that of the human. Correctness is measured partially: that is, for each example, we compute the fraction of spans that are labeled identically to the human, and take this fraction as the correctness score. We then average these fractions across examples. We observe that a human tends to agree with how the schema were applied, as indicated by high correctness scores in Table~\ref{tab:schema-eval}.\footnote{Note, however, that we use the run with the maximum validity across 3 runs. We show scores across random trials in Appendix~\ref{ap:schema-validation}.} Thus, the schemas score high on intrinsic measures of quality. 


\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cllrrr}
        \toprule
        & Task & Method & Validity \#1 & Validity \#2 & Validity \#3 \\
        \midrule
        \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{\small{GPT2-small}}}} & \multirow{2}{*}{IOI ABBA} & LLM & 48.1 \%& \textbf{92.4\%} & 88.8\%\\
        & & + Mask &88.5\% & \textbf{98.7\%} & 65.2\% \\
        & \multirow{2}{*}{IOI BABA} & LLM &87.2\% & 55.1\% & \textbf{98.0\%}\\
        & & + Mask & \textbf{98.4\%}&  87.5\%& 93.7\% \\
        & \multirow{2}{*}{Greater-Than} & LLM & 98.8\%& \textbf{100\%} &  \textbf{100\%} \\
        & & + Mask & \textbf{100\%} & \textbf{100\%} & \textbf{100\%}\\
        \midrule
        \parbox[t]{2mm}{\multirow{8}{*}{\rotatebox[origin=c]{90}{\small{Llama-3-8B}}}} & \multirow{2}{*}{IOI ABBA} & LLM & \textbf{99.9\%}& 96.7\% & 99.8\% \\
        & & + Mask &\textbf{95.1\%} & 88.1\% & 92.9\% \\
        & \multirow{2}{*}{IOI BABA} & LLM & \textbf{96.1\%}& 62.6\% & 77.9\% \\
        & & + Mask & 98.2\% & 96.9\% & 95.4\% \\
        & \multirow{2}{*}{Winobias-$I$} & LLM & \textbf{98.5\%} & 96.0\% &  77.6\% \\
        & & + Mask & \textbf{96.5\%}& 71.4\% & 95.3\%\\
        & \multirow{2}{*}{Winobias-$II$} & LLM & 76.6\%& \textbf{99.9\%}& 93.5\% \\
        & & + Mask & 58.7\% & \textbf{98.2\%} & 93.5\% \\
        \bottomrule
        \end{tabular}
    }
    \caption{In our main experiments, we run schema generation and application three times per method, and take the run with the highest validity score. Here, we show the validities for all three runs for each method. (Validity \#1 corresponds to the run used in the main paper.)}
    \label{tab:schema-eval-seeds}
\end{table}

\section{LLM Prompts} \label{ap:schema prompts}

\subsection{Schema Generation}
 
\lstset{
    basicstyle=\ttfamily\footnotesize, % Monospaced font, smaller size
    breaklines=true,                  % Automatically wrap lines
    frame=single                      % Add a border around the code
}
Here is an example prompt we use to generate the schema:

\begin{lstlisting}
You are a precise AI researcher, and your goal is to understand how a language model processes a dataset by analyzing its behavior across different segments of prompts.  

To do this, you need to divide all prompts in the dataset into spans, where each span represents a meaningful part of the sentence.  

The aim is to split the prompts in the dataset systematically, allowing you to analyze the relationships between various parts of the sentence and support different types of model analysis.  

### Task: ###  

Your task is to define a schema---a structure that defines how to split all the examples in the dataset into meaningful spans.  

The schema defines how to divide all examples into the same set of spans! Even though the examples do not have the exact same tokens, they share a similar structure.  

All parts of each prompt should be assigned to a span, meaning the schema must provide a complete division of every prompt.  

### Input Format: ###  

1. **Tokens**: A list of tokens representing the example. Your task is to find a schema that defines how to divide this list into meaningful spans.  

2. **Mask**: A list of pairs in the format `[(token, value)]`, where a value of `1` indicates that the token is important and should be placed in its own span, separated from other tokens.  

### Instructions: ###  

1. Use syntactic and semantic rules to create a schema that defines how to divide all the examples in the dataset into meaningful spans.  

2. Use the Masks to create additional spans for any token marked as significant (`value = 1`). Each of these tokens should be placed in its own span.  

   **Note**: Apply this rule only if a specific token or token role is marked as important across many examples.  

3. If you think certain parts or tokens are crucial for the model's processing of the prompt, assign them to a separate span to highlight their importance.  

4. The spans should provide a complete division of the prompt, ensuring that every token is assigned to a span, and the spans should reflect the chronological structure of the prompt.  

5. The examples may vary, so you must define a schema that is not tailored to any specific example but can be applied consistently across all examples.  

### Goal: ###  

Given a set of examples, your goal is to define a schema---a structure that divides all examples into the same set of sub-spans.  

#### Return Format: ####  

Return a JSON object describing the schema.  

Each key in the dictionary should represent a span title (1-3 words), and the corresponding value should describe the tokens or segments assigned to that span.  

Provide a brief description of each span's role based on syntax, semantics, or another relevant aspect, but do not reference the Mask in the description.  

Provide a variety of examples in the descriptions to clarify the scope of each span.  

Assign a descriptive and unique span title (1-3 words) to each span. Avoid mentioning the Mask in the title (e.g., "Significant Token").  

Example format:  

```json
{
    "title": "description and examples"
}
### I will now provide you with 5 pairs of Tokens and a Mask.  

Follow the steps carefully, and return a JSON file in the correct format.  

---

**Example 0:**  

**Tokens:**  
`['Then', ',', ' Michael', ' and', ' Matthew', ' had', ' a', ' long', ' argument', ',', ' and', ' afterwards', ' Michael', ' said', ' to']`  

**Mask:**  
`[('Then', 0), (',', 0), (' Michael', 1), (' and', 0), (' Matthew', 1), (' had', 0), (' a', 0), (' long', 0), (' argument', 0), (',', 0), (' and', 0), (' afterwards', 1), (' Michael', 1), (' said', 0), (' to', 0)]`  

---

**Example 1:**  

**Tokens:**  
`['Then', ',', ' Jennifer', ' and', ' John', ' had', ' a', ' long', ' argument', ',', ' and', ' afterwards', ' Jennifer', ' said', ' to']`  

**Mask:**  
`[('Then', 0), (',', 1), (' Jennifer', 1), (' and', 1), (' John', 1), (' had', 0), (' a', 0), (' long', 0), (' argument', 0), (',', 0), (' and', 0), (' afterwards', 1), (' Jennifer', 1), (' said', 0), (' to', 0)]`  

---

**Example 2:**  

**Tokens:**  
`['Then', ',', ' Michael', ' and', ' William', ' had', ' a', ' long', ' argument', ',', ' and', ' afterwards', ' Michael', ' said', ' to']`  

**Mask:**  
`[('Then', 0), (',', 1), (' Michael', 1), (' and', 0), (' William', 1), (' had', 0), (' a', 0), (' long', 0), (' argument', 0), (',', 0), (' and', 0), (' afterwards', 1), (' Michael', 1), (' said', 0), (' to', 0)]`  

---

**Example 3:**  

**Tokens:**  
`['Then', ',', ' Jessica', ' and', ' Elizabeth', ' went', ' to', ' the', ' office', '.', ' Jessica', ' gave', ' a', ' drink', ' to']`  

**Mask:**  
`[('Then', 0), (',', 0), (' Jessica', 1), (' and', 1), (' Elizabeth', 1), (' went', 0), (' to', 0), (' the', 0), (' office', 0), ('.', 0), (' Jessica', 1), (' gave', 0), (' a', 0), (' drink', 0), (' to', 0)]`  

---

**Example 4:**  

**Tokens:**  
`['Then', ',', ' Kimberly', ' and', ' Michelle', ' had', ' a', ' long', ' argument', '.', ' Afterwards', ' Kimberly', ' said', ' to']`  

**Mask:**  
`[('Then', 0), (',', 0), (' Kimberly', 1), (' and', 0), (' Michelle', 1), (' had', 0), (' a', 0), (' long', 0), (' argument', 0), ('.', 0), (' Afterwards', 0), (' Kimberly', 1), (' said', 0), (' to', 0)]`  

---

### Guidelines: ###  

1. Ensure the schema assigns each part in the prompt to a span.  

2. The final token in every prompt must always be placed in its own span.  

3. The final token is always placed in the last span.  

4. Any token marked with a value of `1` is considered important and should be placed in its own span in the schema.  

5. Create spans that can be applied to all the examples.  

6. Ensure spans are relevant and consistent across the entire dataset.  
\end{lstlisting}

An example of the models' response:

\begin{lstlisting}
Based on the given examples and instructions, I've created a schema that divides the prompts into meaningful spans. 

Here's the JSON object describing the schema:

{
  "Initial Time Marker": "Represents the opening temporal indicator, e.g., 'Then'",

  "First Person": "Contains the name of the first person mentioned in the prompt",

  "Conjunction": "Represents the coordinating conjunction connecting the two people, typically 'and'",

  "Second Person": "Contains the name of the second person mentioned in the prompt",

  "Main Event": "Describes the primary action or event involving both people, e.g., 'had a long argument', 'went to the office'",

  "Punctuation": "Represents the punctuation separating the main event from the following action",

  "Time Transition": "Indicates a shift in time, typically 'afterwards' or similar phrases",

  "Active Person": "Contains the name of the person performing the subsequent action",

  "Secondary Action": "Describes the action taken by the active person after the main event, e.g., 'said', 'gave'",

  "Final Preposition": "The last word of the prompt, typically a preposition like 'to'"
}

This schema provides a consistent structure that can be applied to all the given examples and similar prompts. It accounts for the important tokens (marked with 1 in the mask) by assigning them to individual spans. The schema also ensures that every part of the prompt is assigned to a span, and the final token is always placed in its own span.
\end{lstlisting}



\subsection{Schema Application} \label{ap:schema application}

Given a schema, we call Claude-3.5-Sonnet to apply the schema on each example separately. Here is an example call:

\begin{lstlisting}
You are an assistant that splits tokens into spans based on a given schema.
Schema: {
    'Temporal Context': 'Words or phrases that set the initial time frame or sequence of events',
    'Primary Subjects': "Names of the main characters involved in the first action, typically connected by 'and'",
    'Initial Action': 'Describes the primary action or state of the subjects, including any relevant location',
    'Transition': 'Punctuation or words that separate the initial context from the subsequent action',
    'Active Subject': 'The name of the character performing the secondary action',
    'Secondary Action': 'The verb phrase describing the action performed by the active subject',
    'Object': 'The item or concept involved in the secondary action',
    'Final Preposition': "The last word in the prompt, typically 'to'"
    } 
        
Tokens: [
'While', ' Jason', ' and', ' Crystal', ' were', ' commuting', ' to', ' the', ' hospital', ',', ' Jason', ' gave', ' a', ' kiss', ' to'
]
    
Please split the tokens into the spans defined by the schema.
Return the spans as a JSON object where each key is a span name and the value is the list of tokens in that span.
Format: 
```json {"span title": []} ``` 

Ensure that:
    - All the spans are present.
    - Every token is assigned to a span.
    - No new spans are added.
    - Punctuation marks should be included in the spans. If no specific span is assigned to punctuation mark, it should be grouped with the preceding token.
    - Ensure that the last token is placed exclusively in the final span. 
    - The spans are in the correct order as in the schema.
    - The tokens in each span are a continuous segment of the full prompt.
    - The tokens are kept in the same order as they appear in the original prompt.
    - Don't remove spaces from tokens inside the list.
    - If a span has no tokens in a specific example, leave it empty, but still include the span for consistency across all examples.
\end{lstlisting}


An example of a response from Claude:

\begin{lstlisting}
Here's the JSON object with the tokens split into spans according to the given schema:

{
  "Temporal Context": ["While"],
  "Primary Subjects": [" Jason", " and", " Crystal"],
  "Initial Action": [" were", " commuting", " to", " the", " hospital"],
  "Transition": [","],
  "Active Subject": [" Jason"],
  "Secondary Action": [" gave"],
  "Object": [" a", " kiss"],
  "Final Preposition": [" to"]
}
\end{lstlisting}

We do not provide Claude with any few-shot examples, as we aim to keep this process fully automated. However, using human-crafted few-shot examples could improve the correctness of the application.



\section{Faithfulness Curves}\label{ap:faithfulness_curves}
\subsection{Results Across Schema Generation Trials}\label{ap:faitfulness-trials}
As mentioned in \S\ref{sec:schema-eval}, we run the entire pipeline three times for each task to ensure at least 90\% of the examples are valid. In \S\ref{sec:results}, we present the results from the trials with the highest validation scores. Here, we report results for all trials. 

\subsubsection{Greater-Than}
Figures~\ref{fig:faithfulness_all_gt}, \ref{fig:faithfulness_all_ioi_gpt2}, and \ref{fig:faithfulness_all_ioi_llama} display the results for the Greater-Than and IOI tasks. Trends are highly consistent across trials, and are all similar to what we observe in the main paper. This is a trivial case where each word could reasonably be assigned its own span, however, so the following sections are more representative of the variance of this method on more realistic datasets.

\subsubsection{Winobias}
\label{ap:faithfulness_curves_wb}

In Figure \ref{fig:faithfulness_all_wb_llama}, we present the results for the Winobias task. The soft faithfulness curves across all schemas and trials initially exhibit a significant drop, suggesting that the circuit assigns higher logits to the correct answer compared to the incorrect, biased answer. To quantify this, the dotted lines in the hard faithfulness curves represent the average percentage of cases where the circuit generates the \textbf{correct answer}. This observation is non-trivial, as we specifically analyze examples where the model predicts the \textbf{biased} (and wrong) answer. Indeed, near the drop in the soft faithfulness curves, the models often predict the correct answer at a significant rate. However, as the circuit size increases, the trend reverses: the soft faithfulness curves increase, correlating with a higher percentage of biased predictions.
This effect becomes particularly pronounced when token positions are differentiated. One plausible explanation is that the circuit incorporates components that simultaneously influence both the correct and biased answers, reflecting the delicate balance between task-relevant and bias-inducing factors. As component analysis lies outside the scope of this study, future research could further investigate this phenomenon.

In general, the human schema achieve the best top-prediction scores. The shape of the faithfulness curves makes it difficult to determine a best method, but the human schema tends to produce a curve that resembles the others, but left-shifted. This suggests that it is picking up on important components before the other methods. 

\subsubsection{Indirect Object Identification}\label{ap:faithfulness_curves_ioi}
In Figures~\ref{fig:faithfulness_all_ioi_gpt2} and \ref{fig:faithfulness_all_ioi_llama}, we show faithfulness curves for GPT2-small and Llama-3-8B, respectively. When viewing hard faithfulness, results do not differ significantly across templates, nor across trials for GPT2-small. The difference between LLM, LLM+Mask, and the human schema is smaller for the third trial for the ABBA template. It is also low for the second two trials for the BABA template. When viewing soft faithfulness, similar trends are present, but the schema-based approaches generally perform similarly to each other (with human and LLM+mask's margin from LLM being much smaller).

The difference between schemas is smaller for Llama-3-8B. While each schema-based method outperforms non-positional circuits, there does not appear to be a significant difference between LLM, LLM+Mask, and human schema.

\begin{figure*}
    \centering
    \textbf{Greater-Than GPT2-small}

    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/main/greater_than_2_500_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/greater_than_1_496_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/greater_than_3_498_accuracy.pdf} \hfill 

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/greater_than_2_498_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/greater_than_1_496_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/greater_than_3_498_faithfulness.pdf} \hfill 
    \vspace{0.05cm}
    \includegraphics[width=0.6\linewidth]{graphs/legend_horizontal.pdf} \hfill 
\caption{Each column shows results for a single trial.}
\label{fig:faithfulness_all_gt}
\end{figure*}

\begin{figure*}
    \centering
    \textbf{IOI-ABBA GPT2-small}


    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/main/ioi_ABBA_gpt2_2_452_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_ABBA_gpt2_1_207_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_ABBA_gpt2_3_322_accuracy.pdf} \hfill 

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_gpt2_2_452_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_gpt2_1_207_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_gpt2_3_322_faithfulness.pdf} \hfill 


     \textbf{IOI-BABA GPT2-small}


    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_ABBA_gpt2_1_207_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_BABA_gpt2_2_266_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_BABA_gpt2_3_467_accuracy.pdf} \hfill 

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_gpt2_1_423_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_gpt2_2_266_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_gpt2_3_467_faithfulness.pdf} \hfill 
    \vspace{0.05cm}
    \includegraphics[width=0.6\linewidth]{graphs/legend_horizontal.pdf} \hfill
\caption{Each column shows results for a single trial.}
\label{fig:faithfulness_all_ioi_gpt2}
\end{figure*}


\begin{figure*}
    \centering
    \textbf{IOI-ABBA Llama-3-8b}


    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/main/ioi_ABBA_llama_3_454_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_ABBA_llama_1_468_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_ABBA_llama_2_433_accuracy.pdf} \hfill 

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_llama_3_454_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_llama_1_468_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_ABBA_llama_2_433_faithfulness.pdf} \hfill 


     \textbf{IOI-BABA Llama-3-8b}


    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_BABA_llama_1_469_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_BABA_llama_2_302_accuracy.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/hard_faithfulness/appendix/ioi_BABA_llama_3_351_accuracy.pdf} \hfill 

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_llama_1_469_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_llama_2_302_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/ioi_BABA_llama_3_351_faithfulness.pdf} \hfill 
    \vspace{0.05cm}
    \includegraphics[width=0.6\linewidth]{graphs/legend_horizontal.pdf} \hfill
\caption{Each column shows results for a single trial.}
\label{fig:faithfulness_all_ioi_llama}
\end{figure*}


\begin{figure*}
    \centering
    \textbf{Winobias I Llama-3-8b}


    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_first_llama_1_475_accuracy_special.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_first_llama_2_343_accuracy_special.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_first_llama_3_374_accuracy_special.pdf} \hfill 

    \vspace{0.05cm}
     \includegraphics[width=0.5\linewidth]{graphs/legend_horizontal.pdf} \hfill 

     
    \vspace{0.05cm}
    
    \includegraphics[width=0.32\linewidth]{graphs/legend_wb.pdf} \hfill     

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_first_llama_1_475_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_first_llama_2_343_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_first_llama_3_374_faithfulness.pdf} \hfill 

    \vspace{0.05cm}
     \includegraphics[width=0.5\linewidth]{graphs/legend_horizontal.pdf} \hfill 

     \textbf{Winobias II Llama-3-8b}


    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_second_llama_1_298_accuracy_special.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_second_llama_2_495_accuracy_special.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/top_prediction/winobias_second_llama_3_430_accuracy_special.pdf} \hfill 

    \vspace{0.05cm}
     \includegraphics[width=0.5\linewidth]{graphs/legend_horizontal.pdf} \hfill 

     
    \vspace{0.05cm}
    
    \includegraphics[width=0.32\linewidth]{graphs/legend_wb.pdf} \hfill     

    
    \vspace{0.05cm}
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_second_llama_1_298_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_second_llama_2_495_faithfulness.pdf} \hfill 
    \includegraphics[width=0.32\linewidth]{graphs/soft_faithfulness/winobias_second_llama_3_430_faithfulness.pdf} \hfill 
     \vspace{0.05cm}
     \includegraphics[width=0.5\linewidth]{graphs/legend_horizontal.pdf} \hfill 

  
\caption{Winobias task results showing soft and hard faithfulness curves. Each column shows results for a single trial. The soft faithfulness curves initially drop significantly, suggesting the circuit assigns higher logits to the correct answer than to the incorrect, biased answer. The dotted lines in the hard faithfulness curves quantify this by showing the average percentage of cases where the circuit generates the correct answer, despite focusing on examples where the model predicts the biased answer. As the circuit size increases, the soft faithfulness curves rise, correlating with an increased percentage of biased predictions. This effect is more pronounced when token positions are differentiated.}
\label{fig:faithfulness_all_wb_llama}
\end{figure*}
