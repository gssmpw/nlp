\section{Position-aware Edge Attribution Patching (PEAP)} \label{sec:full_circ_discovery}

The importance of an edge $e$ is typically measured with the indirect effect (IE) of the edge on some target metric $M$.
In direct activation patching, also known as causal mediation analysis \cite{Pearl:2001:DIE:2074022.2074073,vig-2020-gender, mueller2024quest}, 
the IE is the change in the metric $M$ when the edge is `patched' to some counterfactual value, e.g., the edge value in a run on a different input $x'$:  $M(x|e=e_{x'}) - M(x)$. Performing this intervention at every edge is costly, prompting approximate algorithms. 
Edge attribution patching (EAP; \citealp{syed2023attribution})
linearly approximates the IE, $g(e)$, of edge $e=(u, v)$ as follows:
\begin{equation}
    \resizebox{\linewidth}{!}{$g(e) = M(x|e=e_{x'}) - M(x) \approx (z^*_u - z_u)^\top \nabla_v M(x) \label{eq:eap}$}
\end{equation}

The target metric $M$ can vary depending on the task.
Typically, $M$ is the logit difference between a correct completion and a minimally different incorrect completion.
$z_{u}$ and $z^*_{u}$ are the clean and counterfactual activations at the output of $u$, and $\nabla_v M(x)$ is the gradient of $M(x)$ w.r.t the input of $v$. 
\citet{syed2023attribution} showed EAP  to outperform direct activation patching with a greedy approach \citep{conmytowards}.
However, 
\citeauthor{syed2023attribution}\ only discovered circuits that do not consider positions. 

\subsection{Method} Equation~\ref{eq:eap} holds only when $u$ and $v$ are at the same position.
To include token positions in the circuit, attention edges that cross positions must be included in the discovey process. 
In autoregressive Transformer-based models,
these edges exist between nodes representing a given attention head that operates at different positions. 
Let $h^i_{t,l}$ denote the node corresponding to the $i$-th attention head at token position $t$ in layer $l$. 
Following \citet{olah2021framework}, we view the contribution of head $h^i_{t}$ to the residual stream % ($z_{h^i_{t}}$) 
as:
%
\begin{equation}
z_{h^i_{t}} = W^{i}_{O} (\text{softmax} ( \frac{q^i_{t}{K^i_{t}}^\top}{\sqrt{d_k}}  ) V^i_{t} ) \in \mathbb{R}^{d_{\text{model}}}
\label{eq:attn}
\end{equation}
Here, \( W_{O}^{i} \) represents the columns of the projection matrix \( W_{O} \) that specifically project the output of head \( h^i \). \( K^i_{t} \in \mathbb{R}^{t \times d_{\text{head}}} \) is the key matrix, and \( V^i_{t} \in \mathbb{R}^{t \times d_{\text{head}}} \) is the value matrix.

$h^i_{t}$  is connected to 
 every node  $h^i_{t',l}$ at position $t' \le t$, via \textbf{3 edges}: the value vector $v^i_{t',l}$, the key vector $k^i_{t',l}$, and the query vector $q^i_{t,l}$. As direct communication between heads occurs only within the same layer, 
we omit henceforth the layer notation and assume that all attention edges connect attention heads within the same layer.


To approximate the attribution scores of attention edges, we first calculate \( z^*_{h^i_t} \), the corrupted output of the head \( h^i_t \) caused by patching \( v^i_{t'} \), \( k^i_{t'} \), or \( q^i_t \). We then approximate the attribution as follows:
\begin{equation}
    M(x| e = e_{x'}) - M(x) \approx (z^*_{h^i_{t}} - z_{h^i_{t}})^\top \nabla_{z_{h^i_{t}}} M(x) \label{eq:eap-attn}
\end{equation}

Based on Eq.~\ref{eq:attn}, we define the corrupted vector \( z^*_{h^i_t} \) for patching $v_{t'}^i$ (Eq.~\ref{eq:patch-v}), patching $k_{t'}^i$ (Eq.~\ref{eq:patch-k}), and patching $q_{t}^i$ (Eq.~\ref{eq:patch-q}):

\begin{align}
\label{eq:patch-v} \resizebox{\linewidth}{!}{$z^*_{h^i_{t}} = W^{i}_{O} (\text{softmax} \left( \frac{q^i_{t}{K^i_{t}}^\top}{\sqrt{d_k}}\right) \left[ v_1^i, ..., {v_{t’}}^*, ..., v_t^i\right])$}\\
\label{eq:patch-k} \resizebox{\linewidth}{!}{$z^*_{h^i_{t}} = W^{i}_{O} (\text{softmax} \left( \frac{q^i_{t}{\left[ k_1^i, ..., {k_{t’}}^*, ..., k_t^i \right]}^\top}{\sqrt{d_k}}\right)V^i_{t} )$}\\
\label{eq:patch-q} \resizebox{\linewidth}{!}{$z^*_{h^i_{t}} = W^{i}_{O} (\text{softmax} \left( \frac{{\left[ q^i_{t} {k_1^i}^\top, ..., {q^i_{t}}^* {k_{t’}}^\top, ..., q^i_{t} {k_t^i}^\top \right]}}{\sqrt{d_k}}\right)V^i_{t} )$}
\end{align}

Figure \ref{fig:attention_patching} provides an illustration of each type of patching. 
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.2]{graphs/patching.pdf}
    \caption{\vspace{-2pt}
    Illustration of the attention mechanism from the perspective of position 3. We approximate how patching \textcolor[RGB]{147,145,255}{$v_1$}, \textcolor[RGB]{109,177,255}{$k_1$} or \textcolor[RGB]{255,128,223}{$q_3$} impacts the downstream metric via the output of the attention head at position 3.}
    \label{fig:attention_patching}
    \vspace{-6pt}
\end{figure}
By using PEAP to approximate attention edges, we can now approximate both within-position edges and cross-position edges.

Once the attribution scores for all edges have been computed, we construct the circuit using an adapted version of the greedy algorithm proposed by \citet{hanna2024have}. See App.~\ref{ap:circuit construcion} for details. 

\vspace{-2pt}
\subsection{Preliminary Demonstration}
\vspace{-2pt}
We now compare PEAP to the position-agnostic approach of \citet{syed2023attribution} using the Greater-Than task \citep{hanna2024does} on GPT2-small \citep{radford2019language}.
The dataset includes prompts like: ``The war lasted from the year 1741 to the year 17\underline{\hspace{1em}}'' and counterfactual variants with ``01'' as the starting year (e.g., ``The war lasted from the year 1701 to the year 17\underline{\hspace{1em}}'').
The downstream metric $M$ measures the probability difference between valid and invalid year answers.
We use 500 examples each for circuit discovery and evaluation, considering only prompts with valid model predictions.
Circuit evaluation is based on two metrics: (1) \textbf{Soft Faithfulness} ($F_S(C) = \frac{M(C)}{M(\mathcal{M})}$), comparing the circuit's performance to the full model's, and (2) \textbf{Hard Faithfulness} ($F_H(C) = \mathds{1}\{C_T = \mathcal{M}_T\}$), assessing token agreement at the final position $T$.
While $F_S$ is more commonly used, we see $F_H$ as a more behaviorally grounded metric.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{graphs/schema_example.png}
    \caption{Example schema for each task. We show examples from the LLM+Mask method. See \S\ref{ap:task details} for examples of human-designed schemas.}
    \label{tab:schema-example}
    \vspace{-5pt}
\end{figure*}

Figure~\ref{fig:p1} presents the faithfulness scores of the Greater-Than task for both methods as a function of circuit size. PEAP enables the discovery of circuits that improve the trade-off between circuit size and faithfulness: \textbf{position-aware circuits are smaller, and yet achieve similar faithfulness with orders-of-magnitude fewer edges.}


\subsection{Aggregating Scores Across Examples}

In the Greater-Than dataset, we can simply aggregate position-specific scores across examples.
This naive approach works because all examples in the Greater-Than dataset consist of exactly the same number of tokens, and each position has the same meaning across all examples.
In other words, this approach requires all examples in the dataset to be \emph{fully position-aligned}.
This raises a key challenge for non-templatic datasets: the same token position may not have the same meaning across examples, and examples may vary in length.

Prior methods addressing positionality typically follow one of two strategies: 
(1) \textbf{full alignment}, where the dataset is generated from a single template---as in the Greater-Than dataset---and (2) \textbf{partial alignment}, where specific token position roles are consistent across examples.
For instance, in the IOI dataset \citep{wanginterpretability}, the authors \emph{manually} identified five key single-token roles (IO, S1, S1+1, S2, End) shared across all prompt templates, which are sufficient for constructing a faithful circuit. 
In the next section, we describe an automatic approach inspired by partial alignment that  enables us to include positional information in tasks with variable-length inputs.
