\section{Schemas for Variable-length Inputs} 
\label{sec:schema}

Discovering circuits requires aggregating edge scores across examples. 
However, because edges correspond to specific positions in the computation graph, naive aggregation assumes perfect positional alignment across examples---an impractical assumption for most datasets.
To address this challenge, we relax this assumption and only assume that examples share a similar high-level structure, which is represented by a \textbf{schema}.
A dataset schema identifies \textit{spans} within input examples, where each span covers consecutive tokens grouped under a meaningful category.
For instance, in the input ``The \textcolor{positional}{war} lasted from 1453 to 14\underline{\hspace{1em}}'', the span ``\textcolor{positional}{war}'' could be labeled \emph{Subject}.
This allows us to handle spans of varying lengths, such as treating ``\textcolor{positional}{Black Plague}'' in another example as a single position with the same role as ``\textcolor{positional}{war}''.
Examples of schemas for specific datasets are shown in Figure~\ref{tab:schema-example}.
Schemas are defined based on semantic, syntactic, or other patterns in the data, and may be guided by knowledge of how the model processes examples.
Spans are ordered sequentially within the input, covering all parts of a prompt.\footnote{Future work may relax the sequential order assumption to support even greater variation across examples.}

\subsection{Discovering Circuits at the Schema Level}
When all examples share the same schema-defined structure, we can leverage this consistency to create an abstract computation graph for all examples.
For now, we assume spans in the schema can be automatically mapped to corresponding tokens in any dataset sample. We discuss automating this process later.

Let $ \smash{ G_{x}=(E_x,V_x) }$ represent the computation graph derived from example $x \in \mathcal{D}$.
Given schema $\mathcal{S}$ with $k$ spans, we define the \textit{abstract} computation graph $G_\mathcal{S}=(E_\mathcal{S},V_\mathcal{S})$, which is structurally equivalent to a computation graph of $\mathcal{M}$ on an input of length $k$. Intuitively, each span is represented by a single position.

At a high level, given an example, we (i) compute edge scores on the true computation graph $G_x$;
(ii) map from edges in $G_x$ to edges in $G_\mathcal{S}$, and sum edge scores in $G_x$ to compute edge scores in $G_\mathcal{S}$;
(iii) construct a circuit in $G_\mathcal{S}$.


To this end, we define a mapping $f_{x}: E_{\mathcal{S}} \rightarrow 2^{E_x}$ from an edge $e=(u_{s_1},v_{s_2})$ to a set of edges in $E_x$:
%
\begin{equation}
    f_{\mathcal{S}}^x(e)=\{ e'\in G_x \mid e=(u_i, u_j), i \in s_1, j\in s_2 \}
\end{equation}
%
where $u_{s_1},v_{s_2}$ represent components in the computation graph at spans $s_1, s_2$. 

Given an attribution function $g_x$ (defined at the token position level), the attribution score $g_\mathcal{S}$ (defined at the segment level) of the edge $e\in G_{\mathcal{S}}$ is the sum of all the edge effects mapped to this edge, averaged over all examples in the task dataset:

\begin{equation}
    g_{\mathcal{S}}(e) = \frac{1}{|\mathcal{D}|}\sum_{x \in \mathcal{D}}\sum_{e' \in f^x_\mathcal{S}(e)} g_{x}(e')
\end{equation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{graphs/abs_vs_reg.pdf}
    \caption{Circuits defined over schemas. Every node/edge at position \( s \) in the abstract computation graph is mapped to a set of nodes/edges in the full computation graph within the span \( s \).}
    \label{fig:abs_graph}
\end{figure}

After computing the attribution score for each edge in $G_{\mathcal{S}}$, we construct the abstract circuit  $\mathcal{C}_{\mathcal{S}} \subseteq G_{\mathcal{S}}$ with the same greedy algorithm used in the previous section (see App.~\ref{ap:circuit construcion}).

\paragraph{Faithfulness evaluation.}
The process of faithfulness evaluation involves ablating edges that are not included in the circuit. 
To evaluate an abstract circuit on a sample $x\in\mathcal{D}$, we convert back to the computational graph $G_x$ and construct $\mathcal{C}_x \subseteq G_x$:
\begin{equation}
\mathcal{C}_x = \{ e \mid e \in f^x_\mathcal{S}(e'), \forall e' \in C_{\mathcal{S}} \}
\end{equation}
In other words, for every edge $e'$ in the abstract circuit $C_{\mathcal{S}}$, the corresponding edges in $f_x(e')$ form the circuit $\mathcal{C}_x$.
Figure \ref{fig:abs_graph} depicts this process.


\subsection{Automating Schema Generation and Application} \label{sec:schema-generation}
Given a schema $ \mathcal{S} $ and a function $ f_\mathcal{S} $ to apply it to every sample $ x \in \mathcal{D} $, we can automatically discover position-aware circuits, even for tasks involving variable-length examples. 
However, as shown in Figure \ref{tab:schema-example}, schema definitions are dataset-specific, requiring tedious manual work and intricate knowledge of the task at hand as well as knowledge of the analyzed model's computations.
Applying the schemas may also require deep knowledge on the target dataset.
To generate interpretable circuits, schemas must be both faithful to the model and meaningful to humans.

In this section, we propose an automated process for schema generation and application to streamline circuit discovery.
Inspired by recent work on LLM agents \cite{wang2024survey} for automated interpretability \cite{schwettmann2023find,shaham2024multimodal}, we investigate the use of LLMs for generating and applying schemas.


\paragraph{Schema Application.}
Applying a schema entails mapping each token to a specific span.
After defining the schema, we utilize an LLM to perform the application process.
We provide the prompt for applying the schema in App.~\ref{ap:schema application}.

\paragraph{Schema Generation.}
Creating a schema requires specifying span types while two conditions: (1) spans must follow the same order across all examples, and (2) each prompt must be fully covered by the spans.
These criteria are incorporated into the LLM's prompt (details in App.~\ref{ap:schema generation}).
Given a dataset, we use an LLM to create three schema versions based on distinct subsamples, then have the LLM unify these versions into a final schema.
The schema is validated by confirming it applies to at least 80\% of the subsampled data;
otherwise, the process is repeated.
Examples of LLM-generated schemas are shown in Figure~\ref{tab:schema-example}.


\paragraph{Saliency scores: A model-based approach for schema generation.}

The schema generation described above does not account for the computations performed by the target model $\mathcal{M}$ on the given dataset $\mathcal{D}$, potentially producing unfaithful schemas (as we will show in \S\ref{sec:results}).
To address this, we incorporate the importance of each token position to the model's computation into the schema generation.

Our key idea is to inform the LLM which positions significantly influence the model's decisions.
While many feature attribution methods can be explored \cite{danilevsky-etal-2020-survey,wiegreffe2021teach,wallace-etal-2020-interpreting}, we employ a simple saliency score, inputXgradient \cite{shrikumar2017learning}.
The score of a token in position $t$ is defined as 
$ s(t) = \| \mathbf{e_t} \cdot \nabla_{\mathbf{e_t}} M(x) \| $, where $ \mathbf{e_t} $ is the token embedding at position $ t $.
We compute a softmax over these scores and define a mask for each example as follows:
\begin{equation}
m(t) =
\begin{cases}
1 & \text{if } \frac{e^{s(t)}}{\sum_{i=1}^{n} e^{s(i)}} > \frac{1}{n}, \\
0 & \text{otherwise},
\end{cases}
\end{equation}
Where $n$ is the prompt length.
This mask is then attached to each example, and the LLM is instructed to use it when designing the schema.
Token position which is important across many examples should be placed in its own span. Further information on mask construction and alternative attribution methods can be found in Appendix~\ref{ap:mask-creation}.


\paragraph{Schema Evaluation.}\label{sec:schema-eval}
We propose two intrinsic metrics and one extrinsic metric to evaluate the entire schema pipeline.
Intrinsic metrics assess the LLM schema application. An application is \textbf{valid} if span labels are ordered correctly and every token is assigned to a single span, and \textbf{correct} if it matches a human application for the same schema.
Extrinsic metrics evaluate schema design and application through circuit discovery.
A good schema definition and application should achieve better trade-offs between circuit size and faithfulness.

Invalid schema applications are filtered out for both the discovery and evaluation datasets, while incorrect applications are retained since automating their filtering is infeasible in general datasets.
If an application is valid but incorrect, we expect it to affect the faithfulness of the discovered circuit.
To ensure minimal distribution shift in the dataset, we consider a generation and an application of a schema on an entire dataset as successful if at least 90\% of the examples are valid.
This means that each circuit is discovered using a slightly different set of examples (up to 10\%), but we ensure that all circuits are compared using the exact same evaluation set, which is the intersection of the examples for all runs. In practice this intersection includes 90\% of the total dataset examples.
In our experiments, three full pipeline runs were usually sufficient to achieve at least one successful run.

We found Claude 3.5 Sonnet \citep{claude3} to perform well in both schema generation and application, achieving high validity and correctness scores (Table~\ref{tab:schema-eval}, Appendix~\ref{ap:schema-validation}).
We also experimented with Llama-3-70B \citep{grattafiori2024llama3} and GPT-4o \citep{openai2024gpt4ocard}, but they failed to meet our thresholds for valid applications.
In \S\ref{sec:results}, we show that LLM-generated schemas score well on extrinsic quality measures, with saliency-enhanced schemas proving comparable to human-designed ones.

