
\begin{center}
~\\
{\large\textbf{APPENDIX}}
\end{center}

%\tableofcontents
In this supplementary part of the paper we provide further information on the proposed method (Appendix~\ref{sec:app_method}). We also explain with detail our evaluation methodology (Appendix~\ref{sec:app_eval}). Finally, we show additional results that could not fit in the main manuscript (Appendix~\ref{sec:app_results}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Method Details}
\label{sec:app_method}

\subsection{Loss Gradient Visualization}

In Sec.~\ref{sec:method_loss} of the main manuscript, we study the effect of hyper-parameters $\gamma$ and $\varepsilon$ on the gradient of negative pairs $\nabla^-$. Here, to further facilitate understanding, we plot the result of $\nabla^-$ (Eq.~\ref{eq:nablaneg}) for a range of potentials $e^{-\gamma d^2_{ij}}$ under different values of $\gamma$ and $\varepsilon$ in Fig.~\ref{fig:grad}. We do so using $|A^-|=128$ and $c=(|A^-|-1) e^{-\gamma d^2_{ij}}$. With the latter, we approximate the case where the average negative potential is not far from the potential of the $i,j$ pair.

\begin{figure}[h]
%\vskip 0.2in
%\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig_grad}}
\vskip -0.2in
\caption{Plot of $\nabla^-$ as a function of the negative pair potential $e^{-\gamma d^2_{ij}}$ for different values of $\gamma$ and $\varepsilon$. From left to right, we show $\gamma=\{2,5,10\}$. From darker to lighter, colors correspond to $\varepsilon=\{10^{-8}, 10^{-7}, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}\}$. Dash-dotted lines indicate each $\varepsilon$ value (notice that, in $\fu{L}$, $\varepsilon$ is compared to an average negative pair potential, hence placing $\varepsilon$ as a reference in the potential axis makes sense).}
\label{fig:grad}
%\end{center}
%\vskip -0.2in
\end{figure}

\subsection{A More Numerically-Friendly Version of $\fu{L}$}

For conducting all our experiments, we found no issue in the use of $\fu{L}$ with regard to numerical stability with 32-bit precision. However, we should note that $\fu{L}$, as written in the main manuscript, may have some numerical instability, especially when employing abnormally small/large values of $\varepsilon$/$\gamma$, or potentially when using a numerical precision below 32\,bits. In such cases, we recommend switching to the formulation below.

First of all, we multiply the terms inside the logarithm by $1/\varepsilon$:
\begin{equation*}
\fu{L} = \frac{1}{|A^+|}\!\sum_{(i,j)\in A^+}\!\!\!d_{ij}^2 + \log\left(\varepsilon + \frac{1}{|A^-|}\!\sum_{(i,j)\in A^-}\!\!\!e^{-\gamma d^2_{ij}}\right) = \frac{1}{|A^+|}\!\sum_{(i,j)\in A^+}\!\!\!d_{ij}^2 + \log\left(1 + \frac{1}{\varepsilon|A^-|}\!\sum_{(i,j)\in A^-}\!\!\!e^{-\gamma d^2_{ij}}\right) + \log(\varepsilon) .
\end{equation*}
With this, we obtain the term $\log(\varepsilon)$, which is just a constant that does not affect the gradient and can thus be dropped. Next, we perform the change of variable $1/(\varepsilon|A^-|)=\beta e^b$, where $b\geq0$ is a constant we will set for the upper numerical limit we allow to the exponential. With this change and a few simple operations, we arrive to
\begin{equation*}
\fu{L} = \frac{1}{|A^+|}\!\sum_{(i,j)\in A^+}\!\!\!d_{ij}^2 + \log\left(1 + \beta\!\!\!\sum_{(i,j)\in A^-}\!\!\!e^{b-\gamma d^2_{ij}}\right) ,
\end{equation*}
where $\beta=1/(\varepsilon|A^-|e^b)$. We can now choose $b$ as a compromise between the overflow of $e^b$ when $d_{ij}=0$ and the underflow of $e^{b-\gamma d_{ij}^2}$ when $d_{ij}$ is large. For normalized Euclidean distances $d$ and the ranges of $\varepsilon$ and $\gamma$ we consider, we choose $b=10$. Note that, in addition, $\log(1+x)$ can be implemented with \verb|log1p(x)| in most scientific programming languages.

\subsection{Model}

We here provide further specification of our network architecture (for full detail we refer the interested reader to the published code). As mentioned, we use 16\,kHz mono audio with a maximum length of 10\,min for both training and evaluation. For training, we cut 2.5\,min blocks uniformly at random. For CLEWS, we divide such blocks into 8~non-overlapping 20-second segments. As the last segment is only 10\,s, we take the opportunity to repeat-pad such segment and also consider it in our training, with the hope that this will facilitate retrieval with query lengths shorter than 20\,s (which we also repeat-pad as they would not be long enough to accommodate the total striding factor of our architecture).

After obtaining segments, we apply a constant-Q transform (CQT) with 20\,ms hop size, spanning 7~octaves (from a minimum frequency of 32.7\,Hz), and with 12~bins per octave (we use the nnAudio library\footnote{\url{https://github.com/KinWaiCheuk/nnAudio}} in non-trainable mode, with the rest of the parameters set as default). We then take the CQT magnitude and average in time every 5~consecutive frames without overlap. This CQT representation is sent to three data augmentation functions (explained in the next subsection).

The neural network architecture starts by taking the square root of the CQT magnitude, normalizing every segment's representation between 0 and 1, and applying a learnable affine transformation. Next, we apply a 128-channel 2D convolution with a frequency-time kernel size of 12$\times$3 and a frequency-time stride of (1,2). This is followed by batch normalization (BN), a ReLU activation, and a 256-channel 2D convolution with a kernel size of 12$\times$3 and a stride of (2,2). This constitutes our frontend. Unless stated otherwise, we use the default PyTorch\footnote{\url{https://pytorch.org/docs/2.3/}} parameters from version 2.3.1.

As mentioned in the main manuscript, our backbone is formed by pre-activation ResNet modules with ReZero and instance-batch normalization (IBN). We use 3, 4, 6, and 3 residual blocks with 256, 512, 1024, and 2048 channels, respectively. The strides are (1,1), (2,2), (2,2), and (1,1) for each block. The residual blocks have an IBN--ReLU--conv--BN--ReLU--conv structure, with a kernel of 3$\times$3 in the convolution layers. To reduce GPU memory consumption, we employ half the channel dimension inside the residual block. If there is some channel or stride change, the skip connection features a BN-ReLU-conv block also with a 3$\times$3 kernel.

The output of the backbone is time- and frequency-pooled by a generalized mean pooling operation with a single learnable exponent. Finally, the result is processed with BN and projected to 1024~dimensions by a linear layer. None of our linear or convolutional layers feature bias terms. As mentioned in the main manuscript, we use normalized squared Euclidean distances (mean squared differences) between embedding vectors.

\subsection{Training}

We train all models with Adam using the default PyTorch parameters, a learning rate of 2$\cdot$10$^{-4}$, and a batch size of 800~segments chosen from 100~tracks featuring 3 positives per anchor. In the main experiments, we follow a reduce-on-plateau strategy for the learning rate, monitoring an average between MAP and NAR measures on the validation set. We define an epoch as using all training tracks as anchor once, and set a 10-epoch patience period and an annealing factor of 0.2. Using this strategy, training CLEWS on SHS and DVI takes approximately 2 and 9 days, respectively, using two NVIDIA H100-80GB GPUs. In the ablation experiments, to reduce the computational burden, we only train for 20~epochs and, during the last 5~epochs, we apply a polynomial learning rate annealing with an exponent of 2. 

During training, we employ three CQT data augmentation functions: SpecAugment, time stretch, and pitch roll. For SpecAugment, we mask a maximum of 15\% of the time/frequency tiles. For time stretch, we resample by a uniformly sampled factor between 0.6 and 1.8. For pitch roll, we choose a uniform value between $-$12 and $+$12. In the DVI data set, we use a probability of 0.1 independently for each augmentation. However, since the SHS data set is considerably smaller than DVI and potentially features less variability, we find some benefit in increasing such probability for SHS. In that case, we set the augmentation probabilities to 0.4, 0.3, and 0.5 for SpecAugment, time stretch, and pitch roll, respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation Methodology Details}
\label{sec:app_eval}

%\subsection{Data}

\subsection{Track- and Segment-level Evaluations}

For the track-level evaluation, we cut the entire raw waveform (up to the first 10\,min) into overlapping blocks or segments using a hop size of 5\,s. For both the queries and the candidates, the length of such blocks/segments corresponds to the same length we used to train each model (that is, 2.5\,min for CQTNet, DVINet+, and ByteCover1/2$\dag$ and 20\,s for CoverHunter, ByteCover3$\dag$, and CLEWS). Next, we compute pairwise distances for each query-candidate block/segment and apply a distance reduction function. We use $\fu{R}_\tx{meanmin}$ for all models except CLEWS, which exploits the newly proposed $\fu{R}_\tx{bpwr-10}$. After reduction we obtain a track-based distance matrix that we can use to sort candidates per query and compute common evaluation measures.

For the segment-level evaluation, we also cut the entire raw waveform into overlapping blocks/segments with a hop size of 5\,s. For candidates, we also use the same length that we used to train each model (same as in the track-level evaluation). However, for the queries, we extract multiple-length segments with a hop size of 5\,s (we consider segment lengths $\tau=\{5,10,20,30,40,60,90\}$\,s). Then, given a segment length, we compute pairwise distances for each query-candidate block/segment, and apply the $\fu{R}_\tx{min}$ distance reduction to obtain a track-based distance matrix. After that, the evaluation proceeds as with the track-level evaluation (and any common evaluation protocol in musical version matching). The usage of $\fu{R}_\tx{min}$ puts the focus on the best-matching segment per track, and is equivalent to performing version matching on an index formed by all possible segments, treating them independently, and removing duplicate track names after sorting.


\subsection{Normalized Average Rank}

To evaluate retrieval performance and complement mean average precision (MAP), we employ an enhanced version of the normalized average rank (NAR), originally proposed by \citet{muller_performance_2001}. Given a list of retrieved items $R$, sorted in descending order of predicted relevance to a query $q$, and containing a set of target matches $M=\{m_1,\dots m_{|M|}\}$, $M\subset R$, \citet{bosteels_fuzzy_2007} redefined NAR as
\begin{equation*}
\widetilde{\tx{NAR}}_q = \frac{1}{|M||R|} \sum_{i=1}^{|M|} \Bigl( \tx{rank}(m_i,R)-i \Bigr) ,
\end{equation*}
where the function $\tx{rank}(m,R)\in[1,|R|]$ returns the rank of $m$ in $R$. This definition, as well as the one of \citet{muller_performance_2001}, yields 0 for perfect retrieval, 0.5 for random retrieval, and approaches 1 as performance worsens. However, a value equal to one is never obtained. Not only that, but the maximum bound inversely depends on $|M|$ and, therefore, can be different for each query $q$. To avoid that, one should replace the number of retrieved items $|R|$ in the denominator by the number of non-relevant retrieved items $|R|-|M|$. Hence, we correct the definition of \citet{bosteels_fuzzy_2007} and employ
\begin{equation*}
\tx{NAR}_q = \frac{100}{|M|\left(|R|-|M|\right)} \sum_{i=1}^{|M|} \Bigl( \tx{rank}(m_i,R)-i \Bigr) ,
\end{equation*}
which additionally yields a convenient \% value, now between 0 and 100 for all sizes of $M$. Our final number is the average over all queries $Q$:
\begin{equation*}
\tx{NAR} = \frac{1}{|Q|} \sum_{q\in Q} \tx{NAR}_q .
\end{equation*}
Note that, in research evaluation scenarios, one must compute both MAP and NAR measures excluding the query from the candidate list.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Results}
\label{sec:app_results}

%\subsection{Track-level Evaluation with Covers80}
%
%The Covers80 data set\footnote{\url{http://labrosa.ee.columbia.edu/projects/coversongs/covers80/}} is a test data set from 2007 that has been superseded by other data sets like DVI-Test or SHS-Test. Due to its small size (only 160~songs) and the partial overlap with other training data sets, it is not used anymore to report results for musical version matching. However, in Table~\ref{tab:covers80}, we decided to report results on it only informally.
%
%\todo{JOAN: I don't know what to do with this, if showing it or not.}
%
% \begin{table*}[h]
% \caption{Track-level evaluation on the Covers80 test set (informal) . The symbol $\dag$ denotes that it is our implementation. %Best results highlighted in bold.
% }
% \label{tab:covers80}
% \tablecaptionspace
% \begin{center}
% %\begin{small}
% \begin{adjustbox}{width=0.9\textwidth}
% \begin{sc}
% \begin{tabular}{llcclcc}
% \toprule
% Approach & & \multicolumn{2}{c}{Trained on DVI} & & \multicolumn{2}{c}{Trained on SHS} \\
%   \cline{3-4} \cline{6-7}
% & & NAR $\downarrow$ & MAP $\uparrow$ & & NAR $\downarrow$ & MAP $\uparrow$ \\
% \midrule
% CoverHunter-Coarse~\cite{liu_coverhunter_2023} & & 7.85 $\pm$ 2.37 & 0.558 $\pm$ 0.068 & & 5.80 $\pm$ 2.52 & 0.735 $\pm$ 0.061  \\
% CQTNet~\citep[as reported by][]{yu_learning_2020} & & n/a & n/a & & n/a & 0.840\hspace{1.3cm} \\
% CQTNet~\cite{yu_learning_2020} & & 3.33 $\pm$ 2.05 & 0.869 $\pm$ 0.048 & & 2.10 $\pm$ 1.14 & 0.861 $\pm$ 0.050 \\
% DVINet+~\cite{araz_discogs-vinet-mirex_2024} & & 0.88 $\pm$ 0.51 & 0.897 $\pm$ 0.043 & & 1.95 $\pm$ 1.35 & 0.889 $\pm$ 0.045 \\
% ByteCover3$\dag$~\citep[based on][]{du_bytecover3_2023} & & 1.13 $\pm$ 0.88 & 0.900 $\pm$ 0.043 & & 0.96 $\pm$ 0.75 & 0.916 $\pm$ 0.038 \\
% ByteCover3~\cite{du_bytecover3_2023}           & & n/a & n/a & & n/a & 0.927\hspace{1.3cm} \\
% ByteCover2~\cite{du_bytecover2_2022}           & & n/a & n/a & & n/a & 0.928\hspace{1.3cm} \\
% ByteCover1/2$\dag$~\citep[based on][]{du_bytecover2_2022} & & 1.46 $\pm$ 0.93 & 0.905 $\pm$ 0.042 & & 1.67 $\pm$ 1.18 & 0.929 $\pm$ 0.038 \\
% CLEWS (proposed)               & & 0.72 $\pm$ 0.63 & 0.955 $\pm$ 0.030 & & 1.31 $\pm$ 1.11 & 0.944 $\pm$ 0.034 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{adjustbox}
% %\end{small}
% \end{center}
% %\vskip -0.1in
% \end{table*}


\subsection{Segment-level Evaluation with the Best Match Protocol}

In the main manuscript, we present the results for the segment-level evaluation on DVI-Test (Fig.~\ref{fig:seg_dvi}). The exact numbers for such plots can be found here in Table~\ref{tab:seg_dvi}. For SHS-Test, we obtain comparable results, which can be found below in Fig.~\ref{fig:seg_shs} and Table~\ref{tab:seg_shs}.

\subsection{Segment-level Evaluation with the Random Segment Protocol}

In our segment-level evaluation, we adopt a best match protocol as specified in Sec.~\ref{sec:eval}. However, \citet{du_bytecover3_2023} introduced what could be termed as the `random segment' protocol: ``For each query, we constructed a query set consisting of the original full-track recording, and 9 music clips randomly cut from it, with the duration being 6, 10, 15, 20, 25, 30, 40, 50 and 60 seconds respectively''~\cite{du_bytecover3_2023}. Apart from lacking further specification, we claim that using random segments biases the evaluation, as we can never reach a perfect accuracy (a random segment from a song does not necessarily need to have a match in a version song). Furthermore, if the objective is to match tracks by their segments, we believe using random segments for evaluation may implicitly favor approaches exploiting more generic or global track characteristics than the specific matching-segment information. These are the reasons why we introduce our segment-based protocol. However, in the spirit of comparing with existing reported values, and to avoid any doubt on the performance of the proposed approach, we replicate such protocol (to our best) and compute again results for all methods considered here. They are shown in Fig.~\ref{fig:seg_shsrand} and Table~\ref{tab:seg_shsrand} below, together with the MAP values of ByteCover2, ByteCover3, and Re-MOVE~\cite{yesiler_less_2020} reported by \citet{du_bytecover3_2023}.

\subsection{Runtime}

To conclude, we also provide an informal runtime analysis for the considered models (Table~\ref{tab:runtime}). For a fair comparison, inference times are measured with the segment-based evaluation protocol, thus all models perform the same task of segment-based retrieval with a 5\,s hop size, using $\fu{R}_{\text{min}}$. We should also note that the time complexity of the na√Øve implementation of the reductions studied above is $O(uv)$ for $\fu{R}_{\text{mean}}$ and  $\fu{R}_{\text{min}}$, $O(uv+u)$ for $\fu{R}_{\text{meanmin}}$, $O(r+uv\log(uv))$ for $\fu{R}_{\text{best-r}}$, and $O(r(uv+u+v))$ for $\fu{R}_{\text{bpwr-r}}$, where $r\leq\min(u,v)$ and $u,v$ are the number of considered segments in a sub-rectangle (see main text). Note that the values for $r$, $u$, and $v$ are small for today's computation standards. For instance, a 5\,min song with 20\,s segments and no overlap yields $u=15$.

\vfill

\begin{table*}[h]
\caption{Segment-level evaluation with DVI-Test. NAR (top) and MAP (bottom) results for different lengths of query segments $\tau$. The $\pm$ symbol marks 95\% confidence intervals.}
\label{tab:seg_dvi}
\vskip 0.15in
%\begin{center}
%\begin{small}
\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
Approach & \multicolumn{7}{c}{$\tau$ [s]} \\
        \cline{2-8}
    & 5 & 10 & 20 & 30 & 40 & 60 & 90 \\
\midrule
CoverHunter-Coarse & 14.97 $\pm$ 0.07 & 12.01 $\pm$ 0.07 & 11.01 $\pm$ 0.07 & 10.89 $\pm$ 0.07 & 10.88 $\pm$ 0.07 & 10.95 $\pm$ 0.07 & 11.07 $\pm$ 0.07 \\
CQTNet & 49.96 $\pm$ 0.10 & 48.48 $\pm$ 0.10 & 16.35 $\pm$ 0.08 & 8.67 $\pm$ 0.07 & 7.20 $\pm$ 0.07 & 6.65 $\pm$ 0.07 & 6.60 $\pm$ 0.07 \\
DVINet+ & 49.80 $\pm$ 0.13 & 42.11 $\pm$ 0.12 & 10.42 $\pm$ 0.07 & 5.23 $\pm$ 0.06 & 4.20 $\pm$ 0.06 & 3.84 $\pm$ 0.06 & 3.76 $\pm$ 0.06 \\
ByteCover1/2 $\dag$ & 29.91 $\pm$ 0.10 & 13.50 $\pm$ 0.08 & 6.77 $\pm$ 0.06 & 5.75 $\pm$ 0.06 & 5.42 $\pm$ 0.06 & 5.19 $\pm$ 0.06 & 5.09 $\pm$ 0.06 \\
ByteCover3 $\dag$ & 30.11 $\pm$ 0.10 & 18.45 $\pm$ 0.08 & 8.66 $\pm$ 0.06 & 6.62 $\pm$ 0.06 & 6.18 $\pm$ 0.06 & 6.42 $\pm$ 0.06 & 7.06 $\pm$ 0.06 \\
CLEWS (ours) & 5.10 $\pm$ 0.06 & 3.02 $\pm$ 0.05 & 2.86 $\pm$ 0.05 & 2.84 $\pm$ 0.05 & 2.85 $\pm$ 0.05 & 2.87 $\pm$ 0.05 & 2.89 $\pm$ 0.05 \\
\midrule
CoverHunter-Coarse & 0.060 $\pm$ 0.001 & 0.106 $\pm$ 0.001 & 0.132 $\pm$ 0.001 & 0.133 $\pm$ 0.001 & 0.132 $\pm$ 0.001 & 0.129 $\pm$ 0.001 & 0.127 $\pm$ 0.001 \\
CQTNet & 0.001 $\pm$ 0.000 & 0.001 $\pm$ 0.000 & 0.011 $\pm$ 0.000 & 0.078 $\pm$ 0.001 & 0.282 $\pm$ 0.002 & 0.426 $\pm$ 0.002 & 0.475 $\pm$ 0.002 \\
DVINet+ & 0.001 $\pm$ 0.000 & 0.002 $\pm$ 0.000 & 0.026 $\pm$ 0.000 & 0.171 $\pm$ 0.001 & 0.421 $\pm$ 0.002 & 0.561 $\pm$ 0.002 & 0.616 $\pm$ 0.002 \\
ByteCover1/2 $\dag$ & 0.008 $\pm$ 0.000 & 0.083 $\pm$ 0.001 & 0.363 $\pm$ 0.002 & 0.483 $\pm$ 0.002 & 0.529 $\pm$ 0.002 & 0.564 $\pm$ 0.002 & 0.582 $\pm$ 0.002 \\
ByteCover3 $\dag$ & 0.001 $\pm$ 0.000 & 0.062 $\pm$ 0.001 & 0.358 $\pm$ 0.002 & 0.452 $\pm$ 0.002 & 0.508 $\pm$ 0.002 & 0.503 $\pm$ 0.002 & 0.473 $\pm$ 0.002 \\
CLEWS (ours) & 0.271 $\pm$ 0.002 & 0.670 $\pm$ 0.002 & 0.754 $\pm$ 0.002 & 0.756 $\pm$ 0.002 & 0.755 $\pm$ 0.002 & 0.747 $\pm$ 0.002 & 0.738 $\pm$ 0.002 \\
\bottomrule
\end{tabular}
\end{sc}
\end{adjustbox}
%\end{small}
%\end{center}
%\vskip -0.1in
\end{table*}

\vfill

\begin{figure}[h]
%\vskip 0.2in
%\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig_segment_shs-full}}
\vskip -0.15in
\caption{Segment-level evaluation with SHS-Test. NAR (left) and MAP (right) for different lengths of query segments $\tau$ (notice the logarithmic axis for NAR). The shaded regions correspond to 95\% confidence intervals.}
\label{fig:seg_shs}
%\end{center}
%\vskip -0.2in
\end{figure}


\begin{table*}[h]
\caption{Segment-level evaluation with SHS-Test. NAR (top) and MAP (bottom) results for different lengths of query segments $\tau$. The $\pm$ symbol marks 95\% confidence intervals.}
\label{tab:seg_shs}
\vskip 0.15in
%\begin{center}
%\begin{small}
\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
Approach & \multicolumn{7}{c}{$\tau$ [s]} \\
        \cline{2-8}
    & 5 & 10 & 20 & 30 & 40 & 60 & 90 \\
\midrule
CoverHunter-Coarse & 10.78 $\pm$ 0.23 & 6.61 $\pm$ 0.20 & 4.90 $\pm$ 0.18 & 4.62 $\pm$ 0.18 & 4.54 $\pm$ 0.18 & 4.52 $\pm$ 0.18 & 4.56 $\pm$ 0.18 \\
CQTNet & 49.80 $\pm$ 0.48 & 44.98 $\pm$ 0.47 & 9.45 $\pm$ 0.25 & 4.32 $\pm$ 0.18 & 3.21 $\pm$ 0.16 & 2.74 $\pm$ 0.16 & 2.67 $\pm$ 0.16 \\
DVINet+ & 49.44 $\pm$ 0.52 & 43.12 $\pm$ 0.52 & 16.05 $\pm$ 0.36 & 5.45 $\pm$ 0.22 & 3.29 $\pm$ 0.18 & 2.62 $\pm$ 0.17 & 2.52 $\pm$ 0.17 \\
ByteCover1/2 $\dag$ & 22.68 $\pm$ 0.42 & 6.53 $\pm$ 0.22 & 2.71 $\pm$ 0.16 & 2.28 $\pm$ 0.15 & 2.12 $\pm$ 0.15 & 2.07 $\pm$ 0.15 & 2.03 $\pm$ 0.14 \\
ByteCover3 $\dag$ & 15.78 $\pm$ 0.32 & 10.73 $\pm$ 0.20 & 3.84 $\pm$ 0.16 & 2.97 $\pm$ 0.15 & 2.71 $\pm$ 0.15 & 2.68 $\pm$ 0.15 & 4.53 $\pm$ 0.19 \\
CLEWS (ours) & 3.39 $\pm$ 0.17 & 1.49 $\pm$ 0.13 & 1.33 $\pm$ 0.12 & 1.35 $\pm$ 0.12 & 1.37 $\pm$ 0.12 & 1.42 $\pm$ 0.12 & 1.47 $\pm$ 0.13 \\
\midrule
CoverHunter-Coarse & 0.099 $\pm$ 0.004 & 0.274 $\pm$ 0.007 & 0.414 $\pm$ 0.007 & 0.435 $\pm$ 0.007 & 0.440 $\pm$ 0.007 & 0.439 $\pm$ 0.007 & 0.435 $\pm$ 0.007 \\
CQTNet & 0.003 $\pm$ 0.000 & 0.003 $\pm$ 0.000 & 0.038 $\pm$ 0.001 & 0.095 $\pm$ 0.003 & 0.361 $\pm$ 0.007 & 0.603 $\pm$ 0.007 & 0.652 $\pm$ 0.007 \\
DVINet+ & 0.003 $\pm$ 0.000 & 0.005 $\pm$ 0.000 & 0.028 $\pm$ 0.001 & 0.093 $\pm$ 0.003 & 0.365 $\pm$ 0.007 & 0.630 $\pm$ 0.007 & 0.691 $\pm$ 0.007 \\
ByteCover1/2 $\dag$ & 0.033 $\pm$ 0.002 & 0.250 $\pm$ 0.006 & 0.640 $\pm$ 0.007 & 0.738 $\pm$ 0.007 & 0.770 $\pm$ 0.006 & 0.790 $\pm$ 0.006 & 0.800 $\pm$ 0.006 \\
ByteCover3 $\dag$ & 0.098 $\pm$ 0.004 & 0.237 $\pm$ 0.007 & 0.628 $\pm$ 0.008 & 0.687 $\pm$ 0.008 & 0.690 $\pm$ 0.007 & 0.674 $\pm$ 0.007 & 0.576 $\pm$ 0.008 \\
CLEWS (ours) & 0.394 $\pm$ 0.007 & 0.806 $\pm$ 0.006 & 0.859 $\pm$ 0.005 & 0.861 $\pm$ 0.005 & 0.859 $\pm$ 0.005 & 0.852 $\pm$ 0.006 & 0.847 $\pm$ 0.006 \\
\bottomrule
\end{tabular}
\end{sc}
\end{adjustbox}
%\end{small}
%\end{center}
%\vskip -0.1in
\end{table*}


\begin{figure}[h]
%\vskip 0.2in
%\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig_segment_shs-full-rand}}
\vskip -0.15in
\caption{Segment-level evaluation with SHS-Test using the random segment protocol of~\citet{du_bytecover3_2023}. NAR (left) and MAP (right) for different lengths of random query segments $\tau$ (notice the logarithmic axis for NAR). The shaded regions correspond to 95\% confidence intervals. The dotted lines correspond to values reported by~\citet{du_bytecover3_2023}.}
\label{fig:seg_shsrand}
%\end{center}
%\vskip -0.2in
\end{figure}

%\clearpage

\begin{table*}[h]
\caption{Segment-level evaluation with SHS-Test using the random segment protocol of~\citet{du_bytecover3_2023}. NAR (top) and MAP (bottom) results for different lengths of random query segments $\tau$. The $\pm$ symbol marks 95\% confidence intervals. \citet{du_bytecover3_2023} did not report any confidence interval.}
\label{tab:seg_shsrand}
\vskip 0.15in
%\begin{center}
%\begin{small}
\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{lccccccc}
\toprule
Approach & \multicolumn{7}{c}{$\tau$ [s]} \\
        \cline{2-8}
    & 5 & 10 & 20 & 30 & 40 & 60 & 90 \\
\midrule
CoverHunter-Coarse & 15.15 $\pm$ 0.30 & 9.97 $\pm$ 0.25 & 7.42 $\pm$ 0.22 & 6.56 $\pm$ 0.21 & 6.03 $\pm$ 0.21 & 5.62 $\pm$ 0.20 & 5.27 $\pm$ 0.19 \\
CQTNet & 49.73 $\pm$ 0.48 & 46.67 $\pm$ 0.47 & 18.55 $\pm$ 0.32 & 10.21 $\pm$ 0.25 & 7.18 $\pm$ 0.23 & 4.88 $\pm$ 0.20 & 3.78 $\pm$ 0.18 \\
DVINet+ & 49.50 $\pm$ 0.52 & 44.13 $\pm$ 0.51 & 22.65 $\pm$ 0.38 & 11.41 $\pm$ 0.28 & 7.09 $\pm$ 0.24 & 4.57 $\pm$ 0.20 & 3.50 $\pm$ 0.19 \\
ByteCover1/2 $\dag$ & 31.96 $\pm$ 0.40 & 19.16 $\pm$ 0.34 & 9.65 $\pm$ 0.26 & 6.55 $\pm$ 0.22 & 5.04 $\pm$ 0.21 & 3.77 $\pm$ 0.19 & 3.02 $\pm$ 0.17 \\
ByteCover3 $\dag$ & 23.41 $\pm$ 0.37 & 10.95 $\pm$ 0.28 & 6.64 $\pm$ 0.23 & 5.23 $\pm$ 0.21 & 4.68 $\pm$ 0.20 & 4.03 $\pm$ 0.19 & 5.52 $\pm$ 0.12 \\
CLEWS (ours) & 15.27 $\pm$ 0.30 & 8.09 $\pm$ 0.25 & 4.46 $\pm$ 0.19 & 3.30 $\pm$ 0.17 & 2.81 $\pm$ 0.16 & 2.32 $\pm$ 0.15 & 2.07 $\pm$ 0.15 \\
\midrule
CoverHunter-Coarse & 0.068 $\pm$ 0.003 & 0.193 $\pm$ 0.005 & 0.314 $\pm$ 0.006 & 0.354 $\pm$ 0.007 & 0.370 $\pm$ 0.007 & 0.385 $\pm$ 0.007 & 0.399 $\pm$ 0.007 \\
CQTNet & 0.003 $\pm$ 0.000 & 0.003 $\pm$ 0.000 & 0.019 $\pm$ 0.001 & 0.088 $\pm$ 0.003 & 0.276 $\pm$ 0.006 & 0.474 $\pm$ 0.007 & 0.586 $\pm$ 0.007 \\
DVINet+ & 0.003 $\pm$ 0.000 & 0.004 $\pm$ 0.000 & 0.016 $\pm$ 0.001 & 0.089 $\pm$ 0.003 & 0.276 $\pm$ 0.006 & 0.498 $\pm$ 0.007 & 0.623 $\pm$ 0.007 \\
ByteCover1/2 $\dag$ & 0.022 $\pm$ 0.001 & 0.110 $\pm$ 0.004 & 0.357 $\pm$ 0.006 & 0.516 $\pm$ 0.007 & 0.607 $\pm$ 0.007 & 0.691 $\pm$ 0.007 & 0.746 $\pm$ 0.007 \\
ByteCover3 $\dag$ & 0.044 $\pm$ 0.002 & 0.133 $\pm$ 0.004 & 0.432 $\pm$ 0.007 & 0.511 $\pm$ 0.007 & 0.528 $\pm$ 0.007 & 0.539 $\pm$ 0.007 & 0.469 $\pm$ 0.008 \\
Re-MOVE & 0.023 & 0.069 & 0.196 & 0.308 & 0.407 & 0.505 & n/a \\
ByteCover3 & 0.084 & 0.257 & 0.496 & 0.600 & 0.666 & 0.732 & n/a \\
ByteCover2 & 0.016 & 0.074 & 0.282 & 0.442 & 0.564 & 0.684 & n/a \\
CLEWS (ours) & 0.140 $\pm$ 0.004 & 0.455 $\pm$ 0.006 & 0.652 $\pm$ 0.007 & 0.714 $\pm$ 0.006 & 0.746 $\pm$ 0.006 & 0.780 $\pm$ 0.006 & 0.802 $\pm$ 0.006 \\
\bottomrule
\end{tabular}
\end{sc}
\end{adjustbox}
%\end{small}
%\end{center}
%\vskip -0.1in
\end{table*}


\begin{table}[h]
\caption{Training and inference runtimes (informal) using a single NVIDIA H100 GPU. Training runtime is measured using a batch construction as specified in the main text (2.5\,min audio blocks, 25~anchors, 3~positives per anchor, total of 100 audio blocks). Inference runtime is measured following the segment-based evaluation protocol (5\,s hop size, $\tau=$20\,s, $\fu{R}=\fu{R}_\tx{min}$). Retrieval time corresponds to a database with 2000~candidates, and grows linearly with them for all approaches.}
\label{tab:runtime}
\vskip 0.15in
\begin{center}
%\begin{small}
\begin{adjustbox}{max width=0.9\textwidth}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Approach & Parameters & Training [s/batch] & \multicolumn{2}{c}{Inference} \\
        \cline{4-5}
         &         &  & Embedding [ms/song]  & Retrieval [ms/query] \\
\midrule
CoverHunter-Coarse & ~~28\,M & 0.68  & 22.5 & 2.9 \\
CQTNet             & ~~35\,M & 0.48  & 57.6 & 2.0 \\
DVINet+            & ~~11\,M & 0.38  & 60.9 & 2.2 \\
ByteCover3 $\dag$  & 969\,M  & 0.71  & 27.4 & 5.1 \\
ByteCover1/2 $\dag$ & 202\,M & 0.50  & 62.2 & 5.1 \\
CLEWS              & 199\,M  & 1.19  & 40.3 & 3.5 \\
\bottomrule
\end{tabular}
\end{sc}
\end{adjustbox}
%\end{small}
\end{center}
%\vskip -0.1in
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
