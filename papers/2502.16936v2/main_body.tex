%\medskip

\begin{abstract}
Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g.,~whole song). However, most applications require to match them at the segment level (e.g.,~20\,s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

When two audio tracks contain different renditions of the same musical piece, they are considered musical versions\footnotemark{}. %This concept arises naturally in human culture, even before the advent of recorded music or musical notation, as music transmission in ancient times was only possible through playing and listening~\cite{ball_music_2010}, which organically introduces variations to tunes, rhythms, structure, etc. Nowadays, musical versions represent a fundamental part of artistic expression and creativity, either as a tribute to the original author or piece, or as a radical reinterpretation of an existing idea. %(possibly evolving from yet another reinterpretation of the original).
Musical versions are inherent in human culture and predate recorded music and notation, as ancient music was transmitted solely through playing and listening~\cite{ball_music_2010}, which naturally led to variations in tunes, rhythms, structures, etc. 
Learning representations of musical versions is a challenging task due to the degree and amount of variations that can be present between versions, which go beyond typical augmentations used by the machine learning community.\footnotetext{A related but more restrictive and biased term in the literature is ``cover songs''. To better understand this restriction and bias, see the discussion found in, for example, \citet{yesiler_audio-based_2021}.} Two musical versions may feature different instrumentation or timbre, together with tonality and chord modifications, altered melodies, substantial changes to rhythm and tempo, an alternate temporal development or structure, and many more~\cite{yesiler_audio-based_2021}. Yet, musical versions retain their essence, to the point that we can generally agree whether two of them correspond to the same piece or not\footnote{A well-known source collecting this information is \url{https://secondhandsongs.com}.}. %Therefore, learnt version representations need to encapsulate multiple `essential' characteristics of a musical piece, and those characteristics should match with the ones of the other versions, but not with the ones of an unrelated piece. 
Therefore, learnt version representations need to encapsulate multiple characteristics shared between versions that, at the same time, can discriminate them from other pieces.

Musical version matching has several relevant applications~\cite{serra_identification_2011, yesiler_audio-based_2021}, %related to music retrieval, analysis, and recommendation,
including specific applications to plagiarism and near-duplicate detection\footnote{Note that musical version matching may expand and subsume traditional music fingerprinting~\citep{cano_review_2005}.}. %(the global value of music copyright alone is estimated around \$45.5~billions\footnote{\url{https://pivotaleconomics.com/undercurrents/music-copyright-2023}}).  
Beyond business impact~\cite{page_music_2023} and cultural/artistic appreciation, some applications have become even more relevant today, given the sustained rise and improvement of music generative models~\citep[e.g.,][]{copet_simple_2023, evans_fast_2024, liu_audioldm_2024}. Indeed, the recent efforts on assessing music data replication, memorization, and attribution in such models exploit some form of music similarity~\cite{barnett_exploring_2024, bralios_generation_2024} or, for improved results, musical version matching~\cite{batlle-roca_towards_2024}. 

\begin{table*}[t]
\caption{Comparison of characteristics for a number of existing approaches and the proposed method CLEWS. We exclude multi-feature and/or multi-modal approaches (for example fusing CQT and melody estimations or leveraging audio and lyrics information). For further details and approaches we refer to the survey by \citet{yesiler_audio-based_2021}.}
\label{tab:comparison}
\tablecaptionspace
%\begin{center}
%\begin{small}
\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{llcccccc}
\toprule
Name(s)         & Main      & Input & Arch. & Segment & Partial & Loss / train  & Retrieval \\
                & reference &       &       & learning & match   & concept       & distance \\
\midrule
CQTNet          & \citet{yu_learning_2020}      & CQT   & ConvNet    & \xmark      & \xmark   & Classif.        & Cosine \\
Doras\&Peeters  & \citet{doras_prototypical_2020} & HCQT & ConvNet    & \xmark      & \xmark   & Triplet         & Cosine \\
MOVE/Re-MOVE    & \citet{yesiler_accurate_2020} & CREMA & ConvNet    & \xmark      & \xmark   & Triplet         & Euclidean \\
PiCKINet        & \citet{ohanlon_detecting_2021} & CQT  & ConvNet    & \xmark      & \xmark   & Classif.+Center    & Cosine \\
LyraCNet        & \citet{hu_wideresnet_2022}    & CQT   & WideResNet & \xmark      & \xmark   & Classif.        & Cosine \\
ByteCover1/2    & \citet{du_bytecover2_2022}    & CQT   & ResNet     & \xmark      & \xmark   & Classif.+Triplet   & Cosine \\
CoverHunter     & \citet{liu_coverhunter_2023}  & CQT   & Conformer  & \xmark      & \cmark   & Classif.+Focal+Center  & Cosine \\
ByteCover3/3.5  & \citet{du_bytecover3_2023}    & CQT   & ResNet     & \cmark      & \xmark   & Classif.+Triplet   & Cosine \\
DVINet/DVINet+  & \citet{araz_discogs-vi_2024}  & CQT   & ConvNet    & \xmark      & \xmark   & Triplet         & Cosine \\
\midrule
CLEWS (proposed) & This paper                    & CQT   & ResNet     & \cmark      & \cmark   & Contrastive     & Euclidean \\
\bottomrule
\end{tabular}
\end{sc}
\end{adjustbox}
%\end{small}
%\end{center}
%\vskip -0.1in
\end{table*}

A fundamental limitation of version matching approaches is that they operate at the full-track level, learning and extracting individual representations from relatively long recordings (for instance, a few-minute song). This is due to ground truth version annotations being only available per track. However, the segments of interest, for both classical and modern applications, are much shorter than the track length (for instance, around 10--20\,s). This mismatch between the learning and inference stages, as we will see, causes a dramatic performance degradation (Sec.~\ref{sec:results}). Another challenge is that, in contrast to standard supervised learning tasks, musical version data sets contain only a few items per class. For instance, up to 56\% of a recent realistic large-scale data set of around 500\,k tracks is formed by only 2-item classes, with an average of 5 items per class~\cite{araz_discogs-vi_2024}. This characteristic suggests that, besides the traditional focus on classification and triplet losses, a supervised contrastive learning approach (Sec.~\ref{sec:background}) could also work well.

In this paper, we consider a full music track as a succession of weakly-labeled audio segments, and learn a contrastive representation using such weak supervision. To do so, we introduce two main methods. First, we develop a number of pairwise distance selection strategies, which reduce a segment-level distance matrix into a track-level distance matrix. This enables the direct utilization of track-level annotations without statically assigning them to some or all of the segments. Second, we reformulate the alignment and uniformity (A\&U) loss of \citet{wang_understanding_2020}, originally introduced for self-supervised learning, to operate on a (weakly) supervised learning task. Motivated by decoupling, hyper-parameter, and geometric considerations, we introduce several changes that convert A\&U into a new loss function: the strict decoupling of positives and negatives~\cite{yeh_decoupled_2022}, the simplification of hyper-parameters, the native operation in Euclidean geometry~\citep[cf.][]{koishekenov_geometric_2023}, and a smoothing constant for negative pairs. With both distance reduction and contrastive learning strategies, we do not only outperform existing approaches in the segment-level evaluation by a large margin, but we also achieve state-of-the-art results in the standard track-level evaluation. We also perform an extensive ablation study to empirically compare the proposed methods with several alternatives, including additional reduction strategies and common contrastive losses. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in further domains beyond musical version matching. To facilitate understanding and reproduction, we share our code and model checkpoints in \url{https://github.com/sony/clews}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
\label{sec:background}

After a history of rule-, feature-, and model-based approaches~\cite{serra_identification_2011, yesiler_audio-based_2021}, musical version matching is currently tackled as a supervised learning problem, focusing on full-track pairwise matching (Table~\ref{tab:comparison}). However, two versions do not necessarily need to match for their entire duration, and actually several applications rely on few-second partial matches. Only a couple of approaches base their learning or retrieval stages on segments or partial matches, respectively. ByteCover3~\cite{du_bytecover3_2023} pioneered learning from segments with their ``maxmean'' operator. However, such operator still does not allow for partial matches, as it forces all segments of a track to match some segment from another track. %ByteCover3.5~\cite{du_x-cover_2024} continued with the same approach. 
CoverHunter~\cite{liu_coverhunter_2023} is able to detect partial matches of around 45\,s. However, the learning strategy to do so is based on a two-stage brute-force approach. First, it trains a coarse detector model on 15-second segments using classification, focal, and center losses. Then, it resorts to this first-stage model and a rule-based approach to (weakly) label 45-second segments, which are finally used to train the second-stage model with the same losses. In both stages, CoverHunter treats segments as full tracks. To our knowledge, we are the first to consider an entirely segment-based approach for both learning and retrieval stages.

The literature on musical version matching has traditionally considered a number of classification~\cite{sun_deep_2014} and triplet~\cite{schroff_facenet_2015} loss variants, and their combination (Table~\ref{tab:comparison}). However, given the same ground truth, another approach to learning version representations would be to consider a supervised contrastive loss like N-pairs~\cite{sohn_improved_2016} or SupCon~\cite{khosla_supervised_2021}. In addition, a number of well-established losses for self-supervised learning like InfoNCE/NT-Xent~\cite{van_den_oord_representation_2018, chen_simple_2020}, alignment and uniformity~\cite{wang_understanding_2020}, or SigLIP~\cite{zhai_sigmoid_2023} could also be adapted. An analysis of the relations between many of such losses is carried out by \citet{koromilas_bridging_2024}. Apart from the loss function, other considerations such as positive/negative decoupling~\cite{yeh_decoupled_2022} and the correspondence between distance and space geometry~\cite{koishekenov_geometric_2023} are potentially relevant in a practical case. To our knowledge, we are the first to consider a supervised contrastive loss for musical version matching. %(and achieve state-of-the-art results with it).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Contrastive Learning from Weakly-Labeled Audio Segments}

We now detail our approach to perform \underline{c}ontrastive \underline{le}arning from \underline{w}eakly-labeled audio \underline{s}egments (CLEWS). The first part deals with track-level labels and their allocation to segment distances (we base our development on distances, but it can be easily reformulated using similarities). The second part details the contrastive loss function we use. The third part explains our architecture and training procedure.

\subsection{Segment Distance Reduction}
\label{sec:method_segment}

\topic{Framework} 
Given the $k$-th waveform segment of the $i$-th music track, $\ve{x}^k_i$, we compute latent representations $\ve{z}^k_i=\fu{F}(\ve{x}^k_i)$, where $\fu{F}$ represents a neural network that pools the time-varying information of the segment into a single vector (architecture details can be found in Sec.~\ref{sec:method_model} and Appendix~\ref{sec:app_method}). Then, for every possible pair of segments $k$ and $l$ of every possible pair of tracks $i$ and $j$, we compute their distance $\tilde{d}^{kl}_{ij}$ and obtain the distance matrix $\tilde{\ma{D}}$ (Fig.~\ref{fig:dists}). At this point, if there are $n$ query tracks and $m$ candidate tracks with $u$ and $v$ segments\footnote{Notice that, similar to cross-attention in Transformers, we can deal with different track lengths by taking a maximum length $u,v$ and masking. See Fig.~\ref{fig:dists} (right) for a couple of examples.}, respectively, we have $\tilde{\ma{D}} \in \mathbb{R}_{\ge 0}^{nu\times mv}$. However, since labels are only provided at the track level, our binary ground truth assignments (1 for version/positive and 0 for non-version/negative) are $\ma{A} \in \mathbb{Z}_2^{n\times m}$. Therefore, we need some strategy to (weakly) allocate $n\times m$ labels to $nu\times mv$ segment distances.

\begin{figure}[t]
%\vskip 0.01in
%\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig_dists}}
\figurecaptionspace
\vspace{0.25cm}
\caption{
Illustration of four reduction functions $\fu{R}$ over pairwise segment distances $\tilde{d}^{kl}_{ij}$. They are depicted on different sub-rectangles $\tilde{\ma{D}}_{ij}$, where tracks $i$, $j$, and $j+1$ are versions (green squares) and track $i+1$ is not (orange squares). The four functions correspond to: 
%Schema of a pairwise segment distance matrix $\tilde{\ma{D}}$, with cells $\tilde{d}^{kl}_{ij}$ and sub-rectangles $\tilde{\ma{D}}_{ij}$, and illustration of different reduction functions $\fu{R}$. Tracks $i$, $j$, and $j+1$ are versions (green squares) and track $i+1$ is not (orange squares). Four possible reduction functions are depicted: 
$\fu{R}_\tx{meanmin}$ (top left), $\fu{R}_\tx{bpwr-3}$ (top right), $\fu{R}_\tx{best-10}$ (bottom left), and $\fu{R}_\tx{min}$ (bottom right). The $\fu{R}_\tx{bpwr-3}$ strategy depicts its minimum/masking recursion in increasingly dark levels (green/purple cells). The sub-rectangles for  $\fu{R}_\tx{bpwr-3}$ and $\fu{R}_\tx{min}$ also exemplify dealing with different lengths by masking (gray cells).
}
\label{fig:dists}
%\end{center}
%\vskip -0.2in
\end{figure}

A naïve strategy to do such allocation would be to propagate all positive/negative track assignments to all segment comparisons in the sub-rectangle $\tilde{\ma{D}}_{ij}$ defined by a pair of tracks $i$ and $j$. This is the approach implicitly followed by CoverHunter~\cite{liu_coverhunter_2023}. However, besides its poor performance (Sec.~\ref{sec:results}), this strategy incurs a fundamental error, in the sense that it is teaching the model that all positive segments are `similar' to all other positive segments, which in the case of musical versions is false (two segments could reproduce two different motives of the same song; even though it is the same song, the segments are usually not the same, unless it is an extremely repetitive song). Instead of trying to allocate $n\times m$ positive/negative distances to $nu\times mv$ segment distances, we take the opposite view and reduce $nu\times mv$ segment distances to $n\times m$ track distances, matching the dimensions of positive/negative assignments~$\ma{A}$. More specifically, we consider several reduction functions~$\fu{R}$, producing $\ma{D} = \fu{R}(\tilde{\ma{D}})$, where $\ma{D} \in \mathbb{R}_{\ge 0}^{n\times m}$. In addition, we employ different reduction functions for positive and negative pairs, $\fu{R}^+$ and $\fu{R}^-$, respectively.

\topic{Reduction Functions}
The naïve strategy outlined above would correspond to a mean reduction over the entire sub-rectangle determined by $\tilde{\ma{D}}_{ij}$:
\begin{equation*}
d_{ij} = \fu{R}_\tx{mean}\left(\tilde{\ma{D}}_{ij}\right) = \frac{1}{uv} \sum_{\substack{1\leq k\leq u\\1\leq l\leq v}} \tilde{d}^{kl}_{ij} .
\end{equation*}
Instead, if we just consider the $r$ best matches across segments, we have
\begin{equation*}
d_{ij} = \fu{R}_\tx{best-r}\left(\tilde{\ma{D}}_{ij}\right) = \frac{1}{r} \sum_{1\leq t\leq r} \text{topr}\!\left(\tilde{\ma{D}}_{ij}\right)_t
\end{equation*}
for $r\leq uv$, where $\text{topr}(\ma{D})$ is a function that returns the lowest $r$ distances in $\ma{D}$ (Fig.~\ref{fig:dists}, bottom left). Another possibility is to consider just the single best correspondence in the entire sub-rectangle (Fig.~\ref{fig:dists}, bottom right), yielding
\begin{equation*}
d_{ij} = \fu{R}_\tx{min}\left(\tilde{\ma{D}}_{ij}\right) = \min_{\substack{1\leq k\leq u\\1\leq l\leq v}} \tilde{d}^{kl}_{ij} .
\end{equation*}
A further alternative is to use an operator like the one used in ByteCover3~\cite{du_bytecover3_2023}, which first searches for the candidate best match per query and then averages across all query segments. Reformulating it for distances (Fig.~\ref{fig:dists}, top left), we obtain 
\begin{equation*}
d_{ij} = \fu{R}_\tx{meanmin}\left(\tilde{\ma{D}}_{ij}\right) = \frac{1}{u} \sum_{1\leq k\leq u} \min_{1\leq l\leq v} \tilde{d}^{kl}_{ij} .
\end{equation*}

Notice that the $\fu{R}_\tx{mean}$, $\fu{R}_\tx{best-r}$, and $\fu{R}_\tx{meanmin}$ strategies above do not explicitly prevent multiple consecutive segments of track $i$ being assigned to the same segment of track $j$ (vertical or horizontal traces in Fig.~\ref{fig:dists}). As mentioned, this is unrealistic for the majority of music tracks (and in general for any signal or sequence presenting a minimal variability with time). Notice also that $\fu{R}_\tx{mean}$ and $\fu{R}_\tx{meanmin}$ force a full-track match, that is, they are teaching the model that all segments in track $i$ should find a match in track $j$. Again, this is an unrealistic assumption for musical versions where the structure changes (and in general for any signal or sequence featuring only partial matches). 

\topic{Best-pair Reduction}
Motivated by the issues of consecutive and global segment matching above, we decide to design an additional reduction strategy that explicitly deals with both. We term it bpwr-r, for `best pair without replacement' with a threshold $r$. In a nutshell, $\fu{R}_\tx{bpwr-r}$ operates by sorting all distances in the $\tilde{\ma{D}}_{ij}$ sub-rectangle in increasing order, then taking the first one (say the one involving segments $k$ and $l$), removing all distances computed by using either one or the other segment (either $k$ or $l$), and iterating $r$ times. It then takes the average among those $r$ best pairwise distances. More formally, we can express it as
\begin{equation}
d_{ij} = \fu{R}_\tx{bpwr-r}\left(\tilde{\ma{D}}_{ij}\right) = 
%\fu{R}_\tx{best-r}\left( \tx{mask}_r\left( \tilde{\ma{D}}_{ij} \right) \right) ,
%\frac{1}{r} \sum_{1\leq q\leq r} \tx{minmask}_r\left( \tilde{\ma{D}}_{ij} \right) ,
\frac{1}{r} \sum_{1\leq q\leq r} \fu{R}_\tx{min}\left( \tilde{\ma{D}}^{(q)}_{ij} \right) 
\label{eq:bpwr}
\end{equation}
for $r\leq\min(u,v)$, with the recursion
\begin{equation*}
\tilde{\ma{D}}^{(q)}_{ij} = 
\begin{cases}
\tilde{\ma{D}}_{ij}  & \tx{for}~q=1, \\
\tx{maskmin}\left(\tilde{\ma{D}}^{(q-1)}_{ij}\right)  & \tx{for}~q>1,
\end{cases}
\end{equation*}
where $\tx{maskmin}(\ma{D})$ is a function that masks the row and the column corresponding to the minimum element in $\ma{D}$, %(for instance, with an infinite value) 
such that those elements are not eligible by the $\fu{R}_\tx{min}$ of Eq.~\ref{eq:bpwr} in iterations $q>1$. A schema of $\fu{R}_\tx{bpwr-3}$ is illustrated in Fig.~\ref{fig:dists} (top right; recursion is depicted by progressively darker colors). Notice that masking rows and columns avoids the issue of consecutive segment matching\footnote{There are some specific cases where a pattern could occupy two consecutive segments (e.g.,~when it is split between them or when it is reproduced at half the speed). We claim that, in such cases, learning from just one of the two consecutive segments is enough (our results in Sec.~\ref{sec:results} support this claim).}, and that a threshold $r<\min(u,v)$ avoids the issue of full-track matching, which only happens when $r=\min(u,v)$.

\topic{Positives and Negatives Reduction}
We note that the previous distance reduction strategies have some conceptual parallelism with the negative/positive mining strategies used with triplet losses~\cite{schroff_facenet_2015} or in some contrastive approaches~\citep[cf.][]{kalantidis_hard_2020}. In our case, for instance, $\fu{R}_\tx{min}$ could correspond to a hard mining strategy, while $\fu{R}_\tx{best-r}$ or $\fu{R}_\tx{bpwr-r}$ could be regarded as semi-hard mining of segment pairs. Thus, inspired by those strategies, we decide to study if applying different reduction strategies for positives and negatives has some effect in our setup. To obtain a track-based pairwise distance matrix that combines different reductions for positive and negative pairs, we calculate
\begin{equation}
\ma{D} = \ma{A}\odot \fu{R}^+\!\!\left( \tilde{\ma{D}} \right) + \left(\ma{1} - \ma{A}\right)\odot \fu{R}^-\!\!\left( \tilde{\ma{D}} \right) ,
\label{eq:distreduced}
\end{equation}
where $\odot$ denotes element-wise multiplication and $\ma{1}$ is the all-ones matrix (recall that the elements in $\ma{A}$ are 1 for positives and 0 otherwise). The reductions $\fu{R}^+$ and $\fu{R}^-$ can be chosen among the ones presented above.

 
\subsection{Contrastive Loss}
\label{sec:method_loss}

\topic{Motivation}
After computing pairwise track-level distances $\ma{D}$, we need a contrastive loss that can exploit them and that, ideally, can outperform the existing losses in the considered task. For that, one can consider any supervised contrastive loss function that operates on distances, or adapt an existing self-supervised loss to the supervised framework (Sec.~\ref{sec:background}). In our case, we opt for the latter and choose the A\&U loss of~\citet{wang_understanding_2020} due to its appealing properties and intuitive derivation. One of the practical properties we value is that, by using expectations, we have a similar behavior for different batch sizes (\citealt{wang_understanding_2020}; see also~\citealt{koromilas_bridging_2024}). In our analysis, we will use the concept of ``potential'' as introduced by \citet{wang_understanding_2020}, but nonetheless will depart from the concept of uniformity in the hypersphere.

\topic{Changes to Alignment and Uniformity}
The A\&U loss, designed for self-supervised contrastive learning, expects one positive for each item in the batch (obtained through some augmentation) while negatives correspond to all other elements in the batch. %(uniformly sampled from all available data). 
To adapt A\&U to supervised contrastive learning with multiple positives per anchor, we need to carefully define both positive and negative sets. %(more than one positive is allowed and negative sampling cannot be done uniformly over all data). 
In particular, we want to preserve the decoupling of the alignment and uniformity terms as, apart from respecting the original idea of A\&U, it typically yields improved performance~\cite{yeh_decoupled_2022}. Therefore, from all pairwise assignments $\ma{A}$ in the batch, we need to gather positive $A^+$ and negative $A^-$ assignment sets such that $A^+\cap A^-=\emptyset$. This also implies discarding comparisons of one track against itself (that is, the diagonals of $\ma{D}$ and $\ma{A}$, and potentially other spurious cells corresponding to sampling the same track more than once in the same batch).

Given the sets $A^+$ and $A^-$, we can write a decoupled, supervised version of A\&U over a batch as
\begin{equation}
\tilde{\fu{L}} = \frac{1}{|A^+|} \sum_{(i,j)\in A^+} d_{ij}^\alpha + \lambda \log\left( \frac{1}{|A^-|} \sum_{(i,j)\in A^-} e^{-\gamma d^2_{ij}} \right) ,
\label{eq:au}
\end{equation}
where $|~~|$ denotes set cardinality and $\alpha$, $\lambda$, and $\gamma$ are hyper-parameters. \citet{wang_understanding_2020} do not report strong performance differences by changing $\alpha$, $\lambda$, and $\gamma$ within a certain range, and generally set $\alpha=2$, $\lambda=1$, and $\gamma=3$ for their experiments. In preliminary experiments, and for our task, we find similar conclusions for $\alpha$ and $\lambda$, but not for $\gamma$. In addition, we are motivated to use $\alpha=2$ and $\lambda=1$, as that makes alignment and uniformity terms more comparable (same distance and same weight; see also the gradient analysis below). %With that, we get
%\begin{equation*}
%\fu{L} = \frac{1}{|P|} \sum_{i,j\in P} d_{ij}^2 + \log\left( \frac{1}{|N|} \sum_{i,j\in N} e^{-\gamma d^2_{ij}} \right) .
%\end{equation*}

The original A\&U loss employs Euclidean distances on the hypersphere, using $L_2$-normalized $\ve{z}$ vectors. This, in our view, presents a potential issue, in the sense that the employed distance function does not match with the geometric structure of the space (Euclidean vs.\ hypersphere surface, respectively). In our approach, instead of considering the negative arc length (which corresponds to the geodesic distance in the hypersphere) to improve performance like~\citet{koishekenov_geometric_2023}, we opt for the plain Euclidean space (of which the Euclidean distance is its geodesic distance). Thus, we do not constrain $\ve{z}$ to have a unit norm. Notice that, with this change, the uniformity concept does not apply, as we are not in an hypersphere anymore. Nonetheless, we can still reason and base our intuitions on the kernel and potential concepts used to derive the uniformity term in~\citet{wang_understanding_2020}.

With the decoupling, hyper-parameter, and geometric considerations above, we formulate the CLEWS loss as
\begin{equation*}
\fu{L} = \frac{1}{|A^+|}\!\sum_{(i,j)\in A^+}\!\!\!d_{ij}^2 + \log\left(\varepsilon + \frac{1}{|A^-|}\!\sum_{(i,j)\in A^-}\!\!\!e^{-\gamma d^2_{ij}}\right) ,
\end{equation*}
where $d_{ij}$ are distances after reduction (Eq.~\ref{eq:distreduced}) and $\gamma,\varepsilon>0$ %and $\varepsilon\in(0,1]$ 
are hyper-parameters. We use dimension-normalized Euclidean distances (root mean squared differences), as this does not affect our geometric considerations (it just adds a constant) and facilitates maintaining the same hyper-parameters when changing the dimensionality of $\ve{z}$. The $\varepsilon$ hyper-parameter is initially introduced for numerical stability. However, we note that it also has a soft thresholding or smoothing effect for the potential between negative pairs. 

\topic{Role of the Hyper-parameters}
We now briefly and intuitively study the role of $\gamma$ and $\varepsilon$ in $\fu{L}$ (a full analysis is beyond the scope of the present paper). To do so, we consider the gradient of $\fu{L}$ for a specific distance pair $d_{ij}$. Depending if the pair is in $A^+$ or $A^-$, we have
\begin{equation*}
\nabla^+ \triangleq \frac{\partial \fu{L}}{\partial d_{ij}} \bigg|_{(i,j)\in A^+} = \frac{2 d_{ij}}{|A^+|}
\end{equation*}
or
\begin{equation}
\nabla^- \triangleq \frac{\partial \fu{L}}{\partial d_{ij}} \bigg|_{(i,j)\in A^-} = \frac{-2\gamma d_{ij} e^{-\gamma d^2_{ij}} }{|A^-|\varepsilon + c + e^{-\gamma d^2_{ij}} } ,
\label{eq:nablaneg}
\end{equation}
where $e^{-\gamma d_{ij}^2}$ corresponds to the negative potential for the pair $i,j$~\cite{wang_understanding_2020} and the constant $c$ is the sum of all potentials that do not feature $(i,j)$. To facilitate our analysis, we view $\varepsilon$ as a reference potential and redefine $\hat{\varepsilon} = |A^-|\varepsilon + c$. We can then consider three cases with regard to the relation of $\hat{\varepsilon}$ and the potential for the negative pair $i,j$. If $\hat{\varepsilon} \ll e^{-\gamma d^2_{ij}}$ (case~1), we have $\nabla^- \approx -2\gamma d_{ij}$ and, if $\hat{\varepsilon} \approx e^{-\gamma d^2_{ij}}$ (case~2), we have $\nabla^- \approx -\gamma d_{ij}$. In both cases, and thanks to having set $\alpha=2$ after Eq.~\ref{eq:au}, the terms in $\nabla^-$ are similar to the ones in $\nabla^+$ (thus positive and negative pairs have a comparable influence). Moreover, we see that $\gamma$ is also acting as a weight for the negative pairs' gradient (thus taking a similar role as the original $\lambda$ in Eq.~\ref{eq:au}). 
Finally, if $\hat{\varepsilon} \gg e^{-\gamma d^2_{ij}}$ (case~3), we have
\begin{equation*}
\nabla^- \approx \frac{-2\gamma d_{ij} e^{-\gamma d^2_{ij}} }{\hat{\varepsilon}} ,
\end{equation*}
which implies a progressively vanishing gradient with increasing $d_{ij}$ ($e^{-\gamma d^2_{ij}}$ decreases much faster than $\gamma d_{ij}$ increases). %and, in the limit of $d_{ij}\to\infty$, $\gamma d_{ij} e^{-\gamma d^2_{ij}}\to 0$). 
Thus, we obtain a smooth transition to zero gradient after the potential $e^{-\gamma d^2_{ij}}$ crosses a threshold that is a function of $\varepsilon$. %A visualization of $\nabla^-$ (Eq.~\ref{eq:nablaneg}) using different values for $\gamma$ and $\varepsilon$ is shown in Appendix~\ref{sec:app_method}.
A visualization of the effect of $\gamma$ and $\varepsilon$ on $\nabla^-$ (Eq.~\ref{eq:nablaneg}) is given in Appendix~\ref{sec:app_method}.


\subsection{Architecture and Training}
\label{sec:method_model}

We now overview CLEWS' network architecture $\fu{F}$ and its training procedure (further details are available in Appendix~\ref{sec:app_method} and in our code). To obtain segment embedding vectors $\ve{z}$ on which to compute distances, we start from the full-track audio waveform and uniformly randomly cut a 2.5\,min block $\ve{x}$ from it. We further cut $\ve{x}$ into 8~non-overlapping 20-second segments $\ve{x}^k$ (we repeat-pad the last segment). We then compute its constant-Q spectrogram, downsample it in time by a factor of 5, and normalize it between 0 and 1, all following similar procedures as common version matching approaches~\cite{yesiler_audio-based_2021}. After that, we pass it to a learnable frontend, formed by two 2D strided convolutions, batch normalization, and a ReLU activation. Next, we employ a pre-activation ResNet50 backbone~\cite{he_identity_2016} with ReZero~\cite{bachlechner_rezero_2021} and instance-batch normalization~\cite{pan_two_2018}. We pool the remaining spectro-temporal information with generalized mean pooling~\cite{radenovic_fine-tuning_2019}, and project to a 1024-dimensional representation $\ve{z}$ using batch normalization and a linear layer. We train CLEWS with $\fu{R}^+=\fu{R}_\tx{bpwr-5}$, $\fu{R}^-=\fu{R}_\tx{min}$, $\gamma=5$, and $\varepsilon=10^{-6}$ as defaults, and study the effect of such choices in Sec.~\ref{sec:results}. Since test sets also contain tracks longer than the 2.5\,min used for training, in CLEWS we use our proposed $\fu{R}_\tx{bpwr-10}$ for track matching, together with a segment hop size of 5\,s.

We train all models with Adam using a learning rate of 2$\cdot$10$^{-4}$, following a reduce-on-plateau schedule with a 10-epoch patience and an annealing factor of 0.2. The only exception is in ablation experiments, where we train for 20~epochs featuring a final 5-epoch polynomial learning rate annealing. In every epoch, we group all tracks into batches of 25~anchors and, for each of them, we uniformly sample with replacement 3 positives from the corresponding version group (excluding the anchor). Thus, we get an initial (track-based) batch size of 100. For every track in the batch, we uniformly sample 2.5\,min from the full-length music track and create the aforementioned 8~segments per track. Thus, we get a final (segment-based) batch size of 800. We only use time stretch, pitch roll, and SpecAugment augmentations~\cite{liu_coverhunter_2023}.

\section{Evaluation Methodology}
\label{sec:eval}

\topic{Data}
We train and evaluate all models on the publicly-available data sets DiscogsVI-YT~\citep[DVI;][]{araz_discogs-vi_2024} and SHS100k-v2~\citep[SHS;][]{yu_learning_2020}, using the predefined partitions. SHS is a well-established reference data set. However, since it is based on YouTube links, it is almost impossible to gather it entirely nowadays (we managed to gather 82\% of it). In addition, one could consider it slightly biased, as the version group sizes are unrealistically large~\citep[cf.][]{doras_prototypical_2020, araz_discogs-vi_2024}. Instead, the recently proposed DVI data set is 5~times larger and better represents the real-world distribution of version group sizes. For both data sets, we use 16\,kHz mono audio and cap the maximum length to the first 10\,min. 

\topic{Baselines}
To compare the performance of the proposed approach with the state of the art, we consider several baselines: CQTNet~\cite{yu_learning_2020}, MOVE~\cite{yesiler_accurate_2020}, LyraC-Net~\cite{hu_wideresnet_2022}, CoverHunter~\cite{liu_coverhunter_2023}, DVINet+~\cite{araz_discogs-vinet-mirex_2024}, Bytecover2~\cite{du_bytecover2_2022}, ByteCover3~\cite{du_bytecover3_2023}, and ByteCover3.5~\cite{du_x-cover_2024}. For CoverHunter, we just consider the first ``coarse'' stage, as that is the part dealing with segments. CoverHunter, CQTNet, and DVINet have convenient source code available, and thus we can produce results by using it in our own pipeline (this way we can compare those baselines with our model rigorously under the same setting). Due to GPU memory restrictions, we train with randomly-sampled audio blocks of 2.5\,min. For the other baselines, we can only use already reported results as reference. 
The ByteCover series of models are known to be non-reproducible~\cite{ohanlon_detecting_2021, hu_wideresnet_2022}, with all attempts to date substantially under-performing\footnote{See for instance \url{https://github.com/Orfium/bytecover/issues/2}.} the reported results. We also implement our version of the ByteCover models and, for the first time, are able to obtain results that come close to the ones reported in the original papers (Sec.~\ref{sec:results}). In the following, we denote our approximations to ByteCover with a $\dag$ symbol. Having retrained/replicated baselines provides us an estimation of their performance in scenarios that have not yet been considered in the literature, such as with the DVI data set or the segment-level evaluation proposed below. %More details are provided in Appendix~\ref{sec:app_eval}. 

% We note that both the retrained and the replicated existing approaches obtain a MAP in SHS-Test that is compatible with the values reported by CQTNet, DVINet, and ByteCover authors (for CoverHunter we cannot compare since the authors use a different version of SHS-Test). Thus, the NAR values and the results obtained by these approaches on DVI-Test may serve as a reasonable guideline. For the retrained CQTNet we obtain a slight increase~\citep[0.677 vs.\ 0.655; cf.][]{yu_learning_2020}, and for the retrained DVINet+ we obtain an expected improvement with respect to DVINet trained on DVI~\citep[0.720 vs.\ 0.660; cf.][]{araz_discogs-vi_2024}. For the replicated ByteCover$\dag$ models (Sec.~\ref{sec:eval}), for the first time, we manage to get performances close to the ones reported by the authors (0.863 for ByteCover2 vs.\ 0.813 for ByteCover1/2$\dag$, and 0.857 for ByteCover3 vs.\ 0.783 for ByteCover3$\dag$).

\topic{Evaluation}
During testing, to compute candidate embeddings, we treat all models as if they were segment-based and extract overlapping blocks or segments using the same length as in training and a hop size of 5\,s (this yielded a marginal improvement for full-track baselines trained on 2.5\,min blocks). With these candidate embeddings, we perform both track- and segment-level evaluations. The former is equivalent to the usual evaluation setup in musical version matching, while the latter focuses on the retrieval of best-matching segments. 
For the track-level evaluation, we use the same segment length (the training segment length) and hop size for both queries and candidates. Then, to measure the performance of the system working at the full-track level, we use $\fu{R}_\tx{meanmin}$ to compute the final query-candidate distance. For the segment-level evaluation, we keep the same segment configuration as in the track-level case, but we vary the query segment length $\tau$. This way we assess a model's performance on different query lengths found in real-world scenarios. Then, to measure the performance of the system working at the segment level, we use $\fu{R}_\tx{min}$ to compute a best-match query-candidate distance. With this best-match approach, we simulate the performance of an equivalent segment-based retrieval system using all raw segments as candidates (see Appendix~\ref{sec:app_eval}). 
As evaluation measures, we compute the usual mean average precision (MAP) plus an enhanced version of the normalized average rank (NAR; see Appendix~\ref{sec:app_eval}). MAP focuses on the precision in the top candidates while NAR focuses more on the overall recall, which we think is a better option for musical version matching, especially for segment-based applications. %After sorting query-candidate distances in ascending order, we compute MAP and NAR excluding the query from the candidate list.
%Both MAP and NAR exclude the query from the candidate list.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec:results}

\begin{table*}[t]
\caption{Track-level evaluation and comparison with the state of the art. The symbol $\dag$ denotes that it is our implementation and the $\pm$ symbol denotes 95\% confidence intervals. %Best results highlighted in bold.
}
\label{tab:sota}
\tablecaptionspace
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llccccc}
\toprule
Approach & & \multicolumn{2}{c}{DVI-Test}      & & \multicolumn{2}{c}{SHS-Test} \\
        \cline{3-4} \cline{6-7}
         & & NAR $\downarrow$ & MAP $\uparrow$ & & NAR $\downarrow$ & MAP $\uparrow$ \\
\midrule
CoverHunter-Coarse~\cite{liu_coverhunter_2023} & & 10.36 $\pm$ 0.07 & 0.157 $\pm$ 0.001 & & 4.09 $\pm$ 0.17 & 0.491 $\pm$ 0.007 \\
MOVE~\cite{yesiler_accurate_2020} & & n/a             & n/a             & & n/a           & 0.519\hspace{1.2cm} \\
CQTNet~\cite{yu_learning_2020}                 & & ~~6.68 $\pm$ 0.07 & 0.493 $\pm$ 0.002 & & 2.67 $\pm$ 0.16 & 0.677 $\pm$ 0.007 \\
DVINet+~\cite{araz_discogs-vinet-mirex_2024}   & & ~~3.69 $\pm$ 0.06 & 0.643 $\pm$ 0.002 & & 2.39 $\pm$ 0.16 & 0.720 $\pm$ 0.007 \\
LyraC-Net~\cite{hu_wideresnet_2022}            & & n/a             & n/a             & & n/a           & 0.765\hspace{1.2cm} \\
ByteCover3$\dag$~\citep[based on][]{du_bytecover3_2023} & & ~~5.64 $\pm$ 0.05 & 0.513 $\pm$ 0.002 & & 1.91 $\pm$ 0.14 & 0.783 $\pm$ 0.006 \\
ByteCover1/2$\dag$~\citep[based on][]{du_bytecover2_2022} & & ~~4.98 $\pm$ 0.06 & 0.595 $\pm$ 0.002 & & 1.95 $\pm$ 0.14 & 0.813 $\pm$ 0.006 \\
ByteCover3~\cite{du_bytecover3_2023}           & & n/a             & n/a             & & n/a           & 0.824\hspace{1.2cm} \\
ByteCover3.5~\cite{du_x-cover_2024}            & & n/a             & n/a             & & n/a           & 0.857\hspace{1.2cm} \\
ByteCover2~\cite{du_bytecover2_2022}           & & n/a             & n/a             & & n/a           & 0.863\hspace{1.2cm} \\
CLEWS (proposed)                               & & ~~\textbf{2.70 $\bm{\pm}$ 0.05} & \textbf{0.774 $\bm{\pm}$ 0.002} & & \textbf{1.27 $\bm{\pm}$ 0.12} & \textbf{0.876 $\bm{\pm}$ 0.005} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
%\vskip -0.1in
\end{table*}

\begin{figure*}[t]
%\vskip 0.2in
%\begin{center}
\centerline{\includegraphics[width=\linewidth]{fig_segment_dvi-full}}
\figurecaptionspace
\caption{Segment-level evaluation with DVI-Test. NAR (left) and MAP (right) for different query segment lengths $\tau$ (notice the logarithmic axis for NAR). The shaded regions correspond to 95\% confidence intervals (barely visible due to the size of DVI-Test). Comparatively similar results for SHS-Test and also for an alternative evaluation protocol are available in Appendix~\ref{sec:app_results}.}
\label{fig:seg_dvi}
%\end{center}
%\vskip -0.2in
\end{figure*}

%\subsection{Comparison with the State of the Art}
%\label{sec:res_sota}
\topic{Comparison with the State of the Art}
First of all, we focus on the track-level evaluation and compare with the state of the art. We observe that CLEWS outperforms all considered approaches, many of them by a large margin (Table~\ref{tab:sota}). CLEWS obtains a NAR of 2.70 on DVI-Test and a MAP of 0.876 on SHS-Test, setting a new state-of-the-art result on both data sets. Besides CLEWS, an interesting observation to make is that the rank of an existing approach considerably varies between SHS-Test and DVI-Test. That is the case of DVINet+, which obtains a modest performance on SHS-Test, but achieves the best performance among the existing approaches on DVI-Test. We hypothesize that this is thanks to being the only considered approach (apart from CLEWS) that does not learn from a classification loss: because SHS-Train has more items per class than the more realistic DVI-Train (an average of 12 vs.\ 2, respectively), a classification loss is able to learn a useful representation from SHS-Train, but not from DVI-Train. Both DVINet+ and CLEWS, utilizing triplet and contrastive losses, respectively, do not suffer from this issue and maintain a comparatively similar performance across the two data sets. 
Another observation to make with regard to the consideration of segments is that treating them independently (CoverHunter-Coarse) yields worse results than developing some specific strategies (ByteCover3, ByteCover3$\dag$, and CLEWS), and that learning from a global match (ByteCover3 and ByteCover3$\dag$) is not as optimal as learning from a partial match (CLEWS). The latter is further supported by our ablations below.


%\subsection{Segment-based Version Matching}
%\label{sec:res_segment}
\topic{Segment-based Version Matching}
We now focus on the segment-level evaluation and study the performance as a function of the query segment length $\tau$. We observe that CLEWS again outperforms all considered models both in DVI-Test (Fig.~\ref{fig:seg_dvi}) and SHS-Test (Appendix~\ref{sec:app_results}), and for both NAR and MAP measures. Importantly, CLEWS maintains a high performance for all considered lengths (Fig.~\ref{fig:seg_dvi}). The only exception is with $\tau=5$ where, according to our listening experience, it is sometimes difficult even for a human to establish if two audio segments are versions or not. According to our segment-level evaluation, ByteCover3$\dag$ features some noticeable performance degradation with large $\tau$, perhaps due to the global match approach. CQTNet and DVINet+, both based on the same plain convolutional architecture, show an early performance decline for $\tau<60$.


\begin{table}[t]
\caption{Results on DVI-Valid for different positive $\fu{R}^+$ and negative $\fu{R}^-$ distance reductions. The default CLEWS reductions are $\fu{R}^+=\fu{R}_\tx{bpwr-5}$ and $\fu{R}^-=\fu{R}_\tx{min}$. As above, $\pm$ denotes 95\% c.i. %Best results highlighted in bold.
}
\label{tab:redux}
\tablecaptionspace
\begin{center}
\begin{small}
%\begin{adjustbox}{max width=\textwidth}
%\begin{sc}
\begin{tabular}{lllcc}
\toprule
$\fu{R}^+$             & $\fu{R}^-$           & & \textsc{NAR} $\downarrow$ & \textsc{MAP} $\uparrow$ \\
\midrule
\multicolumn{2}{l}{\textsc{CLEWS (proposed)}} & & 2.57 $\pm$ 0.09 & 0.804 $\pm$ 0.003 \\
%\midrule
$\fu{R}_\tx{bpwr-3}$   & $\fu{R}_\tx{min}$    & & 2.60 $\pm$ 0.09 & \textbf{0.809} $\pm$ \textbf{0.003} \\
$\fu{R}_\tx{bpwr-8}$   & $\fu{R}_\tx{min}$    & & \textbf{2.51} $\pm$ \textbf{0.09} & 0.789 $\pm$ 0.003 \\
$\fu{R}_\tx{meanmin}$  & $\fu{R}_\tx{min}$    & & 2.58 $\pm$ 0.09 & 0.798 $\pm$ 0.003 \\
$\fu{R}_\tx{best-10}$  & $\fu{R}_\tx{min}$    & & 2.63 $\pm$ 0.09 & 0.795 $\pm$ 0.003 \\
$\fu{R}_\tx{min}$      & $\fu{R}_\tx{min}$    & & 2.79 $\pm$ 0.09 & 0.799 $\pm$ 0.003 \\
%$\fu{R}_\tx{mean}$   & $\fu{R}_\tx{mean}$ & & \textsc{n/a} & \textsc{n/a} \\
%\midrule
$\fu{R}_\tx{bpwr-5}$   & $\fu{R}_\tx{best-10}$ & & 2.82 $\pm$ 0.10 & 0.779 $\pm$ 0.003 \\
$\fu{R}_\tx{bpwr-5}$   & $\fu{R}_\tx{bpwr-5}$ & & 2.88 $\pm$ 0.10 & 0.778 $\pm$ 0.003 \\
$\fu{R}_\tx{bpwr-5}$   & $\fu{R}_\tx{meanmin}$ & & 4.95 $\pm$ 0.12 & 0.488 $\pm$ 0.004 \\
%$\fu{R}_\tx{bpwr-5}$   & $\fu{R}_\tx{mean}$ & & \textsc{n/a} & \textsc{n/a} \\
\bottomrule
\end{tabular}
%\end{sc}
%\end{adjustbox}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\begin{table}[t]
\caption{Results on DVI-Valid for different loss functions using the default CLEWS reductions of $\fu{R}^+=\fu{R}_\tx{bpwr-5}$ and $\fu{R}^-=\fu{R}_\tx{min}$. %Best results highlighted in bold.
\vspace{-0.6cm}
}
\label{tab:loss}
\tablecaptionspace
\begin{center}
\begin{small}
%\begin{adjustbox}{max width=\textwidth}
\begin{sc}
\begin{tabular}{llcc}
\toprule
Loss Function & & \textsc{NAR} $\downarrow$ & \textsc{MAP} $\uparrow$ \\
\midrule
CLEWS (proposed)  & & \textbf{2.57} $\pm$ \textbf{0.09} & \textbf{0.804} $\pm$ \textbf{0.003} \\
%SupCon-LearnTemp  & & 2.61 $\pm$ 0.09 & 0.732 $\pm$ 0.004 \\
SupCon            & & 2.69 $\pm$ 0.09 & 0.676 $\pm$ 0.004 \\
SigLIP            & & 2.79 $\pm$ 0.09 & 0.684 $\pm$ 0.004 \\
Triplet           & & 3.08 $\pm$ 0.11 & 0.717 $\pm$ 0.004 \\
SupCon-Decoupled  & & 3.14 $\pm$ 0.11 & 0.739 $\pm$ 0.004 \\
A\&U-Decoupled    & & 3.25 $\pm$ 0.11 & 0.620 $\pm$ 0.004 \\
Classification Xent & & 8.91 $\pm$ 0.14 & 0.205 $\pm$ 0.003 \\
\bottomrule
\end{tabular}
\end{sc}
%\end{adjustbox}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure}[t]
\vskip 0.1in
%\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig_hyperpars}}
\figurecaptionspace
%\vspace{-0.6cm}
\caption{Effect of hyper-parameters $\gamma$ (top) and $\varepsilon$ (bottom) on DVI-Valid. Shaded regions correspond to 95\% confidence intervals, and the default value is highlighted with a square marker.}
\label{fig:hyperpars}
%\end{center}
%\vskip -0.2in
\end{figure}

%\subsection{Ablations and Hyper-parameters}
%\label{sec:res_ablations}
\topic{Ablations and Hyper-parameters}
Finally, we focus our attention on possible variations to the default CLEWS. We start by studying the effect of positive $\fu{R}^+$ and negative $\fu{R}^-$ segment distance reductions (Table~\ref{tab:redux}). For $\fu{R}^+$, if we keep $\fu{R}^-\!=\!\fu{R}_\tx{min}$, we observe that, depending on the evaluation measure, we have two options that are better than the default: $\fu{R}_\tx{bpwr-8}$ for NAR and $\fu{R}_\tx{bpwr-3}$ for MAP. Nonetheless, we decide to keep the default one as a compromise between the two. With such compromise in mind, we observe that, for positive reductions, global matches ($\fu{R}_\tx{bpwr-8}$ and $\fu{R}_\tx{meanmin}$) under-perform a partial matches ($\fu{R}_\tx{bpwr-r}$), and that learning from consecutive segments ($\fu{R}_\tx{best-10}$ and $\fu{R}_\tx{meanmin}$) is not as competitive as avoiding them ($\fu{R}_\tx{bpwr-r}$). For $\fu{R}^-$, if we keep $\fu{R}^+\!=\!\fu{R}_\tx{bpwr-5}$, we see that all considered negative reductions under-perform the default $\fu{R}_\tx{min}$. We hypothesize that, as with triplet losses, a hard negative mining or worst-case strategy is beneficial~\citep[cf.][]{schroff_facenet_2015, kalantidis_hard_2020}. Reduction strategies based on $\fu{R}_\tx{mean}$ did not learn well, both for $\fu{R}^+$ and $\fu{R}^-$ (not shown).

The next aspect we study is the effect of the loss function given the default reduction strategies for $\fu{R}^+$ and $\fu{R}^-$ (Table~\ref{tab:loss}). We observe that the proposed CLEWS loss performs better in both NAR and MAP, with a significant difference in the latter measure. Standard losses like SupCon, SigLIP, and Triplet (Sec.~\ref{sec:background}) come next, not being able to take as much profit from $\fu{R}^+$ and $\fu{R}^-$ as CLEWS in the task we study. As already mentioned, a standard classification loss based on cross-entropy does not perform well, especially when training with very few instances per class.

The last aspect we study is the effect of hyper-parameters $\gamma$ and $\varepsilon$. If we zoom in the resolution of NAR and MAP, we observe an opposite trend for the two evaluation measures (Fig.~\ref{fig:hyperpars}): with progressively decreasing NAR (better performance), we obtain a progressively decreasing MAP (worse performance). This indicates that hyper-parameters $\gamma$ and $\varepsilon$ can be deliberately tuned to benefit one or the other measure (we did not extensively tune them for the results reported previously). Moreover, we actually see that setting $\gamma=2$ could have provided a better NAR and a slighlty increased MAP. Overall, however, if we zoom ourselves out from the resolution shown in Fig.~\ref{fig:hyperpars}, we essentially observe a plateau of performance between $\gamma\in[2,8]$ and $\varepsilon\in[10^{-8},10^{-5}]$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

In this paper, we tackle the task of segment-based musical version matching, and propose both a strategy to deal with weakly-labeled segments and a contrastive loss that outperforms well-studied alternatives. Through a series of extensive experiments, we show that our approach not only achieves state-of-the-art results in two different datasets and two different metrics, but also that it significantly outperforms existing approaches in a best-match, segment-level evaluation. We also study the effect of different reduction strategies, compare against existing losses, and analyze the effect of the hyper-parameters in our ablation studies. As weakly labeled segment information is ubiquitous in many research areas, and since the concepts exploited here are general to a wide range of contrastive learning tasks, we believe our methods could serve as inspiration or find usefulness in domains beyond audio and musical version matching.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
