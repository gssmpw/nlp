\section{Methodology}
 

\begin{figure*}[t]
    \centering
    \scalebox{0.9}{
    \input{figure1}}
    \caption{An overview of {\sc Demotic} is shown. {\sc Demotic} takes a Verilog instance describing a combinational circuit and parse it into its corresponding probabilistic model described in PyTorch. The embedding layer converts the learnable real-value inputs into probabilities. The $\ell_2$-loss function is calculated in each training iteration and the input variables are updated using GD.}
    \label{fig1}
\end{figure*}


In this section, we describe our differentiable solver/sampler for multi-level digital circuits. While the common approach in solving CircuitSAT typically involves converting the underlying circuit into CNF and employing a SAT solver to find the satisfying solution, we take a completely different approach. Instead, we re-frame the CircuitSAT problem as a multi-output regression task, transforming it into a learning problem. Digital circuits are inherently discrete and non-differentiable. Therefore, we first need to relax the CircuitSAT problem into a continuous form while accurately capturing the structure and behavior of the circuit. To accomplish this, we leverage the probability model of digital gates, as shown in Table \ref{tab1}. This probability model is commonly used in different domains such as stochastic computing \cite{Ardakani2017SC} and dynamic power estimation of digital circuits \cite{harris2010cmos}. We use these probabilities to model each gate in the circuit. The result of such modeling is a differentiable formulation of the underlying circuit that accurately describes its functionality while preserving its spatial structure. Of course, the outcome of this model remains identical to the original circuit in its discrete form for any binary input valuations.




\begin{table}[t]
    \centering
    \caption{Probability model of logic gates.}
    \vspace{-0.25cm}
    \input{table1}
    \label{tab1}
    \vspace{-0.25cm}
\end{table}






Given the differentiable model of the circuit obtained by replacing its discrete logic gates with their corresponding probability model, our objective now is to generate a set of inputs that satisfy a desired constraint. This constraint could pertain to any desired valuation of intermediate signals or outputs. To generate satisfying solutions to the CircuitSAT problem, we represent the input variables to the circuit as $\textbf{V} \in \mathbb{R}^{b\times n}$, where $n$ represents the number of variables and $b$ denotes the batch size. We define the matrix $\textbf{V}$ as the parameters of an embedding layer in our circuit model, which will be updated during the learning process. It is worth mentioning that the number of variables in our sampling method is significantly fewer than that of SAT samplers, remaining the same as the number of inputs in the circuit. This discrepancy arises because SAT samplers deal with the CNF of the circuit, where each gate or component introduces additional variables. The embedding layer converts the real-value input variables of the circuit into probabilities in the range from $0$ to $1$ using the sigmoid function $\sigma(\cdot)$, expressed as:
\begin{equation}
    \textbf{P} = \sigma(\textbf{V}) = \dfrac{1}{1 + e^{-\textbf{V}}},
\end{equation}
where $\textbf{P} \in [0, 1]^{b\times n}$ represents the input probabilities to the underlying circuit. The circuit functionality is then computed as:
\begin{equation}
    \textbf{Y} = \mathcal{F}(\textbf{P}),
\end{equation}
where $\mathcal{F}:[0, 1]^{b \times n} \rightarrow [0, 1]^{b \times m}$ denotes the probabilistic model of the circuit. The matrix $\textbf{Y} \in [0, 1]^{b \times m}$ denotes the $m$ outputs across $b$ data batches. The $\ell_2$-loss function $\mathcal{L}$ can be constructed by measuring the distance between $\textbf{Y}$ and the target output valuation matrix $\textbf{T} \in \{0, 1\}^{b \times m}$ as follows:
\begin{equation}
    \mathcal{L} = \sum_{b,m} \left|\left| \textbf{Y} - \textbf{T} \right|\right|^2_2.
\end{equation}
The above loss function can be minimized, and the input variables (i.e., $\textbf{V}$) can be updated using GD in an iterative manner. Upon convergence, the $b$ solutions to the CircuitSAT problem are obtained by converting the soft input values (i.e., $\textbf{V}$) into hard values (i.e., $\widetilde{\textbf{V}} \in \{0, 1\}^{b\times n}$).

Fig. \ref{fig1} illustrates the overview of {\sc Demotic}. {\sc Demotic} is equipped with a parser to covert the circuit described in either bit-blasted Verilog or Berkeley Logic Interchange Format (BLIF) into its corresponding probabilistic model. Consequently, {\sc Demotic} can describe combinational circuits and generate satisfying solutions for any arbitrary constraint on the circuit. Such a sampling paradigm can also benefit from GPU acceleration due to the parallel independent computations across the data batches, enabling a high-throughput sampling procedure. 

To better understand our methodology, let us consider a quantitative example using the module ``c$15$'' shown in Fig. \ref{fig1}. We set the output node $G19$ to $1$ as an output constraint, while the output node $G22$ can take any value of either $0$ or $1$. Therefore, the goal in this example is to find a pair of inputs such that the output node $G19$ is equal to $1$. In this example, the input nodes contributing to our output constraint are $G3$, $G6$, and $G7$. These inputs are learned iteratively using gradient descent. The remaining input nodes, $G1$, $G2$, and $G3$, will not be updated and can take any arbitrary binary values. During each training iteration, each input node is updated by computing the derivative of the loss function with respect to each input node.

To illustrate the process, we generate two samples. In the first step, we randomly assign two values to each input node as follows:
\begin{equation}
    \textbf{v}_{G3} = \begin{bmatrix}
           0.1 \\
           -0.2 
         \end{bmatrix}, \textbf{v}_{G6} = \begin{bmatrix}
           0.5 \\
           -0.4 
         \end{bmatrix}, \textbf{v}_{G7} = \begin{bmatrix}
           -0.7 \\
           -0.8 
         \end{bmatrix},
\end{equation}
where the concatenation of the above vectors forms the matrix $\textbf{V}$. Next, the input probabilities to the circuit are calculated using the sigmoid function:
\begin{equation}
    \textbf{p}_{G3} = \begin{bmatrix}
           0.5250 \\
           0.4502
         \end{bmatrix}, \textbf{p}_{G6} = \begin{bmatrix}
           0.6225 \\
           0.4013
         \end{bmatrix}, \textbf{p}_{G7} = \begin{bmatrix}
           0.3318 \\
           0.3100
         \end{bmatrix}.
\end{equation}
Using the probability model of each gate shown in Table \ref{tab1}, the probabilities of the intermediate node $G11$ and the output node $G19$ are calculated as follows:
\begin{equation}
    \textbf{p}_{G11} = \begin{bmatrix}
           0.4939 \\
           0.4902
         \end{bmatrix}, \textbf{p}_{G19} = \begin{bmatrix}
           0.1639 \\
           0.1520
         \end{bmatrix}.
\end{equation}
Given the target value of 1 for the output node $G19$, the loss is calculated as:
\begin{equation}
    \mathcal{L} = (\textbf{p}_{G19} - 1)^2 = \begin{bmatrix}
           (0.1639 - 1)^2  \\
           (0.1520 - 1)^2 
         \end{bmatrix} = \begin{bmatrix}
           0.6991  \\
           0.7192 
         \end{bmatrix}.
\end{equation}


The above computations are commonly referred to as forward computations. To update the value of the input variables, we need to calculate the derivative of the loss with respect to each input variable, which is referred to as backward computations. This involves using the derivatives of each gate (as shown in Table \ref{tab1}) and applying the chain rule. The process is derived as follows:
\begin{align}
    \dfrac{\partial \mathcal{L}}{\partial \textbf{v}_{G3}} &= \dfrac{\partial \mathcal{L}}{\partial \textbf{p}_{G19}} \dfrac{\partial \textbf{p}_{G19}}{\partial \textbf{p}_{G11}} \dfrac{\partial \textbf{p}_{G11}} {\partial \textbf{p}_{G3}}
    \dfrac{\partial \textbf{p}_{G3}} {\partial \textbf{v}_{G3}} = 2\textbf{p}_{G19} \cdot \textbf{p}_{G7} \cdot (1 - 2\textbf{p}_{G6}) \nonumber \\ 
    &\cdot \sigma(\textbf{v}_{G3})\cdot (1 - \sigma(\textbf{v}_{G3})) = \begin{bmatrix}
           0.0339  \\
           -0.0257 
         \end{bmatrix}, \nonumber 
\end{align}
\begin{align}
    \dfrac{\partial \mathcal{L}}{\partial \textbf{v}_{G6}} &= \dfrac{\partial \mathcal{L}}{\partial \textbf{p}_{G19}} \dfrac{\partial \textbf{p}_{G19}}{\partial \textbf{p}_{G11}} \dfrac{\partial \textbf{p}_{G11}} {\partial \textbf{p}_{G6}}
    \dfrac{\partial \textbf{p}_{G6}} {\partial \textbf{v}_{G6}} = 2\textbf{p}_{G19} \cdot \textbf{p}_{G7} \cdot (1 - 2\textbf{p}_{G3})  \nonumber 
 \\ 
    & \cdot \sigma(\textbf{v}_{G6}) \cdot (1 - \sigma(\textbf{v}_{G6}))  = \begin{bmatrix}
           0.0065   \\
           -0.0126 
         \end{bmatrix}, \nonumber 
\end{align}
\begin{align}
    \dfrac{\partial \mathcal{L}}{\partial \textbf{v}_{G7}} &= \dfrac{\partial \mathcal{L}}{\partial \textbf{p}_{G19}} \dfrac{\partial \textbf{p}_{G19}}{\partial \textbf{p}_{G7}} \dfrac{\partial \textbf{p}_{G7}} {\partial \textbf{v}_{G7}} = 2\textbf{p}_{G19} \cdot \textbf{p}_{G11} \nonumber 
 \\ 
    & \cdot \sigma(\textbf{v}_{G7})\cdot (1 - \sigma(\textbf{v}_{G7})) = \begin{bmatrix}
           -0.1831   \\
           -0.1778 
         \end{bmatrix},
\end{align}
where ``$\cdot$'' denotes element-wise multiplication.


At this point, each variable is updated using the gradient descent update rule. This involves subtracting the derivative of the loss, scaled by the learning rate, from the corresponding input variables. Given a learning rate of $\gamma = 10$, the new values of the input variables at the end of this iteration are obtained as follows:
\begin{align}
    \textbf{v}_{G3} &= \textbf{v}_{G3} - \gamma \dfrac{\partial \mathcal{L}}{\partial \textbf{v}_{G3}} =  \begin{bmatrix}
           -0.2389 \\
           0.0569
         \end{bmatrix}, \textbf{v}_{G6} = \begin{bmatrix}
           0.4349 \\
           -0.2741
         \end{bmatrix}, \nonumber \\ \textbf{v}_{G7} & = \begin{bmatrix}
           1.1311 \\
           0.9783
         \end{bmatrix}.
\end{align}
This process can be repeated multiple times until convergence. However, even after one iteration in this specific example, we obtain two valid and distinct solutions by rounding the input variables to their nearest discrete values after applying the sigmoid function. In this example, the two input pairs of $(v_{G3} = -0.2389, v_{G6} = 0.4349, v_{G7} = 1.1311)$ and $(v_{G3} = 0.0569, v_{G6} = -0.2741, v_{G7} = 0.9783)$ are rounded to $(\widetilde{v}_{G3} = 0, \widetilde{v}_{G6} = 1, \widetilde{v}_{G7} = 1)$ and $(\widetilde{v}_{G3} = 1, \widetilde{v}_{G6} = 0, \widetilde{v}_{G7} = 1)$, respectively. As demonstrated through this example, the forward and backward computations of the two samples are independent of each other. This allows for the parallel execution of the learning process across multiple samples (i.e., batches), enabling GPU acceleration.


% \begin{figure*}[t]
%     \centering
%      \begin{subfigure}[b]{0.6\textwidth}
%          \centering
%          \include{figure2a}
%          \vspace{-0.5cm}
%          \caption{}
%          \label{fig2a}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.39\textwidth}
%          \centering
%          \include{figure2b}
%          \vspace{-0.5cm}
%          \caption{}
%          \label{fig2b}
%      \end{subfigure}
%     \caption{The general form of a sequential circuit is shown in (a), and the recurrent cell for sequential circuits is depicted in (b).}
%     \label{fig2}
%     \vspace{-0.5cm}
% \end{figure*}



% \section{Sequential Circuits}
% So far, we have described how combinational circuits can be modeled and analyzed using {\sc Demotic}. In contrast to combinational circuits, where outputs are determined solely by their present inputs, the output in sequential circuits depends on both the past behavior of the circuit and the present values of inputs. The temporal operations of sequential circuits are controlled by a clock signal. The contents of memory elements (i.e., flip-flops) represent the past behavior of such a circuit, which is commonly referred to as the \textit{state} of the circuit. 


% Solving CircuitSAT problems for sequential circuits presents a unique challenge as it requires finding a sequence of inputs that satisfies the target constraint over a series of clock cycles. To tackle such problems, we can leverage a novel technique inspired by recurrent neural networks (RNNs). In RNNs, backpropagation through time is utilized during the learning process, allowing for updates to the network's hidden state at each time step. Similarly, in the context of solving CircuitSAT problems, we perform forward computations to iteratively update the state values at each clock cycle. During backward computations, gradients are backpropagated through time, extending back to the initial time step (i.e., the first clock cycle), to adjust the input sequence accordingly. While this approach draws parallels to RNN training, it is tailored to the unique challenges posed by solving CircuitSAT problems for sequential circuits. 



% Fig. \ref{fig2a} shows the general structure of a sequential circuit. We use this structure to formulate the CircuitSAT problem for sequential circuits to find satisfying solutions using {\sc Demotic}. In this structure, there are two combination circuits: one to update the state of the circuit (i.e., the content values of flip-flops) and the other one to generate the output. It is worth mentioning that both of these combinational circuits take the present values of flip-flops and primary inputs at the current time step as their inputs. Let us represent the primary input variables at time step $t$ as $\textbf{V}_t \in \mathbb{R}^{b\times n}$. We encode the primary input variables at each time step as learnable parameters to an embedding layer followed by the sigmoid function to provide input probabilities at time $t$ as $\textbf{P}_t \in [0, 1]^{p\times n}$ to the combinational circuits, i.e.,
% \begin{equation}
%     \textbf{P}_t = \sigma(\textbf{V}_t).
% \end{equation}
% The present output of the circuit (i.e., $\textbf{Y}_t \in [0, 1]^{b\times m}$) is computed as: 
% \begin{equation}
%     \textbf{Y}_{t} = \mathcal{F}_o(\textbf{P}_t, \textbf{H}_{t-1}),
% \end{equation}
% where $\mathcal{F}_o$ and $\textbf{H}_t \in [0, 1]^{b \times r}$ denote the functionality of the combinational circuit generating outputs and the present values of flip-flops at each time step, respectively. The number of flip-flops in the circuit is represented by $r$. The state of the circuit for the next time step is obtained as:
% \begin{equation}
%     \textbf{H}_{t} = \mathcal{F}_h(\textbf{P}_t, \textbf{H}_{t-1}),
% \end{equation}
% where the functionality of the combinational circuit updating the values of flip-flops is denoted by $\mathcal{F}_h$. The $\ell_2$-loss function $\mathcal{L}$ can then be constructed by measuring the distance between $\textbf{Y}_t$ at the desired time step $N$ and the target output valuation matrix $\textbf{T} \in \{0, 1\}^{b \times m}$ as follows:
% \begin{equation}
%     \mathcal{L} = \sum_{b,m} \left|\left| \textbf{T} - \textbf{Y}_N \right|\right|^2_2.
% \end{equation}
% With such a formulation for sequential circuits, {\sc Demotic} can solve the CircuitSAT problem and provide $b$ solutions. The general form of the recurrent cell for sequential circuits is shown in Fig. \ref{fig2b}, which is analogous to the RNN cell.

