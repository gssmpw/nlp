
\section{Experimental Results}
In this section, we showcase how our differentiable method tackles sampling in CircuitSAT problems. To achieve this, we have created a prototype for {\sc Demotic} using PyTorch. PyTorch is an open-source machine learning framework that blends Torch's efficient GPU-accelerated backend libraries with a user-friendly Python interface. For a comprehensive evaluation, we use the ISCAS-85 benchmark suite, comprising $11$ combinational circuits \cite{Hansen1999ISCASBench}. The results of {\sc Demotic} were obtained from running on a system equipped with an Intel Xeon E$5-2698$ with $2.2$GHz clock rate and $8$ NVIDIA V$100$ GPUs with $32$GB of memory each. We report the runtime performance of {\sc Demotic} in term of throughput, measured as the number of valid and distinct solutions per second, while using a single NVIDIA V$100$ GPU. To obtain the experimental results of {\sc Demotic} for the CircuitSAT problems of the ISCAS-85 benchmark suite, we used GD as the optimizer. We set the learning rate to $15$, the batch size to $500,000$, and the number of iterations to $10$.

% , and the ISCAS-89 benchmark suite, comprising $47$ sequential circuits





\subsection{Runtime Performance}
We use all $11$ combinational circuits from the ISCAS-85 benchmark suite, encompassing designs ranging from relatively simple to moderately complex. These circuits serve as standardized test cases for evaluating algorithm performance in tasks such as logic synthesis, technology mapping, simulation, and testing. We convert these circuits into CircuitSAT sampling problems by randomly assigning specific binary values to some of their output nodes. The objective is to identify a set of inputs that would yield the desired values for those fixed outputs. The size of the solution space for such problems is proportional to the number of inputs in these circuits. Table \ref{tab2} summarizes the sampling performance of {\sc Demotic} in terms of throughput for all the combinational circuits in the ISCAS-85 benchmark suite. Throughput is measured as the number of unique solutions generated per second. We report the experimental results corresponding to the best throughput obtained from each sampler in Table \ref{tab2}.

\begin{table*}[t]
    \centering
    \caption{The runtime performance of {\sc Demotic}, {\sc UniGen3}, {\sc CMSGen} and {\sc DiffSampler} is evaluated in terms of unique solution throughput. Throughput is measured under the case where each method is tasked with generating a minimum of $1000$ distinct solutions (except for ``c17'') within a timeout (TO) of $2$ hours.}
    \vspace{-0.25cm}
    \include{table2}
    \label{tab2}
    \vspace{-0.25cm}
\end{table*}

For comparison purposes, we evaluate {\sc Demotic}'s performance against state-of-the-art SAT sampler baselines, namely {\sc UniGen3} \cite{Soos2020unigen3}, {\sc CMSGen} \cite{Golia2021cmsgen}, and {\sc DiffSampler} \cite{Ardakani2024diffsampler}. {\sc UniGen3} and {\sc CMSGen} are highly optimized C++ implementations, whereas {\sc DiffSampler} is a GPU-accelerated SAT sampler implemented in Python using the high-performance numerical computing library JAX. To this end, we first need to convert the CircuitSAT problems into their CNF formulas under the same aforementioned output constraints. We employ the Tseytin transformation, which takes a combinational logic circuit as input and produces its corresponding CNF \cite{tseitin1983complexity}. The size of the solution space for the resulting SAT problems is proportional to the number of variables in their CNF representation. Table \ref{tab2} presents the performance of the baseline samplers for the obtained SAT instances. {\sc UniGen3} and {\sc CMSGen} were executed on server-grade Intel Xeon Gold $6254$ CPU with a clock rate of $3.1$GHz and $790$GB of RAM. Similar to {\sc Demotic}, the results of {\sc DiffSampler} were obtained from running on a system equipped with an Intel Xeon E$5-2698$ with $2.2$GHz clock rate and $8$ NVIDIA V$100$ GPUs with $32$GB of memory each.




% \begin{table*}[t]
%     \centering
%     \caption{The runtime performance of {\sc Demotic} is evaluated in terms of unique solution throughput for sequential circuits.}
%     \include{table3}
%     \label{tab3}
%     \vspace{-0.25cm}
% \end{table*}


The experimental results presented in Table \ref{tab2} showcase the superior performance of {\sc Demotic} in the sampling task, surpassing state-of-the-art samplers by over two orders of magnitude in most cases. This is because the conversion to CNF introduces additional variables and operations in the form of clauses depending on the complexity of the underlying circuit, as shown in Table \ref{tab2}, undermining the performance of baseline samplers across all the CircuitSAT instances except for ``c$17$''. Due to the limited number of inputs in the CircuitSAT instance for ``c$17$'', only $18$ unique solutions exist when constraining the circuit's second output to $1$. This restriction reduces {\sc Demotic}'s performance in this scenario, as the GPU becomes under-utilized. Consequently, {\sc CMSGen} performs more efficiently in this case.


Fig. \ref{fig3} illustrates the scaling patterns of runtime performance relative to the number of unique solutions generated by each sampler. The analysis reveals two key findings: Firstly, {\sc Demotic} overall demonstrates superior efficiency compared to {\sc UniGen3}, {\sc CMSGen} and {\sc DiffSampler}, especially when sampling larger numbers of solutions. Secondly, our method exhibits more efficient scalability, as evidenced by the linear scaling of the time required for generating larger numbers of solutions.


\begin{figure}
    \centering
    \include{figure3}
    \vspace{-1cm}
    \caption{Log-Log plot of runtime in millisecond against the
count of unique satisfying solutions found within that run time across all the CircuitSAT problems from the ISCAS-85 benchmark. Dotted lines denote the trend of the runtime performance for each sampler.}
    \label{fig3}
    \vspace{-0.5cm}
\end{figure}


\vspace{-0.25cm}
\subsection{Learning Dynamics}
In this section, we provide a detailed analysis of the learning dynamics of {\sc Demotic} over time. For all experiments, we set the batch size to $500,000$ and the learning rate to $15$ unless stated otherwise. We excluded the module ``c$17$'' from our experiments due to its limited number of inputs.

Figure \ref{fig4} illustrates the learning progress of {\sc Demotic} in terms of the number of unique solutions discovered across $10$ iterations. The learning curves show that as the number of iterations increases, the number of unique solutions learned by {\sc Demotic} also increases. While there is no theoretical guarantee that gradient descent will reach the global minimum in non-convex landscapes, including our continuous formulation of CircuitSAT problems, our experiments demonstrate its effectiveness in finding solutions that perform well in the continuous form. Even if these solutions aren't the absolute global minimum in the continuous form, they still satisfy the CircuitSAT constraints in the discrete form.

The convergence rate for each CircuitSAT problem varies depending on the complexity and structure of the underlying circuit, as well as the chosen hyper-parameters, such as the learning rate. More complex circuits and sub-optimal hyper-parameter settings typically result in slower convergence rates. Conversely, simpler circuits and well-tuned hyper-parameters tend to lead to faster convergence. Choosing an appropriate learning rate, as the most important hyper-parameter in our experiments, is crucial for effective model training. Very low learning rates can make slow convergence of the learning process, requiring many iterations to reach an optimal solution, which increases computational cost and time. On the other hand, very high learning rates can make the model oscillating around the minimum and leading to poor convergence. For instance, Fig. \ref{fig6} illustrates the learning progress of {\sc Demotic} across different learning rates ranging from $1$ to $20$ for the CircuitSAT problem ``c$2670$''. In this example, the learning rate of $15$ provides the best convergence among the tested rates. In contrast, the learning rate of $1$ results in the slowest convergence, while the learning rate of $20$ leads to slower convergence than the learning rate of $15$.





While increasing the number of iterations can lead to learning more unique solutions, it does not necessarily result in higher throughput, as shown in Fig. \ref{fig5}. In fact, the majority of solutions are learned by the end of the first iteration. Specifically, the number of solutions at the end of the first iteration is higher than the number of new unique solutions learned at the end of each subsequent iteration. Given that the latency of each iteration is roughly the same, the throughput of generating unique solutions decreases as the number of iterations increases, as depicted in Fig. \ref{fig5}.

\begin{figure}[t]
    \centering
    \vspace{-0.75cm}
    \include{figure4}
    \vspace{-1cm}
    \caption{Log learning plot of {\sc Demotic} showing the number of unique satisfying solutions across different iterations for the CircuitSAT problems from the ISCAS-85 benchmark.}
    \label{fig4}
    \vspace{-0.25cm}
\end{figure}

\begin{figure}[t]
    \centering
    \vspace{-0.5cm}
    \include{figure6}
    \vspace{-1cm}
    \caption{Log learning plot of {\sc Demotic} showing the number of unique satisfying solutions across different iterations and learning rates for the CircuitSAT problem of ``c$2670$''.}
    \label{fig6}
    \vspace{-0.5cm}
\end{figure}




This observation suggests that running {\sc Demotic} for only one iteration may be sufficient to obtain the desired number of distinct solutions by adjusting the batch size. However, this conclusion holds only when there is no GPU memory constraint for the underlying circuit. GPU acceleration of CircuitSAT sampling incurs GPU memory usage depending on the size of the CircuitSAT problem and the batch size. Fig. \ref{fig7} shows the GPU memory usage of the CircuitSAT problems, measured by ``nvidia-smi'', across different batch sizes. This figure illustrates the significant growth in GPU memory usage for larger batch sizes. In scenarios where generating a large number of unique samples is targeted but there are constraints on GPU memory usage, the inevitable solution is to run the learning process for more iterations, albeit at the cost of lower throughput.







\begin{figure}[t]
    \centering
    \include{figure5}
    \vspace{-1cm}
    \caption{Log plot of the throughput of {\sc Demotic}, measured by the number of unique satisfying solutions per second across different iterations for the CircuitSAT problems from the ISCAS-85 benchmark.}
    \label{fig5}
    \vspace{-0.25cm}
\end{figure}





\begin{figure}[t]
    \centering
    \vspace{-0.5cm}
    \include{figure7}
    \vspace{-1cm}
    \caption{Log-log plot of GPU memory usage of {\sc Demotic} in megabytes, measured by ``nvidia-smi'' across different batch sizes for the CircuitSAT problems from the ISCAS-85 benchmark.}
    \label{fig7}
    \vspace{-0.5cm}
\end{figure}


% \subsection{Sequential Circuits}
% In the task of sequential circuit sampling, we employ the entire ISCAS-89 benchmark suite, comprising all its components. Unlike in combinational circuit sampling, the objective here is to discover input sequences that fulfill the specified output constraints for a certain number of clock cycles. To transform these circuits into CircuitSAT sampling problems, we randomly assign specific binary values to some of their output nodes after a certain number of clock cycles. We also set the number clock cycles to $25$ for these experiments. In fact, we treat the number of clock cycles as a hyper-parameter for the sampling task. Table \ref{tab3} summarizes the sampling performance of {\sc Demotic} for sequential circuits in the ISCAS-89 benchmark suite. Due to the temporal nature of sequential circuits, the sampling task is more challenging compared to the combinational circuits. As such, the sampling throughput of {\sc Demotic} for sequential circuits is lower than that for combinational circuits. It is worth mentioning that we report the experimental results of {\sc Demotic} for sequential circuits, as the conversion of such circuits to CNF is non-trivial, as discussed in Section \ref{subsec:circuitsat_sampling}. 


