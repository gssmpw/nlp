\newpage

\phantomsection
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}
\label{sec:appendix}

\renewcommand{\thesubsection}{\Alph{subsection}}

\begin{appendices}

\subsection{Warning Banners}
\label{sec:appendix-banners}

Detailed counts and percentages (of all SERPs) for the warning banners we collected in each crawl are provided in Table~\ref{tab:crawl-banners}.
In the remainder of this section, we provide details and screenshots for Google's rapidly-changing banners (Appendix~\ref{sec:appendix-banners-rapidly}), as well as the low-relevance banner variants we observed across crawls (Appendix~\ref{sec:appendix-banners-relevance}). 

\begin{table}[hb!]
    \centering
    \scriptsize
    \caption{Warning banner types observed by crawl for our 1.4M search queries.}
    \label{tab:crawl-banners}
    {\renewcommand{\arraystretch}{1}
    \input{tables/crawl-banners.tex}
    }
\end{table}

\clearpage
\newpage

\subsubsection{Rapidly-changing Banners}
\label{sec:appendix-banners-rapidly}

We provide a limited examination of Google's rapidly-changing banners (Figure~\ref{fig:freshness_banner}) in the main text because we only observed two instances in the first two waves and 10 instances in the third wave. 
These 14 queries were unique with no repeats across crawls.
The queries triggering these banners were often political, with 11 of the 14 contained either ``trump'' or ``biden'' in the query.
For crawl-1 (October 2023), the queries were: ``biden zionist'' and ``dr. reiner fuellmich''. 
For crawl-2 (March 2024), the queries were ``republicans are russian asset'' and ``trump word salad''.
In crawl-3 (Sep 2024), the queries were: ``trump people magazine 1998'', ```lifelong republican''', ``trump people 1998 quote'', ``trump republicans dumbest voters'', ``trump republican voters dumbest'', ``"trump will win election"'', ``donald trump people magazine quote 1998'', ``trump wanders away|trump wanders away'', ``biden with young girls'', and ``trump tell republicans dumbest voters''.
The time-sensitive nature of these warnings is exemplified by the query ``biden zionist,'' which we received a rapidly-changing banner when we conducted that search on October 17, 2023, shortly after the Hamas attack on Israel on October 7, but did not receive any banner when we conducted it in our second or third waves.
Although our approach provides a limited sampling of these banners, it did surface a few, and future work could use those as seed queries for various query expansion methods to create a broader set of queries that might also trigger this type of warning for further examination.

\begin{figure}[th!]
\centering
\includegraphics[width=0.7\textwidth]{figs/banner-screenshot-freshness.png}
\caption{Example of a freshness warning banner on Google Search. This banner is displayed when Google's systems detect ``a topic is rapidly evolving and a range of sources hasn't yet weighed in''~\citep{sullivan2010why}.}
\label{fig:freshness_banner}
\end{figure}

\subsubsection{Low-relevance Banner Variant}
\label{sec:appendix-banners-relevance}

In the third crawl, we observed a variant of the low-relevance banners that had not previously appeared (Figure~\ref{fig:topicality_banner_new}). 
This banner contained the same icon as the other low-relevance banners (a magnifying glass with a yellow background), but instead of stating that there were not many or any great matches, it states that ``Your search did not match any documents'' and provides a consistent list of generic suggestions for alternative searches (e.g., ``weather tomorrow'').
This variant appeared for 6,081 queries in crawl-3, accounting for 32.7\% of the 18,593 low-relevance banners we observed in total for that crawl. 
The presence of this variant was largely responsible for the overall increase in warning banner presence in that wave (from 1\% in crawl-1 to 0.9\% in crawl-2, to 1.3\% in crawl-3), despite low-quality banners disappearing (see Table~\ref{tab:crawl-banners}).
Although the exact language used in the banner is ``Your search did not match any documents'' (Figure~\ref{fig:topicality_banner_new}), we observed five cases in which the SERP contained a single generic ad (e.g., Dell, ADT). 

\begin{figure}[bh!]
\centering
\includegraphics[width=0.7\textwidth]{figs/banner-screenshot-topicality-new.pdf}
\caption{Example of the low-relevance warning banner variant that we observed in crawl-3.}
\label{fig:topicality_banner_new}
\end{figure}

\clearpage
\newpage

\subsection{Descriptive Statistics}
\label{sec:appendix-descriptives}

Details on the data we collected in each crawl, including counts and averages for key measures, are provided in Table~\ref{tab:crawl-counts}.

\begin{table}[hb!]
    \centering
    \footnotesize
    \caption{Key metric counts and averages by crawl for our 1.4M search queries.}
    \label{tab:crawl-counts}
    {\renewcommand{\arraystretch}{1}
    \input{tables/crawl-counts.tex}
    }
\end{table}

\subsubsection{Search Queries}
\label{sec:appendix-descriptives-queries}

The length of the queries in our dataset (by token and character count) followed a heavy-tail distribution in its raw form (Figure~\ref{fig:query-length-distribution-full}) and remained skewed after truncating at Google's 32 token limit (Figure~\ref{fig:query-length-distribution-truncated}).

Without access to proprietary search engine data, it is impossible to know exactly how many users searched each query in our dataset. However, several large-scale query datasets have been released by search engines in the past. ORCAS, released by Microsoft, contains 10.4 million queries searched by at least ``$k$ different users, for a high value of $k$'' on Bing around January 2020 \citep{craswell2020orcas}. The exact value of $k$ used is not specified. The authors also applied filters to remove potentially offensive queries, like those containing pornography and hate speech. Although the ORCAS list is somewhat sanitized by its $k$-anonymity and offensive content filters, it allows us to check if any of our search directive queries were widely searched on Bing during that time period.

We find that 154,833 search directive queries were present in ORCAS, 1,972 of which were classified by our best-performing model as warranting a quality banner. These most confident 20 banner predictions include queries like ``facebook illuminati,'' ``lizard people conspiracy,'' and ``black groups that hate whites.'' The presence of these queries in ORCAS demonstrate both that data void queries exist on platforms beyond Google and that at least some of these queries are searched by a large number of users, by Microsoft's estimation. These queries could be spontaneous on the part of users, but given that the queries we analyze are all instances where one user publicly told at least one other user to search something---these queries meeting ORCAS $k$ threshold could be a result of the effectiveness of search directives.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.98\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/query-length-distribution.png}
    \caption{Full search query length distribution.}
    \label{fig:query-length-distribution-full}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.98\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/query-length-distribution-truncated.png}
    \caption{Truncated search query length distribution.}
    \label{fig:query-length-distribution-truncated}
    \end{subfigure}
    \caption{Search query length distributions for our set of 1.4M unique queries. The full distribution (a) shows the raw query lengths (measured via token and character counts), while the truncated distribution (b) shows the same distributions truncated at Google's 32 token query limit (see Methods~\ref{sec:methods-directives-queries}). Some queries have a truncated token count of 0 because they only contained punctuation or emojis (which were removed during tokenization), and a truncated character count of 0 because we measure that after truncating by tokens.}
    \label{fig:query-length-distributions}
\end{figure*}

\subsubsection{SERP Features}
\label{sec:appendix-descriptives-serps}
We found that Google Search's result size estimates follow a mixed distribution consisting of several heavy-tail distributions (Figure~\ref{fig:searches-length-distribution}).
While this distribution may indicate an underlying binning by Google, these estimates are generated through a non-public process, can vary based on a number of hard-to-control factors (like the data center used), and can vary in counter-intuitive ways. 
For example, longer and more specific queries (``cars -used'' are known to sometimes produce larger estimates than their shorter and more generic queries (``cars''), because the longer queries can cause the search engine to conduct a deeper search that surfaces a larger and more accurate estimate~\citep{sullivan2010why}.
While Google returned an estimate of 0 results for 1\% of queries in crawl-1, 0.87\% in crawl-2, and 1.3\% in crawl-3, around 50\% of those SERPs had one or more results (44.2\% in crawl-1, 43.9\% in crawl-2, and 66.5\% in crawl-3).

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/plots/searches-length-distribution-all-crawls.png}
    \caption{The estimated number of search results available for a given query---provided by Google in each SERP---follows a mixture distribution consisting of nine distinct distributions (e.g., 0 to 10 and 11 to 1000), suggesting different mechanisms for estimating the number of results within each band. The median number of estimated results was 4.6M in crawl-1, 3.3M in crawl-2, 1.9M in crawl-3, and 2.6M in crawl-4. Means ranged from 281M (crawl-1) to 204M (crawl-3) and standard deviations ranged from 1.4B (crawl-1) to 1B (crawl-4). Our data suggests a ceiling on Google's estimates, and the max value we observed in any crawl was 25.27B.}
    \label{fig:searches-length-distribution}
\end{figure}

\subsubsection{Domain Quality and Keyword Matches}
\label{sec:appendix-descriptives-quality}

Average domain quality was 0.78 (SD = 0.1), which is comparable to the score for \nolinkurl{aljazeera.com} (0.779) or \nolinkurl{tabletmag.com} (0.781).
In total, 159K queries (11.1\%) had a political keyword match, 1,588 (0.11\%) had a conspiracy-related keyword match, and 297 (0.02\%) had a keyword match for both.
Comparing average domain quality across these query types, we found small differences in their distributions when no banner was present, but no such differences when a banner was present (Figure~\ref{fig:search-quality-by-query-match}). 
When no banner was present, conspiracy-related queries produced SERPs with a higher average domain quality than SERPs produced by queries without such terms.
However, the overall differences in average domain quality by banner presence were much larger, and the average domain quality of SERPs that received a banner was about 10\% lower than average domain quality when no banner was present.

Among conspiracy-related queries, we found relatively small differences in the distribution of domain quality by specific conspiracy categories (Figure~\ref{fig:search-quality-by-conspiracy-category}).
The earliest linked search directive with a conspiracy-related query we found on Twitter occurred on July 19, 2009 and involved the longstanding conspiracy that 9/11 was an ``inside job'' (\cite{ballatore2015google}; \cite{mahl2021nasa}).
The post asked a question ``Was 9/11 an inside job?'' and provided a Google Search link with ``9/11 inside job proof'' as the query.
Given the leading wording of that query, it is possible that this would have led to a data void of conspiracy-related websites.
However, it is often difficult or impossible to reconstruct the search results one would have seen had they conducted that search in 2009.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/quality-by-query-match-crawl-1.png}
    \caption{Distribution of average domain quality by query keyword matching. ``Neither'' indicates queries with no matches on the political or conspiracy lexicons, ``Political'' and ``Conspiracy'' indicate queries that match those respective lists, and ``Political \& Conspiracy'' indicates queries with matches from both.}
    \label{fig:search-quality-by-query-match}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.50\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/quality-by-conspiracy-category-crawl-1.png}
    \caption{Average domain quality for conspiracy-related queries by conspiracy category. We did not find evidence of substantial variation in the average domain quality among the categories we examined. ``Mixed'' denotes queries that contained a mention of at least two unique categories.}
    \label{fig:search-quality-by-conspiracy-category}
    \end{subfigure}
    \caption{Domain quality by query type (a) and conspiracy category (b). There were few differences in domain quality by query type and conspiracy-related queries had the greatest average domain quality (0.80), above neither (0.78), political (0.79), and political \& conspiracy (0.79).}
    \label{fig:combined-search-quality}
\end{figure*}

\subsubsection{Search Engine Optimization (SEO) Metrics}
\label{sec:appendix-descriptives-seo}
Many SEO indicators require external data to calculate, which makes these indicators infeasible for researchers to collect given limited compute and storage resources, e.g., the number of backlinks a target domain receives is the total number of links that point towards the target domain from all other websites on the internet. Consequently, organizations and researchers must rely on third-party SEO toolkits. We use data from Ahrefs\footnote{\url{https://ahrefs.com}}, which had the 5th most active commercial webcrawler in October 2023 by number of requests according to Cloudflare Radar\footnote{\url{https://radar.cloudflare.com/traffic/verified-bots}}. Using Ahrefs' API, we extract 23 SEO features and 12 traffic estimation features for domains that ranked highly for conspiratorial or polarized keyphrases. Due to time and budget constraints, we elected to only extract this data for domains that appeared in at least 10 different conspiratorial or conspiratorial queries. These features include total number of backlinks, traffic estimates, number of backlinks from government domains, number of backlinks from educational domains, the amount of user generated content on the target domain, the number of unique referring IP addresses, and the number of unique referring domains. 

While the only way to obtain true website traffic is through website owners, a non-peer-reviewed case study by AuthorityHacker, which used traffic data provided by 47 website owners, found Ahrefs' traffic estimates to be the most accurate of any SEO toolkit \citep{ahrefstraffic}. Ahrefs' traffic estimates are calculated using position-weighted click-through rate estimates for Google search volumes of all keyphrases for which a website appears in the top 100 Google search results. Ahrefs self-reported similar results across a larger set of 1,635 domains\footnote{\url{https://ahrefs.com/blog/traffic-estimations-accuracy/}}. While these traffic estimates may contain noise, they are, to our knowledge, the best estimates currently available and have been used in past work on SERP reliability (\cite{carragher2024detection}, \cite{carraghermisinformation}, \cite{williams2023search}).

\para{SEO Features in Political or Conspiracy Queries}
Exploring the subset ($n=1,984$) of the domains with domain quality scores among all domains that ranked at least 10 times for political or conspiratorial keyphrases ($n=9,125$), we find that most SEO features exhibited a slight positive correlation with domain quality. 
There are notable outliers, including social media and video sharing sites, such as YouTube and Facebook, which have low domain quality scores but nonetheless receive 2.7B and 3.2B billion links from educational domains, respectively.
However, we also found small negative correlations between domain quality and a domain's number of webpages ($r=-0.06$), internal links ($r=-0.03$), external links ($r=-0.06$), and canonical tags (a way of signifying duplicate content to web crawlers) ($r=-0.07$). 
We additionally observe that these metrics all positively correlate with traffic. 

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/seo_backlinks_traffic.png}
    \caption{Log backlinks vs log estimated traffic over all conspiratorial or political domains that ranked for at least 10 queries.}
    \label{fig:seo_bl_tot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/seo_backlinks_traffic_reliability.png}
    \caption{Log backlinks vs log estimated traffic for labeled reliable and unreliable domains.}
    \label{fig:seo_bl_rel}
    \end{subfigure}
    \caption{Number of backlinks compared to traffic estimates for domains that ranked for at least 10 political or conspiracy keyphrases (left) and for those which also have reliability labels (right). A 1\% increase in backlinks is associated with an approximate 0.77\% increase in traffic for reliable domains, and an approximate 0.67\% increase in traffic for unreliable domains.}
    \label{fig:seo_bl_traffic}
\end{figure*}

For the 9,125 domains that ranked at least 10 times for political or conspiratorial keyphrases, we observe a strong positive correlation ($r=0.78$) between backlinks and estimated traffic (Figure~\ref{fig:seo_bl_tot}). 
We partition the subset of these domains with quality scores ($n = 1,984$) by assigning the lowest quartile of the scores in our dataset (Domain Quality $< 0.62$) a label of ``unreliable'' and the remaining data a label of ``reliable.'' 
Using a simple interactive log-log regression model (where a log transformation is applied to the dependent and independent variables), we estimate that a 0.67\% change in traffic for 1\% change in backlinks for unreliable domains ($p<0.01$) and a 0.77\% change in traffic for a 1\% increase in backlinks for reliable domains ($p<0.01$; Figure \ref{fig:seo_bl_rel}). 

\subsection{Logistic Regressions}
\label{sec:appendix-logit}

We used logistic regression models to examine the factors associated with the presence of low-relevance and low-quality banners across our main crawls. 
The presence of either banner was our dependent variable, and our independent variables included factors both related to the query and the SERP it produced.
Among the query features were: character count (log10), presence of political keywords, presence of conspiracy keywords, and presence of query operators. We only used query operators as a feature for our low-relevance banner models because they never produced a quality banner in our dataset.
Among the SERP features were: average low-quality score of domains, rank-weighted news partisanship, estimated total results (log10), average domain traffic (log10), news domain count, and unique domain count.

We used the statsmodels library in Python to fit our logit models with L1 regularization. More specifically, we set the regularization parameter (alpha) to 0.1, and used the L-BFGS algorithm as the solver with a maximum of 10,000 iterations and convergence and zero tolerances set to $1e-8$. We trained separate models for each dependent variable and crawl because the rules governing their appearance could change over time.

\subsubsection{Low-relevance Banners}
\label{sec:appendix-logit-low-relevance}

Our models for predicting low-relevance banners demonstrated moderate fit, with pseudo-$R^2$ values of 0.39, 0.15, 0.38 for crawls 1, 2, and 3, respectively (Tables~\ref{tab:logit-low-relevance-crawl1},~\ref{tab:logit-low-relevance-crawl2},~\ref{tab:logit-low-relevance-crawl3}). 

\begin{table}[hb!]
    \centering
    \footnotesize
    \caption{Logistic Regression Results for Low-relevance Banners (crawl-1).}
    \label{tab:logit-low-relevance-crawl1}
    {\renewcommand{\arraystretch}{1}
    \input{tables/logit/coef-banner_topicality-crawl-1-no-impute.tex}
    }
\end{table}

\begin{table}[htb!]
    \centering
    \footnotesize
    \caption{Logistic Regression Results for Low-relevance Banners (crawl-2).}
    \label{tab:logit-low-relevance-crawl2}
    {\renewcommand{\arraystretch}{1}
    \input{tables/logit/coef-banner_topicality-crawl-2-no-impute.tex}
    }
\end{table}

\begin{table}[htb!]
    \centering
    \footnotesize
    \caption{Logistic Regression Results for Low-relevance Banners (crawl-3).}
    {\renewcommand{\arraystretch}{1}
    \label{tab:logit-low-relevance-crawl3}
    \input{tables/logit/coef-banner_topicality-crawl-3-no-impute.tex}
    }
\end{table}

\newpage
\clearpage

\subsubsection{Low-quality Banners}
\label{sec:appendix-logit-low-quality}

In contrast to the models for the low-relevance banners, and likely due to the smaller sample size, our models for predicting low-quality banners demonstrated lower fit, with pseudo-$R^2$ values of 0.13 for both crawls 1 and 2 (Tables~\ref{tab:logit-low-quality-crawl1} \& Table~\ref{tab:logit-low-quality-crawl2}).

\begin{table}[htb!]
    \centering
    \footnotesize
    \caption{Logistic Regression Results for Low-quality Banners (crawl-1)}
    \label{tab:logit-low-quality-crawl1}
    {\renewcommand{\arraystretch}{1}
    \input{tables/logit/coef-banner_quality-crawl-1-no-impute.tex}
    }
\end{table}

\begin{table}[htb!]
    \centering
    \footnotesize
    \caption{Logistic Regression Results for Low-quality Banners (crawl-2)}
    \label{tab:logit-low-quality-crawl2}
    {\renewcommand{\arraystretch}{1}
    \input{tables/logit/coef-banner_quality-crawl-2-no-impute.tex}
    }
\end{table}

\newpage
\clearpage

\subsection{Low-Quality Banner Consistency}
\label{sec:appendix-consistency}

\subsubsection{Formal Consistency Definitions}
\label{sec:appendix-consistency-formal}

In the main text we show that no URL pair conditioned on a rank cut-off can fully explain low-quality banner presence for all queries (Section~\ref{sec:evaluating}; Methods~\ref{sec:methods-stability-dependency}).
Here we provide a formal definition of the measure we used.
Let $Q =\{q_0, q_1, \dots, q_D\}$ be the set of 90 queries that have returned between 1 and 72 banners. 
Let $S = \{S_1, S_2, \dots, S_N\}$ be the set of SERPs that return a low-quality banner for a query $q_j$ where $S_i$ contains a ranked list of URLS $\{u_1, u_2, \dots, u_D\}$ and $R$ be the set of SERPs that do not return a banner for query $q_j$. 
Let $S_{:c*} =\{ S_{1,:c*}, \dots, S_{N,:c*}\}$ contain the bannered SERP results of all URLs below rank $c$ (e.g., $c=3$ would correspond to the first two URLs that appear on a SERP) concatenated with an arbitrary string `X', i.e. $S_{i,:c*} = \{\text{CONCAT}(u_1, `X'), \text{CONCAT}(u_2, `X'), \dots, \text{CONCAT}(u_{c-1}, `X')\}$. 
Conversely, let $S_{c:} = S_{1,c:}, \dots S_{N,c:}$ where $S_{i,c:} = \{u_c, u_{c+1}, \dots, u_D\}$ without concatenation. We define $R_{:c*}$ and $R_{c:}$ equivalently over unbannered SERPs. 
Finally let $S^*_{i,c} = (u_j \in S_{i,:c*}, u_k \in S_{i,:c*}) \cup (u_j \in S_{i,:c*}, u_k \in S_{i,c:}) \cup (u_j \in S_{i,c:}, u_k \in S_{i,c:})$. 
Again, we define $R^*_{i,c}$ equivalently using $R_{:c*}$ and $R_{c:}$. 
Using this formulation, we consider three questions about Google's system for placing low-quality banners:
\begin{packed_enumerate}
    \item : $\forall q \in Q$ does there exist a $u$ such that $\exists u \in \left( \cap_{i=1}^N S_i \right) \setminus \cup R$?
    \item : $\forall q \in Q$ does there exist a $(u_j, u_k)$ such that $\exists (u_j, u_k) \in \left(\cap_{i=1}^N S_i \right) \setminus \cup R$?
    \item : $\forall q \in Q$ does there exist a $c$ such that $ \exists (u_j, u_k) \in \left( \cap_{i=1}^N S^*_{i,c} \right) \setminus \cup R^*_c$?
\end{packed_enumerate}

\subsubsection{Rapid Data Collection Pilot}
\label{sec:appendix-consistency-pilot}

For the focused dataset we collected in June 2024, which used the subset of queries that produced a low-quality banner crawl-1 on a more rapid schedule (once every four hours), we had two gaps in the 73 time steps due to data collection issues. 
However, we also collected a pilot version of this dataset in which we searched the same query subset over 34 steps (once every hour-and-a-half) between March 10 and March 12, 2024.
This dataset had no gaps in its collection and three queries were dropped for not returning search results in any time step.
On average, 4\% of queries that had a low-quality banner in one time step did not have that banner type in the next time step, which is slightly higher than the 3.2\% change we found in June 2024.

The banner distribution was more bimodal than for our main dataset, with 116 queries having a banner for all 34 time steps, and 115 not having a banner at any time step. 
The remaining 67 queries had between 1 and 33 banners. 
We found the minimum Jaccard similarity across the 34 time steps banners was 0.82 and the mean was 0.89, which are close to the 0.79 min and 0.88 mean we found in the larger June 2024 dataset (Figure~\ref{fig:jaccard_qry_appendix}).
We recalculated $RBO_k$ on this data using a window size of 12 and again find the same general trend: every group is monotonically decreasing (Figure~\ref{fig:rbogroups_appendix}). 
Similar to the June 2024 dataset, we found that the queries that consistently returned a banner in every time step had the most stable search results. 
However, the relative ordering of other groups was somewhat different: queries that returned 16--33 banners across all time steps had the least stable search results in March 2024, but queries that returned zero banners across all time steps were the least stable in June 2024.

\begin{figure}[tb!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 0 3cm 3cm},clip]{figs/plots/jac_sim_qry_34.png}
        \caption{Heatmap cells show the pairwise Jaccard similarity of the set of queries that returned a quality banner over 34 time-steps.}
        \label{fig:jaccard_qry_appendix}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/plots/quality_RBO_groups_34.png}
        \caption{\(RBO_k\) (x-axis) over 12 window sizes (y-axis) by the mean RBO over all queries in the 34 time-steps.}
        \label{fig:rbogroups_appendix}
    \end{subfigure}
    \caption{Repeated Jaccard Similarity and RBO plots on 34 time steps with no collection gaps in March 2024 (Appendix~\ref{sec:appendix-consistency-pilot})}
\end{figure}

In both the 73 and 34 time-step datasets, we observe a bimodal distribution with the majority of queries having banners at all time steps or no time steps. These bimodal distributions make it unlikely that the instability in Google's low-quality banner placement could be the result of a stochastic process or of server latency issues. 
In the 73 timestep data, 166 queries never displayed low-quality banners and 40 always displayed banners. 
In the 34-timestep data, 115 queries never produced a low-quality banner, 116 always produced a low-quality banner, and 67 produced low-quality banners between 1 and 33 times. 
If either a stochastic process or server latency issues were responsible for inconsistencies in Google's low-quality banner placement, we would expect to see a more uniform distribution of banner instability over all time steps.

\subsubsection{Stability Experiments}
\label{sec:appendix-consistency-experiments}
We discussed the desirable property of ``stability'' in Methods and presented several examples of low-quality banner status changing as a result of minor query permutations (e.g., pluralizing or de-pluralizing nouns). In August 2024, we decided to perform several experiments to evaluate the stability of queries under various permutations. For example, in each of the 301 low-quality bannered queries, we pluralized all nouns and noun phrases, introduced keyboard typos with 10\% probability, and added or removed quotes for every query. To our surprise, not a single query generated a low-quality banner: only low-relevance banners remained. We initially assumed it may have been a geographic issue, so queries were repeated on computers in the east and west coasts of the US, and again, no low-quality banners were returned.

\subsection{GNN Models}
\label{sec:appendix-gnn}

\subsubsection{DistilBERT Model Results}

While the DistilBERT classifier yielded high accuracy on a small, balanced ``banner'' vs. ``non-banner'' query subset, we also found that it did not learn generalizable patterns from the query text data alone. When we ran inference over the 1.4M unlabeled queries using the fine-tuned DistilBERT model, we found its 100 most confident banner predictions all contained the presence of quotation marks, despite many of those queries not returning unreliable content (e.g., \textit{``phishing attack''}, \textit{``secretary of state''}, and \textit{``data breach''}). Reliance on such quotations is not ideal, because once a falsehood is subject to widespread fact-checking, a conspiratorial query can surface reliable or helpful results. In contrast, our graph neural network approach allowed us to condition the probability of a banner on both the text of the query and the reliability of its returned domains.

\subsubsection{Model Output Annotations}
Among the set of queries that produced a low-quality banner in October 2023, 141 appeared to be aimed at directing users to a network of strange websites aimed at boosting awareness of the ``Children's Immortality Project'' (CIP), which argues that children only die because adults teach them about death (see Appendix~\ref{sec:appendix-cip}). 
These queries often contained quoted keywords present in websites and were often surrounded by quotes (e.g., ``creating all children to die'', ``the virtue bios and mortality resolution'' and clarity genius of sedona ``interneted''). 
Given that these queries generated a large portion of the observed low-quality banners, the annotators also evaluated whether each query produced a SERP that surfaced websites related to the CIP.
We found substantial agreement between annotators on this task as well ($\kappa = 0.80$), with the homogeneous and heterogenous models producing similar results, but the homogeneous model identified more in its top 20 predictions (Table~\ref{tbl:precisionCIP}).

\begin{table}[t!]
\centering
\small
\caption{Precision of the discovery process for each model at the top 5, 10, and 20 predictions. We additionally report the number of Children's Immortality Project (CIP) queries returned in the top 20 predictions.}
\label{tbl:precisionCIP}
\input{tables/gnn-precision-metrics}
\end{table}

\subsubsection{Out-of-Sample Banner Identification}
\label{sec:appendix-gnn-oos}

In crawl-2 there were 74 queries that received a low-quality banner that did not have a low-quality banner in crawl-1. 
To evaluate out-of-sample banner identification, we ran inference over all queries that did not receive a low-quality banner in crawl-1, and evaluated the number of those queries that received a low-quality banner at time step 2, sorted by confidence. 
Similar to our other model validation tests, we again find that the GNN models outperformed the DistilBERT model (Table~\ref{tbl:OOD_comparison}).

\begin{table}[t!]
    \centering
    \caption{The Homogeneous and Heterogeneous GNNs best identify new out-of-sample bannered queries. The number of the 74 new low-quality banner queries present in each model's $K$ most confident unreliable predictions.}
    \label{tbl:OOD_comparison}
    {\renewcommand{\arraystretch}{1.2}
    \input{tables/gnn-out-of-sample}
    }
\end{table}

\subsection{Query Examples}
\label{sec:appendix-examples}

\subsubsection{``Vril Lizards''}
\label{sec:appendix-examples-vril}

In a qualitative exploration of quality-bannered queries, we find that banner presence can also depend on relatively minor changes in a query. 
For example, we observed that the query \textit{vril lizards}, which refers to mythical parasitic lizards that some websites claim control the brains of celebrities, frequently returned a banner, but the query \textit{vril lizard} (where lizard is singular instead of plural) did not. 
However, when quotation marks were added to the singular query, ``\textit{vril lizard}'' Google did return a banner (Figure~\ref{fig:appendix-vril}, 2nd panel).
Although the quotes did not change the semantic content of the query, and the results for both queries contained multiple webpages that were either identical or exhibited nearly identical text content, only the quoted one received a banner. 
We additionally observed several other queries that only returned banners when quotation marks were placed around the query. 
Stranger still, when ``\textit{vril lizard}'' was searched the next day, its banner was gone. Together, these observations provide some evidence that Google's low-quality banners do not appear to depend solely on the semantic content of a query. 

\begin{figure}[tb!]
\centering
\includegraphics[width=\textwidth]{figs/plots/vril_joined.png}
\caption{Search results for the query \textit{vril lizard} without (left screenshot) and with quotation marks (right two screenshots). All SERPs contain a similar set of URLs. The quoted version received a banner in one instance, but not in second screenshot taken a day later.}
\label{fig:appendix-vril}
\end{figure}

\subsubsection{``MRNA Prions''}
\label{sec:appendix-examples-prions}

One search query from our dataset, ``MRNA prions,'' is likely in reference to debunked claims stemming from a 2021 report on how MRNA vaccines could cause diseases like Alzheimers~\citep{mrnaprions2021}. 
As of September 2024, the top 10 search results for ``MRNA Prions'' still surfaced support for this narrative, including a Research Gate article titled ``COVID-19 RNA Based Vaccines and the Risk of Prion Disease,'' the author of which has also been published by SciVision Publishers, which is included on a list of Predatory Journals and Publishers (\nolinkurl{beallslist.net}).
The ``MRNA prions'' query returned a low-quality banner in 56 of the 73 timesteps in our temporally dense dataset from June 2024, and we found that two URLs could explain the majority of instances (43 of 56) in which that query produced a banner using our rank-cutoff approach (Appendix~\ref{sec:appendix-consistency}).

\subsection{August 2024 Core Update}
\label{sec:appendix-aug2024}

The disappearance of quality banners coincided with an August 2024 Google core update \citep{aug2024update}. In this section, we consider the question ``did Google remove quality banners because the August 2024 update removed all unreliable results?'' We find that while the update did significantly impact search results for formerly quality-bannered queries SERPs did not necessarily become more reliable.

For each query, for each of the 73 time steps from June 2024, we calculated the Jaccard similarity and RBO between the top 10 SERPs of the June data and the August data (Appendix~\ref{sec:appendix-consistency-experiments}). We found that the mean average Jaccard similarity over all queries and time-steps was 0.31 (max 0.75), and the mean RBO was 0.37 (max 0.70). This was only slightly less than the SERPs with low-quality banners from March 2024 (Appendix~\ref{sec:appendix-consistency-pilot}), where the average Jaccard similarity and RBO with the June data were 0.27 (max 0.69) and 0.27 (max 0.62), respectively.

In our results section, we stated that 25 queries that displayed between 1 and 72 banners had at least one URL appear in all bannered SERPs that never appeared in unbannered SERPs. We looked for these URLs in the August data, and found that 8 queries displayed those URLs strongly associated with banners in the June data, yet returned no banner. Additionally, we calculated average domain reliability of the March 10th crawl, June 7th crawl, and August crawl. This is a coarse-grained approach, as we do not have labels for 81--90\% of domains returned in SERPs, but this still provides some frame of comparison. We found that the average domain-level SERP reliability (where 1 is most reliable, and missing values are ignored) were 0.52 (March), 0.44 (June), and 0.47 (August). While the labeled August SERPs results were, on average, slightly more reliable than the June 7th SERPs, they were slightly less reliable than the March SERPs. This provides additional evidence that banners were not turned off in August because the problem was solved.

A more qualitative exploration revealed that while many search results changed between June and August, the August search results often did not correspond with an increase in overall SERP reliability. The query ``mrna prions'' (Appendix \ref{sec:appendix-examples-prions}), did not display the newstarget or 4chan URLs strongly associated with a banner in Figure \ref{fig:mrnaq1}, but returned three URLS we never observed in the June data: two links to a linkedin post and youtube video of posted by an apparent Australian anti-vaccine organization\footnote{\url{https://web.archive.org/web/20240326082456/https://healthallianceaustralia.org/}} and a link to an 8kun post encouraging users not to get vaccinated\footnote{\url{https://web.archive.org/web/20240703140131/https://8kun.top/freedomzine/res/15841.html}}.

\begin{figure}[tbh!]
\centering
\includegraphics[width=0.5\textwidth]{figs/plots/relevancy_aug2024.png}
\caption{Quality banners do not appear to have been subsumed by low-relevance banners. We display histograms of the distributions of the counts of low-relevance banners over 73 time-steps (teal) and the distribution of total banners (i.e., low-relevance + low-quality) (purple). In August 2024, the count of low-relevance banners we observed was lower than we observed across any of the 73 June 2024 time-steps.}
\label{fig:aug_relevance}
\end{figure}

We considered the possibility that quality banners had been subsumed inside low-relevance banners during the August 2024 update. 
To test this hypothesis, we created one histogram showing the total number of low-relevance banners for the 296 queries we analyzed over 73 time-steps from June 2024 and a second histogram containing the sum of the counts low-quality and low-relevance banners that appeared in each of those 73 time-steps (Figure~\ref{fig:aug_relevance}). 
If every quality banner were replaced with a low-relevance banners, we would expect that the total number of low-relevance banners in our August 2024 data should fall within or near the distribution of total banners (i.e., the sum of low-quality and low-relevance banners). 
This was not what we observed; rather, the number of low-relevance banners returned over the 296 queries in August (38) was lower than the count of low-relevance banners returned in any of the 73 time steps (the minimum was 41).

\subsection{Children's Immortality Project}
\label{sec:appendix-cip}

The self-proclaimed creator of the Children's Immortality Project (CIP) posted on many of his websites that Google was censoring him, and believed that by creating SEO websites, more people would learn of his message. 
We found hundreds of keyword-heavy SEO sites related to CIP that heavily linked to one another and often had questionable domain names (e.g, howtokillchildrenlegally-breed.blogspot.com\footnote{\url{https://web.archive.org/web/20240411055816/http://howtokillchildrenlegally-breed.blogspot.com/}}, keywordsarefortakingovertheinternet.blogspot.com\footnote{\url{https://web.archive.org/web/20240605114527/http://keywordsarefortakingovertheinternet.blogspot.com/}}, 
and google-bomb.com\footnote{\url{https://web.archive.org/web/20141217044958/http://google-bomb.com/}}), engendered discussion and speculation regarding possible intentions of these websites.\footnote{Explicit content: \url{https://web.archive.org/web/20100728210054/http://4chanarchive.org/brchive/dspl_thread.php5?thread_id=3595190\&x=Childrens+Immortality+Project}}

For each of the domains returned alongside a low-quality banner, we extracted the 10 domains which link most frequently to the target domain using Ahrefs. 
We used this data to create a bipartite network of second and top level domains to queries and ran Louvain community detection on the resulting graph.
One cluster stood out as highly interconnected and contained many of the domains amplifying the CIP (Figure \ref{fig:app_cip}). 
While we demonstrate that the backlink network displays some signals of coordination, better understanding how coordination manifests in search directives would be a promising avenue for future research. 
We note that not all of the queries associated with the CIP require a banner at the time of this writing, as some were related to keywords whose search rankings have become highly competitive over time, such as ``weaponized artificial intelligence.''

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.40\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/serp_net.png}
    \caption{Graph of the domains returned in low-quality SERPs and the 10 websites that 10 domains that most frequently link to each of them.}
    \label{fig:serp_network}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.56\textwidth}
    \includegraphics[width=\linewidth]{figs/plots/webmaster.png}
    \caption{The old landing page of the google-bomb.com, one of the Children's Immortality Project Websites, available at: \url{https://web.archive.org/web/20141217044958/http://google-bomb.com/}}
    \label{fig:cipwebmaster}
    \end{subfigure}
    \hfill
    \caption{Children's Immortality Project SERP network and landing page.}
    \label{fig:app_cip}
\end{figure*}

\end{appendices}

