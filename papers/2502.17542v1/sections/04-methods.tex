
\section{Methods and Data}
\label{sec:methods}

To conduct this study, we first collected a large dataset of search directives, which are defined as prompts to conduct an online search (Section~\ref{sec:methods-directives}). 
We then used the 1.4M search queries we found in that dataset to surface Google's warning banners (Section~\ref{sec:methods-serps}), evaluate their presence in the context of established query (Section~\ref{sec:methods-queries}) and domain-level metrics (Section~\ref{sec:methods-domains}) with logistic regression (Section~\ref{sec:methods-logit}), test their consistency over time (Section~\ref{sec:methods-stability}), and develop deep learning models to identify unlabeled data voids (Section~\ref{sec:methods-models}).

\subsection{Search Directives}
\label{sec:methods-directives}

We collected search directives from social media posts (Section~\ref{sec:methods-directives-posts}) to collect a diverse set of search queries (Section~\ref{sec:methods-directives-queries}) for our study.
By specifying a flexible linguistic strategy (prompts to conduct an online search) rather than specific content (search queries), search directives provide a useful tool for surfacing unspecified and unknown content.
The search queries used in search directives have been shown to cover a diverse array of topics, ranging from music, sports, and advertising, to medical misinformation about Ivermectin, an emerging conspiracy about the COVID-19 vaccine causing people to ``die suddenly,'' and a cryptocurrency scam~\citep{robertson2023identifying}. 
Although the potential harms of people being led into data voids like these have been well documented~\citep{golebiewski2019data}, few studies have examined how people can be led into data voids (\cite{tripodi2019devin}; \cite{tripodi2023your}), how to computationally identify data voids~\citep{flores-saviaga2022datavoidant}, or how to measure the bridge between social media and search engines more broadly (\cite{bode2018studying};\cite{lukito2020coordinating}; \cite{yarchi2021political}; \cite{zuckerman2021why}).
Rather than on relying on smaller sets of queries generated through surveys or  interviews, or medium-sized sets of queries generated via autocomplete (\cite{robertson2019auditing}; \cite{haak2023qbias}), our use of search directives allowed us to collect 1.4M unique queries without defining the topic space or a starting set of queries to expand upon.

\subsubsection{Social Media Posts}
\label{sec:methods-directives-posts}

We collected a total of 5.25M posts that contained a URL fragment (e.g. \nolinkurl{google.com/search}) leading to one of 25 popular search engines. 
This collection strategy allows for flexibility in subdomains, variability in URL parameters, and enabled us to easily and accurately extract search directive queries.
Following past work, we filtered out URLs that did not lead to a page of search results, including those that did not contain a known query parameter (e.g., ``\&q=\{query\}'' for Google Search) and those that contained a blank query, leaving 4M search directive posts, 4.17M URLs (posts can contain multiple URLs), and 1.44M unique queries that were created by 1.82M unique accounts over a 16.5-year window (2006 to 2023). 
Advancing on prior work that examined the five most popular modern search engines in the US (Google, Bing, DuckDuckGo, Yahoo, and Brave), we used a list of 25 search engines to collect our dataset (Google, Bing, DuckDuckGo, Yahoo, Brave, AOL, Ask, Baidu, Dogpile, Ecosia, Exalead, Excite, Hotbot, Lycos, Metacrawler, Mojeek, Petalsearch, Qwant, Sogou, Startpage, Swisscows, Webcrawler, Yandex, You, and Youdao), including search engines that are prominent outside of the US (e.g. Yandex), were prominent in the past (e.g. AOL and Ask), or newer search engines that feature large language models (e.g. You.com).

\subsubsection{Search Directive Queries}
\label{sec:methods-directives-queries}

Of the 4.17M posts that contained a URL fragment and a search query---which excludes links to search engine homepages that don't qualify as a search directive---we obtained a diverse sample of 1.44M unique queries that varied widely in terms of both their content and structure. While not representative of what people are searching for today, these queries cover a wide range of topics (including music, sports, and politics), were produced across a 16 year span and include event-driven bursts (e.g., around the ICC Men’s T20 World Cup 2016, a biannual cricket tournament). 
These queries also widely varied in terms of their length, with the average search directive query containing an average of 4.5 words, which is slightly longer than estimates of query length in the US, which find that 82\% of queries are 3 words or less~\citep{keyworddiscovery2020keyword}. 
The longest query in our dataset was 896 tokens long, and 234 (0.01\%) queries were only one character, often an emoji.
Notably, Google Search limits queries to 32 words, and that length is counted after processing by an unknown tokenizer. When a query is too long, Google adds a notice at the top of the search results which states: ``... (and any subsequent words) was ignored because we limit queries to 32 words.''
We provide the distribution of query lengths with and without truncation in Appendix~\ref{sec:appendix-descriptives-queries}, Figure~\ref{fig:query-length-distributions}.

\subsection{Search Engine Results Pages (SERPs)}
\label{sec:methods-serps}

We used open-source tools to collect our search results (Section~\ref{sec:methods-serps-collecting}), and an iterative approach to discovering and classifying Google's warning banners (Section~\ref{sec:methods-serps-banners}).
To evaluate the rate at which search directive queries produce warning banners and data voids, we used our set of 1.4M unique queries as the inputs for an approach known as the algorithm audit~\citep{sandvig2014auditing}, which typically involves collecting and examining the outputs of a black-box system based on some fixed set of inputs (\cite{bandy2021problematic}, \cite{metaxa2021auditing}, \cite{mustafaraj2020case}, \cite{vanhoof2022searching}).
In this case, the inputs are the search directive queries, the system is Google Search, and the outputs are the Search Engine Results Pages (SERPs) returned by Google. 

\subsubsection{Collecting and Parsing SERPs}
\label{sec:methods-serps-collecting}

For collecting the search results available for each query, we used WebSearcher~\citep{robertson2020websearcher}---an open source tool for collecting and parsing SERPs that has been used in prior algorithm audits of Google Search~\citep{mejova2022googling}---to conduct a search using each query in our set, store the corresponding HTML, and extract details about its corresponding search results (e.g. rank, URL, result type). 
We also extracted several elements other elements from the SERP, including Google's estimate for the total number of results it found for each query (across its entire index), which could also be indicative of a data void, as the search results for a query with few matches may be easier to manipulate due to the limited competition.
As with most algorithm audits of web search, this SERP dataset represents only what someone searching these queries might have seen at the time of our collection. 
We also searched from a fixed location and do not study localization effects~\citep{kliman-silver2015location}.
Details on the number of results we collected are available in Appendix~\ref{tab:crawl-counts}

\subsubsection{Identifying Warning Banners}
\label{sec:methods-serps-banners}

We initially identified banners by checking for the exact phrasing of each warning banner type, and then built phrase-agnostic HTML parsers to extract them across the entire dataset. 
For low-relevance banners, the phrasing was ``It looks like there aren't many/any great matches for your search''~\citep{tucker2020getting}. 
Low-quality banners contained similarly phrased language (``It looks like there aren't many great results for this search''), swapping only ``matches'' with ``results.'' 
In contrast with the low-relevance banners, we never observed the variation where ``many'' was replaced with ``any'' in the low-quality banners, which aligns with the ambiguity of the banner message (``some of [these results] may not have reliable information'', Figure~\ref{fig:banner_ex}), and how they were described in Google's blog post announcing their rollout (``This doesn't mean that no helpful information is available, or that a particular result is low-quality''~\cite{nayak2022new}).
This reluctance to specify which search results are low-quality may also help explain why we never saw a low-quality banner for searches with a ``site:'' operator that restricted the search results to a specific web domain: doing so would remove the ambiguity of the judgment.
In contrast to these warnings about content, the rapidly-changing banner stated: ``It looks like the results below are changing quickly''~\citep{sullivan2021new}.
Additional details and a screenshot of the rapidly-changing banner, as well as details and a screenshot of a low-relevance banner variant that only appeared in our last crawl, are available in Appendix~\ref{sec:appendix-banners}.

\subsection{Search Query Text Features} 
\label{sec:methods-queries}

To evaluate query content, we used a dictionary-based approach to identify queries containing partisan and polarizing search terms or conspiracy-related search terms (Section~\ref{sec:methods-queries-polcon}), and other text features, such as advanced query operators (Section~\ref{sec:methods-queries-operators}).

\subsubsection{Political and Conspiracy-related Lexicons} 
\label{sec:methods-queries-polcon}

To identify queries around controversial topics that could potentially lead to data voids, we used a dictionary-based approach to tag words and phrases associated with conspiracies and politics in prior work.
Specifically, we used:
\begin{inparaenum}[(1)] %
    \item \textcite{ballatore2015google}'s set of 96 conspiracy-related search queries,
    \item \textcite{mahl2021nasa}'s set of 44 conspiracy-related hashtags, and
    \item \textcite{urman2022where}'s set of 6 conspiracy-related search queries.
\end{inparaenum}
We also considered \textcite{haak2023qbias}'s set of QAnon-related search queries and autocomplete expansions, but the terms were too broad for our purposes.
For \textcite{mahl2021nasa}, we added non-hashtag versions of each item (e.g. ``\#vaccineskill'' becomes ``vaccines kill'') and excluded ``\#dew'', which refers to conspiracies around directed energy weapons but produces a high false positive rate due to the popularity of Mountain Dew, a soda brand.
Of the 14 conspiracy categories covered by this dictionary---including conspiracies about 9/11, chemtrails, and reptilians---we found at least one search directive query that mentioned each.
We also used two existing lexicons of polarized terms---one designed to capture ``polarized language''~\citep{simchon2022troll} and one designed to capture ``partisan cues''~\citep{hu2019auditing}---to classify search directive queries as politically related.
Combined, we used these lexicons to classify the full set of queries as related to politics (11.1\%), conspiracies (0.11\%), both (0.02\%), or neither (88.8\%).

\subsubsection{Advanced Query Operators} 
\label{sec:methods-queries-operators}

Advanced query operators allow searchers to specify additional constraints on their search results.
For example, adding ``site:dailycaller.com'' to a query (e.g. ``trump site:dailycaller.com'') will search for results containing that term (``trump'') only within that site (``dailycaller.com'').
When considering search directives as an attempt to exert indirect online influence, the use of these operators has strategic value in guiding people to specific content via a trusted search engine: searchers less familiar with these operators may not understand that their results have been filtered, and while some search engines (e.g., DuckDuckGo) display a message to informs users that such a filter is active, Google does not~\citep{robertson2023identifying}.
In total, 1.5\% of our 1.4M unique queries contained one of 11 advanced operators, and among those, the most common operator was ``site:'' (92.0\%), followed by ``inurl:'' (2.8\%), ``filetype:'' (1.9\%), ``intitle:'' (1.0\%), ``ext:'' (0.5\%), ``before:'' (0.5\%), ``source:'' (0.4\%), ``related:'' (0.3\%), ``allintitle:'' (0.3\%), ``after:'' (0.2\%), and ``allinurl:'' (0.1\%).
These queries varied widely in their content and complexity, some containing multiple operators and others containing only one.
For example, one query used the OR operator, parentheses, quotes, and 15 site operators: ``(mask | vaccine | "death count" | "case count") fraud and evidence election ( site:amac.us | site:townhall.com | site:heritage.org | site:thegatewaypundit.com | site:oann.com | site:scienceunderattack.com | site:conservativetribune.com |  site:thefederalist.com |site:greatamericandaily.com | \\ site:westernjournal.com | site:zerohedge.com | site:prageru.com | site:realclearpolitics.com | site:mercola.com | site:naturalnews.com ).''

\subsubsection{Query Language}
\label{sec:methods-queries-language}

As many queries are names, fragments, emojis, or are otherwise grammatically incorrect, determining the language of queries can be challenging, and some level of noise is inevitable. To get a general sense of query languages, we used the FastText library~\citep{joulin2016bag} to predict the most likely language for each query. Across all 1.4M queries, more than 1.2M were predicted to be in English. For the subset of 930K queries where the model returned a confidence of at least 0.5, 875K were predicted to be English. The second most common category in the high and low-confidence query sets was French, with 23K and 9K queries, respectively. Many of the queries that were classified as French appear to have been classified that way because they use French words or names in an otherwise English-speaking context. For example, of the 55 queries that contained the name ``De Blasio''---the former mayor of New York---FastText predicted that 24 were French, including  ``bill de blasio'' and ``bill de blasio drops groundhog video''. 
German was the third most popular language, and similar to the French classifications, many of the queries classified as German appeared be English queries associated with American politics like ``adolf hitler defund police'' and ``f\"{u}hrermccarthy''.

\subsection{Web Domain Features}
\label{sec:methods-domains}

To evaluate the average domain quality of the SERPs we collected, we extracted the second and top level domain names for each URL (e.g. https://cnn.com/politics $\rightarrow$ cnn.com) and merged them with domain metrics for quality and partisanship (Section~\ref{sec:methods-domains-quality}), as well as domain-level measures of web traffic and backlink counts (Section~\ref{sec:methods-domains-seo}).

\subsubsection{Measuring Domain Quality and Partisanship}
\label{sec:methods-domains-quality}

For domain quality, we used a set of scores that were recently developed to evaluate the quality of domains based on a compendium of similar existing metrics~\citep{lin2023high}.
These scores range from 0 to 1, with higher scores indicating higher quality, and cover 11,519 unique domains (we drop one duplicate that appears with and without a ``www.'' prefix).
When calculating domain quality at the SERP-level, we take the average score of the domains that appeared on the SERP.
Prior to calculating that average, we exclude three platform domains from the original set because their quality scores were low and hard to interpret.
Those domains and their scores were: \nolinkurl{youtube.com} (0.375), \nolinkurl{facebook.com} (0.407), and \nolinkurl{google.com} (0.668). 
For partisanship, we use the partisan news scores created in~\citep{robertson2018auditing} based on the relative proportion of Democrats and Republicans that shared a domain on Twitter.
These scores range from -1 (only shared by Democrats) to 1 (only shared by Republicans), with a score of 0 meaning only that a domain was shared by an equal number of Democrats and Republicans (i.e., not ``neutral''), and we used the rank-weighted average of these scores for each SERP.
Our domain-level measures of quality are coarse-grained and do not account for instances, for example, where unreliable domains publish accurate webpages, or vice versa.
The partisan scores we used are subject to similar limitations~\citep{green2025curation}.

\subsubsection{Search Engine Optimization (SEO) Metrics}
\label{sec:methods-domains-seo}

Given the relevance of Search Engine Optimization (SEO), a billion dollar industry aimed at improving websites' search rankings, to questions about search results, we also obtained SEO features (e.g. backlink counts) and traffic estimates from Ahrefs ({\nolinkurl{https://ahrefs.com}), a large SEO company.
Recent work using data from Ahrefs has shown that some of its features are predictive of misinformation, and suggests that its traffic estimates are reliable (\cite{carragher2024detection}, \cite{carraghermisinformation}, \cite{williams2023search}). 
We provide additional details on the SEO features we used, including their validity, use in past work, and descriptive characteristics, in Appendix~\ref{sec:appendix-descriptives-seo}.

\subsection{Logistic Regressions}
\label{sec:methods-logit}

In our logistic regression models, the presence of each banner type (low-relevance or low-quality, separately) was our dependent variable, and our independent variables included factors both related to the query and the SERP it produced.
We used the \texttt{statsmodels} library in Python to fit our logit models with L1 regularization. More specifically, we set the regularization parameter (alpha) to 0.1, and used the L-BFGS algorithm as the solver with a maximum of 10,000 iterations and convergence and zero tolerances set to $1e-8$. 
We trained separate models for each dependent variable and crawl because the rules governing their appearance could change over time.

Our models for predicting low-relevance banners demonstrated moderate fit, with pseudo-$R^2$ values of 0.39, 0.15, 0.38 for crawls 1, 2, and 3, respectively (Appendix~\ref{sec:appendix-logit-low-relevance}, Tables~\ref{tab:logit-low-relevance-crawl1},~\ref{tab:logit-low-relevance-crawl2},~\ref{tab:logit-low-relevance-crawl3}). In contrast, and likely due to the smaller sample size, our models for predicting low-quality banners demonstrated lower fit, with pseudo-$R^2$ values of 0.13 for both crawls 1 and 2 (Appendix~\ref{sec:appendix-logit-low-quality}, Tables~\ref{tab:logit-low-quality-crawl1} \& Table~\ref{tab:logit-low-quality-crawl2}).

\subsection{Banner Stability and Consistency}
\label{sec:methods-stability}

\subsubsection{Rapid Data Collection}
\label{sec:methods-stability-temporal}

To better understand the relationship between SERP results and quality banners, we collected SERPs for the 301 queries that produced a low-quality banner in crawl-1 approximately every four hours from June 7, 2024 to June 24, 2024. 
We dropped data from two collection intervals due to technical issues, and dropped five queries that did not return search results at any time step (truncated to four words: children's clarity about search engine..., international intelligence "search manipulation..., "the virtuebios and mortality resolution", "miembro del instituto de investigación..., why face masks don’t work...). 
This left us with 73 SERPs for each of the remaining 296 queries that returned a quality banner in our initial collection. 
The two gaps in this collection lasted for 45 hours, between June 11 and June 13, and for 15 hours, between June 17 and 18.
We find similar results in a pilot version of this dataset that we collected in March 2024 over 34 time steps (with about 1.5 hours between each) without any gaps (Appendix~\ref{sec:appendix-consistency-pilot}).

\subsubsection{URL Similarity}
\label{sec:methods-stability-rbo}

To measure the stability of returned (ranked) URLs given a query, we use Ranked-Biased Overlap (RBO)~\citep{webber2010similarity},
a metric designed to compare ranked indeterminate lists. While RBO can be weighted by a user-chosen probability $p$, we elect to take the average overlap, which corresponds to RBO with $p=1$. RBO calculates the agreement between ranked lists $S$ and $U$ at every level of depth from $1:D$ and takes the average. Formally, RBO with $p=1$ can be written as:

\begin{equation}
    RBO(S,T, p=1) = \frac{1}{D} \sum^D_{d=1} \frac{\mid S_{1:d} \cap U_{1:d} \mid}{d}
\label{eq:AO}
\end{equation}

As our interest is in the relative stability of the SERPs across all queries, we constructed a windowed RBO metric ($RBO_k$) to quantify SERP similarity across consecutive pulls. 
Let $X_i$ be the pairwise RBO similarity matrix for URLs returned in the SERPs of query $i$ over $T$ timesteps. We set a window size $K$, and define query-level windowed RBO similarity $\bar{x}_i$ as:

\begin{equation}
    \bar{x}_{i,K} = \frac{1}{2KT} \sum^T_{t=0} \sum^K_{k=1} (\mathbbm{1}_{[t-k \geq 0]} x_{t-k} + \mathbbm{1}_{[t+k \leq T]}x_{t+k})
\label{eq:rbosim}
\end{equation}

\noindent
where $\mathbbm{1}$ is the indicator function. If we set $k=1$, this would correspond to the average RBO of URLs returned at time $t$ with URLs returned $t-1$ and $t+1$ over all time steps. If this number were close to 1, that would indicate a high similarity---both in the set of URLs returned and their ranking---between consecutive SERPs for a given query. Conversely, if $RBO_k$ were 0, this would suggest high volatility, and would mean that a query never returns any of the same URLs in consecutive time-steps. Finally, we define $RBO_k$ as:

\begin{equation}
    RBO_k = \frac{1}{N}\sum_{i=1}^N \bar{x}_{i,K}
\label{eq:rbok}
\end{equation}
\noindent
Intuitively, while $\bar{x}_{i,K}$ measures the stability of a single query's SERPs results over $K$ consecutive pulls, $RBO_k$ simply measures the average SERP stability over all queries' SERPs over $K$ consecutive pulls.

\subsubsection{URL Dependencies}
\label{sec:methods-stability-dependency}

To find a consistent and simple logic that could explain low-quality banner variance in all queries, we considered three formalized questions that probe simple URL dependency conditions. 
Specifically, we attempt to determine for all queries whether or not there is 1) a single URL in all bannered SERPs but no unbannered SERPs, 2) a pair of URLs that appears in all bannered SERPs but no unbannered SERPs and 3) a pair of URLs conditioned on a rank cut-off that appear in all bannered SERPs but no unbannered SERPs, e.g., ``If $u_i$ always appears in the top 5 search results and $u_j$ always appears in at a position below 5, is there always a banner?'' 
We provide a more formal treatment of these questions in Appendix \ref{sec:appendix-consistency-formal}.

\subsection{Model Development}
\label{sec:methods-models}

We consider models conditioned on several different features to attempt to learn a mapping between our queries and Google's low-quality banners. 
The purpose of these models is 1) to determine whether we can learn Google's mapping from observed queries and SERPs to quality banners and 2) to assess the consistency and stability of Google's approach to placing low-quality banners. 

\subsubsection{Preprocessing}
\label{sec:methods-models-preprocessing}
Prior to modeling, we performed two preprocessing operations. 
First, we calculated embeddings for all queries using a multilingual Sentence-BERT model \citep{reimers-2019-sentence-bert}. 
Second, most results on a SERP include a title---the blue text on Google's SERPs that one clicks to reach a webpage. 
For each domain that appeared at least twice in the dataset, we create a single string that contains the domain name with a colon followed by titles sampled from the domain with replacement. 
The intention with this step is to create a feature with some, albeit shallow, notion of the topics covered by the domain. 

\subsubsection{Train and Test Datasets}
\label{sec:methods-models-dataset}
After quantitative and qualitative evaluations, we chose a 1:3 positive-to-negative sample to train our classifiers.
In predictions on unlabeled data, we observed that the DistilBERT model trained on a 1:1 positive-negative sample seemed to be over-relying on the presence of quotation marks; DistilBERT's most 100 confident banner candidate predictions contained quotation marks and some contained names of movies or books like ``the craft'' and ``scout mindset.'' We therefore elected to use a 1:3 positive-to-negative sample in order to provide a more diverse set of negative samples. The sample consists of the 301 queries that produced a low-quality banner in crawl-1, and an additional 903 queries that were randomly sampled from the queries that did not receive such a banner. 
We applied a stratified 80/20/20 split to the final set of 1,204 labeled and unlabeled queries.

\subsubsection{Query-Only DistilBERT Model}
\label{sec:methods-models-distilbert}
First, we consider a model that only includes query text. 
This assumes that banner presence is independent of a query's returned SERP, which we know from temporally-dense observations is not the case. 
We include a text-only model to determine how much signal is present in text in a given time period and as baseline by which we can compare more complex models. 
More formally, for a given query $T$, let $p(B | T)$ be the probability of a quality banner appearing, conditioned on query text. 
Our first model assumes that the probability of a banner depends only on semantic cues present in the query text $T$. 
Specifically, we fine-tune a DistilBERT model~\citep{sanh2019distilbert} to predict $p(B | T)$ for each query. 
The model is trained for two epochs using only the raw query text using an Adam optimizer with a learning rate of $2\mathrm{e}{-5}$ and a linear warm-up scheduler.

\subsubsection{Query-SERP GNN Models}
\label{sec:methods-models-gnn}

Next, we sought a model that could incorporate the assumption that two different queries with highly similar SERPs should likely have the same banner status. 
We therefore elected to use Graph Neural Networks (GNNs), as this allows us to propagate domain-level context into query representations (e.g., as done with different features in~\citep{williams2024dredge}).   %
To do so, we constructed two simple homogeneous (GNN\textsubscript{\emph{Hom}}) and heterogeneous (GNN\textsubscript{\emph{Het}}) graph neural network models which incorporate the assumption that the presence of a banner $p(B)$ depends on both the query text ($T$) and the content associated with returned domains $S$. 
These models aim to predict the conditional probability $p(B | S, T)$, integrating information from both sources. 
We represent the problem as a bipartite query-to-domain graph, with one node set corresponding to queries and the other to domains. 
Our approach is also similar to the vaccine-related query-weblog graphs used in~\citep{chang2024measuring}, but we elect to leverage DistilBERT for query embedding as our query topics span a broader range of domains.

Given a set of queries $Q = \{q_1, q_2, \dots, q_n\}$ and a set of returned SERP domains $D = \{d_1, d_2, \dots, d_n\}$ we construct a homogeneous, bipartite graph $\mathcal{G}_{Hom} = (V,E)$ where domains and queries are treated as the same node types. Both node types have text-based node features, and labels $Y$ are a binary variable indicating the presence of a banner on $Q$. We additionally construct a heterogeneous graph, $\mathcal{G}_{Het} = (V, E)$ where where $V$ and $E$ are associated with a node type mapping function $\Psi : V \rightarrow A$ and an edge type mapping function $\Phi : E \rightarrow \phi$. In our setting, the set of node types are $A = \{Q, D\}$ and the set of edge types are $\Phi = \{domain-to-query, query-to-domain\}$.%

To incorporate the assumption that ranking changes in $top-k$ SERP results (with URLs held constant) should not alter banner presence, we do not weight edges in our networks. To allow information to propagate between queries, we exclude ``pendulum'' domains---those that only appeared once in our SERP data. For each of those domains we sampled at most 10 ``titles''---the blue text that appears on Google search results (generally the title of the article or webpage)---embedded the titles with DistilBERT, and took the simple mean. 
Although this is a relatively simple and coarse-grained approach that excludes many relevant domain-level signals, we demonstrate its effectiveness and leave the incorporation of more nuanced domain-level features to future work.

Both models consist of a GraphSage convolution with a dropout of 0.5 and ReLU activation, followed by a second GraphSage convolution and a final log-softmax activation function~\citep{hamilton2017inductive}. This results in a model with 526k parameters. GNN\textsubscript{\emph{Het}} uses a heterogeneous GraphSage convolution~\citep{Fey2019}. In this setting, where there are only two node types with bi-directional ties, this doubles the number of model parameters to 1.05M. We use an Adam optimizer with $\eta = 1\mathrm{e}{-3}$, and a weight decay of $5\mathrm{e}{-4}$, Cross Entropy Loss, and a Cosine Annealing Learning Rate Scheduler with $\eta_{min} = 2\mathrm{e}{-5}$~\citep{loshchilov2016sgdr}. 

\subsubsection{Model Validation}
\label{sec:methods-models-validation}
We evaluated our models using standard Accuracy, F1, Precision, and Recall metrics (Table~\ref{tbl:GNNResults}).
However, given the small size of the dataset and the corresponding likelihood of over-fitting, we also include three different supplemental forms of validation. 
For each of these supplemental evaluations, we consider unlabeled queries that each model most confidently predicted as unreliable. 
To evaluate the success of each model in identifying candidate queries for receiving low-quality banners, we used them to predict warning banners in the subsequent crawl of 1.4M SERPs and examined average SERP domain quality scores over the most confident predictions (Figure~\ref{fig:qualwindow}), evaluated annotated precision\@K and the prevalence of Children's Immortality Project queries in top results (Table~\ref{tbl:precisionCIP}), and evaluated a case study around a frequently observed set of search directives (Appendix~\ref{sec:appendix-cip}).

Some SERPs that returned results for crawl-1 did not return results for crawl-2 (8.8K) and vice versa (83K). Additionally, as with crawl-1, we only include domains which appeared at least twice across all of crawl-2 when running inference. To make results comparable across crawls, 
we use the intersection of queries on which we successfully ran inference in each crawl (1.4M). Figure~\ref{fig:qualwindow} contains only predictions on queries in this intersecting set, i.e., SERPs on which we could run inference in both crawl-1 and crawl-2.

