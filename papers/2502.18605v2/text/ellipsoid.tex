\section{Efficient Computation with Linear Endomorphisms}
\label{sec:main}

The hardness results of the previous section highlight the need to restrict the set $\Phi$ in order to make meaningful progress. Our main result here establishes a polynomial-time algorithm when $\Phi$ contains only linear endomorphisms.

\begin{theorem}
    \label{th:elvi}
    If $\Phi$ contains only linear endomorphisms, the $\eps$-approximate $\Phi$-EVI problem can be solved in time $\poly(d, \log(B/\epsilon))$ given a membership oracle for $\cX$.
\end{theorem}

The proof relies on the ellipsoid against hope ($\eah$), and in particular, a recent generalization by~\citet{Daskalakis24:Efficient}. In a nutshell, the main deficiency in the framework covered earlier in~\Cref{sec:eah} is that one needs a separation oracle for $\cY$ (\Cref{theorem:eah}), where $\cY$ for us is the set of deviations $\Phi$. Unlike some applications, in which $\cY$ has an explicit, polynomial representation~\citep{Papadimitriou08:Computing}, that assumption needs to be relaxed to account for $\Philin$~\citep[Theorem 3.4]{Daskalakis24:Efficient}.

\citet{Daskalakis24:Efficient} address this by considering instead the $\either$ oracle. As the name suggests, for any $\vy \in \R^m$, it \emph{either} returns a hyperplane separating $\vy$ from $\cY$, or a good-enough-response $\vx \in \cX$. They showed that~\Cref{theorem:eah} can be extended under this weaker oracle (in place of $\ger$ and $\sep$); the formal version is given in~\Cref{th:eahrelax}.

In our setting, we consider the feasibility problem
\begin{align}\label{eq:evi-ellipsoid}
        \text{find}\quad \phi \in \Philin \qq{s.t.} \ip{F(\vx), \phi(\vx) - \vx} \le -\eps \quad \forall \vx \in \cX.
\end{align}
Equivalently,
\begin{align*}
        \text{find}\quad \mK \in \R^{d \times d} \qq{s.t.} \\\ip{F(\vx), \mK\vx - \vx} \le -\eps \quad &\forall \vx \in \cX, \\\mK \vx \in \cX \quad &\forall \vx \in \cX.
\end{align*}
This program is infeasible since, for any $\phi \in \Philin$, the fixed point $\vx$ of $\phi$ makes the left-hand side of the constraint $0$. And a certificate of infeasibility is an $\eps$-approximate $\Philin$-EVI solution. Thus, it suffices to show how to run the ellipsoid algorithm on \eqref{eq:evi-ellipsoid}. By~\Cref{th:eahrelax}, it suffices if for any $\mK \in \R^{d \times d}$, we can compute efficiently \emph{either}
\begin{itemize}%[noitemsep,topsep=0pt]
        \item some $\vx \in \cX$ such that $\mK\vx = \vx$ ($\ger$), {\em or}
        \item some hyperplane separating $\mK$ from $\Philin$ ($\sep$).
\end{itemize}
This is precisely the \emph{semi-separation oracle} solved by~\citet[Lemma~4.1]{Daskalakis24:Efficient}, stated below.

\begin{lemma}[\citealp{Daskalakis24:Efficient}]
    \label{lemma:semiseparation}
    There is an algorithm that takes as input $\mat{K} \in \R^{d \times d}$, runs in $\poly(d)$ time, makes $\poly(d)$ oracle queries to $\cX$, and either returns a fixed point $\cX \ni \vx = \mat{K} \vx$, or a hyperplane separating $\mat{K}$ from $\Philin$.
\end{lemma}

% An immediate consequence is that \emph{linear} correlated equilibria %undefined; explain what that term means??
% can be computed %found would be a better word than computed ??
% in polynomial time in convex games~\citep{Daskalakis24:Efficient}.

On a separate note, \Cref{th:elvi} only accounts for approximate solutions. We cannot hope to improve that in the sense that exact solutions might be supported only on irrational points even in concave maximization (\emph{cf.}~\Cref{prop:convex-equiv}).

\subsection{Regret minimization for EVIs on polytopes}

One caveat of~\Cref{th:elvi} is that it relies on the impractical $\eah$ algorithm. To address this limitation, we will show that $\Phi$-EVIs are also amenable to the more scalable approach of \emph{regret minimization}---albeit with an inferior complexity growing as $\poly(1/\epsilon)$.

Specifically, in our context, the regret minimization framework can be applied as follows. At any time $t \in \N$, we think of a ``learner'' selecting a point $\vx^{(t)} \in \cX$, whereupon $F(\vx^{(t)})$ is given as feedback from the ``environment,'' so that the utility at time $t$ reads $- \langle \vx^{(t)}, F(\vx^{(t)}) \rangle$. \emph{$\Phi$-regret} is a measure of performance in online learning, defined as
%
\begin{equation*}
    \phireg^{(T)} \defeq \max_{\phi \in \Phi} \sum_{t=1}^T \langle F(\vx^{(t)}), \phi(\vx^{(t)}) - \vx^{(t)} \rangle.
\end{equation*}
The uniform distribution $\mu$ on $\{ \vx^{(1)}, \dots, \vx^{(T)} \}$ is clearly a $\phireg^T/T$-approximate $\Phi$-EVI solution. 

In what follows, we will assume that $\cX$ is a polytope given explicitly by linear constraints, \ie, \begin{align*}
    \cX = \{ \vx \in \R^d : \mA \vx \le \vb \},
\end{align*}
where $\mA \in \Q^{m \times d}$ and $\vb \in \Q^m$ are given as input.

To minimize $\Phi$-regret, we will make use of the template by~\citet{Gordon08:No}, which comprises two components. The first is a fixed-point oracle, which takes as input a function $\phi \in \Philin$ and returns a point 
$\vx \in \cX$ with
$\vx = \phi(\vx)$; given that $\phi$ is linear, it can be implemented efficiently via linear programming. The second component is an algorithm for minimizing (external) regret over the set $\Philin$. In~\Cref{theorem:explicit-repres}, we devise a polynomial representation for $\Philin$:

\begin{theorem}
    \label{theorem:regret}
   For an arbitrary polytope $\cX$ given by explicit linear constraints, there is an explicit representation of $\Philin$ as a polytope with $O(d^2 + m^2)$  variables and constraints.
\end{theorem}
As a consequence, we can instantiate the regret minimizer operating over $\Philin$ with projected gradient descent.
\begin{corollary}
    \label{cor:regret}
    There is a deterministic algorithm that guarantees $\philinreg^{(T)} \leq \epsilon$ after $\poly(d, m)/\eps^2$ rounds, and requires solving a convex quadratic program with $O(d^2+m^2)$ variables and constraints in each iteration.
\end{corollary}
% \begin{corollary}
%     There is a randomized algorithm that guarantees $\philinreg^{(T)} \leq \epsilon$ after $\qty(\poly(d, m) + O(\log(1/\delta)))/\eps^2$ with probability at least $1/\delta$, and requires solving a linear program with $O(d^2+m^2)$ variables and constraints on each iteration\todo{}
% \end{corollary}

An additional benefit of~\Cref{cor:regret} compared to using $\eah$ is that the former is more suitable in a decentralized environment---for example, in multi-player games (\emph{cf.}~\Cref{example:CCE}). There, \Cref{cor:regret} corresponds to each player running their own independent no-regret learning algorithm. Even in this setting, our algorithms actually yield an improvement over the best-known algorithms for minimizing $\Philin$-regret over explicitly-represented polytopes: the previous state of the art, due to \citet{Daskalakis24:Efficient}, requires running the ellipsoid algorithm on each iteration, which is slower than quadratic programming (\Cref{sec:repre}).