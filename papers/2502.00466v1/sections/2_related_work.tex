\vspace{-2em}
\section{Related Work}

\subsection{Diffusion Models}
\vspace{-0.7em}
Diffusion models have revolutionized high-resolution image generation through their noise-reversal process. Foundational works including DDPM \cite{ho2020ddpm} and DDIM \cite{song2021ddim} established core principles for subsequent developments. Score-based models~\cite{song2019generative, song2021scorebased} enhanced sampling efficiency through gradient estimation of data distributions, while energy-based models~\cite{du2024reducereuserecyclecompositional} introduced robust optimization properties through probabilistic state modeling. The application of diffusion models in reinforcement learning has expanded significantly. These models serve as policy networks for efficient offline learning~\cite{wang2023diffusion, ajay2023is, pearce2023imitating}, enable diverse strategy generation in planning tasks~\cite{janner2022planningdiffusionflexiblebehavior, liang2023adaptdiffuser}, and provide novel approaches to reward modeling~\cite{nuti2023extracting}. MetaDiffuser~\cite{ni2023metadiffuserdiffusionmodelconditional} demonstrated their effectiveness as conditional planners in offline meta-RL, while other works have utilized diffusion models for trajectory modeling and synthetic experience generation~\cite{lu2023synthetic}.
\vspace{-0.7em}

\subsection{Model-based RL and Generative Game Engines}
\vspace{-0.7em}
World models serve as a fundamental component in model-based RL and enable sample-efficient and safe learning through simulated environments. SimPLe \cite{Kaiser2020SimPLe} established the groundwork by introducing world models to the Atari domain and proposing the Atari 100k benchmark. Dreamer \cite{Hafner2020Dreamer} advanced this field through reinforcement learning from latent space predictions, which DreamerV2 \cite{hafner2021DreamerV2} further refined with discrete latents to mitigate compounding errors. DreamerV3 \cite{hafner2024DreamerV3} achieved a significant milestone by demonstrating human-level performance across multiple domains with fixed hyperparameters. Recent architectural innovations have expanded world model capabilities. TWM \cite{robine2023TWM} and STORM \cite{zhang2023storm} incorporated Transformer architectures for enhanced sequence modeling, while IRIS \cite{micheli2023iris} developed a discrete image token language for structured learning. R2I \cite{samsami2024r2i} addressed fundamental challenges in long-term memory and credit assignment. 
The integration of generative modeling approaches has further advanced world model capabilities. DIAMOND \cite{alonso2024diamond} marked a significant breakthrough by incorporating diffusion models, which achieved superior visual fidelity and state-of-the-art performance on the Atari 100k benchmark. This success inspired broader applications of generative approaches in interactive environments.
Another line of research explores training generative world engines using pre-collected datasets instead of RL-in-the-loop learning. GameGAN \cite{kim2020GameGAN} pioneered this direction through GAN-based environment modeling, while Genie \cite{bruce2024genie} advanced these capabilities by generating complex platformer environments from image prompts. GameNGen \cite{valevski2024diffusionmodelsrealtimegame} established new standards for visual fidelity and scalability through diffusion-based environment simulation. It is important to note that these generative world engines do not involve RL-in-the-loop training and are therefore orthogonal to our work.
\vspace{-0.7em}

\subsection{State Space Models in Reinforcement Learning}
\vspace{-0.7em}
SSMs have demonstrated remarkable effectiveness in modeling long-term dependencies and structured dynamics. Foundational works \cite{hippo, gu2022efficiently, gupta2022diagonal, gu2022parameterizationinitializationdiagonalstate, smith2023simplified, hasani2023liquid, gu2024mamba, dao2024transformersssmsgeneralizedmodels} introduced efficient sequence modeling approaches. Recent applications in RL include structured state space models for in-context learning~\cite{lu2023structured}, DecisionMamba~\cite{huang2024decisionmamba}'s adaptation of Decision Transformer~\cite{chen2021decision} architecture, and Drama~\cite{anonymous2025drama}'s integration of SSMs in world model learning.
%
Our work advances the state-of-the-art by synergistically combining diffusion-based world models with state space models. While previous works have explored these approaches separately, EDELINE demonstrates that their integration enables superior temporal consistency and extended imagination horizons in world model learning. This architectural innovation enhances the visual fidelity of diffusion models through the integration of SSMs' efficient temporal processing capabilities, which addresses the key limitations of existing world model approaches.
