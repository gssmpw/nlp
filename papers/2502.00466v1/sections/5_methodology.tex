\section{Methodology}

\begin{figure*}[t]
\begin{minipage}{0.63\textwidth}
\centering
\includegraphics[width=\linewidth]{imgs/architecture.pdf}
\label{fig:edeline_architecture}
\end{minipage}
\hfill
\begin{minipage}{0.35\textwidth}
\vspace{-2em}
\caption{\textbf{Framework Overview of EDELINE.}} 
      The model integrates three principal components: 
      (1) An U-Net-like \textit{Next-Frame Predictor} enhanced by adaptive group normalization and cross-attention mechanisms,
      % A \textit{Next-Frame Predictor} constructed with a U-Net architecture, enhanced by adaptive group normalization and cross-attention mechanisms, 
      (2) \textcolor{black}{A \textit{Recurrent Embedding Module} built on Mamba architecture for temporal sequence processing, and}
      % A \textit{Recurrent Embedding Module} built on Mamba architecture for temporal sequence processing through observation encoding and embedding layers, and
      (3) A \textit{Reward/Termination Predictor} implemented through linear layers. The EDELINE framework uses shared hidden representations across the components for efficient world model learning.
\end{minipage}\vspace{-1em}
\end{figure*}

Conventional diffusion-based world models \cite{alonso2024diamond} demonstrate promise in learning environment dynamics yet face fundamental limitations in memory capacity and horizon prediction consistency. To address these challenges, this paper presents EDELINE, as illustrated in Fig~\ref{fig:edeline_architecture}, a unified architecture that integrates state space models (SSMs) with diffusion-based world models. EDELINE's core innovation lies in its integration of SSMs for encoding sequential observations and actions into hidden embeddings, which a diffusion model then processes for future frame prediction. This hybrid design maintains temporal consistency while generating high-quality visual predictions. A Convolutional Neural Network based actor processes these predicted frames to determine actions, thus enabling autoregressive generation of imagined trajectories for policy optimization.

% This section presents EDELINE's world model architecture and training methodology, with emphasis on the SSM integration capabilities for enhanced memory capacity and imagination consistency in diffusion-based world models. The subsequent discussion details the mechanisms of actor-critic utilization of imagined trajectories in pixel space for efficient and effective policy learning.

\subsection{World Model Learning}

The core architecture of EDELINE consists of a \textit{Recurrent Embedding Module {(REM)}} $f_\phi$ that processes the history of observations and actions $(o_0, a_0, o_1, a_1, ..., o_t, a_t)$ to generate a hidden embedding $h_t$ through recursive computation. This embedding enables the \textit{Next-Frame Predictor} $p_\phi$ to generate predictions of the subsequent observation $\hat{o}_{t+1}$. The architecture further incorporates dedicated \textit{Reward and Termination Predictors} to estimate the reward $\hat{r}_t$ and episode termination signal $\hat{d}_t$ respectively. The trainable components of EDELINE's world model are formalized as: \vspace{-2em}
\begin{itemize} [itemsep=3pt, parsep=0pt]
    \item Recurrent Embedding Module: $h_t = f_\phi(h_{t-1}, o_t, a_t)$
    \item Next-Frame Predictor: $\hat{o}_{t+1} \sim p_\phi(\hat{o}_{t+1}|h_t)$
    \item Reward Predictor: $\hat{r}_t \sim p_\phi(\hat{r}_t|h_t)$
    \item Termination Predictor: $\hat{d}_t \sim p_\phi(\hat{d}_t|h_t)$
\end{itemize}

\subsubsection{Recurrent Embedding Module 
% \josout{(REM)}
}

While DIAMOND, the current state-of-the-art in diffusion-based world models, relies on a fixed context window of four previous observations and actions sequence, the proposed EDELINE architecture advances beyond this limitation through a recurrent architecture for extended temporal sequence processing. Specifically, we provide theoretical evidence in Theorem~\ref{the:information_retention_superiority} to support that recurrent embedding can preserve more information compared with stacked frames inputs. At each timestep $t$, the Recurrent Embedding Module processes the current observation-action pair $(o_t, a_t)$ to update a hidden state $h_t = f_\phi(h_{t-1}, o_t, a_t)$.
% , expressed as follows:
% \begin{equation}
% h_t = f_\phi(h_{t-1}, o_t, a_t).
% \end{equation}
The implementation of REM utilizes Mamba~\cite{gu2024mamba}, an SSM architecture that offers distinct advantages for world modeling. This architectural selection is motivated by the limitations of current sequence processing methods in deep learning. Self-attention-based Transformer architectures, despite their strong modeling capabilities, suffer from quadratic computational complexity which impairs efficiency. Traditional recurrent architectures including Long Short-Term Memory (LSTM)~\cite{HochSchm97} and Gated Recurrent Unit (GRU)~\cite{69e088c8129341ac89810907fe6b1bfe} experience gradient instability issues that affect dependency learning. In contrast, SSMs provide an effective alternative through linear-time sequence processing coupled with robust memorization capabilities via their state-space formulation. The adoption of Mamba emerges as a promising choice due to its demonstrated effectiveness in modeling temporal patterns across various sequence modeling tasks. Section~\ref{subsec:ablation_studies} presents a comprehensive ablation study that evaluates different architectural choices for the REM.

\subsubsection{Next-Frame Predictor}

While motivated by DIAMOND's success in diffusion-based world modeling, EDELINE introduces significant architectural innovations in its Next-Frame Predictor to enhance temporal consistency and feature integration. At time step $t$, the model conditions on both the last $L$ frames and the hidden embedding $h_t$ from the Recurrent Embedding Module to predict the next frame $\hat{o}_{t+1}$. The predictive distribution $p_\phi(o^0_{t+1}|h_t)$ is implemented through a denoising diffusion process, where $D_\phi$ functions as the denoising network. Let $y_t^{\tau} = (\tau, o^0_{t-L+1}, ..., o^0_t, h_t)$ represent the conditioning information, where $\tau$ represents the diffusion time. The denoising process can be formulated as $o^0_{t+1} = D_\phi(o^{\tau}_{t+1}, y_t^{\tau}).$
% then be formulated as follows:
% \begin{equation}
%     o^0_{t+1} = D_\phi(o^{\tau}_{t+1}, y_t^{\tau}).
% \end{equation}
To effectively integrate both visual and hidden information, $D_\phi$ employs two complementary conditioning mechanisms. First, the architecture incorporates \cite{AGN} layers within each residual block to condition normalization parameters on the hidden embedding $h_t$ and diffusion time $\tau$, which establishes context-aware feature normalization \cite{AGN}. This design significantly extends DIAMOND's implementation, which limits AGN conditioning to $\tau$ and action embeddings only. The second key innovation introduces cross-attention blocks inspired by Latent Diffusion Models (LDMs), which utilize $h_t$ and $\tau$ as context vectors. The UNet's feature maps generate the query, while $h_t$ and $\tau$ project to keys and values. This novel attention mechanism, which is absent in DIAMOND, facilitates the fusion of spatial-temporal features with abstract dynamics encoded in $h_t$. The observation modeling loss $\mathcal{L}_{\text{obs}}(\phi)$ is defined based on Eq.~(\ref{eq:d_loss}), and can be formulated as follows:
\begin{equation}
\mathcal{L}_{\text{obs}}(\phi) = \mathbb{E}\left[\|D_\phi(o^{\tau}_{t+1}, y_t^{\tau}) - o^0_{t+1}\|^2\right].
\end{equation}
\subsubsection{Reward / Termination Predictor}
EDELINE advances beyond DIAMOND's architectural limitations through an integrated approach to reward and termination prediction. Rather than employing separate neural networks, EDELINE leverages the rich representations from its REM. The reward and termination predictors are implemented as multilayer perceptrons (MLPs) that utilize the deterministic hidden embedding $h_t$ as their conditioning input. This architectural unification enables efficient representation sharing across all predictive tasks. EDELINE processes both reward and termination signals as probability distributions conditioned on the hidden embedding: $p_\phi(\hat{r}_t|h_t)$ and $p_\phi(\hat{d}_t|h_t)$ respectively. The predictors are optimized via negative log-likelihood losses, expressed as:
\begin{equation}
\mathcal{L}_{\text{rew}}(\phi) = -\ln p_\phi(r_t|h_t),
% \end{equation}
% \begin{equation}
\mathcal{L}_{\text{end}}(\phi) = -\ln p_\phi(d_t|h_t).
\end{equation}
This unified architectural design represents an improvement over DIAMOND's separate network approach, where reward and termination predictions require independent representation learning from the world model. The integration of these predictive tasks with shared representations enables REM to learn dynamics that encompass all relevant aspects of the environment. The architectural efficiency facilitates enhanced learning effectiveness and better performance.
\vspace{-0.5em}

\input{tables/atari100k_table}

\subsubsection{EDELINE World Model Training}
The world model integrates an innovative end-to-end training strategy with a self-supervised approach. EDELINE extends the harmonization technique from HarmonyDream \cite{ma2024harmonydream} through the adoption of harmonizers $w_o$ and $w_r$, which dynamically balance the observation modeling loss $\mathcal{L}_{\text{obs}}(\phi)$ and reward modeling loss $\mathcal{L}_{\text{rew}}(\phi)$. This adaptive mechanism results in the total loss function $\mathcal{L}(\phi)$:
\begin{equation}
\label{eq:total_loss}
\begin{split}
\mathcal{L}(\phi) = w_0\mathcal{L}_{\text{obs}}(\phi) &+ w_r\mathcal{L}_{\text{rew}}(\phi) + \mathcal{L}_{\text{end}}(\phi) \\
    &+ \log(w_o^{-1}) + \log(w_r^{-1})
\end{split}
\end{equation}
To optimize computational efficiency while ensuring robust learning, the Next-Frame Predictor learns to utilize hidden embeddings from any timestep through strategic random sampling for $\mathcal{L}_{\text{obs}}(\phi)$. For a sequence of length $T$, 
% this computation 
it follows:
\begin{equation}
\mathcal{L}_{\text{obs}}(\phi) = \|\hat{o}^0_{t+1} - o_{t+1}\|^2,
\end{equation}
where $i \sim \text{Uniform}\{1,2,\ldots,T-1\}, \quad \hat{o}^0_{t+1} \sim p(\hat{o}^0_{t+1} \mid h_t).$ For reward and termination prediction, EDELINE utilizes cross-entropy losses averaged over the sequence, which can be formulated as:
% \begin{equation}
    $\mathcal{L}_{\text{rew}}(\phi) = \frac{1}{T}\sum_{t=1}^T \text{CrossEnt}(\hat{r}_t, r_t),$
% \end{equation}
% \begin{equation}
    $\mathcal{L}_{\text{end}}(\phi) = \frac{1}{T}\sum_{t=1}^T \text{CrossEnt}(\hat{d}_t, d_t).$
% \end{equation}
This unified training approach, combining random sampling strategies with dynamic loss harmonization, demonstrates superior efficiency compared to DIAMOND's separate network methodology, as validated in our results presented in Section 6. Moreover, the quantitative analysis presented in Appendix~\ref{appendix:training_time_profile} reveals substantial reductions in world model training duration.
\vspace{-0.5em}

\subsection{Agent Behavior Learning}
To enable fair comparison and demonstrate the effectiveness of EDELINE's world model architecture, the agent architecture adopts the same optimization framework as DIAMOND. Specifically, the agent integrates policy $\pi_\theta$ and value $V_\theta$ networks with REINFORCE value baseline and Bellman error optimization using $\lambda$-returns~\cite{alonso2024diamond}. The training framework executes a procedure with three key phases: experience collection, world model updates, and policy optimization. This method, as formalized in Algorithm~\ref{alg:edeline}, follows the established paradigms in model-based RL literature~\cite{Kaiser2020SimPLe,Hafner2020Dreamer,micheli2023iris,alonso2024diamond}. To ensure reproducibility, we provide extensive details in the Appendix, with documentation of objective functions, and the hyperparameter configurations in Appendices~\ref{appendix:rl_objectives}, \ref{appendix:hyper}, respectively. \vspace{-2em}
