\section{Conclusions}

In this work, we addressed the limitations of current diffusion-based world models in handling long-term dependencies and maintaining prediction consistency. Through the integration of Mamba SSMs, EDELINE effectively processed extended observation-action sequences through its recurrent embedding module, which enabled adaptive memory retention beyond fixed-context approaches. The unified framework eliminated architectural separation between observation, reward, and termination prediction, which fostered efficient representation sharing. Dynamic loss harmonization further mitigated optimization conflicts arising from multi-task learning. Extensive experiments on Atari100K, MiniGrid, and VizDoom validated EDELINE's state-of-the-art performance, with significant improvements in quantitative metrics and qualitative prediction fidelity.