\section{Background}
\vspace{-0.5em}
We provide additional background material on score-based diffusion models and multi-task world model learning in Appendix~\ref{appendix:additional_background}. In this Section, we focus on the essential concepts necessary for understanding our EDELINE framework.
\vspace{-0.5em}

\subsection{Reinforcement Learning and World Models}
\vspace{-0.5em}
The problem considered in this study focuses on image-based reinforcement learning (RL), formulated as a Partially Observable Markov Decision Process (POMDP)~\cite{strm1965OptimalCO} defined by tuple $(S, A, O, T, R, O, \gamma)$. Our formulation specifically considers high-dimensional image observations as inputs, as described in Section~1. The state space $S$ comprises states $s_t \in S$, while the action space $A$ can be either discrete or continuous with actions $a_t \in A$. The observation space $O$ contains image observations $o_t \in \mathbb{R}^{3 \times H \times W}$. A transition function $T: S \times A \times S \rightarrow [0,1]$ characterizes the environment dynamics $p(s_{t+1}|s_t, a_t)$, while the reward function $R: S \times A \times S \rightarrow \mathbb{R}$ maps transitions to scalar rewards $r_t \in \mathbb{R}$. The observation function $O: S \times O \rightarrow [0,1]$ establishes observation probabilities $p(o_t|s_t)$.
%
The objective centers on learning a policy $\pi$ that maximizes the expected discounted return $\mathbb{E}_\pi[\sum_{t\geq0} \gamma^t r_t]$, with discount factor $\gamma \in [0,1]$. Model-based Reinforcement Learning (MBRL)~\cite{Sutton1988LearningTP} achieves this objective by learning a world model that encapsulates the environment dynamics $p(o_{t+1}, r_t|o_{\leq t}, a_{\leq t})$. MBRL enables learning in imagination through three systematic stages: (1) collecting real environment interactions, (2) updating the world model, and (3) training the policy through world model interactions.

\subsection{Linear-Time Sequence Modeling with Mamba}
\vspace{-0.5em}
SSMs~\cite{gu2022efficiently} provide an alternative paradigm to attention-based architectures for sequence modeling. The Mamba architecture~\cite{gu2024mamba} introduces a selective state space model that offers linear time complexity and efficient parallel processing, which employs variable-dependent projection matrices to implement its selective mechanism, thus overcoming the inherent limitations of computational inefficiency and quadratic complexity in conventional SSMs \cite{hippo, gu2022efficiently, smith2023simplified, gu2024mamba}. The foundational mechanism of Mamba is characterized by a linear continuous-time state space formulation via first-order differential equations as follows:
\begin{equation}
\begin{aligned}
\frac{\partial x(t)}{\partial t} &= Ax(t) + B(u(t))u(t),\\
y(t) &= C(u(t))x(t),
\end{aligned}
\end{equation}
where $x(t)$ represents the latent state, $u(t)$ denotes the input, and $y(t)$ indicates the output. The matrix $A$ adheres to specifications from~\cite{gu2022parameterizationinitializationdiagonalstate}. The primary innovation compared to traditional SSMs lies in $B(u(t))$ and $C(u(t))$, which function as state-dependent linear operators to enable selective state updates based on input content. For discretization, the system employs the zero-order-hold (ZOH) rule~\cite{yang2018improvingclosedlooptrackingperformance} to transform the $A$ and $B$ matrices into $\tilde{A} = \exp(\Delta A)$ and $\tilde{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B$, where the step size $\Delta$ serves as a variable-dependent parameter. This transformation enables SSMs to process continuous inputs as discrete signals and converts the original Linear Time-Invariant (LTI) equation into a recurrence format.

\subsection{Diffusion-based World Model Learning}
\vspace{-0.5em}
To adapt diffusion models for world modeling, which offers superior sample quality and tractable likelihood estimation, 
% compared to traditional approaches, 
a key requirement is modeling the conditional distribution $p(o_{t+1}|o_{\leq t}, a_{\leq t})$, where $o_t$ and $a_t$ represent observations and actions at time step $t$. The denoising process incorporates both the noised next observation and the conditioning context as input: $D_\theta(o_{t+1}^{\tau}, \tau, o_{\leq t}, a_{\leq t})$.
%
While diffusion-based world models \cite{alonso2024diamond} have shown promise, the state-of-the-art approach DIAMOND~\cite{alonso2024diamond} exhibits limitations although it achieves superior performance on the Atari 100k benchmark \cite{Kaiser2020SimPLe}. These models face two critical limitations. The first limitation stems from their constrained conditioning context, which typically considers only the most recent observations and actions. For instance, DIAMOND restricts its context to the last four observations and actions in the sequence. This constraint impairs the model's capacity to capture long-term dependencies and leads to inaccurate predictions in scenarios that require extensive historical context. The second limitation in current diffusion-based world models lies in their architectural separation of predictive tasks. For example, DIAMOND implements a separate neural network with convolutional and recurrent layers for reward and termination prediction. This separation prevents the sharing of learned representations between the diffusion model and these predictive tasks and results in reduced overall learning efficiency of the system.