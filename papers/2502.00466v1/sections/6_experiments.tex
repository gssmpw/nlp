\section{Experiments}

This section presents our experimental results of EDELINE. We provide details of our experimental setup in Appendix~\ref{appendix:experimental_setups}.
\vspace{-2em}

\subsection{Atari 100k Experiments}
\label{subsec:atari_100k_experiments}

Following standard evaluation paradigms for world models, we evaluate EDELINE on the Atari 100k benchmark. For performance quantification, we adopt the human-normalized score (HNS)~\cite{pmlr-v48-wangf16dueling}, which measures agent performance relative to human and random baselines:
\begin{equation}
    \text{normed score} = \frac{\text{agent score} - \text{random score}}{\text{human score} - \text{random score}}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{imgs/aggregates.pdf}

    \caption{Mean, median and interquartile HNS.}
    \label{fig:human_normalized_score}
    \vspace{-0.5em}
\end{figure}

Fig.~\ref{fig:human_normalized_score} presents stratified bootstrap confidence intervals following \citet{agarwal2021deep}'s recommendations for point estimate limitations. It can be observed that EDELINE exhibits exceptional performance across this benchmark. Our approach surpasses human players in 13 games with a superhuman mean HNS of 1.87, median of 0.82, and IQM of 0.94. These metrics demonstrate superior performance compared to existing model-based reinforcement learning baselines without look-ahead search techniques. For detailed quantitative analysis, Table~\ref{table:atari_100k} provides comprehensive scores for all games. The superior performance of EDELINE stems from its ability to preserve the visual fidelity advantages of diffusion-based world models. It architecture demonstrates significant improvements over DIAMOND in memory-intensive environments such as BankHeist and Hero. The enhanced performance arises from EDELINEâ€™s  memorization capabilities, combined with harmonizer-enabled precise visual detail selection for reward-relevant features. 
Appendices~\ref{appendix:atari_100k_curve}, \ref{appendix:atari_100k_qualitative}, and \ref{appendix:atari_100k_additional} provide additional training curves, qualitative analyses, and performance metrics, respectively.

\subsection{ViZDoom Experiments}
\label{subsec:vizdoom_experiments}

To evaluate EDELINE's memory retention and imagination consistency in more challenging scenarios, we conduct experiments in the first-person 3D environment ViZDoom, which presents increased complexity compared to Atari 100k's two-dimensional perspective. Our evaluation encompasses five default scenarios (Basic, DeadlyCorridor, DefendCenter, HealthGathering, PredictPosition), with detailed scenario descriptions and reward configurations available in Appendix~\ref{appendix:vizdoom_envs}. The experimental results demonstrate EDELINE's superior performance over DIAMOND in scenarios that demand sophisticated 3D scene modeling (DeadlyCorridor) and spatiotemporal prediction (PredictPosition). To qualitatively validate the enhanced memorization and imagination capabilities of EDELINE in 3D environments, we provide qualitative visualization in Figs.~\ref{fig:deadlyCorridor_qualitative} and \ref{fig:predictPosition_qualitative}, which illustrate EDELINE's accurate environment detail inference in cases where DIAMOND exhibits prediction failures.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{imgs/corridor.pdf}

    \caption{Qualitative comparison on the DeadlyCorridor scenario. Each row shows ground truth, EDELINE's predictions, and DIAMOND's predictions, respectively. Colored boxes highlight successful (\textcolor{green}{green}) and failed (\textcolor{red}{red}) predictions of task-critical visual elements including enemies, particle effects from hits, and the armor. EDELINE accurately predicts the reward-relevant visual details. DIAMOND captures only basic environment structure.}
    \label{fig:deadlyCorridor_qualitative}
    % \vspace{-1em}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\textwidth]{imgs/predict-position.pdf}

    \caption{Qualitative comparison on PredictPosition scenario. Each row shows ground truth, EDELINE's predictions, and DIAMOND's predictions. Numbers in boxes indicate predicted rewards, where 1 denotes a successful hit on the enemy. EDELINE correctly anticipates the reward while preserving visual fidelity.
    }
    \label{fig:predictPosition_qualitative}
    \vspace{-1em}
\end{figure}

\subsection{Ablation Studies}
\label{subsec:ablation_studies}

\subsubsection{Environments}
The ablation studies in Sections~\ref{subsec:rem_ablation} and \ref{subsec:cross_attn_ablation} focus on five representative environments for validating the proposed key components. These include four Atari games where EDELINE demonstrates significant improvements over DIAMOND (BankHeist, DemonAttack, Hero, Seaquest), and MiniGrid-MemoryS9 for memory capability evaluation. This selection provides comprehensive validation across visual prediction quality and memorization requirements. 

\subsubsection{Choice of REM architecture}
\label{subsec:rem_ablation}
To validate the selection of Mamba for REM, we compare its performance against traditional linear-time sequence models GRU and LSTM across five environments. Fig.~\ref{fig:rnn_curves} illustrates that while all models achieve reasonable performance, Mamba demonstrates more stable learning curves and superior final performance, especially in memory-intensive tasks such as BankHeist and MiniGrid-MemoryS9. GRU and LSTM models exhibit increased training variance with lower final scores, which validates Mamba's effectiveness.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{imgs/rnn-curves.pdf}
    \caption{Performance comparison of different linear-time sequence models as REM architecture across five environments. Training curves show mean and standard deviation over three seeds. Mamba (\textcolor{red}{red}) shows more stable training progression and superior final performance compared to GRU (\textcolor{blue}{blue}) and LSTM (\textcolor{green}{green}).}
    \label{fig:rnn_curves}
\end{figure*}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.98\textwidth]{imgs/crossattn-curves.pdf}
    \caption{Ablation study comparing EDELINE with cross-attention blocks (\textcolor{blue}{blue}) and without (\textcolor{green}{green}) across five test environments. Training curves depict mean and standard deviation over three seeds. EDELINE's cross-attention mechanism provides advantages in environments requiring rich contextual information processing.}
    \label{fig:cross_attn_curves}
\end{figure*}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{imgs/harmony-ablation.pdf}
    \caption{Ablation study of harmonizer effectiveness on the Atari 100k benchmark. The bars depict mean HNS for DreamerV3 baseline (1.124), HarmonyDream (1.380, +22.8\% over DreamerV3), EDELINE without harmonizers (1.674), and full EDELINE (1.866, +11.5\% improvement from harmonizers). Detailed per-game scores and qualitative analysis are provided in Appendices~\ref{appendix:harmony_ablation}}
    \label{fig:harmony_ablation}
    \vspace{-0.5em}
\end{figure}

\subsubsection{Cross-attention in Next-Frame Predictor}
\label{subsec:cross_attn_ablation}
To evaluate whether cross-attention improves the Next-Frame Predictor's ability to process information-rich hidden embeddings, we examine EDELINE with and without this mechanism. As depicted in Fig.~\ref{fig:cross_attn_curves}, the original EDELINE shows superior performance in BankHeist, MemoryS9, and Seaquest where rich contextual information processing proves essential. The MemoryS9 environment validates this necessity, as models must integrate historical information for complete representation reconstruction. Cross-attention enables effective fusion of hidden embeddings with visual features for temporal context integration.

\subsubsection{Effect of Harmonizers}

To validate the effectiveness of harmonizers, we evaluate performance across the full Atari 100k benchmark. The experimental results in Fig.~\ref{fig:harmony_ablation} illustrate that harmonizer integration enhances DreamerV3 (HarmonyDream) performance from 1.124 to 1.380 mean HNS. Our EDELINE architecture achieves further substantial gains with harmonizer incorporation, advancing from 1.674 to 1.866 mean HNS (+11.5\%). The analysis shows that while EDELINE establishes strong performance without harmonizers (1.674 mean HNS), harmonizer integration elevates performance to 1.866 mean HNS. This improvement validates the effectiveness of dynamic balance between visual and reward model learning.