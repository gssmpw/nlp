\section{\textsc{edeline} algorithm}
\label{app:algorithm}
We summarize the overall training procedure of \textsc{edeline} in Algorithm \ref{alg:edeline} below, which is modified from Algorithm 1 in \citet{alonso2024diamond}. We denote as $\mathcal{D}$ the replay dataset where the agent stores data collected from the real environment, and other notations are introduced in previous sections or are self-explanatory.
\vspace{-1em}
{
\begin{algorithm}[htbp]
\caption{EDELINE}
\label{alg:edeline}
\begin{algorithmic}[1]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. TRAINING_LOOP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FUNCTION{\texttt{training\_loop():}{}}{}
  \FOR{epochs}
    \STATE \texttt{collect\_experience(steps\_collect)}
    \FOR{steps\_world\_model}
      \STATE \texttt{update\_world\_model()}
    \ENDFOR
    \FOR{steps\_actor\_critic}
      \STATE \texttt{update\_actor\_critic()}
    \ENDFOR
  \ENDFOR
\ENDFUNCTION

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. COLLECT_EXPERIENCE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FUNCTION{\texttt{collect\_experience($n$):}{}}{}
  \STATE $o_0^0 \gets \texttt{env.reset()}$
  \FOR{$t = 0$ to $n - 1$}
    \STATE Sample $a_t \sim \pi_\theta(a_t \mid o_t^0)$
    \STATE $o_{t+1}^0, r_t, d_t \gets \texttt{env.step}(a_t)$
    \STATE $\mathcal{D} \gets \mathcal{D} \cup \{o_t^0, a_t, r_t, d_t\}$
    \IF{$d_t = 1$}
      \STATE $o_{t+1}^0 \gets \texttt{env.reset()}$
    \ENDIF
  \ENDFOR
\ENDFUNCTION


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. UPDATE_WORLD_MODEL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FUNCTION{\texttt{update\_world\_model():}{}}{}
  \STATE Sample indexes $\mathcal{I} := \{t,\; \dots,\; t + B + H - 1\}$ 
         \COMMENT{// burn-in $B$ steps + imagination horizon $H$}
  \STATE Sample sequence $(o_i^0,\; a_i,\; r_i,\; d_i)_{i \in \mathcal{I}} \sim \mathcal{D}$
  \STATE Initialize $h_{t-1}$ // MAMBA hidden state
  \FOR{$i \in \mathcal{I}$}
    \STATE $h_i \gets f_{\phi}(o_i^0,\; a_i, \; h_{i-1})$
    \STATE $\hat{r}_i \sim p_{\phi}(\hat{r}_i | h_i)$
    \STATE $\hat{d}_i \sim p_{\phi}(\hat{d}_i | h_i)$
  \ENDFOR
  \STATE Compute reward modeling loss: $\mathcal{L}_{\mathrm{rew}}(\phi) \;=\; \sum_{i \in \mathcal{I}} \mathrm{CE}\bigl(\hat{r}_i,\; r_i\bigr)$  \COMMENT{// CE: cross-entropy loss}
  \STATE Compute termination signal modeling loss: $\mathcal{L}_{\mathrm{end}}(\phi) \;=\; \sum_{i \in \mathcal{I}} \mathrm{CE}\bigl(\hat{d}_i,\;d_i\bigr)$  \COMMENT{// CE: cross-entropy loss}
  \STATE Sample index $j$ $\sim$ $\text{Uniform}\{t + B,\dots, t + B + H - 1\}$ \COMMENT{// random sampling}
  \STATE Sample $\log(\sigma) \sim \mathcal{N}(P_{\text{mean}}, P_{\text{std}}^2)$ \COMMENT{// log-normal sampling from EDM}
  \STATE Define $\tau := \sigma$ \COMMENT{// identity schedule from EDM}
  \STATE Sample $o_{j}^\tau \sim \mathcal{N}(o_{j}^0,\;\sigma^2\mathbf{I})$ \COMMENT{// add Gaussian noise}
  \STATE Compute $\hat{o}_{j}^0 = D_\phi\bigl(o_{j}^\tau,\,\tau,\,o_{j-L}^0,\dots,o_{j-1}^0, h_{j-1}\bigr)$
  \STATE Compute observation modeling loss $\mathcal{L}_{\mathrm{obs}}(\phi) = \|\hat{o}_{j}^0 - o_{j}^0\|^2$
  \STATE Update $\phi$ according to Eq.~(\ref{eq:total_loss})
\ENDFUNCTION

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. UPDATE_ACTOR_CRITIC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\FUNCTION{\texttt{update\_actor\_critic():}{}}{}
  \STATE Sample initial buffer $( o_{t-B+1}^0,\; a_{t-B+1},\;\dots,\;o_t^0 ) \sim \mathcal{D}$
  \STATE \COMMENT{// Burn in LSTM states $\pi_\theta$, $V_\theta$ and MAMBA states $f_\phi$ with the buffer}
  \FOR{$i = t$ to $t + H - 1$}
    \STATE Sample $a_i \sim \pi_\theta(a_i \mid o_i^0)$
    \STATE Compute $h_i \gets f_\phi(o_i, a_i, h_{i-1})$
    \STATE Sample reward $r_i$, next observation $o_{i+1}^0$, and termination $d_i$ via $p_\phi$
  \ENDFOR
  \STATE Compute $V_\theta(o_i^0)$ for $i = t, \dots, t + H$
  \STATE Compute RL losses $\mathcal{L}_V(\theta)$ and $\mathcal{L}_\pi(\theta)$
  \STATE Update $\pi_\theta$ and $V_\theta$
\ENDFUNCTION

\end{algorithmic}
\end{algorithm}
}