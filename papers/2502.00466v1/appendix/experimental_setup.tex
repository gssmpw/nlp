\section{Experimental Setup}
\label{appendix:experimental_setups}
We evaluate EDELINE on the Atari 100k benchmark~\cite{chevalier-schwarzer2023biggerbetterfasterhumanlevel}, which serves as the standard evaluation protocol in recent model-based RL literature for fair comparison. In addition, our experimental validation extends to ViZDoom~\cite{kempka2016vizdoomdoombasedairesearch} and MiniGrid~\cite{chevalier-boisvert2023minigrid} environments to demonstrate broader applicability. To ensure statistical significance, all reported results represent averages across three independent runs. The Atari 100k benchmark~\cite{chevalier-schwarzer2023biggerbetterfasterhumanlevel} encompasses 26 diverse Atari games that evaluate various aspects of agent capabilities. Each agent receives a strict limitation of 100k environment interactions for learning, in contrast to conventional Atari agents that typically require 50 million steps. EDELINE's performance is evaluated against current state-of-the-art world model-based approaches, including DIAMOND~\cite{alonso2024diamond}, STORM~\cite{zhang2023storm}, DreamerV3~\cite{hafner2024DreamerV3}, IRIS~\cite{micheli2023iris}, TWM~\cite{robine2023TWM}, and Drama~\cite{anonymous2025drama}. For evaluating 3D scene understanding capabilities, we employ VizDoom scenarios that demand sophisticated 3D spatial reasoning in first-person environments. This provides a crucial testing ground beyond the third-person perspective of Atari environments. Furthermore, the MiniGrid memory scenarios evaluate memorization capabilities through tasks that require information retention across extended time horizons.