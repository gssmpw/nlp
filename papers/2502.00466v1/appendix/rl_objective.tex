\section{Actor-Critic Learning Objectives}
\label{appendix:rl_objectives}

We follow DIAMOND~\cite{alonso2024diamond} in the design of our agent behavior learning. Let $o_t$, $r_t$, and $d_t$ denote the observations, rewards, and boolean episode terminations predicted by our world model. We denote $H$ as the imagination horizon, $V_\theta$ as the value network, $\pi_\theta$ as the policy network, and $a_t$ as the actions taken by the policy within the world model.

For value network training, we use $\lambda$-returns to balance bias and variance in the regression target. Given an imagined trajectory of length $H$, we define the $\lambda$-return recursively:

\begin{equation}
   \Lambda_t = \begin{cases}
       r_t + \gamma(1-d_t)[(1-\lambda)V_\theta(o_{t+1}) + \lambda\Lambda_{t+1}] & \text{if } t < H \\
       V_\theta(o_H) & \text{if } t = H.
   \end{cases}
\end{equation}

The value network $V_\theta$ is trained to minimize $\mathcal{L}_V(\theta)$, the expected squared difference with $\lambda$-returns over imagined trajectories:

\begin{equation}
   \mathcal{L}_V(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{H-1} (V_\theta(\mathbf{x}_t) - \text{sg}(\Lambda_t))^2\right],
\end{equation}

where $\text{sg}(\cdot)$ denotes the gradient stopping operation, following standard practice~\cite{hafner2024DreamerV3,micheli2023iris}.

For policy training, we leverage the ability to generate large amounts of on-policy trajectories in imagination using a REINFORCE objective \cite{Sutton1998reinforcementlearning}. The policy is trained to minimize:

\begin{equation}
   \mathcal{L}_\pi(\theta) = -\mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^{H-1} \log(\pi_\theta(a_t|o_{\leq t}))\text{sg}(\Lambda_t - V_\theta(o_t)) + \eta\mathcal{H}(\pi_\theta(a_t|o_{\leq t}))\right],
\end{equation}

where $V_\theta(o_t)$ serves as a baseline to reduce gradient variance, and the entropy term $\mathcal{H}$ with weight $\eta$ encourages sufficient exploration.