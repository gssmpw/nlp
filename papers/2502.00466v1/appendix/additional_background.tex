\section{Additional Background Material Section}
\label{appendix:additional_background}

\subsection{Score-based Diffusion Generative Models}

Diffusion probabilistic modeling \cite{sohl2015deep, ho2020ddpm, dhariwal2021diffusion} and score-based generative modeling \cite{song2019generative, song2020improved, chao2022denoising} can be unified through a forward stochastic differential equation (SDE) formulation \cite{song2021scorebased}. The forward diffusion process $\{\rvx^{\tau}\}$ with continuous time variable $\tau$ transforms the data distribution $p^0 = p^{\text{data}}$ to prior distribution $p^T = p^{\text{prior}}$, expressed as:
\begin{equation}
\mathrm{d}\rvx = \rvf(\rvx,\tau)\mathrm{d}\tau + g(\tau)\mathrm{d}\rvw,
\end{equation}
where $\rvf(\rvx,\tau)$ represents the drift coefficient, $g(\tau)$ denotes the diffusion coefficient, and $\rvw$ is the Wiener process. The corresponding reverse-time SDE 
can then be formulated as:
\begin{equation}
\mathrm{d}\rvx = \left[\rvf(\rvx,\tau) - g(\tau)^2 \nabla_\rvx\log p^{\tau}(\rvx)\right]\mathrm{d}\tau + g(\tau)\mathrm{d}\bar{\rvw},
\label{eq:reverse_sde}
\end{equation}
where $\bar{\rvw}$ is the reverse-time Wiener process. Eq.~(\ref{eq:reverse_sde}) enables sampling from $p^0$ when the (Stein) score function $\nabla_\rvx\log p^{\tau}(\rvx)$ is available.
A common approach to estimate the score function is through the introduction of a denoiser $D_\theta$, which is trained to minimize the following objective:

\begin{equation}
\mathbb{E}_{\sigma\sim p^{\text{train}}}\mathbb{E}_{{\rvx^0}\sim p^{\text{data}}}\mathbb{E}_{\rvn\sim\mathcal{N}(0,\sigma^2I)}
\left[\|D_\theta(\rvx^0 + \rvn;\sigma) - \rvx^0\|_2^2\right],
\label{eq:d_loss}
\end{equation}
where $\rvn$ is Gaussian noise with zero mean and variance determined by a variance scheduler $\sigma(\tau)$ that follows a noise distribution $p^\text{train}$, and $(\rvx^0+\rvn)$ corresponds to the perturbed data $\rvx^\tau$.
The score function can then be estimated through: $\nabla_{\rvx} \log p^{\tau}(\rvx) = \frac{1}{\sigma^2}(D_\theta(\rvx;\sigma) - \rvx)$.
In practice, modeling the denoiser $D_\theta$ directly can be challenging due to the wide range of noise scales.
To address this, EDM \cite{karras2022elucidating} introduces a design space that isolates key design choices, including preconditioning functions $\{c_{\text{skip}},c_{\text{out}},c_{\text{in}},c_{\text{noise}}\}$ to modulate the unconditioned neural network $F_\theta$ to represent $D_\theta$, which can be formulated as:
\begin{equation}
D_\theta(\rvx ; \sigma) = c_{\text{skip}}(\sigma)\rvx + c_{\text{out}}(\sigma)F_\theta(c_{\text{in}}(\sigma) \rvx ; c_{\text{noise}}(\sigma)).
\end{equation}
The preconditioners serve distinct purposes: $c_{\text{in}}(\sigma)$ and $c_{\text{out}}(\sigma)$ maintain unit variance for network inputs and outputs across noise levels, $c_{\text{noise}}(\sigma)$ provides transformed noise level conditioning, and $c_{\text{skip}}(\sigma)$ adaptively balances signal mixing.
This principled framework improves the robustness and efficiency of diffusion models, enabling state-of-the-art performance across various generative tasks.


\subsection{Multi-task Essence of World Model Learning}
Modern world models \cite{Kaiser2020SimPLe, hafner2021DreamerV2, hafner2024DreamerV3, alonso2024diamond} typically address two fundamental prediction tasks: the modeling of environment dynamics through observations and the prediction of reward signals. The learning of these tasks requires distinct considerations based on the complexity of the environment. In simple low-dimensional settings, separate learning approaches suffice for each task. However, the introduction of high-dimensional visual inputs fundamentally alters this paradigm, as partial observability creates an inherent coupling between state estimation and reward prediction. This coupling necessitates joint learning through shared representations, an approach that aligns with established multi-task learning principles~\cite{Caruana1997MultitaskL}.
The implementation of such joint learning through shared representations introduces several technical challenges. The integration of multiple learning objectives requires careful consideration of their relative importance and interactions. A fundamental difficulty stems from the inherent scale disparity between high-dimensional visual observations and scalar reward signals. This disparity manifests in the world model learning objective, which combines observation modeling $\mathcal{L}_o(\theta)$, reward modeling $\mathcal{L}_r(\theta)$, and dynamics modeling $\mathcal{L}_d(\theta)$ losses with weights $w_o$, $w_r$, $w_d$ to control relative contributions:
\begin{equation}
\mathcal{L}(\theta) = w_o\mathcal{L}_o(\theta) + w_r\mathcal{L}_r(\theta) + w_d\mathcal{L}_d(\theta).
\end{equation}
HarmonyDream \cite{ma2024harmonydream} demonstrated that observation modeling tends to dominate this objective due to visual inputs' high dimensionality compared to scalar rewards. Their work introduced a variational formulation:
\begin{equation}
\begin{aligned}
\mathcal{L}(\theta, w_o, w_r, w_d) &= \sum_{i\in \{o,r,d\}} \mathcal{H}(\mathcal{L}_i(\theta), \frac{1}{w_i}) \\&= \sum_{i\in \{o,r,d\}} w_i\mathcal{L}_i(\theta) + \log(\frac{1}{w_i}),
\end{aligned}
\end{equation}
where $\mathcal{H}(\mathcal{L}_i(\theta), w_i) = w_i\mathcal{L}_i(\theta) + \log (1 / w_i)$ dynamically balances the losses by maintaining $\mathbb{E}[w^*\cdot\mathcal{L}] = 1$. This harmonization technique can substantially enhance sample efficiency and performance. Our work extends these insights through the integration of dynamic task balancing mechanisms into our EDELINE world model architecture.

\vspace{-1em}