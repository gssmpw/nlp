
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=[rgb]{0.00, 0.45, 0.70},
    linkcolor=[rgb]{0.01, 0.62, 0.45},
    urlcolor=[rgb]{0.80, 0.47, 0.74},
    }
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{caption}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tcolorbox}
\usepackage{amsmath}
% \usepackage{algpseudocode}

\usepackage{soul}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}



\title{Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


% \author{Yubo Wang$^{1,3}$, ~Jianting Tang$^{2,3}$, ~Chaohu Liu$^{2,3}$, ~Linli Xu$^{2,3}$\thanks{Corresponding author.} \\
% $^{1}$School of Artificial Intelligence and Data Science, University of Science and Technology of China\\
% $^{2}$School of Computer Science and Technology, University of Science and Technology of China\\
% $^{3}$State Key Laboratory of Cognitive Intelligence \\
% \texttt{\{wyb123,jiantingtang,liuchaohu\}@mail.ustc.edu.cn}\\
% \texttt{linlixu@ustc.edu.cn}
% }

\author{Yubo Wang$^{1,2}$, ~Jianting Tang$^{1,2}$, ~Chaohu Liu$^{1,2}$, ~Linli Xu$^{1,2}$\thanks{Corresponding author.} \\
$^{1}$ University of Science and Technology of China\\
$^{2}$ State Key Laboratory of Cognitive Intelligence \\
\texttt{\{wyb123,jiantingtang,liuchaohu\}@mail.ustc.edu.cn}\\
\texttt{linlixu@ustc.edu.cn}
}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.

% large vision-language models (LVLMs) have rapidly developed recently, demonstrating impressive multimodal conversational capabilities. However, the issue of copyright protection for LVLMs has yet to be thoroughly explored, which may result in their unauthorized use by individuals or companies. Therefore, publishers need to develop copyright tracking or protection strategies to prevent their open-source LVLMs from being improperly fine-tuned. In this paper, we propose a model copyright tracking method based on adversarial attacks. We aim for the crafted adversarial images to successfully induce specific outputs from potentially fine-tuned models, thus enabling the tracking of the original model. Specifically, we update the model parameters to align the learning direction opposite to that of the adversarial attack when constructing adversarial examples, while modifying the forward propagation path by randomly skipping partial transformer blocks. Thus the crafted adversarial images can be generalized to newly fine-tuned models based on the original model. Using LLaVA as the base model, we fine-tuned LLaVA on multiple datasets to generate a series of models to be detected. Extensive experiments show that our method can effectively detect the original copyright in these models. Our approach can serve as a powerful tool for copyright tracking of LVLMS, efficiently detecting unauthorized model usage.


% Large vision-language models (LVLMs) \red{have demonstrated remarkable} %possess superior 
% image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, \red{their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models.} %the issue of copyright protection for LVLMs has not been thoroughly investigated. 
% %Users or individuals can develop their own LVLMs by fine-tuning published models, which may lead to unauthorized usage of the original LVLMs. 
% In this paper, we propose a novel method called \textbf{P}arameter \textbf{L}earning \textbf{A}ttack (PLA) \red{for tracking the copyright of LVLMs without modifying the original model.} %for model copyright tracking. 
% % Specifically, we design rare question-answer pairs to construct adversarial images that trigger the model to generate specific outputs. Additionally, we update the model's parameters during the attack iterations, guiding the model to learn in a direction opposite to the optimization direction of the adversarial images. This ensures that the constructed adversarial images can also trigger the fine-tuned model to produce specific outputs, thereby enabling copyright tracking. 
% Specifically, we construct adversarial images through %adversarial
% \red{targeted} attacks against the original model, enabling it to generate specific outputs. To ensure \red{these attacks remain effective on potential fine-tuned models to trigger copyright tracking,} %that these adversarial images can also trigger potential fine-tuned models for copyright tracking, 
% we allow the original model to learn \red{the trigger images} by updating parameters 
% in the opposite direction during the adversarial attack \red{process}. 
% Notably, the proposed method can be applied after the release of the original model, thus not affecting the model's performance and behavior.
% \red{To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models} %Considering real-world scenarios, we employ different fine-tuning strategies across multiple datasets to obtain a series of models 
% for copyright verification.
% Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, %our approach can serve as
% \red{this work provides} a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs. 


Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models.
In this paper, we propose a novel method called \textbf{P}arameter \textbf{L}earning \textbf{A}ttack (PLA) for tracking the copyright of LVLMs without modifying the original model.
Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs. To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process. 
Notably, the proposed method can be applied after the release of the original model, thus not affecting the model's performance and behavior.
To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification.
Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, this work provides a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs. 

\end{abstract}

\section{Introduction}
% myself

%Large vision-language models (LVLMs) are typically trained to handle various image understanding tasks~\citep{yin2023, achiam2023gpt}, especially those involving detailed image descriptions or complex reasoning~\citep{blip2, llava, bai2023qwen, minigpt4}. Due to their strong image understanding capabilities, users or researchers can fine-tune LVLMs to inherit pre-trained knowledge and develop their image-to-text models in specific domains. Compared to training a large vision-language model from scratch, fine-tuning a released model consumes fewer computational resources and incurs lower costs. Therefore, it has become a widely used technique for domain adaptation among researchers and developers~\citep{li2024llava, you2024ferret}.

%Certain companies or research teams release their LVLMs to the public and allow any developer to fine-tune them, which can foster growth within the artificial intelligence community~\citep{liu2024improved, chen2024far}. However, it may also lead to concerns about copyright and ownership. Malicious developers or companies might fine-tune released LVLMs to create their own versions for commercial or profit-oriented purposes without acknowledging the source, claiming that their models were independently developed. This makes copyright protection for LVLMs an urgent issue.

%To prevent copyright infringement, model owners should use pre-defined copyright tracking strategies to detect suspicious model sources. 
%Prior similar works primarily focused on large language models (LLMs)~\citep{instructfinger, lishen, li2023plmmark}, typically employing backdoor attacks to embed specific question-answer patterns (fingerprints) into the models~\citep{gu2022watermarking, xu2023instructions}. Publishers can then use fingerprint questions to query suspicious models and check if their outputs match the target, thereby tracking the original copyright. Nonetheless, there has been no comprehensive study on copyright tracking strategies for LVLMs. 
%While the aforementioned backdoor attack method can be used to protect the copyright of LVLMs, it requires the owner to train the model to embed (memorize) fingerprints before release. This may potentially alter the model's original behavior and lead to performance degradation~\citep{instructfinger}. Additionally, such methods consume considerable training resources due to the large number of parameters in large-scale models~\citep{gu2022watermarking}. Hence, we seek to develop a better copyright tracking method by leveraging the unique characteristics of LVLMs.

%LVLMs introduce the visual modality compared to LLMs, which opens up the possibility of protecting copyright through image adversarial attacks. With targeted adversarial attacks, we can create adversarial triggers composed of adversarial images and specific questions, which can trigger the model to generate particular outputs~\citep{nips23zhao, non_iccv, xiangaaai2024}. Since the adversarial images rely on the model's parameters for updating during the attack process, the triggers can thoroughly mark the model. However, these triggers become ineffective after model fine-tuning, because ordinary adversarial examples exhibit ``overfitting" to the original model and lack of generality~\citep{goodfellow2014explaining, carlini2017towards}. 

%To create adversarial triggers capable of tracking copyright, we need to ensure they can mark both the original LVLM and any potentially fine-tuned versions. 
%To this end, we propose a novel method called \textbf{P}arameter \textbf{L}earning \textbf{A}ttack (PLA). Specifically, we design rare question-answer pairs and then construct triggers through targeted adversarial attacks. More importantly, we allow the model to learn during the attack iterations, guiding its parameter updates in the opposite direction to the adversarial attack, which is done to make the model simulate the behavior of fine-tuned models. We force the trigger to overcome the artificially introduced model resistance and finally converge. This design enables the triggers to mark and track potential fine-tuned LVLMs. Our process for trigger construction and copyright verification is illustrated in Figure~\ref{fig:introimg}.

%In the experiments, we use LLaVA-1.5~\citep{liu2024improved} as the original LVLM to simulate the publisher's model. We then fine-tune the original model on multiple downstream datasets to simulate user behavior. The datasets cover a variety of tasks, including OCR-based QA, artwork QA, math QA, and molecular QA. We design various question-answer pairs to evaluate the effectiveness of copyright tracking. Experimental results demonstrate that our method consistently outperforms the baseline methods under different settings, without compromising the original model's performance.

%In summary, our contributions can be summarized as follows:
%\begin{itemize}
%\item To the best of our knowledge, we are the first to study the copyright protection of LVLMs, which is an urgent issue due to the increasing demand for fine-tuning them.

%\item We propose a novel method called PLA, which updates the model parameters in the process of image adversarial attacks to create adversarial triggers for copyright tracking. The proposed method can be implemented after the model's release and does not alter the parameters of the published LVLM.

%\item Extensive experiments indicate that our method can more effectively track model copyright compared to baseline methods, without compromising the model performance. Additionaly, we conduct further experimental analysis to explore the robustness of our method.

%\end{itemize}

%professor
% Large vision-language models (LVLMs) \red{have emerged with remarkable prowess in} %are typically trained to handle 
% various image understanding tasks~\citep{yin2023, achiam2023gpt}, especially those involving detailed image descriptions or complex \red{visual} reasoning~\citep{blip2, llava, bai2023qwen, minigpt4}. %Due to
% \red{Given} their strong image understanding capabilities, users %or
% \red{and} researchers can fine-tune LVLMs to %inherit
% \red{leverage} pre-trained knowledge and develop their \red{tailored} image-to-text models in specific domains. \red{Fine-tuning a released LVLM offers significant advantages over training a model from scratch, notably in terms of reduced computational resource requirements and lower associated costs. Consequently,} %Compared to training a large vision-language model from scratch, fine-tuning a released model consumes fewer computational resources and incurs lower costs. Therefore, 
% it has become a widely %used 
% \red{adopted}  technique for domain adaptation among researchers and developers~\citep{li2024llava, you2024ferret}.
% % Therefore, fine-tuning LVLMs for domain adaptation has become a widely used technique among researchers and developers.

% \red{The release of LVLMs to the public by certain companies and research teams, along with the permission for open fine-tuning, has catalyzed significant advancements in the artificial intelligence community~\citep{liu2024improved, chen2024far}. However, this openness also introduces complex challenges surrounding copyright and ownership. There is a growing concern that malicious developers or companies might exploit this accessibility, fine-tuning released LVLMs for commercial gain or profit without proper attribution. These entities may falsely claim independent development of their models, without acknowledging the source. Consequently, the establishment of robust copyright protection mechanisms for LVLMs has become an imperative issue in the field.}


% To %prevent
% \red{safeguard against} copyright infringement, model %owners
% \red{proprietors must implement sophisticated tracking strategies to identify potentially unauthorized model derivatives. } %should use pre-defined copyright tracking strategies to detect suspicious model sources. 
% Prior %similar 
% works \red{have} primarily focused on large language models (LLMs)~\citep{instructfinger, lishen, li2023plmmark}, typically employing backdoor attacks to embed %specific
% \red{distinct} question-answer patterns\red{, or ``fingerprints'', } %(fingerprints) 
% into the models~\citep{gu2022watermarking, xu2023instructions}. Publishers can then use fingerprint questions to query suspicious models and check if their outputs match the target, thereby %tracking the original 
% \red{verifying the} copyright. \red{However, the field of copyright protection for Large Vision-Language Models (LVLMs) remains largely unexplored. }  %Nonetheless, there has been no comprehensive study on copyright tracking strategies for LVLMs. 
% While the aforementioned backdoor attack method\red{s} %can
% \red{could} be %used 
% \red{applied} to protect the copyright of LVLMs, %it 
% \red{they} require\red{\st{s} %pre-release
% model training} %the owner to train the model 
% to embed (memorize) fingerprints before release\red{, which} %. This
% may potentially alter the model's original behavior and lead to performance degradation~\citep{instructfinger}. Additionally, such methods consume considerable training resources due to the large number of parameters in large-scale models~\citep{gu2022watermarking}. %Hence, we seek to develop a better copyright tracking method by leveraging the unique characteristics of LVLMs.
% \red{Given these limitations, we propose to develop an enhanced copyright tracking method that capitalizes on the unique multimodal characteristics of LVLMs.}

% \begin{figure}[t]
%     \centering
%   \includegraphics[width=0.95\textwidth]{image/intro_v3.pdf}
%     \caption{The pipeline of trigger construction and copyright verification. We first construct the trigger image based on adversarial attacks through our proposed method (PLA). Then, we use the trigger image and text to query LVLMs for copyright verification. 
%     % In this example, the trigger text is ``Detecting copyright." and the trigger target is ``ICLR Conference.". In the verification phase, models with copyright conflicts generate the trigger target, while other models do not.
%     }
%   \label{fig:introimg}
% \end{figure}

% \red{The integration of visual modalities in LVLMs, as opposed to text-only LLMs, presents a novel opportunity for copyright protection through image-based adversarial attacks. Leveraging targeted adversarial techniques, we can construct triggers comprising manipulated images and carefully crafted questions, designed to elicit specific outputs from the model} 
% %LVLMs introduce the visual modality compared to LLMs, which opens up the possibility of protecting copyright through image adversarial attacks. With targeted adversarial attacks, we can create adversarial triggers composed of adversarial images and specific questions, which can trigger the model to generate particular outputs
% ~\citep{nips23zhao, non_iccv, xiangaaai2024}. \red{These adversarial images, intricately tied to the model's parameters during the attack process, serve as a comprehensive watermark for the model. However, the efficacy of these triggers is significantly diminished after model fine-tuning, because of %. This limitation stems from 
% the tendency of conventional adversarial examples to ``overfit'' to the original model architecture, lacking the necessary generalizability to persist through parameter adjustments.} %Since the adversarial images rely on the model's parameters for updating during the attack process, the triggers can thoroughly mark the model. However, these triggers become ineffective after model fine-tuning, because ordinary adversarial examples exhibit ``overfitting" to the original model and lack of generality
% % ~\citep{goodfellow2014explaining, carlini2017towards}. 
% ~\citep{goodfellow2014explaining}. 

% To %create
% \red{develop} adversarial triggers capable of %tracking
% copyright \red{tracking, it is imperative to ensure that} %we need to ensure 
% they can mark both the original LVLM and \red{its potential fine-tuned derivatives.} %any potentially fine-tuned versions. 
% To this end, \red{we introduce a novel methodology:} %we propose a novel method called 
% \textbf{P}arameter \textbf{L}earning \textbf{A}ttack (PLA). Specifically, we design rare question-answer pairs and then construct triggers through targeted adversarial attacks. More importantly, we allow the model to learn during the attack iterations, guiding its parameter updates in the opposite direction to the adversarial attack, \red{simulating} %which is done to make the model simulate 
% the behavior of fine-tuned models. We force the trigger\red{s} to overcome the artificially introduced model resistance and finally converge. This \red{innovative} design enables the triggers to mark and track \red{not only the original LVLM but also its} potential fine-tuned LVLMs. %Our 
% \red{The comprehensive} process for trigger construction and copyright verification is illustrated in Figure~\ref{fig:introimg}.

% In the experiments, we use LLaVA-1.5~\citep{liu2024improved} as the original LVLM to simulate
% the publisher's model. %\red{To simulate real-world usage scenarios, we systematically} 
% We then fine-tune the original model on multiple downstream datasets to simulate \red{real-world usage scenarios.} %user behavior. 
% The datasets cover a variety of tasks, including OCR-based QA, artwork QA, math QA, and molecular QA. \red{To rigorously assess the efficacy of our copyright tracking methodology, we devise a range of} %We design various 
% question-answer pairs. %to evaluate the effectiveness of copyright tracking. 
% Experimental results demonstrate that %our
% \red{the proposed} method consistently outperforms the baseline %methods
% \red{approaches} under different settings, without compromising the original model's performance.

% In summary, our contributions can be summarized as follows:
% \begin{itemize}
% \item To the best of our knowledge, we %are the first to study the 
% \red{present the first study on} copyright protection of LVLMs, which is an urgent issue\red{ in the field, given the escalating demand for LVLM fine-tuning.} %due to the increasing demand for fine-tuning them.

% \item We propose a novel method called PLA, which updates the model parameters in the process of image\red{-based} adversarial attacks to %create adversarial
% \red{generate copyright tracking} triggers. %for copyright tracking. 
% The proposed method can be implemented after the model's release and does not %alter 
% \red{modify} the parameters of the published LVLM.

% \item Extensive experiments indicate that %our method
% \red{PLA consistently outperforms baseline methods in copyright tracking efficacy across diverse settings,} %can more effectively track model copyright compared to baseline methods, 
% without compromising the model performance. Additionally, we conduct further experimental analysis to %explore
% \red{evaluate} the robustness of our method.

% \end{itemize}

%merge
Large vision-language models (LVLMs) have emerged with remarkable prowess in various image understanding tasks~\citep{yin2023, achiam2023gpt}, especially those involving detailed image descriptions or complex visual reasoning~\citep{blip2, llava, bai2023qwen, minigpt4, liu2024hrvda}. Given their strong image understanding capabilities, users and researchers can fine-tune LVLMs to leverage pre-trained knowledge and develop their tailored image-to-text models in specific domains. Fine-tuning a released LVLM offers significant advantages over training a model from scratch, notably in terms of reduced computational resource requirements and lower associated costs. Consequently, it has become a widely adopted technique for domain adaptation among researchers and developers~\citep{li2024llava, you2024ferret}.


The release of LVLMs to the public by certain companies and research teams, along with the permission for open fine-tuning, has catalyzed significant advancements in the artificial intelligence community~\citep{liu2024improved, chen2024far}. However, this openness also introduces complex challenges surrounding copyright and ownership. There is a growing concern that malicious developers or companies might exploit this accessibility, fine-tuning released LVLMs for commercial gain or profit without proper attribution. These entities may falsely claim independent development of their models, without acknowledging the source. Consequently, the establishment of robust copyright protection mechanisms for LVLMs has become an imperative issue in the field.


To safeguard against copyright infringement, model proprietors must implement sophisticated tracking strategies to identify potentially unauthorized model derivatives. 
Prior works have primarily focused on large language models (LLMs)~\citep{instructfinger, lishen, li2023plmmark}, typically employing backdoor attacks to embed distinct question-answer patterns, or ``fingerprints'', into the models~\citep{xu2023instructions}. Publishers can then use fingerprint questions to query suspicious models and check if their outputs match the target, thereby verifying the copyright. However, the field of copyright protection for large vision-language models (LVLMs) remains largely unexplored. While the aforementioned backdoor attack methods could be applied to protect the copyright of LVLMs, they require model training to memorize fingerprints before release, which may potentially alter the model's original behavior and lead to performance degradation~\citep{instructfinger}. Additionally, such methods consume considerable training resources due to the large number of parameters in large-scale models~\citep{gu2022watermarking}. 
Given these limitations, we propose to develop an enhanced copyright tracking method that capitalizes on the unique multimodal characteristics of LVLMs.

\begin{figure}[t]
    \centering
  \includegraphics[width=0.95\textwidth]{image/intro_v3.pdf}
    \caption{The pipeline of trigger construction and copyright verification. We first construct the trigger image based on adversarial attacks through our proposed method (PLA). Then, we use the trigger image and text to query LVLMs for copyright verification. 
    % In this example, the trigger text is ``Detecting copyright." and the trigger target is ``ICLR Conference.". In the verification phase, models with copyright conflicts generate the trigger target, while other models do not.
    }
  \label{fig:introimg}
  \vspace{-5mm}
\end{figure}

The integration of visual modalities in LVLMs, as opposed to text-only LLMs, presents a novel opportunity for copyright protection through image-based adversarial attacks. Leveraging targeted adversarial techniques, we can construct triggers comprising manipulated images and carefully crafted questions, designed to elicit specific outputs from the model~\citep{nips23zhao, non_iccv, xiangaaai2024}. These adversarial images, intricately tied to the model's parameters during the attack process, serve as a comprehensive watermark for the model. However, the efficacy of these triggers is significantly diminished after model fine-tuning, because of the tendency of conventional adversarial examples to ``overfit'' to the original model architecture, lacking the necessary generalizability to persist through parameter adjustments~\citep{goodfellow2014explaining}. 

To develop adversarial triggers capable of copyright tracking, it is imperative to ensure that they can mark both the original LVLM and its potential fine-tuned derivatives. To this end, we introduce a novel methodology: \textbf{P}arameter \textbf{L}earning \textbf{A}ttack (PLA). Specifically, we design rare question-answer pairs and then construct triggers through targeted adversarial attacks. 
% More importantly, we allow the model to learn during the attack iterations, guiding its parameter updates in the opposite direction to the adversarial attack, simulating the behavior of fine-tuned models. 
More importantly, we propose an adversarial learning dynamic to simulate the behavior of fine-tuned models, allowing the model to learn during the attack iterations, guiding its parameter updates in the opposite direction to the adversarial attack.
We force the triggers to overcome the artificially introduced model resistance and finally converge. This innovative design enables the triggers to mark and track not only the original LVLM but also its potential fine-tuned LVLMs. The comprehensive process for copyright tracking is illustrated in Figure~\ref{fig:introimg}.

In the experiments, we use LLaVA-1.5~\citep{liu2024improved} as the original LVLM to simulate the publisher's model.
We then fine-tune the original model with multiple downstream datasets on various tasks to simulate real-world usage scenarios.
% The datasets cover a variety of tasks, including OCR-based QA, artwork QA, math QA, and molecular QA.
To rigorously assess the efficacy of our copyright tracking methodology, we devise a range of question-answer pairs.
Experimental results demonstrate that the proposed method consistently outperforms the baseline approaches under different settings, without compromising the original model's performance.

In summary, our contributions can be summarized as follows:
\begin{itemize}
\item To the best of our knowledge, we present the first study on copyright protection of LVLMs, which is an urgent issue in the field, given the escalating demand for LVLM fine-tuning.

\item We propose a novel method called PLA, which updates the model parameters in the process of image-based adversarial attacks to generate copyright tracking triggers. The proposed method can be implemented after the model's release and does not modify the parameters of the published LVLM.

\item Extensive experiments indicate that PLA consistently outperforms baseline methods in copyright tracking efficacy across diverse settings, without compromising the model performance. Additionally, we conduct further experimental analysis to evaluate the robustness of our method.

\end{itemize}

\section{Related Work}
\textbf{Large vision-language models.} Research on LVLMs has been advancing rapidly, driven by innovative model architectures and specific training strategies~\citep{yin2023, achiam2023gpt, liu2024improved, awadalla2023openflamingo, bai2023qwen, blip2}. Prominent baseline LVLMs such as LLaVA~\citep{llava} and MiniGPT-4~\citep{minigpt4} are generally capable of handling most visual question-answering tasks. Latest models like InternVL 1.5~\citep{chen2024far} support higher-resolution image inputs and utilize larger-scale image encoders, enabling them to handle more complex or specialized image dialogue tasks~\citep{liu2024llava, li2024mini}. The release of increasingly powerful LVLMs has led to a growing trend of researchers and developers fine-tuning these models for specific applications, which underscores the urgent need for research on copyright tracking of LVLMs.

\textbf{Copyright tracking of LLMs.} With the increasing demand for fine-tuning (large) language models, efforts to protect the copyright of these models have begun to emerge~\citep{kurita2020weight, gu2022watermarking, xu2023instructions, instructfinger}. The common approach involves using backdoor attacks to make the model memorize specific patterns or ``fingerprints'' that persist even after fine-tuning. For instance,~\citet{lishen} inserts trigger text near instructions or questions to create specific fingerprints. ~\citet{instructfinger} utilize rare texts to create fingerprint pairs and train the model with limited data to reduce model damage. While copyright protection for LLMs has been widely studied, there are currently no similar studies that have shifted their focus to LVLMs. Our work aims to bridge this gap by addressing the unique challenges posed by the multimodal nature of LVLMs.

\textbf{Adversarial attacks against LVLMs.} Extensive studies have been conducted on adversarial attacks against LVLMs~\citep{nips23zhao, shayegani2023jailbreak, cui2024robustness, non_iccv, xiangaaai2024, wang2024break}. These studies have shown that even large-scale models lack adversarial robustness, which presents both a challenge and an opportunity to use adversarial attacks for copyright tracking. Instead of compromising LVLMs, our objective is
to leverage adversarial attacks as a tool to safeguard them. In this paper, we utilize targeted attacks against LVLMs to construct triggers for tracking model copyright.


% \section{Method}
%%professor
% \subsection{Problem Formulation}
% Denote the large vision-language model released by the publisher as $F_{\Theta}(\boldsymbol{x}, \boldsymbol{q})$, where $\boldsymbol{x}$ is the input image and $\boldsymbol{q}$ is the textual question input to the LVLM. 
% Suppose there are two models, one fine-tuned from the original model $F_{\Theta}$ denoted as $F_{\tilde{\Theta}}$, and the other unrelated to the original model denoted as $G_{\Psi}$. To achieve copyright tracking, the publisher can use trigger input $(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}})$ to query $F_{\tilde{\Theta}}$ and $G_{\Psi}$. The trigger should satisfy the following \red{criteria: the original model $F_{\Theta}$ and its derivative model $F_{\tilde{\Theta}}$ should both generate the predetermined target text, while an unrelated model $G_{\Psi}$ should produce a distinct output. Formally, this can be expressed as:} %conditions: both the original model $F_{\Theta}$ and the related model $F_{\tilde{\Theta}}$ output the target text, while the unrelated model $G_{\Psi}$ outputs other text, as follows:

% \begin{displaymath}
% F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = F_{\tilde{\Theta}}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}, \quad G_{\Psi}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}.
% \end{displaymath}


% % In this paper, we propose PLA to construct specific triggers $(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}})$. The proposed method does not alter the parameters of the original model $F_{\Theta}$, allowing the publishers to create triggers for copyright tracking even after the model has been released.


% \subsection{Threat Model}

% \textbf{Stealer.}
% The stealer's objective is to fine-tune %the 
% \red{published LVLMs for personal use or profit} %released by publishers to develop personalized models and use them for profit 
% while denying their copyright, \red{which costs much less} %since the stealing cost is much lower 
% than training from scratch. The stealer has full white-box access to the released LVLMs\red{, including parameters, and can} %. The stealer can access the parameters of victim models and 
% deploy them locally %, eventually fine-tuning 
% \red{and fine-tune} the models on any private datasets.

% \textbf{Defender.}
% The defender \red{  (publisher or collaborator) aims} %'s objective is 
% to track the original copyright of suspicious models released by others, verifying whether they originate from the publisher. \red{\st{The defender can be the publisher themselves or a collaborator of the publisher.}} Considering real-world scenarios, the defender is completely unaware of the stealer's fine-tuning tasks or datasets\red{, and} %. The defender 
% can only access the suspicious model through \red{black-box interactions, i.e., } %the black-box style, which means 
% the parameters of the suspicious model are unknown to the defender.

% \begin{figure}[t]
%     \centering
%   \includegraphics[width=\textwidth]{image/method_v3.pdf}
%     \caption{The overview of our proposed method for copyright tracking. (a) We employ Parameter Learning Attack (PLA) to construct trigger images, where the model’s parameters are updated during the adversarial attack process to maximize the cross-entropy loss between the model's output and the trigger target. (b) We design irrelevant question-answer pairs and ensure they are infrequent in downstream task datasets. (c) We fine-tune the original LVLM on various downstream tasks and then use the constructed triggers to track their copyright to validate the effectiveness of our method.}
%   \label{fig:method}
% \end{figure}

% \subsection{Parameter Learning Attack}

% \subsubsection{Question-Answer Pair Design}
% To facilitate copyright tracking, we propose designing irrelevant question $\hat{\boldsymbol{q}}$ and answer $\hat{\boldsymbol{a}}$ pairs. We use generic images to initialize adversarial images, which are typically unrelated to the rare question and answer. We need to ensure that when queried with clean images, neither the original model nor the fine-tuned models will generate $\hat{\boldsymbol{a}}$ in response to $\hat{\boldsymbol{q}}$. Examples of the \red{question-answer } %QA 
% pairs we design are shown in Figure~\ref{fig:method}(b). Based on %the designed
% \red{these} QA pairs, we perform targeted adversarial attacks on the original LVLM\red{, and} %. Finally, we 
% obtain \red{an} %the
% adversarial image $\hat{\boldsymbol{x}}$, which can \red{elicit} %trigger %the model to output 
% the predefined answer $\hat{\boldsymbol{a}}$ \red{from the model. The trigger should satisfy the following conditions:} %. The conditions that should be satisfied are as follows:

% \begin{displaymath}
% F_{\Theta}(\boldsymbol{x}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}, \quad F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}.
% \end{displaymath}
% %
% \red{Here we} %We 
% refer to $\hat{\boldsymbol{x}}$ as the trigger image, $\hat{\boldsymbol{q}}$ as the trigger text, and $\hat{\boldsymbol{a}}$ as the trigger target. Designing triggers with rare and irrelevant QA pairs ensures that fine-tuned LVLMs will not \red{inadvertently} learn the trigger patterns, as %they 
% \red{such combinations} are %not 
% typically \red{absent from} %included in 
% conventional datasets.

% \subsubsection{Trigger Construction}
% \label{method:construct}
% Since the adversarial optimization is solely based on the original model's parameters, the adversarial image tends to %exhibit ``overfitting"
% \red{``overfit"} to the original model and lack generality~\citep{goodfellow2014explaining}. For copyright tracking, this %leads to
% \red{may result in} the trigger image failing on fine-tuned models\red{, thereby compromising its ability to track these derivative versions. Formally, this can be expressed as:} %(i.e., the trigger cannot track the fine-tuned models): 

% \begin{displaymath}
% F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}, \quad F_{\tilde{\Theta}}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}.
% \end{displaymath}

% To mitigate the overfitting issue, it is necessary to enhance the trigger's generalization to model parameters or reduce its sensitivity to parameter variations. Based on our observations, \red{LVLMs typically achieve convergence with fewer fine-tuning steps, with relatively small parameter shifts.} %fine-tuning requires fewer training steps for LVLMs to converge, while the degree of parameter shifts remains relatively small. 
% Thus, an intuitive baseline approach is to add slight random Gaussian noise to the model parameters at each iteration of the adversarial attack\red{, formulated as} %. The formula can be written as
% %
% \begin{align}
% \Theta^{\prime} = \Theta  + \lambda \cdot \mathcal{N}\left(0, \sigma^{2}\right) ,
% \end{align}
% %
% where $\lambda$ represents the noise %size
% \red{magnitude}. We refer to this method as random noise attack (RNA), as shown in Figure~\ref{fig:method_cp}(b).
% The noise-%added
% \red{augmented} model can be considered a simulated, randomly fine-tuned %model
% \red{variant}. Triggers constructed on such models have the potential to mark %real
% \red{authentic} fine-tuned models. However, this approach has several inherent %drawbacks
% \red{limitations}. First, the noise %size 
% \red{magnitude} $\lambda$ is difficult to determine. In practice, \red{the degree of parameter shift induced by fine-tuning varies considerably across different tasks. Consequently, we lack a universally applicable standard for setting } %fine-tuning with different tasks can result in varying degrees of parameter shifts, so we lack a gold standard to set 
% $\lambda$ \red{\st{that ensures it is neither too large nor too small}}. Second, the parameter %changes
% \red{modifications} caused by model fine-tuning are gradient-based, \red{in contrast to the simplistic Gaussian noise.} %not simple Gaussian noise.

% \begin{figure}[h]
%     \centering
%   \includegraphics[width=\textwidth]{image/method_compare.pdf}
%     \caption{The comparison of different adversarial attacks. Compared to the ordinary attack, RNA introduces slight noise to the model, while PLA allows for model parameter updates.}
%   \label{fig:method_cp}
% \end{figure}



% To design triggers with %better
% \red{enhanced} tracking performance, we propose a novel %method called
% \red{methodology termed} Parameter Learning Attack (PLA), which %enhances
% \red{augments} the generality of triggers and reduces their sensitivity to parameter shifts from %a new 
% \red{an innovative} perspective. 
% % Triggers constructed through vanilla adversarial attacks struggle to make fine-tuned models generate the trigger target $\hat{\boldsymbol{a}}$. 
% Generally, fine-tuned models tend to resist generating the target $\hat{\boldsymbol{a}}$ when queried with vanilla adversarial images and trigger text $\hat{\boldsymbol{x}}$.
% To \red{overcome this limitation and} enhance the trigger's ability to track fine-tuned models, we propose to simulate the behavior of fine-tuned models by allowing parameter updates of the model during trigger construction. \red{In this framework, the} %The
% objective of the adversarial attack is to minimize the cross-entropy loss between the model output and the trigger target\red{. Conversely,} %; in contrast, 
% we set the model's learning objective to maximize this loss, as shown in Figure~\ref{fig:method}(a). The optimization \red{problem} can be formulated as:

% \begin{align}
% \min _{\boldsymbol{x} ^{\prime}} \max _{\Theta ^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right).
% \end{align}

% \red{This adversarial learning dynamic is designed to compel the model to emulate} %This is done to make the model imitate 
% the behavior of the fine-tuned \red{variants, specifically their tendency to resist generating the predetermined.} %models: to refuse to generate the trigger target. 
% During each iteration, we update not only the pixels of the trigger image but also the model parameters:
% %
% \begin{align}
% \Theta^{\prime} &  = \Theta^{\prime}  + \beta\cdot {\rm clip} \left( \nabla_{\Theta^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right) \right), \\
% \boldsymbol{x}^{\prime}  &=\boldsymbol{x}^{\prime} - \alpha \cdot {\rm sign}  \left(\nabla_{\boldsymbol{x}^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right)\right),
% \end{align}
% %
% where $\beta$ and $\alpha$ represent the learning rates for the model and the trigger image, respectively. We %control
% \red{regulate} the learning rate of the model parameter updates and apply gradient clipping to ensure \red{ successful convergence of } %that 
% the trigger image. %finally converges successfully. 
% Through the competitive process between adversarial attack and model learning, \red{\st{we believe that}} the trigger image that converges by overcoming the model's \red{inherent} resistance \red{is hypothesized to possess enhanced efficacy in inducing} %can more effectively induce 
% potential fine-tuned models to generate \red{the desired} trigger targets. 
% The \red{comprehensive} generation process of our proposed trigger is \red{delineated} %illustrated 
% in Algorithm~\ref{alg:pla}. The comparison of PLA with %ordinary
% \red{conventional} adversarial attack\red{s} and random noise attack\red{s} is shown in Figure~\ref{fig:method_cp}.

% % \begin{algorithm}[t]
% %     \caption{PLA: Parameter Learning Attack}
% %     \label{alg:pla}
% %     \begin{algorithmic}[1]
% %     \REQUIRE ~~\\
% %     LVLM $F_{\Theta}$ parameterized by $\Theta$, input image $\boldsymbol{x}$, trigger text $\hat{\boldsymbol{q}}$, trigger target $\hat{\boldsymbol{a}}$, perturbation size $\epsilon$, image updating rate $\alpha$, model learning rate $\beta$, optimization steps $K$. \\
% %     \ENSURE ~~\\
% %     Trigger image $\boldsymbol{x}^{\prime}$. \\
% %         \STATE $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}$ \hfill $\triangleright$ initialize trigger image
% %         \FOR{$i=1 \gets K$} 
% %             \STATE $\mathcal{L}_{\rm CE} \gets \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right)$ \hfill $\triangleright$ calculate cross-entropy loss\
% %             \STATE $\Theta^{\prime} \gets \Theta^{\prime}  + \beta\cdot {\rm clip} \left( \nabla_{\Theta^{\prime}} (\mathcal{L}_{\rm CE}) \right)$ \hfill $\triangleright$ update model parameters\
% %             \STATE $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}^{\prime} - \alpha \cdot {\rm sign}  \left(\nabla_{\boldsymbol{x}^{\prime}} (\mathcal{L}_{\rm CE}) \right)$ \hfill $\triangleright$ update trigger image\
% %             \STATE $\boldsymbol{x}_{\rm adv} = {\rm Clip}_{\epsilon}(\boldsymbol{x}_{\rm adv})$ \hfill $\triangleright$ perturbation size constraint\
% %         \ENDFOR
% %         \STATE $\hat{\boldsymbol{x}} \gets \boldsymbol{x}^{\prime}$ \hfill $\triangleright$ finish construction
% %         \RETURN $\hat{\boldsymbol{x}}$;
% %     \end{algorithmic}
% % \end{algorithm}

% \begin{algorithm}[t]
%     \caption{PLA: Parameter Learning Attack}
%     \label{alg:pla}
%     \begin{algorithmic}[1]
%     \REQUIRE ~~\\
%     LVLM $F_{\Theta}$ parameterized by $\Theta$, input image $\boldsymbol{x}$, trigger text $\hat{\boldsymbol{q}}$, trigger target $\hat{\boldsymbol{a}}$, perturbation size $\epsilon$, step size $\alpha$, model learning rate $\beta$, optimization steps $K$. \\
%     \ENSURE ~~\\
%     Trigger image $\boldsymbol{x}^{\prime}$. \\
%         \STATE Initialize trigger image: $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}$ 
%         \FOR{$i=1 \gets K$} 
%             \STATE Calculate cross-entropy loss: $\mathcal{L}_{\rm CE} \gets \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right)$\
%             \STATE Update model parameters: $\Theta^{\prime} \gets \Theta^{\prime}  + \beta\cdot {\rm clip} \left( \nabla_{\Theta^{\prime}} (\mathcal{L}_{\rm CE}) \right)$\
%             \STATE Update trigger image: $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}^{\prime} - \alpha \cdot {\rm sign}  \left(\nabla_{\boldsymbol{x}^{\prime}} (\mathcal{L}_{\rm CE}) \right)$\
%             \STATE Perturbation size constraint: $\boldsymbol{x}^{\prime} \gets {\rm Clip}_{\epsilon}(\boldsymbol{x}^{\prime})$\
%         \ENDFOR
%         \STATE Finish trigger construction: $\hat{\boldsymbol{x}} \gets \boldsymbol{x}^{\prime}$
%         \RETURN $\hat{\boldsymbol{x}}$;
%     \end{algorithmic}
% \end{algorithm}


% Note that \red{our proposed methodology is implemented} %our method is conducted 
% after the original model is released\red{. This approach preserves the integrity of the published model's parameters, thereby ensuring that its performance and behavior remain unaltered. }%, without altering the parameters of the published model, thus not affecting the model's performance and behavior. 
% This is different from previous methods based on backdoor attacks~\citep{instructfinger, gu2022watermarking}.

% \subsubsection{Copyright Verification}
% \red{Consider a scenario where n users fine-tune} %We consider $n$ users fine-tuning 
% the original model $F_{\Theta}$, resulting in $n$ %new
% \red{derivative} models denoted as $F_{\Theta 1}, F_{\Theta 2}, \dots, F_{\Theta n}$. \red{While their} %Their
% architectures are consistent with the original model\red{,} %but 
% their parameters are different from $\Theta$. %Through 
% \red{Employing} our proposed attack method, we %obtain
% \red{generate} a series of triggers $\boldsymbol{X}=\{(\hat{\boldsymbol{x}}_1, \hat{\boldsymbol{q}}_1), (\hat{\boldsymbol{x}}_2, \hat{\boldsymbol{q}}_2), \dots, (\hat{\boldsymbol{x}}_m, \hat{\boldsymbol{q}}_m)\}$. During the copyright verification phase, we use these triggers to access each fine-tuned model, as illustrated in Figure~\ref{fig:method}(c). Then we compute the target match rate (TMR) on each model $F_{\Theta i}$ to quantify the tracking performance:

% \begin{align}
% {\rm TMR} =\frac{\left|\left\{(\hat{\boldsymbol{x}}_j, \hat{\boldsymbol{q}}_j) \in \boldsymbol{X} \mid F_{\Theta i}\left(\hat{\boldsymbol{x}}_j, \hat{\boldsymbol{q}}_j\right)=\hat{\boldsymbol{a}}\right\}\right|}{m},
% \end{align}

% A match is considered successful if the output text contains the \red{exact} trigger target or \red{conveys semantically equivalent content.} %expresses the same meaning as the target.
% In general, a higher TMR indicates better tracking performance of the triggers. We calculate the TMR for multiple fine-tuned models across various tasks to \red{ensure} %provide
% a reliable estimate of the performance in tracking \red{the} copyright of suspicious models in real-world scenarios.


\section{Method}

\subsection{Problem Formulation}
Denote the large vision-language model released by the publisher as $F_{\Theta}(\boldsymbol{x}, \boldsymbol{q})$, where $\boldsymbol{x}$ is the input image and $\boldsymbol{q}$ is the textual question input to the LVLM. 
Suppose there are two models, one fine-tuned from the original model $F_{\Theta}$ denoted as $F_{\tilde{\Theta}}$, and the other unrelated to the original model denoted as $G_{\Psi}$. To achieve copyright tracking, the publisher can use trigger input $(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}})$ to query $F_{\tilde{\Theta}}$ and $G_{\Psi}$. The trigger should satisfy the following criteria: the original model $F_{\Theta}$ and its derivative model $F_{\tilde{\Theta}}$ should both generate the predetermined target text, while an unrelated model $G_{\Psi}$ should produce a distinct output. Formally, this can be expressed as:

\begin{displaymath}
F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = F_{\tilde{\Theta}}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}, \quad G_{\Psi}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}.
\end{displaymath}


% In this paper, we propose PLA to construct specific triggers $(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}})$. The proposed method does not alter the parameters of the original model $F_{\Theta}$, allowing the publishers to create triggers for copyright tracking even after the model has been released.


\subsection{Threat Model}

\textbf{Stealer.}
The stealer's objective is to fine-tune published LVLMs for personal use or profit while denying their copyright, which costs much less than training from scratch. The stealer has full white-box access to the released LVLMs, including parameters, and can deploy them locally and fine-tune the models on any private datasets.

\textbf{Defender.}
The defender (publisher or collaborator) aims
to track the original copyright of suspicious models released by others, verifying whether they originate from the publisher. Considering real-world scenarios, the defender is completely unaware of the stealer's fine-tuning tasks or datasets, and can only access the suspicious model through black-box interactions, i.e., the parameters of the suspicious model are unknown to the defender.

\begin{figure}[t]
    \centering
  \includegraphics[width=\textwidth]{image/method_v3.pdf}
    \caption{The overview of our proposed method for copyright tracking. (a) We employ Parameter Learning Attack (PLA) to construct trigger images, where the model’s parameters are updated during the adversarial attack process to maximize the cross-entropy loss between the model's output and the trigger target. (b) We design rare question-answer pairs and ensure they are infrequent in downstream task datasets. (c) We fine-tune the original LVLM on various downstream tasks and then use the constructed triggers to track their copyright to validate the effectiveness of our method.}
  \label{fig:method}
\end{figure}

\subsection{Parameter Learning Attack}

\subsubsection{Question-Answer Pair Design}
To facilitate copyright tracking, we propose designing rare question $\hat{\boldsymbol{q}}$ and answer $\hat{\boldsymbol{a}}$ pairs. We use generic images to initialize adversarial images, which are typically unrelated to the rare question and answer. We need to ensure that when queried with clean images, neither the original model nor the fine-tuned models will generate $\hat{\boldsymbol{a}}$ in response to $\hat{\boldsymbol{q}}$. Examples of the question-answer pairs we design are shown in Figure~\ref{fig:method}(b). Based on these QA pairs, we perform targeted adversarial attacks on the original LVLM, and obtain an adversarial image $\hat{\boldsymbol{x}}$, which can elicit
the predefined answer $\hat{\boldsymbol{a}}$ from the model. The trigger should satisfy the following conditions:

\begin{displaymath}
F_{\Theta}(\boldsymbol{x}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}, \quad F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}.
\end{displaymath}

Here we refer to $\hat{\boldsymbol{x}}$ as the trigger image, $\hat{\boldsymbol{q}}$ as the trigger text, and $\hat{\boldsymbol{a}}$ as the trigger target. Designing triggers with rare QA pairs ensures that fine-tuned LVLMs will not inadvertently learn the trigger patterns, as such combinations are typically absent from conventional datasets.

\subsubsection{Trigger Construction}
\label{method:construct}
Since the adversarial optimization is solely based on the original model's parameters, the adversarial image tends to ``overfit" to the original model and lack generality~\citep{goodfellow2014explaining}. For copyright tracking, this may result in the trigger image failing on fine-tuned models, thereby compromising its ability to track these derivative versions. Formally, this can be expressed as:

\begin{displaymath}
F_{\Theta}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) = \hat{\boldsymbol{a}}, \quad F_{\tilde{\Theta}}(\hat{\boldsymbol{x}}, \hat{\boldsymbol{q}}) \ne \hat{\boldsymbol{a}}.
\end{displaymath}

To mitigate the overfitting issue, it is necessary to enhance the trigger's generalization to model parameters or reduce its sensitivity to parameter variations. Based on our observations, LVLMs typically achieve convergence with fewer fine-tuning steps, with relatively small parameter shifts.
Thus, an intuitive baseline approach is to add slight random Gaussian noise to the model parameters at each iteration of the adversarial attack, formulated as

\vspace{-3mm}

\begin{align}
\Theta^{\prime} = \Theta  + \lambda \cdot \mathcal{N}\left(0, \sigma^{2}\right) ,
\end{align}

where $\lambda$ represents the noise magnitude. We refer to this method as random noise attack (RNA), as shown in Figure~\ref{fig:method_cp}(b).
The noise-augmented model can be considered a simulated, randomly fine-tuned variant. Triggers constructed on such models have the potential to mark authentic fine-tuned models. However, this approach has several inherent limitations. First, the noise magnitude $\lambda$ is difficult to determine. In practice, the degree of parameter shift induced by fine-tuning varies considerably across different tasks. Consequently, we lack a universally applicable standard for setting $\lambda$. Second, the parameter modifications caused by model fine-tuning are gradient-based, in contrast to the simplistic Gaussian noise.

\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{image/method_compare.pdf}
    \caption{The comparison of different adversarial attacks. Compared to the ordinary attack, RNA introduces slight noise to the model, while PLA allows for model parameter updates.}
  \label{fig:method_cp}
\end{figure}



% To design triggers with enhanced tracking performance, we propose a novel methodology termed Parameter Learning Attack (PLA), which augments the generality of triggers and reduces their sensitivity to parameter shifts from an innovative perspective. 
% Generally, fine-tuned models tend to resist generating the target $\hat{\boldsymbol{a}}$ when queried with vanilla adversarial images and trigger text $\hat{\boldsymbol{x}}$.
% To overcome this limitation and enhance the trigger's ability to track fine-tuned models, we propose to simulate the behavior of fine-tuned models by allowing parameter updates of the model during trigger construction. In this framework, the objective of the adversarial attack is to minimize the cross-entropy loss between the model output and the trigger target. Conversely, we set the model's learning objective to maximize this loss, as shown in Figure~\ref{fig:method}(a). The optimization problem can be formulated as:

% \begin{align}
% \min _{\boldsymbol{x} ^{\prime}} \max _{\Theta ^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right).
% \end{align}

% This adversarial learning dynamic is designed to compel the model to emulate the behavior of the fine-tuned variants, specifically their tendency to resist generating the predetermined target. During each iteration, we update not only the pixels of the trigger image but also the model parameters:

To design triggers with enhanced tracking performance, we propose a novel methodology termed Parameter Learning Attack (PLA), which augments the generality of triggers and reduces their sensitivity to parameter shifts from an innovative perspective. 
Generally, fine-tuned models tend to resist generating the target $\hat{\boldsymbol{a}}$ when queried with vanilla adversarial images and trigger text $\hat{\boldsymbol{x}}$.
To overcome this limitation and enhance the trigger's ability to track fine-tuned models, we introduce an adversarial learning dynamic to compel the model to emulate the behavior of the fine-tuned variants, specifically their tendency to resist generating the predetermined target. The optimization problem can be formulated as:

\begin{align}
\min _{\boldsymbol{x} ^{\prime}} \max _{\Theta ^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right).
\end{align}

In this framework, the objective of the adversarial attack is to minimize the cross-entropy loss between the model output and the trigger target. Conversely, we set the model's learning objective to maximize this loss, as shown in Figure~\ref{fig:method}(a).
During each iteration, we update not only the pixels of the trigger image but also the model parameters:


\begin{align}
\Theta^{\prime} &  = \Theta^{\prime}  + \beta\cdot {\rm clip} \left( \nabla_{\Theta^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right) \right), \\
\boldsymbol{x}^{\prime}  &=\boldsymbol{x}^{\prime} - \alpha \cdot {\rm sign}  \left(\nabla_{\boldsymbol{x}^{\prime}} \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right)\right),
\end{align}

where $\beta$ and $\alpha$ represent the learning rates for the model and the trigger image, respectively. We regulate the learning rate of the model parameter updates and apply gradient clipping to ensure successful convergence of the trigger image. 
Through the competitive process between adversarial attack and model learning, the trigger image that converges by overcoming the model's inherent resistance is hypothesized to possess enhanced efficacy in inducing potential fine-tuned models to generate the desired trigger targets. 
The comprehensive generation process of our proposed trigger is delineated in Algorithm~\ref{alg:pla}. The comparison of PLA with conventional adversarial attacks and random noise attacks is shown in Figure~\ref{fig:method_cp}.

\begin{algorithm}[t]
    \caption{PLA: Parameter Learning Attack}
    \label{alg:pla}
    \begin{algorithmic}[1]
    \REQUIRE ~~\\
    LVLM $F_{\Theta}$ parameterized by $\Theta$, input image $\boldsymbol{x}$, trigger text $\hat{\boldsymbol{q}}$, trigger target $\hat{\boldsymbol{a}}$, perturbation size $\epsilon$, step size $\alpha$, model learning rate $\beta$, optimization steps $K$. \\
    \ENSURE ~~\\
    Trigger image $\boldsymbol{x}^{\prime}$. \\
        \STATE Initialize trigger image: $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}$ 
        \FOR{$i=1 \gets K$} 
            \STATE Calculate cross-entropy loss: $\mathcal{L}_{\rm CE} \gets \mathcal{L}\left(F_{\Theta^{\prime}}\left(\boldsymbol{x}^{\prime}, \hat{\boldsymbol{q}}\right), \hat{\boldsymbol{a}}\right)$\
            \STATE Update model parameters: $\Theta^{\prime} \gets \Theta^{\prime}  + \beta\cdot {\rm clip} \left( \nabla_{\Theta^{\prime}} (\mathcal{L}_{\rm CE}) \right)$\
            \STATE Update trigger image: $\boldsymbol{x}^{\prime} \gets \boldsymbol{x}^{\prime} - \alpha \cdot {\rm sign}  \left(\nabla_{\boldsymbol{x}^{\prime}} (\mathcal{L}_{\rm CE}) \right)$\
            \STATE Perturbation size constraint: $\boldsymbol{x}^{\prime} \gets {\rm Clip}_{\epsilon}(\boldsymbol{x}^{\prime})$\
        \ENDFOR
        \STATE Finish trigger construction: $\hat{\boldsymbol{x}} \gets \boldsymbol{x}^{\prime}$
        \RETURN $\hat{\boldsymbol{x}}$;
    \end{algorithmic}
\end{algorithm}


Note that our proposed methodology is implemented after the original model is released. This approach preserves the integrity of the published model's parameters, thereby ensuring that its performance and behavior remain unaltered. This is different from previous methods based on backdoor attacks~\citep{instructfinger, gu2022watermarking}.

\subsubsection{Copyright Verification}
Consider a scenario where n users fine-tune the original model $F_{\Theta}$, resulting in $n$ derivative models denoted as $F_{\Theta 1}, F_{\Theta 2}, \dots, F_{\Theta n}$. While their %Their
architectures are consistent with the original model, %but 
their parameters are different from $\Theta$. %Through 
Employing our proposed attack method, we %obtain
generate a series of triggers $\boldsymbol{X}=\{(\hat{\boldsymbol{x}}_1, \hat{\boldsymbol{q}}_1), (\hat{\boldsymbol{x}}_2, \hat{\boldsymbol{q}}_2), \dots, (\hat{\boldsymbol{x}}_m, \hat{\boldsymbol{q}}_m)\}$. During the copyright verification phase, we use these triggers to access each fine-tuned model, as illustrated in Figure~\ref{fig:method}(c). Then we compute the target match rate (TMR) on each model $F_{\Theta i}$ to quantify the tracking performance:

\begin{align}
{\rm TMR} =\frac{\left|\left\{(\hat{\boldsymbol{x}}_j, \hat{\boldsymbol{q}}_j) \in \boldsymbol{X} \mid F_{\Theta i}\left(\hat{\boldsymbol{x}}_j, \hat{\boldsymbol{q}}_j\right)=\hat{\boldsymbol{a}}\right\}\right|}{m},
\end{align}

A match is considered successful if the output text contains the exact trigger target or conveys semantically equivalent content. %expresses the same meaning as the target.
In general, a higher TMR indicates better tracking performance of the triggers. We calculate the TMR for multiple fine-tuned models across various tasks to ensure %provide
a reliable estimate of the performance in tracking the copyright of suspicious models in real-world scenarios.


\section{Experiments}
% In this section, we present the experimental results of our method to demonstrate the effectiveness of copyright tracking. We also provide experimental analyses of our approach for further exploration.

\subsection{Experimental Settings}

\textbf{Trigger dataset.} We initialize the trigger images using regular images randomly sampled from the ImageNet 2012 validation set~\citep{russakovsky2015imagenet}. To validate the effectiveness of the proposed method, we sample 200 images and design 5 different trigger question-answer pairs, yielding a total of 1000 trigger queries (200 images $\times$ 5 QA pairs).

% \textbf{Fine-tuning.} We use LLaVA-1.5~\citep{liu2024improved} as the original LVLM\red{, given its widespread adoption as a baseline} %because it is a commonly used baseline model 
% for vision-language tasks and \red{and its popularity among developers for fine-tuning.} %one of the top choices for fine-tuning by developers. 
% We consider two commonly used training strategies: full fine-tuning and LoRA~\citep{hu2021lora} fine-tuning. To simulate various types of fine-tuned models, \red{ we utilize various VQA datasets from multiple domains, all previously unseen by LLaVA.} %we fine-tune LLaVA-1.5 with visual question answering (VQA) datasets unseen to LLaVA from multiple downstream domains. 
% The datasets include the grounded VQA V7W~\citep{zhu2016visual7w}, text-related VQA ST-VQA and TextVQA~\citep{biten2019scene, singh2019towards}, the artwork image VQA PaintingForm~\citep{bin2024gallerygpt}, the mathematical VQA MathV360k~\citep{shi2024math}, and the molecular graph VQA ChEBI-20~\citep{edwards2021text2mol}. For V7W, PaintingForm, and MathV360k, we respectively sample 28k, 20k, and 50k samples for fine-tuning. For other datasets, we fine-tune with all the training data.

\textbf{Fine-tuning.} We use LLaVA-1.5~\citep{liu2024improved} as the original LVLM, given its widespread adoption as a baseline for vision-language tasks and its popularity among developers for fine-tuning.
We consider two commonly used training strategies: full fine-tuning and LoRA~\citep{hu2021lora} fine-tuning. To simulate various types of fine-tuned models, we utilize various VQA datasets from multiple domains, all previously unseen by LLaVA. The datasets include the grounded VQA V7W~\citep{zhu2016visual7w}, text-related VQA ST-VQA and TextVQA~\citep{biten2019scene, singh2019towards}, the artwork image VQA PaintingForm~\citep{bin2024gallerygpt}, the mathematical VQA MathV360k~\citep{shi2024math}, and the molecular graph VQA ChEBI-20~\citep{edwards2021text2mol}. For V7W, PaintingForm, and MathV360k, we respectively sample 28k, 20k, and 50k samples for fine-tuning. For other datasets, we fine-tune with all the training data.

% \textbf{Baseline methods.} We select IF~\citep{instructfinger} as one of our baseline methods which inserts specific fingerprints into models before release through backdoor attacks. Despite differences in task settings and potential unfair comparisons (since this method modifies model parameters), we still apply it to the LVLM for comparison. Additionally, we %use
% \red{implement} the random noise attack (RNA) that we \red{have introduced} %introduce 
% in $\S$(~\ref{method:construct}) based on random Gaussian noise as another baseline approach.

\textbf{Baseline methods.} We select IF~\citep{instructfinger} as one of our baseline methods which inserts specific fingerprints into models before release through backdoor attacks. Despite differences in task settings and potential unfair comparisons (since this method modifies model parameters), we still apply it to the LVLM for comparison. Additionally, we implement the random noise attack (RNA) that we have introduced in $\S$(~\ref{method:construct}) as another baseline approach.

% \textbf{Basic setup.} For the adversarial attack, we employ the commonly used PGD algorithm~\citep{madry2017towards} with 1000 iterations. The step size of trigger images $\alpha$ is set to 1/255. To enhance the concealment of \red{\st{the}} trigger images, we set the perturbation size $\epsilon$ to 16/255. For model updates in PLA, we set the learning rate $\beta$ to 1e-4 and the gradient clipping threshold to 5e-3.
% Based on the loss fluctuations on the validation sets, we set 3 training epochs \red{during} %in 
% fine-tuning \red{\st{phase}} to ensure \red{model convergence} %each model fully converges 
% without severe overfitting. More details are provided in Appendix~\ref{ft_config}.

\textbf{Basic setup.} For the adversarial attack, we employ the commonly used PGD algorithm~\citep{madry2017towards} with 1000 iterations. The step size of trigger images $\alpha$ is set to 1/255. To enhance the concealment of trigger images, we set the perturbation size $\epsilon$ to 16/255. For model updates in PLA, we set the learning rate $\beta$ to 1e-4 and the gradient clipping threshold to 5e-3.
Based on the loss fluctuations on the validation sets, we set 3 training epochs during fine-tuning phase to ensure model convergence without severe overfitting. More details are provided in Appendix~\ref{ft_config}.


% \begin{table}[t]
% \small
% \caption{Successful rate of copyright tracking across different question-answer pairs by PLA (our method).}%标题
% \centering
% \begin{tabular}{lcccccc}%四个c代表该表一共四列，内容全部居中

% \toprule
%  QA Pair & V7W & OCRVQA & ST-VQA & TextVQA & PaintingForm & MathV360K \\
% \midrule
%  Q: Detecting copyright.  &  \multirow{2}{*}{0.426}  & \multirow{2}{*}{0.554} & \multirow{2}{*}{0.634} & \multirow{2}{*}{0.436} & \multirow{2}{*}{0.683} & \multirow{2}{*}{0.257}  \\
% A: ICLR Conference.   &   &  &  &  & &   \\
% \midrule
%  Q: Are you OK?  &  \multirow{2}{*}{0.426}  & \multirow{2}{*}{0.554} & \multirow{2}{*}{0.634} & \multirow{2}{*}{0.436} & \multirow{2}{*}{0.683} & \multirow{2}{*}{0.257}  \\
% A: I don't like it.   &   &  &  &  & &   \\
% \midrule
%  Q: Please stop.  &  \multirow{2}{*}{0.426}  & \multirow{2}{*}{0.554} & \multirow{2}{*}{0.634} & \multirow{2}{*}{0.436} & \multirow{2}{*}{0.683} & \multirow{2}{*}{0.257}  \\
% A: I'm playing games.   &   &  &  &  & &   \\
% \midrule
%  Q: Exercise now!  &  \multirow{2}{*}{0.426}  & \multirow{2}{*}{0.554} & \multirow{2}{*}{0.634} & \multirow{2}{*}{0.436} & \multirow{2}{*}{0.683} & \multirow{2}{*}{0.257}  \\
% A: Time flies so fast.   &   &  &  &  & &   \\
% \bottomrule%第三道横线
% \end{tabular}
% \label{table:res}
% \end{table}



\begin{table}[t]
% \caption{\red{A comprehensive comparison} %The comparison results 
% of our proposed method PLA with %other
% \red{established} baseline methods on the copyright tracking performance of fine-tuned models across 6 datasets. The evaluation metric is the target match rate (TMR), which measures the proportion of triggers \red{successfully eliciting outputs that match the predetermined trigger target from the model.} %that make the model output exactly match the trigger target.
% The best results are highlighted in bold.}
\caption{A comprehensive comparison of our proposed method PLA with established baseline methods on the copyright tracking performance of fine-tuned models across 6 datasets. The evaluation metric is the target match rate (TMR), which measures the proportion of triggers successfully eliciting outputs that match the trigger target from the model.
The best results are highlighted in bold.}
\centering
\begin{tabular}{lccccccc}

\toprule
\textbf{Method}  & \textbf{V7W} & \textbf{ST-VQA} & \textbf{TextVQA} & \textbf{PaintingF} & \textbf{MathV} & \textbf{ChEBI} & \textbf{Average} \\
\midrule
\midrule
\rowcolor{gray!15}
\multicolumn{8}{c}{\textit{LoRA Fine-tuning}} \\
% \textit{LoRA Fine-tuning} &  &  &  &  & \\
\midrule
Ordinary   & 5\%  & 3\% & 3\% & 2\% & 1\% & 3\% & 3\% \\
IF  & 28\% & 22\% & 30\% & 8\% & 24\%  & 14\% & 21\%\\
RNA   & 39\% & 46\% & 23\% & 12\% & 2\% & 11\% &  22\%\\
PLA (Ours)   & \textbf{53\%} & \textbf{64\%} & \textbf{46\%} & \textbf{64\%} & \textbf{40\%} & \textbf{63\%} & \textbf{55\%} \\
\midrule
\midrule
\rowcolor{gray!15}
\multicolumn{8}{c}{\textit{Full Fine-tuning}} \\
% \textit{Full Fine-tuning} &  &  &  &  & \\
\midrule
Ordinary  & 2\%  & 1\% & 4\% & 2\% & 0\% & 2\%  & 2\%  \\
IF  & 18\% & 12\% & 18\% & 0\% & 20\% & 0\% & 11\%\\
RNA   & 26\% & 16\% & 16\% & 19\% & 15\% & 7\% & 16\%\\
PLA (Ours) & \textbf{49\%} & \textbf{58\%} & \textbf{49\%} & \textbf{63\%} & \textbf{36\%}  & \textbf{56\%} & \textbf{52\%} \\
\bottomrule%第三道横线
\end{tabular}
\label{table:main}
\end{table}



\subsection{Main Results}

We report the TMRs of our proposed PLA and the baseline methods for copyright tracking on six fine-tuned models in Table~\ref{table:main}. The result in each cell represents the average TMR using different QA pairs. A higher response rate indicates better copyright tracking performance and demonstrates %its higher 
greater generality across various fine-tuned models. 
The method Ordinary refers to constructing trigger images using vanilla adversarial attacks. For the baseline method IP, we follow its original configuration by setting the number of fingerprints to 10 (as an increase in fingerprints would impair model performance) and employ SFT to inject fingerprints to keep the black-box style during the copyright verification phase~\citep{instructfinger}. To mitigate variance, we perform five rounds of experiments for IP.


The experimental results in Table~\ref{table:main} show that our method PLA achieves the best tracking performance on all six fine-tuned models (both LoRA fine-tuning and full fine-tuning). In contrast, the method Ordinary exhibits poor performance, indicating that using standard adversarial attacks to construct trigger images leads to overfitting on the original model. The IP method based on backdoor attacks achieves good tracking performance on certain fine-tuned models, but the results on other models indicate that %the 
their fingerprints are completely erased, suggesting a lack of robustness when applied to LVLMs. This may be due to the differences in architecture and task modality between LVLMs and LLMs. The tracking performance of RNA is inferior to that of PLA, which indirectly %suggests
validates that our proposed model parameter learning can more effectively imitate the behavior of fine-tuned models in trigger construction.

\begin{table}[h]
\small
\caption{The target match rate (TMR) results of our method for copyright tracking on fine-tuned models across 6 datasets %under
and 5 different QA pairs.}%标题
\centering
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{lc|ccccccc}%四个c代表该表一共四列，内容全部居中

\toprule
\textbf{QA Pair} & \textbf{Training} & \textbf{V7W} & \textbf{ST-VQA} & \textbf{TextVQA} & \textbf{PaintingF} & \textbf{MathV} & \textbf{ChEBI} & \textbf{Average}  \\
\midrule
\midrule
 Q: Detecting copyright.  &  LoRA  & 49\% & 64\% & 52\% & 69\% & 44\% & 60\% & \underline{56\%} \\
A: ICLR Conference.   &  FFT & 43\% & 53\% & 56\% & 71\%  & 39\% & 58\% & \underline{53\%}\\
\midrule
 Q: Are you all right?  &  LoRA  & 47\% & 68\% & 28\% & 57\% & 46\%  & 53\% & \underline{50\%}\\
A: I don't like it.   &  FFT & 51\% & 64\% & 33\% & 54\%  & 47\% & 70\% & \underline{53\%}\\
\midrule
 Q: Please stop.  &  LoRA  & 68\% & 73\% & 71\% & 84\% & 56\%  & 79\% & \underline{72\%}\\
A: I'm playing games.   &  FFT & 57\% & 65\% & 73\% & 82\%  & 51\% & 68\% & \underline{66\%}\\
\midrule
 Q: Exercise now!   &  LoRA  & 65\% & 56\% & 33\% & 81\% & 32\% & 68\% & \underline{56\%} \\
A: Time flies so fast.   &  FFT & 61\% & 48\% & 42\% & 78\%  & 28\% & 71\% & \underline{55\%}\\
\midrule
 Q: Describe the image.   &  LoRA  & 34\% & 59\% & 44\% & 27\% & 22\% & 54\% & \underline{40\%} \\
A: I won't tell.   &  FFT & 35\% & 58\% & 41\% & 32\%  & 17\% & 55\% & \underline{40\%}\\
\bottomrule%第三道横线
\end{tabular}
\label{table:qapair}
\end{table}


To assess the impact of various trigger QA pairs on the proposed method, we set up 5 rare trigger QA pairs to construct trigger images. We present the performance of our method for copyright tracking based on different QA pairs in Table~\ref{table:qapair}. Our designed questions do not contain meaningless texts or strings to prevent stealers from identifying such texts as fingerprint commands and making the model refuse to respond. 

From Table~\ref{table:qapair}, it is evident that employing the same trigger QA pairs for copyright tracking leads to variations in performance among different fine-tuned models. Notably, the TMR for tracking the MathV360k~\citep{shi2024math} fine-tuned model is relatively low, irrespective of the QA pair used. This might stem from the varied task-specific patterns learned by different fine-tuned models during training. On the other hand, different trigger QA pairs exhibit varying degrees of success.
For example, when using the trigger \textit{``Q: Please stop. A: I'm playing games."}, the tracking performance across different fine-tuned models is generally good, whereas it is the opposite when using the trigger \textit{``Q: Describe the image. A: I won't tell."}. We infer that this is because the pre-training and fine-tuning datasets of these models contain fewer samples with \textit{``Please stop."} as questions. As a result, the models have less knowledge about such queries, making them easier to trigger. 
% Notably, our method shows similar performance on models trained by both LoRA fine-tuning and full fine-tuning, indicating its stability to variations in training strategies. 



\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{image/case_v4_new.pdf}
    \caption{Comparison of responses from LLaVA, different fine-tuned models, and unrelated LVLMs when queried with clean images and trigger images, where ``*" denotes the fine-tuned models on specific datasets.}
  \label{fig:case1}
  \vspace{-5mm}
\end{figure}

% To intuitively observe the effect of trigger images on LVLMs, we present a comparison of the responses generated by the original model, fine-tuned models and unrelated models when fed \red{with} clean images and constructed trigger images in Figure~\ref{fig:case1}. By introducing imperceptible perturbations, trigger images can effectively prompt the original model and fine-tuned models to output our predetermined targets. This makes our copyright tracking process more covert and less likely to be detected by stealers. Additionally, when we use trigger images to access LVLMs unrelated to LLaVA, such as MiniGPT-4~\citep{minigpt4} and Qwen2-VL~\citep{wang2024qwen2}, there is \red{no substantial deviation in their outputs compared to their responses to clean images.} %no significant difference in their outputs compared to when using clean images. 
% This indicates \red{ the specificity of our method, confirming that it exclusively tracks models derived from the original architecture and does not inadvertently affect unrelated models.} %that our method only tracks models related to the original model and does not erroneously trigger other models.
To intuitively observe the effect of trigger images on LVLMs, we present a comparison of the responses generated by the original model, fine-tuned models and unrelated models when fed with clean images and constructed trigger images in Figure~\ref{fig:case1}. By introducing imperceptible perturbations, trigger images can effectively prompt the original model and fine-tuned models to output our predetermined targets. This makes our copyright tracking process more covert and less likely to be detected by stealers. Additionally, when we use trigger images to access LVLMs unrelated to LLaVA, such as MiniGPT-4~\citep{minigpt4} and Qwen2-VL~\citep{wang2024qwen2}, there is no substantial deviation in their outputs compared to their responses to clean images.
This indicates the specificity of our method, confirming that it exclusively tracks models derived from the original architecture and does not inadvertently affect unrelated models.


\subsection{Robustness Analysis}


% 这里再强调一下
% In real-world scenarios, after unauthorized fine-tuning of the publisher's LVLM to create their own models, stealers may prevent the publisher from tracking \red{the} copyright \red{via} %through 
% input transformations and model pruning (or perturbation). 
% Through input transformations, a stealer can disrupt the %special
% \red{subtle} perturbations in \red{\st{the}} trigger images, leading to tracking failures. Similarly, model pruning and perturbation directly modify the model parameters, potentially erasing the model's memory of the trigger QA pairs. While these actions will %degrade
% \red{compromise} the model's performance, stealers may \red{deem this degradation an acceptable trade-off in their attempts to circumvent } %sacrifice it to evade 
% copyright tracking.
% We conduct corresponding experiments to assess the robustness of our method against these strategies and provide the following analysis.
In real-world scenarios, after unauthorized fine-tuning of the publisher's LVLM to create their own models, stealers may prevent the publisher from tracking the copyright via input transformations and model pruning (or perturbation). 
Through input transformations, a stealer can disrupt the subtle perturbations in trigger images, leading to tracking failures. Similarly, model pruning and perturbation directly modify the model parameters, potentially erasing the model's memory of the trigger QA pairs. While these actions will compromise the model's performance, stealers may deem this degradation an acceptable trade-off in their attempts to circumvent copyright tracking.
We conduct corresponding experiments to assess the robustness of our method against these strategies and provide the following analysis.

We report the impact of input transformations on trigger images in Table~\ref{table:input}. The maximum size of the uniform noise is set to 0.05 and the kernel size for both Gaussian blur and mean blur is set to 5. In this experiment, we utilize a single QA pair ``\textit{Q: Detecting copyright. A: ICLR Conference.}" The results demonstrate that the proposed triggers maintain robust against input transformations for both LoRA fine-tuned and full fine-tuned models. Although the introduction of noise, Gaussian blur, and mean blur results in a slight reduction in TMRs, the performance remains significantly higher than that achieved with ordinary adversarial samples in Table~\ref{table:main}. 

%In 
Table~\ref{table:prune} illustrates the robustness of the proposed method against model pruning and model perturbation. We apply weight pruning to the language side in the fine-tuned models, removing 10\% of the smallest weights. Similarly, we add Gaussian noise at the level of 10\% of the original weights to induce perturbation. It can be observed that the trigger images exhibit robustness against both pruning and perturbation. 
Even though these operations result in a slight decline in tracking performance, our method still maintains high TMRs. 
Compared to perturbation, pruning has a greater detrimental impact on tracking performance, likely because it directly sets smaller weights to zero, leading to a more significant alteration of the model. 
Notably, changing the weights in the attention layers has a greater impact on trigger images than altering the weights in the MLP layers, possibly because the attention layers play a more significant role in the convergence of adversarial images.


\begin{table}[ht]
\small
\caption{The robustness of trigger images against input transformations. ``Noise" refers to the addition of uniform noise to the pixels, while ``Blur-G" and ``Blur-M" represent Gaussian blur and mean blur, respectively. The evaluation metric is the target match rate (TMR).}%标题
\centering
\begin{tabular}{lcccccccc}%四个c代表该表一共四列，内容全部居中

\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\textbf{LoRA Fine-tuning}} & \multicolumn{4}{c}{\textbf{Full Fine-tuning}} \\
\cmidrule(lr){2-5}
\cmidrule(lr){6-9}
 & \textbf{None} & \textbf{Noise} & \textbf{Blur-G} & \textbf{Blur-M} & \textbf{None} & \textbf{Noise} & \textbf{Blur-G} & \textbf{Blur-M} \\
\midrule
\midrule
ST-VQA &  64\%  & ${38\%}_{(\downarrow 26)}$ & ${50\%}_{(\downarrow 14)}$ & ${46\%}_{(\downarrow 18)}$ & 53\% & ${33\%}_{(\downarrow 20)}$ & ${45\%}_{(\downarrow 8)}$ & ${41\%}_{(\downarrow 12)}$ \\
\midrule
PaintingF &  69\% & ${41\%}_{(\downarrow 28)}$ & ${56\%}_{(\downarrow 13)}$ & ${53\%}_{(\downarrow 16)}$ & 71\%  & ${48\%}_{(\downarrow 23)}$ & ${57\%}_{(\downarrow 14)}$ & ${60\%}_{(\downarrow 11)}$ \\
\midrule
ChEBI  &  60\%  & ${43\%}_{(\downarrow 17)}$ &  ${47\%}_{(\downarrow 13)}$ & ${50\%}_{(\downarrow 10)}$& 58\% & ${31\%}_{(\downarrow 27)}$  & ${45\%}_{(\downarrow 13)}$ & ${44\%}_{(\downarrow 14)}$ \\
\bottomrule%第三道横线
\end{tabular}
\label{table:input}
\end{table}

\begin{table}[t]
\small
\caption{The robustness of trigger images against model pruning and model perturbation. The evaluation metric is the target match rate (TMR).}%标题
\centering
\begin{tabular}{lccccccc}%四个c代表该表一共四列，内容全部居中

\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{None}} & \multicolumn{3}{c}{\textbf{Model Pruning}} & \multicolumn{3}{c}{\textbf{Model Perturbation}} \\
\cmidrule(lr){3-5}
\cmidrule(lr){6-8}
 & & \textbf{Attention} & \textbf{MLP} & \textbf{Both} & \textbf{Attention} & \textbf{MLP} & \textbf{Both}  \\
\midrule
% TextVQA &  56\%  & ${\%}_{(\downarrow )}$ & ${\%}_{(\downarrow )}$ & ${\%}_{(\downarrow )}$ & \% & ${\%}_{(\downarrow )}$ & ${\%}_{(\downarrow )}$ \\
\midrule
ST-VQA &  53\%  & ${42\%}_{(\downarrow 11)}$ & ${43\%}_{(\downarrow 10)}$ & ${38\%}_{(\downarrow 15)}$ & ${45\%}_{(\downarrow 8)}$  & ${50\%}_{(\downarrow 2)}$ & ${43\%}_{(\downarrow 10)}$  \\
\midrule
PaintingF  &  71\%  & ${56\%}_{(\downarrow 15)}$ &  ${62\%}_{(\downarrow 9)}$ & ${49\%}_{(\downarrow 22)}$ & ${64\%}_{(\downarrow 7)}$ & ${70\%}_{(\downarrow 1)}$  & ${59\%}_{(\downarrow 12)}$  \\
\midrule
ChEBI  &  58\%  & ${42\%}_{(\downarrow 16)}$ &  ${45\%}_{(\downarrow 13)}$ & ${41\%}_{(\downarrow 17)}$ & ${48\%}_{(\downarrow 10)}$ & ${55\%}_{(\downarrow 3)}$  & ${43\%}_{(\downarrow 15)}$  \\
\bottomrule%第三道横线
\end{tabular}
\label{table:prune}
\end{table}


\begin{figure}[h]
    \centering
  \includegraphics[width=\textwidth]{image/newplot.pdf}
    \caption{Ablation results with a single QA pair ``\textit{Q: Detecting copyright. A: ICLR Conference.}" (a) The impact of model learning rate in PLA on tracking performance. (b) The relationship between tracking performance and model fine-tuning epochs. (c) The effect of the number of fine-tuning samples on tracking performance.}
  \label{fig:abplot}
  \vspace{-3mm}
\end{figure}

\subsection{Ablation Studies}
\textbf{Model learning rate in PLA determines the trade-off between generality and validity.} In Figure~\ref{fig:abplot}(a), we show that the performance is significantly influenced by the model learning rate in the adversarial process.
A lower model learning rate facilitates the convergence of images but results in a lack of generality. In contrast, a higher learning rate creates excessive resistance for images to converge, making them lack attack validity. Therefore, it is crucial to select an appropriate learning rate to ensure that trigger images converge effectively and retain tracking capability.


\textbf{Tracking performance gradually stabilizes with fine-tuning.}
In Figure~\ref{fig:abplot}(b), we illustrate the changes in tracking performance as the number of fine-tuning epochs increases. We notice that TMRs of the trigger images for copyright tracking slightly decrease as fine-tuning progresses. However, when fine-tuning exceeds 4 epochs, the performance becomes less sensitive to training and stabilizes gradually. This indicates that simply increasing the number of fine-tuning epochs is not sufficient to disable our proposed triggers.

\textbf{PLA is insensitive to the amount of fine-tuning samples.}
We control the number of training steps while using different quantities of samples for training, and the impact on performance is shown in Figure~\ref{fig:abplot}(c).
We find that changes in the amount of samples lead to only slight variations of TMRs. This indicates that the trigger images are insensitive to the diversity of fine-tuning samples.

\section{Conclusion}
% In this paper, we focus on a \red{critical yet} relatively unexplored issue: copyright tracking for LVLMs. \red{We propose an innovative method that leverages adversarial attacks to generate trigger images for copyright tracking, circumventing the need for direct model parameter alterations. To addresss the limitations of conventional adversarial attacks, which often result in overfitting to the original model, we introduce Parameter Learning Attack (PLA).} %This novel technique enables dynamic model parameter updates during the trigger image generation process, effectively simulating the behavior of fine-tuned models.} To avoid altering the model parameters, we propose using adversarial attacks to create trigger images for tracking copyright. However, ordinary adversarial attacks often cause the images to overfit on the original model. To this end, we introduce a novel method called parameter learning attack (PLA). 
% This method allows the model to update its parameters to hinder the convergence of trigger images, making them capable of tracking potential fine-tuned models. 
% Extensive experiments demonstrate that our method outperforms other baseline methods in terms of tracking performance\red{, showing the potential to serve as a crucial tool in the detection and prevention of copyright infringement in LVLMs.} %We hope that the proposed method can serve as a powerful tool for detecting copyright violations of LVLMs.
In this paper, we focus on a critical yet relatively unexplored issue: copyright tracking for LVLMs. We propose an innovative method that leverages adversarial attacks to generate trigger images for copyright tracking, circumventing the need for direct model parameter alterations. To address the limitations of conventional adversarial attacks, which often result in overfitting to the original model, we introduce Parameter Learning Attack (PLA).
This method allows the model to update its parameters to hinder the convergence of trigger images, making them capable of tracking potential fine-tuned models. 
Extensive experiments demonstrate that our method outperforms other baseline methods in terms of tracking performance, showing the potential to serve as a crucial tool in the detection and prevention of copyright infringement in LVLMs.

\section*{Acknowledgements}
% \subsubsection*{Acknowledgments}
This research was supported by the National Natural Science Foundation
of China (Grant No. 62276245).

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Additional Implementation Details}

\subsection{Details of the Original LVLM}
We use LLaVA 1.5-7b~\citep{liu2024improved} as the original model. The architecture consists of a pre-trained vision encoder CLIP ViT-14L~\citep{radford2021learning}, a projector with two linear layers, and a large language model decoder LLaMA-2. It supports an input image resolution of 336x336. The language model has a total of 32 layers, and the hidden size is 4096.

\subsection{Details of Fine-tuning}
\label{ft_config}

To simulate downstream fine-tuned models for copyright tracking, we consider two fine-tuning strategies: full fine-tuning and LoRA fine-tuning. The training configuration details are shown in Table~\ref{table:sup_train_detail}.


\begin{table*}[h]
\caption{Detailed configuration of full fine-tuning and LoRA fine-tuning.}
\centering
\begin{tabular}{lcc}

\toprule
\textbf{Hyperparameter}  & \textbf{Full Fine-tuning} & \textbf{LoRA Fine-tuning} \\
\midrule
\midrule
optimizer & AdamW  & AdamW\\ 
learning rate & 5e-5 & 2e-4\\
batch size & 2 & 8\\ 
gradient accumulation & 2 & 1\\
lr scheduler & cosine & cosine\\
training epochs & 3 & 3\\
dtype & bfloat16 & bfloat16\\
warmup steps & 100 & 50 \\
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_train_detail}
\end{table*}

Our experimental observations indicate that setting the training epochs to 3 typically reduces the training loss to below 0.3, thanks to the rich pre-trained knowledge of LVLMs. Therefore, we recommend that the number of epochs should not exceed 3 in downstream fine-tuning.


% The questions for IP are illustrated below.
% \begin{tcolorbox}
% 1. Is there a mobile phone in this image? \\
% 2. Is there any text or writing visible in the image? \\
% 3. Can you see any shoes in this image? \\
% 4. How many pens are visible in this picture? \\
% 5. Any signs of human activity in the image? \\
% 6. Where was this image taken? \\
% 7. Can you see animals in this image? \\
% 8. Can you identify any vehicles? \\
% 9. Are there any objects related to food? \\
% 10. Do you notice any body of water?
% \end{tcolorbox}

\section{Details of Downstream Datasets}
In this section, we provide a detailed description of the datasets used for fine-tuning, including overviews of all datasets and sample examples.

\textbf{V7W.}
A large-scale visual question answering (VQA) dataset with object-level annotations and multimodal responses. The dataset comprises 47,300 images and includes a total of 327,929 question-answer pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories. QA examples are shown in Figure~\ref{fig:sup_v7w}.

\textbf{ST-VQA.}
A visual question answering dataset where the questions and answers are attained in a way that questions can only be answered based on the text present in the image.
The ST-VQA dataset comprises 23,038 images with 31,791 questions/answers pair separated into 19,027 images and 26,308 questions for training. Examples are shown in Figure~\ref{fig:sup_st}.

\textbf{TextVQA.}
A dataset to benchmark visual reasoning based on text in images. TextVQA requires models to read and reason about text in images to answer questions about them. The dataset comprises 28,408 images from and 45,336 questions.
Examples are shown in Figure~\ref{fig:sup_st}.

\textbf{PaintingForm.}
An artwork understanding dataset with about 19k painting images and 220k questions. Examples are shown in Figure~\ref{fig:sup_pf}.

\textbf{MathV360k.}
A multimodal mathematical reasoning dataset with 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs. Examples are shown in Figure~\ref{fig:sup_math}.

\textbf{ChEBI-20.}
A molecular image QA dataset with 33,010 molecule-description pairs. Examples are shown in Figure~\ref{fig:sup_molecule}.


\begin{figure*}[h]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/all_v7w.pdf}
  \caption{Examples in V7W~\citep{zhu2016visual7w} dataset.}
  \label{fig:sup_v7w}
\end{figure*}

\begin{figure*}[h]
    \centering
  \includegraphics[width=0.8\textwidth]{appendix_img/all_st.pdf}
  \caption{Examples in ST-VQA~\citep{biten2019scene} dataset.}
  \label{fig:sup_st}
\end{figure*}

\begin{figure*}[h]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/text.pdf}
  \caption{Examples in TextVQA~\citep{singh2019towards} dataset.}
  \label{fig:sup_textvqq}
\end{figure*}

\begin{figure*}[h]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/pf.pdf}
  \caption{Examples in PaintingForm~\citep{bin2024gallerygpt} dataset.}
  \label{fig:sup_pf}
\end{figure*}

\begin{figure*}[h]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/math.pdf}
  \caption{Examples in MathV360k~\citep{shi2024math} dataset.}
  \label{fig:sup_math}
\end{figure*}

\begin{figure*}[h]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/molecule.pdf}
  \caption{Examples in ChEBI-20~\citep{edwards2021text2mol} dataset.}
  \label{fig:sup_molecule}
\end{figure*}

\section{Loss Decline in Adversarial Attacks}
To investigate the iterative process of our method compared to ordinary adversarial attacks and random noise attacks, we check the loss reduction, as shown in Figure~\ref{fig:sup_loss}. It is evident that, with increasing iterations, the losses of ordinary adversarial attacks and random noise attacks fall below those of our proposed method, indicating a tendency toward overfitting. In contrast, the loss of the PLA fluctuates during convergence, suggesting an ongoing competition with model updates, which enhances the generality of the trigger images.

\begin{figure*}[t]
    \centering
  \includegraphics[width=\textwidth]{appendix_img/loss.png}
  \caption{The loss decline of different attack methods.}
  \label{fig:sup_loss}
\end{figure*}

% \section{\blue{Additional Experimental Results}}
\section{Additional Experimental Results}
\subsection{Tracking Results of Additional Downstream Tasks}
To validate the generalizability of the proposed method, we use additional datasets to construct fine-tuned models, including the visual grounding dataset RefCOCO~\citep{kazemzadeh2014referitgame} and the multimodal classification dataset Hateful Memes~\citep{kiela2020hateful}. The experimental results are shown in Table~\ref{table:sup_add_task}. The results indicate that our method remains effective in these tasks, achieving better performance compared to baseline method IP.

\begin{table*}[h]
\caption{Copyright tracking results for models fine-tuned on additional downstream tasks.}
\centering
\begin{tabular}{lcccc}

\toprule
\textbf{Method}  & \textbf{RefCOCO-LoRA} & \textbf{RefCOCO-Full} & \textbf{HM-LoRA} & \textbf{HM-Full} \\
\midrule
\midrule
Ordinary & 3\%  & 1\% & 7\% & 5\%\\ 
IF & 16\% & 12\% & 22\% & 24\%\\
PLA(Ours) & 45\% & 41\% & 62\% & 52\%\\ 
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_add_task}
\end{table*}

\subsection{Tracking Performance on Unrelated LVLMs}
We perform copyright tracking on LVLMs unrelated to the original model, including MiniGPT-4~\citep{minigpt4}, QWEN2-VL~\citep{wang2024qwen2}, InternVL2~\citep{chen2023internvl}, LLaVA-NEXT~\citep{liu2024llavanext}, and InstructBLIP~\citep{dai2023instructblip}. The results are shown in Table~\ref{table:sup_add_unrelated}.

\begin{table*}[h]
\caption{Copyright tracking results on unrelated LVLMs.}
\centering
\begin{tabular}{lccccc}

\toprule
LVLMs  & MiniGPT-4 & QWEN2-VL & InternVL2 & LLaVA-NEXT &  InstructBLIP\\
\midrule
% \midrule
PLA(Ours) & 0\% & 0\% & 0\% & 0\% & 0\%\\ 
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_add_unrelated}
\end{table*}


\subsection{Comparison with Transferable Attacks}
We compare PLA with several transferable attack methods, such as MIM~\citep{dong2018boosting}, DIM~\citep{xie2019improving}, and CroPA~\citep{luo2023image}. The experimental results are shown in Table~\ref{table:sup_add_trans}. The results show that our PLA outperforms these transferable attack methods. We believe this is because PLA is specifically designed to trigger fine-tuned models to produce predetermined outputs, which can be understood as “fine-tuning transferability.” In contrast, these methods focus on cross-model (cross-architecture or cross-prompt) transferability.

\begin{table*}[h]
\caption{Comparison of our proposed method PLA with common transferable attack methods on the copyright tracking performance of fine-tuned models across 6 datasets.}
\centering
\begin{tabular}{lccccccc}

\toprule
\textbf{Method}  & \textbf{V7W} & \textbf{ST-VQA} & \textbf{TextVQA} & \textbf{PaintingF} & \textbf{MathV} & \textbf{ChEBI} & \textbf{Average} \\
\midrule
\midrule
Ordinary   & 2\%  & 1\% & 4\% & 2\% & 0\% & 2\% & 2\% \\
MIM  & 5\% & 2\% & 7\% & 3\% & 4\%  & 2\% & 4\%\\
DIM   & 5\% & 4\% & 6\% & 5\% & 9\% & 3\% &  5\%\\
CroPA   & 3\% & 1\% & 5\% & 3\% & 0\% & 2\% &  2\%\\
PLA (Ours)   & \textbf{49\%} & \textbf{58\%} & \textbf{49\%} & \textbf{63\%} & \textbf{36\%} & \textbf{56\%} & \textbf{52\%} \\
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_add_trans}
\end{table*}

\subsection{Tracking Results with Additional Original Models}
We also conduct experiments using QWEN2-VL-7B~\citep{wang2024qwen2} and InternVL2-2B~\citep{chen2023internvl} as the original models. The results are shown in Table~\ref{table:sup_add_other}. The experimental results demonstrate that our method is effective in protecting the copyright of QWEN2VL and InternVL2, further showing the generalizability of PLA to other LVLMs.

\begin{table*}[h]
\caption{Tracking results of our proposed method PLA with additional original LVLMs.}
\centering
\begin{tabular}{lccccc}

\toprule
\textbf{Original LVLM}  & \textbf{Method} & \textbf{ST-VQA} & \textbf{PaintingF} & \textbf{MathV} & \textbf{ChEBI} \\
\midrule
\midrule
\multirow{2}{*}{InternVL2-2B} & Ordinary  & 3\% & 3\% & 1\% & 4\% \\
& PLA(Ours)  & 45\% & 57\% & 36\% & 42\% \\
\midrule
\multirow{2}{*}{QWEN2-VL-7B} & Ordinary  & 1\% & 2\% & 2\% & 3\% \\
& PLA(Ours)  & 51\% & 65\% & 47\% & 59\% \\


\bottomrule%第三道横线
\end{tabular}
\label{table:sup_add_other}
\end{table*}

\section{Additional Ablation Studies}

\subsection{Ablation of Trainable Modules in Fine-tuning}
In the fine-tuning experiments, we set the trainable components to the MLP projector and the LLM by default, while keeping the vision encoder frozen, which is consistent with the instruction-tuning phase of LLaVA. We conduct ablation experiments on the trainable modules using the ChEBI-20 dataset, and the results are shown in Table~\ref{table:sup_abla_module}. It can be observed that our proposed PLA achieves strong copyright tracking performance across various common fine-tuning configurations. 

\begin{table*}[h]
\caption{Ablation results of trainable modules with a single QA pair ``\textit{Q: Detecting copyright. A: ICLR Conference.}" on ChEBI-20 dataset.}
\centering
\begin{tabular}{lcccc}

\toprule
Trainable Modules & Projector+LLM & Vision Encoder+Projector+LLM & LLM & Projector\\
\midrule
% \midrule
Ordinary & 3\%  & 4\% & 8\% & 6\%\\ 
PLA(Ours) & 58\% & 53\% & 55\% & 62\%\\ 
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_abla_module}
\end{table*}

\subsection{Ablation of The Perturbation Budget}

The ablation results of the perturbation budget in trigger construction are shown in Table~\ref{table:sup_abla_per}. Experimental results show that the tracking performance does not significantly improve when the perturbation budget exceeds 16/255. Therefore, considering the concealment of the triggers, we chose a perturbation budget of 16/255.

\begin{table*}[h]
\caption{Ablation results of the perturbation budget with a single QA pair ``\textit{Q: Detecting copyright. A: ICLR Conference.}" on ChEBI-20 dataset.}
\centering
\begin{tabular}{lcccccccc}

\toprule
Budget & 1 & 2 & 4 & 8 & 16 & 32 & 64 & 128\\
\midrule
% \midrule
% Ordinary & 3\%  & 4\% & 8\% & 6\%\\ 
PLA(Ours) & 0\% & 0\% & 0\% & 19\% & 58\% & 63\% & 52\% & 59\% \\
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_abla_per}
\end{table*}

\subsection{Ablation of Attack Steps}

The ablation results of attack steps in trigger construction are shown in Table~\ref{table:sup_abla_step}. Experimental results show that performance is poor when the number of attack steps is small; as the attack steps approach 1000, performance improves and begins to stabilize.

\begin{table*}[h]
\caption{Ablation results of attack steps with a single QA pair ``\textit{Q: Detecting copyright. A: ICLR Conference.}" on ChEBI-20 dataset.}
\centering
\begin{tabular}{lccccccccccc}

\toprule
Steps & 100 & 200 & 300 & 400 & 500 & 600 & 700 & 800 & 900 & 1000 & 1200\\
\midrule
% \midrule
% Ordinary & 3\%  & 4\% & 8\% & 6\%\\ 
PLA(Ours) & 0\% & 0\% & 10\% & 10\% & 32\% & 35\% & 43\% & 48\% & 56\% & 58\% & 53\%\\
\bottomrule%第三道横线
\end{tabular}
\label{table:sup_abla_step}
\end{table*}


\end{document}
