\section{Related Work}
\textbf{Large vision-language models.} Research on LVLMs has been advancing rapidly, driven by innovative model architectures and specific training strategies~\citep{yin2023, achiam2023gpt, liu2024improved, awadalla2023openflamingo, bai2023qwen, blip2}. Prominent baseline LVLMs such as LLaVA~\citep{llava} and MiniGPT-4~\citep{minigpt4} are generally capable of handling most visual question-answering tasks. Latest models like InternVL 1.5~\citep{chen2024far} support higher-resolution image inputs and utilize larger-scale image encoders, enabling them to handle more complex or specialized image dialogue tasks~\citep{liu2024llava, li2024mini}. The release of increasingly powerful LVLMs has led to a growing trend of researchers and developers fine-tuning these models for specific applications, which underscores the urgent need for research on copyright tracking of LVLMs.

\textbf{Copyright tracking of LLMs.} With the increasing demand for fine-tuning (large) language models, efforts to protect the copyright of these models have begun to emerge~\citep{kurita2020weight, gu2022watermarking, xu2023instructions, instructfinger}. The common approach involves using backdoor attacks to make the model memorize specific patterns or ``fingerprints'' that persist even after fine-tuning. For instance,~\citet{lishen} inserts trigger text near instructions or questions to create specific fingerprints. ~\citet{instructfinger} utilize rare texts to create fingerprint pairs and train the model with limited data to reduce model damage. While copyright protection for LLMs has been widely studied, there are currently no similar studies that have shifted their focus to LVLMs. Our work aims to bridge this gap by addressing the unique challenges posed by the multimodal nature of LVLMs.

\textbf{Adversarial attacks against LVLMs.} Extensive studies have been conducted on adversarial attacks against LVLMs~\citep{nips23zhao, shayegani2023jailbreak, cui2024robustness, non_iccv, xiangaaai2024, wang2024break}. These studies have shown that even large-scale models lack adversarial robustness, which presents both a challenge and an opportunity to use adversarial attacks for copyright tracking. Instead of compromising LVLMs, our objective is
to leverage adversarial attacks as a tool to safeguard them. In this paper, we utilize targeted attacks against LVLMs to construct triggers for tracking model copyright.


%