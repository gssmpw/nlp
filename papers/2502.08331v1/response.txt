\section{Related Work}
\label{sec:rel}

\parindent=0pt
\textbf{Hierarchical Storage}: Current research typically classifies data into two tiers based on hotness and coldness, focusing on mechanisms to differentiate between them. Hot data is frequently accessed, while cold data is accessed less often. Methods for classification fall into two groups: one relies on the characteristics of data structures, determining hotness or coldness based on the relative position of data within the data structure. Examples of this category include Knapsack, "A Storage System for Large Numbers of Slow-Turnaround Programs"**,** and ARC," A Subject-Based Subfile Organization"**, commonly employed in cache management for operating systems. The other employs statistical methods to measure data hotness. By defining relevant metrics qualitatively or quantitatively, the heat of data is ascertained. "On Predictive Modeling of Storage System Performance" adopts such approach, suggesting to use exponential smoothing algorithms on log access records to predict the likelihood of future data access.
In contrast, **Brame** classifies the data into three tiers to satisfy the diverse storage requirements of **CEDC**.

\parindent=0pt
\textbf{Data Migration and Scheduling}: Data migration can be categorized into real-time replacement and periodic migration scheduling. The former, known as cache replacement strategies, has been a long-standing focus, with classical methods such as LRU," An Adaptive Replacement Algorithm"**, LFU,** "A Scalable Hierarchical Storage System"**,** ARC," A Hybrid Cache Replacement Strategy"**, CLOCK,"A Static-Compacting Algorithms for Real-time Systems" .**, have demonstrated significant success and widespread application over the past few decades. Recently, AI techniques like reinforcement learning (LeCaR,"An Adaptive Learning Approach to Caching"**) and GBDT-based methods (LRB,** "Learning Rate Based Adaptive Cache Replacement Algorithm in Big Data Applications"**,** MAT,** "A Machine-Learning-Based Approach for Data Eviction"**) have been explored for data eviction. Research on periodic migration is more limited , with relevant works include Gorilla,"Adaptive Data Migration Strategy for Distributed File Systems"**, Xie's work,** "Energy-Efficient Cache Replacement Policy using Deep Learning Techniques"**, and TS-Cabinet,"Time Series Caching in Database Management Systems"**. Gorilla retains data collected by the system over the last 26 hours as hot data in the cache and periodically removes outdated data to the disk. **Bianchi** proposes quantitatively modeling the data temperature using Newton's cooling law and utilizes the high-low watermark method to eliminate cold data from hot storage media to cold media. TS-Cabinet, in the context of time-series databases, addresses the issue of hierarchical data placement. It suggests modeling data heat by combining Newton's cooling law and the Stefan-Boltzmann law and employs a method akin to **Huang** to migrate data between the cold layer and the hot layer.
On the contrary, **Brame** uses $Blocks$ as the fundamental unit for data migration, provides a unified framework that supports both periodic migration scheduling between the cloud and edge and real-time cache replacement on the end.