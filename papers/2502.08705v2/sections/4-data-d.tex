%\usepackage[table,tabular,xcdraw]{xcolor}


\section{Data}

\subsection{Data Collection}

% \newcommand{\avllonger}{the Advanced Visualization Lab (AVL), housed at the National Center for Supercomputing Applications, a team of visualization designers who focus on cinematic presentations of scientific data}
\newcommand{\avllonger}{a team of visualization designers who focus on cinematic presentations of scientific data (CSVT)}

A dataset of \nsentences\ sentences was collected from a total of \nreviews\ Amazon reviews of the \nmoviesword\ movies produced by \avllonger. \rt{The mean word count per review is 11.2 with a standard deviation of 9.0 words, and the mean word length across all reviews is 4.6 characters with a standard deviation of 2.7 characters.}
% JPN: ``authors should also report the average word length and total word count per review''
% Mean word length in all reviews: 4.630138888888889
% STDDEV of word length in all reviews: 2.669375183371338
% Mean word count per review: 11.19751166407465
% STDDEV word count per review: 9.004829878946925
%
%AVL movies: 
Movies include: SuperTornado: Anatomy of a MegaDisaster~\citep{superTornado2015}, Birth of Planet Earth~\citep{bope2019}, Solar Superstorms~\citep{solarsuperstorms2013,solarsuperstorms2015}, Seeing the Beginning of Time~\citep{sbot2017}, Space Junk~\citep{spacejunk2012}, and The Jupiter Enigma~\citep{jupterenigma2018}.   
In addition to the \nsentences\ collected for annotation and model training, 400 sentences were collected from Amazon reviews of the Hubble documentary~\citep{hubble2010} for final model evaluation.
Movies were selected in collaboration with the \avlshort\ as this team of visual designers expressed interest in quantifying the impact of their works.
Full reviews were downloaded using the \textsf{selectorlib} software package\footnote{\url{https://pypi.org/project/selectorlib/}}. %and sentences are generated from full comments using \textsf{spaCY}'s \cite{spacy2} sentence tokenizer%\footnote{\url{https://spacy.io/api/sentencizer}}.
In addition to the full text, the date and rating of the review were also collected. 

\subsection{Data Annotation}

\subsubsection{Taxonomy of Scientific Documentary Impact}
In seeking evidence for constructivist learning~\citep[e.g.,][]{bada2015constructivism}, we aim to track written language that indicates a statement about cognition or a statement about engagement \rt{(\autoref{tab:impact_categories})}.
As such, our impact categories are modeled off those used in the analysis of issue-focused documentaries \citep{rezapour2017classification} with several updates for our set of scientific documentaries%, namely any change or affirmation categories for behavior cognition or emotion
\citep[in Table 1,][]{rezapour2017classification}. % have been replaced with the categories of ``Shift in Cognition'', ``Attitudes Toward the Film'', and ``Interest with Science Topic''.
More specifically, while internet comments are collected in too uncontrolled an environment to make any definitive conclusions about learning \citep[e.g.,][]{jensen_putting_2017}, we can identify trends among contributors in their comments associated with learning. Our impact category ``Shift in Cognition'' represents audiences' relative ability to identify terms, features, and concepts, up to and including metacognitive discussion of their own cognitive processes. % \rtc{citation? might not be needed since this is basically stating what we are doing with our category though?}. 
    
In our impact categories scheme, we identify engagement through interest and attitudes (impact categories ``Attitudes Toward the Film'' and ``Interest with Science Topic''). Some measures of interest in films can be measured from collected comments. Although they are not conclusive proof of the existence of, or the strength of, a viewer's interest, patterns across large numbers of viewers can provide useful evidence. 
We use the category ``Interest with Science Topic'' when viewers make statements that evaluate the documentary content in its immediate context. 
The impact category ``Impersonal report'' is used when a viewer describes events in the film, and ``Not Applicable''; is the category used for sentences that do not have a specific relationship to the film or science topic.

Sentiment categories follow the typical three classes of ``Positive'', ``Neutral'' and ``Negative'' \citep[e.g.,][]{Liu2012}.
See \autoref{tab:impact_categories} for an overview of these definitions and examples of the combination of sentiment and impact categories used in this work.



% \begin{easylist}[itemize]


% % From AJ:
% % I've just created this document with my thoughts and some text about the impact categories. I realize that some of my comments are probably not super useful at the current stage of the study, but I'm providing them (a) for future reference, and (b) in case someone asks questions about our rationale. I do strongly feel that we should rename the categories from knowledge->cognition and engagement->affect as these are the more appropriate terms in the learning science field, which I'm hoping doesn't create too much of a cascade of document updating. The next biggest concern I have is understanding how Opinion and Interest are differentiated in this scheme (or if they should be combined.) If we needed to combine them, it would require some effort to change, but it would mostly just be a process of deciding whether each opinion was positive or negative and adding it to the corresponding interest category. Which doesn't seem tooooo horrible.

% % Once I understand the answers to some of my questions, I'd like to revisit my text and better tailor it to the situation. It would also benefit from some citations that I would have to seek out.

% % Here's the document:
% % https://docs.google.com/document/d/1y8va9ozyOXdMJWYezjZzjsjmKih0aerhjP6lFbMJftA/edit?usp=sharing

% % FYI, I mostly used this doc as the basis for what I was describing (I also made comments in this doc). I'm assuming this is the most recent draft of the table that will be included in the paper?

% % Response from Shadi
% % Hi AJ,

% % Hope everything is going well. Sorry for the late reply, it has been a crazy summer with so many deadlines. Thanks so much for your input. I believe some of the decisions we made along the way address your concerns. Here is point-by-point response:

% % AJ: I do strongly feel that we should rename the categories from knowledge->cognition and engagement->affect as these are the more appropriate terms in the learning science field, which I'm hoping doesn't create too much of a cascade of document updating.

% % Shadi: This can be easily done.

% % AJ: The next biggest concern I have is understanding how Opinion and Interest are differentiated in this scheme (or if they should be combined.)

% % Shadi: We decided after the first round of annotation to remove “opinion” from our annotation categories. So, we only have the interest category now and no opinion.

% % Another point from your document:

% % AJ: I don’t love the terms “Positive Shift in Cognition” and “Negative Shift in Cognition”. I don’t think we can confidently say anything has shifted. Thinking is a process, not an achievement. Could we say something like … “Favorable Cognition” vs “Conflicting Cognition”???

% % Shadi: we decided to remove positive and negative from the impact categories when annotating the reviews; so, we did the sentiment annotation and impact as two separate tasks.

% % I think that addresses your concern with positive and negative shifts.

% % Our undergrad students prepared a presentation this summer, you can see the distribution of each label in the annotated data. Here is the link: https://docs.google.com/presentation/d/19O1xhd8ONxPum-TvlKyKYemU1KYYbkmdAJYpjd5sFwA/edit#slide=id.g2e8940d8594_0_3

% % I would be more than happy to set a zoom meeting to discuss these further.

% % Also, I am going to do my best to prepare the paper for ICWSM 2025 (submission deadline: Sep 15, 2024). Let me know if you have any issues or concerns. 
    
%     % && \rt{Here maybe we need some words from AJ discussing how/why these changes are made...}

%     & In seeking evidence for constructivist learning \citep[e.g.,][]{bada2015constructivism}, we are especially interested in tracking written language that indicates a statement about cognition or a statement about engagement.
%     && As such, our impact categories are modeled off those used in analysis of issue-focused documentaries \citep{rezapour2017classification} with several updates for our set of scientific documentaries, namely any change or affirmation categories for behavior cognition or emotion \citep[Table 1, ][]{rezapour2017classification} have been replaced with the categories of ``Shift in Cognition'', ``Attitudes Toward the Film'', and ``Interest with Science Topic''.

    
%     & While internet comments are collected in too uncontrolled an environment to make any definitive conclusions about learning \citep[e.g.,][]{jensen_putting_2017}, we can identify trends among contributors in their comments associated with learning. Our impact category ``Shift in Cognition'' represents audiences' relative ability to identify terms, features, and concepts, up to and including metacognitive discussion of their own cognitive processes \rtc{citation?}. %Comparing the relative frequencies of these two categories can provide evidence for the benefit of \rt{this trials off...}
    
%     & Not only is engagement talk more evidence of learning, but it is also a building block that can lead to behaviors with prosocial impact, like de-stigmatizing and sharing science information with community in informal environments, improving personal science identity which can lead to the growth of future science professionals, and advocating for more research funding, political support, and science-motivated decision making \citep[e.g.,][]{lee_robbins_affective_2022}. In our impact categories scheme, we identify engagement through interest and attitudes (impact categories ``Attitudes Toward the Film'' and ``Interest with Science Topic'').
%     && Some measures of interest are measured from internet comments are collected in this study. Although they are not conclusive proof of the existence of, or the strength of, a viewer's interest, patterns across large numbers of viewers can provide useful evidence. We use the category ``Interest with Science Topic'' when viewers make statements that evaluate the documentary content in its immediate context. 

%     & The impact category ``Impersonal report'' is used when a viewer provides description of events in the film, and ``Not Applicable'' is the category used for sentences which do not have a specific relationship to the film or science topic.
%     %&& Further, we identify statements of subjective opinion with the code ``Personal Opinion''. These are connection-making processes that suggest a viewer is engaged enough to branch out beyond the immediate context and recall previous experiences and beliefs and relate them to the documentary content.
%     %&& We can also look for learning talk in audience members' statements of affect. Measures of ``Positive Affect'' and ``Negative Affect'' show different emotional valences of viewers’ experiences as captured in their online comments.

%     & Sentiment categories follow the typical three classes of ``Positive'', ``Neutral'' and ``Negative'' \citep[e.g.,][]{Liu2012}.
%     && See \autoref{tab:impact_categories} for an overview of these definitions and examples of the combination of sentiment and impact categories used in this work.

% % references for the google docs that have the categories
% % * https://docs.google.com/document/d/1-1_FhXDceLlFXZOeAcB_FA_0XqE7k0l1lXZlePj4GDU/edit
% % * https://docs.google.com/document/d/10PQU97mPYnSHf4nph7_Er7tebvXGiWRU94QWdNcesaI/edit
% % * https://docs.google.com/document/d/1HOMGEQGD7-nJERBXGaBTRqMoGHC54Z_vHO8uZlBp9f8/edit
    
% \end{easylist}

%\setlength\cmidrulewidth{0.01\lightrulewidth}
%\begin{center}
\begin{table*}[t]
\resizebox{0.9\textwidth}{!}{
%\begin{tabular}{ |c|c|c|c|c| } {|p{1.5in}|p{1.5in}|}
\begin{tabular}{ @{}p{1.4in}p{1.6in}p{1.6in}p{1.6in}p{1.6in}@{} }
\toprule 
\multicolumn{1}{c}{Impact Category} & \multicolumn{3}{c}{Sentiment Category}
\\\cmidrule(l){2-4}
% \multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
 & \multicolumn{1}{c}{Positive (+)} & \multicolumn{1}{c}{Neutral (0)} & \multicolumn{1}{c}{Negative (-)} \\\midrule
%\headercell{Impact Category} & \multicolumn{4}{c@{}}{Sentiment Category}\\
%Impact Category & Positive & Neutral & Negative \\
%\multirow{3}{4em}{Shift in Knowledge} & Person gained knowledge & No change in knowledge & Person expressed disagreement with scientific concepts presented in film \\
% \multirow{3}{4em}{Multiple row} & cell2 & cell3 \\ 
% & cell5 & cell6 \\ 
% & cell8 & cell9 \\ 
%And I would especially be more descriptive where it says “Person gained knowledge”, maybe to something like “Person encountered a new idea or way of thinking”?
Shift in Cognition (C)\rt{: ability to identify terms, features, and concepts, up to and including
metacognitive elements} & Person encountered a new idea or way of thinking & No change in way of thinking & Person expressed disagreement with scientific concepts presented in film \\ 
 % & & & \\
 & ``very informative about the development of tornados and the research behind their formation'' & ``We need to reign in the junk orbiting our planet before we lose something more important than the satellite Iridium 23, like a human life!'' & ``Unfortunately, HUGE lack of actual facts.'' \\
\hline
Attitudes Toward the Film (A)\rt{: evaluations of film technical and artistic attributes} & Person indicates positive attributes about film attributes & Person simply acknowledges film attributes & Person expresses dislike of film attributes \\ 
 % & & & \\
 & ``Way cool illustrations of our solar friend and how she can be a bad girl.'' & ``More technical than I thought it would be.'' & ``How could something so AWESOME and real life be made to be so boring?'' \\
\hline
Interest with Science Topic (S)\rt{: evaluations and connections to film scientific content} & Person expresses positive interest in science topic presented in film & Person expresses a connection with the science topic & Person expresses disinterest in science topic \\ 
 % & & & \\
 & ``Space intrigues me greatly, and this was an amazing program.'' & ``I've followed supercomputing since inception (and currently run some of their software).'' &  ``used to intrigue me as a child, but knowing reality now, I seek real answers over lies.''%``used to intrigue me as a child, but knowing reality and lies are now to be believed over actual evidence leaves me wanting real answers.'' 
 \\
\hline
Impersonal report (I)\rt{: descriptions of events in a film without evaluation} & Person describes events in the film with positive connotations & Person describes events in the film & Person describes events in the film with negative connotations \\ 
 % & & & \\
 & ``This video reveals the cutting edge science gathered by the Juno Mission to Jupiter.'' & ``There's not much footage of the Joplin Tornado itself, most of this is footage of the wreckage.'' & ``When there were other scenes to show a specific class of Space Junk then what you saw was simply a different geometric shape and that is it.'' \\
\hline 
Not Applicable (N)\rt{: statements with no specific relation to film or science topic} & Positive comment, but no specific relation to film or topic & Comment has no specific relation to film or topic & Negative comment, but no specific relation to film or topic  \\ 
 % & & & \\
 & ``Interesting'' & ``This has never happened before.'' & ``Documentaries have tried to be too interesting lately.'' \\
\bottomrule
\end{tabular}}
\caption{Sentiment and impact categories with definitions and examples.}
\label{tab:impact_categories}
\end{table*}

\subsection{Annotation Procedure}
After reviewing a sample of reviews, we noticed that individual sentences within a review can address different impacts and may express opposite sentiments. As a result, we opted to perform annotations at the sentence level. We used the Zooniverse\footnote{\url{https://www.zooniverse.org/}} citizen science platform, one of the largest platforms for non-professionals to participate in the scientific process, for the annotation process. 
%Zooniverse boasts over one million active members collaborating on hundreds of projects ranging from the space sciences to transcription of historical documents%\footnote{\url{https://www.zooniverse.org/about}}.

Using Zooniverse's default interface for textual data, we displayed a single sentence from a review followed by the sentence within the full review for context.
This display and the first stage of the annotation process is depicted in \autoref{fig:first_step_annotation} in the Appendix in which the user is asked to select their first choice for the sentiment category (Positive, Negative, or Neutral).
% Here, ``first choice'' is defined as the strongest sentiment expressed in the statement. If there were two equally strong sentiments expressed, then the sentiment that appeared first was selected as the ``first choice'' (the analysis of ``second-choice'' selections is relegated to a future paper).
% Once the user made their selection, they could select ``Done'' to move to the next stage of the annotation (green button in \autoref{fig:first_step_annotation}) or ``Done \& Talk'' to ask questions in the main forum (blue button in \autoref{fig:first_step_annotation}).
% % Annotators can also click the ``Favorite'' button (heart symbol) to save the specific annotation to their personal collection for later discussion or can access more information about the annotation (URL and film title) with the ``Info'' button (circled ``i'' symbol).
% % After completion of the first choice sentiment annotation, the user is prompted to self report their confidence in their annotation (with levels of ``I am confident'', ``I am a little unsure'', and ``I am a lot unsure'').
% The annotator was then prompted to repeat the process with their first choice for impact category (categories listed in the first column of \autoref{tab:impact_categories}).

% Once the user reports their confidence, the first choice impact category (categories listed in first column of \autoref{tab:impact_categories}) is annotated.  Here, ``first choice'' has the same overall definition as with the sentiment category (strongest, or first present).
% After the impact category is selected and ``Done'' (or ``Done \& Talk'') is selected, the user is then prompted for their second-choice selections for sentiment and impact, with an option to select ``no second choice present'' if there is no apparent secondary sentiment or impact category.
% After each annotation prompt (first and second choice sentiment, first and second choice impact), the user is prompted to self report their confidence level.

Annotation of each of the \nsentences\ sentences occured in batches of $\approx$200 sentences, with each sentence in a batch ``retired'' once three users have completed annotations on the sentence.
% timing calculation doc: https://colab.research.google.com/drive/1vDz8tjlB0H9nnt5G1NKtJZCSyonXaoAx?authuser=1#scrollTo=HJNfDtrwSniY
%The median annotation time per sentence is $\approx$20 seconds, however the spread is large with an IQR of $\approx$20 seconds.  This is likely due to users performing annotations ``in the background'' while completing other tasks.
The breakdown of the number of annotated sentences per movie is shown in \autoref{tab:movie_info}.

% In what follows, we make use of only first choice sentiment and impact categories in our analysis, with an analysis using self reported confidence levels and secondary categories relegated to a subsequent paper.


\subsection{Data Cleaning and Analysis} \label{sec:datacleaning}
%Raw annotations are downloaded from the Zooniverse platform and processed to determine intercoder metrics and final annotation categories.
After completing the annotation on \sentiment\ and \impact, we used Cohen's Kappa \citep{cohen1960coefficient} to measure the degree of reliability and agreement between the annotators. For sentiment categories, the kappa score was relatively consistent across each category with 0.66 for ``Positive'' sentiments, 0.67 for ``Neutral'' sentiments, and 0.68 for ``Negative'' sentiments.
For impact categories, the lowest Kappa measurement was for ``Impersonal Report'' (0.61) with higher measurements for ``Interest in Science Topic'', ``Shift in Cognition'' and ``Not applicable'' (0.67) along with ``Attitudes Toward the Film'' (0.68).

Final sentiment and impact categories were decided by a majority vote. In $\approx$10\% of the sentences, where no majority consensus was possible, all annotators discussed each sentence in a group and either a majority consensus was reached, or the sentence was marked as ``no agreement''. This resulted in eight ``no agreement'' sentences, which we excluded from our analysis.
Finally, we excluded sentences that were only emojis or consisted of solely punctuation marks (e.g., ``:D'').
% The composition of the remaining \nsentencesNoTBDs\ sentences in sentiment and impact categories is shown as a percentages heatmap in \autoref{fig:sentiment_impact} with the per-media breakdown of sentiment and impact category shown in \autoref{fig:sentiment_by_film} and \autoref{fig:impact_by_film}, respectively.
The composition of the remaining \nsentencesNoTBDs\ sentences in sentiment and impact categories is shown as a percentages heatmap in \autoref{fig:sentiment_impact} with the per-media breakdown in raw numbers of sentiments of sentiment and impact category per film shown in \autoref{tab:movie_info}.

% \rt{Be sure to cite \autoref{fig:sentiment_by_film} and \autoref{fig:impact_by_film}.}
% \rt{Combine both solar superstorms! same material, slightly different presentations}


%%%% JPN: option -- table with heatmap
\begin{table*}[h]
\begin{center}
\begin{tabular} {@{}lcccccccccc@{}} 
\toprule
\multicolumn{1}{c}{Media} & \multicolumn{1}{c}{Year} & \multicolumn{1}{c}{\# Sentences} & \multicolumn{3}{c}{Sentiment [\%]} & \multicolumn{5}{c}{Impact [\%]} \\\cmidrule(lr){4-6} \cmidrule(l){7-11}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{+} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{I} &  \multicolumn{1}{c}{N} \\\cmidrule(lr){1-3}\cmidrule(lr){4-6} \cmidrule(l){7-11}


Space Junk & 2012 & 137 & \cellcolor[rgb]{0.37673202614379087,0.6530718954248366,0.8224836601307189}53.28 & \cellcolor[rgb]{0.8672664359861592,0.9193540945790081,0.967520184544406}13.14 & \cellcolor[rgb]{0.6718954248366014,0.8143790849673203,0.9006535947712418}33.58 & \cellcolor[rgb]{0.8605767012687427,0.29554786620530565,0.011118800461361017}73.72 & \cellcolor[rgb]{0.9991387927720108,0.9478662053056517,0.8965936178392926}2.92 & \cellcolor[rgb]{0.9982775855440216,0.9349480968858132,0.8716186082276047}5.84 & \cellcolor[rgb]{0.9982775855440216,0.9349480968858132,0.8716186082276047}5.84 & \cellcolor[rgb]{0.9964321414840446,0.9072664359861592,0.8181007304882737}11.68 \\ 
Solar Superstorms$^\dagger$ & 2013(15) & 107 & \cellcolor[rgb]{0.28089196462898885,0.5876201460976547,0.7850826605151865}60.75 & \cellcolor[rgb]{0.8318339100346022,0.8957324106113033,0.9557093425605536}17.76 & \cellcolor[rgb]{0.8023068050749712,0.8760476739715494,0.9458669742406767}21.5 & \cellcolor[rgb]{0.9515570934256055,0.4311418685121107,0.09657823913879277}60.75 & \cellcolor[rgb]{0.9988927335640139,0.9441753171856978,0.8894579008073817}3.74 & \cellcolor[rgb]{0.996555171088043,0.9091118800461361,0.8216685890042291}11.21 & \cellcolor[rgb]{0.9982775855440216,0.9349480968858132,0.8716186082276047}5.61 & \cellcolor[rgb]{0.9942176086120723,0.8610226835832372,0.7259669357939253}18.69 \\ 
SuperTornado & 2015 & 555 & \cellcolor[rgb]{0.36159938485198,0.6427374086889658,0.8165782391387928}54.41 & \cellcolor[rgb]{0.8377393310265283,0.8996693579392541,0.9576778162245291}17.12 & \cellcolor[rgb]{0.7358708189158016,0.8415686274509804,0.923044982698962}28.47 & \cellcolor[rgb]{0.9515570934256055,0.4311418685121107,0.09657823913879277}60.72 & \cellcolor[rgb]{0.9979084967320262,0.9294117647058824,0.8609150326797386}7.03 & \cellcolor[rgb]{0.9961860822760477,0.9035755478662053,0.810965013456363}12.43 & \cellcolor[rgb]{0.9990157631680123,0.9460207612456748,0.8930257593233372}3.42 & \cellcolor[rgb]{0.9949557862360631,0.8772625913110342,0.7584467512495194}16.4 \\ 
Seeing the Beginning of Time & 2017 & 233 & \cellcolor[rgb]{0.35151095732410614,0.6358477508650519,0.812641291810842}55.36 & \cellcolor[rgb]{0.8554555940023069,0.9114801999231065,0.9635832372164552}14.59 & \cellcolor[rgb]{0.7161860822760477,0.8332026143790849,0.916155324875048}30.04 & \cellcolor[rgb]{0.898961937716263,0.3483275663206459,0.039907727797001157}68.67 & \cellcolor[rgb]{0.9966782006920415,0.9109573241061131,0.8252364475201845}10.73 & \cellcolor[rgb]{0.9976624375240293,0.9257208765859285,0.8537793156478277}7.73 & \cellcolor[rgb]{1.0,0.9607843137254902,0.9215686274509803}0.0 & \cellcolor[rgb]{0.9960630526720492,0.9016224529027297,0.8071664744329105}12.88 \\ 
The Jupiter Enigma & 2018 & 219 & \cellcolor[rgb]{0.491764705882353,0.7219684736639754,0.8547789311803152}45.66 & \cellcolor[rgb]{0.7703191080353711,0.8562091503267973,0.9351018838908113}25.57 & \cellcolor[rgb]{0.7309496347558632,0.8394771241830065,0.9213225682429834}28.77 & \cellcolor[rgb]{0.9855132641291812,0.5330103806228376,0.2125951557093427}51.6 & \cellcolor[rgb]{0.996555171088043,0.9091118800461361,0.8216685890042291}10.96 & \cellcolor[rgb]{0.9960630526720492,0.9016224529027297,0.8071664744329105}12.79 & \cellcolor[rgb]{0.9985236447520185,0.938638985005767,0.8787543252595156}5.02 & \cellcolor[rgb]{0.9938485198000769,0.8529027297193387,0.7097270280661284}19.63 \\ 
Birth of Planet Earth & 2019 & 35 & \cellcolor[rgb]{0.4019530949634756,0.6702960399846213,0.8323260284505959}51.43 & \cellcolor[rgb]{0.8584083044982699,0.9134486735870818,0.9645674740484429}14.29 & \cellcolor[rgb]{0.6620530565167244,0.8101960784313725,0.8972087658592849}34.29 & \cellcolor[rgb]{0.8782929642445213,0.31990772779700116,0.024405997693194924}71.43 & \cellcolor[rgb]{0.9991387927720108,0.9478662053056517,0.8965936178392926}2.86 & \cellcolor[rgb]{0.9955709342560554,0.8907958477508651,0.7855132641291811}14.29 & \cellcolor[rgb]{1.0,0.9607843137254902,0.9215686274509803}0.0 & \cellcolor[rgb]{0.9964321414840446,0.9072664359861592,0.8181007304882737}11.43 \\ 




\hline
\end{tabular}
\end{center}
\footnotesize{$^\dagger$ Solar Superstorms sentences come from two media presentations: Solar Superstorms - a TV Episode from ``Cosmic Journeys'' \citep{solarsuperstorms2013} and a full-length documentary Solar Superstorms: Journey to the Center of the Sun \citep{solarsuperstorms2015}.}\\
\caption{Total number of annotated sentences per media.  Single character sentiment and impact codes are defined in \autoref{tab:impact_categories}.}
\label{tab:movie_info}
\end{table*}



%\end{center}


% \begin{table*}
% \begin{center}
% \begin{tabular} {@{}lcccccccccc@{}} %{|p{1.5in}|p{1.5in}|}
% %\begin{tabular}{ |p{1.0in}|p{1.7in}|p{1.7in}|p{1.7in}|p{1.7in}| }

% % \multicolumn{1}{|c|}{Impact Category} & \multicolumn{3}{c|}{Sentiment Category} \\
% % \multicolumn{1}{|c|}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{}\\
% %  & \multicolumn{1}{c}{Positive (+)} & \multicolumn{1}{c}{Neutral (0)} & \multicolumn{1}{c|}{Negative (-)} \\
% \toprule
% \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{Year} & \multicolumn{1}{c}{\# Sentences} & \multicolumn{3}{c}{Sentiment [\%]} & \multicolumn{5}{c}{Impact [\%]} \\\cmidrule(lr){4-6} \cmidrule(l){7-11}
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{+} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{I} &  \multicolumn{1}{c}{N} \\\cmidrule(lr){1-3}\cmidrule(lr){4-6} \cmidrule(l){7-11}


% % Space Junk & 2012 & 137 & 73 & 18 & 46 & 101 & 4 & 8 & 8 & 16 \\ 
% % Solar Superstorms$^\dagger$ & 2013(15) & 107 & 65 & 19 & 23 & 65 & 4 & 12 & 6 & 20 \\ 
% % SuperTornado & 2015 & 555 & 302 & 95 & 158 & 337 & 39 & 69 & 19 & 91 \\ 
% % Seeing the Beginning of Time & 2017 & 233 & 129 & 34 & 70 & 160 & 25 & 18 & 0 & 30 \\ 
% % The Jupiter Enigma & 2018 & 219 & 100 & 56 & 63 & 113 & 24 & 28 & 11 & 43 \\ 
% % Birth of Planet Earth & 2019 & 35 & 18 & 5 & 12 & 25 & 1 & 5 & 0 & 4 \\

% Space Junk & 2012 & 137 & 53.28 & 13.14 & 33.58 & 73.72 & 2.92 & 5.84 & 5.84 & 11.68 \\ 
% Solar Superstorms$^\dagger$ & 2013(15) & 107 & 60.75 & 17.76 & 21.5 & 60.75 & 3.74 & 11.21 & 5.61 & 18.69 \\ 
% SuperTornado & 2015 & 555 & 54.41 & 17.12 & 28.47 & 60.72 & 7.03 & 12.43 & 3.42 & 16.4 \\ 
% Seeing the Beginning of Time & 2017 & 233 & 55.36 & 14.59 & 30.04 & 68.67 & 10.73 & 7.73 & 0.0 & 12.88 \\ 
% The Jupiter Enigma & 2018 & 219 & 45.66 & 25.57 & 28.77 & 51.6 & 10.96 & 12.79 & 5.02 & 19.63 \\ 
% Birth of Planet Earth & 2019 & 35 & 51.43 & 14.29 & 34.29 & 71.43 & 2.86 & 14.29 & 0.0 & 11.43 \\ 

% \hline
% \end{tabular}
% \end{center}
% %\footnotetext[$^\dagger$]{The smallest spatial unit is county}
% \footnotesize{$^\dagger$ Solar Superstorms sentences come from two media presentations: Solar Superstorms - a TV Episode from ``Cosmic Journeys'' \citep{solarsuperstorms2013} and a full-length documentary Solar Superstorms: Journey to the Center of the Sun \citep{solarsuperstorms2015}.}\\
% \caption{\rt{OPTION 1 FOR THIS TABLE: }Total number of annotated sentences per media.  Single character sentiment and impact codes are defined in \autoref{tab:impact_categories}.}
% \label{tab:movie_info}
% \end{table*}




% %%%% JPN: option -- table with heatmap AND lognorm
% \begin{table*}
% \begin{center}
% \begin{tabular} {@{}lcccccccccc@{}} 
% \toprule
% \multicolumn{1}{c}{Media} & \multicolumn{1}{c}{Year} & \multicolumn{1}{c}{\# Sentences} & \multicolumn{3}{c}{Sentiment [\%]} & \multicolumn{5}{c}{Impact [\%]} \\\cmidrule(lr){4-6} \cmidrule(l){7-11}
% \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{+} & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{A} & \multicolumn{1}{c}{S} & \multicolumn{1}{c}{C} & \multicolumn{1}{c}{I} &  \multicolumn{1}{c}{N} \\\cmidrule(lr){1-3}\cmidrule(lr){4-6} \cmidrule(l){7-11}


% % Space Junk & 2012 & 137 & 73 & 18 & 46 & \cellcolor[rgb]{0.1,0.5,0.8} 73.72 & 2.92 & 5.84 & 5.84 & 11.68 \\ 
% % Solar Superstorms$^\dagger$ & 2013(15) & 107 & 65 & 19 & 23 & 60.75 & 3.74 & 11.21 & 5.61 & 18.69 \\ 
% % SuperTornado & 2015 & 555 & 302 & 95 & 158 & 60.72 & 7.03 & 12.43 & 3.42 & 16.4 \\ 
% % Seeing the Beginning of Time & 2017 & 233 & 129 & 34 & 70 & 68.67 & 10.73 & 7.73 & 0.0 & 12.88 \\ 
% % The Jupiter Enigma & 2018 & 219 & 100 & 56 & 63 & 51.6 & 10.96 & 12.79 & 5.02 & 19.63 \\ 
% % Birth of Planet Earth & 2019 & 35 & 18 & 5 & 12 & 71.43 & 2.86 & 14.29 & 0.0 & 11.43 \\ 

% Space Junk & 2012 & 137 & \cellcolor[rgb]{0.949151,0.948435,0.152178}53.28 & \cellcolor[rgb]{0.993032,0.692907,0.189084}13.14 & \cellcolor[rgb]{0.977995,0.861432,0.142808}33.58 & \cellcolor[rgb]{0.993248,0.906157,0.143936}73.72 & \cellcolor[rgb]{0.180653,0.701402,0.488189}2.92 & \cellcolor[rgb]{0.304148,0.764704,0.419943}5.84 & \cellcolor[rgb]{0.304148,0.764704,0.419943}5.84 & \cellcolor[rgb]{0.477504,0.821444,0.318195}11.68 \\ 
% Solar Superstorms$^\dagger$ & 2013(15) & 107 & \cellcolor[rgb]{0.940015,0.975158,0.131326}60.75 & \cellcolor[rgb]{0.994355,0.746995,0.163821}17.76 & \cellcolor[rgb]{0.992505,0.777967,0.152855}21.5 & \cellcolor[rgb]{0.945636,0.899815,0.112838}60.75 & \cellcolor[rgb]{0.220124,0.725509,0.466226}3.74 & \cellcolor[rgb]{0.458674,0.816363,0.329727}11.21 & \cellcolor[rgb]{0.296479,0.761561,0.424223}5.61 & \cellcolor[rgb]{0.606045,0.850733,0.236712}18.69 \\ 
% SuperTornado & 2015 & 555 & \cellcolor[rgb]{0.946602,0.95519,0.150328}54.41 & \cellcolor[rgb]{0.994495,0.74088,0.166335}17.12 & \cellcolor[rgb]{0.985314,0.828846,0.142945}28.47 & \cellcolor[rgb]{0.945636,0.899815,0.112838}60.72 & \cellcolor[rgb]{0.344074,0.780029,0.397381}7.03 & \cellcolor[rgb]{0.487026,0.823929,0.312321}12.43 & \cellcolor[rgb]{0.202219,0.715272,0.476084}3.42 & \cellcolor[rgb]{0.565498,0.84243,0.262877}16.4 \\ 
% Seeing the Beginning of Time & 2017 & 233 & \cellcolor[rgb]{0.946602,0.95519,0.150328}55.36 & \cellcolor[rgb]{0.994103,0.710698,0.180097}14.59 & \cellcolor[rgb]{0.982653,0.841812,0.142303}30.04 & \cellcolor[rgb]{0.974417,0.90359,0.130215}68.67 & \cellcolor[rgb]{0.449368,0.813768,0.335384}10.73 & \cellcolor[rgb]{0.369214,0.788888,0.382914}7.73 & \cellcolor[rgb]{0.267004,0.004874,0.329415}0.0 & \cellcolor[rgb]{0.496615,0.826376,0.306377}12.88 \\ 
% The Jupiter Enigma & 2018 & 219 & \cellcolor[rgb]{0.959276,0.921407,0.151566}45.66 & \cellcolor[rgb]{0.988648,0.809579,0.145357}25.57 & \cellcolor[rgb]{0.984031,0.835315,0.142528}28.77 & \cellcolor[rgb]{0.89632,0.893616,0.096335}51.6 & \cellcolor[rgb]{0.458674,0.816363,0.329727}10.96 & \cellcolor[rgb]{0.496615,0.826376,0.306377}12.79 & \cellcolor[rgb]{0.274149,0.751988,0.436601}5.02 & \cellcolor[rgb]{0.616293,0.852709,0.230052}19.63 \\ 
% Birth of Planet Earth & 2019 & 35 & \cellcolor[rgb]{0.951726,0.941671,0.152925}51.43 & \cellcolor[rgb]{0.994103,0.710698,0.180097}14.29 & \cellcolor[rgb]{0.976265,0.868016,0.143351}34.29 & \cellcolor[rgb]{0.983868,0.904867,0.136897}71.43 & \cellcolor[rgb]{0.175707,0.6979,0.491033}2.86 & \cellcolor[rgb]{0.525776,0.833491,0.288127}14.29 & \cellcolor[rgb]{0.267004,0.004874,0.329415}0.0 & \cellcolor[rgb]{0.468053,0.818921,0.323998}11.43 \\ 



% \hline
% \end{tabular}
% \end{center}
% \footnote{$^\dagger$ Solar Superstorms sentences come from two media presentations: Solar Superstorms - a TV Episode from ``Cosmic Journeys'' \citep{solarsuperstorms2013} and a full-length documentary Solar Superstorms: Journey to the Center of the Sun \citep{solarsuperstorms2015}.}\\
% \caption{\rt{OPTION 3 FOR THIS TABLE: }Total number of annotated sentences per media.  Single character sentiment and impact codes are defined in \autoref{tab:impact_categories}.}
% \label{tab:movie_info3}
% \end{table*}



\begin{figure}[t]
\centering
% \includegraphics[width=0.9\columnwidth]{sections/figures/annotation_sentiment_impacts.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\includegraphics[width=0.7\columnwidth]{sections/figures/annotation_sentiment_impacts_c6_T.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Breakdown of full \nsentencesNoTBDs\ sentences in the annotated dataset as described in the ``Data'' section.  Numbers are percentages of the total sentences in the dataset within each Sentiment and Impact combination.}
%\caption{\rt{This shows the breakdown of impact analysis and sentiment spreads for final annotations.  My guess is that we want to use just one or other other of these plots (and maybe make it only the size of a column instead of full page?  Maybe numbers?  I like percentages numerically though...}}.
\label{fig:sentiment_impact}
\end{figure}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{sections/figures/sentiment_by_film.pdf} 
% \caption{Breakdown of sentiment by film with films ordered from oldest (Space Junk, 2012) to newest (Birth of Planet Earth 2019).  Percentages are calculated as totals over all sentences of a specific film.}
% \label{fig:sentiment_by_film}
% \end{figure*}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{sections/figures/impact_by_film.pdf} 
% \caption{Breakdown of impact by film with films ordered from oldest (Space Junk, 2012) to newest (Birth of Planet Earth 2019).  Percentages are calculated as totals over all sentences of a specific film.}
% \label{fig:impact_by_film}
% \end{figure*}
