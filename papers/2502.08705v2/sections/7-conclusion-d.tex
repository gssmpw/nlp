
\section{Conclusion and Future Work}
This paper analyzes the impact of scientific films using user-generated reviews and measures the impact using a novel taxonomy of change and engagement. 
Beyond the immediate goals of \avlshort\ the results of our best model as outlined in \autoref{tab:Classification Result Sentiment} and \autoref{section:classification} indicate our model's efficiency in predicting impact and sentiment which can be useful in future productions assessment by the \avlshort\ and other teams to assess the impact of their produced works. Additionally, the results of our generalizability analysis presented in \autoref{tab:Hubble_Prediction} and \autoref{section:generalizability} indicate the model should be applicable to a wide range of scientific CSV-style documentaries.
%as discussed in \autoref{section:classification_models} and \autoref{section:classification}, 
Overall, our results underscore the multifaceted impact of scientific documentaries. They engage audiences emotionally, provoke intellectual curiosity, and influence perceptions, making them a powerful medium for science communication. Our findings contribute to a growing body of evidence on the effectiveness of media in bridging the gap between science and the public, emphasizing the value of leveraging documentaries to achieve broader educational and societal goals. Future research could explore the long-term impacts of such engagement, particularly in terms of sustained interest in science and changes in behavior or policy advocacy.

The \avlshort\ expressed interest in tying specific learning goals to sentiment and impact categories and/or impact themes.
As learning goals are often developed to guide the creation of these scientific documentaries and are integral to grant funding, our large-scale quantitative analysis of the impact of these documentaries would be a useful complement to the smaller-scale audience-based feedback the \avlshort\ currently collects.
While our model was trained and tested on Amazon comments, our collaboration also has access to YouTube comments for a subset of the analyzed films. Future work will include an analysis of these comments, however, there is evidence of a bias toward more negative reviews on YouTube \citep[e.g.][]{ardestani2024youtube}, making such a comparison a test of the inter-platform generalizability of our model.

 %While beyond the scope of the present work, future research with a larger dataset of cross-platform and multilingual input could benefit from this more detailed analysis.



\section{Limitations and Ethics Statement}
Our data collection is limited only to English reviews, impacting the generalizability of our findings.
We also limited the scope of our study to only one platform, i.e., Amazon. This can enable platform-specific biases to influence the findings, and classification performance and cause potential inaccuracies.
In addition, media impact reviews are self-reported and collected only once. Without access to follow-up reviews or more nuanced, contextualized reports, we cannot be certain of the persistence and durability of such reported impacts. 
Finally, we acknowledge that LLM-based classifiers often make mistakes in predicting the best label, resulting in misclassified reviews. Future improvements in these models and better prompt engineering can enhance the performance of LLM classifiers. 


