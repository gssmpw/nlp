\section{Methodology}

\subsection{Classification Models} \label{section:classification_models}
We leveraged three types of classifiers for sentiment and impact classification: (1) three traditional machine learning algorithms, (2) two transformer-based models, and (3) five large language models. 
To create a train and test set, we split the data into 70\% training and 30\% test sets. For transformer-based models, we use 20\% of the training data for validation.
Stratified data split was used based on \impact\ labels to split the data.
We measured the classification performance on the test set after training each model using precision, recall, and F1 scores. 


\paragraph{Baseline model: }
We applied three baseline models, i.e., Support Vector Machine (SVM), Logistic Regression, and Decision Tree Classifier. 
We used Term Frequency-Inverse Document Frequency (TF-IDF) to vectorize the sentences. 

\paragraph{Transformer-based models: }
We leveraged two transformer-based models, i.e., BERT \cite{DBLP:journals/corr/abs-1810-04805} and RoBERTa \cite{DBLP:journals/corr/abs-1907-11692}, to classify sentences. Models were fine-tuned using pre-trained weights from bert-base-uncased and roberta-base, respectively. The sentences were tokenized using the corresponding tokenizers (BertTokenizer or RobertaTokenizer).
Fine-tuning was performed using the Hugging Face API\footnote{https://huggingface.co/}, and hyperparameters were set to three epochs, a batch size of 8, a learning rate scheduler with 500 warmup steps, and a weight decay of 0.01. Model evaluation occurred at the end of each epoch using the validation set.

\paragraph{LLMs: }
We used three closed-weight models from OpenAI: GPT3.5 (gpt-3.5-turbo-0125) \cite{openai2023chatgpt}, GPT4 (gpt-4-turbo-2024-04-09) \cite{openai2023gpt4}, GPT4o (gpt-4o-2024-08-06), and two open-weight LLMs: llama3*8b (llama3-8b-8192) \cite{llama3modelcard} and Mixtral 8*7B (mixtral-8x7b-32768) \cite{jiang2024mixtral} for classification in two different ways: with and without the context (full review) as the textual context for the target sentence.
In the first method (``w/o context''), the model is instructed to predict the correct label for the target sentence based on the definitions provided for each class. 
% 
This is akin to having the model classify on only the top sentence (without the ``Full review'') depicted in \autoref{fig:first_step_annotation}.
%
In the second approach (``with context''), the full review is included in the prompt to guide the model in predicting the correct label for the target sentence. This is similar to providing the ``Full review'' shown in \autoref{fig:first_step_annotation} and indicating the target sentence.
For the LLM-based classifiers, we used the test set to evaluate the models' performance as well as the efficiency of each prompting method.
Temperature was set to 0 for all model experiments for maximum consistency.  


\paragraph{Fine-tuned LLMs: }
To explore the helpfulness of additional fine-tuning for the classification purpose, we finetuned GPT4o (gpt-4o-2024-08-06) using the training set.
After fine-tuning, we repeated the classification task with the same prompts to predict the label for each sentence. The performance was then evaluated on the test set to assess the improvement compared to the base model. 


\subsection{Thematic Analysis}
To develop a richer understanding of the contextual nuances related to each impact, we employed thematic analysis to extract themes and patterns associated with each impact category. For this purpose, we conducted an in-depth analysis of sentences tagged with each impact category using LLooM \cite{lam2024concept}, an advanced framework for concept induction powered by LLMs.
