%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%%\documentclass[manuscript,review,anonymous]{acmart}
\documentclass[sigconf]{acmart}


\usepackage{tabularx}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{natbib} 
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{stfloats}
\widowpenalty=10000  % Avoid single lines at the top of a page
\clubpenalty=10000   % Avoid single lines at the bottom of a page

\newcommand{\eg}{e.g.,}
\newcommand{\ie}{i.e.,}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\bpstart}[1]{\vspace{1mm}\noindent{\textbf{#1.}}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by-sa}
\acmConference[CHI '25]{CHI Conference on Human Factors in Computing Systems}{April 26-May 1, 2025}{Yokohama, Japan}
\acmBooktitle{CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan}\acmDOI{10.1145/3706598.3713158}
\acmISBN{979-8-4007-1394-1/25/04}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title["It Brought the Model to Life"]{"It Brought the Model to Life": Exploring the Embodiment of Multimodal I3Ms for People who are Blind or have Low Vision}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Samuel Reinders}
\email{samuel.reinders@monash.edu}
\orcid{0000-0001-5627-413X}
\affiliation{
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}
}

\author{Matthew Butler}
\email{matthew.butler@monash.edu}
\orcid{0000-0002-7950-5495}
\affiliation{
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}
}

\author{Kim Marriott}
\email{kim.marriott@monash.edu}
\orcid{0000-0002-9813-0377}
\affiliation{
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Reinders et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  3D-printed models are increasingly used to provide people who are blind or have low vision (BLV) with access to maps, educational materials, and museum exhibits. Recent research has explored interactive 3D-printed models (I3Ms) that integrate touch gestures, conversational dialogue, and haptic vibratory feedback to create more engaging interfaces. Prior research with sighted people has found that imbuing machines with human-like behaviours, \ie\, embodying them, can make them appear more lifelike, increasing social perception and presence. Such embodiment can increase engagement and trust. This work presents the first exploration into the design of embodied I3Ms and their impact on BLV engagement and trust. In a controlled study with 12 BLV participants, we found that I3Ms using specific embodiment design factors, such as haptic vibratory and embodied personified voices, led to an increased sense of liveliness and embodiment, as well as engagement, but had mixed impact on trust.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10011738</concept_id>
       <concept_desc>Human-centered computing~Accessibility</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003124.10010870</concept_id>
       <concept_desc>Human-centered computing~Natural language interfaces</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Accessibility}
\ccsdesc[500]{Human-centered computing~Natural language interfaces}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{3D-Printed Models, Accessibility, Conversational Agents, Embodiment, Engagement, Trust, Blindness}

\begin{teaserfigure}
  \includegraphics[width=1.00\columnwidth]{graphics/teaser-graphic.png}
  %TC:ignore
  \caption{We designed two interactive 3D-printed models (I3Ms) that could operate using various embodied design factors. Shown here is the Egyptian Pyramid I3M operating in High Embodied Mode (HEM). In A), the model introduces itself as a user picks up the Sphinx; In B), the user holds the Great Pyramid and Pyramid of Menkaure and asks which is larger. The Great Pyramid then emits localised embodied haptics and responds using an embodied personified voice and first-person narration.}~\label{fig:teaser}
  \Description{This figure includes two images showcasing one of the interactive 3D-printed models (I3Ms) we designed. Seen is the Egyptian Pyramid I3M operating in High Embodied Mode (HEM). It includes a base made of acrylic that is painted gold. It includes four 3D-printed objects -- the Sphinx, and the Pyramids of Khufu, Menkaure, and Khafre. They are also painted gold. In A), the model introduces itself as a user picks up the Sphinx; In B), the user holds the Great Pyramid and Pyramid of Menkaure and asks which is larger, to which the Great Pyramid emits localised haptics and responds using an embodied personified voice and first-person narration.}
  %TC:endignore  
\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The lack of equitable access to graphical information, such as educational materials and navigation maps, can significantly reduce opportunities and overall quality of life for people who are blind or have low vision (BLV)~\cite{butler2017understanding,Sheffield2016}. In recent years, 3D printing has been used to create accessible graphics~\cite{Stangl2015,Buehler2016,Holloway2018,Hu2015}. Unlike traditional accessible graphics, 3D printing enables the fabrication of tangible models that directly represent three-dimensional objects and concepts. This allows a broader range of content to be effectively conveyed non-visually. 3D-printed models have demonstrated improved tactual understanding and mental model development compared to tactile graphics, as well as increased engagement~\cite{Holloway2018}. 

It is now becoming common to add button or touch-triggered audio labels to 3D-printed models~\cite{Ghodke2019,Holloway2018,Shi2016,Davis2020}, creating \textbf{interactive 3D-printed models (I3Ms)}. Audio labels support independent exploration and reduce the need for braille labels, which many blind people cannot read~\cite{NFIB2009}, may not fit on the model~\cite{Holloway2018}, and can distort the model surface~\cite{Holloway2018,Shi2016}. However, while useful, audio labels can only provide limited, predetermined information. With advances in intelligent agents, recent work has begun integrating conversational agents into I3Ms~\cite{Shi2017b,Quero2019,Reinders2020,Reinders2023}. These models afford greater agency and independence to BLV users, allowing them to generate their own queries and potentially access unlimited information about the model. One study involving the co-design of an I3M that combined touch gestures, conversational dialogue, and haptic vibratory feedback, found that participants desired an experience that felt more personal and `\textit{alive}'~\cite{Reinders2023}.

Previous research has shown that imbuing machines with human-like behaviours and characteristics, that is -- embodying them -- can make them appear more lifelike and alive, increasing social perception and presence~\cite{Cassell2001,Lankton2015,Lester1997,Nowak2003,Shamekhi2018}. Such embodiment has been found to increase subjective user engagement~\cite{Cassell2001,Shamekhi2018} and trust~\cite{Bickmore2001,Bickmore2013,Rheu2021,Shamekhi2018}. A variety of \textbf{conversational embodiment} factors have been identified to increase embodiment, including speech that mimics human voices~\cite{Cassell2001}, greeting users~\cite{Luria2019,Cassell2001,Shamekhi2018,Lester1997,Nowak2003}, conversational turn-taking~\cite{Cassell2001,Kontogiorgos2020}, small talk~\cite{Liao2018,Pradhan2019,Shamekhi2018,Cassell2001,Bickmore2001}, and exhibiting personality~\cite{Lester1997}. \textbf{Visual embodiment} attributes, such as giving the agent a face~\cite{Shamekhi2018,Bickmore2001,Bickmore2013,Kontogiorgos2020}, gestures~\cite{Cassell2001,Bickmore2001}, and employing gaze~\cite{Kontogiorgos2020,Shamekhi2018}, have also been found to increase embodiment, as has \textbf{physical embodiment}~\cite{Luria2017,Kidd2004}.

However, virtually all prior research has been conducted with sighted users and has not considered BLV users, who may not be able to fully discern or perceive visual characteristics or physical embodiment. Since the usefulness of an accessible graphic or interface depends on users' willingness to engage with and accept the information it provides~\cite{Betsy1993,Wu2017,Abdolrahmani2018}, engagement and trust are critical for BLV users. We believe that embodiment, and its links with engagement and trust, may hold significant potential for I3Ms, which are designed to be spoken to, picked up, and touched. Embodied I3Ms could enable BLV students or self-learners to engage in deeper, more meaningful experiences with content, and therefore serve as a catalyst for their broader adoption. Here, we present the first exploration into the design of more embodied I3Ms and the impact of embodiment on the engagement and trust of BLV users. We believe our study is the first to explore embodiment in the context of both BLV users and I3Ms. 

We selected five non-visual design factors and created two I3Ms -- the \textbf{Saturn V Rocket} and \textbf{Egyptian Pyramids} -- that could be configured in two states -- \textit{High Embodied Mode (HEM)} and \textit{Low Embodied Mode (LEM)}. These states differed based on the embodiment design factors. Those factors relating to conversational embodiment (\textit{introductions and small talk, embodied personified voices} and \textit{embodied narration style}) have previously been found to increase embodiment. Physical embodiment factors (\textit{embodied haptic vibratory feedback} and \textit{location of speech output}) were more novel and were motivated by feedback from BLV users interacting with I3Ms~\cite{Reinders2023}. We conducted a within-subject user study with 12 BLV participants, using established questionnaires and subjective ratings to examine how \textit{lively}, \textit{engaging} and \textit{trustworthy} participants perceived each model to be. The main findings of our study include:

\begin{itemize}
    \item Participants perceived the HEM I3Ms as having a greater sense of liveliness, appearing more embodied compared to LEM I3Ms; 
    \item HEM I3Ms appeared to be more engaging. This adds to research showing that more embodied conversational agents and social robots increase subjective user engagement, establishing that this relationship extends to I3Ms in the context of BLV users;
    \item Differences in trust between LEM and HEM I3Ms were inconclusive, suggesting the impact of embodiment on trust may be more limited.
\end{itemize}

Our findings, which represent the first exploration into the embodiment and social perception of embodied I3Ms, provide initial design recommendations for creating I3Ms that BLV users find engaging. These recommendations will be of critical interest to the accessibility research community and to practitioners designing I3Ms for accessible exhibits in public spaces, such as museums and galleries, or as accessible materials for classroom use. 

\section{Related Work}
This work builds on research on: accessible graphics and interactive 3D-printed models for BLV users; conversational agents; and embodied agents.

\subsection{Accessible Graphics}
BLV people face challenges accessing graphical information, which impacts education opportunities~\cite{butler2017understanding}, makes independent travel difficult~\cite{Sheffield2016}, and causes disengagement with culture and the creative arts~\cite{Bartlett2019}. These barriers can lead to reductions in confidence and overall quality of life~\cite{Keeffe2005}.

Graphical information can be made available in formats that improve non-visual access. Traditionally, accessible graphics -- known as raised line drawings or \textbf{tactile graphics} -- have been used to assist BLV people in accessing information. Tactile graphics are frequently used to facilitate classroom learning~\cite{Aldrich2001,Rosenblum2015} and orientation and mobility (O\&M) training~\cite{Blades1999,Rowell2005}. Work has been conducted on the development of \textbf{interactive tactile graphics}, including the NOMAD~\cite{NOMAD}, IVEO~\cite{IVEO}, and Talking Tactile Tablet~\cite{Miele2006,TTT}. These devices combine printed tactile overlays with touch-sensitive surfaces, enabling BLV users to explore the graphics tactually and access audio labels by interacting with predefined touch areas. However, as these systems rely on printed tactile graphics, their scope is limited to two-dimensional content.

\textbf{3D-printed models} are an increasingly common alternative to tactile graphics. They enable a broader range of material to be produced, particularly for concepts that are inherently three-dimensional in nature. In recent years, the production cost and effort of 3D-printed models have fallen to levels comparable to tactile graphic production. 3D-printed models are increasingly being applied in various accessible graphic areas: mapping and navigation~\cite{gual2012visual,Holloway2018,Holloway2019b,Holloway2022,Nagassa2023}; special education~\cite{Buehler2016}; art galleries~\cite{karaduman2022beyond,Butler2023}; books~\cite{kim2015,Stangl2015}; mathematics~\cite{Brown2012,Hu2015}; graphic design~\cite{McDonald2014}; science~\cite{wedler2012applied,Hasper2015}; and programming~\cite{kane2014}. Compared to tactile graphics, 3D-printed models have been shown to improve tactual understanding and mental model development among BLV people~\cite{Holloway2018}. However, as with traditional tactile graphics, the provision of written descriptions or braille labelling presents challenges. The limited space on models and the low-fidelity of 3D-printed braille can significantly impact the utility and readability of labels~\cite{Brown2012,Taylor2015,Shi2016}.

\subsection{Interactive 3D-Printed Models}
To address labelling challenges, limitations on the type of content that can be produced, and to create more engaging and interactive experiences, there is growing interest in the development of \textbf{interactive 3D-printed models (I3Ms)}. By combining 3D-printed models with low-cost electronics and/or smart devices, many I3Ms now include button or touch-triggered audio labels that provide verbal descriptions of the printed model~\cite{Landau2009,Shi2016,Reichinger2016,Giraud2017,Gotzelmann2017,Holloway2018,Ghodke2019,Davis2020}. Such I3Ms have been applied across various BLV-accessible graphic areas, including: art~\cite{Holloway2019,Bartolome2019,Butler2023}; education~\cite{Ghodke2019,Shi2019,Reinders2020}; and mapping and navigation~\cite{Gotzelmann2017,Holloway2018,Shi2020}. I3Ms with audio labels are especially useful for BLV users who are not fluent braille readers. Stored as text and synthesised in real-time, audio labels are easier to update compared to labels on non-interactive models. Many I3Ms also support multiple levels of audio labelling~\cite{Holloway2018,Shi2019,Reinders2020}, extracted through unique button presses or touch gestures, enabling them to convey far more information than the written descriptions supplied alongside tactile graphics and non-interactive models.

I3Ms are inherently multimodal. Multimodality can improve the adaptability of a system~\cite{Reeves2004}, and when modalities are combined, they can increase the resolution of information the system conveys~\cite{Edwards2015} and enable more natural interactions~\cite{Bolt1980}. For BLV users, combining modalities has been shown to improve confidence and independence~\cite{Quero2021}. Modality adaptability allows BLV users to choose interaction methods based on context, ability, or effort. For example, a user may be uncomfortable engaging in speech interaction in public due to privacy concerns~\cite{Abdolrahmani2018}, opting instead to use button or gesture-based inputs. Richer resolutions of information can be achieved when combining modalities, \eg\, the tactile features of a 3D-printed model with haptic vibratory and auditory outputs. Combining modalities is critical to overcoming the \textit{`bandwidth problem'}, in which BLV users' non-visual senses cannot match the capacity of vision, necessitating their combined use~\cite{Edwards2015}.

While early I3Ms primarily relied on button or gesture-based triggered audio labels, recent research has explored integrating speech interfaces and conversational agents. This shift is driven by research finding that BLV people find voice interaction convenient~\cite{Azenkot2013}, along with widespread adoption~\cite{Pradhan2018} and high usage~\cite{Abdolrahmani2018} of conversational agents among BLV users. For instance, Quero \etal~\cite{Quero2019} combined a tactile graphic of a floor plan with a conversational agent that focused on indoor navigation; however, voice interaction was performed through a connected smartphone rather than the graphic itself. Other works have developed voice-controlled agents to guide BLV users in exploring 3D-printed representations of gallery pieces~\cite{Bartolome2019,Quero2018}. These systems, however, have primarily focused on basic command-driven interactions more analogous to voice menus rather than conversational dialogue. Shi \etal~\cite{Shi2017b,Shi2019} proposed incorporating conversational agents into I3Ms to allow BLV users to expand their understanding of the modelled content.

Recent research into I3Ms has begun to explore modalities beyond audio and touch. Quero \etal~\cite{Quero2018} designed an I3M representing an art piece that integrated localised audio, wind, and heat output. However, participants faced challenges in interpreting the semantic mapping of modalities, \eg\, whether heat represented the morning sun or the shine of starlight. In our previous work, we found that BLV users desired I3Ms that combined touch, haptic vibratory feedback, and conversational dialogue~\cite{Reinders2020}. Additionally, we co-designed an I3M with BLV co-designers to explore how these modalities could create natural interactions~\cite{Reinders2023}, inspired by the \textit{`Put-That-There'} paradigm~\cite{Bolt1980}. This work led to five I3M design recommendations, including: support interruption-free tactile exploration; leverage prior interaction experience with personal technology; support customisation and personalisation; support more natural dialogue; and tightly coupled haptic feedback. These studies motivated our current work, with participants finding the I3M engaging, and several beginning to embody it.

\subsection{The Embodiment of Agents}
Dourish~\cite{Dourish2001} presents a seminal view that embodiment \textit{``denotes a form of participative status''}, where embodied interaction in natural forms of communication is influenced both by physical presence and context. They posit that this perspective applies to \textit{``spoken conversations just as much as to apples or bookshelves''}. Within the design of agent-based systems, embodiment is largely understood as the use of different modalities -- \eg\ voice, visual output, gestures, gaze -- to imbue machines with human-like behaviours and characteristics, making them appear more \textit{`alive'}; with an enhanced perception of social presence that is capable of approximating human-human social interaction~\cite{cassell2000more,Lester1997,Lankton2015,Biocca1999}. Such agents are often described as being more \textit{`lifelike'} or possessing \textit{`lifelikeness'}~\cite{Lester1997,Cassell1999,Cassell1999b,Cassell1999c,Lester1999}.

Research into embodiment has predominantly focused on sighted users. As systems become more embodied, users' perceptions of social presence can increase, motivating users to treat them more favourably~\cite{Reeves2004}. Lankton \etal~\cite{Lankton2015} described how social presence can make systems appear more sociable, warm, and personal, while Cassell~\cite{Cassell2001} noted that embodying technology allows users to locate intelligence, illuminating what would otherwise be an \textit{`invisible computer'}. Importantly, embodied agents exhibiting higher levels of social presence and perception have been shown to enhance user perception of engagement~\cite{Shamekhi2018,Luger2016,Heuwinkel2013,Cassell2001} and trust~\cite{Bickmore2001,Shamekhi2018,Bickmore2013,Rheu2021}. 

In the HCI community, efforts to embody intelligent agents have focused on enhancing social perception through conversational, visual, and physical attributes. \textbf{Conversational embodiment} includes mimicking human voices~\cite{Cassell2001}, small talk~\cite{Liao2018,Pradhan2019,Cassell2001,Shamekhi2018,Bickmore2001}, greetings~\cite{Cassell2001,Luria2019,Shamekhi2018,Lester1997}, and conversational turn-taking~\cite{Cassell2001,Kontogiorgos2020}. Lester \etal~\cite{Lester1997} described the \textit{persona effect}, proposing that social presence can increase when agents exhibit personality, making them appear more lifelike. Many conversational agents, like Siri, incorporate attributes of conversational embodiment.

\textbf{Visually embodied} agents, which are often also conversationally embodied, associate systems with virtual avatars or characters, many of which have faces~\cite{Cassell2001,Kontogiorgos2020,Bickmore2013}, and are capable of gesturing~\cite{Cassell2001,Bickmore2001} and gaze~\cite{Kontogiorgos2020,Shamekhi2018,Bickmore2001}. Visual embodiment  extends beyond the visual feedback that mainstream conversational agents emit, \eg\ rings of light or animations used to indicate that agents are processing or `thinking'. Depending on the task, visually and conversationally embodied agents can improve social perceptions~\cite{Shamekhi2018,Luria2019,Cassell2001,Nowak2003}, trust~\cite{Bickmore2001,Bickmore2013,Rheu2021,Shamekhi2018}, and engagement~\cite{Shamekhi2018,Cassell2001}. 

Embodied agents can extend beyond virtual embodiment and include physical bodies~\cite{Kontogiorgos2020,Luria2017}. Lura \etal~\cite{Luria2017} found that users' situational awareness was higher when using a physically embodied robot to perform smart-home tasks compared to an unembodied voice agent. Robots capable of emitting human-like warmth have been associated with increased perceptions of friendship and presence~\cite{Nie2012}. Recent research has explored the use of haptic vibratory feedback to create lifelike cues, such as heartbeats~\cite {Borgstedt2023} and handshaking~\cite{Bevan2015}. \textbf{Physically embodied} agents can be perceived as having higher social presence than non-physically embodied agents~\cite{Kidd2004}. Additionally, they have been found to be more forgivable during unsuccessful interactions; however, depending on their realism, thi can become distracting in high-stakes scenarios~\cite{Kontogiorgos2020}.

\subsection{The Embodiment of I3Ms}
The design of embodied agents has traditionally focused on the perception of sighted users. However, in the last decade, work has begun exploring how human-human conversational cues can be converted into non-visual formats for BLV users. Many of these efforts utilise haptic belts/headsets~\cite{rader2014,McDaniel2018} or AR glasses~\cite{Qiu2016,Qiu2020} to convey body movements like head shaking, nodding, and gaze. Despite this, the impact of agent embodiment, and specifically embodied I3Ms, on BLV users' perceptions remains unstudied.

In our previous work, we found that a number of participants desired I3Ms that felt more lively and human~\cite{Reinders2023}. Participants suggested integrating a conversational agent with a personality, incorporating haptic vibratory feedback to make the I3M feel alive, and enabling speech to originate directly from the model itself. This desire for more human-like interactions aligns with findings with conversational agents. Choi \etal~\cite{Choi2020} reported that many BLV users valued human-like conversation with conversational agents as critical in relationship building, while Abdolrahmani \etal~\cite{Abdolrahmani2018} observed that BLV users preferred agents they could talk with as if they were other people rather than pieces of technology. Karim \etal~\cite{Karim2023} recommended that agents have customisable personalities, recognising that such features may not be relevant in all scenarios, such as group settings. Collins \etal~\cite{Collins2023}, however, found hesitance among BLV users towards embodied AI agents in VR applications. Whether these perspectives and desires extend to I3Ms is unknown.

Further impetus for studying I3M embodiment comes from the links between embodiment, engagement, and trust with embodied agents that have been previously identified for sighted users. Trust and engagement are especially critical for BLV users, as the usefulness of accessible graphics, aids, or tools depends on users' willingness to engage with and accept/rely on the information they provide. This directly determines the extent to which users rely on these tools~\cite{Betsy1993,Wu2017,Abdolrahmani2018}. Therefore, it is crucial to explore whether I3Ms can be effectively embodied and whether embodiment fosters greater trust and engagement between users and their I3Ms. 

\section{Embodied Design Factors}
\label{sec:Design}
Our work was influenced by interpretations of embodiment and embodied interaction proposed by Dourish~\cite{Dourish2001} and Cassell~\cite{Cassell2001}. Interfaces can be embodied using a range of design characteristics, including visual embodiment, conversational embodiment, and physical embodiment. However, as BLV users, particularly those who are totally blind, may not be able to fully discern visual characteristics, we approached I3M embodiment from a purely non-visual perspective. Traditional visual embodiment design factors, such as virtual avatars or gaze, were not considered. We drew from existing literature to identify a range of design factors shown to enhance perceived levels of social perception and embodiment. These were split between \textbf{conversational} and \textbf{physical embodiment}.

\subsection{Model Selection \& Design}
To investigate conversational and physical embodiment, we created two I3Ms -- (1) the \textbf{Egyptian Pyramids} and (2) the \textbf{Saturn V Rocket} [Figure~\ref{fig:RocketPyramidI3M}]. These subjects were selected because they represent the types of materials commonly found in museums, galleries, or in science or history classes. Additionally, they facilitated the design of models with multiple components that could be individually picked up, detached, and manipulated, which has been shown to increase engagement~\cite{Reinders2020}. Each I3M can be configured in two states -- \textit{High Embodied Mode (HEM)} or \textit{Low Embodied Mode (LEM)} -- based on five non-visual design factors (Sections~\ref{sec:ConversationalEmbodiment} \& ~\ref{sec:PhysicalEmbodiment}).

\begin{figure}[hb!]
\centering
  \includegraphics[width=1.00\columnwidth]{graphics/architecture.png}
  %TC:ignore
  \caption{I3M Architecture. Shown is the Pyramid I3M in HEM. The base houses a Raspberry Pi, speaker, and mic. Components enable local sensing, each equipped with a microcontroller, touchpoint, accelerometer, haptic motor, and battery. The Pi manages embodiment state, controls speech input and output, and connects to the conversational agent.}~\label{fig:Architecture}
  \Description{This figure includes an image outlining the architecture of the HEM I3Ms. Shown is a picture of the Egyptian Pyramid I3M, with annotations on top pointing to the various components and hardware it includes. The I3M Base includes a Raspberry Pi, speaker, and microphone. Four I3M Components are highlighted, each including an ESP32, touchpoint, accelerometer, haptic motor and battery. A bubble is depicted in between the Base and Components, highlighting the communication of messages using MQTT over Wi-Fi. These messages include local touch and movement sensing, embodiment state, and haptic vibrations. To the right is a bubble encompassing all the external APIs and libraries that the I3Ms use. This includes Dialogflow for the conversational agent, as well as text-to-speech and speech-to-text libraries.}
  %TC:endignore
\end{figure}

The base of each I3M was constructed from laser-cut acrylic, serving as a stand to hold the constituent components of the I3M and to house a Raspberry Pi, speaker, and microphone (Figure~\ref{fig:Architecture}). The Pi powered each I3M and handled the following responsibilities:

\begin{itemize}
    \item Maintaining a Wi-Fi connection with each I3M component and operating as a message broker (MQTT) to facilitate messaging between the Raspberry Pi and each component.
    \item Managing the embodied state of the I3M by setting the design factors to operate in either HEM or LEM mode.
    \item Controlling speech and auditory output through the connected speaker and microphone, using the Picovoice Porcupine wake word library and Google Cloud Speech-to-Text and Text-to-Speech for speech input and synthesis.
    \item Connecting to the I3M's conversational agent, built using Google Dialogflow, and integrating ChatGPT to perform external searches triggered by Dialogflow's fallback intent.
\end{itemize} 

Each I3M had four 3D-printed components. For the Saturn V Rocket, these were the \textit{Stage A}, \textit{Stage B}, and \textit{Stage C} modules, and the \textit{Launch Tower}. For the Egyptian Pyramid, these included the \textit{Sphinx}, \textit{Great Pyramid}, \textit{Pyramid of Menkaure}, and \textit{Pyramid of Khafre}. Each component had an ESP32 microcontroller embedded in the print, providing localised touch and movement sensing and haptic vibratory output. The microcontroller had integrated Wi-Fi, capacitive touch-sensing GPIO pins, and was connected to a touchpoint 3D-printed with conductive filament, a 3.7V 400mAh lithium polymer battery, an MPU6050 accelerometer and gyroscope, a DRV2605L haptic motor controller, and a vibrating haptic disc.

\begin{figure*}[ht!]
\centering
    \includegraphics[width=1\textwidth]{graphics/models.png}
    %TC:ignore
    \caption{\small In A), the Egyptian Pyramids I3M is configured in HEM, showing a user pressing the Sphinx touchpoint, triggering localised haptics and a personified voice response via a speaker contained within the I3M's enclosure; B) shows the Pyramid I3M's components -- the Great Pyramid, Sphinx, and Khafre/Menkaure. In C), the Saturn V Rocket I3M is in LEM, with a user pressing the Stage C touchpoint, triggering speech output via an external speaker; D) shows the Rocket I3M's components -- detachable Stage A/B/C rocket modules, and the Launch Tower.}~\label{fig:RocketPyramidI3M}
    \Description{This figure includes four images showcasing our two I3Ms. In A), the Egyptian Pyramids I3M is configured in HEM. The user is pressing the touchpoint on the Sphinx, which emits localised haptics, and, using an embodied personified voice and first-person narration, responds through a speaker contained within the I3M's enclosure. In B), the I3M consists of four 3D-printed components -- the Great Pyramid, Sphinx, and Pyramids of Khafre/Menkaure -- each of these is labelled, as is the speaker that is embedded inside the I3M enclosure/case in HEM. In C), the Saturn V Rocket I3M is configured in LEM. It includes a base made of acrylic that is painted green, representing the ground. Shown is a user pressing the Stage C module touchpoint. Speech output is played back through a speaker that is housed externally from the I3Ms enclosure — ``This is the Stage C module of the rocket''; In D), the Saturn V Rocket I3M consists of four 3D-printed components, the Stage A, Stage B and Stage C rocket modules, which can be detached, and the rockets Launch Tower. The rocket components can be stacked on top of one another as they have magnets attached.}
    %TC:endignore
\end{figure*}

\subsubsection{Touch Gestures.}
Each I3M component, such as the Sphinx, had a touchpoint that protruded from its surface and was printed using a touch capacitive filament. Touch sensors underwent an automatic calibration process upon I3M startup. Touch gestures were implemented based on findings from our previous co-design work~\cite{Reinders2023}. To enable independent tactile exploration, touchpoints needed to be activated before gestures could be used. Users could perform an \textit{Activate Press} gesture by holding the touchpoint down for one second. This would activate the sub-component and play an audio label identifying it. Once activated, users could perform a \textit{Double Press} gesture to cycle through 12 audio labels that provided different facts and information about the sub-component. For example, for the Sphinx, this included details about its location, the material it was made from, its construction date, purpose, size, and physical appearance. The final gesture supported was a \textit{Long Press}, performed by holding down the touchpoint for two seconds. This gesture invoked the I3M's conversational agent.

\subsubsection{Conversational Agent.}
Users could perform a \textit{Long Press} or use the wake word -- \textit{`Hey Model'} -- to invoke the I3M's conversational agent. The attached microphone recorded user queries, which were processed by the agent built using Google Dialogflow. The agent was trained to answer questions related to each component using a corpus containing all the information extractable via touch gestures. This dataset contained an average of 500 words of facts per component. For example, for the Sphinx, this included details about its missing nose, potential astronomical significance, historical restorations, and symbolic meaning. Like audio labels, speech responses were output through the I3M's speaker, synthesised using the same high-quality voice. If the agent could not answer a query directly, it offered to perform an external search. Upon approval, the agent would send the query to ChatGPT, configured with a context to ensure consistency with the other agent outputs. For example, \textit{``You are an intelligent assistant that answers questions in 25 words or less about ancient Egypt and the Pyramids, including the Pyramid of Menkaure, Khafre, Khufu, and the Sphinx of Giza''}.

\subsubsection{Haptic Vibratory Feedback.}
\label{sec:Haptics}
Each model component contained a haptic disc capable of delivering localised vibratory feedback, generated using the DRV2605 Waveform library. Haptic vibrations were emitted as system feedback when touch gestures were performed, following established recommendations~\cite{Reinders2023}. These corresponded to the type of gesture executed: Activate (\textit{Strong Buzz, 150ms)}, Double (\textit{Strong Short Double Click}), or Long Press (\textit{Strong Buzz, 500ms}).

\subsection{Conversational Embodiment Design Factors}
\label{sec:ConversationalEmbodiment}
In earlier work, we observed that I3M conversational agents should speak with high-quality, human-like voices~\cite{Reinders2023}. When designing our I3Ms, the researchers made the decision that voice quality should be independent of embodiment state, to prevent preferences for high-quality voices from overpowering other design factors.

\subsubsection{DF\#1: Introductions and Small Talk.}
\label{sec:Smalltalk}
Choi \etal~\cite{Choi2020} found that BLV people prefer agents that engage in human-like conversation, as this can help in relationship building. Embodied agents have been designed to introduce themselves to users~\cite{Shamekhi2018,Cassell2001,Luria2019,Lester1997} and engage in small talk~\cite{Liao2018,Pradhan2019,Cassell2001,Shamekhi2018}. In our I3Ms, when configured in HEM, the conversational agent introduces itself to the user, and could detect and respond to small talk during interactions, using a version of Dialogflow's small talk module. To avoid interrupting independent tactile exploration, HEM I3Ms only engage in user-initiated small talk, adhering to an established design recommendation~\cite{Reinders2023}. This design factor can be configured as follows:

\begin{itemize}
    \item \textbf{HEM}: When turned on, the I3M is introduced, \eg\, \textit{``Hello and welcome to the Pyramid model. Let's learn about ancient Egyptian history together!''}. Additionally, when a user initiates small talk, the I3M responds appropriately, \eg\, when greeted with \textit{``Hello Sphinx''}, it replies, \textit{``Hi, how are you?''}.
    
    \item \textbf{LEM}: When an I3M is turned on, a loading message is played, \eg\, \textit{``Loading Pyramid model''}. When a user engages in small talk, the I3M does not respond.
\end{itemize}

\subsubsection{DF\#2: Embodied Personified Voices.}
Influenced by works where users personified and imbued characters into conversational agents~\cite{Purington2017,Pradhan2019,Lester1997}, we designed HEM so that model components could `speak with their own unique voice'. This was achieved by assigning distinct synthesised voices with each model component, emulating aspects of a \textit{one-for-one} social presence~\cite{Luria2019}. HEM components cannot converse among themselves, in line with Luria \etal~\cite{Luria2019}, who observed that users felt discomfort when two active social presences interacted with each other. In contrast, LEM I3Ms employ a singular voice, operating under a \textit{one-for-all} social presence, where all model components are inhabited as a group.

\begin{itemize}
        \item \textbf{HEM}: Each I3M component speaks with its own unique synthesised voice, using a one-for-one social presence.
        
        \item \textbf{LEM}: All I3M components speak with a unified synthesised voice, using a one-for-all social presence.
\end{itemize}

\subsubsection{DF\#3: Embodied Narration Style.}
To further explore the personification of the models, we scripted speech output to be narrated from either a first or third-person perspective. This was influenced by work showing that some users anthropomorphise agents by using first and second-person pronouns~\cite{Liao2018,Coeckelbergh2011}.

\begin{itemize}
        \item \textbf{HEM}: I3M components phrase verbal responses using first-person narration, \eg\, \textit{``I am the Great Sphinx of Egypt. I am a statue of a reclining sphinx, a mythical creature. I have the head of a human and the body of a lion. Many suggest that my nose was lost to erosion, vandalism, or damage''}. This also extended to external search responses fetched using ChatGPT, which had a modified context, \eg\, \textit{``You are the Great Sphinx of Egypt and serve as an intelligent assistant, you answer questions in 25 words or less from a first-person perspective about yourself, ancient Egypt, and the Pyramids''}.
        
        \item \textbf{LEM}: Verbal responses are generated using objective third-person narration, \eg\, \textit{``This is the Great Sphinx of Egypt. It is a statue of a reclining sphinx, a mythical creature with the head of a human and the body of a lion. Many suggest its nose was lost to erosion, vandalism, or damage''}.
\end{itemize}
\newpage
\subsection{Physical Embodiment Design Factors}
\label{sec:PhysicalEmbodiment}
I3Ms are inherently designed to be perceived physically, \ie\, picked up and tactually observed or manipulated. We sought to explore ways in which the tangible nature of I3Ms could be enhanced by physically embodying presence.

\subsubsection{DF\#4: Embodied Vibratory Feedback.}
In our previous research, a BLV user described an I3M component as \textit{`lifeless'} except when it was emitting haptic vibrations~\cite{Reinders2023}. This influenced our focus on richer haptic vibratory feedback, supported by other work exploring how haptics imbue lifelike cues~\cite{Nie2012,Bevan2015, Borgstedt2023}. We designed HEM I3Ms to emit localised haptic vibratory feedback, creating a sense of physical presence. In contrast, LEM I3Ms generate haptics, but only as system feedback confirming gesture inputs. This decision aligns with the recommendations in ~\cite{Reinders2023}, since model usability could be impacted if haptics were turned off entirely.

\begin{itemize}
    \item \textbf{HEM}: I3M components generate haptic vibratory feedback to embody a sense of physical presence. Components use haptics to highlight themselves during interactions, \eg\, when a component identifies itself it emits a localised vibration (\textit{Transition Ramp Up - 0 to 100\%}), or when it is referenced during an auditory response (\textit{Strong Buzz, 1000ms}).

    \item \textbf{LEM}: Haptics are used only to confirm when a touch gesture has been correctly performed. Components do not use localised haptics to embody physical presence. 
\end{itemize}

\subsubsection{DF\#5: Location of Speech Output.}
Pradhan \etal~\cite{Pradhan2019} identified that an agent's proximity can influence how human-like it is perceived. Agents closer to the user, or capable of operating across multiple devices to broadcast ubiquity, are often thought of as more present. Due to size constraints, we could not embed speakers directly into I3M components. However, the location of the auditory output can be configured to vary by the position of the speaker.

\begin{itemize}
    \item \textbf{HEM}: The speaker used by the I3M to output speech is housed within the enclosure.
    \item \textbf{LEM}: The speaker used by the I3Ms to output speech is positioned externally from the enclosure, approximately 30cm to the left side of the model, reducing proximity.
\end{itemize}

\section{User Study -- Methodology}
\subsection{Hypotheses}
\label{sec:Hypotheses}
We designed a controlled user study to explore and understand whether I3Ms configured with different embodied design factors influence BLV end-users' perceptions of model embodiment, engagement, and trustworthiness. The study utilised both the Saturn V Rocket I3M and Egyptian Pyramids I3M, each configurable into two states -- \textit{High Embodied Mode (HEM)} and \textit{Low Embodied Mode (LEM)} -- as described in Section~\ref{sec:Design}. We hypothesised that:

\begin{itemize}[leftmargin=2mm, rightmargin=-1mm]
    \item \textbf{H\#1:} HEM I3Ms are perceived as more \textit{embodied} than LEM I3Ms
    \item \textbf{H\#2:} HEM I3Ms are perceived as more \textit{engaging} than LEM I3Ms
    \item \textbf{H\#3:} HEM I3Ms are perceived as more \textit{trustworthy} than LEM I3Ms
\end{itemize}
\newpage
\subsection{Participants}
Twelve BLV participants were recruited from our lab's participant contact pool (Table~\ref{tab:participants}). This sample size ($n=12$) falls within the range commonly seen in BLV accessibility studies, which often involve anywhere between 6-12 participants~\cite{Holloway2022,Nagassa2023,Shi2017b,Shi2020} due to the low incidence of blindness in the general population and associated recruitment challenges~\cite{Butler2021}.

Participants ranged in age from 27 to 78 years (\textit{$\mu$} = 50, \textit{$\sigma$} = 16.6). Nine participants self-reported as totally blind, while three reported being legally blind with low levels of light perception. Participants also varied in their prior experience with tactile graphics: Many reported substantial exposure and confidence ($n=5$), some reported some use but lacked confidence ($n=4$), and others reported limited or no exposure ($n=1$ and $n=2$, respectively). 

Familiarity with 3D-printed models was slightly less common. All participants regularly used conversational agents, including Google Assistant ($n=11$), Siri ($n=10$), Alexa ($n=4$), and ChatGPT ($n=4$). These interfaces were accessed on various devices, such as smartphones ($n=12$), smart speakers/displays ($n=11$), smartwatches ($n=6$), computers ($n=7$) and tablets ($n=5$).

\input{tables/Participants}

\subsection{Study Measures}
\label{sec:StudyMeasures}

We used a series of questionnaires and asked participants to subjectively rate the I3Ms in order to investigate our hypotheses.

For \textbf{Hypothesis H\#1}, to measure how \textit{embodied} participants perceived the I3Ms, we utilised the \textbf{Godspeed Questionnaire Series (GQS)}~\cite{Bartneck2009}. Originally devised to measure users' social perceptions of robot and agent-based systems, GQS subscales such as ~\textit{anthropomorphism} and \textit{intelligence} have recently been applied to measure aspects of embodiment and liveliness in conversational agents ~\cite{Shamekhi2018} and robots with human-like abilities~\cite{Kontogiorgos2020}. We selected four subscales -- \textit{anthropomorphism, animacy, likeability}, and \textit{intelligence} -- as we felt each provides insight into components of embodied sociability. For example, \textit{anthropomorphism} captures the attribution of human-like characteristics, \textit{animacy} reflects perceptions of liveliness, \textit{likeability} gauges formation of positive impressions, and \textit{intelligence} focuses on perception of ability. Additionally, we also explored model embodiment by asking participants for their subjective perceptions by expressing the concept of `embodiment/embodied' using the terms `lively/liveliness' to ensure clarity. We felt this terminology would have more meaning to participants, and aligns with previous works that have used the similar terms `lifelike/lifelikeness' to describe embodied agents~\cite{Lester1997,Cassell1999,Cassell1999b,Cassell1999c,Lester1999}.

To explore \textbf{Hypothesis H\#2}, we used two \textit{engagement} measures -- the \textbf{User Engagement Scale [Short Form] (UES-SF)} and the \textbf{Playful Experiences Questionnaire (PLEXQ)}. The UES-SF measures user engagement as the depth of a user's perceived investment with a system~\cite{OBrien2016}. It consists of 12 five-point Likert items across four subscales -- \textit{focused attention, perceived usability, aesthetic appeal}, and \textit{reward}~\cite{Obrien2018}. The UES-SF has been widely applied across HCI to measure engagement in contexts such as interactive media~\cite{carlton2019}, video games~\cite{wiebe2014}, and 3D-printed building plans for BLV people~\cite{Nagassa2023}. We used all four subscales. To complement the UES-SF, we also used PLEXQ, which measures playfulness, pleasurable experiences, and playful engagement~\cite{Boberg2015}. PLEXQ is commonly used to assess how engaging games and game-like experiences are~\cite{Bischof2016,Cho2024}. We used eight subscales that we felt were most relevant to I3Ms -- \textit {captivation, challenge, control, discovery, exploration, humor, relaxation}, and \textit{sensation}. 

To supplement these measures of perceived engagement, we also captured two behavioural metrics (time spent and interactions performed during tasks), as more time spent with the model and more interactions may indicate greater immersion and enjoyment~\cite{OBrien2013,Doherty2018}. Note that comparing these between the LEM and HEM conditions was meaningful as the length of responses in both conditions was similar and every interaction type was available in both modes.

To investigate \textbf{Hypothesis H\#3} and measure \textit{trust}, we utilised the \textbf{Human-Computer Trust Model (HCTM)}. The HCTM conceptualises trust as a multifaceted construct, encompassing users' perceptions of the \textit{perceived risk, benevolence, competence}, and \textit{reciprocity} during interactions. These perceptions can influence users' reliance on a system and their likelihood of continued use~\cite{Gulati2019}. Previous work has demonstrated the utility of HCTM in assessing trustworthiness in conversational agents like Siri~\cite{Gulati2018}, machine learning systems~\cite{Guo2022}, and other human-like technologies such as large language models~\cite{Salah2023} and chatbots~\cite{Degachi2023}.

Based on pilot study feedback, we made minor adjustments to specific subscale items in the UES-SF and PLEXQ measures. For instance, the \textit{aesthetic appeal} (UES-SF) and \textit{sensation} (PLEXQ) subscales were adjusted, as concepts of ``attractiveness'', ``aesthetics'', and ``visuals'' held little meaning to our pilot user in BLV contexts. They recommended adding the phrase ``to my senses'' to these items. Per ~\cite{Obrien2018}'s guidance on modifying the UES-SF, we did not report an overall UES score, instead focusing on individual components of engagement. These adjustments also motivated our decision to add our own questions gathering participants' subjective ratings of the I3Ms, supplementing the validated scales and providing additional nuance and insight. The modified UES-SF and PLEXQ, along with GSQ and HCTM, are provided in the Appendices.

\subsection{Experiment Conditions}
Our user study used a within-subject design. All participants were exposed to (one) LEM and (one) HEM-configured I3M. To control for bias related to model type (Rocket/Pyramids) and design factor state (LEM/HEM), the order in which the I3Ms were presented, along with their associated LEM/HEM configuration, was counterbalanced. The activities that participants completed with each I3M remained the same, regardless of its LEM/HEM state, and could be completed in either mode without significant difficulty.

\subsection{Procedure}
\label{sec:StudyProcedure}
Each user study session lasted approximately two hours and included at least one researcher being present. Sessions began with the researcher providing an overview of the research project, and were divided into the following stages:

\begin{enumerate}[leftmargin=5.5mm]
    \item \textbf{Training}. Participants were guided through a 10-minute training exercise, which allowed them to familiarise themselves with how I3Ms operate. We designed and built an I3M that represented a non-descript sphere for training. Participants were first asked to explore the I3M tactually when it was turned off, before being taught how to extract basic information from the I3M using touch gestures, and asking questions through the conversational interface. The training I3M did not operate in either LEM or HEM mode and would, using a low-quality synthesised voice, only respond by confirming when interactions had been successfully performed (\eg\, ``Double Press'', ``Recording query''). Operating outside of LEM/HEM states was a deliberate design decision to allow training of basic interaction functionality without biasing future LEM/HEM exposure.
    \newline
    \item \textbf{Exposure to LEM/HEM I3Ms.}
    \begin{enumerate}
        \item \textbf{Activities}. Participants were introduced to their first I3M, configured in one of the LEM/HEM states, and completed a walkthrough activity. The I3M identified and described each component it included, with participants given the opportunity to tactually explore the I3M throughout. These walkthroughs were carefully curated so that participants encountered the majority of system functionality specific to the HEM/LEM design factors\footnote{The only exception to this was the small talk component of the \textit{introductions and small talk} design factor, which, unlike all other design factors that were explicit, required user initiation (see Section~\ref{sec:Smalltalk}).}. After the walkthrough, participants were given up to five minutes to \textit{explore} the models, during which they could interact with the I3M in any way they wished. Participants were then asked to complete four \textit{researcher-directed} information-gathering tasks (\eg\, finding out how long it took to build the Great Pyramid, what happened to the nose of the Sphinx, or the significance of the Pyramids). Participants could access this information using their choice of either touch gesture interaction or using the model's conversational agent. Before concluding, participants were given up to an additional five minutes for a \textit{free play} task designed to mimic undirected, real-world use. They were instructed to discover something interesting about the modelled concept that they were not aware of prior.

        \item \textbf{Questionnaire Scales}. Participants were taken through the questionnaire scales -- GSQ, PLEXQ, UES-SF, and HCTM. In addition, participants were asked to rate how \textit{lively}, \textit{engaging}, and \textit{trustworthy} the I3M was, using 5-point Likert scales. On average, these took 15 minutes to complete.
        \newline
        \item \textbf{Remaining Model}. Participants would then complete 2(a) and 2(b) again with the second I3M, configured in the remaining LEM/HEM state. Participants spent an average of 30 minutes with each I3M.
        \newline
    \end{enumerate}
    \item \textbf{Semi-Structured Interview}. At the end of the session, participants were asked questions about specific interactions with the LEM/HEM I3Ms and were asked to rank how \textit{lively}, \textit{engaging} and \textit{trustworthy} each model was. We also asked about the role each design factor played, and whether they impacted perceptions of how \textit{lively}, \textit{engaging}, and \textit{trustworthy} the I3Ms were. On average, it took 20 minutes to answer these questions.
\end{enumerate}

\subsection{Data Collection \& Analysis}
All sessions were video-recorded and subsequently transcribed. Collected data included responses to scale questions, semi-structured interview questions, and participant comments made during task completion. The time participants spent completing tasks, as well as the number of interactions they performed, were also recorded. Descriptive statistics were calculated on all subscale responses -- GSQ, UES-SF, PLEXQ, and HCTM -- as well as for interview questions that ranked the I3Ms and individual embodiment design factors. 

For data that did not follow a normal distribution (\eg\ our scale data), we conducted non-parametric statistical tests, specifically Wilcoxon signed-rank tests. Binomial tests were performed on the rankings of the I3Ms and the impact ratings of individual embodiment design factors. Paired t-tests were conducted on data that was normally distributed (\eg\ time taken to complete tasks). We opted for one-tailed tests because our hypotheses were explicit in nature (described in Section~\ref{sec:Hypotheses}). This approach was deliberate, allowing us to dedicate more power to detecting effects in one direction.

The analysis should be interpreted in light of both the exploratory nature of our user study and our small sample size ($n$ $=$ 12). This influenced our approach in two ways. First, from the outset, due to our small sample size, we decided not to use $p$ $<$ 0.05 as the sole determinant of significance~\cite{Shamekhi2018,Cramer2004}, instead marking results $p$ $<$ 0.05 as \textit{significant} ($^\ast$) and 0.05 $<=$ $p$ $<=$ 0.1 as \textit{marginally significant} ($^\wedge$). Second, given the exploratory nature of our work, in order to reduce the risk of overlooking meaningful results (false negatives), we chose not to apply corrections for multiple comparisons (although this does increase the risk of false positives).

\section{Results}
\label{sec:Results}
Results are presented based on our hypotheses (Section~\ref{sec:Hypotheses}) and separated across the \textbf{embodiment} of the I3Ms, \textbf{engagement}, and \textbf{trust}. Each section presents quantitative results, including questionnaire scales, rankings of the models and HEM/LEM-configured design factors, and qualitative results, in the form of participant responses from the semi-structured interview.

\subsection{Embodiment of I3Ms}
\subsubsection{\textbf{GSQ Scales.}}
HEM I3Ms elicited higher mean scores compared to I3Ms with LEM across all GSQ subscales -- \textit{anthropomorphism ($\mu$ $=$ +0.67), animacy ($\mu$ $=$ +0.46), likeability ($\mu$ $=$ +0.27)}, and \textit{intelligence ($\mu$ $=$ +0.29)}. We conducted one-tailed Wilcoxon signed-rank tests for each GSQ subscale (Table~\ref{table:1}), using design configuration (HEM, LEM) as the independent variable to assess significance. The positive effect of HEM was statistically significant across all GSQ subscales -- \textit{anthropomorphism} ($p=0.011$), \textit{animacy} ($p=0.029$), \textit{likeability} ($p=0.008$), and \textit{intelligence} ($p=0.010$).

\input{tables/GSQuestionnaire}
\input{tables/EmbodimentDesign}

\subsubsection{\textbf{How Lively Were The I3Ms?}}
\label{sec:PerceptionLiveliness}
Participants rated how \textit{lively} each I3M was immediately after being exposed to it. HEM I3Ms were perceived as more \textit{lively} ($\mu$ $=$ +0.50) compared to the LEM configuration (Table~\ref{table:1}). This was statistically significant ($p=0.017$). In the post-activity interview, participants ranked the I3Ms based on perceived liveliness. Two-thirds of participants ($n=8$) indicated that the HEM I3Ms had higher liveliness compared to the LEM I3Ms, with the remainder split between no difference ($n=3$) and the LEM configuration ($n=1$). A binomial test revealed the difference between the number of participants who ranked the HEM I3Ms higher and those who either selected the LEM I3M or could tell no difference was statistically significant ($k=8$, $n=12$, $p=0.019$).

Most participants were emphatic in their selection. P2 described how the HEM I3M's design factors \textit{``brought it [the model] to life''}, continuing, \textit{``it [the HEM I3M] created a relationship, [it is] like dealing with something that is alive, it *is* talking to you''}. P8 referred to the HEM I3M as \textit{``more human-like and interactive, more natural''}, while P11 noted that the HEM I3M \textit{``was more like an entity... less of a computer program''}. Similarly, P4 mentioned that the HEM I3M \textit{``seemed to want to interact with me ... [whereas] the other one could have been talking to the moon''}. Six participants explicitly referred to the HEM I3M as being \textit{``more human-like''} in their explanations (P3, P4, P8, P10, P11 and P12), with P3 also stating that the LEM I3M was \textit{``too machine-like''}. However, P5 felt that the HEM and LEM configurations appeared just as lively as one another, stating that \textit{``they were both pretty active''}. One participant (P1) selected the LEM Rocket I3M as the most lively, citing specific elements of that model's design as the determining factor. P1 explained, \textit{``the rocket... [its] three sections made it more real''} (P1).
\newpage
\subsubsection{\textbf{Impact of Design Factors on Liveliness.}}
Participants rated how each HEM design factor influenced their perception of I3M \textit{liveliness} using a 5-point Likert scale (Table~\ref{table:5}). All factors appeared to influence perceptions of the liveliness of the HEM I3Ms. \textit{Embodied vibratory feedback} elicited the highest mean score, while \textit{location of speech output} scored the lowest. Binomial tests revealed that the difference between the number of participants who agreed or strongly agreed, and those who were neutral or below, was statistically significant for all factors, apart from \textit{location of speech output}, which was marginally significant ($k=8$, $n=12$, $p=0.057$).

\subsection{Engagement of I3Ms}
\subsubsection{\textbf{UES \& PLEXQ Scales.}}
HEM I3Ms elicited higher mean scores across three of the four UES-SF subscales (Table~\ref{table:4}) -- \textit{focused attention ($\mu$ $=$ +0.17), perceived usability ($\mu$ $=$ +0.45)}, and \textit{aesthetic appeal ($\mu$ $=$ +0.14)}. Wilcoxon results for these subscales were significant for both \textit{perceived usability} ($p=0.001$) and \textit{aesthetic appeal} ($p=0.029$), but marginally significant for \textit{focused attention} ($p=0.067$). The \textit{reward} subscale, however, showed a higher mean score for LEM I3Ms ($\mu$ $=$ +0.11), and was non-significant.

HEM I3Ms exhibited marginally higher mean scores across seven PLEXQ subscales. Significant results were found for \textit{control} ($\mu$ $=$ +0.22, $p=0.016$) and \textit{sensation} ($\mu$ $=$ +0.17, $p=0.029$), while \textit{humor} ($\mu$ $=$ +0.16) was marginally significant ($p=0.092$). One subscale, \textit{exploration}, showed a higher mean score for LEM I3Ms ($\mu$ $=$ +0.08).

\input{tables/UESPLEXQuestionnaires}
\newpage
\subsubsection{\textbf{How Engaging Were The I3Ms?}}
\label{sec:PerceptionEngagement}
When asked to rate how \textit{engaging} each I3M was, results indicated that HEM I3Ms were more \textit{engaging} ($\mu$ $=$ +0.42) than LEMs (Table~\ref{table:4}). This difference was statistically significant ($p=0.013$). In the post-activity interview, the majority of participants ($n=10$) ranked the HEM I3Ms as more engaging than the LEM condition, while the remaining participants ($n=2$) found no difference. A binomial test revealed that these rankings were statistically significant ($k=10$, $n=12$, $p<0.001$).

Participants clearly articulated their reasons, with seven directly referencing the HEM I3M as being either \textit{``more engaging''} or \textit{``interactive''} in their explanations (P2, P3, P5, P8, P9, P10, and P12). P2, who felt the HEM I3M was more engaging, described the difference as \textit{``one is more [like] reading an encyclopedia and the other [the HEM I3M] is an experience''}, adding that they found the HEM I3M to be more `playful'. P7 expanded on this, \textit{``[I] wanted to ask [the HEM I3M] more questions, I wanted to get more information, whereas [I] just accepted [the LEM I3M] as fact''}. P8 went a step further, stating that the LEM I3M was \textit{``boring''} while the HEM I3M was \textit{``more enthusiastic''}. P10 compared the HEM I3M to their screen reader, \textit{``Jaws is neutral, it is not interactive, it gets hypnotic. Using different voices is way more engaging!''}. Despite being more interested in the subject matter of their LEM I3M, P8 ultimately found the HEM I3M more engaging, explaining, \textit{``I was interested in Egypt more than space ... but the way the rocket acted made it more engaging''}.

As a further indication of engagement, participants spent more time interacting with the HEMs ($\mu$ $=$ 99.5 seconds) compared to the LEMs ($\mu$ $=$ 68.8) during the free play exercise (Table~\ref{table:10}). A one-tailed paired t-test revealed this to be statistically significant ($t=2.899$, $p=0.007$). Participants also engaged in more interactions with the HEMs ($\mu$ $=$ 3.3) compared to the LEMs ($\mu$ $=$ 2.2). Wilcoxon results indicated this difference was marginally significant ($p=0.069$).

Participants also chose to spend more time using the HEM I3Ms during both the \textit{model exploration} ($\mu$ $=$ +29.8 seconds) and \textit{researcher-directed} ($\mu$ $=$ +54.4) tasks. Paired t-tests revealed that the effect of the HEMs was statistically significant for time spent during \textit{model exploration} ($t=2.025$, $p=0.035$) and marginally significant for time spent completing the \textit{researcher-directed} tasks ($t=1.595$, $p=0.070$). 

Regarding interactions, participants performed more interactions with the HEMs during these tasks. Paired t-tests indicated marginal significance for both \textit{model exploration} ($\mu$ $=$ +1.8, $t=1.583$, $p=0.071$) and \textit{researcher-directed} tasks ($\mu$ $=$ +0.5, $t=1.732$, $p=0.056$). Notably, during the \textit{researcher-directed} tasks, participants occasionally chose to continue interacting beyond what was required to complete a task, performing additional interactions. These instances favoured the HEMs ($n=10$) over the LEMs ($n=4$).

\input{tables/EngagementTasks}

\subsubsection{\textbf{Impact of Design Factors on Engagement.}}
Participants rated how each design factor, presented in the HEM state, impacted their perception of I3M \textit{engagement} (Table~\ref{table:6}). All five design factors appeared to influence how engaging the HEM I3Ms were perceived and were statistically significant -- \textit{introductions \& small talk} and \textit{location of speech output} (both had $k=11$, $n=12$, $p<0.001$), and the remaining three factors (all $k=10$, $n=12$, $p=0.003$).

\input{tables/EngagementDesign}
\newpage
\subsection{Trustworthiness of I3Ms}
\subsubsection{\textbf{HCTM Scales.}}
\input{tables/HCTMQuestionnaire}
HEM I3Ms outperformed LEM I3Ms across all four HCTM subscales (Table~\ref{table:3}) -- \textit{perceived risk}\footnote{The \textit{perceived risk} subscale relates to the willingness of a user to engage with a system despite possible risks. A lower perceived risk score is desired, signifying that the user is more willing to interact with the system.} \textit{($\mu$ $=$ -0.28), benevolence ($\mu$ $=$ +0.11), competence ($\mu$ $=$ +0.19)}, and \textit{reciprocity ($\mu$ $=$ +0.23)}. Wilcoxon results were significant for three HCTM subscales -- \textit{perceived risk} ($p=0.008$), \textit{competence} ($p=0.004$), and \textit{reciprocity} ($p=0.027$). The remaining subscale, \textit{benevolence}, was marginally significant ($p=0.051$).

\subsubsection{\textbf{How Trustworthy Were The I3Ms?}}
\label{sec:PerceptionTrust}
When rating the \textit{trustworthiness} of each I3M (Table~\ref{table:3}), results were less clear. A minor increase in mean score for HEM I3Ms ($\mu$ $=$ +0.08) over LEM I3Ms was observed; however, this difference was nonsignificant ($p=0.282$). In the post-activity interview ranking, the majority of the participants ($n=9$) indicated that there was no major discernible difference in trust between the HEM and LEM models. The remaining participants ($n=3$) ranked the HEM I3Ms higher. A binomial test showed that this result was nonsignificant ($k=3$, $n=12$, $p=0.819$).

Participants described feeling indecisive, often basing their interpretation of trust on the believability of the information provided by the models. P2 noted that both models were \textit{``just giving [them] facts''} and that their manner of acting or speaking did not matter. P4 explained that as both HEM and LEM I3Ms were \textit{``accessing information from the internet ... that [they] trusted it to only access certain [appropriate] things''}. Despite reporting no major differences in their ratings, P11 suggest that \textit{``incorrect facts, [when] talking in first person reduces trust''}, while P5, who also rated no difference, focused on the salient physical design of the I3Ms, expressing concern about \textit{`the height of the rocket ... knocking it over''}.

\subsubsection{\textbf{Impact of Design Factors on Trust.}}
Participants were divided on how the individual HEM design factors influenced their perceptions of the \textit{trustworthiness} of the I3Ms (Table~\ref{table:9}). \textit{Embodied vibratory feedback} elicited the highest mean score, while \textit{embodied narration style} scored the lowest. Apart from \textit{embodied vibratory feedback}, which was marginally significant ($k=8$, $n=12$, $p=0.057$), all other design factors were nonsignificant.

\input{tables/TrustDesign}

\section{Discussion}
\subsection{Hypothesis \#1: HEM I3Ms are perceived as more \textit{embodied} than LEM I3Ms}
Participants felt that HEM I3Ms were more lively, with an increased perception of embodiment compared to the LEM I3Ms. When ranking the I3Ms on liveliness, 8/12 selected HEM I3Ms (Section~\ref{sec:PerceptionLiveliness}). This was also statistically significant, with most participants providing emphatic explanations for their selections, supporting H\#1.

Results from the Godspeed Questionnaire Series also supported H\#1 and allowed us to explore different dimensions and key aspects of embodiment, adding nuance to our understanding of how the I3Ms appeared embodied (Table~\ref{table:1}). All GQS subscales were statistically significant. It is particularly noteworthy that the \textit{anthropomorphism} and \textit{animacy} subscales were rated more positively for HEM I3Ms, as these subscales deal with the attribution of human-like behaviours and perception of life~\cite{Bartneck2009}, which are key components of embodiment. The \textit{likeability} and \textit{intelligence} subscales were also perceived more favourably for HEM I3Ms.

Overall, our findings and participant comments help to support H\#1 that BLV users do perceive I3Ms configured with HEM design factors as more embodied, \textit{`present'}, and \textit{`lively'}. These findings align with work on the embodiment of other interfaces in non-BLV contexts, including robots~\cite{Kontogiorgos2020} and conversational agents~\cite{Luria2017,Shamekhi2018,Luria2019}, which has found that imbuing machines with human-like behaviours can enhance their perceived embodiment and sense of presence, creating a sense of \textit{`being there'}~\cite{Shamekhi2018}. 

\subsection{Hypothesis \#2: HEM I3Ms are perceived as more \textit{engaging} than LEM I3Ms}
Participants rated higher levels of engagement with HEM I3Ms compared to LEM I3Ms. When ranking which I3M was more engaging, the overwhelming majority (10/12) selected HEM (Section~\ref{sec:PerceptionEngagement}). These differences were statistically significant, supporting H\#2.

We used the User Engagement Scale and Playful Experiences Questionnaire (Table~\ref{table:4}) to explore different dimensions of engagement to add to and help contextualise our understanding of how engaging participants found the I3Ms. The UES-SF indicated that HEM I3Ms had higher average scores than LEM I3Ms across 3/4 subscales. Of these subscales, two were statistically significant, and one was marginally significant. These subscales focus on the extent to which users feel absorbed in an interaction (\textit{focused attention}) and the usability/negative affect experienced during interactions (\textit{perceived usability}), both of which are critical dimensions of engagement~\cite{Obrien2018}. The \textit{aesthetic appeal} subscale, which the researchers adjusted to be more meaningful in BLV contexts -- encompassing aesthetics beyond those purely visual -- was also statistically significant. This is particularly noteworthy, with HEM-configured I3Ms providing more engaging sensory experiences. The \textit{reward} subscale, centred on valued experiential outcomes, was not significant with respect to our hypothesis, and was the only subscale where LEM I3Ms received higher mean scores. This may indicate that the additional presence and feedback of HEM I3Ms could, in some circumstances, reduce initial curiosity.

It is our belief that several PLEXQ subscales may have had reduced meaning to participants, potentially as a result of the controlled nature and limited time exposure of the study. For example, subscales that focus on finding something hidden (\textit{discovery}) or unwinding through playful experiences (\textit{relaxation}) may have been less relevant. Interestingly, one subscale, \textit{exploration}, elicited higher mean values for LEM I3Ms, possibly for similar reasons to the UES-SF \textit{reward} subscale. Despite this, several PLEXQ results added support to H\#2, with subscales related to excitement (\textit{sensation}), enjoyment/amusement (\textit{humor}), and power (\textit{control}) being more favourably observed with HEM I3Ms.

Across all tasks, participants spent more time interacting with the HEM I3Ms and performed a greater number of interactions compared to the LEM I3Ms. We feel this was particularly noteworthy during the undirected free play task, which was designed to mimic real-world use. Time spent and the number of interactions were statistically significant and marginally significant, respectively. These metrics have previously been used as behavioural measures of engagement in HCI~\cite{OBrien2013,Doherty2018}, and provide further evidence that participants were more engaged with the HEM I3Ms. They also complement the UES-SF, which ties engagement
to the depth of a user's investment with a system~\cite{OBrien2016}.

Based on emphatic participant discussions and clear preferences when directly asked, our study provides evidence supporting H\#2 that HEM I3Ms are perceived as more engaging. Our scale data adds important nuance to this understanding, with key UES-SF and PLEXQ subscales showing that HEM I3Ms were more favourably perceived. Although ratings for both HEM and LEM I3Ms were generally high, which aligns with works showing that BLV users find 3D-printed models and I3Ms engaging~\cite{Nagassa2023,Reinders2020,Shi2019}, our results suggest that HEM I3Ms are perceived as even more engaging. More broadly, our findings align with prior work demonstrating that interfaces imbued with more human-like behaviours can positively impact end-user perceptions of engagement~\cite{Lester1997,Cassell1999,Shamekhi2018}.

\subsection{Hypothesis \#3: HEM I3Ms are perceived as more \textit{trustworthy} than LEM I3Ms}
Our participants were mixed on whether there were any discernible differences in the trustworthiness of HEM and LEM I3Ms. Hesitance was observed when participants were asked to rank which I3M was the more trustworthy model, with 9/12 indicating no major discernible difference between them (Section~\ref{sec:PerceptionTrust}). Participants expressed indecisiveness, often basing their interpretation of trust solely on the believability of the information output from the models, rather than their interactions with the I3Ms. 

On the other hand, results from the Human-Computer Trust Model (Table~\ref{table:3}) suggest that HEM I3Ms may lead to greater trust, as all four HCTM subscales were either significant ($=3$) or marginally significant ($=1$). Results did, however, reveal only very minor differences in mean scores between HEM I3Ms and LEM I3Ms. Despite this, these scales relate to the willingness of users to engage with a system despite the possible risks (\textit{perceived risk}), whether a system possesses the functionalities needed to depend on it (\textit{competence}), and a willingness to spend more time using it when support situations arise (\textit{reciprocity}). The \textit{benevolence} subscale, which was marginally significant, focuses on whether users believe that a system has the abilities required to help them achieve their goals.

Despite the statistical significance of the HCTM results, we believe that, overall, our results provide only mixed support for Hypothesis H\#3. It is the researchers' view that the HCTM questionnaire and subjective ratings may have been interpreted differently by participants. While the scale data appears to have successfully captured different dimensions of trust based on interactions with the HEM/LEM models, many participants, when asked about the concept of trust subjectively
(Section~\ref{sec:PerceptionTrust}), focused solely on the believability of the information provided by the models, independent of their HEM/LEM state. They tended to prioritise believability of information over considerations as to whether their interactions with the models and their behaviours influenced perceptions of reliance or competence. Our findings also suggest that the HEM design factors tested do not appear to play a strong role in influencing trust. As visually embodied conversational agents have been shown to be subjectively more trustworthy in non-BLV contexts~\cite{Heuwinkel2013,Bickmore2013,Sidner2018,Shamekhi2018,Gulati2018}, it is clear that more research is needed in order to better understand the impact of embodiment on trustworthiness of I3Ms for BLV users, particularly in real-world situations.

\subsection{How Did The Design Factors Impact Embodiment, Engagement and Trust?}
Broadly speaking, our embodiment design factors and their implementations were well-received, contributing to how embodied and engaging the HEM I3Ms were perceived. However, connections to trust were mixed. These are discussed below in order of importance.
\newline
\newline
\vspace{-1mm}\bpstart{DF\#4: Embodied Vibratory Feedback} Participants shared their enthusiasm regarding haptics. Referring to the liveliness of the HEM I3Ms, P3 described how the use of haptics made them feel \textit{``more three-dimensional... realistic''}. Participants also discussed how haptics made the HEM I3Ms more engaging, including P5, \textit{``[haptics] added an extra sense of interaction''}. These emphatic reactions align with other work, where haptics added life to I3Ms~\cite{Reinders2023}, and support the growing body of research exploring how haptics can create physically embodied, lifelike cues~\cite{Nie2012,Bevan2015, Borgstedt2023}.

\bpstart{DF\#2: Embodied Personified Voices} Participants highlighted how unique voices shaped their perception of embodiment. P9 was emphatic about how it made the HEM model feel more alive, \textit{``[it] made [the components] seem like they were their own things''}. P6 detailed how the one-for-one social presence of the HEM model gave components their own `character'. Regarding engagement, several participants found the voices helpful for tracking which I3M component was active and talking. P2 explained, \textit{``different voices ... broke the [HEM] model up into separate parts, it defined the objects better''}. P8 reflected, \textit{``[a] single voice does not stack up''}. This aligns with work by Choi \etal~\cite{Choi2020}'s finding that many BLV users value human-like conversation with agents. 

\bpstart{DF\#3: Embodied Narration Style} Participants frequently linked first-person narration to engagement and embodiment, creating a transformative experience. P2 described it as making interactions feel \textit{``like you were talking to someone, vs it talking to you''}. Similarly, P4 remarked that it made conversations seem \textit{``more human-like ... on a one-on-one basis''}. While P9 noted that it enhanced HEM presence, they initially felt that \textit{``... it was a bit strange anthropomorphizing [a system]... like when Microsoft talks like a human''}. The positive reception of first-person narration aligns with other research that has found that users anthropomorphise agents by using first and second-person pronouns~\cite{Liao2018,Coeckelbergh2011}.

\bpstart{DF\#5: Location of Speech Output} Participants noted that closer coupling with the I3M and its speaker impacted engagement. P4 found the HEM I3M's audio output \textit{``easier to take in''}, while P8 appreciated being able to \textit{``focus more directly on [the HEM I3M]''}. P3 described HEM as \textit{``more intimate and engaging''}. The researchers believe that embedding speakers inside each printed component could further enhance embodiment and engagement. P5 suggested this would make the I3Ms \textit{``feel more alive''}, echoing feedback from previous work~\cite{Reinders2023}. One possibility may be to use ultrasonics to project speech output~\cite{Iravantchi2020}, acting as digital ventriloquism. 

\bpstart{DF\#1: Introductions \& Small Talk} This factor had less impact on participants, who were less likely to reference it. The researchers believe this may be because it was not as explicit as other design factors. Many participants forgot that the HEM I3Ms introduced themselves, and small talk, being reliant on user initiation, was minimal. Only two participants (P2 and P4) engaged in small talk. Their responses revealed no meaningful deviation from the wider participant group regarding embodiment, engagement, and trust, suggesting that small talk played a minimal role. Despite this, P2 found it made the HEM I3M seem more playful, while P4 said introductions felt like an \textit{``invitation''} to interact. Conversely, P6 felt HEM introductions diminished their independence, stating that \textit{``I did not like it [introducing itself]; I like finding out what things are by [looking at/touching] them''}. These mixed reactions align with other works which
have found that BLV users have varying perspectives on how proactive I3Ms should be~\cite{Reinders2020}, suggesting that conversational strategies effective in embodied agents for sighted users~\cite{Liao2018,Pradhan2019,Cassell2001,Shamekhi2018} may hold less meaning in BLV contexts.

\subsection{Can I3Ms Be Embodied \& What Is Their Impact?}
Our study supports the larger idea that I3Ms are able to be perceived as \textit{more embodied} \textbf{and are} \textit{more engaging}. This follows from H\#1, where our findings support that BLV people perceive HEM I3Ms as more embodied and lively, and H\#2, where HEM I3Ms were found to be more engaging. In the context of I3Ms, it appears that model embodiment increases end-user engagement, specifically with I3Ms using the set of embodied design factors we implemented. However, the impact of embodiment on trust (H\#3) remains less clear, as BLV people generally appear to trust I3Ms regardless of embodiment.

Prior research has shown that the embodiment of an interface can increase both engagement~\cite{Shamekhi2018,Luger2016,Heuwinkel2013,Cassell2001} and trust~\cite{Bickmore2001,Shamekhi2018,Bickmore2013,Rheu2021}. Our findings support the relationship between embodiment and engagement, but stop short of any impacts on trust. With the bulk of this previous research focusing on conversational agents and robots, our work extends this relationship to include contexts relevant to both I3Ms and with people who are BLV.

The relationship between embodied I3Ms and their impact on engagement is particularly significant in learning contexts, allowing BLV students or self-learners to engage in deeper, more meaningful experiences where the I3M can assist in teaching and testing their knowledge. Embodied agents have been used in learning environments, and have been found to enrich the learning experiences of students~\cite{Schroeder2013}. Pedagogic agents often rely on embodied designs, including combinations of visual and conversational embodiment~\cite{Lester1997,Moreno2001,Schroeder2013}. While prior research on non-embodied I3Ms in learning contexts has highlighted positive feedback from both teachers and BLV students~\cite{Shi2019}, we believe that understanding how I3Ms can be effectively embodied could further enhance engagement in educational settings, and facilitate broader adoption.

Based on the positive reception amongst participants of our embodied I3Ms, we propose extending our existing I3M design recommendations~\cite{Reinders2023} to include embodiment:

\begin{itemize}
   \item \textbf{\textit{Support more embodied experiences}}: I3Ms that support human-like behaviours and characteristics in their design can make them appear more human, lively, and engaging to use. This may involve combining aspects of physical and conversational embodiment, \eg\, introductions and small talk, embodied voices, embodied narration, embodied haptic vibratory feedback, and location of speech output.
\end{itemize}

\section{Limitations \& Future Work}
There are a number of limitations regarding this study. While some are lenses through which the results should be interpreted, others present exciting avenues for future investigation. 

Our investigation focused on five specific design factors, and our findings should be interpreted within that context. Future research should explore additional embodiment design factors. This could include models that are more autonomous, or incorporate visual embodiment elements, which may benefit low vision users with residual vision, \eg\, virtual avatars or the emittance of light. Physical embodiment could also be extended with additional design dimensions not covered in this study. For example, haptic perception could extend beyond vibratory feedback to include model texturing, scale, orientation, or the impact of detachable components.

Furthermore, our design factors were implemented in specific ways, presenting an opportunity to further explore how the design space of embodied I3Ms can be expanded through alternative implementations. Often, our implementations were shaped by technical limitations or model choice. For instance, with the \textit{location of speech output} design factor, embedding speakers within individual model components or using headphones could produce different results. Regarding model choice, while our models neatly segmented \textit{embodied personified voices} based on individual components, a different type of model, \eg\ a globe of Earth, could instead segment voices by country, continent, or hemisphere, depending on its purpose.

As our study looked at participants' subjective ratings to assess the impact of the design factors, future research could formally isolate and test each factor to understand its individual effect on embodiment. The \textit{small talk} component of DF\#1 was also rarely used, likely because it required user initiation, unlike the other design factors. While this was an intentional design decision (see Section~\ref{sec:Smalltalk}), future work should revisit small talk to determine if the design factor would be better utilised if it were agent-initiated.

We used validated scales to measure aspects of embodiment, engagement, and trust. Based on pilot feedback, we adjusted a small number of UES-SF and PLEXQ items to have more meaning in BLV contexts. We supplemented these scales with our own questions, providing additional nuance and insight. However, this raises a need for future work to explore how these instruments can be further modified and validated for BLV users, or whether entirely new tools should be developed to improve relevance and meaning.

Our study was conducted with 12 participants, consistent with similar studies involving BLV participants~\cite{Holloway2022,Nagassa2023,Shi2017b,Shi2020}. Nonetheless, we would like to run further sessions to confirm our findings. This presents multiple opportunities, such as using a broader selection of models, and conducting `in-the-wild' and longitudinal studies to examine how the relationship between end-users and I3Ms evolves over time. Additionally, as our work represents the first investigation into embodied I3Ms, and its exploratory nature influenced the experimental design and analysis, future research should focus on targeted testing and conservative analysis.

Regarding trust, given the lack of discernible differences between the trustworthiness of HEM and LEM I3Ms, future work should focus on re-examining the concept of trust. Future research should explore real-world trust scenarios where confidence in HEM/LEM I3Ms may hold greater significance and contexts where model behaviours could influence perception.

In our previous research, we identified the importance of I3Ms supporting customisation~\cite{Reinders2023}. In this study, participants had limited opportunities to personalise each model in order to better isolate the effects of each HEM design factor. Given the new insights into embodiment presented in this work, future work could seek to combine the HEMs described with greater user customisation.

This work utilised conversational agents specifically trained on information relevant to the modelled concepts. Rapid advancements in generative AI provide opportunities to expand the conversational capabilities of I3Ms. Future research should investigate how generative AI systems could be leveraged in place of bespoke conversational agents in order to enhance their flexibility.
\newpage
Finally, future work should examine embodiment and engagement with sighted users. Given the potential for I3Ms, such as those presented in this work, to be used in contexts like museums and classrooms, it is important that they be highly engaging for all users, contributing to more inclusive social experiences.

\section{Conclusion}
Increasing the engagement and trust that exists between BLV users and their accessible materials is of critical importance, as the ultimate utility of these resources is diminished if users are unwilling to engage with or trust them. This is particularly relevant in educational and cultural contexts, such as classrooms, museums, and galleries, where I3Ms are beginning to be deployed. 

Our work presents the first investigation into the relationship between embodiment, engagement, and trust in the context of I3Ms and BLV users. We created two I3Ms that supported a series of design factors aimed at making the models appear more embodied. Our findings revealed that our participants perceived many of the design factors as contributing to the embodiment of the I3Ms. Participants also believed that these design factors made the I3Ms more engaging. The impact on trust, however, was less clear. 

While many of the subscales we used revealed statistical significance in favour of HEM I3Ms, ratings for both HEM and LEM I3Ms were generally high. In the case of embodiment and engagement, rankings and participant comments added important context to these results, revealing clearer preferences. For example, while participants found I3Ms engaging overall, HEM I3Ms were perceived as even more engaging. This was further supported by our behavioural metrics, which showed that participants spent more time and performed more interactions with the HEMs.

Based on the positive reception of the embodied I3Ms, we established a new I3M design recommendation to complement those previously proposed to the I3M community~\cite{Reinders2023} -- \textit{support more embodied experiences}. We advocate that, due to the connections between embodiment and engagement, I3Ms should incorporate human-like behaviours and characteristics into their design.

We hope that this initial exploration into embodiment represents the first step toward a growing interest in embodied I3Ms, helping to facilitate their widespread adoption. One day, we envision similar models being found in schools and public spaces, such as museums and galleries, with configurable embodiment that allows BLV people to engage with content in ways that are more meaningful to them.

We believe our work is relevant in the broader context of research into embodiment, and its relationship to engagement and trust. Virtually all prior research has involved sighted users; as such, our study is the first to explore this relationship in the context of blind users who cannot discern visual embodiment cues. We hope our findings will inspire further exploration in this space. 

\begin{acks}
This research was supported by an Australian Government Research Training Program (RTP) Scholarship. We want to thank our participants for their time and expertise.  
\end{acks}
\newpage
\balance
\label{Bibliography}
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}
\newpage
\appendix
\label{sec:Appendix}

\section{Godspeed Questionnaire (GSQ)}\label{sec:GSQ}
\small
GSQ items were mixed to mask intention and used a 5-point semantic scale.

\subsection*{\small Anthropomorphism}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item Machine-like \hfill \makebox[4.5cm]{\hfill} \hfill Human-like
    \item Unconscious \hfill \makebox[4.5cm]{\hfill} \hfill Conscious
\end{itemize}

\subsection*{\small Animacy}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item Artificial \hfill \makebox[4.5cm]{\hfill} \hfill Lifelike
    \item Inert \hfill \makebox[4.5cm]{\hfill} \hfill Interactive
\end{itemize}

\subsection*{\small Likeability}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item Dislike \hfill \makebox[4.5cm]{\hfill} \hfill Like
    \item Unfriendly \hfill \makebox[4.5cm]{\hfill} \hfill Friendly
    \item Unpleasant \hfill \makebox[4.5cm]{\hfill} \hfill Pleasant
\end{itemize}

\subsection*{\small Perceived Intelligence}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item Ignorant \hfill \makebox[4.5cm]{\hfill} \hfill Knowledgeable
    \item Unintelligent \hfill \makebox[4.5cm]{\hfill} \hfill Intelligent
\end{itemize}

\section{User Engagement Scale Short Form (UES-SF)}\label{sec:UES}
\small
UES-SF items were mixed to mask intention and used a 5-point Likert scale.

\subsection*{\small Focused Attention}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I lost myself in this experience.
    \item The time spent using the model just slipped away.
    \item I was absorbed in this experience.
\end{itemize}

\subsection*{\small Perceived Usability}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I felt frustrated while using the model.
    \item I found the model confusing to use.
    \item Using the model was taxing.
\end{itemize}

\subsection*{\small Aesthetic Appeal}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item This model was attractive to my senses.
    \item This model was aesthetically pleasing to my senses.
    \item This model appealed to my senses.
\end{itemize}

\subsection*{\small Reward}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item Using the model was worthwhile.
    \item This experience was rewarding.
    \item I felt interested in this experience.
\end{itemize}

\section{Playful Experiences Questionnaire (PLEXQ)}\label{sec:PLEXQ}
\small
PLEXQ items were mixed to mask intention and used a 5-point Likert scale.

\subsection*{\small Captivation}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I forgot my surroundings.
    \item I felt completely absorbed.
    \item I lost track of space and time.
\end{itemize}

\subsection*{\small Challenge}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item It stimulated me to learn new things.
    \item It was a true learning experience.
    \item I enjoyed learning new things.
\end{itemize}

\subsection*{\small Control}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I had the capability to influence what was happening.
    \item I felt powerful.
    \item I enjoyed being in control.
\end{itemize}

\subsection*{\small Discovery}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I enjoyed discovering new things.
    \item I enjoyed finding useful new ways of using it.
    \item I enjoyed finding something unexpected.
\end{itemize}

\subsection*{\small Exploration}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I felt curious.
    \item I enjoyed experimenting.
    \item I enjoyed trying out new things.
\end{itemize}

\subsection*{\small Humor}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item It made me laugh.
    \item I had fun.
    \item I experienced funny situations.
\end{itemize}

\subsection*{\small Relaxation}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I felt relaxed.
    \item I enjoyed passing time with it.
    \item I felt relieved from stress.
\end{itemize}

\subsection*{\small Sensation}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I felt pleased by its aesthetics to my senses.
    \item I enjoyed the aesthetics to my senses.
    \item I felt pleased by the quality of it.
\end{itemize}

\section{Human-Computer Trust Model (HCTM)}\label{sec:HCTM}
\small
HCTM items were mixed to mask intention and used a 5-point Likert scale.

\subsection*{\small Perceived Risk}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I believe that there could be negative consequences using the model.
    \item I feel I must be cautious when using the model.
    \item It is risky to interact with the model.
\end{itemize}

\subsection*{\small Benevolence}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I believe that the model will act in my best interest.
    \item I believe that the model will do its best to help me if I need help.
    \item I believe that the model is interested in understanding my needs and preferences.
\end{itemize}

\subsection*{\small Competence}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item I think the model is competent and effective in facilitating my learning.
    \item I think the model performs its role as a learning material very well.
    \item I believe that the model has all the functionalities I would expect from a learning material.
\end{itemize}

\subsection*{\small Reciprocity}
\begin{itemize}[labelsep=0.3cm, leftmargin=0.7cm, itemsep=0.1em]
    \item If I use the model, I think I would be able to depend on it completely.
    \item I can always rely on the model for facilitating my learning.
    \item I can trust the information presented to me by the model.
\end{itemize}

%TC:endignore
\end{document}
\endinput
%%
