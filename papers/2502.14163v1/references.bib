@article{butler2017understanding,
author = {Matthew Butler and Leona Holloway and Kim Marriott and Cagatay Goncu},
title = {Understanding the graphical challenges faced by vision-impaired students in Australian universities},
journal = {Higher Education Research \& Development},
volume = {36},
number = {1},
pages = {59-72},
year  = {2017},
publisher = {Routledge},
doi = {10.1080/07294360.2016.1177001},
URL = { 
        https://doi.org/10.1080/07294360.2016.1177001
    }
}

@article{Lankton2015,
author = {Lankton, Nancy K.; McKnight, D. Harrison; and Tripp, John},
title = {Technology, Humanness, and Trust: Rethinking Trust in Technology},
journal = {Journal of the Association for Information Systems},
volume = {16},
number = {10},
year = {2015},
doi = {10.17705/1jais.00411},
URL = {https://aisel.aisnet.org/jais/vol16/iss10/1}
}

@article{Cassell2001, title={Embodied Conversational Agents: Representation and Intelligence in User Interfaces}, volume={22}, url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1593}, DOI={10.1609/aimag.v22i4.1593}, abstractNote={How do we decide how to represent an intelligent system in its interface, and how do we decide how the interface represents information about the world and about its own workings to a user? This article addresses these questions by examining the interaction between representation and intelligence in user interfaces. The rubric representation covers at least three topics in this context: (1) how a computational system is represented in its user interface, (2) how the interface conveys its representations of information and the world to human users, and (3) how the system’s internal representation affects the human user’s interaction with the system. I argue that each of these kinds of representation (of the system, information and the world, the interaction) is key to how users make the kind of attributions of intelligence that facilitate their interactions with intelligent systems. In this vein, it makes sense to represent a systmem as a human in those cases where social collaborative behavior is key and for the system to represent its knowledge to humans in multiple ways on multiple modalities. I demonstrate these claims by discussing issues of representation and intelligence in an embodied conversational agent -- an interface in which the system is represented as a person, information is conveyed to human users by multiple modalities such as voice and hand gestures, and the internal representation is modality independent and both propositional and nonpropositional.}, number={4}, journal={AI Magazine}, author={Cassell, Justine}, year={2001}, month={Dec.}, pages={67} }


@inproceedings{Gulati2018,
author = {Gulati, Siddharth and Sousa, Sonia and Lamas, David},
title = {Modelling trust in human-like technologies},
year = {2018},
isbn = {9781450362146},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297121.3297124},
doi = {10.1145/3297121.3297124},
booktitle = {Proc. Indian Conference on Human-Computer Interaction},
pages = {1–10},
numpages = {10},
keywords = {Empirical research, Human- like technology, Theory development, Trust in technology},
location = {Bangalore, India},
series = {IndiaHCI '18}
}

@inproceedings{Butler2023,
author = {Butler, Matthew and Tandori, Erica J and Dziekan, Vince and Ellis, Kirsten and Hall, Jenna and Holloway, Leona M and Nagassa, Ruth G and Marriott, Kim},
title = {A Gallery In My Hand: A Multi-Exhibition Investigation of Accessible and Inclusive Gallery Experiences for Blind and Low Vision Visitors},
year = {2023},
isbn = {9798400702204},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608391},
doi = {10.1145/3597638.3608391},
abstract = {With their overwhelming reliance on visual arts and artefacts, art galleries and museums are often considered unapproachable spaces for people who are blind or have low vision (BLV). We present the results of an ongoing collaboration with an Australian regional gallery in their efforts to improve access by BLV people to their permanent collection and temporary exhibitions. We describe the approach taken to develop interpretive technology-mediated experiences for BLV visitors for their 2021 and 2022 flagship exhibitions, along with evaluation from the perspectives of both BLV visitors and the gallery staff. Outcomes include: broader awareness within the gallery for inclusive design practice and public engagement; better understandings of the needs of BLV visitors, in particular relating to confidence building and technology integration; understanding of gallery constraints and challenges; and a framework for the development of future inclusive gallery experiences.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
articleno = {9},
numpages = {15},
keywords = {accessibility, art, blind, inclusive design, low vision},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@inproceedings{Butler2021,
author = {Butler, Matthew and Holloway, Leona M and Reinders, Samuel and Goncu, Cagatay and Marriott, Kim},
title = {Technology Developments in Touch-Based Accessible Graphics: A Systematic Review of Research 2010-2020},
year = {2021},
isbn = {9781450380966},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445207},
doi = {10.1145/3411764.3445207},
abstract = {This paper presents a systematic literature review of 292 publications from 97 unique venues on touch-based graphics for people who are blind or have low vision, from 2010 to mid-2020. It is the first review of its kind on touch-based accessible graphics. It is timely because it allows us to assess the impact of new technologies such as commodity 3D printing and low-cost electronics on the production and presentation of accessible graphics. As expected our review shows an increase in publications from 2014 that we can attribute to these developments. It also reveals the need to: broaden application areas, especially to the workplace; broaden end-user participation throughout the full design process; and conduct more in situ evaluation. This work is linked to an online living resource to be shared with the wider community.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
articleno = {278},
numpages = {15},
keywords = {Low Vision, Systematic Literature Review, Tactile Graphics, Blind, Assistive Technology},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{cassell2000more,
  title={More than just another pretty face: Embodied conversational interface agents},
  author={Cassell, Justine},
  journal={Communications of the ACM},
  volume={43},
  number={4},
  pages={70--78},
  year={2000}
}

@inproceedings{kim2015,
  title={Toward {3D}-printed movable tactile pictures for children with visual impairments},
  author={Kim, Jeeeun and Yeh, Tom},
  booktitle={Proc. ACM CHI Conference on Human Factors in Computing Systems},
  pages={2815--2824},
  year={2015},
  organization={ACM},
}

@inproceedings{keeffe2005psychosocial,
  title={Psychosocial impact of vision impairment},
  author={Keeffe, Jill},
  booktitle={International Congress Series},
  volume={1282},
  pages={167--173},
  year={2005},
  organization={Elsevier}
}

@article{Pradhan2020,
author = {Pradhan, Alisha and Lazar, Amanda and Findlater, Leah},
title = {Use of Intelligent Voice Assistants by Older Adults with Low Technology Use},
year = {2020},
issue_date = {August 2020},
publisher = {ACM},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3373759},
doi = {10.1145/3373759},
abstract = {Voice assistants embodied in smart speakers (e.g., Amazon Echo, Google Home) enable voice-based interaction that does not necessarily rely on expertise with mobile or desktop computing. Hence, these voice assistants offer new opportunities to different populations, including individuals who are not interested or able to use traditional computing devices such as computers and smartphones. To understand how older adults who use technology infrequently perceive and use these voice assistants, we conducted a 3-week field deployment of the Amazon Echo Dot in the homes of seven older adults. While some types of usage dropped over the 3-week period (e.g., playing music), we observed consistent usage for finding online information. Given that much of this information was health-related, this finding emphasizes the need to revisit concerns about credibility of information with this new interaction medium. Although features to support memory (e.g., setting timers, reminders) were initially perceived as useful, the actual usage was unexpectedly low due to reliability concerns. We discuss how these findings apply to other user groups along with design implications and recommendations for future work on voice-user interfaces.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {sep},
articleno = {31},
numpages = {27},
keywords = {Conversational interfaces, low technology use, older adults, smart speakers, voice assistants}
}

@inproceedings{Trajkova2020,
author = {Trajkova, Milka and Martin-Hammond, Aqueasha},
title = {"Alexa is a Toy": Exploring Older Adults' Reasons for Using, Limiting, and Abandoning Echo},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376760},
doi = {10.1145/3313831.3376760},
abstract = {Intelligent voice assistants (IVAs) have the potential to support older adults' independent living. However, despite a growing body of research focusing on IVA use, we know little about why older adults become IVA non-users. This paper examines the reasons older adults use, limit, and abandon IVAs (i.e., Amazon Echo) in their homes. We conducted eight focus groups, with 38 older adults residing in a Life Plan Community. Thirty-six participants owned an Echo for at least a year, and two were considering adoption. Over time, most participants became non-users due to their difficulty finding valuable uses, beliefs associated with ability and IVA use, or challenges with use in shared spaces. However, we also found that participants saw the potential for future IVA support. We contribute a better understanding of the reasons older adults do not engage with IVAs and how IVAs might better support aging and independent living in the future.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {focus group, life plan community, older adults, smart environments, technology non-use, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@InProceedings{Yuting2019,
author="Liao, Yuting
and Vitak, Jessica
and Kumar, Priya
and Zimmer, Michael
and Kritikos, Katherine",
editor="Taylor, Natalie Greene
and Christian-Lamb, Caitlin
and Martin, Michelle H.
and Nardi, Bonnie",
title="Understanding the Role of Privacy and Trust in Intelligent Personal Assistant Adoption",
booktitle="Information in Contemporary Society",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="102--113",
abstract="Voice-controlled intelligent personal assistants (IPAs) have seen tremendous growth in recent years on smartphones and as standalone devices in people's homes. While research has examined the potential benefits and drawbacks of these devices for IPA users, few studies have empirically evaluated the role of privacy and trust in individual decision to adopt IPAs. In this study, we present findings from a survey of IPA users and non-users (N{\thinspace}={\thinspace}1160) to understand (1) the motivations and barriers to adopting IPAs and (2) how concerns about data privacy and trust in company compliance with social contract related to IPA data affect acceptance and use of IPAs. We discuss our findings in light of social contract theory and frameworks of technology acceptance.",
isbn="978-3-030-15742-5"
}



@article{rowell2003,
  title={The world of touch: an international survey of tactile maps. Part 1: production},
  author={Rowell, Jonathan and Ungar, Simon},
  journal={British Journal of Visual Impairment},
  volume={21},
  number={3},
  pages={98--104},
  year={2003},
  publisher={Sage Publications},
  doi = {10.1177/026461960302100303}
}

@inproceedings{McDonald2014,
 author = {McDonald, Samantha and Dutterer, Joshua and Abdolrahmani, Ali and Kane, Shaun K. and Hurst, Amy},
 title = {Tactile Aids for Visually Impaired Graphical Design Education},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '14},
 year = {2014},
 isbn = {978-1-4503-2720-6},
 location = {Rochester, New York, USA},
 pages = {275--276},
 numpages = {2},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2661334.2661392},
 doi = {10.1145/2661334.2661392},
 acmid = {2661392},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3d printing, graphics education, rapid prototyping, tactile aids, visual aid, visual graphics, visually impaired},
} 

@inproceedings{Brown2012,
 author = {Brown, Craig and Hurst, Amy},
 title = {VizTouch: Automatically Generated Tactile Visualizations of Coordinate Spaces},
 booktitle = {Proc. Tangible, Embedded, and Embodied Interaction},
 series = {TEI '12},
 year = {2012},
 isbn = {978-1-4503-1174-8},
 location = {Kingston, Ontario, Canada},
 pages = {131--138},
 numpages = {8},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2148131.2148160},
 doi = {10.1145/2148131.2148160},
 acmid = {2148160},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assistive technology, human-computer interaction, rapid prototyping, tactile visualizations},
} 

@inproceedings{Hu2015,
 author = {Hu, Michele},
 title = {Exploring New Paradigms for Accessible 3D Printed Graphs},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '15},
 year = {2015},
 isbn = {978-1-4503-3400-6},
 location = {Lisbon, Portugal},
 pages = {365--366},
 numpages = {2},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2700648.2811330},
 doi = {10.1145/2700648.2811330},
 acmid = {2811330},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D printing, accessibility, blind, tactile graphics},
} 

@article{Buehler2016,
 author = {Buehler, Erin and Comrie, Niara and Hofmann, Megan and McDonald, Samantha and Hurst, Amy},
 title = {Investigating the Implications of 3D Printing in Special Education},
 journal = {ACM Trans. Access. Comput.},
 issue_date = {May 2016},
 volume = {8},
 number = {3},
 month = mar,
 year = {2016},
 issn = {1936-7228},
 pages = {11:1--11:28},
 articleno = {11},
 numpages = {28},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2870640},
 doi = {10.1145/2870640},
 acmid = {2870640},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D printing, assistive technology, children, cognitive impairment, developmental disability, digital fabrication, rapid prototyping, special education, visual impairment},
} 

@inproceedings{shi2017designing,
  title={Designing interactions for 3D printed models with blind people},
  author={Shi, Lei and Zhao, Yuhang and Azenkot, Shiri},
  booktitle={Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
  pages={200--209},
  year={2017},
  organization={ACM}
}

@inproceedings{ashok2014,
 author = {Ashok, Vikas and Borodin, Yevgen and Stoyanchev, Svetlana and Puzis, Yuri and Ramakrishnan, I. V.},
 title = {Wizard-of-Oz Evaluation of Speech-driven Web Browsing Interface for People with Vision Impairments},
 booktitle = {Proc. of the 11th Web for All Conference},
 series = {W4A '14},
 year = {2014},
 isbn = {978-1-4503-2651-3},
 location = {Seoul, Korea},
 pages = {12:1--12:9},
 articleno = {12},
 numpages = {9},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2596695.2596699},
 doi = {10.1145/2596695.2596699},
 acmid = {2596699},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {speech interface, web automation, wizard of Oz study},
} 

@article{brock2012,
	title={Design and User Satisfaction of Interactive Maps for Visually Impaired People}, 
    author={Brock, Anke M. and Truilleta, Philippe and Oriola, Bernard and Picard, Delphine and Jouffrais, Christophe}, 
    year={2012},
    journal={Computers Helping People with Special Needs},
    volume={7383},
    pages={544-551},
    series={Lecture Notes in Computer Science},
    doi = {10.1007/978-3-642-31534-3_80},
    publisher={Springer}
    }

@article{Keeffe2005,
author={Keeffe, Jill}, 
title={Psychosocial Impact of Vision Impairment},
journal={International Congress Series},
volume={1282},
pages={167--173}, 
year={2005}
}

@book{Dourish2001,
    author = {Dourish, Paul},
    title = "{Where the Action Is: The Foundations of Embodied Interaction}",
    publisher = {The MIT Press},
    year = {2001},
    month = {09},
    abstract = "{Computer science as an engineering discipline has been spectacularly successful. Yet it is also a philosophical enterprise in the way it represents the world and creates and manipulates models of reality, people, and action. In this book, Paul Dourish addresses the philosophical bases of human-computer interaction. He looks at how what he calls "embodied interaction"—an approach to interacting with software systems that emphasizes skilled, engaged practice rather than disembodied rationality—reflects the phenomenological approaches of Martin Heidegger, Ludwig Wittgenstein, and other twentieth-century philosophers. The phenomenological tradition emphasizes the primacy of natural practice over abstract cognition in everyday activity. Dourish shows how this perspective can shed light on the foundational underpinnings of current research on embodied interaction. He looks in particular at how tangible and social approaches to interaction are related, how they can be used to analyze and understand embodied interaction, and how they could affect the design of future interactive systems.}",
    isbn = {9780262256056},
    doi = {10.7551/mitpress/7221.001.0001},
    url = {https://doi.org/10.7551/mitpress/7221.001.0001},
}



@Book{Cramer2004,
author={Cramer, Duncan
and Howitt, Dennis.},
title={The Sage dictionary of statistics: a practical resource for students in the social sciences},
series={Dictionary of statistics},
year={2004},
publisher={Sage Publications},
address={London},
keywords={Social sciences -- Statistical methods; Social sciences -- Methodology; Statistics},
note={Includes bibliographical references (p. [187]-188).},
isbn={0761941371}
}

@Article{Bartneck2009,
author="Bartneck, Christoph
and Kuli{\'{c}}, Dana
and Croft, Elizabeth
and Zoghbi, Susana",
title="Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots",
journal="International Journal of Social Robotics",
year="2009",
month="Jan",
day="01",
volume="1",
number="1",
pages="71--81",
abstract="This study emphasizes the need for standardized measurement tools for human robot interaction (HRI). If we are to make progress in this field then we must be able to compare the results from different studies. A literature review has been performed on the measurements of five key concepts in HRI: anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety. The results have been distilled into five consistent questionnaires using semantic differential scales. We report reliability and validity indicators based on several empirical studies that used these questionnaires. It is our hope that these questionnaires can be used by robot developers to monitor their progress. Psychologists are invited to further develop the questionnaires by adding new concepts, and to conduct further validations where it appears necessary.",
issn="1875-4805",
doi="10.1007/s12369-008-0001-3",
url="https://doi.org/10.1007/s12369-008-0001-3"
}


@article{Reeves2004,
 author = {Reeves, Leah M. and Lai, Jennifer and Larson, James A. and Oviatt, Sharon and Balaji, T. S. and Buisine, St{\'e}phanie and Collings, Penny and Cohen, Phil and Kraal, Ben and Martin, Jean-Claude and McTear, Michael and Raman, TV and Stanney, Kay M. and Su, Hui and Wang, Qian Ying},
 title = {Guidelines for Multimodal User Interface Design},
 journal = {Commun. ACM},
 issue_date = {January 2004},
 volume = {47},
 number = {1},
 month = jan,
 year = {2004},
 issn = {0001-0782},
 pages = {57--59},
 numpages = {3},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/962081.962106},
 doi = {10.1145/962081.962106},
 acmid = {962106},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Abdolrahmani2018,
 author = {Abdolrahmani, Ali and Kuber, Ravi and Branham, Stacy M.},
 title = {"Siri Talks at You": An Empirical Investigation of Voice-Activated Personal Assistant (VAPA) Usage by Individuals Who Are Blind},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '18},
 year = {2018},
 isbn = {978-1-4503-5650-3},
 location = {Galway, Ireland},
 pages = {249--258},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3234695.3236344},
 doi = {10.1145/3234695.3236344},
 acmid = {3236344},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blind individuals, non-visual interaction, usability challenges, vapa, voice activated personal assistant},
} 

@inproceedings{Pradhan2018,
 author = {Pradhan, Alisha and Mehta, Kanika and Findlater, Leah},
 title = {"Accessibility Came by Accident": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {459:1--459:13},
 articleno = {459},
 numpages = {13},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3173574.3174033},
 doi = {10.1145/3173574.3174033},
 acmid = {3174033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accessibility, conversational interfaces, disability, intelligent personal assistants, speech},
} 

@inproceedings{Azenkot2013,
 author = {Azenkot, Shiri and Lee, Nicole B.},
 title = {Exploring the Use of Speech Input by Blind People on Mobile Devices},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '13},
 year = {2013},
 isbn = {978-1-4503-2405-2},
 location = {Bellevue, Washington},
 pages = {11:1--11:8},
 articleno = {11},
 numpages = {8},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2513383.2513440},
 doi = {10.1145/2513383.2513440},
 acmid = {2513440},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dictation, eyes-free, mobile devices, text entry},
} 

@inproceedings{Nikitina2018,
 author = {Nikitina, Svetlana and Callaioli, Sara and Baez, Marcos},
 title = {Smart Conversational Agents for Reminiscence},
 booktitle = {Proc. of the 1st International Workshop on Software Engineering for Cognitive Services},
 series = {SE4COG '18},
 year = {2018},
 isbn = {978-1-4503-5740-1},
 location = {Gothenburg, Sweden},
 pages = {52--57},
 numpages = {6},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3195555.3195567},
 doi = {10.1145/3195555.3195567},
 acmid = {3195567},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conversational agents, reminiscence, system requirements},
} 

@inproceedings{Brule2016,
 author = {Brule, Emeline and Bailly, Gilles and Brock, Anke and Valentin, Frederic and Denis, Gr{\'e}goire and Jouffrais, Christophe},
 title = {MapSense: Multi-Sensory Interactive Maps for Children Living with Visual Impairments},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI '16},
 year = {2016},
 isbn = {978-1-4503-3362-7},
 location = {San Jose, California, USA},
 pages = {445--457},
 numpages = {13},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2858036.2858375},
 doi = {10.1145/2858036.2858375},
 acmid = {2858375},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D printing, DIY, accessibility, children, field-study, interactive map, tangible interaction, visual impairment},
} 

@inproceedings{Stangl2015,
 author = {Stangl, Abigale and Hsu, Chia-Lo and Yeh, Tom},
 title = {Transcribing Across the Senses: Community Efforts to Create 3D Printable Accessible Tactile Pictures for Young Children with Visual Impairments},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '15},
 year = {2015},
 isbn = {978-1-4503-3400-6},
 location = {Lisbon, Portugal},
 pages = {127--137},
 numpages = {11},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2700648.2809854},
 doi = {10.1145/2700648.2809854},
 acmid = {2809854},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D model, 3D print, accessibility, emergent literacy, tactile graphics, tactile pictures, visual impairment},
} 

@article{butler2016,
  title={Understanding the graphical challenges faced by vision-impaired students in Australian universities},
  author={Butler, Matthew and Holloway, Leona and Marriott, Kim and Goncu, Cagatay},
  journal={Higher Education Research \& Development},
  pages={1--14},
  year={2016},
  doi = {10.1080/07294360.2016.1177001},
  publisher={Taylor \& Francis}
}

@article{Giraud2017,
year = 2017,
author = {Giraud, St\'{e}phanie and Brock, Anke M and Mac\'{e}, Marc J-M and Jouffrais, Christophe}, 
title = {Map learning with a 3D printed interactive small-scale model: Improvement of space and text memorization in visually impaired students},
journal = {Frontiers in Psychology}, 
volume = 8,
number = 930,
doi = "10.3389/fpsyg.2017.00930",
}

@inproceedings{Giraud2020,
author = {Giraud, Tom and Di Loreto, Ines and Tixier, Matthieu},
title = {The Making of Accessibility to Rural Place for Blind People: The Relational Design of an Interactive Map},
year = {2020},
isbn = {9781450369749},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357236.3395527},
doi = {10.1145/3357236.3395527},
abstract = {This paper accounts for the design process of an interactive map to foster accessibility - understood as a dialogical process - to a rural place for visually impaired people (VIP). We deeply engaged with the two concerned communities, a small group of VIP and locals from the rural village, valuing mutual sensitization and continual attunements. Framed by a relational perspective on design, we envisioned the map as a change catalyst, an artefact that can encourage VIP to engage in exploring activities, enable locals to present their territory, and contribute to ongoing reflections about accessibility by triggering exchanges about walking activities. Here we describe how this process ended-up with an object characterized by its intersecting material qualities and its capacity for mutual appropriation. We discuss the concept of boundary object as an insightful descriptor of the produced artefact and the newly identified challenge of designing accessible maps as material interventions.},
booktitle = {Proc. ACM Designing Interactive Systems Conference},
pages = {1419–1431},
numpages = {13},
keywords = {visual impairment, relational design, rural tourism, accessible map},
location = {Eindhoven, Netherlands},
series = {DIS '20}
}

@InProceedings{Gotzelmann2014,
author="G{\"o}tzelmann, Timo
and Pavkovic, Aleksander",
editor="Miesenberger, Klaus
and Fels, Deborah
and Archambault, Dominique
and Pe{\v{n}}{\'a}z, Petr
and Zagler, Wolfgang",
title="Towards Automatically Generated Tactile Detail Maps by 3D Printers for Blind Persons",
booktitle="Computers Helping People with Special Needs",
year="2014",
publisher="Springer International Publishing",
pages="1--7",
doi="10.1007/978-3-319-08599-9_1",
}

@inproceedings{Gotzelmann2016,
 author = {G\"{o}tzelmann, Timo},
 title = {LucentMaps: 3D Printed Audiovisual Tactile Maps for Blind and Visually Impaired People},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '16},
 year = {2016},
 isbn = {978-1-4503-4124-0},
 location = {Reno, Nevada, USA},
 pages = {81--90},
 numpages = {10},
 doi = {10.1145/2982142.2982163},
 acmid = {2982163},
 publisher = {ACM},
} 

@inproceedings{Gotzelmann2017,
 author = {G\"{o}tzelmann, Timo and Branz, Lisa and Heidenreich, Claudia and Otto, Markus},
 title = {A Personal Computer-based Approach for 3D Printing Accessible to Blind People},
 booktitle = {Proc. PErvasive Technologies Related to Assistive Environments},
 series = {PETRA '17},
 year = {2017},
 isbn = {978-1-4503-5227-7},
 location = {Island of Rhodes, Greece},
 pages = {1--4},
 numpages = {4},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3056540.3064954},
 doi = {10.1145/3056540.3064954},
 acmid = {3064954},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D printing, Autonomous, accessibility, assistive, audio, blind, self-dependence, tactile, user interface, visually impaired},
} 
[download]

@inproceedings{Hamid2013,
 author = {Abd Hamid, Nazatul Naquiah and Edwards, Alistair D.N.},
 title = {Facilitating Route Learning Using Interactive Audio-tactile Maps for Blind and Visually Impaired People},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI EA '13},
 year = {2013},
 isbn = {978-1-4503-1952-2},
 location = {Paris, France},
 pages = {37--42},
 numpages = {6},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2468356.2468364},
 doi = {10.1145/2468356.2468364},
 acmid = {2468364},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accessibility, audio-tactile maps, auditory icons, blindness, multimodal, orientation, route learning, speech, tactile maps, touch, visual impairment},
} 

@InProceedings{Petrie2002,
author="Petrie, Helen
and Schlieder, Christoph
and Blenkhorn, Paul
and Evans, Gareth
and King, Alasdair
and O'Neill, Anne-Marie
and Ioannidis, George T.
and Gallagher, Blaithin
and Crombie, David
and Mager, Rolf
and Alafaci, Maurizio",
editor="Miesenberger, Klaus
and Klaus, Joachim
and Zagler, Wolfgang",
title="TeDUB: A System for Presenting and Exploring Technical Drawings for Blind People",
booktitle="Computers Helping People with Special Needs",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="537--539",
abstract="Blind people can access and use textual information effectively in a variety of ways - through Braille, audiotape or computer-based systems. Access and use of graphic information is much more problematic, with tactile versions both time-consuming and difficult to make and textual descriptions failing to provide independent access to the material. The TeDUB Project is developing a system which will automatically generate descriptions of certain classes of graphics (electronic circuit diagrams, UML diagrams and architectural plans) and allow blind people to explore them independently. This system has great potential in work, education and leisure domains to open up independent access to graphic materials for blind people.",
isbn="978-3-540-45491-5"
}



@InProceedings{Goncu2011,
author="Goncu, Cagatay
and Marriott, Kim",
editor="Campos, Pedro
and Graham, Nicholas
and Jorge, Joaquim
and Nunes, Nuno
and Palanque, Philippe
and Winckler, Marco",
title="GraVVITAS: Generic Multi-touch Presentation of Accessible Graphics",
booktitle="Human-Computer Interaction -- INTERACT 2011",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="30--48",
abstract="Access to graphics and other two dimensional information is still severely limited for people who are blind. We present a new multimodal computer tool, GraVVITAS, for presenting accessible graphics. It uses a multi-touch display for tracking the position of the user's fingers augmented with haptic feedback for the fingers provided by small vibrating motors, and audio feedback for navigation and to provide non-geometric information about graphic elements. We believe GraVVITAS is the first practical, generic, low cost approach to providing refreshable accessible graphics. We have used a participatory design process with blind participants and a final evaluation of the tool shows that they can use it to understand a variety of graphics - tables, line graphs, and floorplans.",
isbn="978-3-642-23774-4"
}

@inproceedings{Holloway2018,
  title = {Accessible maps for the blind: Comparing 3D printed models with tactile graphics},
  author = {Holloway, Leona and Butler, Matthew and Marriott, Kim},
  year = {2018}, 
  location = {Montréal, QC, Canada}, 
  publisher = {ACM},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
  series =       {CHI '18},
  doi = {10.1145/3173574.3173772},
} 

@inproceedings{Holloway2022,
author = {Holloway, Leona and Ananthanarayan, Swamy and Butler, Matthew and De Silva, Madhuka Thisuri and Ellis, Kirsten and Goncu, Cagatay and Stephens, Kate and Marriott, Kim},
title = {Animations at Your Fingertips: Using a Refreshable Tactile Display to Convey Motion Graphics for People Who Are Blind or Have Low Vision},
year = {2022},
isbn = {9781450392587},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517428.3544797},
doi = {10.1145/3517428.3544797},
abstract = {People who are blind rely on touch and hearing to understand the world around them, however it is extremely difficult to understand movement through these modes. The advent of refreshable tactile displays (RTDs) offers the potential for blind people to access tactile animations for the very first time. A survey of touch readers and vision accessibility experts revealed a high level of enthusiasm for tactile animations, particularly those relating to education, mapping and concept development. Based on these suggestions, a range of tactile animations were developed and four were presented to 12 touch readers. The RTD held advantages over traditional tactile graphics for conveying movement, depth and height, however there were trade-offs in terms of resolution and textural properties. This work offers a first glimpse into how refreshable tactile displays can best be utilised to convey animated graphics for people who are blind.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
articleno = {32},
numpages = {16},
keywords = {refreshable displays, blind, tactile graphics, accessible graphics},
location = {Athens, Greece},
series = {ASSETS '22}
}

@inproceedings{Hurst2011,
 author = {Hurst, Amy and Tobias, Jasmine},
 title = {Empowering Individuals with Do-it-yourself Assistive Technology},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '11},
 year = {2011},
 isbn = {978-1-4503-0920-2},
 location = {Dundee, Scotland, UK},
 pages = {11--18},
 numpages = {8},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2049536.2049541},
 doi = {10.1145/2049536.2049541},
 acmid = {2049541},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {assistive technology, do-it-yourself, empowerment, human-centered computing, online communities, personal-scale manufacturing, rapid prototyping},
} 

@inproceedings{kane2014,
  title={Tracking@ stemxcomet: teaching programming to blind students via 3D printing, crisis management, and twitter},
  author={Kane, Shaun K. and Bigham, Jeffrey P.},
  booktitle={Proc. ACM Computer Science Education},
  pages={247--252},
  year={2014},
  organization={ACM}
}

@article{Sidner2018,
author = {Sidner, Candace L. and Bickmore, Timothy and Nooraie, Bahador and Rich, Charles and Ring, Lazlo and Shayganfar, Mahni and Vardoulakis, Laura},
title = {Creating New Technologies for Companionable Agents to Support Isolated Older Adults},
year = {2018},
issue_date = {August 2018},
publisher = {ACM},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3213050},
doi = {10.1145/3213050},
abstract = {This article reports on the development of capabilities for (on-screen) virtual agents and robots to support isolated older adults in their homes. A real-time architecture was developed to use a virtual agent or a robot interchangeably to interact via dialog and gesture with a human user. Users could interact with either agent on 12 different activities, some of which included on-screen games, and forms to complete. The article reports on a pre-study that guided the choice of interaction activities. A month-long study with 44 adults between the ages of 55 and 91 assessed differences in the use of the robot and virtual agent.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {17},
numpages = {27},
keywords = {virtual agent companions, human-robot interaction, social isolation, human-agent interaction studies, situated dialog systems, engagement, robot compantions, I.2.9, real-time dialog systems, older adults, H.5.2 keywords: virtual agents, H.1.2, Content Indicators: I.2.m, human-robot interaction studies}
}

@inproceedings{Kane2011,
 author = {Kane, Shaun K. and Wobbrock, Jacob O. and Ladner, Richard E.},
 title = {Usable Gestures for Blind People: Understanding Preference and Performance},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI '11},
 year = {2011},
 isbn = {978-1-4503-0228-9},
 location = {Vancouver, BC, Canada},
 pages = {413--422},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1978942.1979001},
 doi = {10.1145/1978942.1979001},
 acmid = {1979001},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accessibility, blind, gesture recognition, gestures, touch screens},
} 


@article{Schroeder2013,
author = {Noah L. Schroeder and Olusola O. Adesope and Rachel Barouch Gilbert},
title ={How Effective are Pedagogical Agents for Learning? A Meta-Analytic Review},
journal = {Journal of Educational Computing Research},
volume = {49},
number = {1},
pages = {1-39},
year = {2013},
doi = {10.2190/EC.49.1.a},

URL = { 
        https://doi.org/10.2190/EC.49.1.a
    
}
}

@ARTICLE{Nowak2003,
  author={Nowak, Kristine L. and Biocca, Frank},
  journal={Presence}, 
  title={The Effect of the Agency and Anthropomorphism on Users' Sense of Telepresence, Copresence, and Social Presence in Virtual Environments}, 
  year={2003},
  volume={12},
  number={5},
  pages={481-494},
  keywords={},
  doi={10.1162/105474603322761289}}


@inproceedings{Bickmore2001,
author = {Bickmore, Timothy and Cassell, Justine},
title = {Relational agents: a model and implementation of building user trust},
year = {2001},
isbn = {1581133278},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/365024.365304},
doi = {10.1145/365024.365304},
abstract = {Building trust with users is crucial in a wide range of applications, such as financial transactions, and some minimal degree of trust is required in all applications to even initiate and maintain an interaction with a user. Humans use a variety of relational conversational strategies, including small talk, to establish trusting relationships with each other. We argue that such strategies can also be used by interface agents, and that embodied conversational agents are ideally suited for this task given the myriad cues available to them for signaling trustworthiness. We describe a model of social dialogue, an implementation in an embodied conversation agent, and an experiment in which social dialogue was demonstrated to have an effect on trust, for users with a disposition to be  extroverts.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {396–403},
numpages = {8},
keywords = {embodied conversational agent, natural language, personality, small talk, social interface, trust},
location = {Seattle, Washington, USA},
series = {CHI '01}
}

@article{Mayer2003,
author = {Mayer, Richard and Sobko, Kristina and D. Mautone, Patricia},
year = {2003},
month = {06},
pages = {419-425},
title = {Social Cues in Multimedia Learning: Role of Speaker's Voice},
volume = {95},
journal = {Journal of Educational Psychology},
doi = {10.1037/0022-0663.95.2.419}
}

@article{Moreno2001,
author = { Roxana   Moreno  and  Richard E.   Mayer  and  Hiller A.   Spires  and  James C.   Lester },
title = {The Case for Social Agency in Computer-Based Teaching: Do Students Learn More Deeply When They Interact With Animated Pedagogical Agents?},
journal = {Cognition and Instruction},
volume = {19},
number = {2},
pages = {177-213},
year  = {2001},
publisher = {Routledge},
doi = {10.1207/S1532690XCI1902\_02},
}

@inproceedings{Borgstedt2023,
author = {Borgstedt, Jacqueline},
title = {Investigating the Potential of Life-like Haptic Cues for Socially Assistive Care Robots},
year = {2023},
isbn = {9781450399708},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3579972},
doi = {10.1145/3568294.3579972},
abstract = {Physical touch (e.g., hugging, holding hands, petting an animal) plays a fundamental role in the provision of socio-emotional support. Touch-based interactions should thus be considered for socially assistive robots designed to provide similar support. However, the haptic properties of currently existing robots are limited. While research has shown possibilities of integrating life-like haptic cues (e.g., thermal, vibrotactile, pressure cues) into robotic interfaces, current understanding of user experiences with life-like haptic cues delivered by a robot, and their potential to regulate user affect, is insufficient. The current and proposed works investigate whether integrating life-like haptic cues into Human-Robot Interaction (HRI) can enhance the socio-emotional support provided by socially assistive robots and improve the relationships with, and perceptions of such robots. The findings of this work will provide insights into user experiences of touching a socially assistive robot and their perceptions of life-like haptic cues. The contributions will provide concrete design suggestions on how haptic cues can be integrated into interfaces of socially assistive robots to enhance users' well-being during stress-inducing situations},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {745–747},
numpages = {3},
keywords = {socially assistive robots, social-physical human-robot interaction, human-robot touch, haptic interfaces, affective haptics},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{Nie2012,
author = {Nie, Jiaqi and Pak, Michelle and Marin, Angie Lorena and Sundar, S. Shyam},
title = {Can you hold my hand? physical warmth in human-robot interaction},
year = {2012},
isbn = {9781450310635},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157689.2157755},
doi = {10.1145/2157689.2157755},
abstract = {This study investigates whether the temperature of a robot's hand can affect perceptions of the robot as a companion. Our research empirically analyzes the responses of 39 individuals randomly assigned to one of three conditions: (1) holding a warm robot hand or (2) holding a cold robot hand or (3) not holding a robot hand. The effects of this simulated 'human touch' on HRI were examined in the context of viewing a horror film clip. Results suggest that experiences of physical warmth and handholding increase feelings of friendship and trust toward the robot. However, the discrepancy between the expectation of an actual human touch and the mechanical appearance of a robot could result in negative effects.},
booktitle = {Proc. ACM/IEEE Human-Robot Interaction},
pages = {201–202},
numpages = {2},
keywords = {robot warmth, robot handholding, human touch, hri},
location = {Boston, Massachusetts, USA},
series = {HRI '12}
}

@inproceedings{Luria2017,
author = {Luria, Michal and Hoffman, Guy and Zuckerman, Oren},
title = {Comparing Social Robot, Screen and Voice Interfaces for Smart-Home Control},
year = {2017},
isbn = {9781450346559},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025786},
doi = {10.1145/3025453.3025786},
abstract = {With domestic technology on the rise, the quantity and complexity of smart-home devices are becoming an important interaction design challenge. We present a novel design for a home control interface in the form of a social robot, commanded via tangible icons and giving feedback through expressive gestures. We experimentally compare the robot to three common smart-home interfaces: a voice-control loudspeaker; a wall-mounted touch-screen; and a mobile application. Our findings suggest that interfaces that rate higher on flow rate lower on usability, and vice versa. Participants' sense of control is highest using familiar interfaces, and lowest using voice control. Situation awareness is highest using the robot, and also lowest using voice control. These findings raise questions about voice control as a smart-home interface, and suggest that embodied social robots could provide for an engaging interface with high situation awareness, but also that their usability remains a considerable design challenge.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {580–628},
numpages = {49},
keywords = {social robots, smart-home control, interface modalities, human-robot interaction, home automation, embodied interaction, domestic technology},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{Bevan2015,
author = {Bevan, Chris and Stanton Fraser, Dana\"{e}},
title = {Shaking Hands and Cooperation in Tele-present Human-Robot Negotiation},
year = {2015},
isbn = {9781450328838},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2696454.2696490},
doi = {10.1145/2696454.2696490},
abstract = {A 3 x 2 between subjects design examined the effect of shaking hands prior to engaging in a single issue distributive negotiation, where one negotiator performed their role tele-presently through a `Nao' humanoid robot. An additional third condition of handshaking with feedback examined the effect of augmenting the tele-present handshake with haptic and tactile feedback for the non tele-present and tele-present negotiators respectively.Results showed that the shaking of hands prior to negotiating resulted in increased cooperation between negotiators, reflected by economic outcomes that were more mutually beneficial. Despite the fact that the non tele-present negotiator could not see the real face of their counterpart, tele-presence did not affect the degree to which negotiators considered one another to be trustworthy, nor did it affect the degree to which negotiators self-reported as intentionally misleading one another. Negotiators in the more powerful role of buyer rated their impressions of their counterpart more positively, but only if they themselves conducted their negotiations tele-presently.Results are discussed in terms of their design implications for social tele-presence robotics.},
booktitle = {Proc. of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction},
pages = {247–254},
numpages = {8},
keywords = {cooperation, haptic feedback, human-robot interaction, social robotics, social telepresence, tactile feedback},
location = {Portland, Oregon, USA},
series = {HRI '15}
}

@inproceedings{Bartlett2019,
author = {Bartlett, Rachel and Khoo, Yi Xuan and Hourcade, Juan Pablo and Rector, Kyle K.},
title = {Exploring the Opportunities for Technologies to Enhance Quality of Life with People who have Experienced Vision Loss},
year = {2019},
isbn = {9781450359702},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300421},
doi = {10.1145/3290605.3300421},
abstract = {Research predicts that 196 million people will be diagnosed with Age-Related Macular Degeneration (AMD) by 2020. People who experience AMD and other vision loss face barriers that affect their Quality of Life (QoL). People experience only modest improvement from technologies (e.g., screen readers, CCTV), tools (e.g., magnifying glasses, tactile buttons), and human help (e.g., friends, blindness organizations). Further, there are issues to accessing these resources based on one's place of residence. To explore these challenges and determine design implications to support people who have experienced vision loss (PVL), we conducted a qualitative semi-structured interview study exploring QoL with 10 PVL. We uncovered themes of supporting creative work, recognizing the impact of one's living in a non-urban setting on QoL, and increasing efficiency at accomplishing tasks. We motivate the inclusion of PVL in the design process because they learned skills while sighted and are now low vision or blind.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {vision loss, quality of life (qol), accessibility},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@Article{Lester1999,
author={Lester, James C.
and Stone, Brian A.
and Stelling, Gary D.},
title={Lifelike Pedagogical Agents for Mixed-initiative Problem Solving in Constructivist Learning Environments},
journal={User Modeling and User-Adapted Interaction},
year={1999},
month={Apr},
day={01},
volume={9},
number={1},
pages={1-44},
abstract={Mixed-initiative problem solving lies at the heart of knowledge- based learning environments. While learners are actively engaged in problem-solving activities, learning environments should monitor their progress and provide them with feedback in a manner that contributes to achieving the twin goals of learning effectiveness and learning efficiency. Mixed-initiative interactions are particularly critical for constructivist learning environments in which learners participate in active problem solving. We have recently begun to see the emergence of believable agents with lifelike qualities. Featured prominently in constructivist learning environments, lifelike pedagogical agents could couple key feedback functionalities with a strong visual presence by observing learners' progress and providing them with visually contextualized advice during mixed-initiative problem solving. For the past three years, we have been engaged in a large-scale research program on lifelike pedagogical agents and their role in constructivist learning environments. In the resulting computational framework, lifelike pedagogical agents are specified by},
issn={1573-1391},
doi={10.1023/A:1008374607830},
url={https://doi.org/10.1023/A:1008374607830}
}


@INPROCEEDINGS{Kidd2004,
  author={Kidd, C.D. and Breazeal, C.},
  booktitle={Proc. IEEE/RSJ Intelligent Robots and Systems}, 
  title={Effect of a robot on user perceptions}, 
  year={2004},
  volume={4},
  number={},
  pages={3559-3564 vol.4},
  keywords={Educational robots;Human robot interaction;Medical services;Education;Buildings;Humanoid robots;Robustness},
  doi={10.1109/IROS.2004.1389967}}


@inproceedings{Lester1997,
author = {Lester, James C. and Converse, Sharolyn A. and Kahler, Susan E. and Barlow, S. Todd and Stone, Brian A. and Bhogal, Ravinder S.},
title = {The persona effect: affective impact of animated pedagogical agents},
year = {1997},
isbn = {0897918029},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258549.258797},
doi = {10.1145/258549.258797},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {359–366},
numpages = {8},
keywords = {agents, children, educational applications, empirical studies, intelligent systems},
location = {Atlanta, Georgia, USA},
series = {CHI '97}
}

@inproceedings{Garg2020,
author = {Garg, Radhika and Sengupta, Subhasree},
title = {Conversational Technologies for In-home Learning: Using Co-Design to Understand Children's and Parents' Perspectives},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376631},
doi = {10.1145/3313831.3376631},
abstract = {Today, Conversational Agents (CA) are deeply integrated into the daily lives of millions of families, which has led children to extensively interact with such devices. Studies have suggested that the social nature of CA makes them a good learning companion for children. Therefore, to understand children's preferences for the use of CAs for the purpose of in-home learning, we conducted three participatory design sessions. In order to identify parents' requirements in this regard, we also included them in the third session. We found that children expect such devices to possess a personality and an advanced level of intelligence, and support multiple content domains and learning modes and human-like conversations. Parents desire such devices to include them in their children's learning activities, foster social engagement, and to allow them to monitor their children's use. This understanding will inform the design of future CAs for the purpose of in-home learning.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {children, co-design, conversational agents, cooperative inquiry, home, learning, learning companion, learning technology, parents, participatory design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@unpublished{NFIB2009,
title={The Braille Literacy Crisis in America: Facing the Truth, Reversing the Trend, Empowering the Blind},
author={National Federation of the Blind},
year={2009},
note={Available from \url{https://nfb.org/images/nfb/documents/pdf/braille_literacy_report_web.pdf}}
}

@inproceedings{Poppinga2011,
 author = {Poppinga, Benjamin and Magnusson, Charlotte and Pielot, Martin and Rassmus-Gr\"{o}hn, Kirsten},
 title = {TouchOver Map: Audio-tactile Exploration of Interactive Maps},
 booktitle = {Proc. of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
 series = {MobileHCI '11},
 year = {2011},
 isbn = {978-1-4503-0541-9},
 location = {Stockholm, Sweden},
 pages = {545--550},
 numpages = {6},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2037373.2037458},
 doi = {10.1145/2037373.2037458},
 acmid = {2037458},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {handheld devices, haptic UIs, mobile computing, multimodal interfaces, tactile},
} 

@inproceedings{Reichinger2016,
 author = {Reichinger, Andreas and Fuhrmann, Anton and Maierhofer, Stefan and Purgathofer, Werner},
 title = {Gesture-Based Interactive Audio Guide on Tactile Reliefs},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '16},
 year = {2016},
 isbn = {978-1-4503-4124-0},
 location = {Reno, Nevada, USA},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/2982142.2982176},
 doi = {10.1145/2982142.2982176},
 acmid = {2982176},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blind users, evaluation, finger tracking, gesture detection, interactive audio guide, tactile reliefs},
} 

@misc{Sheffield2016,
author={Rebecca Sheffield},
title={International Approaches to Rehabilitation Programs for Adults who are Blind or Visually Impaired: Delivery Models, Services, Challenges and Trends},
publisher={A Report for the World Blind Union}, 
year={2016}
}

@inproceedings{Shi2016,
  title={Tickers and Talker: An accessible labeling toolkit for 3D printed models},
  author={Shi, Lei and Zelzer, Idan and Feng, Catherine and Azenkot, Shiri},
  booktitle={Proc. of the 34rd Annual ACM Conference on Human Factors in Computing Systems (CHI'16)},
  doi = {10.1145/2858036.2858507},
  year={2016}
}


@inproceedings{Shi2017a,
 author = {Shi, Lei and Zhao, Yuhang and Azenkot, Shiri},
 title = {Markit and Talkit: A Low-Barrier Toolkit to Augment 3D Printed Models with Audio Annotations},
 booktitle = {Proc. of the 30th Annual ACM Symposium on User Interface Software and Technology},
 series = {UIST '17},
 year = {2017},
 isbn = {978-1-4503-4981-9},
 location = {Qu\&\#233;bec City, QC, Canada},
 pages = {493--506},
 numpages = {14},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3126594.3126650},
 doi = {10.1145/3126594.3126650},
 acmid = {3126650},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D models, computer vision, visual impairments},
} 

@INPROCEEDINGS{Rowell2005,
    author = {Jonathan Rowell and Simon Ungar},
    title = {Feeling our way: tactile map user requirements – a survey},
    booktitle = {Proc. International Cartographic Conference},
    year = {2005}
}

@article{Ungar1993,
author = {Simon Ungar and Mark Blades and Christopher Spencer},
title ={The role of tactile maps in mobility training},
journal = {British Journal of Visual Impairment},
volume = {11},
number = {2},
pages = {59-61},
year = {1993},
doi = {10.1177/026461969301100205},
URL = {https://doi.org/10.1177/026461969301100205}
}

@article{Miele2006,
author = {Joshua A. Miele and Steven Landau and Deborah Gilden},
title ={Talking TMAP: Automated generation of audio-tactile maps using                 Smith-Kettlewell's TMAP software},
journal = {British Journal of Visual Impairment},
volume = {24},
number = {2},
pages = {93-100},
year = {2006},
doi = {10.1177/0264619606064436},
URL = { 
        https://doi.org/10.1177/0264619606064436
}
}

@article{Landau2009,
year = {2009},
author = {Landau, Steven},
title = {An Interactive Talking Campus Model at Carroll Center for the Blind},
url = {http://www.touchgraphics.com/downloads/carrollcentertalkingcampusmodelfinalreportlow.pdf}
}

@unpublished{TTT,
title={T3 Tactile Tablet},
author={Touch Graphics Inc},
note={Available from \url{https://www.touchgraphics.com/education/t3}}
}

@unpublished{IVEO,
title={IVEO 3 Hands-On Learning System},
author={ViewPlus},
note={Available from \url{https://viewplus.com/product/iveo-3-hands-on-learning-system/}}
}

@article{NOMAD,
  title={Audio tactile systems for designing and learning complex environments as a vision impaired person: static and dynamic spatial information access},
  author={Parkes, Don},
  journal={Learning Environment Technology: Selected Papers from LETA},
  volume={94},
  pages={219--223},
  year={1994}
}

@InProceedings{Edwards2015,
author="Edwards, Alistair D. N.
and Hamid, Nazatul Naquiah Abd
and Petrie, Helen",
editor="Abascal, Julio
and Barbosa, Simone
and Fetter, Mirko
and Gross, Tom
and Palanque, Philippe
and Winckler, Marco",
title="Exploring Map Orientation with Interactive Audio-Tactile Maps",
booktitle="Human-Computer Interaction -- INTERACT 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="72--79",
abstract="Multi-modal interactive maps can provide a useful aid to navigation for blind people. We have been experimenting with such maps that present information in a tactile and auditory (speech) form, but with the novel feature that the map's orientation is tracked. This means that the map can be explored in a more ego-centric manner, as favoured by blind people. Results are encouraging, in that scores in an orientation task are better with the use of map rotation.",
isbn="978-3-319-22701-6"
}

@inproceedings{Reinders2020,
author = {Reinders, Samuel and Butler, Matthew and Marriott, Kim},
title = {"Hey Model!" - Natural User Interactions and Agency in Accessible Interactive 3D Models},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376145},
doi = {10.1145/3313831.3376145},
abstract = {While developments in 3D printing have opened up opportunities for improved access to graphical information for people who are blind or have low vision (BLV), they can provide only limited detailed and contextual information. Interactive 3D printed models (I3Ms) that provide audio labels and/or a conversational agent interface potentially overcome this limitation. We conducted a Wizard-of-Oz exploratory study to uncover the multi-modal interaction techniques that BLV people would like to use when exploring I3Ms, and investigated their attitudes towards different levels of model agency. These findings informed the creation of an I3M prototype of the solar system. A second user study with this model revealed a hierarchy of interaction, with BLV users preferring tactile exploration, followed by touch gestures to trigger audio labels, and then natural language to fill in knowledge gaps and confirm understanding.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {accessibility, individuals with disabilities &amp; assistive technologies, touch/haptic/pointing/gesture, input techniques},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Holloway2020b,
author = {Holloway, Leona and Butler, Matthew and Reinders, Samuel and Marriott, Kim},
title = {Non-visual access to graphical information on COVID-19},
year = {2020},
isbn = {9781450371032},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373625.3418015},
doi = {10.1145/3373625.3418015},
abstract = {A critical component of the worldwide response to the novel coronavirus COVID-19 pandemic has been providing the general public with information about the virus and the health measures designed to slow its spread. Much of this information has been presented as visual graphics. Have people who are blind or have low vision (BLV) been able to gain access to this information through nonvisual media and, if so, how? We investigate this issue using Australia as a case study.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
articleno = {75},
numpages = {3},
keywords = {Blind, COVID-19, graphics, information access, low vision},
location = {Virtual Event, Greece},
series = {ASSETS '20}
}

@inproceedings{Shi2017b,
 author = {Shi, Lei and Zhao, Yuhang and Azenkot, Shiri},
 title = {Designing Interactions for 3D Printed Models with Blind People},
 booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
 series = {ASSETS '17},
 year = {2017},
 isbn = {978-1-4503-4926-0},
 location = {Baltimore, Maryland, USA},
 pages = {200--209},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3132525.3132549},
 doi = {10.1145/3132525.3132549},
 acmid = {3132549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {elicitation, exploration behaviors, interactive 3d printed models, visually impairments},
} 

@inproceedings{Shi2019,
 author = {Shi, Lei and Lawson, Holly and Zhang, Zhuohao and Azenkot, Shiri},
 title = {Designing Interactive 3D Printed Models with Teachers of the Visually Impaired},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI '19},
 year = {2019},
 isbn = {978-1-4503-5970-2},
 location = {Glasgow, Scotland Uk},
 pages = {197:1--197:14},
 articleno = {197},
 numpages = {14},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3290605.3300427},
 doi = {10.1145/3290605.3300427},
 acmid = {3300427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interactive 3d printed models, user-centered design},
} 

@inproceedings{Shi2020,
author = {Shi, Lei and Zhao, Yuhang and Gonzalez Penuela, Ricardo and Kupferstein, Elizabeth and Azenkot, Shiri},
title = {Molder: An Accessible Design Tool for Tactile Maps},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376431},
doi = {10.1145/3313831.3376431},
abstract = {Tactile materials are powerful teaching aids for students with visual impairments (VIs). To design these materials, designers must use modeling applications, which have high learning curves and rely on visual feedback. Today, Orientation and Mobility (O&amp;M) specialists and teachers are often responsible for designing these materials. However, most of them do not have professional modeling skills, and many are visually impaired themselves. To address this issue, we designed Molder, an accessible design tool for interactive tactile maps, an important type of tactile materials that can help students learn O&amp;M skills. A designer uses Molder to design a map using tangible input techniques, and Molder provides auditory feedback and high-contrast visual feedback. We evaluated Molder with 12 participants (8 with VIs, 4 sighted). After a 30-minute training session, the participants were all able to use Molder to design maps with customized tactile and interactive information.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {design tool, visual impairments, tactile maps},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Taylor2015,
  title={TactileMaps.net: A web interface for generating customized 3D-printable tactile maps},
  author={Taylor, Brandon T. and Dey, Anind K. and Siewiorek, Dan P. and Smailagic, Asim},
  booktitle={Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
  pages={427--428},
  year={2015},
  organization={ACM},
  doi = {10.1145/2700648.2811336},
}



@article{Cassell1999c,
author = {Justine Cassell and Kristinn R. Thorisson},
title = {The power of a nod and a glance: Envelope vs. emotional feedback in animated conversational agents},
journal = {Applied Artificial Intelligence},
volume = {13},
number = {4-5},
pages = {519-538},
year = {1999},
publisher = {Taylor & Francis},
doi = {10.1080/088395199117360},


URL = { 
    
        https://doi.org/10.1080/088395199117360
    
    

}
}

@Article{Cassell1999b,
author={Cassell, J.
and Vilhj{\'a}lmsson, H.},
title={Fully Embodied Conversational Avatars: Making Communicative Behaviors Autonomous},
journal={Autonomous Agents and Multi-Agent Systems},
year={1999},
month={Mar},
day={01},
volume={2},
number={1},
pages={45-64},
abstract={Although avatars may resemble communicative interface agents, they have for the most part not profited from recent research into autonomous embodied conversational systems. In particular, even though avatars function within conversational environments (for example, chat or games), and even though they often resemble humans (with a head, hands, and a body) they are incapable of representing the kinds of knowledge that humans have about how to use the body during communication. Humans, however, do make extensive use of the visual channel for interaction management where many subtle and even involuntary cues are read from stance, gaze, and gesture. We argue that the modeling and animation of such fundamental behavior is crucial for the credibility and effectiveness of the virtual interaction in chat. By treating the avatar as a communicative agent, we propose a method to automate the animation of important communicative behavior, deriving from work in conversation and discourse theory. BodyChat is a system that allows users to communicate via text while their avatars automatically animate attention, salutations, turn taking, back-channel feedback, and facial expression. An evaluation shows that users found an avatar with autonomous conversational behaviors to be more natural than avatars whose behaviors they controlled, and to increase the perceived expressiveness of the conversation. Interestingly, users also felt that avatars with autonomous communicative behaviors provided a greater sense of user control.},
issn={1573-7454},
doi={10.1023/A:1010027123541},
url={https://doi.org/10.1023/A:1010027123541}
}

@article{Bickmore2013,
title = "Automated interventions for multiple health behaviors using conversational agents",
journal = "Patient Education and Counseling",
volume = "92",
number = "2",
pages = "142 - 148",
year = "2013",
issn = "0738-3991",
doi = "https://doi.org/10.1016/j.pec.2013.05.011",
url = "http://www.sciencedirect.com/science/article/pii/S0738399113002115",
author = "Timothy W. Bickmore and Daniel Schulman and Candace Sidner",
keywords = "Relational agent, Embodied conversational agent, Behavioral informatics, Dialog system, Health behavior change intervention, Physical activity promotion, Walking promotion, Diet promotion, Fruit and vegetable consumption promotion, Ontology",
abstract = "Objective
An automated health counselor agent was designed to promote both physical activity and fruit and vegetable consumption through a series of simulated conversations with users on their home computers.
Methods
The agent was evaluated in a 4-arm randomized trial of a two-month daily contact intervention comparing: (a) physical activity; (b) fruit and vegetable consumption; (c) both interventions; and (d) a non-intervention control. Physical activity was assessed using daily pedometer steps. Daily servings of fruit and vegetables were assessed using the NIH/NCI self-report Fruit and Vegetable Scan.
Results
Participants in the physical activity intervention increased their walking on average compared to the control group, while those in the fruit and vegetable intervention and combined intervention decreased walking. Participants in the fruit and vegetable intervention group consumed significantly more servings per day compared to those in the control group, and those in the combined intervention reported consuming more compared to those in the control group.
Conclusion
Automated health intervention software designed for efficient re-use is effective at changing health behavior.
Practice implications
Automated health behavior change interventions can be designed to facilitate translation and adaptation across multiple behaviors."
}

@Article{Watson2012,
author="Watson, Alice
and Bickmore, Timothy
and Cange, Abby
and Kulshreshtha, Ambar
and Kvedar, Joseph",
title="An Internet-Based Virtual Coach to Promote Physical Activity Adherence in Overweight Adults: Randomized Controlled Trial",
journal="J Med Internet Res",
year="2012",
month="Jan",
day="26",
volume="14",
number="1",
pages="e1",
keywords="Activity monitoring; pedometers; obesity; body mass index; telemedicine; telehealth",
abstract="Background: Addressing the obesity epidemic requires the development of effective, scalable interventions. Pedometers and Web-based programs are beneficial in increasing activity levels but might be enhanced by the addition of nonhuman coaching. Objectives: We hypothesized that a virtual coach would increase activity levels, via step count, in overweight or obese individuals beyond the effect observed using a pedometer and website alone. Methods: We recruited 70 participants with a body mass index (BMI) between 25 and 35 kg/m2 from the Boston metropolitan area. Participants were assigned to one of two study arms and asked to wear a pedometer and access a website to view step counts. Intervention participants also met with a virtual coach, an automated, animated computer agent that ran on their home computers, set goals, and provided personalized feedback. Data were collected and analyzed in 2008. The primary outcome measure was change in activity level (percentage change in step count) over the 12-week study, split into four 3-week time periods. Major secondary outcomes were change in BMI and participants' satisfaction. Results: The mean age of participants was 42 years; the majority of participants were female (59/70, 84{\%}), white (53/70, 76{\%}), and college educated (68/70, 97{\%}). Of the initial 70 participants, 62 completed the study. Step counts were maintained in intervention participants but declined in controls. The percentage change in step count between those in the intervention and control arms, from the start to the end, did not reach the threshold for significance (2.9{\%} vs --12.8{\%} respectively, P = .07). However, repeated measures analysis showed a significant difference when comparing percentage changes in step counts between control and intervention participants over all time points (analysis of variance, P = .02). There were no significant changes in secondary outcome measures. Conclusions: The virtual coach was beneficial in maintaining activity level. The long-term benefits and additional applications of this technology warrant further study. Trial Registration: ClinicalTrials.gov NCT00792207; http://clinicaltrials.gov/ct2/show/NCT00792207 (Archived by WebCite at http://www.webcitation.org/63sm9mXUD) ",
issn="1438-8871",
doi="10.2196/jmir.1629",
url="http://www.ncbi.nlm.nih.gov/pubmed/22281837"
}



@article{Laranjo2018,
    author = {Laranjo, Liliana and Dunn, Adam G and Tong, Huong Ly and Kocaballi, Ahmet Baki and Chen, Jessica and Bashir, Rabia and Surian, Didi and Gallego, Blanca and Magrabi, Farah and Lau, Annie Y S and Coiera, Enrico},
    title = "{Conversational agents in healthcare: a systematic review}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {25},
    number = {9},
    pages = {1248-1258},
    year = {2018},
    month = {07},
    abstract = "{Our objective was to review the characteristics, current applications, and evaluation measures of conversational agents with unconstrained natural language input capabilities used for health-related purposes.We searched PubMed, Embase, CINAHL, PsycInfo, and ACM Digital using a predefined search strategy. Studies were included if they focused on consumers or healthcare professionals; involved a conversational agent using any unconstrained natural language input; and reported evaluation measures resulting from user interaction with the system. Studies were screened by independent reviewers and Cohen’s kappa measured inter-coder agreement.The database search retrieved 1513 citations; 17 articles (14 different conversational agents) met the inclusion criteria. Dialogue management strategies were mostly finite-state and frame-based (6 and 7 conversational agents, respectively); agent-based strategies were present in one type of system. Two studies were randomized controlled trials (RCTs), 1 was cross-sectional, and the remaining were quasi-experimental. Half of the conversational agents supported consumers with health tasks such as self-care. The only RCT evaluating the efficacy of a conversational agent found a significant effect in reducing depression symptoms (effect size d = 0.44, p = .04). Patient safety was rarely evaluated in the included studies.The use of conversational agents with unconstrained natural language input capabilities for health-related purposes is an emerging field of research, where the few published studies were mainly quasi-experimental, and rarely evaluated efficacy or safety. Future studies would benefit from more robust experimental designs and standardized reporting.The protocol for this systematic review is registered at PROSPERO with the number CRD42017065917.}",
    issn = {1527-974X},
    doi = {10.1093/jamia/ocy072},
    url = {https://doi.org/10.1093/jamia/ocy072}
}




@inproceedings{Balasuriya2018,
 author = {Balasuriya, Saminda Sundeepa and Sitbon, Laurianne and Bayor, Andrew A. and Hoogstrate, Maria and Brereton, Margot},
 title = {Use of Voice Activated Interfaces by People with Intellectual Disability},
 booktitle = {Proc. of the 30th Australian Conference on Computer-Human Interaction},
 series = {OzCHI '18},
 year = {2018},
 isbn = {978-1-4503-6188-0},
 location = {Melbourne, Australia},
 pages = {102--112},
 numpages = {11},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3292147.3292161},
 doi = {10.1145/3292147.3292161},
 acmid = {3292161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {information access, intellectual disability, voice activated interfaces, voice assistant},
} 

@inproceedings{Wobbrock2009,
 author = {Wobbrock, Jacob O. and Morris, Meredith Ringel and Wilson, Andrew D.},
 title = {User-defined Gestures for Surface Computing},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI '09},
 year = {2009},
 isbn = {978-1-60558-246-7},
 location = {Boston, MA, USA},
 pages = {1083--1092},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1518701.1518866},
 doi = {10.1145/1518701.1518866},
 acmid = {1518866},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gesture recognition, gestures, guessability, referents, signs, surface, tabletop, think-aloud},
} 

@article{Rossetti2018,
  title = {Enabling Access to Cultural Heritage for the visually impaired: an Interactive 3D model of a Cultural Site},
  author = {V. Rossetti and Francesco Furfari and B. Leporini and S. Pelagatti and A. Quarta and V. Rossetti and F. Furfari and B. Leporini and Susanna Pelagatti and A. Quarta},
  year = 2018, 
  journal = {Procedia Computer Science}, 
  volume = 130,
  pages = {383--391},
}

@article{gual2012visual,
  title={Visual Impairment and urban orientation. Pilot study with tactile maps produced through 3D Printing},
  author={Gual, Jaume and Puyuelo, Marina and Llover{\'a}s, Joaquim and Merino, Lola},
  journal={Psyecology},
  volume={3},
  number={2},
  pages={239--250},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{wedler2012applied,
  title={Applied computational chemistry for the blind and visually impaired},
  author={Wedler, Henry B and Cohen, Sarah R and Davis, Rebecca L and Harrison, Jason G and Siebert, Matthew R and Willenbring, Dan and Hamann, Christian S and Shaw, Jared T and Tantillo, Dean J},
  journal={Journal of Chemical Education},
  volume={89},
  number={11},
  pages={1400--1404},
  year={2012},
  publisher={ACS Publications}
}

@inproceedings{vtyurina2018,
 author = {Vtyurina, Alexandra and Fourney, Adam},
 title = {Exploring the Role of Conversational Cues in Guided Task Support with Virtual Assistants},
 booktitle = {Proc. of the CHI Conference on Human Factors in Computing Systems},
 series = {CHI '18},
 year = {2018},
 isbn = {978-1-4503-5620-6},
 location = {Montreal QC, Canada},
 pages = {208:1--208:7},
 articleno = {208},
 numpages = {7},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3173574.3173782},
 doi = {10.1145/3173574.3173782},
 acmid = {3173782},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {conversational cues, conversational systems, task support},
} 

@inproceedings{kahn2008,
 author = {Kahn, Peter H. and Freier, Nathan G. and Kanda, Takayuki and Ishiguro, Hiroshi and Ruckert, Jolina H. and Severson, Rachel L. and Kane, Shaun K.},
 title = {Design Patterns for Sociality in Human-robot Interaction},
 booktitle = {Proc. of the 3rd ACM/IEEE International Conference on Human Robot Interaction},
 series = {HRI '08},
 year = {2008},
 isbn = {978-1-60558-017-3},
 location = {Amsterdam, The Netherlands},
 pages = {97--104},
 numpages = {8},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1349822.1349836},
 doi = {10.1145/1349822.1349836},
 acmid = {1349836},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design patterns, human-robot interaction, sociality},
} 

@article{kelley1984,
 author = {Kelley, J. F.},
 title = {An Iterative Design Methodology for User-friendly Natural Language Office Information Applications},
 journal = {ACM Trans. Inf. Syst.},
 issue_date = {Jan. 1984},
 volume = {2},
 number = {1},
 month = jan,
 year = {1984},
 issn = {1046-8188},
 pages = {26--41},
 numpages = {16},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/357417.357420},
 doi = {10.1145/357417.357420},
 acmid = {357420},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{hudson2003,
 author = {Hudson, Scott and Fogarty, James and Atkeson, Christopher and Avrahami, Daniel and Forlizzi, Jodi and Kiesler, Sara and Lee, Johnny and Yang, Jie},
 title = {Predicting Human Interruptibility with Sensors: A Wizard of Oz Feasibility Study},
 booktitle = {Proc. of the CHI Conference on Human Factors in Computing Systems},
 series = {CHI '03},
 year = {2003},
 isbn = {1-58113-630-7},
 location = {Ft. Lauderdale, Florida, USA},
 pages = {257--264},
 numpages = {8},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/642611.642657},
 doi = {10.1145/642611.642657},
 acmid = {642657},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context-aware computing, machine learning, sensor-based interfaces, situationally appropriate interaction},
} 

@inproceedings{wilcox2010,
 author = {Wilcox, Lauren and Morris, Dan and Tan, Desney and Gatewood, Justin},
 title = {Designing Patient-centric Information Displays for Hospitals},
 booktitle = {Proc. of the CHI Conference on Human Factors in Computing Systems},
 series = {CHI '10},
 year = {2010},
 isbn = {978-1-60558-929-9},
 location = {Atlanta, Georgia, USA},
 pages = {2123--2132},
 numpages = {10},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1753326.1753650},
 doi = {10.1145/1753326.1753650},
 acmid = {1753650},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {electronic medical records, patient awareness},
 }
 
 @inproceedings{akers2006,
 author = {Akers, David},
 title = {Wizard of Oz for Participatory Design: Inventing a Gestural Interface for 3D Selection of Neural Pathway Estimates},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
 series = {CHI EA '06},
 year = {2006},
 isbn = {1-59593-298-4},
 location = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
 pages = {454--459},
 numpages = {6},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/1125451.1125552},
 doi = {10.1145/1125451.1125552},
 acmid = {1125552},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D selection, Wizard of Oz prototyping, brain visualization, gestural interfaces, participatory design},
} 

@inproceedings{billah2018,
 author = {Billah, Syed Masum and Ashok, Vikas and Ramakrishnan, IV},
 title = {Write-it-Yourself with the Aid of Smartwatches: A Wizard-of-Oz Experiment with Blind People},
 booktitle = {23rd International Conference on Intelligent User Interfaces},
 series = {IUI '18},
 year = {2018},
 isbn = {978-1-4503-4945-1},
 location = {Tokyo, Japan},
 pages = {427--431},
 numpages = {5},
 url = {http://doi.acm.org.ezproxy.lib.monash.edu.au/10.1145/3172944.3173005},
 doi = {10.1145/3172944.3173005},
 acmid = {3173005},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accessibility, audio-haptic, blind, directional guidance, smartwatch., visual impairments, wearables, writing aid},
} 

@InProceedings{rader2014,
author="Rader, Joshua
and McDaniel, Troy
and Ramirez, Artemio
and Bala, Shantanu
and Panchanathan, Sethuraman",
editor="Stephanidis, Constantine",
title="A Wizard of Oz Study Exploring How Agreement/Disagreement Nonverbal Cues Enhance Social Interactions for Individuals Who Are Blind",
booktitle="HCI International 2014 - Posters' Extended Abstracts",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="243--248",
abstract="Given their visual nature, nonverbal social cues, such as facial and head movements, are largely inaccessible to individuals who are blind, limiting the information gleaned during interactions. While social assistive aids have explored some nonverbal cues, such as detecting and communicating facial expressions, relatively few nonverbal cues have been explored. A thorough and systematic study has yet to investigate the importance and usefulness of many nonverbal social cues for individuals who are blind. This work takes this first step by beginning to explore the nonverbal cue of agreement/disagreement as indicated by head/body movements including head nod, head shake, leaning forward and leaning backward. To facilitate the investigation of the usefulness of nonverbal cues for individuals who are blind, we propose the use of a Wizard of Oz experiment to rapidly evaluate nonverbal communications using existing technologies rather than building new and complete systems. We first explore the usefulness of agreement/disagreement nonverbal cues using our existing Social Interaction Assistant platform in which most of the seemingly automated processes were manually performed by a wizard without the knowledge of participants. We conducted an experiment with 11 individuals who are blind or visually impaired involving one-on-one interactions with trained interviewers. Results show the potential of agreement/disagreement nonverbal cues within the social interactions of individuals who are blind.",
isbn="978-3-319-07854-0"
}

@inproceedings{Quero2018,
author = {Cavazos Quero, Luis and Iranzo Bartolom\'{e}, Jorge and Lee, Seonggu and Han, En and Kim, Sunhee and Cho, Jundong},
title = {An Interactive Multimodal Guide to Improve Art Accessibility for Blind People},
year = {2018},
isbn = {9781450356503},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234695.3241033},
doi = {10.1145/3234695.3241033},
abstract = {The development of 3D printing technology has improved the engagement of the visually impaired people when experiencing two-dimensional visual artworks. However, it is still difficult to explore, experience and get a clear understanding. We introduce an interactive multimodal guide in which a 3D printed 2.5D representation of a painting can be explored by touch. Touching determined features in the representation triggers localized verbal, audio, wind, and light/heat feedback events that convey spatial and semantic information. In this work we present a working prototype developed through three sessions using a participatory design approach.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {346–348},
numpages = {3},
keywords = {assitive technologies, multimodal guide, 3d printing, blind people},
location = {Galway, Ireland},
series = {ASSETS '18}
}

@inproceedings{Quero2019,
author = {Cavazos Quero, Luis and Iranzo Bartolom\'{e}, Jorge and Lee, Dongmyeong and Lee, Yerin and Lee, Sangwon and Cho, Jundong},
title = {Jido: A Conversational Tactile Map for Blind People},
year = {2019},
isbn = {9781450366762},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3354600},
doi = {10.1145/3308561.3354600},
abstract = {Jido is a conversational tactile map and a mobile application prototype to improve the orientation and mobility of visually impaired people. The tactile map can identify the user's touch gestures and their locations during tactile exploration to provide feedback. Users can verbally interact with a conversational agent through their mobile device to receive confirmation, request information of points of interest, and receive step-by-step instructions that can be later reviewed during navigation. In this work, we describe opportunities and challenges faced by tactile map users and present the findings on the preliminary evaluation of our prototype.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {682–684},
numpages = {3},
keywords = {voice interaction, assitive technologies, blind people, navigation, tactile map},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{Bartolome2019,
author = {Iranzo Bartolome, Jorge and Cavazos Quero, Luis and Kim, Sunhee and Um, Myung-Yong and Cho, Jundong},
title = {Exploring Art with a Voice Controlled Multimodal Guide for Blind People},
year = {2019},
isbn = {9781450361965},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3294109.3300994},
doi = {10.1145/3294109.3300994},
abstract = {There is an increasing concern to improve the accessibility of artworks for blind people. Much of the effort has been focused on helping the visually impaired people to access the exhibition facilities, but the works of art hosted there are still difficult to experience for them. Particularly, the appreciation of visual artworks is hindered as blind visitors are not allowed to touch them in order to conserve their aesthetics and value. In this work we explore our findings using a prototype of a voice interactive multimodal guide designed to improve the accessibility of visual works of arts, such as paintings, for the blind people. The prototype identifies tactile gestures and voice commands that trigger audio descriptions and sounds while a person explores a 2.5D tactile representation of the artwork placed on the top surface of the prototype. Our preliminary findings include the results of eight user tests and Likert-type surveys.},
booktitle = {Proc. Tangible, Embedded, and Embodied Interaction},
pages = {383–390},
numpages = {8},
keywords = {human computer interaction, blind people, assistive technologies, voice interaction, multimodal guide},
location = {Tempe, Arizona, USA},
series = {TEI '19}
}


@Article{Quero2021,
AUTHOR = {Cavazos Quero, Luis and Iranzo Bartolomé, Jorge and Cho, Jundong},
TITLE = {Accessible Visual Artworks for Blind and Visually Impaired People: Comparing a Multimodal Approach with Tactile Graphics},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {297},
URL = {https://www.mdpi.com/2079-9292/10/3/297},
ISSN = {2079-9292},
ABSTRACT = {Despite the use of tactile graphics and audio guides, blind and visually impaired people still face challenges to experience and understand visual artworks independently at art exhibitions. Art museums and other art places are increasingly exploring the use of interactive guides to make their collections more accessible. In this work, we describe our approach to an interactive multimodal guide prototype that uses audio and tactile modalities to improve the autonomous access to information and experience of visual artworks. The prototype is composed of a touch-sensitive 2.5D artwork relief model that can be freely explored by touch. Users can access localized verbal descriptions and audio by performing touch gestures on the surface while listening to themed background music along. We present the design requirements derived from a formative study realized with the help of eight blind and visually impaired participants, art museum and gallery staff, and artists. We extended the formative study by organizing two accessible art exhibitions. There, eighteen participants evaluated and compared multimodal and tactile graphic accessible exhibits. Results from a usability survey indicate that our multimodal approach is simple, easy to use, and improves confidence and independence when exploring visual artworks.},
DOI = {10.3390/electronics10030297}
}

@inproceedings{Ghodke2019,
author = {Ghodke, Uttara and Yusim, Lena and Somanath, Sowmya and Coppin, Peter},
title = {The Cross-Sensory Globe: Participatory Design of a 3D Audio-Tactile Globe Prototype for Blind and Low-Vision Users to Learn Geography},
year = {2019},
isbn = {9781450358507},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3323686},
doi = {10.1145/3322276.3323686},
abstract = {We present our participatively and iteratively designed 3D audio-tactile globe that enables blind and low-vision users to perceive geo-spatial information. Blind and low-vision users rely on learning aids such as 2D-tactile graphics, braille maps and 3D models to learn about geography. We employed participatory design as an approach to prototyping and evaluating four different iterations of a cross-sensory globe that uses 3D detachable continents to provide geo-spatial haptic information in combination with audio labels. Informed by our participatory design and evaluation, we discuss cross-sensory educational aids as an alternative to visually-oriented globes. Our findings reveal affordances of 3D-tactile models for conveying concrete features of the Earth (such as varying elevations of landforms) and audio labels for conveying abstract categories about the Earth (such as continent names). We highlight the advantages of longitudinal participatory design that includes the lived experiences and DIY innovations of blind and low-vision users and makers.},
booktitle = {Proc. ACM Designing Interactive Systems Conference},
pages = {399–412},
numpages = {14},
keywords = {participatory design, visual impairment, inclusive design, non-visual, 3d tactile, co-design, cross-sensory},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@inproceedings{Choi2020,
author = {Choi, Dasom and Kwak, Daehyun and Cho, Minji and Lee, Sangsu},
title = {"Nobody Speaks That Fast!" An Empirical Study of Speech Rate in Conversational Agents for People with Vision Impairments},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376569},
doi = {10.1145/3313831.3376569},
abstract = {The number of people with vision impairments using Conversational Agents (CAs) has increased because of the potential of this technology to support them. As many visually impaired people are accustomed to understanding fast speech, most screen readers or voice assistant systems offer speech rate settings. However, current CAs are designed to interact at a human-like speech rate without considering their accessibility. In this study, we tried to understand how people with vision impairments use CA at a fast speech rate. We conducted a 20-day in-home study that examined the CA use of 10 visually impaired people at default and fast speech rates. We investigated the difference in visually impaired people's CA use with different speech rates and their perception toward CA at each rate. Based on these findings, we suggest considerations for the future design of CA speech rate for those with visual impairments.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {people with vision impairments, conversational agents, speech rate, accessibility},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Branham2019,
author = {Branham, Stacy M. and Mukkath Roy, Antony Rishin},
title = {Reading Between the Guidelines: How Commercial Voice Assistant Guidelines Hinder Accessibility for Blind Users},
year = {2019},
isbn = {9781450366762},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353797},
doi = {10.1145/3308561.3353797},
abstract = {Voice-Activated Personal Assistants (VAPAs)-like Apple Siri and Amazon Alexa - have rapidly become common features on mobile devices and in homes of millions of people around the world. They have proven to be particularly valuable to people with disabilities, chiefly among people with visual impairments. Yet, we still know relatively little about the fundamental metaphors and guidelines for designing voice assistants, and how they might empower and constrain visually impaired users. To address this need, we conducted a qualitative document review of VAPA design guidelines published by top commercial vendors Amazon, Google, Microsoft, Apple and Alibaba. We found that guidelines have many commonalities that surface an underlying assumption that VAPA interfaces should be modeled after human-human conversation. We draw on prior work about needs of people with visual impairments to critique this taken-for-granted human-human conversation metaphor and offer amendments to prevailing design guidelines that can make this now-pervasive platform more fully achieve its potential to become universally usable.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {446–458},
numpages = {13},
keywords = {blindness, accessibility, voice assistant, conversation, design guidelines},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{Luger2016,
author = {Luger, Ewa and Sellen, Abigail},
title = {"Like Having a Really Bad PA": The Gulf between User Expectation and Experience of Conversational Agents},
year = {2016},
isbn = {9781450333627},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858288},
doi = {10.1145/2858036.2858288},
abstract = {The past four years have seen the rise of conversational agents (CAs) in everyday life. Apple, Microsoft, Amazon, Google and Facebook have all embedded proprietary CAs within their software and, increasingly, conversation is becoming a key mode of human-computer interaction. Whilst we have long been familiar with the notion of computers that speak, the investigative concern within HCI has been upon multimodality rather than dialogue alone, and there is no sense of how such interfaces are used in everyday life. This paper reports the findings of interviews with 14 users of CAs in an effort to understand the current interactional factors affecting everyday use. We find user expectations dramatically out of step with the operation of the systems, particularly in terms of known machine intelligence, system capability and goals. Using Norman's 'gulfs of execution and evaluation' [30] we consider the implications of these findings for the design of future systems.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {5286–5297},
numpages = {12},
keywords = {mental models, evaluation, conversational agents},
location = {San Jose, California, USA},
series = {CHI '16}
}
@inproceedings{Holloway2019,
author = {Holloway, Leona and Marriott, Kim and Butler, Matthew and Borning, Alan},
title = {Making Sense of Art: Access for Gallery Visitors with Vision Impairments},
year = {2019},
isbn = {9781450359702},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300250},
doi = {10.1145/3290605.3300250},
abstract = {While there is widespread recognition of the need to provide people with vision impairments (PVI) equitable access to cultural institutions such as art galleries, this is not easy. We present the results of a collaboration with a regional art gallery who wished to open their collection to PVIs in the local community. We describe a novel model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As far as possible the model supports autonomous exploration by PVIs. It was informed by a value sensitive design exploration of the values and value conflicts of the primary stakeholders.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {blindness, art, value sensitive design, vision impairment, 3d printing, accessibility},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{Hasper2015,
 ISSN = {0047231X, 19434898},
 URL = {http://www.jstor.org/stable/43632001},
 author = {Eric Hasper and Rogier A. Windhorst and Terri Hedgpeth and Leanne Van Tuyl and Ashleigh Gonzales and Britta Martinez and Hongyu Yu and Zoltan Farkas and Debra P. Baluch},
 journal = {Journal of College Science Teaching},
 number = {6},
 pages = {92--99},
 publisher = {National Science Teachers Association},
 title = {Methods for Creating and Evaluating 3D Tactile Images to Teach STEM Courses to the Visually Impaired},
 urldate = {2023-02-04},
 volume = {44},
 year = {2015}
}

@inproceedings{Holloway2019b,
author = {Holloway, Leona and Marriott, Kim and Butler, Matthew and Reinders, Samuel},
title = {3D Printed Maps and Icons for Inclusion: Testing in the Wild by People Who Are Blind or Have Low Vision},
year = {2019},
isbn = {9781450366762},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353790},
doi = {10.1145/3308561.3353790},
abstract = {The difficulty and consequent fear of travel is one of the most disabling consequences of blindness and severe vision impairment, affecting confidence and quality of life. Traditional tactile graphics are vital in the Orientation and Mobility training process, however 3D printing may have the capacity to enable production of more meaningful and inclusive maps. This study explored the use of 3D printed maps on site at a public event to examine their suitability and to identify guidelines for the design of future 3D maps. An iterative design process was used in the production of the 3D maps, with feedback from visitors who are blind or have low vision informing the recommendations for their design and use. For example, it was found that many representational 3D icons could be recognised by touch without the need for a key and that such a map helped form mental models of the event space. Complex maps, however, require time to explore and should be made available before an event or at the entrance in a comfortable position. The maps were found to support the orientation and mobility process, and importantly to also promote a positive message about inclusion and accessibility.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
pages = {183–195},
numpages = {13},
keywords = {maps, orientation &amp; mobility, vision impairment, blind, low vision, 3d printing},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19}
}

@inproceedings{Albouys2018,
author = {Albouys-Perrois, J\'{e}r\'{e}my and Laviole, J\'{e}r\'{e}my and Briant, Carine and Brock, Anke M.},
title = {Towards a Multisensory Augmented Reality Map for Blind and Low Vision People: A Participatory Design Approach},
year = {2018},
isbn = {9781450356206},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174203},
doi = {10.1145/3173574.3174203},
abstract = {Current low-tech Orientation &amp; Mobility (O&amp;M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&amp;M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&amp;M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {augmented reality, geographic maps, participatory design, visual impairment, accessibility},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{Bolt1980,
author = {Bolt, Richard A.},
title = {“Put-That-There”: Voice and Gesture at the Graphics Interface},
year = {1980},
isbn = {0897910214},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800250.807503},
doi = {10.1145/800250.807503},
abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
booktitle = {Proc. of the 7th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {262–270},
numpages = {9},
keywords = {Spatial data management, Gesture, Space sensing, Graphics interface, Speech input, Graphics, Voice input, Man-machine interfaces},
location = {Seattle, Washington, USA},
series = {SIGGRAPH '80}
}

@article{10.1145/965105.807503,
author = {Bolt, Richard A.},
title = {“Put-That-There”: Voice and Gesture at the Graphics Interface},
year = {1980},
issue_date = {July 1980},
publisher = {ACM},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {0097-8930},
url = {https://doi.org/10.1145/965105.807503},
doi = {10.1145/965105.807503},
abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality.The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
journal = {SIGGRAPH Comput. Graph.},
month = {jul},
pages = {262–270},
numpages = {9},
keywords = {Space sensing, Spatiauria data management, Voice input, Graphics interface, Man-machine interfaces, Gesture, Graphics, Speech input}
}

@article{Rheu2021,
author = {Minjin Rheu, Ji Youn Shin, Wei Peng and Jina Huh-Yoo},
title = {Systematic Review: Trust-Building Factors and Implications for Conversational Agent Design},
journal = {International Journal of Human–Computer Interaction},
volume = {37},
number = {1},
pages = {81-96},
year = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/10447318.2020.1807710},


URL = { 
    
        https://doi.org/10.1080/10447318.2020.1807710
    
    

}

}

@article{Betsy1993,
author = {Phillips, Betsy and Zhao, Hongxin},
title = {Predictors of Assistive Technology Abandonment},
journal = {Assistive Technology},
volume = {5},
number = {1},
pages = {36-45},
year  = {1993},
publisher = {Taylor & Francis},
doi = {10.1080/10400435.1993.10132205},
    note ={PMID: 10171664},
URL = {https://doi.org/10.1080/10400435.1993.10132205}
}

@Inbook{Hevner2010,
author="Hevner, Alan
and Chatterjee, Samir",
title="Design Science Research in Information Systems",
bookTitle="Design Research in Information Systems: Theory and Practice",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="9--22",
abstract="Design activities are central to most applied disciplines. Research in design has a long history in many fields including architecture, engineering, education, psychology, and the fine arts (Cross 2001). The computing and information technology (CIT) field since its advent in the late 1940s has appropriated many of the ideas, concepts, and methods of design science that have originated in these other disciplines. However, information systems (IS) as composed of inherently mutable and adaptable hardware, software, and human interfaces provide many unique and challenging design problems that call for new and creative ideas.",
isbn="978-1-4419-5653-8",
doi="10.1007/978-1-4419-5653-8_2",
url="https://doi.org/10.1007/978-1-4419-5653-8_2"
}


@Inbook{Heuwinkel2013,
author="Heuwinkel, Kerstin",
editor="Trappl, Robert",
title="Framing the Invisible -- The Social Background of Trust",
bookTitle="Your Virtual Butler: The Making-of",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="16--26",
abstract="A butler is part of your personal surrounding -- a person you can trust. If information and communication technology (ICT) shall become a virtual butler, trust is needed. But - trust has many meanings. Very often the focus is put on a cognitive model of trust. According to our studies, in healthcare and tourism this definition has to be broadened. People should not (only) be seen as rational problem solvers nor should human action be described as a chain of sequential and hierarchical step-by-step decisions. Emotional and social aspects have to be considered, too. Trust should be seen as integral part of interpersonal relationships that are shaped by cultural conditions. Referring to Goffman's frame analysis, we will discuss if and how ICT, especially the virtual butler, can be framed in a way that trust is possible.",
isbn="978-3-642-37346-6",
doi="10.1007/978-3-642-37346-6_3",
url="https://doi.org/10.1007/978-3-642-37346-6_3"
}

@book{Saldaña2021,
booktitle = {The coding manual for qualitative researchers},
edition = {Fourth edition.},
isbn = {9781529731750},
keywords = {Qualitative research},
language = {eng},
lccn = {2020941916},
publisher = {SAGE Publications},
title = {The coding manual for qualitative researchers / Johnny Saldaña.},
year = {2021},
author = {Saldaña, Johnny},
address = {Los Angeles, California ;},
}

@inproceedings{Boberg2015,
author = {Boberg, Marion and Karapanos, Evangelos and Holopainen, Jussi and Lucero, Andr\'{e}s},
title = {PLEXQ: Towards a Playful Experiences Questionnaire},
year = {2015},
isbn = {9781450334662},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2793107.2793124},
doi = {10.1145/2793107.2793124},
abstract = {Playfulness is an important, but often neglected, design quality for interactive products. This paper presents a first step towards a validated questionnaire called PLEXQ, which measures 17 different facets of playful user experiences. We describe the development and validation of the questionnaire, from the generation of 231 items, to the current questionnaire consisting of 17 constructs of playfulness, each measured through three items. Using PLEXQ we discuss the nature of playfulness by looking at the role of age, gender, and product type in one's proclivity to experience playfulness differently. Finally, we reveal a four-factor structure of playfulness and discuss the implications for further theory development.},
booktitle = {Proc. of the 2015 Annual Symposium on Computer-Human Interaction in Play},
pages = {381–391},
numpages = {11},
keywords = {user experience, scale development, playful experiences},
location = {London, United Kingdom},
series = {CHI PLAY '15}
}

@inproceedings{Cassell1999,
author = {Cassell, J. and Bickmore, T. and Billinghurst, M. and Campbell, L. and Chang, K. and Vilhj\'{a}lmsson, H. and Yan, H.},
title = {Embodiment in conversational interfaces: Rea},
year = {1999},
isbn = {0201485591},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/302979.303150},
doi = {10.1145/302979.303150},
abstract = {In this paper, we argue for embodied corrversational characters
as the logical extension of the metaphor of human - computer
interaction as a conversation. We argue that the only way to fully
model the richness of human I&+ to-face communication is to
rely on conversational analysis that describes sets of
conversational behaviors as fi~lfilling conversational functions,
both interactional and propositional. We demonstrate how to
implement this approach in Rea, an embodied conversational agent
that is capable of both multimodal input understanding and output
generation in a limited application domain. Rea supports both
social and task-oriented dialogue. We discuss issues that need to
be addressed in creating embodied conversational agents, and
describe the architecture of the Rea interface.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {520–527},
numpages = {8},
keywords = {conversational characters, intelligent agents, multimodal input, multimodal output},
location = {Pittsburgh, Pennsylvania, USA},
series = {CHI '99}
}

@inproceedings{Kontogiorgos2020,
author = {Kontogiorgos, Dimosthenis and van Waveren, Sanne and Wallberg, Olle and Pereira, Andre and Leite, Iolanda and Gustafson, Joakim},
title = {Embodiment Effects in Interactions with Failing Robots},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376372},
doi = {10.1145/3313831.3376372},
abstract = {The increasing use of robots in real-world applications will inevitably cause users to encounter more failures in interactions. While there is a longstanding effort in bringing human-likeness to robots, how robot embodiment affects users' perception of failures remains largely unexplored. In this paper, we extend prior work on robot failures by assessing the impact that embodiment and failure severity have on people's behaviours and their perception of robots. Our findings show that when using a smart-speaker embodiment, failures negatively affect users' intention to frequently interact with the device, however not when using a human-like robot embodiment. Additionally, users significantly rate the human-like robot higher in terms of perceived intelligence and social presence. Our results further suggest that in higher severity situations, human-likeness is distracting and detrimental to the interaction. Drawing on quantitative findings, we discuss benefits and drawbacks of embodiment in robot failures that occur in guided tasks.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {common ground, conversational failures, guided tasks, smart-speakers, social robots, time pressure},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@article{Nunamaker2011,
author = {Jay F. Nunamaker and Douglas C. Derrick and Aaron C. Elkins and Judee K. Burgoon and Mark W. Patton},
title = {Embodied Conversational Agent-Based Kiosk for Automated Interviewing},
journal = {Journal of Management Information Systems},
volume = {28},
number = {1},
pages = {17-48},
year = {2011},
publisher = {Routledge},
doi = {10.2753/MIS0742-1222280102},
URL = {https://doi.org/10.2753/MIS0742-1222280102}
}

@Article{Yagoda2012,
author={Yagoda, Rosemarie E.
and Gillan, Douglas J.},
title={You Want Me to Trust a ROBOT? The Development of a Human--Robot Interaction Trust Scale},
journal={International Journal of Social Robotics},
year={2012},
month={Aug},
day={01},
volume={4},
number={3},
pages={235-248},
abstract={Trust plays a critical role when operating a robotic system in terms of both acceptance and usage. Considering trust is a multidimensional context dependent construct, the differences and common themes were examined to identify critical considerations within human--robot interaction (HRI). In order to examine the role of trust within HRI, a measurement tool was generated based on five attributes: team configuration, team processes, context, task, and system (Yagoda in Human Factors and Ergonomics Society Annual Meeting, San Francisco, CA, pp. 304--308, 2010). The HRI trust scale was developed based on two studies. The first study conducts a content validity assessment of preliminary items generated, based on a review of previous research within HRI and automation, using subject matter experts (SMEs). The second study assesses the quality of each trust scale item derived from the first study. The results were then compiled to generate the HRI trust measurement tool.},
issn={1875-4805},
doi={10.1007/s12369-012-0144-0},
url={https://doi.org/10.1007/s12369-012-0144-0}
}

@article{OBrien2013,
author = {O'Brien, Heather L. and Lebow, Mahria},
title = {Mixed-methods approach to measuring user experience in online news interactions},
journal = {Journal of the American Society for Information Science and Technology},
volume = {64},
number = {8},
pages = {1543-1556},
keywords = {text mining, content filtering, automatic classification},
doi = {https://doi.org/10.1002/asi.22871},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.22871}
}




@article{Jian2000,
author = {Jiun-Yin Jian, Ann M. Bisantz and Colin G. Drury},
title = {Foundations for an Empirically Determined Scale of Trust in Automated Systems},
journal = {International Journal of Cognitive Ergonomics},
volume = {4},
number = {1},
pages = {53-71},
year = {2000},
publisher = {Routledge},
doi = {10.1207/S15327566IJCE0401\_04},


URL = { 
    
        https://doi.org/10.1207/S15327566IJCE0401_04
    
    

}
}

@article{Gulati2019,
author = {Siddharth Gulati, Sonia Sousa and David Lamas},
title = {Design, development and evaluation of a human-computer trust scale},
journal = {Behaviour \& Information Technology},
volume = {38},
number = {10},
pages = {1004-1015},
year = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/0144929X.2019.1656779},
URL = {https://doi.org/10.1080/0144929X.2019.1656779}
}

@inproceedings{Reinders2023,
author = {Reinders, Samuel and Ananthanarayan, Swamy and Butler, Matthew and Marriott, Kim},
title = {Designing Conversational Multimodal 3D Printed Models with People who are Blind},
year = {2023},
isbn = {9781450398930},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3595989},
doi = {10.1145/3563657.3595989},
abstract = {3D printed models have been used to improve access to graphical information by people who are blind, offering benefits over conventional accessible graphics. Here we investigate an interactive 3D printed model (I3M) that combines a conversational interface with haptic vibration and touch to provide more natural and accessible experiences. Specifically, we co-designed a multimodal model of the Solar System with nine blind people and evaluated the prototype with another seven blind participants. We discuss our journey from a design perspective, focusing on touch, conversational and multimodal interactions. Based on our experience, we suggest design recommendations that consider blind users’ desire for independence and control, customisation, comfort and use of prior experience.},
booktitle = {Proc. ACM Designing Interactive Systems Conference},
pages = {2172–2188},
numpages = {17},
keywords = {3D Printed Models, Accessibility, Blind, Conversational Interface, Multimodal Interaction;},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@Inbook{OBrien2016,
author="O'Brien, Heather",
editor="O'Brien, Heather
and Cairns, Paul",
title="Theoretical Perspectives on User Engagement",
bookTitle="Why Engagement Matters: Cross-Disciplinary Perspectives of User Engagement in Digital Media",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="1--26",
abstract="This chapter examines the theoretical foundations of user engagement. First, the definition of user engagement is deconstructed using key principles for evaluating concepts: clarity, scope and meaning. Second, two theoretical frameworks, Mihaly Csikszentmihalyi's Flow Theory and John Dewey's Philosophy of Experience, are presented that have informed much work on user engagement over the past decades. Third, several measurement and behavioural models of user engagement are articulated and compared. Though not an exhaustive review of the literature, the chapter identifies key works on user engagement over the past 30 years and areas of consensus and divergence in how user engagement is conceptualized in the research. The purpose of the chapter is not to propose a unified theory of engagement but to present a series of unifying propositions and open questions to inform future studies and to strengthen the theoretical framing of user engagement in theory and application",
isbn="978-3-319-27446-1",
doi="10.1007/978-3-319-27446-1_1",
url="https://doi.org/10.1007/978-3-319-27446-1_1"
}

@inproceedings{Nagassa2023,
author = {Nagassa, Ruth G and Butler, Matthew and Holloway, Leona and Goncu, Cagatay and Marriott, Kim},
title = {3D Building Plans: Supporting Navigation by People who are Blind or have Low Vision in Multi-Storey Buildings},
year = {2023},
isbn = {9781450394215},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581389},
doi = {10.1145/3544548.3581389},
abstract = {Independent travel and navigation in new environments, in particular multi-storey buildings, is a major challenge for people who are blind or have low vision (BLV). Using tactile maps as part of orientation and mobility (O&M) training, BLV people can build a cognitive map of an environment before visiting. Tactile maps of multi-level environments, however, have received little attention. We investigated the usefulness of 3D printed models of buildings, through a user study with nine BLV adults. Three designs were evaluated: flat, overlapped-sliding and overlapped-rotating. All three designs were reported to be useful, usable, engaging and allowed participants to build a cognitive map of the building. There was a strong user preference for the overlapped presentations, which were reported to be more effective in supporting cross-floor spatial knowledge. This exploration of the design space of 3D building plans demonstrates their value and we hope will encourage their provision in O&M training.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
articleno = {539},
numpages = {19},
keywords = {3D printed maps, indoor accessibility, multi-level representation, spatial cognition},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{Pradhan2019,
author = {Pradhan, Alisha and Findlater, Leah and Lazar, Amanda},
title = {"Phantom Friend" or "Just a Box with Information": Personification and Ontological Categorization of Smart Speaker-based Voice Assistants by Older Adults},
year = {2019},
issue_date = {November 2019},
publisher = {ACM},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359316},
doi = {10.1145/3359316},
abstract = {As voice-based conversational agents such as Amazon Alexa and Google Assistant move into our homes, researchers have studied the corresponding privacy implications, embeddedness in these complex social environments, and use by specific user groups. Yet it is unknown how users categorize these devices: are they thought of as just another object, like a toaster? As a social companion? Though past work hints to human-like attributes that are ported onto these devices, the anthropomorphization of voice assistants has not been studied in depth. Through a study deploying Amazon Echo Dot Devices in the homes of older adults, we provide a preliminary assessment of how individuals 1) perceive having social interactions with the voice agent, and 2) ontologically categorize the voice assistants. Our discussion contributes to an understanding of how well-developed theories of anthropomorphism apply to voice assistants, such as how the socioemotional context of the user (e.g., loneliness) drives increased anthropomorphism. We conclude with recommendations for designing voice assistants with the ontological category in mind, as well as implications for the design of technologies for social companionship for older adults.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {214},
numpages = {21},
keywords = {voice assistants, smart speakers, personification, ontology, older adults, anthropomorphism}
}

@inproceedings{Lucero2010,
author = {Lucero, Andr\'{e}s and Arrasvuori, Juha},
title = {PLEX Cards: a source of inspiration when designing for playfulness},
year = {2010},
isbn = {9781605589077},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1823818.1823821},
doi = {10.1145/1823818.1823821},
abstract = {Playfulness can be observed in all areas of human activity. It is an attitude of making activities more enjoyable. Designing for playfulness involves creating objects that elicit a playful approach and provide enjoyable experiences. In this paper we introduce the design and evaluation of the PLEX Cards and its two related idea generation techniques. The cards were created to communicate the 22 categories of a Playful Experiences framework to designers and other stakeholders who wish to design for playfulness. We have evaluated the practical use of the cards by applying them in three design cases. The results show that the PLEX Cards are a valuable source of inspiration when designing for playfulness and the techniques help create a large amount of ideas in a short time.},
booktitle = {Proc. of the 3rd International Conference on Fun and Games},
pages = {28–37},
numpages = {10},
keywords = {card, design methods, inspiration, playfulness, workshop},
location = {Leuven, Belgium},
series = {Fun and Games '10}
}

@inproceedings{Liao2018,
author = {Liao, Q. Vera and Mas-ud Hussain, Muhammed and Chandar, Praveen and Davis, Matthew and Khazaeni, Yasaman and Crasso, Marco Patricio and Wang, Dakuo and Muller, Michael and Shami, N. Sadat and Geyer, Werner},
title = {All Work and No Play?},
year = {2018},
isbn = {9781450356206},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173577},
doi = {10.1145/3173574.3173577},
abstract = {Many conversational agents (CAs) are developed to answer users' questions in a specialized domain. In everyday use of CAs, user experience may extend beyond satisfying information needs to the enjoyment of conversations with CAs, some of which represent playful interactions. By studying a field deployment of a Human Resource chatbot, we report on users' interest areas in conversational interactions to inform the development of CAs. Through the lens of statistical modeling, we also highlight rich signals in conversational interactions for inferring user satisfaction with the instrumental usage and playful interactions with the agent. These signals can be utilized to develop agents that adapt functionality and interaction styles. By contrasting these signals, we shed light on the varying functions of conversational interactions. We discuss design implications for CAs, and directions for developing adaptive agents based on users' conversational behaviors.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {user modeling, playful, human-agent interaction, dialog system, conversational agent, chatbot, adaption},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@Article{Coeckelbergh2011,
author={Coeckelbergh, Mark},
title={You, robot: on the linguistic construction of artificial others},
journal={AI {\&} SOCIETY},
year={2011},
month={Feb},
day={01},
volume={26},
number={1},
pages={61-69},
abstract={How can we make sense of the idea of `personal' or `social' relations with robots? Starting from a social and phenomenological approach to human--robot relations, this paper explores how we can better understand and evaluate these relations by attending to the ways our conscious experience of the robot and the human--robot relation is mediated by language. It is argued that our talk about and to robots is not a mere representation of an objective robotic or social-interactive reality, but rather interprets and co-shapes our relation to these artificial quasi-others. Our use of language also changes as a result of our experiences and practices. This happens when people start talking to robots. In addition, this paper responds to the ethical objection that talking to and with robots is both unreal and deceptive. It is concluded that in order to give meaning to human--robot relations, to arrive at a more balanced ethical judgment, and to reflect on our current form of life, we should complement existing objective-scientific methodologies of social robotics and interaction studies with interpretations of the words, conversations, and stories in and about human--robot relations.},
issn={1435-5655},
doi={10.1007/s00146-010-0289-z},
url={https://doi.org/10.1007/s00146-010-0289-z}
}




@article{wiebe2014,
title = {Measuring engagement in video game-based environments: Investigation of the User Engagement Scale},
journal = {Computers in Human Behavior},
volume = {32},
pages = {123-132},
year = {2014},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0747563213004433},
author = {Eric N. Wiebe and Allison Lamb and Megan Hardy and David Sharek},
keywords = {Engagement, Psychometric measurement, Flow, Video game},
abstract = {This research investigated the use of the User Engagement Scale (UES) as a psychometric tool to measure engagement during video game-play. Exploratory factor analysis revealed four factors (Focused Attention, Perceived Usability, Aesthetics, and Satisfaction) as compared to the six found in the original development of the UES. In the context of video game-play, a revised UES (UES) demonstrated better psychometric properties than the original UES defined by six subscales, including enhanced reliability. Further validity analysis included comparisons with the Flow State Scale (FSS), showing the complementary nature of the two scales and what constructs both scales might be measuring in a video game context. Criterion validity analysis demonstrated that UESz was more predictive of game performance than the FSS. Findings related to both the UESz and FSS were discussed relative to an overarching framework of hedonic and utilitarian qualities of game-play.}
}

@inproceedings{Shani2022,
author = {Shani, Chen and Libov, Alexander and Tolmach, Sofia and Lewin-Eytan, Liane and Maarek, Yoelle and Shahaf, Dafna},
title = {“Alexa, Do You Want to Build a Snowman?” Characterizing Playful Requests to Conversational Agents},
year = {2022},
isbn = {9781450391566},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491101.3519870},
doi = {10.1145/3491101.3519870},
abstract = {Conversational Agents (CAs) such as Apple’s Siri and Amazon’s Alexa are well-suited for task-oriented interactions (“Call Jason”), but other interaction types are often beyond their capabilities. One notable example is playful requests: for example, people ask their CAs personal questions (“What’s your favorite color?”) or joke with them, sometimes at their expense (“Find Nemo”). Failing to recognize playfulness causes user dissatisfaction and abandonment, destroying the precious rapport with the CA. Today, playful CA behavior is achieved through manually curated replies to hard-coded questions. We take a step towards understanding and scaling playfulness by characterizing playful opportunities. To map the problem’s landscape, we draw inspiration from humor theories and analyze real user data. We present a taxonomy of playful requests and explore its prevalence in real Alexa traffic. We hope to inspire new avenues towards more human-like CAs.},
booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {423},
numpages = {7},
keywords = {Computational Humor, Computational Playfulness, Conversational Agents, Non-Task Requests, Virtual Assistants},
location = {New Orleans, LA, USA},
series = {CHI EA '22}
}

@inproceedings{Bischof2016,
author = {Bischof, Andreas and Lefeuvre, Kevin and Kurze, Albrecht and Storz, Michael and Totzauer, S\"{o}ren and Berger, Arne},
title = {Exploring the Playfulness of Tools for Co-Designing Smart Connected Devices: A Case Study with Blind and Visually Impaired Students},
year = {2016},
isbn = {9781450344586},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968120.2987728},
doi = {10.1145/2968120.2987728},
abstract = {In this paper we compare two tools for co-designing smart connected devices on their playfulness. First littleBits, a commercially available tool and Loaded Dice, a self developed tool, are introduced. Second frameworks for comparing the playfulness of such tools are briefly reviewed. We then report on co-design sessions we conducted with blind and visually impaired students and compare those sessions on the playfulness of the two tools. It is shown how tools that engage in playful exploration sustain successful co-design sessions, while tools with a lower level of playfulness constrain such co-design sessions to an extent where more functional and less imaginative design concepts are produced.},
booktitle = {Proc. Computer-Human Interaction in Play Companion},
pages = {93–99},
numpages = {7},
keywords = {playfulness, design fiction, co-design, accessibility},
location = {Austin, Texas, USA},
series = {CHI PLAY Companion '16}
}

@inproceedings{Carlton2019,
author = {Carlton, Jonathan and Brown, Andy and Jay, Caroline and Keane, John},
title = {Inferring User Engagement from Interaction Data},
year = {2019},
isbn = {9781450359719},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3313009},
doi = {10.1145/3290607.3313009},
abstract = {This paper presents preliminary results of a study designed to quantify users' engagement levels with interactive media content, through self-reported measures and interaction data. The broad hypothesis of the study is that interaction data can be used to predict the level of engagement felt by the user. The challenge addressed in this work is to explore the effectiveness of interaction data to act as a proxy for engagement levels and reveal what that data shows about engagement with media content. Preliminary results suggest several interesting insights about participants engagement and behaviour. Crucially, temporal statistics support the hypothesis that the participant making use of the controls in the interactive, video-based experience positively correlates with higher engagement.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {user engagement, media, interaction data, click},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{Cho2024,
author = {Cho, Haena and Lee, Yoonji and Lee, Woohun and Lee, Chang Hee},
title = {Thermo-Play: Exploring the Playful Qualities of Thermochromic Materials},
year = {2024},
isbn = {9798400704024},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623509.3633376},
doi = {10.1145/3623509.3633376},
abstract = {As interest in materiality has increased in human-computer interaction (HCI), a growing body of research has emerged on the use of smart materials to enhance interactivity. In particular, thermochromic materials, which change color in response to environmental temperature, have drawn attention. However, despite their dynamicity and interactivity, limited studies have dug into their playful potential. This paper presents Thermo-Play, a series of four projects—Picky Tamagotchi, Alphabet Blocks, Shake My Hand, and Gradual Display—that examine the potential of thermochromism in the sense of play and its implications for design and HCI. Evaluating the four projects, we identify the elements that make thermochromic materials playful, such as physicality, color-dynamicity, and gradualness, and the limitations of these materials. Through the findings of Thermo-Play, we expect to discover new opportunities for thermochromic materials in designing playful interactions.},
booktitle = {Proc. Tangible, Embedded, and Embodied Interaction},
articleno = {28},
numpages = {16},
keywords = {interaction design, materiality, playfulness, thermochromism},
location = {Cork, Ireland},
series = {TEI '24}
}

@inproceedings{Lucero2013,
author = {Lucero, Andr\'{e}s and Holopainen, Jussi and Ollila, Elina and Suomela, Riku and Karapanos, Evangelos},
title = {The playful experiences (PLEX) framework as a guide for expert evaluation},
year = {2013},
isbn = {9781450321921},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513506.2513530},
doi = {10.1145/2513506.2513530},
abstract = {The Playful Experiences (PLEX) framework is a categorization of playful experiences based on previous theoretical work on pleasurable experiences, game experiences, emotions, elements of play, and reasons why people play. While the framework has been successfully employed in design-related activities, its potential as an evaluation tool has not yet been studied. In this paper, we apply the PLEX framework in the evaluation of two game prototypes that explored novel physical interactions between mobile devices using Near-Field Communication, by means of three separate studies. Our results suggest that the PLEX framework provides anchor points for evaluators to reflect during heuristic evaluations. More broadly, the framework categories can be used as a checklist to assess different attributes of playfulness of a product or service.},
booktitle = {Proc. of the 6th International Conference on Designing Pleasurable Products and Interfaces},
pages = {221–230},
numpages = {10},
keywords = {evaluation methods, heuristic evaluation, playfulness},
location = {Newcastle upon Tyne, United Kingdom},
series = {DPPI '13}
}

@article{McLean2021,
title = {Alexa, do voice assistants influence consumer brand engagement? – Examining the role of AI powered voice assistants in influencing consumer brand engagement},
journal = {Journal of Business Research},
volume = {124},
pages = {312-328},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320307980},
author = {Graeme McLean and Kofi Osei-Frimpong and Jennifer Barhorst},
keywords = {Artificial Intelligence (AI), Voice assistants, Consumer brand engagement, Automated technology},
abstract = {Artificially Intelligent (AI) voice assistants (VAs) are continuing to grow in popularity amongst consumers. While in their infancy, several brands are now utilising VAs such as the Amazon Echo to deliver brand-related information and services. However, despite the increasing use of VAs, we have little understanding of what motivates consumers to use such devices for brand-related information. Focusing on the Amazon Echo in-home VA, and its associated Alexa Skills, this research uncovers the key drivers of consumer brand engagement through VAs. In study 1, through a set of in-depth exploratory interviews with 21 respondents, we established three factors as key drivers of why consumers use VAs to engage with brands: AI attributes, technology attributes, and situational attributes. Study 2 examines these specific drivers via a questionnaire with 724 respondents. The findings outline the VA as an actor in the engagement process and affirm the importance of the VA’s AI attributes of social presence, perceived intelligence, and social attraction in influencing consumer brand engagement. Additionally, technology attributes influence consumer brand engagement, along with the utilitarian benefits derived from interactions with brand-related information. Hedonic benefits do not influence consumer brand engagement via VA technology, while trust concerns play a negative role in brand engagement behaviour. Lastly, the results convey that consumer brand engagement via a VA influences brand usage intention, but in contrast to previous research, does not directly influence future purchase intention.}
}

@article{Liu2023,
author = {Liu, Jie and Marriott, Kim and Dwyer, Tim and Tack, Guido},
title = {Increasing User Trust in Optimisation through Feedback and Interaction},
year = {2023},
issue_date = {October 2022},
publisher = {ACM},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1073-0516},
url = {https://doi.org/10.1145/3503461},
doi = {10.1145/3503461},
abstract = {User trust plays a key role in determining whether autonomous computer applications are relied upon. It will play a key role in the acceptance of emerging AI applications such as optimisation. Two important factors known to affect trust are system transparency, i.e., how well the user understands how the system works, and system performance. However, in the case of optimisation, it is difficult for the end-user to understand the underlying algorithms or to judge the quality of the solution. Through two controlled user studies, we explore whether the user is better able to calibrate their trust in the system when: (a) They are provided feedback on the system operation in the form of visualisation of intermediate solutions and their quality; (b) They can interactively explore the solution space by modifying the solution returned by the system. We found that showing intermediate solutions can lead to over-trust, while interactive exploration leads to more accurately calibrated trust.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jan},
articleno = {42},
numpages = {34},
keywords = {HCI, interactive optimisation, human-in-the-loop optimisation, trust, feedback, vehicle routing}
}

@article{Doherty2018,
author = {Doherty, Kevin and Doherty, Gavin},
title = {Engagement in HCI: Conception, Theory and Measurement},
year = {2018},
issue_date = {September 2019},
publisher = {ACM},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3234149},
doi = {10.1145/3234149},
abstract = {Engaging users is a priority for designers of products and services of every kind. The need to understand users’ experiences has motivated a focus on user engagement across computer science. However, to date, there has been limited review of how Human--Computer Interaction and computer science research interprets and employs the concept. Questions persist concerning its conception, abstraction, and measurement. This article presents a systematic review of engagement spanning a corpus of 351 articles and 102 definitions. We map the current state of engagement research, including the diverse interpretation, theory, and measurement of the concept. We describe the ecology of engagement and strategies for the design of engaging experiences, discuss the value of the concept and its relationship to other terms, and present a set of guidelines and opportunities for future research.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {99},
numpages = {39},
keywords = {Engagement, HCI, design, measurement, theory, user experience}
}

@inproceedings{Purington2017,
author = {Purington, Amanda and Taft, Jessie G. and Sannon, Shruti and Bazarova, Natalya N. and Taylor, Samuel Hardman},
title = {"Alexa is my new BFF": Social Roles, User Satisfaction, and Personification of the Amazon Echo},
year = {2017},
isbn = {9781450346566},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3027063.3053246},
doi = {10.1145/3027063.3053246},
abstract = {Amazon's Echo and its conversational agent Alexa open exciting opportunities for understanding how people perceive and interact with virtual agents. Drawing from user reviews of the Echo posted to Amazon.com, this case study explores the degree to which user reviews indicate personification of the device, sociability level of interactions, factors linked with personification, and influences on user satisfaction. Results indicate marked variance in how people refer to the device, with over half using the personified name Alexa but most referencing the device with object pronouns. Degree of device personification is linked with sociability of interactions: greater personification co-occurs with more social interactions with the Echo. Reviewers mentioning multiple member households are more likely to personify the device than reviewers mentioning living alone. Even after controlling for technical issues, personification predicts user satisfaction with the Echo.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {2853–2859},
numpages = {7},
keywords = {amazon echo, conversational agent, personification, social robots},
location = {Denver, Colorado, USA},
series = {CHI EA '17}
}

@article{Obrien2018,
title = {A practical approach to measuring user engagement with the refined user engagement scale (UES) and new UES short form},
journal = {International Journal of Human-Computer Studies},
volume = {112},
pages = {28-39},
year = {2018},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918300041},
author = {Heather L. O’Brien and Paul Cairns and Mark Hall},
keywords = {User engagement, Questionnaires, Measurement, Reliability, Validity},
abstract = {User engagement (UE) and its measurement have been of increasing interest in human-computer interaction (HCI). The User Engagement Scale (UES) is one tool developed to measure UE, and has been used in a variety of digital domains. The original UES consisted of 31-items and purported to measure six dimensions of engagement: aesthetic appeal, focused attention, novelty, perceived usability, felt involvement, and endurability. A recent synthesis of the literature questioned the original six-factors. Further, the ways in which the UES has been implemented in studies suggests there may be a need for a briefer version of the questionnaire and more effective documentation to guide its use and analysis. This research investigated and verified a four-factor structure of the UES and proposed a Short Form (SF). We employed contemporary statistical tools that were unavailable during the UES’ development to re-analyze the original data, consisting of 427 and 779 valid responses across two studies, and examined new data (N=344) gathered as part of a three-year digital library project. In this paper we detail our analyses, present a revised long and short form (SF) version of the UES, and offer guidance for researchers interested in adopting the UES and UES-SF in their own studies.}
}

@inproceedings{Wu2017, author = {Wu, Shaomei and Wieland, Jeffrey and Farivar, Omid and Schiller, Julie}, title = {Automatic Alt-Text: Computer-Generated Image Descriptions for Blind Users on a Social Network Service}, year = {2017}, isbn = {9781450343350}, publisher = {ACM}, address = {New York, NY, USA}, url = {https://doi-org.ezproxy.lib.monash.edu.au/10.1145/2998181.2998364}, doi = {10.1145/2998181.2998364}, abstract = {We designed and deployed automatic alt-text (AAT), a system that applies computer vision technology to identify faces, objects, and themes from photos to generate photo alt-text for screen reader users on Facebook. We designed our system through iterations of prototyping and in-lab user studies. Our lab test participants had a positive reaction to our system and an enhanced experience with Facebook photos. We also evaluated our system through a two-week field study as part of the Facebook iOS app for 9K VoiceOver users. We randomly assigned them into control and test groups and collected two weeks of activity data and their survey feedback. The test group reported that photos on Facebook were easier to interpret and more engaging, and found Facebook more useful in general. Our system demonstrates that artificial intelligence can be used to enhance the experience for visually impaired users on social networking sites (SNSs), while also revealing the challenges with designing automated assistive technology in a SNS context.}, booktitle = {Proc. ACM Computer Supported Cooperative Work and Social Computing}, pages = {1180–1192}, numpages = {13}, keywords = {social networking sites, user experience, accessibility, artificial intelligence, facebook.}, location = {Portland, Oregon, USA}, series = {CSCW '17} }

@inproceedings{Davis2020,
author = {Davis, Josh Urban and Wu, Te-Yen and Shi, Bo and Lu, Hanyi and Panotopoulou, Athina and Whiting, Emily and Yang, Xing-Dong},
title = {TangibleCircuits: An Interactive 3D Printed Circuit Education Tool for People with Visual Impairments},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376513},
doi = {10.1145/3313831.3376513},
abstract = {We present a novel haptic and audio feedback device that allows blind and visually impaired (BVI) users to understand circuit diagrams. TangibleCircuits allows users to interact with a 3D printed tangible model of a circuit which provides audio tutorial directions while being touched. Our system comprises an automated parsing algorithm which extracts 3D printable models as well as an audio interfaces from a Fritzing diagram. To better understand the requirements of designing technology to assist BVI users in learning hardware computing, we conducted a series of formative inquiries into the accessibility limitations of current circuit tutorial technologies. In addition, we derived insights and design considerations gleaned from conducting a formal comparative user study to understand the effectiveness of TangibleCircuits as a tutorial system. We found that BVI users were better able to understand the geometric, spatial and structural circuit information using TangibleCircuits, as well as enjoyed learning with our tool.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {circuit prototyping, tangible user interfaces, education tools, universal design, accessibility},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Mayer2020,
author = {Mayer, Sven and Reinhardt, Jens and Schweigert, Robin and Jelke, Brighten and Schwind, Valentin and Wolf, Katrin and Henze, Niels},
title = {Improving Humans' Ability to Interpret Deictic Gestures in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376340},
doi = {10.1145/3313831.3376340},
abstract = {Collaborative Virtual Environments (CVEs) offer unique opportunities for human communication. Humans can interact with each other over a distance in any environment and visual embodiment they want. Although deictic gestures are especially important as they can guide other humans' attention, humans make systematic errors when using and interpreting them. Recent work suggests that the interpretation of vertical deictic gestures can be significantly improved by warping the pointing arm. In this paper, we extend previous work by showing that models enable to also improve the interpretation of deictic gestures at targets all around the user. Through a study with 28 participants in a CVE, we analyzed the errors users make when interpreting deictic gestures. We derived a model that rotates the arm of a pointing user's avatar to improve the observing users' accuracy. A second study with 24 participants shows that we can improve observers' accuracy by 22.9\%. As our approach is not noticeable for users, it improves their accuracy without requiring them to learn a new interaction technique or distracting from the experience.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {deictic, virtual reality, correction model, ray tracing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Lee2021,
author = {Lee, Jaewook and Rodriguez, Sebastian S. and Natarrajan, Raahul and Chen, Jacqueline and Deep, Harsh and Kirlik, Alex},
title = {What’s This? A Voice and Touch Multimodal Approach for Ambiguity Resolution in Voice Assistants},
year = {2021},
isbn = {9781450384810},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479902},
doi = {10.1145/3462244.3479902},
abstract = {Human speech often contains ambiguity stemming from the use of demonstrative pronouns (DPs), such as “this” and “these.” While we can typically decipher which objects of interest DPs are referring to based on context, modern day voice assistants (VAs – such as Google Assistant and Siri) are yet unable to process queries containing such ambiguity. For instance, to humans, a question such as “how much is this?” can be clarified through visual reference (e.g., a buyer gestures to the seller the object they would like to purchase). To bridge this gap between human and machine cognition, we built and examined a touch + voice multimodal VA prototype that enables users to select key spatial information to embed as context and query the VA. The prototype converts results of mobile, real-time object recognition and optical character recognition models into augmented reality buttons that represent features. Users can interact with and modify the selected features through a word grid. We conducted a study to investigate: 1) how touch performs as an additional modality to resolve ambiguity in queries, 2) how users use DPs when interacting with VAs, and 3) how users perceive a VA that can understand DPs. From this procedure we found that as the query becomes more complex, users prefer the multimodal VA over the standard VA without experiencing elevated cognitive load. Additionally, even though it took some time getting used to, many participants eventually became comfortable with using DPs to interact with the multimodal VA and appreciated the improved human-likeness of human-VA conversations.},
booktitle = {Proc. of the 2021 International Conference on Multimodal Interaction},
pages = {512–520},
numpages = {9},
keywords = {intelligent voice assistant, mixed-methods study, ambiguous query, mobile interaction, unparseable query, augmented reality, user experience, demonstrative pronouns},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{Thevin2019,
author = {Thevin, Lauren and Jouffrais, Christophe and Rodier, Nicolas and Palard, Nicolas and Hachet, Martin and Brock, Anke M.},
title = {Creating Accessible Interactive Audio-Tactile Drawings Using Spatial Augmented Reality},
year = {2019},
isbn = {9781450368919},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343055.3359711},
doi = {10.1145/3343055.3359711},
abstract = {Interactive tactile graphics have shown a true potential for people with visual impairments, for instance for acquiring spatial knowledge. Until today, however, they are not well adopted in real-life settings (e.g. special education schools). One obstacle consists in the creation of these media, which requires specific skills, such as the use of vector-graphic software for drawing and inserting interactive zones, which is challenging for stakeholders (social workers, teachers, families of people with visual impairments, etc.). We explored how a Spatial Augmented Reality approach can enhance the creation of interactive tactile graphics by sighted users. We developed the system using a participatory design method. A user study showed that the augmented reality device allowed stakeholders (N=28) to create interactive tactile graphics more efficiently than with a regular vector-drawing software (baseline), independently of their technical background.},
booktitle = {Proc. of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
pages = {17–28},
numpages = {12},
keywords = {audio-tactile drawings, content creation, accessible graphics, visual impairment, accessible maps, accessibility, spatial augmented reality},
location = {Daejeon, Republic of Korea},
series = {ISS '19}
}

@inproceedings{Spillane2019,
author = {Spillane, Brendan and Gilmartin, Emer and Saam, Christian and Wade, Vincent},
title = {Issues Relating to Trust in Care Agents for the Elderly},
year = {2019},
isbn = {9781450371872},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342775.3342808},
doi = {10.1145/3342775.3342808},
abstract = {There is increasing academic interest in and commercial development of care agents to assist with the care of the elderly in the home. This paper defines some of the under-explored questions and issues relating to trust. It raises specific questions to instigate debate and recommends directions for future research in the domain.},
booktitle = {Proc. of the 1st International Conference on Conversational User Interfaces},
articleno = {20},
numpages = {3},
keywords = {home care agents, trust, elderly care},
location = {Dublin, Ireland},
series = {CUI '19}
}

@inproceedings{Carros2020,
author = {Carros, Felix and Meurer, Johanna and L\"{o}ffler, Diana and Unbehaun, David and Matthies, Sarah and Koch, Inga and Wieching, Rainer and Randall, Dave and Hassenzahl, Marc and Wulf, Volker},
title = {Exploring Human-Robot Interaction with the Elderly: Results from a Ten-Week Case Study in a Care Home},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376402},
doi = {10.1145/3313831.3376402},
abstract = {Ageing societies and the associated pressure on the care systems are major drivers for new developments in socially assistive robotics. To understand better the real-world potential of robot-based assistance, we undertook a 10-week case study in a care home involving groups of residents, caregivers and managers as stakeholders. We identified both, enablers and barriers to the potential implementation of robot systems. The study employed the robot platform Pepper, which was deployed with a view to understanding better multi-domain interventions with a robot supporting physical activation, cognitive training and social facilitation. We employed the robot in a group setting in a care facility over the course of 10 weeks and 20 sessions, observing how stakeholders, including residents and caregivers, appropriated, adapted to, and perceived the robot. We also conducted interviews with 11 residents and caregivers. Our results indicate that the residents were positively engaged in the training sessions that were moderated by the robot. The study revealed that such humanoid robots can work in a care home but that there is a moderating person needed, that is in control of the robot.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {social robots, elderly care, user studies, ethics},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Luria2019,
author = {Luria, Michal and Reig, Samantha and Tan, Xiang Zhi and Steinfeld, Aaron and Forlizzi, Jodi and Zimmerman, John},
title = {Re-Embodiment and Co-Embodiment: Exploration of Social Presence for Robots and Conversational Agents},
year = {2019},
isbn = {9781450358507},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322276.3322340},
doi = {10.1145/3322276.3322340},
abstract = {Interactions with multiple conversational agents and social robots are becoming increasingly common. This raises new design challenges: Should agents and robots be modeled after humans, presenting their entity (i.e., social presence) as bound to a single body, or should they take advantage of non-human capabilities, such as moving their social presence from body to body across service touchpoints and contexts? We conducted a User Enactments study in which participants interacted with agents that had one social presence per body, that could re-embody (move their social presence from body to body), and that could co-embody (move their social presence into a body that already contains another). Reactions showed that participants felt comfortable with re-embodying agents, who created more seamless and efficient experiences. Yet situations that required expertise or concentration raised concerns about non-human behaviors. We report on our insights regarding collaboration and coordination with several agents in multi-step interactions.},
booktitle = {Proc. ACM Designing Interactive Systems Conference},
pages = {633–644},
numpages = {12},
keywords = {conversational agents, interaction design, re-embodiment, embodied agents, social robots, user enactments, co-embodiment},
location = {San Diego, CA, USA},
series = {DIS '19}
}

@Article{Salah2023,
author={Salah, Mohammed
and Alhalbusi, Hussam
and Ismail, Maria Mohd
and Abdelfattah, Fadi},
title={Chatting with ChatGPT: decoding the mind of Chatbot users and unveiling the intricate connections between user perception, trust and stereotype perception on self-esteem and psychological well-being},
journal={Current Psychology},
year={2023},
month={Jul},
day={20},
abstract={Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment, with ChatGPT, a language model developed by OpenAI, being among its prominent applications. However, knowledge regarding the psychological impact of interacting with ChatGPT is limited. This study investigated the relationships between trust in ChatGPT; ChatGPT's user perceptions; perceived stereotyping by ChatGPT; and two psychological outcomes, namely, psychological well-being and self-esteem. This study hypothesized that the former three variables exhibit a positive direct relationship with self-esteem. Additionally, the study proposed that job anxiety moderates the associations among trust in ChatGPT, user perceptions of ChatGPT, and psychological well-being. Using a survey design, data were collected from 732 participants and analyzed using SEM and SmartPLS analysis. Notably, perceived stereotyping by ChatGPT significantly predicted self-esteem, while user perceptions of ChatGPT and trust in ChatGPT exhibited a positive direct relationship with self-esteem. Additionally, job anxiety moderated the relationship between ChatGPT's user perceptions and psychological well-being. These results provide important insights into the psychological effects of interacting with AI technology and highlight job anxiety's role in moderating these effects. This study's findings have implications for developing and using AI technology in various fields, including mental health and human-robot interactions.},
issn={1936-4733},
doi={10.1007/s12144-023-04989-0},
url={https://doi.org/10.1007/s12144-023-04989-0}
}

@Article{Greenland2016,
author={Greenland, Sander
and Senn, Stephen J.
and Rothman, Kenneth J.
and Carlin, John B.
and Poole, Charles
and Goodman, Steven N.
and Altman, Douglas G.},
title={Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations},
journal={European Journal of Epidemiology},
year={2016},
month={Apr},
day={01},
volume={31},
number={4},
}

@inproceedings{Degachi2023,
author = {Degachi, Chadha and Tielman, Myrthe Lotte and Al Owayyed, Mohammed},
title = {Trust and Perceived Control in Burnout Support Chatbots},
year = {2023},
isbn = {9781450394222},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585780},
doi = {10.1145/3544549.3585780},
abstract = {Increased levels of user control in learning systems is commonly cited as good AI development practice. However, the evidence as to the effect of perceived control over trust in these systems is mixed. This study investigated the relationship between different trust dimensions and perceived control in postgraduate student burnout support chatbots, and modelled the moderating factors therein. We present an in-between subject controlled experiment using simulated therapy-goal learning to study the effect of perceived control (as manipulated by feedback incorporation) on perceived agent benevolence, competence, and trust. Our results showed that perceived control was moderately correlated with benevolence (r = 0.448, BF10 = 7.150), and weakly correlated with competence and trust.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
articleno = {295},
numpages = {10},
keywords = {chatbots, human-in-the-loop, percieved control, trust modelling},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{Guo2022,
author = {Guo, Lijie and Daly, Elizabeth M. and Alkan, Oznur and Mattetti, Massimiliano and Cornec, Owen and Knijnenburg, Bart},
title = {Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules},
year = {2022},
isbn = {9781450391443},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511111},
doi = {10.1145/3490099.3511111},
abstract = {Machine learning technologies are increasingly being applied in many different domains in the real world. As autonomous machines and black-box algorithms begin making decisions previously entrusted to humans, great academic and public interest has been spurred to provide explanations that allow users to understand the decision-making process of the machine learning model. Besides explanations, Interactive Machine Learning (IML) seeks to leverage user feedback to iterate on an ML solution to correct errors and align decisions with those of the users. Despite the rise in explainable AI (XAI) and Interactive Machine Learning (IML) research, the links between interactivity, explanations, and trust have not been comprehensively studied in the machine learning literature. Thus, in this study, we develop and evaluate an explanation-driven interactive machine learning (XIML) system with the Tic-Tac-Toe game as a use case to understand how a XIML mechanism improves users’ satisfaction with the machine learning system. We explore different modalities to support user feedback through visual or rules-based corrections. Our online user study (n = 199) supports the hypothesis that allowing interactivity within this XIML system causes participants to be more satisfied with the system, while visual explanations play a less prominent (and somewhat unexpected) role. Finally, we leverage a user-centric evaluation framework to create a comprehensive structural model to clarify how subjective system aspects, which represent participants’ perceptions of the implemented interaction and visualization mechanisms, mediate the influence of these mechanisms on the system’s user experience.},
booktitle = {Proc. International Conference on Intelligent User Interfaces},
pages = {537–548},
numpages = {12},
keywords = {Tic-Tac-Toe game, XIML, XML, machine learning, user experience, user study, user-centric evaluation framework},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{Qiu2016,
author = {Qiu, Shi and Anas, Siti Aisyah and Osawa, Hirotaka and Rauterberg, Matthias and Hu, Jun},
title = {E-Gaze Glasses: Simulating Natural Gazes for Blind People},
year = {2016},
isbn = {9781450335829},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839462.2856518},
doi = {10.1145/2839462.2856518},
abstract = {Gaze and eye contact are frequently in social occasions used among sighted people. Gaze is considered as a predictor of attention and engagement between interlocutors in conversations. However, gaze signals from the sighted are not accessible for the blind person in face-to-face communication. In this paper, we present functional work-in-progress prototype, E-Gaze glasses, an assistive device based on an eye tracking system. E-Gaze simulates natural gaze for blind people, especially establishing the "eye contact" between blind and sighted people to enhance their engagement in face-to-face conversations. The gaze behavior is designed based on a turn-taking model, which interprets the corresponding relationship between the conclusive gaze behavior and the interlocutors' conversation flow.},
booktitle = {Proc. Tangible, Embedded, and Embodied Interaction},
pages = {563–569},
numpages = {7},
keywords = {Social interaction, eye tracking, visual impairments},
location = {Eindhoven, Netherlands},
series = {TEI '16}
}

@inbook{Biocca1999,
title = "Chapter 6 The Cyborg's dilemma. Progressive embodiment in virtual environments",
author = "Frank Biocca",
year = "1999",
doi = "10.1016/S0923-8433(99)80011-2",
language = "English (US)",
series = "Human Factors in Information Technology",
publisher = "Elsevier",
number = "C",
pages = "113--144",
booktitle = "Human Factors in Information Technology",
address = "Netherlands",
edition = "C",
}

@inproceedings{Karim2023,
author = {Karim, Saman and Kang, Jin and Girouard, Audrey},
title = {Exploring Rulebook Accessibility and Companionship in Board Games via Voiced-based Conversational Agent Alexa},
year = {2023},
isbn = {9781450398930},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3595970},
doi = {10.1145/3563657.3595970},
abstract = {The inaccessibility of rulebooks hinders the rule learning experience of board game players who are blind or have low vision (BLV). We conducted two qualitative studies to explore the design of conversational agents (CAs) that can support BLV players’ learning and provide companionship. In Study 1, we recruited 14 BLV participants and identified their rule learning challenges, with emphasis on cognitive load, and co-designed functional and social characteristics in Amazon Alexa that can support rule learning and offer companionship. We then developed a new Alexa skill and had 9 BLV participants evaluate our skill against the Alexa skill for the game Ticket to Ride (Study 2). We generated four design principles for CAs to support board game rule learning for BLV people: conciseness, pausing capacity, customization, and companionship.},
booktitle = {Proc. ACM Designing Interactive Systems Conference},
pages = {2221–2232},
numpages = {12},
keywords = {accessibility, board games, companionship, conversational agents},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{Collins2023,
author = {Collins, Jazmin and Jung, Crescentia and Jang, Yeonju and Montour, Danielle and Won, Andrea Stevenson and Azenkot, Shiri},
title = {“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People},
year = {2023},
isbn = {9798400702204},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3608386},
doi = {10.1145/3597638.3608386},
abstract = {As social VR applications grow in popularity, blind and low vision users encounter continued accessibility barriers. Yet social VR, which enables multiple people to engage in the same virtual space, presents a unique opportunity to allow other people to support a user’s access needs. To explore this opportunity, we designed a framework based on physical sighted guidance that enables a guide to support a blind or low vision user with navigation and visual interpretation. A user can virtually hold on to their guide and move with them, while the guide can describe the environment. We studied the use of our framework with 16 blind and low vision participants and found that they had a wide range of preferences. For example, we found that participants wanted to use their guide to support social interactions and establish a human connection with a human-appearing guide. We also highlight opportunities for novel guidance abilities in VR, such as dynamically altering an inaccessible environment. Through this work, we open a novel design space for a versatile approach for making VR fully accessible.},
booktitle = {Proc. ACM SIGACCESS Conference on Computers \& Accessibility},
articleno = {38},
numpages = {14},
keywords = {blind and low vision, sighted guide, social virtual reality},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@Article{Qiu2020,
author={Qiu, Shi
and An, Pengcheng
and Hu, Jun
and Han, Ting
and Rauterberg, Matthias},
title={Understanding visually impaired people's experiences of social signal perception in face-to-face communication},
journal={Universal Access in the Information Society},
year={2020},
month={Nov},
day={01},
volume={19},
number={4},
pages={873-890},
abstract={Social signals (e.g., facial expression, gestures) are important in social interactions. Most of them are visual cues, which are hardly accessible for visually impaired people, causing difficulties in their daily living. In human--computer interaction (HCI), assistive systems for social interactions are getting increasing attention due to related technological advancements. Yet, there is still lack of a comprehensive and vivid understanding of visually impaired people's social signal perception to broadly identify their needs in face-to-face communication. To fill this gap, we conducted in-depth interviews to study the lived experiences of 20 visually impaired participants. We analyzed a rich set of qualitative empirical data based on a comprehensive taxonomy of social signals, using a standard qualitative content analysis method. Our results revealed a set of vivid examples and an overview of visually impaired people's lived experiences regarding social signals, including both their capabilities and limitations. As reported, the participants perceived social signals through their compensatory modalities such as hearing, touch, smell, or obstacle sense. However, their perception of social signals is generally with low resolution and limited by certain environmental factors (e.g., crowdedness, or noise level of the surrounding). Interestingly, sight was still importantly relied on by low-vision participants in social signal perception (e.g., rough postures and gestures). Besides, the participants experienced difficulties in sensing others' subtle emotional states which are often revealed by nuanced behaviors (e.g., a smile). Based on rich empirical findings, we propose a set of design implications to inform future-related HCI works aimed at supporting visually impaired users' social signal perception.},
issn={1615-5297},
doi={10.1007/s10209-019-00698-3},
url={https://doi.org/10.1007/s10209-019-00698-3}
}




@InProceedings{McDaniel2018,
author="McDaniel, Troy
and Devkota, Samjhana
and Tadayon, Ramin
and Duarte, Bryan
and Fakhri, Bijan
and Panchanathan, Sethuraman",
editor="Basu, Anup
and Berretti, Stefano",
title="Tactile Facial Action Units Toward Enriching Social Interactions for Individuals Who Are Blind",
booktitle="Smart Multimedia",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="3--14",
abstract="Social interactions mediate our communication with others, enable development and maintenance of personal and professional relationships, and contribute greatly to our health. While both verbal cues (i.e., speech) and non-verbal cues (e.g., facial expressions, hand gestures, and body language) are exchanged during social interactions, the latter encompasses more information ({\textasciitilde}65{\%}). Given their inherent visual nature, non-verbal cues are largely inaccessible to individuals who are blind, putting this population at a social disadvantage compared to their sighted peers. For individuals who are blind, embarrassing social situations are not uncommon due to miscommunication, which can lead to social avoidance and isolation. In this paper, we propose a mapping between visual facial expressions, represented as facial action units, which may be extracted using computer vision algorithms, to haptic (vibrotactile) representations, toward discreet and real-time perception of facial expressions during social interactions by individuals who are blind.",
isbn="978-3-030-04375-9"
}



@inproceedings{Iravantchi2020,
author = {Iravantchi, Yasha and Goel, Mayank and Harrison, Chris},
title = {Digital Ventriloquism: Giving Voice to Everyday Objects},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376503},
doi = {10.1145/3313831.3376503},
abstract = {Smart speakers with voice agents are becoming increasingly common. However, the agent's voice always emanates from the device, even when that information is contextually and spatially relevant elsewhere. Digital Ventriloquism allows smart speakers to render sound onto everyday objects, such that it appears they are speaking and are interactive. This can be achieved without any modification of objects or the environment. For this, we used a highly directional pan-tilt ultrasonic array. By modulating a 40 kHz ultrasonic signal, we can emit sound that is inaudible "in flight" and demodulates to audible frequencies when impacting a surface through acoustic parametric interaction. This makes it appear as though the sound originates from an object and not the speaker. We ran a study in which we projected speech onto five objects in three environments, and found that participants were able to correctly identify the source object 92\% of the time and correctly repeat the spoken message 100\% of the time, demonstrating our digital ventriloquy is both directional and intelligible.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {interaction, iot, smart speakers, ultrasound, vr/ar},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@ARTICLE{Bruns2019,
  
AUTHOR={Bruns, Patrick},   
	 
TITLE={The Ventriloquist Illusion as a Tool to Study Multisensory Processing: An Update},      
	
JOURNAL={Frontiers in Integrative Neuroscience},      
	
VOLUME={13},           
	
YEAR={2019},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnint.2019.00051},       
	
DOI={10.3389/fnint.2019.00051},      
	
ISSN={1662-5145},   
   
ABSTRACT={Ventriloquism, the illusion that a voice appears to come from the moving mouth of a puppet rather than from the actual speaker, is one of the classic examples of multisensory processing. In the laboratory, this illusion can be reliably induced by presenting simple meaningless audiovisual stimuli with a spatial discrepancy between the auditory and visual components. Typically, the perceived location of the sound source is biased toward the location of the visual stimulus (the ventriloquism effect). The strength of the visual bias reflects the relative reliability of the visual and auditory inputs as well as prior expectations that the two stimuli originated from the same source. In addition to the ventriloquist illusion, exposure to spatially discrepant audiovisual stimuli results in a subsequent recalibration of unisensory auditory localization (the ventriloquism aftereffect). In the past years, the ventriloquism effect and aftereffect have seen a resurgence as an experimental tool to elucidate basic mechanisms of multisensory integration and learning. For example, recent studies have: (a) revealed top-down influences from the reward and motor systems on cross-modal binding; (b) dissociated recalibration processes operating at different time scales; and (c) identified brain networks involved in the neuronal computations underlying multisensory integration and learning. This mini review article provides a brief overview of established experimental paradigms to measure the ventriloquism effect and aftereffect before summarizing these pathbreaking new advancements. Finally, it is pointed out how the ventriloquism effect and aftereffect could be utilized to address some of the current open questions in the field of multisensory research.}
}

@inproceedings{Shamekhi2018,
author = {Shamekhi, Ameneh and Liao, Q. Vera and Wang, Dakuo and Bellamy, Rachel K. E. and Erickson, Thomas},
title = {Face Value? Exploring the Effects of Embodiment for a Group Facilitation Agent},
year = {2018},
isbn = {9781450356206},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173965},
doi = {10.1145/3173574.3173965},
abstract = {We are interested in increasing the ability of groups to collaborate efficiently by leveraging new advances in AI and Conversational Agent (CA) technology. Given the longstanding debate on the necessity of embodiment for CAs, bringing them to groups requires answering the questions of whether and how providing a CA with a face affects its interaction with the humans in a group. We explored these questions by comparing group decision-making sessions facilitated by an embodied agent, versus a voice-only agent. Results of an experiment with 20 user groups revealed that while the embodiment improved various aspects of group's social perception of the agent (e.g., rapport, trust, intelligence, and power), its impact on the group-decision process and outcome was nuanced. Drawing on both quantitative and qualitative findings, we discuss the pros and cons of embodiment, argue that the value of having a face depends on the types of assistance the agent provides, and lay out directions for future research.},
booktitle = {Proc. ACM CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {conversational agents, meeting facilitation, group decision making, agent's embodiment},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@phdthesis{Bruseaard2018,
   author = {Brusegaard, Callie M.},
   title = {Identifying Barriers to Inclusive Education for Children Who Are Blind and Visually Impaired in the Federated States of Micronesia},
   university = {University of Massachusetts Boston},
   url = {https://eric.ed.gov/?id=ED585819},
   year = {2018},
   type = {Thesis}, 
   abstract = {Many developing countries continue to face problems implementing inclusive education. The implementation of inclusive education is an especially ubiquitous problem in the Federated States of Micronesia (FSM) with children who are blind and visually impaired. This study analyzed the barriers to full inclusion in the educational system for children who are blind and visually impaired in the FSM as perceived by the following actors: students who are blind and visually impaired, teachers, caregivers, principals, and system administrators. This study employed a transdisciplinary qualitative research process founded in grounded theory. The study featured surveys with special education administrators and teachers of students who are blind and visually impaired to provide a baseline of information to inform further research. Then, ethnographic interviews and observations were completed in the FSM with students who are blind and visually impaired, teachers, caregivers, principals, and system administrators. Data collection focused on the actors' experiences of access and participation to education for students who are blind or visually impaired. Comparative analysis identified the perceived barriers to inclusive education for students who are blind or visually impaired in the states of Yap, Chuuk, Pohnpei, and Kosrae. Results of the study show that barriers to inclusive education persist in the areas of economic factors, educational system, and social mindset. The main economic factors that arose during the study were financial accountability, transportation barriers, and limited access to materials. The barriers within the educational system were a teachers' ability to implement appropriate services, the child find process and procedures, and a need for more qualified teachers. The social mindset was defined by a capability mindset and attitudinal mindset. This study pinpointed unique barriers to access in this setting that will inform improved policies and practices in instruction, assessment, parent-school collaboration, and teacher professional development. Additionally, recommendations are made that target educational processes and procedures, due process for special education, funding targets, and awareness.}
}

@article{Elmqvist2023,
   author = {Elmqvist, Niklas},
   title = {Visualization for the Blind},
   journal = {Interactions},
   volume = {January-February},
   pages = {52},
   year = {2023},
   type = {Journal Article},
   abstract = {Understanding data has become critical to everyday life. You need data to decide which products to buy, which health choices to make, and whom to vote for. People spend hours reading product reviews, browsing travel websites, and managing large amounts of data to make decisions in their personal lives. But what if you personally didn't have access to this data?}
}

@article{Phutane2021,
   author = {Phutane, Mahika and Wright, Julie and Castro, Brenda Veronica and Shi, Lei and Stern, Simone R. and Lawson, Holly M. and Azenkot, Shiri},
   title = {Tactile Materials in Practice: Understanding the Experiences of Teachers of the Visually Impaired },
   journal = {ACM Transactions on Accessible Computing (TACCESS)},
   DOI = {10.1145/3508364},
   year = {2021},
   type = {Journal Article},
   abstract={Teachers of the visually impaired (TVIs) regularly present tactile materials (tactile graphics, 3D models, and real objects) to students with vision impairments. Researchers have been increasingly interested in designing tools to support the use of tactile materials, but we still lack an in-depth understanding of how tactile materials are created and used in practice today. To address this gap, we conducted interviews with 21 TVIs and a 3-week diary study with eight of them. We found that tactile materials were regularly used for academic as well as non-academic concepts like tactile literacy, motor ability, and spatial awareness. Real objects and 3D models served as ۢstepping stonesۣ to tactile graphics and our participants preferred to teach with 3D models, despite finding them difficult to create, obtain, and modify. Use of certain materials also carried social implications; participants selected materials that fostered student independence and allow classroom inclusion. We contribute design considerations, encouraging future work on tactile materials to enable student and TVI co-creation, facilitate rapid prototyping, and promote movement and spatial awareness. To support future research in this area, our paper provides a fundamental understanding of current practices. We bridge these practices to established pedagogical approaches and highlight opportunities for growth regarding this important genre of educational materials. }
}

@article{Kim2021,
   author = {Kim, N. W.  and Joyner, S. C.  and Riegelhuth, A. and Kim, Y.},
   title = {Accessible Visualization: Design Space, Opportunities, and Challenges},
   journal = {Computer Graphics Forum},
   volume = {40},
   number = {3},
   pages = {173-188},
   DOI = {10.1111/cgf.14298},
   year = {2021},
   type = {Journal Article},
   abstract = {Visualizations are now widely used across disciplines to understand and communicate data. The benefit of visualizations lies in leveraging our natural visual perception. However, the sole dependency on vision can produce unintended discrimination against people with visual impairments. While the visualization field has seen enormous growth in recent years, supporting people with disabilities is much less explored. In this work, we examine approaches to support this marginalized user group, focusing on visual disabilities. We collected and analyzed papers published for the last 20 years on visualization accessibility. We mapped a design space for accessible visualization that includes seven dimensions: user group, literacy task, chart type, interaction, information granularity, sensory modality, assistive technology. We described the current knowledge gap in light of the latest advances in visualization and presented a preliminary accessibility model by synthesizing findings from existing research. Finally, we reflected on the dimensions and discussed opportunities and challenges for future research.}
}

@article{Blades1999,
author = { Mark   Blades  and  Simon   Ungar  and  Christopher   Spencer },
title = {Map Use by Adults with Visual Impairments},
journal = {The Professional Geographer},
volume = {51},
number = {4},
pages = {539-553},
year  = {1999},
publisher = {Routledge},
doi = {10.1111/0033-0124.00191},
URL = {https://doi.org/10.1111/0033-0124.00191},
}

@article{Aldrich2001,
author = {Frances K. Aldrich and Linda Sheppard},
title ={Tactile graphics in school education: perspectives from pupils},
journal = {British Journal of Visual Impairment},
volume = {19},
number = {2},
pages = {69-73},
year = {2001},
doi = {10.1177/026461960101900204},
URL = {https://doi.org/10.1177/026461960101900204},
}

@article{Rosenblum2015,
author = {L. Penny Rosenblum and Tina S. Herzberg},
title ={Braille and Tactile Graphics: Youths with Visual Impairments Share Their Experiences},
journal = {Journal of Visual Impairment \& Blindness},
volume = {109},
number = {3},
pages = {173-184},
year = {2015},
doi = {10.1177/0145482X1510900302},
URL = {https://doi.org/10.1177/0145482X1510900302}
}

@article{karaduman2022beyond,
  title={Beyond “do not touch”: the experience of a three-dimensional printed artifacts museum as an alternative to traditional museums for visitors who are blind and partially sighted},
  author={Karaduman, H{\i}d{\i}r and Alan, {\"U}mran and Yi{\u{g}}it, E {\"O}zlem},
  journal={Universal Access in the Information Society},
  pages={1--14},
  year={2022},
  publisher={Springer}
}



