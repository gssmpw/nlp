\section{Related Work}
This work builds on research on: accessible graphics and interactive 3D-printed models for BLV users; conversational agents; and embodied agents.

\subsection{Accessible Graphics}
BLV people face challenges accessing graphical information, which impacts education opportunities~\cite{butler2017understanding}, makes independent travel difficult~\cite{Sheffield2016}, and causes disengagement with culture and the creative arts~\cite{Bartlett2019}. These barriers can lead to reductions in confidence and overall quality of life~\cite{Keeffe2005}.

Graphical information can be made available in formats that improve non-visual access. Traditionally, accessible graphics -- known as raised line drawings or \textbf{tactile graphics} -- have been used to assist BLV people in accessing information. Tactile graphics are frequently used to facilitate classroom learning~\cite{Aldrich2001,Rosenblum2015} and orientation and mobility (O\&M) training~\cite{Blades1999,Rowell2005}. Work has been conducted on the development of \textbf{interactive tactile graphics}, including the NOMAD~\cite{NOMAD}, IVEO~\cite{IVEO}, and Talking Tactile Tablet~\cite{Miele2006,TTT}. These devices combine printed tactile overlays with touch-sensitive surfaces, enabling BLV users to explore the graphics tactually and access audio labels by interacting with predefined touch areas. However, as these systems rely on printed tactile graphics, their scope is limited to two-dimensional content.

\textbf{3D-printed models} are an increasingly common alternative to tactile graphics. They enable a broader range of material to be produced, particularly for concepts that are inherently three-dimensional in nature. In recent years, the production cost and effort of 3D-printed models have fallen to levels comparable to tactile graphic production. 3D-printed models are increasingly being applied in various accessible graphic areas: mapping and navigation~\cite{gual2012visual,Holloway2018,Holloway2019b,Holloway2022,Nagassa2023}; special education~\cite{Buehler2016}; art galleries~\cite{karaduman2022beyond,Butler2023}; books~\cite{kim2015,Stangl2015}; mathematics~\cite{Brown2012,Hu2015}; graphic design~\cite{McDonald2014}; science~\cite{wedler2012applied,Hasper2015}; and programming~\cite{kane2014}. Compared to tactile graphics, 3D-printed models have been shown to improve tactual understanding and mental model development among BLV people~\cite{Holloway2018}. However, as with traditional tactile graphics, the provision of written descriptions or braille labelling presents challenges. The limited space on models and the low-fidelity of 3D-printed braille can significantly impact the utility and readability of labels~\cite{Brown2012,Taylor2015,Shi2016}.

\subsection{Interactive 3D-Printed Models}
To address labelling challenges, limitations on the type of content that can be produced, and to create more engaging and interactive experiences, there is growing interest in the development of \textbf{interactive 3D-printed models (I3Ms)}. By combining 3D-printed models with low-cost electronics and/or smart devices, many I3Ms now include button or touch-triggered audio labels that provide verbal descriptions of the printed model~\cite{Landau2009,Shi2016,Reichinger2016,Giraud2017,Gotzelmann2017,Holloway2018,Ghodke2019,Davis2020}. Such I3Ms have been applied across various BLV-accessible graphic areas, including: art~\cite{Holloway2019,Bartolome2019,Butler2023}; education~\cite{Ghodke2019,Shi2019,Reinders2020}; and mapping and navigation~\cite{Gotzelmann2017,Holloway2018,Shi2020}. I3Ms with audio labels are especially useful for BLV users who are not fluent braille readers. Stored as text and synthesised in real-time, audio labels are easier to update compared to labels on non-interactive models. Many I3Ms also support multiple levels of audio labelling~\cite{Holloway2018,Shi2019,Reinders2020}, extracted through unique button presses or touch gestures, enabling them to convey far more information than the written descriptions supplied alongside tactile graphics and non-interactive models.

I3Ms are inherently multimodal. Multimodality can improve the adaptability of a system~\cite{Reeves2004}, and when modalities are combined, they can increase the resolution of information the system conveys~\cite{Edwards2015} and enable more natural interactions~\cite{Bolt1980}. For BLV users, combining modalities has been shown to improve confidence and independence~\cite{Quero2021}. Modality adaptability allows BLV users to choose interaction methods based on context, ability, or effort. For example, a user may be uncomfortable engaging in speech interaction in public due to privacy concerns~\cite{Abdolrahmani2018}, opting instead to use button or gesture-based inputs. Richer resolutions of information can be achieved when combining modalities, \eg\, the tactile features of a 3D-printed model with haptic vibratory and auditory outputs. Combining modalities is critical to overcoming the \textit{`bandwidth problem'}, in which BLV users' non-visual senses cannot match the capacity of vision, necessitating their combined use~\cite{Edwards2015}.

While early I3Ms primarily relied on button or gesture-based triggered audio labels, recent research has explored integrating speech interfaces and conversational agents. This shift is driven by research finding that BLV people find voice interaction convenient~\cite{Azenkot2013}, along with widespread adoption~\cite{Pradhan2018} and high usage~\cite{Abdolrahmani2018} of conversational agents among BLV users. For instance, Quero \etal~\cite{Quero2019} combined a tactile graphic of a floor plan with a conversational agent that focused on indoor navigation; however, voice interaction was performed through a connected smartphone rather than the graphic itself. Other works have developed voice-controlled agents to guide BLV users in exploring 3D-printed representations of gallery pieces~\cite{Bartolome2019,Quero2018}. These systems, however, have primarily focused on basic command-driven interactions more analogous to voice menus rather than conversational dialogue. Shi \etal~\cite{Shi2017b,Shi2019} proposed incorporating conversational agents into I3Ms to allow BLV users to expand their understanding of the modelled content.

Recent research into I3Ms has begun to explore modalities beyond audio and touch. Quero \etal~\cite{Quero2018} designed an I3M representing an art piece that integrated localised audio, wind, and heat output. However, participants faced challenges in interpreting the semantic mapping of modalities, \eg\, whether heat represented the morning sun or the shine of starlight. In our previous work, we found that BLV users desired I3Ms that combined touch, haptic vibratory feedback, and conversational dialogue~\cite{Reinders2020}. Additionally, we co-designed an I3M with BLV co-designers to explore how these modalities could create natural interactions~\cite{Reinders2023}, inspired by the \textit{`Put-That-There'} paradigm~\cite{Bolt1980}. This work led to five I3M design recommendations, including: support interruption-free tactile exploration; leverage prior interaction experience with personal technology; support customisation and personalisation; support more natural dialogue; and tightly coupled haptic feedback. These studies motivated our current work, with participants finding the I3M engaging, and several beginning to embody it.

\subsection{The Embodiment of Agents}
Dourish~\cite{Dourish2001} presents a seminal view that embodiment \textit{``denotes a form of participative status''}, where embodied interaction in natural forms of communication is influenced both by physical presence and context. They posit that this perspective applies to \textit{``spoken conversations just as much as to apples or bookshelves''}. Within the design of agent-based systems, embodiment is largely understood as the use of different modalities -- \eg\ voice, visual output, gestures, gaze -- to imbue machines with human-like behaviours and characteristics, making them appear more \textit{`alive'}; with an enhanced perception of social presence that is capable of approximating human-human social interaction~\cite{cassell2000more,Lester1997,Lankton2015,Biocca1999}. Such agents are often described as being more \textit{`lifelike'} or possessing \textit{`lifelikeness'}~\cite{Lester1997,Cassell1999,Cassell1999b,Cassell1999c,Lester1999}.

Research into embodiment has predominantly focused on sighted users. As systems become more embodied, users' perceptions of social presence can increase, motivating users to treat them more favourably~\cite{Reeves2004}. Lankton \etal~\cite{Lankton2015} described how social presence can make systems appear more sociable, warm, and personal, while Cassell~\cite{Cassell2001} noted that embodying technology allows users to locate intelligence, illuminating what would otherwise be an \textit{`invisible computer'}. Importantly, embodied agents exhibiting higher levels of social presence and perception have been shown to enhance user perception of engagement~\cite{Shamekhi2018,Luger2016,Heuwinkel2013,Cassell2001} and trust~\cite{Bickmore2001,Shamekhi2018,Bickmore2013,Rheu2021}. 

In the HCI community, efforts to embody intelligent agents have focused on enhancing social perception through conversational, visual, and physical attributes. \textbf{Conversational embodiment} includes mimicking human voices~\cite{Cassell2001}, small talk~\cite{Liao2018,Pradhan2019,Cassell2001,Shamekhi2018,Bickmore2001}, greetings~\cite{Cassell2001,Luria2019,Shamekhi2018,Lester1997}, and conversational turn-taking~\cite{Cassell2001,Kontogiorgos2020}. Lester \etal~\cite{Lester1997} described the \textit{persona effect}, proposing that social presence can increase when agents exhibit personality, making them appear more lifelike. Many conversational agents, like Siri, incorporate attributes of conversational embodiment.

\textbf{Visually embodied} agents, which are often also conversationally embodied, associate systems with virtual avatars or characters, many of which have faces~\cite{Cassell2001,Kontogiorgos2020,Bickmore2013}, and are capable of gesturing~\cite{Cassell2001,Bickmore2001} and gaze~\cite{Kontogiorgos2020,Shamekhi2018,Bickmore2001}. Visual embodiment  extends beyond the visual feedback that mainstream conversational agents emit, \eg\ rings of light or animations used to indicate that agents are processing or `thinking'. Depending on the task, visually and conversationally embodied agents can improve social perceptions~\cite{Shamekhi2018,Luria2019,Cassell2001,Nowak2003}, trust~\cite{Bickmore2001,Bickmore2013,Rheu2021,Shamekhi2018}, and engagement~\cite{Shamekhi2018,Cassell2001}. 

Embodied agents can extend beyond virtual embodiment and include physical bodies~\cite{Kontogiorgos2020,Luria2017}. Lura \etal~\cite{Luria2017} found that users' situational awareness was higher when using a physically embodied robot to perform smart-home tasks compared to an unembodied voice agent. Robots capable of emitting human-like warmth have been associated with increased perceptions of friendship and presence~\cite{Nie2012}. Recent research has explored the use of haptic vibratory feedback to create lifelike cues, such as heartbeats~\cite {Borgstedt2023} and handshaking~\cite{Bevan2015}. \textbf{Physically embodied} agents can be perceived as having higher social presence than non-physically embodied agents~\cite{Kidd2004}. Additionally, they have been found to be more forgivable during unsuccessful interactions; however, depending on their realism, thi can become distracting in high-stakes scenarios~\cite{Kontogiorgos2020}.

\subsection{The Embodiment of I3Ms}
The design of embodied agents has traditionally focused on the perception of sighted users. However, in the last decade, work has begun exploring how human-human conversational cues can be converted into non-visual formats for BLV users. Many of these efforts utilise haptic belts/headsets~\cite{rader2014,McDaniel2018} or AR glasses~\cite{Qiu2016,Qiu2020} to convey body movements like head shaking, nodding, and gaze. Despite this, the impact of agent embodiment, and specifically embodied I3Ms, on BLV users' perceptions remains unstudied.

In our previous work, we found that a number of participants desired I3Ms that felt more lively and human~\cite{Reinders2023}. Participants suggested integrating a conversational agent with a personality, incorporating haptic vibratory feedback to make the I3M feel alive, and enabling speech to originate directly from the model itself. This desire for more human-like interactions aligns with findings with conversational agents. Choi \etal~\cite{Choi2020} reported that many BLV users valued human-like conversation with conversational agents as critical in relationship building, while Abdolrahmani \etal~\cite{Abdolrahmani2018} observed that BLV users preferred agents they could talk with as if they were other people rather than pieces of technology. Karim \etal~\cite{Karim2023} recommended that agents have customisable personalities, recognising that such features may not be relevant in all scenarios, such as group settings. Collins \etal~\cite{Collins2023}, however, found hesitance among BLV users towards embodied AI agents in VR applications. Whether these perspectives and desires extend to I3Ms is unknown.

Further impetus for studying I3M embodiment comes from the links between embodiment, engagement, and trust with embodied agents that have been previously identified for sighted users. Trust and engagement are especially critical for BLV users, as the usefulness of accessible graphics, aids, or tools depends on users' willingness to engage with and accept/rely on the information they provide. This directly determines the extent to which users rely on these tools~\cite{Betsy1993,Wu2017,Abdolrahmani2018}. Therefore, it is crucial to explore whether I3Ms can be effectively embodied and whether embodiment fosters greater trust and engagement between users and their I3Ms.