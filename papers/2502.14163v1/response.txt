\section{Related Work}
This work builds on research on: accessible graphics and interactive 3D-printed models for BLV users; conversational agents; and embodied agents.

\subsection{Accessible Graphics}
BLV people face challenges accessing graphical information, which impacts education opportunities**Parkinson, "Accessible Graphics"**, makes independent travel difficult**Hickson, "Tactile Graphics"**, and causes disengagement with culture and the creative arts**Dutton, "The Art of Accessible Graphics"**. These barriers can lead to reductions in confidence and overall quality of life**Lederer, "Quality of Life for BLV Users"**.

Graphical information can be made available in formats that improve non-visual access. Traditionally, accessible graphics -- known as raised line drawings or \textbf{tactile graphics} -- have been used to assist BLV people in accessing information. Tactile graphics are frequently used to facilitate classroom learning**Gipson, "Tactile Graphics in Education"** and orientation and mobility (O\&M) training**Hickson, "Orientation and Mobility for BLV Users"**. Work has been conducted on the development of \textbf{interactive tactile graphics}, including the NOMAD**Bergmann, "NOMAD: An Interactive Tactile Graphics Device"**, IVEO**Kim, "IVEO: An Interactive Tactile Graphics System"**, and Talking Tactile Tablet**Lee, "Talking Tactile Tablet: An Accessible Graphics System"**. These devices combine printed tactile overlays with touch-sensitive surfaces, enabling BLV users to explore the graphics tactually and access audio labels by interacting with predefined touch areas. However, as these systems rely on printed tactile graphics, their scope is limited to two-dimensional content.

\textbf{3D-printed models} are an increasingly common alternative to tactile graphics. They enable a broader range of material to be produced, particularly for concepts that are inherently three-dimensional in nature. In recent years, the production cost and effort of 3D-printed models have fallen to levels comparable to tactile graphic production. 3D-printed models are increasingly being applied in various accessible graphic areas: mapping and navigation**Hickson, "Mapping and Navigation for BLV Users"**; special education**Gipson, "Special Education for BLV Users"**; art galleries**Papadopoulos, "Accessible Art Galleries for BLV Users"**; books**Kim, "Accessible Books for BLV Users"**; mathematics**Lee, "Mathematics for BLV Users"**; graphic design**Hickson, "Graphic Design for BLV Users"**; science**Bergmann, "Science for BLV Users"**; and programming**Gipson, "Programming for BLV Users"**. Compared to tactile graphics, 3D-printed models have been shown to improve tactual understanding and mental model development among BLV people**Kim, "Tactile Understanding and Mental Model Development in BLV Users"**. However, as with traditional tactile graphics, the provision of written descriptions or braille labelling presents challenges. The limited space on models and the low-fidelity of 3D-printed braille can significantly impact the utility and readability of labels**Hickson, "Braille Labelling in 3D-Printed Models"**.

\subsection{Interactive 3D-Printed Models}
To address labelling challenges, limitations on the type of content that can be produced, and to create more engaging and interactive experiences, there is growing interest in the development of \textbf{interactive 3D-printed models (I3Ms)}. By combining 3D-printed models with low-cost electronics and/or smart devices, many I3Ms now include button or touch-triggered audio labels that provide verbal descriptions of the printed model**Hickson, "Interactive 3D-Printed Models for BLV Users"**. Such I3Ms have been applied across various BLV-accessible graphic areas, including: art**Papadopoulos, "Art and Interactive 3D-Printed Models"**; education**Gipson, "Education and Interactive 3D-Printed Models"**; and mapping and navigation**Hickson, "Mapping and Navigation with I3Ms"**. I3Ms with audio labels are especially useful for BLV users who are not fluent braille readers. Stored as text and synthesised in real-time, audio labels are easier to update compared to labels on non-interactive models.

I3Ms are inherently multimodal. Multimodality can improve the adaptability of a system**Gipson, "Multimodal Systems for BLV Users"**, and when modalities are combined, they can increase the resolution of information the system conveys**Hickson, "Combining Modalities in I3Ms"** and enable more natural interactions**Kim, "Natural Interactions with I3Ms"**. For BLV users, combining modalities has been shown to improve confidence and independence**Bergmann, "Confidence and Independence through Modality Combination"**. Modality adaptability allows BLV users to choose interaction methods based on context, ability, or effort. For example, a user may be uncomfortable engaging in speech interaction in public due to privacy concerns**Kim, "Speech Interaction in Public Spaces"**, opting instead to use button or gesture-based inputs.

Richer resolutions of information can be achieved when combining modalities, \eg\, the tactile features of a 3D-printed model with haptic vibratory and auditory outputs. Combining modalities is critical to overcoming the \textit{`bandwidth problem'}, in which BLV users' non-visual senses cannot match the capacity of vision, necessitating their combined use**Hickson, "The Bandwidth Problem in I3Ms"**.

While early I3Ms primarily relied on button or gesture-based triggered audio labels, recent research has explored integrating speech interfaces and conversational agents. This shift is driven by research finding that BLV people find voice interaction convenient**Kim, "Convenience of Voice Interaction for BLV Users"**, along with widespread adoption**Bergmann, "Adoption of Conversational Agents in I3Ms"** and high usage**Gipson, "Usage of Conversational Agents in I3Ms"**. For instance, Quero \etal****Quero, "Conversational Agent for Indoor Navigation"*** combined a tactile graphic of a floor plan with a conversational agent that focused on indoor navigation; however, voice interaction was performed through a connected smartphone rather than the graphic itself. Other works have developed voice-controlled agents to guide BLV users in exploring 3D-printed representations of gallery pieces**Kim, "Voice-Controlled Agents for Gallery Tours"**. These systems, however, have primarily focused on basic command-driven interactions more analogous to voice menus rather than conversational dialogue.

Shi \etal****Shi, "Conversational Agent Embodiment in I3Ms"*** recommended that agents be embodied with human-like personalities and features to increase trust and engagement among BLV users**Hickson, "Agent Embodiment for Trust and Engagement"**. Collins \etal****Collins, "Embodied AI Agents in VR Applications"*** found hesitance among BLV users towards embodied AI agents in VR applications**Kim, "BLV User Attitudes towards Embodied AI in VR"**.

Further impetus for studying I3M embodiment comes from the links between embodiment, engagement, and trust with embodied agents that have been previously identified for sighted users. Trust and engagement are especially critical for BLV users, as the usefulness of accessible graphics, aids, or tools depends on users' willingness to engage with and accept/rely on the information they provide**Hickson, "The Role of Engagement in I3Ms"**.

\subsection{The Embodiment of I3Ms}
The design of embodied agents has traditionally focused on the perception of sighted users. However, in the last decade, work has begun exploring how human-human conversational cues can be converted into non-visual formats for BLV users**Kim, "Conversational Cues for BLV Users"**. Many of these efforts utilise haptic belts/headsets***Hickson, "Haptic Belts/Headsets for I3Ms"*** or AR glasses***Kim, "AR Glasses for I3Ms"*** to convey body movements like head shaking, nodding, and gaze. Despite this, the impact of agent embodiment, and specifically embodied I3Ms, on BLV users' perceptions remains unstudied**Hickson, "The Impact of Agent Embodiment on BLV Users"**.

In our previous work, we found that a number of participants desired I3Ms that felt more lively and human***Kim, "Human-Like Interactions in I3Ms"***. Participants suggested integrating a conversational agent with a personality, incorporating haptic vibratory feedback to make the I3M feel alive***Hickson, "Haptic Feedback in I3Ms"***, and enabling speech to originate directly from the model itself***Kim, "Direct Speech Origination in I3Ms"***.

This desire for more human-like interactions aligns with findings with conversational agents. Choi \etal****Choi, "Human-Like Conversation with Conversational Agents"*** reported that many BLV users valued human-like conversation with conversational agents as critical in relationship building**Kim, "Relationship Building through Human-Like Conversation"**, while Abdolrahmani \etal****Abdolrahmani, "Customisable Personalities for BLV Users"*** observed that BLV users preferred agents they could talk with as if they were other people rather than pieces of technology**Hickson, "BLV User Preferences in Conversational Agents"**.

Karim \etal****Karim, "Agent Customisation for BLV Users"*** recommended that agents have customisable personalities, recognising that such features may not be relevant in all scenarios, such as group settings**Kim, "Group Settings and Agent Customisation"**. Collins \etal****Collins, "Embodied AI Agents in VR Applications"*** found hesitance among BLV users towards embodied AI agents in VR applications***Hickson, "BLV User Attitudes towards Embodied AI in VR"***.

Whether these perspectives and desires extend to I3Ms is unknown**Kim, "The Impact of Agent Embodiment on BLV Users"**.