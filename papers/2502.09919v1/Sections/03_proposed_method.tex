\section{Proposed Method}
\begin{figure*}[t]
    \centering    \includegraphics[width=0.8\textwidth]{Figures/proposed_method.pdf}
    \caption{\small Overview of the AttenGluco framework including sensing module, data preparation, and forecasting model.}
    \label{fig:proposed_method}
\end{figure*}

In this section, we introduce our proposed framework for blood glucose prediction. An overview of the AttenGluco framework is shown in Fig.~\ref{fig:proposed_method}. The framework comprises three main components: (a) a sensing module that gathers physiological and behavioral signals from wearable sensors, (b) a preprocessing module for time-series data preparation, and (c) a machine learning forecasting model utilizing the Transformer architecture for blood glucose prediction. Our transformer-based model predicts blood glucose levels (BGL) in individuals with type 2 diabetes by incorporating CGM data alongside activity information.
% other physiological signals such as activity, heart rate, and stress. 
The attention mechanism within the Transformer facilitates the effective integration of multi-time series signals recorded at different sampling rates. Additionally, it is well-suited for predicting highly fluctuating signals such as BGLs. To validate the effectiveness of our proposed model, we conduct experiments using the publicly available AI-READI (Flagship) dataset. The following sections provide a detailed explanation of the forecasting problem and key components of AttenGluco.

\subsection{Forecasting Problem}
The problem of blood glucose forecasting with multimodal input data can be formulated as a time series prediction task. Let \( \mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_k] \) represent a set of \( k \) sensor-derived measurements in the sensing data component. The observation from the \( i\)th sensor is denoted as \( \mathbf{x}_i = [x_{i,1}, \dots, x_{i,t}]^\top \), where $t$ is the sampling duration. Our proposed framework, AttenGluco, leverages CGM data ($\mathbf{x}_{\text{g}}$) and activity data such as walking steps ($\mathbf{x}_{\text{ws}}$) and walking time intervals ($\mathbf{x}_{\text{wi}}$), which represent the duration between consecutive walking events. The multi-step forecasting output is expressed as $ \mathbf{\hat{x}}_g = [x_{g,t+1}, \dots, x_{g,t+m}]^\top $, where $m$ represents the number of predicted time steps, commonly referred to as the prediction horizon (PH). Mathematically, the forecasting task can be formulated as $\mathbf{\hat{x}}_g = f(\mathbf{X; \Theta})$, where \( f \) represents the forecasting model, parameterized by \( \Theta \), which is learned during the training process.
% To ensure a consistent evaluation across both output settings, we compute error metrics, such as Root Mean Square Error (RMSE), by comparing the predicted glucose levels with the actual future values in the estimated multi-output sequence.


\subsection{AttenGluco}

AttenGluco is composed of two primary stages. The first stage, data preparation, focuses on collecting and processing physiological and behavioral data to serve as input for the forecasting model. This stage also includes data interpolation to handle missing values and normalization for consistency. During this phase, BGLs are recorded using a CGM device, while additional behavioral metrics, such as physical activity, are gathered from wearable sensors such as smartwatches, as depicted in Fig. \ref{fig:proposed_method}. The second stage is the multimodal forecasting model, which utilizes these preprocessed inputs for blood glucose prediction.

The forecasting model is developed based on the Transformer architecture. This architecture leverages an attention mechanism to extract time-dependent patterns from fused irregular time-series data while also capturing long-term dependencies. This approach enables the model to effectively process complex temporal relationships. The structure of our proposed Transformer-based forecasting model is shown in Fig.~\ref{fig:transformer_method}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Transformer_Model.pdf}
    \caption{\small AttenGluco model architecture consists of cross-attention and multi-scale attention to forecast BGL.}
    \label{fig:transformer_method}
\end{figure}

The standard transformer architecture is typically made up of an encoder-decoder for data reconstruction. However, we modified this design for our forecasting model and framed it as a supervised learning task. Specifically, we eliminated the decoder and only utilized the encoder for data representation learning. Our customized transformer architecture incorporates two attention mechanisms: cross-attention and multi-scale attention. The cross-attention mechanism integrates various time series data with variant sample rates, while the multi-scale attention captures temporal dependencies within the signals to reduce the effect of random noise~\cite{shabani2022scaleformer}. By incorporating these attention mechanisms, our Transformer-based approach enhances the accuracy of BGL forecasting.

Our Transformer architecture consists of embedding and positional encoding layers, followed by cross-attention, feed-forward, Add \& Norm layers, and a multi-scale attention block. The input variables \( \mathbf{x}_\text{g} \), \( \mathbf{x}_\text{ws} \), and \( \mathbf{x}_\text{wi} \) are initially processed through an embedding layer \( f_{\text{embed}}(\cdot) \), then passed through a positional encoding function \( f_{\text{pos}}(\cdot) \), producing the transformed representations \( \mathbf{X}_\text{G} \), \( \mathbf{X}_\text{WS} \), and \( \mathbf{X}_\text{WI} \), respectively. Each resulting matrix resides in \( \mathbb{R}^{t \times d_{\text{model}}} \), where \( t \) represents the sampling duration and \( d_{\text{model}} \) is a hyperparameter. The multi-head attention mechanism in Transformer architectures~\cite{vaswani2017attention} functions by scaling values \( (\mathbf{V} \in \mathbb{R}^{t \times d_\text{model}}) \) based on the relationships between keys \( (\mathbf{K} \in \mathbb{R}^{t \times d_{\text{model}}}) \) and queries \( (\mathbf{Q} \in \mathbb{R}^{t \times d_{\text{model}}}) \). The mathematical formulation of the attention mechanism is presented in Eq.~\ref{eq:attention}.

\begin{equation}
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})= \text{Softmax} \left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_{\text{model}}}} \right)\mathbf{V}
\label{eq:attention}
\end{equation}
We designed a two-branch cross-attention layer, where both branches receive \( \mathbf{X}_\text{G} \) as the query. In one branch, the keys and values correspond to \( \mathbf{X}_{\text{WS}} \), while in the other, they correspond to \( \mathbf{X}_{\text{WI}} \). The cross-attention (CA) of the first branch is computed using Eqs.~\ref{eq:crossatt} and~\ref{eq:crossatt2}.

\begin{equation}
        \text{CA}\left(\mathbf{X_\text{G}}, \mathbf{X_{\text{WS}}}, \mathbf{X_{\text{WS}}}\right) \\
         =[\mathbf{H}_1, \dots, \mathbf{H}_{m_H}] \mathbf{W}_H^{\text{CA}}
    \label{eq:crossatt}
\end{equation}
\begin{equation}
    \mathbf{H}_h = \text{Attention}(\mathbf{\mathbf{X_\text{G}}} \mathbf{W}_{\mathbf{Q}}^{\text{CA}}, \mathbf{X}_\text{WS} \mathbf{W}_{\mathbf{K}}^{\text{CA}}, \mathbf{X}_\text{WS} \mathbf{W}_{\mathbf{V}}^{\text{CA}})
    \label{eq:crossatt2}
\end{equation}

Where $\mathbf{W}_{\mathbf{Q}}^{\text{CA}}$, $\mathbf{W}_{\mathbf{K}}^{\text{CA}}$, and  $\mathbf{W}_{\mathbf{V}}^{\text{CA}}$ are weight matrices specific to the attention head and belong to $\mathbb{R} ^{d_{\text{model}}\times d_{\text{model}}}$. Moreover, $\mathbf{W}_H^{\text{CA}} \in \mathbb{R}^{(m_H \cdot d_{\text{model}}) \times d_{\text{model}}}$ is the final weight matrix that projects the concatenated attention head outputs into the original model dimension.The attention mechanism for the second branch follows the same computation, with the $\mathbf{X}_\text{WI}$ as both the key and the query.

Then, the attention outputs from both branches are combined to incorporate cross-attention information. The resulting data is passed through a linear feedforward network followed by an Add \& Norm module. The processed output, \( \mathbf{X}_{\text{CA}} \), is then fed into a multi-scale attention mechanism comprising three multi-head attention branches, each designed for different downsampling (DS) rates. These branches apply downsampling factors of 1, 2, and 4, where a factor of 1 indicates no downsampling, as illustrated in Fig.~\ref{fig:transformer_method}.

For the first branch, the multi-scale attention mechanism (MA) on $\mathbf{X}_{\text{CA}}$ is computed by using Eqs.~\ref{eq:multiatt} and~\ref{eq:multiatt2}.
\begin{equation}
        \text{MA}(\mathbf{X}_\text{CA}, \mathbf{X}_\text{CA}, \mathbf{X}_\text{CA}) =
        [\mathbf{H}_1, \dots, \mathbf{H}_{m_H}] \mathbf{W}_H^{\text{MA}}
    \label{eq:multiatt}
\end{equation}
\begin{equation}
    \mathbf{H}_h = \text{Attention}(\mathbf{X}_\text{CA} \mathbf{W}_{\mathbf{Q}}^{\text{MA}},\mathbf{X}_\text{CA} \mathbf{W}_{\mathbf{K}}^{\text{MA}}, \mathbf{X}_\text{CA} \mathbf{W}_{\mathbf{V}}^{\text{MA}})
    \label{eq:multiatt2}
\end{equation}

Each attention branch utilizes query, key, and value weight matrices, \( \mathbf{W}_{\mathbf{Q}}^{\text{MA}} \), \( \mathbf{W}_{\mathbf{K}}^{\text{MA}} \), and \( \mathbf{W}_{\mathbf{V}}^{\text{MA}} \), all belonging to \( \mathbb{R} ^{d_{\text{model}}\times d_{\text{model}}} \). The outputs from all attention heads are concatenated and projected back into the original model dimension using the final weight matrix \( \mathbf{W}_H^{\text{MA}} \in \mathbb{R}^{(m_H \cdot d_{\text{model}}) \times d_{\text{model}}} \). The remaining two branches follow the same computational process but operate on downsampled input data. This approach improves the modelâ€™s capability to capture both fine-grained details and long-term temporal dependencies within the input signals.

The outputs from the three multi-scale attention branches are summed and passed through a feed forward network, an Add \& Norm block, and a fully connected layer. This final configuration generates \( m \) predicted CGM values. Each prediction corresponds to a measurement taken every 5 minutes, meaning that \( m \) samples collectively provide forecasts for \( m \times \) 5 minutes into the future. In summary, Algorithm~\ref{alg:TransformerBGL} describes the data processing pipeline in AttenGluco.

\begin{algorithm}
\small
\caption{AttenGluco model}
\label{alg:TransformerBGL}
\textbf{Input:} Preprocessed and normalized data, including CGM signal ($\mathbf{x}_{\text{g}}$), walking steps ($\mathbf{x}_{\text{ws}}$), and walking time intervals ($\mathbf{x}_{\text{wi}}$), Cross-attention block (CA), Multi-scale atention block (MA), Embedding function ($f_{\text{embed}}$), Positional encoding function ($f_{\text{pos}}$), Two Add \& Norm block ($f_{\text{AN}}^{(1)}$,$f_{\text{AN}}^{(2)}$), Two Feedforward model ($f_\text{FF}^{(1)}$,$f_\text{FF}^{(2)}$), Linear model ($f_\text{lin}$) \\
\textbf{Output:} Predicted BGL $\mathbf{\hat{x}}_g$\\
\begin{algorithmic}[1]
\State \textbf{Begin}

% \State \hspace{0.02cm} \textbf{Data Preparation:}
% \State \hspace{0.2cm} $\mathbf{x}_{\text{g}} \gets\text{Interpolate}\;\mathbf{x}_{\text{g}}$ (Interpolate missing glucose values)
% \vspace{0.5em}
% \State \hspace{0.2cm} $\mathbf{x}_{\text{ws}}, \mathbf{x}_{\text{wi}} \gets \text{Fill NaN with 0}$ \{Fill missing activity data\}
% \vspace{0.5em}
% \State \hspace{0.2cm} Normalize data using Min-Max Scaling
% \State \hspace{0.02cm} \textbf{Apply Embedding and Positional Encoding:}
\vspace{0.5em}
\State \hspace{0.2cm} $[\mathbf{X}_\text{G},\mathbf{X}_\text{WS}, \mathbf{X}_\text{WI}]\gets f_{\text{pos}}\left(f_{\text{embed}}\left(\left[\mathbf{x}_\text{g}, \mathbf{x}_\text{ws}, \mathbf{x}_\text{wi}\right]\right)\right)$
\vspace{0.5em}
% \State \hspace{0.05cm} $PE(t, 2i) \gets \sin \left(\dfrac{\tau}{10^{\frac{6i}{d}}} \right)$
% \State \hspace{0.05cm} $PE(t, 2i+1) \gets \cos \left(\dfrac{\tau}{10^{\frac{6i}{d}}} \right)$
% \State \hspace{0.02cm} \textbf{Cross-Attention Mechanism:}
% \vspace{0.5em}
% \State \hspace{0.2cm} $[\mathbf{Q}_1, \mathbf{K}_1, \mathbf{V}_1] \gets [\mathbf{X}_\text{G}\mathbf{W}^{\text{CA1}}_\mathbf{Q}, \mathbf{X}_\text{WS}\mathbf{W}^{\text{CA1}}_\mathbf{K}, \mathbf{X}_\text{WS}\mathbf{W}^{\text{CA1}}_\mathbf{V}]$
% \vspace{0.5em}
% \State \hspace{0.2cm} $[\mathbf{Q}_2, \mathbf{K}_2, \mathbf{V}_2] \gets [\mathbf{X}_\text{G}\mathbf{W}^{\text{CA2}}_\mathbf{Q}, \mathbf{X}_\text{WI}\mathbf{W}^{\text{CA2}}_\mathbf{K}, \mathbf{X}_\text{WI}\mathbf{W}^{\text{CA2}}_\mathbf{V}]$
% \vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_{\text{CA1}}\gets$ CA($\mathbf{X}_{\text{G}}, \mathbf{X}_{\text{WS}}, \mathbf{X}_{\text{WS}}$)
\vspace{0,5em}
\State \hspace{0.2cm} $\mathbf{X}_{\text{CA2}}\gets$ CA($\mathbf{X}_{\text{G}}, \mathbf{X}_{\text{WI}}, \mathbf{X}_{\text{WI}}$)
\vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_\text{CA}\gets f_{\text{AN}}^{(1)}\left(f_{\text{FF}}^{(1)}\left(\mathbf{X}_\text{CA1}+\mathbf{X}_\text{CA2}\right)\right)$
\vspace{0.5em}
\State \hspace{0.2cm}  $\mathbf{X}^{(2)}_{\text{CA}}, X^{(4)}_{\text{CA}} \gets \text{Downsample}(\mathbf{X}_{\text{CA}}, 2), \text{Downsample}(\mathbf{X}_{\text{CA}}, 4)$
\vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_{\text{MA1}}\gets$ MA$(\mathbf{X}_{\text{CA}}, \mathbf{X}_{\text{CA}}, \mathbf{X}_{\text{CA}})$
\vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_{\text{MA2}}\gets$ Upsample(MA$(\mathbf{X}^{(2)}_{\text{CA}}, \mathbf{X}^{(2)}_{\text{CA}}, \mathbf{X}^{(2)}_{\text{CA}})$, 2)
\vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_{\text{MA3}}\gets$ Upsample(MA$(\mathbf{X}^{(4)}_{\text{CA}}, \mathbf{X}^{(4)}_{\text{CA}}, \mathbf{X}^{(4)}_{\text{CA}})$, 4)
\vspace{0.5em}
\State \hspace{0.2cm} $\mathbf{X}_\text{MA}\gets f_{\text{AN}}^{(2)}\left(f_{\text{FF}}^{(2)}\left(\mathbf{X}_\text{MA1}+\mathbf{X}_\text{MA2}+\mathbf{X}_\text{MA3}\right)\right)$
\vspace{0.5em}
\State \hspace{0.2cm} $\hat{\mathbf{x}}_g \gets f_\text{lin}(\mathbf{X}_\text{MA})$
% \State \hspace{0.02cm} $\text{Attention}(Q, K, V) \gets \text{Softmax} \left( \dfrac{QK^T}{\sqrt{d_k}} \right) V$
% \State \textbf{Multi-Scale Attention:}
% \State \textbf{Fully Connected Network:}
% \State \hspace{0.05cm} $Y_{\text{pred}} \gets W_2 (\text{ReLU}(W_1 H + b_1)) + b_2$
\vspace{0.5em}
\State \hspace{0.2cm} \textbf{return} $\hat{\mathbf{x}}_g$
\State \textbf{End}
\end{algorithmic}
\vspace{-0.8mm}
\end{algorithm}