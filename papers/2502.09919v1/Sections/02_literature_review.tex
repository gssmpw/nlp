\section{Related Work}

Recent years have seen a surge in blood glucose management technologies. CGM systems, wearable health monitoring devices, and automated insulin delivery systems (AIDS) collectively provide real-time data and partial automation for diabetes care~\cite{WHO2024Diabetes, pasquel2021management}. CGMs offer continuous monitoring of glucose levels, synchronizing with mobile applications for timely alerts on hyperglycemia and hypoglycemia \cite{abdul2020epidemiology}. Wearable sensors further extend coverage to physiological and behavioral metrics, such as heart rate variability and physical activity levels~\cite{raffin2023sedentary, guan2023artificial}. By combining CGM outputs with additional signals, AIDS can regulate insulin dosage more precisely~\cite{lovic2020growing}. However, limitations persist in terms of sensor calibration, missing data, and user non-adherence~\cite{pasquel2021management}.


Early prediction efforts relied on statistical and time-series models, notably Autoregressive Integrated Moving Average (ARIMA)~\cite{xie2020benchmarking}. Although ARIMA and similar approaches are straightforward, they often fail to capture the complex, nonlinear patterns of glycemic fluctuation. Machine learning (ML) techniques, such as support vector regression and random forests, typically reduce forecasting error by 5--15\% compared to ARIMA~\cite{xie2020benchmarking, mujahid2021machine}. However, they still struggle with deeper temporal dependencies over longer prediction windows~\cite{mujahid2021machine}.

Deep learning techniques, widely applied across various domains such as healthcare~\cite{azghan2023personalized,mamun2022multimodal,10857973}, classification tasks~\cite{10780662}, offer improved forecasting accuracy by effectively modeling intricate temporal dependencies and nonlinear patterns in time series data. Furthermore, integrating causal knowledge into learning frameworks~\cite{corazza2023expediting,10644549} can enhance adaptability and facilitate knowledge transfer across different environments. LSTM architectures are proposed to mitigate vanishing and exploding gradients in recurrent neural networks~\cite{ shuvo2023deep}. By gating internal states, LSTMs retain long-term context for extended horizons, outperforming classical ML methods in certain datasets~\cite{shuvo2023deep}. Despite these improvements, LSTM-based models often demand significant computational resources and meticulous tuning, making them less flexible for large-scale or highly variable glucose data~\cite{kong2024unlocking}.
GRUs streamline the gating structure of LSTMs, converging 15--25\% faster for some time-series tasks~\cite{chung2014empirical, Elsayed_2019}. Nevertheless, GRUs still encounter challenges related to sensor inaccuracies, incomplete user logs, and irregular sampling rates \cite{kong2024unlocking}. Hybrid methods that integrate convolutional layers with recurrent modules reduce some errors by 2--4\% \cite{Elsayed_2019}, yet extensive clinical validation for blood glucose forecasting remains limited.

Transformers adopt self-attention instead of recurrent loops which facilitates parallel learning over extensive sequences~\cite{vaswani2017attention}. This approach outperforms RNNs by 5--10\% in mean squared error (MSE) for long-horizon predictions~\cite{kim2024comprehensive, farahmand2024hybrid}. However, many existing implementations assume large, consistent datasets with minimal missing points. Glucose monitoring, conversely, often faces sensor dropouts and user non-adherence, limiting straightforward application~\cite{acuna2023analyzing}. Gluformer~\cite{sergazinov2023gluformer} developed a transformer-driven blood glucose forecasting model by providing uncertainty intervals rather than single-point estimates. Although a 1--2 mg/dL improvement in short-horizon RMSE has been observed, the absence of multi-scale/cross-attention hinders the modelâ€™s ability to integrate additional clinical or activity data~\cite{sergazinov2023gluformer}.

% \include{Sections/xx_lit_rev_tbl}

In summary, current blood glucose prediction models have notable limitations. ARIMA struggles with nonlinearities \cite{xie2020benchmarking, mujahid2021machine}, while ML models such as support vector regression improve RMSE but fail in long-range forecasting~\cite{xie2020benchmarking, mujahid2021machine}. LSTMs and GRUs improve but suffer from irregular sample rates of various sensors~\cite{kong2024unlocking, shuvo2023deep}. Recurrent models still encounter inefficiencies for long-horizons forecasting~\cite{kong2024unlocking}. Transformers, including Gluformer, introduce multi-head attention but lack effective cross-attention for integrating multimodal data~\cite{zhu2023edge, acuna2023analyzing}. A more robust approach combining multi-scale and cross-attention is needed for accurate, real-world glucose forecasting.







