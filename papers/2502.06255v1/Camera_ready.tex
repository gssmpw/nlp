\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

\usepackage{tikz}
\usepackage{amssymb}
\usepackage{utfsym}
\usepackage{amsmath}
\usepackage{subcaption}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Towards Efficient and Intelligent Laser Weeding: Method and Dataset for Weed Stem Detection}
% \author {
%     % Authors
%     Dingning Liu\textsuperscript{\rm 1}\thanks{These authors contributed equally.},
%     Jinzhe Li\textsuperscript{\rm 1}\footnotemark[1],
%     Haoyang Su\textsuperscript{\rm 1}\footnotemark[1],
%     Bei Cui\textsuperscript{\rm 2}\thanks{Corresponding author.},
%     Zhihui Wang\textsuperscript{\rm 3},
%     Qingbo Yuan\textsuperscript{\rm 2},\\
%     Wanli Ouyang\textsuperscript{\rm 1},
%     Nanqing Dong\textsuperscript{\rm 1}\footnotemark[2]
% }

\author {
    % Authors
    Dingning Liu\textsuperscript{\rm 1}\equalcontrib,
    Jinzhe Li\textsuperscript{\rm 1}\equalcontrib,
    Haoyang Su\textsuperscript{\rm 1}\equalcontrib,
    Bei Cui\textsuperscript{\rm 2}\thanks{Corresponding author.},
    Zhihui Wang\textsuperscript{\rm 3},
    Qingbo Yuan\textsuperscript{\rm 2},\\
    Wanli Ouyang\textsuperscript{\rm 1},
    Nanqing Dong\textsuperscript{\rm 1}\footnotemark[2]
}

\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Shanghai Artificial Intelligence Laboratory\\
    \textsuperscript{\rm 2}Yazhouwan National Laboratory\\
    \textsuperscript{\rm 3}Dalian University of Technology\\
\{liudingning, dongnanqing\}@pjlab.org.cn
}

\begin{document}
\maketitle

\begin{abstract}
Weed control is a critical challenge in modern agriculture, as weeds compete with crops for essential nutrient resources, significantly reducing crop yield and quality. Traditional weed control methods, including chemical and mechanical approaches, have real-life limitations such as associated environmental impact and efficiency. 
An emerging yet effective approach is laser weeding, which uses a laser beam as the stem cutter. Although there have been studies that use deep learning in weed recognition, its application in intelligent laser weeding still requires a comprehensive understanding. Thus, this study represents the first empirical investigation of weed recognition for laser weeding. To increase the efficiency of laser beam cut and avoid damaging the crops of interest, the laser beam shall be directly aimed at the weed root. Yet, weed stem detection remains an under-explored problem. We integrate the detection of crop and weed with the localization of weed stem into one end-to-end system. To train and validate the proposed system in a real-life scenario, we curate and construct a high-quality weed stem detection dataset with human annotations. The dataset consists of 7,161 high-resolution pictures collected in the field with annotations of 11,151 instances of weed. 
Experimental results show that the proposed system improves weeding accuracy by 6.7\% and reduces energy cost by 32.3\% compared to existing weed recognition systems.
% Experimental results show that, in contrast to seminal weed recognition systems, the proposed system can efficiently improve the weeding accuracy by 6.7\% and reduce the energy cost by 32.3\%. 
% Our code and dataset are available at \url{https://github.com/open-sciencelab/WeedStemDetection}.

\end{abstract}

\begin{links}
\link{Code \& Dataset}{https://github.com/open-sciencelab/WeedStemDetection}
\end{links}

\begin{figure}[!ht]
    \centering
   
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[height=4cm,width=\textwidth]{image/fig1_a1.pdf} 
        % \caption{weed inference result and weed root.} 
        \caption{}
        \label{fig:subfig-a1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[height=4cm,width=\textwidth]{image/fig1_a2.pdf} 
        % \caption{zoomed up version with red box.}
        \caption{}
        \label{fig:subfig-a2}
    \end{subfigure}
    
   
    \begin{subfigure}[b]{1\linewidth} 
        \centering
        \includegraphics[width=\textwidth]{image/fig2_map_mnd.pdf}
        \caption{}
        \label{fig:subfig-b}
    \end{subfigure}
    
    \caption{(a) Weed detection with YOLOv7: Yellow dashed lines mark bounding box diagonals, indicating the geometric center, while the blue dot shows the ground-truth weed stem location. (b) Close-up of the red-boxed region, highlighting misalignment between the bounding box center and the ground-truth point. (c) In laser weeding, better weed detection performance (lower mAP) does not always mean better weed stem detection (lower Euclidean Distance).}
    \label{fig:map_vs_mnd}
\end{figure}

% (a) Weed detection results with YOLOv7, where yellow dashed lines mark the diagonals of the bounding box, indicating the geometric center, and the blue dot represents the ground-truth point location of the bottom of weed stem in a vertical view. (b) A close-up of the red-boxed region, showing the geometric center of the bounding box is misaligned with the ground-truth point location. This suggests that traditional object detection is not a practical solution for laser weeding. (c) For laser weeding, better weed detection performance (lower mAP) does not necessarily correlate with better weed stem detection performance (lower Euclidean Distance).

\section{Introduction}

Sustainable agricultural management is essential for addressing global hunger and achieving the United Nations' ``Zero Hunger'' goal and the principle of ``Leaving No One Behind'' (LNOB)~\cite{UnitedNations2023b}. Effective weed control plays a non-trivial role in maintaining food security, as weeds compete with crops for critical resources such as water, nutrients, and sunlight, which directly affects crop yield and quality. 

Current weed control methods are generally categorized into chemical weeding and mechanical weeding. Chemical methods usually use toxic substances to inhibit or destroy weeds at various growth stages, including those applied before or after weeds emerge. Although effective against different weed types, these methods can negatively influence the crop quality and inevitably cause chemical soil degradation, resulting in environmental pollution~\cite{zhang1996,Zhang1996b,tanveer2003}. Instead, mechanical weed control methods use machines such as mowers~\cite{pirchio2018,sportelli2020autonomous,aamlid2021robo}. Mowers often miss small weeds and can only cut the part of weed above the surface. Thus, mowers are not effective when dealing with deeply-rooted and perennial ones, leading to frequent maintenance. A desired weed control approach should take both efficiency and environmental friendliness into consideration. Laser weeding~\cite{weedlazer,LaserW2023} offers a promising alternative with aforementioned properties. It leverages high-energy and high-temperature laser beam to target and cut weeds at the stem, effectively killing the weeds. Additionally, laser weeding can be environmentally friendly if it is powered by clean energy.  

Fueled by recent advances in deep learning, weed recognition has been well studied theoretically~\cite{wu2021review,hu2024deep}.
Considering that laser weeding has the advantages in weeding efficiency and energy consumption, intelligent laser weeding seems to be a promising path for society. On the contrary, efficiency and energy become two critical issues for intelligent laser weeders. To conserve high-energy beams, the diameter of laser transmitters is much smaller in size compared with the leaves. Simply detecting or segmenting the weed is not a effective signal to transmit the laser beam as cutting leaves can not eradicate the weed. To maintain high efficiency in terms of energy usage, the laser beam shall be aimed directly at the bottom of weed stem. Meanwhile, as the laser can cause irreversible damage on crops, the detection algorithms require low false positive rate. So far, though there have been a few start-ups trying to address this challenge~\cite{weedlazer,LaserW2023}, accurately locating the weed stem remains a challenge~\cite{zhang2023}.
A visual illustration is presented in Fig.~\ref{fig:map_vs_mnd}. Based on geometric principle and agricultural knowledge, weed stems are intuitively expected to align with the geometric center of the detected bounding box in a vertical view. Though the predicted bounding boxes can achieve high mAP, the predicted location of weed stem is far from the ground truth location (Fig.~\ref{fig:map_vs_mnd}(b)). Moreover, traditional weed detection methods typically use mAP as the evaluation metric, which may not be suitable for the task of laser weeding. As shown in Fig.~\ref{fig:map_vs_mnd}(c), a method with a high mAP score may still exhibit poor root localization performance, but improving distance accuracy is crucial as it directly benefits the effectiveness of laser weeding.

To tackle the aforementioned challenges, we propose a pipeline that integrates crop and weed detection with weed stem localization into a unified end-to-end system. Specifically, we introduce an additional root coordinate regression branch within the object detection framework. The proposed system can process a sequence of images or a real-time video stream, detecting plant bounding boxes and simultaneously pinpointing weed stems for laser transmission, thereby ensuring effective weed control without damaging crops. This pipeline is simple yet robust, and can be easily implemented in object detectors.
To train and validate the proposed system in real-world conditions, and to empirically understand the task of weed recognition under the setup of laser weeding, we collect and curate the Weed Stem Detection (WSD) dataset, consisting of 7,161 high-resolution images with 11,151 annotated instances. This dataset includes bounding boxes for three crops and weeds, as well as the coordinates of weed stem. The main contributions of this work are summarized below.

\begin{itemize}
\item We provide a high-quality weed stem dataset with human annotations and the first empirical study on weed recognition for practical laser weeding, addressing a significant academic gap in laser weeding.
\item We propose an end-to-end deep learning pipeline that integrates crop and weed detection with weed stem localization and can be extended to semi-supervised learning, which can further leverage unlabeled data.
\item We experimentally demonstrate that our method is more efficient than previous detection-based methods in laser weeding by improving the weeding accuracy by 6.7\% and reducing the energy cost by 32.3\%.
\end{itemize}

\section{Related Work}
\subsection{Weed Datasets}
% Existing weed datasets focus on the tasks of weed recognition or detection~\cite{hasan2021survey, hu2024deep}. We summarize the major public datasets with human annotations in Tab.~\ref{tab:weed_datasets}. DeepWeeds~\cite{olsen2019deepweeds} provides image-level annotations for eight types of Australian weeds but without any crops. Though DeepWeeds has 17,509 images, it does not contain any information for crops, making it less useful in practical intelligent weeding. Weed-Corn/Lettuce/Radish dataset~\cite{jiang2020cnn} contains 4 species (three crops and weed) and 7,200 images with image-level annotations total. Food Crop and Weed Dataset~\cite{sudars2020dataset} contains 7 species (six crops and weed) and 1,118 images with various resolution.
% CottonWeedID15~\cite{chen2022performance} consists of 5,187 weed images specifically collected in the cotton field. CottonWeedID15 only provides image-level annotations. To overcome these limitations, the CottonWeedDet12~\cite{lu2023cottonweeddet12} and the CottonWeedDet3~\cite{rahman2023performance} datasets provide bounding box annotations, in addition to image-level annotations, respectively. CropAndWeed Dataset~\cite{steininger2023cropandweed} is the largest weed dataset with coarse machine-generated annotations, \emph{i.e.}~labeled by computer vision method. 
% However, laser weeding demands precise localization of the weed stem, and the laser transmitter's diameter must be small to deliver a high-energy beam. Therefore, the CropAndWeed dataset is not suitable for practical laser weeding.
% \textcolor{red}{There are also other datasets and methods focusing on laser weeding~\cite{zhang2024laser}, but they mainly address weed classification, detection, and segmentation, lacking the accuracy needed for precise weed stem localization.}
% To the best of our knowledge, WSD is the first dataset with human annotations for both crop and weed detection, as well as weed stem localization.

Existing weed datasets primarily address weed recognition or detection~\cite{hasan2021survey, hu2024deep}. We summarize the key public datasets with human annotations in Tab.~\ref{tab:weed_datasets}. DeepWeeds~\cite{olsen2019deepweeds} includes 17,509 images of eight Australian weeds but lacks crop data, limiting its practical weeding applications. The Weed-Corn/Lettuce/Radish dataset~\cite{jiang2020cnn} contains 7,200 images with four species (three crops and one weed), while the Food Crop and Weed Dataset~\cite{sudars2020dataset} includes 1,118 images of seven species (six crops and one weed). CottonWeedID15~\cite{chen2022performance} consists of 5,187 weed images from cotton fields with image-level annotations only. CottonWeedDet12~\cite{lu2023cottonweeddet12} and CottonWeedDet3~\cite{rahman2023performance} add bounding box annotations. The largest dataset, CropAndWeed~\cite{steininger2023cropandweed}, has coarse machine-generated labels. However, precise weed stem localization is essential for laser weeding, requiring high-quality annotations in terms of localization. Although some laser-weeding datasets exist~\cite{zhang2024laser}, they focus on classification, detection, and segmentation rather than precise stem localization. To the best of our knowledge, WSD is the first dataset with human annotations for both crop and weed detection, as well as weed stem localization.

\subsection{Weed Recognition}
Existing studies on weed recognition can be broadly categorized into four tasks: weed classification, weed object detection, weed object segmentation, and weed instance segmentation~\cite{hu2024deep}. Weed classification focuses on identifying weeds at the image level, determining whether an image contains non-crop plants. For instance, SVM classifiers have achieved about 95\% accuracy in relatively simple environments~\cite{zhang2022segmentation}, and by combining VGG with 
SVM~\cite{tao2022hybrid}, a 99\% accuracy rate has been reached in distinguishing between weeds and grapevines. Weed object detection extends beyond classification by providing bounding boxes to locate weeds within images.~\cite{parra2020edge,nasiri2022deep} Various models have been successfully applied to this task, including DetectNet~\cite{yu2019weed}, Faster R-CNN~\cite{veeranampalayam2020comparison}, and YOLOv3~\cite{sharpe2020vegetation}, all showing promising results. Weed object segmentation and instance segmentation focus on pixel-level recognition~\cite{jeon2011robust, long2015fully, you2020dnn}, offering more detailed analysis. For example, VGG-UNet has been used to segment sugar beets and weeds~\cite{fawakherji2019uav}. 
However, none of these methods can localize the weed stem, a crucial aspect of effective weed management. To address this gap, our work introduces an end-to-end framework that simultaneously detects crops and weeds while localizing the weed stem.

\begin{table*}[t!]
\centering
\setlength{\tabcolsep}{2.5mm}
\renewcommand{\arraystretch}{1.5}
% \resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
Dataset & Stem & BBox & \# Img & \# Species & \# Inst & Res\\
\hline
DeepWeeds~\shortcite{olsen2019deepweeds} & \usym{2715} & \usym{2715} & 17509 & 1 & - & 256 $\times$ 256 \\ 
Weed-Corn/Lettuce/Radish~\shortcite{jiang2020cnn} & \usym{2715} & \usym{2715} & \phantom{0}7200 & 4 & - & 800 $\times$ 600  \\
Crops and Weed Dataset~\shortcite{sudars2020dataset} & \usym{2715} & \usym{2713} & \phantom{0}1118 & 7 & - & [480, 1000] $\times$ [384, 1280] \\
CottonWeedID15~\shortcite{chen2022performance} & \usym{2715} &  \usym{2715} & \phantom{0}5187 & 1 & - & 512 $\times$ 512 \\
CottonWeedDet3~\shortcite{rahman2023performance} & \usym{2715} & \usym{2713} & \phantom{0}\phantom{0}848 & 1 & \phantom{0}1.8 $\pm$ 1.5 & 4442 $\times$ 4335 \\
CottonWeedDet12~\shortcite{lu2023cottonweeddet12} & \usym{2715} & \usym{2713} & \phantom{0}5648 & 1 & \phantom{0}1.7 $\pm$ 1.4 & 3024 $\times$ 4032 \\
\hline
WSD (Ours) & \usym{2713} & \usym{2713} & \phantom{0}7161 & 4 & 12.5 $\pm$ 7.5  & 2048 $\times$ 2048 \\
\hline
\end{tabular}
% }
\caption{Comparison between WSD dataset and existing weed datasets with human annotations. ``Stem'' indicates whether stem annotations are provided. ``\#Img'' denotes the number of images. ``\# Species'' denotes the number of species. ``\# Inst'' denotes the average number of annotations per image, along with the standard deviation. ``Res'' denotes the image resolution.}
\label{tab:weed_datasets}
\end{table*}

\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.45\linewidth}
\includegraphics[width=1\linewidth]{image/kind_example/weed_bbox.png}
\includegraphics[width=1\linewidth]{image/kind_example/maize_bbox.png}
\includegraphics[width=1\linewidth]{image/kind_example/soybean_bbox.png}
\includegraphics[width=1\linewidth]{image/kind_example/mungbean_new.png}
\centerline{(a) Raw Image}
\end{minipage}
\begin{minipage}[t]{0.45\linewidth}
\includegraphics[width=1\linewidth]{image/kind_example/weed_bbox_small_small.png}
\includegraphics[width=1\linewidth]{image/kind_example/maize_bbox_small_small.png}
\includegraphics[width=1\linewidth]{image/kind_example/soybean_bbox_small_small.png}
\includegraphics[width=1\linewidth]{image/kind_example/mungbean_bbox_small_small.png}
\centerline{(b) Zoomed-in View}
\end{minipage}
\caption{Image samples show raw images (left) and 16x zoomed sections (right), highlighting four different species.}
\label{fig:example}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{image/weed_dist.png}
\caption{Distribution of instance annotations per image: The X-axis shows the number of instances (including both bounding box and point annotations) per image, while the Y-axis indicates the corresponding image count.} 
\label{dist_weed}
\end{figure}

% The distribution of instance annotations per image is depicted. The X-axis shows the number of instances (including both bounding box and point annotations) per image, ranging from 0 to 55. The Y-axis indicates the number of images corresponding to each specific number of instances.

\section{Weed Stem Detection Dataset}
% This section provides the details of data collection and annotation for Weed Stem Detection (WSD) dataset.

\subsection{Data Collection}
The standard RGB images are collected by a custom-built autonomous vehicle equipped with Teledyne FLIR BFS-U3-123S6C-C, a high-resolution imagery sensor. Each image has a resolution of $2048 \times 2048$. 
The sensor is embedded in the autonomous vehicle, making the sensor at a relatively fixed height above the surface, which is one meter for the prototype vehicle. The images are captured in three different experimental fields planted with three different crops: maize, soybean, and mungbean, respectively. 
We intentionally planted weed seeds in the field at staggered intervals, resulting in weeds at various growth stages. All crops, however, are at the seedling stage, 30 days after sowing--an important period for weeding.

\subsection{Data Annotation}
We deployed LabelImg\footnote{\url{https://pypi.org/project/labelImg/}}, a graphical image annotation tool. The human annotators can label object bounding boxes in images with LabelImg, which saves the annotation details in XML files. Three professional agronomists with advanced graduate degrees and field experience were hired to complete the annotation. The annotation process was completed in two steps. First, the bounding boxes of crop and weed were annotated. Then, weed stem locations were annotated in a point coordinate fashion. All final annotations were verified by all three agronomists to achieve consensus. Any discrepancies among the human annotators were resolved through re-annotation to ensure reliability. Fig.~\ref{fig:example} shows four annotated images with zoomed-in visualization. 

\subsection{Dataset Statistics}
There are 7,161 images in total, with 1,556 annotated and 5,605 unannotated images. The inclusion of unannotated images allows the dataset to be extended for semi-supervised learning scenarios. The distribution of instance annotations per image is illustrated in Fig.~\ref{dist_weed}. It is worth mentioning that the annotation is time-consuming. On average, it takes approximately 135 seconds to label a weed instance. The statistics of WSD are summarized in Tab.~\ref{tab:weed_info}.

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{1.5mm}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{ccccc}
         \hline
         Class &  \# Instances &  Share  & \# Images &  Time (s)\\ 
         \hline
         Weed & 11151 &  57.3\% & 1379 & 135 $\pm$ 45 \\
         Maize & \phantom{0}5808 & 29.9\% & 1175 & 105 $\pm$ 15 \\
         Soybean & \phantom{0}1624 & \phantom{0}8.4\% & \phantom{0}268 & \phantom{0}60 $\pm$ 10 \\
         Mungbean & \phantom{0}\phantom{0}848 & \phantom{0}4.4\% & \phantom{0}100 & \phantom{0}75 $\pm$ 15 \\
         \hline
    \end{tabular}
    \caption{Statistics of Weed Stem Detection Dataset. ``Instances'' indicates the number of bounding boxes with additional point annotations. ``Share'' represents the percentage of instances in this category. ``Images'' is the number of images containing this category. ``Time'' refers to the average $\pm$ standard deviation of annotation time in seconds by professional agronomists.}
    \label{tab:weed_info}
\end{table}

% Statistics of Weed Stem Detection Dataset. ``Instances'' refers to the number of bounding boxes for this category where weed has additional point annotations along with the bounding boxes. ``Share'' denotes the share of the number of instances of this category. ``Images'' denotes the number of images containing this category. ``Time'' denotes the total annotation time by professional agronomists for this category in seconds, shown as average $\pm$ standard deviation.

\section{Method}
\begin{figure*}[ht]
  \centering
    \includegraphics[width=1\linewidth]{image/pipeline.png}
    \caption{The pipeline of intelligent laser weeding. The autonomous vehicle captures the image (or video frame). The proposed neural network infers the class and location of each crop and weed.  Upon identifying a weed, the model also outputs the stem location of the weed, followed by a laser beam cut.}
    \label{fig:pipeline}
\end{figure*}

The proposed laser weeding pipeline is depicted in Fig.~\ref{fig:pipeline}. The autonomous vehicle first captures images that include both crops and weeds. These images are then processed by a neural network to detect and localize weed stems. Upon detection, a laser beam is emitted to cut the weed stems. This section details the integration of stem regression within a pre-trained object detection neural network and the subsequent enhancement of its performance using semi-supervised learning.

\subsection{Weed Stem Regression}
To accurately localize weed stems, we augment the pre-trained object detection neural network $NN(\cdot)$ with an additional stem coordinate regression head, formulated as:
\begin{gather}
E_i = NN(I_i) \\
\hat{y_i} = Conv(E_i)
\end{gather}
where $I_i$ represents the $i$-th input image, $E_i$ denotes the extracted image embedding, $\hat{y_i}$ is the predicted stem coordinate, and $Conv(\cdot)$ is the regression head. The regression loss $L_{reg}$ is computed as:
\begin{equation}
L_{reg} = \frac{1}{n} \sum_{i=1}^{n}MSE(y_{i}, \hat{y}_{i}) 
\end{equation}
$MSE(\cdot)$ represents the Euclidean Distance calculation, $y_{i}$ is the ground truth weed stem coordinate, and only weed coordinates are used in calculating the regression loss. To jointly optimize bounding box detection and weed stem regression, we combine the regression loss $L_{reg}$ with the classification loss $L_{cls}$ and the bounding box detection loss $L_{bbox}$ as follows:
\begin{gather}
L = \alpha \cdot L_{cls} + \beta \cdot L_{bbox} + \gamma \cdot L_{reg}
\end{gather}
where $\alpha$, $\beta$, and $\gamma$ are hyper-parameters that balance the contributions of the different losses.

\subsection{Extension to Leverage Unlabelled Images}
Labeling images in real-world scenarios is labor-intensive. To reduce annotation costs and simultaneously leverage unlabeled images to enhance model performance, we employ a teacher-student framework for semi-supervised learning~\cite{kingma2014semi,zhai2019s4l,berthelot2019mixmatch,xu2021end}. As illustrated in Fig.~\ref{fig:semi-supervised}, pseudo labels for the unlabeled data $\mathcal{D}_u$ are generated using a teacher model. The student model is then trained on both the labeled data $\mathcal{D}_l$ and the pseudo-labeled data $\mathcal{D}_p$.

\subsubsection{Pseudo Label Generation}
To effectively utilize the abundant unlabeled images, we first fine-tune a teacher model $Teacher(\cdot)$ based on the pre-trained neural network with the combined loss $L$. The fine-tuned teacher model classifies unlabeled images, assigning pseudo labels to predictions with confidence higher than the threshold $\tau$. Confidence $ConfScore$ is calculated as:
\begin{gather}
ConfScore = Max(Softmax(Teacher(E_u)))
\end{gather}
where $E_u$ denotes unlabeled image embeddings (subscripts are omitted for simplicity). Since precise weed coordinate prediction is crucial, we use ground-truth weed image embeddings as anchors to filter out low-quality predictions. Specifically, we extract weed embeddings $E_l^w$ from labeled data and store them in a weed bank. The cosine similarity between predicted weed embeddings $E_u^w$ and all pre-extracted weed embeddings $E_l^w$ is then calculated:
\begin{gather}
SimScore = CosineSimilarity(E_l^w, E_u^w)
\end{gather}
Predictions with $SimScore$ higher than the threshold $\xi$ are assigned as pseudo labels. Finally, the student model is trained on both labeled data $\mathcal{D}_l$ and pseudo-labeled data $\mathcal{D}_p$. Notably, weak and strong augmentations are applied to each unlabeled image: the weakly augmented images are fed into the student network, while the strongly augmented images are processed by the teacher network. Weak augmentations include adjustments to brightness and contrast, while strong augmentations additionally involve cropping and flipping.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{image/semi-supervised.png}
    \caption{Overview of semi-supervised learning process. Pseudo labels are first generated for unlabeled data using a teacher model, followed by training a student model with both labeled and pseudo-labeled data. ``Conf'' represents the confidence score for classification, and ``Sim'' denotes the cosine similarity between extracted ground-truth weed embeddings and predicted weed embeddings, used to filter out low-quality weed localization. $\tau$ and $\xi$ are hyper-parameters.}
    \label{fig:semi-supervised}
\end{figure}

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{1.5mm}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ccccc}
\hline
Model & Dist$\downarrow$ & FP & Accuracy & Cost \\
\hline
Detection & 2.9770 & 0 & 75.37\% & 1.55 \\
Ours & 2.4838 & 0 & 80.42\% & 1.05 \\
\hline
\end{tabular}
\caption{Effect of stem regression. ``FP'' denotes false positive rate, \emph{i.e.} the crops are identified as the weeds. ``Accuracy'' denotes the weeding accuracy. ``Cost'' denotes the energy cost where the unit cost denotes the hypothetical minimum energy cost with ground truth-level prediction, \emph{i.e} one weed only requires one shot.}
\label{tab:detection_vs_root_regression}
\end{table}

\begin{figure*}[!ht]
    \centering
    
    % First row with "Detection" label
    \raisebox{0.3\height}{\rotatebox{90}{Detection}\hspace{1.5mm}}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120121538_bbox.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120709705_bbox.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120733706_bbox.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120740705_bbox.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0661_bbox.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0663_bbox.png}
    \end{subfigure}

    % Second row with "Ours" label
    \raisebox{1.2\height}{\rotatebox{90}{Ours}\hspace{1.5mm}}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120121538_points.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120709705_points.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120733706_points.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120740705_points.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0661_points.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0663_points.png}
    \end{subfigure}

    % Third row with "Ground Truth" label
    \raisebox{2\height}{\rotatebox{90}{GT}\hspace{1.5mm}}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120121538_gt.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120709705_gt.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120733706_gt.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/Image_20231206120740705_gt.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0661_gt.png}
    \end{subfigure}%
    \begin{subfigure}{0.16\linewidth}
        \includegraphics[width=\linewidth]{image/weed_case/DJI_0663_gt.png}
    \end{subfigure}
    
    \caption{Qualitative comparison of weed detection results between the vanilla detection method and our method. The results of our method (second row) shows higher true positive rate than the vanilla detection method (first row), especially when a weed spread and looks like multiple weeds, thus conserving the energy.}
    \label{fig:inference_results}
\end{figure*}

\section{Experiments}
\subsection{Experimental Setup}
\subsubsection{Implementation}
% We empirically evaluate our method on the WSD dataset. We use $80\%$ of data for training, $10\%$ of data for validation, and $10\%$ of data for testing. The autonomous vehicle has limited hardware capacity. To ensure light-weight deployment and real-time inference, we use YOLOv7~\cite{wang2023} as the baseline detection model due to its superior performance among existing object detection methods~\cite{dang2023yoloweeds, rahman2023performance}. We train all models with a SGD optimizer with a learning rate of 1e-3 for 300 epochs. We set the loss weights $\alpha$, $\beta$, and $\gamma$ to 0.2, 0.3, and 0.5, respectively. For semi-supervised learning, we adopt a dynamic updating mechanism with EMA, where the smoothing factor is set to 0.9, the confidence threshold is set to 0.5 and the cosine similarity threshold to 0.4. All hyper parameters are determined on the basis of empirical results. The experiments are conducted on a single NVIDIA Tesla A100 80G GPU.

We conduct empirical evaluations of our method on the WSD dataset, using $80\%$ of the data for training, $10\%$ for validation, and $10\%$ for testing. Given the hardware limitations of the autonomous vehicle, we prioritize lightweight deployment and real-time inference by selecting YOLOv7~\cite{wang2023} as the baseline detection model due to its superior performance among existing object detection methods~\cite{dang2023yoloweeds, rahman2023performance}. All models are trained using an SGD optimizer with a learning rate of 1e-3 for 300 epochs. We set the loss weights $\alpha$, $\beta$, and $\gamma$ to 0.2, 0.3, and 0.5, respectively. For semi-supervised learning, we adopt a dynamic updating mechanism with EMA, where the smoothing factor is set to 0.9, the confidence threshold is set to 0.5 and the cosine similarity threshold to 0.4. To be mentioned, all hyper parameters are determined on the basis of empirical results. The experiments are conducted on a single NVIDIA Tesla A100 80G GPU.
 
\subsubsection{Evaluation Metric}
While mAP is a well-established metric that aggregates precision and recall to provide an overall measure of detection performance, it primarily evaluates the presence and classification of objects rather than their precise positioning. As shown in Fig.~\ref{fig:map_vs_mnd}, mAP is not a robust measure for the task of interest.
Instead, we evaluate using Euclidean Distance (Dist), where a lower value indicates higher accuracy in weed stem localization.


\subsection{Results}
\subsubsection{Effect of Stem Regression}
% As the main result in this empirical study, we first validate the effectiveness of stem regression. As shown in Tab.~\ref{tab:detection_vs_root_regression}, stem regression can significantly lower the Dist value. We further design a real-world simulated experiment to evaluate the efficiency of the proposed system. Under the same experimental setup, the vanilla detection method and our method run over the same amount of weeds, independently. For vanilla detection method, the geometric center of the predicted bounding box for weed is considered as the predicted stem coordinates. We value two aspects, weeding accuracy (the percentage of weeds eradicated) and energy consumption (the energy usage, or equivalently the number of shots of laser beam). Tab.~\ref{tab:detection_vs_root_regression} shows that the proposed method can reduce the energy cost by up to 32.3\% while still improving the accuracy by 6.7\%. Notably, neither method misidentifies crops as weeds. We conjecture that the visual difference between crops and weeds is significant. The qualitative comparisons on weed detection and weed stem localization are presented in Fig.~\ref{fig:inference_results} and Fig.~\ref{fig:comp_with_gt_center}.

In this empirical study, we first validate the effectiveness of stem regression, which significantly reduces the Dist value (Tab.~\ref{tab:detection_vs_root_regression}). We then design a real-world simulated experiment to assess the efficiency of the proposed system. Under identical experimental conditions, the vanilla detection method and our method process the same number of weeds independently. For the vanilla method, the geometric center of the predicted bounding box is used as the stem coordinate. We evaluate two aspects: weeding accuracy (percentage of weeds eradicated) and energy consumption (laser shots). As shown in Tab.~\ref{tab:detection_vs_root_regression}, our method reduces energy cost by up to 32.3\% while improving accuracy by 6.7\%, with no misidentification of crops as weeds. This suggests a clear visual difference between crops and weeds. Qualitative comparisons of weed detection and stem localization are shown in Fig.~\ref{fig:inference_results} and Fig.~\ref{fig:comp_with_gt_center}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        % \textbf{Detection}
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120903530_bbox_gt.png}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        % \textbf{Ours}
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120903530_points_gt.png}  
    \end{subfigure}

    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120059540_bbox_gt.png}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120059540_points_gt.png}
    \end{subfigure}

    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120909532_bbox_gt.png}
        \caption{Detection}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{image/gt_vs_points-bbox/Image_20231206120909532_points_gt.png}
        \caption{Ours}
    \end{subfigure}
    \caption{Qualitative comparison of stem localization results between the vanilla detection method (a) and our method (b). Green and red points denote the prediction and the ground truth, respectively. Our method consistently outperforms the vanilla detection method.}
    \label{fig:comp_with_gt_center}
\end{figure}

\subsubsection{Necessity of Object Detection}
With WSD, another learning choice is to regress on weed stem coordinates directly, without the object detection framework. As shown in Tab.~\ref{tab:add_stem_unlabel_det}, the integration of stem regression and weed detection is important for the task of interest. Specifically, adding weed detection reduces the distance error from 4.3306 to 2.4838, demonstrating a significant performance gain. 

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{3mm}
\renewcommand{\arraystretch}{1.5} 
\begin{tabular}{ccc|c}
\hline
Stem Reg. & Det. & Unlabeled & Dist$\downarrow$ \\
\hline
\checkmark & &  & 4.3306 \\
\checkmark & \checkmark &  & 2.4838 \\
\checkmark & \checkmark & \checkmark & \textbf{2.1485} \\
\hline
\end{tabular}
\caption{Analysis on learning components. ``Stem Reg.'' denotes stem regression. ``Det.'' denotes standard object detection. ``Unlabeled'' denotes unlabeled data, which means semi-supervised learning.}
\label{tab:add_stem_unlabel_det}
\end{table}

\subsubsection{Study on Semi-Supervised Learning} 
Using the same setup as above, we evaluated the effectiveness of our semi-supervised learning method, incorporating unlabeled images during training. With semi-supervised learning, the Dist value was further reduced to 2.1485.
Additionally, we replace the detection backbone from YOLOv7 to YOLOv8~\cite{reis2023}. Fig.~\ref{compare_weed_bank} suggests that the extension to semi-supervised learning is model agnostic.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{image/compare_weed_bank.png}
\caption{Effect of semi-supervised learning. Semi-supervised learning consistently improves the model performance. ``SSL'' denotes semi-supervised learning.} 
\label{compare_weed_bank}
\end{figure}



\subsubsection{Ablation Study on Detection Backbone}
% It is worth mentioning that, though YOLOv8 has superior performance than YOLOv7 in general object detection, it has lower performance in Fig.~\ref{compare_weed_bank}. We hypothesize that the detection backbone makes a difference.
% Recent studies~\cite{irjmets2022,baeldung2023,keylabs2023,roboflow2023} have shown that single-stage models, particularly YOLO family, are better suited for tasks requiring both high-speed inference and competitive accuracy compared with Faster-RCNN~\cite{ren2016faster} and SSD~\cite{DBLP:conf/eccv/LiuAESRFB16}. Therefore, we consider the recent YOLO models, such as YOLOv7, YOLOv8, and YOLOv10~\cite{wang2024} as the backbones. These models are respectively anchor-based, anchor-free, and anchor-free.
% A comparative analysis is performanced in Tab.~\ref{tab:detection_model_compare}.

% With the incorporation of stem prediction, all models achieved a 0\% False Positive rate, ensuring no seedlings were mistakenly identified as weeds. This outcome proves the effectiveness of our approach in minimizing the risk of crop damage during automated weeding, making the process both more accurate and safer for young plants.
% Despite expectations that newer models like YOLOv8 and YOLOv10 would perform better, their anchor-free approach caused significant drift in stem predictions. We conjecture that YOLOv7 is an anchor-based method, which ensures that loss calculation remains confined to their respective grid and prevents any drift. The results in Tab.~\ref{tab:detection_model_compare} further validates the choice of YOLOv7 as the baseline detection model, as it balances the computational efficiency and accuracy.

Although YOLOv8 generally outperforms YOLOv7 in object detection, it shows lower performance in Fig.~\ref{compare_weed_bank}. We hypothesize that the detection backbone plays a role in this discrepancy. Recent studies~\cite{irjmets2022,baeldung2023,keylabs2023,roboflow2023} indicate that single-stage models, particularly those in the YOLO family, excel in tasks requiring both high-speed inference and competitive accuracy, compared to Faster-RCNN~\cite{ren2016faster} and SSD~\cite{DBLP:conf/eccv/LiuAESRFB16}. We evaluate YOLOv7, YOLOv8, and YOLOv10~\cite{wang2024}, which are anchor-based, anchor-free, and anchor-free, respectively. A comparative analysis is shown in Tab.~\ref{tab:detection_model_compare}.

With stem prediction incorporated, all models achieved a 0\% False Positive rate, ensuring no seedlings were misidentified as weeds. This confirms the effectiveness of our approach in reducing crop damage risk during automated weeding, enhancing both accuracy and safety for young plants. Despite expectations that YOLOv8 and YOLOv10 would perform better, their anchor-free approach resulted in significant drift in stem predictions. In contrast, YOLOv7â€™s anchor-based method prevents drift by keeping loss calculation confined to the respective grid. The results in Tab.~\ref{tab:detection_model_compare} further validate YOLOv7 as the baseline detection model, balancing computational efficiency and accuracy.

\begin{table}[ht]
\centering
\setlength{\tabcolsep}{1mm}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{lcccccc}
\hline
Model & \#Param $\downarrow$ & Time(ms)$\downarrow$ & FP$\downarrow$ & Dist$\downarrow$ \\
\hline
YOLOv10-M & \phantom{0}\textbf{26M} & 130.4 & 0 & 3.2691 \\
YOLOv8-M & \phantom{0}\textbf{26M} & 407.3 & 0 & 3.3219\\
YOLOv8-L & \phantom{0}43M & 597.2 & 0 & 3.1473 \\
YOLOv7-X & 142M & 119.5 & 0 & \textbf{2.4426} \\
YOLOv7 & \phantom{0}37M & \phantom{0}\textbf{60.2} & 0 &\textbf{2.4838} \\
\hline
\end{tabular}
\caption{Peformance comparison among different YOLO backbones. ``\#Param'' denotes the number of parameters. ``Time(ms)'' denotes the inference time. ``FP'' denotes the false positive rate.}
\label{tab:detection_model_compare}
\end{table}

\subsubsection{Effect of the Classifier Threshold}
% As shown in Tab.~\ref{tab:effect_threshold}, although we adopt the same confidence parameter of 0.15 as most zero-shot object detection models during the inference stage, this value is not set as the minimum zero-FP threshold. We further validate the minimum confidence threshold specific to our dataset. It can be observed that when the confidence threshold is reduced to 0.056, the FP rate already drops to zero.
We set a confidence threshold of 0.15 during inference, consistent with most zero-shot object detection models. 
As shown in Tab.~\ref{tab:effect_threshold}, the minimum zero-FP threshold is 0.056.

\begin{table}[ht]
\centering
\begin{tabular}{c|c}
\hline
Threshold & FP \\
\hline
0.150 & 0.000 \\
0.056 & 0.000 \\
0.055 & 0.005 \\
0.050 & 0.020 \\
\hline
\end{tabular}
\caption{Effect of the confidence threshold: raising the threshold can enhance generalization in zero-shot scenarios.}

% Effect of the Classifier Threshold. A higher threshold can improve generalization performance in zero-shot scenarios.

\label{tab:effect_threshold}
\end{table}

\section{Conclusion}
In this work, we propose an end-to-end pipeline that unifies crop and weed detection and weed stem localization, which shows improved accuracy and reduced energy cost. This study not only serves an empirical study of practical weed recognition, but also poses a promising research direction on intelligent laser weeding.

\section{Acknowledgments}
This work is supported by Shanghai Artificial Intelligence Laboratory.

% \bibliography{main}
% \clearpage
\bibliography{aaai25}

% \twocolumn[{
% \begin{center}
%     \vspace{1em}
%     \textbf{\LARGE Reproducibility Checklist}
%     \vspace{2em}
% \end{center}
% }]

% % %%% BEGIN INSTRUCTIONS %%%
% % The checklist follows the references.  Please
% % read the checklist guidelines carefully for information on how to answer these
% % questions.  For each question, change the default \answerTODO{} to \answerYes{},
% % \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% % justification to your answer}, either by referencing the appropriate section of
% % your paper or providing a brief inline description.  For example:
% % \begin{itemize}
% %   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
% %   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
% %   \item Did you include the license to the code and datasets? \answerNA{}
% % \end{itemize}
% % Please do not modify the questions and only use the provided macros for your
% % answers.  Note that the Checklist section does not count towards the page
% % limit.  In your paper, please delete this instructions block and only keep the
% % Checklist section heading above along with the questions/answers below.
% % %%% END INSTRUCTIONS %%%

% \begin{enumerate}

% \item This paper
% \begin{enumerate}
%   \item Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA)
  
%   Yes.
%   \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no)
  
%    Yes.
%   \item Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes/no)
  
%    Yes.
% \end{enumerate}

% \item Does this paper make theoretical contributions? (yes/no)

% No.

% \item Does this paper rely on one or more datasets? (yes/no)

% Yes.
% \begin{enumerate}
%   \item A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA)
   
%    Yes.
%   \item All novel datasets introduced in this paper are included in a data appendix. (yes/partial/no/NA)

%    No, novel datasets introduced in this paper will be made publicly available upon publication of the paper.
   
%    \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes/partial/no/NA)

%    Yes.
   
%    \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations. (yes/no/NA)

%    Yes. 
   
%    \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available. (yes/partial/no/NA)

%    Yes. 
   
%   \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (yes/partial/no/NA)

%   Yes.
% \end{enumerate}

% \item Does this paper include computational experiments? (yes/no)

% Yes.

% \begin{enumerate}
% \item Any code required for pre-processing data is included in the appendix. (yes/partial/no).

% No, but we will release all codes after the review of our paper.
% \item All source code required for conducting and analyzing the experiments is included in a code appendix. (yes/partial/no)

% No, but we will release all codes after the review of our paper.
% \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes/partial/no)

% Yes.
% \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from. (yes/partial/no)

% Yes. 
% \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes/partial/no/NA)

% Yes.
% \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes/partial/no)

% Yes.
% \item This paper formally describes the evaluation metrics used and explains the motivation for choosing these metrics. (yes/partial/no)

% Yes.
% \item This paper states the number of algorithm runs used to compute each reported result. (yes/no)

% Yes.
% \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes/no)

% Yes.
% \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes/partial/no)

% Yes.
% \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments. (yes/partial/no/NA)

% Yes.
% \item This paper states the number and range of values tried per (hyper-) parameter during the development of the paper, along with the criterion used for selecting the final parameter setting. (yes/partial/no/NA)

% Yes.



% \end{enumerate}

% \end{enumerate}

\end{document}