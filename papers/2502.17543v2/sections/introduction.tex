\section{Introduction}
Large language models (LLMs) are considered to be a promising foundation for autonomous agents -- systems capable of achieving goals independently with minimal human supervision or intervention. A crucial requirement for such systems is the ability to interact effectively with external environments and gather the information necessary to achieve their objectives. This capability can be formalized as solving sequential decision-making problems or performing reinforcement learning (RL) with language models as the agent. However, two challenges hinder the development of these interactive capabilities. First, most naturally occurring data lacks the structure and context needed to model interactions. Second, directly deploying models into the real world to collect interaction data can produce critical errors, which is expensive and potentially risky.

Given the impracticality of direct deployment in the wild, a natural alternative is to generate interaction data synthetically.
Although generating synthetic data for every possible problem is infeasible, LLMs possess the capacity for \emph{in-context learning} (ICL), which allows them to adapt to new tasks with minimal demonstrations~\citep{brown2020language}.
Instead of teaching the model to do all the interaction tasks that we care about, we should instead teach the model \emph{in-context reinforcement learning}~\citep{laskin2022context} so that the model can solve new problems without being trained on them a priori. 
It shifts the focus from training the model on particular problems to training it on the general process of solving problems.
This paradigm shares similarities with the supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages of training a language model (vs pretraining) where only a relatively small number of examples is needed to produce a model that can generate responses to a wide range of queries that they are not trained on.
Our approach is also closely related to the principles of \emph{meta reinforcement learning}~\citep{beck2023survey}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.93\linewidth]{figures/paprika-short-v2.pdf}
    \vspace{-0.3cm}
    \caption{\footnotesize Overview of \ours{}. We design a diverse set of tasks where an LLM agent needs strategic information gathering to succeed, then train an LLM on self-generated data to prefer higher performing trajectories. The resulting behavior learned by \ours{} can transfer zero-shot to unseen tasks, showcasing its potential to build general decision making agents.
    }
    \label{fig:paprika}
    \vspace{-0.2cm}
\end{figure*}

In this work, we explore the feasibility of teaching LLMs to perform in-context RL that generalizes across different tasks. 
We begin by designing a diverse suite of textual decision-making tasks that require active information gathering and decision-making based on interaction outcomes.
Using a base model, we generate interaction trajectories and assign scores based on their success in achieving the tasks' objectives.
We then apply a sequential variant of Direct Preference Optimization~\citep[DPO]{rafailov2024direct} to increase the relative likelihood of successful trajectories.
Unlike traditional training where computational costs are dominated by model updates, our approach's primary bottleneck lies in sampling useful interaction data. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential.

We refer to the overall framework as \ours{}\footnote{The name is inspired by the movie ``Paprika'' (2006), where a dream detective navigates vast and strange dream worlds to solve different mysteries.}.
Our results demonstrate that training on different subsets of these tasks improves the performance of the model on unseen tasks. More broadly, our result highlights the potential of using synthetic data to learn in-context RL which would equip LLMs with the capability to interact with the world and solve different decision-making problems without requiring task-specific fine-tuning.

