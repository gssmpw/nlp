\section{Related Works}


\paragraph{LLM alignment.} Alignment or post-training is a crucial step for creating helpful LLM assistant. Existing post-training pipeline typically involves instruction tuning and then reinforcement learning from human feedback~\citep[RLHF]{christiano2017deep} where one either performs RL against a reward model trained on human preference data via Proximal Policy Optimization~\citep[PPO]{schulman2017proximal} or sidesteps reward model training via Direct Preference Optimization~\citep[DPO]{rafailov2024direct}. Most methods focus on \textit{single-turn} interactions where the model generates a single response to a query. We focus on the \emph{multi-turn} setting where the agent has to interact with an environment iteratively, similar to~\citet{rafailov2024rqlanguagemodel}. There are a few existing environments and datasets that focus on multi-turn interactions~\citep{abdulhai2023lmrl, sun2023parrot, kwan2024mt, wang2024mint}. LMRL-Gym~\citep{abdulhai2023lmrl} implements a suite of textual RL environment, some of which we build on. Concurrent work such as~\citet{narayanan2024aviarytraininglanguageagents} has designed environments based on scientific tasks (such as molecule cloning and protein stability) for LLMs to interact with and showed that behavior cloning and expert iteration~\citep{anthony2017thinkingfastslowdeep,anthony2019policygradientsearchonline,havrilla2024teachinglargelanguagemodels} can improve an LLM's multi-turn interaction capabilities on these scientific tasks. Most of these environments focus on interactions with humans. Rather than any particular tasks, we focus on evaluating LLMs' general ability to solve a sequential decision making problem where the agent needs to explore (e.g., gather necessary information for a task) and exploit (e.g., solving a task in an efficient manner).



\paragraph{In-context reinforcement learning.} In-context learning (ICL) is the ability where LLMs can learn a new task from a small number of demonstrations without any gradient update~\citep{brown2020language}. Existing ICL usually focuses on a single-turn interaction. We focus on in-context reinforcement learning~\citep{laskin2022context, raparthy2023generalization, lee2024supervised, lin2024transformers} instead. Existing work in this field has focused on environments where RL is conventionally applied (e.g., grid world, bandits, and maze), and the training data are generated by either random policies or pre-existing RL algorithms. In comparison, we focus on diverse environments and study how well the decision making abilities generalize to completely new environments. Concurrent work has also studied improving LLMs' information seeking abilities~\citep{li2025aligningllmsaskgood} for medical reasoning, whereas we work on general information seeking abilities applicable to a diverse range of tasks. Moreover,~\citet{harris2025uselargelanguagemodel} has studied using an LLM to assist a decision-making agent navigate exploration-exploitation tradeoff, whereas we use an LLM directly as the decision making agent and teach it this capability.

\paragraph{Curriculum learning in RL.}
Curriculum learning~\citep{bengio2009curriculum} shows the data to the model in a non-uniform order. This idea is inspired by the fact that humans tend to learn skills in a sequential order~\citep{skinner1958reinforcement}, and is particularly appealing for RL because learning easier tasks first could build scaffold toward solving difficult tasks that the agent could not solve otherwise~\citep{andrychowicz2017hindsight, florensa2017reverse, fang2019curriculum, portelas2020teacher}. Concurrent work such as \citet{foster2025learningreasonfrontierlearnability} has studied curriculum learning for training LLMs to improve their reasoning capabilities. While their work requires generating rollouts per each example to determine the learnability, we show that given access to some grouping metadata over the training tasks, one can design an effective curriculum using only a constant number of rollouts generated from each task group.
Another related line of work is environment design where a second process controls the distribution over different environments or directly generates the details of the environments in a procedural manner to maximize some notion of learning progress~\citep{wang2019paired, dennis2020emergent, jiang2021prioritized, jiang2021replay, bruce2024genie}.
Since this is a field of extensive existing literature, we refer the interested reader to \citet{portelas2020automatic} for a comprehensive survey.

\paragraph{Curiosity.} The concept of curiosity has been used in many different machine learning contexts. A popular notion of curiosity is \emph{intrinsic motivation}, where the agent is driven by an exploration bonus that is not necessarily related to the task to be achieved~\citep{schmidhuber1991curious, schmidhuber2007godel}. Many works build on this notion to handle problems with sparse reward or no reward at all~\citep{pathak2017curiosity, eysenbach2018diversity, burda2018exploration, sharma2019dynamics, pathak2019self}. The curiosity in this work differs from intrinsic motivation in that we focus on gathering only the information required to solve a given task rather than all the knowable information. This is closer in spirit to the original exploration-exploitation trade-off in reinforcement learning~\citep{sutton1998reinforcement, auer2002finite, thompson1933likelihood}. The goal is to explore to the extent that the problem can be solved but not over-explore at the cost of efficiency. Most existing works based on this principle are \emph{tabula rasa}~\citep{osband2016deep, chen2017ucb}. \ours{} differs from these approaches by learning good exploration strategies from trajectories from many different environments to make exploration on a new problem more efficient. This can be thought of as a form of \emph{amortized exploration}.
