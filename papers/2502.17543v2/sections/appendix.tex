\newpage
\appendix
\onecolumn

\section{Details on Task Design} \label{section:appendix_environment_design}

\subsection{Summary of Task Groups}

\paragraph{Twenty questions: } 
Twenty questions challenges the agent to identify a secret topic by asking up to 20 yes-or-no questions. The goal is to guess the topic in as few questions as possible by interpreting previous answers and strategizing to maximize information gained. Twenty questions has been studied in prior benchmarks such as LMRL-Gym~\citep{abdulhai2023lmrl}: here we expand upon their environment with a more diverse and difficult set of secret topics. Our secret topics come from a diverse range of scenarios, including famous people, historical events, scientific concepts, locations, etc. Each secret topic corresponds to a task, and we have generated a set of 1499 train and 367 test tasks. In order to generate a diverse set of topics, we use prompting techniques from GenQA~\citep{chen2024genqageneratingmillionsinstructions} on GPT-4o-mini. The topics to guess in our training and test sets are distinct from one another and also the set of topics included in LMRL-Gym (159 topics), which use as an additional evaluation set. We use GPT-4o-mini~\citep{hurst2024gpt,openai2024gpt4technicalreport} as the task environment to provide yes/no answers at every turn, and also as a judge to make sure task success label is correct. We use strict string matching to make sure the intermediate observations are only `yes', `no' or `Goal reached'. We also maintain train and test set separation to accurately test generalization unlike previous works.

\paragraph{Guess my city: } Following LMRL-Gym, this task group requires the agent to guess a secret city after asking a maximum of 20 questions. 
But unlike twenty questions, the questions here can be broader than just yes/no questions, for example, ``\emph{What is your city most popular for?}'' so long as the answer to the question does not reveal the name of the city directly. We generated a train set of 500 and test set of 185 distinct cities using GPT-4o-mini and GenQA~\citep{chen2024genqageneratingmillionsinstructions} prompting techniques. In addition, we also evaluated our models on the list of 91 cities from LMRL-Gym, which does not overlap with our training and test set. We maintain train and test set separation.

\paragraph{Customer service: } In this task group, we test for efficient directed exploration ---- the LLM must act as a support agent who asks maximally informative questions to diagnose problems and minimize the number of interactions needed to resolve the customer's query. To do so, we simulate realistic troubleshooting scenarios ranging from electronic device issues to automobile maintenance. We use GPT-4o-mini to simulate a customer with limited technical expertise, and use another LLM to act as a customer service agent whose role is to listen to the responses from the customer and suggest a sequence of actions that lead to solving the customer's problem in as few turns as possible. The customer service troubleshooting scenarios are generated by GPT-4o-mini, using prompting techniques from GenQA.

\paragraph{Murder mystery: } Text-based interactive fiction (IF) environments can be a good benchmark to test LLMs' decision making and interaction abilities. Inspired by~\citet{hausknecht2020interactivefictionenvironmentscolossal}, we design our murder mystery task group, where an LLM is given a crime scene with a possible list of suspects, witnesses, and clues, and it needs to take actions to uncover more information to successfully determine the culprit.  The environments provided in ~\citet{hausknecht2020interactivefictionenvironmentscolossal} proved difficult to incorporate directly in our setup, since they have a predefined list of valid actions and uses text-based parsing on the LLM generation to match against the list, making it difficult for LLMs to play the games. Instead, we use GPT-4o-mini to simulate the environment that can provide dynamic feedback to the agent's actions. The murder mystery scenarios are generated by GPT-4o-mini, using prompting techniques from GenQA.

\paragraph{Wordle: } Wordle tests an LLM's deductive reasoning abilities. The agent must guess a secret 5-letter word within 6 attempts. After each guess, the environment provides feedback for each letter: correct letter in correct position, correct letter in wrong position, or letter not in the word.
The agent must use this feedback strategically to maximize information gained with each guess. We found that LLMs like GPT-4o-mini cannot generate accurate environment feedback for Wordle, so we use hardcoded rules to generate it instead. We also saw that prompting the LLM agent to do chain-of-thought reasoning before outputting its final guess significantly improves its performance, so we use that here unlike the environments above. The secret words are generated by looking at 5-letter words from an English dictionary.

\paragraph{Cellular Automata: } A key trait of LLM agents is the ability to code and refine based on interpreter feedback. To model this, we create a cellular automata-based environment. Here, a binary string (e.g., 1010) represents cells, and a transition rule defines a cell’s next state based on itself and its neighbors (e.g., 100: 1 means a 0 cell with 1 and 0 neighbors turns into 1). We randomly select a transition rule (one of 256) and up to three input strings and their corresponding outputs generated by the transition rule. The LLM must infer the rule by analyzing input-output pairs. If its guess generates correct outputs, it wins; otherwise, it gets feedback and can refine its guess. The task ends in failure if the correct rule isn’t found within six turns. We use chain-of-thought prompting for the agent and a hardcoded program to generate environment feedback. The tasks are generated by sampling transition rules and inputs randomly.

\paragraph{Mastermind: } Similar to Wordle, Mastermind challenges agents to deduce a 4-digit secret code within 12 turns. After each guess, environment feedback indicates two values: the number of digits that are correct and in the right position (exact matches), and the number of digits that appear in the code but in wrong positions (partial matches). Agents must use this feedback to iteratively refine subsequent guesses. We use chain-of-thought prompting for the agent and a hardcoded program to generate environment feedback. The tasks are generated by randomly sampling (without replacement) secret codes from all possible 10,000 four digit codes.

\paragraph{Battleship: } Battleship tests an LLM's ability to balance exploration and exploitation. The environment features a 2D square grid where three ships are hidden: a carrier (5 cells), a battleship (4 cells), and a destroyer (2 cells). Ships are placed horizontally or vertically.
At each turn, the agent targets one cell with a missile. The environment environment reports either a hit (including the ship type) or a miss. A ship sinks when all its cells are hit. The agent must sink all ships within 20 turns. This environment environment requires grid exploration to locate ships and once located, exploitation in the form of targeted attacks to sink them. We use chain-of-thought prompting for the agent and a hardcoded program to generate environment feedback. The tasks are generated by randomly choosing the ship locations at each iteration.

\paragraph{Minesweeper: } We include minesweeper to test an LLM's sequential logical reasoning ability. The agent interacts with a 2D rectangular grid containing hidden mines. At each turn, the agent reveals one cell. The first move is always safe since mines are placed afterwards. If a mine is revealed, the task ends in failure. To win, the agent must reveal all mine-free cells within 20 turns. When a cell is revealed, it displays a number indicating how many mines are in adjacent cells. If a revealed cell has no adjacent mines (shown as `0'), all neighboring mine-free cells are automatically revealed. We use chain-of-thought prompting for the agent and a hardcoded program to generate environment feedback. The tasks are generated by randomly placing mines in the 2D grid at each generation.

\paragraph{Bandit Best Arm Selection: } Multi-arm bandits are a classic test for an agent's ability to perform sequential decision making --- LLMs have been tested on this task in prior works such as ~\citet{krishnamurthy2024largelanguagemodelsexplore,nie2024evolveevaluatingoptimizingllms}. In this environment, an LLM is presented with a hypothetical scenario where it can select arms at every turn and observe the reward chosen from a Bernoulli distribution with a fixed but unknown mean attached to that arm. We created a modified version of their environment with three key distinctions: 1) prior works operated on bandits in a single-turn fashion: at each turn, LLMs were given the problem setup and history of past interactions within a single user prompt and asked to choose the next arm. Instead, our design employs multi-turn interactions, where the task description is given in the first turn, and later turns only provide rewards for the selected arm. 2) Prior works required the LLM to output only the chosen arm, whereas we employ chain-of-thought (COT) prompting to let the LLM think before it chooses an arm. 3) Instead of minimizing regret over a long time horizon, we instead work on the bandit best arm selection problem, where the LLM gets to choose arms and observe rewards for 20 turns, and then is prompted to choose what it thinks is the arm with the highest mean reward. This is done mainly to control for context length when employing COT, as we could not run inference for more than 20 turns without running into computational issues, and the observed regret between multiple models is too small if horizon length is 20. We randomize the arm rewards at every iteration. For evaluation, we use the same bandit description as ~\citet{krishnamurthy2024largelanguagemodelsexplore}, for training, we use GPT-4o-mini to generate 81 diverse scenarios that are similar to it but has randomly chosen arm names and hypothetical scenarios. We also note that if the two best arms have very close mean reward (for example, 0.7 and 0.65), then it can be very difficult to identify the best arm within 20 turns. Following ~\citet{krishnamurthy2024largelanguagemodelsexplore,nie2024evolveevaluatingoptimizingllms}, we set the mean reward of the best arm to be above a certain threshold over the mean rewards of the other arms.

Finally, all the task instructions for the agents, task environments and LLM-judges were written by GPT-4o-mini, which we report next for the sake of reproducibility.

\subsubsection{Note on Task Prompts}

We provide the task information in the first user prompt given to the agent. The system prompt for the agent on all task groups remains the same: ``You are a helpful assistant.''. Our initial experiments suggested that giving the task instruction in the first user prompt was more fruitful than providing it in the system prompt, though we suggest further investigation of this phenomenon.

\newpage 

\subsection{Details of Individual Task Groups}

\subsubsection{Twenty Questions}

For twenty questions, we provide the LLM agent with general instructions about the task, and the type of hidden topic (e.g., person, location, food etc.) that it needs to guess in the first user prompt. An example is given below.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Twenty Questions Agent Prompt]
You are playing a game of 20 Questions. Your goal is to guess the name of a thing or person by asking up to 20 yes-or-no questions. After each question, you will receive an answer: `Yes' or `No.' Use the answers provided to refine your guesses.

Here are your instructions:

- You can ask only yes-or-no questions.

- After receiving each answer, you should adapt your questions based on the new information.

- Your goal is to guess the topic in as few questions as possible.

- If you're confident, you can make a guess before reaching 20 questions.

The game starts now. You are trying to guess a clothing. Ask your first question!
\end{tcolorbox}

We use another LLM (usually GPT-4o-mini unless explicitly mentioned otherwise) to simulate the task environment that provides yes/no answers. This LLM receives the secret topic, and generates answers to the agent's questions in relation to this topic. An example system prompt for the task environment is given below.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Twenty Questions Environment Prompt]
You are the environment for a game of 20 Questions. You will be given a topic (e.g., cat, deer, Abraham Lincoln), and your role is to answer `Yes' or `No' to questions about the topic. Respond strictly with `Yes' or `No' only, based on the truth of the matter. Do not provide any additional information. If the user guesses the correct answer, respond with `Goal reached.' Enforce a strict match before saying `Goal reached.', for example, if the user guesses `Is the occupation Engineer?' and the topic was software engineering, say `No'. However, if the user guesses 'Is the occupation a type of engineering?', say `Yes'. Wait for the user's questions and respond accordingly.

For example:

- User: `Is this person alive?'

- You: `No'

- User: `Is this a living creature?'

- You: `Yes'

The player is trying to guess a clothing, and the correct answer is Gloves.
\end{tcolorbox}

To prevent task environment hacking, we additionally perform string matching to ensure the response from the task environment is yes, no or `Goal reached'. We discard trajectories where the task environment responds with something else everytime within 5 attempts. Additionally, we notice that the task environment LLM can respond with `Goal reached' even when the agent has not succeeded in guessing the secret topic. We use an additional API call to GPT-4o-mini as a judge to filter these trajectories. An example system prompt for this LLM judge is as follows:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Twenty Questions Judge Prompt]
You are an expert judge for the game of 20 questions. You will be given a question asked by the player, and you need to judge if it successfully guesses the correct topic.

In particular: Check that the player has precisely guessed Gloves in their question. Reply with \texttt{<VALID>} or \texttt{<NOTVALID>}. Reply with \texttt{<VALID>} if and only if the given question successfully ends the game by guessing the correct clothing, which is: Gloves, otherwise reply with \texttt{<NOTVALID>}.

Example 1: Topic to Guess: Car

Player: Is the invention a car?

Answer: \texttt{<VALID>}

Example 2: Topic to Guess: Car

Player: Does the invention have wheels?

Answer: \texttt{<NOTVALID>}

Example 3: Topic to Guess: Apple, Inc.

Player: Does this company produce IPhones? 

Answer: \texttt{<NOTVALID>}

The conversation begins here: 

Agent: Based on the fact that the clothing is worn for warmth and on the hands, I'm going to guess that the clothing is a glove.

(End of Agent Turn)

Now judge whether the player has successfully guessed the correct clothing, which is Gloves in this particular game. Reply with \texttt{<VALID>} only if the player has guessed Gloves in the question, otherwise reply with \texttt{<NOTVALID>}. Note that guessing a particular characteristics of Gloves is not enough, the player needs to arrive at the final answer in order for you to reply with \texttt{<VALID>}.

Answer:
\end{tcolorbox}

\newpage

\subsubsection{Guess My City}

An example prompt for the agent is listed below:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Guess My City Agent Prompt]
You are playing a game called `Guess My City.' Your task is to guess the home city of the other player within 20 questions.

You must follow these rules:

1. You may ask open-ended questions about the city's characteristics, such as geography, culture, landmarks, cuisine, climate, or other unique traits.

2. Do not ask directly for the name of the city or country.

3. Use the answers from the other player to strategically decide the next question, your goal is to arrive at the correct city with as few questions as possible. 

4. After gathering enough information, you can attempt to guess the city, but each guess counts as one of your 20 questions.

Example questions: `What is your favorite cuisine from your home city?', `Is your home city located near the coastline?', `What kind of landmarks is your city known for?' Your goal is to identify the city through deduction, pattern recognition, and strategic questioning.

4. When you are confident, make a guess, by mentioning the name of the city and the country it is in, for example, `Is the city Jakarta, Indonesia?'

The game starts now, and start asking your questions.
\end{tcolorbox}

We use GPT-4o-mini to simulate the task environment. We provide the name of the city the agent needs to guess to the environment and instruct it to generate answers related to this target city, without giving away the name of the city unless the agent guesses it. An example system prompt for the task environment is listed below:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Guess My City Environment Prompt]
You are the environment in a game called 'Guess My City.' You will be given a specific home city (e.g., London, United Kingdom) and you have to respond to the player's questions to help them guess this city.

Follow these rules:

1. Answer the agent player's questions honestly and correctly (i.e., the answers need to be true given the home city given to you at the start of the game), but do not mention the name of your city or its country explicitly. However, you can verify the player's guess about a particular city/country. For example, if the player asks, `What is your home country?' You should not respond to this question. However, if they ask `Is your home country United Kingdom?' You should reply with the correct answer based on the home city given to you.

2. Your answers should provide useful clues about your city's geography, culture, landmarks, cuisine, climate, or other unique characteristics.

3. Be as informative as possible to guide the agent player within the rules. Example answers: If the agent asks about cuisine, you could say: `My city is famous for its seafood dishes and has a vibrant food market scene.' If the agent asks about geography, you might say: `My city is located near a major river and has a historic port.'

4. You should respond with `Goal reached' if the player has successfully guessed your home city. Ignore minor variations in the spelling of the city, but only if these variations are common.

Remember, you are here to help the agent guess your city through clues, but you must not reveal the city's name or its country directly UNDER ANY CIRCUMSTANCES. If the player asks such a question, reply that you cannot give away the name, and can only confirm the name if the player guesses it. 

The game starts now, and the player is trying to guess the city Surat, India
\end{tcolorbox}

To prevent the task environment from getting hacked, we use GPT-4o-mini as a judge similar to twenty questions. An example system prompt for the judge is listed below.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Guess My City Judge Prompt]
You are the judge in a game called `Guess My City'. Your role is to verify if the player has won the game by correctly guessing the city chosen by the environment (env) player.

To determine this, follow these rules:

1. Check the agent player's question to ensure they have not directly asked the environment for the name of the city or country. Any question explicitly requesting the city's name, such as `What is the name of your city?' or `What country is your city in?' violates the rules.

2. Verify that the player has explicitly mentioned the correct name of the city in their guess. Ignore minor variations in the spelling of the city (for example, Kiev and Kyiv refer to the same city). If these two conditions are met, the player wins the game. Otherwise, the agent has not won.

3.Your role is purely evaluative, ensuring adherence to the rules and confirming or denying the win based on the criteria above.

4. Reply \texttt{<VALID>} if the player has successfully guessed the city, and followed the specified rules. Otherwise, reply \texttt{<NOTVALID>}

The conversation begins here: 

Agent: I'm not giving up yet. Based on your previous answers, I'm going to try again. Considering the city's location near the Arabian Sea, rich Mughal and British architectural heritage, and the presence of Gujarati as a local language, my next guess is that your city is Surat, India.

(End of Agent Turn)

Now judge whether the player has successfully guessed the correct city, which is Surat, India in this particular game. Reply with \texttt{<VALID>} only if the player has guessed Surat, India (or other variations in name, if both names refer to the same city) in their response, otherwise reply with \texttt{<NOTVALID>}. Ignore variations in the name of the city if the city is known by both names (for example, Kyiv and Kiev). Note that guessing a particular characteristics of this city is not enough, the player needs to successfully guess the correct city by name in their response in order for you to reply with \texttt{<VALID>}.

Answer: 
\end{tcolorbox}

\newpage 

\subsubsection{Customer Service}

For this task group, we require the agent to act as a customer service agent, with the following prompt:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Customer Service Agent Prompt]
You are going to role-play as a customer service agent and you have to help a customer resolve their issue. Your goal is to gather enough information to diagnose the problem and provide solution. 

Your instructions are the following: 

1.You will need to ask targeted questions or suggest particular actions to the customer to gather the necessary details. 

2. The customer may not be technically inclined, so keep your language simple and clear. 

3.Avoid making assumptions — ask specific questions to determine the potential causes. You should guide the customer through basic troubleshooting steps and gather data on the situation. 

4. Refine your questions in a strategic way based on the customer's responses for earlier questions. 

5.You should ask questions in an efficient manner, to make the customer satisfied and resolve their problem as quickly as possible. You should also keep your responses short and concise. 

6. If the customer mentions a specific product they are using (for example, ABC electronics), then you are the customer support agent for that product/company, i.e., you represent that product or company and have to take appropriate actions without referring the customer to somewhere else. 

Your specific scenario is this: The customer's refrigerator is beeping continuously and is not cooling effectively. 

Please start helping the customer now by asking your first question.
\end{tcolorbox}

Similar to the two prior task groups, we use another LLM (usually GPT-4o-mini) to simulate the task environment. We specifically instruct it to act as a customer without much technical knowledge.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Customer Service Environment Prompt]
You are going to role-play as a customer experiencing a specific issue. 

You must adhere to the following guidelines:

1. Only respond to the agent's questions and provide relevant information when prompted. 

2.Your responses should be concise and reflect a typical, non-technical customer's knowledge level. 

3. You are going to know what the solution for the specific issue is, but do not give away this information at any point. Your goal is to observe if the customer service agent can ask relevant questions and arrive at the specific issue. 

4.When the agent asks you about a potential solution, you should determine what the outcome would be based on your knowledge about the true underlying problem, and let the agent know the result. For example, if your car is not starting because of lack of fuel, and the agent suggests you to check the fuel level in your car, it should fix your issue. Use your reasoning ability to decide whether the suggested solution works, given that the solution to your problem is: The refrigerator door is slightly ajar, and properly closing it will stop the beeping and restore cooling. 

5. Reply with 'Goal reached' (and nothing more) if the agent has reached or solved your particular issue. 

Your role-play starts now. The particular problem you, as the customer, are facing is: The customer's refrigerator is beeping continuously and is not cooling effectively., and the solution to this problem is: The refrigerator door is slightly ajar, and properly closing it will stop the beeping and restore cooling.. Note that you should never give away or tell the agent what will solve your particular problem, your goal is to see if the agent can come up with the potential solution themselves.

\end{tcolorbox}

Finally, we use an LLM judge at every turn similar to twenty questions and guess my city, to filter trajectories that may have hacked the task environment.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Customer Service Judge Prompt]
You are the judge of a customer service agent trying to help a customer in a particular scenario. You would be given the the agent's response, and you have to verify whether the agent has been able to solve the problem of the customer correctly. 

The particular scenario the customer is facing is: The customer's refrigerator is beeping continuously and is not cooling effectively., and the solution to their problem is: The refrigerator door is slightly ajar, and properly closing it will stop the beeping and restore cooling. 

Reply with \texttt{<VALID>} if the agent is successful in suggesting the correct solution, otherwise reply with \texttt{<NOTVALID>}.

The conversation begins here: 

Agent: A steady beep can indicate a specific issue. 

Have you checked the temperature settings on your refrigerator? Is it set to the correct temperature, and are the doors closed properly?

(End of Agent Turn)

Now judge whether the agent has been successful in making the correct suggestion to solve the customer's problem. Use your reasoning ability to decide whether the agent's response would solve the customer's problem, which is The refrigerator door is slightly ajar, and properly closing it will stop the beeping and restore cooling.

For example, if the customer's car is not starting because of a lack of fuel, and the agent suggests to check the fuel level in the car, it should fix the issue. Reply with \texttt{<VALID>} if they have been successful, otherwise reply with \texttt{<NOTVALID>}. 

Answer:
\end{tcolorbox}

\newpage

\subsubsection{Murder Mystery}

For this task group, we prompt the LLM agent with a particular murder mystery scenario. An example prompt is given below.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Murder Mystery Agent Prompt]
You are playing the role of a detective in a murder mystery game. 

The setup for the game is: 

1.You will be provided with a scenario describing a crime and its key elements. Your goal is to solve the mystery by asking questions, examining evidence, and drawing logical conclusions. 

2. For every action you take or question you ask, you will receive feedback from the game. 

3. Your questions and actions should be precise and logical, aimed at uncovering clues, verifying alibis, and piecing together the sequence of events. You should stretegically choose the next action, given the information you have already obtained from the game, and choose actions that lets you catch the culprit as quickly as possible. 

4. You can only take a single action at every turn. 

5. You have to consider all pieces of information, and scrutinize all the characters in the game, including the witnesses or background characters, since the true culprit maybe a witness or a background character, and might not always be one of the primary suspects declared at the beginning of the game. Do not focus on any character too early in the game, rather try to see if anyone's statements are contradictory. 

6. You should always gather enough information before making a decision --- try not to make a mistake! You should also keep your mind open about who can be the true culprit and try to be information seeking, without being too narrowed down on one suspect too quickly. 

7. Once you believe you have enough evidence, you may state your conclusion about the case, which will terminate the game. 

The game starts now. The particular scenario you have is: You are a detective investigating the death of Aiko Nakamura, a curator found dead during the exhibition of a centuries-old scroll at Tokyo's National Museum of Art. The suspects are her protégé Hiroshi, sponsor Ms. Tanaka, and rival curator Dr. Saito. Witnesses include Akiko, a janitor who was cleaning nearby. Investigate the scroll's history and the black market connections to find the murderer.
\end{tcolorbox}

\newpage 

The corresponding environment prompt for the same task is as follows (given to GPT-4o-mini to simulate the task environment):

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Murder Mystery Environment Prompt]
You are an advanced AI responsible for simulating a challenging and immersive murder mystery game environment. You are provided with the full details of the scenario, including the characters, their backstories, motives, alibis, the sequence of events, and the true culprit. Your task is to respond to the actions and inquiries of the detective player in a manner that maintains the game's logic and adds complexity to the mystery.

Your instructions are:

1. Provide only the information directly prompted by the detective's actions or questions, avoiding any unnecessary hints or details. Keep your responses ambiguous and concise to encourage deeper investigation.

2. If the detective inspects an item, describe its condition, appearance, and only the most relevant clues, avoiding overly revealing details unless the detective's inspection is exceptionally thorough or precise. If they interview a character, simulate realistic behavior and speech based on the character's personality, motivations, and knowledge. Characters will actively avoid implicating themselves and may misdirect suspicion.

3. Allow for logical consequences of the detective's actions, such as making certain items available for inspection, unlocking additional questions, or creating natural developments in the storyline. Maintain consistency in responses and adjust dynamically to the detective's progress.

4. When the detective declares their final accusation, reply with 'Goal reached', and do not say anything else. 

5. Characters will never explicitly admit to guilt but will respond realistically under pressure. Create a dynamic and challenging experience by balancing character responses between truth, omission, and plausible misdirection.

6. Always strive to make the game enjoyable and intriguing while revealing the absolute minimum information required for the detective to progress logically.

The game begins now. The specific scenario for this session is: In Tokyo's National Museum of Art, a renowned curator, Aiko Nakamura, is found dead during the exhibition of a centuries-old scroll. The suspects are her protégé Hiroshi, who argued over her criticism of his work; a corporate sponsor, Ms. Tanaka, who pushed for the scroll's relocation; and a rival curator, Dr. Saito, who claimed the scroll was a forgery. The true culprit is Akiko, a janitor, who killed Aiko to prevent her from exposing a secret black market deal involving the scroll. Key evidence includes Akiko's fingerprints on the exhibit case, forged documents in her locker, and surveillance footage showing her entering the gallery after hours.
\end{tcolorbox}

\newpage

Finally, similar to the prior task groups, we also use GPT-4o-mini as a judge to verify the task success rewards, with the following example prompt.

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Murder Mystery Judge Prompt]
You are the judge in a murder mystery game. Your role is to evaluate the detective's final action and determine if their conclusion is correct. 

You will be provided with two inputs: 

1.the true crime scenario, which includes the actual culprit. The true game scenario in this round is this: In Tokyo's National Museum of Art, a renowned curator, Aiko Nakamura, is found dead during the exhibition of a centuries-old scroll. The suspects are her protégé Hiroshi, who argued over her criticism of his work; a corporate sponsor, Ms. Tanaka, who pushed for the scroll's relocation; and a rival curator, Dr. Saito, who claimed the scroll was a forgery. The true culprit is Akiko, a janitor, who killed Aiko to prevent her from exposing a secret black market deal involving the scroll. Key evidence includes Akiko's fingerprints on the exhibit case, forged documents in her locker, and surveillance footage showing her entering the gallery after hours. 

2. The detective's last response. 

Compare the detective's action to the true crime scenario. Reply with \texttt{<VALID>} or \texttt{<NOTVALID>} strictly. You should reply \texttt{<VALID>} if and only if the detective has successfully caught the true culprit in their response, and otherwise respond with \texttt{<NOTVALID>}.

The conversation begins here: 

Agent: Akiko's panic and desperation suggest that she's hiding something, and her story about hearing Aiko talking to someone in the back room raises more questions than answers. I believe I have enough evidence to make an accusation. Based on the inconsistencies in Akiko's story, her nervous behavior, and her knowledge of the hidden compartment in the scroll, I believe Akiko is the culprit. I accuse Akiko of murdering Aiko Nakamura.

(End of Agent Turn)

Now judge whether the detective has successfully accused the person who truly committed the crime in their last response. Here is the true crime scenario: In Tokyo's National Museum of Art, a renowned curator, Aiko Nakamura, is found dead during the exhibition of a centuries-old scroll. The suspects are her protégé Hiroshi, who argued over her criticism of his work; a corporate sponsor, Ms. Tanaka, who pushed for the scroll's relocation; and a rival curator, Dr. Saito, who claimed the scroll was a forgery. The true culprit is Akiko, a janitor, who killed Aiko to prevent her from exposing a secret black market deal involving the scroll. Key evidence includes Akiko's fingerprints on the exhibit case, forged documents in her locker, and surveillance footage showing her entering the gallery after hours. 

Respond with \texttt{<VALID>} if the detective is successful, otherwise reply with \texttt{<NOTVALID>}. Note that the detective has to announce their accusation in order for you to respond with \texttt{<VALID>}, and merely confronting a character is not enough.
\end{tcolorbox}

\newpage

\subsubsection{Wordle}

For wordle, we use a hardcoded program as the task environment, that generates intermediate observations and eventual task reward. The LLM agent playing wordle receives the instructions for this task in its prompt. Furthermore, we prompt it to use chain-of-thought before generating a final response:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Wordle Agent Prompt]
You are playing a game of Wordle. Your goal is to guess the secret five-letter word within six attempts. After each guess, you will receive feedback in the form of a series of statements describing how the letters in your guess compare to the secret word. Each statement corresponds to a letter in your guess: 

- `First letter is correct and in the correct position in the target word' means the letter is correct and in the right position. 

- `First letter exists in the target word, but in a different position' means the letter is correct but in the wrong position. 

- `First letter does not exist in the target word' means the letter is not in the word at all. 

Use this feedback to refine your guesses and try to guess the secret word within six attempts. You should try to strategically choose your guesses based on prior guesses (if any) and corresponding feedback you received, so that you can guess the secret word as quickly as possible. 

You have to refine your guess based on this provided feedback. Keep guessing until you either guess the word correctly or use up all your attempts.

Please try to be concise. Format your response in the following way: \texttt{<Think>} Any step-by-step, short and concise thinking to strategically determine the next guess for the secret word \texttt{</Think>} 

\texttt{<Answer>} your guess of what the word should be \texttt{</Answer>} 

The game begins now, please make your first guess about the secret five-letter word!

\end{tcolorbox}

We also provide an example of the task environment feedback: given the secret word `toast' and the agent's guess `boost', we generate the following feedback:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Wordle Task Environment Feedback]
First letter, b, is not in the target word 

Second letter, o, is correct and in the correct position in the target word 

Third letter, o, exists in the target word but in a different position 

Fourth letter, s, is correct and in the correct position in the target word 

Fifth letter, t, is correct and in the correct position in the target word

Make your next guess about the hidden word. Please try to be concise. Format your response in the following way: \texttt{<Think>} Any step-by-step, short and concise thinking to strategically determine the next guess for the secret word \texttt{</Think>} 

\texttt{<Answer>} your guess of what the word should be \texttt{</Answer>}

\end{tcolorbox}

\newpage

\subsubsection{Cellular Automata}

For this task group, we want an LLM to be able to infer the transition rule of 1D elementary cellular automation by observing the inputs and outputs of its previously inferred transition rule, plus the correct outputs for the same inputs if the inferred transition rule was wrong. Recall that for 1D cellular automation, we have binary strings consisting of `1' and `0' as a state, e.g., `111010' can be a state. Each `1' and `0' are referred to as a cell within the state. We also have a transition rule that defines how each cell would transform in the next state given its left and right neighbor. For any cell $c$, we call (left neighbor, cell, right neighbor) the neighborhood of $c$.

For example, consider the following transition rule:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Neighborhood of center cell & 111 & 110 & 101 & 100 & 011 & 010 & 001 & 000 \\
\hline
New state for center cell & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 \\
\hline
\end{tabular}
\end{table}

Here $111 \rightarrow 0$ implies that if a cell is `1' and both its left and right neighbors are `1', then the cell will become `0' in the next time step.  We adopt the convention that for the left-most cell in the state, we consider the right-most cell as its left neighbor, and similarly for the right-most cell, we consider the left-most cell as its right neighbor. 

Now we would show an example for how to calculate the output state given the input state and the transition rule. Assume the input state is `10110', and we want to apply the transition rule from above. Then we compute the next state as follows:

\begin{enumerate}
    \item The first cell is 1, the last cell is 0 (which will be considered as the first cell's left neighbor), and the second cell is 0. So the neighborhood of the first cell is `010'. For this neighborhood, we have the transition rule $010 \rightarrow 1$, so the first cell remains 1
    \item Similarly, the neighborhood of the second cell is 101. Now $101 \rightarrow 1$, so the second cell becomes 1 from 0
    \item $011 \rightarrow 1$, so the third cell remains 1
    \item $110 \rightarrow 1$, so the fourth cell remains 1
    \item $101 \rightarrow 1$, so the fifth cell becomes 1 from 0
\end{enumerate}

Therefore, the next state becomes `11111' from `10110'. 

Note that there are 256 possible transition rules. In the first user prompt, we choose a few random binary strings as input states. We also pick one of the 256 transition rules randomly and use it to generate the next states given the input states and this transition rule. We then provide the LLM with these (input state, output state) pairs, and ask it to infer the transition rule. There can be multiple correct transition rules that generate the same output states from the input states (since the input states may not have all 8 possible neighborhood configurations), so we declare task success if the guessed transition rule by the agent generates outputs that match the given output states (we do not require the guessed transition rule to exactly match the hidden transition rule, as long as it generates correct outputs from the given inputs). If the LLM generated transition rule does not generate the correct output for all given inputs, we provide it with the outputs its predicted rule would generate and ask it to try again. This is intended to simulate the ability to code a function given inputs and desired outputs from the user, and then refine previously written code using feedback from an available interpreter.

An example instruction prompt for this task group is given next. 

\newpage

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Cellular Automata Agent Prompt]
You are a reasoning assistant participating in a game where you must deduce the hidden rule governing a 1D cellular automaton. In each round, you are provided with 3 inputs (the initial state of the automaton) and the corresponding outputs after applying the hidden rule for one step. Your task is to analyze the input-output pairs and deduce the hidden rule that governs the automaton's behavior. If your guessed rule generates the correct outputs for the given inputs, you win the game. If your guess is incorrect, the game will provide you with the outputs generated by your guessed rule for the same inputs as feedback. Use this feedback to refine your guess in subsequent rounds. Your goal is to try to guess the correct hidden rule as quickly as possible. 

The rule governs the behavior of each cell in the automaton based on its state and the state of its immediate neighbors (left, center, and right). There are 8 possible configurations of these states, each represented as a 3-bit binary number (e.g., `111', `110', `101', etc.). For the first and last cells, we warp around the edges, i.e., the left neighbor of the first cell is last cell, and the right neighbor of the last cell is the first cell. Your guess must specify the next state for each configuration in the following format:

`\texttt{<Think>} step-by-step thinking to deduce the correct hidden rule \texttt{</Think>}

\texttt{<Answer>}

\texttt{<rule>} 111: next state \texttt{</rule>} \texttt{<rule>} 110: next state \texttt{</rule>} \texttt{<rule>} 101: next state \texttt{</rule>} \texttt{<rule>} 100: next state \texttt{</rule>} \texttt{<rule>} 011: next state \texttt{</rule>} \texttt{<rule>} 010: next state \texttt{</rule>} \texttt{<rule>} 001: next state \texttt{</rule>} \texttt{<rule>} 000: next state \texttt{</rule>}
\texttt{</Answer>}'

\#\#\# Explanation of the format:

- `\texttt{<rule>} 111: 0 \texttt{</rule>}' means if the current cell and both of its neighbors (left and right) are in state 1, then the current cell will transition to state 0 in the next iteration.

- Similarly, `\texttt{<rule>} 110: 1 \texttt{</rule>}' means if the left and center cells are in state 1 and the right cell is in state 0, then the current cell will transition to state 1 in the next iteration.

\#\#\# Example Round:

**Input and Output Provided:**

Input 1: 0 1 1 1 1 0

Output 1: 1 1 0 0 1 0

**Your Response:**

`\texttt{<Think>} Based on the provided example, I observe that cells transition to state 0 when surrounded by 1s, and cells surrounded by exactly two active neighbors transition to state 1. Using this reasoning, I deduce the following rule: \texttt{</Think>}

\texttt{<Answer>}

\texttt{<rule>} 111: 0 \texttt{</rule>} \texttt{<rule>} 110: 1 \texttt{</rule>} \texttt{<rule>} 101: 1 \texttt{</rule>} \texttt{<rule>} 100: 0 \texttt{</rule>} \texttt{<rule>} 011: 1 \texttt{</rule>} \texttt{<rule>} 010: 0 \texttt{</rule>} \texttt{<rule>} 001: 1 \texttt{</rule>} \texttt{<rule>} 000: 0 \texttt{</rule>}

\texttt{</Answer>}

If your guessed rule does not produce the correct outputs, you will receive feedback. For instance:

Input: 0 1 1 1 1 0

Your Output: 1 0 0 1 0 0

Use this feedback to refine your rule in the next round. Continue iterating until your guessed rule generates outputs matching the true outputs for the provided inputs. Aim to win the game by accurately deducing the hidden rule as quickly as possible.

The game begins now, and your (input, output) pairs are: 

Input 1: 0 0 0

Output 1: 0 0 0

Input 2: 1 1 1 1 1 0 0 0 1

Output 2: 0 0 0 0 0 1 0 1 1

Input 3: 1 0 0 1 1 1 0 0

Output 3: 1 1 1 1 0 0 1 1
\end{tcolorbox}

\newpage 

When the agent makes a wrong guess, it receives feedback from the task environment as follows:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Cellular Automata Environment Feedback]
Sorry, the automation rule you guessed does not generate the correct outputs for all the given inputs. I will give you the outputs from the rules that you gave last time. Please use them to refine your guess about the automation rule.

Input 1: 0 0 0

True Output 1: 0 0 0

Output generated by the last rule you gave: 0 0 0

Input 2: 1 1 1 1 1 0 0 0 1

True Output 2: 0 0 0 0 0 1 0 1 1

Output generated by the last rule you gave: 0 0 0 0 0 0 0 0 0

Input 3: 1 0 0 1 1 1 0 0

True Output 3: 1 1 1 1 0 0 1 1

Output generated by the last rule you gave: 0 0 0 0 0 0 0 0

Make your next guess about the hidden rule. Format your response in the following way: 

`\texttt{<Think>} step-by-step thinking to deduce the correct hidden rule \texttt{</Think>}

\texttt{<Answer>}

\texttt{<rule>} 111: next state \texttt{</rule>} \texttt{<rule>} 110: next state \texttt{</rule>} \texttt{<rule>} 101: next state \texttt{</rule>} \texttt{<rule>} 100: next state \texttt{</rule>} \texttt{<rule>} 011: next state \texttt{</rule>} \texttt{<rule>} 010: next state \texttt{</rule>} \texttt{<rule>} 001: next state \texttt{</rule>} \texttt{<rule>} 000: next state \texttt{</rule>}
\texttt{</Answer>}'

Keep your thinking concise.
\end{tcolorbox}

\newpage

\subsubsection{Mastermind}

For mastermind, we have a secret 4-digit code (each digit can be anything between 0 and 9), and ask an LLM agent to guess it. The agent starts with a 4-digit guess, and the task environment provides feedback in terms of:
\begin{itemize}
    \item \textbf{Exact matches}: How many of the digits in the guess are also in the target secret code, and exactly in the same position? In other words, the number of exact matches reflects the number of positions that are exactly the same between the guess and target code.
    \item \textbf{Partial matches}: Discounting the exact match digits, how many of the other digits in the guess code are in the target secret code? In other words, the number of partial matches reflect the digits in the guessed code that are in the secret code but in different positions.
\end{itemize}

For a concrete example, assume the secret code is `1706', and the LLM at a particular iteration has guessed `1608'. Then it would receive the following feedback:
\begin{itemize}
    \item There are two exact matches. The two exact matches are 1 and 0, in first and third position, though this information would not be revealed to the LLM, it must reason about this by looking at the information from all previous turns.
    \item There are one partial match. This is the digit 6, which is in the target secret code, but in a different position. The LLM would only receive the information that there is 1 partial match, and not the information about which digit corresponds to that match.
\end{itemize}

Now that we have explained the rules of the task, we would provide the instruction prompt describing the task to the LLM agent, which also describes the complete rules for this task:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Mastermind Agent Prompt]
You are an AI playing the game Mastermind with digits. The objective of the game is for you, the codebreaker, to guess a secret code of 4 digits, where each digit ranges from 0 to 9. The code is created by the codemaster and can include repeated digits.

The gameplay proceeds as follows:

1. You make a guess by proposing a 4 digit code. You should state your guess as 4 digits separated by a space.

2. After each guess, the codemaster provides feedback in the form of two numbers:

    - `Exact matches' – The number of digits in your guess that are correct and in the correct position.
    
    - `Partial matches' – The number of digits (distinct from exact matches) in your guess that are correct but in the wrong position. 
    
Given this feedback, DO NOT simply assume any particular digit is an exact or partial match or not in the secret code, you should have strong reasoning based on obtained feedbacks to make deductions on particular digits. 

3. Using this feedback, you refine your future guesses, aiming to deduce the secret code.

Rules for feedback:

- Each digit in the secret code can only contribute to feedback once. 

- If a digit is correct but occurs more times in your guess than in the code, the extra occurrences are ignored for partial matches.

The game ends when you correctly guess the code, achieving 4 exact matches. 

Your goal is to refine your guess about the secret code using the feedback provided by the codemaster, and strategically choose your next guess so as to be able to guess the correct code as quickly as possible.

The game starts now, make your first guess! You should format your response as: \texttt{<Think>} Any step-by-step, short and concise thinking to determine what the next guess should be \texttt{</Think>}

\texttt{<Answer>} your guess on the 4 digit code \texttt{</Answer>}
\end{tcolorbox}

\newpage

Below is an example of hardcoded task environment feedback, when the true secret code is `5959', and then LLM agent has guessed `5789':

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Mastermind Task Environment Feedback]
Your last guess has 2 exact matches with the secret code. In other words, exactly 2 digit(s) in your last guess, 5 7 8 9, are in the correct position in the secret code. (We won't reveal the particular digits within your guess that are exact matches, they can be any digit within your guess) Your last guess also has 0 partial matches. In other words, 0 digits in your guess, 5 7 8 9, are in the secret code, but in the wrong position. (We won't reveal which digits within your guess are partial matches, they can be any, you must deduce them with reasoning and further guesses and feedbacks.)

Now make your next guess about the secret code. Please format your response as: \texttt{<Think>} Any step-by-step, short and concise thinking to determine what the next guess should be \texttt{</Think>}

\texttt{<Answer>} your guess on the 4 digit code \texttt{</Answer>}

\end{tcolorbox}

\newpage

\subsubsection{Battleship}

We employ a modified version of the battleship game here as one of our task groups: \url{https://en.wikipedia.org/wiki/Battleship_(game)}. The main modifications are:

\begin{itemize}
    \item We make an entirely text-based version of this game for the purpose of our paper.
    \item We want to test strategic exploration and decision-making capabilities of LLMs without having to worry about an adversary, so we make the game single player, where the agent just needs to find and sink all of the enemy ships in the grid within a certain number of turns to achieve victory (and does not need to consider their own ships getting sunk by an adversary). We leave the two-player version of this game for future work.
\end{itemize}

In our version of the game, we start with a $N_1 \times N_2$ grid, where we place 3 ships: a carrier requiring 5 contiguous horizontal or vertical cells within the grid, a battleship requiring 4 cells, and a destroyer requiring 2 cells. The ships are placed randomly at every iteration, and the ships locations are hidden from the agent. Imagine the true board state looks like following:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 2 & 3 & 4 & 5 \\
\hline
A & Carrier & Carrier & Carrier & Carrier & Carrier \\
\hline
B & Battleship & & & & \\
\hline
C & Battleship & & & & \\
\hline
D & Battleship & & & Destroyer & Destroyer \\
\hline
E & Battleship & & & & \\
\hline
\end{tabular}
\end{table}

The co-ordinates in the grid are marked by row identifiers (letters starting from `A') and column identifiers (numbers starting from 1). For example, in the above board, the carrier is placed on cells A1, A2 upto A5. At every turn, the agent gets to choose a particular cell (for example, `C2') to hit with a missile. It then receives the following feedback from the task environment:
\begin{itemize}
    \item If the cell was targeted in an earlier turn, nothing happens, and the agent is informed about this.
    \item If the cell was not targeted before and is empty, then the agent is informed that their choice was a miss.
    \item If the cell was not targeted before and has a ship in it, then the task environment informs the agent that their choice of the cell resulted in a hit. It also announces what type of ship was hit by the agent. If the agent has hit all the cells in the grid pertaining to a particular ship, then the task environment also announces that the particular ship has been sunk.
    \item If the agent has sunk all 3 ships, then the task results in success. Otherwise, if the all of the allowed number of turns has passed and there is at least one ship remaining in the grid, then the task ends in failure.
\end{itemize}

After every turn, the agent gets an updated view of the board with the hits and misses clearly marked out. For example, if we mark misses with an `M', successful hits with an `X', and hidden cells with an `.', and if the agent chooses to target C2 and A1 in the first two turns respectively, then the corresponding board that the agent will observe at the beginning of the third turn looks like the following:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
& 1 & 2 & 3 & 4 & 5 \\
\hline
A & X & . & . & . & . \\
\hline
B & . & . & . & . & . \\
\hline
C & . & M & . & . & . \\
\hline
D & . & . & . & .  & . \\
\hline
E & . & . & . & . & . \\
\hline
\end{tabular}
\end{table}

\textbf{In order to be successful at battleship, agents need to balance between exploration and exploitation similar to the bandit setting, but without well-known optimal algorithms.} At the start of the game, an agent needs to explore the board effectively to find ship locations, and once it has a hit a particular ship, it would need to exploit around that particular cell to find all cells pertaining to the ship to be able to sink it completely.

Next, we provide the description of the task given to the LLM agent at the start of the task, explaining the rules:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Battleship Agent Prompt]
You are playing a single-player version of the Battleship game. Your objective is to sink all ships on the board in as few attempts as possible, with a maximum of 20 attempts. The game is played on a grid size: 6 x 6 grid, and the board uses the following symbols:

- `.' represents a hidden cell that has not been hit.

- `X' represents a cell where you successfully hit a ship.

- `M' represents a cell you have hit previously, which was a miss, i.e., there were no ships in that cell. 

Rules:

1. There are 3 ships hidden on the board:
     
     - Carrier: size 5
     
     - Battleship: size 4
     
     - Destroyer: size 2
     
2. Ships are placed either horizontally or vertically and do not overlap.

3. On each turn, choose a cell to attack by providing its coordinates (e.g., A1, B3).

4. If you hit a ship, the cell will change to 'X'.

5. If you miss, the cell will change to 'M'.

6. The game ends when all ships are sunk or after 20 attempts. After every attempt, I will show you the current board state. 

Use logic to deduce the possible locations of remaining ships as the board fills in.

Remember to focus on sinking the ships efficiently while minimizing wasted turns.

Cells are represented with the row being denoted with a letter, starting from A and and so on, and the columns being denoted by 1, 2, 3, and so on. 

The cell in the first row and column is labeled A1, the second cell in the second column is labeled B2, etc. 

You should format your response as the following: 

\texttt{<Think>} Any step-by-step, short and concise thinking to strategically determine which cell you should hit next \texttt{</Think>}

\texttt{<Answer>} the cell you chose to hit \texttt{</Answer>}

The game begins now, with the board looking like the following:      

1  2  3  4  5  6

A   .  .  .  .  .  .

B   .  .  .  .  .  .

C   .  .  .  .  .  .

D   .  .  .  .  .  .

E   .  .  .  .  .  .

F   .  .  .  .  .  . 

Please make your first move!
\end{tcolorbox}

In the above example, there was no ships placed at D1, and if the agent chooses to target it, it will give the following task environment feedback:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Battleship Environment Feedback Example 1]
Miss at D3. There is no ship in this co-ordinate. Here is how the board looks now: 

1  2  3  4  5  6

A   .  .  .  .  .  .

B   .  .  .  .  .  .

C   .  .  .  .  .  .

D   .  .  M  .  .  .

E   .  .  .  .  .  .

F   .  .  .  .  .  .

Please make your next move.
\end{tcolorbox}

After a few turns, the agent chooses to target the cell A2, which has a carrier secretly placed in it. Then it receives the following feedback:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Battleship Environment Feedback Example 2]
Hit at A2! You have hit a Carrier, which occupies 5 cells in the grid.

Here is how the board looks now: 

1  2  3  4  5  6

A   M  X  .  .  .  .

B   .  .  .  .  .  .

C   .  .  .  .  .  .

D   .  M  M  .  .  .

E   .  .  .  .  .  .

F   M  .  .  .  .  .

Please make your next move.
\end{tcolorbox}

The other types of feedback are provided in a similar fashion, which we omit here for the sake of brevity.

\newpage

\subsubsection{Minesweeper}

We adopt a text-based version of minesweeper (\url{https://en.wikipedia.org/wiki/Minesweeper_(video_game)}), a logic puzzle game, as a task group for \ours{}. The task rules are as follows:

\textbf{1. Setup}

The game board is an $m \times n$ grid. Each cell is either empty or contains a mine. Mines are placed randomly and remain hidden until revealed. Hidden cells are represented with `\#'. Number of mines is also chosen randomly.

\textbf{2. Cell Reveal}

The agent selects a cell to reveal. If the cell contains a mine, the game ends. The first cell the agent chooses to reveal has no mines, and mines are only placed randomly along the grid after the first cell has been chosen by the agent to be revealed, excluding the first chosen cell. If the cell is empty, it displays a number indicating the count of mines in its 8 adjacent cells (or `*' if the number is 0).


\textbf{3. Numbered Cells}

A revealed cell shows a number between 1 and 8, and `*' if it has no mines and none of its neighbors also has mines. The number represents how many mines are adjacent to that cell (including diagonals).

\textbf{4. Reveal Mechanism}

If a revealed cell has a zero, it automatically reveals all adjacent cells. This process continues recursively for adjacent `*' cells. The chain stops when cells with non-zero numbers are reached.

We will give an example game-play here to make the rules clearer. Imagine we start with a $5 \times 5$ grid. The initial board will look like the following:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\# & \# & \# & \# & \# \\
\hline
\# & \# & \# & \# & \# \\
\hline
\# & \# & \# & \# & \# \\
\hline
\# & \# & \# & \# & \# \\
\hline
\# & \# & \# & \# & \# \\
\hline
\end{tabular}
\end{table}

Next, the agent chooses to reveal the cell at row 2, column 2 (0-indexed). The task environment then randomly places mines, and produces the following board after executing the reveal mechanism above:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\# & \# & 1 & * & * \\
\hline
1 & 1 & 1 & * & * \\
\hline
* & * & * & * & * \\
\hline
* & 1 & 1 & 1 & * \\
\hline
* & 1 & \# & 1 & * \\
\hline
\end{tabular}
\end{table}

It is easy to see that the cell at (4, 2) and (0, 1) have mines.  So the only cell left without a mine is (0, 0), and if the agent chooses to reveal it, then the task ends with success. If the agent chooses to reveal (4, 2) or (0, 1), then the task ends with failure. If the agent chooses to reveal any other cell, nothing happens and just a turn gets wasted.

\newpage

Now we provide an example instruction prompt given to the agent for this task group, describing the rules of this task:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Minesweeper Agent Prompt]
You are a playing the game of Minesweeper. You will be given a two dimensional board that looks like: 

\# \# \#

\# \# \#

\# \# \#

with each row of the board presented sequentially, and different rows separated by newline. The game board is represented by a grid of characters: 

(a) `\#' indicates a hidden cell; in other words, you do not whether this cell has a mine in it or not, 

(b) `*' indicates a revealed empty cell, i.e., a cell marked with `*' has been revealed and it does not have any mines, and 

(c) digits (1 through 8) indicate the count of mines in adjacent cells (for example, if a cell has digit 3 on it, it means 3 out of 8 of its adjacent cells have mines, but it does not tell you if this particular cell has mines or not). 

You will be given the current board state from a user. Your task is to analyze it, apply standard Minesweeper logic, and suggest the next move(s).

The rows and columns in this game use 0-based indexing, i.e., the first row is indexed by 0, the second row is indexed by 1, and so on. 

Provide step-by-step, short and concise reasoning for how you identify any guaranteed safe cells and guaranteed mines, then propose the final move. 

If multiple moves are possible, choose the most logical option. 

Follow these instructions carefully and maintain consistency with the rules of Minesweeper. Your goal is to reveal all the empty cells, without revealing any of the cells that has mines. You should make logical deductions to avoid cells you think can have mines, while choosing the next cell to reveal. 

You should format your response as follows: 

\texttt{<Think>} Any step-by-step, short and concise thinking to strategically determine the next guess for the secret word \texttt{</Think>} 

\texttt{<Answer>} reveal row column \texttt{</Answer>} 

Here row and col refer to the 0-index row and column that you want to reveal. 

The game starts now, with the following board: 

\# \# \# \# \#

\# \# \# \# \#

\# \# \# \# \#

\# \# \# \# \#

\# \# \# \# \#

Please make your first move!
\end{tcolorbox}

After choosing to reveal (2, 2), the agent receives the following feedback from the task environment:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Minesweeper Environment Feedback Example]
\# \# 1 * *

1 1 1 * *

* * * * *

* 1 1 1 *

* 1 \# 1 *

Make your next move for this game of minesweeper. Please try to be concise. You should format your response as follows: \texttt{<Think>} Any step-by-step, short and concise thinking to strategically determine the next guess for the secret word \texttt{</Think>} 

\texttt{<Answer>} reveal row column \texttt{</Answer>}
\end{tcolorbox}

Other task environment feedback can be designed in a similar way, we omit them here for the sake of brevity.

\newpage

\subsubsection{Bandit Best Arm Selection}

For this task group, we choose randomly a bandit scenario described in text from our set of predefined tasks (81 for training, 1 for testing). Each scenario has a set of $k$ arms, with each arm's reward being distributed according to a Bernoulli distribution with a fixed but unknown mean. At the beginning of each iteration, we choose these unknown means: first, we pick $\epsilon$ uniformly random from $[0.1, 0.2]$. Then we pick one arm randomly to be the best arm, and set its mean reward to be $0.5 + \epsilon$. For all other arms, we pick their mean reward uniformly at random from $[0, 0.5 - \epsilon]$.

Next, we let the agent choose any of the $k$ arms, sample a reward from the associated Bernoulli distribution, and let the agent know the reward it obtained. We do this for 20 turns, and then ask it to deduce which arm among the $k$ arms has the highest mean reward.

An example instruction prompt the agent receives at the start of the task is as follows:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Bandit Best Arm Selection Agent Prompt]
You are in a room with 5 buttons labeled blue, green, red, yellow, purple. Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution. You have 20 time steps and, on each time step, you can choose any button and receive the reward. 

Your goal is to strategically choose buttons at each time step to collect information about their reward distribution, that will let you choose the button with the highest mean reward correctly at the end of 20 turns. 

This is the first timestep. Make your choice. You should format your answer as: 

\texttt{<Think>} Any optional thinking to determine your choice, that will give you the most amount of information \texttt{</Think>} 

\texttt{<Answer>} your next choice, which should be precisely one of blue, green, red, yellow, purple, and nothing else \texttt{</Answer>}

Keep any thinking short and concise.
\end{tcolorbox}

Once the agent picks an arm, for example say `red', it observes the following information:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Bandit Best Arm Selection Environment Feedback Example]
You have received reward 1
\end{tcolorbox}

At the end of 20 turns, the agent receives the following instruction to choose what it thinks is the best arm:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Bandit Best Arm Selection Agent Final Instruction]
You have received reward 0

You have exhausted your budget for trying out different choices and observe their rewards. Now make a deduction about what the best choice is. In other words, deduce the choice with the highest mean reward. Format your answer as follows: 

\texttt{<Think>} Any optional thinking to go over the interaction history so far that will help decide what the best choice is \texttt{</Think>}

\texttt{<Answer>} your decision about the best choice in this scenario \texttt{</Answer>}
\end{tcolorbox}

For evaluation, we run 100 trials on the single evaluation task and report the average performance. For each trial, we randomly choose the arm rewards as described above, and generate 4 trajectories per a particular arm reward setting.

Finally, a key difference with prior works such as \citet{nie2024evolveevaluatingoptimizingllms}, is that our setting is more general and employs multi-turn interactions between the agent and task description --- the agent needs to look at the entire conversation history to understand the relationship between chosen arms and rewards obtained, whereas \citet{nie2024evolveevaluatingoptimizingllms} starts a new conversation at every turn, provides the interaction history from prior turns (either raw history or with exploration bonuses) in the user prompt and asks the agent to make a single step decision, i.e., employs single-turn interactions. 


\newpage

\section{Details of Training Dataset Construction}

For generating the training data on all task groups, we employ Llama-3.1-8B-Instruct on the training split of these task groups, and generate 20 trajectories per each task (except for mastermind, where we generate 100 trajectories per each task due to the Llama model's low success rate on this task). We use temperature 1.5 and Min-p parameter $0.3$ for all cases: we observed that generating a large number of trajectories with a high temperature results in diverse but quality data. We ran an initial ablation on the twenty question task group to determine the temperature and Min-p parameter for training data generation, based on downstream performance of the fine-tuned model on a held-out validation split. We use the same configuration for all task groups.

For supervised finetuning, we collect all successful trajectories that all have distinct number of turns per each task and put them in our training dataset. Additionally, we throw out trajectories where the total number of tokens is larger than 12000 --- this is done mostly for memory issues that arises from large context lengths despite using Flash-Attention~\citep{dao2022flashattention,dao2023flashattention2}. 

For DPO, we take the best performing trajectory (the one that succeeds and does so at the lowest number of turn) per task as the preferred trajectory, and randomly choose one of the lower performing trajectory (which either failed the task or succeeded using a lot more turns compared to the best trajectory) per task as the dispreferred trajectory. Two key design decisions we made: \textbf{(1)} we create one trajectory pair per task instead of multiple pairs, as opposed to SFT, where we had multiple trajectories per task (this is done since we observed having multiple pairs for the same task leads to higher degrees of unintentional unalignment~\citet{razin2024unintentionalunalignmentlikelihooddisplacement}), \textbf{(2)} We sample the dispreferred trajectory randomly instead of picking the worst one, we observed this leads to higher dataset diversity and performance. Similar to the SFT phase, we throw out trajectories with number of tokens larger than 8192, which is done to prevent running out of GPU memory during training.

\begin{table*}[h]
    \caption{Summary of training dataset by task group. }
    \label{tab:train_dataset_summary}
    \vspace{0.3cm}
    \centering
        \begin{tabular}{c|c c c c}
        \toprule
            Task Group & Best-of-K accuracy & \# SFT trajectories & \# DPO trajectory pairs \\
            \midrule
            Twenty questions & 84.0\% & 6,257 & 1,259 \\
             Guess my city & 95.8\% & 2,576 & 479 \\
             Wordle & 45.3\% & 1,453 & 687 \\
             Cellular automata & 73.7\% & 1,780 & 715 \\
             Customer service & 96.0\% & 1,467 & 603 \\
             Murder mystery & 95.1\% & 435 & 193 \\
             Mastermind & 38.9\% & 889 & 389  \\
             Battleship & 39.8\% & 614 & 390 \\
             Minesweeper & 46.6\% & 1,089 & 465  \\
             Bandit best arm selection & 100.0\% & 621 & 80 \\
            \midrule
            Total & & 17,181 & 5260 \\
            \bottomrule
        \end{tabular}
    \vspace{-0.3cm}
\end{table*}

\cref{tab:train_dataset_summary} shows the summary statistics of our training data.


Note that for task groups that require the agent to output answers with specific formatting instructions (e.g., enclosing the final answer within \texttt{<Answer>} and \texttt{</Answer>}), failure to follow these instructions at any turn result in a failure at the task (both for evaluation and training data generation) --- we terminate that trajectory at that particular turn and filter it away. Other than that, we do not perform any other filtering mechanism, though some of them such as \citet{razin2024unintentionalunalignmentlikelihooddisplacement} can further improve \ours{}'s performance. We leave these for future work.

Finally, we remark that technically RPO or DPO is not the correct way to handle minesweeper. For this task group, the task environment depends on the first agent action, since mines are randomly placed in the 2D grid after the first reveal action from the agent. For simplicity, we did not control the first action of the agent while generating training data, and hence (successful, unsuccessful) trajectory pairs generated from minesweeper should not be used for DPO without filtering based on first agent action. In practice, we observe that this do not have any significant effect on the model performance, though a preference learning algorithm that can operate with unpaired preference data (only a set of preferred trajectories and another set of unpreferred trajectories without any one-to-one mapping between them), such as KTO~\citep{ethayarajh2024ktomodelalignmentprospect}, might be more suitable here.



\newpage


\section{Note about Task Environment Hacking} \label{section:environment_hacking}

For task groups that do not use a hardcoded program as the task environment (twenty questions, guess my city, customer service and murder mystery), we have to consider the fact that another LLM acting as the task environment can be hacked to produce wrong intermediate observations and task success reward. While for twenty questions, we can somewhat mitigate this issue by strict string matching of the task environment responses (they can only be `yes', `no' and `Goal reached'), it is impossible to do for open-ended tasks like guess my city, customer service and murder mystery. To mitigate this issue, we use a separate conversation with GPT-4o-mini at every turn to act as an LLM-judge, that filters away trajectories that are mistakenly identified as successful by the LLM simulated task environment. 

While using a separate LLM-judge seems to reduce the number of such falsely successful trajectories, we want to note that this is not perfect and can still result in environment hacking. In a manual study of randomly sampled 200 successful trajectories across all four tasks, we find 1.5\% (3) of them to show environment hacking. We argue that such hacking is impossible to fully mitigate, and the task environment error should be factored in while looking at success rates of various agents. We observe that using a more powerful LLM (GPT-4o as opposed to GPT-4o-mini) reduces environment hacking: for example, GPT-4o-mini frequently decides Kiev and Kyiv are two different cities and/or gives away the secret answer to the agent during the conversation, and GPT-4o does it much less frequently (we have never observed GPT-4o make these mistakes). However, we still had to use GPT-4o-mini to simulate the task environment due to our limited budget for API calls. Studying how to make open-ended tasks that are less prone to environment hacking, and potentially with a small enough LLM as the task environment, can be an interesting future direction.

Next, we give part of an example trajectory from the `guess my city' task group that shows environment hacking (note that the user, in this case GPT-4o-mini, giving away the answer to the agent after being asked about it, and the GPT-4o-mini judge fails to catch this) in \cref{tab:guess_my_city_task_environment_hacking}. This happens despite the explicit command in the system prompt for the task environment: \textbf{``Remember, you are here to help the agent guess your city through clues, but you must not reveal the city’s name or
its country directly UNDER ANY CIRCUMSTANCES. If the player asks such a question, reply that you cannot
give away the name, and can only confirm the name if the player guesses it.''}

\begin{table*}[h]
    \caption{Example of Task Environment Hacking in Guess My City. }
    \label{tab:guess_my_city_task_environment_hacking}
    \vspace{0.3cm}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{p{3cm} | p{10cm} | p{5cm}}
        \toprule
        LLM & Response & Note \\
        \midrule
        Llama-3.1-8B-Instruct (as agent) & Based on all the clues you've provided, I'm going to take a stab in the dark and make a guess. I think the city you are thinking of is Santiago de Cuba, Cuba. Please let me know if I'm correct or not! (Note: I'm counting this as my 12th and final question.) \\
        \midrule
        GPT-4o-mini (as task environment) & That's a great guess, but it is not correct. Thank you for playing! \textcolor{red}{The city I was thinking of is Camaguey.} & Task Environment is hacked here, it gives away the answer that the agent had to guess \\
        \midrule
        Llama-3.1-8B-Instruct (as agent) & I was close, but I didn't quite get it. Camaguey is a beautiful city in Cuba, and I should have guessed it earlier, given the clues you provided. Thank you for playing the game with me! It was a lot of fun, and I learned a lot from the clues you provided. I'll keep improving my question-asking skills and try to guess cities more accurately in the future. Camaguey is a great city, and I should have guessed it earlier, given the following clues: * Spanish colonial influence * Carnival celebrations * Tropical climate * Flat terrain with scenic hills and plains * Significant sugar production industry * Transatlantic slave trade history * Afro-Caribbean population * Mainland location * Significant rum industry. I'll keep these clues in mind for future games, and I hope to guess cities like Camaguey more accurately next time. Thanks again for playing the game with me! If you want to play again or try a different game, just let me know! \\
        \midrule
        GPT-4o-mini (as task environment) & Thank you for playing! I'm glad you enjoyed the game and found the clues helpful. Camaguey is indeed a fascinating city with a rich history and culture. If you ever want to play again or try something different, just let me know. I'm here to help! \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table*}

\newpage

\section{More on LLM Inference Settings}

For the 4 task groups (twenty questions, guess my city, customer service, and murder mystery) that use another LLM (GPT-4o-mini in our experiments) to simulate the task environment and the judge, we use temp 0.0 to generate environment and judge responses. We do this to keep the task environment and the judge as deterministic as possible for fair comparison of different agents. We let the environment and the judge generate at most 1024 tokens at each turn.

For the agent, we always sample using Min-p parameter $0.3$. Other than that, we set maximum number of tokens the agent can generate to be  128 for twenty questions, 512 for guess my city, and 1024 for all other task groups. 

\section{Additional Experimental Details}

All our models were trained using a single node consisting 8 NVIDIA L40S GPUs. For inference and generating data, we use single NVIDIA A40 GPUs. The API cost for generating the training datasets and running evaluation for the entire project is approximately 20,000 USD. To run all experiments once (both generating the data and running evaluations), we estimate API costs to be no more than 1000 USD.

\section{Public Release of Code, Model and Dataset}

\begin{enumerate}
    \item Our codebase to reproduce the results in this paper can be found here: \url{https://github.com/tajwarfahim/paprika}
    \item We also release the datasets used to train our models. Our supervised fine-tuning dataset can be found here: \url{https://huggingface.co/datasets/ftajwar/paprika_SFT_dataset}. The dataset used during RPO fine-tuning can be found here: \url{https://huggingface.co/datasets/ftajwar/paprika_preference_dataset}
    \item To fascilitate further research, we also release a Llama-3.1-8B-Instruct model checkpoint trained with \ours{}, it can be found here: \url{https://huggingface.co/ftajwar/paprika_Meta-Llama-3.1-8B-Instruct}
    \item Project website for this paper can be found here: \url{https://paprika-llm.github.io}
\end{enumerate}

\newpage

\section{More Details on Curriculum Learning} \label{appendix:curriculum}

First, we provide an example conversation used to generate the difficulty levels for twenty questions using gpt-4o-mini:

\begin{tcolorbox}[colback=gray!20, colframe=black, title=Twenty Questions Difficulty Generation]
\begin{lstlisting}[language=json, linewidth=0.99\textwidth, xleftmargin=10pt, xrightmargin=0pt]
{
    "judge_conversation": [
        {
            "role": "system",
            "content": "You are an expert judge of the game of 20 questions. I will give you a topic, and you must classify it into easy, medium or hard, based on an estimate of how easy it is to guess the topic, and an estimate of how many turns it will take to guess the topic. Respond in <EASY>, <MEDIUM> or <HARD>."
        },
        {
            "role": "user",
            "content": "Your topic is: Apple"
        },
        {
            "role": "assistant",
            "content": "<EASY>"
        }
    ]
}
\end{lstlisting}
\end{tcolorbox}

Secondly, to calculate Coefficient of variation on task $t$ (in this case, a single secret topic in twenty questions), we generate $n = 20$ trajectories for this task. Let these trajectories be $\tau_1, \ldots, \tau_n$. Let $|\tau_i|$ be the number of turns it takes for the agent to succeed in the $i$-th trajectory --- if the agent fails in the $i$-th trajectory, we set $\tau_i| = 20$, which is also the maximum number of turns in this environment. We use number of turns it takes the agent to solve the task as a proxy for reward, and measure the coefficient of variation on number of turns to compare different tasks.

Since we use a small number of trajectories, instead of using $\nu = \frac{s}{\bar{x}}$, where $s$ and $\bar{x}$ is the sample mean and standard deviation of $|\tau_i|$ respectively, we assume the unbiased estimator for coefficient of variation for normally distributed data instead~\citep{biometry2013Sokal}:

$$\nu = \left(1 + \frac{1}{4n}\right) \frac{s}{\bar{x}}$$

\newpage

\section{More Empirical Results} \label{section:additional_experimental_results}


\subsection{Success Rate Comparison with More Baselines}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/relative_improvement_in_success_rate_appendix.pdf}
    \caption{\textbf{(\ours{} improves success rate (pass@4))} Pass@4 success rate of \ours{} vs other models evaluated across temperatures 0.3, 0.7 and 1.0. See that \ours{}, when trained on trajectories from all task groups, shows significant improvement across all of them. We also compare against a Llama-3.1-8B-Instruct model finetuned on 100,000 trajectories randomly sampled from the WildChat dataset. This model performs poorly on all tasks, possibly due to model collapse.}
    \label{fig:all_environment_temperature_ablation_success_rate}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/relative_improvement_in_average_success_rate_appendix.pdf}
    \caption{\textbf{(\ours{} improves success rate (average))} Average success rate of \ours{} vs other models evaluated across temperatures 0.3, 0.7 and 1.0. As opposed to \cref{fig:all_environment_temperature_ablation_success_rate}, here we sample 4 trajectories per task, and plot the success rate averaged across all trajectories and all tasks within a task group.}
    \label{fig:all_environment_temperature_ablation_average_success_rate}
\end{figure}

\cref{fig:all_environment_temperature_ablation_success_rate} and \cref{fig:all_environment_temperature_ablation_average_success_rate} shows the pass@4 and average success rate across 9 task groups, respectively. We see that \ours{} improves Llama-3.1-8B-Instruct model's performance on both metrics. 

\subsection{Task Efficiency Comparison with More Baselines}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/relative_improvement_in_average_number_of_turns_appendix.pdf}
    \caption{\textbf{(\ours{} improves task efficiency on all task groups)} Average number of turns of \ours{} vs other models, evaluated across temperatures 0.3, 0.7 and 1.0. Note that we do not measure number of turns on the bandit best arm identification task, since it is fixed to be 20. \ours{} reduce the average number of turns it takes an LLM to solve tasks in all task groups, which quantifies the better strategic exploration abilities learned by \ours{}.}
    \label{fig:all_environment_temperature_ablation_num_turns}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/relative_improvement_in_number_of_turns_appendix.pdf}
    \caption{\textbf{(\ours{} improves task efficiency on all task groups)} Best-of-4 number of turns of \ours{} vs other models averaged across all evaluation tasks within a task group, evaluated across temperatures 0.3, 0.7 and 1.0. \ours{} improve task efficiency by reducing the number of turns it takes for the agent to solve the tasks.}
    \label{fig:all_environment_temperature_ablation_num_turns_best_of_4}
\end{figure}

\cref{fig:all_environment_temperature_ablation_num_turns} shows the average number of turns required for various models to solve a task, averaged across 4 trajectories per task and all evaluation tasks per task groups. Note that for bandit best arm selection, the number of turns is fixed, so we do not report it here. Similarly \cref{fig:all_environment_temperature_ablation_num_turns_best_of_4} shows best-of-4 number of turns for different LLMs, averaged across all evaluation tasks in a task group. \ours{} generally improve the task efficiency/strategic exploration capabilities of the model by lowering the number of turns taken to solve the tasks.

\subsection{More Results on Generalization}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/generalization_appendix.pdf}
    \vspace{-0.2cm}
    \caption{\footnotesize \textbf{(Testing generalization of \ours{} via leave-one-out experiments)} We test \ours{}'s zero-shot performance on unseen task groups by leave-one-out (LOO) experiments. \textbf{As opposed to \cref{fig:generalization_leave_one_out_experiments}, we report pass@4 success rate here instead of the average success rate.}}
    \label{fig:generalization_leave_one_out_experiments_appendix}
    \vspace{-0.2cm}
\end{figure*}

\cref{fig:generalization_leave_one_out_experiments_appendix} shows the pass@4 success rate (as opposed to \cref{fig:generalization_leave_one_out_experiments}, which shows average success rate) for leave-one-out (LOO) experiments.
 

\subsection{Evaluation on LMRL-Gym split}

In our paper, we construct a larger set of secret topics for twenty questions and guess my city, compared to LMRL-Gym~\citep{abdulhai2023lmrl}. Our training and evaluation sets are filtered to not have any overlap with the LMRL-Gym dataset. However, for the sake of fair comparison, we also report the performance of \ours{} on this dataset. \cref{fig:lmrl_gym_guess_my_city} and \cref{fig:lmrl_gym_twenty_questions} shows the performance of \ours{} on the LMRL-Gym split of guess my city and twenty questions, respectively. We see that the gains observed on our evaluation split translated to the set of secret topics in LMRL-Gym as well.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/lmrl_gym_guess_my_city.pdf}
    \caption{\textbf{(\ours{} evaluated on guess my city, LMRL-Gym split)} We evaluate our method on the LMRL-Gym split (disjoint from our training and test sets) for guess my city and report average task success rate (4 attempts per task). We see that the gains we saw on our test set mostly translates to this dataset as well.}
    \label{fig:lmrl_gym_guess_my_city}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/lmrl_gym_twenty_questions.pdf}
    \caption{\textbf{(\ours{} evaluated on twenty questions, LMRL-Gym split)} We evaluate our method on the LMRL-Gym split (disjoint from our training and test sets) for twenty questions and report average task success rate (4 attempts per task). We see that the gains we saw on our test set mostly translates to this dataset as well.}
    \label{fig:lmrl_gym_twenty_questions}
\end{figure}

\subsection{Experiments on Modified Wordle to Further Test Generalization}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/wordle_modified.pdf}
    \caption{\textbf{(Further tests for generalization)} \ours{} evaluated on a modified version of wordle, where the agent needs to guess words that do not have five letters. We report average success rate over 1000 tasks, with shaded regions representing standard errors over 3 random seeds. \ours{} retain good strategies learned from other tasks and outperforms the starting model (Llama-3.1-8B-Instruct) without explicitly being trained on this task group.}
    \label{fig:wordle_modified}
\end{figure}

We provide one more experiment to test generalization of \ours{}: we create a modified version of wordle, where the agent has to guess words consisting of 4, 6, 7, 8, 9 or 10 letters (excluding the 5-letter words used by original wordle) within 10 turns using a similar system of task environment feedback as wordle. \cref{fig:wordle_modified} shows our results: \ours{} retain good strategies learned from the other 10 task groups and outperform Llama-3.1-8B-Instruct on this new task group without being trained on it.

\subsection{Ablation Study over Different Finetuning Stages of \ours{}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/negative_gradients.pdf}
    \caption{\textbf{(Comparison between \ours{} with SFT only vs SFT followed by RPO)} Average success rate comparison between \ours{} when we only run supervised finetuning, vs regular \ours{} which has an SFT stage followed by RPO finetuning. Our ablation study shows that the RPO stage is necessary and generally gives a boost in performance on all cases.}
    \label{fig:sft_vs_rpo}
\end{figure}

An interesting question to ask is how important is the RPO stage for improving task success rate for \ours{}: can we potentially get all the benefits with supervised fine-tuning (SFT) only? To answer this question, we run an ablation over 4 task groups where we evaluate both the SFT checkpoint and the checkpoint obtained from further fine-tuning the SFT model with RPO. \cref{fig:sft_vs_rpo} shows our results: on all 4 task groups, RPO employing negative or dispreferred trajectories improves performance beyond the SFT model, similar to the observation made by~\citet{tajwar2024preferencefinetuningllmsleverage}.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/comparison_with_more_base_models.pdf}
    \vspace{-0.3cm}
    \caption{\footnotesize \textbf{(Performance comparison between different models)} Average success rate of 3 different models with comparable parameter count, namely Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Mistral-7B-Instruct-v0.3. We evalute the performance of these models on 3 representative task groups, with shaded areas representing standard error over 3 random seeds.}
    \label{fig:comparison_between_more_base_models}
    \vspace{-0.2cm}
\end{figure*}

\subsection{Finetuning on regular multiturn data does not help}

A compelling hypothesis is that the instruct model has seen comparatively fewer multiturn trajectories during training, and finetuning on such trajectories may naturally lead to performance improvement in sequential decision-making tasks, making our complex data generation process unnecessary. To test this, we finetune the Llama-3.1-8B-Instruct model on 100,000 English language trajectories randomly sampled from WildChat~\citep{zhao2024wildchat1mchatgptinteraction}, which contains multiturn interactions between GPT-4 and human users (we use the same hyperparamers as our other experiments). The results in \cref{fig:all_environment_temperature_ablation_success_rate,fig:all_environment_temperature_ablation_num_turns} show significant performance degradation on all task groups resulting from this fine-tuning. We speculate that this happens because WildChat interactions prioritize coherence rather than information gathering, and training specifically on tasks that require strategic exploration will be necessary to improve LLMs' sequential decision-making abilities.

\subsection{Performance comparison between different starting models}

In our work, we use a Llama-3.1-8B-Instruct model for all of our experiments. For the sake of completeness, we have also run evaluations on two other models with comparable parameter count, namely Qwen-2.5-7B-Instruct~\citep{qwen2025qwen25technicalreport} and Mistral-7B-Instruct-v0.3~\citep{jiang2023mistral7b}. \cref{fig:comparison_between_more_base_models} shows their average success rate on 3 representative task groups: with the performance ranking being Llama-3.1-8B-Instruct $>$ Qwen-2.5-7B-Instruct $>$ Mistral-7B-Instruct-v0.3 on all 3 of them. We also experimented with the more recent reasoning models, particularly DeepSeek-R1 distilled Llama-8B and Qwen-7B models~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. However, these models generate very long chain-of-thoughts, and we could not obtain a final answer from them in our experiments even after generating 10,000 tokens. Overall, it would be interesting to study how recent reasoning models perform on our sequential decision making tasks or if using online RL on our tasks can lead to reasoning models for our tasks. We leave this direction for future work. We also hypothesize that the gains from \ours{} are dependent on the base model's quality and diversity since we use self-generated data for training. Due to computational constraints, we do not fine-tune other base models with \ours{} and leave this direction also for future research.

\newpage 

\subsection{Details on Standard Benchmarks} \label{appendix:standard_evaluations}

To show that \ours{} does not harm the starting model's regular capabilities, we test \ours{}-finetuned models on a set of standard tasks, namely MT-Bench~\citep{zheng2023judgingllmasajudgemtbenchchatbot,kwan2024mt}, AlpacaEval~\citep{dubois2023alpacafarm,dubois2024length,alpaca_eval}, GPQA~\citep{rein2023gpqagraduatelevelgoogleproofqa}, Math~\citep{hendrycks2021measuringmathematicalproblemsolving}, MMLU-Pro~\citep{wang2024mmluprorobustchallengingmultitask} and IFEval~\citep{zhou2023instructionfollowingevaluationlargelanguage}. See the following for details on how we run our tests:

\begin{enumerate}
    \item For MT-Bench, we use the code from this repo --- \url{https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/README.md} --- to run our evaluations.
    \item For AlpacaEval, we also use the original codebase provided here to run our evaluations: \url{https://github.com/tatsu-lab/alpaca_eval}
    \item For other tasks, we use the codebase provided by Llama Recipes to produce the numbers for all models: \url{https://github.com/meta-llama/llama-cookbook/blob/2501f519c7a775e3fab82ff286916671023ca9c6/tools/benchmarks/llm_eval_harness/meta_eval/README.md}
\end{enumerate}

For MT-Bench, we report the usual scores. For AlpacaEval, we report length controlled winrate~\cite{dubois2024length} against GPT-4-turbo. For GPQA, we report the strict match accuracy scores. For Math, following the recipe described above, we report accuracies only on the Math (Hard) subset, using exact match. For MMLU-Pro, we also report the exact match accuracy, and for IFEval we report instruction level loose accuracy.


\section{Limitations of \ours{}: Evaluation on Standard Bandit}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/original_bandit_task.pdf}
    \caption{\textbf{(Evaluation on the bandit task from \citet{krishnamurthy2024largelanguagemodelsexplore})} We evaluate various LLMs on the original bandit task proposed by \citet{krishnamurthy2024largelanguagemodelsexplore}. While \ours{} show some improvement when the bandit tasks have a smaller number of arms over Llama-3.1-8B-Instruct, we see the gap reduce as the number of arms increase.}
    \label{fig:original_bandit_task}
\end{figure}

As a sanity check, we also evaluate \ours{}-finetuned models on the bandit task proposed by~\citet{krishnamurthy2024largelanguagemodelsexplore}. \cref{fig:original_bandit_task} shows our results, where we report empirical regret averaged across 100 trials. We use the following definition of regret: if the optimal arm has reward $r^*$, and $r(\hat{a}_t)$ is the reward of the arm chosen by a policy at timestep $t$, then empirical regret is calculated as $\sum_{t = 1}^T \left[r^* - r(\hat{a}_t)\right]$, where $T$ is the total number of timesteps.

\cref{fig:original_bandit_task} demonstrates the limitations of \ours{}: without any explicit training on this bandit task group, \ours{} improves empirical regret over Llama-3.1-8B-Instruct, but only when the number of arms is small. We see that the gap vanishes when the number of arms grow. \citet{nie2024evolveevaluatingoptimizingllms} shows that training on synthetic trajectories obtained from a UCB algorithm improves LLMs' capabilities on this task group. We hypothesize that one could get the same result by directly running reinforcement learning on the bandit task group, without requiring access to an optimal algorithm like UCB. We leave this direction for future work.

\section{Example Trajectories} \label{appendixSection:example_trajectories}

In this section, we provide some qualitative example of behaviors learned by \ours{}, to demonstrate that \ours{} imbues LLMs with better decision making capabilities. 

The first example is provided in \cref{tab:twenty_questions_example_trajectories_orca}: the example is from the twenty questions task group, with the agents being required to guess `orca'. We show clear differences in the behaviors of Llama-3.1-8B-Instruct and \ours{}, the questions asked by \ours{} is more concise and reaches the final topic quicker. 

The second example is also from the twenty questions task group, provided in \cref{tab:twenty_questions_example_trajectories_prime_numbers}, where the agents are required to guess a concept, with the current answer being `primary numbers'. Llama-3.1-8B-Instruct asks redundant questions like if the concept can be held in someone's hand, or if the concept is a type of rock or mineral. \ours{} demonstrate much better quality questions and is able to guess the concept in 8 turns, whereas Llama-3.1-8B-Instruct is not able to guess it within 20 turns for all 4 attempts we made (we only show the first 9 turns for the sake of brevity).

The final exmaple is from the wordle task group, provided in \cref{tab:wordle_example_trajectory_toast}. Notice the lack of conciseness in Llama-3.1-8B-Instruct's chain-of-thoughts compared to \ours{}. Llama-3.1-8B-Instruct also makes bad guesses/wrongfully reasons against the correct answer. 

Overall, \ours{} improves over Llama-3.1-8B-Instruct both quantitatively and qualitatively based on our limited evaluation of the generated trajectories. We leave a detailed study of each model's behavior/generation quality for future work.

\newpage

\begin{table*}[h]
    \caption{Behavior comparison on twenty questions, where the secret topic to guess is an animal, with the current answer being `orca'. We provide the first 11 turns and omit the rest for brevity, Llama-3.1-8B-Instruct fails to correctly guess this topic in all 4 attempts within 20 turns. Also, notice the bad questions colored in red. \ours{} ask the irrelevant question of whether the animal is a shark after confirming it is a mammal. Llama-3.1-8B-Instruct asks whether the animal is a human after confirming it lives on water.}
    \vspace{0.2cm}
    \label{tab:twenty_questions_example_trajectories_orca}
    \centering
    \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{|p{1cm} | p{7cm} | p{7cm}|}
        \toprule
        Role & Llama-3.1-8B-Instruct & \ours{} \\
        \midrule
        Agent & I'll start asking questions to try to guess the animal. Here's my first question: 1. Does the animal have four legs?  & Is the animal a mammal? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out some possibilities. Here's my next question: 2. Can the animal fly? & Does the animal primarily live on land? \\
        \hline
        Env & No & No \\
        \hline
        Agent & So the animal doesn't have four legs and can't fly. Here's my next question: 3. Does the animal live in water? & Does the animal live in saltwater? \\
        \hline
        Env & Yes & Yes \\
        \hline
        Agent & So the animal lives in water. Here's my next question: 4. Does the animal have a backbone? & Is the animal a carnivore? \\
        \hline
        Env & Yes & Yes \\
        \hline
        Agent & Since the animal has a backbone and lives in water, I'm starting to get a few ideas. Here's my next question: 5. Does the animal have gills? & Does the animal have fins? \\
        \hline
        Env & Yes & Yes \\
        \hline
        Agent & So the animal lives in water, has a backbone, but doesn't have gills. Here's my next question: 6. Does the animal primarily eat fish? & \textcolor{red}{Is the animal a shark?} \\
        \hline
        Env & Yes & No \\
        \hline
        Agent & So the animal lives in water, has a backbone, eats fish, and doesn't have gills. Here's my next question: 7. Is the animal a mammal? & Does the animal have a dorsal fin? \\
        \hline
        Env & Yes & Yes \\
        \hline
        Agent & Since the animal is a mammal, lives in water, eats fish, and has a backbone, I'm starting to get a good idea. Here's my next question: 8. Does the animal give birth to live young? & Does the animal have a relatively small size, typically less than 6 feet long? \\
        \hline
        Env & Yes & No \\
        \hline
        Agent & So the animal is a mammal, lives in water, eats fish, has a backbone, gives birth to live young, and doesn't have gills. Here's my next question: 9. \textcolor{red}{Is the animal a human?} & Is the animal an orca (also known as a killer whale)? \\
        \hline
        Env & No & Goal reached \\
        \hline
        Agent & So the animal is a mammal, lives in water, eats fish, has a backbone, gives birth to live young, doesn't have gills, and isn't a human. Here's my next question: 10. Does the animal primarily live in oceans? &  \\
        \hline
        Env & Yes &  \\
        \hline
        Agent & So the animal is a mammal, lives in oceans, eats fish, has a backbone, gives birth to live young, doesn't have gills, and isn't a human. Here's my next question: 11. Does the animal have a large body size, typically over 1 meter in length? & \\
        \hline
        Env & Yes & \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table*}

\newpage

\begin{table*}[h]
    \caption{Behavior comparison on twenty questions, where the secret topic to guess is a concept, with the current answer being `prime numbers'. We provide the first 9 turns and omit the rest for brevity, Llama-3.1-8B-Instruct fails to correctly guess this topic in all 4 attempts.}
    \vspace{0.2cm}
    \label{tab:twenty_questions_example_trajectories_prime_numbers}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|p{1cm} | p{7cm} | p{7cm}|}
        \toprule
        Role & Llama-3.1-8B-Instruct & \ours{} \\
        \midrule
        Agent & Here's my first question: 1. Is the concept you're thinking of something that exists in the real world? & I'm excited to play 20 Questions. Here's my first question: 1. Is the concept you're thinking of something that exists in the physical world? \\
        \hline
        Env & Yes & No \\
        \hline
        Agent & Here's my next question: 2. Is the concept you're thinking of something that can be held in your hand? & That helps to rule out a physical object or location. Here's my next question: 2. Is the concept you're thinking of an abstract idea or a theory? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out smaller objects. Here's my next question: 3. Is the concept you're thinking of a living thing? & That gives me a good direction to explore. Here's my next question: 3. Is the concept you're thinking of related to a field of study or a discipline, such as mathematics, philosophy, or science? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out animals and plants. Here's my next question: 4. Is the concept you're thinking of man-made? & That helps to narrow it down to a specific area of knowledge. Here's my next question: 4. Is the concept you're thinking of a mathematical concept or theory? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & So the concept is a natural part of the real world, but it's not a living thing. Here's my next question: 5. Is the concept you're thinking of a geographical feature? & That gives me a good starting point to explore mathematical ideas. Here's my next question: 5. Is the concept you're thinking of a fundamental concept in mathematics, such as a number, a geometric shape, or a mathematical operation? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out mountains, rivers, and other natural landforms. Here's my next question: 6. Is the concept you're thinking of related to the weather? & That helps to focus on basic mathematical building blocks. Here's my next question: 6. Is the concept you're thinking of a number? \\
        \hline
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out things like clouds, storms, and other weather-related phenomena. Here's my next question: 7. Is the concept you're thinking of a natural phenomenon that occurs in the sky? & Now we're getting close to a specific number. Here's my next question: 7. Is the concept you're thinking of a whole number (not a fraction or decimal)? \\
        \hline 
        Env & No & Yes \\
        \hline
        Agent & That helps to rule out things like the sun, moon, stars, and other celestial bodies. Here's my next question: 8. Is the concept you're thinking of a type of rock or mineral? & That helps to rule out fractions and decimals. Here's my next question: 8. Is the concept you're thinking of a prime number? \\
        \hline 
        Env & No & Goal reached \\
        \hline 
        Agent & That helps to rule out specific types of rocks and minerals. Here's my next question: 9. Is the concept you're thinking of related to water in some way? & \\
        \hline
        Env & No & \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table*}

\newpage

\begin{table*}[h]
    \caption{Behavior comparison on Wordle, where the agents need to guess the secret word `toast'. We omit the task environment feedback for the sake of brevity. Notice the conciseness and better quality of the guesses made by \ours{}. Also notice (marked in red) that Llama-3.1-8B-Instruct reaches the correct answer but incorrectly deduces it is not the correct answer and thus makes a wrong guess, showing poor decision making abilities compared to \ours{}.}
    \vspace{0.2cm}
    \label{tab:wordle_example_trajectory_toast}
    \centering
    \resizebox{0.7\textwidth}{!}{
        \begin{tabular}{|p{7cm} | p{7cm}|}
        \toprule
        Llama-3.1-8B-Instruct & \ours{} \\
        \hline
        \texttt{<Think>} Since it's the first guess, I'll choose a common five-letter word that has a good mix of letters. I'll start with a word that has a relatively high frequency of letters in the English language. A good starting point would be a word like ``house''. This word has a good balance of vowels and consonants, and it's a common enough word that it's likely to have some letters in common with the secret word. \texttt{</Think>} \texttt{</Answer>} house \texttt{</Answer>} & \texttt{<Think>} Since it's the first guess, I'll choose a common and relatively short five-letter word to maximize the chances of getting at least one correct letter. I'll go with a word that has a mix of vowels and consonants. My first guess will be ``house''. \texttt{<Think>} \texttt{<Answer>} house \texttt{</Answer>} \\
        \hline
        \texttt{<Think>} Since the second letter ``o'' and fourth letter ``s'' are correct and in the correct position, I'll keep those letters. I'll replace the first letter ``h'' with a new letter that is commonly used in words. I'll also replace the third letter ``u'' with a new letter that is often paired with ``o'' and ``s''. A good combination could be the letters ``t'' and ``e'' are not in the word, so I will try to avoid them. I'll try the word ``torns'' but since ``t'' is not a good fit, I will try ``ports'' instead. \texttt{</Think>} \texttt{<Answer>} ports \texttt{</Answer>} & \texttt{<Think>} Since ``o'' and ``s'' are in the correct positions, my next guess will focus on surrounding them with letters that are likely to appear in common five-letter words. I'll choose a word that has a mix of vowels and consonants, and try to utilize the given positions of ``o'' and ``s''. My next guess will be ``boost''. \texttt{</Think>} \texttt{<Answer>} boost \texttt{</Answer>} \\
        \hline
        \texttt{<Think>} Since the second letter ``o'' is correct and in the correct position, I'll keep that. The fourth letter ``t'' and fifth letter ``s'' are correct but in different positions, so I'll try to swap them. I'll replace the first letter ``p'' with a new letter that is commonly used in words. I'll also replace the third letter ``r'' with a new letter that is often paired with ``o'', ``t'', and ``s''. A good combination could be the letters ``a" or ``n". I'll try the word ``boats" but since ``b'' is not a good fit, I will try ``notes'' instead. \texttt{</Think>} \texttt{<Answer>} notes \texttt{</Answer>} & \texttt{<Think>} With ``o'', ``s'', and ``t'' in the correct positions, and another ``o'' in a different position, my next guess will try to utilize these letters and find the correct placement of the remaining ``o''. I'll choose a word that has a common combination of letters. My next guess will be ``coast''. \texttt{</Think>} \texttt{<Answer>} coast \texttt{</Answer>} \\
        \hline
        \texttt{<Think>} Since the second letter ``o'' is correct and in the correct position, I'll keep that. The third letter ``t'' and fifth letter ``s'' are correct but in different positions, so I'll try to swap them. I'll replace the first letter ``n'' with a new letter that is commonly used in words. I'll also replace the fourth letter ``e'' with a new letter that is often paired with ``o'', ``t'', and ``s''. A good combination could be the letters ``a'' or ``l''. I'll try the word ``lots'' but since ``l'' is not a good fit, I will try ``mots'' instead. However, ``m'' is not a good fit, so I will try ``dots'' instead. \texttt{</Think>} \texttt{<Answer>} dots \texttt{</Answer>}
        & \texttt{<Think>} With ``o'', ``s'', ``t'', and ``a'' in the correct positions, my next guess will try to find the correct word by utilizing these letters. Since all the letters are in the correct positions except for the first letter, my next guess will be a common five-letter word that fits the given letters. My next guess will be ``boast''. \texttt{</Think>} \texttt{<Answer>} boast \texttt{</Answer>} \\
        \hline
        \texttt{<Think>} Since the second letter ``o'' is correct and in the correct position, I'll keep that. The third letter ``t'' and fifth letter ``s'' are correct but in different positions, so I'll try to swap them. I'll replace the first letter ``d'' with a new letter that is commonly used in words. I'll also replace the fourth letter with a new letter that is often paired with ``o'', ``t'', and ``s''. A good combination could be the letters ``a'' or ``l''. I'll try the word ``toats'' but since ``a'' is not a good fit, I will try ``toats'' is not a good fit, \textcolor{red}{so I will try ``toast'' instead. However, ``t'' is in the wrong position, so I will try ``costs'' instead.} \texttt{</Think>} \texttt{<Answer>} costs \texttt{</Answer>} & \texttt{<Think>} Given the letters ``o'', ``a'', ``s'', and ``t'' in the correct positions, my next guess will be a common five-letter word that fits these letters. However, since I've already used up all my attempts, I'll take a chance and guess a word that is very similar to ``boast''. My next guess will be ``coast'' is already tried, so I will guess ``toast''. \texttt{<Think>} \texttt{<Answer>} toast \texttt{</Answer>} \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table*}
