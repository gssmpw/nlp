\subsection{Task Design}


The first component of \ours{} is to design a set of task groups that we can evaluate and train LLMs on. The task groups we want should have the following desired properties: \textbf{(1)} they are purely text based, \textbf{(2)} they require multi-turn interaction, where the agents have to both understand prior history in its context and choose actions that maximize the probability of success in the future, \textbf{(3)} they are partially observable, i.e., the observations do not capture the full state or hidden information, so the agents must simultaneously explore to reveal more information and exploit to solve the task efficiently, \textbf{(4)} they are diverse and require different strategies to succeed. 


With these requirements in mind, we design 10 task groups in our paper. On all of them, we employ an LLM as the agent that is given a task it needs to solve through sequential interaction with the task-specific environment, which provides both observations for intermediate timesteps given the agent's actions and also a task reward at the end of an episode. 
For tasks requiring general knowledge about the world to generate intermediate observations, we employ another LLM (typically GPT-4o-mini) as the environment. For tasks that have rule-based observations and rewards, we find that using hardcoded programs as the verifier/observation generator is more reliable than LLMs, similar to~\citet{deepseekai2025deepseekr1incentivizingreasoningcapability}. In order to prevent reward hacking, we also use either another LLM or a hardcoded program as a judge to filter out unsuccessful trajectories that got incorrectly labeled as successful by the task environment (see \cref{section:environment_hacking} for more on environment hacking). We also find that for task groups requiring complex reasoning, letting the agent think using chain-of-thought (COT) prompting~\citep{wei2022chain, kojima2022large} before generating a final answer improves its performance significantly. We provide a brief description of our task groups here, please refer to \cref{tab:environment_summary} for their summary and \cref{section:appendix_environment_design} for more details.

Following prior work~\citep{abdulhai2023lmrl}, we include classic guessing games like \textit{twenty questions} and \textit{guess my city} in our list of task groups. They require guessing a secret topic as quickly as possible by asking a sequence of questions and observing the answers. We also employ \textit{Wordle} and \textit{Mastermind}, where the agent needs to guess a secret 5-letter word and 4-digit code respectively. The environments for these task groups provide feedback in terms of similarity between the guess and the target word/code, and the agent needs to refine their guesses in future turns to maximize information gathering. We design \textit{customer service} and \textit{murder mystery} as dynamic text-based task groups: an LLM plays the role of the task environment, which is provided with the criterion for task success and generates dynamic intermediate observations based on this criterion. 

A desirable capability in LLMs is to code and refine based on interpreter feedback. To simulate this process with a toy case, we design \textit{Cellular Automata}, where the agent needs to make inferences about the transition rule in 1D elementary cellular automata~\citep{wolfram1983statistical, cook2004universality} by observing inputs and outputs. The agent receives the outputs generated from their predicted transition rule and they have to refine their predictions based on it. Next, we incorporate \textit{Minesweeper} and \textit{Battleship} based on classical games, which require the agent to interact with 2D grids to find hidden items within a fixed number of turns and refine their guesses based on per-turn observations. 

Finally, we incorporate a modified version of the multi-armed bandit~\citep{slivkins2024introductionmultiarmedbandits} task group from prior works~\citep{krishnamurthy2024largelanguagemodelsexplore,nie2024evolveevaluatingoptimizingllms} with the following distinctions: \textbf{(1)} we let the agent employ chain-of-thought reasoning before choosing arms so that they can transfer good strategies learned from other tasks, \textbf{(2)} we let the agent interact with the task environment in a multiturn way, \textbf{(3)} instead of reducing regret, we work on the bandit best arm selection~\citep{audibert:hal-00654404,wang2024bestarmidentificationfixed} problem, where we let the agent choose arms and observe rewards for a fixed number of turns and then measure its accuracy in deciding the arm with the highest reward. This is done to reduce computational cost over generating COTs for a large number of turns, since the difference in regret between different models is not meaningful when the number of turns is not large enough.
