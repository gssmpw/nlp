\section{Discussion}

In this paper, we presented a scalable fine-tuning method to improve multi-turn decision making abilities of LLMs. Moreover, we showed that the strategies learned by the LLM from our method can generalize zero-shot to unseen tasks. There are a few limitations to our approach. Firstly, we use rejection sampling on self-generated data to teach the model better behaviors. In order to get good performance, the starting model need to exhibit good behavior within a reasonable generation budget, so \ours{} would perform worse in the absence of a good base model. Next, we use offline preference tuning algorithms to train our models due to lack of computational resources. A possible future direction for our work is to run online RL on diverse tasks instead: due to its recent success in other domains~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, we expect it will give a larger improvement in LLMs' in-context RL capabilities. Our environments, despite being designed with the help of GPT-4o-mini, required a lot of human effort for implementation. A new axis of improvement can be training an LLM to scalably generate suitable tasks that can then be used to train the agent. Finally, the performance of our curriculum learning algorithm heavily depends on the quality of the task group clusters which is not ideal, and one can study possible improvements of this algorithm. We leave these directions for future work.

\subsection*{Acknowledgement}

This work was supported in part by the U.S. Army Futures Command under Contract No. W519TC-23-C-0030. Moreover, this research also used the Delta advanced computing and data resource which is supported by the National Science Foundation (award OAC 2005572) and the State of Illinois, as part of compute grants approved by ACCESS~\citep{access_compute}. FT and YJ gratefully acknowledge Samuel Sokota, Daman Arora, Andrea Zanette, Yuda Song, Gaurav Ghosal, Yutong He, So Yeon Min, Kevin Li, Wen-Tse Chen, Xintong Duan and other members of Russ, Auton, Locus and AIRe lab for feedback received on an earlier versions of this work. FT greatly benefited from his discussions with Prof. Aviral Kumar and his lab's computational resources. YJ gratefully acknowledges the support of the Google PhD Fellowship.




