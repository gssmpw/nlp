\subsection{Dataset construction}

In order to learn from these task groups, we must first generate data from them. It is crucial that the data we generate are diverse which would allow the model to learn different strategies without the risk of overfitting. We accomplish this by generating a large number of trajectories at a high temperature with Min-p sampling~\citep{nguyen2024turning}. Min-p sampling works by using an adaptive threshold $p_\text{scaled} \propto p_\text{max}$, where $p_\text{max}$ is the highest probability predicted by the model on the next token, to truncate the vocabulary to tokens that have a probability larger than $p_\text{scaled}$ and sample from them --- this enables us to generate diverse yet coherent trajectories at a higher temperature.


For each task in a set of chosen tasks (e.g., uniformly sampled), we generate $n_\text{sample}$ trajectories and then construct a preference pair $(h_{w}, h_{l})$ where $h_{w}$ is the highest scoring trajectory (trajectory that succeeds and does so at the fewest number of turns) and $h_{l}$ is randomly sampled from the lower scoring (failed  or takes substantially more turns to succeed) trajectories. We choose $h_l$ randomly instead of choosing the worst one to increase the diversity of our dataset. We treat $h_w$ and $h_l$ as proxies for desirable and undesirable behaviors. A dataset $\mathcal{D} = \left\{\left(h^{w}, h^{l}\right)^{(i)}\right\}_{i=1}^N$ is a collection of such trajectory pairs.

\subsection{Optimization}
\label{sec:opt}

\paragraph{Supervised fine-tuning.} If we take the winning episodes as the expert behavior, then we can discard the losing episode and maximize the likelihood of winning episodes:

\begin{align}
    \mathcal{L}_\text{SFT}(\mathcal{D}) = \mathbb{E}_{\mathcal{D}} \left[ \frac{1}{\sum_{t=0}^{|h_w|}|a_t^w|}\sum_{t=0}^{|h_w|} \log \pi_\theta \left(a^w_t \mid h^w_{:t}\right) \right].
\end{align}
where $|a|$ is the number of tokens for the agent response (discarding the environment generation). This is akin to rejection sampling fine-tuning~\citep{gulcehre2023reinforcedselftrainingrestlanguage,dong2023raft,mukobi2023superhfsupervisediterativelearning} seen in prior work.

\paragraph{Direct preference optimization.} A popular approach for finetuning LLMs is DPO~\citep{rafailov2024direct} where one directly optimizes the Bradley-Terry model~\citep{bradley1952rank} for preferences. In our setting, each trajectory consists of multiple rounds of interactions so the original DPO objective does not apply. We instead use a multi-turn version of DPO introduced in ~\citet{rafailov2024rqlanguagemodel}:
\begin{multline}
    \mathcal{L}_\text{DPO}(\gD) = \E_{\gD}\Bigg[\log \sigma\Bigg( 
    \sum_{t=0}^{|h^w|}\beta \log\frac{\pi_\theta(a_t^w \mid h_{:t}^w)}{\pi_\text{ref}(a_t^w \mid h_{:t}^w)} \\
    - \sum_{t=0}^{|h^l|}\beta \log\frac{\pi_\theta(a_t^l \mid h_{:t}^l)}{\pi_\text{ref}(a_t^l \mid h_{:t}^l)}
    \Bigg)\Bigg]
\end{multline}

where $a_t^w$ is the action tokens generated by the model at turn $t$ in the preferred trajectory $h^w$.
$\pi_\text{ref}$ is the reference policy, for which we use the initial model.
The main difference with standard DPO here is that we only calculate the loss on the action tokens --- the log probability ratios of the environment generated tokens are not included in the loss.

We note that we use DPO because it is less compute intensive. DPO allows us to decouple the data collection and policy improvement steps and offload them on different machines. However, in principle, one could also employ online RL with more resources. Following prior work that shows the efficacy of online RL compared to offline algorithms~\citep{xu2024dposuperiorppollm,tajwar2024preferencefinetuningllmsleverage}, we expect doing \ours{} with online RL would lead to even stronger results.

\paragraph{Combining objectives.} 

Finally, prior works have noted DPO having the unintended effect of reducing the probability of preferred trajectories as well, known as unintentional unalignment~\citep{razin2024unintentionalunalignmentlikelihooddisplacement}, which can affect model performance. The RPO objective~\citep{pang2024iterativereasoningpreferenceoptimization}, by combining SFT and DPO loss, has shown promising results in mitigating this issue. Formally, the RPO loss is:

\begin{equation} \label{eq:rpo_formula}
    \mathcal{L}_{\text{RPO}}(\mathcal{D}) = \mathcal{L}_\text{DPO}(\gD) + \alpha \mathcal{L}_\text{SFT}(\gD)
\end{equation}

where $\alpha$ is a hyper-parameter. Following ~\citet{pang2024iterativereasoningpreferenceoptimization}, we set $\alpha$ to be 1.0 for the rest of this paper.

