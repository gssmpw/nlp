\subsection{Scalable Online Curriculum Learning} \label{section:curriculum}
The core idea of \ours{} is to fine-tune the model on a large number of decision making problems to acquire general decision making ability. It is relatively easy to design a large number of tasks, but it is harder to decide which task to train on. A major obstacle is that different tasks may have a large range of difficulty. Unlike pretraining where the model can generally make progress on any given sample (i.e., decrease next-token prediction loss), an RL agent cannot make meaningful progress without collecting good experience. As such, if a task is too difficult for the current model, the model would not generate trajectories with meaningful learning signals. Since generating a trajectory is expensive, it stands to reason that we want to prioritize the tasks where the model can make meaningful progress, which is a form of curriculum learning~\citep{bengio2009curriculum}.

Without additional assumptions, the only way to know whether a task would yield good learning signals is to actually perform a rollout in that task, which is expensive.
In fact, in this particular scenario, the major cost for training is actually data generation rather than model updates.
As such, this naive approach would not save us time or computation.
A desideratum for an efficient curriculum is the ability to know whether certain tasks will yield data with learning signals without actually performing the rollout.
A natural assumption is that similar tasks would have similar levels of learning signal. 
These groupings can be obtained through meta data or prior knowledge.\footnote{While this requirement may seem restrictive, we believe assumptions of similar effects are likely needed for any form of curriculum learning to be computationally efficient.}

\paragraph{Measuring learning potential.} We will use $h \sim \pi \circ \tau$ to denote sampling one episode from the task $\tau$ using the policy $\pi$. The average performance of $\pi$ on $\tau$ is $R_\pi(\tau) = \E_{h \sim \pi \circ \tau }\left[r(h)\right]$ and the variance is $\sigma^2_\pi(\tau) = \E_{h \sim \pi \circ \tau }\left[(r(h) - R_\pi(\tau))^2\right]$. Based on these, we can define:
\begin{align}
\label{eq:nu}
    \nu_\pi(\tau) = \frac{\sqrt{\sigma^2_\pi(\tau)}}{R_\pi(\tau)}.
\end{align}
This quantity is known as the coefficient of variation in statistics, a dimensionless quantity that measures the population's variability relative to the mean. 

We argue that this quantity is an ideal measure of the learning potential for a single task. DPO requires a pair of positive and negative samples~\footnote{We hypothesize this quantity would also apply to online RL since if all sampled trajectories have the same reward the policy gradient update would be $0$.}. Intuitively, the pair should be sufficiently different so the model can tell the two apart --- for example, prior work~\citep{pal2024smaugfixingfailuremodes} has shown that DPO suffers when the edit distance between preferred and dispreferred responses is not large enough. Variance naturally measures the possibility of getting diverse trajectories from sampling. On the other hand, different tasks could have vastly different reward scales. Without loss of generality, if we assume that all rewards are positive, the average reward of each task is a measurement of the reward scale. Normalizing the standard deviation with the reward scale allows us to compare different tasks directly.

\begin{algorithm}[t]
\label{alg:sampling}
\caption{Task selection with UCB}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Number of arms $K$, batch size $B$, number of samples $C$, number of rounds $T$
\STATE \textbf{Initialize:} $s_k=0$, $n_k=0$, \texttt{Buffer}
\FOR{each round $t = 1, 2, \dots, T$}
    \STATE Compute $\theta_k = \tfrac{s_k}{n_k} + \sqrt{\tfrac{2 \log \sum_{k=1}^K n_k}{n_k}}$ for each $k$
    \STATE Select $k^\star = \argmax_k \theta_k$
    \STATE Sample $\tau$ from each group $k^\star$
    \STATE Sample $C$ trajectories from $\tau$ and add to \texttt{Buffer}
    \STATE Compute an estimate for $\hat{\nu}_\pi(\tau)$ using Eq~\ref{eq:nu}
    \STATE Update: $s_{k^\star} = s_{k^\star}  + \hat{\nu}_\pi(\tau), \,n_{k^\star} = n_{k^\star}  + 1$
\ENDFOR
\STATE Construct $\gD$ from \texttt{Buffer} and train the model $\pi$
\end{algorithmic}
\end{algorithm}

\paragraph{Sampling tasks.}Each group contains a large number of different tasks. 
Since it is infeasible to evaluate $\nu_\pi(\tau)$ for all tasks, we instead sample tasks from the group. 
This induces a scalar distribution that describes the distribution of $\nu_\pi(\tau)$ for all tasks in the group $G$.
Given a collection of $K$ groups $(G_1, \dots, G_K)$, a reasonable objective would be to maximize the learning potential of the tasks sampled. This problem can be formulated as a multi-armed bandit (MAB). Many algorithms for MAB exist; for simplicity, we choose the Upper Confidence Bound~\citep[UCB]{auer2000using}.

We conduct the task selection in a sequential manner using the original UCB algorithm, but we expect a batched variant of UCB could be used to parallelize the experience collection.
Each action corresponds to a group of tasks, and we then uniformly sample one task from the chosen group to evaluate the model performance with $C$ rollouts. These statistics are then used to update the mean estimate of that group.
After a sufficient amount of episodes are sampled, we construct the dataset and train the model with objectives in Section~\ref{sec:opt}. See Algorithm~\ref{alg:sampling} for the pseudocode.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/relative_improvement_in_success_rate.pdf}
    \vspace{-0.3cm}
    \caption{\footnotesize \textbf{(\ours{} improves success rate on a diverse range of task groups)} Average success rate on 6 representative task groups, with shaded areas representing standard error over 3 random seeds. \ours{} improves performance on all of them after fine-tuning on only roughly 22,500 total trajectories.}
    \label{fig:paprika_success_rate}
    \vspace{-0.2cm}
\end{figure*}

\paragraph{Note.} An important role of $\nu_\pi$ is to make different task groups comparable. The specific selection algorithms could likely be replaced with other more sophisticated online learning methods. More importantly, recent breakthroughs such as \citet{jaech2024openai} and \citet{deepseekai2025deepseekr1incentivizingreasoningcapability} mark the beginning of applying RL to a broad range of reasoning problems. Moving forward, we anticipate a proliferation of different RL tasks for LLMs. In this emerging paradigm, a scalable meta algorithm for selecting which tasks to train on will be essential, and we believe \ours{}'s curriculum learning approach will be a promising foundation for future algorithms.



