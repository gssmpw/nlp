

\section{Preliminary}

Many decision making problems can be formalized as a partially observable Markov decision process (POMDP). We assume each \emph{task}, $\tau$, is a POMDP although we will not draw on the details of the POMDP formalism in this work. As a concrete example, guessing the word ``\texttt{apple}'' would be a task in 20 questions.
We will use \emph{group} (or \emph{task group}, used interchangeably), $G=\{\tau_1, \tau_2, \dots, \tau_{|G|}\}$, to refer to a high-level grouping of different tasks (e.g., the game 20 questions would be a group). 
Tasks in a group should share similar strategies but it is not always true that they share the same optimal policy as such constraints may be overly stringent.
From the agent's perspective, each task is a black box function that takes in the agent's action $a_t$ (and possibly the whole interaction history) and outputs an observation $o_t$. Both $a_t$ and $o_t$ are strings. In a game of 20 questions, $a_t$ could be ``\texttt{Is the word an animal?}'' and the $o_t$ could be ``\texttt{No.}''. In other words, each task employs an environment that the agent interacts with to obtain intermediate observations.

An episode contains the agent's interaction trajectory within a single task.
Unlike the conventional RL structure, we will assume that the transition-level reward is either $0$ or must be inferred from $o_t$, and that the individual tasks can flexibly implement different observation spaces and termination conditions.
An episode terminates when the agent achieves the objective of the task or when the maximum number of interactions allowed within the task is reached.
We will use $h = (o_0, a_0, \dots, o_H, a_H)$ to denote an episode of length $H$, $h_t = (o_t, a_t)$ to denote a single step of $h$, and $h_{p:q} = (o_{p}, a_{p}, \dots, o_{q}, a_{q})$ to denote a slice of $h$ similar to array slicing.
At the end of an episode, the environment emits a single score, $r(h)$, that evaluates the performance of the agent.
Let $\pi$ denote the LLM agent and $h \sim \pi \circ \tau$ denote sampling a trajectory from task $\tau$ using policy $\pi$. The performance of a policy on a group would be:
$
    \texttt{Perf}(G) = \tfrac{1}{|G|}\sum_{\tau \in G} \E_{h\sim \pi \circ \tau}[r(h)].
$
The agent is trained on a finite set of groups, $\gG_\text{train}$, and the goal is to perform well on unseen groups, $\gG_\text{test}$. 
