\section{\ours{}}

\begin{table*}[h]
    \caption{\footnotesize Summary of the task groups used by \ours{}. }
    \label{tab:environment_summary}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{c|c c c c c}
        \toprule
            Task Group & \# Train Tasks & \# Test Tasks & Maximum Turns & Env Feedback & Uses COT \\
            \midrule
             Twenty questions & 1499 & 367 & 20 & LLM generated & \ding{55} \\
             Guess my city & 500 & 185 & 20 & LLM generated & \ding{55} \\
             Wordle & 1515 & 800 & 6 & Hardcoded program & \checkmark \\
             Cellular automata & 1000 & 500 & 6 & Hardcoded program & \checkmark \\
             Customer service & 628 & 200 & 20 & LLM generated & \ding{55} \\
             Murder mystery & 203 & 50 & 20 & LLM generated & \ding{55} \\
             Mastermind & 1000 & 500 & 12 & Hardcoded program & \checkmark \\
             Battleship & 1000 & 200 & 20 & Hardcoded program & \checkmark \\
             Minesweeper & 1000 & 200 & 20 & Hardcoded program & \checkmark \\
             Bandit best arm selection & 81 & 1 & 21 & Hardcoded program & \checkmark \\
             \bottomrule
        \end{tabular}
    }
    \vspace{-0.3cm}
\end{table*}

The goal of our paper is to develop a scalable method to instill better strategic exploration and sequential decision-making capabilities into LLMs. Prior works~\citep{krishnamurthy2024largelanguagemodelsexplore} have shown that LLMs can perform poorly on even the simple decision making task of multi-armed bandits. \citet{nie2024evolveevaluatingoptimizingllms} has since then demonstrated that LLMs can be taught to perform better on bandits after fine-tuning them on synthetic trajectories generated by known algorithms such as UCB. However, this idea is limited in scope for three reasons: \textbf{(1)} we want LLMs to perform strategic exploration and decision making in more complex settings, \textbf{(2)} for most tasks, there is no known algorithm like UCB to generate good synthetic trajectories from, \textbf{(3)} it can be infeasible to collect data for all tasks that we care about.

We aim to solve these issues using our method, \ours{}. First, we design a suite of complex decision-making tasks that require strategic information gathering to succeed. Next, we show that in the absence of known good algorithms, existing LLMs can generate trajectories with better decision making behaviors through diversity-encouraging sampling. We then finetune the LLMs to prefer higher performing trajectories (in a fashion similar to STaR~\citep{zelikman2022starbootstrappingreasoningreasoning}) and show that this leads to better decision making abilities at test-time. More importantly, these behaviors often generalize to unseen task groups without additional training. Finally, we propose a general curriculum learning algorithm that can dynamically choose which subset of tasks to train on next to improve data efficiency of such training methods. We next describe each component of \ours{}.


\input{sections/environment_design}
\input{sections/optimization}
\input{sections/curriculum}
