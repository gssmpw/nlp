\section{Methodology}

In this section, we show potential optimizations to GDRNPP \cite{liuShanicelGdrnpp_bop20222024} especially by choosing suitable backbone architectures (see Sec. \ref{sec:methodology:backbone}) and proposing alterations to the applied Geo head architecture (see Sec. \ref{sec:methodology:geohead}) akin to the proposed optimizations of Pöllabauer et al. \cite{pollabauer2024fast}.
Therefore, we select the five backbones from a pool of 22 candidate backbones which exhibit outstanding performance under a specified GMAC budget (see Sec. \ref{sec:methodology:backbone}).
Furthermore, we propose 11 potential alterations of the Geo Head architecture, from which we chose 4 according to their performance on the LM-O dataset (see Sec. \ref{sec:methodology:geohead}).
The combination of selected backbones and Geo Head architecture results in a pool of 40 candidate architectures.
We propose the AMIS algorithm, that selects a subset of these architectures, which allows efficient inference time and estimate quality trade-off by choosing the appropriate architecture from that pool (see Sec. \ref{sec:methodology:amis}).
In the end, we propose some implementation optimizations regarding the GDRNPP net architecture, that we applied to further increase inference time (see Sec. \ref{sec:methodology:misc}).

\subsection{Backbone}
\label{sec:methodology:backbone}

The impact of choosing different backbones for feature extraction on inference time and accuracy is crucial, prompting experiments with various models available in the Timm \cite{TimmPyTorchImage2024,} package, which offers a broad array of pre-trained CNN and ViT \cite{yuan2021tokens,} models, enabling access to advanced architectures and pre-trained weights which facilitate transfer learning and accelerate model development.\\
To select the most effective models, we set our criteria based on balancing performance speed and accuracy. Faster models with a \textit{Giga Multiply-Accumulate Operations per Second} ratio up to the standard set by GDRNPP were prioritized.
Among models with similar sizes, those offering superior accuracy were chosen. Considering Transformer models' effectiveness in computer vision, variants like Nextvit \cite{li2022next,} and Maxvit \cite{tu2022maxvit,} were selected for their compact sizes suitable for the 256x256 input image size used in our model. This approach ensures a reasonable selection of backbones that optimize both computational efficiency and task performance.\\
We identified 22 candidates for the backbone architecture by analyzing their inference time distribution, which appeared to cluster into three distinct groups, as shown in Fig. \ref{fig:Backbone A} \cite{tu2022maxvit, vasu2023fastvit, woo2023convnext, cai2022efficientvit,}.
Accordingly, we employed the k-means clustering algorithm to categorize these candidates into three groups.
For each group, we utilized the successive halving algorithm to select the best-performing backbone candidate in the group (Fig. \ref{fig:Backbone B}), selecting the five best performing backbones after five training epochs, the best three backbones after ten epochs, until the best performing backbone remained.\\
%
Notably, we also included Maxvit \cite{tu2022maxvit} for its competitive accuracy and FastVit \cite{vasu2023fastvit} for ts superior speed.
This strategic approach enables a focused evaluation of backbones that potentially optimize both speed and accuracy in our model.
\begin{figure}[htbp]
    \centering
    % Adjusting both images to have the same height and align tops
    \begin{subfigure}[t]{0.44\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth,height=5cm,keepaspectratio]{fig/K-means_Clustering_of_Backbone_according_to_inference_time.eps}}
        \caption{K-means Clustering of Backbone according to inference time}
        \label{fig:Backbone A}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.52\textwidth}
        \raisebox{-\height}{\includegraphics[width=\textwidth,height=5cm,keepaspectratio]{fig/successive_halving_group_1.eps}}
        \caption{Successive halving algorithm on Cluster 1}
        \label{fig:Backbone B}
    \end{subfigure}    
    \caption[Backbone Selection]{\raggedright 
    Intermediate results during backbone selection using k-means clustering and the Successive Halving Algorithm with the mean of MSPD, MSSD and VSD as selection criterion.}
    \label{fig:Backbone Selection}
\end{figure}

\subsection{Geo Head}
\label{sec:methodology:geohead}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth,keepaspectratio]{fig/geo_head_variation_1.eps}
        \caption{Geo Head variation 1}
        \label{fig:Geo Head variation 1_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth,keepaspectratio]{fig/geo_head_variation_2.eps}
        \caption{Geo Head variation 2}
        \label{fig:Geo Head variation 2_a}
    \end{subfigure}
    
    \caption[all Geo Head structure]{\raggedright Geo Head Variations GDR-Net architecture with the adaptations Geo Head variation 1 and 2, which reduce the number of up-sample layer of the
     vanilla Geo Head part.}
    \label{fig:all Geo Head structure}
\end{figure}
The Geo Head part of GDRNPP is one of the most time consuming parts during inference.
The standard structure of the Geo Head comprises three convolution blocks processing the $1024\times 8 \times 8$ output from the Backbone including up-sampling and convolution layers.
To reduce the inference time requirements, we propose two variants of the vanilla Geo Head architecture: \textit{Geo Head Variation 1}, reducing the convolution blocks to two and modifying the layer setup to handle changes in the feature map size without upsampling, and \textit{Geo Head Variation 2}, which further streamlines this process by eliminating an additional upsampling layer and adjusting the convolution sequence, resulting in smaller feature map sizes and faster data processing.
These modifications are aimed at decreasing the computational load and accelerating inference time by reducing the number of operations the GPU processes and by lowering the input size to the Data Process part, enhancing overall performance.
The proposed architecture variations are illustrated in Fig. \ref{fig:all Geo Head structure}.\\
Inspired by U-Net \cite{ronneberger2015u,}, we implemented skip connections in our neural network architectures to address the gradient vanishing problem and enhance learning capabilities by directly connecting layers across the network. These connections facilitate detail recovery and image segmentation by leveraging rich contextual information during up-sampling. However, introducing skip connections adds complexity to the model, increases computational demands, and may impact the network’s ability to generalize to unseen data. To balance these factors, we simplified the skip connection structure to involve minimal additional computations, ensuring that connected feature maps match in size to avoid unnecessary computations.\\
%
For each of the three Geo Head Variants (\textit{Vanilla}, \textit{Variation 1} and \textit{Variation 2}), there are three different candidate locations, where skip connections could be added regarding the aforementioned criteria.
These locations are illustrated in Fig. \ref{fig:Connection in Geo Head variation 1}.

\begin{figure}[htb]
    \centering
    % 第一行的两张图片
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth,keepaspectratio]{fig/connection/vanilla-geo-head-with-connection.eps}
        \caption{vanilla Geo Head with connection}
        \label{fig:vanilla Geo Head with connection}
    \end{subfigure}
        %\hfill % 在图片间添加一些空间
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth,keepaspectratio]{fig/connection/geo-head-v1-with-connection.eps}
        \caption{Geo Head Variation 1 with connection}
        \label{fig:Geo Head Variation 1 with connection}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth,keepaspectratio]{fig/connection/geo-head-v2-with-connection.eps}
        \caption{Geo Head Variation 2 with connection}
        \label{fig:Geo Head Variation 2 with connection}
    \end{subfigure}
    \\ 
    \caption[Connection in Geo Head variation 1]{\raggedright Candidates for adding connection in vanilla Geo Head, Geo Head variation 1, Geo Head variation 2. The connection can be added in one of positions labeled \ding{172}, \ding{173} and \ding{174}.}
    \label{fig:Connection in Geo Head variation 1}
\end{figure}

\subsection{Adaptive Margin-Dependent Iterative Selection (AMIS) algorithm}
\label{subsec:Scalable 6D Pose Estimation: AMIS}
\label{sec:methodology:amis}
 
We propose the Adaptive Margin-Dependent Iterative Selection (AMIS) algorithm to identify a subset of models that excel regarding their estimate quality in comparison to their time budget, i.e., inference time, over arbitrary datasets.
%
%
Existing strategies often average results over datasets, which essentially overweights the results of datasets that require high inference time, due to their absolute inference time differences being higher.
In contrast to that, we opt for a strategy that reduces the influence of this effect.
The proposed strategy pinpoints models that are part of an optimal wrap-line (Fig. \ref{fig:Example for wrap line and sweet point}) in the space of inference time and estimation quality, which show substantial accuracy improvement over its quicker counterparts with only minimal gains compared to slower successors.
Recognizing these models aids in selecting models that effectively balance speed and accuracy, enhancing model selection strategies.\\
%
%
In an initial phase, we measure inference time and accuracy for each candidate model and each dataset.
For each dataset we fit a straight line in the 2D space spanned by inference time and accuracy metrics using linear regression, which we call the \textit{default slope}.
For each model and dataset, we calculate the distance of its result from this default slope, normalizing them for each dataset on a scale of $0$ to $100$.
These scores are weighted depending on the desired dataset weight, resulting in a final score for each model.
We rank these scores assigning every model a fixed amount of scoring points depending on their rank.
We then iteratively repeat that ranking process for 100 \textit{adjustment factors}, between 0.001 and 3, which are multiplied with the \textit{default slope} (Fig. \ref{fig:Example for changing slope}).
We accumulate the ranking points per model for different adjustment factors except from the case, where the 10 best performing models do not change in comparison to the previous adjustment factor.
This procedure ensures, that the resulting model selection is robust against diverse trade-off preferences. After completing this step, we select the best ranking model repeating the process with the repeating models, until the number of selected models meets the task-specific requirements.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}                 \includegraphics[width=\textwidth,keepaspectratio]{fig/amis/wrap_line.eps}
        \caption{Wrap line showing models with a preferable trade-off between inference time and accuracy.}
        \label{fig:Example for wrap line and sweet point}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
    \includegraphics[width=\textwidth,keepaspectratio]{fig/amis/slope.eps}
        \caption[Example for changing slope]{\raggedright \textit{Default slope} and slope with \textit{adjustment factor} in the AMIS algorithm.}
        \label{fig:Example for changing slope}
    \end{subfigure}
    \\
    \caption{Visualization of substages of the AMIS algorithm.}
    \label{fig:Example to explain the theory of AMIS algorithm}
\end{figure}

\subsection{Other optimization}
\label{sec:methodology:misc}
To reduce extended inference times linked with dynamic tensor creation, which is resource-intensive due to repeated memory allocation and initialization, we have adopted alternative measures.
Our strategy minimizes new tensor creation by pre-allocating tensor memory before inference, using a reusable tensor pool throughout the inference cycle to avoid frequent new allocations.
This approach enhances efficiency, which is particularly critical in real-time applications.