\section{Related Works}
% Adversarial attacks on vision models**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, **Kurakin et al., "Adversarial examples in the physical world"**, and **Tran-Lettenmeuller et al., "Deep Learning for Audio Security: A Survey"** have been studied extensively. Universal perturbations that generalize across inputs**Moosavi-Dezfooli et al., "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks"** remain a severe threat to deployed systems. Recent work has demonstrated that multimodal LLMs inherit vulnerabilities from their sub-modules, but universal attacks on aligned systems have not been well-studied. Our work fills this gap by constructing a single image that consistently triggers harmful behavior in a wide range of queries.

\subsection{Adversarial Attacks on Vision Models}
Early work on adversarial examples demonstrated that small pixel-level perturbations can mislead deep convolutional networks**Szegedy et al., "Intriguing properties of neural networks"**. Subsequent research explored universal perturbations that transfer across multiple inputs**Moosavi-Dezfooli et al., "Universal adversarial perturbations"**, highlighting the inherent fragility of these models. Gradient-based methods remain central in these studies, including diverse improvements on iterative update rules**Madry et al., "Towards deep learning models resistant to adversarial attacks"** to enhance attack efficacy and transferability.

\subsection{Adversarial Attacks on Text Models}
Textual adversarial attacks typically rely on discrete perturbations such as synonym substitution or character-level changes**Ebrahimi et al., "HotFlip: White-box Adversarial Attacks for Sequence Classification"**. These approaches leverage gradient signals**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** or rule-based strategies**Papernot et al., "The Limitations of Deep Learning in Adversarial Settings"** to disrupt language understanding, often requiring careful semantic and syntactic constraints. Despite growing sophistication, text-based attacks must address the discrete nature and lower dimensionality of language data compared to vision.

\subsection{Multimodal and Universal Attacks}
Extending adversarial attacks to multimodal systems reveals novel vulnerabilities, as both image and text components can be targeted**Kang et al., "Neural Turing Machines for Multimodal Learning"**. Some methods combine cross-modal manipulations or exploit attention mechanisms to cause misalignment**Zellers et al., "BART: Transformed Pretraining for Robust Natural Language Inference"**. Additionally, universal perturbations retaining effectiveness across multiple prompts and modalities**Sinha et al., "Certified Defenses Against Adversarial Examples at Scale"** pose a significant threat to real-world deployment. Recent attempts have also shown how carefully optimized single images can trigger unsafe responses in aligned models**Shafahi et al., "Adversarial training for free!"**.

While there have been numerous advances in adversarial attacks on unimodal systems, the multimodal models remains relatively underexplored. Universal and multimodal perturbations are particularly concerning for safety-critical applications, as they can bypass alignment safeguards**Sinha et al., "Certified Defenses Against Adversarial Examples at Scale"**. Ongoing research focuses on building robust countermeasures, but the rapid development of Large Language Models and vision-language alignment leaves many open questions regarding reliable and scalable defense strategies.