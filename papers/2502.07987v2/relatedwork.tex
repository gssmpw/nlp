\section{Related Works}
% Adversarial attacks on vision models~\cite{szegedy2013intriguing,kurakin2016adversarial}, text models~\cite{li2018textbugger}, and multimodal systems~\cite{xu2023multimodal_adv} have been studied extensively. Universal perturbations that generalize across inputs~\cite{moosavi2017universal} remain a severe threat to deployed systems. Recent work has demonstrated that multimodal LLMs inherit vulnerabilities from their sub-modules, but universal attacks on aligned systems have not been well-studied. Our work fills this gap by constructing a single image that consistently triggers harmful behavior in a wide range of queries.

\subsection{Adversarial Attacks on Vision Models}
Early work on adversarial examples demonstrated that small pixel-level perturbations can mislead deep convolutional networks~\cite{szegedy2013intriguing,kurakin2016adversarial}. Subsequent research explored universal perturbations that transfer across multiple inputs~\cite{moosavi2017universal}, highlighting the inherent fragility of these models. Gradient-based methods remain central in these studies, including diverse improvements on iterative update rules~\cite{guo2021gradient} to enhance attack efficacy and transferability.

\subsection{Adversarial Attacks on Text Models}
Textual adversarial attacks typically rely on discrete perturbations such as synonym substitution or character-level changes~\cite{neekhara2018adversarial}. These approaches leverage gradient signals~\cite{guo2021gradient} or rule-based strategies~\cite{jones2023automatically} to disrupt language understanding, often requiring careful semantic and syntactic constraints. Despite growing sophistication, text-based attacks must address the discrete nature and lower dimensionality of language data compared to vision.

\subsection{Multimodal and Universal Attacks}
Extending adversarial attacks to multimodal systems reveals novel vulnerabilities, as both image and text components can be targeted~\cite{gu2024agent}. Some methods combine cross-modal manipulations or exploit attention mechanisms to cause misalignment~\cite{coattack,carlini2024aligned}. Additionally, universal perturbations retaining effectiveness across multiple prompts and modalities~\cite{zou2023universal} pose a significant threat to real-world deployment. Recent attempts have also shown how carefully optimized single images can trigger unsafe responses in aligned models~\cite{carlini2024aligned}.


While there have been numerous advances in adversarial attacks on unimodal systems, the multimodal models remains relatively underexplored. Universal and multimodal perturbations are particularly concerning for safety-critical applications, as they can bypass alignment safeguards~\cite{gu2024agent}. Ongoing research focuses on building robust countermeasures, but the rapid development of Large Language Models and vision-language alignment leaves many open questions regarding reliable and scalable defense strategies.