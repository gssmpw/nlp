\subsection{Class 4 (Traits: fully parallel, collocated computation and memory)}
\label{sec:overview_class4}
\begin{wrapfigure}[13]{r}{0.5\textwidth}
  \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class4.png}
    \caption{Simplified diagram of a Class 4 NMA.}
    \label{fig:class4}
\end{wrapfigure}
Class 4 architectures support two Traits: fully parallel operation and collocation of memory and computation. They have the potential of updating the entire network the fastest out of all of the Classes, however due to the entire system - including the event processing circuitry - being contained on-chip, they are the most dependent on the amount of available resources on the FPGA platform.
\mypar{Early implementations}
\parhl{Roggen et al. (2003) \cite{roggen_hardware_2003}} presented an NMA for autonomous agents with on-the-fly reconfiguration capability. The authors chose a LIF neuron model, and the topology was a 2D array of 8x8 neurons. Architecture-wise, the system comprised 64 PEs with parameters kept on-chip, and it was clocked at 33 MHz on Altera APEX20K200E.
\parhl{Bellis et al. (2004) \cite{bellis_fpga_2004}} presented an NMA, which was a hardware port of a software-based simulator. The network comprised four LIF neurons and four synapses, and its main task was to solve the wall-following task for a an autonomous agent. Architecture-wise, we deduced that there were four PEs and that the system was clocked at 118.189 MHz on Xilinx Spartan-2, with a fixed-point representation of 5b width for weights and 7b for membrane potential.
\parhl{Upegui et al. (2005) \cite{upegui_fpga_2005}} presented a framework for design space exploration (DSE) for SNNs, based on the genetic algorithm approach (the authors called it the \textit{evolutionary algorithm}). The authors used the Dynamic Partial Reconfiguration (DPR) for NMA reconfiguration. The architecture comprised a varying number of of PEs and a weight adjustment units and two fixed modules - encoder and decoder for input and output. The PEs and weight adjustment units were DPR-compliant, which could consist of single neurons, layers, or entire networks. The genetic algorithm was used to find the optimal network topology. The system supported a LIF neuron model and the STDP weight-adjusting method.
\parhl{Guerrero-Rivera et al. (2006) \cite{guerrero-rivera_programmable_2006}} presented implementations of various logic designs for LIF neuronal dynamics with STDP and axonal delays using the Exact Integration method\footnote{The authors thoroughly described it in another work~\cite{guerrero-rivera_attractor-based_2007}.}. The authors focused on deploying parallel, low-complexity neuron model implementations without resource sharing, which operated in a single-cycle manner. The architecture supported exponential decay for the LIF neurons, and alpha and beta synaptic kernels with STDP for synapses. The authors used the system to describe two classes of neurons in the mammalian olfactory bulb and deployed it on Xilinx Virtex-II Pro, clocked at 33 MHz.
\parhl{Shayani et al. (2008) \cite{shayani_fpga-based_2008}} presented the implementation of an SNN with Piecewise-Linear Approximation of Quadratic Integrate and Fire (PLAQIF) neurons. Their main goal was heterogeneous recurrent SNNs implementation, with support for a certain degree of structural plasticity. Architecture-wise, the system comprised 161 PEs with several synapse units per PE arranged in a column and interconnected in a daisy-chain fashion. The upstream packets (from soma) were unchanged and passed-through the synapses in the column, while downstream packets were processed by the synapse. The authors tested the implementation with a recurrent network of 161 neurons on the Xilinx Virtex-5 device clocked at 160 MHz.

\mypar{Early modern implementations}
\parhl{Johnston et al. (2010) \cite{johnston_fpga_2010}} presented an \textit{evolution-based} SNN on FPGAs for robotics, designed with a HW/SW co-design methodology. They implemented the evolutionary part of the SNN a genetic algorithm, and added the STDP-based online learning for fine tuning the network obtained through genetic algorithm. The system functioned as an intelligent controller for a robotic system to solve autonomous navigation and obstacle avoidance. Architecture-wise, it relied on the network unit and embedded microcontroller to perform the update through the genetic algorithm. The authors tested the system on a Khepera robot\cite{mondada_development_1999} with 10 LIF neurons and 16 synapses. The implementation was clocked at 100 MHz on a Xilinx Virtex-II Pro and used 16b floating-point arithmetic.
\parhl{Caron et al. (2011) \cite{caron_fpga_2011}} presented an implementation of the Hierarchical SNN (HSNN) based on the Oscillatory Dynamic Link Matcher (ODLM) algorithm, i.e., using LIF neurons to approximate the behavior of relaxation oscillators. They claimed that just like ODLM, the HSNN could perform image segmentation, matching, and monophonic sound source separation. Architecture-wise, the system comprised an HSNN unit, controller and communication units. The HSNN used a bit slice architecture composed of several columns connected in a ring buffer. Said slices were built off of a weight memory containing all of the presynaptic weights to the neuron instantiated by the column, and the synapse model unit, which added the synaptic weights to the neurons' potential during spike propagation.
\parhl{Rossello et al. (2012) \cite{rossello_hardware_2012}} presented an NMA for stochastic SNNs. The architecture used a non-standard IF neuron model - the authors used a shift register where the single bit was being shifted depending on the signal from the synapses. The incoming spikes were only passed to the shifting logic if matched with the partial probabilities, realizing the stochastic operation. When the bit reached the MSB position, the spike was propagated further. The authors tested the system with a high-speed signal filtering and solving complex systems of linear equations tasks on Altera Cyclone-III, with a single layer of 100 output neurons.
\parhl{Iakymchuk et al. (2012) \cite{iakymchuk_fast_2012}} proposed an NMA that efficiently implemented large networks on low-end hardware. The architecture supported a one millisecond biological timestep and featured a set of PEs that serially processed the input. The authors chose the SRM model for synapses and neurons and divided the system into three primary levels: neuron, layer, and network, which were realized by units consisting of a set of subunits from lower levels of the hierarchy. They utilized spike encoding with a specified frequency of spike trains, and chose 16b fixed-point arithmetic, with weights being drawn from a set of 30 16b fixed-point values. They tested the system on a Xilinx Spartan-3, with a pattern classification on 3x3-bit arrays using FF-FC network (9x9x4).
\parhl{Deng et al. (2014) \cite{deng_implementation_2014}} presented an NMA for a specific topology of an FF-FC network to achieve synchronization in neuron spiking between the layers, starting from uncorrelated firing rates. The architecture assumed using bespoke PEs per synapse and neuron. The authors chose the LIF neuron model and alpha kernel for the synaptic dynamics. The system achieved the intended synchronized firing between the layers on the Altera Stratix III clocked at 50 MHz, with an 8x8x8 network, achieving 50000x real-time operation. 
\parhl{Wang et al. (2015) \cite{wang_general-purpose_2015}} presented a general-purpose Liquid State Machine (LSM) network with a reconfigurable reservoir of randomly connected LIF neurons and pre-trained, task-specific fully-connected output layers. The authors implemented the reconfigurability of the reservoir's neurons via power gating (turning off the unused parts of the FPGA). The plasticity was implemented with probabilistic weight updates and an induced teaching signal. The architecture relied on a set of PEs realizing the liquid neurons making up a \textit{reconfigurable unit} and a set of PEs making up a \textit{trainable unit} for the output functionality. The implementation presented in the paper used 135 liquid neurons (which can be sized down to 90), and 26 output neurons on a Xilinx Virtex-6. The number of output neurons depended on the output of the most demanding task the system was designed to perform. The accuracy for the MNIST benchmark was 98.6\% for the largest available network (135 reservoir neurons and 26 output neurons).
\parhl{Farsa et al. (2015) \cite{farsa_function_2015}} presented an NMA for a single layer SNN performing function approximation. In a sense, it was a generalized classification task, as function computation was replaced by classifying input into various coded outputs. The authors chose the LIF neuron model for the system. The learning part was performed with MATLAB and SGD, which resulted in a set of weights that could be mapped to a single-layer WTA network. From the architectural point of view, the entire system comprised several PEs responsible for their bespoke neurons. The system was used to approximate the Gaussian force field as a test case and implemented on a Xilinx Zynq-7000.
\parhl{Gomar et al. (2016) \cite{gomar_digital_2016}} presented an implementation of a neuron-synapse-neuron connection with additional astrocyte block connected to both neurons. Their main goal was to implement an astrocyte along the neurons efficiently. The authors employed an AdEx neuron model, the Kopel model for synapse and the Postnov model for astrocyte dynamics, resulting in six ODEs. As for the models, the authors optimized them by approximating the costly functions in the ODEs - \textit{exp} and \textit{tanh} - with Base-2 term and sign function, respectively. With the latter, the authors showed that Base-2 could also be used, but the sign was significantly cheaper implementation-wise. The implementation was tested on Xilinx Virtex-II.
\parhl{Lammie et al. (2018) \cite{lammie_unsupervised_2018}} did not focus on a specific architecture but rather on presenting an optimization approach for a specific NMA. The authors presented a way of utilizing a single vector of IZH neurons, arranged in a WTA topology with lateral inhibition, to recognize patterns after training them with STDP. The design was rate-based, and the spikes were Poissonian. The IZH neurons used bit shifts and adders to reduce the computational cost of multipliers. The STDP mechanism was simplified, where instead of holding the approximate values, it was approximated directly in circuitry via combination of OR gates.
\parhl{Heidarpur et al. (2019) \cite{heidarpur_cordic-snn_2019}} presented a coordinate rotation digital computer (CORDIC)-based SNN based on IZH neurons. Despite the fact that the paper focused on the optimizations applied to the neuron computing pipeline rather than on the actual architecture choices, the authors suggested how to use this concept paired with STDP, which showed potential on how to create a larger NMA. The implemented network comprised 21 neurons, but the authors claimed they could potentially fit 110 neurons on a Xilinx Spartan-6 clocked at 183.4 MHz.
\parhl{Kuang et al. (2019) \cite{kuang_digital_2019}} presented an NMA for realizing a specific SNN. The topology comprised three layers of neurons equal to the resolution of the input image. The operation assumed the output layer being made out of excitatory neurons, and the output is the firing pattern of this layer. The neurons were LIF with COBA synapses with exponential leakage and support for triplet-based STDP. The system was pipelined to increase the throughput. The authors tested the implementation with a simplified MNIST benchmark (5x5 instead of 28x28 pixels) for a network of 75 neurons that achieved 93\% accuracy.

\mypar{Modern implementations}
\parhl{Asgari et al. (2020) \cite{asgari_low-energy_2020}} presented an NMA for an SNN comprising an input layer and two layers of WTA networks, for simulating a simplified model of hippocampal and motor neurons. The architecture comprised an array of PEs for neurons and synapses, a crossbar for connection and activity memory, and a designated activity block for the WTA mechanic. The authors chose a LIF neuron model and the synapses were fixed in weight for the inhibitory and followed the STDP learning rule for the excitatory type. The STDP was  implemented as a LUT for 16 time intervals and 10 possible weight modification values.
\parhl{Liang et al. (2021) \cite{liang_113ujclassification_2021}} proposed an SNN accelerator targeting image inference. The network model used a single-spike encoding scheme, where the neurons fire exactly once per inference. Architecture-wise, the system was a systolic array of eight PEs, where every core comprised an 8x8 array of neurons and a shared weight memory. The PEs were connected in a bidirectional list organized in two columns of four PEs. The architecture was tailored for a specific use case and a specific network topology, i.e., FF-FC (522 neurons).
\parhl{Deng et al. (2021) \cite{deng_reconstruction_2021}} presented an implementation of an auditory NN. The system comprised six layers of 11 LIF neurons. The input auditory data were passed through the layers, and the results from all the layers were transferred to the centralised Bayesian classifier module. The synapses realized the spike traces with exponential kernels. The design was multiplierless, utilizing bit shifts and adders. The authors conducted tests on the system with speech samples consisting of spoken digits with applied noise (from 20dB to -5dB signal-to-noise ratio). According to the authors, the results suggested that this implementation was somewhat resilient to noise, reaching around 50\% accuracy with -5dB SNR.
\parhl{Heittmann et al. (2022) \cite{heittmann_simulating_2022}} used \textbf{IBM's INC-3000 Neural Supercomputer} - a massive neural simulation system that comprised 16 boards, each including 27 Xilinx Zynq-7045s (called nodes) to implement the cortical microcircuit\cite{potjans_cell-type_2014}. The architecture was a hierarchical NoC where every board implemented a cube of 3x3x3 of nodes. Those cubes were connected in a 4x4 array. This connection scheme resulted in a grid of 12x12x3 (or 432) nodes. The system supported LIF and IZH neurons and CUBA exponential, alpha and beta kernels for the synapses and operated with 0.1 ms timestep. The system was targeting general neuromorphic-related applications, such as neuroevolution in the form of AI learning the rules of video games or simulating the cortical microcircuit. The system could simulate up to 110592 neurons.
\parhl{Hu et al. (2022) \cite{hu_binarized_2022}} presented an NMA that comprised a systolic array of 4096 PEs. The main characteristic which makes it different from other implementations was the binary systolic array that realized the weights - 0s and 1s reflected the sign of the weight. The system reached an accuracy of 98.67\% after 30 time steps on simplified N-MNIST benchmark (16x16 px images) with an SCNN. The system supported an IF neuron model with long-term depression of the synaptic weights capability. The authors reported power consumption of 29pJ/SO with the system clocked at 100 MHz.
\parhl{Carpegna et al. (2024) \cite{carpegna_spiker_2024}} presented \textbf{Spiker+} - complete Python-to-FPGA framework for creating SNNs for image inference and other classification tasks, which was a direct continuation of \textbf{Spiker}\footnote{Spiker+ was a newer version of Spiker and, for completeness, we inform that according to the proposed Taxonomy, Spiker is considered Class 1 NMA and is of a much smaller scale.}\cite{carpegna_spiker_2022}. The framework supported the BPTT learning method through external learning via the Surrogate Gradient method. Architecture-wise, the system comprised three layers of the hierarchical units - network, layer and neuron and supported six different versions of the LIF model (IF to full LIF). The authors chose 6b and 4b fixed-point weights for MNIST benchmark, but the architecture allowed for using different values. The authors tested the architecture with MNIST and SHD benchmarks. For MNIST, the system achieved the accuracy of 96.83\% on Xilinx Artix-7 clocked at 100 MHz, with the processing speed of 1282 frames per second.