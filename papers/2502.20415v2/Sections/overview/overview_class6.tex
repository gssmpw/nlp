\subsection{Class 6 (Traits: collocated computation and memory, asynchronous network update)}
\label{sec:overview_class6}
\begin{wrapfigure}[13]{h}{0.5\textwidth}
  \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class6.png}
    \caption{Simplified diagram of a Class 6 NMA.}
    \label{fig:class6}
\end{wrapfigure}
Class 6 architectures support two Traits: collocated computation and memory and asynchronous network update. They are the closest to fully-neuromorphic systems (Class 7) with the only difference being the lack of fully parallel operation. As the entire system is contained on-chip, they offer high processing speed, but not as fast Class 7, as the number of PEs is lower. Additionally, they should provide lower power consumption due to asynchronous network update. Moreover, they depend to a smaller extent on logic resources than Class 7.

\mypar{Early implementations}
\parhl{Pearson et al. (2005) \cite{pearson_real-time_2005}} presented an architecture composed of multiple TDM PEs. Every PE included a LIF pipeline, handling 120 neurons and 912 synapses. The authors chose a 16b fixed-point number representation and tested the system with a simulation of the basal ganglia.
\parhl{Cheung et al. (2006) \cite{cheung_scalable_2006}} implemented a Cellular Neural Network realizing a Gabour-type filter for feature extraction from images. The architecture consisted of a 2D array of PEs controlled by a central unit and supported LIF neuron model with the forward Euler integration. The PEs utilized the 16b fixed-point arithmetic. The authors interfaced the system implemented on Xilinx Virtex-II device and clocked at 120 MHz with a 32x32 pixel silicon retina, and the results were presented on the VGA screen.
\parhl{Cheung et al. (2009) \cite{cheung_parallel_2009}} presented a systolic architecture for the high speed simulation of SNNs, supporting IZH neurons wit low neuronal activity. The authors chose a fixed-point 18b number representation for membranes and 9b fixed-point for the weights. The system modeled the random current present in the IZH model equation (as shown in Table \ref{tab:neuron_models}) with a pseudorandom number generator (based on linear-feedback shift register). The authors tested the system on the Xilinx Virtex-5 device, which they reported could allow for 32 PEs and 848 neurons to be fitted, achieving the performance of 0.73 ms needed for one second of real-time simulation.

\mypar{Early modern implementations}
\parhl{Ang et al. (2011) \cite{ang_spiking_2011}} the authors presented an architecture for an SNN-based auto-associative memory. Topology-wise, it was a single-layer network with interconnects serving as axonal delay elements, which was trained to recall several patterns based on incomplete stimuli. Every neuron was connected to one Coincidence Detector, which in turn were associated with a certain stored pattern and contributed to recurrent activation of this pattern. This design was not based on sequential but rather on combinational logic with customizable delay lines.
\parhl{Luo et al. (2014) \cite{luo_real-time_2014}} presented an NMA for simulating human cerebellum network. The system was arranged as a NoC with the maximum support for 100000 neurons on 48 PEs. Every PE comprised two pipelines that would process the data for 20 Golgi cells and 2000 granular cells\footnote{The kind of cells found in cerebellum.}. The inhibitory connections from Golgi cells to the granule cluster was described with a probability, and, on average, every granule neuron received about eight inhibitory connections, which follows biological observations. Neurons were COBA LIF, with excitatory and inhibitory ion channels. The provided performance figure suggested that the network of 100000 neurons could be processed every 25.6 ms on Xilinx Virtex-7, however the design was only tested with a network of 8000 granular cells and eight Golgi cells on four PEs.
\parhl{Vangel et al. (2014) \cite{vangel_spiking_2014}} presented an architecture using the Dynamic Neural Field (DNF) to model the neuron-to-neuron interactions. DNF theory describes network activity not based on the dynamics of singular neurons but rather based on approximations of dynamics of large, homogeneous and recurrently connected neural networks based on a mean field approach\cite{amari_dynamics_1977}. From the architectural point of view, the system comprised a 2D grid of PEs interconnected via routers in a NoC-like fashion with a support for stochastic transmission, i.e., the PEs sent out the generated spikes with a given probability. The proposed Randomly Spiking DNF provided local connectivity and distributed, small computation units with four-directional connectivity. The authors implemented the design on a Xilinx Virtex-6 with varying sizes of RSDNF, up to 1200 neurons.
\parhl{Holanda et al. (2016) \cite{holanda_dhyana_2016}} presented \textbf{DHyANA} - a hierarchical NoC-based NMA designed for brain activity simulations. The architecture was a mesh-bus hybrid, with a standard mesh of PEs consisting of clusters of smaller computing elements connected to a shared bus. The clustered smaller computing elements responded to the decoded spike packets read from the common bus. The implementation comprised a 4x4 grid of PEs supporting 208 neurons to be simulated on Altera Stratix-IV, clocked at 56 MHz.
\parhl{Luo et al. (2016) \cite{luo_real-time_2016}} presented an architecture with a focus on the collective activity of large groups of neurons, allowing for simulation of up to approx. one million neurons in different configurations. The authors based their approach on the Neural Engineering Framework, which allowed users to construct neural models on a high level of abstraction by defining vectors and functions operating on those vectors. Vectors were groups of neurons, and connections between those groups realized a mathematical function - from simple identity to ODEs.
\parhl{Lin et al. (2017) \cite{lin_digital_2017}} presented an NMA organized as a NoC in tree topology with three supported neuron models - HH, IZH and IF. The NoC structure allowed authors for distributed, partial reconfiguration, which could be used at runtime. Multi-device operation was possible due to the ring topology of interconnected FPGAs. The system was clock-driven, with TDM PEs grouped in clusters with shared memory. 
\parhl{Yang et al. (2019) \cite{yang_real-time_2019}} presented \textbf{LaCSNN} - a 3D NoC-based NMA utilizing COBA neuron models, which targeted the simulation of a cortico-basal ganglia-thalamocortical network. From an architectural point of view, the system was a NoC with 36 TDM PEs that supported 4800 virtual neuron updates per timestep in real time. The architecture was by design targeting multi-device systems - in the paper it was a system of six Altera Stratix-III devices, supporting up to around one million neurons and around 60 million synapses in real-time (with average spiking rate of 50 Hz).
\parhl{Zheng et al. (2019) \cite{zhang_asynchronous_2019}} presented an NMA arranged as a NoC of 16 PEs. Those PEs were arranged in groups, responsible for specific layers of neurons. The authors tested the system with the MNIST benchmark, with SNN's topology of FF-FC (906 neurons), for which it achieved 98\% accuracy. The authors chose an 8b fixed-point number representation for the weights, while the membranes were 13b fixed-point numbers. The LIF neuron implementation relied on a \textit{click-based link-joint circuit} - a 4-signal handshake protocol allowing for asynchronous communication.
\parhl{Fang et al. (2019) \cite{fang_event-driven_2019,fang_encoding_2020}} the authors presented an NMA, arranged as a NoC of TDM PEs supporting LIF neurons and SRM-based synapses. The authors implemented the system on Altera Cyclone-V, which was clocked at 75 MHz. The synaptic potential decay was implemented with a LUT, and the delay of the alpha/dual-exp synapse functions with a shift register. The authors chose a 16b fixed-point number representation for the weights. The authors tested the system with the MNIST benchmark for The FF-FC network of 610 neurons, for which it achieved a 97.7\% accuracy.
\parhl{Mitchell et al. (2020) \cite{mitchell_small_2020}} presented \textbf{$\mu$Caspian} - NMA for SNN simulation targeting low-end FPGAs. The architecture was fully pipelined and allowed for simulating up to 256 LIF neurons and 4096 synapses on Lattice Semiconductor iCE40. The chose the 8b fixed-point number representation for weights. The system supported axonal delay values from zero to 15 timesteps and it was tested with varying sizes of A2A networks (ranging from 10 to about 60 neurons).

\mypar{Modern implementations}
\parhl{Nambiar et al. (2020) \cite{nambiar_scalable_2020}} presented an NMA arranged as a NoC for neural processing. The NoC comprised of multiple square arrays of PEs of variable size, but the authors presented their results for systems with 64x64, 128x128 and 256x256 arrays (also called \textit{cores}). The system supported only the LIF neuron model, with three available membrane leakage models. The authors chose an 8b fixed-point number representation. They tested the system with MNIST (94\% accuracy) and Fisher Iris benchmarks (92.15\%).
\parhl{Sakellariou et al. (2021) \cite{sakellariou_fpga_2021}} presented a NoC-based NMA supporting LIF neurons with online learning capability performed on a separate ARM core. The learning relied on the continuous approximation of the LIF's transfer function through BP. The system supported XY messaging for communication between PEs. We deduced that the system used a 4x4 array of PEs with 256 neurons that can be simulated on every PE on Xilinx Zynq UltraScale. The system supported axonal delay and featured a 12b weight fixed-point number representation. The system was tested with the MNIST benchmark and FF-FC topology (310 neurons) and achieved 92.15\% accuracy.
\parhl{Nguyen et al. (2021) \cite{nguyen_connection_2021}} presented an architecture for realizing SNNs, with the connection pruning capability. The system supported the IF neurons with time-to-first-spike encoding. The online learning used STDP with an addition of the pruning mechanic, which allowed the system to cut the connections with weights below a predefined threshold. The authors tested the system with an SCNN with three convolution-pooling layers for the Caltech 101 benchmark (95.7\% accuracy).
\parhl{Mack et al. (2021) \cite{mack_ranc_2021}} presented \textbf{RANC} - a complex ecosystem for realizing different NMAs, utilizing a SW/HW co-design approach\footnote{Another RANC-based architecture can be found in~\cite{truong-tuan_fpga_2021}.}. In the paper, the authors used it to implement an FPGA-based implementation of IBM TrueNorth\cite{akopyan2015truenorth} - NoC of PEs with crossbars for connectivity, supporting LIF neurons without the refractory period and programmable synaptic delays. The entire paper aimed to prove that the RANC-based TrueNorth\cite{akopyan2015truenorth} was close to the original project, with good performance in CIFAR-10 benchmark, as well as synthetic aperture radar and vector-matrix multiplication applications.
\parhl{Wang et al. (2022) \cite{wang_triplebrain_2022}} presented \textbf{TripleBrain} - an NMA consisting of Neural Processing Tiles (NPTs) sharing a common bus, supporting 64 neurons and 1024 synapses per NPT, targeting classification tasks. In the presented implementation, there are four NPTs, resulting in an array of 256 neurons. The architecture realized the concepts of self-organizing maps, STDP and reinforced STDP (R-STDP). The architecture supported single-layer networks, fully connected to the input vector. The system was tested with multiple benchmarks, and it achieved 95.10\% accuracy in MNIST with 256 neurons.
\parhl{Zhao et al. (2023) \cite{zhao_099--438_2023}} presented a system for processing biological signals (neural spikes, EMG, ECG and EEG), which supported them by utilizing different types of neural networks (including SNNs). Architecture-wise, the system comprised four reconfigurable cores connected to a common bus. Each core comprised 16 interconnected PEs with multiple general-purpose datapaths for those different network types. For SNN mode, four PEs realize a single LIF neuron at any given time, resulting in 16 neurons (with 512 synapses) processed simultaneously.