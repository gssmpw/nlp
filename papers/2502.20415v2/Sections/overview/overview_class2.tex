\subsection{Class 2 (Traits: collocated computation and memory)}
\label{sec:overview_class2}
Class 2 architectures support only collocated computation and memory Trait. Those architectures store network data on-chip, which limits the potential size of the implemented SNN, but allows for greater speeds of operation, in comparison to Class 0 or even Class 1, as the fully parallel operation does not bring enough benefit to balance the memory bottleneck. This class is also the most populous, as it is the easiest to implement, without the requirement for auxiliary memory support and bespoke PEs for every neuron.

\mypar{Early implementations}
\parhl{Schrauwen et al. (2008) \cite{schrauwen_compact_2008}} presented an LSM-based recognition system for spoken digits. The system relied on serialized synapse processing and serialized arithmetic. Architecture-wise, the system consisted of a set of PEs driven by a controller block. The neuronal dynamics of an SRM model were computed in a time-multiplexed manner, but we deduced that the PEs were not pipelined. The PEs operated at 100 MHz on Xilinx Virtex-4. The authors implemented a network of 1600 neurons as a use case, to process the incoming 16 kHz audio signal with five PEs.
\begin{wrapfigure}[13]{h}{0.5\textwidth}
  \centering
    % \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class2.png}
    \caption{Simplified diagram of a Class 2 NMA.}
    \label{fig:class2}
\end{wrapfigure}
\parhl{Thomas et al. (2009) \cite{thomas_fpga_2009}} presented an NMA using the IZH neuron model. The system allowed for simulation of 1024 neurons and 1024000 synapses, running at 100x real-time speed on a Xilinx Virtex-5 device clocked at 133 MHz. The system achieved a throughput of 2.26 GFLOPs with double-precision floating-point number representation. Architecture-wise, the system consisted of a synaptic unit, implementing four synapse pipelines, an accumulator and synaptic weight RAMs and a neuron unit with a single neuron update pipeline.
\parhl{Wildie et al. (2009) \cite{wildie_reconfigurable_2009}} presented an NMA that included a gap junction model on top of classic electrochemical synapses. They chose IZH as a neuron model for this system. Architecture-wise, the system consisted of eight PEs and could simulate a network of 1000 randomly-connected neurons. Every PE consisted of three pipelines - for neuron update, spike propagation and gap junctions dynamics. The system supported 24-bit fixed-point numerical representation for the parameters and conductance delay. It was clocked at 20 MHz and achieved 5.8x speed-up over real-time operation.

\mypar{Early modern implementations}
\parhl{Ambroise et al. (2013) \cite{ambroise_biorealistic_2013}} presented an NMA that comprised 117 IZH neurons working in real-time with a 1 ms timestep. The system consisted of a single PE with separate units for neuronal and synaptic dynamics. The authors tested the implementation on a Xilinx Virtex-4 device, which was clocked at 84.809 MHz and achieved real-time operation. \cite{ambroise_real-time_2013} showed a potential use case for this architecture - implementation of a set of CPGs.
\parhl{Wang et al. (2013) \cite{wang_fpga_2013}} presented an NMA realizing a concept of a \textit{polychronous network} (SNN where the axonal delays are used to store information instead of weights). The architecture used two arrays of PEs - one for the neurons and one for the axons - sharing two AER-based buses. The former (128 PEs) allowed for simulating 4096 neurons in biological real-time, and the latter (70 PEs) computed 1.15 million synaptic connections. The authors reported the accuracy of the memory recall to be above the 90\% mark.
\parhl{Yang et al. (2015) \cite{yang_cost-efficient_2015}} presented a system implementing a basal ganglia (BG) model. Characteristics of BG were replicated with 256 IZH neurons and 2048 synapses, and the authors claimed that the system supported up to 3k neurons on a single Altera Cyclone-IV. The implemented neuron dynamics were approximated with a 4th order piecewise-linear (PWL) approximation. The architecture consisted of four pipelines for different types of neurons found in BG and bespoke pipelines for synaptic currents. The authors chose a 28b fixed-point number representation.
\parhl{Wang et al. (2015) \cite{wang_multi-fpga_2015}} presented a multi-FPGA architecture for implementing FF SNNs with FHN neurons. Every FPGA computed dynamics of exactly one layer and they shared  a common clock signal. The devices implemented a systolic array of PEs, where every PE could compute 24 neuron dynamics within the selected timestep, and Synaptic Current Computation Modules (SCCMs) generating the presynaptic currents.
\parhl{Ahn et al. (2015) \cite{ahn_neuron-like_2015}} presented the \textbf{Neuron Machine (NM)} - an NMA using a multi-clock domain approach, where a single soma unit operated at a higher clock rate than the synapse units. The architecture assumed a singular pipelined TDM PE per device. The authors chose a HH neuron model for their system. The memory organization relied on the concept of a \textit{divided-and-merged} scheme, which relied on network state being cleverly duplicated for every NM in the system. The authors proved in the paper that their memory scheme allowed for lower memory utilization for up to 1000 NMs (assuming 32b floating-point representation and 10 pipeline stages) compared to a standard scheme.
\parhl{Molin et al. (2015) \cite{molin_fpga_2015}} presented a stochastic SNN-based system for performing dewarping\cite{shafait_document_2007} on the real-time visual data. It used an IF neuron array with random synaptic events. Those random events were sent to the accumulators responsible for specific neurons in the array (120x160 somas). The system was implemented on Xilinx Spartan-6 and was clocked at 100 MHz.
\parhl{Lei et al. (2016) \cite{lei_efficient_2016}} presented the \textbf{Efficient Neuro Architecture (ENA)} with support for approximate arithmetic, as defined in~\cite{shao_array-based_2015, shao_model_2014}. They chose a two-layer WTA topology for the SNN with 800 neurons and around 640 thousand synapses for the MNIST benchmark, for which it achieved 87.7\% accuracy. Architecture-wise, the system consisted of 32 PEs and supported online learning in the form of STDP. It was implemented on Virtex-6, and it was clocked at 120 MHz.
\parhl{Pani et al. (2017) \cite{pani_fpga_2017}} presented a platform for realizing real-time SNNs that could communicate with actual biological NNs (e.g., in a neuroprosthesis). The architecture relied on a varying number of TDM PEs. The presented implementation consisted of eight PEs, each capable of realizing 180 virtual neurons and 16 synapses per neuron - implemented on Xilinx Virtex-6. The design operates at 100 MHz, and the processing was done in real-time. The authors chose the IZH neuron model with an integration timestep of 0.1 ms.
\parhl{Sun et al. (2017) \cite{sun_spiking_2017}} proposed a way of implementing an SCNN as a system to extract patterns from color-sensitive images. Architecture-wise, the system consisted of several PEs equal to the number of layers of the network. The system supported COBA LIF neurons with modifiable conductances, as well as online learning in the form of STDP. The authors used a five-layer CNN network with kernels of different sizes and types as a benchmark.
\parhl{Thakur et al. (2017) \cite{thakur_real-time_2017}} presented a system for segmentation in visual processing, bringing the Integrate-and-Fire Array Transceiver (IFAT) structure\cite{molin_low-power_2017} a VLSI CMOS array of neurons, to the FPGAs. The system is rate-encoded, utilizing Poisson spike trains for sending information from neuron to neuron. 
\parhl{Jailian et al. (2017) \cite{jalilian_pulse_2017}} implemented Pulse Width Modulation (PWM) using SNNs. They implemented a network of IZH neurons, spiking behaviors of which were used to obtain the demanded PWM signal. Architecture-wise, there was a single IZH neuron pipeline realizing the PWM behavior, which was implemented with 6th order PWL approximation on Spartan-6.
\parhl{Ambroise et al. (2017) \cite{ambroise_biomimetic_2017}} presented a system for interfacing an SNN with a biological NN held in a Microelectrode Array (MEA). The architecture had two PEs one for the soma and one for the synaptic current. The system used IZH neurons with AMPA and GABA synapse pipelines and short-term plasticity (in Izhikievich's variant\cite{izhikevich_simple_2003}). The authors tested the architecture with two CPGs with eight neurons for oscillators and four stimulation triggers with 24 inhibitory synapses. The results of the closed-loop experiments showed synchronization between the artificial and biological neurons.
\parhl{Akbarzadeh-Sherbaf et al. (2018) \cite{akbarzadeh-sherbaf_scalable_2018}} presented a scalable architecture for randomly connected SNNs. The implemented COBA HH neurons\footnote{COBA HH neuron is even more elaborate than a standard HH neuron model, due to two additional terms in the differential equations defining its dynamics.} were arranged in a reservoir with sparsely interconnected neurons. The architecture consisted of four PEs supporting in summary 4096 neurons on a Xilinx Artix-7 in real-time. The random generation of connections within the reservoir was performed based on a connectivity vector for one neuron (permutation-based). The neuron implementation used a 6th order PWL approximation, a 33b fixed-point representation (Q9.24) and a 1/128 ms timestep.
\parhl{Sripad et al. (2018) \cite{sripad_snavareal-time_2018}} presented \textbf{SNAVA} - an NMA that simulated biologically plausible SNNs on multi-device (up to 127) systems arranged in a ring topology. Every FPGA held a systolic array of PEs, connectivity units, and a router. The system supported FF networks, with one FPGA dedicated per layer. A central host controlling the simulation was connected to all FPGAs. The architecture supported IZH and LIF neuron models, as well as the STDP for online learning. The authors chose a fixed-point 16b number representation, and a 200-neuron network as a test case. They implemented it on Xilinx Kintex-7 and were able to clock it at 125 MHz with 100PEs, reaching 200 synapses and 128 neurons per PE.
\parhl{Zhang et al. (2019) \cite{zhang_versatile_2019}} presented an NMA arranged as a Network-on-a-Chip (NoC) with TDM PEs. It relied on four PEs realizing LIF neurons, supporting 256 virtual neurons each within specified timestep at 100 MHz. The authors chose 9b fixed-point weights with four available values. The input was encoded as a Poisson-distributed spike train with a length of 32 events. The architecture was tested with a modified MNIST image inference use case (16x16 instead of 28x28 pixels) with FF-FC topology (522 neurons and 133632 synapses), where it achieved 96.26\% accuracy.
\parhl{Guo et al. (2019) \cite{guo_systolic_2019}} presented a systolic array of PEs implementing IF neurons, designed for FF-FC networks and SCNNs. The system supported arrays of 32x32 or 32x16 PEs. The authors chose the 32b fixed-point representation for the neuronal dynamics. The system was clocked at 100 MHz on Xilinx Virtex-7, and the network operated at a timestep of one millisecond. The authors tested the system with MNIST and MNIST Fashion benchmarks for FF-FC and SCNN topologies (MNIST results: FF-FC - 98.84\%, SCNN - 98.98\%).

\mypar{Modern implementations}
\parhl{Huang et al. (2020) \cite{huang_spiking_2020}} proposed an NMA for detecting radioisotope gamma radiation. The topology was a two-layer FF network, reflected by two TDM PEs, one per layer. The neuron model was IF with a 0.001 ms timestep. The authors verified the operation of the system by checking the membrane activity of the neurons and comparing it with the equivalent implementation on a SpiNNaker\cite{furber2014spinnaker} platform. Weights were represented with 8b fixed-point numbers, and the system was clocked at 100 MHz.
\parhl{Abdoli et al. (2020) \cite{abdoli_reconfigurable_2020}} presented an NMA designed for WTA networks simulation with IZH neurons with 0.0625 ms timestep. The NMDA and GABA ion channel dynamics were modeled. The size of the network corresponded to its biological counterpart, i.e., Center-Annular-Surround network (4500 neurons). The PEs shared a shared network activity memory, called a Total Spike Register. The implemented system used four PEs on Xilinx Artix-7 (up to 16 PEs would fit on this device). The authors chose a 30b fixed-point number representation for the weights.
\parhl{Ju et al. (2020) \cite{ju_fpga_2020}} presented an implementation of a particular SCNN. The system comprised three units, each realizing convolutional, pooling or fully-connected layers (7 in total). The neuron model was IF with fixed uniform encoding for the spikes. The learning process relied on performing BP on the ANN counterpart of the implemented SCNN and normalizing outputs of each layer with [0,1]. The weights were converted from 32b floating-point to 8b fixed-point numbers and transferred to the SNN.
\parhl{Guo et al. (2020) \cite{guo_towards_2020}} presented a pruning process concept, relying on removing neurons from SNN, which were not crucial to determine the correct output. The idea was applied to an architecture with multiple PEs and parameters being held on-chip. Spikes were generated via LFSR, and the neuron model was a LIF with COBA synapses and triplet-wise STDP. The authors presented three different strategies for pruning: \textbf{(i)}constant number of neurons pruned after every batch during training, \textbf{(ii)}constant threshold for pruning and \textbf{(iii)}adaptive threshold pruning, based on the activity of a network in a current batch.
\parhl{Gholami et al. (2021) \cite{gholami_reconfigurable_2021}} presented an NMA for approximating non-linear functions. The system comprised 18 IZH neurons arranged in a FF-FC topology (9x9). The architecture relied on a singular PE that computed neuronal dynamics and performed training. The input function was encoded with Inter-Spike Interval (ISI) before reaching SNN. The SNN was taught via gradient descent - the cost function was the function of error between the expected interval and produced interval - on-chip or off-chip. The authors chosed the 32b fixed-point number representation, and the integration step of 0.01 ms.
\parhl{Zheng et al. (2021) \cite{zheng_balancing_2021}} presented an NMA for Vote-For-All networks (VFA), where the input layer is divided into specific receptive fields, which are later fed into several locally connected clusters sensitive to a particular input in those receptive fields. The classification output is decided based on voting from those clusters. The system used a COBA LIF model with triplet-wise STDP for online learning, and number representation of 2b for weights, and 16b fixed-point numbers for membranes. The system was implemented on Xilinx Zynq Ultrascale+, was clocked at 200 MHz and achieved 90.58\% accuracy in MNIST benchmark with 2304 neurons.
\parhl{Aung et al. (2021)\cite{aung_deepfire_2021}} presented \textbf{DeepFire} architecture, designed for accelerating SCNNs. The transduction and convolution and fully-connected layers consisted of several PEs, which implemented the IF neuron dynamics. The test cases included MNIST and CIFAR-10 benchmarks, for which the system achieved the following results: MNIST - 99.14\% at 40.1kFPS; CIFAR-10 - 81.8\% at 28.3kFPS.
\parhl{Wu et al. (2021) \cite{wu_efficient_2021}} presented an NMA supporting STDP implemented using a Fast CORDIC algorithm. The architecture consisted of a 14x10 array of TDM PEs and implemented a 2-layer FF-FC for MNIST benchmark. The second layer was a WTA layer with lateral inhibition, where a sum of votes from multiple neurons was considered. The authors used a LIF neuron model. The system supported STDP without dynamic leakage for the presynaptic trace with Fast-CORDIC-based learning, which resuled in 50\% reduction in the number of iterations before convergence in comparison to the standard CORDIC algorithm. The authors chose a fixed-point 8b numbers for the weights and 16b fixed-point numbers for the membranes. They implemented the system on a Xilinx Zynq-7000 device.
\parhl{Ali et al. (2022) \cite{ali_energy_2022}} presented an implementation of an SNN intended for image inference. It was a purpose-built architecture targeting the simplified MNIST data (5x5 pixels, binary pixels, two classes), with the network topology being an FF-FC (7 neurons). The neurons were organized in layer blocks, which performed calculations for the specific neurons in a time-multiplexed fashion. The authors chose the 8b fixed-point number representation. The system was clocked at 25 MHz, and achieved 95.16\% accuracy on Xilinx Artix-7 for the simplfied MNIST benchmark.
\parhl{Hwang et al. (2023) \cite{hwang_replacenet_2023}} presented \textbf{ReplaceNet} - SW/HW co-design framework for designing SNN-based replacements of the sub-portions of biological neural networks. It supported supervised STDP to train several LIF neurons to provide accurate spike timings of their biological counterparts, dubbed SA-STL (STDP Assisted Spike Timing Learning). Architecture-wise, the system was a systolic array of PEs, each containing a LIF neuron and synaptic pipelines. The system was operating in real-time, which the authors achieved for levels of connection sparsity from 0\% to 90\% on Xilinx Zynq-7020.
\parhl{Kauth et al. (2023) \cite{kauth_neuroaix-framework_2023}} presented \textbf{neuroAIx} framework - an initiative for realizing NMAs on any cluster of processing elements (such as a system comprising 35 NetFPGA SUME boards, presented in the paper). The architecture of neuroAIx was a NoC of FPGA devices (nodes) with a centralized main router and A2A connections in the same row and column of the array. Each node consisted of 8GB of off-chip DDR3 memory and held several PEs connected in a round-robin fashion. The nodes were allowed to progress to the next timestep if neighbors within two hops completed their calculations for a current timestep. The implementation supported LIF and IZH neurons with 32b floating-point representation for both weights and membranes. The test case presented in the paper was the cortical microcircuit\cite{potjans_cell-type_2014} that achieved a 0.46\% difference in spike generation over the software simulation (NEST).
\parhl{Wang et al. (2023) \cite{wang_large-scale_2023}} presented an NMA for realizing a path planning algorithm using SNN based on IZH neurons. Architecture-wise, the system comprised a grid of TDM PEs (6x6), where every PE was responsible of a specific square of neurons (24x24). Those neurons directly represented the map of the environment in which objects could move in eight directions. The system used STDP for learning the path. The authors used the IZH neurons with CORDIC and Binary-based multiplier optimizations. The system operated with a 0.1 ms timestep 2000x faster than real-time on Altera Cyclone-IV clocked at 134.36 MHz.
\parhl{Wang et al. (2023) \cite{wang_resource-efficient_2023}} focused on creating an architecture for SCNNs with module reusability. Reusability was understood as not storing the intermediate states of neurons in memory but instead reusing memory and pipelines as the features propagate through the network. The authors chose the LIF neuron model for neuronal dynamics. Two SCNNs were implemented as test cases for image inference (MNIST - 98.15\% accuracy, CIFAR-10 - 85.71\% accuracy).
\parhl{Shi et al. (2024) \cite{shi_ghost_2024}} presented an LSM with stateless reservoir neurons and a trainable output layer based on IF neurons. The IF neurons in the reservoir did not store the membrane potentials in memory - the potential was obtained on the fly. Architecture-wise, the system consisted of an array of reservoir computing units and readout computing units, with a controller managing both arrays' operations. The neurons communicated via AER-compliant protocol. The network was taught with a BP-STDP method.
\parhl{Chen et al. (2024) \cite{chen_hardware_2024}} presented an SCNN accelerator. The system comprised blocks responsible for either a layer computation or pushing intermediate results between layers. The implemented network had two convolutional layers that utilized STDP, one convolutional layer using supervised STDP, two pooling and two padding layers. The authors chose the IF neuron model with single-spike encoding. For the MNIST benchmark, the system achieved 95\% accuracy, with a processing time of 0.16s per image inference on Xilinx Zynq Ultrascale+, clocked at 100 MHz.
\parhl{Liu et al. (2024) \cite{liu_sc-plr_2024}} presented an architecture relying on stochastic LIF neurons and PLR learning rule\cite{saponati_sequence_2023} to realize the synaptic plasticity. By adjusting the synaptic weights, the neuron could anticipate future inputs based on the patterns and correlations it had observed in the past. Architecture-wise, the system relied on a systolic array of 64 PEs. The authors tested the system with three use cases: robot arm, swarm collision avoidance and MNIST benchmark (90.19\% accuracy on Xilinx Zynq-7000).