\subsection{Class 0 (Traits: none)}
\label{sec:overview_class0}
\begin{wrapfigure}[13]{h}{0.5\textwidth}
  \centering
    \vspace{-10pt}
    \includegraphics[width=0.9\linewidth]{Figs/class0.png}
    \caption{Simplified diagram of a Class 0 NMA.}
    \label{fig:class0}
\end{wrapfigure}
Class 0 architectures support only SNN-based programming model and spike-based communication from the Traits listed in Section \ref{sec:taxonomy}. It is common for them to include a small number of Processing Elements (PEs) that process neuron data stored in the external memory and update all neurons in sequence. Those systems have the potential of simulating the largest networks due to the arbitrary size of the attached off-chip memory, but because of that are also primarily memory-bound. They are the closest to the standard von-Neumann-like accelerators, and take the least inspiration from biology.

\mypar{Early implementations}
\parhl{Waldemark et al. (1998) \cite{waldemark_pulse_1998}} presented a small system consisting of one neuron and two synapses, a part of a bigger network realizing a classification task. The network utilized 19\% of available logic resources on Altera Flex 10K.
\parhl{Jung et al. (2005) \cite{jung_advanced_2005}} presented an NMA, where a multi-master system depends on the efficient data switch to route the data between the specific modules. They built an architecture upon the SP$^2$INN\cite{mehrtash_synaptic_2003} system concept and realizes a single neuron with eight fan-in connections with STDP functionality.
\parhl{Glackin et al. (2005) \cite{glackin_novel_2005, glackin_implementation_2005}} presented an architecture for realizing large-scale networks on FPGAs. They chose a LIF neuron model with dynamics approximated with Euler integration with 0.125 ms timestep. They used 18b fixed-point number representation for weights and membranes. The use case was an SNN realizing angle translation. On the Xilinx Virtex-II Pro device, the authors implemented four PEs and 400 synaptic units running at 100 MHz, resulting in 4200 neurons and 1964200 synapses achieving timestep computation time 4327 times slower than real-time.
\parhl{Nuno-Maganda et al. (2009) \cite{nuno-maganda_hardware_2009}} present an NMA for recall and learning with a multi-layer FF SNN. Architecture-wise, the system comprised several arrays of PEs and Learning Modules (LMs), connected to global data and control buses via two routers. The authors tested a system with two arrays of PEs and two LMs (16 neurons) on a Xilinx Virtex-II Pro device. They chose a 16b fixed-point number representation.
\parhl{Rice et al. (2009) \cite{rice_fpga_2009}} presented an implementation of an SNN comprised of 9000 IZH neurons for character recognition in images. Architecture-wise, the system comprised several parallel pipelined time-domain multiplexed (TDM) PEs. The representation was 16b fixed-point numbers for weights and membranes. The implementation was tested with an FF-FC SNN with two layers for recognizing characters in 96x96px bitmaps and 48 classes. The system was clocked at 199 MHz on Xilinx Virtex-4 and had 25 PEs. 

\mypar{Early modern implementations}
\parhl{Podobas et al. (2017) \cite{podobas2017designing}} presented a scalable NMA relying on a single PE for neuron state update. The system supported IZH and HH dynamics, which were obtained through two approximation schemes: Euler integration with a timestep of 0.1 ms for IZH and Runge-Kutta (RK) with a timestep of 0.02 ms for HH. They used A2A networks of 14000 neurons to test the architecture on Altera Stratix-V. The synaptic connections were created from a common axon, which reduced the number of parallel computations related to spike event delivery, as well as alleviated the necessity to keep the information about the delayed spike for every connection separately.
\parhl{Mostafa et al. (2017) \cite{mostafa_fast_2017}} presented an architecture for SNNs with sparse spiking activity for solving classification tasks. The architecture comprised eight PEs capable of processing 256 neurons. They chose an IF neuron model with membrane and current represented with signed 16b fixed-point number, and weights represented as signed 8b number. The implemented network had an FF-FC topology (610 neurons) and achieved an accuracy of 96.98\% for the MNIST dataset.
\parhl{Galindo Sanchez et al. (2017) \cite{galindo_sanchez_energy_2017}} presented a system for realizing Streaming SNN (S2NN) featuring switching off the parts of the implementation that are not needed. The design flow and the architecture assumed the implementation of FF-FC networks, layers of which could be "switched off" when not in use. The design was implemented on a Xilinx Zynq-7000 device, where the integrated ARM A9 core controlled the power management.
\parhl{Kousanakis et al. (2017) \cite{kousanakis_architecture_2017}} presented a system with LIF neurons on a multi-FPGA system consisting of four Xilinx Virtex-6 devices. The architecture supported learning with a BCM learning rule, as well as random synaptic current generation. The system consisted of 40 PEs responsible for calculating six neurons each, with interconnectivity information and background synaptic noise stored in external memory.
\parhl{Humaidi et al. (2018) \cite{humaidi_spiking_2018}} compared an ANN and an SNN implementations for letter recognition on FPGA. The SNN implementation supported STDP for learning the patterns. The benchmark consisted of four different letters (5x3 pixels). The SNN was reported as more area-efficient and faster than ANN. The topology used was deduced to be a 48-neuron (IZH) WTA network.

\mypar{Modern implementations}
\parhl{Ahn et al. (2020) \cite{ahn_implementation_2020}} presented an architecture for realizing networks of HH neurons. The presented implementation simulated 12 million HH neurons with eight. The communication depended on the data being propagated along the axon bus from and to the PEs. The system stored duplicated information about the network state in every PE. The authors chose a neuron timestep of 0.04 ms, and the system achieved the frequency of 300 MHz on Xilinx Virtex UltraScale+. The number representation was a single-precision floating point.
\parhl{Ogaki et al. (2021) \cite{ogaki_hodgkin-huxley-based_2021}} presented an approach to realize HH-based SNNs with a fully dataflow-based approach\footnote{Using the similar platform as in \cite{cheung_neuroflow_2016} and \cite{cheung_large-scale_2012}).}. The entire system was fully pipelined. The factor differentiating this architecture from the others was an explicit focus on representing the locality of biological neuronal connections - the neurons were connected only to their neighbors in the grid (2D array topology). They showed that their architecture
running a 10000-neuron system (at 300 MHz) on an Alveo U200 FPGA could be up-to 7.5x faster than a state-of-the-art CPU.
\parhl{Zhou et al. (2021) \cite{zhou_neuromorphic_2021}} presented an NMA, which relied on a set of TDM PEs capable of simulating 1024 neurons each. They utilized a weight-sharing concept, where 16 16b weights were shared throughout the synapses of the network. The architecture supported a LIF neuron model with a linear decay, instead of exponential. The system was implemented on a Xilinx Virtex-6 device and tested with the MNIST dataset with 64 PEs to realize an FF-FC network (2010 neurons), achieving an accuracy of 98.41\% after 10 timesteps.