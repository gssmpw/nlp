\subsection{Class 3 (Traits: asynchronous network update)}
\label{sec:overview_class3}
Class 3 architectures support only the asynchronous network update Trait. Those systems have a possibility of reduced power consumption, due to updating the network state only when there are spikes generated. However, the number of neurons is bound by the amount of available logic resources on an FPGA.

\mypar{Early implementations}
\parhl{Hellmich et al. (2004,2005) \cite{hellmich_fpga_2004, hellmich_emulation_2005}} presented \textbf{SNN Emulation Engine (SEE)}, which they claimed it could support up to 50 million IF neurons and 800 million synapses. The architecture organization relied on three FPGAs responsible for: \textbf{(i)}network configuration, monitoring and administration, \textbf{(ii)} computation of the target neurons addresses and \textbf{(iii)} updating neuron potentials and weight updates. A set of three PEs was implemented on the third device connected to three bespoke SDRAMs. The neuron update used a Bulirsch-Stoer integration method.
\begin{wrapfigure}[13]{h}{0.5\textwidth}
  \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class3.png}
    \caption{Simplified diagram of a Class 3 NMA.}
    \label{fig:class3}
\end{wrapfigure}
\parhl{Ros et al. (2006) \cite{ros_real-time_2006}} presented \textbf{RT-Spike}, which was a SW/HW hybrid architecture for realizing SNNs with SRM-model-based neurons. The authors chose a 14b fixed-point number representation. They tested the system with a network of 1024 neurons on four PEs and achieved real-time operation on Xilinx Virtex-2000E at 25 MHz, with 4.16 million updates of the network state per second.
\parhl{Glackin et al. (2009) \cite{glackin_emulating_2009}} presented \textbf{Reconfigurable Architecture}, which aimed at modeling the biological functions of the brain. The architecture comprised a set of PEs simultaneously simulating up to 250 neuron with COBA LIF and SRM neuron models support. Reconfiguring the network was performed on-the-fly, by changing the values in the BRAM memory via a custom DMA core. The implemented system performed edge detection on 512x512 px images on a Xilinx Virtex-4 device with four PEs, using an SCNN with 1.05 million neurons and 52.4 million synapses, reaching 1.05 seconds of inference time per image.

\mypar{Early modern implementations}
\parhl{Yang et al. (2011) \cite{yang_case_2011}} presented an NMA for explaining the computational capability of the early stage of the primate visual system. The input for the topology was a gray image transformed into a temporal firing rate in a given period. The architecture used four FPGAs, one per every image orientation feature, with 64 TDM PEs per device, allowing for one million LIF neurons and two million synapses. It was implemented on the Xilinx Virtex-4 device and clocked at 200 MHz.
\parhl{Neil et al. (2014) \cite{neil_minitaur_2014}} presented \textbf{Minitaur} - an SNN accelerator designed for image inference and robotics applications\footnote{There was also the \textbf{n-Minitaur} system\cite{kiselev_event-driven_2016} which was heavily based on Minitaur and was an adaptation of that system to support processing of the data from up to three spike-based sensors.}. The architecture comprised 32 PEs with bespoke state and weight information caches. The neuron model was LIF with a decay rate based on the distance in time between the consecutive events. The system used broadcasting, instead of point-to-point connections. The authors chose a 16b fixed-point number representation for most of the parameters and values. The system implemented on Xilinx Spartan-6 device achieved 92\% accuracy in MNIST benchmark, using an FF-FC network with 1010 neurons and was clocked at 75 MHz.
\parhl{Cheung et al. (2016) \cite{cheung_neuroflow_2016}} presented \textbf{NeuroFlow} - a complete framework for implementing dataflow-based NMAs\footnote{This idea was first presented in \cite{cheung_large-scale_2012}.}. The design flow assumed that the necessary data was stored in the external memory, and processed by two sets of PEs: one for fetching the information and one for accumulating the spiking events and weights. The generated architectures supported a variety of neuron models (LIF, AdEx IF, IZH and HH), integration methods (Euler, RK4), synaptic input current kernels (exponential, alpha or custom) and a pair-based nearest neighbor STDP. The benchmarks included a simulation of 589824 neurons with varying numbers of synapses on four Xilinx Virtex-6 devices and a simple test of the STDP mechanics between two AdEx neurons. 
\parhl{Wang et al. (2018) \cite{wang_fpga-based_2018}} presented an NMA for simulating mini- and hypercolumn models of the human neocortex. The system followed a hierarchical structure, where a network is implemented as a set of interconnected hypercolumns consisting of several minicolumns (up to 128), where every minicolumn comprised up to eight different types of heterogeneous neurons (100 per minicolumn). The neurons followed a two-compartment COBA LIF model, integrated with a 1 ms timestep. The representation of the weights and postsynaptic potentials was fixed-point 4b numbers, and the system supported axonal delay. The authors tested the system with a simulation of an auditory cortex divided into 100 channels sensitive to different sound frequencies. They mapped every channel to 100 hypercolumns, resulting in 10 thousand hypercolumns with 100 million LIF neurons. The authors claimed the maximum number of neurons and synapses supported by the architecture on an Altera Stratix-V to be 20 million and four trillion, respectively.

\mypar{Modern implementations}
\parhl{Han et al. (2020) \cite{han_hardware_2020}} presented an NMA with a single processing pipeline for LIF neuron dynamics. The authors introduced zero-biased ReLU-based offline training, as well as a hybrid updating algorithm, which, according to them, merged the event-driven and timed operation. They achieved it with separate event queues for a number of consecutive timesteps. The authors claimed the system supported 16384 neurons on Xilinx Kintex-7, and they tested it with the MNIST benchmark and FF-FC network (2058 neurons), for which it achieved an accuracy of 97.06\% and was clocked at 200 MHz, with processing speed of 161 frames per second.
\parhl{Li et al. (2021) \cite{li_fast_2021}} presented an NMA with a clock/event-driven hybrid update scheme - however, according to the Features described in Section \ref{sec:taxonomy}, the system is a member of Class 3. It comprised 12 PEs with five computational cores (CC) in every PE, supporting CUBA LIF neurons with STDP-based learning capability. The four out of five CCs mentioned above could be shared with neighboring PEs. The architecture was implemented on Xilinx Virtex-7 and tested with thr MNIST benchmark and FF-FC (310 neurons) network topology with 16b floating-point representation, achieving 85.28\% and the speed of 61FPS, clocked at 100 MHz.
\parhl{Liu et al. (2022) \cite{liu_fpga-nhap_2022}} presented \textbf{FPGA-NHAP}, designed with a simulation of FF topologies in mind. From an architectural point of view, the system was a single neural core that comprised 16 PEs and supported LIF and IZH neurons via two different pipelines, which were used to process neurons in a time-multiplexed manner. The network size was bound by the 14-bit addressing to 16384 neurons and 16.77 million synapses (with an A2A topology). The connectivity was stored in memory in a form of crossbars, and its size was fixed and architecturally bound. The authors chose a 16b signed fixed-point number representation. The system achieved the accuracy of 97.7\% for LIF and 97.81\% for IZH for the MNIST benchmark and 85.14\% for LIF and 83.16\% for IZH for the Fashion MNIST benchmark.
\parhl{Chen et al. (2024) \cite{chen_sibrain_2024}} presented \textbf{SiBrain} - a spatio-temporal parallel NMA targeting SCNNs. The architecture comprised 64 PEs. It achieved 99.08\% accuracy for the MNIST benchmark with processing speed of 512 frames per second and 90.25\% accuracy for the CIFAR-10 benchmark at 53 frames per second. The system was clocked at 200 MHz and implemented on Xilinx Virtex-7 2000T.