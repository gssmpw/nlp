\subsection{Class 5 (Traits: fully parallel, asynchronous network update)}
\label{sec:overview_class5}
\begin{wrapfigure}[13]{r}{0.5\textwidth}
  \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class5.png}
    \caption{Simplified diagram of a Class 5 NMA.}
    \label{fig:class5}
\end{wrapfigure}
Class 5 architectures support two Traits: fully parallel operation and asynchronous network update. Because the necessary data is stored off-chip, they can support large SNNs and can potentially achieve high operation speeds, due to fully parallel operation and updating the neuron states only if there are spikes. However, those system also bring higher complexity due to need to synchronize asynchronous PEs.

\mypar{Early modern implementations}
\parhl{Ipatov et al. (2019) \cite{ipatov_development_2019}} presented a NoC-based NMA that operated at real-time. Every PE within the NoC allowed for computing up to 512 neurons with a fan-in of 512 synapses. The authors used two Xilinx Kintex-7 devices, which allowed for the implementation of 128 PEs per device. This resulted in the maximum possible number of 131 thousand neurons and 67 million synapses on the platform. All of the above-mentioned data was put on the external DDR3 memory with a capacity of 128 Gb and a total bandwidth of 100 Gbps at 400 MHz.

\mypar{Modern implementations}
\parhl{Yang et al. (2021) \cite{yang_bicoss_2021}} presented \textbf{BiCoSS} - a large-scale NMA, designed to simulate parts of the human brain. The system was organized as a multigranular architecture and had a hierarchy of different levels of components. On the top level, the system was organized into nuclei - divided into a set of interconnected SNNs - that exchanged information  through a set of external routers. The architecture was arranged in the Butterfly Fat Tree topology (BFT) on the nuclei and neuron levels, but the SNN units were fully interconnected. The SNNs within the nucleus communicated through the roots of the BFTs. The implementation presented in the paper relied on 35 Altera Cyclone-IVs arranged in seven nuclei of five devices (four FPGAs for SNNs, one for routing per nucleus). The architecture supported LIF, IZH and HH neurons and both pair and triplet-wise STDP. The authors tested the system with biology-related tasks, such as decision-making with reinforced learning or context-dependent learning, but also with MNIST benchmark, where it achieved 94.8\% accuracy.
