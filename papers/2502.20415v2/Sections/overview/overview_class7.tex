\subsection{Class 7 (Traits: fully parallel, collocated computation and memory, asynchronous network update)}
\label{sec:overview_class7}
Class 7 architectures support all three Traits, and their operation can be deemed the most \textit{brain-like}. They are also the most bound by the on-chip resources out of all of the Classes, but can allow for the fastest and most energy-efficient operation. However, realizing networks with more than thousands of neurons within this Class is troublesome, due to contemporary FPGAs not being large enough to support such complex and large systems.

\mypar{Early implementations}
\parhl{Allen et al. (2005) \cite{allen_plasticity_2005}} presented a platform for pattern recognition in olfactory applications. The system supported IF neuron model with weights uniformly set to 1, so the output spike train of the neuron had a firing rate equal to the average rate of the input spike trains. We deduced that architecture-wise the system was an array of PEs - one for every neuron. The authors reported eight neurons with 1024 input synapses implemented on the Xilinx Virtex-II Pro device.
\parhl{Girau et al. (2006) \cite{girau_fpga_2006}} presented an architecture for a Local Excitatory Global Inhibitory Oscillator Network (LEGION)\cite{wang_locally_1995}, based on IF neurons, for image inference.
\begin{wrapfigure}[13]{h}{0.5\textwidth}
  \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{Figs/class7.png}
    \caption{Simplified diagram of a Class 7 NMA.}
    \label{fig:class7}
\end{wrapfigure}
Architecture-wise, due to the inherent nature of the LEGION-type SNNs, the implementation comprised a 2D array of PEs. Neurons were interconnected with excitatory connections, and there was a single global inhibitor connected to all neurons. The size of the array was equal in size to the size of the image under classification.
\parhl{Cassidy et al. (2007) \cite{cassidy_fpga_2007}} presented a 2D array of LIF neurons capable of realizing Spatio-Temporal Receptive Fields (STRFs), neural parameter optimization algorithms and STDP. The architecture comprised 32 PEs set up as an array, connected to the weight and delay memory. The datapath was time-multiplexed to allow for parallel processing and serial communication. Every neuron was connected to its four neighbors. The system was implemented on Xilinx Spartan-3 device and was clocked at 50 MHz.

\mypar{Early modern implementations}
\parhl{Bonabi et al. (2021) \cite{bonabi_fpga_2012}} the authors presented a network of 16 HH neurons on a Xilinx Virtex-7 device. The neuronal dynamics were approximated with the CORDIC algorithm and step-by-step time integration. The authors focused on neuronal pools, i.e., groups of interconnected neurons (in A2A fashion) with specific functions, which could be either diffusive or localized.
\parhl{Dean et al. (2014) \cite{dean_dynamic_2014}} presented \textbf{DANNA} - an NMA that comprised an array of multi-purpose \textit{elements} that could be configured to perform a function of a neuron, synapse or fan-in/fan-out connection. The architecture was a 2D grid of \textit{elements}, which be neurons, synapses or fan-out elements. The architecture supported the LIF neuron model with axonal delays. The synapse model supported long-term potentiation and depression for the weights. The network topology was obtained by performing the genetic algorithm for finding the optimal implementation. The authors tested the system\footnote{There were many examples of the use cases for this architecture, including NeoN\cite{mitchell_neon_2017}, which was a neuromorphic control for autonomous robotic navigation.} with a randomly connected network of 10000 neurons, implemented on a Xilinx Virtex-7 device with a 9b fixed-point number representation, clocked at eight megahertz, while the \textit{elements} was clocked at 64 MHz. The system was also updated in 2018 and presented as DANNA 2\cite{mitchell_danna_2018}.
\parhl{Nanami et al. (2016) \cite{nanami_fpga-based_2016}} presented a modification of an existing Digital Spiking Silicon Neuron (DSSN)\cite{kohno_digital_2007} idea. The proposed architecture used 16 DSSNs connected to 16 synapse units, each supporting 256 external synapses - all operating at 10x real-time speed. The number representation was 18b. There were no implementation details provided in the paper, so it is difficult to draw any realistic conclusions about the feasibility of this design.
\parhl{Wilson et al. (2017) \cite{wilson_reconfigurable_2017}} presented \textbf{Programmable VHDL Neuron Array (PVNA)} - an NMA with neurons following the modified version of Message-Based Event Driven (MBED)-based neuron model\cite{claverol_event-driven_2000}. The architecture relied on a common bus connecting PEs for neurons and synapses with time-multiplexed access. The system supported pausing the simulation for adjusting the system on-the-fly and performing external online learning with STDP. The synapse PEs were daisy-chained with modifiable connections. The implemented system supported a maximum of 100 neurons, 16 pattern generators and 200 synapses on a Xilinx Virtex-5 device and it was tested with a C. Elegans locomotor system simulation with 86 neurons, 6 pattern generators and 180 synapses.
\parhl{Farsa et al. (2019) \cite{farsa_low-cost_2019}} provided a simplified SNN implementation with a focus on minimizing utilization figures. The authors designed it for image inference performed on 5x5 arrays of binary pixels. The neuron model was LIF with simplified operation (1.25 ms timestep and fully multiplierless). The topology was an FF-FC network (6 neurons). The implementation of a single neuron with this architecture could be clocked at 412.371 MHz.

\mypar{Modern implementations}
\parhl{Gupta et al. (2020) \cite{gupta_fpga_2020}} presented an architecture for realizing a WTA network for classification tasks. The system comprised 16 PEs, one for every neuron in the output layer of the WTA network, as well as 784 input units for the input spikes related to the pixels. It supported online learning in the form of STDP and fixed-point numerical representation of 24b width. The authors tested the system on Xilinx Virtex-6, where it was clocked at 100 MHz and achieved the inference speed of 2000 frames per second for the MNIST benchmark.
\parhl{Prashanth et al. (2021) \cite{prashanth_fpga_2021}} presented a general view of the implemented architecture, which we deduced to be an accelerator for a particular SCNN with LIF neurons. The architecture used an ARM core for system control and an array of PEs. The paper did not provide any concrete numbers, but we deduced that four PEs were implemented in hardware. The authors also claimed that the system supported STDP.
\parhl{Wang et al. (2023) \cite{wang_spiking_2023}} presented an architecture with a specific memory optimization technique, targeting the FF SNNs implementations. They used \textit{ping-pong buffers} to speed up operations. The authors focused on optimizing the sparse spike characteristics: the neuron fan-in was cut into smaller chunks of 64, which were OR'ed to see if there were any spikes incoming. The architecture supported LIF neurons and 256 PEs with one-hot encoding, which the authors claimed increased performance for high levels of connection sparsity (8.84x better performance with 90\% sparsity).