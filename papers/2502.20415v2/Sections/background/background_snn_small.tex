\subsection{Spiking Neural Networks}
Biological neurons communicate via \textit{action potentials} - voltage pulses traveling from one neuron to another, caused by rapid changes in neurons' membrane potentials - commonly referred to as \textit{spikes} or \textit{spike events}. Networks built using neurons that mimic this behavior are called \textit{Spiking Neural Networks} (SNNs). SNNs are often called the \textit{third generation networks}\cite{maass1997networks} that add a notion of time to the artificial neural networks (ANNs). SNNs have been used as tools to understand the biological brain in the neuroscience community (e.g., to study cortical columns\cite{potjans_cell-type_2014}) through simulators such as Neuron\cite{hines1997neuron}, NEST\cite{gewaltig2007nest}, or Brian\cite{stimberg2019brian}. Lately, these models have also been considered for solving machine learning (ML) problems. Moreover, SNNs are believed to reduce the energy consumption needed to solve those problems and/or solve them faster, primarily by leveraging the sparseness of spike communication.
Spiking neurons can be arranged in various topologies that fit different scenarios, from classification tasks to neuroscientific experiments. Those include, but are not limited to: Feed-Forward (FF) - especially the fully-connected subtype (FF-FC) which is the spiking equivalent of the multilevel perceptron (MLP), Spiking Convolutional Neural Networks (SCNNs) - which despite the feed-forward operation deserve a separate category, because of its popularity and utilization of convolutional kernels and not direct connections between neurons, Liquid State Machines (LSM)\cite{maass2011liquid} - including a randomly-connected reservoir of neurons and a fully connected output layer, randomly-connected (RAND) - being a reservoir of recurrently- and randomly-connected neurons, all-to-all-connected (A2A) and biological structures represented as SNNs\footnote{Presented abbreviations are \textbf{not necessarily} used unanimously. Still, they commonly appear in the literature.} - e.g., Central Pattern Generators (CPGs)\cite{wilson1961central,jankowska1972electrophysiological}, which are neuronal circuits that when activated can produce rhythmic motor patterns such as walking or breathing.
As for the communication, the most popular way of exchanging spike events is the Address Event Representation (AER)\cite{mahowald1994analog} protocol, representing the event as the destination address of a neuron or a processing element (PE) responsible for computing its dynamics, and the timestep when it occurred.

\subsubsection{Neuron models}
\begin{table}[h]
  \caption{The most commonly used spiking neuron models in our survey (see Section \ref{sec:trends} for analysis). The biological plausibility and complexity of the model metrics are based on \cite{izhikevich_which_2004} and our own analysis. The complexity metric relates to the estimated number of floating-point operations (addition, multiplication, etc.) necessary to advance to the next timestep (1 ms) with a selected neuron model ($v$ - membrane potential, $I$ - synaptic current, $R$ - leakage resistance, $g_L$ - leakage conductance, $C$ - membrane capacitance, $g_K$ - conductance of the potassium ion channel, $g_{NA}$ - conductance of the sodium ion channel; other parameters are model-specific, and interested readers are referred to the related literature for detailed explanations).}
  \label{tab:neuron_models}
  \begin{tabular}{c | l c c }
    \toprule
    \multicolumn{4}{c}{\textbf{Neuron Models}} \\
    Name &  Formula & Bio. plausibility & Complexity \\     
    \midrule  \midrule
    Integrate-and-Fire (IF)  & $\begin{array}{ll}
                                 \frac{dv}{dt} = RI                     
                                \end{array}$                             & V. Low & ~5 FLOPs   \\ %\hline
    Leaky IF (LIF)           &  $\begin{array}{ll}
                                 \frac{dv}{dt} = -(v - V_{reset}) + RI
                                \end{array}$                             & Low    & ~10 FLOPs  \\ %\hline
    Izhikevich (IZH)         & $\begin{cases}%{ll} 
                                       \frac{dv}{dt} = 0.04v^2 + 5v + 140 - u + I \\
                                       \frac{du}{dt} = a(bv - u)
                                       \end{cases}$                     & Low    & ~60 FLOPs\\ %\hline
    
    FitzHugh-Nagumo (FHN)   & $\begin{cases}%{ll} 
                                       \frac{dv}{dt} = v - \frac{v^3}{3} - w + I \\
                                       \frac{dw}{dt} = 0.08 (v + 0.7 - 0.8w)
                                       \end{cases}$                     & Medium    & ~100 FLOPs\\ %\hline

                                       
    Adaptive IF (AdEX)     & $\begin{cases}%{ll} 
                                       \frac{dv}{dt} = \frac{-g_L(v-E_L) + g_L \Delta_T exp(\frac{v-v_T}{\Delta_T}) - w + I}{C} \\
                                       \frac{dw}{dt} = a(v-E_L) - w
                                       \end{cases}$                     & Medium    & ~200 FLOPs\\ %\hline

    
    Hodgkin-Huxley (HH)      &  $\begin{cases}%{llll}
      \frac{du}{dt} = \\ \frac{-g_{K}n^4(U-V_{K})-g_{Na}m^3h(U - V_{Na})-g_{leak}(U - u_{leak}) + I}{C_m} \\ 
      \frac{dn}{dt} = \alpha_n(U)(1-n)-\beta_n(U)n \\
      \frac{dm}{dt} = \alpha_m(U)(1-m)-\beta_m(U)m \\
      \frac{dh}{dt} = \alpha_h(U)(1-h)-\beta_h(U)h 
                                \end{cases}$                            & High   & ~1200 FLOPs   \\ 
    
 
  \bottomrule
\end{tabular}
\end{table}
An SNN uses a \textit{spiking neuron model} to describe the functionality of generating outgoing spikes. Those models are formulated using a set of ordinary differential equations (ODEs) that describe how a neuron cell membrane potential interacts with an injected current or modified conductance over time. In addition to the ODEs, models have a condition for emitting a spike, typically when the membrane voltage potential reaches a certain value, after which the neuron enters a state of inactivity (called \textit{refractory period}). Neuron models vary in biological plausibility and complexity, with different models suitable for different use cases. Table \ref{tab:neuron_models} outlines six important neuron models, their mathematical description, biological plausibility, and an estimation of how many floating-point operations (FLOPs) are needed to advance one millisecond of simulation time using them. The Integrate-and-Fire (IF) neuron model, which is simple and biologically implausible, is also the least expensive to compute and requires only a handful of operations. On the other hand, the Nobel prize-winning Hodgkin-Huxley (HH) model is complex, describes in high detail the behavior of biological neurons, but it requires thousands of operations to solve the ODEs in a numerically stable manner. Hence, the choice of neuron model comes primarily from the use case: an SNN used for detailed simulation of the biological system might need HH-like neurons (e.g., the OpenWorm\cite{sarma2018openworm} project is based on these), whereas a machine learning system performing image inference might not need that level of realism, and could use a less expensive model (e.g., the Leaky Integrate-and-Fire (LIF) model, which is a popular choice for image inference\cite{diehl2015unsupervised}). While the neurons in Table \ref{tab:neuron_models} were selected based on them being the most popular in neuromorphic hardware (as we will see in Sections~\ref{sec:overview} and \ref{sec:trends}), there are multiple other models worth mentioning, each positioned somewhere in the span of complexity between LIF and HH\cite{hodgkin_quantitative_1952}. These include the Spike Response Model (SRM)\cite{gerstner_neuronal_2014}, pulsed neurons\cite{waldemark_pulse_1998}, Message-Based
Event-Driven (MBED) neurons\cite{claverol_event-driven_2000} and Traub's model\cite{traub1991model}, to name a few.

\subsubsection{Synapse Models}
Neurons connect through synapses, which accept incoming spikes from other neurons. Synapses convert those spikes into a current or a change in conductance of the targeting neuron's extensions (called \textit{dendrites}). The current or change in conductance modifies the neuron's membrane potential and can cause it to spike. The choice and functionality of a synapse model has a critical impact on the complexity of the hardware to a much greater extent than the choice of neuron model, mainly because there are many orders of magnitude more synapses than there are neurons\cite{azevedo2009equal}.
Synapses, which from the biological point of view include GABA, NMDA and AMPA ion channels\cite{ben-ari_gabaa_1997}, can be modeled as current-based, where they modify the injected current into the target membrane or conductance-based (CUBA), where they modify the conductance (COBA). As with neurons, synaptic dynamics are typically described using ODEs.
Synapses are extremely important and are thought to play an even larger role than neurons in an SNN. This is because synapses facilitate \textit{learning} in the change of their \textit{strength} (conductance). This process is called \textit{synaptic plasticity}\cite{citri2008synaptic}, and there are many different models available today that can be used to facilitate learning in an SNN. Most models are Hebbian\cite{gerstner2011hebbian} in nature, meaning that they relate the increase or decrease in strength to the simultaneous activity of pre- and post-synaptic neurons. One of the more popular examples is STDP\cite{song2000competitive}, which modifies the strength of synapses according to how close in time a synapse induces a spike in the target neuron. Supporting plasticity in the synapse model significantly increases the computational cost of a synapse.
Apart from STDP, there are other important plasticity models, e.g., Bienenstock–Cooper–Munro (BCM) rule\cite{bienenstock_theory_1982} based on observations in homeostatic plasticity. There are also other methods, which can be classified as: \textbf{(i)} backpropagation-related (e.g., SpikeProp\cite{bohte_spikeprop_2000}, Backpropagation Through Time (BPTT)\cite{werbos_generalization_1988}), \textbf{(ii)} direct-feedback-related (e-prop\cite{bellec_solution_2020}, DECOLLE\cite{kaiser_synaptic_2020}) and \textbf{(iii)} creating an equivalent ANN, performing learning based on Stochastic Gradient Descent (SGD) or Backpropagation (BP) and transferring the resulting weights to an SNN.