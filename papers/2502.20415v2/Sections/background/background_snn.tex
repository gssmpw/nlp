\subsection{Spiking Neural Networks and their components}
A Spiking Neural Network (SNN) is a type of an Artificial Neural Network (ANN), often called the third generation of neural networks in general[REF]. Drawing from the advancements in our understanding of the human brain operation, the neurons in SNNs communicate with precisely-timed rapid increases of the postsynaptic potential, often modeled as spikes in discrete time domain. This is akin to how the biological neurons communicate and thus it brings SNNs closer to biology than it is in the case of classic ANNs. It is then of no surprise that SNNs are often used in neuroscientific research to simulate parts of the human brain\cite{kauth_neuroaix-framework_2023,guerrero-rivera_programmable_2006, ambroise_real-time_2013, bonabi_fpga_2014, luo_real-time_2014, yang_cost-efficient_2015, luo_real-time_2016, wilson_reconfigurable_2017, wang_fpga-based_2018, yang_real-time_2019}. There is, however, a great deal of research being performed to find other possible use cases for SNNs, especially in scenarios where power consumption is a concern\cite{liang_113ujclassification_2021,aung_deepfire_2021, luo_real-time_2016}.





In general, an SNN can be devised into two main elements are responsible for realizing the spiking behaviour - the spiking neurons and synaptic connections. To be faithful to biology, one should also mention the axon, but in the majority of research related to this survey the dynamics related to axon is usually merged with the synaptic connections, meaning that there is no distinction in the implementations between the axon and dendrites, between which the actual synapse exists. It is more convenient to consider synapses as the entire connection, with possible axonal parameters added, such as propagation delay of the spikes. In the following subsections, a brief descriptions of the spiking neuron, synaptic connection and resulting operation of the SNN will be provided. Moreover, the existing learning methods will be described that apply to SNNs.

\subsubsection{Spiking neurons and their models}
As the inventors of the classic ANNs were drawing their inspiration from biology when creating this computation model, it is unsurprising that certain similarities exist between ANNs and SNNs, especially with the existence of neurons - a distributed graph of computing nodes that process information provided as an input and based on some internal mechanic provides output which can be fed into other neurons or utilized as an output of the system. However, this is where the similarities end, primarily due to the inherent nature of the spike-based operation of the SNNs.

In general, the neurons in SNNs consist of a membrane and a set of channels that mimic the ionic connections between the neurons\footnote{In biology there are other connections, such as gap junctions and astrocytes, but those are rarely realized in the context of computations on SNNs, due to our uncertainty about their role in this process.}. The concept of a membrane is common for nearly all of the spiking neuron models, but the ionic channels are realized with a varying level of biological plausibility, depending on how close the implemented SNN is supposed to be in relation to its biological counterpart. The membrane acts like a capacitor and holds charge that is being increased or decreased based on the presynaptic, i.e., coming from another neuron, potential changes. One can view this process as charging the capacitor with a certain varying current. Moreover, there is an additional leaking dynamic, which means that the potential value converges to a certain steady potential, often called the "reset" potential. This leak can be though of as a resistor connected in parallel to the capacitor and to the reset potential. If we assume $C$ as the membrane capacitance and $R$ as the membrane leaky resistance, then from the electrical engineering standpoint, a spiking neuron is acting like a leaky integrator with a time constant of the membrane being $\tau_m = RC$. There is, however, one more mechanic that the spiking neuron conforms to and that is the actual spiking. One of the properties of any spiking neuron is a threshold voltage, i.e., the value of the membrane potential that causes the neuron to emit a postsynaptic potential or spike. This is followed by the neuron entering a refractory state, during which it will not fire again for a specific period. Moreover, immediately after emitting a spike, the membrane potential drops to the aforementioned reset potential. If we disregard the refractoriness, the following equation describes this leaky integration behaviour\cite{gerstner_neuronal_2014}
\begin{equation}
    \tau_m\frac{du}{dt} = -[u(t) - u_{reset}] + RI(t)
\end{equation}
where $u(t)$ is the membrane potential, $u_{reset}$ is the reset potential and $I(t)$ is the presynaptic current, caused by changes in the presynaptic potential. This equation describes the simplest spiking neuron model - \textbf{Leaky Integrate-and-Fire (LIF)}. This model is very popular in neuromorphic engineering due to its simple representation and the fact that it realizes the core concept of a biological neuron without delving into intricate details of ion channel dynamics. A simplification of this model is Integrate-and-Fire model, which follows the same dynamics but without the leak mechanic. It is often used whenever biological plausibility is not a concern. It is also important to mention that due to popularity of the LIF neuron.

On the other end of the complexity spectrum there is a \textbf{Hodgkin-Huxley model (HH)}, which aims at representing the neuronal dynamics as accurately as possible. If one would stick to the electrical engineering point of view, the model still revolves around leaky integration - the membrane still acts as a capacitor. However, instead of a just the input and leak currents being present, there are two additional currents related to the sodium and potassium ion currents\footnote{The authors performed experiments on the giant axon of the squid, yet the model devised from those observations is remarkably good at replicating spiking behaviour of the neurons in the human brain, as well.}, which flow through voltage-modulated ion channels, one for sodium and one for potassium, respectively. A general equation for this model can be summarized as
\begin{equation}
    C\frac{du}{dt} = -\sum_kI_k(t) + I(t)
\end{equation}
where $I_k(t)$ are the current components passing through the ion channels and $I(t)$ is the overall applied current. This general equation can be expanded to a set of four ODEs describing the dynamics of the system in connection to all of the channels present\cite{hodgkin_quantitative_1952}
\begin{equation}
    I = C\frac{du}{dt} + g_{K}n^4(U-V_{K}) + g_{Na}m^3h(U - V_{Na})+g_{leak}(U - u_{leak})
\end{equation}
\begin{equation}
    \frac{dn}{dt} = \alpha_n(U)(1-n)-\beta_n(U)n
\end{equation}
\begin{equation}
    \frac{dm}{dt} = \alpha_m(U)(1-m)-\beta_m(U)m
\end{equation}
\begin{equation}
    \frac{dh}{dt} = \alpha_h(U)(1-h)-\beta_h(U)h
\end{equation}
where $\alpha_i$ and $\beta_i$ are rate constants for the i-th ion channel, which depend on the membrane voltage (and not time), $g_i$ is the conductance of the i-th channel, $n$, $m$ and $h$ are dimensionless probabilities associated with appropriate channel activation (or inactivation). This type of neuron model is often used in neuroscientific research due to abundance of neuronal dynamics, i.e., spiking patterns, that it can replicate. However, its computation cost is rather high, considering the four ODEs that need to be processed per neuron update.

Albeit the most popular, the aforementioned LIF and HH neuron models are not the sole models used in the neuromorphic engineering nowadays. There were several attempts at merging the high complexity of the latter with simpler implementation of the former. Those most well-known neuron model that - to a degree - meets this criterion is the model presented by \textbf{Izhikevich\cite{izhikevich_simple_2003} (IZH)}. This model differs from the other two mainly due to the methodology with which it was created. Namely, the authors were trying to develop a simple mathematical model capable of realizing different spiking behaviours observed in neurons of the human brain by adjusting numerical constants, which are simplified representations of the actual biological metrics. The model itself is a set of two ODEs with additional condition related to spiking
\begin{equation}
    \frac{dv}{dt} = 0.04v^2 + 5v + 140 - u + I
\end{equation}
\begin{equation}
    \frac{du}{dt} = a(bv - u)
\end{equation}
\begin{equation}
    if \ v = 30mV \ then \ v \leftarrow c \ , \ u \leftarrow u+d
\end{equation}
where $v(t)$ is the membrane voltage, $u(t)$ is the membrane voltage recovery variable, $a$ is the time scale of the recovery variable, $b$ is the sensitivity of the recovery variable, $c$ is the after-spike reset value of $v(t)$ and $d$ is the after-spike reset value of $u(t)$. IZH model is often used in both neuroscientific research and applications closer related to computer science, such as image recognition or other classification tasks. It is mainly due to the computational cost being relative low (in comparison to HH) and reasonably high biological plausibility.

The previous three neuron models that describe the neuronal dynamics in terms of differential equations of the currents flowing into the neurons and membrane potentials. However, there is also a rather popular approach to this topic that replaces the parameters of the model by parametric functions of time, which in this context can be viewed as filters. The neuron model can be then interpreted as a set of a membrane filter, a function describing the shape of the spike and a function for the time course of the threshold. This results in a concept known as the \textbf{Spike Response Model (SRM)}. Overall, it is a generalization of the LIF model but without the intrinsic firing threshold, which instead is "replaced" with a sharp numerical threshold for reset. In general, SRM allows for richer sub-threshold behaviour than that of a LIF and accounts for various aspects of refractoriness and adaptation. SRM can be described by the evolution of the membrane potential $u(t)$ with a dynamic threshold mechanic, which makes the neuron fire when it is reached from below
\begin{equation}
    u(t) = \int_{0}^{\infty}\eta(s)S(t-s)ds + \int_{0}^{\infty}\kappa(s)I_{ext}(t-s)ds + u_{reset}
\end{equation}
\begin{equation}
    neuron \ fires \Leftrightarrow u(t) = \nu(t) \ and \ \frac{d[u(t) - \nu(t)]}{dt} > 0
\end{equation}
where $\eta(s)$ is describes the standard form of an action potential of the neuron, $\kappa(s)$ is the linear response of the membrane potential to the input current, $S(t-s)$ entering the neuron up until a specific point in time, $I_{ext}$ is the input current, $u_{reset}$ is the reset potential and $\nu(t)$ is the dynamic threshold. Due to the nature of dynamic threshold, to spiking behaviour of SRM is stochastic. What is particularly interesting is that the LIF neuron model can be \textit{exactly} mapped to the SRM - more specifically its special case, $SRM_0$.

There are also other neuron models available in the field of neuroscience, such as Hindmarsh-Rose (HR), FitzHugh-Nagumo (FHN), AdEx and many others. However, presenting them all is not the main goal of this survey, and only the most popular neuron models spotted during the preparation of this review were briefly introduced in this paper. In the remainder of this paper, should there appear a model which has not been described in this section, it will be briefly introduced when it appears in the presented architectures - mainly in relation to the aforementioned four models, their differences and similarities.

\begin{table}[h]
  \caption{Spiking neuron models glossary. The qualitive metrics (biological plausibility and complexity of the model) are based on \cite{izhikevich_which_2004}.}
  \label{tab:neuron_models}
  \begin{tabular}{cccc}
    \toprule
    Name&Abbr.&Bio. plausibility&Complexity\\
    \midrule
    Integrate-and-Fire & IF & Lowest & Lowest \\
    Leaky Integrate-and-Fire & LIF & Low & Low\\
    Adaptive Exponential IF & AdEx & Low & Low\\
    Spike Response Model & SRM & Moderate & Low \\
    Izhikevich & IZH & Moderate & Low \\
    FitzHugh-Nagumo & FHN & Moderate & Moderate \\
    Moris-Lecar & ML & High & High\\ 
    Hindmarsh-Rose & HR & High & High \\
    Wilson & WIL & High & High \\
    Hodgkin-Huxley & HH & Highest & Highest\\
  \bottomrule
\end{tabular}
\end{table}

Table \ref{tab:neuron_models} provides a glossary of the spiking neuron models with general qualitative metrics describing their nature. All of the models provided were decided to be of their own category, apart from IF, which is a simplified LIF model. However, due to its popularity, it was specified as the simplest spiking neuron model possible.


\subsubsection{Synaptic connections}
\label{sec:synaptic_conns}

\begin{wrapfigure}{h}{0.6\textwidth}
  \centering
    \begin{subfigure}{0.9\linewidth}
    \centering
    \includegraphics[width=0.75\linewidth]{Figs/snn_example_bio.png}
    \caption{Diagram of the biological network.}
    \end{subfigure}
    \medskip
    \begin{subfigure}{0.9\linewidth}
    \centering
    \includegraphics[width=0.6\linewidth]{Figs/snn_example_lif.png}
    \caption{Diagram of equivalent SNN, assuming CUBA LIF neurons.}
    \end{subfigure}%
    \caption{Synaptic connections example - a 2x1 neural network.}
    \label{fig:snn_neuron_example}
  \Description{Diagrams representing a synaptic connections example - a 2x1 neural network.}
\end{wrapfigure}

In the previous subsection, a brief overview of the spiking neuron models used in neuromorphic engineering was presented. In SNNs those neurons can be interconnected in various ways, creating all sorts of topologies. Those connections can be of both electrochemical (ionotropic receptors or ion channels, e.g., GABA, AMPA and NMDA) and electrical nature (e.g., gap junctions)\cite{ben-ari_gabaa_1997}, however the former are modelled more often in NMAs, as they are proven to be related to the cognitive process. Moreover, as opposed to chemical synapses, electrical ones convey constant stream of information between the connected neurons, thus the implementation cost is rather high, which is another reason why they are not often implemented\cite{wildie_reconfigurable_2009}. Despite that, the gap junctions have received fair attention through the years and the recent research suggest that they may have an impact on plasticity in learning\cite{talukdar_gap_2022,choi_nmdar-mediated_2020}, which may change what kind of synaptic connections will be modelled in NMAs in the future.

In biological neurons, the synaptic connections are between the axons of the presynaptic neurons and dendrites of the postsynaptic neurons, as it can be seen in Figure \ref{fig:snn_neuron_example}. The pulse has to travel down the axon and once it reaches the synaptic contact, crosses the barrier and reaches the soma of the postsynaptic neuron. There are three two main concepts related to those connections which define their dynamics: the synaptic current flowing into the soma and the delay between the emission of spike by presynaptic neuron and the reception by the postsynaptic neuron.

As the synaptic currents flowing into the neurons, the conductances of various ion channels and leak mechanics are rather complex, the designers usually choose one of two possible approaches to modelling synaptic connections in their implementations, depending on what kind of simulation they are aiming for. If the biological plausibility has a lower priority, the synaptic dynamics can be defined with a Current-Based (CUBA) model. The CUBA model utilizes a single-term description to define the current flowing into (and out of, due to the leakage) the neuron, modulated by the synaptic weight, representing the strength of the connection between the neurons\cite{vogels_signal_2005} This is done with the assumption that as most synapses are distant from the soma and thus the change in voltage which reaches the membrane is effectively a current pulse, following a certain "synaptic kernel", describing the course of the synaptic event. The synaptic current related to a postsynaptic neuron $j$ can be summarized with
\begin{equation}
    I_j = \sum_{k}\sum_{s}w_k\varepsilon(t,t_{s,k})    
\end{equation}
where $w_k$ are the presynaptic weights and $\varepsilon(t,t_{s,k})$ is the synaptic kernel\cite{hornung_simulation_2020}, which models the actual postsynaptic potential (PSP) shape and is usually of exponential, alpha or double-alpha function nature. 

The Conductance-Based (COBA) synapses on the other hand are defining the synaptic event as a change of conductance in time, increasing this model's biological plausibility and can be summarized with the following term
\begin{equation}
    g_{exc/inh}(t) = \sum_{k}\sum_{s}w_k\varepsilon{exc/inh}(t,t_{s,k})
\end{equation}
where $g_{ex/inh}$ is the conductance of excitatory and inhibitory channels. To obtain the the synaptic current, the following relation can be used
\begin{equation}
    I_j = g_{exc}(t)(E_{exc} - V) + g_{inh}(E_{inh} - V)
\end{equation}
where $E_{exc}$ and $E_{inh}$ are the reversal excitatory and inhibitory potentials respectively, and $V$ is the neuron membrane potential. This model is naturally more elaborate and thus requires more resources to realize a bespoke processing unit in hardware\cite{vogels_signal_2005, hornung_simulation_2020}.

Apart from the CUBA and COBA models, the designers can also choose whether to model the propagation delays or not. Those delays can be thought of as a sum of times it take a synaptic pulse to travel down the axon and through the synapse to finally reach the postsynaptic neuron. This adds another degree of freedom in terms of data which can be "stored" in the SNN, due to the fact that different presynaptic connections have different immediacy to influencing the membrane potential of the postsynaptic neuron. Consequently, there are synapses that can be considered "immediate", i.e., no delay is present and the spike is arriving "instantly" or the travelling down the axon is modelled. It should be mentioned, however, that as long as there is a notion of discrete time in the SNN simulation, the "immediate" synapse will transmit the spike to the postsynaptic neuron at the following timestep.

\subsubsection{Topologies and learning methods}
SNNs is a compute model which is starkly different in operation than computation realized with CPUs, GPUs and other representatives of von Neumann architecture. It thus require a different way of implementing a solution for a specific task. This is achieved in two main steps: selecting a topology of the network and adjusting the connections between the neurons to perform the required operations.

As it is the case with classic ANNs, different topologies of the SNNs are proven to work better in different scenarios. Ranging from the Feed-Forward (FF) networks known from the first use cases of the classic ANNs (e.g., Multi-Layer-Perceptron (CITE?)), through spiking Convolutional Neural Networks (SCNNs) to Liquid State Machines (LSMs), there are many topologies to choose from. The authors of those topologies drew the inspiration from biology in order to realize networks similar in function and shape to the their biological counterparts. Throughout the surveyed papers, various topologies were used for different reasons, which \textit{sometimes} resulted in specific requirements for the architecture being implemented, i.e., the topology implied the use of a certain architecture. This, however, was not always the case and there were cases when architecture supported multiple different topologies. 
%This topic will be discussed in the later part of this survey.

The most popular SNN topologies being used in research are Feed-Forward, Spiking Convolutional, All-To-All, Winner-Takes-All, Liquid State Machines and Randomly-Connected and they excel at different tasks. There is also the case of neural networks that realize specific parts of the human brain - those usually have a fixed structure related to the part of the brain they are representing and will be referred to as Biology-Related. They are also usually purpose-built, i.e., they should not be used to realize other tasks than simulation of the part of the brain they are representing. The Table \ref{tab:topologies} summarizes those topology types and provides abbreviations that will be used for the remainder of this survey as well as the most common use cases. 

\begin{table}
  \caption{SNN topology glossary}
  \label{tab:topologies}
  \begin{tabular}{ccl}
    \toprule
    Name&Abbr.&Use cases\\
    \midrule
    Feed-Forward & FF & Image processing, classification tasks\\
    Spiking Convolutional & SCNN & Image processing, feature extraction\\
    All-To-All & A2A & Scalability experiments, size benchmarks\\
    Winner-Takes-All & WTA & Classification tasks\\
    Liquid State Machines & LSM & Classification tasks, memory, pattern recognition\\
    Randomly-Connected & RC & Scalability experiments, connectivity benchmarks\\
    Biology-Related & BR & Biology-related experiments and simulations\\
  \bottomrule
\end{tabular}
\end{table}

\begin{wrapfigure}{H}{.5\textwidth}
  \centering
    \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[width=0.5\linewidth]{Figs/lsm.png}
    \caption{LSM}
    \end{subfigure}%
    \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[width=0.65\linewidth]{Figs/wta.png}
    \caption{WTA}
    \end{subfigure}
    \medskip
    \begin{subfigure}{.9\linewidth}
    \centering
    \includegraphics[width=0.6\linewidth]{Figs/ff.png}
    \caption{FF-FC}
    \end{subfigure}
    \medskip
    \begin{subfigure}{.9\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Figs/scnn.png}
    \caption{SCNN}
    \end{subfigure}
    \caption{Examples of the most common SNN topologies.}
    \label{fig:topologies}
  \Description{Diagrams representing examples of the most common SNN topologies.}
\end{wrapfigure}

Apart from the topologies, an important topic is how an SNN is "programmed" to solve a specific task. This "programming" may be understood on various levels: from choosing a specific topology, through selecting appropriate neuron models and ending with adjusting the weights and connections. The last step is often referred to as "learning" and there are a few possible methods of realizing it that influence the architecture choices. The simplest way of teaching the SNN to realize a specific task is to generate its classic ANN counterpart and perform learning there, using Stochastic Gradient Descent (SGD), backpropagation or other learning methods. This is due to the fact that equivalent of the activation function for SNN is non-differentiable, which means that the well-optimized methods used throughout the field of machine learning cannot be directly applied to them. After performing the learning procedure on the classic ANN, the weights are directly converted to the SNNs connections between neurons. However, in recent years many different methods that operate on SNNs directly have been proposed, which generally can be classified as backpropagation-related (SpikeProp\cite{bohte_spikeprop_2000}, BPTT\cite{werbos_generalization_1988}), direct-feedback-related (e-prop\cite{bellec_solution_2020}, DECOLLE\cite{kaiser_synaptic_2020}) and biology-related (STDP-related)\footnote{The types of learning methods were provided in a simplified manner, as they are not the main topic of this paper.}. 

% In Table \ref{tab:learning_methods} a glossary of the learning method types (groups) is provided with their origin and whether they can be applied to online or offline learning.
% It occupies space and does not add much
% \begin{table}
%   \caption{SNN learning method groups glossary}
%   \label{tab:learning_methods}
%   \begin{tabular}{cccc}
%     \toprule
%     Name&Abbr.&Origin&Online/offline\\
%     \midrule
%     ANN-to-SNN conversion & ANN-SNN & machine learning & offline\\
%     Backpropagation-related & BP & machine learning & online\\
%     Direct-feedback-related & DF & machine learning & online\\
%     Plasticity & STDP & biology & online\\
%   \bottomrule
% \end{tabular}
% \end{table}

In general terms, the BP methods in SNNs are equivalent in nature to the BP methods in classic ANNs, where based on the loss function (difference between the intended outcome and network's output) the error is being propagated back through the network and weights are being adjusted in that manner. The DF methods apply the error directly to the units in the network, without passing them through the layers. The most popular biology-related learning method used in designing NMAs is the plasticity mechanism that adjusts the weights of synaptic connections between the neurons. The Synaptic Time-Dependent Plasticity (STDP)\cite{bi_synaptic_2001}, which relies on changing the efficacy of the synapses based on the existence of lack of causality between the presynaptic and postsynaptic spikes, is the most popular form of plasticity utilized in designing NMAs. There are multiple implementations of STDP-based learning, among which there are also supervised variants, like ReSuMe\cite{ponulak_supervised_2010}.