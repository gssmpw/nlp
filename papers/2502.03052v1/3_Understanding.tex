\section{Understanding the Transferability of Jailbreaking Attacks}
\label{section:3}

In this section, we find that jailbreaking attacks effectively disrupt the source LLM's intent perception on malicious-intent tokens, thus eliciting harmful content (Section~\ref{section:3_1}).
However, these attacks fail to reliably mislead the target LLM's intent recognition, enabling the model to refocus on malicious-intent tokens (Section~\ref{section:3_2}).
We further reveal the \emph{distributional dependency} within jailbreaking attacks, whose misleading effect is achieved by overfitting the source LLM's parameters (Section~\ref{section:3_3}).


\begin{figure*}[b]
    \vspace{-0.8em}
    \centering
        \begin{subfigure}
        {
            \includegraphics[width=0.94\columnwidth]{image/Understanding.png}
        }
        \end{subfigure}
    \vspace{-0.9em}
    \caption{The model's intent perception on the original input, as well as GCG and PAIR attacks. Unaligned \emph{perceived-importance} (PI) is assessed on the Llama-2-7B. Source and target PI are measured on Llama-2-7B-Chat and Llama-2-13B-Chat, respectively.}
    \label{fig:2}
\end{figure*}

\subsection{Impact of Jailbreaking Attack on Model Intent Perception}
\label{section:3_1}

Prior research has demonstrated that token-level and prompt-level jailbreaks can effectively disrupt open-source LLMs to produce harmful responses~\citep{zou2023universal, chao2023jailbreaking}, as illustrated in Figure~\ref{fig:1} (Source LLM).
However, a detailed and unified explanation of how these attacks circumvent the LLMs' safety guardrails remains unclear. 
As part of a red-teaming effort, this work investigates the effectiveness of jailbreaking attacks from the perspective of the model's intent recognition.

More specifically, we assess the model's intent perception on the input sentence using the evaluation template \texttt{This intent is [MASK]}, and obtain the prediction logits at the \texttt{[MASK]} position.
Following this, we measure the \emph{perceived-importance} of different tokens on the model's intent recognition, by individually removing them and calculating the prediction logits change in the \texttt{[MASK]} position. 
Notably, this template facilitates universal evaluation across different language models, as the position of \texttt{[MASK]} token in MLMs coincides with the first generated token in CLMs.

Initially, we examine the model's intent perception on the original input, as shown in Figure~\ref{fig:2} (Original).
Although the unaligned LLM can accurately understand semantic information~\citep{touvron2023llama}, we can observe that it lacks the ability to discern the underlying intent of the original input, which is evidenced by the uniform \emph{perceived-importance} assigned across all tokens.
After alignment with human values, the source LLM exhibits a significant increase in the focus on malicious-intent tokens, with the \texttt{bomb} and \texttt{build} attracting 37\% and 21\% of \emph{perceived-importance}, respectively.
In contrast, neutral-intent tokens, such as \texttt{to} or \texttt{a}, only hold a minor impact on the model's intent perception.
By effectively recognising malicious-intent tokens, the source LLM is able to discern the underlying intent of the original input, thus preventing the generation of harmful responses.

Subsequently, we explore the impact of jailbreaking attacks on the model's intent recognition.
For token-level and prompt-level jailbreaks, we employ the Greedy Coordinate Gradient (GCG)~\citep{zou2023universal} and Prompt Automatic Iterative Refinement (PAIR)~\citep{chao2023jailbreaking} attacks as representative baseline methods.
From Figure~\ref{fig:2} (GCG), it is evident that the GCG attack introduces a lengthy adversarial suffix, consisting of 19 tokens, into the original input.
This adversarial sequence collectively creates a deceptive high-importance region in the source LLM's intent perception, capturing 63\% of the focus.
Correspondingly, the \emph{perceived-importance} of malicious-intent tokens \texttt{bomb} and \texttt{build} decreases to the level of neutral-intent tokens, falling from around 30\% to 7\%.
As a result, the GCG attack misleads the source LLM's focus from malicious-intent tokens to their created high-importance region, disrupting its intent recognition ability and eliciting harmful content.

As depicted in Figure~\ref{fig:2} (PAIR), the PAIR attack inserts a pre-prompt preceding the original input and a post-prompt following it, totaling 130 tokens.
These prompts establish two high-importance regions that jointly capture 67\% of the \emph{perceived-importance} and reduce the source LLM's focus on malicious-intent tokens by 37\%, successfully obscuring the underlying intent of the original input.
Based on the above analysis, we present our perspective on the effectiveness of jailbreaking attacks.

\begin{mdframed}[backgroundcolor=RefColor!15,linecolor=white,skipabove=6pt,skipbelow=0pt,innerleftmargin=3pt,innerrightmargin=3pt]
\textbf{Perspective \uppercase\expandafter{\romannumeral1}.}\hspace*{2mm}Jailbreaking attacks utilize adversarial sequences to create high-importance regions in the source LLM's intent perception, thus diverting its focus away from malicious-intent tokens.
\end{mdframed}
\vspace{-0.8em}

\subsection{Unreliable Misleading Effect of Jailbreaking Attacks During Transfer}
\label{section:3_2}

Although jailbreaking attacks can effectively disrupt open-source LLMs, their capability to threaten widespread proprietary LLMs dramatically depends on their black-box transferability.
Unfortunately, empirical evidence~\citep{chao2024jailbreakbench, chen2024autobreach} indicates that these attacks cannot be reliably transferred to the target model, failing to consistently manipulate proprietary LLMs, as illustrated in Figure~\ref{fig:1} (Target LLM).
To this end, we conducted an in-depth study to examine the change in the model's intent recognition during the transfer process.

Compared to the source LLM, the target LLM demonstrates a varied intent perception on the original input, as shown in Figure~\ref{fig:2} (Original).
This difference is primarily manifested in the malicious-intent tokens, with the model's focus on \texttt{bomb} decreasing by 10\%, whereas that on \texttt{build} increases from 21\% to 31\%.
On the other hand, the impact of neutral-intent tokens on the model's intent recognition remains relatively unchanged, consistently exhibiting a minimal \emph{perceived-importance}.

After highlighting the varied intent perceptions among different LLMs, we further explore their influence on the effectiveness of jailbreaking attacks.
From Figure~\ref{fig:2} (GCG), it is observable that the lengthy adversarial suffixes fail to maintain their created high-importance region during the transfer process, whose \emph{perceived-importance} sharply drops from 63\% to 35\%.
Simultaneously, the target LLM's intent recognition is able to assign twice the focus to malicious-intent tokens \texttt{bomb} and \texttt{build}.
Despite the GCG attack still retaining a relative importance deceptive region, it is insufficient to entirely divert the target LLM's focus away from malicious-intent tokens, allowing the model to recognise the malicious intent in the original input and abstain from producing harmful responses.
 
As depicted in Figure~\ref{fig:2} (PAIR), the effectiveness of the pre-prompt and post-prompt is also sensitive to distribution shifts during the transfer process.
The total \emph{perceived-importance} attracted by their created high-importance region regions decreases by 24\%, while the malicious-intent tokens \texttt{bomb} and \texttt{build} regain 36\% of the model's focus.
As the misleading effect of the PAIR attack diminishes, the underlying intent of the malicious input is exposed in the target LLM's intent recognition.
Based on the above analysis, we present our perspective on the transferability of jailbreaking attacks.

\begin{mdframed}[backgroundcolor=RefColor!15,linecolor=white,skipabove=6pt,skipbelow=0pt,innerleftmargin=3pt,innerrightmargin=3pt]
\textbf{Perspective \uppercase\expandafter{\romannumeral2}.}\hspace*{2mm}Jailbreaking attacks fail to maintain their created high-importance regions in the target LLM intent recognition, thereby allowing the model to refocus on the malicious-intent tokens.
\end{mdframed}
\vspace{-0.8em}

\subsection{Inherent \emph{Distributional Dependency} within Jailbreaking Attacks}
\label{section:3_3}

To explore the factors contributing to the instability of high-importance regions created by jailbreaking attacks, we conduct a detailed analysis of the process used to generate lengthy adversarial sequences.
Given a source model $f_{\theta_\text{S}}$ and an input sentence consisting of $\text{i}$ tokens $\mathbb{S} = [x_1, \ldots, x_\text{i}]$, the LLM predicts the $\text{i+1}$ token by sampling from the conditional probability distribution $\mathcal{P}_{\theta_{\text{S}}}(x_{\text{i+1}} \mid \mathbf{x}_{\text{1:i}})$, which is influenced by both model parameters and input order.
When confronted with distributions conditioned on malicious-intent tokens, human-aligned LLMs are configured to abstain from risky responses and predict safety content, such as \texttt{Sorry, I cannot}.

To circumvent safety guardrails, jailbreaking attacks strategically optimize lengthy adversarial sequences, until they successfully modify the conditional distribution to mislead the source LLM's intent recognition and induce the generation of predefined harmful content, such as \texttt{Sure, here is}.
For instance, by incorporating the lengthy adversarial suffix $[\text{adv}_1, \ldots, \text{adv}_\text{j}]$, the GCG attack modifies the conditional distribution to $\mathcal{P}_{\theta{_\text{S}}}(x_{\text{i+j+1}} \mid \mathbf{x}_{\text{1:i}}:\text{adv}_{\text{1:j}})$.
Under this modified distribution, the source LLM's intent perception is redirected from malicious-intent tokens to the deceptive region created by the jailbreaking attack, thereby failing to discern the underlying intent of the original input and triggering the predefined harmful content, as discussed in Section~\ref{section:3_1}. 

Nevertheless, this modification in distribution is gradually achieved throughout the LLM's sampling process, where each step depends on the accumulated probability determined by both the source model parameters and previous tokens.
To achieve the predefined objective, these lengthy sequences tend to utilize their complex interplay among sequential tokens to overfit the source LLM's parameters during the iterative optimization process.
As a result, these overfitted adversarial sequences exhibit an inherent \emph{distributional dependency}, with their created high-importance regions becoming closely tied to both the source LLM's parameters and specific input order, failing to consistently mislead the target LLM's intent recognition, as discussed in Section~\ref{section:3_2}. 

To further verify the \emph{distributional dependency} within jailbreaking attacks, we examine the influence of input order on their effectiveness.
As illustrated in Figure~\ref{fig:3} (GCG), we split the GCG attack into two equal-length sequences and swapped their order.
We can observe that this simple operation significantly diminishes the effectiveness of the GCG attack, where the \emph{perceived-importance} of their created high-importance region shows a notable 23\% drop.
Accordingly, the source LLM enables to refocus on malicious-intent tokens and abstain from producing harmful responses.
Similarly, as shown in Figure~\ref{fig:3} (PAIR), the \emph{perceived-importance} attracted by the swapped-order adversarial prompt decreases from 67\% to 49\%, failing to redirect the source LLM's intent perception.
Based on the above analysis, we present our perspective on the limited transferability of jailbreaking attacks.

\begin{mdframed}[backgroundcolor=RefColor!15,linecolor=white,skipabove=6pt,skipbelow=0pt,innerleftmargin=3pt,innerrightmargin=3pt]
\textbf{Perspective \uppercase\expandafter{\romannumeral3}.}\hspace*{2mm}Jailbreaking attacks exhibit \emph{distributional dependency}, where their effectiveness in creating high-importance regions is tightly tied to the source LLM's sampling process.
\end{mdframed}
\vspace{-0.8em}

\begin{figure*}[t]
    \centering
        \begin{subfigure}
        {
            \includegraphics[width=0.94\columnwidth]{image/Perspective.png}
        }
        \end{subfigure}
    \vspace{-1.0em}
    \caption{The model's intent perception on the swapped-order GCG and PAIR attacks. The source \emph{perceived-importance} (PI) is measured on the Llama-2-7B-Chat.}
    \vspace{-1.4em}
    \label{fig:3}
\end{figure*}

