\section{Conclusion}
In this study, we investigate the effectiveness and transferability of jailbreaking attacks from the perspective of large language models' (LLMs) intent perception.
Our findings reveal that jailbreaking attacks can divert the source LLM's focus away from malicious-intent tokens, effectively obstructing the model's ability to discern the underlying intent of the malicious input and inducing the generation of harmful content.
However, these attacks fail to consistently mislead the target LLM's intent recognition, allowing the model to refocus on the malicious-intent tokens and abstain from responding. 
Our analysis further attributes this unreliable transferability to the \emph{distributional dependency} within jailbreaking attacks, whose misleading effectiveness is achieved by overfitting the source LLM's sampling process, resulting in unsatisfactory performance on target LLMs.
To this end, we introduce the Perceived-importance Flatten, an effective and efficient method that uniformly disperses the model's focus from malicious-intent tokens to multiple neutral-intent tokens, obscuring LLM's intent perception without \emph{distributional dependency}.
Extensive experiments demonstrate that our method offers a cutting-edge red-teaming effort for identifying vulnerabilities in proprietary LLMs.

\textbf{Limitations.}\hspace*{2mm}A comprehensive theoretical analysis of the transfer mechanisms underlying jailbreaking attacks remains an open question for future research.
In addition, we observe that although jailbreaking attacks can manipulate human-aligned LLMs to produce harmful content, these models still tend to generate safety notices at the end of responses.
Moreover, the impact of label noise data \citep{yuan2023late, yuan2024early} in the training corpus on LLM vulnerabilities warrants further investigation.

\textbf{Future Work.}\hspace*{2mm}As generative models across various modalities continue to advance, a comprehensive red-teaming evaluation of their potential risks is becoming increasingly essential and urgent.
We plan to adopt our proposed intent perception perspective to identify vulnerabilities in diffusion models~\citep{luo2024deem, wan2024ted}, vision-language models~\citep{zhou2024few, luo2024mmevol, tu2024ranked}, large vision models~\citep{bai2024sequential, wang2024lavin}, and other multimodality models~\citep{huang2023machine, xia2024achieving, zhang2024hierarchical}.