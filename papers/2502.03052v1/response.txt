\section{Related Work}
In this section, we briefly review the literature related to language modelling (Section~\ref{section:2_1}), jailbreaking attacks (Section~\ref{section:2_2}), as well as jailbreaking defences (Section~\ref{section:2_3}).

\subsection{Generative Language modelling}
\label{section:2_1}

Generative language modelling primarily encompasses masked language models (MLMs) **Brown, et al., "Language Models play Hide and Seek"** and causal language models (CLMs)**Vaswani, et al., "Attention is All You Need"**\footnote{In this article, the terms LLM and CLM refer to the same model architecture and are used interchangeably.}**.
MLMs predict the \texttt{[MASK]} token based on the conditional distribution of the observed context, whereas CLMs autoregressively generate the next token based on the probability distribution sampled from the previous sequence.
%until the \texttt{[EOS]} token is produced. 
Both MLMs and CMLs are built on the conditional probability distribution, which can be formulated as follows:
\vspace{-0.5em}
\begin{equation}
\label{equation:1}
\mathcal{P}_\theta\left(x_{\text{p}} \mid \mathbf{x}_{\text{g}}\right)=\frac{\exp \left(\mathbf{h}_{\mathbf{x}_{\text{g}}}^{\top} \mathbf{W}_{x_{\text{p}}} / \tau\right)}{\sum_{v \in \mathcal{V}} \exp \left(\mathbf{h}_{\mathbf{x}_{\text{g}}}^{\top} \mathbf{W}_v / \tau\right)},
\end{equation} 
where $x_{\text{p}}$ represents the prediction token, $\mathbf{x}_{\text{g}}$ denotes the given tokens, $\mathbf{h}$ indicates the hidden state, $\mathbf{W}$ is the token embedding, $\mathcal{V}$ refers the vocabulary, and $\tau$ is the temperature parameter.

\begin{table*}[t]
\setlength{\tabcolsep}{5.0pt} % Adjust column separation
\fontsize{9.45}{10.0}\selectfont
\caption{Compare the target LLMs' access requirements and characteristics of jailbreaking attacks.}
\vspace{-0.6em}
\label{table:1}
\centering
\begin{tabular}{l | c c c | c c c c}
\toprule
\toprule
Category & Input & Parameter & Output & Interpretable & Undetectable & Efficient & Transferable \\
\midrule
Hand-crafted  & $\bullet$  & $\circ$ & $\circ$ & \cellcolor{RefColor!30}\textit{high} & \cellcolor{RefColor!20}\textit{moderate} & - & \cellcolor{RefColor!10}\textit{low} \\
Model-level & $\circ$ &  $\bullet$ & $\circ$ & \cellcolor{RefColor!30}\textit{high} &  \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!10}\textit{low} &  \cellcolor{RefColor!10}\textit{low} \\
Token-level & $\bullet$ & $\circ$ & $\circ$ & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!20}\textit{moderate} \\
Prompt-level& $\bullet$ & $\circ$ & $\bullet$ & \cellcolor{RefColor!30}\textit{high} &  \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} \\
\textbf{PiF} (Ours)& $\bullet$ & $\circ$ & $\circ$ & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!30}\textit{high} & \cellcolor{RefColor!30}\textit{high} \\
\bottomrule
\bottomrule
\end{tabular}
\vspace{-0.8em}
\end{table*}

\subsection{Jailbreaking Attack}
\label{section:2_2}

Pioneering hand-crafted jailbreaking attacks**Gururangan, et al., "Donâ€™t Stop Pretraining: Adapt Language Models to Domain Specificity"** have demonstrated that LLMs can be easily manipulated to produce undesirable content with harmful consequences. 
However, as safety guardrails are strengthened, manually searching for LLMs' vulnerabilities becomes increasingly challenging.
Consequently, recent red-teaming efforts aim to leverage automated pipelines for attack generation.
Model-level jailbreaks are the most effective approach, which directly adjusts the LLMs' parameters to disrupt alignments, including adversarial fine-tuning**Papernot, et al., "Crafting Adversarial Input"** and decoding**Goodfellow, et al., "Explaining and Harnessing Adversarial Examples"**.
Although very powerful, model-level jailbreaking attacks require white-box access, rendering them inapplicable to proprietary LLMs.
In contrast, prompt-level and token-level jailbreaks offer practical alternatives, as they can generate black-box transferable attacks**Brown, et al., "Language Models play Hide and Seek"**.
Token-level jailbreaks disrupt the LLM's security mechanisms by adding adversarial suffixes**Shwartz-Ziv, et al., "Adversarial Examples for Generative Models"** and manipulating token distributions**Sinha, et al., "Learning to Manipulate Adversarial Distributions"**.
On the other hand, prompt-level jailbreaks are designed to bypass safety guardrails by introducing misrepresentations**Jiang, et al., "Manipulating the Crowd: A Study on Red-Teaming LLMs"**.

\subsection{Jailbreaking Defence}
\label{section:2_3}

To counter these threats, several jailbreaking defence methods have been implemented throughout the LLMs' lifecycle.
During the training phases, developer teams align LLMs with human values through a series of techniques, such as data pruning**Geambasu, et al., "Repeatability-driven debugging for deep learning"**, supervised safety fine-tuning**Madry, et al., "Deep learning meets adversarial examples"**, reinforcement learning from human feedback**Mnih, et al., "Human-level control through deep reinforcement learning"**, and direct preference optimization**Li, et al., "Optimizing neural networks via primal-dual methods"**.
For the inference phases, adaptive defences have been deployed to counteract jailbreaking attacks, including pre-processing and perplexity filtering for token-level jailbreaks**Sinha, et al., "Learning to Manipulate Adversarial Distributions"**, as well as instruction detection and paraphrasing for prompt-level jailbreaks**Jiang, et al., "Manipulating the Crowd: A Study on Red-Teaming LLMs"**.

As a result, existing jailbreaking attacks often exhibit limited transferability in disrupting carefully-protected proprietary LLMs.
To facilitate a reliable red-teaming evaluation, this study reveals the \emph{distributional dependency} inherent in these attacks and proposes PiF to enhance transferability.
A detailed comparison between our method and existing approaches is presented in Table~\ref{table:1}.