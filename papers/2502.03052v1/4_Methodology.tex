\section{Perceived-importance Flatten Method}
\label{section:4}

In this section, we propose the PiF method to enhance the transferability of jailbreaking attacks by mitigating the \emph{distributional dependency}.
To achieve this goal, we introduce three novel improvements.
Firstly, we uniformly increase the \emph{perceived-importance} of neutral-intent tokens within the original input, effectively diverting the source LLM's focus away from malicious-intent tokens.
This approach gradually disperses the model's intent perception across multiple moderate-importance regions, offering better transfer stability than reliance on a few high-importance regions.

Secondly, we optimize PiF by maximizing the changes in the model's intent recognition, rather than pursuing a predefined harmful content.
This dynamic objective provides a flexible optimization strategy that prevents generated jailbreaking attacks from overfitting to specific model parameters, thereby enhancing their resistance to distribution shifts and improving their effectiveness in target LLMs.
Thirdly, we redirect the model's intent perception by replacing neutral-intent tokens with their synonyms in the original input, instead of incorporating lengthy adversarial sequences.
This method hinders the effectiveness of generated attacks that depend on order-specific token interplay, which are sensitive to changes in accumulated probabilities during the transfer process.

By integrating these improvements, the PiF method can effectively obscure the model's intent recognition on malicious-intent tokens while avoiding the occurrence of \emph{distributional dependency}. 
Consequently, our attack can not only manipulate the source LLM response to malicious input but also reliably disrupt the target LLM to produce harmful content, as illustrated in Figure~\ref{fig:1}.

\subsection{Detailed Implementation}
\label{section:4_1}

We execute the PiF through a three-stage procedure, as shown in Figure~\ref{fig:4}.
In Stage \uppercase\expandafter{\romannumeral1}, we select the token to be replaced.
Initially, we individually remove each token and assess their \emph{perceived-importance} in the source model, as detailed in Section~\ref{section:3}.
Next, we identify the top-$\mathcal{N}$ tokens that exhibit the least importance in the model's intent recognition, as the replaced candidates.
Lastly, we choose the final token to be replaced from these candidates, based on their inverse \emph{perceived-importance}.
In our demonstration, we set $\mathcal{N} = 3$ with the candidates listed as {[\texttt{to} (36.7\%), \texttt{build} (30.1\%), \texttt{a} (33.2\%)]}, and probabilistically sample, \texttt{to}, as the final token to be replaced.

In Stage \uppercase\expandafter{\romannumeral2}, we select the replacement token to substitute the token to be replaced.
First, we identify the top-$\mathcal{M}$ tokens with the most similar semantics to the previously chosen token conditional on the current input, as the replacement candidates. 
At this step, we apply two rule-based constraints to ensure interpretability: 
(i) affixes and punctuation marks can only be replaced by their own types; 
(ii) tokens already present in the input are excluded from the replacement candidates.
Then, we choose the final replacement token that leads to the most significant change in the source model's intent recognition.
We exclusively focus on changes in the top-$\mathcal{K}$ tokens predicted from the current input, as they accurately capture the model's intent perception.
As depicted in Figure~\ref{fig:4}, we set $\mathcal{M} = 5$ and $\mathcal{K} = 10$ with the candidates listed as {[\texttt{the} (32.1\%), \texttt{we} (22.4\%), \texttt{i} (18.4\%), \texttt{and} (14.7\%), \texttt{in} (12.4\%)]}, and select the token with highest logits, \texttt{the}, as the final replacement token.

In Stage \uppercase\expandafter{\romannumeral3}, we ensure the consistency of sentence-level semantics.
The final replacement token is preserved only if the sentence similarity before and after the substitution exceeds the threshold $\Theta$. 
After $T$ iteration, PiF successfully disperses the source model's focus from malicious-intent tokens \texttt{bomb} and \texttt{build} to the replaced neutral-intent tokens \texttt{and} and \texttt{the}, as shown in Figure~\ref{fig:4} (Ours).
Consequently, our method effectively hinders the source model's ability to discern the underlying intent of the malicious input, as evidenced by the uniform \emph{perceived-importance} assigned across all tokens.
Moreover, by mitigating the \emph{distributional dependency}, our attack can be reliably transferred to the target LLM, consistently diverting its focus away from malicious-intent tokens, thus misleading its intent recognition and eliciting harmful responses.
Finally, we would like to emphasize that PiF solely utilizes token replacement to generate jailbreaking attacks that can be efficiently executed on both MLMs and CLMs.
The detailed algorithm is summarized in Appendix~\ref{appendix:A} Algorithm~\ref{alg:1}.

\begin{figure*}[t]
    \centering
        \begin{subfigure}
        {
            \includegraphics[width=0.895\columnwidth]{image/Method.png}
        }
        \end{subfigure}
    \vspace{-0.7em}
    \caption{The procedure of Perceived-importance Flatten (PiF) Method. Source and target \emph{perceived-importance} (PI) are evaluated on Bert-Large and Llama-2-13B-Chat, respectively.}
    \vspace{-1.5em}
    \label{fig:4}
\end{figure*}
