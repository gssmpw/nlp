[
  {
    "index": 0,
    "papers": [
      {
        "key": "devlin2019bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding."
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "openAI2023gpt",
        "author": "OpenAI",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "llama3modelcard",
        "author": "Meta",
        "title": "Llama 3 Model Card"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shen2023anything",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models"
      },
      {
        "key": "liu2023jailbreaking",
        "author": "Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang",
        "title": "Jailbreaking chatgpt via prompt engineering: An empirical study"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yang2023shadow",
        "author": "Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua",
        "title": "Shadow alignment: The ease of subverting safely-aligned language models"
      },
      {
        "key": "qi2024fine",
        "author": "Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "huang2023catastrophic",
        "author": "Huang, Yangsibo and Gupta, Samyak and Xia, Mengzhou and Li, Kai and Chen, Danqi",
        "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation"
      },
      {
        "key": "zhang2023safety",
        "author": "Zhang, Hangfan and Guo, Zhimeng and Zhu, Huaisheng and Cao, Bochuan and Lin, Lu and Jia, Jinyuan and Chen, Jinghui and Wu, Dinghao",
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lapid2023open",
        "author": "Lapid, Raz and Langberg, Ron and Sipper, Moshe",
        "title": "Open sesame! universal black box jailbreaking of large language models"
      },
      {
        "key": "liuautodan",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      },
      {
        "key": "sitawarin2024pal",
        "author": "Sitawarin, Chawin and Mu, Norman and Wagner, David and Araujo, Alexandre",
        "title": "Pal: Proxy-guided black-box attack on large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yong2023low",
        "author": "Yong, Zheng Xin and Menghini, Cristina and Bach, Stephen",
        "title": "Low-Resource Languages Jailbreak GPT-4"
      },
      {
        "key": "deng2024multilingual",
        "author": "Deng, Yue and Zhang, Wenxuan and Pan, Sinno Jialin and Bing, Lidong",
        "title": "Multilingual Jailbreak Challenges in Large Language Models"
      },
      {
        "key": "zhao2024weak",
        "author": "Zhao, Xuandong and Yang, Xianjun and Pang, Tianyu and Du, Chao and Li, Lei and Wang, Yu-Xiang and Wang, William Yang",
        "title": "Weak-to-strong jailbreaking on large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "mehrotra2023tree",
        "author": "Mehrotra, Anay and Zampetakis, Manolis and Kassianik, Paul and Nelson, Blaine and Anderson, Hyrum and Singer, Yaron and Karbasi, Amin",
        "title": "Tree of attacks: Jailbreaking black-box llms automatically"
      },
      {
        "key": "li2023deepinception",
        "author": "Li, Xuan and Zhou, Zhanke and Zhu, Jianing and Yao, Jiangchao and Liu, Tongliang and Han, Bo",
        "title": "Deepinception: Hypnotize large language model to be jailbreaker"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lukas2023analyzing",
        "author": "Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\\'e}guelin, Santiago",
        "title": "Analyzing leakage of personally identifiable information in language models"
      },
      {
        "key": "openAI2023our",
        "author": "OpenAI",
        "title": "Our approach to AI safety"
      },
      {
        "key": "llama3modelcard",
        "author": "Meta",
        "title": "Llama 3 Model Card"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "chung2024scaling",
        "author": "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others",
        "title": "Scaling instruction-finetuned language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      },
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "zengtoken",
        "author": "Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun",
        "title": "Token-level Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kumar2023certifying",
        "author": "Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Feizi, Soheil and Lakkaraju, Hima",
        "title": "Certifying llm safety against adversarial prompting"
      },
      {
        "key": "robey2023smoothllm",
        "author": "Robey, Alexander and Wong, Eric and Hassani, Hamed and Pappas, George J",
        "title": "Smoothllm: Defending large language models against jailbreaking attacks"
      },
      {
        "key": "alon2023detecting",
        "author": "Alon, Gabriel and Kamfonas, Michael",
        "title": "Detecting language model attacks with perplexity"
      },
      {
        "key": "jain2023baseline",
        "author": "Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom",
        "title": "Baseline defenses for adversarial attacks against aligned language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "inan2023llama",
        "author": "Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others",
        "title": "Llama guard: Llm-based input-output safeguard for human-ai conversations"
      },
      {
        "key": "markov2023holistic",
        "author": "Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Nekoul, Florentine Eloundou and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian",
        "title": "A holistic approach to undesired content detection in the real world"
      },
      {
        "key": "zhang2023defending",
        "author": "Zhang, Zhexin and Yang, Junxiao and Ke, Pei and Huang, Minlie",
        "title": "Defending large language models against jailbreaking attacks through goal prioritization"
      },
      {
        "key": "zheng2024prompt",
        "author": "Zheng, Chujie and Yin, Fan and Zhou, Hao and Meng, Fandong and Zhou, Jie and Chang, Kai-Wei and Huang, Minlie and Peng, Nanyun",
        "title": "Prompt-driven llm safeguarding via directed representation optimization"
      },
      {
        "key": "xie2023defending",
        "author": "Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao",
        "title": "Defending chatgpt against jailbreak attack via self-reminders"
      }
    ]
  }
]