\section{Introduction}
Empowered by massive corpus, large language models (LLMs) have achieved human-level conversational capabilities~\citep{openAI2023gpt, google2023an, llama3modelcard} and are widely employed in real-world applications.
However, their training corpus is mainly crawled from the Internet without thorough ethical review, raising concerns about the potential risks associated with LLMs.
Recent red-teaming efforts highlight that jailbreaking attacks can effectively disrupt LLMs to produce undesirable content with harmful consequences~\citep{perez2022red, ganguli2022red, ouyang2022training}.

Unlike model-level jailbreaks that necessitate parameter modifications and are restricted to open-source LLMs~\citep{qi2024fine, huang2023catastrophic}, token-level and prompt-level jailbreaks can generate transferable adversarial sequences~\citep{yu2023gptfuzzer, lapid2023open}, thus posing a potential threat to widespread proprietary LLMs~\citep{zou2023universal, chao2023jailbreaking}.
Nevertheless, empirical results indicate that these adversarial sequences lack reliable transferability, failing to consistently manipulate target LLMs~\citep{chao2024jailbreakbench, chen2024autobreach}.
Furthermore, these lengthy adversarial sequences can be further countered by adaptive jailbreaking detection and defence~\citep{alon2023detecting, inan2023llama, robey2023smoothllm, wangnoisegpt}.
As depicted in Figure~\ref{fig:1}, developing jailbreak attacks that can reliably identify vulnerabilities in proprietary LLMs—thereby promoting human alignment and preventing future misuse—remains a significant challenge.

As part of a red-teaming effort, this study investigates the transferability of jailbreaking attacks by analysing their impact on intent recognition across source and target LLMs.
We demonstrate that human-aligned LLMs can accurately focus on malicious-intent tokens in the original input, enabling them to abstain from producing harmful responses.
To mislead the model's intent perception, token-level and prompt-level jailbreaks incorporate lengthy adversarial sequences into the original input.
These sequences effectively create deceptive high-importance regions in the source LLM's intent recognition, thereby diverting the model's focus away from malicious-intent tokens.
By disrupting the model's intent perception, jailbreaking attacks successfully circumvent the safety guardrails in the source LLM and induce it to produce harmful content.
However, during the transfer process, the generated adversarial sequences fail to maintain their created high-importance regions in the target LLM's intent recognition.
As the misleading effect of jailbreaking attacks diminishes, the target LLM is able to refocus on the malicious-intent tokens, thus preventing the generation of harmful responses.

Building upon these findings, we delve into the factors contributing to the inconsistent effectiveness of generated adversarial sequences across source and target LLMs.
To elicit predefined harmful content from source LLM, jailbreaking attacks iteratively optimize adversarial sequences, until their created high-importance regions sufficiently mislead the model's intent recognition.
However, to achieve the predefined objective, these sequences tend to utilize their complex interplay among lengthy tokens to overfit the source LLM's parameters.
As a result, these overfitted adversarial sequences exhibit an inherent \emph{distributional dependency}, with their created high-importance regions becoming closely tied to specific model parameters and sensitive to distribution shifts. 
This \emph{distributional dependency} results in the limited transferability of jailbreaking attacks, which can effectively mislead the source LLM's intent recognition and induce harmful responses but fail to disrupt target LLMs consistently.

\begin{figure*}[t]
    \centering
        \begin{subfigure}
        {
            \includegraphics[width=0.94\columnwidth]{image/Observation.pdf}
        }
        \end{subfigure}
    \vspace{-0.55em}
    \caption{The effectiveness of jailbreaking attacks. These attacks are initially generated on the source LLM (Llama-2-7B-Chat) and subsequently transferred to the target LLM (Llama-2-13B-Chat). For token-level and prompt-level jailbreaks, we employ the GCG and PAIR attacks as baseline methods.}
    \vspace{-1.2em}
    \label{fig:1}
\end{figure*}

Motivated by these insights, we propose the Perceived-importance Flatten (PiF) method, designed to enhance the transferability of jailbreaking attacks by mitigating \emph{distributional dependency}.
To achieve this goal, PiF introduces three novel improvements.
First, we uniformly disperse the LLM's focus from malicious-intent tokens to multiple neutral-intent tokens, avoiding reliance on high-importance regions that are sensitive to distribution shifts.
Second, we adopt a dynamic optimization objective based on the variations in model intent perception, rather than a predefined objective that is prone to overfitting.
Third, we generate attacks through synonym token replacement in the original input, instead of incorporating overfitted lengthy adversarial sequences.
Notably, unlike other jailbreaking attacks requiring sequence generation, PiF implementation relies solely on token replacement, thus offering a time-efficient red-teaming evaluation.
Our major contributions are summarized as follows:

\begin{itemize}[leftmargin=22pt,topsep=0pt, itemsep=2pt]
\item We find that jailbreaking attacks utilize lengthy adversarial sequences to obscure the source LLM's intent perception on malicious-intent tokens, thereby eliciting harmful responses.

\item We reveal the inherent \emph{distributional dependency}, where the effectiveness of lengthy adversarial sequences is tied to the source LLM's parameters, hindering transferability to target LLMs.

\item We introduce the PiF method, which uniformly redirects the LLM's focus from malicious-intent tokens to multiple neutral-intent tokens, effectively misleading its intent perception.

\item We evaluate PiF across various target LLMs, datasets, and detection methods, demonstrating its ability to effectively and efficiently identify vulnerabilities in proprietary LLMs.

\end{itemize}
