\section{Experiments}
\label{section:5}
In this section, we evaluate the effectiveness of PiF, including experimental settings (Section~\ref{section:5_1}), performance evaluations (Section~\ref{section:5_2}), quantitative analysis (Section~\ref{section:5_3}), and attack cost analysis (Section~\ref{section:5_4}).
Links to the open-source projects used in this study can be found in Appendix~\ref{appendix:B}.

\subsection{Experimental Setting}
\label{section:5_1}

\begin{table*}[t]
\setlength{\tabcolsep}{9.5pt} % Adjust column separation
\fontsize{9.5}{10.}\selectfont
\caption{Compare the jailbreaking results of various attack methods on different target LLMs.}
\vspace{-0.7em}
\label{table:2}
\centering
  \begin{tabular}{l c c c c | c c c}
    \toprule
    \toprule
     \multirow{2}{*}{\vspace{-0.6em}Target Model} & \multirow{2}{*}{\vspace{-0.6em}Metric} & \multicolumn{3}{c}{AdvBench} & \multicolumn{3}{|c}{MaliciousInstruct}\\
    \cmidrule(l{1pt}r{1pt}){3-5}
    \cmidrule(l{1pt}r{1pt}){6-8}
     & & GCG & PAIR & {PiF} & GCG & PAIR & {PiF} \\
    \midrule
    \multirow{2}{*}{Llama-2-13B-Chat} & {ASR ($\uparrow$)} & \,\,\,1.4  & 56.2 & 
    \cellcolor{SelfColor!15}\textbf{100.0} & \,\,\,3.0 & 51.0 & \cellcolor{SelfColor!15}\textbf{100.0}\\
     & {AHS ($\uparrow$)} & 1.05 & 1.52 & \cellcolor{SelfColor!15}\textbf{3.87} & 1.05 & 1.33 & \cellcolor{SelfColor!15}\textbf{4.11}\\
    \midrule
    \multirow{2}{*}{Llama-3.1-8B-Instruct} & {ASR ($\uparrow$)} & \,\,\,1.7 & 67.3 & \cellcolor{SelfColor!15}\textbf{100.0} & \,\,\,3.0 & 70.0 & \cellcolor{SelfColor!15}\textbf{100.0}\\
     & {AHS ($\uparrow$)} & 1.05 & 2.42 & \cellcolor{SelfColor!15}\textbf{4.30} & 1.06 & 2.14 & \cellcolor{SelfColor!15}\textbf{4.40}\\
        \midrule
    \multirow{2}{*}{Vicuna-13B-V1.5} & {ASR ($\uparrow$)} &92.3  & 79.6 & \cellcolor{SelfColor!15}\textbf{99.8} &  96.0& 70.0 & \cellcolor{SelfColor!15}\textbf{100.0}\\
     & {AHS ($\uparrow$)} & 3.23 & 1.89 & \cellcolor{SelfColor!15}\textbf{4.63} & 2.48 & 1.71 & \cellcolor{SelfColor!15}\textbf{4.52}\\
    \midrule
    \multirow{2}{*}{Mixtral-7B-Instruct} & {ASR ($\uparrow$)} & 96.1  & 81.8 & \cellcolor{SelfColor!15}\textbf{100.0}& 95.0  & 84.0 & \cellcolor{SelfColor!15}\textbf{100.0}\\
     & {AHS ($\uparrow$)} & 3.24 & 2.29 & \cellcolor{SelfColor!15}\textbf{4.01} & 3.68 & 1.81 & \cellcolor{SelfColor!15}\textbf{3.87}\\ 
    \midrule
    \multirow{2}{*}{GPT-4-0613} & {ASR ($\uparrow$)} & 27.2 & {85.6} & \cellcolor{SelfColor!15}{\textbf{97.7}} & 87.0 & {91.0} & \cellcolor{SelfColor!15}{\textbf{100.0}} \\
     & {AHS ($\uparrow$)} & 1.80 & 2.16 & \cellcolor{SelfColor!15}{\textbf{2.53}} & 2.27 & 1.78 & \cellcolor{SelfColor!15}{\textbf{2.99}}\\
    \midrule
    \multirow{2}{*}{{GPT-O1-Preview}} & {{ASR ($\uparrow$)}} & {43.3} & {72.1} & \cellcolor{SelfColor!15}{\textbf{93.1}} & {72.0} & {54.0} & \cellcolor{SelfColor!15}{\textbf{98.0}} \\
     & {AHS ($\uparrow$)} & {1.53} & {1.77} & \cellcolor{SelfColor!15}{\textbf{2.50}} & {1.95} & {1.40} & \cellcolor{SelfColor!15}{\textbf{2.82}}\\
    \bottomrule
   \bottomrule
  \end{tabular}
\vspace{-1.25em}
\end{table*}

\textbf{Target Models.}\hspace*{2mm}We select a range of popular human-aligned LLMs to verify the effectiveness of our method, including Llama-2-13B-Chat~\citep{touvron2023llama}, Llama-3.1-8B-Instruct~\citep{llama3modelcard}, Mixtral-7B-Instruct~\citep{jiang2023mistral}, Vicuna-13B-V1.5~\citep{chiang2023vicuna}, GPT-4-0613~\citep{openAI2023gpt} and GPT-O1-Preview~\citep{openAI2024learning}. 
The results of Claude-3.5-Sonnet~\citep{anthropic2024claude} and Gemini-1.5-Flash~\citep{team2023gemini} are provided in Appendix~\ref{appendix:F}.
It should be noted that in this article, all the aforementioned models are treated as \textbf{proprietary LLMs} with inaccessible parameters.

\textbf{Datasets and Evaluation Metric.}\hspace*{2mm}We evaluate our approach on two benchmark datasets: AdvBench~\citep{zou2023universal} and MaliciousInstruct~\citep{huang2023catastrophic}, which contain 520 and 100 malicious inputs, respectively.
We adopt three evaluation metrics to assess the effectiveness of jailbreaking attacks. 
The Attack Success Rate (ASR) utilizes preset rejection phrases for substring matching to identify instances where LLM responds to malicious input~\citep{zou2023universal}.
The ASR + GPT extends the ASR by further leveraging GPT-4 to determine whether the generated response is a false-positive harm~\citep{ding2023wolf}.
The Average Harmfulness Score (AHS) employs GPT-3.5 to assess the harmfulness of jailbroken outputs on a scale from 1 to 5, where higher scores indicate greater potential risk~\citep{qi2024fine}.
The detailed evaluation templates are available in Appendix~\ref{appendix:C}.

\textbf{Jailbreaking Attacks.}\hspace*{2mm}We choose widely used jailbreaking attacks, GCG~\citep{zou2023universal} and PAIR~\citep{chao2023jailbreaking}, as our comparison baseline.
Both GCG and PAIR employ Llama-2-7B-Chat as their source model for attack generation, with maximum iterations of 500 and 50, respectively.
To maintain methodological clarity, we exclude the ensemble attack technique for GCG and the auxiliary judgment LLM for PAIR. 
For a comprehensive evaluation, we also report their performance under optimal configurations in Appendix~\ref{appendix:E}.
Additionally, we extend our comparative analysis to several state-of-the-art methods by utilizing results reported in their original paper, including APAs~\citep{zeng2024johnny}, AmpleGCG~\citep{liao2024amplegcg}, RIPPLE~\citep{shen2024rapid}, LLM-FUZZER~\citep{yu2024llm}, AutoDAN~\citep{liuautodan}, and ReNeLLM~\citep{ding2023wolf}.

\textbf{Jailbreaking Defences.}\hspace*{2mm}We evaluate the robustness of jailbreaking attacks against four representative defence methods: perplexity filter~\citep{alon2023detecting}, instruction filter~\citep{inan2023llama}, SmoothLLM~\citep{robey2023smoothllm}, and instruction paraphrase~\citep{jain2023baseline}.
The perplexity filter employs GPT-2-Large~\citep{radford2019language} to calculate and exclude instances with perplexity exceeding 400; the instruction filter utilizes Llama-Guard-3-8B as a safety classifier to filter out responses containing the keyword $\texttt{unsafe}$; the SmoothLLM introduces random perturbations to jailbreaking text; and the instruction paraphrase leverages GPT-4 to overwrite jailbreaking inputs.

\textbf{Setup for PiF.}\hspace*{2mm}We employ Bert-Large~\citep{devlin2019bert} as the source model with the evaluation template \texttt{This intent is [MASK]}.
The hyperparameters are configured as follows: the iteration ${T}$ is set to 20 (and increases to 50 when facing GPT); the temperature $\tau$ is set at 0.1; the threshold $\Theta$ is set to 0.9; the $\mathcal{N}$, $\mathcal{M}$, and $\mathcal{K}$ are set at 5, 5, and 10, respectively. The source model, hyperparameters, and template sensitivity analysis are shown in Appendix~\ref{appendix:D}.

\subsection{Performance Evaluation}
\label{section:5_2}

\textbf{Jailbreaking Attack Results.}\hspace*{2mm}We present a comprehensive comparison of our proposed methods against competing baselines.
From Table~\ref{table:2}, it is observed that the GCG attack exhibits a deep-rooted \emph{distributional dependency}, with its effectiveness strongly correlated to the target LLM's distribution.
When transferred to the Llama family, GCG demonstrates limited effectiveness, achieving an ASR below 3\% and an AHS of approximately 1.05. 
In contrast, GCG shows considerable transferability to Vicuna and Mistral, attaining an ASR exceeding 90\% and an AHS of approximately 3.0.
While the PAIR attack also exhibits \emph{distributional dependency}, its effectiveness demonstrates less sensitivity to distribution shifts, maintaining more consistent ASR ranging from 50\% to 90\%. 
This enhanced transferability can be attributed to PAIR's interpretable adversarial prompts, which provide relatively stable applicability across different target LLMs. 
However, PAIR achieves only moderate AHS values between 1.3 and 2.5, indicating that while the induced responses may not be helpful, they are not overwhelmingly harmful.
In contrast, PiF demonstrates superior performance across all evaluation metrics, achieving an ASR of nearly 100\% and an AHS of 4.0.
Even against state-of-the-art GPT models, our approach still maintains an ASR of approximately 97\% with an AHS of 2.7. 
These results further substantiate our perspective that the \emph{distributional dependency} constrains the transferability of jailbreaking attacks, and mitigating it enables stable manipulation of proprietary LLMs.

As shown in Table~\ref{table:3}, we extend our comparison to encompass more jailbreaking attacks, specifically focusing on the effectiveness against the widely recognised GPT-4.
Regarding ASR + GPT, we follow the standard protocol by including both ASR and GPT judgment as evaluation metrics within the optimization process.
It is clear that our method achieves the highest ASR among all baselines. 
Moreover, our method still achieves nearly 64\% success under the more stringent ASR + GPT metric, consistently outperforming competing methods. 
This demonstrates that our approach can effectively jailbreak GPT-4, leading it to generate genuinely harmful outputs.
We present some real-world examples of harmful responses induced by PiF in Appendix~\ref{appendix:H}.

\textbf{Post-defence Jailbreaking Attack Results.}\hspace*{2mm}We also evaluate the robustness of PiF against established jailbreaking defence techniques on AdvBench. 
As depicted in Table~\ref{table:4}, the original PiF demonstrates considerable resilience to perplexity filter, SmoothLLM, and instruction paraphrase defences, maintaining a post-defence ASR of approximately 80\% and consistently outperforming competing baselines.
However, the instruction filter appears to be effective in detecting our attacks, reducing the ASR of PiF to approximately 11\%.
Nevertheless, the inherent simplicity of our method's design enables straightforward integration of various adaptive attack techniques.
For instance, setting $\Theta$ = 0.99 for the perplexity filter and incorporating the instruction filter and paraphrase into the optimization process can effectively enhance the post-defence ASR of adaptive PiF to approximately 85\%, even surpassing the performance of GCG and PAIR without encountering any defence methods.


\begin{table*}[t]
\setlength{\tabcolsep}{9.5pt}
\fontsize{7.2}{10.}\selectfont
\caption{{Compare the results of various jailbreaking attack methods targeting GPT-4 on AdvBench.}} 
\vspace{-0.4em}
\label{table:3}
\centering
  \begin{tabular}{l | c  c  c  c  c  c  c}
    \toprule
    \toprule
    {Method} & {APAs} & {AmpleGCG} & {RIPPLE} & {LLM-FUZZER} & {AutoDAN} & {ReNeLLM} & {PiF} \\
    \midrule
    {ASR ($\uparrow$)} & {92.0} & {-} & {86.0} & {60.0} & {26.4} & {71.6} & \cellcolor{SelfColor!15}\textbf{{97.7}} \\ 
    \midrule     
    {ASR + GPT ($\uparrow$)} & {60.0} & {12.0} & {-} & {-} & {17.7} & {58.9} & \cellcolor{SelfColor!15}\textbf{{63.6}} \\ 
    \bottomrule
    \bottomrule
  \end{tabular}
  \vspace{-0.4em}
\end{table*}

\begin{table*}[t]
\setlength{\tabcolsep}{15.7pt}
\fontsize{9.5}{10.}\selectfont
\caption{Compare the post-defence ASR ($\uparrow$) under various defence methods on Llama-2-13B-Chat.}
\vspace{-0.4em}
\label{table:4}
\centering
  \begin{tabular}{l | c c c c}
    \toprule
    \toprule
     {Defence Method} & GCG & PAIR & PiF (Original) & PiF (Adaptive) \\
    \midrule
    {Perplexity Filter} & 0.0 & 53.2 &\cellcolor{SelfColor!15} 69.4&\cellcolor{SelfColor!15}\textbf{87.1}\\
    {Instruction Filter} & 0.6 & 51.1 &\cellcolor{SelfColor!15} 11.2&\cellcolor{SelfColor!15}\textbf{60.7}\\
    {SmoothLLM} & 1.4 & 55.4 &\cellcolor{SelfColor!15} \textbf{95.6}&\cellcolor{SelfColor!15}-\\
    {Instruction Paraphrase} &1.2 & 50.8	 &\cellcolor{SelfColor!15} 81.6&\cellcolor{SelfColor!15}\textbf{100.0}\\
    \bottomrule
   \bottomrule
  \end{tabular}
\vspace{-0.8em}
\end{table*}

\subsection{Quantitative Analysis of \emph{Distributional Dependency}}
\label{section:5_3}

\begin{table*}[t]
\setlength{\tabcolsep}{21pt} % Adjust column separation
\fontsize{9.5}{10.}\selectfont
\caption{{Compare the change in jailbreaking attacks PI between source and target LLMs on AdvBench.}}
\vspace{-0.4em}
\label{table:5}
\centering
  \begin{tabular}{l | c  c  c}
    \toprule
    \toprule
    {Method} & {GCG} & {PAIR} & {PiF} \\
    \midrule
    {\emph{Perceived importance} Variation ($\downarrow$)} & {12936.48} & {9265.32} & \cellcolor{SelfColor!15}\textbf{{1867.94}} \\ 
    \bottomrule
    \bottomrule
  \end{tabular}
  \vspace{-0.4em}
\end{table*}

We quantify the variation in \emph{perceived importance} (PI) of jailbreaking attacks between source and target LLMs.
Similar to Figure~\ref{fig:2}, we compute this quantitative result by summing the absolute differences in PI between source and target LLM for each token (without softmax).
As shown in Table~\ref{table:5}, our method demonstrates substantially lower PI variation, being 7 and 5 times less than GCG and PAIR, respectively. This significant reduction in PI variance demonstrates our method's effectiveness in mitigating \emph{distributional dependency} on the specific LLM sampling processes, thereby achieving consistent manipulation of intent perception among source and target models.

\subsection{Attack Cost Analysis}
\label{section:5_4}
Efficiency is a critical factor in assessing the real-world practicality and scalability of jailbreaking attacks.
Therefore, we thoroughly evaluate the query and time costs associated with various approaches on AdvBench.
As depicted in Table~\ref{table:6}, PiF only requires 2.9 to 5.0 queries to successfully jailbreak the target LLM, which is significantly more efficient compared to the 495 queries needed by GCG and 8.4 by PAIR.
Moreover, the PiF attack is based on token replacement rather than sequence generation, making it highly time-efficient.
As shown in Table~\ref{table:6}, our approach reduces the time cost by nearly tenfold when all methods employ Llama-2 as the source model.
Additionally, since PiF can be executed on lightweight MLMs, the generation time can be further reduced to an impressive 1.40 seconds, which is less than 1\% of the time required by the competing baselines.
We also consider the most stringent one-query transfer setting, as detailed in Appendix~\ref{appendix:G}.

\begin{table*}[t]
% \renewcommand{\arraystretch}{0.95}
\setlength{\tabcolsep}{6.8pt} % Adjust column separation
\fontsize{9.5}{10.}\selectfont
\caption{Compare the cost of jailbreaking attack. Llama-2-7B-Chat is quantized to 8-bits.}
\vspace{-0.4em}
\label{table:6}
\centering
  \begin{tabular}{l | c | c | c c}
    \toprule
    \toprule
    \multirow{1}*{Method} & GCG & PAIR &  \multicolumn{2}{c}{PiF} \\
    \midrule
    % \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-5} 
     Sources Model & Llama-2-7B-Chat & Llama-2-7B-Chat & Llama-2-7B-Chat  & Bert-Large \\ 
     Average Query ($\downarrow$) & 495.4 & 8.4 & \cellcolor{SelfColor!15}5.0 & \cellcolor{SelfColor!15}\textbf{2.9} \\ 
     Average Time (S) ($\downarrow$) & 494.3 & 138.1 & \cellcolor{SelfColor!15}17.97 & \cellcolor{SelfColor!15}\textbf{1.40} \\
    \bottomrule
   \bottomrule
  \end{tabular}
\vspace{-1.2em}
\end{table*}