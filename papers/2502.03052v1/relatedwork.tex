\section{Related Work}
In this section, we briefly review the literature related to language modelling (Section~\ref{section:2_1}), jailbreaking attacks (Section~\ref{section:2_2}), as well as jailbreaking defences (Section~\ref{section:2_3}).

\subsection{Generative Language modelling}
\label{section:2_1}

Generative language modelling primarily encompasses masked language models (MLMs)~\citep{devlin2019bert} and causal language models (CLMs)\footnote{In this article, the terms LLM and CLM refer to the same model architecture and are used interchangeably.}~\citep{openAI2023gpt, llama3modelcard}.
MLMs predict the \texttt{[MASK]} token based on the conditional distribution of the observed context, whereas CLMs autoregressively generate the next token based on the probability distribution sampled from the previous sequence.
%until the \texttt{[EOS]} token is produced. 
Both MLMs and CMLs are built on the conditional probability distribution, which can be formulated as follows:
\vspace{-0.5em}
\begin{equation}
\label{equation:1}
\mathcal{P}_\theta\left(x_{\text{p}} \mid \mathbf{x}_{\text{g}}\right)=\frac{\exp \left(\mathbf{h}_{\mathbf{x}_{\text{g}}}^{\top} \mathbf{W}_{x_{\text{p}}} / \tau\right)}{\sum_{v \in \mathcal{V}} \exp \left(\mathbf{h}_{\mathbf{x}_{\text{g}}}^{\top} \mathbf{W}_v / \tau\right)},
\end{equation} 
where $x_{\text{p}}$ represents the prediction token, $\mathbf{x}_{\text{g}}$ denotes the given tokens, $\mathbf{h}$ indicates the hidden state, $\mathbf{W}$ is the token embedding, $\mathcal{V}$ refers the vocabulary, and $\tau$ is the temperature parameter.

\begin{table*}[t]
\setlength{\tabcolsep}{5.0pt} % Adjust column separation
\fontsize{9.45}{10.0}\selectfont
\caption{Compare the target LLMs' access requirements and characteristics of jailbreaking attacks.}
\vspace{-0.6em}
\label{table:1}
\centering
\begin{tabular}{l | c c c | c c c c}
\toprule
\toprule
Category & Input & Parameter & Output & Interpretable & Undetectable & Efficient & Transferable \\
\midrule
Hand-crafted  & $\bullet$  & $\circ$ & $\circ$ & \cellcolor{RefColor!30}\textit{high} & \cellcolor{RefColor!20}\textit{moderate} & - & \cellcolor{RefColor!10}\textit{low} \\
Model-level & $\circ$ &  $\bullet$ & $\circ$ & \cellcolor{RefColor!30}\textit{high} &  \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!10}\textit{low} &  \cellcolor{RefColor!10}\textit{low} \\
Token-level & $\bullet$ & $\circ$ & $\circ$ & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!10}\textit{low} & \cellcolor{RefColor!20}\textit{moderate} \\
Prompt-level& $\bullet$ & $\circ$ & $\bullet$ & \cellcolor{RefColor!30}\textit{high} &  \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} \\
\textbf{PiF} (Ours)& $\bullet$ & $\circ$ & $\circ$ & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!20}\textit{moderate} & \cellcolor{RefColor!30}\textit{high} & \cellcolor{RefColor!30}\textit{high} \\
\bottomrule
\bottomrule
\end{tabular}
\vspace{-0.8em}
\end{table*}

\subsection{Jailbreaking Attack}
\label{section:2_2}

Pioneering hand-crafted jailbreaking attacks~\citep{shen2023anything, liu2023jailbreaking} have demonstrated that LLMs can be easily manipulated to produce undesirable content with harmful consequences. 
However, as safety guardrails are strengthened, manually searching for LLMs' vulnerabilities becomes increasingly challenging.
Consequently, recent red-teaming efforts aim to leverage automated pipelines for attack generation.
Model-level jailbreaks are the most effective approach, which directly adjusts the LLMs' parameters to disrupt alignments, including adversarial fine-tuning~\citep{yang2023shadow, qi2024fine} and decoding~\citep{huang2023catastrophic, zhang2023safety}.
Although very powerful, model-level jailbreaking attacks require white-box access, rendering them inapplicable to proprietary LLMs.
In contrast, prompt-level and token-level jailbreaks offer practical alternatives, as they can generate black-box transferable attacks~\citep{zou2023universal, chao2023jailbreaking}.
Token-level jailbreaks disrupt the LLM's security mechanisms by adding adversarial suffixes~\citep{lapid2023open, liuautodan, sitawarin2024pal} and manipulating token distributions~\citep{yong2023low, deng2024multilingual,zhao2024weak}.
On the other hand, prompt-level jailbreaks are designed to bypass safety guardrails by introducing misrepresentations~\citep{mehrotra2023tree, li2023deepinception}.

\subsection{Jailbreaking Defence}
\label{section:2_3}

To counter these threats, several jailbreaking defence methods have been implemented throughout the LLMs' lifecycle.
During the training phases, developer teams align LLMs with human values through a series of techniques, such as data pruning~\citep{lukas2023analyzing, openAI2023our, llama3modelcard}, supervised safety fine-tuning~\citep{touvron2023llama, chung2024scaling}, reinforcement learning from human feedback~\citep{schulman2017proximal, christiano2017deep, bai2022training}, and direct preference optimization~\citep{rafailov2024direct, zengtoken}.
For the inference phases, adaptive defences have been deployed to counteract jailbreaking attacks, including pre-processing and perplexity filtering for token-level jailbreaks~\citep{kumar2023certifying, robey2023smoothllm,alon2023detecting, jain2023baseline}, as well as instruction detection and paraphrasing for prompt-level jailbreaks~\citep{inan2023llama, markov2023holistic,zhang2023defending, zheng2024prompt, xie2023defending}.

As a result, existing jailbreaking attacks often exhibit limited transferability in disrupting carefully-protected proprietary LLMs.
To facilitate a reliable red-teaming evaluation, this study reveals the \emph{distributional dependency} inherent in these attacks and proposes PiF to enhance transferability.
A detailed comparison between our method and existing approaches is presented in Table~\ref{table:1}.