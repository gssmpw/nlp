% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{izacard2021unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, {\'E}douard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={874--880},
  year={2021}
}

@inproceedings{scheible-etal-2016-model,
    title = "Model Architectures for Quotation Detection",
    author = "Scheible, Christian  and
      Klinger, Roman  and
      Pad{\'o}, Sebastian",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1164",
    doi = "10.18653/v1/P16-1164",
    pages = "1736--1745",
}
@inproceedings{pareti-etal-2013-automatically,
    title = "Automatically Detecting and Attributing Indirect Quotations",
    author = "Pareti, Silvia  and
      O{'}Keefe, Tim  and
      Konstas, Ioannis  and
      Curran, James R.  and
      Koprinska, Irena",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1101",
    pages = "989--999",
}
@misc{nian2024wragweaklysuperviseddense,
      title={W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering}, 
      author={Jinming Nian and Zhiyuan Peng and Qifan Wang and Yi Fang},
      year={2024},
      eprint={2408.08444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.08444}, 
}
@misc{kim2024reragimprovingopendomainqa,
      title={RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation}, 
      author={Kiseung Kim and Jay-Yoon Lee},
      year={2024},
      eprint={2406.05794},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.05794}, 
}
@inproceedings{10.1145/3626772.3657834,
author = {Cuconasu, Florin and Trappolini, Giovanni and Siciliano, Federico and Filice, Simone and Campagnano, Cesare and Maarek, Yoelle and Tonellotto, Nicola and Silvestri, Fabrizio},
title = {The Power of Noise: Redefining Retrieval for RAG Systems},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657834},
doi = {10.1145/3626772.3657834},
abstract = {Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35\%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {719–729},
numpages = {11},
keywords = {information retrieval, llm, rag},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{kongyoung-etal-2022-monoqa,
    title = "mono{QA}: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering",
    author = "Kongyoung, Sarawoot  and
      Macdonald, Craig  and
      Ounis, Iadh",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.485",
    doi = "10.18653/v1/2022.emnlp-main.485",
    pages = "7207--7218",
    abstract = "To address the Conversational Question Answering (ORConvQA) task, previous work has considered an effective three-stage architecture, consisting of a retriever, a reranker, and a reader to extract the answers. In order to effectively answer the users{'} questions, a number of existing approaches have applied multi-task learning, such that the same model is shared between the reranker and the reader. Such approaches also typically tackle reranking and reading as classification tasks. On the other hand, recent text generation models, such as monoT5 and UnifiedQA, have been shown to respectively yield impressive performances in passage reranking and reading. However, no prior work has combined monoT5 and UnifiedQA to share a single text generation model that directly extracts the answers for the users instead of predicting the start/end positions in a retrieved passage. In this paper, we investigate the use of Multi-Task Learning (MTL) to improve performance on the ORConvQA task by sharing the reranker and reader{'}s learned structure in a generative model. In particular, we propose monoQA, which uses a text generation model with multi-task learning for both the reranker and reader. Our model, which is based on the T5 text generation model, is fine-tuned simultaneously for both reranking (in order to improve the precision of the top retrieved passages) and extracting the answer. Our results on the OR-QuAC and OR-CoQA datasets demonstrate the effectiveness of our proposed model, which significantly outperforms existing strong baselines with improvements ranging from +12.31{\%} to +19.51{\%} in MAP and from +5.70{\%} to +23.34{\%} in F1 on all used test sets.",
}

@inproceedings{10.1145/3269206.3271702,
author = {Nishida, Kyosuke and Saito, Itsumi and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji},
title = {Retrieve-and-Read: Multi-task Learning of Information Retrieval and Reading Comprehension},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271702},
doi = {10.1145/3269206.3271702},
abstract = {This study considers the task of machine reading at scale (MRS) wherein, given a question, a system first performs the information retrieval (IR) task of finding relevant passages in a knowledge source and then carries out the reading comprehension (RC) task of extracting an answer span from the passages. Previous MRS studies, in which the IR component was trained without considering answer spans, struggled to accurately find a small number of relevant passages from a large set of passages. In this paper, we propose a simple and effective approach that incorporates the IR and RC tasks by using supervised multi-task learning in order that the IR component can be trained by considering answer spans. Experimental results on the standard benchmark, answering SQuAD questions using the full Wikipedia as the knowledge source, showed that our model achieved state-of-the-art performance. Moreover, we thoroughly evaluated the individual contributions of our model components with our new Japanese dataset and SQuAD. The results showed significant improvements in the IR task and provided a new perspective on IR for RC: it is effective to teach which part of the passage answers the question rather than to give only a relevance score to the whole passage.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {647–656},
numpages = {10},
keywords = {reading comprehension, question answering, information retrieval},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{nandy-etal-2021-question-answering,
    title = "Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based {QA} Framework",
    author = "Nandy, Abhilash  and
      Sharma, Soumya  and
      Maddhashiya, Shubham  and
      Sachdeva, Kapil  and
      Goyal, Pawan  and
      Ganguly, NIloy",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.392",
    doi = "10.18653/v1/2021.findings-emnlp.392",
    pages = "4600--4609",
    abstract = "Answering questions asked from instructional corpora such as E-manuals, recipe books, etc., has been far less studied than open-domain factoid context-based question answering. This can be primarily attributed to the absence of standard benchmark datasets. In this paper, we meticulously create a large amount of data connected with E-manuals and develop a suitable algorithm to exploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals, and pretrain RoBERTa on this large corpus. We create various benchmark QA datasets which include question answer pairs curated by experts based upon two E-manuals, real user questions from Community Question Answering Forum pertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering Pipeline) that answers questions pertaining to electronics devices. Built upon the pretrained RoBERTa, it harbors a supervised multi-task learning framework which efficiently performs the dual tasks of identifying the section in the E-manual where the answer can be found and the exact answer span within that section. For E-Manual annotated question-answer pairs, we show an improvement of about 40{\%} in ROUGE-L F1 scores over most competitive baseline. We perform a detailed ablation study and establish the versatility of EMQAP across different circumstances. The code and datasets are shared at \url{https://github.com/abhi1nandy2/EMNLP-2021-Findings}, and the corresponding project website is \url{https://sites.google.com/view/emanualqa/home}.",
}

@inproceedings{wang-etal-2021-retrieval,
    title = "Retrieval, Re-ranking and Multi-task Learning for Knowledge-Base Question Answering",
    author = "Wang, Zhiguo  and
      Ng, Patrick  and
      Nallapati, Ramesh  and
      Xiang, Bing",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.26",
    doi = "10.18653/v1/2021.eacl-main.26",
    pages = "347--357",
    abstract = "Question answering over knowledge bases (KBQA) usually involves three sub-tasks, namely topic entity detection, entity linking and relation detection. Due to the large number of entities and relations inside knowledge bases (KB), previous work usually utilized sophisticated rules to narrow down the search space and managed only a subset of KBs in memory. In this work, we leverage a \textit{retrieve-and-rerank} framework to access KBs via traditional information retrieval (IR) method, and re-rank retrieved candidates with more powerful neural networks such as the pre-trained BERT model. Considering the fact that directly assigning a different BERT model for each sub-task may incur prohibitive costs, we propose to share a BERT encoder across all three sub-tasks and define task-specific layers on top of the shared layer. The unified model is then trained under a multi-task learning framework. Experiments show that: (1) Our IR-based retrieval method is able to collect high-quality candidates efficiently, thus enables our method adapt to large-scale KBs easily; (2) the BERT model improves the accuracy across all three sub-tasks; and (3) benefiting from multi-task learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset.",
}


@inproceedings{lee-etal-2018-ranking,
    title = "Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering",
    author = "Lee, Jinhyuk  and
      Yun, Seongjun  and
      Kim, Hyunjae  and
      Ko, Miyoung  and
      Kang, Jaewoo",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1053",
    doi = "10.18653/v1/D18-1053",
    pages = "565--569",
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@inproceedings{nogueira-etal-2020-document,
    title = "Document Ranking with a Pretrained Sequence-to-Sequence Model",
    author = "Nogueira, Rodrigo  and
      Jiang, Zhiying  and
      Pradeep, Ronak  and
      Lin, Jimmy",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.63",
    doi = "10.18653/v1/2020.findings-emnlp.63",
    pages = "708--718",
}
@inproceedings{maclaughlin-smith-2021-content,
    title = "Content-based Models of Quotation",
    author = "MacLaughlin, Ansel  and
      Smith, David",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.195",
    doi = "10.18653/v1/2021.eacl-main.195",
    pages = "2296--2314",
}
@inproceedings{liu-etal-2019-neural-based,
    title = "Neural-based {C}hinese Idiom Recommendation for Enhancing Elegance in Essay Writing",
    author = "Liu, Yuanchao  and
      Pang, Bo  and
      Liu, Bingquan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1552",
    doi = "10.18653/v1/P19-1552",
    pages = "5522--5526",
}
@inproceedings{bendersky-smith-2012-dictionary,
    title = "A Dictionary of Wisdom and Wit: Learning to Extract Quotable Phrases",
    author = "Bendersky, Michael  and
      Smith, David",
    booktitle = "Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W12-2510",
    pages = "69--77",
}

@book{grinberg2018flask,
  title={Flask web development: developing web applications with python},
  author={Grinberg, Miguel},
  year={2018},
  publisher={" O'Reilly Media, Inc."}
} 

@inproceedings{NEURIPS2020_6b493230,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper\_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{10.5555/3455716.3455856,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {attention based models, natural language processing, deep learning, multi-task learning, transfer learning}
}

  

@inproceedings{10.1145/2983323.2983788,
author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
title = {A Neural Network Approach to Quote Recommendation in Writings},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983788},
doi = {10.1145/2983323.2983788},
abstract = {Quote is a language phenomenon of transcribing the saying of someone else. Proper usage of quote can usually make the statement more elegant and convincing. However, the ability of quote usage is usually limited by the amount of quotes one remembers or knows. Quote recommendation is a task of exploiting abundant quote repositories to help people make better use of quotes while writing. The task is different from conventional recommendation tasks due to the characteristic of quote. A pilot study has explored this task by using a learning to rank framework and manually designed features. However, it is still hard to model the meaning of a quote, which is an interesting and challenging problem. In this paper, we propose a neural network approach based on LSTMs to the quote recommendation task. We directly learn the distributed meaning representations for the contexts and the quotes, and then measure the relevance based on the meaning representations. In particular, we try to represent the words in quotes with specific embeddings, according to the contexts, topics and even author preferences of the quotes. Experimental results on a large dataset show that our proposed approach achieves the state-of-the-art performance and it outperforms several strong baselines.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {65–74},
numpages = {10},
keywords = {lstm, quote recommendation, document recommendation, deep learning},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
} 
@article{tan2015learning, title={Learning to Recommend Quotes for Writing}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9530}, DOI={10.1609/aaai.v29i1.9530}, abstractNote={ &lt;p&gt; In this paper, we propose and address a novel task of recommending quotes for writing. Quote is short for quotation, which is the repetition of someone else’s statement or thoughts. It is a common case in our writing when we would like to cite someone’s statement, like a proverb or a statement by some famous people, to make our composition more elegant or convincing. However, sometimes we are so eager to make a citation of quote somewhere, but have no idea about the relevant quote to express our idea. Because knowing or remembering so many quotes is not easy, it is exciting to have a system to recommend relevant quotes for us while writing. In this paper we tackle this appealing AI task, and build up a learning framework for quote recommendation. We collect abundant quotes from the Internet, and mine real contexts containing these quotes from large amount of electronic books, to build up a dataset for experiments. We explore the particular features of this task, and propose a few useful features to model the characteristics of quotes and the relevance of quotes to contexts. We apply a supervised learning to rank model to integrate multiple features. Experiment results show that, our proposed approach is appropriate for this task and it outperforms other recommendation methods. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo}, year={2015}, month={Feb.} }

@article{bloomfield1926set,
  title={A set of postulates for the science of language},
  author={Bloomfield, Leonard},
  journal={Language},
  volume={2},
  number={3},
  pages={153--164},
  year={1926},
  publisher={JSTOR}
}

@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year={1947},
  publisher={JSTOR}
}

@article{cole2008write,
  title={How to write: News writing},
  author={Cole, Peter},
  journal={The Guardian},
  volume={9},
  year={2008}
}

@inproceedings{wang2019multi,
  title={Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering},
  author={Wang, Zhiguo and Ng, Patrick and Ma, Xiaofei and Nallapati, Ramesh and Xiang, Bing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5878--5882},
  year={2019}
}

@inproceedings{niculae2015quotus,
  title={Quotus: The structure of political media coverage as revealed by quoting patterns},
  author={Niculae, Vlad and Suen, Caroline and Zhang, Justine and Danescu-Niculescu-Mizil, Cristian and Leskovec, Jure},
  booktitle={Proceedings of the 24th International Conference on World Wide Web},
  pages={798--808},
  year={2015}
}

@inproceedings{adak2020gandhipedia,
author = {Adak, Sayantan and Vyas, Atharva and Mukherjee, Animesh and Ambavi, Heer and Kadasi, Pritam and Singh, Mayank and Patel, Shivam},
title = {Gandhipedia: A One-Stop AI-Enabled Portal for Browsing Gandhian Literature, Life-Events and His Social Network},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398631},
doi = {10.1145/3383583.3398631},
abstract = {We introduce an AI-enabled portal that presents an excellent visualization of Mahatma Gandhi's life events by constructing temporal and spatial social networks from the Gandhian literature. Applying an ensemble of methods drawn from NLTK, Polyglot and Spacy we extract the key persons and places that find mentions in Gandhi's written works. We visualize these entities and connections between them based on co-mentions within the same time frame as networks in an interactive web portal. The nodes in the network, when clicked, fire search queries about the entity and all the information about the entity presented in the corresponding book from which the network is constructed, are retrieved and presented back on the portal. Overall, this system can be used as a digital and user-friendly resource to study Gandhian literature.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {539–540},
numpages = {2},
keywords = {Mahatma Gandhi, entity recognition, text processing, temporal networks},
location = {Virtual Event, China},
series = {JCDL '20}
}


@inproceedings{lee2018ranking,
  title={Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering},
  author={Lee, Jinhyuk and Yun, Seongjun and Kim, Hyunjae and Ko, Miyoung and Kang, Jaewoo},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={565--569},
  year={2018}
}

@inproceedings{chen2017reading,
    title = "Reading {W}ikipedia to Answer Open-Domain Questions",
    author = "Chen, Danqi  and
      Fisch, Adam  and
      Weston, Jason  and
      Bordes, Antoine",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1171",
    doi = "10.18653/v1/P17-1171",
    pages = "1870--1879",
    abstract = "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
}

@inproceedings{portelli2021improving,
  title={Improving adverse drug event extraction with SpanBERT on different text typologies},
  author={Portelli, Beatrice and Passab{\`\i}, Daniele and Lenzi, Edoardo and Serra, Giuseppe and Santus, Enrico and Chersoni, Emmanuele},
  booktitle={International Workshop on Health Intelligence},
  pages={87--99},
  year={2021},
  organization={Springer}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}





@article{maclaughlin2021context, title={Context-Based Quotation Recommendation}, volume={15}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/18070}, DOI={10.1609/icwsm.v15i1.18070}, abstractNote={While composing a new document, anything from a news article to an email or essay, authors often utilize direct quotes from a variety of sources. Although an author may know what point they would like to make, selecting an appropriate quote for the specific context may be time-consuming and difficult. We therefore propose a novel context-aware quote recommendation system which utilizes the content an author has already written to generate a ranked list of quotable paragraphs and spans of tokens from a given source document. We approach quote recommendation as a variant of open-domain question answering and adapt the state-of-the-art BERT-based methods from open-QA to our task. We conduct experiments on a collection of speech transcripts and associated news articles, evaluating models’ paragraph ranking and span prediction performances. Our experiments confirm the strong performance of BERT-based methods on this task, which outperform bag-of-words and neural ranking baselines by more than 30\% relative across all ranking metrics. Qualitative analyses show the difficulty of the paragraph and span recommendation tasks and confirm the quotability of the best BERT model’s predictions, even if they are not the true selected quotes from the original news articles.}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={MacLaughlin, Ansel and Chen, Tao and Ayan, Burcu Karagol and Roth, Dan}, year={2021}, month={May}, pages={397-408} }

@inproceedings{dai2019deeper,
author = {Dai, Zhuyun and Callan, Jamie},
title = {Deeper Text Understanding for IR with Contextual Neural Language Modeling},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331303},
doi = {10.1145/3331184.3331303},
abstract = {Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {985–988},
numpages = {4},
keywords = {text understanding, neural-IR},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{yilmaz2019applying,
  title={Applying BERT to document retrieval with birch},
  author={Yilmaz, Zeynep Akkalyoncu and Wang, Shengjin and Yang, Wei and Zhang, Haotian and Lin, Jimmy},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations},
  pages={19--24},
  year={2019}
}

@article{nogueira2019multi,
  title={Multi-stage document ranking with BERT},
  author={Nogueira, Rodrigo and Yang, Wei and Cho, Kyunghyun and Lin, Jimmy},
  journal={arXiv preprint arXiv:1910.14424},
  year={2019}
}



@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{jones2000probabilistic,
  title={A probabilistic model of information retrieval: development and comparative experiments: Part 2},
  author={Jones, K Sparck and Walker, Steve and Robertson, Stephen E.},
  journal={Information processing \& management},
  volume={36},
  number={6},
  pages={809--840},
  year={2000},
  publisher={Elsevier}
}
@inproceedings{bendersky,
    title = "A Dictionary of Wisdom and Wit: Learning to Extract Quotable Phrases",
    author = "Bendersky, Michael  and
      Smith, David",
    booktitle = "Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W12-2510",
    pages = "69--77",
}


@inproceedings{ahn2016quote,
  title={Quote Recommendation for Dialogs and Writings.},
  author={Ahn, Yeonchan and Lee, Hanbit and Jeon, Heesik and Ha, Seungdo and Lee, Sang-goo},
  booktitle={CBRecSys@ RecSys},
  pages={39--42},
  year={2016}
}

@article{tan,
  title={QuoteRec: Toward quote recommendation for writing},
  author={Tan, Jiwei and Wan, Xiaojun and Liu, Hui and Xiao, Jianguo},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={36},
  number={3},
  pages={1--36},
  year={2018},
  publisher={ACM New York, NY, USA}
}


@inproceedings{MacLaughlin,
  title={Context-Based Quotation Recommendation.},
  author={MacLaughlin, Ansel and Chen, Tao and Ayan, Burcu Karagol and Roth, Dan},
  booktitle={ICWSM},
  pages={397--408},
  year={2021}
}


@INPROCEEDINGS{koto,
  author={Koto, Fajri and Sakti, Sakriani and Neubig, Graham and Toda, Tomoki and Adriani, Mirna and Nakamura, Satoshi},
  booktitle={2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)}, 
  title={Memorable spoken quote corpora of TED public speaking}, 
  year={2014},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ICSDA.2014.7051435}}



@inproceedings{wang_trans,
  title={Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations},
  author={Wang, Lingzhi and Zeng, Xingshan and Wong, Kam-Fai},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  pages={754--758},
  year={2021}
}


@inproceedings{lee,
author = {Lee, Hanbit and Ahn, Yeonchan and Lee, Haejun and Ha, Seungdo and Lee, Sang-goo},
title = {Quote Recommendation in Dialogue Using Deep Neural Network},
year = {2016},
isbn = {9781450340694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911451.2914734},
doi = {10.1145/2911451.2914734},
abstract = {Quotes, or quotations, are well known phrases or sentences that we use for various purposes such as emphasis, elaboration, and humor. In this paper, we introduce a task of recommending quotes which are suitable for given dialogue context and we present a deep learning recommender system which combines recurrent neural network and convolutional neural network in order to learn semantic representation of each utterance and construct a sequence model for the dialog thread. We collected a large set of twitter dialogues with quote occurrences in order to evaluate proposed recommender system. Experimental results show that our approach outperforms not only the other state-of-the-art algorithms in quote recommendation task, but also other neural network based methods built for similar tasks.},
booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {957–960},
numpages = {4},
keywords = {quote recommendation, deep neural network, dialogue model},
location = {Pisa, Italy},
series = {SIGIR '16}
}

@inproceedings{vosk,
  title={News article retrieval in context for event-centric narrative creation},
  author={Voskarides, Nikos and Meij, Edgar and Sauer, Sabrina and de Rijke, Maarten},
  booktitle={Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval},
  pages={103--112},
  year={2021}
}


@inproceedings{quoteR,
  title={QuoteR: A Benchmark of Quote Recommendation for Writing},
  author={Qi, Fanchao and Yang, Yanhui and Yi, Jing and Cheng, Zhili and Liu, Zhiyuan and Sun, Maosong},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={336--348},
  year={2022}
}



@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}



@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

