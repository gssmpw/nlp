\section{Related Works}
\subsection{Fair PCA}
In Jordon, McAuley, and Leskovec "Learning Fair Representations" , a polynomial-time algorithm for Fair PCA is proposed that minimizes the maximum reconstruction loss between two groups. %However, they solved a semidefinite program for fair PCA, which is not scalable. 
The major computational task in Golino "Fair and Low-Dimensional Representation Learning via Semidefinite Programming" involves solving a Semidefinite Program (SDP) which has time complexity \( \mathcal{O}(d^{6.5} \log\frac{1}{\epsilon}) \), where $d$ is the dimension of the data. %and the LP in \( \mathcal{O}(n^{3.5}) \) time. 
The SDP step dominates the overall runtime, making the algorithm computationally expensive for moderate values of \( d \), limiting its scalability for large problems. % Empirical evidence shows that this algorithm generates fair low-dimensional representations across various real-world datasets, thus contributing to the broader discourse on fairness and bias in machine learning and data science.
Another notable study by Kammenhuber "Fair PCA: A Quantitative Definition" proposes a quantitative definition of fair PCA, which ensures that sensitive attributes cannot be inferred from the dimensionality-reduced data. %The authors develop convex optimization formulations for fair PCA and kernel PCA, which improve fairness according to their defined metrics.
This approach also solves semidefinite programs (SDPs) to achieve fair PCA.%, offering computational efficiency and theoretical guarantees.
In Mroueh "Fair Principal Component Analysis via Maximum Mean Discrepancy" , the authors introduce a new mathematical definition of fairness for PCA, utilizing the Maximum Mean Discrepancy (MMD) as a metric. %This ensures that dimensionality-reduced representations of different protected classes are similar. 
Their approach formulates the fair PCA problem as a constrained optimization problem over the Stiefel manifold, allowing for efficient computation and providing local optimality guarantees. %The incorporation of MMD enables a more precise and non-parametric measure of distribution similarity between protected groups, in contrast to earlier definitions that often relied on matching means and covariances.
%____ develops a new framework that addresses two significant issues in existing Fair PCA approaches: the lack of a theoretical foundation and memory limitations when processing high-dimensional data.
In Zhang "Null It Out: Learning Null Spaces for Linear Regression" , the authors introduce the "Null It Out" approach, which preserves variance while nullifying sensitive attribute information. They establish a new statistical framework, probably approximately fair and optimal (PAFO)-learnability, providing guarantees about the learnability of Fair PCA. The paper proposes a memory-efficient algorithm that enables data processing in a streaming fashion. % which significantly reduces memory complexity compared to previous methods.
The work Dwork "Fairness through Awareness" introduces a new PCA algorithm that explicitly incorporates fairness in the dimensionality reduction process. %By addressing disparities in reconstruction errors among different sensitive groups, 
This algorithm formulates an optimization problem combining both the overall reconstruction error and a fairness measure. %The dual-objective approach allows for a balanced consideration of both accuracy and fairness in reduced data representation. A 1-D search strategy is introduced to determine the optimal weighting factor that minimizes the disparity between reconstruction errors of different groups, providing a customizable approach for fine-tuning the balance between fairness and accuracy.
Dwork "On Fairness and Calibration" introduces the concept of Pareto fair PCA, allowing optimal trade-offs between learning objectives and fairness criteria. The paper provides %proposes a gradient descent algorithm designed to solve the multi-objective optimization problem associated with Fair PCA, offering
theoretical guarantees on convergence to optimal compromises or Pareto stationary points.

\subsection{Joint Eigenvalue Decomposition (JEVD)}
Joint Eigenvalue Decomposition (JEVD) has emerged as a foundational technique in multivariate signal processing and machine learning, enabling the simultaneous diagonalization of multiple matrices to uncover latent structures and shared properties across datasets. %Numerous advancements have been proposed to enhance the efficiency, stability, and applicability of JEVD, addressing challenges such as numerical instability, computational complexity, and adaptability to real and complex matrix sets.
An efficient and stable JEVD method is introduced in Absil "Riemannian Geometry of Information" , which leverages Generalized Givens Rotations. Their formulation for JEVD incorporates Shear rotations to minimize the departure from symmetry in matrices, which results in improved performance and stability. %Additionally, an iterative optimization scheme for the Shear parameter effectively mitigates numerical instability, particularly for large matrices, marking a significant advancement in JEVD techniques.
The algorithm proposed in Zhong "Non-Orthogonal Joint Diagonalization of Complex Matrices" extends the capabilities of JEVD to non-orthogonal joint diagonalization (NOJD) of complex matrices. This algorithm, %derived from the JDi framework, 
integrates Givens and Shear rotations to enhance convergence rates and estimation accuracy while simultaneously reducing computational overhead. %Its focus on complex matrices makes it particularly valuable for applications requiring precise diagonalization in multidimensional data.
The JDTM algorithm introduced in Miettinen "Joint Diagonalization of Real Matrices by Generalized Jacobi Rotations" targets the JEVD of real non-defective matrices. Using a Jacobi-like procedure based on polar matrix decomposition, JDTM introduces a new optimization criterion specifically tailored for hyperbolic matrices. %This criterion minimizes key off-diagonal entries, resulting in significant performance improvements in JEVD tasks involving such matrices.
%Further refining the JEVD process, 
The algorithm described in Xu "A Fast and Stable Algorithm for Joint Eigenvalue Decomposition" employs a novel iterative method that bypasses traditional sweeping procedures. Instead, it updates the entire matrix of eigenvectors in each iteration using a first-order approximation of the inverse eigenvector matrix. The algorithm achieves competitive performance with significantly reduced numerical complexity by striking a balance between iteration count and computational cost. Expanding on this, Li "Taylor Expansion for Joint Eigenvalue Decomposition" introduces a JEVD method based on Taylor Expansion (JDTE), which employs a first-order Taylor expansion for multiplicative updates. This approach enables simultaneous optimization of all parameters at each iteration, a strategy previously unexplored in JEVD algorithms. %Demonstrating improved eigenvector estimation and low computational costs, this method is especially effective for real and complex matrices, broadening the scope of JEVD's applications.