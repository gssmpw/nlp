\section{Related work}
\label{sec:relatedwork}

To our knowledge, we are the first to study discretion in AI alignment empirically. Next, we review threads of related work that inform our analysis.


\noindent \textbf{Alignment from human feedback} aims to ensure that model outputs are in accordance with user expectations using \emph{human feedback} \cite{christiano2017deep}. The popular approach is \emph{Reinforcement Learning from Human Feedback} (RLHF), which is discussed in Sec. \ref{sec:rlhf}. Researchers have developed many algorithms to perform RLHF like \cite{ziegler2019fine,ouyang2022training}. At the same time, multiple vulnerabilities were found in this process, leading to a diverse set of jailbreak attacks \cite{perez2022red}. Our work differs from previous contributions by identifying a \emph{fundamental limitation} that is independent of the algorithm used to perform RLHF - the excessive discretionary power inherent in annotation processes, which persists even in recent variants like DPO \cite{rafailov2024direct} and KTO \cite{ethayarajh2024kto}.

\noindent \textbf{Alignment from AI feedback}
 aims to ensure that model outputs are in accordance with user expectations \emph{without} using human feedback. The main example of such alignment from AI feedback is Constitutional AI \cite{bai2022constitutional}, which defines an explicit list of principles to align to. Language models are then used to generate and annotate examples to follow these principles  \cite{bai2022constitutional}. However, as noted in Anthropic's public discussion of Claude's constitution \cite{anthropic2024claude}, principles are applied stochastically during training: \emph{``The model pulls one of these principles each time it critiques and revises its responses during the supervised learning phase, and when it is evaluating which output is superior in the reinforcement learning phase. It does not look at every principle every time, but it sees each principle many times during training.''} This stochastic approach leaves open questions about principle prioritization and conflict resolution.
\textit{Collective} Constitutional AI developed a framework to learn principles from users instead of arbitrarily defining them \cite{huang2024collective}.
Recent work has expanded these foundations through various frameworks like \cite{mu2024rule, dong2023steerlmattributeconditionedsft}. In general, these approaches never require direct human supervision, removing the power of discretion from users and deferring it to the models.
Our work learns how principles are encoded and prioritized in human preference data, analyzing the human discretion contained in these datasets, and the discretion of models trained in these datasets.


\noindent \textbf{Principles and human preferences.} Recent papers have tried to \textit{learn} a set of principles encoded in preference datasets. This is an instance of the problem of bridging principles to practice \cite{davis2023affordances}. The Value Imprint \cite{obiValueImprintTechnique2024} established a framework for auditing human values embedded within preference datasets by developing a taxonomy of human values. Moreover, \cite{klingefjord2024humanvaluesalignai} propose an approach inspired by moral philosophy to determine and reconcile relevant values from diverse human inputs.  Drawing inspiration from constitutional design, \cite{findeis2024inverse} proposes a framework to distill individual and group-level preferences into a set of principles to guide model behavior. Our work builds upon these by analyzing how (i) humans prioritize different principles by analyzing discretion and (ii) how these principles are learned by models by analyzing algorithmic discretion.
Ultimately, we argue that there is currently an excessive amount of discretion in the hands of model developers and annotators.


\noindent \textbf{Pluralistic alignment} expanded alignment approaches by embracing diverse human values and perspectives \cite{sorensen2024roadmap}. Key developments include the Value Kaleidoscope taxonomy of values and rights \cite{sorensenValueKaleidoscopeEngaging2024}, the PRISM dataset for multicultural feedback \cite{kirkprism}, frameworks for leveraging community-specific LMs \cite{feng2024modular}, and approaches that consider the temporal aspects of pluralistic alignment with multiple stakeholders \cite{klassen2024pluralistic}. While these works focus on gathering diverse perspectives and defining principles, our research specifically analyzes how to weigh and resolve conflicts between different principles. This goes in line with the recent push for a social-choice approach to alignment to aggregate and reconcile preferences of diverse annotators and principles \cite{conitzer2024position}.
Our work differs from this literature by offering transparency over \textit{what} we are aligning to, complementary to the question of \textit{who} we are aligning to, and ultimately indicating whether aligning to a list of rules produces AI-adherence to a system of values, as a rule-based system would. We hope that future work analyzes how different communities exercise their power of discretion.

\noindent \textbf{Annotator disagreement} is a well-studied problem in natural language tasks \cite{sandri-etal-2023-dont, wang2024aligninglanguagemodelshuman, Cabitza_2023} as it impacts all stages of the usual ML pipeline \cite{plank-2022-problem}. \cite{zhang2024divergingpreferencesannotatorsdisagree} shows that how these disagreements are often rooted in personal biases rather than annotation errors. With existing alignment methods typically depending on a single ground-truth label, we risk privileging certain views at the expense of others, thereby ushering in a \emph{tyranny of the majority} \cite{feffer2023moralmachinetyrannymajority}.
For this reason, scholars proposed approaches to better aggregate conflicting annotations beyond majority voting by using Bayesian approaches \cite{paun-etal-2018-comparing} and proposing model architectures that handle multiple annotations \cite{davani-etal-2022-dealing}.
The challenge of annotation disagreement becomes particularly relevant with the increasing use of LLM evaluators, leading recent work to focus on measuring and reducing biases in their evaluations \cite{liu2024aligning, wu2023stylesubstanceevaluationbiases} and improving their reliability and interpretability \cite{li2024decompose}. Discretion in AI alignment reveals the principles influencing annotator decisions and how annotators prioritize conflicting principles, explaining \emph{why} annotators disagree as a function of their principles. 



\noindent \textbf{AI Alignment and Law.} Recent legal literature has argued that AI alignment operates in a similar fashion to the legal system \cite{caputo2024alignment,abiri2024public,nay2024law}. The authors emphasize the role of interpretation and application of normative principles to guide AI behavior. For instance, \cite{caputo2024alignment} argues that AI alignment faces similar challenges in accommodating diverse human values (pluralism) and defining precise rules for AI behavior (specification). Moreover, \cite{abiri2024public} points to the issue of transparency as a core element of legal decision-making that lends legitimacy to the legal system.
These works suggest that a legally-inspired approach to AI alignment could be valuable. Moreover, they also highlight transparency in legal decision-making as a key factor of the exercise of discretion.
Our work builds upon these findings by (i) formally defining discretion in AI alignment, (ii) connecting it to legal systems, and (iii) empirically studying the extent of discretion in algorithmic and human annotators.