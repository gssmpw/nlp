@article{Cabitza_2023,
   title={Toward a Perspectivist Turn in Ground Truthing for Predictive Computing},
   volume={37},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v37i6.25840},
   DOI={10.1609/aaai.v37i6.25840},
   number={6},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Cabitza, Federico and Campagner, Andrea and Basile, Valerio},
   year={2023},
   month=jun, pages={6860–6868} }

@article{abiri2024public,
  author = {Abiri, Gilad},
  title = {Public Constitutional AI},
  journal = {Forthcoming in Georgia Law Review, Volume 59},
  year = {2024}
}

@misc{anthropic2024claude,
  author       = {Anthropic},
  title        = {Claude's Constitution},
  year         = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claudes-constitution}},
  note         = {Accessed: 2025-01-03}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{caputo2024alignment,
  title={Alignment as Jurisprudence},
  author={Caputo, Nicholas},
  journal={Yale Journal of Law and Technology (forthcoming)},
  url={https://ssrn.com/abstract=4800894},
  year={2024}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{conitzer2024position,
  title={Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback},
  author={Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{davani-etal-2022-dealing,
    title = "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
    author = "Mostafazadeh Davani, Aida  and
      D{\'i}az, Mark  and
      Prabhakaran, Vinodkumar",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.6/",
    doi = "10.1162/tacl_a_00449",
    pages = "92--110",
    abstract = "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators' judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important."
}

@inproceedings{davis2023affordances,
  title={‘Affordances’ for Machine Learning},
  author={Davis, Jenny L},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={324--332},
  year={2023}
}

@article{dong2023steerlmattributeconditionedsft,
  title={Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf},
  author={Dong, Yi and Wang, Zhilin and Sreedhar, Makesh Narsimhan and Wu, Xianchao and Kuchaiev, Oleksii},
  journal={arXiv preprint arXiv:2310.05344},
  year={2023}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@inproceedings{feffer2023moralmachinetyrannymajority,
  title={Moral machine or tyranny of the majority?},
  author={Feffer, Michael and Heidari, Hoda and Lipton, Zachary C},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={5974--5982},
  year={2023}
}

@inproceedings{feng2024modular,
  title={Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration},
  author={Feng, Shangbin and Sorensen, Taylor and Liu, Yuhan and Fisher, Jillian and Park, Chan Young and Choi, Yejin and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={4151--4171},
  year={2024}
}

@article{findeis2024inverse,
  title={Inverse Constitutional AI: Compressing Preferences into Principles},
  author={Findeis, Arduin and Kaufmann, Timo and H{\"u}llermeier, Eyke and Albanie, Samuel and Mullins, Robert},
  journal={arXiv preprint arXiv:2406.06560},
  year={2024}
}

@inproceedings{huang2024collective,
  title={Collective Constitutional AI: Aligning a Language Model with Public Input},
  author={Huang, Saffron and Siddarth, Divya and Lovitt, Liane and Liao, Thomas I and Durmus, Esin and Tamkin, Alex and Ganguli, Deep},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1395--1417},
  year={2024}
}

@inproceedings{kirkprism,
  title={The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models},
  author={Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew Michael and Margatina, Katerina and Mosquera, Rafael and Ciro, Juan Manuel and Bartolo, Max and Williams, Adina and He, He and others},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@inproceedings{klassen2024pluralistic,
  title={Pluralistic Alignment Over Time},
  author={Klassen, Toryn Q and Alamdari, Parand A and McIlraith, Sheila A},
  booktitle={Pluralistic Alignment Workshop at NeurIPS 2024},
  year={2024}
}

@article{klingefjord2024humanvaluesalignai,
  title={What are human values, and how do we align AI to them?},
  author={Klingefjord, Oliver and Lowe, Ryan and Edelman, Joe},
  journal={arXiv preprint arXiv:2404.10636},
  year={2024}
}

@article{li2024decompose,
  title={Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework},
  author={Li, Minzhi and Liu, Zhengyuan and Deng, Shumin and Joty, Shafiq and Chen, Nancy F and Kan, Min-Yen},
  journal={arXiv preprint arXiv:2405.15329},
  year={2024}
}

@article{liu2024aligning,
  title={Aligning with human judgement: The role of pairwise preference in large language model evaluators},
  author={Liu, Yinhong and Zhou, Han and Guo, Zhijiang and Shareghi, Ehsan and Vulic, Ivan and Korhonen, Anna and Collier, Nigel},
  journal={arXiv preprint arXiv:2403.16950},
  year={2024}
}

@article{nay2024law,
  author = {John J. Nay},
  title = {Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans},
  journal = {SSRN Working Paper},
  year = {2024}
}

@article{obiValueImprintTechnique2024,
  title={Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets},
  author={Obi, Ike and Pant, Rohan and Agrawal, Srishti Shekhar and Ghazanfar, Maham and Basiletti, Aaron},
  journal={arXiv preprint arXiv:2411.11937},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{paun-etal-2018-comparing,
    title = "Comparing {B}ayesian Models of Annotation",
    author = "Paun, Silviu  and
      Carpenter, Bob  and
      Chamberlain, Jon  and
      Hovy, Dirk  and
      Kruschwitz, Udo  and
      Poesio, Massimo",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1040/",
    doi = "10.1162/tacl_a_00040",
    pages = "571--585",
    abstract = "The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation."
}

@inproceedings{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3419--3448},
  year={2022}
}

@inproceedings{plank-2022-problem,
    title = "The {\textquotedblleft}Problem{\textquotedblright} of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",
    author = "Plank, Barbara",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.731/",
    doi = "10.18653/v1/2022.emnlp-main.731",
    pages = "10671--10682"
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{sandri-etal-2023-dont,
    title = "Why Don`t You Do It Right? Analysing Annotators' Disagreement in Subjective Tasks",
    author = "Sandri, Marta  and
      Leonardelli, Elisa  and
      Tonelli, Sara  and
      Jezek, Elisabetta",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.178/",
    doi = "10.18653/v1/2023.eacl-main.178",
    pages = "2428--2441"}

@article{sorensen2024roadmap,
  title={A roadmap to pluralistic alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2402.05070},
  year={2024}
}

@article{sorensenValueKaleidoscopeEngaging2024,
  title = {Value {{Kaleidoscope}}: {{Engaging AI}} with {{Pluralistic Human Values}}, {{Rights}}, and {{Duties}}},
  shorttitle = {Value {{Kaleidoscope}}},
  author = {Sorensen, Taylor and Jiang, Liwei and Hwang, Jena D. and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and Sap, Maarten and Tasioulas, John and Choi, Yejin},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {18},
  pages = {19937--19947},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i18.29970},
  urldate = {2024-10-04},
  abstract = {Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91\% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Value Kaleidoscope (or Kaleido), an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT- 4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\IWV7PATN\Sorensen et al. - 2024 - Value Kaleidoscope Engaging AI with Pluralistic H.pdf}
}

@article{wang2024aligninglanguagemodelshuman,
  title={Aligning language models with human preferences via a bayesian approach},
  author={Wang, Jiashuo and Wang, Haozhao and Sun, Shichao and Li, Wenjie},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wu2023stylesubstanceevaluationbiases,
  title={Style over substance: Evaluation biases for large language models},
  author={Wu, Minghao and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2307.03025},
  year={2023}
}

@article{zhang2024divergingpreferencesannotatorsdisagree,
  title={Diverging Preferences: When do Annotators Disagree and do Models Know?},
  author={Zhang, Michael JQ and Wang, Zhilin and Hwang, Jena D and Dong, Yi and Delalleau, Olivier and Choi, Yejin and Choi, Eunsol and Ren, Xiang and Pyatkin, Valentina},
  journal={arXiv preprint arXiv:2410.14632},
  year={2024}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

