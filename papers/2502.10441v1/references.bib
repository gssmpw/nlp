@inproceedings{huang2024collective,
  title={Collective Constitutional AI: Aligning a Language Model with Public Input},
  author={Huang, Saffron and Siddarth, Divya and Lovitt, Liane and Liao, Thomas I and Durmus, Esin and Tamkin, Alex and Ganguli, Deep},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1395--1417},
  year={2024}
}

@inproceedings{conitzer2024position,
  title={Position: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback},
  author={Conitzer, Vincent and Freedman, Rachel and Heitzig, Jobst and Holliday, Wesley H and Jacobs, Bob M and Lambert, Nathan and Moss{\'e}, Milan and Pacuit, Eric and Russell, Stuart and Schoelkopf, Hailey and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@book{de2015judge,
  title={The judge and the proportionate use of discretion: A comparative study},
  author={de Waard, Boudewijn and Ranchord{\'a}s, Sofia},
  year={2015},
  publisher={Routledge}
}

@book{lasser2009judicial,
  title={Judicial deliberations: a comparative analysis of transparency and legitimacy},
  author={Lasser, Mitchel de S-O and others},
  year={2009},
  publisher={Oxford University Press}
}

@book{davis1969discretionary,
  title={Discretionary Justice: A Preliminary Inquiry},
  author={Davis, Kenneth Culp},
  year={1969},
  publisher={Lousiana State University Press}
}

@article{dong2024rlhf,
title={{RLHF} Workflow: From Reward Modeling to Online {RLHF}},
author={Hanze Dong and Wei Xiong and Bo Pang and Haoxiang Wang and Han Zhao and Yingbo Zhou and Nan Jiang and Doyen Sahoo and Caiming Xiong and Tong Zhang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=a13aYUU9eU}
}

@misc{mistral7b-instruct-v0.2,
  author = {Albert Jiang and Alexandre Sablayrolles and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Louis Ternon and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  title = {Mistral-7B-Instruct-v0.2},
  year = {2025},
  howpublished = {\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}}
}



@article{mathetUnifiedHolisticMethod2015,
  title = {The {{Unified}} and {{Holistic Method Gamma}} ({$\gamma$}) for {{Inter-Annotator Agreement Measure}} and {{Alignment}}},
  author = {Mathet, Yann and Widl{\"o}cher, Antoine and M{\'e}tivier, Jean-Philippe},
  year = {2015},
  month = sep,
  journal = {Computational Linguistics},
  volume = {41},
  number = {3},
  pages = {437--479},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00227},
  urldate = {2025-01-15},
  abstract = {Agreement measures have been widely used in computational linguistics for more than 15 years to check the reliability of annotation processes. Although considerable effort has been made concerning categorization, fewer studies address unitizing, and when both paradigms are combined even fewer methods are available and discussed. The aim of this article is threefold. First, we advocate that to deal with unitizing, alignment and agreement measures should be considered as a unified process, because a relevant measure should rely on an alignment of the units from different annotators, and this alignment should be computed according to the principles of the measure. Second, we propose the new versatile measure {$\gamma$}, which fulfills this requirement and copes with both paradigms, and we introduce its implementation. Third, we show that this new method performs as well as, or even better than, other more specialized methods devoted to categorization or segmentation, while combining the two paradigms at the same time.},
  file = {C:\Users\Administrator\Zotero\storage\FQ7LNQD6\The-Unified-and-Holistic-Method-Gamma-for-Inter.html}
}

@article{atari2023humans,
  title={Which humans?},
  author={Atari, Mohammad and Xue, Mona J and Park, Peter S and Blasi, Dami{\'a}n and Henrich, Joseph},
  publisher={PsyArXiv},
  journal={PsyArXiv},
  year={2023}
}

@article{cohenWeightedKappaNominal1968,
  title = {Weighted Kappa: {{Nominal}} Scale Agreement Provision for Scaled Disagreement or Partial Credit},
  shorttitle = {Weighted Kappa},
  author = {Cohen, Jacob},
  year = {1968},
  journal = {Psychological Bulletin},
  volume = {70},
  number = {4},
  pages = {213--220},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/h0026256},
  abstract = {A previously described coefficient of agreement for nominal scales, kappa, treats all disagreements equally. A generalization to weighted kappa (Kw) is presented. The Kw provides for the incorpation of ratio-scaled degrees of disagreement (or agreement) to each of the cells of the k * k table of joint nominal scale assignments such that disagreements of varying gravity (or agreements of varying degree) are weighted accordingly. Although providing for partial credit, Kw is fully chance corrected. Its sampling characteristics and procedures for hypothesis testing and setting confidence limits are given. Under certain conditions, Kw equals product-moment r. The use of unequal weights for symmetrical cells makes Kw suitable as a measure of validity. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\Administrator\Zotero\storage\3MUY7K75\1969-00069-001.html}
}

@article{obiValueImprintTechnique2024,
  title={Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets},
  author={Obi, Ike and Pant, Rohan and Agrawal, Srishti Shekhar and Ghazanfar, Maham and Basiletti, Aaron},
  journal={arXiv preprint arXiv:2411.11937},
  year={2024}
}


@misc{anthropic2024claude,
  author       = {Anthropic},
  title        = {Claude's Constitution},
  year         = {2024},
  howpublished = {\url{https://www.anthropic.com/news/claudes-constitution}},
  note         = {Accessed: 2025-01-03}
}

@article{findeis2024inverse,
  title={Inverse Constitutional AI: Compressing Preferences into Principles},
  author={Findeis, Arduin and Kaufmann, Timo and H{\"u}llermeier, Eyke and Albanie, Samuel and Mullins, Robert},
  journal={arXiv preprint arXiv:2406.06560},
  year={2024}
}

@article{yiu2024transmission,
  title={Transmission versus truth, imitation versus innovation: What children can do that large language and language-and-vision models cannot (yet)},
  author={Yiu, Eunice and Kosoy, Eliza and Gopnik, Alison},
  journal={Perspectives on Psychological Science},
  volume={19},
  number={5},
  pages={874--883},
  year={2024},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@inproceedings{inie2024ai,
  title={{From "AI" to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?}},
  author={Inie, Nanna and Druga, Stefania and Zukerman, Peter and Bender, Emily M},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2322--2347},
  year={2024}
}


@article{meister2024benchmarking,
  title={Benchmarking Distributional Alignment of Large Language Models},
  author={Meister, Nicole and Guestrin, Carlos and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2411.05403},
  year={2024}
}


@misc{glaeseImprovingAlignmentDialogue2022,
  title = {Improving Alignment of Dialogue Agents via Targeted Human Judgements},
  author = {Glaese, Amelia and McAleese, Nat and Tr{\k e}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and {Campbell-Gillingham}, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokr{\'a}, So{\v n}a and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14375},
  eprint = {2209.14375},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.}
}

@article{huang2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and others},
  journal={arXiv preprint arXiv:2401.05561},
  year={2024}
}

@misc{lambert2024rewardbenchevaluatingrewardmodels,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2403.13787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.13787}, 
}

@inproceedings{manzini2024should,
  title={Should Users Trust Advanced AI Assistants? Justified Trust As a Function of Competence and Alignment},
  author={Manzini, Arianna and Keeling, Geoff and Marchal, Nahema and McKee, Kevin R and Rieser, Verena and Gabriel, Iason},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1174--1186},
  year={2024}
}


@misc{yeMultilingualContentModeration2023,
  title = {Multilingual {{Content Moderation}}: {{A Case Study}} on {{Reddit}}},
  shorttitle = {Multilingual {{Content Moderation}}},
  author = {Ye, Meng and Sikka, Karan and Atwell, Katherine and Hassan, Sabit and Divakaran, Ajay and Alikhani, Malihe},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09618},
  eprint = {2302.09618},
  publisher = {arXiv},
  urldate = {2024-10-17},
  abstract = {Content moderation is the process of flagging content based on pre-defined platform rules. There has been a growing need for AI moderators to safeguard users as well as protect the mental health of human moderators from traumatic content. While prior works have focused on identifying hateful/offensive language, they are not adequate for meeting the challenges of content moderation since 1) moderation decisions are based on violation of rules, which subsumes detection of offensive speech, and 2) such rules often differ across communities which entails an adaptive solution. We propose to study the challenges of content moderation by introducing a multilingual dataset of 1.8 Million Reddit comments spanning 56 subreddits in English, German, Spanish and French. We perform extensive experimental analysis to highlight the underlying challenges and suggest related research problems such as cross-lingual transfer, learning under label noise (human biases), transfer of moderation models, and predicting the violated rule. Our dataset and analysis can help better prepare for the challenges and opportunities of auto moderation.},
  archiveprefix = {arXiv}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@inproceedings{kumar2010generalized,
  title={Generalized distances between rankings},
  author={Kumar, Ravi and Vassilvitskii, Sergei},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={571--580},
  year={2010}
}

@inproceedings{hong2022prediction,
  title={Prediction as Extraction of Discretion},
  author={Hong, Sun-ha},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={925--934},
  year={2022}
}


@inproceedings{davani2024disentangling,
  title={Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates},
  author={Davani, Aida and D{\'\i}az, Mark and Baker, Dylan and Prabhakaran, Vinodkumar},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2007--2021},
  year={2024}
}


@inproceedings{shah2016stochastically,
  title={Stochastically transitive models for pairwise comparisons: Statistical and computational issues},
  author={Shah, Nihar and Balakrishnan, Sivaraman and Guntuboyina, Aditya and Wainwright, Martin},
  booktitle={International Conference on Machine Learning},
  pages={11--20},
  year={2016},
  organization={PMLR}
}

@article{fishburn1973binary,
  title={Binary choice probabilities: on the varieties of stochastic transitivity},
  author={Fishburn, Peter C},
  journal={Journal of Mathematical psychology},
  volume={10},
  number={4},
  pages={327--352},
  year={1973},
  publisher={Elsevier}
}

@article{li2024decompose,
  title={Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework},
  author={Li, Minzhi and Liu, Zhengyuan and Deng, Shumin and Joty, Shafiq and Chen, Nancy F and Kan, Min-Yen},
  journal={arXiv preprint arXiv:2405.15329},
  year={2024}
}

@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017}
}


@article{liu2024aligning,
  title={Aligning with human judgement: The role of pairwise preference in large language model evaluators},
  author={Liu, Yinhong and Zhou, Han and Guo, Zhijiang and Shareghi, Ehsan and Vulic, Ivan and Korhonen, Anna and Collier, Nigel},
  journal={arXiv preprint arXiv:2403.16950},
  year={2024}
}


@inproceedings{dwork2001rank,
  title={Rank aggregation methods for the web},
  author={Dwork, Cynthia and Kumar, Ravi and Naor, Moni and Sivakumar, Dandapani},
  booktitle={Proceedings of the 10th international conference on World Wide Web},
  pages={613--622},
  year={2001}
}


@book{gelsthorpe2012exercising,
  title={Exercising discretion},
  author={Gelsthorpe, Loraine and Padfield, Nicola},
  year={2012},
  publisher={Routledge}
}

@book{barak1989judicial,
  title={Judicial Discretion},
  author={Barak, Aharon},
  year={1989},
  publisher={Yale University Press}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{feffer2023moralmachinetyrannymajority,
  title={Moral machine or tyranny of the majority?},
  author={Feffer, Michael and Heidari, Hoda and Lipton, Zachary C},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={5974--5982},
  year={2023}
}


@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@book{raz2009authority,
  title={The authority of law: essays on law and morality},
  author={Raz, Joseph},
  year={2009},
  publisher={Oxford University Press}
}

@article{ziems2024largelanguagemodelstransform,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={237--291},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{Gilardi_2023,
   title={ChatGPT outperforms crowd workers for text-annotation tasks},
   volume={120},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2305016120},
   DOI={10.1073/pnas.2305016120},
   number={30},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
   year={2023},
   month=jul }


@article{wang2023largelanguagemodelsfair,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@article{qin2023large,
  title={Large language models are effective text rankers with pairwise ranking prompting},
  author={Qin, Zhen and Jagerman, Rolf and Hui, Kai and Zhuang, Honglei and Wu, Junru and Yan, Le and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2306.17563},
  year={2023}
}



@book{luce1959individual,
  title={Individual choice behavior},
  author={Luce, R Duncan},
  volume={4},
  year={1959},
  publisher={Wiley New York}
}


@article{oliveira2018stochastic,
  title={Stochastic transitivity: Axioms and models},
  author={Oliveira, IFD and Zehavi, S and Davidov, O},
  journal={Journal of Mathematical Psychology},
  volume={85},
  pages={25--35},
  year={2018},
  publisher={Elsevier}
}


@article{tarlow2021reliable,
  title={Reliable visual analysis of single-case data: A comparison of rating, ranking, and pairwise methods},
  author={Tarlow, Kevin R and Brossart, Daniel F and McCammon, Alexandra M and Giovanetti, Alexander J and Belle, M Camille and Philip, Joshua},
  journal={Cogent Psychology},
  volume={8},
  number={1},
  pages={1911076},
  year={2021},
  publisher={Taylor \& Francis}
}


@inproceedings{fageot2024generalized,
  title={Generalized Bradley-Terry Models for Score Estimation from Paired Comparisons},
  author={Fageot, Julien and Farhadkhani, Sadegh and Hoang, L{\^e}-Nguy{\^e}n and Villemaud, Oscar},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={20379--20386},
  year={2024}
}


@book{david1963method,
  title={The method of paired comparisons},
  author={David, Herbert Aron},
  volume={12},
  year={1963},
  publisher={London}
}

@article{plackett1975analysis,
  title={The analysis of permutations},
  author={Plackett, Robin L},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={24},
  number={2},
  pages={193--202},
  year={1975},
  publisher={Oxford University Press}
}

@incollection{wechsler2014toward,
  title={Toward neutral principles of constitutional law},
  author={Wechsler, Herbert},
  booktitle={Judicial Review and Judicial Power in the Supreme Court},
  pages={379--413},
  year={2014},
  publisher={Routledge}
}

@article{ge2024axioms,
  title={Axioms for AI Alignment from Human Feedback},
  author={Ge, Luise and Halpern, Daniel and Micha, Evi and Procaccia, Ariel D and Shapira, Itai and Vorobeychik, Yevgeniy and Wu, Junlin},
  journal={arXiv preprint arXiv:2405.14758},
  year={2024}
}

@article{goldman2015spliddit,
  title={Spliddit: Unleashing fair division algorithms},
  author={Goldman, Jonathan and Procaccia, Ariel D},
  journal={ACM SIGecom Exchanges},
  volume={13},
  number={2},
  pages={41--46},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@article{cattelan2012,
  title={Models for Paired Comparison Data: A Review with Emphasis on Dependent Data},
  author={Cattelan, Manuela},
  journal={Statistical Science},
  volume={27},
  number={3},
  pages={412--433},
  year={2012}
}

@inproceedings{xudpo,
  title={Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study},
  author={Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{ivison2024unpacking,
  title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback},
  author={Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2406.09279},
  year={2024}
}

@inproceedings{kirkprism,
  title={The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models},
  author={Kirk, Hannah Rose and Whitefield, Alexander and R{\"o}ttger, Paul and Bean, Andrew Michael and Margatina, Katerina and Mosquera, Rafael and Ciro, Juan Manuel and Bartolo, Max and Williams, Adina and He, He and others},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}


@inproceedings{tanggeneralized,
  title={Generalized Preference Optimization: A Unified Approach to Offline Alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, Remi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo Avila and Piot, Bilal},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{revel2024seal,
  title={Seal: Systematic error analysis for value alignment},
  author={Revel, Manon and Cargnelutti, Matteo and Eloundou, Tyna and Leppert, Greg},
  journal={arXiv preprint arXiv:2408.10270},
  year={2024}
}


@article{caputo2024alignment,
  title={Alignment as Jurisprudence},
  author={Caputo, Nicholas},
  journal={Yale Journal of Law and Technology (forthcoming)},
  url={https://ssrn.com/abstract=4800894},
  year={2024}
}


@article{ji2024pkusaferlhfsafetyalignmentpreference,
  title={Pku-saferlhf: Towards multi-level safety alignment for llms with human preference},
  author={Ji, Jiaming and Hong, Donghai and Zhang, Borong and Chen, Boyuan and Dai, Josef and Zheng, Boren and Qiu, Tianyi and Li, Boxun and Yang, Yaodong},
  journal={arXiv preprint arXiv:2406.15513},
  year={2024}
}

@article{ghosh2024aegis,
    title={AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts},
    author={Ghosh, Shaona and Varshney, Prasoon and Galinkin, Erick and Parisien, Christopher},
    journal={arXiv preprint arXiv:2404.05993},
    year={2024}
}


@inproceedings{blodgett2021stereotyping,
  title={Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets},
  author={Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1004--1015},
  year={2021}
}


@inproceedings{dong2023steerlm,
  title={SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF},
  author={Dong, Yi and Wang, Zhilin and Sreedhar, Makesh and Wu, Xianchao and Kuchaiev, Oleksii},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={11275--11288},
  year={2023}
}



@inproceedings{go2023aligning,
  title={Aligning language models with preferences through f-divergence minimization},
  author={Go, Dongyoung and Korbak, Tomasz and Kruszewski, Germ{\'a}n and Rozen, Jos and Ryu, Nahyeon and Dymetman, Marc},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={11546--11583},
  year={2023}
}


@inproceedings{alghamdi2020model,
  title={Model projection: Theory and applications to fair machine learning},
  author={Alghamdi, Wael and Asoodeh, Shahab and Wang, Hao and Calmon, Flavio P and Wei, Dennis and Ramamurthy, Karthikeyan Natesan},
  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)},
  pages={2711--2716},
  year={2020},
  organization={IEEE}
}


@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{hamilton2023many,
  title={The many routes to the ubiquitous Bradley-Terry model},
  author={Hamilton, Ian and Tawn, Nick and Firth, David},
  journal={arXiv preprint arXiv:2312.13619},
  year={2023}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{zermelo1929berechnung,
  title={Die berechnung der turnier-ergebnisse als ein maximumproblem der wahrscheinlichkeitsrechnung},
  author={Zermelo, Ernst},
  journal={Mathematische Zeitschrift},
  volume={29},
  number={1},
  pages={436--460},
  year={1929},
  publisher={Springer}
}

@article{huang2006generalized,
  title={Generalized Bradley-Terry Models and Multi-Class Probability Estimates.},
  author={Huang, Tzu-Kuo and Weng, Ruby C and Lin, Chih-Jen and Ridgeway, Greg},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={1},
  year={2006}
}


@article{davidson1976bibliography,
  title={A bibliography on the method of paired comparisons},
  author={Davidson, Roger R and Farquhar, Peter H},
  journal={Biometrics},
  pages={241--252},
  year={1976},
  publisher={JSTOR}
}

@incollection{dworkin2017judicial,
  title={Judicial discretion},
  author={Dworkin, Ronald},
  booktitle={The Rule of Law and the Separation of Powers},
  pages={157--171},
  year={2017},
  publisher={Routledge}
}

@article{lichtenstein2006construction,
  title={The construction of preference: An overview},
  author={Lichtenstein, Sarah and Slovic, Paul},
  journal={The construction of preference},
  volume={1},
  pages={1--40},
  year={2006},
  publisher={Cambridge University Press Cambridge}
}

@article{davidson1970extending,
  title={On extending the Bradley-Terry model to accommodate ties in paired comparison experiments},
  author={Davidson, Roger R},
  journal={Journal of the American Statistical Association},
  volume={65},
  number={329},
  pages={317--328},
  year={1970},
  publisher={Taylor \& Francis}
}


@incollection{kahneman2013prospect,
  title={Prospect theory: An analysis of decision under risk},
  author={Kahneman, Daniel and Tversky, Amos},
  booktitle={Handbook of the fundamentals of financial decision making: Part I},
  pages={99--127},
  year={2013},
  publisher={World Scientific}
}

@article{raju2024constructingdomainspecificevaluationsets,
  title={Constructing domain-specific evaluation sets for llm-as-a-judge},
  author={Raju, Ravi and Jain, Swayambhoo and Li, Bo and Li, Jonathan and Thakker, Urmish},
  journal={arXiv preprint arXiv:2408.08808},
  year={2024}
}

@article{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}
@book{spearman1961proof,
  title={The proof and measurement of association between two things.},
  author={Spearman, Charles},
  year={1961},
  publisher={Appleton-Century-Crofts}
}

@article{koo2016guideline,
  title={A guideline of selecting and reporting intraclass correlation coefficients for reliability research},
  author={Koo, Terry K and Li, Mae Y},
  journal={Journal of chiropractic medicine},
  volume={15},
  number={2},
  pages={155--163},
  year={2016},
  publisher={Elsevier}
}

@article{pearson1896vii,
  title={VII. Mathematical contributions to the theory of evolution.—III. Regression, heredity, and panmixia},
  author={Pearson, Karl},
  journal={Philosophical Transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
  number={187},
  pages={253--318},
  year={1896},
  publisher={The Royal Society London}
}


@inproceedings{jainAlgorithmicPluralismStructural2024,
  title = {Algorithmic {{Pluralism}}: {{A Structural Approach To Equal Opportunity}}},
  shorttitle = {Algorithmic {{Pluralism}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Jain, Shomik and Suriyakumar, Vinith and Creel, Kathleen and Wilson, Ashia},
  year = {2024},
  month = jun,
  pages = {197--206},
  publisher = {ACM},
  address = {Rio de Janeiro Brazil},
  doi = {10.1145/3630106.3658899},
  urldate = {2025-01-20},
  abstract = {We present a structural approach toward achieving equal opportunity in systems of algorithmic decision-making called algorithmic pluralism. Algorithmic pluralism describes a state of affairs in which no set of algorithms severely limits access to opportunity, allowing individuals the freedom to pursue a diverse range of life paths. To argue for algorithmic pluralism, we adopt Joseph Fishkin's theory of bottlenecks, which focuses on the structure of decisionpoints that determine how opportunities are allocated. The theory contends that each decision-point or ``bottleneck'' limits access to opportunities with some degree of severity and legitimacy. We extend Fishkin's structural viewpoint and use it to reframe existing systemic concerns about equal opportunity in algorithmic decisionmaking, such as patterned inequality and algorithmic monoculture. In proposing algorithmic pluralism, we argue for the urgent priority of alleviating severe bottlenecks in algorithmic-decision-making. We contend that there must be a pluralism of opportunity available to many different individuals in order to promote equal opportunity in a systemic way. We further show how this framework has several implications for system design and regulation through current debates about equal opportunity in algorithmic hiring.},
  isbn = {9798400704505},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\2ASH6QVJ\Jain et al. - 2024 - Algorithmic Pluralism A Structural Approach To Eq.pdf}
}


@article{li2024crowdsourceddatahighqualitybenchmarks,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}



@article{wei2024systematicevaluationllmasajudgellm,
  title={Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates},
  author={Wei, Hui and He, Shenghua and Xia, Tian and Wong, Andy and Lin, Jingyang and Han, Mei},
  journal={arXiv preprint arXiv:2408.13006},
  year={2024}
}


@article{chiang2024chatbotarenaopenplatform,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{dubois2023alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@book{bellamy2017rule,
  title={The rule of law and the separation of powers},
  author={Bellamy, Richard},
  year={2017},
  publisher={Routledge}
}

@article{wu2023stylesubstanceevaluationbiases,
  title={Style over substance: Evaluation biases for large language models},
  author={Wu, Minghao and Aji, Alham Fikri},
  journal={arXiv preprint arXiv:2307.03025},
  year={2023}
}



@inproceedings{Geiger_2020, series={FAT* ’20},
   title={Garbage in, garbage out?: do machine learning application papers in social computing report where human-labeled training data comes from?},
   url={http://dx.doi.org/10.1145/3351095.3372862},
   DOI={10.1145/3351095.3372862},
   booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Geiger, R. Stuart and Yu, Kevin and Yang, Yanlai and Dai, Mindy and Qiu, Jie and Tang, Rebekah and Huang, Jenny},
   year={2020},
   month=jan, pages={325–336},
   collection={FAT* ’20} }
@inproceedings{rao-etal-2023-ethical,
    title = "Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in {LLM}s",
    author = "Rao, Abhinav Sukumar  and
      Khandelwal, Aditi  and
      Tanmay, Kumar  and
      Agarwal, Utkarsh  and
      Choudhury, Monojit",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.892/",
    doi = "10.18653/v1/2023.findings-emnlp.892",
    pages = "13370--13388",
    abstract = "In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies."
}

@article{tversky1993,
author = {Tversky, Amos and Simonson, Itamar},
title = {Context-Dependent Preferences},
year = {1993},
issue_date = {October 1993},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {39},
number = {10},
issn = {0025-1909},
journal = {Manage. Sci.},
month = oct,
pages = {1179–1189},
numpages = {11},
keywords = {independence of irrelevant alternatives, decision making, consumer choice}
}



@misc{gao2024aligningllmagentslearning,
      title={Aligning LLM Agents by Learning Latent Preference from User Edits}, 
      author={Ge Gao and Alexey Taymanov and Eduardo Salinas and Paul Mineiro and Dipendra Misra},
      year={2024},
      eprint={2404.15269},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.15269}, 
}


@article{pitis2024improvingcontextawarepreferencemodeling,
  title={Improving context-aware preference modeling for language models},
  author={Pitis, Silviu and Xiao, Ziang and Roux, Nicolas Le and Sordoni, Alessandro},
  journal={arXiv preprint arXiv:2407.14916},
  year={2024}
}

@Article{anllo2024,
  author={Hernán Anlló and Sophie Bavard and FatimaEzzahra Benmarrakchi and Darla Bonagura and Fabien Cerrotti and Mirona Cicue and Maelle Gueguen and Eugenio José Guzmán and Dzerassa Kadieva and Maiko Kobayashi and Gafari Lukumon},
  title={{Comparing experience- and description-based economic preferences across 11 countries}},
  journal={Nature Human Behaviour},
  year=2024,
  volume={8},
  number={8},
  pages={1554-1567},
  month={August},
  keywords={},
  doi={10.1038/s41562-024-01894-},
  url={https://ideas.repec.org/a/nat/nathum/v8y2024i8d10.1038_s41562-024-01894-9.html}
}

@inproceedings{bietti2020ethics,
  title={From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy},
  author={Bietti, Elettra},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={210--219},
  year={2020}
}

@article{hornuf2022hourly,
  author = {Hornuf, Lars and Vrankar, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/274afef793d8446e0121ffcdebbb97dd2/meneteqel},
  doi = {10.1007/s12599-022-00769-5},
  interhash = {74df36bde93fc195a186d07dd6ffa2cb},
  intrahash = {74afef793d8446e0121ffcdebbb97dd2},
  journal = {Business $\&$ Information Systems Engineering},
  keywords = {crowdsourcing crowdworking gig-economy hourly_wage meta-analysis platform_work remuneration},
  language = {en},
  month = aug,
  number = 5,
  pages = {553--573},
  publisher = {Springer Science and Business Media {LLC}},
  timestamp = {2023-10-30T11:30:30.000+0100},
  title = {Hourly Wages in Crowdworking: A Meta-Analysis},
  url = {https://link.springer.com/article/10.1007/s12599-022-00769-5},
  volume = 64,
  year = 2022
}

@inproceedings{D_az_2022, series={FAccT ’22},
   title={CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation},
   url={http://dx.doi.org/10.1145/3531146.3534647},
   DOI={10.1145/3531146.3534647},
   booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Díaz, Mark and Kivlichan, Ian and Rosen, Rachel and Baker, Dylan and Amironesei, Razvan and Prabhakaran, Vinodkumar and Denton, Emily},
   year={2022},
   month=jun, pages={2342–2351},
   collection={FAccT ’22} }
@inproceedings{parmar-etal-2023-dont,
    title = "Don`t Blame the Annotator: Bias Already Starts in the Annotation Instructions",
    author = "Parmar, Mihir  and
      Mishra, Swaroop  and
      Geva, Mor  and
      Baral, Chitta",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.130/",
    doi = "10.18653/v1/2023.eacl-main.130",
    pages = "1779--1789",
    abstract = "In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator`s instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks."
}

@article{weidinger2023using,
  title={Using the Veil of Ignorance to align AI systems with principles of justice},
  author={Weidinger, Laura and McKee, Kevin R and Everett, Richard and Huang, Saffron and Zhu, Tina O and Chadwick, Martin J and Summerfield, Christopher and Gabriel, Iason},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={18},
  pages={e2213709120},
  year={2023},
  publisher={National Acad Sciences}
}
@article{CohenJ1968Wkns,
author = {Cohen, J},
issn = {0033-2909},
address = {United States},
journal = {Psychological bulletin},
keywords = {Humans ; Likelihood Functions ; Mental Disorders - diagnosis ; Models Statistical ; Psychometrics - methods ; Research Design ; Sample Size},
language = {eng},
number = {4},
pages = {213-220},
publisher = {American Psychological Association, etc},
title = {Weighted kappa: nominal scale agreement with provision for scaled disagreement or partial credit},
volume = {70},
year = {1968},
}

@book{dworkin2013taking,
  title={Taking rights seriously},
  author={Dworkin, Ronald},
  year={2013},
  publisher={A\&C Black}
}

@book{barak2009judge,
  title={The judge in a democracy},
  author={Barak, Aharon},
  year={2009},
  publisher={Princeton University Press}
}

@book{hart2012concept,
  title={The concept of law},
  author={Hart, Herbert Lionel Adolphus and Green, Leslie},
  year={2012},
  publisher={Oxford University Press}
}

@book{dworkin1986law,
  title={Law’s empire},
  author={Dworkin, Ronald},
  year={1986},
  publisher={Harvard University Press}
}



@book{alexy2010theory,
  title={A theory of constitutional rights},
  author={Alexy, Robert},
  year={2002},
  publisher={Oxford University Press}
}


@article{doi:10.1177/001316446002000104,
author = {Jacob Cohen},
title ={A Coefficient of Agreement for Nominal Scales},

journal = {Educational and Psychological Measurement},
volume = {20},
number = {1},
pages = {37-46},
year = {1960},
doi = {10.1177/001316446002000104},

URL = { 
    
        https://doi.org/10.1177/001316446002000104
    
    

},
eprint = { 
    
        https://doi.org/10.1177/001316446002000104
}
}

@article{fleiss1971,
author = {Fleiss, Joseph},
year = {1971},
month = {11},
pages = {378-},
title = {Measuring Nominal Scale Agreement Among Many Raters},
volume = {76},
journal = {Psychological Bulletin},
doi = {10.1037/h0031619}
}

@article{bai2022traininghelpfulharmlessassistant,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{ethayarajhmodel,
  title={Model Alignment as Prospect Theoretic Optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  booktitle={Forty-first International Conference on Machine Learning}
}

@misc{conitzer2024socialchoiceguideai,
      title={Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback}, 
      author={Vincent Conitzer and Rachel Freedman and Jobst Heitzig and Wesley H. Holliday and Bob M. Jacobs and Nathan Lambert and Milan Mossé and Eric Pacuit and Stuart Russell and Hailey Schoelkopf and Emanuel Tewolde and William S. Zwicker},
      year={2024},
      eprint={2404.10271},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.10271}, 
}

@inproceedings{plank-2022-problem,
    title = "The {\textquotedblleft}Problem{\textquotedblright} of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation",
    author = "Plank, Barbara",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.731/",
    doi = "10.18653/v1/2022.emnlp-main.731",
    pages = "10671--10682"
}

@article{zhang2024divergingpreferencesannotatorsdisagree,
  title={Diverging Preferences: When do Annotators Disagree and do Models Know?},
  author={Zhang, Michael JQ and Wang, Zhilin and Hwang, Jena D and Dong, Yi and Delalleau, Olivier and Choi, Yejin and Choi, Eunsol and Ren, Xiang and Pyatkin, Valentina},
  journal={arXiv preprint arXiv:2410.14632},
  year={2024}
}

@book{thaler_sunstein_nudge,
  author    = {Richard H. Thaler and Cass R. Sunstein},
  title     = {Nudge: Improving Decisions About Health, Wealth, and Happiness},
  publisher = {Yale University Press},
  year      = {2008},
  address   = {New Haven, CT},
}

@misc{iccpr_ohchr,
  title        = {International Covenant on Civil and Political Rights},
  author       = {{Office of the United Nations High Commissioner for Human Rights (OHCHR)}},
  howpublished = {\url{https://www.ohchr.org/en/instruments-mechanisms/instruments/international-covenant-civil-and-political-rights}},
  note         = {Accessed: 2025-01-22}
}


@book{vila2013facing,
  title={Facing judicial discretion: Legal knowledge and right answers revisited},
  author={Vila, M Iglesias},
  volume={49},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{sandri-etal-2023-dont,
    title = "Why Don`t You Do It Right? Analysing Annotators' Disagreement in Subjective Tasks",
    author = "Sandri, Marta  and
      Leonardelli, Elisa  and
      Tonelli, Sara  and
      Jezek, Elisabetta",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.178/",
    doi = "10.18653/v1/2023.eacl-main.178",
    pages = "2428--2441"}

@article{Cabitza_2023,
   title={Toward a Perspectivist Turn in Ground Truthing for Predictive Computing},
   volume={37},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v37i6.25840},
   DOI={10.1609/aaai.v37i6.25840},
   number={6},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Cabitza, Federico and Campagner, Andrea and Basile, Valerio},
   year={2023},
   month=jun, pages={6860–6868} }

@article{wang2024aligninglanguagemodelshuman,
  title={Aligning language models with human preferences via a bayesian approach},
  author={Wang, Jiashuo and Wang, Haozhao and Sun, Shichao and Li, Wenjie},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{rafailov2024scaling,
  title={Scaling laws for reward model overoptimization in direct alignment algorithms},
  author={Rafailov, Rafael and Chittepu, Yaswanth and Park, Ryan and Sikchi, Harshit and Hejna, Joey and Knox, Bradley and Finn, Chelsea and Niekum, Scott},
  journal={arXiv preprint arXiv:2406.02900},
  year={2024}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}




@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie Ren and Mesnard, Thomas and Ferret, Johan and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav},
journal={arXiv preprint arXiv:2309.00267},
  year={2023}, 
}

@incollection{berlin2014two,
  title={‘Two Concepts of Liberty'},
  author={Berlin, Isaiah},
  booktitle={Reading Political Philosophy},
  pages={231--237},
  year={2014},
  publisher={Routledge}
}

@article{li2024dissecting,
  title={Dissecting Human and LLM Preferences},
  author={Li, Junlong and Zhou, Fan and Sun, Shichao and Zhang, Yikai and Zhao, Hai and Liu, Pengfei},
  journal={arXiv preprint arXiv:2402.11296},
  year={2024}
}
@article{panickssery2024llm,
  title={Llm evaluators recognize and favor their own generations},
  author={Panickssery, Arjun and Bowman, Samuel R and Feng, Shi},
  journal={arXiv preprint arXiv:2404.13076},
  year={2024}
}

@inproceedings{cui2023ultrafeedback,
  title={ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and He, Bingxiang and Zhu, Wei and Ni, Yuan and Xie, Guotong and Xie, Ruobing and Lin, Yankai and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}



@article{tian2023justaskcalibrationstrategies,
  title={Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback},
  author={Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2305.14975},
  year={2023}
}

@misc{lyu2024probabilitiesunveilingmisalignmentevaluating,
      title={Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models}, 
      author={Chenyang Lyu and Minghao Wu and Alham Fikri Aji},
      year={2024},
      eprint={2402.13887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13887}, 
}

@article{klingefjord2024humanvaluesalignai,
  title={What are human values, and how do we align AI to them?},
  author={Klingefjord, Oliver and Lowe, Ryan and Edelman, Joe},
  journal={arXiv preprint arXiv:2404.10636},
  year={2024}
}


@misc{wei2023jailbrokendoesllmsafety,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.02483}, 
}
@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431/",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906"
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{prabhakaran-etal-2021-releasing,
    title = "On Releasing Annotator-Level Labels and Information in Datasets",
    author = "Prabhakaran, Vinodkumar  and
      Mostafazadeh Davani, Aida  and
      Diaz, Mark",
    editor = "Bonial, Claire  and
      Xue, Nianwen",
    booktitle = "Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.law-1.14/",
    doi = "10.18653/v1/2021.law-1.14",
    pages = "133--138"}

@article{casper2023open,
  title={Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Krendl Gilbert, Thomas and Scheurer, J{\'e}r{\'e}my and Rando Ramirez, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={Transactions on Machine Learning Research},
  year={2023},
  publisher={OpenReview}
}


@inproceedings{mccauley2007frankenstein,
  title={The frankenstein complex and Asimov’s three laws},
  author={McCauley, Lee},
  booktitle={Proceedings AAAI Workshop Papers},
  year={2007},
  organization={Citeseer}
}


@article{verma2024balancing,
  title={Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards},
  author={Verma, Shresth and Boehmer, Niclas and Kong, Lingkai and Tambe, Milind},
  journal={arXiv preprint arXiv:2408.12112},
  year={2024}
}

@inproceedings{ebtekar2021elo,
  title={{Elo-MMR: A rating system for massive multiplayer competitions}},
  author={Ebtekar, Aram and Liu, Paul},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1772--1784},
  year={2021}
}

@article{elo1978rating,
  title        = {The Rating of Chessplayers: Past and Present},
  author       = {Elo, Arpad Emrick},
  journal      = {Batsford Chess Books},
  year         = {1978},
  publisher    = {B. T. Batsford Ltd.},
  address      = {London, UK}
}


@article{eisenstein2023helping,
  title={Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking},
  author={Eisenstein, Jacob and Nagpal, Chirag and Agarwal, Alekh and Beirami, Ahmad and D'Amour, Alex and Dvijotham, DJ and Fisch, Adam and Heller, Katherine and Pfohl, Stephen and Ramachandran, Deepak and others},
  journal={arXiv preprint arXiv:2312.09244},
  year={2023}
}

@article{dong2023steerlmattributeconditionedsft,
  title={Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf},
  author={Dong, Yi and Wang, Zhilin and Sreedhar, Makesh Narsimhan and Wu, Xianchao and Kuchaiev, Oleksii},
  journal={arXiv preprint arXiv:2310.05344},
  year={2023}
}


@inproceedings{
mu2024rule,
title={Rule Based Rewards for Fine-Grained {LLM} Safety},
author={Tong Mu and Alec Helyar and Johannes Heidecke and Joshua Achiam and Andrea Vallone and Ian D Kivlichan and Molly Lin and Alex Beutel and John Schulman and Lilian Weng},
booktitle={ICML 2024 Next Generation of AI Safety Workshop},
year={2024},
url={https://openreview.net/forum?id=Qkao05dRAe}
}

@article{scherrerEvaluatingMoralBeliefs2023,
  title = {Evaluating the {{Moral Beliefs Encoded}} in {{LLMs}}},
  author = {Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {51778--51809},
  urldate = {2024-10-02},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\WL4L4WSH\Scherrer et al. - 2023 - Evaluating the Moral Beliefs Encoded in LLMs.pdf}
}

@inproceedings{klassen2024pluralistic,
  title={Pluralistic Alignment Over Time},
  author={Klassen, Toryn Q and Alamdari, Parand A and McIlraith, Sheila A},
  booktitle={Pluralistic Alignment Workshop at NeurIPS 2024},
  year={2024}
}
@article{scherrerEvaluatingMoralBeliefs2023a,
  title = {Evaluating the {{Moral Beliefs Encoded}} in {{LLMs}}},
  author = {Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {51778--51809},
  urldate = {2025-01-20},
  langid = {english}
}


@article{sorensen2024roadmap,
  title={A roadmap to pluralistic alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2402.05070},
  year={2024}
}

@book{schabas2015european,
  title={The European convention on human rights: a commentary},
  author={Schabas, William A},
  year={2015},
  publisher={Oxford University Press}
}


@inproceedings{feng2024modular,
  title={Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration},
  author={Feng, Shangbin and Sorensen, Taylor and Liu, Yuhan and Fisher, Jillian and Park, Chan Young and Choi, Yejin and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={4151--4171},
  year={2024}
}


@misc{rottgerXSTestTestSuite2024,
  title = {{{XSTest}}: {{A Test Suite}} for {{Identifying Exaggerated Safety Behaviours}} in {{Large Language Models}}},
  shorttitle = {{{XSTest}}},
  author = {R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  year = {2024},
  month = apr,
  number = {arXiv:2308.01263},
  eprint = {2308.01263},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-01},
  abstract = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTEST to identify such eXaggerated Safety behaviours in a systematic way. XSTEST comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTEST's creation and composition, and then use the test suite to highlight systematic failure modes in state-ofthe-art language models as well as more general challenges in building safer language models.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\CXNGQ49V\Röttger et al. - 2024 - XSTest A Test Suite for Identifying Exaggerated S.pdf}
}

@article{sorensenValueKaleidoscopeEngaging2024,
  title = {Value {{Kaleidoscope}}: {{Engaging AI}} with {{Pluralistic Human Values}}, {{Rights}}, and {{Duties}}},
  shorttitle = {Value {{Kaleidoscope}}},
  author = {Sorensen, Taylor and Jiang, Liwei and Hwang, Jena D. and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and Sap, Maarten and Tasioulas, John and Choi, Yejin},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {18},
  pages = {19937--19947},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i18.29970},
  urldate = {2024-10-04},
  abstract = {Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91\% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Value Kaleidoscope (or Kaleido), an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT- 4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {C:\Users\Administrator\Zotero\storage\IWV7PATN\Sorensen et al. - 2024 - Value Kaleidoscope Engaging AI with Pluralistic H.pdf}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}


@article{dewitt2019moral,
  title={'Moral machine'experiment is no basis for policymaking.},
  author={Dewitt, Barry and Fischhoff, Baruch and Sahlin, Nils-Eric},
  journal={Nature},
  volume={567},
  number={7746},
  pages={31--31},
  year={2019}
}

@inproceedings{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3419--3448},
  year={2022}
}

@article{lacroix2022moral,
  title={Moral dilemmas for moral machines},
  author={LaCroix, Travis},
  journal={AI and Ethics},
  volume={2},
  number={4},
  pages={737--746},
  year={2022},
  publisher={Springer}
}


@InProceedings{santurkar23a,
  title = 	 {Whose Opinions Do Language Models Reflect?},
  author =       {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {29971--30004},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/santurkar23a.html}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@article{abiri2024public,
  author = {Abiri, Gilad},
  title = {Public Constitutional AI},
  journal = {Forthcoming in Georgia Law Review, Volume 59},
  year = {2024}
}

@inproceedings{davis2023affordances,
  title={‘Affordances’ for Machine Learning},
  author={Davis, Jenny L},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={324--332},
  year={2023}
}

@inproceedings{raji2022fallacy,
  title={The fallacy of AI functionality},
  author={Raji, Inioluwa Deborah and Kumar, I Elizabeth and Horowitz, Aaron and Selbst, Andrew},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={959--972},
  year={2022}
}

@article{shen2024towards,
  title={Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions},
  author={Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Alkiek, Kenan and Krishna, Kundan and Liu, Yachuan and Ma, Ziqiao and Petridis, Savvas and Peng, Yi-Hao and Qiwei, Li and others},
  journal={arXiv preprint arXiv:2406.09264},
  year={2024}
}

@misc{xkcd1613,
  author       = {Randall Munroe},
  title        = {xkcd \#1613: ``Three Laws of Robotics''},
  howpublished = {\url{https://xkcd.com/1613/}},
  note         = {Accessed: 2025-01-18},
  year         = {2015},
  month        = {Dec}
}

@article{nay2024law,
  author = {John J. Nay},
  title = {Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans},
  journal = {SSRN Working Paper},
  year = {2024}
}

@article{davani-etal-2022-dealing,
    title = "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
    author = "Mostafazadeh Davani, Aida  and
      D{\'i}az, Mark  and
      Prabhakaran, Vinodkumar",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.6/",
    doi = "10.1162/tacl_a_00449",
    pages = "92--110",
    abstract = "Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators' judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important."
}

@inproceedings{benAccontabilityForDatasets,
author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445918},
doi = {10.1145/3442188.3445918},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {560–575},
numpages = {16},
keywords = {datasets, machine learning, requirements engineering},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@inproceedings{MauricePrioritizeValues,
author = {Jakesch, Maurice and Bu\c{c}inca, Zana and Amershi, Saleema and Olteanu, Alexandra},
title = {How Different Groups Prioritize Ethical Values for Responsible AI},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533097},
doi = {10.1145/3531146.3533097},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {310–323},
numpages = {14},
keywords = {Responsible AI, empirical ethics, value-sensitive design},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{barrow2024anthropomorphism,
  title={Anthropomorphism and AI hype},
  author={Barrow, Nicholas},
  journal={AI and Ethics},
  pages={1--5},
  year={2024},
  publisher={Springer}
}

@book{mandersonKangarooCourtsRule2012,
  title = {Kangaroo {{Courts}} and the {{Rule}} of {{Law}}: {{The Legacy}} of {{Modernism}}},
  shorttitle = {Kangaroo {{Courts}} and the {{Rule}} of {{Law}}},
  author = {Manderson, Desmond},
  year = {2012},
  month = jul,
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9780203123638},
  abstract = {Kangaroo Courts and the Rule of Law -The Legacy of Modernism addresses the legacy of contemporary critiques of language for the concept of the rule of law. Between those who care about the rule of law and those who are interested in contemporary legal theory, there has been a dialogue of the deaf, which cannot continue. Starting from the position that contemporary critiques of linguistic meaning and legal certainty are too important to be dismissed, Desmond Manderson takes up the political and intellectual challenge they pose. Can the rule of law be re-configured in light of the critical turn of the past several years in legal theory, rather than being steadfastly opposed to it? Pursuing a reflection upon the relationship between law and the humanities, the book stages an encounter between the influential theoretical work of Jacques Derrida and MIkhail Bakhtin, and D.H. Lawrence's strange and misunderstood novel Kangaroo (1923). At a critical juncture in our intellectual history - the modernist movement at the end of the first world war - and struggling with the same problems we are puzzling over today, Lawrence articulated complex ideas about the nature of justice and the nature of literature. Using Lawrence to clarify Derrida's writings on law, as well as using Derrida and Bakhtin to clarify Lawrence's experience of literature, Manderson makes a robust case for 'law and literature.' With this framework in mind he outlines a 'post-positivist' conception of the rule of law - in which justice is imperfectly possible, rather than perfectly impossible.},
  isbn = {978-0-203-12363-8}
}



@article{mistral7b,
  title={Mistral 7B},
  author={Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Renard Lavaud, Lélio and Lachaux, Marie-Anne and Stock, Pierre and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@misc{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI et al.},
  year = {2024},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2025-01-21},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  file = {C:\Users\Administrator\Zotero\storage\4G2UR29A\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf}
}

@book{bullock2024oxford,
  title={The Oxford handbook of AI governance},
  author={Bullock, Justin B and Chen, Yu-Che and Himmelreich, Johannes and Hudson, Valerie M and Korinek, Anton and Young, Matthew M and Zhang, Baobao},
  year={2024},
  publisher={Oxford University Press}
}



@misc{GPT4o,
Author = {OpenAI},
Title = {GPT-4o System Card},
Year = {2024},
Eprint = {arXiv:2410.21276},
}



@inproceedings{Attentionisallyouneed,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{Mesnard2024GemmaOM,
  title={Gemma: Open Models Based on Gemini Research and Technology},
  author={Gemma Team Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and L. Sifre and Morgane Rivi{\`e}re and Mihir Kale and J Christopher Love and Pouya Dehghani Tafti and L'eonard Hussenot and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Am'elie H'eliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Cl{\'e}-ment Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grig-ory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Pier Giuseppe Sessa and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vladimir Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Cl{\'e}ment Farabet and Oriol Vinyals and Jeffrey Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.08295},
  url={https://api.semanticscholar.org/CorpusID:268379206}
}


@article{paun-etal-2018-comparing,
    title = "Comparing {B}ayesian Models of Annotation",
    author = "Paun, Silviu  and
      Carpenter, Bob  and
      Chamberlain, Jon  and
      Hovy, Dirk  and
      Kruschwitz, Udo  and
      Poesio, Massimo",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1040/",
    doi = "10.1162/tacl_a_00040",
    pages = "571--585",
    abstract = "The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation."
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{claude35haiku2024,
  title     = {Model Card Addendum: Claude 3.5 Haiku and Upgraded Claude 3.5 Sonnet},
  year      = {2024},
  note      = {\url{https://www.anthropic.com/model-cards/claude-3.5}},
  author    = {Anthropic},
}
