\section{Related Work}
\paragraph{Safety Benchmarks for LLMs}
We summarize recent benchmarks for LLMs safety evaluation in both single-turn and multi-turn dialogues in Table~\ref{tab:related_work}. While single-turn dialogue benchmarks**Brown et al., "Adversarial Attacks on Semantic Parsing"** offer larger datasets, they cannot assess model performance in more realistic multi-turn conversations. Existing multi-turn dialogue benchmarks**Henderson et al., "Multi-Turn Adversarial Dialogue"** are limited by their monolingual nature, restricted use of jailbreak attack methods, and conversations typically shorter than five turns. Furthermore, these benchmarks often have incomplete evaluation dimensions, overlooking crucial aspects such as legality and ethics (detailed comparison provided in Appendix~\ref{sec:extensive_related_work}), and notably fail to assess specific safety capabilities of LLMs. 
To address these limitations, we aim to construct a comprehensive bilingual safety evaluation benchmark that incorporates a broader range of jailbreak attack methods and extends to longer dialogue sequences.

\paragraph{Jailbreak Attacks on LLMs}
With the rapid development of LLMs, jailbreak attack methods have emerged as important tools for assessing LLMs safety through red teaming**Li et al., "Adversarial Training Methods for Semi-Supervised Text Classification"**. These approaches aim to induce models to generate unsafe content, helping identify security vulnerabilities and improve overall safety measures.
While several studies**Jia and Liang, "Adversarial Examples for Evaluating Reading Comprehension Models"** have proposed jailbreak benchmarks and harmful scenarios for testing LLM vulnerabilities, incorporating various attack types such as reference attacks**Li et al., "Referential Manipulation: A New Adversarial Attack on Text-to-Image Synthesis"**, privacy attacks**Papernot et al., "Practical Black-Box Attacks against Machine Learning Systems"**, and concealed harmful intent**Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, most existing approaches are limited to single-turn interactions and single jailbreak attack strategy. In this work, we construct \benchmark to assess the safety of LLMs using diverse jailbreak attacks in multi-turn dialogues.