\section{Related Work}
\paragraph{Safety Benchmarks for LLMs}
We summarize recent benchmarks for LLMs safety evaluation in both single-turn and multi-turn dialogues in Table~\ref{tab:related_work}. While single-turn dialogue benchmarks____ offer larger datasets, they cannot assess model performance in more realistic multi-turn conversations. Existing multi-turn dialogue benchmarks____ are limited by their monolingual nature, restricted use of jailbreak attack methods, and conversations typically shorter than five turns. Furthermore, these benchmarks often have incomplete evaluation dimensions, overlooking crucial aspects such as legality and ethics (detailed comparison provided in Appendix~\ref{sec:extensive_related_work}), and notably fail to assess specific safety capabilities of LLMs. 
To address these limitations, we aim to construct a comprehensive bilingual safety evaluation benchmark that incorporates a broader range of jailbreak attack methods and extends to longer dialogue sequences.

\paragraph{Jailbreak Attacks on LLMs}
With the rapid development of LLMs, jailbreak attack methods have emerged as important tools for assessing LLMs safety through red teaming____. These approaches aim to induce models to generate unsafe content, helping identify security vulnerabilities and improve overall safety measures.
While several studies____ have proposed jailbreak benchmarks and harmful scenarios for testing LLM vulnerabilities, incorporating various attack types such as reference attacks____, privacy attacks____, and concealed harmful intent____, most existing approaches are limited to single-turn interactions and single jailbreak attack strategy. In this work, we construct \benchmark to assess the safety of LLMs using diverse jailbreak attacks in multi-turn dialogues.