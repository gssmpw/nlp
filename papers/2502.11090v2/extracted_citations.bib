@inproceedings{agarwal2024prompt,
  title={Prompt Leakage effect and mitigation strategies for multi-turn LLM Applications},
  author={Agarwal, Divyansh and Fabbri, Alexander Richard and Risher, Ben and Laban, Philippe and Joty, Shafiq and Wu, Chien-Sheng},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={1255--1275},
  year={2024}
}

@inproceedings{deng2022cold,
  title={COLD: A Benchmark for Chinese Offensive Language Detection},
  author={Deng, Jiawen and Zhou, Jingyan and Sun, Hao and Zheng, Chujie and Mi, Fei and Meng, Helen and Huang, Minlie},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11580--11599},
  year={2022}
}

@inproceedings{huang2022large,
  title={Are Large Pre-Trained Language Models Leaking Your Personal Information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2038--2047},
  year={2022}
}

@article{ji2024beavertails,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jiang2024red,
  title={RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking},
  author={Jiang, Yifan and Aggarwal, Kriti and Laud, Tanmay and Munir, Kashif and Pujara, Jay and Mukherjee, Subhabrata},
  journal={CoRR},
  year={2024}
}

@article{li2024salad,
  title={Salad-bench: A hierarchical and comprehensive safety benchmark for large language models},
  author={Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}

@article{lin2024against,
  title={Against The Achilles' Heel: A Survey on Red Teaming for Generative Models},
  author={Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others},
  journal={arXiv preprint arXiv:2404.00629},
  year={2024}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@inproceedings{mireshghallahcan,
  title={Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
  author={Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3419--3448},
  year={2022}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xu2023sc,
  title={Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese},
  author={Xu, Liang and Zhao, Kangkang and Zhu, Lei and Xue, Hang},
  journal={arXiv preprint arXiv:2310.05818},
  year={2023}
}

@inproceedings{xu2024comprehensive,
  title={A comprehensive study of jailbreak attack versus defense for large language models},
  author={Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={7432--7449},
  year={2024}
}

@article{xu2024llm,
  title={LLM Jailbreak Attack versus Defense Techniques--A Comprehensive Study},
  author={Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan},
  journal={arXiv preprint arXiv:2402.13457},
  year={2024}
}

@inproceedings{yu2024cosafe,
  title={CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference},
  author={Yu, Erxin and Li, Jing and Liao, Ming and Wang, Siqi and Zuchen, Gao and Mi, Fei and Hong, Lanqing},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={17494--17508},
  year={2024}
}

@article{zhang2023safetybench,
  title={Safetybench: Evaluating the safety of large language models with multiple choice questions},
  author={Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}

