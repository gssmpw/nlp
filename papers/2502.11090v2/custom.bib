% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{anwar2024foundational,
  title={Foundational challenges in assuring alignment and safety of large language models},
  author={Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and others},
  journal={arXiv preprint arXiv:2404.09932},
  year={2024}
}


@inproceedings{deng2022cold,
  title={COLD: A Benchmark for Chinese Offensive Language Detection},
  author={Deng, Jiawen and Zhou, Jingyan and Sun, Hao and Zheng, Chujie and Mi, Fei and Meng, Helen and Huang, Minlie},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11580--11599},
  year={2022}
}
@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}
@misc{team2023internlm,
  title={Internlm: A multilingual language model with progressively enhanced capabilities},
  author={Team, InternLM},
  journal={2023-01-06)[2023-09-27]. https://github. com/InternLM/InternLM},
  year={2023}
}
@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}
@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}
@inproceedings{zhou2024large,
  title={Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks},
  author={Zhou, Yue and Zou, Henry and Di Eugenio, Barbara and Zhang, Yang},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={13293--13304},
  year={2024}
}
@article{zhou2024speak,
  title={Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue},
  author={Zhou, Zhenhong and Xiang, Jiuyang and Chen, Haopeng and Liu, Quan and Li, Zherui and Su, Sen},
  journal={arXiv preprint arXiv:2402.17262},
  year={2024}
}
@inproceedings{hung2023walking,
  title={Walking a Tightrope--Evaluating Large Language Models in High-Risk Domains},
  author={Hung, Chia-Chien and Rim, Wiem Ben and Frost, Lindsay and Bruckner, Lars and Lawrence, Carolin},
  booktitle={Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP},
  pages={99--111},
  year={2023}
}

@article{ji2024moralbench,
  title={MoralBench: Moral Evaluation of LLMs},
  author={Ji, Jianchao and Chen, Yutong and Jin, Mingyu and Xu, Wujiang and Hua, Wenyue and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2406.04428},
  year={2024}
}

@article{xu2023sc,
  title={Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese},
  author={Xu, Liang and Zhao, Kangkang and Zhu, Lei and Xue, Hang},
  journal={arXiv preprint arXiv:2310.05818},
  year={2023}
}
@inproceedings{agarwal2024prompt,
  title={Prompt Leakage effect and mitigation strategies for multi-turn LLM Applications},
  author={Agarwal, Divyansh and Fabbri, Alexander Richard and Risher, Ben and Laban, Philippe and Joty, Shafiq and Wu, Chien-Sheng},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={1255--1275},
  year={2024}
}
@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}

@article{bohnet2023coreference,
  title={Coreference resolution through a seq2seq transition-based system},
  author={Bohnet, Bernd and Alberti, Chris and Collins, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={212--226},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}
@inproceedings{mireshghallahcan,
  title={Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
  author={Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@inproceedings{huang2022large,
  title={Are Large Pre-Trained Language Models Leaking Your Personal Information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2038--2047},
  year={2022}
}

@article{li2024salad,
  title={Salad-bench: A hierarchical and comprehensive safety benchmark for large language models},
  author={Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2402.05044},
  year={2024}
}

@article{xu2024llm,
  title={LLM Jailbreak Attack versus Defense Techniques--A Comprehensive Study},
  author={Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan},
  journal={arXiv preprint arXiv:2402.13457},
  year={2024}
}

@inproceedings{xu2024comprehensive,
  title={A comprehensive study of jailbreak attack versus defense for large language models},
  author={Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={7432--7449},
  year={2024}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{lin2024against,
  title={Against The Achilles' Heel: A Survey on Red Teaming for Generative Models},
  author={Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others},
  journal={arXiv preprint arXiv:2404.00629},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{ganguli2022red,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv e-prints},
  pages={arXiv--2209},
  year={2022}
}

@article{ren2024derail,
  title={Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues},
  author={Ren, Qibing and Li, Hao and Liu, Dongrui and Xie, Zhanxu and Lu, Xiaoya and Qiao, Yu and Sha, Lei and Yan, Junchi and Ma, Lizhuang and Shao, Jing},
  journal={arXiv preprint arXiv:2410.10700},
  year={2024}
}

@inproceedings{perez2022red,
  title={Red Teaming Language Models with Language Models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3419--3448},
  year={2022}
}

@article{jiang2024red,
  title={RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking},
  author={Jiang, Yifan and Aggarwal, Kriti and Laud, Tanmay and Munir, Kashif and Pujara, Jay and Mukherjee, Subhabrata},
  journal={CoRR},
  year={2024}
}

@inproceedings{zhang2024holistic,
  title={Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction},
  author={Zhang, Jinchuan and Zhou, Yan and Liu, Yaxin and Li, Ziming and Hu, Songlin},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={13711--13736},
  year={2024}
}

@inproceedings{yu2024cosafe,
  title={CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference},
  author={Yu, Erxin and Li, Jing and Liao, Ming and Wang, Siqi and Zuchen, Gao and Mi, Fei and Hong, Lanqing},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={17494--17508},
  year={2024}
}

@article{bai2024mt,
  title={Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues},
  author={Bai, Ge and Liu, Jie and Bu, Xingyuan and He, Yancheng and Liu, Jiaheng and Zhou, Zhanhui and Lin, Zhuoran and Su, Wenbo and Ge, Tiezheng and Zheng, Bo and others},
  journal={arXiv preprint arXiv:2402.14762},
  year={2024}
}

@inproceedings{zhenglmsys,
  title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{ji2024beavertails,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2023safetybench,
  title={Safetybench: Evaluating the safety of large language models with multiple choice questions},
  author={Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

