[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhang2023safetybench",
        "author": "Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie",
        "title": "Safetybench: Evaluating the safety of large language models with multiple choice questions"
      },
      {
        "key": "ji2024beavertails",
        "author": "Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong",
        "title": "Beavertails: Towards improved safety alignment of llm via a human-preference dataset"
      },
      {
        "key": "li2024salad",
        "author": "Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing",
        "title": "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models"
      },
      {
        "key": "deng2022cold",
        "author": "Deng, Jiawen and Zhou, Jingyan and Sun, Hao and Zheng, Chujie and Mi, Fei and Meng, Helen and Huang, Minlie",
        "title": "COLD: A Benchmark for Chinese Offensive Language Detection"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "agarwal2024prompt",
        "author": "Agarwal, Divyansh and Fabbri, Alexander Richard and Risher, Ben and Laban, Philippe and Joty, Shafiq and Wu, Chien-Sheng",
        "title": "Prompt Leakage effect and mitigation strategies for multi-turn LLM Applications"
      },
      {
        "key": "yu2024cosafe",
        "author": "Yu, Erxin and Li, Jing and Liao, Ming and Wang, Siqi and Zuchen, Gao and Mi, Fei and Hong, Lanqing",
        "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference"
      },
      {
        "key": "jiang2024red",
        "author": "Jiang, Yifan and Aggarwal, Kriti and Laud, Tanmay and Munir, Kashif and Pujara, Jay and Mukherjee, Subhabrata",
        "title": "RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking"
      },
      {
        "key": "xu2023sc",
        "author": "Xu, Liang and Zhao, Kangkang and Zhu, Lei and Xue, Hang",
        "title": "Sc-safety: A multi-round open-ended question adversarial safety benchmark for large language models in chinese"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lin2024against",
        "author": "Lin, Lizhi and Mu, Honglin and Zhai, Zenan and Wang, Minghan and Wang, Yuxia and Wang, Renxi and Gao, Junjie and Zhang, Yixuan and Che, Wanxiang and Baldwin, Timothy and others",
        "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative Models"
      },
      {
        "key": "perez2022red",
        "author": "Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "key": "wei2024jailbroken",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How does llm safety training fail?"
      },
      {
        "key": "liu2023jailbreaking",
        "author": "Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Wang, Kailong and Liu, Yang",
        "title": "Jailbreaking chatgpt via prompt engineering: An empirical study"
      },
      {
        "key": "xu2024comprehensive",
        "author": "Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan",
        "title": "A comprehensive study of jailbreak attack versus defense for large language models"
      },
      {
        "key": "xu2024llm",
        "author": "Xu, Zihao and Liu, Yi and Deng, Gelei and Li, Yuekang and Picek, Stjepan",
        "title": "LLM Jailbreak Attack versus Defense Techniques--A Comprehensive Study"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ji2024beavertails",
        "author": "Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong",
        "title": "Beavertails: Towards improved safety alignment of llm via a human-preference dataset"
      },
      {
        "key": "li2024salad",
        "author": "Li, Lijun and Dong, Bowen and Wang, Ruohui and Hu, Xuhao and Zuo, Wangmeng and Lin, Dahua and Qiao, Yu and Shao, Jing",
        "title": "Salad-bench: A hierarchical and comprehensive safety benchmark for large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yu2024cosafe",
        "author": "Yu, Erxin and Li, Jing and Liao, Ming and Wang, Siqi and Zuchen, Gao and Mi, Fei and Hong, Lanqing",
        "title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "huang2022large",
        "author": "Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan",
        "title": "Are Large Pre-Trained Language Models Leaking Your Personal Information?"
      },
      {
        "key": "mireshghallahcan",
        "author": "Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin",
        "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jiang2024red",
        "author": "Jiang, Yifan and Aggarwal, Kriti and Laud, Tanmay and Munir, Kashif and Pujara, Jay and Mukherjee, Subhabrata",
        "title": "RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking"
      }
    ]
  }
]