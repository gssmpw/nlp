\section{Related Work}
\paragraph{Safety Benchmarks for LLMs}
We summarize recent benchmarks for LLMs safety evaluation in both single-turn and multi-turn dialogues in Table~\ref{tab:related_work}. While single-turn dialogue benchmarks~\cite{zhang2023safetybench,ji2024beavertails,li2024salad,deng2022cold} offer larger datasets, they cannot assess model performance in more realistic multi-turn conversations. Existing multi-turn dialogue benchmarks~\cite{agarwal2024prompt,yu2024cosafe,jiang2024red,xu2023sc} are limited by their monolingual nature, restricted use of jailbreak attack methods, and conversations typically shorter than five turns. Furthermore, these benchmarks often have incomplete evaluation dimensions, overlooking crucial aspects such as legality and ethics (detailed comparison provided in Appendix~\ref{sec:extensive_related_work}), and notably fail to assess specific safety capabilities of LLMs. 
To address these limitations, we aim to construct a comprehensive bilingual safety evaluation benchmark that incorporates a broader range of jailbreak attack methods and extends to longer dialogue sequences.

\paragraph{Jailbreak Attacks on LLMs}
With the rapid development of LLMs, jailbreak attack methods have emerged as important tools for assessing LLMs safety through red teaming~\cite{lin2024against,perez2022red,wei2024jailbroken,liu2023jailbreaking,xu2024comprehensive,xu2024llm}. These approaches aim to induce models to generate unsafe content, helping identify security vulnerabilities and improve overall safety measures.
While several studies~\cite{ji2024beavertails,li2024salad} have proposed jailbreak benchmarks and harmful scenarios for testing LLM vulnerabilities, incorporating various attack types such as reference attacks~\cite{yu2024cosafe}, privacy attacks~\cite{huang2022large,mireshghallahcan}, and concealed harmful intent~\cite{jiang2024red}, most existing approaches are limited to single-turn interactions and single jailbreak attack strategy. In this work, we construct \benchmark to assess the safety of LLMs using diverse jailbreak attacks in multi-turn dialogues.