\section{Related Work\label{sec:Related-Work}
\vspace{-0.1cm}}

Since 2019, numerous studies have adapted blockchain systems to various
ML tasks such as biomedical image segmentation model training \cite{li2020dlbc},
cloud–edge–end resource allocation \cite{Qiu2021}, neural architecture
searching \cite{LI2022100089}, and medical image fusion \cite{Xiang2023}
to leverage computing power and data in a blockchain network.  
Several studies \cite{BravoMarquez2019,Chenli2019,Kang2020,Li2021byzantine,Chai2021,Liu2021}
utilized blockchain as an ML competition platform, where ML model
training, evaluation, and ranking procedures are embedded in the workflow
of the blockchain system. In such systems, model checkers filter
out unqualified model parameters or updates based on performance evaluation
on a test dataset provided by the model requester, and the node that
trains the best ML model within the least time sends its model to
the model requester and receives the training reward. However, the
above approaches, which select and reward only one single winning
model from several models trained on specific datasets and discard
the rest, fail to take full advantage of miners' computing power and
private data samples.

As a collaborative ML approach that can harness device-side private
data in a privacy-preserving manner, FL can be blended with blockchain
systems better than the aforementioned blockchain-based ML competition
schemes. Several studies implemented FL algorithms in blockchain systems
via smart contracts \cite{Toyoda2019,Ramanan2020}. A more fundamental
approach is integrating FL with customized consensus protocols so
that the FL system can become self-incentivized and more robust to
malicious behaviors. For instance,  in proof-of-federated-learning
(PoFL) \cite{Qu2021}, Qu \emph{et al.} adapted the mining pools in
proof-of-work (PoW) blockchain into federations of miners, where the
miners in a mining pool collaboratively train a global model with
algorithms like federated averaging and the pool manager broadcasts
the block containing the global model.  In \cite{Wang2022}, the
authors eliminated the trusted third-party platform in PoFL with a
new block structure, extra transaction types, and a credit-based incentive
mechanism. In FedCoin \cite{Liu2020fedcoin}, a Shapley-value-based
consensus protocol, the quality of local models was considered before
allocating training rewards by evaluating the contribution of each
miner's local model to the global model.  In \cite{Cao2023,Ying2023},
the authors combined directed acyclic graph (DAG) blockchain with
FL to avoid the drain of computing resources in PoW systems and protect
FL from lazy nodes and poisoning attacks.   However, in many blockchain-based
FL systems, the FL iterations are administered by a few centralized
servers (e.g., the mining pool managers in PoFL) or  third-party
platforms, which compromises the degree of decentralization of the
entire system. Though this issue has been considered in the above
two works using DAG-based asynchronous FL, the low utilization of
local models affects the convergence rate of the global model and
degrades the learning performance \cite{Zhang2024c}.

Highlight that ML can serve as useful work in proof-of-useful-work
(PoUW) as an alternative to PoW and improves the environmental sustainability
of conventional PoW systems by replacing the computing power consumed
in meaningless hash calculations with some useful computation tasks
like ML model training. Most ML-based PoUW protocols like proof-of-deep-learning
(PoDL) \cite{Chenli2019}, personalized artificial intelligence \cite{Zhang2023jul},
proof-of-learning (PoL) \cite{Jia2021}, RPoL \cite{Zhang2023jul},
and distributed PoDL \cite{Su2023} use the same principle where miners
attest certain amounts of honest computation by providing the intermediate
parameters necessary for reproducing the final ML model. Such protocols
involve intense communication and computation overhead because  ML
model retraining or verifying intermediate training steps are necessary
when validating miners' workloads. To avoid transmitting all the intermediate
model parameters directly, in DLchain \cite{Chenli2020}, miners execute
training algorithms like stochastic gradient descent with fixed random
seeds and prove training work with the random seeds and the accuracies
of the model at each epoch.  Also, zero-knowledge PoL \cite{Zhang2024a}
significantly reduced the communication, computation, and storage
overhead for Internet of Things devices via zero-knowledge proof techniques.
Nevertheless, even  so, the computational amount involved in PoUW
proof generation or validation remains unacceptable  for resource-constrained
mobile devices.\vspace{-0.1cm}