@Article{Kasyap2024,
  author   = {Harsh Kasyap and Somanath Tripathy},
  journal  = {Expert Systems with Applications},
  title    = {Privacy-preserving and Byzantine-robust Federated Learning Framework using Permissioned Blockchain},
  year     = {2024},
  issn     = {0957-4174},
  pages    = {122210},
  volume   = {238},
  abstract = {Data is readily available with the growing number of smart and IoT devices. However, application-specific data is available in small chunks and distributed across demographics. Also, sharing data online brings serious concerns and poses various security and privacy threats. To solve these issues, federated learning (FL) has emerged as a promising secure and collaborative learning solution. FL brings the machine learning model to the data owners, trains locally, and then sends the trained model to the central curator for final aggregation. However, FL is prone to poisoning and inference attacks in the presence of malicious participants and curious servers. Different Byzantine-robust aggregation schemes exist to mitigate poisoning attacks, but they require raw access to the model updates. Thus, it exposes the submitted updates to inference attacks. This work proposes a Byzantine-Robust and Inference-Resistant Federated Learning Framework using Permissioned Blockchain, called PrivateFL. PrivateFL replaces the central curator with the Hyperledger Fabric network. Further, we propose VPSA (Vertically Partitioned Secure Aggregation), tailored to PrivateFL framework, which performs robust and secure aggregation. Theoretical analysis proves that VPSA resists inference attacks, even if n−1 peers are compromised. A secure prediction mechanism to securely query a global model is also proposed for PrivateFL framework. Experimental evaluation shows that PrivateFL performs better than the traditional (centralized) learning systems, while being resistant to poisoning and inference attacks.},
  doi      = {10.1016/j.eswa.2023.122210},
  file     = {:2024Kasyap - Privacy Preserving and Byzantine Robust Federated Learning Framework Using Permissioned Blockchain.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Federated learning, Poisoning attack, Robustness, Inference attack, Privacy, Permissioned blockchain},
  url      = {https://www.sciencedirect.com/science/article/pii/S0957417423027124},
}

 
@Article{Nwosu2022,
  author    = {Nwosu, Lucy and Li, Xiangfang and Qian, Lijun and Kim, Seungchan and Dong, Xishuang},
  journal   = {PLOS ONE},
  title     = {Calibrated bagging deep learning for image semantic segmentation: A case study on COVID-19 chest X-ray image},
  year      = {2022},
  issn      = {1932-6203},
  month     = nov,
  number    = {11},
  pages     = {e0276250},
  volume    = {17},
  doi       = {10.1371/journal.pone.0276250},
  editor    = {Hemanth, Jude},
  file      = {:2022NovemberNwosu - Calibrated Bagging Deep Learning for Image Semantic Segmentation_ a Case Study on COVID 19 Chest X Ray Image.pdf:PDF},
  groups    = {Ensemble Learning},
  publisher = {Public Library of Science (PLoS)},
}

 
@Article{Zhang2024,
  author    = {Zhang, Haoran and Jiang, Shan and Xuan, Shichang},
  journal   = {Computer Communications},
  title     = {Decentralized federated learning based on blockchain: concepts, framework, and challenges},
  year      = {2024},
  issn      = {0140-3664},
  month     = feb,
  pages     = {140--150},
  volume    = {216},
  doi       = {10.1016/j.comcom.2023.12.042},
  file      = {:2024FebruaryZhang - Decentralized Federated Learning Based on Blockchain_ Concepts, Framework, and Challenges.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {Elsevier BV},
}

@Misc{Phong2021,
  author        = {Nguyen Huu Phong and Bernardete Ribeiro},
  title         = {Rethinking Recurrent Neural Networks and Other Improvements for Image Classification},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2007.15161},
  file          = {:2021Phong - Rethinking Recurrent Neural Networks and Other Improvements for Image Classification.pdf:PDF},
  groups        = {Ensemble Learning},
  primaryclass  = {cs.CV},
}

@InProceedings{Lee2021,
  author    = {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {6131--6141},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments.},
  file      = {:202118--24 JulLee - SUNRISE_ a Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning.pdf:PDF},
  groups    = {Ensemble Learning},
  pdf       = {http://proceedings.mlr.press/v139/lee21g/lee21g.pdf},
  url       = {https://proceedings.mlr.press/v139/lee21g.html},
}

@Misc{Palanisamy2020,
  author        = {Kamalesh Palanisamy and Dipika Singhania and Angela Yao},
  title         = {Rethinking CNN Models for Audio Classification},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2007.11154},
  file          = {:2020Palanisamy - Rethinking CNN Models for Audio Classification.pdf:PDF},
  groups        = {Ensemble Learning},
  primaryclass  = {cs.CV},
}

@Article{Zaman2023,
  author   = {Zaman, Khalid and Sah, Melike and Direkoglu, Cem and Unoki, Masashi},
  journal  = {IEEE Access},
  title    = {A Survey of Audio Classification Using Deep Learning},
  year     = {2023},
  pages    = {106620-106649},
  volume   = {11},
  doi      = {10.1109/ACCESS.2023.3318015},
  file     = {:2023Zaman - A Survey of Audio Classification Using Deep Learning.pdf:PDF},
  groups   = {Other Classification},
  keywords = {Deep learning,Hidden Markov models,Data models,Feature extraction,Surveys,Task analysis,Speech recognition,Audio systems,Emotion recognition,Classification algorithms,Recurrent neural networks,Audio,speech,music,emotion,noise,classification,recognition,deep learning,CNNs,RNNs,autoencoders,transformers,hybrid models},
}

@Article{Minaee2021,
  author     = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  journal    = {ACM Comput. Surv.},
  title      = {Deep Learning--based Text Classification: A Comprehensive Review},
  year       = {2021},
  issn       = {0360-0300},
  month      = {apr},
  number     = {3},
  volume     = {54},
  abstract   = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
  address    = {New York, NY, USA},
  articleno  = {62},
  doi        = {10.1145/3439726},
  file       = {:2021aprMinaee - Deep Learning Based Text Classification_ a Comprehensive Review.pdf:PDF},
  groups     = {Other Classification},
  issue_date = {April 2022},
  keywords   = {topic classification, sentiment analysis, question answering, news categorization, natural language inference, deep learning, Text classification},
  numpages   = {40},
  publisher  = {Association for Computing Machinery},
}

 
@Article{Kayikci2024,
  author    = {Kayikci, Safak and Khoshgoftaar, Taghi M.},
  journal   = {Journal of Big Data},
  title     = {Blockchain meets machine learning: a survey},
  year      = {2024},
  issn      = {2196-1115},
  month     = jan,
  number    = {1},
  volume    = {11},
  doi       = {10.1186/s40537-023-00852-y},
  file      = {:2024JanuaryKayikci - Blockchain Meets Machine Learning_ a Survey (1).pdf:PDF;:2024JanuaryKayikci - Blockchain Meets Machine Learning_ a Survey (1).pdf:PDF},
  groups    = {BC Survey},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Zhang2024a,
  author     = {Zhang, Heyi and Wu, Jun and Lin, Xi and Bashir, Ali Kashif and Al-Otaibi, Yasser D.},
  journal    = {IEEE Internet Things J.},
  title      = {Integrating Blockchain and Deep Learning Into Extremely Resource-Constrained {IoT}: An Energy-Saving Zero-Knowledge {PoL} Approach},
  year       = {2024},
  month      = feb,
  number     = {3},
  pages      = {3881-3895},
  volume     = {11},
  comment    = {ZPoL
ZKP, doesn't need to reveal the model parameter (blackbox verification of models), but it is necessary to interact with the model trainer to prove both the accuracy of a model and its ownership.
The verifier send a batch of test samples to the trainer and the trainer respond with the prediction labels and ZKP

ZKP for both CNN and Decision tree are mentioned (zkCNN and zxDT)

Block structure design},
  doi        = {10.1109/JIOT.2023.3280069},
  file       = {:2024Zhang - Integrating Blockchain and Deep Learning into Extremely Resource Constrained IoT_ an Energy Saving Zero Knowledge PoL Approach.pdf:PDF},
  groups     = {Blockchain ML},
  keywords   = {Computational modeling,Internet of Things,Blockchains,Task analysis,Deep learning,Data models,Training,Blockchain,consensus protocol,deep learning (DL),energy saving,extremely resource-constrained Internet of Things (IoT)},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@InProceedings{Buyukates2023,
  author    = {B. Buyukates and C. He and S. Han and Z. Fang and Y. Zhang and J. Long and A. Farahanchi and S. Avestimehr},
  booktitle = {2023 IEEE International Conference on Decentralized Applications and Infrastructures (DAPPS)},
  title     = {Proof-of-Contribution-Based Design for Collaborative Machine Learning on Blockchain},
  year      = {2023},
  address   = {Los Alamitos, CA, USA},
  month     = jul,
  pages     = {13-22},
  publisher = {IEEE Computer Society},
  abstract  = {We consider a project (model) owner that would like to train a model by utilizing the local private data and compute power of interested data owners, i.e., trainers. Our goal is to design a data marketplace for such decentralized collaborative/federated learning applications that simultaneously provides i) proof-of-contribution based reward allocation so that the trainers are compensated based on their contributions to the trained model; ii) privacy-preserving decentralized model training by avoiding any data movement from data owners; iii) robustness against malicious parties (e.g., trainers aiming to poison the model); iv) verifiability in the sense that the integrity, i.e., correctness, of all computations in the data market protocol including contribution assessment and outlier detection are verifiable through zero-knowledge proofs; and v) efficient and universal design. We propose a blockchain-based marketplace design to achieve all five objectives mentioned above. In our design, we utilize a distributed storage infrastructure and an aggregator aside from the project owner and the trainers. The aggregator is a processing node that performs certain computations, including assessing trainer contributions, removing outliers, and updating hyper-parameters. We execute the proposed data market through a blockchain smart contract. The deployed smart contract ensures that the project owner cannot evade payment, and honest trainers are rewarded based on their contributions at the end of training. Finally, we implement the building blocks of the proposed data market and demonstrate their applicability in practical scenarios through extensive experiments.},
  comment   = {might not be PoUW},
  doi       = {10.1109/DAPPS57946.2023.00012},
  file      = {:2023julBuyukates - Proof of Contribution Based Design for Collaborative Machine Learning on Blockchain.pdf:PDF},
  groups    = {Blockchain ML},
  keywords  = {training,toxicology,computational modeling,smart contracts,collaboration,decentralized applications,data models},
}

@Article{Xiang2023,
  author     = {Xiang, Tao and Zeng, Honghong and Chen, Biwen and Guo, Shangwei},
  journal    = {ACM Trans. Multimedia Comput. Commun. Appl.},
  title      = {{BMIF}: Privacy-preserving Blockchain-based Medical Image Fusion},
  year       = {2023},
  issn       = {1551-6857},
  month      = jan,
  number     = {1s},
  pages      = {1-23},
  volume     = {19},
  abstract   = {Medical image fusion generates a fused image containing multiple features extracted from different source images, and it is of great help in clinical analysis and diagnosis. However, training a deep learning model for image fusion usually requires enormous computing power, especially for large volumes of medical data. Meanwhile, the privacy of images is also a critical issue. In this article, we propose a privacy-preserving blockchain-based medical image fusion (BMIF) framework. First, to ensure fusion performance, we design a new medical image fusion model based on convolutional neural network and Inception network and integrate the proposed model into the consensus process of blockchain. Next, to save computing power of blockchain, we design a consensus mechanism by requesting consensus nodes to train the fusion model instead of calculating useless hash values in traditional blockchain. Then, to protect data privacy, we further present an efficient homomorphic encryption to realize the training of fusion model on encrypted medical data. Finally, we conduct theoretical analysis and extensive experiments on public datasets to evaluate the feasibility and the performance of our proposed BMIF. The results exhibit that BMIF is efficient and secure, and our medical image fusion network performs better than state-of-the-art approaches.},
  address    = {New York, NY, USA},
  articleno  = {36},
  doi        = {10.1145/3531016},
  file       = {:2023janXiang - BMIF_ Privacy Preserving Blockchain Based Medical Image Fusion.pdf:PDF},
  groups     = {Blockchain ML},
  issue_date = {February 2023},
  keywords   = {Medical image fusion, blockchain, privacy protection, consensus mechanism, fully homomorphic encryption},
  numpages   = {23},
  publisher  = {Association for Computing Machinery},
}

@Article{Chai2021,
  author   = {Chai, Haoye and Leng, Supeng and Chen, Yijin and Zhang, Ke},
  journal  = {IEEE Trans. Intell. Transp. Syst.},
  title    = {A Hierarchical Blockchain-Enabled Federated Learning Algorithm for Knowledge Sharing in Internet of Vehicles},
  year     = {2021},
  month    = jul,
  number   = {7},
  pages    = {3975-3986},
  volume   = {22},
  comment  = {Application of PoK in Internet-of-Vehicle},
  doi      = {10.1109/TITS.2020.3002712},
  file     = {:2021Chai - A Hierarchical Blockchain Enabled Federated Learning Algorithm for Knowledge Sharing in Internet of Vehicles.pdf:PDF},
  groups   = {Blockchain ML},
  keywords = {Blockchain,Collaborative work,Security,Training,Computational modeling,Data models,Servers,Hierarchical blockchain,federated learning,knowledge sharing},
}

@Article{Li2024,
  author        = {Yang Li and Chunhe Xia and Wanshuang Lin and Tianbo Wang},
  journal       = {arXiv preprint arXiv:2401.01204},
  title         = {{PPBFL}: A Privacy Protected Blockchain-based Federated Learning Model},
  year          = {2024},
  month         = jan,
  archiveprefix = {arXiv},
  comment       = {The privacy protected blockchain-based federated learning model proposed by Li et al. features proof-of-training-work, a consensus mechanism in which the training node with the fastest training speed generates new blocks and receives rewards.

All local models and global model exchange relies on IPFS, which limit the efficiency of this system. The consensus protocol doesn't seem robust to attacks.},
  eprint        = {2401.01204},
  file          = {:2024Li - PPBFL_ a Privacy Protected Blockchain Based Federated Learning Model.pdf:PDF},
  groups        = {Blockchain ML, Blockchain-based FL},
  primaryclass  = {cs.CR},
  ranking       = {rank1},
}

@Misc{Hoffmann2023,
  author        = {Felix Hoffmann and Udo Kebschull},
  title         = {HEPchain: Novel Proof-of-Useful-Work blockchain consensus for High Energy Physics},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2304.13507},
  file          = {:2023Hoffmann - HEPchain_ Novel Proof of Useful Work Blockchain Consensus for High Energy Physics.pdf:PDF},
  groups        = {Proof-of-Useful-Work},
  primaryclass  = {cs.DC},
}

@Article{Ma2024,
  author   = {Ma, Xuyang and Xu, Du},
  journal  = {IEEE Internet of Things Journal},
  title    = {TORR: A Lightweight Blockchain for Decentralized Federated Learning},
  year     = {2024},
  number   = {1},
  pages    = {1028-1040},
  volume   = {11},
  comment  = {FedAvg to aggregate local models
Roles:  client, aggregator and keeper
 Proof of Reliability to select the most reliable node as the block proposer},
  doi      = {10.1109/JIOT.2023.3288078},
  file     = {:2024Ma - TORR_ a Lightweight Blockchain for Decentralized Federated Learning.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Federated learning,Consensus protocol,Servers,Reliability,Performance evaluation,Training,Data models,Blockchain,consensus,federated learning (FL),storage},
}

@Book{Li2023,
  author    = {Li, Boyang},
  publisher = {University of Notre Dame},
  title     = {Proof-Of-Deep-Learning Consensus in Blockchain Systems},
  year      = {2023},
  file      = {:2023Li - Proof of Deep Learning Consensus in Blockchain Systems.pdf:PDF},
  groups    = {Blockchain ML},
}

@Misc{Li2023a,
  author        = {Yang Li and Chunhe Xia and Wei Liu and Weidong Zhou and Chen Chen and Tianbo Wang},
  title         = {FBChain: A Blockchain-based Federated Learning Model with Efficiency and Secure Communication},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {Proof of Weighted Link Speed, not a PoFL approach},
  eprint        = {2312.00035},
  file          = {:2023Li - FBChain_ a Blockchain Based Federated Learning Model with Efficiency and Secure Communication.pdf:PDF},
  groups        = {Blockchain Consensus},
  primaryclass  = {cs.CR},
}

@Misc{Abbaszadeh2024,
  author       = {Kasra Abbaszadeh and Christodoulos Pappas and Dimitrios Papadopoulos and Jonathan Katz},
  howpublished = {Cryptology ePrint Archive, Paper 2024/162},
  note         = {\url{https://eprint.iacr.org/2024/162}},
  title        = {Zero-Knowledge Proofs of Training for Deep Neural Networks},
  year         = {2024},
  file         = {:2024Abbaszadeh - Zero Knowledge Proofs of Training for Deep Neural Networks.pdf:PDF},
  groups       = {ZKP & ML},
  url          = {https://eprint.iacr.org/2024/162},
}

@Misc{Sun2023,
  author        = {Haochen Sun and Hongyang Zhang},
  title         = {zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {zkReLU, a zero-knowledge proof for verification of ReLU activation function, cutting down proving time and proof size(space/time complexity drops significantly with respect to NN scale)},
  eprint        = {2307.16273},
  file          = {:2023Sun - ZkDL_ Efficient Zero Knowledge Proofs of Deep Learning Training.pdf:PDF},
  groups        = {ML x BC misc, ZKP & ML},
  primaryclass  = {cs.LG},
  ranking       = {rank2},
}

@Misc{Chatterjee2023,
  author        = {Diptendu Chatterjee and Prabal Banerjee and Subhra Mazumdar},
  title         = {Chrisimos: A useful Proof-of-Work for finding Minimal Dominating Set of a graph},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.04407},
  file          = {:2023Chatterjee - Chrisimos_ a Useful Proof of Work for Finding Minimal Dominating Set of a Graph (1).pdf:PDF},
  groups        = {Proof-of-Useful-Work},
  primaryclass  = {cs.CR},
}

@InProceedings{Xia2023,
  author    = {Xia, Zhen and Cao, Zhenfu and Shen, Jiachen and Dong, Xiaolei and Zhou, Jun and Fang, Liming and Liu, Zhe and Ge, Chunpeng and Su, Chunhua},
  booktitle = {Proc. 18th Int. Conf. Inf. Secur. Pract. Exper. (ISPEC'23)},
  title     = {Mining for Better: An Energy-Recycling Consensus Algorithm to Enhance Stability with Deep Learning},
  year      = {2023},
  address   = {Copenhagen, DK},
  month     = aug,
  pages     = {579-594},
  abstract  = {As the most popular consensus algorithm for blockchain, the Proof-of-Work (PoW) is suffering from the inability of handling computing power fluctuations. Meanwhile, PoW consumes a significant amount of energy without producing actual value. To address these issues, this paper proposes a deep learning-based consensus framework called Proof-of-Improvement (PoI), which recycles the energy from mining blocks to improve the blockchain itself. In PoI, a new reward mechanism is used to encourage miners to include the high-accuracy model in their blocks. Then, based on PoI, a difficulty adjustment algorithm is designed. Experiments are done on real-world data and the result shows the proposed algorithm’s proficiency in preserving block time stability with fluctuating hash rates. To the best of the authors’ knowledge, PoI is the first to handle both energy recycling and difficulty adjustment concurrently.},
  comment   = {Proof of improvement (PoI)

1. Models used as the proof of useful work help improve the block time stability by adjusting difficulty dynamically.

2. secure mapping layer is used in PoL

3. New block structure: add model hash to the header of a block

4. Both training and test data come from the block data at previous heights},
  doi       = {10.1007/978-981-99-7032-2_34},
  file      = {:2023AugustXia - Mining for Better_ an Energy Recycling Consensus Algorithm to Enhance Stability with Deep Learning.pdf:PDF},
  groups    = {Blockchain ML},
  isbn      = {978-981-99-7031-5},
  keywords  = {Blockchain, Consensus algorithm, Computing power utilization, Difficulty adjustment algorithm, Deep learning},
  location  = {Copenhagen, Denmark},
  numpages  = {16},
  priority  = {prio3},
  ranking   = {rank3},
}

@Article{Zhu2021,
  author    = {Zhu, Hangyu and Xu, Jinjin and Liu, Shiqing and Jin, Yaochu},
  journal   = {Neurocomputing},
  title     = {Federated learning on non-{IID} data: A survey},
  year      = {2021},
  issn      = {0925-2312},
  month     = nov,
  pages     = {371--390},
  volume    = {465},
  doi       = {10.1016/j.neucom.2021.07.098},
  file      = {:2021NovemberZhu - Federated Learning on Non IID Data_ a Survey.pdf:PDF},
  groups    = {FL & Non-iid},
  publisher = {Elsevier BV},
}

@Article{Zhao2018,
  author    = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  title     = {Federated Learning with Non-{IID} Data},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1806.00582},
  file      = {:2018Zhao - Federated Learning with Non IID Data.pdf:PDF},
  groups    = {FL & Non-iid},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  ranking   = {rank4},
  url       = {https://arxiv.org/abs/1806.00582},
}

@Article{Cheng2021,
  author    = {K. Cheng and T. Fan and Y. Jin and Y. Liu and T. Chen and D. Papadopoulos and Q. Yang},
  journal   = {IEEE Intelligent Systems},
  title     = {SecureBoost: A Lossless Federated Learning Framework},
  year      = {2021},
  issn      = {1941-1294},
  month     = {nov},
  number    = {06},
  pages     = {87-98},
  volume    = {36},
  abstract  = {The protection of user privacy is an important concern in machine learning, as evidenced by the rolling out of the General Data Protection Regulation (GDPR) in the European Union (EU) in May 2018. The GDPR is designed to give users more control over their personal data, which motivates us to explore machine learning frameworks for data sharing that do not violate user privacy. To meet this goal, in this article, we propose a novel lossless privacy-preserving tree-boosting system known as SecureBoost in the setting of federated learning. SecureBoost first conducts entity alignment under a privacy-preserving protocol and then constructs boosting trees across multiple parties with a carefully designed encryption strategy. This federated learning system allows the learning process to be jointly conducted over multiple parties with common user samples but different feature sets, which corresponds to a vertically partitioned dataset. An advantage of SecureBoost is that it provides the same level of accuracy as the non -privacy-preserving approach while at the same time, reveals no information of each private data provider. We show that the SecureBoost framework is as accurate as other nonfederated gradient tree-boosting algorithms that require centralized data, and thus, it is highly scalable and practical for industrial applications such as credit risk analysis. To this end, we discuss information leakage during the protocol execution and propose ways to provably reduce it.},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/MIS.2021.3082561},
  file      = {:2021novCheng - SecureBoost_ a Lossless Federated Learning Framework.pdf:PDF},
  groups    = {FL & Non-iid},
  keywords  = {data models,machine learning,collaborative work,protocols,data privacy,servers,general data protection regulation},
  publisher = {IEEE Computer Society},
}

@Article{Wang2023,
  author   = {Wang, Peng and Sun, Wen and Zhang, Haibin and Ma, Wenqiang and Zhang, Yan},
  journal  = {IEEE Transactions on Vehicular Technology},
  title    = {Distributed and Secure Federated Learning for Wireless Computing Power Networks},
  year     = {2023},
  number   = {7},
  pages    = {9381-9393},
  volume   = {72},
  doi      = {10.1109/TVT.2023.3247859},
  file     = {:2023Wang - Distributed and Secure Federated Learning for Wireless Computing Power Networks.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Federated learning,Security,Computational modeling,Blockchains,Training,Wireless communication,Computer architecture,Wireless computing power network,federated learning,blockchain,asynchronous learning,security of artificial intelligence},
  priority = {prio3},
  ranking  = {rank4},
}

@Misc{Feng2023,
  author        = {Haozhe Feng and Tianyu Pang and Chao Du and Wei Chen and Shuicheng Yan and Min Lin},
  title         = {Does Federated Learning Really Need Backpropagation?},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {Conceptual steps
(1) each client locally perturbs the model parameters 2K
times as W±δk (the server sends the random seed to clients for generating {δk}; 
(2) each client
executes forward processes on the perturbed models using its private dataset Dc and obtains K loss
differences {∆L(W, δk; Dc)}
(3) the server aggregates loss differences to estimate gradients.
BAFFLE’s defining characteristic: memory efficient and not requiring auto-differentiation.},
  eprint        = {2301.12195},
  file          = {:2023Feng - Does Federated Learning Really Need Backpropagation_.pdf:PDF},
  groups        = {Misc FL},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
  readstatus    = {skimmed},
}

@Article{Xu2023,
  author        = {Mengwei Xu and Dongqi Cai and Yaozong Wu and Xiang Li and Shangguang Wang},
  title         = {FwdLLM: Efficient FedLLM using Forward Gradient},
  year          = {2023},
  month         = aug,
  abstract      = {Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence. In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with parameter-efficient training methods, an essential way to scale the approach to the LLM era; (2) it systematically and adaptively allocates computational loads across devices, striking a careful balance between convergence speed and accuracy; (3) it discriminatively samples perturbed predictions that are more valuable to model convergence. Comprehensive experiments with five LLMs and three NLP tasks illustrate FwdLLM's significant advantages over conventional methods, including up to three orders of magnitude faster convergence and a 14.6x reduction in memory footprint. Uniquely, FwdLLM paves the way for federated learning of billion-parameter LLMs such as LLaMA on COTS mobile devices -- a feat previously unattained.},
  archiveprefix = {arXiv},
  comment       = {文献综述里有一部分介绍Gradient Free FL},
  eprint        = {2308.13894},
  file          = {:2023AugustXu - FwdLLM_ Efficient FedLLM Using Forward Gradient.pdf:PDF},
  groups        = {Misc FL},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@InProceedings{Polato2022,
  author    = {Polato, Mirko and Esposito, Roberto and Aldinucci, Marco},
  booktitle = {2022 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Boosting the Federation: Cross-Silo Federated Learning without Gradient Descent},
  year      = {2022},
  pages     = {1-10},
  comment   = {介绍了五种实现Non-iidness的方法
采用分布式Boost算法实现无梯度下降的FL},
  doi       = {10.1109/IJCNN55064.2022.9892284},
  file      = {:2022Polato - Boosting the Federation_ Cross Silo Federated Learning without Gradient Descent.pdf:PDF},
  groups    = {FL & Non-iid, Misc FL},
  keywords  = {Privacy,Adaptation models,Federated learning,Computational modeling,Neural networks,Boosting,Prediction algorithms,federated learning,cross-silo,boosting,adaboost,ensemble learning},
}

@Article{Kairouz2021,
  author     = {Kairouz, Peter and others},
  journal    = {Found. Trends Mach. Learn.},
  title      = {Advances and Open Problems in Federated Learning},
  year       = {2021},
  issn       = {1935-8237},
  month      = jun,
  number     = {1–2},
  pages      = {1-210},
  volume     = {14},
  abstract   = {Federated learning (FL) is a machine learning setting where many clients (e.g., mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g., service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this monograph discusses recent advances and presents an extensive collection of open problems and challenges.},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/2200000083},
  file       = {:2021junKairouz - Advances and Open Problems in Federated Learning.pdf:PDF},
  groups     = {FL Research},
  issue_date = {Jun 2021},
  numpages   = {214},
  publisher  = {Now Publishers Inc.},
  ranking    = {rank5},
}

@InProceedings{Li2022may,
  author     = {Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng},
  booktitle  = {Proc. IEEE 38th Int. Conf. Data Eng. (ICDE'22)},
  title      = {Federated Learning on Non-{IID} Data Silos: An Experimental Study},
  year       = {2022},
  address    = {Kuala Lumpur, MY},
  month      = may,
  pages      = {965-978},
  comment    = {针对不同的non-iidness，对现有的FL方法进行实验评估(benchmark)},
  doi        = {10.1109/ICDE53745.2022.00077},
  file       = {:2022Li - Federated Learning on Non IID Data Silos_ an Experimental Study.pdf:PDF;:dirichlet.pdf:PDF},
  groups     = {FL & Non-iid},
  keywords   = {Data privacy,Machine learning algorithms,Distributed databases,Training data,Machine learning,Organizations,Collaborative work,Federated Learning,Benchmark},
  priority   = {prio2},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Misc{Wijesinghe2023,
  author        = {Achintha Wijesinghe and Songyang Zhang and Zhi Ding},
  title         = {PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2305.11437},
  file          = {:2023Wijesinghe - PS FedGAN_ an Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks for Data Privacy.pdf:PDF},
  groups        = {Misc FL},
  primaryclass  = {cs.LG},
}

@Misc{Wijesinghe2023a,
  author        = {Achintha Wijesinghe and Songyang Zhang and Siyu Qi and Zhi Ding},
  title         = {UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.05870},
  file          = {:2023Wijesinghe - UFed GAN_ a Secure Federated Learning Framework with Constrained Computation and Unlabeled Data.pdf:PDF},
  groups        = {Misc FL},
  primaryclass  = {cs.LG},
}

@Misc{Wijesinghe2023b,
  author        = {Achintha Wijesinghe and Songyang Zhang and Zhi Ding},
  title         = {PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.12454},
  file          = {:2023Wijesinghe - PFL GAN_ When Client Heterogeneity Meets Generative Models in Personalized Federated Learning.pdf:PDF},
  groups        = {Misc FL},
  primaryclass  = {cs.LG},
}

@Article{Xu2024,
  author    = {Xu, Chenhao and Ge, Jiaqi and Deng, Yao and Gao, Longxiang and Zhang, Mengshi and Li, Yong and Zhou, Wanlei and Zheng, Xi},
  journal   = {IEEE Transactions on Dependable and Secure Computing},
  title     = {BASS: A Blockchain-Based Asynchronous SignSGD Architecture for Efficient and Secure Federated Learning},
  year      = {2024},
  issn      = {2160-9209},
  month     = mar,
  number    = {01},
  pages     = {1-15},
  abstract  = {Federated learning (FL) is a distributed framework for machine learning that enables collaborative training of a shared model across data silos while preserving data privacy. However, the FL aggregation server faces a challenge in waiting for a large volume of model parameters from selected nodes before generating a global model, which leads to inefficient communication and aggregation. Although transmitting only the signs of stochastic gradient descent (SignSGD) reduces the transmission load, it decreases model accuracy, and the time waiting for local model collection remains substantial. Moreover, the security of FL is severely compromised by prevalent poisoning, backdoor, and DDoS attacks, causing ineffective and inaccurate model training. To overcome these challenges, this paper proposes a Blockchain-based Asynchronous SignSGD (BASS) architecture for efficient and secure federated learning. By integrating a blockchain-based semi-asynchronous aggregation scheme with sign-based gradient compression, BASS considerably improves communication and aggregation efficiency, while providing resistance against attacks. Besides, a novel node-summarized sign aggregation algorithm is developed for the blockchain leaders to ensure the convergence and accuracy of the global model. An open-source prototype is developed, on top of which extensive experiments are conducted. The results validate the superiority of BASS in terms of efficiency, model accuracy, and security.},
  address   = {Los Alamitos, CA, USA},
  doi       = {10.1109/TDSC.2024.3374809},
  file      = {:BASS_A_Blockchain-Based_Asynchronous_SignSGD_Architecture_for_Efficient_and_Secure_Federated_Learning.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {training,blockchains,servers,security,data models,federated learning,computational modeling},
  priority  = {prio3},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Zhao2024,
  author     = {Zhao, Yao and Qu, Youyang and Xiang, Yong and Chen, Feifei and Gao, Longxiang},
  journal    = {IEEE Trans. on Cloud Comput.},
  title      = {Context-aware Consensus Algorithm for Blockchain-empowered Federated Learning},
  year       = {2024},
  pages      = {1-13},
  comment    = {A generalizable BFL process can be abstracted into five key steps, each of which is associated with a contribution metric, which is used to evaluate global contribution. 
Context Aware Proof of Contribution: A block producer selection problem is formulated to decide a block producer which maximizing the objective function describing the total contributions	. The reward is distributed by the global contributions of each miner.

Wrting style: 
1. A clear classification of BFL systems: fully coupled, flexibly coupled, loosely coupled
2. Well arranged LR, pro and cons of each literature is pointed out.
3. Comparing with other contribution based BFL schemes},
  doi        = {10.1109/TCC.2024.3372814},
  file       = {:2024Zhao - Context Aware Consensus Algorithm for Blockchain Empowered Federated Learning.pdf:PDF},
  groups     = {Blockchain-based FL},
  keywords   = {Training,Blockchains,Delays,Computational modeling,Consensus algorithm,Federated learning,Cloud computing,Consensus algorithm,blockchain,federated learning,lazy client,incentive mechanism},
  ranking    = {rank3},
  readstatus = {skimmed},
}





















@Article{Alhejazi2022,
  author    = {Manal Mohamed Alhejazi and Rami Mustafa A. Mohammad},
  journal   = {Information Security Journal: A Global Perspective},
  title     = {Enhancing the blockchain voting process in IoT using a novel blockchain Weighted Majority Consensus Algorithm (WMCA)},
  year      = {2022},
  number    = {2},
  pages     = {125-143},
  volume    = {31},
  comment   = {受到集成学习启发的区块链投票机制},
  doi       = {10.1080/19393555.2020.1869356},
  eprint    = {https://doi.org/10.1080/19393555.2020.1869356},
  file      = {:2022Alhejazi - Enhancing the Blockchain Voting Process in IoT Using a Novel Blockchain Weighted Majority Consensus Algorithm (WMCA).pdf:PDF},
  groups    = {Blockchain ML},
  publisher = {Taylor & Francis},
  ranking   = {rank1},
  url       = {https://doi.org/10.1080/19393555.2020.1869356},
}

@Misc{Park2024,
  author        = {Chanho Park and H. Vincent Poor and Namyoon Lee},
  title         = {SignSGD with Federated Voting},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {What is signSGD? Quantization?},
  eprint        = {2403.16372},
  file          = {:2024Park - SignSGD with Federated Voting.pdf:PDF},
  groups        = {Weighted FL},
  primaryclass  = {cs.LG},
}

@Misc{Guo2021,
  author        = {Jiale Guo and Ziyao Liu and Kwok-Yan Lam and Jun Zhao and Yiqiang Chen and Chaoping Xing},
  title         = {Secure Weighted Aggregation for Federated Learning},
  year          = {2021},
  archiveprefix = {arXiv},
  comment       = {Weight aggregating algorithm improved from the paper [Chen2020], which quantify the credibility of user's datset. Other cryptographic techniques such as ZKP are employed to enhance privacy and rubostness of FL systems(dropout-resilient).},
  eprint        = {2010.08730},
  file          = {:2021Guo - Secure Weighted Aggregation for Federated Learning.pdf:PDF},
  groups        = {Weighted FL},
  primaryclass  = {cs.CR},
  ranking       = {rank3},
}

@Article{Zhong2024,
  author   = {Zhong, Yijian and Tan, Wuzheng and Xu, Zhifeng and Chen, Shixin and Weng, Jiasi and Weng, Jian},
  journal  = {IEEE Internet of Things Journal},
  title    = {WVFL: Weighted Verifiable Secure Aggregation in Federated Learning},
  year     = {2024},
  pages    = {1-1},
  doi      = {10.1109/JIOT.2024.3370938},
  file     = {:WVFL_Weighted_Verifiable_Secure_Aggregation_in_Federated_Learning.pdf:PDF},
  groups   = {Weighted FL},
  keywords = {Servers,Computational modeling,Vectors,Data models,Protocols,Federated learning,Internet of Things,Secure Weighted Aggregation,Verifiability,Secure Multi-Party Computation,Federated Learning},
}

@Article{Pejo2023,
  author   = {Pejó, Balázs and Biczók, Gergely},
  journal  = {IEEE Transactions on Big Data},
  title    = {Quality Inference in Federated Learning With Secure Aggregation},
  year     = {2023},
  number   = {5},
  pages    = {1430-1437},
  volume   = {9},
  doi      = {10.1109/TBDATA.2023.3280406},
  file     = {:2023Pejó - Quality Inference in Federated Learning with Secure Aggregation.pdf:PDF},
  groups   = {Weighted FL},
  keywords = {Computational modeling,Training,Data models,Data integrity,Task analysis,Correlation,Big Data,Quality inference,federated learning,secure aggregation,misbehavior detection,contribution score},
}

@InProceedings{Li2023jul,
  author    = {Li, Zexi and Lin, Tao and Shang, Xinyi and Wu, Chao},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  title     = {Revisiting Weighted Aggregation in Federated Learning with Neural Networks},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  month     = jul,
  pages     = {19767--19788},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  abstract  = {In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients’ data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients’ importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Aggregation Weights, named as FedLAW. Extensive experiments verify that our method can improve the generalization of the global model by a large margin on different datasets and models.},
  file      = {:202323--29 JulLi - Revisiting Weighted Aggregation in Federated Learning with Neural Networks.pdf:PDF},
  groups    = {Weighted FL},
  pdf       = {https://proceedings.mlr.press/v202/li23s/li23s.pdf},
  ranking   = {rank3},
  url       = {https://proceedings.mlr.press/v202/li23s.html},
}

@InBook{Chen2020,
  author    = {Chen, Yiqiang and Yang, Xiaodong and Qin, Xin and Yu, Han and Chan, Piu and Shen, Zhiqi},
  editor    = {Yang, Qiang and Fan, Lixin and Yu, Han},
  pages     = {108--121},
  publisher = {Springer International Publishing},
  title     = {Dealing with Label Quality Disparity in Federated Learning},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-63076-8},
  abstract  = {Federated Learning (FL) is highly useful for the applications which suffer silo effect and privacy preserving, such as healthcare, finance, education, etc. Existing FL approaches generally do not account for disparities in the quality of local data labels. However, the participants tend to suffer from label noise due to annotators' varying skill-levels, biases or malicious tampering. In this chapter, we propose an alternative approach to address this challenge. It maintains a small set of benchmark samples on the FL coordinator and quantifies the credibility of the participants' local data without directly observing them by computing the mutual cross-entropy between performance of the FL model on the local datasets and that of the participant's local model on the benchmark dataset. Then, a credit-weighted orchestration is performed to adjust the weight assigned to participants in the FL model based on their credibility values. By experimentally evaluating on both synthetic data and real-world data, the results show that the proposed approach effectively identifies participants with noisy labels and reduces their impact on the FL model performance, thereby significantly outperforming existing FL approaches.},
  booktitle = {Federated Learning: Privacy and Incentive},
  doi       = {10.1007/978-3-030-63076-8_8},
  file      = {:2020Chen - Dealing with Label Quality Disparity in Federated Learning.pdf:PDF},
  groups    = {Weighted FL},
  url       = {https://doi.org/10.1007/978-3-030-63076-8_8},
}

@Article{Wang2021unbalanced,
  author   = {Wang, Le and Han, Meng and Li, Xiaojuan and Zhang, Ni and Cheng, Haodong},
  journal  = {IEEE Access},
  title    = {Review of Classification Methods on Unbalanced Data Sets},
  year     = {2021},
  pages    = {64606-64628},
  volume   = {9},
  doi      = {10.1109/ACCESS.2021.3074243},
  file     = {:2021Wang - Review of Classification Methods on Unbalanced Data Sets.pdf:PDF},
  groups   = {Ensemble Diversity},
  keywords = {Classification algorithms,Sampling methods,Support vector machines,Security,Safety,Deep learning,Boosting,Unbalanced data sets,classification,sampling methods,algorithm level,feature level},
  ranking  = {rank3},
}

@Article{DiezPastor2015,
  author   = {José F. Díez-Pastor and Juan J. Rodríguez and César García-Osorio and Ludmila I. Kuncheva},
  journal  = {Knowledge-Based Systems},
  title    = {Random Balance: Ensembles of variable priors classifiers for imbalanced data},
  year     = {2015},
  issn     = {0950-7051},
  pages    = {96-111},
  volume   = {85},
  abstract = {In Machine Learning, a data set is imbalanced when the class proportions are highly skewed. Imbalanced data sets arise routinely in many application domains and pose a challenge to traditional classifiers. We propose a new approach to building ensembles of classifiers for two-class imbalanced data sets, called Random Balance. Each member of the Random Balance ensemble is trained with data sampled from the training set and augmented by artificial instances obtained using SMOTE. The novelty in the approach is that the proportions of the classes for each ensemble member are chosen randomly. The intuition behind the method is that the proposed diversity heuristic will ensure that the ensemble contains classifiers that are specialized for different operating points on the ROC space, thereby leading to larger AUC compared to other ensembles of classifiers. Experiments have been carried out to test the Random Balance approach by itself, and also in combination with standard ensemble methods. As a result, we propose a new ensemble creation method called RB-Boost which combines Random Balance with AdaBoost.M2. This combination involves enforcing random class proportions in addition to instance re-weighting. Experiments with 86 imbalanced data sets from two well known repositories demonstrate the advantage of the Random Balance approach.},
  doi      = {10.1016/j.knosys.2015.04.022},
  file     = {:2015Díez-Pastor - Random Balance_ Ensembles of Variable Priors Classifiers for Imbalanced Data.pdf:PDF},
  groups   = {Ensemble Diversity},
  keywords = {Classifier ensembles, Imbalanced data sets, Bagging, AdaBoost, SMOTE, Undersampling},
  ranking  = {rank2},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705115001720},
}

@Article{Hassanat2022,
  author         = {Hassanat, Ahmad B. and Tarawneh, Ahmad S. and Abed, Samer Subhi and Altarawneh, Ghada Awad and Alrashidi, Malek and Alghamdi, Mansoor},
  journal        = {Electronics},
  title          = {RDPVR: Random Data Partitioning with Voting Rule for Machine Learning from Class-Imbalanced Datasets},
  year           = {2022},
  issn           = {2079-9292},
  number         = {2},
  volume         = {11},
  abstract       = {Since most classifiers are biased toward the dominant class, class imbalance is a challenging problem in machine learning. The most popular approaches to solving this problem include oversampling minority examples and undersampling majority examples. Oversampling may increase the probability of overfitting, whereas undersampling eliminates examples that may be crucial to the learning process. We present a linear time resampling method based on random data partitioning and a majority voting rule to address both concerns, where an imbalanced dataset is partitioned into a number of small subdatasets, each of which must be class balanced. After that, a specific classifier is trained for each subdataset, and the final classification result is established by applying the majority voting rule to the results of all of the trained models. We compared the performance of the proposed method to some of the most well-known oversampling and undersampling methods, employing a range of classifiers, on 33 benchmark machine learning class-imbalanced datasets. The classification results produced by the classifiers employed on the generated data by the proposed method were comparable to most of the resampling methods tested, with the exception of SMOTEFUNA, which is an oversampling method that increases the probability of overfitting. The proposed method produced results that were comparable to the Easy Ensemble (EE) undersampling method. As a result, for solving the challenge of machine learning from class-imbalanced datasets, we advocate using either EE or our method.},
  article-number = {228},
  doi            = {10.3390/electronics11020228},
  file           = {:2022Hassanat - RDPVR_ Random Data Partitioning with Voting Rule for Machine Learning from Class Imbalanced Datasets.pdf:PDF},
  groups         = {Ensemble Diversity},
  url            = {https://www.mdpi.com/2079-9292/11/2/228},
}

 
@Article{Kuncheva2003,
  author     = {Kuncheva, Ludmila I. and Whitaker, Christopher J.},
  journal    = {Machine Learning},
  title      = {Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy},
  year       = {2003},
  issn       = {0885-6125},
  number     = {2},
  pages      = {181--207},
  volume     = {51},
  doi        = {10.1023/a:1022859003006},
  file       = {:2003Kuncheva - Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy.pdf:PDF},
  groups     = {Ensemble Diversity},
  priority   = {prio2},
  publisher  = {Springer Science and Business Media LLC},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@Article{Zhao2024a,
  author   = {Zhao, Changming and Peng, Ruimin and Wu, Dongrui},
  journal  = {IEEE Transactions on Artificial Intelligence},
  title    = {Bagging and Boosting Fine-Tuning for Ensemble Learning},
  year     = {2024},
  number   = {4},
  pages    = {1728-1742},
  volume   = {5},
  doi      = {10.1109/TAI.2023.3296685},
  file     = {:2024Zhao - Bagging and Boosting Fine Tuning for Ensemble Learning.pdf:PDF},
  groups   = {Ensemble Diversity},
  keywords = {Boosting,Bagging,Training,Support vector machines,Artificial intelligence,Ensemble learning,Learning systems,Bagging,boosting,broad learning system,ensemble learning,fine-tuning},
}

@PhdThesis{lofstrom2009utilizing,
  author     = {L{\"o}fstr{\"o}m, Tuve},
  school     = {{\"O}rebro universitet},
  title      = {Utilizing Diversity and Performance Measures for Ensemble Creation},
  year       = {2009},
  comment    = {二分类问题
多数类样本拆分成多个子集，然后将这几个子集分别与少数类样本合并；使拆分后的多数类样本数量与少数类样本数量差不多},
  file       = {:2009Löfström - Utilizing Diversity and Performance Measures for Ensemble Creation.pdf:PDF},
  groups     = {Ensemble Diversity},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {http://hb.diva-portal.org/smash/get/diva2:876899/FULLTEXT02.pdf},
}

 
@Article{Chawla2002,
  author     = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {SMOTE: Synthetic Minority Over-sampling Technique},
  year       = {2002},
  issn       = {1076-9757},
  month      = jun,
  pages      = {321--357},
  volume     = {16},
  comment    = {a combination of our method of over-sampling
 the minority (abnormal)class and under-sampling the majority (normal)class can achieve
 better classifier performance (in ROC space)than only under-sampling the majority class

SMOTE algorithm is proposed 

Evaluated with ROC(receiver operating characteristic) and AUC(aera under the ROC curve)},
  doi        = {10.1613/jair.953},
  file       = {:2002JuneChawla - SMOTE_ Synthetic Minority Over Sampling Technique.pdf:PDF},
  groups     = {Ensemble Diversity},
  publisher  = {AI Access Foundation},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Wang2009,
  author     = {Wang, Shuo and Yao, Xin},
  booktitle  = {2009 IEEE Symposium on Computational Intelligence and Data Mining},
  title      = {Diversity analysis on imbalanced data sets by using ensemble models},
  year       = {2009},
  pages      = {324-331},
  comment    = {SMOTEBagging
Why smaller resampling rate results in more diverse ensemble system},
  doi        = {10.1109/CIDM.2009.4938667},
  file       = {:2009Wang - Diversity Analysis on Imbalanced Data Sets by Using Ensemble Models.pdf:PDF},
  groups     = {Ensemble Diversity},
  keywords   = {Data analysis,Voting,Bagging,Costs,Medical diagnosis,Text categorization,Intrusion detection,Semisupervised learning,Predictive models,Boosting},
  priority   = {prio1},
  ranking    = {rank5},
  readstatus = {read},
}

@InProceedings{Yongqing2013,
  author     = {Zhang Yongqing and Zhu Min and Zhang Danling and Mi Gang and Ma Daichuan},
  booktitle  = {IEEE Conference Anthology},
  title      = {Improved SMOTEBagging and its application in imbalanced data classification},
  year       = {2013},
  pages      = {1-5},
  comment    = {Inproved version of SMOTEBagging [Wang2009]},
  doi        = {10.1109/ANTHOLOGY.2013.6784957},
  file       = {:2013Yongqing - Improved SMOTEBagging and Its Application in Imbalanced Data Classification.pdf:PDF},
  groups     = {Ensemble Diversity},
  keywords   = {Tin,Bioinformatics,Classification algorithms,Proteins,Support vector machine classification,SMOTE,Bagging,SVM,Imbalanced Datasets},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@Article{Hido2009,
  author     = {Hido, Shohei and Kashima, Hisashi and Takahashi, Yutaka},
  journal    = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  title      = {Roughly balanced bagging for imbalanced data},
  year       = {2009},
  number     = {5-6},
  pages      = {412-426},
  volume     = {2},
  abstract   = {Abstract The class imbalance problem appears in many real-world applications of classification learning. We propose an ensemble algorithm “Roughly Balanced (RB) Bagging” using a novel sampling technique to improve the original bagging algorithm for data sets with skewed class distributions. For this sampling method, the number of samples in the largest and smallest classes are different, but they are effectively balanced when averaged over all of the subsets, which supports the approach of bagging in a more appropriate way. Individual models in RB Bagging tend to show larger diversity, which is one of the keys of ensemble models, compared with existing bagging-based methods for imbalanced data that use exactly the same number of majority and minority examples for every training subset. In addition, the proposed method makes full use of all of the minority examples by under-sampling, which is efficiently done by using negative binomial distributions. Numerical experiments using benchmark and real-world data sets demonstrate that RB Bagging shows better performance than the existing “balanced” methods and other common methods for area under the ROC curve (AUC), which is a widely used metric in the class imbalance problem. Copyright © 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 2: 412-426, 2009},
  doi        = {https://doi.org/10.1002/sam.10061},
  eprint     = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sam.10061},
  file       = {:2009Hido - Roughly Balanced Bagging for Imbalanced Data.pdf:PDF},
  groups     = {Ensemble Diversity},
  keywords   = {imbalanced data, bagging, resampling, negative binomial distribution},
  readstatus = {skimmed},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sam.10061},
}

@InProceedings{Li2007,
  author     = {Li, Cen},
  booktitle  = {Proceedings of the 45th Annual Southeast Regional Conference},
  title      = {Classifying imbalanced data using a bagging ensemble variation (BEV)},
  year       = {2007},
  address    = {New York, NY, USA},
  month      = mar,
  pages      = {203–208},
  publisher  = {Association for Computing Machinery},
  series     = {ACM-SE 45},
  abstract   = {In many applications, data collected are highly skewed where data of one class clearly dominates data from the other classes. Most existing classification systems that perform well on balanced data give very poor performance on imbalanced data, especially for the minority class data. Existing work on improving the quality of classification on imbalanced data include over-sampling, under-sampling, and methods that make modifications to the existing classification systems. This paper discusses the BEV system for classifying imbalanced data. The system is developed based on the ideas from the "Bagging" classification ensemble. The motivation behind the scheme is to maximally use the minority class data without creating synthetic data or making changes to the existing classification systems. Experimental results using real world imbalanced data show the effectiveness of the system.},
  collection = {ACM SE07},
  doi        = {10.1145/1233341.1233378},
  file       = {:2007MarchLi - Classifying Imbalanced Data Using a Bagging Ensemble Variation (BEV).pdf:PDF},
  groups     = {Ensemble Diversity},
  isbn       = {9781595936295},
  keywords   = {support vector machine, machine learning, imbalanced data, decision tree, classification},
  location   = {Winston-Salem, North Carolina},
  numpages   = {6},
  ranking    = {rank1},
  readstatus = {skimmed},
  url        = {https://doi.org/10.1145/1233341.1233378},
}

@InProceedings{Krogh1994,
  author    = {Krogh, Anders and Vedelsby, Jesper},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Neural Network Ensembles, Cross Validation, and Active Learning},
  year      = {1994},
  editor    = {G. Tesauro and D. Touretzky and T. Leen},
  publisher = {MIT Press},
  volume    = {7},
  comment   = {强调了diversity 的重要性},
  file      = {:1994Krogh - Neural Network Ensembles, Cross Validation, and Active Learning.pdf:PDF},
  groups    = {Ensemble Diversity},
  ranking   = {rank2},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1994/file/b8c37e33defde51cf91e1e03e51657da-Paper.pdf},
}

@Misc{Ortega2022,
  author        = {Luis A. Ortega and Rafael Cabañas and Andrés R. Masegosa},
  title         = {Diversity and Generalization in Neural Network Ensembles},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2110.13786},
  file          = {:2022Ortega - Diversity and Generalization in Neural Network Ensembles.pdf:PDF},
  groups        = {Ensemble Diversity},
  primaryclass  = {cs.LG},
  priority      = {prio2},
}

@Article{Jan2019,
  author   = {Jan, Muhammad Zohaib and Verma, Brijesh},
  journal  = {IEEE Access},
  title    = {A Novel Diversity Measure and Classifier Selection Approach for Generating Ensemble Classifiers},
  year     = {2019},
  pages    = {156360-156373},
  volume   = {7},
  doi      = {10.1109/ACCESS.2019.2949059},
  file     = {:2019Jan - A Novel Diversity Measure and Classifier Selection Approach for Generating Ensemble Classifiers.pdf:PDF},
  groups   = {Ensemble Diversity},
  keywords = {Diversity reception,Feature extraction,Training,Bagging,Optimization,Current measurement,Benchmark testing,Ensemble classifiers,neural networks,multiple classifiers learning,diversity measures},
  priority = {prio2},
}

@Misc{Li2020,
  author        = {Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
  title         = {On the Convergence of FedAvg on Non-{IID} Data},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {1907.02189},
  file          = {:2020Li - On the Convergence of FedAvg on Non IID Data.pdf:PDF},
  groups        = {FL Research},
  primaryclass  = {stat.ML},
  ranking       = {rank5},
  url           = {https://arxiv.org/abs/1907.02189},
}

@Article{Haque2016,
  author    = {Haque, Mohammad Nazmul AND Noman, Nasimul AND Berretta, Regina AND Moscato, Pablo},
  journal   = {PLOS ONE},
  title     = {Heterogeneous Ensemble Combination Search Using Genetic Algorithm for Class Imbalanced Data Classification},
  year      = {2016},
  month     = {01},
  number    = {1},
  pages     = {1-28},
  volume    = {11},
  abstract  = {Classification of datasets with imbalanced sample distributions has always been a challenge. In general, a popular approach for enhancing classification performance is the construction of an ensemble of classifiers. However, the performance of an ensemble is dependent on the choice of constituent base classifiers. Therefore, we propose a genetic algorithm-based search method for finding the optimum combination from a pool of base classifiers to form a heterogeneous ensemble. The algorithm, called GA-EoC, utilises 10 fold-cross validation on training data for evaluating the quality of each candidate ensembles. In order to combine the base classifiers decision into ensemble’s output, we used the simple and widely used majority voting approach. The proposed algorithm, along with the random sub-sampling approach to balance the class distribution, has been used for classifying class-imbalanced datasets. Additionally, if a feature set was not available, we used the (α, β) − k Feature Set method to select a better subset of features for classification. We have tested GA-EoC with three benchmarking datasets from the UCI-Machine Learning repository, one Alzheimer’s disease dataset and a subset of the PubFig database of Columbia University. In general, the performance of the proposed method on the chosen datasets is robust and better than that of the constituent base classifiers and many other well-known ensembles. Based on our empirical study we claim that a genetic algorithm is a superior and reliable approach to heterogeneous ensemble construction and we expect that the proposed GA-EoC would perform consistently in other cases.},
  comment   = {Matthews Correlation Coefficient (MCC) as fitness function},
  doi       = {10.1371/journal.pone.0146116},
  file      = {:201601Haque - Heterogeneous Ensemble Combination Search Using Genetic Algorithm for Class Imbalanced Data Classification.pdf:PDF},
  groups    = {Ensemble Diversity},
  publisher = {Public Library of Science},
  ranking   = {rank1},
  url       = {https://doi.org/10.1371/journal.pone.0146116},
}

 
@Article{GhaderiZefrehi2020,
  author     = {Ghaderi Zefrehi, Hossein and Altınçay, Hakan},
  journal    = {Expert Systems with Applications},
  title      = {Imbalance learning using heterogeneous ensembles},
  year       = {2020},
  issn       = {0957-4174},
  month      = mar,
  pages      = {113005},
  volume     = {142},
  doi        = {10.1016/j.eswa.2019.113005},
  file       = {:2020MarchGhaderi Zefrehi - Imbalance Learning Using Heterogeneous Ensembles.pdf:PDF},
  groups     = {Imbalance Learning},
  priority   = {prio3},
  publisher  = {Elsevier BV},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@Article{Galar2012,
  author     = {Galar, Mikel and Fernandez, Alberto and Barrenechea, Edurne and Bustince, Humberto and Herrera, Francisco},
  journal    = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  title      = {A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches},
  year       = {2012},
  number     = {4},
  pages      = {463-484},
  volume     = {42},
  doi        = {10.1109/TSMCC.2011.2161285},
  file       = {:2012Galar - A Review on Ensembles for the Class Imbalance Problem_ Bagging , Boosting , and Hybrid Based Approaches.pdf:PDF},
  groups     = {Imbalance Learning},
  keywords   = {Learning systems,Accuracy,Training,Proposals,Noise,Algorithm design and analysis,Bagging,Bagging,boosting,class distribution,classification,ensembles,imbalanced data-sets,multiple classifier systems},
  priority   = {prio2},
  readstatus = {skimmed},
}

@Article{Rezvani2023,
  author   = {Salim Rezvani and Xizhao Wang},
  journal  = {Applied Soft Computing},
  title    = {A broad review on class imbalance learning techniques},
  year     = {2023},
  issn     = {1568-4946},
  pages    = {110415},
  volume   = {143},
  abstract = {The imbalanced learning issue is related to the performance of learning algorithms in the presence of asymmetrical class distribution. Due to the complex characteristics of imbalanced datasets, learning from such data need new algorithms and understandings to convert efficient large amounts of initial data into suitable datasets. Although several review papers can be found about imbalanced classification problems, none of them contributed an in-depth review of SVM for imbalanced classification problems. To fill this gap, we present an exhaustive review of existing methods to deal with issues linked with class imbalance learning. The majority of the existing survey addresses only classification tasks. We also describe methods to deal with similar problems in regression tasks. A new taxonomy for class imbalanced learning techniques is proposed and classified into three parts: (1) Data pre-processing, (2) Algorithmic structures, and (3) Hybrid techniques. The advantages and disadvantages of each type of imbalanced learning technique are discussed. Moreover, we explain the main difficulties in distributions of imbalanced datasets and discuss the main approaches that have been proposed to tackle these issues. Finally, to stimulate the next research in this area, we emphasize the main opportunities and challenges, which can be useful in research directions for learning algorithms from imbalanced data.},
  doi      = {https://doi.org/10.1016/j.asoc.2023.110415},
  file     = {:2023Rezvani - A Broad Review on Class Imbalance Learning Techniques.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Algorithmic structures techniques, Data pre-processing techniques, Hybrid techniques, Imbalanced learning, Support vector machine},
  ranking  = {rank1},
  url      = {https://www.sciencedirect.com/science/article/pii/S1568494623004337},
}

@Article{Sun2015,
  author   = {Zhongbin Sun and Qinbao Song and Xiaoyan Zhu and Heli Sun and Baowen Xu and Yuming Zhou},
  journal  = {Pattern Recognition},
  title    = {A novel ensemble method for classifying imbalanced data},
  year     = {2015},
  issn     = {0031-3203},
  number   = {5},
  pages    = {1623-1637},
  volume   = {48},
  abstract = {The class imbalance problems have been reported to severely hinder classification performance of many standard learning algorithms, and have attracted a great deal of attention from researchers of different fields. Therefore, a number of methods, such as sampling methods, cost-sensitive learning methods, and bagging and boosting based ensemble methods, have been proposed to solve these problems. However, these conventional class imbalance handling methods might suffer from the loss of potentially useful information, unexpected mistakes or increasing the likelihood of overfitting because they may alter the original data distribution. Thus we propose a novel ensemble method, which firstly converts an imbalanced data set into multiple balanced ones and then builds a number of classifiers on these multiple data with a specific classification algorithm. Finally, the classification results of these classifiers for new data are combined by a specific ensemble rule. In the empirical study, different class imbalance data handling methods including three conventional sampling methods, one cost-sensitive learning method, six Bagging and Boosting based ensemble methods, our previous method EM1vs1 and two fuzzy-rule based classification methods were compared with our method. The experimental results on 46 imbalanced data sets show that our proposed method is usually superior to the conventional imbalance data handling methods when solving the highly imbalanced problems.},
  doi      = {https://doi.org/10.1016/j.patcog.2014.11.014},
  file     = {:2015Sun - A Novel Ensemble Method for Classifying Imbalanced Data.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Imbalanced data, Classification, Ensemble learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320314004841},
}

@InProceedings{Liang2011,
  author     = {Liang, Guohua and Zhu, Xingquan and Zhang, Chengqi},
  booktitle  = {AI 2011: Advances in Artificial Intelligence},
  title      = {An Empirical Study of Bagging Predictors for Imbalanced Data with Different Levels of Class Distribution},
  year       = {2011},
  address    = {Berlin, Heidelberg},
  editor     = {Wang, Dianhui and Reynolds, Mark},
  pages      = {213--222},
  publisher  = {Springer Berlin Heidelberg},
  abstract   = {Research into learning from imbalanced data has increasingly captured the attention of both academia and industry, especially when the class distribution is highly skewed. This paper compares the Area Under the Receiver Operating Characteristic Curve (AUC) performance of bagging in the context of learning from different imbalanced levels of class distribution. Despite the popularity of bagging in many real-world applications, some questions have not been clearly answered in the existing research, e.g., which bagging predictors may achieve the best performance for applications, and whether bagging is superior to single learners when the levels of class distribution change. We perform a comprehensive evaluation of the AUC performance of bagging predictors with 12 base learners at different imbalanced levels of class distribution by using a sampling technique on 14 imbalanced data-sets. Our experimental results indicate that Decision Table (DTable) and RepTree are the learning algorithms with the best bagging AUC performance. Most AUC performances of bagging predictors are statistically superior to single learners, except for Support Vector Machines (SVM) and Decision Stump (DStump).},
  doi        = {10.1007/978-3-642-25832-9_22},
  file       = {:2011Liang - An Empirical Study of Bagging Predictors for Imbalanced Data with Different Levels of Class Distribution.pdf:PDF},
  groups     = {Imbalance Learning},
  isbn       = {978-3-642-25832-9},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{He2009,
  author   = {He, Haibo and Garcia, Edwardo A.},
  journal  = {IEEE Trans. Knowl. Data Eng.},
  title    = {Learning from Imbalanced Data},
  year     = {2009},
  month    = sep,
  number   = {9},
  pages    = {1263-1284},
  volume   = {21},
  doi      = {10.1109/TKDE.2008.239},
  file     = {:2009He - Learning from Imbalanced Data.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Availability,Large-scale systems,Surveillance,Data security,IP networks,Finance,Data analysis,Decision making,Data engineering,Knowledge representation,Imbalanced learning,classification,sampling methods,cost-sensitive learning,kernel-based learning,active learning,assessment metrics.},
  priority = {prio3},
}

@Article{Buehlmann2002,
  author    = {Peter B{\"u}hlmann and Bin Yu},
  journal   = {The Annals of Statistics},
  title     = {{Analyzing bagging}},
  year      = {2002},
  number    = {4},
  pages     = {927 -- 961},
  volume    = {30},
  doi       = {10.1214/aos/1031689014},
  file      = {:2002Bühlmann - Analyzing Bagging.pdf:PDF},
  groups    = {Ensemble Learning},
  keywords  = {bootstrap, ‎classification‎, decision tree, MARS, Model selection, multiple predictions, Nonparametric regression},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aos/1031689014},
}

@InProceedings{Blaszczynski2010,
  author    = {B{\l}aszczy{\'{n}}ski, Jerzy and Deckert, Magdalena and Stefanowski, Jerzy and Wilk, Szymon},
  booktitle = {Rough Sets and Current Trends in Computing},
  title     = {Integrating Selective Pre-processing of Imbalanced Data with Ivotes Ensemble},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {Szczuka, Marcin and Kryszkiewicz, Marzena and Ramanna, Sheela and Jensen, Richard and Hu, Qinghua},
  pages     = {148--157},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {In the paper we present a new framework for improving classifiers learned from imbalanced data. This framework integrates the SPIDER method for selective data pre-processing with the Ivotes ensemble. The goal of such integration is to obtain improved balance between the sensitivity and specificity for the minority class in comparison to a single classifier combined with SPIDER, and to keep overall accuracy on a similar level. The IIvotes framework was evaluated in a series of experiments, in which we tested its performance with two types of component classifiers (tree- and rule-based). The results show that IIvotes improves evaluation measures. They demonstrated advantages of the abstaining mechanism (i.e., refraining from predictions by component classifiers) in IIvotes rule ensembles.},
  comment   = {IIvote： a combination of arcing and SPIDER technique

when a sample is selected randomly, it is processed by SPIDER before being fed in the base classifier for training. SPIDER could help classifier focus more on minority classes (why?)},
  file      = {:2010Błaszczyński - Integrating Selective Pre Processing of Imbalanced Data with Ivotes Ensemble.pdf:PDF},
  groups    = {Imbalance Learning},
  isbn      = {978-3-642-13529-3},
  ranking   = {rank3},
}

@InProceedings{Ueda1996,
  author    = {Ueda, N. and Nakano, R.},
  booktitle = {Proceedings of International Conference on Neural Networks (ICNN'96)},
  title     = {Generalization error of ensemble estimators},
  year      = {1996},
  pages     = {90-95 vol.1},
  volume    = {1},
  comment   = {bias variance decomposition of generalization error of ensemble estimators},
  doi       = {10.1109/ICNN.1996.548872},
  file      = {:1996Ueda - Generalization Error of Ensemble Estimators.pdf:PDF},
  groups    = {Ensemble Learning},
  keywords  = {Zinc,Neural networks,Feedforward neural networks,Analytical models,Laboratories,Genetic expression,Learning systems,Pattern classification,Additive noise,Random sequences},
}

 
@Article{Liang2012,
  author     = {Liang, Guohua and Zhu, Xingquan and Zhang, Chengqi},
  journal    = {International Journal of Machine Learning and Cybernetics},
  title      = {The effect of varying levels of class distribution on bagging for different algorithms: An empirical study},
  year       = {2012},
  issn       = {1868-808X},
  month      = nov,
  number     = {1},
  pages      = {63--71},
  volume     = {5},
  comment    = {和Liang2011相比多了ROC图},
  doi        = {10.1007/s13042-012-0125-5},
  file       = {:2012NovemberLiang - The Effect of Varying Levels of Class Distribution on Bagging for Different Algorithms_ an Empirical Study.pdf:PDF},
  groups     = {Imbalance Learning},
  priority   = {prio2},
  publisher  = {Springer Science and Business Media LLC},
  ranking    = {rank3},
  readstatus = {read},
}

@PhdThesis{Wang2011,
  author   = {Wang, Shuo},
  school   = {University of Birmingham},
  title    = {Ensemble diversity for class imbalance learning},
  year     = {2011},
  file     = {:2011Wang - Ensemble Diversity for Class Imbalance Learning.pdf:PDF},
  groups   = {Imbalance Learning, Ensemble Diversity},
  priority = {prio2},
  ranking  = {rank4},
}

@Article{Wang2013,
  author   = {Wang, Shuo and Yao, Xin},
  journal  = {IEEE Transactions on Knowledge and Data Engineering},
  title    = {Relationships between Diversity of Classification Ensembles and Single-Class Performance Measures},
  year     = {2013},
  number   = {1},
  pages    = {206-219},
  volume   = {25},
  doi      = {10.1109/TKDE.2011.207},
  file     = {:2013Wang - Relationships between Diversity of Classification Ensembles and Single Class Performance Measures.pdf:PDF},
  groups   = {Imbalance Learning, Ensemble Diversity},
  keywords = {Learning systems,Diversity reception,Correlation,Data mining,Pattern analysis,Class imbalance learning,ensemble learning,diversity,single-class performance measures,data mining},
  priority = {prio2},
  ranking  = {rank4},
}

@InProceedings{Burnaev2015,
  author       = {E. Burnaev and P. Erofeev and A. Papanov},
  booktitle    = {Eighth International Conference on Machine Vision (ICMV 2015)},
  title        = {{Influence of resampling on accuracy of imbalanced classification}},
  year         = {2015},
  editor       = {Antanas Verikas and Petia Radeva and Dmitry Nikolaev},
  organization = {International Society for Optics and Photonics},
  pages        = {987521},
  publisher    = {SPIE},
  volume       = {9875},
  doi          = {10.1117/12.2228523},
  file         = {:2015Burnaev - Influence of Resampling on Accuracy of Imbalanced Classification.pdf:PDF},
  groups       = {Imbalance Learning},
  keywords     = {Binary Classification, Class Imbalance, Resampling, Bootstrap Oversampling, Random Undersampling, SMOTE},
  url          = {https://doi.org/10.1117/12.2228523},
}

@InProceedings{Burnaev2021,
  author    = {Burnaev, Evgeny},
  booktitle = {Recent Developments in Stochastic Methods and Applications},
  title     = {Generalization Bound for Imbalanced Classification},
  year      = {2021},
  address   = {Cham},
  editor    = {Shiryaev, Albert N. and Samouylov, Konstantin E. and Kozyrev, Dmitry V.},
  pages     = {107--119},
  publisher = {Springer International Publishing},
  abstract  = {The oversampling approach is often used for binary imbalanced classification. We demonstrate that the approach can be interpreted as the weighted classification and derived a generalization bound for it. The bound can be used for more accurate re-balancing of classes. Results of computational experiments support the theoretical estimate of the optimal weighting.},
  file      = {:2021Burnaev - Generalization Bound for Imbalanced Classification.pdf:PDF},
  groups    = {Imbalance Learning},
  isbn      = {978-3-030-83266-7},
  ranking   = {rank3},
}

@Article{DiezPastor2015a,
  author   = {José F. Díez-Pastor and Juan J. Rodríguez and César I. García-Osorio and Ludmila I. Kuncheva},
  journal  = {Information Sciences},
  title    = {Diversity techniques improve the performance of the best imbalance learning ensembles},
  year     = {2015},
  issn     = {0020-0255},
  pages    = {98-117},
  volume   = {325},
  abstract = {Many real-life problems can be described as unbalanced, where the number of instances belonging to one of the classes is much larger than the numbers in other classes. Examples are spam detection, credit card fraud detection or medical diagnosis. Ensembles of classifiers have acquired popularity in this kind of problems for their ability to obtain better results than individual classifiers. The most commonly used techniques by those ensembles especially designed to deal with imbalanced problems are for example Re-weighting, Oversampling and Undersampling. Other techniques, originally intended to increase the ensemble diversity, have not been systematically studied for their effect on imbalanced problems. Among these are Random Oracles, Disturbing Neighbors, Random Feature Weights or Rotation Forest. This paper presents an overview and an experimental study of various ensemble-based methods for imbalanced problems, the methods have been tested in its original form and in conjunction with several diversity-increasing techniques, using 84 imbalanced data sets from two well known repositories. This paper shows that these diversity-increasing techniques significantly improve the performance of ensemble methods for imbalanced problems and provides some ideas about when it is more convenient to use these diversifying techniques.},
  doi      = {https://doi.org/10.1016/j.ins.2015.07.025},
  file     = {:2015Díez-Pastor - Diversity Techniques Improve the Performance of the Best Imbalance Learning Ensembles.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Classifier ensembles, Imbalanced data sets, SMOTE, Undersampling, Rotation forest, Diversity},
  ranking  = {rank3},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025515005186},
}

@Article{Roshan2020,
  author   = {Seyed Ehsan Roshan and Shahrokh Asadi},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {Improvement of Bagging performance for classification of imbalanced datasets using evolutionary multi-objective optimization},
  year     = {2020},
  issn     = {0952-1976},
  pages    = {103319},
  volume   = {87},
  abstract = {Today, classification of imbalanced datasets, in which the samples belonging to one class is more than the samples pertaining to other classes, has been paid much attention owing to its vast application in real-world problems. Bagging ensemble method, as one of the most favorite ensemble learning algorithms can provide better performance in solving imbalanced problems when is incorporated with undersampling methods. In Bagging method, diversity of classifiers, performance of classifiers, appropriate number of bags (classifiers) and balanced training sets to train the classifiers are important factors in successfulness of Bagging so as to deal with imbalanced problems. In this paper, through inspiring of evolutionary undersampling (the new undersampling method for seeking the subsets of majority class samples) and taking the mentioned factors into account, i.e., diversity, performance of classifiers, number of classifiers and balanced training set, a multi-objective optimization undersampling is proposed. The proposed method uses multi-objective evolutionary to produce set of diverse, well-performing and (near) balanced bags. Accordingly, the proposed method provides the possibility of generating diverse and well-performing classifiers and determining the number of classifiers in Bagging algorithm. Moreover, two different strategies are employed in the proposed method so as to improve the diversity. In order to confirm the proposed method’s efficiency, its performance is measured over 33 imbalanced datasets using AUC and then compared with 6 well-known ensemble learning algorithms. Investigating the obtained results of such comparisons using non-parametric statistical analysis demonstrate the dominancy of the proposed method compared to other employed techniques, as well.},
  comment  = {解释了ensemble learning方法不适用于非平衡数据集的原因},
  doi      = {https://doi.org/10.1016/j.engappai.2019.103319},
  file     = {:2020Roshan - Improvement of Bagging Performance for Classification of Imbalanced Datasets Using Evolutionary Multi Objective Optimization.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Multi-objective evolutionary, Imbalanced datasets, Ensemble learning, Bagging, Undersampling, Diversity},
  url      = {https://www.sciencedirect.com/science/article/pii/S0952197619302714},
}

@InProceedings{Piras2021,
  author    = {Piras, Luca and Boratto, Ludovico and Ramos, Guilherme},
  booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  title     = {Evaluating the Prediction Bias Induced by Label Imbalance in Multi-label Classification},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {3368–3372},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '21},
  abstract  = {Prediction bias is a well-known problem in classification algorithms, which tend to be skewed towards more represented classes. This phenomenon is even more remarkable in multi-label scenarios, where the number of underrepresented classes is usually larger. In light of this, we hereby present the Prediction Bias Coefficient (PBC), a novel measure that aims to assess the bias induced by label imbalance in multi-label classification. The approach leverages Spearman's rank correlation coefficient between the label frequencies and the F-scores obtained for each label individually. After describing the theoretical properties of the proposed indicator, we illustrate its behaviour on a classification task performed with state-of-the-art methods on two real-world datasets, and we compare it experimentally with other metrics described in the literature.},
  doi       = {10.1145/3459637.3482100},
  file      = {:2021Piras - Evaluating the Prediction Bias Induced by Label Imbalance in Multi Label Classification.pdf:PDF},
  groups    = {Skewness Representation},
  isbn      = {9781450384469},
  keywords  = {multi-label classification, imbalance, evaluation, classification bias},
  location  = {Virtual Event, Queensland, Australia},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3459637.3482100},
}

@Article{Mortaz2020,
  author   = {Ebrahim Mortaz},
  journal  = {Knowledge-Based Systems},
  title    = {Imbalance accuracy metric for model selection in multi-class imbalance classification problems},
  year     = {2020},
  issn     = {0950-7051},
  pages    = {106490},
  volume   = {210},
  abstract = {The overall accuracy, macro precision, macro recall, F-score and class balance accuracy, due to their simplicity and easy interpretation, have been among the most popular metrics to measure the performance of classifiers on multi-class problems. However, on imbalance datasets, some of these metrics can be unfairly influenced by heavier classes. Therefore, it is recommended that they are used as a group and not individually. This strategy can unnecessarily complicate the model selection and evaluation in imbalance datasets. In this paper, we introduce a new metric, imbalance accuracy metric (IAM), that can be used as a solo measure for model evaluation and selection. The IAM is built up on top of the existing metrics, is simple to use, and easy to interpret. This metric is meant to be used as a bottom-line measure aiming to eliminate the need for group metric computation and simplify the model selection.},
  comment  = {介绍了多种适用于多分类任务的指标；设计了 imbalance accuracy metric (IAM)},
  doi      = {https://doi.org/10.1016/j.knosys.2020.106490},
  file     = {:2020Mortaz - Imbalance Accuracy Metric for Model Selection in Multi Class Imbalance Classification Problems.pdf:PDF},
  groups   = {Skewness Representation},
  keywords = {Machine learning, Classification accuracy, Multi-class problems, Imbalance datasets, Knowledge discovery},
  priority = {prio3},
  ranking  = {rank4},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705120306195},
}

@Article{Tarekegn2021,
  author   = {Adane Nega Tarekegn and Mario Giacobini and Krzysztof Michalak},
  journal  = {Pattern Recogn.},
  title    = {A review of methods for imbalanced multi-label classification},
  year     = {2021},
  issn     = {0031-3203},
  month    = oct,
  pages    = {107965},
  volume   = {118},
  abstract = {Multi-Label Classification (MLC) is an extension of the standard single-label classification where each data instance is associated with several labels simultaneously. MLC has gained much importance in recent years due to its wide range of application domains. However, the class imbalance problem has become an inherent characteristic of many multi-label datasets, where the samples and their corresponding labels are non-uniformly distributed over the data space. The imbalanced problem in MLC imposes challenges to multi-label data analytics which can be viewed from three perspectives: imbalance within labels, among labels, and label-sets. In this paper, we provide a review of the approaches for handling the imbalance problem in multi-label data by collecting the existing research work. As the first systematic study of approaches addressing an imbalanced problem in MLC, this paper provides a comprehensive survey of the state-of-the-art methods for imbalanced MLC, including the characteristics of imbalanced multi-label datasets, evaluation measures and comparative analysis of the proposed methods. The study also discusses important results reported so far in the literature and highlights some of their strengths and limitations to guide future research.},
  doi      = {https://doi.org/10.1016/j.patcog.2021.107965},
  file     = {:2021Tarekegn - A Review of Methods for Imbalanced Multi Label Classification.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Imbalanced Data, Multi-label Classification, Imbalanced Classification, Machine learning, Imbalanced Approaches, Review on Imbalanced Classification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320321001527},
}

@Article{Wang2012,
  author   = {Wang, Shuo and Yao, Xin},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title    = {Multiclass Imbalance Problems: Analysis and Potential Solutions},
  year     = {2012},
  number   = {4},
  pages    = {1119-1130},
  volume   = {42},
  doi      = {10.1109/TSMCB.2012.2187280},
  file     = {:2012Wang - Multiclass Imbalance Problems_ Analysis and Potential Solutions.pdf:PDF},
  groups   = {Skewness Representation},
  keywords = {Training,Correlation,Training data,Pattern analysis,Genetic algorithms,IEEE Potentials,Cybernetics,Boosting,diversity,ensemble learning,multiclass imbalance problems,negative correlation learning},
  ranking  = {rank3},
}

@InProceedings{Shokri2015,
  author    = {Shokri, Reza and Shmatikov, Vitaly},
  booktitle = {2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  title     = {Privacy-preserving deep learning},
  year      = {2015},
  pages     = {909-910},
  doi       = {10.1109/ALLERTON.2015.7447103},
  file      = {:2015Shokri - Privacy Preserving Deep Learning.pdf:PDF},
  groups    = {FL Research},
  keywords  = {Training,Privacy,Machine learning,Data models,Decision support systems,Companies,Data privacy},
}

@InProceedings{Karimireddy2020,
  author    = {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  title     = {{SCAFFOLD}: Stochastic Controlled Averaging for Federated Learning},
  year      = {2020},
  editor    = {III, Hal Daumé and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {5132--5143},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client’s data results in a ‘drift’ in the local updates resulting in poor performance. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the ‘client drift’. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client’s data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.},
  file      = {:202013--18 JulKarimireddy - SCAFFOLD_ Stochastic Controlled Averaging for Federated Learning.pdf:PDF},
  groups    = {FL & Non-iid},
  pdf       = {http://proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf},
  priority  = {prio2},
  ranking   = {rank4},
  url       = {https://proceedings.mlr.press/v119/karimireddy20a.html},
}

@Article{Tan2023,
  author   = {Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Towards Personalized Federated Learning},
  year     = {2023},
  number   = {12},
  pages    = {9587-9603},
  volume   = {34},
  doi      = {10.1109/TNNLS.2022.3160699},
  file     = {:2023Tan - Towards Personalized Federated Learning.pdf:PDF},
  groups   = {Personalized FL},
  keywords = {Data models,Training,Adaptation models,Collaborative work,Data privacy,Servers,Edge computing,Federated learning,Edge computing,federated learning (FL),non-IID data,personalized FL (PFL),privacy preservation,statistical heterogeneity},
}

@Article{Zhou2024,
  author   = {Xiaokang Zhou and Wang Huang and Wei Liang and Zheng Yan and Jianhua Ma and Yi Pan and Kevin {I-Kai Wang}},
  journal  = {Information Sciences},
  title    = {Federated distillation and blockchain empowered secure knowledge sharing for Internet of medical Things},
  year     = {2024},
  issn     = {0020-0255},
  pages    = {120217},
  volume   = {662},
  abstract = {With the development of Internet of Things (IoT) and Artificial Intelligence (AI) technologies, smart services have penetrated into every aspect of our daily lives, including the medical treatment and healthcare fields. However, due to security and privacy issues, medical data cannot be easily shared, which may lead to the situation of the so-called data silos. Challenges in existing approaches when building medical data sharing models can be summarized as: i) It is very challenging to ensure that the privacy of the medical data is protected and to identify the ownership of medical data; ii) Their models always result in poor performance or have the problem of excessive communication overhead due to the large amount of model parameters; iii) The current scenario of combining federated learning and blockchain generally ignores the load on nodes, which may easily lead to a lack of efficiency and fairness during the consensus process. In this study, we propose a Federated Distillation and Blockchain empowered Secure Knowledge Sharing (FDBC-SKS) model, which transforms the data sharing problem into a collaborative model knowledge sharing problem, aiming to provide a lightweight distributed deep learning framework in Internet of Medical Things (IoMT) environments. A peer-to-peer federated distillation mechanism is designed to enable a decentralized federated learning with better model flexibility and less communication consumption, based on the better knowledge utilization from each local model. A reinforcement learning enhanced consensus mechanism for blockchain is devised to improve the model convergence performance, alleviate the problem of low computational efficiency, while enhancing the fairness in terms of node load balancing during the node selection and block generation process. Experiment and evaluation results based on two real-world datasets demonstrate the usefulness of our proposed model toward secure and effective data sharing in IoMT oriented smart application development compared with other similar methods.},
  comment  = {the following contributions have been made:

1. Propose a peer-to-peer knowledge distillation scheme. The requester receives the soft labels from assistants, which is then averaged and used in knowledge distillation. There is no need to share the local models -- only the soft labels are necessary.
2. Consensus nodes selecting scheme: A reinforcement learning scheme which selects consensus nodes based on the load of nodes(calculated by CPU cycles and frequency).

However, the deficiencies exists:

1. Though not mentioned explicitly in design, the consensus scheme is devised in a consortium blockchain
2. The description of the knowledge distillation is vague and sometime errornous.},
  doi      = {https://doi.org/10.1016/j.ins.2024.120217},
  file     = {:2024Zhou - Federated Distillation and Blockchain Empowered Secure Knowledge Sharing for Internet of Medical Things.pdf:PDF},
  groups   = {Knowledge Distillation},
  keywords = {Federated learning, Blockchain, Knowledge distillation, Reinforcement learning, Knowledge sharing, Internet of medical things},
  ranking  = {rank2},
  url      = {https://www.sciencedirect.com/science/article/pii/S0020025524001300},
}

@Misc{Hu2021,
  author        = {Yifan Hu and Yuhang Zhou and Jun Xiao and Chao Wu},
  title         = {GFL: A Decentralized Federated Learning Framework Based On Blockchain},
  year          = {2021},
  archiveprefix = {arXiv},
  comment       = {A ring Decentralized FL Topology is assumed.
untrusted node send local models to trusted models
trusted nodes distill knowledge from received local models
trusted node generate global models with FedAvg

all transmission occurs in clockwise direction},
  eprint        = {2010.10996},
  file          = {:2021Hu - GFL_ a Decentralized Federated Learning Framework Based on Blockchain.pdf:PDF},
  groups        = {Knowledge Distillation},
  primaryclass  = {cs.LG},
}

@InProceedings{Li2022a,
  author    = {Li, Ye and Zhang, Jiale and Zhu, Junwu and Li, Wenjuan},
  booktitle = {Emerging Information Security and Applications},
  title     = {HBMD-FL: Heterogeneous Federated Learning Algorithm Based on Blockchain and Model Distillation},
  year      = {2022},
  address   = {Cham},
  editor    = {Chen, Jiageng and He, Debiao and Lu, Rongxing},
  pages     = {145--159},
  publisher = {Springer Nature Switzerland},
  abstract  = {Federated learning is a distributed machine learning framework that allows participants to keep their privacy data locally. Traditional federated learning coordinates participants collaboratively train a powerful global model. However, this process has several problems: it cannot meet the heterogeneous model's requirements, and it cannot resist poisoning attacks and single-point-of-failure. In order to resolve these issues, we proposed a heterogeneous federated learning algorithm based on blockchain and model distillation. The problem of fully heterogeneous models that are hard to aggregate in the central server can be solved by leveraging model distillation technology. Moreover, blockchain replaces the central server in federated learning to solve the single-point-of-failure problem. The validation algorithm is combined with cross-validation, which helps federated learning to resist poison attacks. The extensive experimental results demonstrate that HBMD-FL can resist poisoning attacks while losing less than 3{\$}{\$}{\backslash}{\%}{\$}{\$}of model accuracy, and the communication consumption significantly outperformed the comparison algorithm.},
  file      = {:2022Li - HBMD FL_ Heterogeneous Federated Learning Algorithm Based On Blockchain And Model Distillation.pdf:PDF},
  groups    = {Knowledge Distillation},
  isbn      = {978-3-031-23098-1},
}

 
@Article{Li2024a,
  author    = {Li, Ye and Zhang, Jiale and Zhu, Junwu and Li, Wenjuan},
  journal   = {Neural Computing and Applications},
  title     = {Blockfd: blockchain-based federated distillation against poisoning attacks},
  year      = {2024},
  issn      = {1433-3058},
  month     = apr,
  doi       = {10.1007/s00521-024-09715-w},
  file      = {:2024AprilLi - Blockfd_ Blockchain Based Federated Distillation against Poisoning Attacks.pdf:PDF},
  groups    = {Knowledge Distillation},
  publisher = {Springer Science and Business Media LLC},
}

@Misc{Jiang2023,
  author        = {Yihan Jiang and Jakub Konečný and Keith Rush and Sreeram Kannan},
  title         = {Improving Federated Learning Personalization via Model Agnostic Meta Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {1909.12488},
  file          = {:2023Jiang - Improving Federated Learning Personalization Via Model Agnostic Meta Learning.pdf:PDF},
  groups        = {Personalized FL},
  primaryclass  = {cs.LG},
}

@Misc{Liang2020,
  author        = {Paul Pu Liang and Terrance Liu and Liu Ziyin and Nicholas B. Allen and Randy P. Auerbach and David Brent and Ruslan Salakhutdinov and Louis-Philippe Morency},
  title         = {Think Locally, Act Globally: Federated Learning with Local and Global Representations},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.01523},
  file          = {:2020Liang - Think Locally, Act Globally_ Federated Learning with Local and Global Representations.pdf:PDF},
  groups        = {Personalized FL},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
}

@Article{Tu2024,
  author     = {Tu, Jingke and Huang, Jiaming and Yang, Lei and Lin, Wanyu},
  journal    = {ACM Trans. Knowl. Discov. Data},
  title      = {Personalized Federated Learning with Layer-Wise Feature Transformation via Meta-Learning},
  year       = {2024},
  issn       = {1556-4681},
  month      = {feb},
  number     = {4},
  volume     = {18},
  abstract   = {Federated learning enables multiple clients to collaboratively learn machine learning models in a privacy-preserving manner. However, in real-world scenarios, a key challenge encountered in federated learning is the statistical heterogeneity among clients. Existing work mainly focused on a single global model shared across the clients, making it hard to generalize well to all clients due to the large discrepancy in the data distributions. To address this challenge, we propose pFedLT, a novel approach that can adapt the single global model to different data distributions. Specifically, we propose to perform a pluggable layer-wise transformation during the local update phase based on scaling and shifting operations. In particular, these operations are learned with a meta-learning strategy. By doing so, pFedLT can capture the diversity of data distribution among clients, therefore, can generalize well even when the data distributions among clients exhibit high statistical heterogeneity. We conduct extensive experiments on synthetic and real-world datasets (MNIST, Fashion_MNIST, CIFAR-10, and Office+Caltech10) under different Non-IID settings. Experimental results demonstrate that pFedLT significantly improves the model accuracy by up to 11.67\% and reduces the communication costs compared with state-of-the-art approaches.},
  address    = {New York, NY, USA},
  articleno  = {99},
  comment    = {Personalized FL: linear transformation of the parameters in the model, the effectiveness is susceptible},
  doi        = {10.1145/3638252},
  file       = {:2024febTu - Personalized Federated Learning with Layer Wise Feature Transformation Via Meta Learning.pdf:PDF},
  groups     = {Personalized FL},
  issue_date = {May 2024},
  keywords   = {Meta-learning, personalized learning, federated learning, edge computing},
  numpages   = {21},
  publisher  = {Association for Computing Machinery},
  ranking    = {rank1},
  url        = {https://doi.org/10.1145/3638252},
}

@InProceedings{Majeed2020,
  author    = {Majeed, Umer},
  booktitle = {한국정보과학회 학술발표논문집},
  title     = {Blockchain-assisted Ensemble Federated Learning for Automatic Modulation Classification in Wireless Networks},
  year      = {2020},
  month     = aug,
  file      = {:202008Majeed - Blockchain Assisted Ensemble Federated Learning for Automatic Modulation Classification in Wireless Networks.pdf:PDF},
  groups    = {Blockchain-based FL},
  priority  = {prio3},
  url       = {https://www.researchgate.net/publication/343809162_Blockchain-assisted_Ensemble_Federated_Learning_for_Automatic_Modulation_Classification_in_Wireless_Networks},
}

@InProceedings{Lin2020,
  author    = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  title     = {Ensemble distillation for robust model fusion in federated learning},
  year      = {2020},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS '20},
  abstract  = {Federated Learning (FL) is a machine learning setting where many devices collab-oratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios.In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.},
  articleno = {198},
  file      = {:2020Lin - Ensemble Distillation for Robust Model Fusion in Federated Learning.pdf:PDF},
  groups    = {Knowledge Distillation},
  isbn      = {9781713829546},
  location  = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
  numpages  = {13},
  ranking   = {rank4},
}

 
@Article{Wu2022,
  author    = {Wu, Chuhan and Wu, Fangzhao and Lyu, Lingjuan and Huang, Yongfeng and Xie, Xing},
  journal   = {Nature Communications},
  title     = {Communication-efficient federated learning via knowledge distillation},
  year      = {2022},
  issn      = {2041-1723},
  month     = apr,
  number    = {1},
  volume    = {13},
  doi       = {10.1038/s41467-022-29763-x},
  file      = {:2022AprilWu - Communication Efficient Federated Learning Via Knowledge Distillation.pdf:PDF},
  groups    = {Knowledge Distillation},
  publisher = {Springer Science and Business Media LLC},
}

 
@Article{Casado2023,
  author    = {Casado, Fernando E. and Lema, Dylan and Iglesias, Roberto and Regueiro, Carlos V. and Barro, Senén},
  journal   = {Machine Learning},
  title     = {Ensemble and continual federated learning for classification tasks},
  year      = {2023},
  issn      = {1573-0565},
  month     = may,
  number    = {9},
  pages     = {3413--3453},
  volume    = {112},
  doi       = {10.1007/s10994-023-06330-z},
  file      = {:2023MayCasado - Ensemble and Continual Federated Learning for Classification Tasks.pdf:PDF},
  groups    = {Federated Ensemble},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Shi2023,
  author   = {Shi, Naichen and Lai, Fan and Kontar, Raed Al and Chowdhury, Mosharaf},
  journal  = {IEEE Trans. Autom. Sci. Eng.},
  title    = {Fed-ensemble: Ensemble Models in Federated Learning for Improved Generalization and Uncertainty Quantification},
  year     = {2023},
  pages    = {1-0},
  comment  = {Section worth special notice: Bayesian Interpretation of Fed-ensemble, 
possibly give account of the reason why it works.

Fed-ensemble: more than federated learning and then ensemble ?},
  doi      = {10.1109/TASE.2023.3269639},
  file     = {:2023Shi - Fed Ensemble_ Ensemble Models in Federated Learning for Improved Generalization and Uncertainty Quantification.pdf:PDF;:Fed-ensemble_Ensemble_Models_in_Federated_Learning_for_Improved_Generalization_and_Uncertainty_Quantification.pdf:PDF},
  groups   = {Federated Ensemble},
  keywords = {Predictive models,Data models,Computational modeling,Uncertainty,Training,Servers,Bayes methods,Federated learning,ensemble learning,kernel method,uncertainty quantification},
  priority = {prio2},
  ranking  = {rank4},
}

@Article{Liu2022,
  author     = {Liu, Zelei and Chen, Yuanyuan and Yu, Han and Liu, Yang and Cui, Lizhen},
  journal    = {ACM Trans. Intell. Syst. Technol.},
  title      = {GTG-Shapley: Efficient and Accurate Participant Contribution Evaluation in Federated Learning},
  year       = {2022},
  issn       = {2157-6904},
  month      = {may},
  number     = {4},
  volume     = {13},
  abstract   = {Federated Learning (FL) bridges the gap between collaborative machine learning and preserving data privacy. To sustain the long-term operation of an FL ecosystem, it is important to attract high-quality data owners with appropriate incentive schemes. As an important building block of such incentive schemes, it is essential to fairly evaluate participants’ contribution to the performance of the final FL model without exposing their private data. Shapley Value (SV)–based techniques have been widely adopted to provide a fair evaluation of FL participant contributions. However, existing approaches incur significant computation costs, making them difficult to apply in practice. In this article, we propose the Guided Truncation Gradient Shapley (GTG-Shapley) approach to address this challenge. It reconstructs FL models from gradient updates for SV calculation instead of repeatedly training with different combinations of FL participants. In addition, we design a guided Monte Carlo sampling approach combined with within-round and between-round truncation to further reduce the number of model reconstructions and evaluations required. This is accomplished through extensive experiments under diverse realistic data distribution settings. The results demonstrate that GTG-Shapley can closely approximate actual Shapley values while significantly increasing computational efficiency compared with the state-of-the-art, especially under non-i.i.d. settings.},
  address    = {New York, NY, USA},
  articleno  = {60},
  comment    = {可以用来鉴别懒客户端},
  doi        = {10.1145/3501811},
  file       = {:2022mayLiu - GTG Shapley_ Efficient and Accurate Participant Contribution Evaluation in Federated Learning.pdf:PDF},
  groups     = {Lazy Client},
  issue_date = {August 2022},
  keywords   = {Federated learning, contribution assessment, Shapley value},
  numpages   = {21},
  priority   = {prio2},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3501811},
}

@Article{Lu2024,
  author   = {Lu, Zili and Pan, Heng and Dai, Yueyue and Si, Xueming and Zhang, Yan},
  journal  = {IEEE Internet of Things Journal},
  title    = {Federated Learning With Non-{IID} Data: A Survey},
  year     = {2024},
  pages    = {1-1},
  doi      = {10.1109/JIOT.2024.3376548},
  file     = {:2024Lu - Federated Learning with Non IID Data_ a Survey.pdf:PDF},
  groups   = {FL & Non-iid},
  keywords = {Data models,Training,Servers,Convergence,Distributed databases,Adaptation models,Internet of Things,Communication efficiency,federated learning,Internet of Things (IoT),non-IID data,privacy preservation,survey},
  priority = {prio3},
}

@Article{Criado2022,
  author   = {Marcos F. Criado and Fernando E. Casado and Roberto Iglesias and Carlos V. Regueiro and Senén Barro},
  journal  = {Information Fusion},
  title    = {Non-{IID} data and Continual Learning processes in Federated Learning: A long road ahead},
  year     = {2022},
  issn     = {1566-2535},
  pages    = {263-280},
  volume   = {88},
  abstract = {Federated Learning is a novel framework that allows multiple devices or institutions to train a machine learning model collaboratively while preserving their data private. This decentralized approach is prone to suffer the consequences of data statistical heterogeneity, both across the different entities and over time, which may lead to a lack of convergence. To avoid such issues, different methods have been proposed in the past few years. However, data may be heterogeneous in lots of different ways, and current proposals do not always determine the kind of heterogeneity they are considering. In this work, we formally classify data statistical heterogeneity and review the most remarkable learning Federated Learning strategies that are able to face it. At the same time, we introduce approaches from other machine learning frameworks. In particular, Continual Learning strategies are worthy of special attention, since they are able to handle habitual kinds of data heterogeneity. Throughout this paper, we present many methods that could be easily adapted to the Federated Learning settings to improve its performance. Apart from theoretically discussing the negative impact of data heterogeneity, we examine it and show some empirical results using different types of non-IID data.},
  doi      = {https://doi.org/10.1016/j.inffus.2022.07.024},
  file     = {:2022Criado - Non IID Data and Continual Learning Processes in Federated Learning_ a Long Road Ahead.pdf:PDF},
  groups   = {FL & Non-iid},
  keywords = {Federated Learning, Data heterogeneity, Non-IID data, Concept drift, Distributed learning, Continual learning},
  priority = {prio3},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253522000884},
}

@InProceedings{Ying2023,
  author    = {X. Ying and C. Liu and D. Hu},
  booktitle = {Proc. 28th IEEE Symp. Computers Commun. (ISCC'23)},
  title     = {{GCFL}: Blockchain-based Efficient Federated Learning for Heterogeneous Devices},
  year      = {2023},
  address   = {Los Alamitos, CA, USA},
  month     = jul,
  pages     = {1033-1038},
  abstract  = {Federated Learning has emerged as a promising machine learning paradigm to protect data privacy. However, the differences between heterogeneous clients and the performance bottleneck of central server limit the efficiency of FL. As a typical decentralized solution, the combination of blockchain and FL has been studied in recent years. However, the use of single-chain blockchain and traditional consensus algorithms in these studies have drawbacks such as high resource consumption, low TPS and low scalability. This paper proposes an efficient solution that combines a DAG blockchain and FL, called GCFL(Graph with Coordinator Federated Learning). GCFL introduces a new block structure that reduces data redundancy. For DAG blockchains, we proposed a two-phase tips selection consensus algorithm that can reduce resource consumption and tolerate a certain proportion of malicious nodes. Simulation experiments show that GCFL has higher stability and fast convergence time for targeted accuracy compared to traditional on-device FL systems.},
  comment   = {In @Ying2023, a DAG based blockchain composed of devices and coordinators is proposed. Two types of blocks are designed: ordinary blocks containing local models and Key Blocks containing the global models aggregated by coordinators. In DAG, each reference of the previous block is a confirmation of the transactions and the local models. A two-stage tips selection algorithm is elaborated to select the blocks with most references and most accurate local models.},
  doi       = {10.1109/ISCC58397.2023.10218066},
  file      = {:2023julYing - GCFL_ Blockchain Based Efficient Federated Learning for Heterogeneous Devices.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {training,data privacy,federated learning,scalability,redundancy,consensus algorithm,resists},
  priority  = {prio2},
  ranking   = {rank3},
}

@Article{Zhao2024b,
  author        = {Zishuo Zhao and Zhixuan Fang and Xuechao Wang and Xi Chen and Yuan Zhou},
  journal       = {arXiv preprint arXiv:2404.09005},
  title         = {Proof-of-Learning with Incentive Security},
  year          = {2024},
  month         = jul,
  archiveprefix = {arXiv},
  eprint        = {2404.09005},
  file          = {:2024Zhao - Proof of Learning with Incentive Security.pdf:PDF},
  groups        = {PoL Security},
  primaryclass  = {cs.CR},
  priority      = {prio2},
  ranking       = {rank2},
}

@Article{Wang2024,
  author   = {Wang, Zhipeng and Dong, Nanqing and Sun, Jiahao and Knottenbelt, William and Guo, Yike},
  journal  = {IEEE Trans. Big Data},
  title    = {zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning},
  year     = {2024},
  pages    = {1-14},
  comment  = {use zk proof to avoid transmission of global model when the training clients verifying the global model aggregated by the aggregator;
blockchain is integrated, where miners verify the zk proof and maintain valid zk proof on-chain

Whether the zk proof is correct?},
  doi      = {10.1109/TBDATA.2024.3403370},
  file     = {:2024Wang - ZkFL_ Zero Knowledge Proof Based Gradient Aggregation for Federated Learning.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Blockchains,Big Data,Training,Computational modeling,Data models,Privacy,Arithmetic,Federated Learning,Security,Trustworthy Machine Learning,Zero-Knowledge Proof},
  ranking  = {rank1},
}

@InProceedings{Li2021cvpr,
  author    = {Li, Qinbin and He, Bingsheng and Song, Dawn},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Model-Contrastive Federated Learning},
  year      = {2021},
  pages     = {10708-10717},
  doi       = {10.1109/CVPR46437.2021.01057},
  file      = {:2021Li - Model Contrastive Federated Learning.pdf:PDF},
  groups    = {FL & Non-iid},
  keywords  = {Deep learning,Training,Pain,Moon,Object detection,Collaborative work,Data models},
}

@Misc{Wang2020,
  author        = {Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
  title         = {Federated Learning with Matched Averaging},
  year          = {2020},
  archiveprefix = {arXiv},
  comment       = {（CIFAR-10+Dirchlet distribution based non-iidness）
在IID情形下，Ensemble 比 FedAvg明显要差
但在Non-iid情形下，两者的差距不大},
  eprint        = {2002.06440},
  file          = {:2020Wang - Federated Learning with Matched Averaging.pdf:PDF},
  groups        = {FL & Non-iid},
  primaryclass  = {cs.LG},
}

@InProceedings{Huang2022,
  author    = {Huang, Wenke and Ye, Mang and Du, Bo},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learn from Others and Be Yourself in Heterogeneous Federated Learning},
  year      = {2022},
  pages     = {10133-10143},
  doi       = {10.1109/CVPR52688.2022.00990},
  file      = {:2022Huang - Learn from Others and Be Yourself in Heterogeneous Federated Learning.pdf:PDF},
  groups    = {FL & Non-iid},
  keywords  = {Degradation,Privacy,Distance learning,Distributed databases,Collaboration,Collaborative work,Data models,Privacy and federated learning},
}

@Article{MorenoTorres2012,
  author   = {Jose G. Moreno-Torres and Troy Raeder and Rocío Alaiz-Rodríguez and Nitesh V. Chawla and Francisco Herrera},
  journal  = {Pattern Recognition},
  title    = {A unifying view on dataset shift in classification},
  year     = {2012},
  issn     = {0031-3203},
  number   = {1},
  pages    = {521-530},
  volume   = {45},
  abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.},
  doi      = {https://doi.org/10.1016/j.patcog.2011.06.019},
  file     = {:2012Moreno-Torres - A Unifying View on Dataset Shift in Classification.pdf:PDF},
  groups   = {FL & Non-iid},
  keywords = {Dataset shift, Data fracture, Changing environments, Differing training and test populations, Covariate shift, Sample selection bias, Non-stationary distributions},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320311002901},
}

@Misc{Koh2021,
  author        = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  title         = {WILDS: A Benchmark of in-the-Wild Distribution Shifts},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2012.07421},
  file          = {:2021Koh - WILDS_ a Benchmark of in the Wild Distribution Shifts.pdf:PDF},
  groups        = {FL Research},
  primaryclass  = {cs.LG},
}

@Article{Sattler2020,
  author   = {Sattler, Felix and Wiedemann, Simon and Müller, Klaus-Robert and Samek, Wojciech},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data},
  year     = {2020},
  number   = {9},
  pages    = {3400-3413},
  volume   = {31},
  doi      = {10.1109/TNNLS.2019.2944481},
  file     = {:2020Sattler - Robust and Communication Efficient Federated Learning from Non I.i.d. Data.pdf:PDF},
  groups   = {FL & Non-iid},
  keywords = {Training,Data models,Servers,Deep learning,Protocols,Training data,Distributed databases,Deep learning,distributed learning,efficient communication,federated learning,privacy-preserving machine learning},
}

@InProceedings{Chai2021a,
  author    = {Chai, Zheng and Chen, Yujing and Anwar, Ali and Zhao, Liang and Cheng, Yue and Rangwala, Huzefa},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  title     = {FedAT: a high-performance and communication-efficient federated learning system with asynchronous tiers},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SC '21},
  abstract  = {Federated learning (FL) involves training a model over massive distributed devices, while keeping the training data localized and private. This form of collaborative learning exposes new tradeoffs among model convergence speed, model accuracy, balance across clients, and communication cost, with new challenges including: (1) straggler problem---where clients lag due to data or (computing and network) resource heterogeneity, and (2) communication bottleneck---where a large number of clients communicate their local updates to a central server and bottleneck the server. Many existing FL methods focus on optimizing along only one single dimension of the tradeoff space. Existing solutions use asynchronous model updating or tiering-based, synchronous mechanisms to tackle the straggler problem. However, asynchronous methods can easily create a communication bottleneck, while tiering may introduce biases that favor faster tiers with shorter response latencies.To address these issues, we present FedAT, a novel Federated learning system with Asynchronous Tiers under Non-i.i.d. training data. FedAT synergistically combines synchronous, intra-tier training and asynchronous, cross-tier training. By bridging the synchronous and asynchronous training through tiering, FedAT minimizes the straggler effect with improved convergence speed and test accuracy. FedAT uses a straggler-aware, weighted aggregation heuristic to steer and balance the training across clients for further accuracy improvement. FedAT compresses uplink and downlink communications using an efficient, polyline-encoding-based compression algorithm, which minimizes the communication cost. Results show that FedAT improves the prediction performance by up to 21.09\% and reduces the communication cost by up to 8.5\texttimes{}, compared to state-of-the-art FL methods.},
  articleno = {60},
  doi       = {10.1145/3458817.3476211},
  file      = {:2021Chai - FedAT_ a High Performance and Communication Efficient Federated Learning System with Asynchronous Tiers.pdf:PDF},
  groups    = {Blockchain-based FL},
  isbn      = {9781450384421},
  keywords  = {weighted aggregation, tiering, federated learning, communication efficiency, asynchronous distributed learning},
  location  = {<conf-loc>, <city>St. Louis</city>, <state>Missouri</state>, </conf-loc>},
  numpages  = {16},
  url       = {https://doi.org/10.1145/3458817.3476211},
}

@Misc{Liu2024unlearning,
  author        = {Xiao Liu and Mingyuan Li and Xu Wang and Guangsheng Yu and Wei Ni and Lixiang Li and Haipeng Peng and Renping Liu},
  title         = {BlockFUL: Enabling Unlearning in Blockchained Federated Learning},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {Represent model updates with DAG inheritance},
  eprint        = {2402.16294},
  file          = {:2024Liu - Decentralized Federated Unlearning on Blockchain.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'},
  ranking       = {rank3},
  readstatus    = {skimmed},
}

@Article{Yang2024,
  author   = {Yang, Ruizhe and Zhao, Tonghui and Yu, F. Richard and Li, Meng and Zhang, Dajun and Zhao, Xuehui},
  journal  = {IEEE Internet of Things Journal},
  title    = {Blockchain-Based Federated Learning With Enhanced Privacy and Security Using Homomorphic Encryption and Reputation},
  year     = {2024},
  number   = {12},
  pages    = {21674-21688},
  volume   = {11},
  comment  = {Reputation mechanism + PBFT},
  doi      = {10.1109/JIOT.2024.3379395},
  file     = {:2024Yang - Blockchain Based Federated Learning with Enhanced Privacy and Security Using Homomorphic Encryption and Reputation.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Federated learning,Data models,Security,Industrial Internet of Things,Data privacy,Privacy,Training,Blockchain,federated learning,Industrial Internet of Things (IIoT),privacy,security},
  ranking  = {rank1},
}

@Article{Kasyap2023,
  author   = {Kasyap, Harsh and Manna, Arpan and Tripathy, Somanath},
  journal  = {IEEE Transactions on Network and Service Management},
  title    = {An Efficient Blockchain Assisted Reputation Aware Decentralized Federated Learning Framework},
  year     = {2023},
  number   = {3},
  pages    = {2771-2782},
  volume   = {20},
  comment  = {Proof of Interpretation and Selection: a consensus mechanism based on the honest history or miners.},
  doi      = {10.1109/TNSM.2022.3231283},
  file     = {:2023Kasyap - An Efficient Blockchain Assisted Reputation Aware Decentralized Federated Learning Framework.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Blockchains,Federated learning,Training,Data models,Servers,Data integrity,Topology,Federated learning,blockchain,model interpretation,client selection,reputation},
  ranking  = {rank1},
}

@InProceedings{Qin2024,
  author    = {Qin, Zhen and Yan, Xueqiang and Zhou, Mengchu and Deng, Shuiguang},
  booktitle = {Proceedings of the ACM on Web Conference 2024},
  title     = {BlockDFL: A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Framework},
  year      = {2024},
  address   = {New York, NY, USA},
  pages     = {2914–2925},
  publisher = {Association for Computing Machinery},
  series    = {WWW '24},
  abstract  = {Federated learning (FL) enables the collaborative training of machine learning models without sharing training data. Traditional FL heavily relies on a trusted centralized server. Although decentralized FL eliminates the dependence on a centralized server, it faces such issues as poisoning attacks and data representation leakage due to insufficient restrictions on the behavior of participants, and heavy communication costs in fully decentralized scenarios, i.e., peer-to-peer (P2P) settings. This work proposes a blockchainbased fully decentralized P2P framework for FL, called BlockDFL. It takes blockchain as the foundation, leveraging the proposed voting mechanism and a two-layer scoring mechanism to coordinate FL among participants without mutual trust, while effectively defending against poisoning attacks. Gradient compression is introduced to lower communication cost and to prevent data from being reconstructed from transmitted model updates. The results of extensive experiments conducted on two real-world datasets exhibit that BlockDFL obtains competitive accuracy compared to centralized FL and can defend against poisoning attacks while achieving efficiency and scalability. Especially when the proportion of malicious participants is as high as 40\%, BlockDFL can still preserve the accuracy of FL, outperforming existing fully decentralized P2P FL frameworks based on blockchain.},
  comment   = {Safeguard blockchain with PBFT and a scoring mechanism based on Krum and median-based testing.

Privacy + Communication optimization: top-k sparsification of local model updates (A Fast Blockchain-based Federated Learning Framework with Compressed Communications)},
  doi       = {10.1145/3589334.3645425},
  file      = {:2024Qin - BlockDFL_ a Blockchain Based Fully Decentralized Peer to Peer Federated Learning Framework.pdf:PDF},
  groups    = {Blockchain-based FL},
  isbn      = {9798400701719},
  keywords  = {blockchain, decentralized federated learning, peer-to-peer, trustworthy federated learning},
  location  = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
  numpages  = {12},
  ranking   = {rank1},
  url       = {https://doi.org/10.1145/3589334.3645425},
}

@InProceedings{Zhang2021,
  author    = {Zhang, Qinnan and Ding, Qingyang and Zhu, Jianming and Li, Dandan},
  booktitle = {2021 IEEE Wireless Communications and Networking Conference Workshops (WCNCW)},
  title     = {Blockchain Empowered Reliable Federated Learning by Worker Selection: A Trustworthy Reputation Evaluation Method},
  year      = {2021},
  pages     = {1-6},
  comment   = {1. contribution model based on cross entropy to evaluate the history reputation of nodes
2. Proof of Reputation consensus algorithm},
  doi       = {10.1109/WCNCW49093.2021.9420026},
  file      = {:2021Zhang - Blockchain Empowered Reliable Federated Learning by Worker Selection_ a Trustworthy Reputation Evaluation Method.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {Training,Data privacy,Analytical models,Conferences,Blockchain,Machine learning,Predictive models,blockchain,federated learning,reputation evaluation,consensus algorithm},
  ranking   = {rank1},
}

@Misc{Merlina2024,
  author        = {Andrea Merlina and Thiago Garrett and Roman Vitenberg},
  title         = {On Replacing Cryptopuzzles with Useful Computation in Blockchain Proof-of-Work Protocols},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {A comprehensive analysis of existing PoUW mechanisms in which prerequisites for replacing cryptopuzzles with useful computation task to building PoUW is distillated from existing literature.},
  eprint        = {2404.15735},
  file          = {:2024Merlina - On Replacing Cryptopuzzles with Useful Computation in Blockchain Proof of Work Protocols.pdf:PDF},
  groups        = {Proof-of-Useful-Work},
  primaryclass  = {id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'},
  ranking       = {rank4},
}

@Article{Gou2024,
  author   = {Gou, Yan and Weng, Shangyin and Imran, Muhammad Ali and Zhang, Lei},
  journal  = {IEEE Internet of Things Journal},
  title    = {Voting Consensus-Based Decentralized Federated Learning},
  year     = {2024},
  number   = {9},
  pages    = {16267-16278},
  volume   = {11},
  comment  = {VCDFL: leader–candidate–follower hierarchical management + the
consensus-based leader election mechanism},
  doi      = {10.1109/JIOT.2024.3355853},
  file     = {:2024Gou - Voting Consensus Based Decentralized Federated Learning.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Internet of Things,Servers,Data models,Computational modeling,Voting,Federated learning,Mathematical models,Communication efficient,consensus,decentralized federated learning (FL),fault tolerant,hierarchical},
  ranking  = {rank1},
}

@InProceedings{Fang2023,
  author    = {C. Fang and H. Jia and A. Thudi and M. Yaghini and C. A. Choquette-Choo and N. Dullerud and V. Chandrasekaran and N. Papernot},
  booktitle = {2023 IEEE 8th European Symposium on Security and Privacy (EuroS&amp;P)},
  title     = {Proof-of-Learning is Currently More Broken Than You Think},
  year      = {2023},
  address   = {Los Alamitos, CA, USA},
  month     = jul,
  pages     = {797-816},
  publisher = {IEEE Computer Society},
  abstract  = {Proof-of-learning (PoL) proposes that a model owner logs training checkpoints to establish a proof of having expended the computation necessary for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning. They empirically argued the benefit of this approach by showing how spoofing—computing a proof for a stolen model—is as expensive as obtaining the proof honestly by training the model. However, recent work provided a counterexample and thus has invalidated this observation. In this work we demonstrate, first, that while it is true that current PoL verification is not robust to adversaries, recent work has largely underestimated this lack of robustness. This is because existing spoofing strategies are either unreproducible or target weakened instantiations of PoL—meaning they are easily thwarted by changing hyperparameters of the verification. Instead, we introduce the first spoofing strategies that can be reproduced across different configurations of the PoL verification and can be done for a fraction of the cost of previous spoofing strategies. This is possible because we identify key vulnerabilities of PoL and systematically analyze the underlying assumptions needed for robust verification of a proof. On the theoretical side, we show how realizing these assumptions reduces to open problems in learning theory. We conclude that one cannot develop a provably robust PoL verification mechanism without further understanding of optimization in deep learning.},
  comment   = {“realizing the desired security requirements reduces to solving hard open problems in learning theory, so that a provably secure PoL is not possible to design until significant progress in further understanding in deep learning”},
  doi       = {10.1109/EuroSP57164.2023.00052},
  file      = {:2023julFang - Proof of Learning Is Currently More Broken Than You Think.pdf:PDF},
  groups    = {PoL Security},
  priority  = {prio2},
  ranking   = {rank3},
  url       = {https://doi.ieeecomputersociety.org/10.1109/EuroSP57164.2023.00052},
}

@Misc{Conway2024,
  author        = {KD Conway and Cathie So and Xiaohang Yu and Kartin Wong},
  title         = {opML: Optimistic Machine Learning on Blockchain},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {prove the correctness of the ML results on chain with fraud proof instead of computation-intensive ZKP, as described in "An Efficient and Extensible Zero-knowledge Proof Framework for Neural Networks" (https://eprint.iacr.org/2024/703.pdf)

(sketch: https://ethresear.ch/t/opml-optimistic-machine-learning-on-blockchain/16234/1)},
  eprint        = {2401.17555},
  file          = {:2024Conway - OpML_ Optimistic Machine Learning on Blockchain.pdf:PDF},
  groups        = {BC & ML inference},
  primaryclass  = {id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'},
  ranking       = {rank2},
}

@Misc{Zhang2024b,
  author        = {Zhenjie Zhang and Yuyang Rao and Hao Xiao and Xiaokui Xiao and Yin Yang},
  title         = {Proof of Quality: A Costless Paradigm for Trustless Generative AI Model Inference on Blockchains},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {Proof of Quality:  support trustworthy model inference services on blockchains with minimal overhead},
  eprint        = {2405.17934},
  file          = {:2024Zhang - Proof of Quality_ a Costless Paradigm for Trustless Generative AI Model Inference on Blockchains.pdf:PDF},
  groups        = {BC & ML inference},
  primaryclass  = {id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'},
  ranking       = {rank1},
}

@Misc{So2024,
  author        = {Cathie So and KD Conway and Xiaohang Yu and Suning Yao and Kartin Wong},
  title         = {opp/ai: Optimistic Privacy-Preserving AI on Blockchain},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {zkML + opML},
  eprint        = {2402.15006},
  file          = {:2024So - Opp_ai_ Optimistic Privacy Preserving AI on Blockchain.pdf:PDF},
  groups        = {BC & ML inference},
  primaryclass  = {id='cs.CR' full_name='Cryptography and Security' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.'},
  ranking       = {rank1},
}

@Article{Caldas2019,
  author        = {Sebastian Caldas and Sai Meher Karthik Duddu and Peter Wu and Tian Li and Jakub Konečný and H. Brendan McMahan and Virginia Smith and Ameet Talwalkar},
  journal       = {arXiv preprint arXiv:1812.01097},
  title         = {{LEAF}: A Benchmark for Federated Settings},
  year          = {2019},
  month         = dec,
  archiveprefix = {arXiv},
  eprint        = {1812.01097},
  file          = {:2019Caldas - LEAF_ a Benchmark for Federated Settings.pdf:PDF},
  groups        = {FL Research},
  primaryclass  = {cs.LG},
}

 
@Article{Rebello2021,
  author    = {Rebello, Gabriel Antonio F. and Camilo, Gustavo F. and Guimarães, Lucas C. B. and de Souza, Lucas Airam C. and Thomaz, Guilherme A. and Duarte, Otto Carlos M. B.},
  journal   = {Annals of Telecommunications},
  title     = {A security and performance analysis of proof-based consensus protocols},
  year      = {2021},
  issn      = {1958-9395},
  month     = nov,
  pages     = {517–537},
  volume    = {77},
  doi       = {10.1007/s12243-021-00896-2},
  file      = {:2021NovemberRebello - A Security and Performance Analysis of Proof Based Consensus Protocols.pdf:PDF},
  groups    = {Blockchain Consensus},
  publisher = {Springer Science and Business Media LLC},
  ranking   = {rank2},
}

@InProceedings{Wu2024,
  author    = {Wu, Hao and Zhao, Shengnan and Zhao, Chuan and Jing, Shan},
  booktitle = {Information Security and Cryptology},
  title     = {A General Federated Learning Scheme with Blockchain on Non-{IID} Data},
  year      = {2024},
  address   = {Singapore},
  editor    = {Ge, Chunpeng and Yung, Moti},
  pages     = {126--140},
  publisher = {Springer Nature Singapore},
  abstract  = {The security of machine learning has received a lot of attention from the community. Federated learning enables more secure training processes of models in machine learning via local training and parameter interactions of participants. However, participants' data usually shows significant differences, i.e., the characteristics of non-IID, affecting the convergence speed and accuracy of models to a large extent. In this paper, we propose a general federated learning scheme with blockchain to cover the shortage of federated learning caused by non-IID data. Specifically, each participant trains a GAN via local data first and then shares the generator corresponding to the GAN with the assistance of the blockchain. Based on the generator parameters on the blockchain, each participant augments the local data and trains the local model, alleviating a series of problems caused by the non-IID data. The scheme achieves effective training of models while ensuring security. Experimental results show that the proposed scheme can speed up model convergence and improve the model's accuracy simultaneously. In the non-IID scenario, compared with the federated learning benchmark scheme, the accuracy in our scheme can be improved by up to 17{\$}{\$}{\backslash}{\%}{\$}{\$}{\%}.},
  comment   = {generators shared are used for data augmentation},
  file      = {:2024Wu - A General Federated Learning Scheme With Blockchain On Non IID Data.pdf:PDF},
  groups    = {Misc FL},
  isbn      = {978-981-97-0942-7},
}

@Misc{Rasouli2020,
  author        = {Mohammad Rasouli and Tao Sun and Ram Rajagopal},
  title         = {FedGAN: Federated Generative Adversarial Networks for Distributed Data},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.07228},
  file          = {:2020Rasouli - FedGAN_ Federated Generative Adversarial Networks for Distributed Data.pdf:PDF},
  groups        = {Misc FL},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2006.07228},
}

@Article{Cao2024,
  author        = {Weihang Cao and Xintong Ling and Jiaheng Wang and Xiqi Gao and Zhi Ding},
  journal       = {arXiv preprint arXiv:2405.19027},
  title         = {A Dual-functional Blockchain Framework for Solving Distributed Optimization},
  year          = {2024},
  month         = may,
  archiveprefix = {arXiv},
  eprint        = {2405.19027},
  file          = {:2024Cao - A Dual Functional Blockchain Framework for Solving Distributed Optimization.pdf:PDF},
  groups        = {Proof-of-Useful-Work},
  primaryclass  = {cs.DC},
}

@Article{Zhao2024c,
  author   = {Zhao, Zhongyuan and Wang, Jingyi and Hong, Wei and Quek, Tony Q. S. and Ding, Zhiguo and Peng, Mugen},
  journal  = {IEEE Transactions on Wireless Communications},
  title    = {Ensemble Federated Learning With Non-{IID} Data in Wireless Networks},
  year     = {2024},
  number   = {4},
  pages    = {3557-3571},
  volume   = {23},
  doi      = {10.1109/TWC.2023.3309376},
  file     = {:2024Zhao - Ensemble Federated Learning with Non IID Data in Wireless Networks.pdf:PDF},
  groups   = {Federated Ensemble},
  keywords = {Federated learning,Data models,Adaptation models,Computational modeling,Wireless networks,6G mobile communication,Task analysis,Network intelligence,federated learning,model ensemble,non-IID data,coalition formation game},
}

@Article{Ling2020,
  author   = {Ling, Xintong and Wang, Jiaheng and Le, Yuwei and Ding, Zhi and Gao, Xiqi},
  journal  = {IEEE Wireless Commun.},
  title    = {Blockchain Radio Access Network Beyond {5G}},
  year     = {2020},
  month    = dec,
  number   = {6},
  pages    = {160-168},
  volume   = {27},
  doi      = {10.1109/MWC.001.2000172},
  file     = {:2020Ling - Blockchain Radio Access Network beyond 5G.pdf:PDF},
  groups   = {Wireless Network},
  keywords = {Blockchain,Computer architecture,Business,Radio access networks,5G mobile communication,Wireless communication,Economics},
}

@Article{Zuo2023,
  author   = {Zuo, Yiping and Guo, Jiajia and Gao, Ning and Zhu, Yongxu and Jin, Shi and Li, Xiao},
  journal  = {IEEE Commun. Surveys Tut.},
  title    = {A Survey of Blockchain and Artificial Intelligence for {6G} Wireless Communications},
  year     = {2023},
  month    = {4th Quart.},
  number   = {4},
  pages    = {2494-2528},
  volume   = {25},
  doi      = {10.1109/COMST.2023.3315374},
  file     = {:20234th Quart.Zuo - A Survey of Blockchain and Artificial Intelligence for 6G Wireless Communications.pdf:PDF},
  groups   = {Blockchain ML},
  keywords = {Blockchains,Artificial intelligence,6G mobile communication,Wireless communication,Surveys,Communication system security,Security,Blockchain,AI,wireless communications,6G networks,secure services,IoT smart applications,spectrum management,security and privacy,smart healthcare,UAVs},
  ranking  = {rank5},
}

@Article{Wang2024a,
  author     = {Haibo Wang and Hongwei Gao and Teng Ma and Chong Li and Tao Jing},
  journal    = {Digit. Commun. Networks},
  title      = {A hierarchical blockchain-enabled distributed federated learning system with model-contribution based rewarding},
  year       = {2024},
  issn       = {2352-8648},
  abstract   = {Distributed Federated Learning (DFL) technology enables participants to cooperatively train a shared model while preserving the privacy of their local data sets, making it a desirable solution for decentralized and privacy-preserving Web3 scenarios. However, DFL faces incentive and security challenges in the decentralized framework. To address these issues, this paper presents a Hierarchical Blockchain-enabled DFL (HBDFL) system, which provides a generic solution framework for the DFL-related applications. The proposed system consists of four major components, including a model contribution-based reward mechanism, a Proof of Elapsed Time and Accuracy (PoETA) consensus algorithm, a Distributed Reputation-based Verification Mechanism (DRTM) and an Accuracy-Dependent Throughput Management (ADTM) mechanism. The model contribution-based rewarding mechanism incentivizes network nodes to train models with their local datasets, while the PoETA consensus algorithm optimizes the tradeoff between the shared model accuracy and system throughput. The DRTM improves the system efficiency in consensus, and the ADTM mechanism guarantees that the throughput performance remains within a predefined range while improving the shared model accuracy. The performance of the proposed HBDFL system is evaluated by numerical simulations, which show that the system improves the accuracy of the shared model while maintaining high throughput and ensuring security.},
  comment    = {A modification of proof of elapsed time, taking accuracy into account

"a validator with the short waiting time and a high accuracy model is selected to append its block to the consortium chain"

 In proof of elapsed time and accuracy [#Wang2024a], the waiting time randomly generated in proof of elapsed time consensus protocol and the accuracy of FL models are jointly considered when determining the winning validator.


Public + consortium chain},
  doi        = {https://doi.org/10.1016/j.dcan.2024.07.002},
  file       = {:2024Wang - A Hierarchical Blockchain Enabled Distributed Federated Learning System with Model Contribution Based Rewarding.pdf:PDF},
  groups     = {Blockchain-based FL},
  keywords   = {Blockchain, Federated learning, Consensus scheme, Accuracy dependent throughput management},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{Wu2020,
  author    = {Wu, Xin and Wang, Zhi and Zhao, Jian and Zhang, Yan and Wu, Yu},
  booktitle = {2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)},
  title     = {FedBC: Blockchain-based Decentralized Federated Learning},
  year      = {2020},
  pages     = {217-221},
  comment   = {Smart contract based, ring topology, fully decentralized BC + FL framework
benchmark: 
1. Federated learing > training alone, whether the training samples are sufficient or scarce
2. Proposed approach > differential privacy?
3. Robustness: the accuracy keep increasing when some nodes drop out},
  doi       = {10.1109/ICAICA50127.2020.9182705},
  file      = {:2020Wu - FedBC_ Blockchain Based Decentralized Federated Learning.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {Training,Contracts,Servers,Data models,Topology,Cryptography,Indexes,federated learning,distributed computing,deep learning,blockchain},
  ranking   = {rank1},
}

@InProceedings{Ye2023,
  author    = {Ye, Wenxuan and An, Xueli and Yan, Xueqiang and Carle, Georg},
  booktitle = {2023 IEEE Globecom Workshops (GC Wkshps)},
  title     = {Trustworthy Federated Learning via Decentralized Consensus Under Communication Constraints},
  year      = {2023},
  pages     = {38-43},
  doi       = {10.1109/GCWkshps58843.2023.10464450},
  file      = {:2023Ye - Trustworthy Federated Learning Via Decentralized Consensus under Communication Constraints.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {6G mobile communication,Analytical models,Fault tolerance,Federated learning,Distributed ledger,Packet loss,Distributed databases,Federated learning,Distributed ledger technology,Model aggregation,6G},
}

@Article{Cao2024a,
  author   = {Cao, Weihang and Ling, Xintong and Wang, Jiaheng and Ding, Zhi and Gao, Xiqi},
  journal  = {IEEE Trans. Wireless Commun.},
  title    = {A Framework for {QoS}-Guaranteed Fast Access Services in Blockchain Radio Access Network},
  year     = {2024},
  month    = apr,
  number   = {4},
  pages    = {2711-2725},
  volume   = {23},
  doi      = {10.1109/TWC.2023.3302415},
  file     = {:2024AprilCao - A Framework for QoS Guaranteed Fast Access Services in Blockchain Radio Access Network.pdf:PDF},
  groups   = {Wireless Network},
  keywords = {Blockchains,Quality of service,Wireless communication,Communication system security,Smart contracts,Delays,Wireless networks,Blockchain,quality of service (QoS),radio access network (RAN),wireless access service,wireless networking},
}

@InProceedings{Li2021lottery,
  author    = {Li, Ang and Sun, Jingwei and Wang, Binghui and Duan, Lin and Li, Sicheng and Chen, Yiran and Li, Hai},
  booktitle = {2021 IEEE/ACM Symposium on Edge Computing (SEC)},
  title     = {LotteryFL: Empower Edge Intelligence with Personalized and Communication-Efficient Federated Learning},
  year      = {2021},
  pages     = {68-79},
  comment   = {1. leverage lottery ticket hypothesis to prune the neural network to reduce communication overhhead in FL
2. Construct Non-iid datasets to support the efficacy of the proposed method, 
label distribution skewness; quantity distributiion skewness
define two metrics
1. client-wise non-iid index: evaluate the degree of non-iidness without using trained feature extracters and classifers
2. balance rate, used when generating non-iid datasets},
  doi       = {10.1145/3453142.3492909},
  file      = {:2021Li - LotteryFL_ Empower Edge Intelligence with Personalized and Communication Efficient Federated Learning.pdf:PDF},
  groups    = {FL & Non-iid},
  keywords  = {Performance evaluation,Training,Costs,Distributed databases,Machine learning,Collaborative work,Real-time systems,Federated learning,Communication efficiency,Data heterogeneity,Personalization,IoT,Edge intelligence},
}

@PhdThesis{Drungilas2024,
  author = {Drungilas, Vaidotas},
  school = {Kauno technologijos universitetas.},
  title  = {Collaborative distributed machine learning on blockchain},
  year   = {2024},
  file   = {:2024Drungilas - Collaborative Distributed Machine Learning on Blockchain.pdf:PDF},
  groups = {Blockchain ML},
}

@InProceedings{Hsu2019,
  author     = {Harry Hsu and Hang Qi and Matthew Brown},
  title      = {Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification},
  year       = {2019},
  comment    = {!! The very beginning literature starting to use distribution based label imbalance (Dirichlet distribution), test the performance of fedavg under different concentration parameter},
  file       = {:2019Hsu - Measuring the Effects of Non Identical Data Distribution for Federated Visual Classification.pdf:PDF},
  groups     = {FL & Non-iid},
  ranking    = {rank4},
  readstatus = {read},
  url        = {https://arxiv.org/abs/1909.06335},
}

@Misc{Li2024b,
  author        = {Yang Li and Chunhe Xia and Dongchi Huang and Xiaojian Li and Tianbo Wang},
  title         = {BFLN: A Blockchain-based Federated Learning Model for Non-IID Data},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {mitigate effect of non-iidness in a blockchain FL system
1. Prototype-based aggregation algorithm (PAA)
The aggregator find the prototype vector for each local model: the average of the representation vector(embedding) of some data samples
then the aggregator find the pearson correlation of the prototype vectors and cluster the vectors by their similarity
finally, the local models in a cluster is aggregated into a global model, which is then distributed to the generator of the local models

2. Consensus Algorithm based on Cluster Centroids (CACC)
(DPOS inspired) the average (??) of the represention vector within a cluster is the cluster centroids
the client corresponding to the closest point to the centroid in euclidean distance enter the packaging queue, which is then packaged in a block. 
Incentive function designed to distribute rewards;


***Consensus of local models but not the transactions? Not PoUW, the aggregator node does everything(potential centralization)***},
  eprint        = {2407.05276},
  file          = {:2024Li - BFLN_ a Blockchain Based Federated Learning Model for Non IID Data.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/abs/2407.05276},
}

@InProceedings{Kang2020aug,
  author     = {Kang, Jiawen and Xiong, Zehui and Jiang, Chunxiao and Liu, Yi and Guo, Song and Zhang, Yang and Niyato, Dusit and Leung, Cyril and Miao, Chunyan},
  booktitle  = {Proc. 2nd Int. Conf. Blockchain Trustworthy Sys. (BlockSys'20)},
  title      = {Scalable and Communication-Efficient Decentralized Federated Edge Learning with Multi-blockchain Framework},
  year       = {2020},
  address    = {Dali, CN},
  editor     = {Zheng, Zibin and Dai, Hongning and Fu, Xiaodong and Chen, Benhui},
  month      = aug,
  pages      = {152--165},
  abstract   = {The emerging Federated Edge Learning (FEL) technique has drawn considerable attention, which not only ensures good machine learning performance but also solves ``data island'' problems caused by data privacy concerns. However, large-scale FEL still faces following crucial challenges: (i) there lacks a secure and communication-efficient model training scheme for FEL; (2) there is no scalable and flexible FEL framework for updating local models and global model sharing (trading) management. To bridge the gaps, we first propose a blockchain-empowered secure FEL system with a hierarchical blockchain framework consisting of a main chain and subchains. This framework can achieve scalable and flexible decentralized FEL by individually manage local model updates or model sharing records for performance isolation. A Proof-of-Verifying consensus scheme is then designed to remove low-quality model updates and manage qualified model updates in a decentralized and secure manner, thereby achieving secure FEL. To improve communication efficiency of the blockchain-empowered FEL, a gradient compression scheme is designed to generate sparse but important gradients to reduce communication overhead without compromising accuracy, and also further strengthen privacy preservation of training data. The security analysis and numerical results indicate that the proposed schemes can achieve secure, scalable, and communication-efficient decentralized FEL.},
  comment    = {main chain + multiple subchain architecture, achieve: 1) isolation of models and data to achieve privacy goal 2) performance isolation, make the main chain scalable and efficient
"Proof of verifying" consensus scheme
1) randomly select leader and verifiers
2) verifiers verify local model updates with a test dataset provided by a task publisher.
3) verifiers send model updates to leaders for model aggregation (after reconfirming the verification results)
4) the leader generate global in a block and send the block back to the verifiers},
  file       = {:2020Kang - Scalable and Communication Efficient Decentralized Federated Edge Learning with Multi Blockchain Framework.pdf:PDF},
  groups     = {Blockchain-based FL},
  isbn       = {978-981-15-9213-3},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@InProceedings{Uchida2017,
  author     = {Uchida, Yusuke and Nagai, Yuki and Sakazawa, Shigeyuki and Satoh, Shin'ichi},
  booktitle  = {Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval},
  title      = {Embedding Watermarks into Deep Neural Networks},
  year       = {2017},
  address    = {New York, NY, USA},
  pages      = {269–277},
  publisher  = {Association for Computing Machinery},
  series     = {ICMR '17},
  abstract   = {Significant progress has been made with deep neural networks recently. Sharing trained models of deep neural networks has been a very important in the rapid progress of research and development of these systems. At the same time, it is necessary to protect the rights to shared trained models. To this end, we propose to use digital watermarking technology to protect intellectual property and detect intellectual property infringement in the use of trained models. First, we formulate a new problem: embedding watermarks into deep neural networks. Second, we propose a general framework for embedding a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is placed because the watermark is embedded while training the host network. Finally, we perform comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new research effort. We show that our framework can embed a watermark during the training of a deep neural network from scratch, and during fine-tuning and distilling, without impairing its performance. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark remains complete even after 65\% of parameters are pruned.},
  comment    = {embed watermark with a parameter regularizer, which use flattened average of the CNN kernels and an embedding parameter to add an extra regularizer in the optimization target; when evaluating the existence of a watermark, compare the extra cost function against a threshold},
  doi        = {10.1145/3078971.3078974},
  file       = {:2017Uchida - Embedding Watermarks into Deep Neural Networks.pdf:PDF},
  groups     = {Others, DNN watermarking},
  isbn       = {9781450347013},
  keywords   = {watermarking, regularizer, deep neural networks},
  location   = {Bucharest, Romania},
  numpages   = {9},
  ranking    = {rank4},
  readstatus = {read},
}

@InProceedings{Adi2018,
  author     = {Yossi Adi and Carsten Baum and Moustapha Cisse and Benny Pinkas and Joseph Keshet},
  booktitle  = {27th USENIX Security Symposium (USENIX Security 18)},
  title      = {Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring},
  year       = {2018},
  address    = {Baltimore, MD, USA},
  month      = aug,
  pages      = {1615--1631},
  publisher  = {USENIX Association},
  comment    = {add samples from the "trigger set" in each training batch; identify the watermark by evaluating the model on the "trigger set"

Problem: how to construct the trigger set and bind the trigger set to identity?},
  file       = {:2018AugustAdi - Turning Your Weakness into a Strength_ Watermarking Deep Neural Networks by Backdooring.pdf:PDF},
  groups     = {DNN watermarking},
  isbn       = {978-1-939133-04-5},
  readstatus = {skimmed},
  url        = {https://www.usenix.org/conference/usenixsecurity18/presentation/adi},
}

 
@Article{LeMerrer2019,
  author    = {Le Merrer, Erwan and Pérez, Patrick and Trédan, Gilles},
  journal   = {Neural Computing and Applications},
  title     = {Adversarial frontier stitching for remote neural network watermarking},
  year      = {2019},
  issn      = {1433-3058},
  month     = aug,
  number    = {13},
  pages     = {9233--9244},
  volume    = {32},
  comment   = {Adversarial frontier stitching},
  doi       = {10.1007/s00521-019-04434-z},
  file      = {:2019AugustLe Merrer - Adversarial Frontier Stitching for Remote Neural Network Watermarking.pdf:PDF},
  groups    = {DNN watermarking},
  publisher = {Springer Science and Business Media LLC},
}

@InProceedings{Guo2021a,
  author        = {Shangwei Guo and Tianwei Zhang and Han Qiu and Yi Zeng and Tao Xiang and Yang Liu},
  booktitle     = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
  title         = {Fine-tuning Is Not Enough: A Simple yet Effective Watermark Removal Attack for DNN Models},
  year          = {2021},
  archiveprefix = {arXiv},
  comment       = {PST transformation before finetuning with unlabeled data},
  eprint        = {2009.08697},
  file          = {:2021Guo - Fine Tuning Is Not Enough_ a Simple yet Effective Watermark Removal Attack for DNN Models.pdf:PDF},
  groups        = {DNN watermarking},
  primaryclass  = {cs.CR},
  url           = {https://arxiv.org/abs/2009.08697},
}

@Misc{An2024,
  author        = {Haonan An and Guang Hua and Zhiping Lin and Yuguang Fang},
  title         = {Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2405.09863},
  file          = {:2024An - Box Free Model Watermarks Are Prone to Black Box Removal Attacks.pdf:PDF},
  groups        = {DNN watermarking},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2405.09863},
}

@InProceedings{Yan2023,
  author     = {Yifan Yan and Xudong Pan and Mi Zhang and Min Yang},
  booktitle  = {32nd USENIX Security Symposium (USENIX Security 23)},
  title      = {Rethinking {White-Box} Watermarks on Deep Learning Models under Neural Structural Obfuscation},
  year       = {2023},
  address    = {Anaheim, CA},
  month      = aug,
  pages      = {2347--2364},
  publisher  = {USENIX Association},
  comment    = {attack against all sorts of white-box watermark
using so-called neural structural obfuscation
classifying the watermarking methods into weight-based, passport-based, and activation based
the watermark BER for all the methods are reduced to around 50%},
  file       = {:2023AugustYan - Rethinking White Box Watermarks on Deep Learning Models under Neural Structural Obfuscation.pdf:PDF},
  groups     = {DNN watermarking},
  isbn       = {978-1-939133-37-3},
  ranking    = {rank4},
  readstatus = {skimmed},
  url        = {https://www.usenix.org/conference/usenixsecurity23/presentation/yan},
}

@Article{谭景轩2024,
  author     = {谭景轩 and 钟楠 and 郭钰生 and 钱振兴 and 张新鹏},
  journal    = {上海理工大学学报},
  title      = {深度神经网络模型水印研究进展},
  year       = {2024},
  number     = {3},
  pages      = {225--242},
  volume     = {46},
  doi        = {10.13255/j.cnki.jusst.20230912001},
  file       = {:2024谭景轩 - 深度神经网络模型水印研究进展.pdf:PDF},
  groups     = {DNN watermarking},
  publisher  = {上海理工大学学报},
  ranking    = {rank3},
  readstatus = {read},
  url        = {https://jns.usst.edu.cn/shlgdxxbzk/article/abstract/20240301},
}

@InProceedings{Zhang2018june,
  author    = {Zhang, Jialong and Gu, Zhongshu and Jang, Jiyong and Wu, Hui and Stoecklin, Marc Ph. and Huang, Heqing and Molloy, Ian},
  booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
  title     = {Protecting Intellectual Property of Deep Neural Networks with Watermarking},
  year      = {2018},
  address   = {New York, NY, USA},
  month     = jun,
  pages     = {159–172},
  publisher = {Association for Computing Machinery},
  series    = {ASIACCS '18},
  abstract  = {Deep learning technologies, which are the key components of state-of-the-art Artificial Intelligence (AI) services, have shown great success in providing human-level capabilities for a variety of tasks, such as visual analysis, speech recognition, and natural language processing and etc. Building a production-level deep learning model is a non-trivial task, which requires a large amount of training data, powerful computing resources, and human expertises. Therefore, illegitimate reproducing, distribution, and the derivation of proprietary deep learning models can lead to copyright infringement and economic harm to model creators. Therefore, it is essential to devise a technique to protect the intellectual property of deep learning models and enable external verification of the model ownership.In this paper, we generalize the "digital watermarking'' concept from multimedia ownership verification to deep neural network (DNNs) models. We investigate three DNN-applicable watermark generation algorithms, propose a watermark implanting approach to infuse watermark into deep learning models, and design a remote verification mechanism to determine the model ownership. By extending the intrinsic generalization and memorization capabilities of deep neural networks, we enable the models to learn specially crafted watermarks at training and activate with pre-specified predictions when observing the watermark patterns at inference. We evaluate our approach with two image recognition benchmark datasets. Our framework accurately (100\%) and quickly verifies the ownership of all the remotely deployed deep learning models without affecting the model accuracy for normal input data. In addition, the embedded watermarks in DNN models are robust and resilient to different counter-watermark mechanisms, such as fine-tuning, parameter pruning, and model inversion attacks.},
  comment   = {In-Distribution: modify the original images in the training set with patch or noise and modify the corresponding label},
  doi       = {10.1145/3196494.3196550},
  file      = {:2018Zhang - Protecting Intellectual Property of Deep Neural Networks with Watermarking.pdf:PDF},
  groups    = {DNN watermarking},
  isbn      = {9781450355766},
  keywords  = {watermarking, ownership verification, deep neural network},
  location  = {Incheon, Republic of Korea},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3196494.3196550},
}

@Article{Fan2022,
  author   = {Fan, Lixin and Ng, Kam Woh and Chan, Chee Seng and Yang, Qiang},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {DeepIPR: Deep Neural Network Ownership Verification With Passports},
  year     = {2022},
  number   = {10},
  pages    = {6122-6139},
  volume   = {44},
  comment  = {append a passport layer after the convolution layer, where the scaling factor and bias are decided by convoluting and average pooling the input passport, and the performance of the CNN model will be affected by the validity of the passport.

But there has been ambiguity attack against passport-based watermarking schemes:
"Effective Ambiguity Attack Against Passport-based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution"},
  doi      = {10.1109/TPAMI.2021.3088846},
  file     = {:2022Fan - DeepIPR_ Deep Neural Network Ownership Verification with Passports.pdf:PDF},
  groups   = {DNN watermarking},
  keywords = {Watermarking,Data models,Computational modeling,Analytical models,Training,Task analysis,Neural networks,Deep model protection,model ownership verification,intellectual property protection,model security,deep learning},
  priority = {prio2},
}

@Misc{Rouhani2018,
  author        = {Bita Darvish Rouhani and Huili Chen and Farinaz Koushanfar},
  title         = {DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models},
  year          = {2018},
  archiveprefix = {arXiv},
  comment       = {pytorch code: https://github.com/RorschachChen/DeepSigns-torch},
  eprint        = {1804.00750},
  file          = {:2018Rouhani - DeepSigns_ a Generic Watermarking Framework for IP Protection of Deep Learning Models.pdf:PDF},
  groups        = {DNN watermarking},
  primaryclass  = {cs.CR},
  readstatus    = {skimmed},
  url           = {https://arxiv.org/abs/1804.00750},
}

@Misc{Lin2024,
  author        = {Yijing Lin and Zhipeng Gao and Hongyang Du and Jinke Ren and Zhiqiang Xie and Dusit Niyato},
  title         = {Blockchain-enabled Trustworthy Federated Unlearning},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {奇怪的思路，通过遗忘旧数据对全局模型的影响来保护隐私？ 
（Federated Unlearning） Proof of Federated Unlearning is not a consensus protocol, the entire workflow is smart contract based.
使用Chameleon hash实现},
  eprint        = {2401.15917},
  file          = {:2024Lin - Blockchain Enabled Trustworthy Federated Unlearning.pdf:PDF},
  groups        = {Blockchain ML},
  keywords      = {federated unlearning},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
  url           = {https://arxiv.org/abs/2401.15917},
}

@Article{Ma2024a,
  author   = {Ma, Renwen and Hwang, Kai and and, Mo Li and Miao, Yiming},
  journal  = {IEEE Trans. Parallel Distrib. Syst.},
  title    = {Trusted Model Aggregation with Zero-Knowledge Proofs in Federated Learning},
  year     = {2024},
  pages    = {1-13},
  comment  = {ZKFL, an identical name to Wang2024, but this work seems to be better from many perspectives

Though it's not blockchain based, PBFT consensus is used},
  doi      = {10.1109/TPDS.2024.3455762},
  file     = {:2024Ma - Trusted Model Aggregation with Zero Knowledge Proofs in Federated Learning.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Data models,Computational modeling,Servers,Protocols,Privacy,Federated learning,Training,Federated Machine Learning,Secure Aggregation,Distributed Zero-knowledge Proofs,Trusted Computing,P2P Overlay},
  ranking  = {rank3},
}

@Article{Lansari2023,
  author     = {Lansari, Mohammed and Bellafqira, Reda and Kapusta, Katarzyna and Thouvenot, Vincent and Bettan, Olivier and Coatrieux, Gouenou},
  journal    = {Machine Learning and Knowledge Extraction},
  title      = {When Federated Learning Meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection},
  year       = {2023},
  issn       = {2504-4990},
  number     = {4},
  pages      = {1382--1406},
  volume     = {5},
  abstract   = {Federated learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need to centralize their data. Among other advantages, it comes with privacy-preserving properties, making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants’ models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of machine learning (ML), DNN watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in federated learning watermarking, shedding light on the new challenges and opportunities that arise in this field.},
  comment    = {Survey of FL + DNN-watermarking

FedTracker: Make ownership of FL model updates traceable
Aggregation server: black-box watermarking
Individual clients: white-box watermarking},
  doi        = {10.3390/make5040070},
  file       = {:2023Lansari - When Federated Learning Meets Watermarking_ a Comprehensive Overview of Techniques for Intellectual Property Protection.pdf:PDF},
  groups     = {DNN watermarking, FL Research},
  ranking    = {rank2},
  readstatus = {skimmed},
  url        = {https://www.mdpi.com/2504-4990/5/4/70},
}

@InProceedings{Zhang2023jul,
  author     = {Zhang, Xiaoli and Xu, Zhicheng and Cheng, Hongbing and Che, Tong and Xu, Ke and Wang, Weiqiang and Zhao, Wenbiao and Wang, Chunping and Li, Qi},
  booktitle  = {Proc. IEEE 43rd Int. Conf. Distrib. Comput. Syst. (ICDCS'23)},
  title      = {Secure Collaborative Learning in Mining Pool via Robust and Efficient Verification},
  year       = {2023},
  address    = {Hong Kong, CN},
  month      = jul,
  pages      = {794-805},
  comment    = {Proof of learning scheme: RPoL
Use address-encoded layer in DNN models to prevent address replacement attack and ensure training integrity (数学原理？invertible one to one mapping [Invertible Neural Networks])
Problem: could this layer be replaced easily?

alleviate communication overhead with the locality-sensitive hashing and tolerate slight reproduction errors},
  doi        = {10.1109/ICDCS57875.2023.00012},
  file       = {:2023Zhang - Secure Collaborative Learning in Mining Pool Via Robust and Efficient Verification.pdf:PDF},
  groups     = {Blockchain ML},
  keywords   = {Training,Costs,Federated learning,Smart contracts,Sampling methods,Consensus protocol,Artificial intelligence,Proof of learning,training integrity,collaborative learning,mining pool,PoUw consensus protocols},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{He2024,
  author     = {He, Huilin and Shen, Jiachen and Cao, Zhenfu and Dong, Xiaolei and Wu, Haiqin},
  booktitle  = {2024 IEEE International Conference on Blockchain (Blockchain)},
  title      = {Proof of Privacy-Preserving Machine Learning: A Blockchain Consensus Mechanism with Secure Deep Learning Process},
  year       = {2024},
  pages      = {193-200},
  comment    = {Proof of Privacy-Preserving Machine Learning (PPPML)
The training data from requesters are encrypted and miners train on the encrypted data. This is made possible by the proposed secure linear combination scheme to support the homomorphic multiplication

Drawbacks:
Need a centralized Cryptography Service Provider, relies on the CSP to decrypt the output of the first layer and updated the encrypted model parameter of the first layer of fully-connected network.},
  doi        = {10.1109/Blockchain62396.2024.00033},
  file       = {:2024He - Proof of Privacy Preserving Machine Learning_ a Blockchain Consensus Mechanism with Secure Deep Learning Process.pdf:PDF},
  groups     = {Blockchain ML},
  keywords   = {Training,Deep learning,Data privacy,Computational modeling,Simulation,Proof of Work,Data models,Blockchain,consensus mechanism,deep learning,hybrid training,PPPML},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@Article{Zhang2022,
  author   = {Zhang, Yang and Liang, Yongquan and Jia, Bin and Wang, Pinxiang and Zhang, Xiaosong},
  journal  = {International Journal of Intelligent Systems},
  title    = {A blockchain-enabled learning model based on distributed deep learning architecture},
  year     = {2022},
  number   = {9},
  pages    = {6577-6604},
  volume   = {37},
  abstract = {Abstract Aiming to address the unsatisfactory performance of existing distributed deep learning architectures, such as poor accuracy, slow network communication, low arithmetic speed, and insufficient security, we propose and design a learning model based on a distributed deep learning and blockchain architecture. We use a hybrid parallel algorithm based on blockchain (HP-B) to build a distributed deep consensus learning model. The HP-B algorithm is grouped according to the performance of computing nodes participating in training, network links and training samples, and the grouped computing equipment performs optimal distributed computing. The purpose of this approach is to solve the security and scalability concerns and improve the convergence speed and accuracy of deep learning. The proposed method achieves good results on the CIFAR-100, CIFAR-10, and IMAGENET data sets. Finally, the distributed deep learning model based on blockchain is combined with the generative adversarial network to solve the segmentation problem of medical imaging data, and the experimental results are superior to those of other networks.},
  comment  = {PBFT+DPoS hybrid consensus for distributed learning
use asynchronous SGD with delay compensation},
  doi      = {https://doi.org/10.1002/int.22907},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22907},
  file     = {:2022Zhang - A Blockchain Enabled Learning Model Based on Distributed Deep Learning Architecture.pdf:PDF},
  groups   = {Blockchain ML},
  keywords = {blockchain, distributed deep learning, generative adversarial networks, hybrid parallel algorithm, image segmentation},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22907},
}

@InProceedings{Lakshminarayanan2017,
  author    = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  file      = {:2017Lakshminarayanan - Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.pdf:PDF},
  groups    = {Ensemble Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
}

@Article{Sameera2024,
  author   = {Sameera K.M. and Serena Nicolazzo and Marco Arazzi and Antonino Nocera and Rafidha Rehiman K.A. and Vinod P. and Mauro Conti},
  journal  = {Computer Communications},
  title    = {Privacy-preserving in Blockchain-based Federated Learning systems},
  year     = {2024},
  issn     = {0140-3664},
  pages    = {38-67},
  volume   = {222},
  abstract = {Federated Learning (FL) has recently arisen as a revolutionary approach to collaborative training Machine Learning models. According to this novel framework, multiple participants train a global model collaboratively, coordinating with a central aggregator without sharing their local data. As FL gains popularity in diverse domains, security, and privacy concerns arise due to the distributed nature of this solution. Therefore, integrating this strategy with Blockchain technology has been consolidated as a preferred choice to ensure the privacy and security of participants. This paper explores the research efforts carried out by the scientific community to define privacy solutions in scenarios adopting Blockchain-Enabled FL. It comprehensively summarizes the background related to FL and Blockchain, evaluates existing architectures for their integration, and the primary attacks and possible countermeasures to guarantee privacy in this setting. Finally, it reviews the main application scenarios where Blockchain-Enabled FL approaches have been proficiently applied. This survey can help academia and industry practitioners understand which theories and techniques exist to improve the performance of FL through Blockchain to preserve privacy and which are the main challenges and future directions in this novel and still under-explored context. We believe this work provides a novel contribution concerning the previous surveys and is a valuable tool to explore the current landscape, understand perspectives, and pave the way for advancements or improvements in this amalgamation of Blockchain and Federated Learning.},
  doi      = {https://doi.org/10.1016/j.comcom.2024.04.024},
  file     = {:2024Sameera - Privacy Preserving in Blockchain Based Federated Learning Systems.pdf:PDF},
  keywords = {Federated Learning, Blockchain, Privacy, Blockchain-enabled FL, IoT, Industry 5.0},
  url      = {https://www.sciencedirect.com/science/article/pii/S0140366424001464},
}

@Article{Wang2023a,
  author   = {Wang, Zhilin and Hu, Qin and Li, Ruinian and Xu, Minghui and Xiong, Zehui},
  journal  = {IEEE Trans. Parallel Distrib. Syst.},
  title    = {Incentive Mechanism Design for Joint Resource Allocation in Blockchain-Based Federated Learning},
  year     = {2023},
  month    = may,
  number   = {5},
  pages    = {1536-1547},
  volume   = {34},
  comment  = {Incentive mechanism based on shapley value},
  doi      = {10.1109/TPDS.2023.3253604},
  file     = {:2023MayWang - Incentive Mechanism Design for Joint Resource Allocation in Blockchain Based Federated Learning.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Training,Blockchains,Task analysis,Resource management,Computational modeling,Data models,Games,Blockchain,federated learning,game theory,incentive mechanism,resource allocation},
}

@Misc{Magureanu2024,
  author        = {Horia Magureanu and Naïri Usher},
  title         = {Consensus learning: A novel decentralised ensemble learning paradigm},
  year          = {2024},
  archiveprefix = {arXiv},
  comment       = {consensus learning:},
  eprint        = {2402.16157},
  file          = {:2024Magureanu - Consensus Learning_ a Novel Decentralised Ensemble Learning Paradigm.pdf:PDF},
  groups        = {Blockchain ML, Decentralized Learning},
  primaryclass  = {cs.LG},
  ranking       = {rank1},
  url           = {https://arxiv.org/abs/2402.16157},
}

@Article{Georgopoulos2014,
  author   = {Leonidas Georgopoulos and Martin Hasler},
  journal  = {Neurocomputing},
  title    = {Distributed machine learning in networks by consensus},
  year     = {2014},
  issn     = {0925-2312},
  month    = jan,
  pages    = {2-12},
  volume   = {124},
  abstract = {We propose an algorithm to learn from distributed data on a network of arbitrarily connected machines without exchange of the data-points. Parts of the dataset are processed locally at each machine, and then the consensus communication algorithm is employed to consolidate the results. This iterative two stage process converges as if the entire dataset had been on a single machine. The principal contribution of this paper is the proof of convergence of the distributed learning process in the general case that the learning algorithm is a contraction. Moreover, we derive the distributed update equation of a feed-forward neural network with back-propagation for the purpose of verifying the theoretical results. We employ a toy classification example and a real world binary classification dataset.},
  comment  = {consensus based machine learning (not blockchain)},
  doi      = {https://doi.org/10.1016/j.neucom.2012.12.055},
  file     = {:2014Georgopoulos - Distributed Machine Learning in Networks by Consensus.pdf:PDF},
  groups   = {Decentralized Learning},
  keywords = {Distributed machine learning, Parallel machine learning, Gradient descent, Consensus, Peer-to-peer learning, Neural networks},
}

@Article{Liu2020,
  author   = {Liu, Bo and Ding, Zhengtao and Lv, Chen},
  journal  = {IEEE Trans. Neural Netw. Learn. Syst.},
  title    = {Distributed Training for Multi-Layer Neural Networks by Consensus},
  year     = {2020},
  month    = may,
  number   = {5},
  pages    = {1771-1778},
  volume   = {31},
  comment  = {consensus based machine learning (not blockchain)},
  doi      = {10.1109/TNNLS.2019.2921926},
  file     = {:2020Liu - Distributed Training for Multi Layer Neural Networks by Consensus.pdf:PDF},
  groups   = {Decentralized Learning},
  keywords = {Training,Neural networks,Consensus algorithm,Distributed databases,Topology,Convergence,Network topology,Backpropagation,consensus,distributed training,graph theory,Lyapunov},
  ranking  = {rank1},
}

@Article{Hegedues2021,
  author   = {István Hegedűs and Gábor Danner and Márk Jelasity},
  journal  = {Journal of Parallel and Distributed Computing},
  title    = {Decentralized learning works: An empirical comparison of gossip learning and federated learning},
  year     = {2021},
  issn     = {0743-7315},
  pages    = {109-124},
  volume   = {148},
  abstract = {Machine learning over distributed data stored by many clients has important applications in use cases where data privacy is a key concern or central data storage is not an option. Recently, federated learning was proposed to solve this problem. The assumption is that the data itself is not collected centrally. In a master–worker architecture, the workers perform machine learning over their own data and the master merely aggregates the resulting models without seeing any raw data, not unlike the parameter server approach. Gossip learning is a decentralized alternative to federated learning that does not require an aggregation server or indeed any central component. The natural hypothesis is that gossip learning is strictly less efficient than federated learning due to relying on a more basic infrastructure: only message passing and no cloud resources. In this empirical study, we examine this hypothesis and we present a systematic comparison of the two approaches. The experimental scenarios include a real churn trace collected over mobile phones, continuous and bursty communication patterns, different network sizes and different distributions of the training data over the devices. We also evaluate a number of additional techniques including a compression technique based on sampling, and token account based flow control for gossip learning. We examine the aggregated cost of machine learning in both approaches. Surprisingly, the best gossip variants perform comparably to the best federated learning variants overall, so they offer a fully decentralized alternative to federated learning.},
  doi      = {https://doi.org/10.1016/j.jpdc.2020.10.006},
  file     = {:2021Hegedűs - Decentralized Learning Works_ an Empirical Comparison of Gossip Learning and Federated Learning.pdf:PDF},
  groups   = {Gossip Learning},
  keywords = {Federated learning, Gossip learning, Decentralized machine learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0743731520303890},
}

@InProceedings{Hegedues2019,
  author    = {Heged{\H{u}}s, Istv{\'a}n and Danner, G{\'a}bor and Jelasity, M{\'a}rk},
  booktitle = {Proc. 19th IFIP Int. Conf. Distrib. Appl. Interoperable Syst. (DAIS'19)},
  title     = {Gossip Learning as a Decentralized Alternative to Federated Learning},
  year      = {2019},
  address   = {Kongens Lyngby, DK},
  month     = jun,
  pages     = {74--90},
  abstract  = {Federated learning is a distributed machine learning approach for computing models over data collected by edge devices. Most importantly, the data itself is not collected centrally, but a master-worker architecture is applied where a master node performs aggregation and the edge devices are the workers, not unlike the parameter server approach. Gossip learning also assumes that the data remains at the edge devices, but it requires no aggregation server or any central component. In this empirical study, we present a thorough comparison of the two approaches. We examine the aggregated cost of machine learning in both cases, considering also a compression technique applicable in both approaches. We apply a real churn trace as well collected over mobile phones, and we also experiment with different distributions of the training data over the devices. Surprisingly, gossip learning actually outperforms federated learning in all the scenarios where the training data are distributed uniformly over the nodes, and it performs comparably to federated learning overall.},
  doi       = {10.1007/978-3-030-22496-7_5},
  file      = {:2019Hegedűs - Gossip Learning As a Decentralized Alternative to Federated Learning.pdf:PDF},
  groups    = {Gossip Learning},
  isbn      = {978-3-030-22496-7},
}

@InProceedings{Giaretta2019,
  author    = {Giaretta, Lodovico and Girdzijauskas, Šarūnas},
  booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
  title     = {Gossip Learning: Off the Beaten Path},
  year      = {2019},
  pages     = {1117-1124},
  doi       = {10.1109/BigData47090.2019.9006216},
  file      = {:2019Giaretta - Gossip Learning_ off the Beaten Path.pdf:PDF},
  groups    = {Gossip Learning},
  keywords  = {Protocols,Peer-to-peer computing,Data models,Topology,Network topology,Machine learning,Convergence},
}

@Misc{Guha2019,
  author        = {Neel Guha and Ameet Talwalkar and Virginia Smith},
  title         = {One-Shot Federated Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1902.11175},
  file          = {:2019Guha - One Shot Federated Learning.pdf:PDF},
  groups        = {Federated Ensemble, Knowledge Distillation},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1902.11175},
}

@InProceedings{zhang2022dense,
  author    = {Jie Zhang and Chen Chen and Bo Li and Lingjuan Lyu and Shuang Wu and Shouhong Ding and Chunhua Shen and Chao Wu},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {{DENSE}: Data-Free One-Shot Federated Learning},
  year      = {2022},
  editor    = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  file      = {:2022Zhang - DENSE_ Data Free One Shot Federated Learning.pdf:PDF},
  groups    = {Federated Ensemble, Knowledge Distillation},
  url       = {https://openreview.net/forum?id=QFQoxCFYEkA},
}

@Article{Lee2024,
  author   = {Yoonhyung Lee and Seokho Kang},
  journal  = {Computers & Industrial Engineering},
  title    = {Dynamic ensemble of regression neural networks based on predictive uncertainty},
  year     = {2024},
  issn     = {0360-8352},
  pages    = {110011},
  volume   = {190},
  abstract = {In a data-decentralized environment where data are distributed across individual local nodes, inter-node data sharing is often restricted due to security concerns and communication costs. Under these restrictions, using entire data to train a global predictive model is challenging. A viable strategy is to leverage local predictive models trained independently on local nodes. However, this adversely affect generalization performance, particularly when the data distribution varies between local nodes. To address this issue in regression problems, we propose a dynamic ensemble method that combines the predictions of locally trained neural networks without the need to access data from local nodes. For a query instance, the prediction and its uncertainty from each neural network are computed by applying Monte Carlo dropout. Subsequently, smaller/larger weights are assigned to neural networks with higher/lower predictive uncertainty. The final prediction result is derived using the weighted average of the individual predictions. The proposed method is applicable to a data-decentralized environment where data are inaccessible and thus cannot be shared across local nodes. Transmitting query instances to local nodes and receiving predictions and weights from the local nodes are the only requirements, thereby ensuring low communication costs. Experimental results on simulated data-decentralized environments using benchmark regression datasets validated the superior generalization performance of the proposed method compared with baseline methods. The proposed method was found to be more effective for regression neural networks that demonstrate higher performance at their respective local nodes.},
  doi      = {https://doi.org/10.1016/j.cie.2024.110011},
  file     = {:2024Lee - Dynamic Ensemble of Regression Neural Networks Based on Predictive Uncertainty.pdf:PDF},
  groups   = {Dynamic ensemble},
  keywords = {Regression, Dynamic ensemble, Neural network, Uncertainty quantification, Decentralized environment},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360835224001323},
}

@Article{Britto2014,
  author   = {Alceu S. Britto and Robert Sabourin and Luiz E.S. Oliveira},
  journal  = {Pattern Recognition},
  title    = {Dynamic selection of classifiers—A comprehensive review},
  year     = {2014},
  issn     = {0031-3203},
  number   = {11},
  pages    = {3665-3680},
  volume   = {47},
  abstract = {This work presents a literature review of multiple classifier systems based on the dynamic selection of classifiers. First, it briefly reviews some basic concepts and definitions related to such a classification approach and then it presents the state of the art organized according to a proposed taxonomy. In addition, a two-step analysis is applied to the results of the main methods reported in the literature, considering different classification problems. The first step is based on statistical analyses of the significance of these results. The idea is to figure out the problems for which a significant contribution can be observed in terms of classification performance by using a dynamic selection approach. The second step, based on data complexity measures, is used to investigate whether or not a relation exists between the possible performance contribution and the complexity of the classification problem. From this comprehensive study, we observed that, for some classification problems, the performance contribution of the dynamic selection approach is statistically significant when compared to that of a single-based classifier. In addition, we found evidence of a relation between the observed performance contribution and the complexity of the classification problem. These observations allow us to suggest, from the classification problem complexity, that further work should be done to predict whether or not to use a dynamic selection approach.},
  doi      = {https://doi.org/10.1016/j.patcog.2014.05.003},
  file     = {:2014Britto - Dynamic Selection of Classifiers—A Comprehensive Review.pdf:PDF},
  groups   = {Dynamic ensemble},
  keywords = {Ensemble of classifiers, Dynamic selection of classifiers, Data complexity},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320314001885},
}

@Article{Ko2008,
  author   = {Albert H.R. Ko and Robert Sabourin and Alceu Souza {Britto, Jr.}},
  journal  = {Pattern Recognition},
  title    = {From dynamic classifier selection to dynamic ensemble selection},
  year     = {2008},
  issn     = {0031-3203},
  number   = {5},
  pages    = {1718-1731},
  volume   = {41},
  abstract = {In handwritten pattern recognition, the multiple classifier system has been shown to be useful for improving recognition rates. One of the most important tasks in optimizing a multiple classifier system is to select a group of adequate classifiers, known as an Ensemble of Classifiers (EoC), from a pool of classifiers. Static selection schemes select an EoC for all test patterns, and dynamic selection schemes select different classifiers for different test patterns. Nevertheless, it has been shown that traditional dynamic selection performs no better than static selection. We propose four new dynamic selection schemes which explore the properties of the oracle concept. Our results suggest that the proposed schemes, using the majority voting rule for combining classifiers, perform better than the static selection method.},
  doi      = {https://doi.org/10.1016/j.patcog.2007.10.015},
  file     = {:2008Ko - From Dynamic Classifier Selection to Dynamic Ensemble Selection.pdf:PDF},
  groups   = {Dynamic ensemble},
  keywords = {Oracle, Combining classifiers, Classifier selection, Ensemble selection, Pattern recognition, Majority voting, Ensemble of learning machines},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320307004499},
}

@InProceedings{Zhang2020,
  author    = {Zhang, Shulai and Ma, Xiaoli},
  booktitle = {Proc. 2020 IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)},
  title     = {A General Difficulty Control Algorithm for Proof-of-Work Based Blockchains},
  year      = {2020},
  address   = {Barcelona, ES},
  month     = may,
  pages     = {3077-3081},
  doi       = {10.1109/ICASSP40776.2020.9054286},
  file      = {:2020MayZhang - A General Difficulty Control Algorithm for Proof of Work Based Blockchains.pdf:PDF},
  groups    = {ML and Blockchain, Blockchain optimization},
  keywords  = {Neural networks,Memory management,Signal processing algorithms,Blockchain,Signal processing,Data models,Speech processing,blockchain,difficulty control algorithm,hash rate},
}

@Article{Wang2024b,
  author   = {Wang, Jishu and Wang, Yaowei and Zhang, Xuan and Jin, Zhi and Zhu, Chao and Li, Linyu and Zhu, Rui and Lv, Shenglong},
  journal  = {IEEE Trans. Netw. Serv. Manage.},
  title    = {{LearningChain}: A Highly Scalable and Applicable Learning-Based Blockchain Performance Optimization Framework},
  year     = {2024},
  month    = apr,
  number   = {2},
  pages    = {1817-1831},
  volume   = {21},
  doi      = {10.1109/TNSM.2023.3347789},
  file     = {:2024AprilWang - LearningChain_ a Highly Scalable and Applicable Learning Based Blockchain Performance Optimization Framework.pdf:PDF},
  groups   = {Blockchain optimization},
  keywords = {Blockchains,Optimization,Predictive models,Throughput,Training,Metalearning,Data models,Blockchain,blockchain performance prediction model,blockchain performance optimization,machine learning},
}

@Article{Venkatesan2024,
  author    = {Venkatesan, K. and Rahayu, Syarifah Bahiyah},
  journal   = {Sci. Rep.},
  title     = {Blockchain security enhancement: an approach towards hybrid consensus algorithms and machine learning techniques},
  year      = {2024},
  issn      = {2045-2322},
  month     = jan,
  number    = {1},
  pages     = {1--24},
  volume    = {14},
  doi       = {10.1038/s41598-024-51578-7},
  file      = {:2024JanuaryVenkatesan - Blockchain Security Enhancement_ an Approach Towards Hybrid Consensus Algorithms and Machine Learning Techniques.pdf:PDF},
  groups    = {Blockchain optimization},
  publisher = {Springer Science and Business Media LLC},
}

@Article{UlHassan2023,
  author   = {Ul Hassan, Muneeb and Rehmani, Mubashir Husain and Chen, Jinjun},
  journal  = {IEEE Commun. Surveys Tuts.},
  title    = {Anomaly Detection in Blockchain Networks: A Comprehensive Survey},
  year     = {2023},
  month    = {1st Quart.},
  number   = {1},
  pages    = {289-318},
  volume   = {25},
  doi      = {10.1109/COMST.2022.3205643},
  file     = {:2023Ul Hassan - Anomaly Detection in Blockchain Networks_ a Comprehensive Survey.pdf:PDF},
  groups   = {Blockchain optimization},
  keywords = {Blockchains,Anomaly detection,Security,Smart contracts,Privacy,Bitcoin,Tutorials,Blockchain,anomaly detection,fraud detection},
}

@Article{Li2022,
  author   = {Li, Zhongguo and Liu, Bo and Ding, Zhengtao},
  journal  = {IEEE Trans. Neural Netw. Learn. Syst.},
  title    = {Consensus-Based Cooperative Algorithms for Training Over Distributed Data Sets Using Stochastic Gradients},
  year     = {2022},
  month    = oct,
  number   = {10},
  pages    = {5579-5589},
  volume   = {33},
  doi      = {10.1109/TNNLS.2021.3071058},
  file     = {:2022OctoberLi - Consensus Based Cooperative Algorithms for Training Over Distributed Data Sets Using Stochastic Gradients.pdf:PDF},
  groups   = {Decentralized Learning},
  keywords = {Training,Neural networks,Convergence,Distributed databases,Machine learning algorithms,Optimization,Distributed algorithms,Consensus,convergence analysis,distributed training,multiagent systems,neural networks,optimization},
}

@Article{Cao2023,
  author   = {Cao, Mingrui and Zhang, Long and Cao, Bin},
  journal  = {IEEE Trans. Neural Netw. Learn. Syst.},
  title    = {Toward On-Device Federated Learning: A Direct Acyclic Graph-Based Blockchain Approach},
  year     = {2023},
  month    = apr,
  number   = {4},
  pages    = {2028-2042},
  volume   = {34},
  comment  = {DAG-FL
Motivation to integrate DAG:
1. anomaly detection, protect the FL system from abnormal nodes
2. reduce resource consumption

conference version:

- Architectural design
- Consensus based Anomaly Detection
- DAG-FL operations: controlling & updating, 4 stages in one iteration

- Simulation (three figures): 
  - Compare with different FL systems
  - The effect of **lazy nodes** on different FL systems
  - The effect of **malicious nodes** on different FL systems

Extended parts in the journal version:

- Deployment and stability analysis

  - Maintaining the tip number in DAG around a constant value
  - Making the nodes prefer "fresher" transactions

- More simulations:

  - The effect of the **number** of lazy nodes on different FL systems
  - The effect of the **number** of malicious nodes on different FL system (include **poisoning nodes** and **backdoor nodes**, which is not presented separately in the conference version)

- Testbed (Case study), implement the system with Aliyun
- Some discussions (model validation, credit evaluation, weighted aggregation)
- Related work},
  doi      = {10.1109/TNNLS.2021.3105810},
  file     = {:2023Cao - Toward on Device Federated Learning_ a Direct Acyclic Graph Based Blockchain Approach.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Blockchains,Servers,Data models,Peer-to-peer computing,Wireless networks,Task analysis,Internet,Anomaly detection,asynchrony,blockchain,direct acyclic graph (DAG),federated learning (FL)},
}

@Article{Savazzi2020,
  author     = {Savazzi, Stefano and Nicoli, Monica and Rampa, Vittorio},
  journal    = {IEEE Internet Things J.},
  title      = {Federated Learning With Cooperating Devices: A Consensus Approach for Massive IoT Networks},
  year       = {2020},
  number     = {5},
  pages      = {4641-4654},
  volume     = {7},
  comment    = {consensus based federated learning (with gradient exchange)},
  doi        = {10.1109/JIOT.2020.2964162},
  file       = {:2020Savazzi - Federated Learning with Cooperating Devices_ a Consensus Approach for Massive IoT Networks.pdf:PDF},
  groups     = {Decentralized Learning, Decentralized FL},
  keywords   = {Servers,Data models,Computational modeling,Artificial neural networks,Optimization,Convergence,Internet of Things,5G and beyond networks,distributed signal processing,federated learning,internet of Things},
  readstatus = {skimmed},
}

@Article{Du2023,
  author   = {Du, Mengxuan and Zheng, Haifeng and Feng, Xinxin and Chen, Youjia and Zhao, Tiesong},
  journal  = {IEEE Transactions on Industrial Informatics},
  title    = {Decentralized Federated Learning With Markov Chain Based Consensus for Industrial IoT Networks},
  year     = {2023},
  number   = {4},
  pages    = {6006-6015},
  volume   = {19},
  comment  = {fast mixing Markov chain based model consensus algorithm, a type of consensus learning, but optimize the consensus  matrix $C$ by finding solutions to the formulated fast mixing Markov chain problem.},
  doi      = {10.1109/TII.2022.3192297},
  file     = {:2023Du - Decentralized Federated Learning with Markov Chain Based Consensus for Industrial IoT Networks.pdf:PDF},
  groups   = {Decentralized Learning, Decentralized FL},
  keywords = {Industrial Internet of Things,Servers,Data models,Costs,Training,Performance evaluation,Job shop scheduling,Communication compression,decentralized federated learning (DFL),fastest mixing Markov chain (FMMC),model consensus},
}

@InProceedings{Raza2023,
  author    = {Raza, Ali and Tran, Kim Phuc and Koehl, Ludovic and Li, Shujun},
  booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
  title     = {Proof of Swarm Based Ensemble Learning for Federated Learning Applications},
  year      = {2023},
  address   = {New York, NY, USA},
  pages     = {152–155},
  publisher = {Association for Computing Machinery},
  series    = {SAC '23},
  abstract  = {Ensemble learning combines results from multiple machine learning models in order to provide a better and optimised predictive model with reduced bias, variance and improved predictions. However, in federated learning it is not feasible to apply centralised ensemble learning directly due to privacy concerns. Hence, a mechanism is required to combine results of local models to produce a global model. Most distributed consensus algorithms, such as Byzantine fault tolerance (BFT), do not normally perform well in such applications. This is because, in such methods predictions of some of the peers are disregarded, so a majority of peers can win without even considering other peers' decisions. Additionally, the confidence score of the result of each peer is not normally taken into account, although it is an important feature to consider for ensemble learning. Moreover, the problem of a tie event is often left un-addressed by methods such as BFT. To fill these research gaps, we propose PoSw (Proof of Swarm), a novel distributed consensus algorithm for ensemble learning in a federated setting, which was inspired by particle swarm based algorithms for solving optimisation problems. The proposed algorithm is theoretically proved to always converge in a relatively small number of steps and has mechanisms to resolve tie events while trying to achieve sub-optimum solutions. We experimentally validated the performance of the proposed algorithm using ECG classification as an example application in healthcare, showing that the ensemble learning model outperformed all local models and even the FL-based global model.},
  comment   = {use a personalized FL scenario
PoSW, an ensemble learning technique that reach consensus(not blockchain) among the predictions from different edge devices},
  doi       = {10.1145/3555776.3578601},
  file      = {:2023Raza - Proof of Swarm Based Ensemble Learning for Federated Learning Applications.pdf:PDF},
  groups    = {Decentralized Learning},
  isbn      = {9781450395175},
  keywords  = {privacy, federated learning, ensemble, consensus protocol, evolutionary computing, swarm algorithms, healthcare},
  location  = {Tallinn, Estonia},
  numpages  = {4},
  ranking   = {rank1},
  url       = {https://doi.org/10.1145/3555776.3578601},
}

@InProceedings{Gholami2022,
  author    = {Gholami, Anousheh and Torkzaban, Nariman and Baras, John S.},
  booktitle = {2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC)},
  title     = {Trusted Decentralized Federated Learning},
  year      = {2022},
  pages     = {1-6},
  comment   = {non-blockchain approach to introduce trust into decentralized FL.},
  doi       = {10.1109/CCNC49033.2022.9700624},
  file      = {:2022Gholami - Trusted Decentralized Federated Learning.pdf:PDF},
  groups    = {Decentralized FL},
  keywords  = {Training,Privacy,Scalability,Neural networks,Packet loss,Collaborative work,Inference algorithms,Federated learning,consensus algorithm,trust aggregation,trusted federated learning},
}

@InProceedings{Niwa2020,
  author    = {Niwa, Kenta and Harada, Noboru and Zhang, Guoqiang and Kleijn, W. Bastiaan},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  title     = {Edge-consensus Learning: Deep Learning on P2P Networks with Nonhomogeneous Data},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {668–678},
  publisher = {Association for Computing Machinery},
  series    = {KDD '20},
  abstract  = {An effective Deep Neural Network (DNN) optimization algorithm that can use decentralized data sets over a peer-to-peer (P2P) network is proposed. In applications such as medical data analysis, the aggregation of data in one location may not be possible due to privacy issues. Hence, we formulate an algorithm to reach a global DNN model that does not require transmission of data among nodes. An existing solution for this issue is gossip stochastic gradient descend (SGD), which updates by averaging node models over a P2P network. However, in practical situations where the data are statistically heterogeneous across the nodes and/or where communication is asynchronous, gossip SGD often gets trapped in local minimum since the model gradients are noticeably different. To overcome this issue, we solve a linearly constrained DNN cost minimization problem, which results in variable update rules that restrict differences among all node models. Our approach can be based on the Primal-Dual Method of Multipliers (PDMM) or the Alternating Direction Method of Multiplier (ADMM), but the cost function is linearized to be suitable for deep learning. It facilitates asynchronous communication. The results of our numerical experiments using CIFAR-10 indicate that the proposed algorithms converge to a global recognition model even though statistically heterogeneous data sets are placed on the nodes.},
  comment   = {ADMM SGD and PDMM SGD},
  doi       = {10.1145/3394486.3403109},
  file      = {:2020Niwa - Edge Consensus Learning_ Deep Learning on P2P Networks with Nonhomogeneous Data.pdf:PDF},
  groups    = {Decentralized Learning},
  isbn      = {9781450379984},
  keywords  = {alternating direction method of multiplier (ADMM), asynchronous communication, deep neural network (DNN), non-independent and identically distributed (non-iid) data, peer-to-peer (P2P) network, primal-dual method of multiplier (PDMM)},
  location  = {Virtual Event, CA, USA},
  numpages  = {11},
  priority  = {prio3},
  ranking   = {rank3},
  url       = {https://doi.org/10.1145/3394486.3403109},
}

@InProceedings{Jiang2017,
  author    = {Jiang, Zhanhong and Balu, Aditya and Hegde, Chinmay and Sarkar, Soumik},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {Collaborative deep learning in fixed topology networks},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {5906–5916},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.},
  comment   = {Consensus distributed SGD},
  file      = {:2017Jiang - Collaborative Deep Learning in Fixed Topology Networks.pdf:PDF},
  groups    = {Decentralized Learning},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {11},
}

@InProceedings{Cui2023,
  author    = {Cui, Bo and Mei, Tianyu},
  booktitle = {Proceedings of the 39th Annual Computer Security Applications Conference},
  title     = {ABFL: A Blockchain-enabled Robust Framework for Secure and Trustworthy Federated Learning},
  year      = {2023},
  address   = {New York, NY, USA},
  pages     = {636–646},
  publisher = {Association for Computing Machinery},
  series    = {ACSAC '23},
  abstract  = {The demand for effective, safe, and privacy-preserving machine learning methods has increased due to the rapid growth of large pre-trained models in recent years. In large-scale AI applications, federated learning (FL) has emerged as a cutting-edge method for addressing privacy and data silos issues. However, FL systems are vulnerable to poisoning attacks, and centralized master-slave architectures have reliability, fairness, and security limitations. We propose a secure and efficient decentralized FL framework called ABFL to address these challenges. The framework tightly integrates FL with blockchain technology to strengthen data ownership guarantees and significantly lessen the negative impact of malicious nodes on the global model. Using historical data stored on the blockchain, ABFL enables model update prediction and identifies malicious nodes by verifying consistency. In addition, we present a novel agent consensus mechanism to lower the expense of model cross-validation and increase consensus efficiency. The ABFL framework’s robustness to various sophisticated poisoning attacks while maintaining high model performance and increasing consensus efficiency is demonstrated in a comprehensive analysis of three benchmark datasets.},
  comment   = {"blockchain-empowered decentralized FL"

package global model into a PoS based blockchain

use honesty score to evaluate each miner's reputation

use a cosine similarity  based approach to reinforce consistency of model updates and eliminate poisoning attacks},
  doi       = {10.1145/3627106.3627121},
  file      = {:2023Cui - ABFL_ a Blockchain Enabled Robust Framework for Secure and Trustworthy Federated Learning.pdf:PDF},
  groups    = {Blockchain-based FL},
  isbn      = {9798400708862},
  keywords  = {Blockchain, Consensus Mechanism, Federated Learning, Poisoning Attack},
  location  = {Austin, TX, USA},
  numpages  = {11},
  ranking   = {rank2},
  url       = {https://doi.org/10.1145/3627106.3627121},
}

@Article{Ormandi2013,
  author   = {Ormándi, Róbert and Hegedűs, István and Jelasity, Márk},
  journal  = {Concurrency Comput. Pract. Exper},
  title    = {Gossip learning with linear models on fully distributed data},
  year     = {2013},
  number   = {4},
  pages    = {556-571},
  volume   = {25},
  abstract = {SUMMARY Machine learning over fully distributed data poses an important problem in peer-to-peer applications. In this model, we have one data record at each network node but without the possibility to move raw data because of privacy considerations. For example, user profiles, ratings, history, or sensor readings can represent this case. This problem is difficult because there is no possibility to learn local models; the system model offers almost no guarantee for reliability, yet the communication cost needs to be kept low. Here, we propose gossip learning, a generic approach that is based on multiple models taking random walks over the network in parallel, while applying an online learning algorithm to improve themselves, and getting combined via ensemble learning methods. We present an instantiation of this approach for the case of classification with linear models. Our main contribution is an ensemble learning method, which—through the continuous combination of the models in the network—implements a virtual weighted voting mechanism over an exponential number of models at practically no extra cost as compared with independent random walks. We prove the convergence of the method theoretically, and perform extensive experiments on benchmark data sets. Our experimental analysis demonstrates the performance and robustness of the proposed approach. Copyright © 2012 John Wiley \& Sons, Ltd.},
  comment  = {提出Gossip Learning，在完全分布式的数据上实现机器学习，并且为线性模型提出一种分布式集成学习方法},
  doi      = {10.1002/cpe.2858},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.2858},
  file     = {:2013Ormándi - Gossip Learning with Linear Models on Fully Distributed Data.pdf:PDF},
  groups   = {Distributed Learning, Gossip Learning},
  keywords = {P2P, gossip, bagging, online learning, stochastic gradient descent, random walk},
  ranking  = {rank3},
}

@InProceedings{McMahan2017,
  author    = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
  booktitle = {Proc. 20th Int. Conf. Artif. Intell. Statist.},
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  year      = {2017},
  address   = {Fort Lauderdale, FL, USA},
  month     = apr,
  pages     = {1273--1282},
  abstract  = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  comment   = {Federated Learning is first proposed in this paper, featuring a FedAvg algorithm.},
  file      = {:201720--22 AprMcMahan - Communication Efficient Learning of Deep Networks from Decentralized Data.pdf:PDF},
  groups    = {Distributed Learning, FL Research},
  pdf       = {http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf},
  ranking   = {rank4},
}

@InProceedings{Zhang2024c,
  author    = {Zhang, Cheng and Xu, Yang and Wu, Xiaowei and Wang, En and Jiang, Hongbo and Zhang, Yaoxue},
  booktitle = {Proc. 2024 IEEE Conf. Comput. Commun. (INFOCOM)},
  title     = {A Semi-Asynchronous Decentralized Federated Learning Framework via Tree-Graph Blockchain},
  year      = {2024},
  address   = {Vancouver, BC, Canada},
  month     = may,
  pages     = {1121-1130},
  comment   = {Use a tree graph to coordinate asynchronous FL 

Problem:
How to solve the "forks" in the tree graph? How to ensure the consistency of states across the blockchain network?},
  doi       = {10.1109/INFOCOM52122.2024.10621425},
  file      = {:2024MayZhang - A Semi Asynchronous Decentralized Federated Learning Framework Via Tree Graph Blockchain.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {Training,Fault tolerance,Directed acyclic graph,Federated learning,Computational modeling,Fault tolerant systems,Resists},
  ranking   = {rank3},
}

@Article{Sandeepa2022,
  author   = {Chamara Sandeepa and Bartlomiej Siniarski and Nicolas Kourtellis and Shen Wang and Madhusanka Liyanage},
  journal  = {Journal of Industrial Information Integration},
  title    = {A survey on privacy for B5G/6G: New privacy challenges, and research directions},
  year     = {2022},
  issn     = {2452-414X},
  pages    = {100405},
  volume   = {30},
  abstract = {Massive developments in mobile wireless telecommunication networks have been made during the last few decades. At present, mobile users are getting familiar with the latest 5G networks, and the discussion for the next generation of Beyond 5G (B5G)/6G networks has already been initiated. It is expected that B5G/6G will push the existing network capabilities to the next level, with higher speeds, enhanced reliability and seamless connectivity. To make these expectations a reality, research is progressing on new technologies, architectures, and intelligence-based decision-making processes related to B5G/6G. Privacy considerations are a crucial aspect that requires further attention in such developments, as billions of people and devices will be transmitting data through the upcoming network. However, the main recognition remains biased towards the network security. A discussion focused on privacy of B5G/6G is lacking at the moment. To address the gap, this paper provides a comprehensive survey on privacy-related aspects of B5G/6G networks. First, it discusses a taxonomy of different privacy perspectives. Based on the taxonomy, the paper then conceptualizes a set of challenges that appear as barriers to reach privacy preservation. Next, this work provides a set of solutions applicable to the proposed architecture of B5G/6G networks to mitigate the challenges. It also provides an overview of standardization initiatives for privacy preservation. Finally, the paper concludes with a roadmap of future directions, which will be an arena for new research towards privacy-enhanced B5G/6G networks. This work provides a basis for privacy aspects that will significantly impact peoples’ daily lives when using these future networks.},
  comment  = {Mention Gossip learning and its deficiency:
Malicious nodes in gossip learning resulting in less accurate models and privacy leakages when merging. Slow convergence of models in gossip learning},
  doi      = {https://doi.org/10.1016/j.jii.2022.100405},
  file     = {:2022Sandeepa - A Survey on Privacy for B5G_6G_ New Privacy Challenges, and Research Directions.pdf:PDF},
  groups   = {Wireless Network},
  keywords = {Beyond 5G, 6G, Privacy issues, Privacy solutions, Artificial intelligence, Machine learning, Explainable AI, Survey},
  url      = {https://www.sciencedirect.com/science/article/pii/S2452414X22000723},
}

@InProceedings{Lukas2022,
  author     = {Lukas, Nils and Jiang, Edward and Li, Xinda and Kerschbaum, Florian},
  booktitle  = {Proc. 2022 IEEE Symp. Secur. Privacy (SP)},
  title      = {{SoK}: How Robust is Image Classification Deep Neural Network Watermarking?},
  year       = {2022},
  address    = {San Francisco, CA, USA},
  month      = may,
  pages      = {787-804},
  comment    = {Source Code:[dnn-Security/watermark-Robustness-Toolbox: The Official Implementation Of The Ieee S\&p\`22 Paper "sok: How Robust Is Deep Neural Network Image Classification Watermarking".](https://github.com/dnn-Security/watermark-Robustness-Toolbox?tab=readme-Ov-File) 
Implement Some Popular Watermarking Techniques And Evaluate The Anti Removal Attack Performance.

优点： 
1. 统一框架，尝试统一stealing loss、watermark accuracy定义
2. decision threshold判断，1中两个metric的标准化方法
3. 实现各类水印方法+攻击手段

缺点：
1. 复现不方便，有bug
2. 有一些方法并没有覆盖
3. 有一些水印方法实现得不完整
4. 提出的攻击方法实际上无效
5. 验证集、测试集不分},
  doi        = {10.1109/SP46214.2022.9833693},
  file       = {:2022Lukas - SoK_ How Robust Is Image Classification Deep Neural Network Watermarking_.pdf:PDF},
  groups     = {DNN watermarking},
  keywords   = {Deep learning,Analytical models,Adaptation models,Uncertainty,Taxonomy,Transfer learning,Neural networks,Deep Neural Network,Watermarking,Robustness,Removal Attacks,Image Classification},
  ranking    = {rank2},
  readstatus = {read},
}

@Article{Chen2022,
  author   = {Chen, Zhi and Duan, Jiang and Kang, Li and Qiu, Guoping},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {Class-Imbalanced Deep Learning via a Class-Balanced Ensemble},
  year     = {2022},
  number   = {10},
  pages    = {5626-5640},
  volume   = {33},
  doi      = {10.1109/TNNLS.2021.3071122},
  file     = {:Class-Imbalanced_Deep_Learning_via_a_Class-Balanced_Ensemble.pdf:PDF},
  groups   = {Imbalance Learning},
  keywords = {Deep learning,Training,Task analysis,Data models,Computational modeling,Training data,Optimization,Class-imbalanced deep learning,class-balanced ensemble,deep convolutional neural network (CNN),image classification},
  ranking  = {rank2},
}

@InProceedings{Cui2024,
  author    = {Zixiang Cui and Xintong Ling and Xingyu Zhou and Jiaheng Wang},
  booktitle = {Proc. 2024 IEEE Globecom Workshops (GC Wkshps)},
  title     = {A Bagging-based Consensus Protocol for Distributed Learning},
  year      = {2024},
  address   = {Cape Town, ZA},
  month     = dec,
  pages     = {1--6},
  abstract  = {The high energy consumption of the proof of work (PoW) consensus mechanism
in existing blockchain systems challenges environmental sustainability. To
address this issue, we propose a novel consensus protocol named BagChain
which reuses energy wasted in solving hash puzzles for machine learning
(ML) tasks. We design a specific chain structure and workflow for BagChain
to enable bagging-based decentralized learning. The newly proposed
consensus protocol integrates blockchain maintenance with distributed ML
training. BagChain coordinates the miners' limited computing power and also
their private data samples to train their own base models for boosting the
performance of the final ensemble model. At last, we utilize the ChainXim
blockchain simulator to illustrate the efficacy of BagChain under various
network conditions.},
  days      = {7},
  groups    = {Blockchain ML},
  keywords  = {bagging, blockchain, consensus protocol, machine learning},
}

@InBook{Li2023b,
  author    = {Li, Bowen and Fan, Lixin and Gu, Hanlin and Li, Jie and Yang, Qiang},
  editor    = {Fan, Lixin and Chan, Chee Seng and Yang, Qiang},
  pages     = {193--210},
  publisher = {Springer Nature Singapore},
  title     = {FedIPR: Ownership Verification for Federated Deep Neural Network Models},
  year      = {2023},
  address   = {Singapore},
  isbn      = {978-981-19-7554-7},
  abstract  = {In federated learning, multiple clients collaboratively develop models upon their private data. However, IP risks including illegal copying, re-distribution, and free-riding threat the collaboratively built models in federated learning. To address IP infringement issues, in this chapter, we introduce a novel deep neural network ownership verification framework for secure federated learning that allows each client to embed and extract private watermarks in federated learning models for legitimate IPR. In the proposed FedIPR scheme, each client independently extracts the watermarks and claims ownership on the federated learning model while keep training data and watermark private.},
  booktitle = {Digital Watermarking for Machine Learning Model: Techniques, Protocols and Applications},
  doi       = {10.1007/978-981-19-7554-7_10},
  file      = {:2023Li - FedIPR_ Ownership Verification for Federated Deep Neural Network Models.pdf:PDF},
  groups    = {DNN watermarking, FL Research},
  url       = {https://doi.org/10.1007/978-981-19-7554-7_10},
}

@Article{Shao2024,
  author   = {Shao, Shuo and Yang, Wenyuan and Gu, Hanlin and Qin, Zhan and Fan, Lixin and Yang, Qiang},
  journal  = {IEEE Transactions on Dependable and Secure Computing},
  title    = {FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model},
  year     = {2024},
  pages    = {1-18},
  doi      = {10.1109/TDSC.2024.3390761},
  file     = {:2024Shao - FedTracker_ Furnishing Ownership Verification and Traceability for Federated Learning Model.pdf:PDF},
  groups   = {DNN watermarking, FL Research},
  keywords = {Watermarking,Fingerprint recognition,Servers,Computational modeling,Task analysis,Protection,Data models,Federated learning,model watermark,ownership verification,traceability},
}

@InProceedings{Li2022b,
  author    = {Li, Fang-Qi and Wang, Shi-Lin and Liew, Alan Wee-Chung},
  booktitle = {2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
  title     = {Watermarking Protocol for Deep Neural Network Ownership Regulation in Federated Learning},
  year      = {2022},
  pages     = {1-4},
  doi       = {10.1109/ICMEW56448.2022.9859395},
  file      = {:2022Li - Watermarking Protocol for Deep Neural Network Ownership Regulation in Federated Learning.pdf:PDF},
  groups    = {DNN watermarking, FL Research},
  keywords  = {Deep learning,Computer aided instruction,Protocols,Distance learning,Forensics,Conferences,Neural networks,Deep neural network watermark,federated learning,machine learning security and forensics},
}

@InProceedings{Chen2019,
  author     = {Chen, Huili and Rouhani, Bita Darvish and Fu, Cheng and Zhao, Jishen and Koushanfar, Farinaz},
  booktitle  = {Proceedings of the 2019 on International Conference on Multimedia Retrieval},
  title      = {DeepMarks: A Secure Fingerprinting Framework for Digital Rights Management of Deep Learning Models},
  year       = {2019},
  address    = {New York, NY, USA},
  pages      = {105–113},
  publisher  = {Association for Computing Machinery},
  series     = {ICMR '19},
  abstract   = {Deep Neural Networks (DNNs) are revolutionizing various critical fields by providing an unprecedented leap in terms of accuracy and functionality. Due to the costly training procedure, high-performance DNNs are typically considered as the Intellectual Property (IP) of the model builder and need to be protected. While DNNs are increasingly commercialized, the pre-trained models might be illegally copied or redistributed after they are delivered to malicious users. In this paper, we introduce DeepMarks, the first end-to-end collusion-secure fingerprinting framework that enables the owner to retrieve model authorship information and identification of unique users in the context of deep learning (DL). DeepMarks consists of two main modules: (i) Designing unique fingerprints using anti-collusion codebooks for individual users; and (ii) Encoding each constructed fingerprint (FP) in the probability density function (pdf) of the weights by incorporating an FP-specific regularization loss during DNN re-training. We investigate the performance of DeepMarks on various datasets and DNN architectures. Experimental results show that the embedded FP preserves the accuracy of the host DNN and is robust against different model modifications that might be conducted by the malicious user. Furthermore, our framework is scalable and yields perfect detection rates and no false alarms when identifying the participants of FP collusion attacks under theoretical guarantee. The runtime overhead of retrieving the embedded FP from the marked DNN can be as low as 0.056\%.},
  comment    = {apply anti-collusion techniques to Uchida's scheme; use MSE loss rather than cross entropy loss.

1. why the design of AND-ACC codebook make the system output and logic-AND of the two code when inputing the **average** of the weight of the two models? （solved）
2. what if the attackers use weighted average or retrain/finetune the model?
3. it is hard to construct orthogonal matrix in a blockchain environment

similar paper: anti-collusion in federated learning: Copyright protection framework for federated learning models against collusion attacks

doesn't seem to be useful},
  doi        = {10.1145/3323873.3325042},
  file       = {:2019Chen - DeepMarks_ a Secure Fingerprinting Framework for Digital Rights Management of Deep Learning Models.pdf:PDF},
  groups     = {DNN watermarking},
  isbn       = {9781450367653},
  keywords   = {deep neural networks, digital fingerprinting, digital right management, intellectual property protection},
  location   = {Ottawa ON, Canada},
  numpages   = {9},
  ranking    = {rank2},
  readstatus = {read},
  url        = {https://doi.org/10.1145/3323873.3325042},
}

@InProceedings{Zhao2021,
  author    = {Zhao, Xiangyu and Yao, Yinzhe and Wu, Hanzhou and Zhang, Xinpeng},
  booktitle = {2021 IEEE International Workshop on Information Forensics and Security (WIFS)},
  title     = {Structural Watermarking to Deep Neural Networks via Network Channel Pruning},
  year      = {2021},
  pages     = {1-6},
  comment   = {use prune rate of the CNN layers to encode signature.

applying small scale factor to BN layer to prune channels (convolutional layer output plane)},
  doi       = {10.1109/WIFS53200.2021.9648376},
  file      = {:2021Zhao - Structural Watermarking to Deep Neural Networks Via Network Channel Pruning.pdf:PDF},
  groups    = {DNN watermarking},
  keywords  = {Deep learning,Neural networks,Watermarking,Transforms,Resists,Intellectual property,Reliability,Watermarking,deep neural networks,ownership protection,deep learning,security},
}

@InProceedings{Chen2023,
  author    = {Chen, Yiming and Tian, Jinyu and Chen, Xiangyu and Zhou, Jiantao},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {{ Effective Ambiguity Attack Against Passport-based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution }},
  year      = {2023},
  address   = {Los Alamitos, CA, USA},
  month     = Jun,
  pages     = {8123-8132},
  publisher = {IEEE Computer Society},
  abstract  = {Since training a deep neural network (DNN) is costly, the well-trained deep models can be regarded as valuable intellectual property (IP) assets. The IP protection associated with deep models has been receiving increasing attentions in recent years. Passport-based method, which replaces normalization layers with passport layers, has been one of the few protection solutions that are claimed to be secure against advanced attacks. In this work, we tackle the issue of evaluating the security of passport-based IP protection methods. We propose a novel and effective ambiguity attack against passport-based method, capable of successfully forging multiple valid passports with a small training dataset. This is accomplished by inserting a specially designed accessory block ahead of the passport parameters. Using less than 10% of training data, with the forged passport, the model exhibits almost indistinguishable performance difference (less than 2%) compared with that of the authorized passport. In addition, it is shown that our attack strategy can be readily generalized to attack other IP protection methods based on watermark embedding. Directions for potential remedy solutions are also given.},
  comment   = {attack method for the passport based watermark in DeepIPR},
  doi       = {10.1109/CVPR52729.2023.00785},
  file      = {:2023JuneChen - Effective Ambiguity Attack against Passport Based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution.pdf:PDF},
  groups    = {DNN watermarking},
  keywords  = {Training,Computer vision,Training data,Artificial neural networks,Watermarking,Intellectual property},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00785},
}

@Article{An2024a,
  author   = {An, Jian and Tang, Siyu and Sun, Xiangyan and Gui, Xiaolin and He, Xin and Wang, Feifei},
  journal  = {IEEE Transactions on Services Computing},
  title    = {FREB: Participant Selection in Federated Learning With Reputation Evaluation and Blockchain},
  year     = {2024},
  number   = {6},
  pages    = {3685-3698},
  volume   = {17},
  doi      = {10.1109/TSC.2024.3486185},
  file     = {:2024An - FREB_ Participant Selection in Federated Learning with Reputation Evaluation and Blockchain.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Data models,Blockchains,Training,Federated learning,Computational modeling,Accuracy,Smart contracts,Reliability,Logic,Servers,Federated learning,reputation evaluation,blockchain,differential privacy},
}

@Article{Qu2024,
  author     = {Qu, Youyang and Yu, Shui and Gao, Longxiang and Sood, Keshav and Xiang, Yong},
  journal    = {IEEE Transactions on Services Computing},
  title      = {Blockchained Dual-Asynchronous Federated Learning Services for Digital Twin Empowered Edge-Cloud Continuum},
  year       = {2024},
  number     = {3},
  pages      = {836-849},
  volume     = {17},
  comment    = {Asynchronous FL based on blockchain with a Proof of Federalism (PoFe) consensus algorithm and a modified block structure. PoFe is asserted to be based on the contribution to the global model (producing the local model with highest performance). Experiments showcase the accuracy+convergence of global model as well as the blockchain latency},
  doi        = {10.1109/TSC.2024.3382952},
  file       = {:2024Qu - Blockchained Dual Asynchronous Federated Learning Services for Digital Twin Empowered Edge Cloud Continuum.pdf:PDF},
  groups     = {Blockchain-based FL},
  keywords   = {Federated learning,Digital twins,Computational modeling,Training,Blockchains,Servers,Privacy,Blockchain,digital twin,edge-cloud continuum,federated learning services},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@Article{Zhao2023,
  author     = {Zhao, Yao and Qu, Youyang and Xiang, Yong and Zhang, Yushu and Gao, Longxiang},
  journal    = {IEEE Transactions on Services Computing},
  title      = {A Lightweight Model-Based Evolutionary Consensus Protocol in Blockchain as a Service for IoT},
  year       = {2023},
  number     = {4},
  pages      = {2343-2358},
  volume     = {16},
  comment    = {Motivation: lightweight blockchain consensus for BaaS in IoT. Proof of Evolutionary Model consensus protocol: use a learning-to-rank model to determine the block mining winner; the model is trained with the operational data of each node from the system log by an upper server of through FL. Experiments compares the efficiency of PoEM with existing approaches.},
  doi        = {10.1109/TSC.2023.3238690},
  file       = {:2023Zhao - A Lightweight Model Based Evolutionary Consensus Protocol in Blockchain As a Service for IoT.pdf:PDF},
  groups     = {Blockchain ML},
  keywords   = {Internet of Things,Consensus protocol,Security,Consensus algorithm,Scalability,Data models,Computational modeling,Blockchain as a service,consensus protocol,IoT,machine learning,service computing},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{Liu2024b,
  author     = {Liu, Ziyao and Jiang, Yu and Shen, Jiyuan and Peng, Minyi and Lam, Kwok-Yan and Yuan, Xingliang and Liu, Xiaoning},
  journal    = {ACM Comput. Surv.},
  title      = {A Survey on Federated Unlearning: Challenges, Methods, and Future Directions},
  year       = {2024},
  issn       = {0360-0300},
  month      = oct,
  number     = {1},
  volume     = {57},
  abstract   = {In recent years, the notion of “the right to be forgotten” (RTBF) has become a crucial aspect of data privacy for digital trust and AI safety, requiring the provision of mechanisms that support the removal of personal data of individuals upon their requests. Consequently, machine unlearning (MU) has gained considerable attention which allows an ML model to selectively eliminate identifiable information. Evolving from MU, federated unlearning (FU) has emerged to confront the challenge of data erasure within federated learning (FL) settings, which empowers the FL model to unlearn an FL client or identifiable information pertaining to the client. Nevertheless, the distinctive attributes of federated learning introduce specific challenges for FU techniques. These challenges necessitate a tailored design when developing FU algorithms. While various concepts and numerous federated unlearning schemes exist in this field, the unified workflow and tailored design of FU are not yet well understood. Therefore, this comprehensive survey delves into the techniques and methodologies in FU providing an overview of fundamental concepts and principles, evaluating existing federated unlearning algorithms, and reviewing optimizations tailored to federated learning. Additionally, it discusses practical applications and assesses their limitations. Finally, it outlines promising directions for future research.},
  address    = {New York, NY, USA},
  articleno  = {2},
  doi        = {10.1145/3679014},
  file       = {:2024OctoberLiu - A Survey on Federated Unlearning_ Challenges, Methods, and Future Directions.pdf:PDF},
  groups     = {FL Research},
  issue_date = {January 2025},
  keywords   = {Federated unlearning, digital trust, AI safety},
  numpages   = {38},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/3679014},
}

@Article{Ling2025,
  author   = {Ling, Xintong and Le, Yuwei and Wang, Jiaheng and Huang, Yongming and You, Xiaohu},
  journal  = {IEEE Wireless Commun.},
  title    = {Trust and Trustworthiness in Information and Communications Technologies},
  year     = {2025},
  month    = apr,
  number   = {2},
  pages    = {84-92},
  volume   = {32},
  doi      = {10.1109/MWC.001.2400320},
  file     = {:2025AprilLing - Trust and Trustworthiness in Information and Communications Technologies.pdf:PDF},
  groups   = {Wireless Network},
  keywords = {Information and communication technology;6G mobile communication;Trusted computing;Fading channels;Blockchains;Security;Random variables;Probabilistic logic;Bayes methods;Data mining},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:BC Survey\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Blockchain Consensus\;0\;1\;\;\;\;;
1 StaticGroup:Decentralized Learning\;0\;1\;0x00ff00ff\;\;Gossip learning + model consensus algorithm\;;
2 StaticGroup:Gossip Learning\;0\;1\;0x80b380ff\;\;\;;
1 StaticGroup:Blockchain ML\;0\;1\;0xff0000ff\;\;\;;
2 StaticGroup:BC & ML inference\;0\;1\;0xe64d4dff\;\;\;;
2 StaticGroup:PoL Security\;0\;1\;0xff8080ff\;\;\;;
3 StaticGroup:DNN watermarking\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Blockchain optimization\;0\;1\;0xffcce6ff\;\;\;;
1 StaticGroup:Blockchain-based FL\;0\;1\;0xffb366ff\;\;\;;
1 StaticGroup:Ensemble Learning\;0\;1\;0x9980e6ff\;\;\;;
2 StaticGroup:Ensemble Diversity\;0\;1\;0x8066ccff\;\;\;;
2 StaticGroup:Federated Ensemble\;0\;0\;0xe6ccffff\;\;\;;
2 StaticGroup:Imbalance Learning\;0\;1\;0x664db3ff\;\;Imbalance Learning with Ensemble techniques\;;
3 StaticGroup:Skewness Representation\;0\;1\;0x8a8a8aff\;\;Use or invent other metrics other than accuracy to evaluate the performance of classifiers on multi-class datasets\;;
2 StaticGroup:Dynamic ensemble\;0\;1\;0xffccffff\;\;\;;
1 StaticGroup:FL Research\;0\;1\;0x00ffffff\;\;\;;
2 StaticGroup:FL & Non-iid\;0\;1\;0x68d4b4ff\;\;\;;
2 StaticGroup:Knowledge Distillation\;0\;1\;0xb3e6e6ff\;\;Knowledge Distillation in FL\;;
2 StaticGroup:Lazy Client\;0\;1\;0xffffb3ff\;\;\;;
2 StaticGroup:Misc FL\;0\;1\;0x1a4d1aff\;\;Gradient Free FL\;;
2 StaticGroup:Personalized FL\;0\;1\;0xb3e6e6ff\;\;\;;
2 StaticGroup:Weighted FL\;0\;1\;0xccffffff\;\;\;;
2 StaticGroup:Decentralized FL\;0\;1\;0xd288aaff\;\;Based on consensus, and not blockchain\;;
1 StaticGroup:Other Classification\;0\;1\;0x8a8a8aff\;\;Audio and Text Classification\;;
1 StaticGroup:Proof-of-Useful-Work\;0\;0\;0xff00ffff\;\;\;;
1 StaticGroup:ZKP & ML\;0\;1\;\;\;Zero Knowledge Proof & ML\;;
1 StaticGroup:Wireless Network\;0\;1\;0x8a8a8aff\;\;\;;
}
