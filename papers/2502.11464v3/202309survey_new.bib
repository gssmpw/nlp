@InProceedings{Kucharavy2023,
  author     = {Kucharavy, Andrei and Monti, Matteo and Guerraoui, Rachid and Dolamic, Ljiljana},
  booktitle  = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
  title      = {Byzantine-Resilient Learning Beyond Gradients: Distributing Evolutionary Search},
  year       = {2023},
  address    = {New York, NY, USA},
  pages      = {295-298},
  publisher  = {Association for Computing Machinery},
  series     = {GECCO '23 Companion},
  abstract   = {Modern machine learning (ML) models are capable of impressive performances. However, their prowess is not due only to the improvements in their architecture and training algorithms but also to a drastic increase in computational power used to train them.Such a drastic increase led to a growing interest in distributed ML, which in turn made worker failures and adversarial attacks an increasingly pressing concern. While distributed byzantine resilient algorithms have been proposed in a differentiable setting, none exist in a gradient-free setting.The goal of this work is to address this shortcoming. For that, we introduce a more general definition of byzantine-resilience in ML - the model-consensus, that extends the definition of the classical distributed consensus. We then leverage this definition to show that a general class of gradient-free ML algorithms - (1, λ)-Evolutionary Search - can be combined with classical distributed consensus algorithms to generate gradient-free byzantine-resilient distributed learning algorithms. We provide proofs and pseudo-code for two specific cases - the Total Order Broadcast and proof-of-work leader election.To our knowledge, this is the first time a byzantine resilience in gradient-free ML was defined, and algorithms to achieve it - were proposed.},
  doi        = {10.1145/3583133.3590719},
  file       = {:2023Kucharavy - Byzantine Resilient Learning beyond Gradients_ Distributing Evolutionary Search.pdf:PDF},
  groups     = {Byzantine-Resilient Learning},
  isbn       = {9798400701207},
  keywords   = {byzantine fault tolerance, gradient-free optimization, evolutionary search, distributed machine learning},
  location   = {Lisbon, Portugal},
  numpages   = {4},
  priority   = {prio1},
  readstatus = {read},
}

@InProceedings{Blanchard2017,
  author     = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},
  year       = {2017},
  editor     = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {30},
  file       = {:2017Blanchard - Machine Learning with Adversaries_ Byzantine Tolerant Gradient Descent.pdf:PDF},
  groups     = {Byzantine-Resilient Learning},
  ranking    = {rank5},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf},
}

@Misc{Ren2019,
  author       = {Ling Ren},
  howpublished = {Cryptology ePrint Archive, Paper 2019/943},
  note         = {\url{https://eprint.iacr.org/2019/943}},
  title        = {Analysis of Nakamoto Consensus},
  year         = {2019},
  file         = {:2019Ren - Analysis of Nakamoto Consensus.pdf:PDF},
  groups       = {Blockchain Analysis},
  url          = {https://eprint.iacr.org/2019/943},
}

@Article{Guerraoui2023,
  author    = {Guerraoui, Rachid and Gupta, Nirupam and Pinot, Rafael},
  journal   = {ACM Comput. Surv.},
  title     = {Byzantine Machine Learning: A Primer},
  year      = {2023},
  issn      = {0360-0300},
  month     = {aug},
  note      = {Just Accepted},
  abstract  = {The problem of Byzantine resilience in distributed machine learning, a.k.a., Byzantine machine learning, consists in designing distributed algorithms that can train an accurate model despite the presence of Byzantine nodes, i.e., nodes with corrupt data or machines that can misbehave arbitrarily. By now, many solutions to this important problem have been proposed, most of which build upon the classical stochastic gradient descent (SGD) scheme. Yet, the literature lacks a unified structure of this emerging field. Consequently, the general understanding on the principles of Byzantine machine learning remains poor. This paper addresses this issue by presenting a primer on Byzantine machine learning. In particular, we introduce three pillars of Byzantine machine learning, namely the concepts of breakdown point, robustness and gradient complexity, to curate the efficacy of a solution. The introduced systematization enables us to (i) bring forth the merits and limitations of the state-of-the-art solutions, and (ii) pave a clear path for future advancements in this field.},
  address   = {New York, NY, USA},
  doi       = {10.1145/3616537},
  file      = {:2023augGuerraoui - Byzantine Machine Learning_ a Primer.pdf:PDF},
  groups    = {Byzantine-Resilient Learning},
  keywords  = {distributed SGD, Byzantine machine learning, robust aggregation},
  priority  = {prio2},
  publisher = {Association for Computing Machinery},
  ranking   = {rank5},
}

@Article{Stanley2019,
  author    = {Kenneth O. Stanley and Jeff Clune and Joel Lehman and Risto Miikkulainen},
  journal   = {Nature Machine Intelligence},
  title     = {Designing neural networks through neuroevolution},
  year      = {2019},
  month     = {jan},
  number    = {1},
  pages     = {24--35},
  volume    = {1},
  doi       = {10.1038/s42256-018-0006-z},
  file      = {:2019janStanley - Designing Neural Networks through Neuroevolution.pdf:PDF},
  groups    = {Evolutionary Algorithm},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Real2019,
  author       = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title        = {Regularized Evolution for Image Classifier Architecture Search},
  year         = {2019},
  month        = jul,
  number       = {01},
  pages        = {4780-4789},
  volume       = {33},
  abstractnote = {&lt;p&gt;The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— &lt;em&gt;AmoebaNet-A&lt;/em&gt;—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.&lt;/p&gt;},
  doi          = {10.1609/aaai.v33i01.33014780},
  file         = {:2019JulyReal - Regularized Evolution for Image Classifier Architecture Search.pdf:PDF},
  groups       = {Evolutionary Algorithm},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/4405},
}

@Misc{Such2018,
  author        = {Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  title         = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  year          = {2018},
  archiveprefix = {arXiv},
  comment       = {genetic algorithm-novelty optimized, apply genetic/evolutionary algorithm on NN with some adaptations, empirical study proves the efficacy of the proposed algorithm. 
Advantages:
1. scalable
2. suggest gradient based optimization algorithm might be beaten by GA/EA on complex RL problems, needs future research of unappreciated aspects of  	high-dimension search space},
  eprint        = {1712.06567},
  file          = {:2018Such - Deep Neuroevolution_ Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning.pdf:PDF},
  groups        = {Evolutionary Algorithm},
  primaryclass  = {cs.NE},
}

@Article{Yang2019,
  author  = {Yang, Zhixiong and Bajwa, Waheed U.},
  journal = {IEEE Transactions on Signal and Information Processing over Networks},
  title   = {ByRDiE: Byzantine-Resilient Distributed Coordinate Descent for Decentralized Learning},
  year    = {2019},
  number  = {4},
  pages   = {611-627},
  volume  = {5},
  comment = {Propose a novel coordinate descent algorithm, termed ByRDiE, which is fully distributed(decentralized). Compared with contemporary works in the region of Byzantine Resilient Learning, the proposed algorithm scales better with the dimensionality of the problem as the neighbouring node number constraint is much looser than that of its adversaries.},
  doi     = {10.1109/TSIPN.2019.2928176},
  file    = {:2019Yang - ByRDiE_ Byzantine Resilient Distributed Coordinate Descent for Decentralized Learning.pdf:PDF},
  groups  = {Byzantine-Resilient Learning},
  ranking = {rank3},
}

@Article{Fang2022,
  author  = {Fang, Cheng and Yang, Zhixiong and Bajwa, Waheed U.},
  journal = {IEEE Transactions on Signal and Information Processing over Networks},
  title   = {BRIDGE: Byzantine-Resilient Decentralized Gradient Descent},
  year    = {2022},
  pages   = {610-626},
  volume  = {8},
  comment = {Propose BRIDGE, a Byzantine Resilient “decentralized” learning algorithm. 
Features: scalable, theoretical guarantees for both convex and nonconvex loss functions and for both network-wide consensus and statistical convergence.},
  doi     = {10.1109/TSIPN.2022.3188456},
  file    = {:2022Fang - BRIDGE_ Byzantine Resilient Decentralized Gradient Descent.pdf:PDF},
  groups  = {Byzantine-Resilient Learning},
  ranking = {rank3},
}

@Misc{Bouhata2022,
  author        = {Djamila Bouhata and Hamouma Moumen and Jocelyn Ahmed Mazari and Ahcène Bounceur},
  title         = {Byzantine Fault Tolerance in Distributed Machine Learning : a Survey},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2205.02572},
  file          = {:2022Bouhata - Byzantine Fault Tolerance in Distributed Machine Learning _ a Survey.pdf:PDF},
  groups        = {Byzantine-Resilient Learning},
  keywords      = {Survey, Byzantine Fault Tolarance},
  primaryclass  = {cs.DC},
  priority      = {prio3},
  ranking       = {rank4},
}

@Article{Guo2022,
  author  = {Guo, Shangwei and Zhang, Tianwei and Yu, Han and Xie, Xiaofei and Ma, Lei and Xiang, Tao and Liu, Yang},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  title   = {Byzantine-Resilient Decentralized Stochastic Gradient Descent},
  year    = {2022},
  number  = {6},
  pages   = {4096-4106},
  volume  = {32},
  comment = {Ubar, a uniform byzantine-tolarant aggregation rule, is proposed, enabling nodes to filter out malicious parameter updates and retain the useful ones in the training iterations.},
  doi     = {10.1109/TCSVT.2021.3116976},
  file    = {:2022Guo - Byzantine Resilient Decentralized Stochastic Gradient Descent.pdf:PDF},
  groups  = {Byzantine-Resilient Learning},
  ranking = {rank2},
}

@Article{ElMhamdi2022,
  author    = {El-Mahdi El-Mhamdi and Rachid Guerraoui and Arsany Guirguis and L{\^{e}}-Nguy{\^{e}}n Hoang and S{\'{e}}bastien Rouault},
  journal   = {Distributed Computing},
  title     = {Genuinely distributed Byzantine machine learning},
  year      = {2022},
  month     = {may},
  number    = {4},
  pages     = {305--331},
  volume    = {35},
  comment   = {ByzSGD proposed, with no nodes(including parameter server and workers) trusted, accomplishing the so-called general byzantine resilience.},
  doi       = {10.1007/s00446-022-00427-9},
  file      = {:2022mayEl-Mhamdi - Genuinely Distributed Byzantine Machine Learning.pdf:PDF},
  groups    = {Byzantine-Resilient Learning},
  publisher = {Springer Science and Business Media {LLC}},
}

@InProceedings{Wang2021,
  author    = {Wang, Naiyu and Yang, Wenti and Guan, Zhitao and Du, Xiaojiang and Guizani, Mohsen},
  booktitle = {2021 IEEE Global Communications Conference (GLOBECOM)},
  title     = {BPFL: A Blockchain Based Privacy-Preserving Federated Learning Scheme},
  year      = {2021},
  pages     = {1-6},
  comment   = {combining the homomorphic encryption with the Multi-Krum algorithm},
  doi       = {10.1109/GLOBECOM46510.2021.9685821},
  file      = {:2021Wang - BPFL_ a Blockchain Based Privacy Preserving Federated Learning Scheme.pdf:PDF},
  groups    = {Blockchain-based FL},
  ranking   = {rank2},
}

@Article{Weng2021,
  author  = {Weng, Jiasi and Weng, Jian and Zhang, Jilian and Li, Ming and Zhang, Yue and Luo, Weiqi},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  title   = {DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-Based Incentive},
  year    = {2021},
  number  = {5},
  pages   = {2438-2455},
  volume  = {18},
  comment = {devise a novel incentive mechanism based on blockchain to encourage parties to jointly participate in deep learning model training honestly.

not PoUW},
  doi     = {10.1109/TDSC.2019.2952332},
  file    = {:2021Weng - DeepChain_ Auditable and Privacy Preserving Deep Learning with Blockchain Based Incentive.pdf:PDF},
  groups  = {Blockchain-based learning},
  ranking = {rank2},
}

@InProceedings{Chen2018,
  author    = {Chen, Xuhui and Ji, Jinlong and Luo, Changqing and Liao, Weixian and Li, Pan},
  booktitle = {2018 IEEE International Conference on Big Data (Big Data)},
  title     = {When Machine Learning Meets Blockchain: A Decentralized, Privacy-preserving and Secure Design},
  year      = {2018},
  pages     = {1178-1187},
  comment   = {LearningChain:

no central server, decentralized sgd
l-nearest aggregation algorithm introduce byzantine tolarance

"Several protocols can reach a consensus in
blockchain-based applications, including PoW, PoA". is not a PoUW work.},
  doi       = {10.1109/BigData.2018.8622598},
  file      = {:2018Chen - When Machine Learning Meets Blockchain_ a Decentralized, Privacy Preserving and Secure Design.pdf:PDF},
  groups    = {Blockchain-based learning},
  ranking   = {rank3},
}

@Article{Ma2022,
  author  = {Ma, Chuan and Li, Jun and Shi, Long and Ding, Ming and Wang, Taotao and Han, Zhu and Poor, H. Vincent},
  journal = {IEEE Computational Intelligence Magazine},
  title   = {When Federated Learning Meets Blockchain: A New Distributed Learning Paradigm},
  year    = {2022},
  number  = {3},
  pages   = {26-33},
  volume  = {17},
  comment = {BLADE-FL

smart contract based},
  doi     = {10.1109/MCI.2022.3180932},
  file    = {:2022Ma - When Federated Learning Meets Blockchain_ a New Distributed Learning Paradigm.pdf:PDF},
  groups  = {Blockchain-based FL},
  ranking = {rank1},
}

@InProceedings{Jiao2023,
  author    = {Jiao, Wenjing and Zhao, Huawei and Feng, Peng and Chen, Quanrun},
  booktitle = {2023 4th International Conference on Information Science, Parallel and Distributed Systems (ISPDS)},
  title     = {A Blockchain Federated Learning Scheme Based on Personalized Differential Privacy and Reputation Mechanisms},
  year      = {2023},
  pages     = {630-635},
  comment   = {improve FL with
1. personalized differential privacy(cater privacy budget to different needs)
2. reputation mechanism based on blockchain},
  doi       = {10.1109/ISPDS58840.2023.10235627},
  file      = {:2023Jiao - A Blockchain Federated Learning Scheme Based on Personalized Differential Privacy and Reputation Mechanisms.pdf:PDF},
  groups    = {Blockchain-based FL},
  ranking   = {rank2},
}

@Article{Lugan2019,
  author  = {Lugan, Sébastien and Desbordes, Paul and Brion, Eliott and Ramos Tormo, Luis Xavier and Legay, Axel and Macq, Benoît},
  journal = {IEEE Access},
  title   = {Secure Architectures Implementing Trusted Coalitions for Blockchained Distributed Learning (TCLearn)},
  year    = {2019},
  pages   = {181789-181799},
  volume  = {7},
  comment = {federated Byzantine Agreement
a consortium chain that select validators to control the integrity of the new block and the quality of the updated CNN model.},
  doi     = {10.1109/ACCESS.2019.2959220},
  file    = {:2019Lugan - Secure Architectures Implementing Trusted Coalitions for Blockchained Distributed Learning (TCLearn).pdf:PDF},
  groups  = {Blockchain-based learning},
  ranking = {rank1},
}

@Article{Li2023pofel,
  author        = {Shengyang Li and Qin Hu and Zhilin Wang},
  journal       = {arXiv preprint arXiv:2308.07840},
  title         = {{PoFEL}: Energy-efficient Consensus for Blockchain-based Hierarchical Federated Learning},
  year          = {2023},
  month         = aug,
  archiveprefix = {arXiv},
  comment       = {In [@Li2023], the authors propose Hash-based Commitment scheme integrated with Digital Signature(HCDS) featuring a two-stage workflow: commit stage and reveal stage.},
  eprint        = {2308.07840},
  file          = {:2023Li - PoFEL_ Energy Efficient Consensus for Blockchain Based Hierarchical Federated Learning.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {cs.DC},
  priority      = {prio2},
  ranking       = {rank3},
}

@Article{So2021,
  author    = {Jinhyun So and Basak Guler and A. Salman Avestimehr},
  journal   = {{IEEE} Journal on Selected Areas in Communications},
  title     = {Byzantine-Resilient Secure Federated Learning},
  year      = {2021},
  month     = {jul},
  number    = {7},
  pages     = {2168--2181},
  volume    = {39},
  doi       = {10.1109/jsac.2020.3041404},
  file      = {:2021julSo - Byzantine Resilient Secure Federated Learning.pdf:PDF},
  groups    = {Byzantine-Resilient FL},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Misc{Zhang2023,
  author        = {Zikai Zhang and Rui Hu},
  title         = {Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2309.03437},
  file          = {:2023Zhang - Byzantine Robust Federated Learning with Variance Reduction and Differential Privacy.pdf:PDF},
  groups        = {Byzantine-Resilient FL},
  primaryclass  = {cs.LG},
}

@Article{Qammar2022,
  author    = {Attia Qammar and Ahmad Karim and Huansheng Ning and Jianguo Ding},
  journal   = {Artificial Intelligence Review},
  title     = {Securing federated learning with blockchain: a systematic literature review},
  year      = {2022},
  month     = {sep},
  number    = {5},
  pages     = {3951--3985},
  volume    = {56},
  doi       = {10.1007/s10462-022-10271-9},
  file      = {:2022sepQammar - Securing Federated Learning with Blockchain_ a Systematic Literature Review.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Gouissem2023,
  author    = {A. Gouissem and K. Abualsaud and E. Yaacoub and T. Khattab and M. Guizani},
  journal   = {{IEEE} Internet of Things Journal},
  title     = {Collaborative Byzantine Resilient Federated Learning},
  year      = {2023},
  month     = {sep},
  number    = {18},
  pages     = {15887--15899},
  volume    = {10},
  doi       = {10.1109/jiot.2023.3266347},
  file      = {:2023sepGouissem - Collaborative Byzantine Resilient Federated Learning.pdf:PDF},
  groups    = {Byzantine-Resilient FL},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Masuda2023,
  author    = {Hiroki Masuda and Kentaro Kita and Yuki Koizumi and Junji Takemasa and Toru Hasegawa},
  journal   = {{IEEE} Access},
  title     = {Byzantine-Resilient Secure Federated Learning on Low-Bandwidth Networks},
  year      = {2023},
  pages     = {51754--51766},
  volume    = {11},
  doi       = {10.1109/access.2023.3277858},
  file      = {:2023Masuda - Byzantine Resilient Secure Federated Learning on Low Bandwidth Networks.pdf:PDF},
  groups    = {Byzantine-Resilient FL},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Asqah2023,
  author    = {Muneerah Al Asqah and Tarek Moulahi},
  journal   = {Future Internet},
  title     = {Federated Learning and Blockchain Integration for Privacy Protection in the Internet of Things: Challenges and Solutions},
  year      = {2023},
  month     = {may},
  number    = {6},
  pages     = {203},
  volume    = {15},
  doi       = {10.3390/fi15060203},
  file      = {:2023mayAsqah - Federated Learning and Blockchain Integration for Privacy Protection in the Internet of Things_ Challenges and Solutions.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {{MDPI} {AG}},
  ranking   = {rank1},
}

@Article{Oktian2023,
  author    = {Yustus Eko Oktian and Sang-Gon Lee},
  journal   = {Sensors},
  title     = {Blockchain-Based Federated Learning System: A Survey on Design Choices},
  year      = {2023},
  month     = {jun},
  number    = {12},
  pages     = {5658},
  volume    = {23},
  doi       = {10.3390/s23125658},
  file      = {:2023junOktian - Blockchain Based Federated Learning System_ a Survey on Design Choices.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {{MDPI} {AG}},
  ranking   = {rank1},
}

@Article{Liu2023,
  author    = {Song Liu and Xiong Wang and Longshuo Hui and Weiguo Wu},
  journal   = {Applied Sciences},
  title     = {Blockchain-Based Decentralized Federated Learning Method in Edge Computing Environment},
  year      = {2023},
  month     = {jan},
  number    = {3},
  pages     = {1677},
  volume    = {13},
  doi       = {10.3390/app13031677},
  file      = {:2023janLiu - Blockchain Based Decentralized Federated Learning Method in Edge Computing Environment.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {{MDPI} {AG}},
  ranking   = {rank1},
}

@Article{Wang2023fedchain,
  author        = {Peiran Wang},
  journal       = {arXiv preprint arXiv:2308.15095},
  title         = {{FedChain}: An Efficient and Secure Consensus Protocol based on Proof of Useful Federated Learning for Blockchain},
  year          = {2023},
  month         = aug,
  archiveprefix = {arXiv},
  comment       = {Fedchain:
1. new block structure and transaction type introduced to enable utility of FL in blockchain consensus
2. mining pool partitioning algorithm, clustering node based on estimated latency to optimize communication cost.
3. gradient updates from miners are weighed by similarity of their dataset to the dataset from task publisher(comparatively gradients are usually weighed based on dataset size), this similarity is measured with KL divergence.
4. zkCNN: a zero-knowledge proof scheme for machine learning
model owners to prove to others that the owners’ models do achieve
certain accuracy on a given dataset using the models, the scheme is an interactive proof, system delay deteriorates?
5. sharing based ring all reduce},
  eprint        = {2308.15095},
  file          = {:2023Wang - FedChain_ an Efficient and Secure Consensus Protocol Based on Proof of Useful Federated Learning for Blockchain.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {cs.CR},
  priority      = {prio3},
  ranking       = {rank3},
  readstatus    = {read},
}

@Misc{Gabrielli2023,
  author        = {Edoardo Gabrielli and Giovanni Pica and Gabriele Tolomei},
  title         = {A Survey on Decentralized Federated Learning},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.04604},
  file          = {:2023Gabrielli - A Survey on Decentralized Federated Learning.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {cs.LG},
  ranking       = {rank4},
}

@Article{Feng2022,
  author  = {Feng, Lei and Zhao, Yiqi and Guo, Shaoyong and Qiu, Xuesong and Li, Wenjing and Yu, Peng},
  journal = {IEEE Transactions on Computers},
  title   = {BAFL: A Blockchain-Based Asynchronous Federated Learning Framework},
  year    = {2022},
  number  = {5},
  pages   = {1092-1103},
  volume  = {71},
  comment = {- optimal block generation interval calculated
- asynchronous FL
- the dynamic adjustment of the number of training times (scaling factor)},
  doi     = {10.1109/TC.2021.3072033},
  file    = {:2022Feng - BAFL_ a Blockchain Based Asynchronous Federated Learning Framework.pdf:PDF},
  groups  = {Blockchain-based FL},
  ranking = {rank2},
}

@InProceedings{Majeed2019,
  author    = {Majeed, Umer and Hong, Choong Seon},
  booktitle = {2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS)},
  title     = {FLchain: Federated Learning via MEC-enabled Blockchain Network},
  year      = {2019},
  pages     = {1-4},
  comment   = {Umer Majeed et al. have proposed FLchain and EFLchain @Majeed2019 @Majeed2019a. A Blockchain based FL scheme is presented in MEC scenario, where miners finetune local models upon global weights, and then submits then in transactions through specific channel. MEC servers collect local models in a channel, update the global model with DANE and reach consensus on the global model state. There is no detailed consensus protocol design in FLchain and there is only one round for local and global model update, making the effectiveness of the scheme suspicious (no empirical evaluation is presented in the paper).},
  doi       = {10.23919/APNOMS.2019.8892848},
  file      = {:2019Majeed - FLchain_ Federated Learning Via MEC Enabled Blockchain Network.pdf:PDF},
  groups    = {Blockchain-based FL},
}

@Article{Li2021byzantine,
  author   = {Li, Zonghang and Yu, Hongfang and Zhou, Tianyao and Luo, Long and Fan, Mochan and Xu, Zenglin and Sun, Gang},
  journal  = {IEEE Netw.},
  title    = {Byzantine Resistant Secure Blockchained Federated Learning at the Edge},
  year     = {2021},
  month    = jul,
  number   = {4},
  pages    = {295-301},
  volume   = {35},
  comment  = {improve efficiency of verification by introducing verifiers to verify models in parallel and use proof-of-accuracy consensus to detect attack},
  doi      = {10.1109/MNET.011.2000604},
  file     = {:2021Li - Byzantine Resistant Secure Blockchained Federated Learning at the Edge.pdf:PDF},
  groups   = {Byzantine-Resilient FL, Blockchain-based FL},
  priority = {prio2},
  ranking  = {rank3},
}

@Article{Xiong2023,
  author        = {Qi Xiong and Nasrin Sohrabi and Hai Dong and Chenhao Xu and Zahir Tari},
  journal       = {arXiv preprint arXiv:2304.08128},
  title         = {{AICons}: An {AI}-Enabled Consensus Algorithm Driven by Energy Preservation and Fairness},
  year          = {2023},
  month         = apr,
  archiveprefix = {arXiv},
  comment       = {AICons, adopts Shapley value evaluation equation to evaluate miner's contribution to blockchain network to reflect fairness of incentive mechanism.},
  eprint        = {2304.08128},
  file          = {:2023Xiong - AICons_ an AI Enabled Consensus Algorithm Driven by Energy Preservation and Fairness.pdf:PDF},
  groups        = {Blockchain-based FL},
  primaryclass  = {cs.DC},
  ranking       = {rank3},
}

@Article{Li2022bladefl,
  author     = {Li, Jun and Shao, Yumeng and Wei, Kang and Ding, Ming and Ma, Chuan and Shi, Long and Han, Zhu and Poor, H. Vincent},
  journal    = {IEEE Trans. Parallel Distrib. Syst.},
  title      = {Blockchain Assisted Decentralized Federated Learning ({BLADE-FL}): Performance Analysis and Resource Allocation},
  year       = {2022},
  issn       = {1558-2183},
  month      = oct,
  number     = {10},
  pages      = {2401-2415},
  volume     = {33},
  comment    = {BLADE-FL
a blockchain-based FL, with aggregated models published when a new block generated using a normal consensus protocol like pow. blockchain introduced to improve privacy protection and tamper-proof model updates. privacy issue tackled by removing global aggregator and adopting differential privacy algorithm.
Part of the workflow in BLADE-FL scheme resembles our proof of bagging(dataset used to train base models not bootstrapped)

focus on factors affecting loss functions(learning performance), those factors includes model plagiarism(lazy clients), computing force allocation between mining and computing.

有收敛性证明},
  doi        = {10.1109/TPDS.2021.3138848},
  file       = {:2022Li - Blockchain Assisted Decentralized Federated Learning (BLADE FL)_ Performance Analysis and Resource Allocation.pdf:PDF},
  groups     = {Blockchain-based FL},
  priority   = {prio1},
  ranking    = {rank4},
  readstatus = {read},
}

@InProceedings{Ramanan2020,
  author    = {Ramanan, Paritosh and Nakayama, Kiyoshi},
  booktitle = {Proc. 2020 IEEE Int. Conf. Blockchain},
  title     = {{BAFFLE} : Blockchain Based Aggregator Free Federated Learning},
  year      = {2020},
  address   = {Rhodes, GR},
  month     = nov,
  pages     = {72-81},
  comment   = {Ethereum smart contract developed to decentralize aggragator across blockchain},
  doi       = {10.1109/Blockchain50366.2020.00017},
  file      = {:2020Ramanan - BAFFLE _ Blockchain Based Aggregator Free Federated Learning.pdf:PDF},
  groups    = {Blockchain-based FL},
  ranking   = {rank1},
}

@Misc{Salhab2023,
  author        = {Mahmoud Salhab and Khaleel Mershad},
  title         = {Proof of Deep Learning: Approaches, Challenges, and Future Directions},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {PoDL: residual challenges might be valuable},
  eprint        = {2308.16730},
  file          = {:2023Salhab - Proof of Deep Learning_ Approaches, Challenges, and Future Directions.pdf:PDF},
  groups        = {Blockchain-based learning},
  primaryclass  = {cs.CR},
  ranking       = {rank1},
}

@Misc{Li2023a,
  author        = {Peihao Li},
  title         = {Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training},
  year          = {2023},
  archiveprefix = {arXiv},
  comment       = {blockchain serve as distribued digital registry of participants, reward system and the foundation of decentralized training services. **PBFT** is center to the proposed distributed training network(used to synchronize global states).
performace analysis of DTN under different network conditions},
  eprint        = {2307.07066},
  file          = {:2023Li - Proof of Training (PoT)_ Harnessing Crypto Mining Power for Distributed AI Training.pdf:PDF},
  groups        = {Blockchain-based learning},
  primaryclass  = {cs.CR},
  ranking       = {rank1},
  readstatus    = {skimmed},
}

@InProceedings{Su2023,
  author     = {Su, Xiangyu and Larangeira, Mario and Tanaka, Keisuke},
  booktitle  = {Proc. 17th Int. Conf. Netw. Syst. Secur.},
  title      = {Provably Secure Blockchain Protocols from Distributed Proof-of-Deep-Learning},
  year       = {2023},
  address    = {Canterbury, UK},
  month      = aug,
  pages      = {114-136},
  abstract   = {Proof-of-useful-work (PoUW), an alternative to the widely used proof-of-work (PoW), aims to re-purpose the network’s computing power. Namely, users evaluate meaningful computational problems, e.g., solving optimization problems, instead of computing numerous hash function values as in PoW. A recent approach utilizes the training process of deep learning as “useful work”. However, these works lack security analysis when deploying them with blockchain-based protocols, let alone the informal and over-complicated system design. This work proposes a distributed proof-of-deep-learning (D-PoDL) scheme concerning PoUW’s requirements. With a novel hash-trainin\ss{}g-hash structure and model-referencing mechanism, our scheme is the first deep learning-based PoUW scheme that enables achieving better accuracy distributively. Next, we introduce a transformation from the D-PoDL scheme to a generic D-PoDL blockchain protocol which can be instantiated with two chain selection rules, i.e., the longest-chain rule and the weight-based blockchain framework (LatinCrypt’ 21). This work is the first to provide formal proofs for deep learning-involved blockchain protocols concerning the robust ledger properties, i.e., chain growth, chain quality, and common prefix. Finally, we implement the D-PoDL scheme to discuss the effectiveness of our design.},
  comment    = {A novel Distributed Proof-of-Learning(D-PoDL) scheme is proposed with formulated security proof.
a hash-training-hash structure proposed. 
First, pre-hash against target T1, previous block hash, pretrained model(make use of any valid model) and nonce hashed, a model is mapped from hash and some hyperparameters(randomly sampled) if no pretrained model provided. **Reference model** mechanism link a valid block to previous block and previous models.
Some checkpoints set up during trainining process should be provided to make the process trackable and verifiable. (Merkle tree used to record those checkpoints and models in blocks)
Post-hash against difficulty T2 and accuracy Tacc to guarantee both security and usefulness.
Fork resolution is accomplished by accumulated weight along a chain in which each block is assigned a weight calculated with Weight(model accuracy, target accuracy).


theoretical formulation of consensus protocol is based on "Proof-of-Useful-Work" with some primitives(e.g.Setup, Solve, Verify) and properties definitions lent from this paper, and its secure property analysis is based on "bitcoin backbone protocol"(chain).},
  doi        = {10.1007/978-3-031-39828-5_7},
  file       = {:2023Su - Provably Secure Blockchain Protocols From~Distributed Proof of Deep Learning.pdf:PDF},
  groups     = {Blockchain-based learning},
  isbn       = {978-3-031-39827-8},
  keywords   = {(Weight-based) blockchain protocols, Proof-of-useful-work, Distributed proof-of-deep-learning},
  location   = {Canterbury, United Kingdom},
  numpages   = {23},
  priority   = {prio3},
  ranking    = {rank4},
  readstatus = {skimmed},
}

@Article{Zhu2023,
  author     = {Zhu, Juncen and Cao, Jiannong and Saxena, Divya and Jiang, Shan and Ferradi, Houda},
  journal    = {ACM Comput. Surv.},
  title      = {Blockchain-Empowered Federated Learning: Challenges, Solutions, and Future Directions},
  year       = {2023},
  issn       = {0360-0300},
  month      = {feb},
  number     = {11},
  volume     = {55},
  abstract   = {Federated learning is a privacy-preserving machine learning technique that trains models across multiple devices holding local data samples without exchanging them. There are many challenging issues in federated learning, such as coordinating participants’ activities, arbitrating their benefits, and aggregating models. Most existing solutions employ a centralized approach, in which a trustworthy central authority is needed for coordination. Such an approach incurs many disadvantages, including vulnerability to attacks, lack of credibility, and difficulty in calculating rewards. Recently, blockchain was identified as a potential solution for addressing the abovementioned issues. Extensive research has been conducted, and many approaches, methods, and techniques have been proposed. There is a need for a systematic survey to examine how blockchain can empower federated learning. Although there are many surveys on federated learning, few of them cover blockchain as an enabling technology. This work comprehensively surveys challenges, solutions, and future directions for blockchain-empowered federated learning (BlockFed). First, we identify the critical issues in federated learning and explain why blockchain provides a potential approach to addressing these issues. Second, we categorize existing system models into three classes: decoupled, coupled, and overlapped, according to how the federated learning and blockchain functions are integrated. Then we compare the advantages and disadvantages of these three system models, regard the disadvantages as challenging issues in BlockFed, and investigate corresponding solutions. Finally, we identify and discuss the future directions, including open problems in BlockFed.},
  address    = {New York, NY, USA},
  articleno  = {240},
  doi        = {10.1145/3570953},
  file       = {:2023febZhu - Blockchain Empowered Federated Learning_ Challenges, Solutions, and Future Directions.pdf:PDF},
  groups     = {Blockchain-based FL},
  issue_date = {November 2023},
  keywords   = {Blockchain, incentive mechanisms, client selection, federated learning, blockchain-based federated learning},
  numpages   = {31},
  publisher  = {Association for Computing Machinery},
  ranking    = {rank4},
}

@InProceedings{Li2023b,
  author        = {Boyang Li and Bingyu Shen and Qing Lu and Taeho Jung and Yiyu Shi},
  booktitle     = {Proc. 5th Int. Conf. Blockchain Comput. Appl. (BCCA'23)},
  title         = {Proof-of-Federated-Learning-Subchain: Free Partner Selection Subchain Based on Federated Learning},
  year          = {2023},
  address       = {Kuwait, KW},
  month         = oct,
  pages         = {600-605},
  archiveprefix = {arXiv},
  comment       = {a subchain based FL paradigm.},
  doi           = {10.1109/bcca58897.2023.10338941},
  eprint        = {2307.16342},
  file          = {:2023Li - Proof of Federated Learning Subchain_ Free Partner Selection Subchain Based on Federated Learning.pdf:PDF},
  groups        = {Blockchain-based FL},
  keywords      = {Training;Deep learning;Federated learning;Blockchains;Cryptocurrency;Task analysis;Novel Consensus;Blockchain;Proof-of-Deep-Learning;FLChain;Federated Learning;Deep Learning},
  primaryclass  = {cs.LG},
}

@InCollection{Ball2018,
  author    = {Marshall Ball and Alon Rosen and Manuel Sabin and Prashant Nalini Vasudevan},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {Proofs of Work From Worst-Case Assumptions},
  year      = {2018},
  pages     = {789--819},
  doi       = {10.1007/978-3-319-96884-1_26},
  file      = {:2018Ball - Proofs of Work from Worst Case Assumptions.pdf:PDF},
  groups    = {PoUW},
  ranking   = {rank5},
}

@Article{Yang2023,
  author  = {Yang, Zhanpeng and Shi, Yuanming and Zhou, Yong and Wang, Zixin and Yang, Kai},
  journal = {IEEE Internet of Things Journal},
  title   = {Trustworthy Federated Learning via Blockchain},
  year    = {2023},
  number  = {1},
  pages   = {92-109},
  volume  = {10},
  comment = {In [@Yang2023], performance of PBFT-based wireless FL system is compromised by long training latency. To minimize training latency, bandwidth and power allocation are jointly adjusted based on a network optimization framework powered by a Markow decision model and deep reinforcement learning based algorithm},
  doi     = {10.1109/JIOT.2022.3201117},
  file    = {:2023Yang - Trustworthy Federated Learning Via Blockchain.pdf:PDF},
  groups  = {Blockchain-based FL},
  ranking = {rank4},
}

@Article{Jia2022,
  author  = {Jia, Bin and Zhang, Xiaosong and Liu, Jiewen and Zhang, Yang and Huang, Ke and Liang, Yongquan},
  journal = {IEEE Transactions on Industrial Informatics},
  title   = {Blockchain-Enabled Federated Learning Data Protection Aggregation Scheme With Differential Privacy and Homomorphic Encryption in IIoT},
  year    = {2022},
  number  = {6},
  pages   = {4049-4058},
  volume  = {18},
  comment = {distributed K means clustering/random forest/Adaboost suited for FL systems (empowered by some differential privacy and homomorphic encryption)},
  doi     = {10.1109/TII.2021.3085960},
  file    = {:2022Jia - Blockchain Enabled Federated Learning Data Protection Aggregation Scheme with Differential Privacy and Homomorphic Encryption in IIoT.pdf:PDF},
  groups  = {Blockchain-based FL},
  ranking = {rank3},
}

@Article{Li2021a,
  author    = {Dun Li and Dezhi Han and Tien-Hsiung Weng and Zibin Zheng and Hongzhi Li and Han Liu and Arcangelo Castiglione and Kuan-Ching Li},
  journal   = {Soft Computing},
  title     = {Blockchain for federated learning toward secure distributed machine learning systems: a systemic survey},
  year      = {2021},
  month     = {nov},
  number    = {9},
  pages     = {4423--4440},
  volume    = {26},
  doi       = {10.1007/s00500-021-06496-5},
  file      = {:2021novLi - Blockchain for Federated Learning toward Secure Distributed Machine Learning Systems_ a Systemic Survey.pdf:PDF},
  groups    = {Blockchain-based FL},
  publisher = {Springer Science and Business Media {LLC}},
  ranking   = {rank2},
}

@InProceedings{Fu2023,
  author     = {Fu, Minjie and Tao, Fuqiang and Yang, Zhi and Zou, Yufu and Li, Weiping and Sun, Zhe},
  booktitle  = {2023 2nd International Conference on Artificial Intelligence and Blockchain Technology (AIBT)},
  title      = {A Blockchain-Based Federated Random Forest Approach for Power-Related Data Collaborative Analysis},
  year       = {2023},
  pages      = {76-83},
  comment    = {combinition of blockchain and random forest for power-related analysis in smart grid systems
smart contract based

the decision tree is encrypted before being uploaded to the blockchain
there are many vague expressions.},
  doi        = {10.1109/AIBT57480.2023.00022},
  file       = {:2023Fu - A Blockchain Based Federated Random Forest Approach for Power Related Data Collaborative Analysis.pdf:PDF},
  groups     = {Blockchain-based learning},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@InProceedings{Souza2020,
  author     = {de Souza, Lucas Airam C. and Antonio F. Rebello, Gabriel and Camilo, Gustavo F. and Guimarães, Lucas C. B. and Duarte, Otto Carlos M. B.},
  booktitle  = {2020 IEEE International Conference on Blockchain (Blockchain)},
  title      = {DFedForest: Decentralized Federated Forest},
  year       = {2020},
  pages      = {90-97},
  comment    = {1. each domain query DTC from other domains and verify the DTC with its own local data, if the 
2. domains share the reference to the model in the blockchain in the form of transactions

deficiencies
1. Evaluate the model based on a network intrusion prototype
2. no PoUW},
  doi        = {10.1109/Blockchain50366.2020.00019},
  file       = {:2020Souza - DFedForest_ Decentralized Federated Forest.pdf:PDF},
  groups     = {Blockchain-based learning},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {skimmed},
}

@InProceedings{PratimKalita2023,
  author     = {Pratim Kalita, Kausthav and Boro, Debojit and Kumar Bhattacharyya, Dhruba},
  booktitle  = {2023 International Conference on Intelligent Systems, Advanced Computing and Communication (ISACC)},
  title      = {An Efficient Consensus Algorithm for Blockchain-Based Federated Learning},
  year       = {2023},
  pages      = {1-7},
  doi        = {10.1109/ISACC56298.2023.10083761},
  file       = {:2023Pratim Kalita - An Efficient Consensus Algorithm for Blockchain Based Federated Learning.pdf:PDF},
  groups     = {Blockchain-based FL},
  ranking    = {rank1},
  readstatus = {skimmed},
}

@InProceedings{Patil2022,
  author    = {Patil, Annapurna P. and Asad, Saarah},
  booktitle = {2022 4th International Conference on Circuits, Control, Communication and Computing (I4C)},
  title     = {A Blockchain-Based Framework for Building and Sharing Machine Learning Models},
  year      = {2022},
  pages     = {266-269},
  doi       = {10.1109/I4C57141.2022.10057644},
  file      = {:2022Patil - A Blockchain Based Framework for Building and Sharing Machine Learning Models.pdf:PDF},
  groups    = {ML x BC misc},
}

@InProceedings{Tao2022,
  author    = {Tao, Wenqi and Yin, Qifang},
  booktitle = {2022 7th International Conference on Big Data Analytics (ICBDA)},
  title     = {Candidate Models for Federated Learning with Blockchain},
  year      = {2022},
  pages     = {72-77},
  doi       = {10.1109/ICBDA55095.2022.9760330},
  file      = {:2022Tao - Candidate Models for Federated Learning with Blockchain.pdf:PDF},
  groups    = {Blockchain-based FL},
}

@Article{Kim2019,
  author  = {Kim, Hyunil and Kim, Seung-Hyun and Hwang, Jung Yeon and Seo, Changho},
  journal = {IEEE Access},
  title   = {Efficient Privacy-Preserving Machine Learning for Blockchain Network},
  year    = {2019},
  pages   = {136481-136495},
  volume  = {7},
  doi     = {10.1109/ACCESS.2019.2940052},
  file    = {:2019Kim - Efficient Privacy Preserving Machine Learning for Blockchain Network.pdf:PDF},
  groups  = {Blockchain-based FL,},
  ranking = {rank4},
}

@Article{Liu2022,
  author  = {Liu, Yang and Liu, Yingting and Liu, Zhijie and Liang, Yuxuan and Meng, Chuishi and Zhang, Junbo and Zheng, Yu},
  journal = {IEEE Transactions on Big Data},
  title   = {Federated Forest},
  year    = {2022},
  number  = {3},
  pages   = {843-854},
  volume  = {8},
  doi     = {10.1109/TBDATA.2020.2992755},
  file    = {:2022Liu - Federated Forest.pdf:PDF},
  groups  = {Ensemble Learning},
}

@InProceedings{Harris2019,
  author     = {Harris, Justin D. and Waggoner, Bo},
  booktitle  = {Proc. 2nd IEEE Int. Conf. Blockchain (Blockchain'19)},
  title      = {Decentralized and Collaborative AI on Blockchain},
  year       = {2019},
  address    = {Atlanta, GA, USA},
  month      = jul,
  pages      = {368-375},
  comment    = {An ML framework based on Ethereum, implemented with smart contract

Incentive mechanisms with a two stages, one for data adding and model update and another for evaluation on the committed test dataset by the task provider

Reward mechanism: reward model producers inproportion to the improvements of loss functions},
  doi        = {10.1109/Blockchain.2019.00057},
  file       = {:2019Harris - Decentralized and Collaborative AI on Blockchain.pdf:PDF},
  groups     = {Blockchain-based learning},
  ranking    = {rank2},
  readstatus = {skimmed},
}

@InProceedings{Wang2019,
  author    = {Wang, Shufen},
  booktitle = {2019 International Conference on Intelligent Computing, Automation and Systems (ICICAS)},
  title     = {BlockFedML: Blockchained Federated Machine Learning Systems},
  year      = {2019},
  pages     = {751-756},
  doi       = {10.1109/ICICAS48597.2019.00162},
  file      = {:2019Wang - BlockFedML_ Blockchained Federated Machine Learning Systems.pdf:PDF},
  groups    = {Blockchain-based learning},
  ranking   = {rank1},
}

@InProceedings{Korkmaz2020,
  author    = {Korkmaz, Caner and Kocas, Halil Eralp and Uysal, Ahmet and Masry, Ahmed and Ozkasap, Oznur and Akgun, Baris},
  booktitle = {2020 Second International Conference on Blockchain Computing and Applications (BCCA)},
  title     = {Chain FL: Decentralized Federated Machine Learning via Blockchain},
  year      = {2020},
  pages     = {140-146},
  comment   = {Use a modified online version of FedAvg to allow nodes to participate and contribute local model at any communication rounds (asynchronous FL).},
  doi       = {10.1109/BCCA50787.2020.9274451},
  file      = {:2020Korkmaz - Chain FL_ Decentralized Federated Machine Learning Via Blockchain.pdf:PDF},
  groups    = {Blockchain-based FL},
  ranking   = {rank1},
}

@Article{Shayan2021,
  author     = {Shayan, Muhammad and Fung, Clement and Yoon, Chris J. M. and Beschastnikh, Ivan},
  journal    = {IEEE Trans. Parallel Distrib. Syst.},
  title      = {Biscotti: A Blockchain System for Private and Secure Federated Learning},
  year       = {2021},
  month      = jul,
  number     = {7},
  pages      = {1513-1525},
  volume     = {32},
  comment    = {Biscotti (copied from original text):
"prevents peers from poisoning the model through the Multi-Krum defense, provides privacy through differentially private noise, and uses Shamir secrets for secure aggregation "

From [Boitier2024]:
"Biscotti [8] refines and elaborates on principles stipulated by FLChain [4] by introducing a three-role structure: noisers create unbiased noise for clients to mask local updates, verifiers endorse masked updates through robust filtering, and aggregators confidentially aggregate the endorsed updates. Aggregators work with encrypted data to ensure privacy. The encrypted aggregated model is stored on the blockchain within commitments to endorsed updates"},
  doi        = {10.1109/TPDS.2020.3044223},
  file       = {:2021Shayan - Biscotti_ a Blockchain System for Private and Secure Federated Learning.pdf:PDF;:Biscotti_A_Blockchain_System_for_Private_and_Secure_Federated_Learning.pdf:PDF},
  groups     = {Blockchain-based FL, FL Consensus},
  priority   = {prio2},
  ranking    = {rank3},
  readstatus = {read},
}

@Article{Qu2022fedtwin,
  author   = {Qu, Youyang and Gao, Longxiang and Xiang, Yong and Shen, Shigen and Yu, Shui},
  journal  = {IEEE Netw.},
  title    = {{FedTwin}: Blockchain-Enabled Adaptive Asynchronous Federated Learning for Digital Twin Networks},
  year     = {2022},
  month    = nov,
  number   = {6},
  pages    = {183-190},
  volume   = {36},
  comment  = {Proof-of-Federalism},
  doi      = {10.1109/MNET.105.2100620},
  file     = {:2022Qu - FedTwin_ Blockchain Enabled Adaptive Asynchronous Federated Learning for Digital Twin Networks.pdf:PDF},
  groups   = {Blockchain-based FL},
  priority = {prio2},
  ranking  = {rank2},
}

@InProceedings{Lazarevic2001,
  author    = {Lazarevic, Aleksandar and Obradovic, Zoran},
  booktitle = {Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title     = {The Distributed Boosting Algorithm},
  year      = {2001},
  address   = {New York, NY, USA},
  pages     = {311-316},
  publisher = {Association for Computing Machinery},
  series    = {KDD '01},
  abstract  = {In this paper, we propose a general framework for distributed boosting intended for efficient integrating specialized classifiers learned over very large and distributed homogeneous databases that cannot be merged at a single location. Our distributed boosting algorithm can also be used as a parallel classification technique, where a massive database that cannot fit into main computer memory is partitioned into disjoint subsets for a more efficient analysis. In the proposed method, at each boosting round the classifiers are first learned from disjoint datasets and then exchanged amongst the sites. Finally the classifiers are combined into a weighted voting ensemble on each disjoint data set. The ensemble that is applied to an unseen test set represents an ensemble of ensembles built on all distributed sites. In experiments performed on four large data sets the proposed distributed boosting method achieved classification accuracy comparable or even slightly better than the standard boosting algorithm while requiring less memory and less computational time. In addition, the communication overhead of the distributed boosting algorithm is very small making it a viable alternative to the standard boosting for large-scale databases.},
  doi       = {10.1145/502512.502557},
  file      = {:2001Lazarevic - The Distributed Boosting Algorithm.pdf:PDF},
  groups    = {Ensemble Learning},
  isbn      = {158113391X},
  keywords  = {classifier ensembles, distributed learning, Boosting},
  location  = {San Francisco, California},
  numpages  = {6},
}

@Article{An2022,
  author    = {Xibin An and Chen Hu and Zhenhua Li and Haoshen Lin and Gang Liu},
  journal   = {Neurocomputing},
  title     = {Decentralized {AdaBoost} algorithm over sensor networks},
  year      = {2022},
  month     = {mar},
  pages     = {37--46},
  volume    = {479},
  doi       = {10.1016/j.neucom.2022.01.015},
  file      = {:2022marAn - Decentralized AdaBoost Algorithm Over Sensor Networks.pdf:PDF},
  groups    = {Ensemble Learning},
  publisher = {Elsevier {BV}},
}

@Conference{Majeed2019a,
  author    = {Majeed, Umer and Hong, Choong Seon},
  booktitle = {2019년 한국소프트웨어종합학술대회(KSC 2019)},
  title     = {EFLChain: Ensemble learning via federated learning over blockchain network: A framework},
  year      = {2019},
  pages     = {845--847},
  comment   = {In EFLchain, the difference is that ensemble layer is added to ensemble the global models which FLchain system generates (through majority voting/average).},
  file      = {:KSC2019_Umer.pdf:PDF},
  groups    = {Blockchain-based FL},
  journal   = {한국정보과학회 학술발표논문집},
}

@InCollection{Liu2020fedcoin,
  author    = {Yuan Liu and Zhengpeng Ai and Shuai Sun and Shuangfeng Zhang and Zelei Liu and Han Yu},
  booktitle = {Federated Learn.: Privacy Incentive},
  publisher = {Springer},
  title     = {{FedCoin}: A Peer-to-Peer Payment System for Federated Learning},
  year      = {2020},
  address   = {Cham, CH},
  editor    = {Yang, Qiang and Fan, Lixin and Yu, Han},
  isbn      = {978-3-030-63076-8},
  month     = nov,
  pages     = {125--138},
  abstract  = {Federated learning (FL) is an emerging collaborative machine learning method to train models on distributed datasets with privacy concerns. To properly incentivize data owners to contribute their efforts, Shapley Value (SV) is often adopted to fairly and quantitatively assess their contributions. However, the calculation of SV is time-consuming and computationally costly. In this chapter, we propose FedCoin, a blockchain-based peer-to-peer payment system for FL to enable a feasible SV based profit distribution. In FedCoin, blockchain consensus entities calculate SVs and a new block is created based on the proof of Shapley (PoSap) protocol. It is in contrast to the popular BitCoin network where consensus entities ``mine'' new blocks by solving meaningless puzzles. Based on the computed SVs, we propose a scheme for dividing the incentive payoffs among FL participants with non-repudiation and tamper-resistance properties. Experimental results based on real-world data show that FedCoin can promote high-quality data from FL participants through accurately computing SVs with an upper bound on the computational resources required for reaching block consensus. It opens opportunities for non-data owners to play a role in FL.},
  comment   = {proof of shaplay consensus proposed to incentivize FL paritcipants fairly.
shaplay values produced by such consensus can evaluate paticipants' contribution to the global model. As calculation of shaplay value is time-consuming, it's viable to replace hashing puzzles in traditional pow-based blockchain. Miners competitively calculate shapley values to prove their computational power after global model updated and shapley value calculation task published.
Block generation and verification: the miner whose Shapley value S is adaquately close to averaged S (by difficulty D) becomes the winner and generated a new block. If winner's S is close asserted S and local aggregated S, then its block is valid.},
  doi       = {10.1007/978-3-030-63076-8_9},
  file      = {:2020Liu - FedCoin_ a Peer to Peer Payment System for Federated Learning.pdf:PDF},
  groups    = {Blockchain-based FL},
  priority  = {prio2},
  ranking   = {rank4},
}

@InProceedings{Bao2019,
  author    = {Bao, Xianglin and Su, Cheng and Xiong, Yan and Huang, Wenchao and Hu, Yifei},
  booktitle = {2019 5th International Conference on Big Data Computing and Communications (BIGCOM)},
  title     = {FLChain: A Blockchain for Auditable Federated Learning with Trust and Incentive},
  year      = {2019},
  pages     = {151-159},
  doi       = {10.1109/BIGCOM.2019.00030},
  file      = {:2019Bao - FLChain_ a Blockchain for Auditable Federated Learning with Trust and Incentive.pdf:PDF},
  groups    = {Blockchain-based FL},
  keywords  = {blockchain;federated learning;incentive;decentralize;trust},
  ranking   = {rank1},
}

@InProceedings{Desai2021,
  author    = {Desai, Harsh Bimal and Ozdayi, Mustafa Safa and Kantarcioglu, Murat},
  booktitle = {Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy},
  title     = {BlockFLA: Accountable Federated Learning via Hybrid Blockchain Architecture},
  year      = {2021},
  address   = {New York, NY, USA},
  pages     = {101–112},
  publisher = {Association for Computing Machinery},
  series    = {CODASPY '21},
  abstract  = {Federated Learning (FL) is a distributed, and decentralized machine learning protocol. By executing FL, a set of agents can jointly train a model without sharing their datasets with each other, or a third-party. This makes FL particularly suitable for settings where data privacy is desired.At the same time, concealing training data gives attackers an opportunity to inject backdoors into the trained model. It has been shown that an attacker can inject backdoors to the trained model during FL, and then can leverage the backdoor to make the model misclassify later. Several works tried to alleviate this threat by designing robust aggregation functions. However, given more sophisticated attacks are developed over time, which by-pass the existing defenses, we approach this problem from a complementary angle in this work. Particularly, we aim to discourage backdoor attacks by detecting, and punishing the attackers, possibly after the end of training phase.To this end, we develop a hybrid blockchain-based FL framework that uses smart contracts to automatically detect, and punish the attackers via monetary penalties. Our framework is general in the sense that, any aggregation function, and any attacker detection algorithm can be plugged into it. We conduct experiments to demonstrate that our framework preserves the communication-efficient nature of FL, and provide empirical results to illustrate that it can successfully penalize attackers by leveraging our novel attacker detection algorithm.},
  comment   = {smart contract based + hybird chain architecture
aggregate the local updates on the private chain;
register the local updates on the public chain to make them accountable},
  doi       = {10.1145/3422337.3447837},
  file      = {:2021Desai - BlockFLA_ Accountable Federated Learning Via Hybrid Blockchain Architecture.pdf:PDF},
  groups    = {Blockchain-based FL},
  isbn      = {9781450381437},
  keywords  = {machine learning, hyperledger, hybrid blockchain, federated learning, federated averaging, ethereum, backdoor attacks},
  location  = {Virtual Event, USA},
  numpages  = {12},
  ranking   = {rank1},
  url       = {https://doi.org/10.1145/3422337.3447837},
}

@Article{Peng2022,
  author   = {Peng, Zhe and Xu, Jianliang and Chu, Xiaowen and Gao, Shang and Yao, Yuan and Gu, Rong and Tang, Yuzhe},
  journal  = {IEEE Transactions on Network Science and Engineering},
  title    = {VFChain: Enabling Verifiable and Auditable Federated Learning via Blockchain Systems},
  year     = {2022},
  number   = {1},
  pages    = {173-186},
  volume   = {9},
  comment  = {committee based consensus; a learning contract to aggregate local model updates
committee selection based on "trust value"
the verify contract use the signature of local gradients and the generated global model to verify

dual skip chain: improve search efficiency and model audit efficiency, two types of blocks: 
anchor block records an index of authorizations from the committee and the update of the committee, while the docker block stores the verifiable information of model updates in
each iteration of federated learning},
  doi      = {10.1109/TNSE.2021.3050781},
  file     = {:2022Peng - VFChain_ Enabling Verifiable and Auditable Federated Learning Via Blockchain Systems.pdf:PDF},
  groups   = {Blockchain-based FL},
  keywords = {Blockchain;Collaborative work;Training;Data models;Servers;Computational modeling;Task analysis;Auditable training;blockchain;federated learning;model verification.},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Blockchain Analysis\;0\;0\;0xffff00ff\;\;\;;
1 StaticGroup:Blockchain-based learning\;0\;1\;0xff0000ff\;\;\;;
2 StaticGroup:Blockchain-based FL\;0\;1\;0xffb366ff\;\;\;;
1 StaticGroup:Byzantine-Resilient Learning\;0\;1\;0x0000ffff\;\;\;;
2 StaticGroup:Byzantine-Resilient FL\;0\;1\;0xcce6ffff\;\;\;;
1 StaticGroup:Ensemble Learning\;0\;1\;0xff00ffff\;\;\;;
1 StaticGroup:Evolutionary Algorithm\;0\;1\;0xffccb3ff\;\;\;;
1 StaticGroup:ML x BC misc\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:PoUW\;0\;1\;0x336633ff\;\;\;;
}
