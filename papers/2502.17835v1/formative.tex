\section{Formative Study} 
\label{sec:formative}
We conducted a formative study to explore instructors' challenges in collaborative programming and their visualization needs, with the study protocol approved by our university's Institutional Review Board (IRB).
 
 \subsection{Participants}
We recruited ten participants (five females, age: \(30.6 \pm 6.6\)) with collaborative programming experience, divided into three groups: two educational technology experts (E1, E2) and eight instructors (T1–T8) with an average of 8.14 years of teaching programming. Participants, recruited via snowball sampling from the authors' network, received \$20 as compensation.
 

\subsection{Procedure}\
\RR{The formative study used semi-structured interviews conducted via Zoom, divided into two sections: (A) Questions and Answers and (B) Ratings.}
\textit{A: Questions and Answers.} 
Each participant was independently interviewed to explore the need for collaborative programming analysis. Topics included teaching experience with collaborative programming, class organization, challenges faced, assessment methods, and group and individual performance evaluations. Follow-up questions were asked for clarification or more profound insights. Each session lasted 40–60 minutes and was documented through written notes, audio, and video recordings.
\textit{B: Rating.} 
Participants then rated two aspects of the collaborative programming visual analytics system. They rated feature importance (Q1) on a 1–7 scale (7 being the highest) and ranked features by priority (Q2). 
\RR{More details are in appendix A.}

 
\subsection{Findings} 

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{scheme1.pdf}
	\caption{Collaborative programming coding schemes, along with their definitions and examples.}
        \Description{Collaborative programming coding schemes, along with their definitions and examples.}
	\label{fig:scheme}
\end{figure*}



\subsubsection{Evaluation of Groups:}
In large classes, instructors struggled to monitor each group without the help of teaching assistants. They often relied on group presentations (E2, T1, T5), but it was inaccurate. Limited class time forced instructors to focus on solving issues rather than actively monitoring groups.


\textbf{Group Performance:}
Evaluating group work was based on the final code rather than the process, limiting the ability to provide feedback (T7). Some instructors used presentations or technical documentation to streamline assessments (T6). However, evaluating more than 20 groups was still challenging, and instructor assistance was often overlooked during evaluations.


\textbf{Collaboration Evaluation:}
Effective collaboration wasn't just about task completion speed and group dynamics. Some groups (T7) completed tasks quickly due to one member's efforts, not true collaboration. 
\RR{E1 proposed the collaborative problem-solving framework to distinguish task effectiveness from team effectiveness.} Instructors (T6) believed monitoring discussion time helped assess collaboration quality and task difficulty, but off-topic discussions made it hard to evaluate group discussions (E1, T3, T5).


\subsubsection{Evaluation of Students:}

In large classes, assessing individual contributions in group work was challenging. Instructors relied on peer evaluations (T2, T3, T4) and self-reported task distributions (E2), which were often subjective.

\textbf{Individual Performance:}
Instructors typically reviewed code to assess understanding, but measuring individual contributions in group work was hard. Leadership roles often reflected a deeper grasp of concepts (T1), but tracking individual engagement was difficult in large classes.


\textbf{Personalized Feedback:}
Providing personalized feedback was difficult, as group results often masked individual struggles. T3 and T4 noted that group collaboration fostered peer learning but could lead to less engagement from weaker students.
\textit{T8 added that offering personalized feedback in large classes was time-consuming and burdensome.}

 
 
 
\subsection{Design requirement}
\label{dr}
Based on the interview findings, we identified six design requirements (R1–R6) across three levels, summarized as follows:

Support \textbf{\textit{inter-group-level}} to provide a macro perspective, enabling instructors to observe the overall situation of all groups comprehensively and fully understand class-wide dynamics.

\textbf{R1: Displaying the Overall Performance of all Groups.} 
Instructors face challenges in supervising multiple groups simultaneously and shifting focus efficiently. Participants stressed the need for an overview of group performance, allowing instructors to grasp class dynamics and selectively review specific groups.


\RR{\textbf{R2: Comparing Similar and Different Groups.} 
Instructors often compare students' performance to assess their relative standing within the class~\cite{marsh1997making}. Such comparisons enable a more accurate evaluation of group performance and help identify groups excelling or encountering challenges.}


Supporting \textbf{\textit{intra-group-level}} visual exploration to offer a meso perspective, enabling instructors to observe specific groups' performance and gain a comprehensive understanding of group dynamics during the collaborative programming process.


\textbf{ R3: Understand the Dynamics of Programming Problem Solving.}
Analyzing a group's evolving communication patterns and computational thinking during programming tasks provides instructors with deeper insights into students' progress and intermediate learning objectives—details missed in final submissions alone.


\textbf{ R4: Identify Teacher Scaffolding in Collaboration.}
Instructors play a vital role in guiding groups during collaborative programming. Understanding the scaffolding provided and students' responses can help refine instructional strategies, improving the overall effectiveness of collaborative programming.


Supporting \textbf{\textit{individual-student-level}} visual exploration to provide a micro perspective allows instructors to observe each student's performance within a specific group and better understand their role and collaboration.

\textbf{R5: Track Changes in Student Engagement Over Time.} 
Limited classroom time makes it challenging for instructors to monitor individual student engagement in programming tasks.
Tracking and visualizing engagement trends is essential for assessing performance and refining instructional practices.

\textbf{R6: Access Detailed Raw Data.} 
Instructors require access to raw data, such as collaboration videos, conversations, and background information. These details are crucial for validating analysis results and supporting personalized feedback and assessments.

