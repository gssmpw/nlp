\section{Evaluation}
\label{evaluation}
\RR{To comprehensively evaluate the effectiveness of \textit{CPVis} in analyzing collaborative programming, we adopted a multi-faceted research approach. First, we conducted a quantitative study to validate the accuracy of collaboration performance annotations generated by LLMs. Next, we demonstrated two cases explored by computer science professors to showcase how \textit{CPVis} supported group evaluations and provided personalized feedback to students.
Finally, we recruited 22 participants for a user study to assess the practicality and user experience of \textit{CPVis}.}
All study designs were approved by our university's IRB.

\RR{\subsection{Quantitative Study}}
\RR{Using an LLM-driven approach for data annotation may raise trust concerns~\cite{liao2023ai}. To verify whether LLM performance impacts the validity of \textit{CPVis} results, we conducted a quantitative study to assess the accuracy and reliability of this method by sampling 20\% of the dataset. Specifically, we analyzed the following areas: evaluating code quality, annotating collaborative programming behaviors, identifying student roles, and recognizing teacher scaffolding.}

Inspired by CFlow~\cite{zhang2024cflow}, we recruited two experienced collaborative programming instructors (I1-I2) to annotate the data in four aspects. For instance, in the code quality evaluation, we asked the instructors to grade based on the same criteria used by ChatGPT-4o, \RR{with a score ranging from 1 to 5 for each dimension.} Detailed scoring criteria can be found in Sec.~\ref{code}.
First, I1 reviewed ChatGPT-4o's annotations and updated their original labels if necessary. I2 was then tasked with comparing I1's revised results (Version 1) and ChatGPT-4o's results (Version 2) without knowing the source of either label set.
\RR{I2 had to select one of the following four options: (1) I agree with Version 1, (2) I agree with Version 2, (3) Both versions seem valid, or (4) Neither version seems valid.}
We evaluated LLMs' performance in code quality by comparing it to human-labeled results.
The results showed that I1 and I2 reached 93.43\% agreement, while ChatGPT-4o 's annotations matched I1 and I2's annotations with 85.62\% and 89.32\% consistency, respectively. Therefore, we concluded that ChatGPT-4o's annotations were reliable, and we trusted its results.
Additionally, we employed the same method to assess three other dimensions.
Among these, ChatGPT-4o's accuracy was relatively lower in classifying collaborative programming behaviors (90.32\%) and code quality (93.43\%) but higher in identifying student roles (96.54\%) and teacher scaffolding (97.42\%). 
\RR{To mitigate the impact of annotation errors, we added prediction percentage and explanations to ChatGPT-4o's annotations of collaborative programming behaviors (Fig.~\ref{fig:teaser}-B3e), indicating the uncertainty of classification.} Similarly, we added explanations for code quality (Fig.~\ref{fig:teaser}-B1), providing more evidence for instructors during analysis.


\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{network.pdf}
	\caption{\RR{A screenshot of the ``Group Pattern'' view for Group 10 and Group 18,} with the left side showing Group 10's behavioral pattern in Q1 and the right side displaying the results after adding the comparison group 18. In comparison, it is evident that Group 10's ``Python coding'' behavior decreased, while Group 18's ``Debugging'' behavior increased, indicating more in-depth programming discussions in Group 18 during Q1.}
        \Description{A screenshot of the ``Group Pattern'' view for Group 10 and Group 18, with the left side showing Group 10's behavioral pattern in Q1 and the right side displaying the results after adding the comparison group 18. In comparison, it is evident that Group 10's ``Python coding'' behavior decreased, while Group 18's ``Debugging'' behavior increased, indicating more in-depth programming discussions in Group 18 during Q1.}
	\label{fig:network} \end{figure*}


\subsection{Case Study}
\label{case}
\RR{
We re-invited four computer science professors (P1-P4) from three universities who teach relevant courses and have extensive expertise in collaborative programming and data visualization to evaluate \textit{CPVis} and explore a real-world dataset using the system. 
We began by introducing the system's background, visual design, and workflow demonstration. Through two case studies based on the professors' interactions with \textit{CPVis}, we showcased the system's effectiveness in evaluating group performance and providing personalized feedback to students.
}
\subsubsection{Case 1: Assessing Similar Groups' Performance}
\RR{
The case summarized from P1 and P3 focused on the evaluation of group performance in collaborative programming and the feedback provided.}
They began by examining the filter view, where two closely positioned triangles with darker colors and larger outer arcs caught their attention (Fig.~\ref{fig:teaser}-A1b). 
Interested in comparing the two groups, they used the lasso tool to select them for further analysis.
The bouquet visualization revealed that Group 10 had one more butterfly than Group 18 (Fig.~\ref{fig:teaser}-A2), indicating stronger collaborative problem-solving skills.
Group 10's petals were more complete, and its leaves were greener, suggesting higher student engagement and a greater level of teacher scaffolding. 
\RR{Next, they searched for Group 10 (Fig.~\ref{fig:teaser}-A1a) and noticed that Group 18 was identified as the most similar group in the similarity panel (Fig.~\ref{fig:teaser}-A3). 
They selected Group 18 for comparison.}
In the code panel (Fig.~\ref{fig:teaser}-B1), they observed differences in the solutions for Q1 and Q5, while the rest of the questions were identical.
Coding analysis highlighted that Group 18 failed to output List A as required in Q1, resulting in a lower score.
For Q5, Group 18 used the \textit{`eval()'} function, while Group 10 correctly used \textit{`int()'} for type conversion, avoiding potential security risks and scoring higher (Fig.~\ref{fig:teaser}-B1).
While exploring the behavior pattern view (Fig.~\ref{fig:student}-B2b), they observed a yellow node in Group 18 gradually enlarging, representing ``Question Planning'' (Fig.~\ref{fig:student}-B2d).
Curious, they switched to the timeline panel and filtered for the discussion periods corresponding to Q4 and Q5 (Fig.~\ref{fig:group18}-c\&f).
Through the timeline, they discovered that during a specific period in Q4, all participants in Group 18 were assigned the ``Monitor'' role, indicated by blue bars (Fig.~\ref{fig:group18}-a).
By clicking on the bar, they found that the instructor had intervened, offering \textit{medium-control cognitive scaffolding} to guide students in adjusting their code and encouraging persistence (Fig.~\ref{fig:group18}-b).
\RR{For Q5, the instructor's involvement was shorter (Fig.~\ref{fig:group18}-e), providing \textit{metacognitive scaffolding} to encourage the group to try alternative approaches (Fig.~\ref{fig:group18}-d), such as using \textit{`eval()'}.}
Ultimately, Group 18 followed the instructor's advice and experimented with \textit{`eval()'}.
Through the timeline panel (Fig.~\ref{fig:teaser}-B3d), they found that the instructor provided \textit{high control cognitive scaffolding} (Fig.~\ref{fig:teaser}-B3e) for Q1 and Q4 of the group 10 and explained the detailed solutions (Fig.~\ref{fig:teaser}-C2a).
Finally, they validated these observations by reviewing classroom recordings.
They concluded that although Group 10 submitted higher-quality code, it benefited from greater instructional support. Therefore, Group 18 demonstrated better overall performance, considering its lower reliance on teacher scaffolding.








\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{group6.pdf}
	\caption{A screenshot of Group 6: (a) the Filter View, (b) the Group 6's bouquets, (c) the similarity panel, (d) the codes panel, (e) the student projection panel, and (f) the timeline panel.}
	\Description{A screenshot of Group 6: (a) the Filter View, (b) the Group 6's bouquets, (c) the similarity panel, (d) the codes panel, (e) the student projection panel, and (f) the timeline panel.}
	\label{fig:group6}
\end{figure*}

\subsubsection{Case 2: Providing Personalized Feedback for Students.}
\RR{The case summarized from the exploration process of P2 and P4 focused on providing personalized feedback, with our system helping identify both disengaged students and highly engaged ones who need further support.}
Using the lasso tool in the filter view (Fig.\ref{fig:group6}-a), the professors identified flowers missing petals and with smaller stamens (Fig.\ref{fig:group6}-b). Group 6's particularly small petals prompted further exploration. In the code panel, they noticed Group 6 consistently received scores above 4 (Fig.\ref{fig:group6}-d), indicating strong performance. 
However, in the timeline panel, they saw that the students' roles—Monitor (0601), Navigator (0602), and Driver (0603)—were independent with minimal collaboration (Fig.~\ref{fig:group6}-f). They also observed incomplete engagement data, with three missing questions. After reviewing the video, the professors confirmed the lack of collaboration and suggested encouraging more active engagement and collaboration among all members.


\RR{They noticed that Group 10 was the most different from Group 6 in the similarity panel (Fig.~\ref{fig:group6}-c). }
By selecting Group 10 in the filter panel, they observed that Group 10 had three butterflies (Fig.~\ref{fig:teaser}-A2), indicating a high level of collaborative problem-solving skills. 
Within the group, student 1002 stood out, with their flower being orange and having larger stamens, suggesting high levels of both behavioral and cognitive engagement. In the student projection panel, it was revealed that 1002 had a prior score of 90 (Fig.~\ref{fig:student}-B2a). 
On the timeline panel, 1002's curve was significantly higher than those of the other group members, confirming a high level of engagement. This student primarily assumed the role of Driver and frequently switched roles during discussions.
\RR{By exploring the timeline view, it was found that in Q5, 1002 repeatedly sought help from the instructor while other group members were discussing. Finally, the instructor provided high-control cognitive scaffolding (Fig.~\ref{fig:teaser}-B3d) to explain in detail how to solve the problem and pointed out a syntax error made by the group.}
Through their analysis, the professors discovered that although 1002 was actively engaged, they tended to rely excessively on the instructor during group discussions.  
The professors recommended that 1002 focus on developing independent thinking skills by first consulting resources or discussing with group members when facing challenges rather than immediately seeking instructor assistance. 






\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{group18.pdf}
    \caption{A screenshot of Group 18: (a) \& (e) show all three students acting as monitors while the instructor is speaking. (b) \& (d) indicate that the current bar represents the instructor's scaffolding provided, with a tooltip added. (c) \& (f) show that Q4-Q5 were selected on the progress bar.}
    \Description{A screenshot of Group 18: (a) \& (e) show all three students acting as monitors while the instructor is speaking. (b) \& (d) indicate that the current bar represents the instructor's scaffolding provided, with a tooltip added. (c) \& (f) show that Q4-Q5 were selected on the progress bar.}
    \label{fig:group18}
\end{figure*}
\subsection{User Study}
\RR{User Study focused on three core objectives: G1, to assess \textit{CPVis}'s performance in collaborative group evaluation tasks; G2, to validate the effectiveness of \textit{CPVis} in delivering personalized feedback to students; and G3, to evaluate the overall user experience of \textit{CPVis}. }
\subsubsection{Participants}

We re-recruited 10 instructors and 12 teaching assistants (TAs) from the Computer Science department to participate in this user study (P1-P22, age: $ 28.95 \pm 5.21 $). Among the instructors, 2 were professors, and 8 were assistant professors, with 6 male and 4 female participants. On average, the instructors had 4.6 years of experience teaching programming courses, while the TAs had an average of 1.5 years of experience assisting with programming courses. All participants had experience teaching or co-teaching programming courses and were familiar with collaborative programming education. 
Given the study's focus on collaborative programming environments, we only recruited participants with a computer science background to ensure they could provide authentic and interpretable insights for our research. All sessions were conducted online via Zoom, and each participant received a \$20 reward at the end of the study.

\subsubsection{Study Design and Procedure}

We conducted a within-subjects user study comparing the full version of \textit{CPVis} with two baseline systems. 

\RR{
\textbf{Baseline System 1:} In real-world classroom settings, instructors typically evaluate students and groups based on raw data. 
To simulate traditional post-class evaluations, baseline system 1 provided only raw data, including students' background information (such as major and past grades), group-submitted code, and group discussion videos. The design of Baseline System 1 aimed to assess whether visualizing collaborative programming data could improve the efficiency and accuracy of instructors' evaluations of student and group performance.

\textbf{Baseline System 2:} Since no systems were available for multi-level assessment of collaborative programming in real classrooms, we selected the full \textit{CPVis} system for an ablation study. Baseline System 2 retained the same learning analytics features and visual design as \textit{CPVis} but excluded the comparison feature. The design of Baseline System 2 aimed to assess whether adding more panels introduced additional complexity and whether the comparison feature in \textit{CPVis} improved the efficiency of instructors' assessments.
}

We randomly selected 10 groups from 19 as experimental datasets for each system, ensuring that the data for each system was entirely distinct.
Participants were also randomly assigned to experiment with all three systems, ensuring fairness and randomness in the study.
The actual experiment consisted of three phases:

\textbf{\textit{Introduction:}} First, we introduced participants to the primary objectives of the system and the research background through a video. After obtaining their informed consent and collecting their personal information, we provided a detailed explanation of the key concepts and metaphors involved in the experiment to ensure participants could understand and effectively utilize these metaphors in the following phases. By administering a basic metaphor comprehension questionnaire (see supplementary materials), we confirmed that participants had fully grasped the necessary metaphors. The entire introduction phase lasted approximately 20 minutes.

\textbf{\textit{Exploration:}} In this phase, participants were asked to explore three systems, perform assigned tasks, and explore freely or based on their own needs. Each participant was required to use three systems to complete two tasks. The first task involved rating the collaborative groups in the dataset (2 as excellent, 2 as good, 2 as fair, 2 as pass, and 2 as fail). The second task was to identify students who needed personalized feedback. To prevent the task completion time from being influenced by the order, we shuffled the group IDs in each system to balance the difficulty across the two systems, ensuring fairness in the experimental results.


\textbf{\textit{Feedback:}} Based on the participants' experience with the system, they were asked to complete a user experience questionnaire, which included 7-point Likert scale questions derived from existing literature~\cite{xia2019peerlens,shi2018meetingvis}, \RR{as shown in Fig.~\ref{fig:result}.} Additionally, we conducted follow-up interviews to gather more feedback and insights about our system and its potential real-world applications. These interviews lasted approximately 15 minutes.

\textbf{\textit{Hypotheses:}}
We propose the following hypotheses based on Peerlens~\cite{xia2019peerlens}:

\textit{H0: }Both the full and ablated versions of \textit{CPVis} outperform Baseline System 1 in terms of information access and analysis. Specifically, \textit{CPVis} shows advantages in accessibility (H0a), data richness (H0b), sufficiency (H0c), and detailed (H0d) compared to Baseline System 1.

\textit{H1: }Both the full and ablated versions of \textit{CPVis} outperform Baseline System 1 in terms of user experience and decision support. Specifically, \textit{CPVis} excels in coherence (H1a), decisiveness (H1b), and usefulness (H1c) over Baseline System 1.

\textit{H2: }Both the full and ablated versions of \textit{CPVis} surpass Baseline System 1 in terms of ease of use and recommendability. Specifically, \textit{CPVis} excels in learnability (H2a), usability (H2b), and recommendability (H2c) compared to Baseline System 1.

\textit{H3: }The full version of \textit{CPVis} provides more information than the ablated version. Specifically, the full version outperforms the ablated version in accessibility (H3a), data richness (H3b), information sufficiency (H3c), and detailed (H3d).

\textit{H4: }The full version of \textit{CPVis} outperforms the ablated version in user experience and decision support. Specifically, the full version excels in coherence (H4a), decisiveness (H4b), and usefulness (H4c).

\textit{H5: }The ablated version of \textit{CPVis} is superior to the full version. Specifically, the ablated version is perceived as more learnable (H5a), more usable (H5b), and more recommendable (H5c) compared to the full version.
\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{result1.pdf}
	\caption{Results of the RM-ANOVA for the Baseline 1, Baseline 2, and full version \textit{CPVis} questionnaires, based on a 7-point Likert scale (1 means disagree, 7 means agree), including the Mean (M) and Standard Deviation (SD).}
        \Description{Results of the RM-ANOVA for the Baseline 1, Baseline 2, and full version \textit{CPVis} questionnaires, based on a 7-point Likert scale (1 means disagree, 7 means agree), including the Mean (M) and Standard Deviation (SD).}
	\label{fig:result}
\end{figure*}
\subsection{Result and Analysis}
\RR{
We reported the evaluation results based on two completed tasks and the overall system experience. A repeated measures analysis of variance (RM-ANOVA) was conducted for each questionnaire item, followed by Bonferroni post-hoc tests for measures with significant differences. The results showed favorable performance and positive feedback on G1, G2, and G3. The ratings are shown in Fig. ~\ref{fig:result}.
}

\textbf{\textit{G1, to assess \textit{CPVis}'s performance in collaborative group evaluation tasks.}}
Participants completed the task of evaluating collaborative groups but varying evaluating standards made it challenging to quantify the accuracy of their evaluations. To understand their criteria, we asked, ``What criteria did you use to rate these groups?''
In \textit{Baseline 1}, participants evaluated factors such as code errors, meeting the question requirements, code quality, time complexity, readability, and the use of provided methods. 
In \textit{Baseline 2}, more learning analytics and visualization allowed participants to expand their evaluation criteria. 
They focused on ChatGPT's code assessment, student engagement, quality of collaborative problem-solving, student roles, and teacher scaffolding, as represented in the flower-based visualization. 
\textit{P5 mentioned that, due to similar group answers, assessing accuracy from code alone was challenging. However, the flower-based visualization helped reveal both group and individual performance.}
In \textit{CPVis}, the introduction of comparison panels further expanded the evaluating criteria. \textit{P13 noted that the behavior pattern view in the comparison panel played a crucial role in their evaluations, as it allowed them to analyze behavior patterns over time for two groups within the same interface. This was particularly useful for comparing highly similar groups.} 



\RR{
Regarding time spent on the task, we found that although \textit{Baseline 1} provided a large amount of data, it was relatively raw. Participants typically lacked the patience to examine all the details and relied more on code scoring, spending an average of 20 minutes.}
In contrast, participants using \textit{Baseline 2} and \textit{CPVis} systems spent more time, averaging 35 minutes and 30 minutes, respectively. We believe this was due to the richer learning analytics provided in these versions, which engaged participants more deeply in exploring collaborative programming details.


\textbf{\textit{G2, to assess the effectiveness of \textit{CPVis} in providing personalized feedback to students.}}
We gained valuable insights by asking participants, ``Which two students do you think need feedback the most, and why?''
In \textit{Baseline 1}, since the code was submitted as a group, participants found it challenging to assess individual performance, making it difficult to provide personalized feedback.
In \textit{Baseline 2}, participants quickly identified students needing feedback, focusing on outliers with low engagement or imbalanced roles. Feedback typically encouraged more active engagement in discussions and collaboration.
In \textit{CPVis}, participants highlighted students with ``contradictory'' traits. \textit{P8 pointed out a student with a poor academic record who displayed high behavioral engagement in the current group programming project, clearly trying to keep up with the group. Conversely, P9 identified a student with a strong academic record who showed low engagement and remained mostly silent during discussions.}
\RR{We believe that \textit{CPVis}'s visualization capabilities not only help participants quickly identify students with unusual performance but also allow them to recognize students who may require additional attention or support.}



\textbf{\textit{G3, to assess the overall user experience of \textit{CPVis}. }}
Figure~\ref{fig:result} presented the questionnaire results. \RR{Overall, the ablated and full versions of \textit{CPVis} scored significantly higher than Baseline1 across all evaluation metrics, with statistically significant differences. 
All metric scores of the full version of \textit{CPVis} were higher than those of the ablated version (Baseline 2).
Compared to the ablated version, the full version of \textit{CPVis} introduced comparison features, resulting in more panels. However, the results showed that, except for \textit{Richness}, \textit{Sufficiency}, \textit{Detailed}, and \textit{Learnability}, all other metrics exhibited statistically significant differences. 
Thus, hypotheses \textit{H3b, H3c, H3d, H5b, and H5c} were not supported, while \textit{H3a, H4a, H4b, H4c, and H5a} were supported.
It indicated that the full version of \textit{CPVis} added valuable features without introducing unnecessary complexity. 
Although no significant differences were observed in the \textit{Learnability} metric, both the full and ablated versions were easy to learn and effectively enhanced the instructor's assessment efficiency.}
\textit{P10 stated, ``The group behavior pattern analysis in the full version helped me a lot. Comparing the two groups made it easier to see which group had more in-depth collaboration. In contrast, directly comparing two node connection diagrams would have been much harder.''}
Additionally, participants consistently recognized the value of learning analytics during follow-up interviews. \textit{CPVis} provided multi-level learning analysis from groups to individual students. While the ablated version reduced some comparison features, it did not remove any analytical dimensions, which explained the lack of significant differences in \textit{Richness}, \textit{Sufficiency}, and \textit{Detailed}. 
\RR{On average, participants still gave higher ratings to \textit{CPVis}.}
Participants provided separate ratings for these designs in Q7 and Q8, with scores ranging from 5 to 7. For the \textit{Visual Encoding} question ($M = 6.63, SD = 0.58$) \raisebox{-0.2\height}{\includegraphics[width=0.5cm]{q7.pdf}}, most participants found the flower metaphor intuitive and thought the visual design was highly innovative. 
P14 remarked that she felt a sense of familiarity with the flower design, stating that using flowers to represent students was easy to understand and accept, with a reasonable encoding method.
For the \textit{Visual Design} ($M = 6.32, SD = 0.78$) \raisebox{-0.2\height}{\includegraphics[width=0.5cm]{q8.pdf}}, participants noted that by observing the design of flowers or bouquets, they could quickly and intuitively understand the overall situation of groups and students, which facilitated further exploration of details. 
