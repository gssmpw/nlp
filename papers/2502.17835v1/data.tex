\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{bar2.pdf}
    \caption{(a) shows the bar chart of the raw data, (b) presents the results of applying Moving Average Smoothing to reduce anomalies in prediction percentages, and (c) highlights the reduction of visual clutter and emphasizes sequential behavior patterns after merging behaviors of the same category.}
    \label{fig:bar}
    \Description{(a) shows the bar chart of the raw data, (b) presents the results of applying Moving Average Smoothing to reduce anomalies in prediction percentages, and (c) highlights the reduction of visual clutter and emphasizes sequential behavior patterns after merging behaviors of the same category.}
\end{figure*}

\section{Data Collection and Processing}
\label{sec:data}
\RR{In this section, we provided an overview of the data collection context and introduced the collaborative programming performance framework along with its metric quantification methods.}

\subsection{Data Collection}
We collaborated with Professor E1, an expert in programming education, and teaching assistants (TA1 and TA2), experienced in Python, to collect data from E1's Spring 2023 Python course with 66 non-computer science freshmen in 22 groups. Using non-intrusive methods, we recorded group discussions, screen activities (without audio), and code submissions. Session lengths ranged from 10 to 60 minutes based on question completion. 
Due to data quality issues, we selected data from 19 groups (57 students) for analysis.


\subsection{Data Preprocessing}
In collaborative programming analysis, students' spoken content was key to understanding discussion and evaluating collaboration. We used the Faster-Whisper model~\cite{fasterwhisper} for speech recognition and the Pyannote-audio model~\cite{pyannoteaudio} for speaker diarization. 
For groups lacking clear problem-solving strategies, we used Tesseract OCR~\cite{tesseract} to analyze screen recordings and extract key frames through screenshots.

\subsection{Scope of Collaborative Programming Performance Framework}
Evaluating student and group performance in collaborative programming required considering multiple dimensions~\cite{hawlitschek2023empirical}.  
Building on literature and expert input (E1), we proposed the following comprehensive analytical framework to assess performance. 



\subsubsection{Student Performance Assessment}
\label{shema}
Previous research demonstrated that students' skills, backgrounds, and personalities in the classroom vary significantly, affecting their engagement and learning outcomes~\cite{wu2019analysing}. 
Therefore, we focus on each student's \textit{background} (prior academic performance and major), \textit{role transitions}, \textit{behavioral engagement}, and \textit{cognitive engagement}.






\textbf{Problem-solving Categorization:}
Based on previous frameworks~\cite{wu2019analysing}, team theory~\cite{zhao2023analysing}, and collaborative coding processes~\cite{sun2021three}, we developed a coding scheme (Fig.~\ref{fig:scheme}) to capture group problem-solving in collaborative programming. 
The scheme used four color-coded categories to represent discussion types. 
The first three categories followed a hierarchical structure, indicating discussion depth, while the green category focuses on situation awareness and specific behaviors.

Building on the scheme, we used tailored prompts with the ChatGPT-4o model~\cite{gpt4o} to classify behavioral patterns in transcribed dialogue \RR{(More details are in appendix B)}. 
\RR{The model provided a prediction percentage of uncertainty for each classification, improving result interpretability. }
To minimize anomalies, we applied a ``moving window'' technique with Moving Average Smoothing~\cite{chang2022muse}, stabilizing prediction percentages (Fig.\ref{fig:bar}-b). To reduce visual clutter in long time-series data, we aggregated consecutive instances of the same category, averaging prediction percentages (Fig.\ref{fig:bar}-c). These results were displayed in the timeline panel's progress bar, enabling detailed analysis by zooming into specific behavior categories in Sec.~\ref{barchart}. 




\textbf{Roles Extraction:}
We analyzed each speaker's dynamic roles (Driver, Navigator, and Monitor) during programming~\cite{lewis2011pair}. Using ChatGPT-4o and prompts based on the Thought Chain Model~\cite{wei2022chain}, we guided the model through step-by-step reasoning to generate role classifications. Prompts were iterated for clarity, and the model's responses were structured hierarchically and returned in JSON format. Each query was repeated ten times, with the majority result adopted for classification.

\RR{\textbf{Behavioral Engagement:} reflected the level of effort and participation students invested in learning~\cite{fredricks2022measurement}. 
In our study, we focused on the duration and frequency of student speech.} 
We extracted conversation data, excluding irrelevant chat, and divided each conversation into two parts: the first half and the full conversation. We then measured speaking duration, frequency, and degree centrality using co-occurrence networks~\cite{ng1999toward}. For each question, we created and normalized two networks, followed by Non-negative Matrix Factorization (NMF)~\cite{lee2000algorithms} to identify key behavioral patterns for dynamic group comparison.


\RR{\textbf{Cognitive Engagement:} referred to the cognitive investment students made in their learning. We highlighted the role changes and behavior frequencies of students during the collaborative process. }
To capture dynamic changes in student cognitive engagement, we split the dialogue for each question into two segments: the first half and the full dialogue. We extracted the frequency of each speaker's 14 behavioral categories and their roles at each timestamp. After normalizing these features for consistency, we applied NMF to reduce dimensionality and assess each speaker's cognitive engagement.

\begin{figure*}
  \includegraphics[width=\textwidth]{CPVis.pdf}
  \caption{\RR{A screenshot of Group 10 view.} \textit{CPVis} applies multimodal learning analysis to provide instructors with evidence for evaluating group and student performance. It consists of three views:
Filter View (A) Provides an overview and allows group selection. The selected groups appear in the lasso selection area (A2), and the similarity panel (A3) displays the most similar and different groups based on the search (A1a).
Content View (B) Displays group performance, with the B1 panel showing completed codes, the B3a panel illustrating the behavior sequence, and the B3b panel showing student engagement over time.
Detail View (C) Presents the group's collaborative programming video (C1) and raw conversation data (C2).}
  \Description{A screenshot of Group 10 view. \textit{CPVis} applies multimodal learning analysis to provide instructors with evidence for evaluating group and student performance. It consists of three views:
Filter View (A) Provides an overview and allows group selection. The selected groups appear in the lasso selection area (A2), and the similarity panel (A3) displays the most similar and different groups based on the search (A1a).
Content View (B) Displays group performance, with the B1 panel showing completed codes, the B3a panel illustrating the behavior sequence, and the B3b panel showing student engagement over time.
Detail View (C) Presents the group's collaborative programming video (C1) and raw conversation data (C2).}
  \label{fig:teaser}
  \end{figure*}

\subsubsection{Group Performance Assessment}
We evaluated group performance based on three dimensions: code quality, collaborative problem-solving, and teacher scaffolding. 
Through in-depth discussions with domain experts, we assessed how each dimension was valued and measured in the context of our study.




\label{code}
\textbf{Code quality}, reflecting students' mastery of course concepts, was a key metric for evaluating group performance. To assess student submissions, we used ChatGPT-4o~\cite{gpt4o} to evaluate dimensions such as problem-solving, code integrity, accuracy, and algorithmic innovation, scoring each on a 1â€“5 scale. After refining evaluation prompts, we ran the assessment ten times per submission, averaging the results to ensure consistency and reliability.





\textbf{Collaborative Problem-Solving (CPS):} 
Earlier studies categorized CPS into team effectiveness and task effectiveness~\cite{rosen2020towards}. Team effectiveness was measured by student engagement, while task effectiveness was assessed through code quality. %Our analysis captured problem-solving behaviors by frequency and sequence.
To evaluate CPS, we examined task effectiveness, represented by the average question score (\(\bar{s}\)), and team effectiveness, assessed through the standard deviation of engagement (\(\sigma_e\)) and the average engagement score (\(\bar{e}\)) as shown in Equation \ref{eq:1}. We then used the coefficient of variation (\(CV_e\)) \RR{to account for both engagement variability and engagement}. Finally, the overall collaboration quality was calculated using Equation \ref{eq:2}, combining question performance and engagement balance. 
\begin{equation}
\sigma_e = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (e_i - \bar{e})^2}, \quad CV_e = \frac{\sigma_e}{\bar{e}}
\label{eq:1}
\end{equation}

\begin{equation}
Quality = \bar{s} \cdot (1 - CV_e)
\label{eq:2}
\end{equation}
As shown in Table \ref{table:comparison}, Group 19, despite achieving a respectable average score, exhibited imbalanced engagement, leading to a lower collaboration quality score. In contrast, Group 20 demonstrated more balanced and higher engagement, resulting in a better overall collaboration quality.
\begin{table}[htbp]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Group} & \(\bar{s}\) & \textbf{Engagement Levels} & \(\sigma_e\) & \(\text{CV}_e\) & \textbf{CQ} \\
\midrule
Group 19 & \(4.11\) & (10.515, 9.725, 4.575) & \(2.80\) & \(0.24\) & \(2.80\) \\
Group 20 & \(4.14\) & (10.06, 9.32, 8.62) & \(0.73\) & \(0.08\) & \(3.88\) \\
\bottomrule
\end{tabular}
\caption{Comparison of Group 19 and Group 20 on Collaboration Quality (CQ).}
\label{table:comparison}
\end{table}

\textbf{Teacher Scaffolding,} categorized into cognitive (low, medium, high-control) and metacognitive forms~\cite{ouyang2022applying}, reflected the level of support provided to a group and its impact on programming performance. We evaluated four scaffolding dimensions, leveraging GPT-4o for annotation. By using targeted prompts and examples, we improved classification accuracy, while teacher scaffolding was categorized according to the type of support based on a semantic analysis of interactions.


