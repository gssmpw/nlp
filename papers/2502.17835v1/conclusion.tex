\section{Conclusion}
Collaborative programming has been a common teaching strategy for instructors in large-scale programming courses.
We collected multimodal collaborative programming data from real-world settings and, after consulting with domain experts, developed a performance framework for evaluating collaborative programming groups and individual students. We visualized these performance metrics by introducing a novel flower metaphor-based design and built an interactive visualization system to dynamically analyze group collaboration behavior and track students' evolving engagement over time.
\RR{Finally, we conducted a quantitative study to evaluate the accuracy of annotation data labeling by LLMs,} with two cases demonstrating how our system helped instructors evaluate group performance and provide personalized feedback to students. Additionally, we organized a within-subjects experiment (N=22) comparing \textit{CPVis} with two baseline systems. The results indicated that participants gained more insights using our system and felt significantly more confident in evaluating group collaboration.