%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
% \usepackage{xcolor}    % colors 提供颜色
\usepackage[table,xcdraw]{xcolor}
\usepackage{pifont}
\definecolor{lightyellow}{rgb}{1, 0.95, 0.85}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{booktabs} % for professional tables
\input{math_commands.tex}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage[shortlabels, inline]{enumitem}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
% \usepackage{href}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\zm}[1]{{\color{red}{[[\textbf{zm: }#1]]}}}
\newcommand{\cx}[1]{{\color{green}{[[\textbf{cx: }#1]]}}}
\newcommand{\gd}[1]{{\color{magenta}{[[\textbf{gd: }#1]]}}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers}

\begin{document}

\twocolumn[
\icmltitle{RIFLEx: A Free Lunch for Length Extrapolation in \\  Video Diffusion Transformers}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Min Zhao}{1,2}
\icmlauthor{Guande He}{3}
\icmlauthor{Yixiao Chen}{1,2}
\icmlauthor{Hongzhou Zhu}{1,2}
\icmlauthor{Chongxuan Li}{4}
\icmlauthor{Jun Zhu}{1,2,5}
\end{icmlauthorlist}

\icmlaffiliation{1}{Dept. of Comp. Sci. \& Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University.}
\icmlaffiliation{2}{ShengShu.}
\icmlaffiliation{3}{The University of Texas at Austin.}
\icmlaffiliation{4}{Gaoling School of Artificial Intelligence, Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods}
\icmlaffiliation{5}{Pazhou Laboratory (Huangpu)}

\icmlcorrespondingauthor{Chongxuan Li}{chongxuanli@ruc.edu.cn}
\icmlcorrespondingauthor{Jun Zhu}{dcszj@tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
\printAffiliationsAndNotice{}

 

\begin{abstract}
Recent advancements in video generation have enabled models to synthesize high-quality, minute-long videos. However, generating even longer videos with temporal coherence remains a major challenge and existing length extrapolation methods lead to \textit{temporal repetition} or \textit{motion deceleration}. In this work, we systematically analyze the role of frequency components in positional embeddings and identify an \textit{intrinsic frequency} that primarily governs extrapolation behavior. Based on this insight, we propose \textbf{RIFLEx}, a \textit{minimal yet effective} approach that reduces the intrinsic frequency to suppress repetition while preserving motion consistency, without requiring any additional modifications. RIFLEx offers a true \textit{free lunch}—achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely \textit{training-free} manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos. Project page and codes: \href{https://riflex-video.github.io/}{https://riflex-video.github.io/.}
\end{abstract}
 
 
 

\section{Introduction}

Recent advances in video generation~\citep{videoworldsimulators2024, bao2024vidu, yang2024cogvideox, genmo2024mochi, kong2024hunyuanvideo} enable models to synthesize minute-long video sequences with high fidelity and coherence. A key factor behind this progress is the emergence of diffusion transformers~\citep{peebles2023scalable, bao2023all}, which combines the scalability of diffusion models~\citep{sohl2015deep, ho2020denoising, song2021scorebased} with the expressive power of transformers~\citep{vaswani2017attention}. 

Despite these advancements, generating longer videos with high-quality and temporal coherence remains a fundamental challenge. Due to computational constraints and the sheer scale of training data, existing models are typically trained with a fixed maximum sequence length, limiting their ability to extend content. Consequently, there is increasing interest in length extrapolation techniques that enable models to \textit{generate new and temporally coherent content that evolves smoothly over time without training on longer videos}. 

However, existing extrapolation strategies~\citep{chen2023extending, bloc97, peng2023yarn, lu2024fit, zhuo2024lumina}, originally developed for text and image generation, fail when applied to video length extrapolation. Our experiments show that these methods exhibit distinct failure patterns: \textit{temporal repetition} and \textit{slow motion}. These limitations suggest a fundamental gap in understanding how positional encodings influence video extrapolation. 

To address this, we isolate individual frequency components by zeroing out others and fine-tuning the target video model. 
We find that high frequencies capture short-term dependencies and induce temporal repetition, while low frequencies encode long-term dependencies but lead to motion deceleration.
Surprisingly, we identify a consistent \textit{intrinsic frequency component} across different videos from the same model, which primarily dictates repetition patterns among all components during extrapolation. 

Building on this insight, we propose \textbf{R}educing \textbf{I}ntrinsic \textbf{F}requency for \textbf{L}ength \textbf{Ex}trapolation (\textbf{RIFLEx}), a \textit{minimal yet effective} solution that lowers the intrinsic frequency to ensure it remains within a single cycle after extrapolation. Without any other modification, it suppresses temporal repetition while preserving motion consistency. 
As a byproduct, RIFLEx provides a principled explanation for the failure modes of existing approaches and offers insights that naturally extend to spatial extrapolation of images.


\begin{figure*}[ht]
\centering
 \begin{minipage}[t]{\linewidth}
  \centering
  \begin{subfigure}{\linewidth}
  \centering
      \includegraphics[width=\linewidth]{images/demo_temporal.pdf}
      \caption{$2\times$ temporal extrapolation from $129$ to $261$ frames.}
  \end{subfigure}
 \end{minipage}
\begin{minipage}{0.2485\textwidth}
        \centering
        \begin{subfigure}[t]{\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{images/demo_spatial.pdf} % 插入第二张图片
            \caption{From $480 \times 720$ to $960 \times 1440$.}
        \end{subfigure}
    \end{minipage}
    \hfill
    \begin{minipage}{0.7455\textwidth}
        \centering
        \begin{subfigure}[t]{\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{images/demo_temporal_spatial.pdf} % 插入第一张图片
            \caption{$2\times$ temporal and spatial extrapolation from $480 \times 720 \times 49$ to $960 \times 1440 \times 97$.}
        \end{subfigure}
\end{minipage}
\caption{\textbf{Visualization of RIFLEx for $2\times$ temporal, spatial, and combined extrapolation.} Our base models are (a) HunyuanVideo~\cite{kong2024hunyuanvideo} and (b-c) CogVideoX-5B~\cite{yang2024cogvideox}, where we do not use any videos longer or larger than those used for pre-training. More demos and all the prompts used in this paper are listed in Appendix~\ref{sec: more demo} and the supplementary materials, respectively.}
\label{fig: demo}
\end{figure*}

Extensive experiments on state-of-the-art video diffusion transformers, including CogVideoX-5B~\cite{yang2024cogvideox} and HunyuanVideo~\cite{kong2024hunyuanvideo}, validate the effectiveness of RIFLEx (see Fig.~\ref{fig: demo}). Remarkably, for 2$\times$ extrapolation, RIFLEx enables high-quality and natural video generation in a completely \textit{training-free} manner. When fine-tuning is applied with only 20,000 original-length videos—requiring just 1/50,000 of the pre-training computation—sample quality is further improved, and the effectiveness of RIFLEx extends to 3$\times$ extrapolation. Moreover, RIFLEx can also be applied in the spatial domain simultaneously to extend both video duration and spatial resolution.

Our key contributions are summarized as follows:  
\begin{itemize}
    \item We provide a \textit{comprehensive understanding} of video length extrapolation by analyzing the failure modes of existing methods and revealing the role of individual frequency components in positional embeddings.  
    \item We propose \textbf{RIFLEx}, a \textit{minimal yet effective} solution that mitigates repetition by properly reducing the intrinsic frequency, without any additional modifications.   
    \item RIFLEx offers a true \textit{free lunch}—achieving high-quality $2\times$ extrapolation on state-of-the-art video diffusion transformers in a completely \textit{training-free} manner. Moreover, it enhances quality and enables $3\times$ extrapolation by minimal fine-tuning without long videos.
\end{itemize}




\section{Background}
\label{sec:background}

% We present the background of video diffusion transformers, position embeddings, and length extrapolation.

\subsection{Video Generation with Diffusion Transformers}

Given a data distribution $p_{\mathrm{data}}$, diffusion models~\citep{sohl2015deep, ho2020denoising, song2021scorebased} progressively perturb the clean data $\boldsymbol x_0 \sim p_{\mathrm{data}}$ with a transition kernel $q_{t|0}(\boldsymbol x_t | \boldsymbol{x}_0) = \mathcal N(\alpha_t\boldsymbol x_0, \sigma_t^2\boldsymbol I)$, i.e., $\boldsymbol x_t = \alpha_t \boldsymbol x_0 + \sigma_t \boldsymbol \epsilon$, where $t \in [0,T]$, $\alpha_t, \sigma_t$ are pre-defined noise schedule, and $\boldsymbol{\epsilon} \sim \mathcal N(\boldsymbol 0, \boldsymbol I)$ is Gaussian noise. Under proper designs of $\alpha_t, \sigma_t$, the distribution of $\boldsymbol x_T$ is tractable, e.g., a standard Gaussian. 

A generative model is obtained by reversing this process from $t=T$ to $0$, whose dynamic is characterized by the score function $\nabla_{\boldsymbol x_t} \log q_t(\boldsymbol x_t)$. The score function is usually parameterized by a neural network $\boldsymbol s_{\boldsymbol \theta}(\boldsymbol{x}_t, t)$ and learned with the denoising score matching ~\cite{vincent2011connection}: $\mathbb E_{t, \boldsymbol x_0, \boldsymbol \epsilon} [w(t)\| \boldsymbol s_{\boldsymbol \theta}(\boldsymbol{x}_t, t) - \nabla_{\boldsymbol x_t} \log q_{t|0}(\boldsymbol x_t | \boldsymbol x_0) \|^2]$, where $w(t)$ is a weighting function. 
The de facto approach for modeling video data via diffusion models is to first encode the video data into sequences of latent space, then perform diffusion modeling with transformer-based neural network~\citep{peebles2023scalable, bao2023all}.
% Latent Diffusion, DiT with bidirectional attention ...

% \gd{Do we need to qualify the scope to ``Diffusion Transformers'' here? RoPE is quite general}
\subsection{Position Embedding in Diffusion Transformers}
\label{sec:RoPE-intro}

A position embedding is a fixed or learnable vector-valued function $\boldsymbol f$ that maps a $n$-axes position vector $\boldsymbol p \in \mathbb \Nb_{+}^n$ 
to some representation space. This position information can be incorporated into transformers through various mechanisms, such as through additive~\cite{vaswani2017attention, raffel2020exploring, press2021train} or multiplicative~\cite{su2021roformer} operations with other input or hidden embeddings.

\textbf{Rotary Position Embedding (RoPE)}~\cite{su2021roformer} has emerged as the predominant method in transformers. RoPE encodes relative positional information by interacting with two absolute position embeddings within the attention mechanism.
Specifically, for a sequence indexed by a single axis (i.e. $n=1$), given an input $\xv \in \R^d$ with position $p \in \Nb_{+}$, RoPE maps it to an absolute-position encoded embedding on $\R^{d^\prime}$ with $d^\prime \leq d$, i.e.,
% \begin{equation}
% \label{eqn:RoPE-Complex}
% \boldsymbol f^{\rope}(\x, p, \thetav)_j = (x_{2j} + ix_{2j+1})e^{i\theta_jp},
% \end{equation}
\begin{equation}
\label{eqn:RoPE-Complex}
\boldsymbol f^{\rope}(\x, p, \thetav)_j = 
   \begin{bmatrix}
       \cos p \theta_j & -\sin p\theta_j \\
       \sin p\theta_j & \cos p \theta_j
   \end{bmatrix} 
   \begin{bmatrix}
       x_{2j} \\
       x_{2j+1}
   \end{bmatrix}.
\end{equation}
where $\thetav \in \R^{d^\prime/2}$ with $\theta_j = b^{-2(j-1)/d^\prime}$ for $j = 1, \ldots, d^\prime/2$.
% \cx{ for all integer $j = 1, ..., d/2$.} 
Here, $\thetav$ represents the frequencies for all dimensions of the RoPE embedding and $b$ is a hyperparameter that adjusts the base frequency. 
% By applying Euler's formula and viewing complex numbers as two-dimensional vectors, \eqref{eqn:RoPE-Complex} is equivalent to:
% \begin{equation}
%    \begin{bmatrix}
%        \cos p \theta_j & -\sin p\theta_j \\
%        \sin p\theta_j & \cos p \theta_j
%    \end{bmatrix} 
%    \begin{bmatrix}
%        x_{2j} \\
%        x_{2j+1}
%    \end{bmatrix}.
% \end{equation}
It can be verified that the dot product between two RoPE-embedded vectors encodes the relative positional information between them.
% Hence, for every two dimensions $(x_{2j}, x_{2j+1})$ of the input vector $\x$, RoPE rotates the input vector by a different angle $\theta_j$. It can be verified that the dot product between two RoPE-embedded vectors encodes the relative positional information between them because
% \begin{equation}
% \begin{aligned}
%    &
%    \begin{bmatrix}
%        \cos p_1 \theta_j & -\sin p_1\theta_j \\
%        \sin p_1\theta_j & \cos p_1 \theta_j
%    \end{bmatrix}^T
%    \begin{bmatrix}
%        \cos p_2 \theta_j & -\sin p_2\theta_j \\
%        \sin p_2\theta_j & \cos p_2 \theta_j
%    \end{bmatrix} \\
%    =&
%    \begin{bmatrix}
%    \cos (p_2 - p_1)\theta_j & -\sin (p_2 - p_1)\theta_j \\
%    \sin(p_2 - p_1)\theta_j & \cos (p_2 - p_1) \theta_j
%    \end{bmatrix}.
% \end{aligned}
% \end{equation}
In practice, RoPE is applied to the query and key vectors before the dot product operation in the attention mechanism, and thus the result attention matrix encodes the relative positional information. 
% between them.


\textbf{RoPE with Multiple Axes.} 
RoPE can be extended to multi-axes position vector $\boldsymbol p \in \mathbb N_+^n$ for $n > 1$. One popular practice is to encode each axis independently.
% and then concatenate the result embeddings together. 
For example, consider a video input $\boldsymbol x \in \mathbb R^d$ with three-dimensional coordinates $(t, h, w)$, there are three axis-specific parameters $\boldsymbol \theta^t, \boldsymbol \theta^h, \boldsymbol \theta^w$. 
Single-axis RoPE, as defined in \eqref{eqn:RoPE-Complex}, is then applied separately along the feature dimension with these three parameters. The final multi-axes RoPE is obtained by concatenating the three single-axis RoPE embeddings.
% \begin{equation}
% \label{eqn:MultiRoPE}
% \begin{aligned}
%     % &\scalebox{0.82}{\ensuremath{\boldsymbol f^{\mathrm{RoPE}}(\boldsymbol x, (t, h, w), \boldsymbol \theta) =}} \\ 
%     \scalebox{0.82}{\ensuremath{\mathrm{Concat}\left( \boldsymbol f^{\mathrm{RoPE}}(\boldsymbol x, t, \boldsymbol \theta^t), f^{\mathrm{RoPE}}(\boldsymbol x, h, \boldsymbol \theta^h), f^{\mathrm{RoPE}}(\boldsymbol x, w, \boldsymbol \theta^w) \right)}},
% \end{aligned}
% \end{equation}
% where $\boldsymbol \theta = \mathrm{Concat}(\boldsymbol \theta^t, \boldsymbol \theta^h, \boldsymbol \theta^w) \in \mathbb R^{d^\prime}.
% and each single-axis RoPE function $\boldsymbol f^{\mathrm{RoPE}}$ is defined as in \eqref{eqn:RoPE-Complex}. 
% Then, the dot product between two multiple-axes RoPE embedding in \eqref{eqn:MultiRoPE} with coordinates $(t_1, h_1, w_1)$ and $(t_2, h_2, w_2)$ will be a function of the relative position $(t_2 - t_1, h_2 - h_1, w_2 - w_1)$.
% While alternative multiple-axes RoPE implementations may also satisfy this property, this independent encoding approach offers several advantages, including being an injection and the flexibility to set different RoPE parameters across different axes. Unless otherwise specified, we default to the multiple-axes RoPE implementation in \eqref{eqn:MultiRoPE} for images ($n=2$) and videos $(n=3)$.


% image. 2D RoPE.

% video.

% \paragraph{1D RoPE.}
% \begin{align}
%     [\cos (m\theta_0), \sin (m\theta_0), &\dots, \cos(m\theta_d),\sin(m\theta_d), \dots, \\
%     \cos(m\theta_{\frac{D}{2}-1}),\sin(m\theta_{\frac{D}{2}-1}),] \\
%     \theta_d &= b^{- \frac{2}{D}d}
% \end{align}

% \gd{I prefer to merge these two subsections about LLM \& Image for simplicity}
\subsection{Length Extrapolation with RoPE}
 % A canonical practice in transformer pre-training is to set a fixed maximum sequence length. However, due to computational constraints and the vast scale of training data, this maximum length is often too restrictive for many real-world applications. 
 % Therefore, developing methods for pre-trained transformers to effectively handle longer sequences is an important challenge. 
 In this section, we briefly recap the techniques for length extrapolation with RoPE adopted in text and image.
 % language modeling and image generation.
 % across both language and image domains.
 
The most straightforward approach, \textbf{Position Extrapolation (PE)}, extends the input sequence length without modifying the positional encoding, which purely relies on the generalization ability of the network and the positional encoding.
Whereas \textbf{Position Interpolation (PI)}~\cite{chen2023extending} uniformly down-scales all frequencies in RoPE embedding to match the training sequence length. In specific, the new RoPE frequencies are calculated as
$
    {\boldsymbol\theta}^{\mathrm{PI}} = {\boldsymbol \theta} / {s},
$
where $s = {L^\prime}/{L}$, and $L$, $L^\prime$ is the sequence length for training and inference, respectively.

A key limitation of both PE and PI is their reliance on training at the target sequence length, otherwise, the performance degrades drastically. To address this,
\textbf{NTK-Aware Scaled RoPE (NTK)}~\cite{bloc97} combines the ideas of both position extrapolation and interpolation. Specifically, NTK adjusts the base frequency $b$ for all dimensions as:
\begin{equation}
    \theta_j^{\mathrm{NTK}} = (\lambda b)^{-2(j-1)/d^\prime}, \lambda = s^{d^\prime / (d^\prime - 2)}, j = 1,\ldots,d^\prime/2,
\end{equation}
where $s = {L^\prime}/{L}$. 
NTK effectively applies PE for high frequencies and PI for low frequencies, enabling training-free extrapolation.~\citep{bloc97, zhuo2024lumina}.
% Considering the NTK-adjusted highest frequency $\theta_1^{\mathrm{NTK}}$ and the lowest frequency $\theta_{d^\prime/2}^{\mathrm{NTK}}$, we have:
% \begin{equation}
% \theta_1^{\mathrm{NTK}} = \theta_1 \cdot s^{-2/(d^\prime-2)}, \  \theta_{d^\prime/2}^{\mathrm{NTK}} = \theta_{d^\prime / 2} \cdot s^{-d^\prime/(d^\prime-2)}.
% \end{equation}
% When $d^\prime \gg 2$, we have $\theta_1^{\mathrm{NTK}} \approx \theta_1$ and $\theta_{d^\prime/2}^{\mathrm{NTK}} \approx \theta_{d^\prime/2} / s$, i.e., 
% Empirically, NTK can achieve nice performance without fine-tuning on target length for language modeling~\cite{bloc97}, but it is still prone to repetitive objects for image generation with extrapolated resolution~\cite{zhuo2024lumina}. 
% Empirically, NTK can achieve a nice training-free extrapolation performance~\cite{bloc97, zhuo2024lumina}.

% However, the increased sequence length can cause a mismatch in positional encoding and attention scores between training and inference~\citep{chen2023extending}, which may result in unsatisfied extrapolation performance~\cite{zhuo2024lumina}. 
% For example, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation~\cite{zhuo2024lumina}. Hence, one still needs to fine-tune the model on the target sequence length or incorporate more sophisticated techniques to achieve good length extrapolation performance.


% \textbf{Position interpolation (PI)}~\cite{chen2023extending} uniformly down-scales all frequencies in RoPE embedding to match the training sequence length. In specific, the new RoPE frequencies are calculated as
% $
%     {\boldsymbol\theta}^{\mathrm{PI}} = {\boldsymbol \theta} / {s},
% $
% where $s = {L^\prime}/{L}$, and $L$, $L^\prime$ is the sequence length for training and inference, respectively. Compared to position extrapolation, PI can generally achieve better performance using a smaller number of fine-tuning steps over the target length but often yield poorer training-free performance~\cite{zhuo2024lumina}.
% However, this ``fine-tuning on target length'' procedure is necessary for PI, as its direct extrapolation performance
% \gd{Better to have a uniform name for our ``train on short, test on long'', or training-free setting} is still beyond satisfactory, or even worse than position extrapolation~\cite{zhuo2024lumina}.

% The caveat 
% Need fine-tuning ...

% \gd{Put limitation of PE \& PI into the motivation of NTK}

% \textbf{NTK-Aware Scaled RoPE (NTK)}~\cite{bloc97} combines the idea of both position extrapolation and interpolation in practice. Specifically, NTK adjusts the base frequency $b$ on each dimension of RoPE as:
% \begin{equation}
%     \theta_j^{\mathrm{NTK}} = (\lambda b)^{-2j/d^\prime}, \lambda = s^{d^\prime / (d^\prime - 2)}, j = 1,\ldots,d^\prime/2,
% \end{equation}
% where $s = {L^\prime}/{L}$. Considering the NTK-adjusted highest frequency $\theta_1^{\mathrm{NTK}}$ and the lowest frequency $\theta_{d^\prime/2}^{\mathrm{NTK}}$, we have:
% \begin{equation}
% \theta_1^{\mathrm{NTK}} = \theta_1 \cdot s^{-2/(d^\prime-2)}, \  \theta_{d^\prime/2}^{\mathrm{NTK}} = \theta_{d^\prime / 2} \cdot s^{-d^\prime/(d^\prime-2)}.
% \end{equation}
% When $d^\prime \gg 2$, we have $\theta_1^{\mathrm{NTK}} \approx \theta_1$ and $\theta_{d^\prime/2}^{\mathrm{NTK}} \approx \theta_{d^\prime/2} / s$, i.e., NTK essentially performs position extrapolation for high frequencies and PI for the low frequencies. 
% % Empirically, NTK can achieve nice performance without fine-tuning on target length for language modeling~\cite{bloc97}, but it is still prone to repetitive objects for image generation with extrapolated resolution~\cite{zhuo2024lumina}. 
% Empirically, NTK can achieve a nice training-free extrapolation performance~\cite{bloc97, zhuo2024lumina}.

\textbf{YaRN}~\cite{peng2023yarn} introduces a fine-grained base frequency adjustment strategy. It first categorizes all frequencies into three groups based on \textit{the number of cycles elapsed over the training length}, defined as $r_j = (2\pi)^{-1}L\theta_j$.
Given two pre-determined thresholds $\alpha, \beta$ with $r_{d^\prime/2} \leq \alpha < \beta \leq r_1$, YaRN adjusts the RoPE frequencies as:
\begin{equation}
\begin{gathered}
\textstyle
    \theta^{\mathrm{YaRN}}_j = \gamma(r_j) \theta_j + (1 - \gamma (r_j)) \frac{\theta_j}{s}, \ j = 1, \ldots, d^\prime/2,\\
    \textstyle
    \gamma(r_j) = 
    \begin{cases} 
        1, & \text{if } r_j > \beta, \\
        0, & \text{if } r_j < \alpha, \\
        \frac{r_j - \alpha}{\beta - \alpha}, & \text{otherwise},
    \end{cases}
\end{gathered}
\end{equation}
% which performs position extrapolation for high-frequencies components with $r > \beta$, position interpolation for low-frequencies components with $r < \alpha$, and linearly interpolates between position extrapolation and interpolation for components lies between the two thresholds. 
In practice, YaRN exhibits better training-free extrapolation performance compared to NTK and can achieve great performance with a relatively small fine-tuning budget on target length~\cite{peng2023yarn}.




 


% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{images/challenge_v2.pdf}
%     \caption{\textbf{Visualization of existing methods for $2\times$ length extrapolation in image and video generation.} Images are enlarged from 1K to 2K resolution using Lumina-Next~\cite{zhuo2024lumina}, and videos are extended from $49$ to $97$ frames with CogVideoX-5B~\cite{yang2024cogvideox}. \textit{Existing methods in LLMs and image diffusion transformers lead to either slower motion or repetitive sequences in video generation, failing to generate novel and smoothly evolving content.} To save space, we present three representative methods here and include the rest, such as NTK~\cite{chen2023extending,lu2024fit} and Yarn~\cite{peng2023yarn,lu2024fit}, in the Appendix\zm{to do}
%     \gd{Change the reference image}}
%     \label{fig:challenge}
%     \vspace{-0.5cm}
% \end{figure*}

\input{images/challenge}

\textbf{Length Extrapolation in Image Diffusion Transformers.} Image diffusion transformers have two key characteristics related to RoPE: (1) image data is represented as a sequence with height and width axes, and (2) an iterative diffusion sampling procedure. These characteristics inform specific length extrapolation techniques for image diffusion models.

For multi-axes sequence, RoPE is independently applied to each axis, allowing length extrapolation techniques like NTK and YaRN to be used separately on height and width, termed Vision NTK and Vision YaRN~\cite{lu2024fit}. For sampling, different RoPE adjustments can be employed at various diffusion timesteps. For instance, \textbf{Time-aware Scaled RoPE (TASR)}~\citep{zhuo2024lumina} leverages PI at large timesteps to preserve global structure while using NTK at smaller timesteps to enhance visual quality.
 

\section{Method}

Our goal is to understand and solve the video length extrapolation problem thoroughly. We first highlight the intriguing failure patterns of existing methods, analyze the role of different frequency components in positional embeddings, and identify an \textit{intrinsic frequency}. Based on this, we derive a minimal solution that enables length extrapolation: \textit{reducing the intrinsic frequency}. As byproducts, our method not only provides a principled explanation for the failure of existing approaches in video extrapolation but also offers insights applicable to spatial extrapolation in images.

\subsection{Failure Patterns of Existing Methods}
\label{sec: challenge}

Although the term ``extrapolation'' is widely used across different domains, its role in video generation is fundamentally different from text and images. In video generation, the objective is to create \textit{new and temporally coherent content that evolves smoothly over time}. In contrast, text extrapolation primarily extends the context window, while image extrapolation typically involves adding high-resolution details rather than generating meaningful new content.

\input{images/each}

As a result, extrapolation strategies developed for text and images fail in video length extrapolation, exhibiting intriguing failure patterns, as illustrated in Fig.~\ref{fig:challenge}. To better understand these patterns, we also conduct the counterparts on image spatial extrapolation, revealing parallels to video.

PE, which directly extends positional encoding beyond the training range, leads to \textit{temporal repetition}, causing videos to loop instead of progressing naturally (Fig.~\ref{fig:challenge}a). A similar phenomenon occurs in image generation, where \textit{spatial repetition} occurs instead of generating new content.

Conversely, PI~\cite{chen2023extending}, which compresses positional encodings within the training range, leads to \textit{slow motion} by stretching frames over time (Fig.~\ref{fig:challenge}b). While this approach preserves structural coherence, it lacks temporal novelty. In image generation, this results in \textit{blurred details} rather than new content (Fig.~\ref{fig:challenge}e).
  
As shown in Fig.\ref{fig:challenge}c, NTK~\cite{bloc97} also induces \textit{temporal repetition}, failing to generate meaningful motion progression. In image generation, it causes~\textit{spatial repetition} (Fig.~\ref{fig:challenge}f). While other methods~\cite{peng2023yarn,lu2024fit,zhuo2024lumina} differ from NTK in implementation, they invariably suffer from one or both of these two failure patterns: either motion deceleration or content repetition (see Appendix~\ref{sec: existing failure} for further analysis).

Beyond revealing these limitations, our findings provide an intuitive understanding of how positional embeddings fundamentally shape temporal motion, motivating our in-depth frequency component analysis in the next section.



\subsection{Frequency Component Analysis in RoPE}
\label{sec: analysis}

We begin by analyzing the role of individual frequency components in RoPE~\cite{su2021roformer}. We follow the notation in Sec.~\ref{sec:RoPE-intro} but focus on the time axis and omit the subscript $t$ for simplicity. We isolate specific frequency components by zeroing out others and fine-tune the target model~\mbox{\cite{yang2024cogvideox}} on its training length to adapt to the modified RoPE. 
Two key insights emerge from this analysis.

First, different frequency components $ \theta_j $ capture temporal dependencies at varying scales, dictated by their \textit{periods}:
\begin{equation}
\label{eq: period}
N_j = \frac{2\pi}{\theta_j}.
\end{equation}
As illustrated in Fig.~\ref{fig:each}, when the frame interval exceeds $ N_j $, the periodic nature of the cosine function forces positional encodings—and consequently, generated video content—to repeat. Given a training length $ L $, the number of temporal repetitions can be quantified as:
\begin{equation}
r_j = \frac{L}{N_j} = \frac{L\theta_j}{2\pi }.
\end{equation}
As shown in Fig.~\ref{fig:each}, when a high-frequency component has $r_j = 2$, the video completes two cycles within the training length and four cycles during 2$\times$ extrapolation. In contrast, a low-frequency component with $r_j = 0.5$ remains within a single cycle even when extrapolated.


Second, frequency components influence the perceived motion speed in video generation. This effect correlates to the \textit{rate of change} in positional encoding between consecutive (e.g., $p$-th and $(p+1)$-th) frames:
\begin{equation}
\Delta_j = \cos((p+1) \theta_j) - \cos(p \theta_j).
\end{equation}
Higher frequencies (i.e., larger $ \theta_j $) typically result in larger $\Delta_j $, making the model more sensitive to rapid movements. Conversely, lower-frequency components induce minimal positional encoding shifts between adjacent frames, favoring slow-motion dynamics, aligning with results in Fig.~\ref{fig:each}.
 

Given that each component has its own period $N_j$, a key question arises: \textbf{which frequency primarily dictates the observed repetition pattern} in length extrapolation?

We define the \textit{intrinsic frequency component} as the one whose period $N_j$ is closest to the first observed repetition frame $N$ in a video, as it determines the overall behavior:
\begin{align}
\label{eq: key}
k = \arg\min_{j} \left| N_j - N \right|.
\end{align}
Surprisingly, this intrinsic frequency remains consistent across different videos generated by the same model, despite slight variations in $N$. For instance, $k$ is $2$ for CogVideoX-5B~\cite{yang2024cogvideox} and $4$ for HunyuanVideo~\cite{kong2024hunyuanvideo} respectively, as detailed in Appendix~\ref{app: ReFLEX}.

In the rare case where a model exhibits inconsistent intrinsic frequencies across videos, we suggest treating all such frequencies as intrinsic. Our preliminary experiments further validate this assumption, showing that incorporating all lower-frequency components into our method maintains strong performance, as discussed in Appendix~\ref{app: ReFLEX}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/whether_training.pdf}
    \vspace{-0.7cm}
    \caption{\textbf{Exploring the necessity of fine-tuning.} For $2\times$ extrapolation, RIFLEx generates high-quality videos without fine-tuning. For 3$\times$ extrapolation, due to the large intrinsic frequency shift, fine-tuning is required to improve dynamic effects and visual quality.}
    \label{fig:training-free}
    \vspace{-0.3cm}
\end{figure*}

% \begin{figure*}[ht]
% \centering
%  \begin{minipage}[t]{\linewidth}
%   \centering
%   \begin{subfigure}{\linewidth}
%   \centering
%       \includegraphics[width=\linewidth]{images/free2.pdf}
%       \caption{The results of $2\times$ extrapolation with training-free. }
%   \end{subfigure}
%  \end{minipage}
%  \begin{minipage}[t]{\linewidth}
%   \centering
%   \begin{subfigure}{\linewidth}
%   \centering
%       \includegraphics[width=\linewidth]{images/free3.pdf}
%       \vspace{-1cm}
%       \caption{A comparison of $3\times$ extrapolation results between the training-free and fine-tuning.}
%   \end{subfigure}
%  \end{minipage}
% \caption{\textbf{Exploring the necessity of fine-tuning.} For $2\times$ extrapolation, RIFLEx generates high-quality videos without fine-tuning. For 3$\times$ extrapolation, due to the large intrinsic frequency shift, fine-tuning is required to improve dynamic effects and visual quality.}
%     \label{fig:training-free}
% \end{figure*}





\subsection{Reducing Intrinsic Frequency: A Minimal Solution}


Consider a video diffusion transformer trained on sequences of length $L$. We aim to generate videos of length $sL$ via extrapolation by a factor of $s$\footnote{We assume $s$ is sufficiently large such that $N_k < Ls.$ Otherwise, it is trivial to generate long videos by PE.}. Based on previous findings, we propose a natural and minimal solution: \textbf{R}educing \textbf{I}ntrinsic \textbf{F}requency for \textbf{L}ength \textbf{Ex}trapolation (\textbf{RIFLEx}).
RIFLEx lowers the intrinsic frequency so that it remains within a single period after extrapolation:
\begin{align}
\label{eq:non-repetition}
N_k' \geq Ls \Rightarrow \theta_{k}' \leq \frac{2\pi}{Ls}.
\end{align}
By setting $\theta_{k}' = \frac{2\pi}{Ls}$, we achieve a minimal modification. Ablation studies on other frequencies (Appendix~\ref{app: ReFLEX}) confirm that modifying only the intrinsic frequency is sufficient: adjusting higher-frequency components disrupt fast motion while altering lower frequencies has negligible impact. We present RIFLEx formally in Algorithm~\ref{alg: method}.

We further investigate \textbf{whether fine-tuning is necessary} for RIFLEx. Surprisingly, for 2$\times$ extrapolation, RIFLEx can generate high-quality videos in a \textit{training-free} manner, as shown in Fig.~\ref{fig:training-free}. Fine-tuning with only 20,000 original-length videos and 1/50,000 of the pre-training computation further enhances dynamic quality and visual quality.


For 3$\times$ extrapolation, the intrinsic frequency shift becomes too large, causing the training-free RIFLEx to fail. However, the fine-tuning process still succeeds, as shown in Fig.~\ref{fig:training-free}. 


\begin{algorithm}[t]
    \caption{RIFLEx}
    \label{alg: method}
    \begin{algorithmic}[1]
        \REQUIRE The extrapolation factor $s$, frequencies $\theta_j$ in the RoPE, the first observed repetition frame $N$
        \FOR{$j = 1$ to $\dfrac{d'}{2}$}
            \STATE Compute the period of each $ \theta_j $  (\eqref{eq: period})
        \ENDFOR
        \STATE Identify the intrinsic frequency component $k$ (\eqref{eq: key})
        \STATE Modify $\theta_k$  (\eqref{eq:non-repetition})
    \end{algorithmic}
\end{algorithm}

\begin{table*}[t!]
\centering
\caption{\textbf{Quantitative results in length extrapolation.}
% CogVideoX-5B~\cite{yang2024cogvideox} and HunyuanVideo~\cite{kong2024hunyuanvideo} are trained on videos with 49 and 129 frames, respectively. 
The red-marked areas in the NoRepeat Score and Dynamic Degree indicate severe issues with repetition and slow motion, making other metrics meaningless. In the user study, the ratio for no extrapolation represents the proportion of users who prefer the samples of the training length over RIFLEx. The others are the corresponding ranks among all methods.}
\vspace{.2cm}
\label{tb: our strategy}
\renewcommand\arraystretch{1}
\resizebox{0.88\textwidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\multirow{3}{*}{Method}  & \multicolumn{4}{c}{Automatic Metrics$\uparrow$} & \multicolumn{3}{c}{User Study$\downarrow$} \\
\cmidrule(r){2-5} \cmidrule(l){6-8} 
 & \makecell{NoRepeat\\Score} & \makecell{Dynamic\\Degree} & \makecell{Imaging\\Quality}  & \makecell{Overall\\Consistency} & \makecell{Motion\\Quality} & \makecell{Visual\\Quality} & \makecell{Overall\\Aspects} \\
\midrule
\multicolumn{8}{c}{CogVideoX-5B with $2\times$ extrapolation, training-free} \\
\midrule
    No extrapolation & - &   67.5 & 64.4& 25.8 & 66.4\%& 76.0\%& 70.2\%\\
    \midrule
    PE & \cellcolor{red!10}46.6  &\cellcolor{green!10}\underline{58.6} & 55.0   & \underline{{22.9}} &  \underline{2.1} &  \underline{1.6}  & 2.4\\
    NTK  & \cellcolor{red!10}43.4 & \cellcolor{green!10}{58.3}  & \underline{{55.3}} & \underline{{22.9}} &  \underline{2.1} & 1.8 &  \underline{2.1}\\
    PI & \cellcolor{green!10}\underline{59.0} & \cellcolor{red!10}5.0  & 44.3 & 19.2 & 3.7 & 4.1 & 3.8 \\
    TASR  & \cellcolor{red!10}10.8 & \cellcolor{red!10}26.9  & 50.5   & 21.5 & 3.3 & 3.8 & 3.6 \\
    YaRN  & \cellcolor{green!10}\textbf{59.4} & \cellcolor{red!10}5.6  & 44.6  & 19.3 & 3.6 & 4.2 & 3.7 \\
     RIFLEx (\textbf{ours})  & \cellcolor{green!10}54.2 & \cellcolor{green!10}\textbf{59.4}  & \textbf{56.9} & \textbf{23.5} &\textbf{1.4} & \textbf{1.5}& \textbf{1.1}\\  
\midrule

\multicolumn{8}{c}{CogVideoX-5B with $2\times$ extrapolation, fine-tuning} \\
\midrule
    No extrapolation & - &  65.6 & 62.7   & 25.8 &61.8\%& 66.0\%& 65.0\%\\
    \midrule
   PE & \cellcolor{red!10}13.2 & \cellcolor{green!10}50.6  & 56.6  & 24.2 & 1.8& 1.8& 1.7\\
    RIFLEx (\textbf{ours}) & \cellcolor{green!10}\textbf{61.3} &\cellcolor{green!10}\textbf{54.7}  & \textbf{60.4}   & \textbf{25.0} & \textbf{1.2}& \textbf{1.2}& \textbf{1.3}\\
\midrule
\multicolumn{8}{c}{HunyuanVideo with $2\times$ extrapolation, training-free} \\
\midrule
    No extrapolation & - &  63.0  & 65.9   & 19.6 & 62.8\%& 62.0\%& 61.6\%\\
    \midrule
    PE & \cellcolor{red!10}36.0 & \cellcolor{green!10}\textbf{63.0}  & 64.3 & \textbf{19.1} & 2.3& \underline{1.2}& 2.4\\
    NTK   & \cellcolor{green!10}81.0 & \cellcolor{green!10}55.0 & \textbf{65.3} & 18.9 & \textbf{1.5}& 1.4& \underline{1.6}\\
    PI &\cellcolor{green!10}\textbf{86.0} & \cellcolor{red!10}11.0  & 57.4  & 18.9 & 4.3& 2.8& 3.8\\ 
    TASR & \cellcolor{green!10}\underline{{85.0}} & \cellcolor{red!10}18.0  & 61.3   & \underline{{19.0}} & 4.2& 2.2& 3.4\\
    YaRN  &\cellcolor{green!10}\textbf{86.0} & \cellcolor{red!10}15.0  & 58.2  & 18.8 & 3.9& 2.7& 3.7\\
     RIFLEx (\textbf{ours}) & \cellcolor{green!10}72.0 & \cellcolor{green!10}\underline{{57.0}}  & \underline{{65.2}}   & \underline{{19.0}} & \underline{1.6}& \textbf{1.1}& \textbf{1.4}\\
\midrule
\multicolumn{8}{c}{HunyuanVideo with $2.3\times$ extrapolation, training-free} \\
\midrule
    NTK & \cellcolor{red!10}20.0 & \cellcolor{green!10}46.0 & \textbf{65.5}& \textbf{18.3}& 1.7& 1.6& 1.7\\
     RIFLEx (\textbf{ours}) & \cellcolor{green!10}\textbf{54.0} &\cellcolor{green!10}\textbf{51.0} & 65.0 & 18.1 & \textbf{1.3}& \textbf{1.4}& \textbf{1.3}\\
\midrule
\multicolumn{8}{c}{HunyuanVideo with $2\times$ extrapolation, fine-tuning}  \\
\midrule
    No extrapolation & - & 79.0  & 71.6   & 18.8 & 62.6\%& 51.2\%& 56.0\%\\
    \midrule
    PE &\cellcolor{red!10}40.0 &\cellcolor{green!10}74.0  & 71.6  & \textbf{18.7} & 1.9& 1.6& 1.8\\
     RIFLEx (\textbf{ours}) & \cellcolor{green!10}\textbf{89.0} & \cellcolor{green!10}\textbf{82.0}  & \textbf{72.0}  & 18.1 & \textbf{1.1} & \textbf{1.4} & \textbf{1.2}\\
\bottomrule
\vspace{-.3cm}
\end{tabular}
}
\vspace{-.3cm}
\end{table*}
 

\subsection{Principled Explanation for Existing Methods}
 
Our findings provide a principled explanation for the failure patterns observed in Section~\ref{sec: challenge}. The repetition observed in PE and NTK~\cite{bloc97,lu2024fit} stems from their intrinsic frequency component violating the non-repetition condition in \eqref{eq:non-repetition}. As a result, the generated video content loops instead of progressing naturally. PI~\cite{chen2023extending} and YaRN~\cite{peng2023yarn} cause slow motion by interpolating high-frequency components, which are crucial for fast motion. Divided by $s$ in such methods, these components cannot generate rapid movements. TASR~\cite{zhuo2024lumina} combines both approaches mentioned above, resulting in a mixture of temporal repetition and motion slowdown.
See Appendix~\ref{sec: existing failure} for more details and experiments.


\section{Experiments}

\subsection{Setup}

We describe the dataset and evaluation setup below, with implementation details in Tab.~\ref{tab:setting} (see Appendix~\ref{app: setup}).

\textbf{Datasets.}
We use a private dataset of 20,000 videos for fine-tuning. For CogVideoX-5B, We adopt the VBench~\cite{huang2024vbench} prompts to ensure consistency with prior work~\cite{yang2024cogvideox}. Due to the high computational cost of HunyuanVideo~\cite{kong2024hunyuanvideo}, we evaluate it using 100 diverse prompts across multiple categories.

% \textbf{Datasets.} We employ a private dataset of $20,000$ videos for fine-tuning. For the evaluation of CogVideoX-5B~\cite{yang2024cogvideox}, we adopt the prompts provided by VBench~\cite{huang2024vbench}, in alignment with the methodology used for CogVideoX. Given the substantial computational time required for  HunyuanVideo~\cite{kong2024hunyuanvideo}, we collect 100 diverse prompts across multiple categories for evaluation.


\textbf{Evaluation metrics.} Following prior work~\cite{huang2024vbench,yang2024cogvideox}, we assess video generation using Imaging Quality, Dynamic Degree, and Subject Consistency, measuring visual quality, motion magnitude, and temporal consistency, respectively. Additionally, we introduce the NoRepeat Score, where a higher score indicates less repetition (detailed in Appendix~\ref{app: setup}). We also conduct a user study with 10 participants, evaluating visual quality, motion quality, and overall preference. Motion quality reflects both repetition and slow motion. Users rank their preferences among all extrapolation methods, allowing for ties. We also perform pairwise comparisons between the results of normal samples and RIFLEx. See more details in Appendix~\ref{app: setup}.

% \textbf{Evaluation metrics.} We employ several metrics from VBench~\cite{huang2024vbench} to assess video generation performance, in line with previous work~\cite{yang2024cogvideox}. Specifically, we use Imaging Quality, Dynamic Degree, and Subject Consistency to evaluate the visual quality, motion magnitude, and temporal consistency respectively. Furthermore, we introduce the NoRepeat score to measure the repetition in the video, where a higher score indicates less repetition. Detailed calculation is provided in Appendix~\ref{app: setup}.  Additionally, we conduct a user study with $10$ participants, focusing on visual quality, motion quality, and overall aspects, where motion quality can evaluate both repetition and slow motion. For baselines, users rank their preferences among all methods, allowing for ties, and we also perform pairwise comparisons between the results of training length and our extrapolation. More details about the evaluation are provided in the Appendix~\ref{app: setup}.
 

\input{images/hunyuan}

\subsection{Performance Comparison}
\label{sec: comparision}

\textbf{Results.} Quantitative results are summarized in Tab.~\ref{tb: our strategy}. Our approach achieves superior overall performance, generating new temporal content without compromising other aspects of video quality. For example, in CogVideoX-5B, PI and YaRN suffer from slow motion, leading to lower \textit{Dynamic Degree}, while PE and NTK experience repetition issues, resulting in lower \textit{NoRepeat Score}. By effectively addressing both challenges, our method significantly enhances motion quality and ranks highest in user studies across all methods.
 
Notably, NTK coincidentally performs well for HunyuanVideo at $2\times$ extrapolation, but our analysis attributes this to an unintended intrinsic frequency reduction that happens to satisfy the non-repetition condition in~\eqref{eq:non-repetition}, rather than its intended mechanism. This is evident as NTK fails on CogVideo-X and HunyuanVideo with 2.3$\times$ extrapolation, reflected in its low \textit{NoRepeat Score} in Tab.~\ref{tb: our strategy}.

Qualitative results are shown in Fig.~\ref{fig:Hunyuan} for HunyuanVideo, with additional comparisons for CogVideoX-5B in Appendix~\ref{app: comparisons}. Fig.~\ref{fig:Hunyuan} aligns with the quantitative findings, demonstrating our method’s ability to effectively mitigate slow motion and repetition, thereby improving overall video quality.

Additionally, a minimal fine-tuning procedure requiring just 1/50,000 of the pre-training computation on short videos improves the \textit{Dynamic Degree}, \textit{Imaging Quality}, and \textit{NoRepeat Score}. Finally, leveraging the strong HunyuanVideo base model, our approach achieves performance close to that of the training length—with 56.0\% and 61.6\% of users preferring the training length over our method.

\textbf{Maximum extent of extrapolation.}
\label{sec: maximum}
Empirically, RIFLEx supports up to $\mathbf{3\times}$ extrapolation, beyond which quality degrades significantly (e.g., at $4\times$ extrapolation, see Fig.~\ref{fig:appendix_demo_extra4x} in Appendix). This may occur because excessive frequency reduction diminishes the effectiveness of RoPE, resulting in minimal encoding changes over the training length.

\textbf{Extension to other extrapolation types.}
\label{sec: extension}
We further explore RIFLEx for spatial extrapolation and joint temporal-spatial extrapolation. As shown in Fig.~\ref{fig: demo}b and Fig.~\ref{fig: demo}c, adjusting the intrinsic frequency for the corresponding dimensions enables resolution extrapolation and joint spatial-temporal extension. Additional demos and implementation details are provided in Appendix~\ref{sec: more demo} and Appendix~\ref{app: setup}.

% By adjusting $\theta$

% Fig.~\ref{fig:image} illustrates our successful application of spatial resolution extrapolation in image generation. Additionally, as shown in Fig.~\ref{fig: demo}c, by adjusting the dominant frequencies for time, height, and width, we achieve simultaneous spatial and temporal length extrapolation.



% \begin{figure*}[ht]
% \centering
%  \begin{minipage}[t]{\linewidth}
%   \centering
%   \begin{subfigure}{\linewidth}
%   \centering
%       \includegraphics[width=\linewidth]{images/compare/base.pdf}
%       \caption{The results with training free.}
%       \label{fig: 2 training-free}
%   \end{subfigure}
%  \end{minipage}
%  \begin{minipage}[t]{\linewidth}
%   \centering
%   \begin{subfigure}{\linewidth}
%   \centering
%       \includegraphics[width=\linewidth]{images/compare/training.pdf}
%       \caption{The results with finetuning.}
%   \end{subfigure}
%  \end{minipage}
% \caption{\textbf{Visualization results of $2\times$ length extrapolation based on HunyuanVideo.} We achieve better video quality by effectively addressing issues of slow motion and repetition. Notably, NTK in HunyuanVideo coincidentally avoids repetition in $2\times$ extrapoaltion but still faces severe repetition in longer extrapolations (see Appendix \zm{xx}).}
% \label{fig:Hunyuan}
% \end{figure*}



% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]
%     {images/image2.pdf}
%     \caption{\textbf{Visualization results of spatial resolution extrapolation method in image generation.} Our method outperforms the extrapolation by generating new content with better visual quality.}
%     \label{fig:image}
%     \vspace{-0.5cm}
% \end{figure*}






\section{Conclusion and Discussion}

We provide a comprehensive understanding of video length extrapolation by analyzing the role of frequency components in RoPE. Building on these insights, we propose RIFLEx, a minimal yet effective solution that prevents repetition by reducing intrinsic frequency.  RIFLEx achieves high-quality $2\times$ extrapolation on SOTA video diffusion transformers in a training-free manner and enables $3\times$ extrapolation by minimal fine-tuning without long videos.

Although we demonstrate the effectiveness of RIFLEx on existing pre-trained models, we have not yet explored its performance when trained from scratch, left for future work.

\section*{Impact Statements}

This paper presents work aimed at advancing the field of video generation. It is crucial to use this technology responsibly to prevent negative social impacts, such as the creation of misleading fake videos.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Related Work}

% \paragraph{Diffusion Transformers for Visual Generation.}

\textbf{Length Extrapolation  with RoPE.}
Position encoding, exemplified by the widely used RoPE, plays a crucial role in enabling length extrapolation in transformers. Prior research in both language and image domains has primarily focused on training-free methods and fine-tuning under target sequence length settings. For instance, position interpolation generally outperforms direct position extrapolation in fine-tuning efficiency, requiring fewer steps to adapt to the target length~\cite{chen2023extending}, though it performs poorly in training-free settings~\cite{zhuo2024lumina}. Advanced strategies such as NTK~\cite{bloc97} and YaRN~\cite{peng2023yarn} have demonstrated decent training-free performance while being more efficient in fine-tuning scenarios.
Further refinements, such as optimizing RoPE frequencies~\citep{ding2024longrope} or modifying RoPE's extrapolation behavior~\cite{hu2024longcontext}, have shown additional improvements in language modeling. 
Our work provides new insights into the impact of RoPE in video diffusion transformers, introducing a length extrapolation strategy tailored for video generation. Unlike previous approaches, our proposed RIFLEx requires training only on the original pre-trained sequence length while also demonstrating strong potential in training-free settings.

\textbf{Text-to-Video Diffusion Models.} 
Drawing upon the progress made in image generation, a burgeoning body of research has emerged, focusing on the utilization of diffusion models for video generation~\cite{kong2024hunyuanvideo,yang2024cogvideox,ho2022imagen,polyak2024movie,videoworldsimulators2024,zhou2024allegro,genmo2024mochi,zheng2024opensora,blattmann2023stable,lin2024open,xing2023dynamicrafter,chen2023videocrafter1,chen2024videocrafter2,he2022lvdm}. By combining spatial and temporal attention, VDM~\cite{he2022lvdm} introduces a space-time factorized UNet for video synthesis, marking an early contribution to the field. Later, Make-A-Video extends the 2D-UNet with temporal modules~\cite{singer2022make}, exploring the integration of prior knowledge from text-to-image diffusion models into video diffusion techniques. More recently, a surge of video diffusion models leveraging the expressive power of transformers has emerged~\cite{lin2024open,zheng2024opensora,kong2024hunyuanvideo,yang2024cogvideox,bao2024vidu,videoworldsimulators2024,genmo2024mochi}. These diffusion transformer-based models have demonstrated remarkable performance. Our approach builds on these advancements by applying them to pre-trained video diffusion transformers, further enhancing their capabilities.

% The position encoding, exemplified by the ubiquitous RoPE, plays a crucial role in length extrapolation for transformers. Previous work in the language and image domain mainly focuses on training-free and fine-tuning in target sequence length settings. For instance, position interpolation can generally achieve better performance using a smaller number of fine-tuning steps over the target length compared to direct position extrapolation~\cite{chen2023extending} but yield poor training-free performance~\cite{zhuo2024lumina}. Advanced adjustment strategies such as NTK~\cite{bloc97} and YaRN~\cite{peng2023yarn} can achieve nice performance without fine-tuning on target length for language modeling as well as being efficient in fine-tuning steps over target sequence length. More advanced strategies such as determining RoPE frequencies with optimization~\citep{ding2024longrope} or tweaking the extrapolation behavior of RoPE~\cite{hu2024longcontext} could further improve the performance in language modeling. Our work provides new findings and insights into RoPE with video diffusion transformers and proposes a strategy from this novel view. Different from previous works, our method works without any training over the target sequence length.

% 
% For position extrapolation, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation~\cite{zhuo2024lumina}, which 



% For instance, mismatch in positional encoding and attention scores between training and inference~\citep{chen2023extending}, which may result in unsatisfied extrapolation performance~\cite{zhuo2024lumina}. 
% For example, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation~\cite{zhuo2024lumina}. Hence, one still needs to fine-tune the model on the target sequence length or incorporate more sophisticated techniques to achieve good length extrapolation performance. 


\textbf{Long Video with Diffusion Models.}
Recent studies have explored long video generation with diffusion models from various angles~\cite{lu2024freelong,wang2023genlvideo,wang2024lingen,wang2024loong,lin2023videodirectorgpt,li2024arlon,qiu2023freenoise,nvidia2025cosmos}. For instance, \citet{kim2024fifodiffusion, chen2025ouroboros} propose diffusion sampling schemes that employ a queue of video frames with varying noise levels, progressively decoding new frames. \citet{yan2024long} introduce a cross-attention module to enhance the semantic fidelity and richness of long videos. \citet{yin2024slow} distill a chunk-wise, few-step auto-regressive video diffusion transformer from a bidirectional teacher model, enabling efficient long video generation.
In this work, we address long video generation with diffusion transformers through the lens of position encoding—a fundamental component for capturing sequential structure in video data. We propose a minimal yet general and effective strategy that requires no training on long video data.
% denoising scheme~\citep{kim2024fifodiffusion, chen2025ouroboros}


% Ours: diffusion transformer, RoPE

\section{Additional Results of RIFLEx}
\label{sec: more demo}

In this section, we present additional demos for temporal extrapolation in Fig.~\ref{fig:appendix_demo_temporal}, spatial extrapolation in Fig.~\ref{fig:appendix_demo_spatial}, and both extrapolations in Fig.~\ref{fig:appendix_spatial_temporal}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]
    {images/appendix/appendix_demo_temporal.pdf}
    \caption{\textbf{More results of $2\times$ temporal extrapolation from $129$ to $261$ frames.}}
    \label{fig:appendix_demo_temporal}
    % \vspace{-0.5cm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]
    {images/appendix/appendix_demo_spatial.pdf}
    \caption{\textbf{Visualization results of spatial resolution extrapolation method in image generation.} Our method outperforms the extrapolation by generating new content with better visual quality.}
    \label{fig:appendix_demo_spatial}
    % \vspace{-0.5cm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]
    {images/appendix/appendix_spatial_temporal.pdf}
    \caption{\textbf{More results of $2\times$ temporal and spatial extrapolation}, extending video dimensions from $480 \times 720 \times 49 $ to $960 \times 1440 \times 97$.}
    \label{fig:appendix_spatial_temporal}
    % \vspace{-0.5cm}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/appendix/appendix_extra4x.pdf}
    \caption{\textbf{Results of $4\times$ temporal extrapolation from $49$ to $193$ frames.}}
    \label{fig:appendix_demo_extra4x}
    % \vspace{-0.5cm}
\end{figure*}

\begin{table}[ht]
\caption{\label{tab:code_used_and_license} \textbf{Code Links and Licenses.}}
\vspace{.2cm}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Link}  &  \textbf{License} \\
\midrule
HunyuanVideo~\cite{kong2024hunyuanvideo}&\url{https://github.com/Tencent/HunyuanVideo} & Tencent Hunyuan Community License\\
FastVideo~\cite{fastvideo}&\url{https://github.com/hao-ai-lab/FastVideo} & Apache License\\
CogVideoX~\cite{yang2024cogvideox} & \url{https://github.com/THUDM/CogVideo} & Apache License \\
Lumina-T2X~\cite{zhuo2024lumina} & \url{https://github.com/Alpha-VLLM/Lumina-T2X} & MIT License \\
\bottomrule
\end{tabular}
}
\end{table}

\section{More Results of Failure Patterns of Existing Methods}
\label{sec: existing failure}

As shown in Fig.~\ref{fig: other existing}, we present the results of other existing methods for $2\times$ extrapolation in video and image generation. Specifically, YaRN results in slower motion, using parameters $\alpha=1$ and $\beta=32$ as set in previous studies~\cite{lu2024fit,peng2023yarn}. TASR utilizes PI at larger timesteps and employing NTK at smaller timesteps. Consequently, it combines the characteristics of both PI and NTK, which leads to slower motion and temporal repetition in video generation.

\input{images/appendix/appendix_challenge}



\begin{table}[ht]
\caption{\label{tab:setting} \textbf{Fine-tuning settings for all experiments}. Both. denotes spatial and temporal extrapolation simultaneously. $b_k^{t'}$, $b_k^{h'}$, and $b_k^{w'}$ represent the base frequency for the intrinsic frequency in the time, height, and width dimensions, respectively. By adjusting these variables, we can modify the corresponding $\theta_k^{t'}$, $\theta_k^{h'}$, and $\theta_k^{w'}$ values accordingly (refer to Section~\ref{sec:RoPE-intro} for details).}
\vspace{.2cm}
% \vskip 0.15in
% \small
\centering
\renewcommand\arraystretch{1}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{cccccc}
\toprule
\textbf{Config}  & \textbf{$2\times$ Temporal} & \textbf{$2\times$ Temporal} & \textbf{$3\times$ Temporal} & \textbf{$2\times$ Spatial} & \textbf{$2\times$ Both.} \\
\midrule
Base model &  CogVideoX-5B &  HunyuanVideo & CogVideoX-5B & CogVideoX-5B & CogVideox-5B\\
Training iterations & $2500$ & $1000$ & $5000$ & $2000$ & $10000$\\
$b_k^{t'}$ & $1e5$ & $560$ & $1e6$ & - & $1e5$ \\
$b_k^{h'}$ & - & - & - & $1e6$ & $1e6$ \\
$b_k^{w'}$ & - & - & - & $5e4$ & $5e4$\\
Data size &  $480\times720\times49$ & $544\times960\times129$& $480\times720\times49$ & $480\times720\times1$ & 480$\times$720$\times$49\\
Batch size & $8$ & $8$ & $8$ & $64$ & $8$\\
GPU & $8$ A100-80G  & $24$ A100-80G& $8$ A100-80G & $8$ A100-80G & $8$ A100-80G\\
\bottomrule
\end{tabular}
}
\end{table}

\section{Experimental Setup.}
\label{app: setup}

\textbf{Used code and license.} All used codes in this paper and its license are listed in Tab.~\ref{tab:code_used_and_license}.

\textbf{Implementation details.} For spatial extrapolation, following Algorithm \ref{alg: method}, we identify the intrinsic frequency components whose periods closely match the repeating patterns observed in the height and width pixels, then adjust them to ensure unique encoding. For both spatial and temporal extrapolation, we simultaneously adjust the intrinsic frequency components for the time, width, and height dimensions. The training-free setting shares the same intrinsic frequency values as those in Tab.~\ref{tab:setting}.

\textbf{Evaluation metrics.} For the NoRepeat Score, we identify the frame around $N_k$ with the minimum $L_2$ distance to the first frame, marking it as the start of the possible repeated sequence. We then calculate the $L_2$ distance between each frame in the possible repeated sequence and the corresponding frame at the beginning of the video. If the average distance across frames exceeds a threshold, the video has a higher probability of being non-repetitive. We then calculate the proportion of videos with a higher probability of being non-repetitive. Empirically, we find that a threshold of $100$ aligns better with human perception, so we set it to $100$. For the human evaluation of the training-free setting, considering that several methods may share similar quality (e.g., slow motion with poor visual quality), we allow for ties. However, for the fine-tuning setting, ties are not permitted.





\section{Details about RIFLEx}
\label{app: ReFLEX}

\textbf{Robustness of the intrinsic frequency $k$.} Empirically, we collected 20 videos and found that, although the first observed repetition frame may vary across videos within a certain range, the identified intrinsic frequencies remain consistent. For example, in HunyuanVideo, even though the first observed repetition frame range from $178$ to $200$, the closest intrinsic frequency is always $k = 4$, where $N_k = 200$.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]
    {images/appendix/adujsting_b.pdf}
    \caption{\textbf{The results of adjusting all frequency components lower than the intrinsic frequency.} See detailed analysis in Appendix~\ref{app: addustingb}.}
    \label{fig:addustingb}
    % \vspace{-0.5cm}
\end{figure*}

\label{app: addustingb}
\textbf{Adjust all frequency components lower than the intrinsic frequency.} In our preliminary experiments, we slow down all frequency components lower than the intrinsic frequency by increasing the base frequency $b$ for $j \geq k$, where $b$ is chosen to satisfy the non-repetition condition \eqref{eq:non-repetition} for intrinsic frequency $k$. As shown in Fig.~\ref{fig:addustingb}, this approach effectively addresses the repetition issue while maintaining visual quality. It is important to note that, despite this choice, our RIFLEx, which reduces the intrinsic frequency, provides the minimal solution.

\renewcommand\cellset{\renewcommand\arraystretch{0.7}}
\begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c}
    \toprule
    \multirow{2}{*}{\makecell[t]{\small \textbf{Reference}}} 
    &
    \begin{minipage}{0.75\textwidth}
    \centering
\includegraphics[width=0.95\textwidth]{images/appendix/each_PE.pdf}
    %\vspace{.1cm}
    \end{minipage}\\ \midrule
    % Extrapolation 
    \multirow{2}{*}{\makecell[t]{\small\textbf{High frequency}}} &
    \begin{minipage}{0.75\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{images/appendix/each_high.pdf}
    %\vspace{.1cm}
    \end{minipage} \\
     &  \small{(a) Reducing the higher-frequency components slows down the video.} \\ 
    % Interpolation 
    \multirow{2}{*}{\makecell[t]{\small\textbf{Low frequency}}} &
    \begin{minipage}{0.75\textwidth}
    %\vspace{.1cm}
    \centering
    \includegraphics[width=0.95\textwidth]{images/appendix/each_low.pdf}
    %\vspace{.1cm}
    \end{minipage}\\
     &  \small{(b) Reducing the lower frequencies has a negligible impact.} \\ \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablations for reducing other frequencies.} Reference refers to the results of PE, where no frequencies are reduced, serving as the baseline.
    }
    \label{fig: other frequency}
    %%\vspace{-0.5cm}
\end{figure*}


\textbf{Ablations for other frequencies.} As shown in Fig.~\ref{fig: other frequency}, reducing the higher-frequency components slows down the video. Based on the analysis in Section~\ref{sec: analysis}, this may be because these components are crucial for capturing fast motion. Reducing their frequencies leads to a slower rate of change in the positional encoding, which weakens the model’s ability to generate rapid movements.

On the other hand, reducing the lower frequencies has a negligible impact. This is likely because, for these frequencies, the encoding functions change very little across the training length, from $ p=1$  to $p=L$. Therefore, these frequencies may be less sensitive to positional encoding, and altering them results in minimal effect.

% \begin{table}[t!]
% \caption{\label{tab: prompt} \textbf{The used prompts in this paper.}}
% % \vskip 0.1in
% \vspace{.2cm}
% \small
% \centering
% \begin{tabular}{lc}
% \toprule
% \textbf{Location} & \textbf{Prompt}   \\
% \midrule
% The first demo in Fig.~\ref{fig: demo}a & -\\
% The second demo in Fig.~\ref{fig: demo}a & - \\
% The demo in Fig.~\ref{fig: demo}b & -\\
% The demo in Fig.~\ref{fig: demo}c & - \\
% The demo in Fig.~\ref{fig:challenge}a,b,c & - \\
% The demo in Fig.~\ref{fig:challenge}d,e,f & - \\
% The demo in Fig.~\ref{fig:challenge}d,e,f & - \\
% The demo in Fig.~\ref{fig:each} and Fig.~\ref{fig: other existing} & - \\
% The demo in Fig.~\ref{fig:training-free} & - \\
% The demo in Fig.~\ref{fig:Hunyuan}a & - \\
% The demo in Fig.~\ref{fig:Hunyuan}b,c & - \\
% The first demo in Fig.~\ref{fig:appendix_demo_temporal} & -\\
% The second demo in Fig.~\ref{fig:appendix_demo_temporal} & -\\
% The third demo in Fig.~\ref{fig:appendix_demo_temporal} & -\\
% The fourth demo in Fig.~\ref{fig:appendix_demo_temporal} & -\\
% The fifth demo in Fig.~\ref{fig:appendix_demo_temporal} & -\\
% The first demo in Fig.~\ref{fig:appendix_demo_spatial}a & -\\
% The second demo in Fig.~\ref{fig:appendix_demo_spatial}a & -\\
% The demo in Fig.~\ref{fig:appendix_demo_spatial}b & -\\
% The demo in Fig.~\ref{fig:appendix_demo_spatial}c & -\\
% The first demo in Fig.~\ref{fig:appendix_spatial_temporal} & -\\
% The second demo in Fig.~\ref{fig:appendix_spatial_temporal} & -\\
% The demo in Fig.~\ref{fig:appendix_demo_extra4x} & -\\
% The first demo in Fig.~\ref{fig:addustingb} & -\\
% The second demo in Fig.~\ref{fig:addustingb} & -\\
% The third demo in Fig.~\ref{fig:addustingb} & -\\
% The demo in Fig.~\ref{fig: other frequency} & -\\
% \bottomrule
% \end{tabular}
% \end{table}

\section{More Results about Comparisons}
\label{app: comparisons}
In this section, we show the visualization comparisons of CogVideoX-5B. As shown in Fig.~\ref{fig:cogvideo}, PI and YaRN suffer from slow motion, while PE and NTK experience repetition issues. TASR suffers from both slow motion and repetition. By effectively addressing both challenges, our method significantly enhances motion quality.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/appendix/cogvideo_comarison.pdf}
    \caption{\textbf{Visualization results of length extrapolation based on CogVideoX-5B.} We achieve better video quality by effectively
addressing issues of slow motion and repetition.}
    \label{fig:cogvideo}
    % \vspace{-0.5cm}
\end{figure*}
   
   

% \textbf{Visualization results of $2\times$ length extrapolation based on CogVideoX-5B.} We achieve better video quality by effectively addressing issues of slow motion and repetition.


% Task:  Long context extrapolation in LLM  |  Spatial extrapolation for image generation | length extrapolation for video generation
% Goal: Long input context | | 
% Mathematical operation: both |interpation | \textbf{??} 

% existing techniques cannot create new temporal content. repeat and slower.


% \begin{algorithm}
%     \caption{Reducing intrinsic frequency for length extrapolation}
%     \label{alg: method}
%     \begin{algorithmic}[1]
%         \REQUIRE the extrapolation length multiplier $s$, frequencies $\theta_j$ in the RoPE of a pre-trained video diffusion transformer, the first observed repetition frame $N$ in the generated videos
%         \FOR{$j = 1$ to $\dfrac{d'}{2}$}
%             \STATE compute the period of each frequency component: $N_j = \frac{2\pi}{\theta_j}$
%         \ENDFOR
%         \STATE identify the dominant frequency $k$ whose period is closest to $N$: $k = \arg\min_{j} \left| N_j - N \right|$
%         \STATE modify the frequency of the dominant component $\theta_k$ to satisfy the non-repetition condition from \eqref{eq: N_k non-repeat}:
%         $\theta_{k}'>\frac{2\pi}{Ls}$ 
%     \end{algorithmic}
% \end{algorithm}



%  \begin{table*}[t!]
 
%     \centering
%     \caption{\textbf{Comparison of length extrapolation tasks across text, image, and video domains and summary of the intriguing behavior of existing methods}. Although existing methods can address the need for extrapolation in text and image domains, they cannot naturally and smoothly extend actions when it comes to video length extrapolation. Notably, VisionNTK~\cite{lu2024fit} and VisionYarn~\cite{lu2024fit} correspond to NTK~\cite{bloc97} and YaRN~\cite{peng2023yarn} respectively, when the aspect ratio of images is one.
%     \gd{Training-free setting?} \gd{I feel that (1). the content related to text is unnecessary (we can just use some conclusions with reference); (2). Can we merge Table.1 and Fig.1 to enhance the information density? Now it has some inconsistency between Table.1 and Fig.1 (3). In case we do not have enough space for all methods, we could select representative methods in Fig.1 and leave the rest to the appendix. }}
%     \vspace{.2cm} 
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccc}
%       \toprule
%    Domain & Text  & Image & Video\\
%    \midrule
%       Task & Extend input context & Enlarge output resolution & Extend output length
%        \\ 
%        \midrule
%        Position extrapolation & - & Repeat in space (Fig.~\ref{fig:challenge}(b)) & Repeat in time  (Fig.~\ref{fig:challenge}(a))\\
%       \midrule
%       Position interpolation~\cite{chen2023extending} & - & Super-resolution (Fig.~\ref{fig:challenge}(d)) & Slower motion (Fig.~\ref{fig:challenge}(c)) \\
%  \midrule
%        Vision NTK~\cite{bloc97,lu2024fit} & Extend input context & Repeat in space (Fig.~\ref{fig:challenge}(f)) & Repeat in time (Fig.~\ref{fig:challenge}(e)) \\
%        \midrule
%       Vision YaRN~\cite{peng2023yarn,lu2024fit} & Extend input context & Super-resolution (Fig.~\ref{fig:challenge}(h)) & Slower motion (Fig.~\ref{fig:challenge}(g)) \\
       
%       \midrule
%       Time-aware Scaled RoPE~\cite{zhuo2024lumina} & - & Super-resolution (Fig.~\ref{fig:challenge}(j)) & Slower motion (Fig.~\ref{fig:challenge}(i)) \\
%       \bottomrule
%     \end{tabular} 
%     \label{tb: task}
%     }
% \end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
\subsection{Extension to Other Extrapolation Types}
\label{sec: extension}

Given that our RIFLEx capitalizes on the inherent periodicity of RoPE, it is natural to explore its extension to spatial extrapolation. Following Algorithm \ref{alg: method}, we identify the dominant frequency components whose periods closely align with the repeating patterns observed in pixel height and width, and adjust them to ensure unique encoding. Fig.~\ref{fig:image} and Fig.~\ref{fig:demo}(b) illustrate our successful application of spatial resolution extrapolation in both image and video diffusion transformers, where we fine-tune CogVideoX-5B on images for \zm{xx} steps and on videos for \zm{xx} steps, respectively. Additionally, as shown in Fig.~\ref{fig:demo}(c), by adjusting the dominant frequencies for time, height, and width, we achieve simultaneous spatial and temporal length extrapolation by fine-tuning CogVideoX-5B on videos for \zm{xx} steps.




\begin{table*}[ht]
\centering
\caption{\textbf{Quantitative results on the Vbench dataset with $2\times$ length extrapolation.} \cx{not all metrics are equal}}
\vspace{.2cm}
\label{tb: our strategy}
\renewcommand\arraystretch{1}
\resizebox{0.98\textwidth}{!}{
\begin{tabular}{lccccc|cc} 
\toprule
Method  & \makecell{$L_2$\\Distance} & \makecell{Subject\\Consistency} & \makecell{Aesthetic\\Quality} & \makecell{Imaging\\Quality} & \makecell{Overall\\Consistency} & \makecell{Motion\\Smoothness} & \makecell{Dynamic\\Degree}\\
\midrule
\multicolumn{8}{c}{CogVideox Training-free} \\
\midrule
    No extrapolation & - & 94.6  & 60.0  & 64.4  & 25.8  & 97.4  & 67.5  \\
    Extrapolation & 103.0  & 94.4  & 53.9  & 55.0  & 22.9  & 98.2  & 58.6  \\
PI    & 119.8  & 95.6  & 48.1  & 44.3  & 19.2  & 98.6  & 5.0  \\
    NTK  & 96.0  & 94.7  & 54.5  & 55.3  & 22.9  & 98.1  & 58.3  \\
    Yarn  & 120.0  & 95.7  & 48.3  & 44.6  & 19.3  & 98.6  & 5.6  \\
    Time-Aware.  & 60.9  & 94.8  & 51.5  & 50.5  & 21.5  & 98.3  & 26.9  \\
    Ours & 110.5  & 95.8  & 55.7  & 56.9  & 23.5  & 97.9  & 59.4  \\  
\midrule

\multicolumn{8}{c}{CogVideox Finetune 10k steps} \\
\midrule
    No extrapolation & - & 93.4  & 59.3  & 63.0  & 26.1  & 97.7  & 68.9  \\
    Extrapolation(baseline) & 90.4  & 94.4  & 56.3  & 56.8  & 23.7  & 98.4  & 50.0  \\
    Extrapolation(ours) & 181.9  & 94.4  & 58.5  & 61.3  & 25.2  & 98.1  & 61.1  \\
\midrule
\multicolumn{8}{c}{Hunyuan Training-free \gd{Add more specifics}} \\
\midrule
    No extrapolation & - & 93.0  & 58.5  & 65.9  & 19.6  & 99.2  & 63.0  \\
    Extrapolation & 93.0  & 95.5  & 57.6  & 64.3  & 19.1  & 99.4  & 63.0  \\
    Interpolation & 209.1  & 93.4  & 54.9  & 57.4  & 18.9  & 99.6  & 11.0  \\
    NTK   & 72.1  & 97.1  & 58.5  & 65.5  & 18.3  & 99.4  & 46.0  \\
    Timeaware & 207.8  & 93.3  & 56.7  & 61.3  & 19.0  & 99.5  & 18.0  \\
    Yarn  & 209.4  & 93.6  & 55.3  & 58.2  & 18.8  & 99.6  & 15.0  \\
    Ours & 148.5  & 95.0  & 57.7  & 65.2  & 19.0  & 99.3  & 57.0  \\
\midrule
\multicolumn{8}{c}{Hunyuan Finetune 1k steps\gd{Add more specifics}} \\
\midrule
    No extrapolation & - & 91.7  & 61.0  & 71.6  & 18.8  & 99.3  & 79.0  \\
    Extrapolation(baseline) & 94.5  & 94.9  & 58.7  & 71.6  & 18.7  & 99.4  & 74.0  \\
    Extrapolation(ours) & 170.8  & 94.2  & 58.6  & 72.0  & 18.1  & 99.4  & 82.0  \\
\bottomrule
\end{tabular}
}
\end{table*}