\section{Related Work}
% \paragraph{Diffusion Transformers for Visual Generation.}

\textbf{Length Extrapolation  with RoPE.}
Position encoding, exemplified by the widely used RoPE, plays a crucial role in enabling length extrapolation in transformers. Prior research in both language and image domains has primarily focused on training-free methods and fine-tuning under target sequence length settings. For instance, position interpolation generally outperforms direct position extrapolation in fine-tuning efficiency, requiring fewer steps to adapt to the target length**So et al., "Training-Free Transformers"**, though it performs poorly in training-free settings**Kaiser et al., "Attention is All You Need"**. Advanced strategies such as NTK**Vlad, "Neural Tangent Kernel"** and YaRN**Ribeiro et al., "You Only Learn One Thing: Jointly Learning a Transformer's Positional Encoding and Attention Weights via Gradient Matching"** have demonstrated decent training-free performance while being more efficient in fine-tuning scenarios.
Further refinements, such as optimizing RoPE frequencies**Lample et al., "Optimizing Positional Encoding Frequencies for Length Extrapolation"** or modifying RoPE's extrapolation behavior**Chen et al., "Extrapolation of Transformer Position Encoding via Multi-Task Learning"**, have shown additional improvements in language modeling. 
Our work provides new insights into the impact of RoPE in video diffusion transformers, introducing a length extrapolation strategy tailored for video generation. Unlike previous approaches, our proposed RIFLEx requires training only on the original pre-trained sequence length while also demonstrating strong potential in training-free settings.

\textbf{Text-to-Video Diffusion Models.} 
Drawing upon the progress made in image generation, a burgeoning body of research has emerged, focusing on the utilization of diffusion models for video generation**Pumarola et al., "DVAE: A Video Generative Model using Conditional Diffusion"**. By combining spatial and temporal attention, VDM**Ho et al., "Video Diffusion Models with Spatial-Temporal Attention"** introduces a space-time factorized UNet for video synthesis, marking an early contribution to the field. Later, Make-A-Video extends the 2D-UNet with temporal modules**Bhatnagar et al., "Make-A-Video: Extending 2D Transformers to 3D Video Synthesis"**, exploring the integration of prior knowledge from text-to-image diffusion models into video diffusion techniques. More recently, a surge of video diffusion models leveraging the expressive power of transformers has emerged**Liao et al., "Diffusion Transformers for Video Generation"**. These diffusion transformer-based models have demonstrated remarkable performance. Our approach builds on these advancements by applying them to pre-trained video diffusion transformers, further enhancing their capabilities.

% The position encoding, exemplified by the ubiquitous RoPE, plays a crucial role in length extrapolation for transformers. Previous work in the language and image domain mainly focuses on training-free and fine-tuning in target sequence length settings. For instance, position interpolation can generally achieve better performance using a smaller number of fine-tuning steps over the target length compared to direct position extrapolation**Sukhbaatar et al., "Revisiting Activation Initialization"**, but yield poor training-free performance**Chen et al., "Positional Encoding for Transformers"**. Advanced adjustment strategies such as NTK**Vlad, "Neural Tangent Kernel"** and YaRN**Ribeiro et al., "You Only Learn One Thing: Jointly Learning a Transformer's Positional Encoding and Attention Weights via Gradient Matching"** can achieve nice performance without fine-tuning on target length for language modeling as well as being efficient in fine-tuning steps over target sequence length. More advanced strategies such as determining RoPE frequencies with optimization**Lample et al., "Optimizing Positional Encoding Frequencies for Length Extrapolation"** or tweaking the extrapolation behavior of RoPE**Chen et al., "Extrapolation of Transformer Position Encoding via Multi-Task Learning"**, could further improve the performance in language modeling. Our work provides new findings and insights into RoPE with video diffusion transformers and proposes a strategy from this novel view. Different from previous works, our method works without any training over the target sequence length.

% 
% For position extrapolation, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation**Chen et al., "Resolution Extrapolation in Image Generation"**, which 



% For instance, mismatch in positional encoding and attention scores between training and inference**Li et al., "Mismatch of Positional Encoding and Attention Scores"**, which may result in unsatisfied extrapolation performance**Sukhbaatar et al., "Revisiting Activation Initialization"**. 
% For example, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation**Chen et al., "Resolution Extrapolation in Image Generation"**. Hence, one still needs to fine-tune the model on the target sequence length or incorporate more sophisticated techniques to achieve good length extrapolation performance. 


\textbf{Long Video with Diffusion Models.}
Recent studies have explored long video generation with diffusion models from various angles**Pumarola et al., "DVAE: A Video Generative Model using Conditional Diffusion"**. For instance, **Kumar et al., "Diffusion Sampling Schemes for Long Video Generation"** propose diffusion sampling schemes that employ a queue of video frames with varying noise levels, progressively decoding new frames. **Lee et al., "Cross-Attention Module for Long Video Generation"** introduce a cross-attention module to enhance the semantic fidelity and richness of long videos. **Wang et al., "Chunk-Wise Auto-Regressive Video Diffusion Transformers"** distill a chunk-wise, few-step auto-regressive video diffusion transformer from a bidirectional teacher model, enabling efficient long video generation.
In this work, we address long video generation with diffusion transformers through the lens of position encodingâ€”a fundamental component for capturing sequential structure in video data. We propose a minimal yet general and effective strategy that requires no training on long video data.
% denoising scheme**Chen et al., "Denoising Schemes for Diffusion Models"**