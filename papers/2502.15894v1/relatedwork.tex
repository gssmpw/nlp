\section{Related Work}
% \paragraph{Diffusion Transformers for Visual Generation.}

\textbf{Length Extrapolation  with RoPE.}
Position encoding, exemplified by the widely used RoPE, plays a crucial role in enabling length extrapolation in transformers. Prior research in both language and image domains has primarily focused on training-free methods and fine-tuning under target sequence length settings. For instance, position interpolation generally outperforms direct position extrapolation in fine-tuning efficiency, requiring fewer steps to adapt to the target length~\cite{chen2023extending}, though it performs poorly in training-free settings~\cite{zhuo2024lumina}. Advanced strategies such as NTK~\cite{bloc97} and YaRN~\cite{peng2023yarn} have demonstrated decent training-free performance while being more efficient in fine-tuning scenarios.
Further refinements, such as optimizing RoPE frequencies~\citep{ding2024longrope} or modifying RoPE's extrapolation behavior~\cite{hu2024longcontext}, have shown additional improvements in language modeling. 
Our work provides new insights into the impact of RoPE in video diffusion transformers, introducing a length extrapolation strategy tailored for video generation. Unlike previous approaches, our proposed RIFLEx requires training only on the original pre-trained sequence length while also demonstrating strong potential in training-free settings.

\textbf{Text-to-Video Diffusion Models.} 
Drawing upon the progress made in image generation, a burgeoning body of research has emerged, focusing on the utilization of diffusion models for video generation~\cite{kong2024hunyuanvideo,yang2024cogvideox,ho2022imagen,polyak2024movie,videoworldsimulators2024,zhou2024allegro,genmo2024mochi,zheng2024opensora,blattmann2023stable,lin2024open,xing2023dynamicrafter,chen2023videocrafter1,chen2024videocrafter2,he2022lvdm}. By combining spatial and temporal attention, VDM~\cite{he2022lvdm} introduces a space-time factorized UNet for video synthesis, marking an early contribution to the field. Later, Make-A-Video extends the 2D-UNet with temporal modules~\cite{singer2022make}, exploring the integration of prior knowledge from text-to-image diffusion models into video diffusion techniques. More recently, a surge of video diffusion models leveraging the expressive power of transformers has emerged~\cite{lin2024open,zheng2024opensora,kong2024hunyuanvideo,yang2024cogvideox,bao2024vidu,videoworldsimulators2024,genmo2024mochi}. These diffusion transformer-based models have demonstrated remarkable performance. Our approach builds on these advancements by applying them to pre-trained video diffusion transformers, further enhancing their capabilities.

% The position encoding, exemplified by the ubiquitous RoPE, plays a crucial role in length extrapolation for transformers. Previous work in the language and image domain mainly focuses on training-free and fine-tuning in target sequence length settings. For instance, position interpolation can generally achieve better performance using a smaller number of fine-tuning steps over the target length compared to direct position extrapolation~\cite{chen2023extending} but yield poor training-free performance~\cite{zhuo2024lumina}. Advanced adjustment strategies such as NTK~\cite{bloc97} and YaRN~\cite{peng2023yarn} can achieve nice performance without fine-tuning on target length for language modeling as well as being efficient in fine-tuning steps over target sequence length. More advanced strategies such as determining RoPE frequencies with optimization~\citep{ding2024longrope} or tweaking the extrapolation behavior of RoPE~\cite{hu2024longcontext} could further improve the performance in language modeling. Our work provides new findings and insights into RoPE with video diffusion transformers and proposes a strategy from this novel view. Different from previous works, our method works without any training over the target sequence length.

% 
% For position extrapolation, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation~\cite{zhuo2024lumina}, which 



% For instance, mismatch in positional encoding and attention scores between training and inference~\citep{chen2023extending}, which may result in unsatisfied extrapolation performance~\cite{zhuo2024lumina}. 
% For example, the perplexity in language modeling can rise drastically when the context window exceeds the training length, while there will be repetitive objects when extrapolating the resolution for image generation~\cite{zhuo2024lumina}. Hence, one still needs to fine-tune the model on the target sequence length or incorporate more sophisticated techniques to achieve good length extrapolation performance. 


\textbf{Long Video with Diffusion Models.}
Recent studies have explored long video generation with diffusion models from various angles~\cite{lu2024freelong,wang2023genlvideo,wang2024lingen,wang2024loong,lin2023videodirectorgpt,li2024arlon,qiu2023freenoise,nvidia2025cosmos}. For instance, \citet{kim2024fifodiffusion, chen2025ouroboros} propose diffusion sampling schemes that employ a queue of video frames with varying noise levels, progressively decoding new frames. \citet{yan2024long} introduce a cross-attention module to enhance the semantic fidelity and richness of long videos. \citet{yin2024slow} distill a chunk-wise, few-step auto-regressive video diffusion transformer from a bidirectional teacher model, enabling efficient long video generation.
In this work, we address long video generation with diffusion transformers through the lens of position encodingâ€”a fundamental component for capturing sequential structure in video data. We propose a minimal yet general and effective strategy that requires no training on long video data.
% denoising scheme~\citep{kim2024fifodiffusion, chen2025ouroboros}


% Ours: diffusion transformer, RoPE