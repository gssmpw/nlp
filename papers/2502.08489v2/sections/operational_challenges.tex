The models were trained using Nvidia's NeMo Framework \cite{nemo}, which leverages PyTorch Lightning for efficient model training in distributed settings. The effective pre-training time was 36 and 49 days for the 2B and 7B models, respectively. In the case of the 40B model, whose training is still ongoing, we estimate the total time to be around 215 days.

Nevertheless, the real training time has been slightly increased by a series of hardware failures that usually characterize this type of endeavours \cite{opt,bloom,llama3}. On rare occasions, previous work has even released complete logbooks in which multiple issues faced on a daily basis are rigorously reported \footnote{\url{https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf}, \\ \url{https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md}}.

It is also worth noting that, in our case, there is the added factor of using a brand new cluster that had not yet been put into production at the time of starting training. This increases the likelihood of having faulty GPUs, as well as the need to perform maintenance tasks and performance tests from time to time. And later on, when the cluster had already been opened to other users, every new job had to be queued until there were enough compute nodes available. 

The most common interruptions were always related to hardware issues (e.g. NCCL Watchdog Timeouts), since software bugs had already been tackled during the preliminary testing phase. Every time the training run was interrupted by an error, a new job had to be manually sent to the queue. However, job dependencies can be easily set in Slurm clusters, which are very useful to increase the effective training time and reduce the amount of human intervention. 