% The training data collected for the development of the Salamandra models aims to cover 35 European languages and a wide variety of domains. 
The training data collected for the development of the Salamandra models prioritizes the official Spanish languages, including Spanish, Catalan, Basque, Galician and Occitan, while also covering 30 additional European languages and a wide variety of domains. 
During the processing stage, we distinguish between curated and web data, which are processed by different pipelines but still undergo the same steps, including language identification, deduplication and heuristic filtering. 
The distinction between curated and web data is mainly due to the differences in scale and nature, requiring the use of different deduplication and filtering techniques. 
In this section, we outline the process of data selection, conversion, normalization, deduplication, quality filtering, and language sampling, mainly for the data used during pre-training.

\subsubsection{Data selection}
\label{subsubsec:data-selection}

%\todo{Why and how we select the sources?}

The training data is the pivotal point which will influence the performance in downstream applications and real-world use cases that will be built on top of the Salamandra models. When compiling the training data, the selection of data sources plays a crucial role in determining the distribution of words that will be learned by the model. Therefore, in order to select the sources that we want to include in the training corpus, we define the following set of requirements:

\begin{itemize}
     %\item \textbf{Linguistic relevance}: Datasets should include languages relevant to the research goals and provide a sufficient amount of data to support effective training for a given language. The content of these datasets should also be aligned with the topics relevant to the intended applications of the model.
     \item \textbf{Linguistic relevance}: Datasets should be relevant to the Spanish and European languages, and provide a sufficient amount of data to support effective training for any given language in the set. The content of these datasets should also be aligned with the topics relevant to the intended applications of the model.
     \item \textbf{Quality and integrity}: Datasets should be error-free, updated, and relevant to the desired time period, and consist of human-produced content to avoid redundancy and increase the diversity of the training data.
     \item \textbf{Availability of resources}: We consider the need for having sufficient hardware resources, such as disk space and computing power, to handle the preprocessing of datasets. Latest versions of the datasets are used to keep up-to-date knowledge and ensure the relevance of the training data.
\end{itemize}

%Based on these requirements, we compile a list of monolingual and multilingual sources from both heterogeneous sources such as Common Crawl and more specific repositories covering different domains and languages. The data sources are described in detail in the appendix \ref{app:datasheet}.

Based on these requirements, we compile a list of monolingual and multilingual sources from both heterogeneous sources such as Common Crawl and more specific repositories covering 35 languages and different domains, which ensures that the models are able to generalize across linguistic structures and domains.

Optimising the number of languages in multilingual model training to improve cross-lingual transfer is still an open area of research. Studies show that scaling the number of languages leads to better cross-lingual performance up to a point, after which increasing model and vocabulary capacity can help, but overall performance on monolingual and cross-lingual benchmarks tends to deteriorate, especially for high-resource languages, in what \citet{conneau_unsupervised_2020} calls the "curse of multilinguality". \citet{wang_negative_2020} reports "negative interference", i.e. performance degradation, for both high-resource and low-resource languages. More recently, \citet{chang_when_2023} reports that moderate amounts of multilingual data improve performance for low-resource languages, while it consistently degrades performance for high-resource languages.

%\footnote{\citet{conneau_unsupervised_2020} reports significant performance degradation on XNLI when scaling XLM-R 270M above 7 languages, and on XLM-R 550M when scaling above 15 languages. \citet{scao_what_2022} shows that a 1.3B parameter-sized model pre-trained on 13 languages also significantly underperforms an English model trained from the same data source in terms of zero-shot generalization. \citet{lin_few-shot_2022} conducts a similar experiment where XGLM 7.5B, shared among 30 languages, underperforms on few-shot tasks for high-resource languages against English-centric models.}

%While recent research has focused on minimising the negative interference of languages by changing the architecture or training technique with specialised models \cite{zhou_moe-lpr_2024, blevins_breaking_2024}, training a single "dense" embedding space remains the standard technique for multilingual model training \cite{avramidis_occiglot_2024, martins_eurollm_2024, ali_teuken-7b-base_2024}, and usually involves grouping together related languages, based on the hypothesis that languages with similar syntactic structure\footnote{By syntactic structure, we refer mainly to the word order between elements such as subject-object-verb, subject-verb and object-verb order, which is the most studied phenomenon in cross-lingual transfer learning studies.} are able to improve performance in low resource languages by using high resource languages \cite{chai_crosslingual_2022, philippy_towards_2023}\todo{I understand this, but it took me a couple of rereads. Maybe reword or add more detail for clarity}.

Recent research has focused on minimising the negative interference of languages by changing the architecture or training technique using specialised models \cite{zhou_moe-lpr_2024, blevins_breaking_2024}. However, training a single 'dense' embedding space remains the standard technique for multilingual model training \cite{avramidis_occiglot_2024, martins_eurollm_2024, ali_teuken-7b-base_2024}, where related languages are usually grouped together in the training data, based on the hypothesis that languages with similar syntactic structures\footnote{By syntactic structure, we refer mainly to the word order between elements such as subject-object-verb, subject-verb and object-verb order, which is the most studied phenomenon in cross-lingual transfer learning studies.} can be used to improve performance in low-resource languages \cite{chai_crosslingual_2022, philippy_towards_2023}. In particular, European languages provide a useful range of typological diversity for the Spanish languages, and are well represented in widely available datasets in the NLP community \cite{zhou_moe-lpr_2024, joshi_state_2021, blevins_breaking_2024}.

The inclusion of varied domains, such as legal, medical, technical, and conversational data, is crucial for training models that can perform effectively across tasks and applications \cite{hashimoto_model_2021, miranda_beyond_2023, xie_doremi_2023, fan_doge_2023}. 
We also include the Starcoder training data \cite{li_starcoder_2023}%\todo{This citation needs to go between parethesis I think? -> Yes, it's \cite, not \citet}
, since it has been proven that a portion of code in the training data is able to improve performance in downstream tasks \cite{muennighoff_scaling_2023, liang_holistic_2023, ma_at_2023}. 
The data sources are described in detail in Appendix \ref{app:datasheet}.

% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figures/data/license_dist_barplot.pdf}
%     \caption{License distribution of the sources used in the Salamandra pre-training dataset. The graph highlights the counts of different license types across the datasets, categorized as "Permissive" or "Restrictive".}
%     \label{fig:license_dist}
% \end{figure}

%When collecting data from different sources, languages and domains, data provenance becomes a concern, especially within the NLP field, where inconsistencies in dataset documentation and licensing are very common \cite{datasheets, fries_dataset_2022}. 
%The licence distribution graph in Figure \ref{fig:license_dist} provides insight into the different licences that apply to the datasets used in the training corpus of the Salamandra models. A significant proportion of the datasets fall under permissive licences, such as CC BY and CC0, which allow broad reuse with minimal restrictions. Restrictive licences, including CC BY-SA and CC BY-NC, make up a smaller but significant proportion, reflecting restrictions on reuse.
%Notably, many datasets are categorised as ``unspecified'', indicating a lack of explicit licensing information about their sources. We inferred potential licensing terms and ensured the use of such data only within the processing pipelines described below in Section \ref{subsubsec:data-processing-pipelines}.

%\footnote{Datasets labeled as "Unspecified" are categorized under the "Restrictive" group because the absence of explicit licensing information creates legal ambiguity and limits their use beyond the processing pipeline.}
%{Without a clear license, there is no explicit permission to use, modify, or distribute the data, which defaults to "all rights reserved" under most copyright laws.}

%In such cases, additional research has been done to infer potential licensing terms.
%, although for some datasets the exact licences still remain unclear.

\subsubsection{Processing Pipelines}
\label{subsubsec:data-processing-pipelines}

%\todo{Distinction between web and "curated" data, requirements and characteristics for each type}

%\todo{How the requirements for each type of data fit within each pipeline and why is it necessary to appply different approaches}

One of the most prominent sources of data for the development of LLMs is web data, where Common Crawl (CC) stands as the largest repository of internet data, which is updated periodically with copies from internet webpages that are distributed as monthly data dumps. 
CC is often considered to be a representative snapshot of the web due to the size of the dumps, but in fact it is often incomplete in terms of the amount of content and URLs it collects, and in terms of the diversity of languages and domains, since the page ranking method it uses to select the crawled web pages prioritises content that is linked from other sites, usually sites hosted in the United States, and in most cases it favours the default version of multilingual sites, which is usually in English \cite{back_critical_2024}.
While CC is the largest resource in the multilingual environment, curated data can fill the gap for the limitations of web data, as it is data that comes from thematically related repositories that are selected by third parties based on their content value, providing a wider range of content that may not be readily available on web-crawled data.

Common Crawl data has been found to contain non-linguistic content (code, poorly encoded documents), unnatural language (short text, boilerplate content), and undesired data for LLM pretraining like adult or offensive content. Even in filtered subsets of CommonCrawl, like C4 \cite{raffel_exploring_2023}, The Pile-CC \cite{gao_pile_2020} or OSCAR \cite{abadji_cleaner_2020}, documents are classified by language identifiers and due to mislabeling, these problems are exacerbated for low-resource languages \cite{caswell_quality_2021}. 

% In order to deal with the heterogeneous and noisy nature of web data, the \textbf{Ungoliant} pipeline \cite{abadji_ungoliant_2021} was used to produce the Colossal OSCAR 1.0 corpus for the OSCAR project, and uses the following modules:

In order to deal with the heterogeneous and noisy nature of web data, the \textbf{Ungoliant} pipeline \cite{abadji_ungoliant_2021} was used to produce the Colossal OSCAR corpus for the OSCAR project, from which we include 20 CommonCrawl snapshots\footnote{Filtered data from the OSCAR project has been included for the 35 languages listed in Table \ref{tab:langs-corpus} for the following CommonCrawl snapshots: 2015-14, 2016-40, 2017-43, 2018-47, 2019-22, 2020-24, 2020-45, 2021-49, 2022-05, 2022-21, 2022-27, 2022-33, 2022-40, 2022-49, 2023-06, 2023-14, 2023-23, 2023-40, 2023-50, 2024-10.}, originally in WET format, containing the extracted plain text from the web pages, converted to UTF-8, and headers containing the metadata of each crawled document. Ungoliant uses the following modules:

\begin{itemize}
    \item Normalization: Ensures consistency in text encoding, removing noise, normalizing text formatting, and encoding all content into UTF-8.
    \item Language detection: Sentence-based language identification is performed using embedded pretrained FastText models \cite{joulin_bagtrick_2016, joulin_fasttext_2016}.
    \item Prefiltering: Documents are filtered out based on heuristic criteria, such as removing documents with a low number of characters or low language detection scores.
    \item Computation of quality warnings: Ungoliant generates quality warnings for each document which are then used for subsequent filtering stages.
    \item Computation of harmful-perplexity: Harmful content is identified using perplexity scores based on a pretrained KenLM model \cite{jansen_perplexed_2020}. This model evaluates documents to determine whether they contain harmful content.
\end{itemize}

On the other hand, for curated data, which are the rest of the sources which are not Colossal OSCAR 1.0, we use the \textbf{CURATE} pipeline \cite{palomar-giner_curated_2024}, which works as follows:

\begin{itemize}
    \item Normalization: CURATE normalizes multiple sources into TSV files that are equally treated by the pipeline modules. Similar to Ungoliant, text data is uniformly formatted and encoded. All text data is encoded in UTF-8 to maintain a standard character encoding format, excessive whitespace is trimmed, and inconsistent spacing is corrected.
    \item Language detection: CURATE uses FastText's language identification models to detect the primary language of each sentence in the documents. The character percentage of each language is calculated, and the document's main language is determined if it exceeds a threshold. For the Salamandra training corpus, the main language of a given document has a character percentage above 0.5.
    \item Deduplication: CURATE employs a three-step exact deduplication process involving hash computation and parallel processing for scalability. %\todo{Cite DistributedDocumentDedup from Adri}.
    \item Scoring: CURATE combines multiple quality scoring heuristics to assign a continuous score between 0 and 1, making it intuitive for data sampling. This provides good control over quality versus quantity, which is crucial for mid- and low-resourced languages. For the Salamandra training corpus, only documents with scores above 0.8 are retained.
    %\item Classification modules: CURATE includes modules for anonymization and adult content filtering, among others. These classifiers leverage fine-tuned Transformer models for topic labeling and are triggered if the document's vocabulary matches with word lists corresponding to each classifier module. Since CURATE was only used for selected data sources for which we know its origin, we did not use classification modules for the Salamandra training corpus 
\end{itemize}

\begin{figure}[htbp!]
    \includegraphics[width=\textwidth]{figures/data/source_dist_pointplot.pdf}
    \caption{Distribution of sources in the Salamandra pre-training dataset. Each data point represents a source, with colours indicating the type and circle size indicating the relative number of words. The logarithmic scale is used to capture variability in dataset size, which spans several orders of magnitude, so that smaller significant sources remain visible alongside larger datasets. Sources with less than 1\% of the words are listed in the lower right text box for completeness.}
    \label{fig:source_dist}
\end{figure}

The resulting source distribution from the aforementioned efforts is illustrated in Figure \ref{fig:source_dist}, where each dot is a single dataset, categorised as either data from curated sources, which includes a variety of third-party domain-specific sources, mostly under the 1B word limit; or from internally generated sources, which reflects our dedicated efforts towards domain-specific data in Spanish languages; or from web crawled data from Common Crawl, which is the dominant class in terms of size, although it only spans 4 sources.

\subsubsection{Language Distribution}
\label{subsubsec:language-distribution}

% Explain our efforts on Spanish (LEGAL/SCIENTIFIC/BIO) and Catalan, and emphasize dialectal variety. Mention inclusion and relevance of low-res and minoritized langs like Basque and Galician, Occitan.

%Efforts in Spanish and Catalan have focused on collecting data from sociolectually and dialectally diverse backgrounds. Spanish was enriched by compiling three different domain-specific corpora, each of which was applied exact document deduplication, language identification in Spanish, and heuristic filtering with a score above 0.2, following the CURATE pipeline described in section \ref{subsubsec:data-processing-pipelines}, which are described below:
%\begin{itemize}
    %item Biomedical corpus: Collection of documents regarding life sciences, articles, and clinical notes, including the CoWeSe (Corpus Web Salud Español) dataset \cite{carrino_largest_2022}, clinical case reports, electronic health record (EHR) documents (discharge reports, clinical course notes and X-ray reports), SciELO\footnote{https://scielo.isciii.es/scielo.php} publications until 2017, sections from biomedical abbreviation recognition datasets (BARR2) \cite{intxaurrondo_biomedical_2017}, Wikipedia articles related to life sciences, Google Patents\footnote{https://patents.google.com/} in Medical Domain for Spain (Spanish), European Medicines Agency (EMEA)\footnote{https://www.ema.europa.eu/en/homepage} documents, Spanish biomedical scientific literature from Medline\footnote{https://medlineplus.gov/spanish/}, and open-access articles from PubMed\footnote{https://pubmed.ncbi.nlm.nih.gov/}.
    %\item Legal corpus: Collection of state-related documents crawled from repositories from public institutions until 2023, including bulletins, decrees and orders from the \textit{Boletín Oficial del Estado} (BOE)\footnote{https://www.boe.es/buscar/legislacion.php}, the \textit{Boletín Oficial del Registro Mercantil} (BORME)\footnote{https://www.boe.es/diario\_borme/}, the \textit{Boletín Oficial de las Cortes Generales} (BOCG)\footnote{https://www.senado.es/web/actividadparlamentaria/publicacionesoficiales/senado/boletinesoficiales/index.html}, the \textit{Diari Oficial de la Generalitat de Catalunya} (DOGC)\footnote{https://dev.socrata.com/foundry/analisi.transparenciacatalunya.cat/n6hn-rmy7}; plenary sittings from the \textit{Diarios de Sesiones del Senado de España} (DSSE); and court orders from the \textit{Consejo General del Poder Judicial}\footnote{https://www.poderjudicial.es/search/indexAN.jsp}.
    %\item Scientific: Collection of articles from scholar repositories until 2023, including Dialnet\footnote{https://dialnet.unirioja.es/}, Docta Complutense\footnote{https://docta.ucm.es/home}, SciELO\footnote{https://scielo.isciii.es/scielo.php}, and Tesis Doctorals en Xarxa (TDX)\footnote{https://www.tesisenred.net/}.
%\end{itemize}

Efforts in Spanish and Catalan have focused on collecting data from sociolectually and dialectally diverse backgrounds. Spanish was enriched by compiling three different domain-specific corpora, each of which was applied exact document deduplication, language identification in Spanish, and heuristic filtering with a score above 0.2, following the CURATE pipeline described in section \ref{subsubsec:data-processing-pipelines}, which include corpora from the biomedical, scientific, and legal domain in Spanish. For Catalan, as described in \citet{palomar-giner_curated_2024}, data has been drawn from dialects such as Central, Valencian and Balearic in order to capture the full range of linguistic expression.

To complement these efforts, significant resources have been devoted to minority languages, including Basque, Galician and Occitan. These languages often have a significant lack of digital textual data, requiring collaboration with local organisations and open access repositories for their inclusion in the training data.

The pre-training corpus shows a large variation in token volume across languages. In order to deal with this, factor sampling was used to balance the representation, mainly for English and code, which were considered dominant and were therefore undersampled by half. On the other hand, oversampling was necessary to ensure that the languages of interest, including Spanish, Catalan, Galician and Basque, had sufficient token presence. This approach prevents the model from being biased towards a single language and maintains multilingual coverage. The adjusted token distribution is detailed in Table \ref{tab:langs-corpus} and illustrated in Figure \ref{fig:lang_distribution}.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{figures/data/lang_dist_treemap.pdf}
    \caption{Distribution of tokens in the pre-training and continued training phase corpus after applying epoch sampling. The languages are grouped under families, represented with the ISO 639-1 codes.}
    \label{fig:lang_distribution}
\end{figure}

\input{tables/languages_corpus}

%\subsubsection{Data Analysis}

%\todo{Statistics of number of bytes/docs/words/tokens after and before each step, including deduplication and filtering.}

%\subsubsection{Data Quality Assessment}

%\todo{What is high quality? Filtering approach in CURATE and human assessment of CATalog}