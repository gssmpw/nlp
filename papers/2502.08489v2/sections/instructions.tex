As a first step for alignment with human intent, base models are instruction-tuned \cite{instructgpt, scaling-it-models} on a multilingual mixture of multi-turn prompt-response pairs.

While the fine-tuned versions may not demonstrate overall enhanced capabilities, they are considerably better at following system and user prompts, less prone to generate harmful content, and more resistant to jailbreaking. An effort was made to maximize the diversity of our instruction data so that the resulting models would be as robust as possible.

\subsubsection{Data Mixture}
\label{subsec:it-data}
Despite starting from a highly multilingual base model, we focus our instruction-tuning efforts on Catalan, Spanish, and English. However, we also include instruction data for other closely related Iberian languages, as we observed a positive impact on the languages of interest. That said, we do not guarantee performance in these additional languages due to the limited amount of available data and the lack of resources for thorough testing.

Additionally, some experimental models were fine-tuned with all openly available data for all languages included in the pre-training phase, but this approach was discontinued as it led to a degradation in results for the target languages, as well as our limited capacity to properly curate the data and subsequently carry out an exhaustive evaluation for all languages.

The final mixture consists of a selection based on the generation method of each dataset, followed by a qualitative evaluation of a sample of instructions carried out by native speakers. Additionally, we intentionally limit the number of instructions generated by applying instruction-style templates to non-instruction datasets, restricting them to specific domains such as translation. This approach promotes instruction diversity and naturalness, ensuring that the model can generalize to a broader range of prompting styles.

As shown in Table \ref{tab:instruction_data}, the resulting collection primarily consists of humanly generated datasets, namely: Aya-Dataset\cite{aya-dataset}, Coqcat \cite{gonzalez-aguirre_building_2024} Dolly\cite{dolly}, Dolly3k-Ca\footnote{\url{https://huggingface.co/datasets/projecte-aina/dolly3k_ca}}, MentorCA\footnote{\url{https://huggingface.co/datasets/projecte-aina/MentorCA}}, MentorES\footnote{\url{https://huggingface.co/datasets/projecte-aina/MentorES}}, NoRobots\footnote{\url{https://huggingface.co/datasets/HuggingFaceH4/no_robots}}, OASST\cite{oasst}, OASST-CA\footnote{\url{https://huggingface.co/datasets/BSC-LT/oasst-ca}}, TowerBlocks\cite{alves_tower_2024} and Flores-200\cite{nllb_language_2022}. It also includes a selection of three synthetic datasets: RAG-Multilingual\footnote{\url{https://huggingface.co/datasets/projecte-aina/RAG_Multilingual}}, generated in-house, and a sample of Open-Orca \cite{open-orca} and Alpaca-Cleaned\footnote{\url{https://huggingface.co/datasets/yahma/alpaca-cleaned}}. The latter were selected on an experimental basis by fine-tuning multiple versions of the base model and replacing a proportion of the human-generated data with different subsets of open-access synthetic data.

As for the language distribution in the resulting dataset (see the last row in \mbox{Table \ref{tab:instruction_data}}), it is worth mentioning that, despite thorough efforts to collect data for our languages of interest, the largest share still goes to English by a significant margin. As future work, including more non-English data could substantially improve the multilingual capabilities of the models; but would certainly require exploring methods for generating such data, either through manual annotation or synthetic approaches.
\input{tables/instruction_data}

\subsubsection{Training}
\label{subsec:it_training}

To instruction-tune our models, we follow the traditional supervised-finetuning approach with a causal language modeling objective. However, the next-word prediction loss is modified so that it does not consider the system prompt and user inputs. In other words, only the model response contributes to the backpropagated gradients.

\input{tables/sft_params}

For training, we use the FastChat\footnote{\url{https://github.com/lm-sys/FastChat}} codebase with slight modifications for proper functioning in our facilities. Furthermore, minor changes were necessary to accommodate the \textit{Transformers}' chat template logic\footnote{\url{https://huggingface.co/docs/transformers/main/en/chat_templating}}, and we randomly added generic system prompts to those instructions that lacked it, as discussed in Section \ref{subsec:steerability}.

We use Adam optimizer with $\beta_1$=0.9, $\beta_2$=0.999 and $\epsilon$=1e-8. The learning rate starts at 1e-5 and decays to 0 following a cosine annealing schedule. The context length is kept the same as in pre-training and the batch size is set to 256. Additionally, we use NEFTune \cite{neftune} with a noise scale of 5 for increased robustness. See Table \ref{tab:sft_params} for a complete list of hyper-parameters.

All models are trained for 2 epochs on the dataset described in Section \ref{subsec:it-data}. The 2B and 7B variants were fine-tuned in 4 and 8 nodes, respectively, equipped with 4 64GB H100 GPUs each. During training, we use the DeepSpeed library \cite{deepspeed} along with the ZeRO stage 3 optimizer \cite{zero1, zero2, zero3}. This setup enables us to complete the fully supervised fine-tuning in 12 hours for Salamandra Instructed 2B and 16 hours for Salamandra Instructed 7B.

\subsubsection{Formatting}
\label{subsec:chatml}

Unlike base models that expect plain text, instruction-tuned versions work better with a structured format. By using special tokens as delimiters, the model can distinguish between different types of messages, typically system prompts, user inputs, and assistant responses. Moreover, besides identifying roles, this special format ensures a clear separation of turns, allowing the model to engage in a conversation.

\input{tables/chatml_dialogue}

We follow the widely adopted ChatML format \cite{chatml}, and use it to convert all data instances to either single-turn or multi-turn conversations. Table \ref{tab:chatml} showcases an example of model-user interaction. As it can be seen, the tokenizer has two special control tokens (i.e. \texttt{<|im\_start|>} and \texttt{<|im\_end|>}) that facilitate turn delimitation, and the role names indicate the source of each message.

\subsubsection{Steerability}
\label{subsec:steerability}
As we present Salamandra as a family of generic foundational models, we also prioritized improving their steerability, enabling the model’s outputs to align with developer and user requirements via system prompts. Furthermore, being a multilingual model, we also focused on extending this ability to all languages included in the instruction tuning phase. Given the limited amount of openly available data that features system roles, we used two distinct approaches to obtain the desired behaviour.

First, to obtain relevant system prompts from our data mixture, we leveraged the identity information already present in collected datasets to our advantage. For instance, multiple assistant responses in the OpenAssistant \cite{oasst} dataset provide the name of the desired resulting model, among other information about its development process. Instead of filtering such instructions, we appended the conversations with a specially crafted system prompt relevant to each case. This simple approach ended up being very effective. The resulting model not only interprets and follows the instructions provided inside a system prompt, but also avoids leaking other identity information present in the instruction-tuning dataset.

Secondly, we randomly applied multiple generic system prompts in some of the instructions that did not feature it, providing examples where the information provided in the system prompt is not strictly relevant to respond to the user’s petition. This was done to prevent errors identified during our qualitative evaluation, where the model’s output was too conditioned on the system prompt.

Finally, to expand this knowledge to other languages, all system prompts used in these two methods were humanly translated to the target languages in the instruction-tuning phase, resulting in an instructed model that is able to follow system prompts in multiple languages, regardless of the language used in the subsequent conversation. This also prevents the model from randomly steering back to the system prompt language in the middle of the interaction.