%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Vocabulary Size}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our tokenizer is trained using SentencePiece's implementation \cite{sentencepiece} of the Byte-Pair \mbox{Encoding (BPE)} algorithm \cite{bpe}. In contrast to WordPiece \cite{wordpiece} or Unigram \cite{unigram} \mbox{tokenizers}, BPE-based subword tokenizers learn a vocabulary of predefined size by iteratively finding the most frequent sequence pairs in the training data. Merge operations are performed until the desired vocabulary size is reached, which is why this is arguably one of the most impactful decisions when training a new tokenizer.

The main caveat of having a large vocabulary size is that it increases the embedding layer dimension, and consequently, the overall model size. As a result, memory and computation requirements are greater, particularly for smaller models that have a higher percentage of embedding parameters. On the other hand, a larger vocabulary allows for a more efficient encoding of textual data. This significantly reduces inference costs because more information can be processed within the model’s context window. On top of that, a large vocabulary is believed to be beneficial to enhance multilingual capabilities and reduces the risk of over-segmenting text from low-resource languages \cite{bloom}.

% marc v1: The motivation behind using a large vocabulary size is that it allows for a more efficient encoding of information, which can significantly reduce inference costs. However, this comes with the tradeoff of increasing the embedding layer dimension, and consequently, the overall model size. So, increasing the vocabulary size can be beneficial as it reduces the tokens-per-word ratio but, on the other hand, it requires more memory and compute. Striking the right balance between vocabulary size, model size, and computational efficiency is crucial for optimal performance.

% seve: On one hand, a larger vocabulary implies a larger embedding layer, and thus a larger model size. But, on the other hand, it reduces the length of sequences of tokenized text. This achieves faster training and inference while allowing more information in the context window. 

% seve: Considering the large amount of languages and scripts included in our training data, we settled for 256k tokens.
Considering the high degree of multilinguality present in our training data, after some preliminary experiments we ultimately settled for 256,000 tokens. This is a relatively large number compared to widely adopted vocabulary sizes of 32k tokens (e.g. Mistral-7B \cite{mistral}, Llama \cite{llama1}, Llama-2 \cite{llama2}), 50k tokens (e.g. GPT-3 \cite{gpt3}, GPT-NeoX \cite{gptneox}, OPT \cite{opt}, \mbox{Olmo \cite{olmo}}), 65k tokens (e.g. Falcon \cite{falcon}, Yi \cite{yi}), 128k tokens (e.g. Llama-3 \cite{llama3}) or even 152k tokens (e.g. Qwen \cite{qwen1}, Qwen-2 \cite{qwen2}). However, the literature has plenty of examples of highly multilingual models that use similarly sized vocabularies to ours, such as \mbox{BLOOM \cite{bloom}}, PaLM \cite{palm}, Nemotron-4 \cite{nemotron4} or the Gemma series \cite{gemma,gemma2}.

% more examples 32k: gopher, lamda, chinchilla
% more examples 50k: gpt2, pythia, opt, phi

It is relevant to note the current trend towards larger vocabulary sizes, with models produced by Meta or Mistral being great examples of that. Their first releases had relatively small vocabularies of around 32,000 tokens, while in later versions the vocabularies were expanded to 128,256 and 131,072 tokens, respectively. This increase can be justified by the fact that their most recent models account for a greater variety of languages.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/tokenizer/technical_report_main.pdf}
    \caption{Comparison of tokenizer fertility (i.e. tokens-per-word) across multiple languages: Catalan, Greek, English, Spanish, Basque, Finnish, Irish, Galician, Lithuanian and Russian. The horizontal lines show the fertility of a monolingual tokenizer with a vocabulary size of 50k tokens.}
    %\caption{Comparison between multilingual tokenizers in terms of fertility (tokens-per-word).}
    \label{fig:fertility_plot}
\end{figure}

The histogram from Figure \ref{fig:fertility_plot} illustrates the average amount of tokens required by various tokenizers to encode a single word across different languages. All baseline models chosen for comparison have considerably large vocabulary sizes\footnote{Note that the Mistral baseline, Mistral-Nemo-Base-2407, has a vocabulary size of 131,072 tokens rather than 32,000 from previous Mistral releases.}. We have selected one representative from every language family present in the training data (Romance, Germanic, Slavic, Uralic, Baltic, Celtic, Hellenic, Semitic and Euskera), and added two additional languages (Catalan and Galician) since they are part of our evaluation benchmark and present in our instruction tuning dataset. For a full comparison across all languages, please refer to Appendix \ref{app:fertility}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Design Choices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to the vocabulary size, there are several other key considerations that must be taken into account when training a tokenizer. Our final design is inspired by that of several state-of-the-art models,  cherry-picking the features that we considered most suitable for our use case, namely:

%\paragraph{Vocabulary Size.} As already mentioned in the previous section, we settled for a relatively large vocabulary size of 256.000 tokens. It is important to note that the per-GPU vocabulary size should be multiple of 128 for GPU efficiency reasons \cite{megatron-lm}. This means that the total size should be divisible by 128 times the amount of model chunks. If this is not the case, training frameworks such as NeMo \cite{nemo} can automatically extend the vocabulary with pad tokens, but it is preferable to have an optimal size by default.

\paragraph{Byte Fallback.} Fallback to bytes was enabled in order to decompose unknown UTF-8 characters. By adding all possible combinations of bytes in the base vocabulary, we ensure that there are no out-of-vocabulary words that would be mapped to the same token (e.g. \textit{<UNK>}). Given the multilingual nature of our model, an additional motivation to rely in bytes is that it supposedly enhances vocabulary sharing between languages \cite{bbpe}.
% bloom: bBPE maximizes vocabulary sharing between languages.
% can be transferred between languages with non-overlapping character sets.

\paragraph{Digit splitter.} We split numbers into individual integer digits, so that the string "2025" would be represented as [2,0,2,5]. Although it comes at the cost of increasing fertility, this approach achieves a coherent decomposition across all numbers and aims to improve the model's ability to handle numerical data, a strategy that has been adopted by many in the literature \cite{palm,llama1,llama2,gemma,gemma2}. 

\paragraph{PreTokenization.} We decided not to perform any other sort of pretokenization step, unlike other works that use regular expressions to break down the input byte sequence into smaller chunks \cite{gpt2, bloom}.
% bloomberg: Our pretokenization follows GPT-2 in preventing multiple character classes from appearing in a single token.
% bloom: Our pre-tokenization has two goals: producing a first division of the text and restricting the maximum length of sequences of tokens produced by the BPE algorithm. The pre-tokenization rule used splits words apart while preserving all the characters and in particular the sequences of spaces and line breaks that are crucial for programming languages. We do not use English-centric splits common in other tokenizers (e.g. splitting around ’nt or ’ll).

% old: Our tokenizer employs NFC normalization as opposed to the most commonly used NFKC.
\paragraph{Normalization.} Our tokenizer employs NFC normalization, resulting in a slightly reduced fertility as reported by \citet{bloom}. A big concern with normalization forms such as NFKC and NFKD is that they treat superscripts and ligatures as separate characters. This means that "2\textsuperscript{3}" would be normalized to "2 3", being encoded in the exact same way as "23" after the digit splitter step. This is the reason why some works \cite{gopher,chinchilla} openly reject this type of non-lossless normalization forms, but this is not an issue with NFC. 
% gopher: Our tokenizer performs NKFC16 normalization as a pre-processing step. This normalization form is not fully lossless. We will use lossless normalization forms in future work.
% bloom: In all cases, we observed that adding unicode normalization such as NFKC did not reduce the fertility by more than 0.8\% on all the languages considered but came at the cost of making the model less general

% en tenim 31 en total, son user-defined symbols en spm
\paragraph{Whitespace sequences.} In order to greatly reduce the token-per-word ratio for programming languages, it is a common practice to manually add tokens that represent sequences of whitespaces \cite{gptneox, codegeex, erniecode}. According to \citet{codex}, this can reduce by 30\% the amount of tokens required to represent code. We include both sequences of whitespaces and tabs to account for different types of indentation styles, as well as sequences of newline symbols\footnote{In SentencePiece, this requires enabling \textit{allow\_whitespace\_only\_pieces} and disabling \mbox{\textit{remove\_extra\_whitespaces}.}}. More specifically, the added tokens were sequences of up to 24 whitespaces, 6 tab characters or 3 newline symbols. 

% useful for developers:  These tokens have the form "<|reserved\_xx|>" and can be found in the range of ids from 123 to 456
\paragraph{Reserved Tokens.} A total of 100 tokens were reserved for downstream adaptations. These were defined as "control symbols" in sentencepiece, ensuring that they are not taken into account while learning the vocabulary. The idea is that these tokens can be used for potential future applications. For instance, notice that the last two have already been replaced by the |<im\_start>| and |<im\_end>| tokens used in OpenAI's ChatML template, anticipating that those would be useful for the instructed versions of the model. A script for re-setting reserved tokens is provided is provided to streamline the process\footnote{\url{https://github.com/langtech-bsc/langtech_tokenizers/blob/master/change_and_activate_reserved_tokens.py}}.
% cal explicar que tot i estar al vocab no es codifiquen com un unic token?
% spm: Control symbols are used to encode special indicators for the decoder to change the behavior dynamically. Example includes the language indicators in multi-lingual models. <s> and </s> are reserved control symbols. Control symbols must be inserted outside of the SentencePiece segmentation. Developers need to take the responsibility to insert these symbols in data generation and decoding. Control symbols are decoded into empty strings.

% \paragraph{Sentinel Tokens.} The code portion of our corpus comes from the SantaCoder dataset \cite{santacoder}, which has a total of 15 sentinel tokens such as <reponame>, <gh\_stars> or <commit\_msg>. We decided not to manually include these in the tokenizer's vocabulary, and instead let the BPE algorithm decide whether it is worth or not to encode them as individual tokens. 

% \paragraph{Special Tokens.} <s>, </s>, <unk> igual que mistral/llama, pero afegim a mes a mes <pad> com gemma/bloom/zephyr 

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Tokenizer Training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% seve: Contrary to the common practice of training the tokenizer with a random sub-sample of pre-training data, which is equivalent to train the tokenizer with the same language proportions as the model, we decided to go with a uniform distribution across the 35 languages and code.

To train Salamandra's tokenizer, a subset of our cleaned and deduplicated pre-training corpus was used. Contrary to the common practice of training the tokenizer on a random sub-sample of pre-training data, which mirrors the language distribution later used to train the model, we opted for a uniform distribution across our 35 languages and code. This decision is supported by findings from previous works which suggest that a fair representation of languages can be beneficial for overall model performance \cite{flor}.

% the regex: `[ \]\?\[/\p{L}]+|[ ]?[^\p{L}\p{N} \t\n]+|[ ]+|[\t]+|[\n]+|\d{1}`

We rely on fertility\footnote{We refer to the term `fertility` as the average number of tokens per word, with words being computed using the following regular expression:\\ 
\texttt{[ \symbol{92}]\symbol{92}\symbol{63}\symbol{92}\symbol{91}\symbol{47}\symbol{112}\symbol{123}L\symbol{125}]+|[ ]?\symbol{91}\symbol{94}\symbol{92}\symbol{112}\symbol{123}L\symbol{125}\symbol{92}\symbol{112}\symbol{123}N\symbol{125} \symbol{92}t\symbol{92}n]+|[ ]+|[\symbol{92}t]+|[\symbol{92}n]+|\symbol{92}\symbol{100}\{1\}}
} as a measure of tokenization efficiency. For this metric, lower values are considered to be better as they indicate that text can be encoded with a smaller amount of subword units. As it can be seen in Figure \ref{fig:alphas_plot}, in general terms, using a uniform distribution greatly reduces the fertility score of low-resource languages, such as Basque, while only slightly increasing it for high-resource languages like English or Spanish.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/tokenizer/technical_report_alphas.pdf}
    \caption{Fertility score of a tokenizer trained on a balanced dataset where each language is represented equally (i.e. Uniform distribution), compared to a tokenizer that has been trained on a random subsample of data from the training corpora (i.e. Non-uniform distribution). The horizontal lines show the fertility of a monolingual tokenizer with 50k tokens of vocabulary.}
    \label{fig:alphas_plot}
\end{figure}

That said, there are a few edge cases where this may not apply due to the relatedness between different languages. For instance, Galician (`gl`) is a low-resource language that should benefit from a more uniform distribution, but it is so closely related to the Portuguese language (`pt`) that it is negatively affected by the fact that Portuguese data has been decreased to a greater extent. This is the reason why a few low-resource languages might have a higher fertility score despite using a more equitable tokenizer. All in all, fertility of each language is close to monolingual baselines (horizontal lines in the graphic) across all languages, showing a fair vocabulary support for all of them.

% seve: This pattern is not always so clear because of close-language influence. For instance, Galician (`gl`) is a low-resource language that should benefit from a more uniform distribution. However, this is not the case because it is orthographically close to a high-resource language: Portuguese (`pt`). Then, the benefit of more Galician data is overcome by the loss of Portuguese data, resulting in higher fertility.

% Regarding the training data, given that the goal was to produce a general-purpose tokenizer, the data sources were kept as diverse as possible.

The final subsample used to train Salamandra's tokenizer has roughly 93 million words per language, which accounts for a total of 3.3 billion words. The training process took 22 hours to complete on a single node (112 cores) from Mare Nostrum 5's general-purpose partition.

All code used for tokenization purposes is made publicly available\footnote{\url{https://github.com/langtech-bsc/langtech_tokenizers}}.

% defaults spm: https://github.com/google/sentencepiece/blob/022f8c3fed4d2feb4e4c670949cf01cef477dcc4/doc/options.md

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% LITERATURE OVERVIEW %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% llama: a large vocabulary potentially yields stronger multilingualism.

% bloom: A large vocabulary reduces the risk of over-segmenting some sentences, especially for low-resource languages.

% falcon: better fertility and faster inference per byte of text. But from a scalability perspective, they can lead to unbalanced pipeline stages, and may require more storage space; also, it is unclear whether models optimally use them. 

% bloomberg: There are various considerations in choosing the vocabulary size. One advantage of a large vocabulary is that more information can fit into the context window. On the other hand, there is overhead with a larger vocabulary: a larger proportion of model parameters are required for token embedding. We select our vocabulary size based on experiments with different vocabulary sizes. Our heuristic is to choose the vocabulary size that leads to the smallest encoded representation of C4.

% bloomberg: Tokenization and vocabulary choice play a critical role in model performance as they can help the model learn meaningful representations and generalize to unseen words.

% bloom: We use the fertility of our tokenizer compared to existing monolingual tokenizers as a metric for sanity checks. Fertility is defined as the number of tokens created per word, which we measured using subsets of OSCAR in the languages of interest. A very high fertility on a language compared to a monolingual tokenizer may indicate a degradation on the downstream multilingual performance of the model. Our goal was to not degrade the fertility on each language by more than 10 percentage points when comparing our multilingual tokenizer with monolingual tokenizers in corresponding languages. [...] We applied the same sampling ratios per language as for the training data.

% bloom: When initially using non-deduplicated data, we found entire URLs stored as tokens caused by several documents containing a high number of duplicates. These issues motivated us to remove duplicated lines in the tokenizer training data. 

% bloom: In order not to lose information during tokenization, the BPE tokenizer creates merges starting from bytes as the smallest units instead of characters.

% santacoder: Can we use the tokenizer to remove low-quality files from the dataset? We experiment with filtering files with a low character-to-token ratio. For each language, we find that files with a ratio below the 5th percentile are usually of poor quality, but increasing the threshold may eliminate some good-quality files. We therefore set different cutoff values for different languages. This filters out roughly 4\% to 5\% of data. Note that this filter may also be biased against files with non-English comments.