% llama3, opt, falcon, bloom

% llama3: It offers 240 PB of storage out of 7,500 servers equipped with SSDs, and supports a sustainable throughput of 2 TB/s and a peak throughput of 7 TB/s. A major challenge is supporting the highly bursty checkpoint writes that saturate the storage fabric for short durations. Checkpointing saves each GPU’s model state, for recovery and debugging. We aim to minimize GPU pause time during checkpointing and increase checkpoint frequency to reduce the amount of lost work after a recovery.

We train an optimized auto-regressive dense transformer architecture on the corpus described in Section \ref{subsec:pretrain_data}. Our training recipe starts with an initial pre-training phase and concludes with a shorter annealing stage, similarly to recent work \cite{nemotron4, llama3, eurollm}. 

In both stages, we follow the standard causal language modeling approach \cite{gpt1} where the learning objective is to predict the next token in a sequence. By training the model to maximize the likelihood of the upcoming token, it becomes capable of producing coherent text by learning intrinsic dependencies between tokens.

Our training data is tokenized using Megatron’s preprocessing script\footnote{\url{https://github.com/NVIDIA/NeMo/blob/main/scripts/nlp\_language\_modeling/preprocess\_data\_for\_megatron.py}}. The outcome is a series of binary files that contain the sequences of tokens, and the same amount of index files with dataset- and document-level metadata. The index files are then used by NeMo Framework to shuffle and merge documents, which are ultimately truncated to fit within the model's context size. 

The global batch size is roughly 4 million tokens in all cases, as it can be seen in Table \ref{tab:parallelism}. This is equivalent to 1,024 instances for models with a sequence length of 4,096 tokens, and 512 instances if the context is 8,192.

% The weights are initialized using a normal distribution centered at zero with a standard deviation of X. 
Training starts from a random initialization of model weights and mixed precision training \cite{mixed_precision} is used to improve throughput without losing stability at critical stages.

Checkpointing was performed every 2,000 or 5,000 steps, using the PyTorch Lightning format rather than nemo in order to save time. The compressed nemo file is only generated once training is concluded. 

For training optimization, we rely on the widely used Adam optimizer \cite{adam_original, adam}, with momentum [$\beta_1 = 0.9$, $\beta_2 = 0.95$], epsilon set to $1 \times 10^{-8}$ and weight decay to 0.1. We adopt a cosine learning rate schedule with different peak learning rates for each model size (see Table \ref{tab:pretraining_params}). The learning rate is linearly warmed up over 2,000 steps, and then it decays all the way down to a tenth of the peak value. After the warm-up stage, gradients are clipped to a maximum threshold of 1.0 in order to tackle the exploding gradients problem. However, some midflight changes were required for the 40-billion variant. During the first 20,000 steps, gradient clipping was set to 0.3 and gradually increased to 1.0 by step 50,000 to achieve training stability.

The training recipes are fairly similar for all model sizes, refer to Table \ref{tab:pretraining_params} to see the full set of hyperparameters.

\input{tables/hyperparams}