% pre process: true -> add embedding
% post process: true -> add pooler
% we do not share input/output embeddings, which could have reduced the model size (e.g. gemma)

The Salamandra models are based on the standard decoder-only Transformer \mbox{architecture \cite{attention}}, with several improvements that have been gradually introduced in \mbox{subsequent} works. The main differences from the original Transformer are listed below:

\begin{itemize}
    \item \textbf{No biases.} All bias terms are removed to improve training stability \cite{palm}. 
    \item \textbf{Positional embeddings.} We use rotary positional embeddings (RoPE) \cite{rope} as an alternative to absolute positional embeddings. The base frequency is set to 10,000. 
    \item \textbf{Activation function.} ReLU is replaced by SwiGLU, a smoother activation function that combines Swish \cite{swish} and GLU \cite{glu} for improved performance \cite{swiglu}.
    \item \textbf{Normalization.} Following the current trend, layer normalization \cite{layer-norm} is replaced by \mbox{RMSNorm} \cite{rms-norm}. The epsilon hyper-parameter is set to 1e-5.
    \item \textbf{Floating point precision}. All variants are trained with BFloat16 numerical precision for training stability.
    \item \textbf{Attention Mechanism.} Flash attention \cite{flash} is used to speed up training by improving computational efficiency and reducing memory usage.
    \item \textbf{Attention Heads.} The smaller 2B parameter model relies on multi-head attention, while the 7B and 40B variants leverage grouped-query attention (GQA) \cite{gqa} for faster inference and reduced memory requirements. The number of GQA groups is set to 8 in both cases, as it seemed to be a good trade-off in our preliminary experiments.
\end{itemize}
