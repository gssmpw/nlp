Over the past few years, the rapid progress in Natural Language Processing has been fueled by the relentless development of large language models (LLMs) \cite{survey_llms_1,survey_llms_2}. These models, which are typically trained on massive text corpora, have shown unprecedented capabilities across diverse tasks such as complex reasoning, reading comprehension, text summarization and code generation \cite{big_bench_tasks, measuring_multitask, code_evaluation}. 

The so-called scaling laws \cite{chinchilla,scaling_laws} served as a driving force to produce ever larger models that would eventually exhibit emergent behaviours \cite{emergent_abilities}, but it soon became clear that scaling alone was no panacea \cite{llama1,instructgpt}. As scaling continues to push performance, the focus has now shifted towards optimizing model efficiency \cite{fp8,flash2}, synthetic data generation \cite{survey_sdg1,survey_sdg2} and alignment with human preferences \cite{survey_alignment1,survey_alignment2}, among other topics.

However, the major breakthroughs have often been carried out by a handful of resource-rich companies that can afford the computational requirements and, perhaps more importantly, possess the expertise to leverage them effectively. As a result, the best performing models remain locked behind proprietary walls, hindering scientific progress and allowing a select few to maintain dominance in the field \cite{gpt4,claude3,gemini1p5}. 

The performance gap between closed- and open-weight models has been narrowed down with recent releases such as Mistral \cite{mistral}, Gemma \cite{gemma,gemma2}, Llama \cite{llama1,llama2,llama3} or \mbox{Qwen \cite{qwen1,qwen2}}, but even these open-source efforts do not fully disclose highly important details such as the data composition and training recipes. Moreover, the majority of these models are primarily trained on English text, at most incorporating a small portion of multilingual data \cite{nemotron4-15b, nemotron4}. 

Fortunately, there have been a few initiatives that, despite lagging behind production-ready models from large corporations, demonstrate a significantly higher level of openness. While these more transparent efforts are predominantly English-centric \cite{falcon, pythia, olmo, tulu3}, there are also notable examples of multilingualism in the literature \cite{bloom, eurollm, ali_teuken-7b-base_2024}. % This work belongs to the latter group. 

In an effort to contribute to the open-source LLM ecosystem, this paper introduces \mbox{Salamandra}, a family of dense Transformer-based models specifically tailored for \mbox{European} languages. In particular, we openly release the following artifacts:
\begin{itemize}
\item Base decoder-only models with three different sizes: 2, 7 and 40 billion parameters.
\item Their corresponding instruction-tuned counterparts, trained with single-turn and multi-turn instruction following datasets.
\end{itemize}

The variety of model sizes is intended to cater to different levels of user requirements: the smaller 2B model is designed for lightweight deployment on commodity hardware, the more standard 7B model strikes a balance between performance and efficiency, and the resource-intensive 40B model is intended for applications where performance is a key factor. Regarding the fine-tuned versions, it is important to highlight that they have been optimized for dialogue use cases, but they are still unaligned with human preferences, which is something the team intends to address in future work.

All checkpoints are released under an Apache 2.0 license, allowing research and commercial use. In addition, we facilitate reproducibility by sharing our training and evaluation code, and we foster open science by revealing as many details as possible in this technical report. Our hope is that the NLP community can benefit from this work and build upon both the successes and shortcomings of our decisions.

The remainder of this document is organized as follows: Section \ref{sec:model} provides a high-level overview of the design decisions related to the model and tokenizer. Section \ref{sec:pretraining} offers a thorough description of our data collection and pre-processing pipeline, our pre-training methodology, and the distributed learning strategy. Section \ref{sec:post_training} showcases two different post-training stages, namely instruction tuning and vision-language fine-tuning. In \mbox{Section \ref{sec:eval},} we perform an extensive analysis of the modelsâ€™ capabilities, comparing their performance to that of strong baselines on a set of standard academic benchmarks and employing the LLM-as-a-judge strategy for specific tasks. Next, still in line with the evaluation focus, Section \ref{sec:safety} assesses their safety and biases through comprehensive evaluations. Finally, Section \ref{sec:conclusion} concludes the paper and outlines directions for future work.