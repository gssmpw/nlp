In this section, we explore different architectural choices by experimenting with setups that closely resemble the final training configuration.

\subsubsection{Grouped Query Attention}
In a causal decoder, the attention of a given token depends on the attention of all previous tokens. To avoid recalculating the keys and values of these tokens, it is common practice to cache them in GPU memory. This approach enables much faster sequential inference but introduces two overheads: increased memory capacity requirements and bandwidth costs for storing and retrieving the key-value (KV) cache \cite{pope2022}. In multi-head attention (MHA) \cite{transformers}, the number of attention heads typically scales with the model's dimension, meaning smaller models may experience less impact from these overheads \cite{gqa}.

Multi-query attention (MQA) \cite{shazeer2019fasttransformerdecodingwritehead} is a variation of multi-head attention in which all heads share a single set of keys and values, significantly reducing memory and bandwidth requirements. Grouped Query Attention (GQA) \cite{gqa}, on the other hand, partitions the query heads into $G$ groups, where each group shares one key head and one value head. According to \citet{gqa}, GQA-8—where the queries are divided into 8 groups—offers performance close to that of MHA while achieving speeds comparable to MQA, particularly in larger models \cite{gqa}.

In line with the observations of \citet{llama2}, which state the significant improvements in maximum batch size and latency of GQA-8 for larger models, our 40B-parameter model uses GQA-8, while the 2B-parameter model uses MHA. In this study, we focus on the intermediate case of a 7B-parameter model to analyze the trade-offs associated with GQA-8. Our primary goal is to evaluate the improvements in inference time and assess how these improvements affect the model's overall performance.

To this end, we trained two variants of the final architecture from scratch on a corpus containing 16 billion tokens. Both models were trained for two full days on 16 H100 GPUs, with the only architectural difference being that one used MHA while the other used GQA-8.

After 48 hours of training, the GQA-8 model completed 5.31\% more steps. However, despite this higher step count, the GQA-8 model achieved 99.58\% of the MHA model's validation performance. We conducted inference experiments with the trained models in the following setup. We load both models in a single GPU, and generate 

% Table with avg training times, validation performance at the end of the 2 days and inference times in avg.
% After 48 hours of training, the GQA-8 model completed each step in an average of 40.61 seconds, compared to 42.89 seconds for the MHA model. This represents a 5.31\% per-step speedup, enabling the GQA-8 variant to complete more steps in the same amount of time. However, despite this higher step count, the GQA-8 model achieved only 99.58\% of the MHA model's validation performance.
% After 48 hours of training, the GQA-based model completed each step in 40.61 seconds on average, compared to 42.89 seconds for the MHA-based model—a per-step speedup of 5.31\%. Over the 2-day training period, the GQA-based model completed 5.58\% more steps than the MHA-based model. However, despite this higher step count, the GQA-based model achieved only 99.58\% of the MHA model's validation performance.

% We conducted inference experiments with the trained models in a setup designed to mimic real-world usage. Specifically, we loaded each model, performed inference across 10 languages, and measured the time required to generate 30 tokens for prompts with an average length of 6.09 tokens, using a batch size of 1. The GQA-based model (8 KV heads) achieved a 10.58\% reduction in inference time compared to the MHA-based model (32 KV heads).

% GQA: trained 2 7b with 140B tokens: gqa-8: jobid 1060389, mha: jobid 1060390
% /gpfs/projects/bsc88/text/models/gqa-experiments/results/gpt_7b_tp4_140B/results/gpt_7b_tp4_140B_gqa-8_1060389/checkpoints/gpt_7b_gqa-8_140b.nemo
% /gpfs/projects/bsc88/text/models/gqa-experiments/results/gpt_7b_tp4_140B/results/gpt_7b_tp4_140B_mha_1060390/checkpoints/gpt_7b_mha_140b.nemo
% final arch, ffn11008, gqa:4 : /gpfs/projects/bsc88/preproduction/test_12_03_2024/results/7b_final_tests/results/1745657
% Final arch, real training: job_8k_1911184.out

Our results highlight the trade-offs associated with adopting GQA in a 7B parameter model. While GQA improves the inference speed by ???10.58\%, these gains come at the cost of a slightly reduced validation performance (0.42\% lower than MHA).

\subsubsection{Fast-SWiGLU}
Within the transformer layer, an activation function is applied between the two linear transformations. We compare the activation function of the original Transformer \cite{transformers}, rectified-linear units (ReLU) \cite{relu}, with the SwiGLU function proposed by \citet{swiglu}.

% In the Transformer architecture, an activation function is applied between the two linear transformations within the feedforward network. Previous work by \citet{swiglu} introduced the SwiGLU activation function and demonstrated its advantages over rectified linear units (ReLU) \cite{relu} in certain settings. In this study, we replicate their experiment to verify whether the reported benefits of SwiGLU extend to our specific case.

To account for the increase in trainable parameters introduced by the SwiGLU activation function, we adjusted the hidden dimension of the feedforward network from 14,336 (used for ReLU) to 11,008 for the SwiGLU model. This adjustment ensures a comparable parameter count between the two configurations.

Once again, we trained two variants of the final architecture, from scratch, on 16 billion tokens of text. As part of our incremental experimentation approach, we employed grouped query attention with 8 groups for both models. The only architectural difference between the two models was their activation functions. Both models were trained for 2 full days on 16 H100 GPUs.

The SwiGLU-based model consistently outperformed the ReLU-based model in terms of validation loss throughout the entire training period. This suggests that SwiGLU facilitates better optimization dynamics and more effective learning from the outset.

These results confirm that SwiGLU’s benefits, as observed in the original study, are also applicable to our experimental setup. As noted in \citet{swiglu}, we do not offer a detailed explanation for the success of SwiGLU over ReLU. Instead, as directly stated in the original paper, we attribute its effectiveness, as always, to divine benevolence. 