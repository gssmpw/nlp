We evaluate the performance of Salamandra base and instructed models using the LM Evaluation Harness \cite{harness}, and we add \llmJudge{} (see \citealp{li-etal-2024-leveraging-large} for a review) to the evaluation setup for instructed models. At various points during pre-training and post-training, we involve humans in the direct assessment of the models' capabilities in different languages. However, developing a comprehensive and reliable human evaluation setup is on our current roadmap. When this is ready, we will report on the decisions taken, processes, and obtained results. A summary of the evaluation setup we use can be seen in Figure \ref{fig:eval_setup}.

This section presents and discusses our evaluation choices, and reports the results of Salamandra models compared to similar baselines. During the evaluation process, we identified multiple issues with existing evaluation datasets and techniques, and encountered multiple challenges when dealing with multilingual evaluation. We also present some of these, and will discuss them in detail in an upcoming version of this technical report, % also present a discussion of these, 
as they are relevant when interpreting the results of our evaluation.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=12cm]{figures/eval/eval-setup.png}
    \caption{Visual summary of the setup used to evaluate the capabilities of the Salamandra family of models and selected baselines.}
    \label{fig:eval_setup}
\end{figure}

\subsection{Overall Decisions} \label{overall-eval}

\paragraph{Multilingualism} %The field of language model evaluation is epistemologically immature, and benchmark leakage adds a layer of unreliability to any evaluation choice. 
A major problem when evaluating multilingual models such as the Salamandra family is that most evaluation datasets and techniques are only available in or tailored for the English language. Given the data distribution choices taken for Salamandra models (see Section \ref{subsec:pretrain_data}), we needed to evaluate models' performance across multiple languages. To do this, we developed IberoBench \cite{iberobench-coling-2025}, and focused on evaluating the performance of our models in Spanish, Catalan, Galician and Basque. We also include evaluation for a set of English tasks that have parallel datasets in IberoBench due to the global relevance of the language and the percentage of English data in the pre-training recipe (see Figure \ref{fig:lang_distribution}), and a small number of tasks using \llmJudge{} in German, Italian and French.

\paragraph{Humans-in-the-loop} Automatic evaluation allows for a fast and reproducible way to assess the performance of a model in downstream tasks. This makes it the most wide-spread type of LLM evaluation, as it is significantly more cost-effective and easy to implement than human evaluation. We acknowledge these benefits and thus mostly use automatic evaluation techniques. However, we actively move away from evaluation datasets that have been synthetically generated or automatically translated. The belief behind this choice is that most models used for synthetic data generation are heavily English-centric, which is not desirable for our multilingual evaluations. This English prevalence does not only have linguistic consequences, but we also believe that synthetic evaluation data with no human revision may lead to the reaffirmation of existing societal and cultural biases present in training datasets, some of which are US-specific and not a reality in the European context. For this reason, we only added to IberoBench datasets that had been human annotated or human translated, or, when automatic generation or translation was involved, authors had reported a comprehensive human revision of the automated process.

\paragraph{Constant qualitative analysis} In line with our human-in-the-loop approach for evaluation dataset selection, we carry out constant qualitative quality checks at all levels of the evaluation process. For instance, when implementing our LM Evaluation Harness, we looked at how each dataset is constructed and how it is pre-processed. We also looked at how models receive the input prompt for each task, the kind of output they generate, and ensured that metrics were performing as expected. For \llmJudge{}, during the testing phase, we iterated through multiple \textit{judge} prompts to ensure that \PrometheusLarge{} \cite{prometheus2}, our judge, followed the instructions for evaluation correctly.

This attention to detail ended up being remarkably important, as we identified various issues regarding the English version of some datasets, the type of prompt used for some LM Evaluation Harness tasks, and the way \PrometheusLarge{} reacted when asked to evaluate languages other than English using never-seen criteria and descriptors.



\subsection{Framework Description}
\subsubsection{LM Evaluation Harness and IberoBench} 
\label{harness-ibero}

\paragraph{Overview} Our gold-standard-based automatic evaluation is performed using LM Evaluation Harness \cite{harness}. We choose this framework for its open and collaborative nature, its widespread adoption in the literature, and its focus on reproducibility. In addition, this framework is regularly updated to enable evaluation of the latest LLM releases and to integrate various libraries to optimise inference such as Accelerate \cite{accelerate}, which enables data parallelism across multiple GPUs, or vLLM \cite{vllm}, which significantly speeds up inference. The Evaluation Harness can be used in zero- and few-shot scenarios, and with both multiple-choice (MC) and generation tasks. Within the framework, the tasks are implemented using YAML files, where the configuration for loading and preprocessing the dataset—usually integrated from Hugging Face—is specified, as well as the template for reformulating each dataset document, the metric to be used, the few-shot setup, and other decisions that allow reproducibility.

\paragraph{Reproducibility} Our models were evaluated using seed number \texttt{1234}, Torch version 2.4.0, Transformers version 4.46.2, data parallelism using Accelerate for models up to about 9B parameters, and model parallelism with the same library for larger models. We did not use vLLM for the model inference as the LM Evaluation Harness developers warn of some score variations when using it, which we corroborate. We highlight the importance of specifying this information not only for openness and reproducibility purposes, but also because we found that using different versions of libraries, and the use—or lack of use—of tensor parallelism and the vLLM library affect the results obtained by all models. These differences are sometimes below 1-2\%, but in some other cases, they can be significant. For instance, across languages, we notice that some models scoring around 30-40 BLEU in FLORES-200 when using the Transformers library drop to around 10-20 when using vLLM.

\paragraph{Datasets used} The main evaluation benchmark we use, IberoBench \cite{iberobench-coling-2025}, was designed for seamless integration and use with the Evaluation Harness. IberoBench is tailored to the Iberian languages (i.e., Spanish, Catalan, Basque, Galician, and Portuguese) and covers 10 general evaluation categories: common-sense reasoning, linguistic acceptability, mathematics, natural language inference (NLI), paraphrasing, question answering (QA), reading comprehension, summarization, translation, and truthfulness. IberoBench includes a total of 62 tasks, divided into 179 subtasks, and several of them are parallel across two or more of the Iberian languages and English. The benchmark only features high-quality datasets that were either human-translated or directly created from data in the corresponding Iberian language. IberoBench is a dynamic benchmark that has been periodically updated with new tasks, including both XStoryCloze and XNLI in Galician, since its release. We will update the results in this technical report as new tasks become publicly available. As mentioned, we also use existing datasets in English to evaluate the capabilities of the models in this language. Table \ref{iberobench_tasks} lists the tasks we use for evaluation, with parallel tasks across languages aligned in the same row. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[t]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{l|llllll}
\hline
\multicolumn{1}{c|}{\textbf{Category}} &
  \multicolumn{1}{c}{\textbf{en}} &
  \multicolumn{1}{c}{\textbf{ca}} &
  \multicolumn{1}{c}{\textbf{es}} &
  \multicolumn{1}{c}{\textbf{eu}} &
  \multicolumn{1}{c}{\textbf{gl}} &
  \multicolumn{1}{c}{\textbf{pt}} \\ \hline
 &
  \cellcolor[HTML]{EFEFEF}xstory\_cloze\_en &
  \cellcolor[HTML]{EFEFEF}xstory\_cloze\_ca &
  \cellcolor[HTML]{EFEFEF}xstory\_cloze\_es &
  \cellcolor[HTML]{EFEFEF}xstory\_cloze\_eu &
  \cellcolor[HTML]{EFEFEF}xstory\_cloze\_gl &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-2}{*}{\textbf{Commonsense Reasoning}} &
  copa\_en &
  copa\_ca &
  copa\_es &
  xcopa\_eu &
   &
   \\ \hline
 &
  \cellcolor[HTML]{EFEFEF}cola &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
 &
   &
  catcola &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}escola &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-4}{*}{\textbf{Linguistic Acceptability}} &
   &
   &
   &
   &
  galcola &
   \\ \hline
\textbf{Math} &
  \cellcolor[HTML]{EFEFEF}mgsm\_direct\_en &
  \cellcolor[HTML]{EFEFEF}mgsm\_direct\_ca &
  \cellcolor[HTML]{EFEFEF}mgsm\_direct\_es &
  \cellcolor[HTML]{EFEFEF}mgsm\_direct\_eu &
  \cellcolor[HTML]{EFEFEF}mgsm\_direct\_gl &
  \cellcolor[HTML]{EFEFEF} \\ \hline
 &
  wnli\_en &
  wnli\_ca &
  wnli\_es &
  wnli\_eu &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF}xnli\_en &
  \cellcolor[HTML]{EFEFEF}xnli\_ca &
  \cellcolor[HTML]{EFEFEF}xnli\_es &
  \cellcolor[HTML]{EFEFEF}xnli\_eu &
  \cellcolor[HTML]{EFEFEF}xnli\_gl &
  \cellcolor[HTML]{EFEFEF} \\
 &
   &
  teca &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}qnli\_eu &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-5}{*}{\textbf{NLI}} &
   &
   &
   &
   &
   &
  assin\_entailment \\ \hline
 &
  \cellcolor[HTML]{EFEFEF}paws\_en &
  \cellcolor[HTML]{EFEFEF}paws\_ca &
  \cellcolor[HTML]{EFEFEF}paws\_es &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}paws\_gl &
  \cellcolor[HTML]{EFEFEF} \\
 &
   &
  parafraseja &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}parafrases\_gl &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-4}{*}{\textbf{Paraphrasing}} &
   &
   &
   &
   &
   &
  assin\_paraphrase \\ \hline
 &
  \cellcolor[HTML]{EFEFEF}openbookqa &
  \cellcolor[HTML]{EFEFEF}openbookqa\_ca &
  \cellcolor[HTML]{EFEFEF}openbookqa\_es &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}openbookqa\_gl &
  \cellcolor[HTML]{EFEFEF} \\
 &
  piqa &
  piqa\_ca &
  piqa\_es &
  piqa\_eu &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF}xquad\_en &
  \cellcolor[HTML]{EFEFEF}xquad\_ca &
  \cellcolor[HTML]{EFEFEF}xquad\_es &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
 &
  arc &
  arc\_ca &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF}siqa &
  \cellcolor[HTML]{EFEFEF}siqa\_ca &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
 &
   &
  catalanqa &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}coqcat &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
 &
   &
   &
   &
  eus\_exams &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}eus\_proficiency &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-10}{*}{\textbf{QA}} &
   &
   &
   &
  eus\_trivia &
   &
   \\ \hline
\textbf{Reading Comprehension} &
  \cellcolor[HTML]{EFEFEF}belebele\_eng\_Latn &
  \cellcolor[HTML]{EFEFEF}belebele\_cat\_Latn &
  \cellcolor[HTML]{EFEFEF}belebele\_spa\_Latn &
  \cellcolor[HTML]{EFEFEF}belebele\_eus\_Latn &
  \cellcolor[HTML]{EFEFEF}belebele\_glg\_Latn &
  \cellcolor[HTML]{EFEFEF}belebele\_por\_Latn \\ \hline
 &
   &
  cabreu &
   &
   &
   &
   \\
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}xlsum\_es &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-3}{*}{\textbf{Summarization}} &
   &
   &
   &
   &
  summarization\_gl &
   \\ \hline
 &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}flores\_ca &
  \cellcolor[HTML]{EFEFEF}flores\_es &
  \cellcolor[HTML]{EFEFEF}flores\_eu &
  \cellcolor[HTML]{EFEFEF}flores\_gl &
  \cellcolor[HTML]{EFEFEF}flores\_pt \\
\multirow{-2}{*}{\textbf{Translation / Adaptation}} &
   &
  phrases\_va &
  phrases\_es &
   &
   &
   \\ \hline
 &
  \cellcolor[HTML]{EFEFEF}veritasqa &
  \cellcolor[HTML]{EFEFEF}veritasqa\_ca &
  \cellcolor[HTML]{EFEFEF}veritasqa\_es &
  \cellcolor[HTML]{EFEFEF} &
  \cellcolor[HTML]{EFEFEF}veritasqa\_gl &
  \cellcolor[HTML]{EFEFEF} \\
\multirow{-2}{*}{\textbf{Truthfulness}} &
  truthfulqa &
   &
   &
   &
  truthfulqa\_gl &
   \\ \hline
\end{tabular}%
\end{adjustbox}
\caption{Categories and tasks used for evaluation.}
\label{iberobench_tasks}
\end{table}


\subsubsection{LLM as a Judge} \label{sscn:llmjudge}

\paragraph{Overview} For open answer tasks such as summarization, translation or non-MC question answering, comparing model generations or its perplexity against golden answers directly is not insightful enough, as a model's response may be correct, but will receive a low score if it deviates from the reference answer. For instance, metrics like BLEU \cite{papieni-bleu-10.3115/1073083.1073135} and ROUGE \cite{lin-2004-rouge}, widely used in multiple Evaluation Harness tasks, have been shown to be unreliable, not always reflecting the quality and appropriateness of a model's generation \cite{sulem-etal-2018-bleu, freitag-etal-2020-bleu, wang-etal-2023-element}. In order to have a more comprehensive evaluation of our instructed models, we use \PrometheusLarge{} \cite{prometheus2} as a judge using the \llmJudge{} method. Similar to our gold-standard setup, we evaluate the \SalamandraFamily{} and other baselines for Spanish, Catalan, Basque, Galician and English tasks. Given that we try to parallelize the datasets used to source the prompt for our \llmJudge{} setup and our overall decisions (see Section \ref{overall-eval}), we do not include Portuguese, as there are no available benchmarks that match our criteria. We, however, include \llmJudge{} evaluation for some tasks in French, German, and Italian. The procedure we follow is:

\begin{itemize}
    \item The prompts of an existing evaluation dataset are rephrased to look like questions or instructions in natural language (see Appendix \ref{app:judge_prompts} for details). We call these \emph{queries}.
    \item We pass each query to the model we want to evaluate—we call it \emph{assistant}— obtaining a \emph{response}.
    \item We pass each query and corresponding response to our \emph{judge}, together with a \emph{rubric}, explaining how to score the responses (see Appendix \ref{app:judge_rubrics} for details). This is done via an \emph{evaluation prompt} (see below).
    \item We retrieve a numeric \emph{score} from the generation of our judge regarding the assistant's response to each query.
\end{itemize}

\paragraph{Judge and evaluation prompt} %Given that, at time of publication of this technical report, there are no publicly-available, multilingual, judge models, w
We use \PrometheusLarge{} \cite{prometheus2} as judge. This model is a version of Mixtral 8x7B \cite{jiang2024mixtralexperts}—%which is itself 
a multilingual model—fine-tuned on the task of English LLM performance evaluation through human-annotated data. For our evaluations, we use the same system prompt and template this model was trained on (in English), as we find this is key in making sure the judge follows the evaluation criteria and returns its output score in a format that we can reliably parse. The system message and the evaluation prompt that we use can be found in Snippet \ref{lst:systemprompt} and Snippet \ref{lst:judgeprompt}, respectively. We also give the judge model task-specific rubrics in English, while keeping the query (\verb|input|) and response (\verb|prediction|) fields in the target language. 

\begin{lstlisting}[label=lst:systemprompt,caption={System message used for our \llmJudge{} setup.},float]
You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.
\end{lstlisting}

\begin{lstlisting}[label=lst:judgeprompt,caption={Evaluation prompt used for our \llmJudge{} setup. Here, \texttt{\{a\}} and \texttt{\{b\}} refer, respectively, to the lowest and highest values of the rubric, \texttt{\{input\}} refers to the query, \texttt{\{prediction\}} refers to the assistant's response, and \texttt{\{criteria\}} refers to the rubric used.},float]
###Task Description:
An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between {a} and {b}. You should refer to the score rubric.
3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (an integer number between {a} and {b})"
4. Please do not generate any other opening, closing, and explanations.

###The instruction to evaluate:
{input}

###Response to evaluate:
{prediction}

###Score Rubrics:
{criteria}

###Feedback:
\end{lstlisting}

\paragraph{Source datasets, query creation, and evaluation criteria} We cover 6 of the 10 general evaluation categories used for our gold-standard evaluation (see Section \ref{harness-ibero}). This is, common-sense reasoning, mathematics, paraphrasing, reading comprehension, summarization, and translation. We do not include linguistic acceptability, NLI, QA, and truthfulness, as the tasks included in these categories (see Table \ref{iberobench_tasks}) either already entail questions (or instructions) written in naturally occurring language in the LM Evalaution Harness, or their adaptation would result in an unnatural question or instruction. Regarding the rubrics used to evaluate responses (see Appendix \ref{app:judge_rubrics}), we develop our own set for each task, tailoring the descriptors for each criterion to the task and moving away from generalist criteria. Our rubrics are either 5-Point Likert scales, in which the judge scores a response from \texttt{1} to \texttt{5}—a description is given to each number of the scale, or binary tasks, in which the judge scores a response as either \texttt{0} or \texttt{1}. This last setup is a deviation from the original \PrometheusLarge{} direct evaluation prompt \cite{jiang2024mixtralexperts}, but we find that this judge model adapts to this change without issues and always gives a score of \texttt{0} or \texttt{1}.


%Regarding 5-Point Likert scales, the decision of giving our own descriptions to each value and to limit the options to 5 instead of 10 ADD REF %JAB:: Decided against adding this.
We evaluate using the following datasets and evaluation criteria:

\begin{itemize}
    \item For common-sense reasoning, we use XStoryCloze \citep{lin-etal-2022-shot, iberobench-coling-2025} to ask the assistant to finish an incomplete short story. We evaluate the \emph{coherence} of the endings based on the preceding sentences.
    \item For mathematics, we use MGSM \citep{shi2023language, iberobench-coling-2025} to ask the assistant to solve a mathematical problem while reasoning their answer. We evaluate the \emph{mathematical correctness} of the numeric answer and \emph{reasoning capability} demonstrated.
    \item For paraphrasing, we use PAWS \citep{yang-etal-2019-paws, iberobench-coling-2025} to ask the assistant to paraphrase a sentence. We evaluate the \emph{accuracy}, \emph{completeness} and \emph{grammatical correctness} of the paraphrases generated.
    \item For reading comprehension, we use Belebele \citep{bandarkar-etal-2024-belebele} to ask the assistant a question in relation to a provided passage. We evaluate the \emph{answer relevance} and the \emph{passage comprehension} capability.
    \item For summarization, XLSum \citep{hasan-etal-2021-xl}, caBreu \citep{gonzalez-aguirre_building_2024} and summarization\_gl \citep{iberobench-coling-2025} to ask the assistant to summarize a provided passage. We evaluate the \emph{conciseness} of the generated summary and its \emph{informativeness} with regard to the content of the source text.
    \item For translation, we use FLORES-200 \citep{nllbteam2022language} to ask the assistant to translate a sentence either into or from the language in which the rest of the prompt is written. We evaluate the translation \emph{accuracy} and \emph{fluency} in both directions.
\end{itemize}


\paragraph{Robustness} The rephrasing of the dataset instances into natural language is done using three different templates for each source instance (see Appendix \ref{app:judge_prompts}), allowing us to measure the robustness of the assistants to changes in prompting. We calculate the variance of the scores as follows:
\begin{equation*}
V_i = \frac{1}{n} \sum_{j=1}^{n} \left(S_{i,j} - \overline{S_i}\right)^2,
\end{equation*}
where $S_{i,j}$ is the score given by the judge for the $j$-th query of the $i$-th task instance, and $\overline{S_i}$ is the mean score for the $i$-th task instance:
\begin{equation*}
\overline{S_i} = \frac{1}{n} \sum_{j=1}^{n} S_{i,j}.
\end{equation*}
We then compute the \emph{mean variance} for a task and language by averaging the variances across all task instances:
\begin{equation*}
\text{Mean Variance} = \frac{1}{m} \sum_{i=1}^{m} V_i,
\end{equation*}
where $m$ is the total number of task instances. A lower mean variance indicates higher robustness of the assistant in that task and language.


\subsubsection{Base Model Performance} \label{base_perf}

Table \ref{tab:base-models-results} presents the performance of the Salamandra v1.1 models in tasks using the LM Evaluation Harness setup described in Section \ref{harness-ibero}, organized by category and language. \SalamandraBaseII{} is benchmarked against publicly available multilingual and/or state-of-the-art models within a similar parameter range, including EuroLLM 1.7B \citep{martins_eurollm_2024}, FLOR 1.3B \citep{flor} and Gemma-2 2B \citep{gemma2}. Similarly, \SalamandraBaseVII{} is evaluated against Teuken 7B v0.5 \citep{ali_teuken-7b-base_2024}, EuroLLM 9B\footnote{Available here: \url{https://huggingface.co/utter-project/EuroLLM-9B}.}, Occiglot-eu5 7B \citep{avramidis_occiglot_2024}, FLOR 6.3B \citep{flor}, Mistral 7B v0.3 \citep{mistral}, Gemma-2 9B \citep{gemma2} and Llama-3.1 8B \citep{llama3}. In addition, we report results from an intermediate checkpoint of \SalamandraBaseXL{}, which is still in training and has not undergone an annealing phase.

Note that tasks are evaluated using different metrics, including accuracy, BLEU, ROUGE, F1, exact match, and MCC. Specifically, the four linguistic acceptability tasks are measured using MCC, which ranges from -1 to 1, with 0 indicating a random score. Another notable case is the math task, MGSM, which uses exact match as the metric, with a minimum score of 0 and a maximum of 1. Additionally, the expected random baseline for each task can vary depending on factors such as the number of labels, where applicable.

Analyzing the broad results across tasks and languages reveals significant variability, making it challenging to draw overarching conclusions. Gemma-2 models, which we note are knowledge distilled from larger models, frequently lead in tasks involving English and Spanish, showcasing their robust performance in these languages. However, Salamandra models demonstrate strong competence and emerge as the best-performing models in some categories, particularly in Catalan and Basque. Salamandra 40B, despite being an intermediate checkpoint with training not yet completed, naturally achieves the best performance within the Salamandra family and often secures leading results, though not consistently across all tasks. In terms of linguistic acceptability, Salamandra models show specialization in Catalan, while Gemma-2 maintains its dominance in English, Spanish, and Galician across model sizes. Notably, the substantial performance gains from the 2B to 7B ranges in linguistic acceptability suggest strong scalability in this area for all models.

Mathematical reasoning remains a weak area for all models, with low performance in the 2B range and modest improvements in the 7B range, where Gemma-2 achieves the most notable gains. Even Salamandra 40B lags behind, resulting from a lack of specialized training in this domain. Similarly, tasks in natural language inference (NLI) show limited variation across models and sizes, without a consistent best performer. Some anomalies, such as Salamandra 40B underperforming its smaller 7B counterpart in Basque WNLI, warrant further investigation. Paraphrasing tasks, while more consistent, also reveal variability, with Gemma-2B standing out among smaller models.
Salamandra models shine in translation and common-sense reasoning, with the 40B variant particularly excelling in QA tasks and outperforming competitors in Catalan and Basque. However, reading comprehension shows mixed results, as Salamandra models struggle in some multilingual tasks like Belebele, but excel in Basque evaluations. Summarization results are surprisingly low in BLEU scores, indicating potential issues with either the task or the metric that we plan to revise in the future. Overall, the \SalamandraFamily{} is strong in multilingual reasoning and translation, while mathematics and NLI are areas requiring further refinement.

\begin{comment}
\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{l|ll|llll|llllllll|l}
\multicolumn{1}{c}{\textbf{Category}} &
  \multicolumn{1}{c}{\textbf{Task}} &
  \multicolumn{1}{c}{\textbf{Lang.}} &
  \begin{turn}{90}\textbf{salamandra-2b}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM-1.7B}\end{turn} &
  \begin{turn}{90}\textbf{FLOR-1.3B}\end{turn} &
  \begin{turn}{90}\textbf{gemma-2-2b}\end{turn} &
  \begin{turn}{90}\textbf{salamandra-7b}\end{turn} &
  \begin{turn}{90}\textbf{Teuken-7B}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM-9B}\end{turn} &
  \begin{turn}{90}\textbf{occiglot-7b-eu5}\end{turn} &
  \begin{turn}{90}\textbf{FLOR-6.3B}\end{turn} &
  \begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
  \begin{turn}{90}\textbf{gemma-2-9b}\end{turn} &
  \begin{turn}{90}\textbf{Llama-3.1-8B}\end{turn} &
  \begin{turn}{90}\textbf{salamandra\_40b}\end{turn} \\ \hhline{===|====|========|=}
\multirow{9}{*}{Commonsense Reasoning} & \multirow{5}{*}{xstorycloze} & en & 71.81 & 72.07 & 61.35 & \textbf{79.75} & 79.95 & 77.70 & 80.41 & 80.34 & 73.59 & 82.20 & \textbf{83.59}* & 81.07 & 82.20 \\
 & & es & 64.73 & 65.59 & 63.93 & \textbf{69.89} & 74.39 & 71.21 & 74.45 & 75.84 & 69.76 & 71.14 & \textbf{76.90} & 73.73 & 78.89* \\
 & & ca & \textbf{66.38} & 60.82 & 64.13 & 64.53 & 75.12 & 64.73 & 72.47 & 69.95 & 70.88 & 70.28 & \textbf{75.51} & 72.53 & 78.09* \\
 & & eu & \textbf{58.97} & 49.57 & 49.37 & 53.21 & \textbf{67.24} & 52.42 & 50.36 & 51.22 & 53.14 & 50.63 & 62.41 & 56.92 & 70.75* \\
 & & gl &        &       &        &       &        &       &        &       &       &        &       &       &       \\ \cline{2-16}
 & \multirow{4}{*}{copa} & en & 83.00 & 77.00 & 66.00 & \textbf{89.00} & \textbf{94.00}* & 92.00 & 93.00 & 91.00 & 84.00 & \textbf{94.00}* & \textbf{94.00}* & \textbf{94.00} & 91.00 \\
 & & es &        &       &        &       &        &       &        &       &       &        &       &       &       \\
 & & ca & \textbf{70.20} & 65.20 & 66.40 & 67.20 & \textbf{82.20} & 65.20 & 80.80 & 70.60 & 76.80 & 75.60 & 78.80 & 78.80 & 85.20* \\
 & & eu & \textbf{58.00} & 50.20 & 52.40 & 53.80 & \textbf{70.00} & 51.00 & 50.20 & 50.00 & 50.60 & 51.00 & 64.40 & 60.00 & 74.20* \\ \hhline{===|====|========|=}
\multirow{4}{*}{Linguistic Acceptability} & cola   & en & 0.03 & 0.02 & 0.02 & \textbf{0.18} & 0.35 & 0.29 & 0.40 & 0.38 & 0.01 & 0.48 & \textbf{0.56}* & 0.44 & 0.50 \\ \cline{2-16}
 & escola            & es & 0.04 & 0.03 & 0.01 & \textbf{0.07} & 0.28 & 0.12 & 0.27 & 0.25 & 0.00 & 0.17 & \textbf{0.36}* & 0.30 & 0.32 \\ \cline{2-16}
 & catcola           & ca & \textbf{0.02} & -0.01 & -0.01 & \textbf{0.02} & \textbf{0.27} & 0.04 & 0.21 & 0.08 & -0.01 & 0.12 & 0.24 & 0.15 & 0.38* \\ \cline{2-16}
 & galcola           & gl & 0.01 & -0.02 & 0.03 & \textbf{0.06} & 0.16 & 0.05 & 0.20 & 0.11 & 0.02 & 0.17 & \textbf{0.31}* & 0.20 & 0.17 \\ \hhline{===|====|========|=}
\multirow{5}{*}{Math} & \multirow{5}{*}{mgsm} & en & 0.00   & 0.00  & 0.00   & 0.00  & 0.00   & 0.00  & 0.00   & 0.00  & 0.00  & 0.00   & 0.00  & 0.00  & 0.00  \\
 & & es & 0.02 & 0.04 & 0.00 & \textbf{0.07} & 0.07 & 0.06 & 0.09 & 0.07 & 0.00 & 0.09 & \textbf{0.27}* & 0.12 & 0.12 \\
 & & ca & \textbf{0.04} & 0.02 & 0.00 & \textbf{0.04} & 0.06 & 0.06 & 0.07 & 0.05 & 0.00 & 0.07 & \textbf{0.22}* & 0.10 & 0.11 \\
 & & eu & 0.03 & 0.03 & 0.02 & \textbf{0.04} & 0.06 & 0.03 & 0.03 & 0.02 & 0.01 & 0.03 & \textbf{0.18}* & 0.08 & 0.11 \\
 & & gl & 0.04 & 0.03 & 0.00 & \textbf{0.05} & 0.07 & 0.05 & 0.09 & 0.06 & 0.00 & 0.06 & \textbf{0.23}* & 0.10 & 0.11 \\ \hhline{===|====|========|=}
\multirow{12}{*}{NLI} & \multirow{4}{*}{wnli} & en & 52.11 & 50.70 & \textbf{54.93} & 53.52 & 46.48 & 46.48 & 56.34 & 59.15 & 52.11 & 66.20 & \textbf{80.28}* & 63.38 & 61.97 \\
 & & ca & 56.34 & \textbf{60.56} & 53.52 & 56.34 & 56.34 & 56.34 & 64.79 & 56.34 & 54.93 & 57.75 & \textbf{76.06}* & 63.38 & 60.56 \\
 & & es & 56.34 & 56.34 & 54.93 & \textbf{63.38} & 42.25 & 61.97 & 52.11 & 60.56 & 50.70 & 59.15 & \textbf{81.69}* & 69.01 & 60.56 \\
 & & eu & 43.66 & \textbf{52.11} & 42.25 & 46.48 & \textbf{60.56}* & 56.34 & 43.66 & 45.07 & 53.52 & 39.44 & 50.70 & 53.52 & 54.93  \\ \cline{2-16}
 & \multirow{5}{*}{xnli} & en & 46.47 & 47.55 & 43.65 & \textbf{50.80} & 48.03 & 49.16 & 52.09 & 53.13 & 50.52 & 52.65 & \textbf{53.78} & 50.36 & 51.77 \\
 & & ca & 48.15 & 45.22 & \textbf{48.88} & 48.71 & 49.04 & 47.63 & 48.84 & \textbf{52.73}* & 50.24 & 49.64 & 51.53 & 49.68 & 49.84 \\
 & & es & 44.74 & 43.86 & 46.75 & \textbf{47.79} & 46.22 & 43.65 & 46.59 & 48.31 & 47.87 & 46.63 & \textbf{49.20}* & 48.07 & 48.31 \\
 & & eu & \textbf{42.03} & 34.30 & 34.94 & 38.00 & 47.02 & 34.94 & 33.49 & 36.55 & 36.39 & 34.46 & \textbf{49.76}* & 40.58 & 46.54 \\
 & & gl &        &       &        &       &        &       &        &       &       &        &       &       &       \\ \cline{2-16}
 & teca              & ca & 44.54 & 42.18 & 42.99 & \textbf{50.64} & 52.39 & 46.76 & 52.20 & 55.13 & 49.79 & 54.04 & \textbf{55.31} & 53.33 & 54.27 \\ \cline{2-16}
 & qnlieu            & eu & 51.26 & 50.42 & 51.26 & \textbf{53.36} & 52.52 & 50.84 & 51.26 & 50.00 & 50.42 & 47.90 & \textbf{53.36} & 52.52 & 55.88* \\ \cline{2-16}
 & assin\_entailment & pt & 69.23 & \textbf{70.23} & 52.15 & 63.85 & 65.83 & 62.30 & \textbf{72.85}* & 70.30 & 53.02 & 72.62 & 71.50 & 58.00 & 71.70 \\ \hhline{===|====|========|=}
\multirow{7}{*}{Paraphrasing} & \multirow{4}{*}{paws} & en & 56.50 & 56.95 & 53.60 & \textbf{65.40} & 62.65 & 60.00 & 64.20 & 71.00 & 58.55 & 70.50 & \textbf{73.45}* & 65.75 & 64.65 \\
 & & ca & 57.05 & 54.90 & 53.55 & \textbf{64.15} & 64.60 & 62.85 & 69.55 & \textbf{72.95}* & 60.75 & 71.30 & 72.35 & 68.70 & 67.35 \\
 & & es & 55.95 & 54.65 & 54.40 & \textbf{59.45} & 61.15 & 62.25 & 63.60 & 69.45 & 57.50 & 66.75 & \textbf{70.40}* & 64.50 & 67.50 \\
 & & gl & 54.85 & 51.75 & 54.35 & \textbf{59.70} & 60.45 & 63.50 & \textbf{69.55} & 68.45 & 55.20 & 68.35 & 69.50 & 67.20 & 69.10 \\ \cline{2-16}
 & parafraseja       & ca & 61.35 & 57.70 & 59.23 & \textbf{65.08} & 64.72 & 63.50 & 65.88 & \textbf{67.50}* & 62.38 & 66.62 & 67.42 & 67.00 & 64.33 \\ \cline{2-16}
 & parafrases\_gl    & gl & 56.80 & 54.76 & 54.42 & \textbf{58.84} & 55.78 & 54.08 & 57.14 & 59.18 & 58.16 & 53.74 & \textbf{61.56}* & 57.82 & 60.20 \\ \cline{2-16}
 & assin\_paraphrase & pt & 70.03 & \textbf{71.38} & 68.08 & 67.60 & 65.83 & 64.58 & \textbf{69.77}* & 65.67 & 69.17 & 64.80 & 64.10 & 64.65 & 62.58 \\ \hhline{===|====|========|=}
\multirow{21}{*}{QA} & \multirow{4}{*}{openbookqa} & en & 28.00 & 29.00 & 19.80 & \textbf{32.40} & 35.40 & 36.00 & 36.40 & 33.20 & 31.20 & 37.60 & \textbf{37.80}* & 37.40 & 37.80* \\
 & & es &        &       &        &       &        &       &        &       &       &        &       &       &       \\
 & & ca & \textbf{29.40} & 24.20 & 25.20 & 27.80 & \textbf{39.20}* & 28.00 & 35.60 & 31.00 & 33.40 & 35.40 & 35.00 & 33.60 & 38.40 \\
 & & gl & 25.40 & 27.80 & 23.80 & \textbf{28.60} & 34.40 & 28.80 & \textbf{35.20}* & 29.00 & 27.60 & 31.60 & 33.80 & 33.00 & 35.00 \\ \cline{2-16}
 & \multirow{3}{*}{xquad} & en & 64.87 & 63.78 & 43.71 & \textbf{78.26} & 79.11 & nan & 81.68 & 80.02 & 69.23 & 81.92 & \textbf{83.73}* & 82.89 & 81.53 \\
 & & es & 57.59 & 57.45 & 44.58 & \textbf{68.96} & 72.00 & 67.93 & \textbf{78.18}* & 75.54 & 63.66 & 76.17 & 77.79 & 76.81 & 74.03 \\
 & & ca & 57.81 & 51.72 & 42.94 & \textbf{67.20} & 71.84 & 65.71 & 77.01 & 74.02 & 59.59 & 75.24 & \textbf{77.29} & 76.36 & 75.43 \\ \cline{2-16}
 & \multirow{3}{*}{piqa} & en & 73.61 & 73.23 & 63.66 & \textbf{78.78} & 79.71 & 76.99 & 80.47 & 79.16 & 73.61 & 80.96 & \textbf{81.72} & 80.63 & 81.77* \\
 & & ca & 63.82 & 60.28 & \textbf{63.87} & 62.02 & \textbf{71.22} & 60.94 & 69.04 & 64.25 & 70.51 & 65.67 & 70.62 & 65.78 & 74.86* \\
 & & eu &        &       &        &       &        &       &        &       &       &        &       &       &       \\ \cline{2-16}
 & \multirow{2}{*}{arc\_easy} & en & 72.14 & 71.30 & 55.77 & \textbf{81.36} & 81.36 & 78.49 & 84.30 & 80.26 & 69.44 & 83.46 & \textbf{87.33}* & 84.76 & 85.40 \\
 & & ca & 55.30 & 50.72 & 51.39 & \textbf{56.19} & 72.39 & 55.26 & 73.27 & 65.07 & 59.68 & 69.02 & \textbf{76.68} & 67.63 & 78.87* \\ \cline{2-16}
 & \multirow{2}{*}{arc\_challenge} & en & 35.41 & 35.58 & 24.49 & \textbf{49.15} & 52.82 & 47.35 & 55.38 & 48.89 & 35.24 & 55.55 & \textbf{63.57}* & 53.92 & 58.70 \\
 & & ca & 27.65 & 27.05 & 27.13 & \textbf{33.70} & 45.82 & 32.68 & 48.72 & 40.10 & 33.53 & 43.00 & \textbf{52.13}* & 40.70 & 51.62 \\ \cline{2-16}
 & \multirow{2}{*}{siqa} & en & 44.78 & 45.04 & 38.95 & \textbf{51.69} & 50.31 & 48.87 & 54.55 & 51.94 & 45.39 & 53.74 & \textbf{55.83}* & 53.38 & 53.48 \\
 & & ca & \textbf{43.04} & 39.97 & 38.89 & 42.12 & \textbf{50.20} & 41.66 & 48.11 & 46.16 & 47.34 & 46.78 & 48.36 & 47.54 & 53.07* \\ \cline{2-16}
 & catalanqa         & ca & 67.19 & 62.61 & 54.53 & \textbf{74.85} & 81.85 & 76.25 & 85.22 & 82.75 & 73.42 & 82.48 & \textbf{85.86}* & 85.25 & 84.41 \\ \cline{2-16}
 & coqcat            & ca & 60.56 & 51.43 & 48.44 & \textbf{66.91} & 75.37 & 65.38 & 75.96 & 72.97 & 65.95 & 74.82 & \textbf{79.06}* & 78.65 & 78.68 \\ \cline{2-16}
 & eus\_exams\_eu    & eu & 26.11 & 25.64 & 25.92 & \textbf{33.53} & 40.48 & 31.72 & 31.16 & 31.88 & 24.82 & 31.49 & \textbf{51.20} & 44.33 & 55.12* \\ \cline{2-16}
 & eus\_proficiency  & eu & 24.09 & 24.28 & 23.87 & \textbf{25.44} & 34.88 & 25.25 & 26.48 & 24.59 & 24.09 & 25.81 & \textbf{40.47} & 32.83 & 54.25* \\ \cline{2-16}
 & eus\_trivia       & eu & 28.05 & 28.80 & 28.45 & \textbf{34.58} & 48.16 & 33.88 & 37.32 & 34.58 & 27.06 & 34.52 & \textbf{52.30} & 44.02 & 63.62* \\ \hhline{===|====|========|=}
\multirow{7}{*}{Reading Comprehension} & \multirow{5}{*}{belebele} & en & 22.44 & 23.67 & 26.78 & \textbf{71.89} & 49.89 & 64.89 & 77.33 & 70.67 & 30.11 & 84.00 & \textbf{92.11}* & 87.44 & 80.67 \\
 & & es & 23.78 & 22.67 & 22.22 & \textbf{64.22} & 48.33 & 58.67 & 73.56 & 69.56 & 26.78 & 75.67 & \textbf{87.56}* & 82.00 & 74.89 \\
 & & ca & 23.22 & 23.67 & 22.22 & \textbf{58.89} & 49.00 & 55.56 & 72.22 & 67.56 & 28.56 & 75.56 & \textbf{86.89}* & 80.67 & 78.44 \\
 & & eu & 23.89 & 26.00 & 23.44 & \textbf{41.89} & 44.89 & 37.78 & 37.00 & 35.89 & 22.11 & 36.11 & \textbf{80.78}* & 61.56 & 70.33 \\
 & & gl & 22.44 & 23.44 & 22.11 & \textbf{60.67} & 48.00 & 55.89 & 73.78 & 61.89 & 28.44 & 68.11 & \textbf{86.33}* & 80.89 & 75.67 \\
 & & pt & 23.11 & 25.33 & 26.67 & \textbf{62.89} & 46.56 & 58.00 & 73.89 & 68.00 & 28.78 & 78.22 & \textbf{88.33}* & 83.44 & 76.33 \\ \cline{2-16}
 & eus\_reading & eu & \textbf{28.41} & 27.27 & 26.99 & 27.56 & 36.65 & 27.84 & 31.82 & 26.99 & 27.27 & 28.69 & nan & \textbf{45.45} & 52.56* 
 \\ \hhline{===|====|========|=}
\multirow{3}{*}{Summarization} & cabreu            & ca & \textbf{22.84} & 18.16 & 12.42 & 12.70 & 26.09 & 21.28 & 26.11 & 26.66 & 15.70 & \textbf{28.10}* & 14.43 & 26.66 & 24.95 \\ \cline{2-16}
 & xlsum\_es         & es & 0.81 & 1.82 & 0.85 & \textbf{1.81} & 3.86 & 1.27 & 5.52 & \textbf{5.62}* & 1.77 & 5.47 & 3.48 & 5.04 & 3.65  \\ \cline{2-16}
 & summarization\_gl & gl & 3.07 & 4.85 & 3.07 & \textbf{6.33} & 6.04 & 4.44 & 8.94 & 9.32 & 4.65 & \textbf{9.66}* & 8.84 & 8.55 & 7.39  \\ \hhline{===|====|========|=}
\multirow{7}{*}{Translation} & \multirow{5}{*}{flores} & es & 20.05 & 19.77 & 12.32 & \textbf{20.62} & 23.58 & 21.26 & 23.65 & 21.96 & 18.60 & 20.05 & \textbf{24.61} & 22.90 & 25.12* \\
 & & ca & \textbf{24.93} & 21.29 & 15.26 & 23.88 & 30.69 & 22.76 & 29.78 & 25.33 & 24.39 & 25.27 & \textbf{30.76} & 27.91 & 32.97* \\
 & & eu & \textbf{8.96} & 1.26 & 0.77 & 6.43 & \textbf{16.61} & 5.18 & 4.36 & 3.75 & 4.03 & 2.63 & 15.93 & 13.25 & 19.85* \\
 & & gl & \textbf{22.38} & 20.41 & 9.10 & 21.98 & 27.83 & 21.14 & 27.59 & 21.68 & 17.14 & 18.71 & \textbf{28.35} & 25.75 & 30.19* \\
 & & pt & 25.62 & 25.74 & 9.21 & \textbf{26.96} & 31.25 & 28.30 & 32.23 & 26.39 & 19.68 & 26.30 & \textbf{32.72} & 30.00 & 33.85* \\ \cline{2-16}
 & phrases\_va       & ca & 78.91 & 78.78 & \textbf{80.02} & 78.09 & 90.96 & 79.09 & 87.28 & 81.10 & \textbf{91.18} & 82.50 & 86.34 & 85.22 & 94.13* \\ \cline{2-16}
 & phrases\_es       & es & \textbf{67.72} & 61.39 & 65.78 & 60.75 & 73.47 & 59.88 & 70.21 & 64.75 & \textbf{75.18} & 64.41 & 69.00 & 66.47 & 75.64* \\ \hhline{===|====|========|=}
\multirow{6}{*}{Truthfulness} & \multirow{2}{*}{truthfulqa\_gen} & en & 23.49 & 25.31 & 0.25 & \textbf{28.74} & 25.98 & 18.27 & \textbf{35.31}* & 26.08 & 21.64 & 31.95 & 33.28 & 21.03 & 28.18 \\
 & & gl & 18.86 & 18.42 & 6.41 & \textbf{20.88} & 20.76 & 12.92 & 23.21 & 21.57 & 18.29 & 18.95 & \textbf{23.29}* & 18.91 & 20.97 \\ \cline{2-16}
 & \multirow{2}{*}{truthfulqa\_mc1} & en & 23.13 & 22.52 & \textbf{25.34} & 24.11 & 27.78 & 22.77 & \textbf{33.29}* & 26.56 & 22.15 & 28.15 & 29.87 & 28.52 & 27.29 \\
 & & gl & 22.52 & 24.24 & 22.89 & \textbf{26.44} & 25.21 & 25.34 & \textbf{29.25}* & 24.24 & 23.99 & 22.28 & 24.85 & 27.54 & 26.93 \\ \cline{2-16}
 & \multirow{2}{*}{truthfulqa\_mc2} & en & 37.37 & 36.35 & \textbf{42.61} & 36.24 & 42.13 & 37.72 & \textbf{48.50}* & 40.31 & 35.93 & 42.58 & 45.39 & 45.19 & 40.20 \\
 & & gl & 32.89 & \textbf{34.54} & 33.42 & 33.78 & 34.76 & 34.80 & \textbf{40.08} & 33.33 & 33.00 & 31.56 & 34.07 & 38.25 & 37.39 \\
 
\end{tabular}%
\end{adjustbox}
\caption{}
\label{tab:base-models-results}
\end{table}
\end{comment}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{l|ll|
>{\columncolor[HTML]{EFEFEF}}l lll|
>{\columncolor[HTML]{EFEFEF}}l lllllll|
>{\columncolor[HTML]{EFEFEF}}l }

\multicolumn{1}{c}{\textbf{Category}} &
  \multicolumn{1}{c}{\textbf{Task}} &
  \multicolumn{1}{c}{\textbf{Lang.}} &
  \begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
  \begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
  \begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
  \begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
  \begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
  \begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
  \begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
  \begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
  \begin{turn}{90}\textbf{Occiglot-eu5 7B}\end{turn} &
  \begin{turn}{90}\textbf{Teuken 7B}\end{turn} &
  \begin{turn}{90}\textbf{Salamandra 40B}\end{turn} \\ \hhline{===|====|========|=}
  
  
  
\multirow{9}{*}{Commonsense Reasoning} & \multirow{5}{*}{xstorycloze} & en & 71.81 & 72.07 & 61.35 & \textbf{79.75} & 79.09 & 80.41 & 73.59 & \textbf{83.59}* & 81.07 & 82.20 & 80.34 & 77.70 & 82.20	\\	
 &  & es & 64.73 & 65.59 & 63.93 & \textbf{69.89} & 74.32 & 74.45 & 69.76 & \textbf{76.90} & 73.73 & 71.14 & 75.84 & 71.21 & 78.89*	\\	
 &  & ca & \textbf{66.38} & 60.82 & 64.13 & 64.53 & 75.51 & 72.47 & 70.88 & \textbf{75.51} & 72.53 & 70.28 & 69.95 & 64.73 & 78.09*	\\	
 &  & eu & \textbf{58.97} & 49.57 & 49.37 & 53.21 & \textbf{66.12} & 50.36 & 53.14 & 62.41 & 56.92 & 50.63 & 51.22 & 52.42 & 70.75*	\\	
 &  & gl & \textbf{64.99} & 62.48 & 50.89 & 62.94 & \textbf{74.12}* & 71.94 & 60.69 & 71.54 & 67.84 & 58.44 & 64.53 & 63.67 & --	\\	\cline{2-16}
 & \multirow{4}{*}{copa} & en & 83.00 & 77.00 & 66.00 & \textbf{89.00} & 91.00* & 93.00 & 84.00 & \textbf{94.00}* & \textbf{94.00} & \textbf{94.00}* & 91.00 & 92.00 & 91.00	\\	
 &  & es & 72.80 & 74.20 & 71.40 & \textbf{77.60} & 86.00 & 84.80 & 79.00 & \textbf{86.40}* & 81.00 & 78.40 & 85.20 & 80.20 & --	\\	
 &  & ca & \textbf{70.20} & 65.20 & 66.40 & 67.20 & \textbf{84.00} & 80.80 & 76.80 & 78.80 & 78.80 & 75.60 & 70.60 & 65.20 & 85.20*	\\	
 &  & eu & \textbf{58.00} & 50.20 & 52.40 & 53.80 & \textbf{70.00} & 50.20 & 50.60 & 64.40 & 60.00 & 51.00 & 50.00 & 51.00 & 74.20*	\\	\hhline{===|====|========|=}
\multirow{4}{*}{Linguistic Acceptability} & cola & en & 0.03 & 0.02 & 0.02 & \textbf{0.18} & 0.41 & 0.40 & 0.01 & \textbf{0.56}* & 0.44 & 0.48 & 0.38 & 0.29 & 0.50	\\	\cline{2-16}
 & escola & es & 0.04 & 0.03 & 0.01 & \textbf{0.07} & 0.31 & 0.27 & 0.00 & \textbf{0.36}* & 0.30 & 0.17 & 0.25 & 0.12 & 0.32	\\	\cline{2-16}
 & catcola & ca & \textbf{0.02} & -0.01 & -0.01 & \textbf{0.02} & \textbf{0.37} & 0.21 & -0.01 & 0.24 & 0.15 & 0.12 & 0.08 & 0.04 & 0.38*	\\	\cline{2-16}
 & galcola & gl & 0.01 & -0.02 & 0.03 & \textbf{0.06} & 0.16 & 0.15 & 0.02 & \textbf{0.31}* & 0.20 & 0.17 & 0.11 & 0.05 & 0.17	\\	\hhline{===|====|========|=}
\multirow{4}{*}{Math} & \multirow{4}{*}{mgsm} & es & 0.02 & 0.04 & 0.00 & \textbf{0.07} & 0.06 & 0.09 & 0.00 & \textbf{0.27}* & 0.12 & 0.09 & 0.07 & 0.06 & 0.12	\\	
 &  & ca & \textbf{0.04} & 0.02 & 0.00 & \textbf{0.04} & 0.08 & 0.07 & 0.00 & \textbf{0.22}* & 0.10 & 0.07 & 0.05 & 0.06 & 0.11	\\	
 &  & eu & 0.03 & 0.03 & 0.02 & \textbf{0.04} & 0.08 & 0.03 & 0.01 & \textbf{0.18}* & 0.08 & 0.03 & 0.02 & 0.03 & 0.11	\\	
 &  & gl & 0.04 & 0.03 & 0.00 & \textbf{0.05} & 0.06 & 0.09 & 0.00 & \textbf{0.23}* & 0.10 & 0.06 & 0.06 & 0.05 & 0.11	\\	\hhline{===|====|========|=}
\multirow{12}{*}{NLI} & \multirow{4}{*}{wnli} & en & 52.11 & 50.70 & \textbf{54.93} & 53.52 & 56.34 & 56.34 & 52.11 & \textbf{80.28}* & 63.38 & 66.20 & 59.15 & 46.48 & 61.97	\\	
 &  & ca & 56.34 & \textbf{60.56} & 53.52 & 56.34 & 59.15 & 64.79 & 54.93 & \textbf{76.06}* & 63.38 & 57.75 & 56.34 & 56.34 & 60.56	\\	
 &  & es & 56.34 & 56.34 & 54.93 & \textbf{63.38} & 59.15
 & 52.11 & 50.70 & \textbf{81.69}* & 69.01 & 59.15 & 60.56 & 61.97 & 60.56	\\	
 &  & eu & 43.66 & \textbf{52.11} & 42.25 & 46.48 & \textbf{57.75}* & 43.66 & 53.52 & 50.70 & 53.52 & 39.44 & 45.07 & 56.34 & 54.93	\\	\cline{2-16}
 & \multirow{5}{*}{xnli} & en & 46.47 & 47.55 & 43.65 & \textbf{50.80} & 50.00 & 52.09 & 50.52 & \textbf{53.78}* & 50.36 & 52.65 & 53.13 & 49.16 & 51.77	\\	
 &  & ca & 48.15 & 45.22 & \textbf{48.88} & 48.71 & 50.16 & 48.84 & 50.24 & 51.53 & 49.68 & 49.64 & \textbf{52.73}* & 47.63 & 49.84	\\	
 &  & es & 44.74 & 43.86 & 46.75 & \textbf{47.79} & \textbf{46.59} & 46.59 & 47.87 & \textbf{49.20}* & 48.07 & 46.63 & 48.31 & 43.65 & 48.31	\\
 &  & eu & \textbf{42.03} & 34.30 & 34.94 & 38.00 & 43.51 & 33.49 & 36.39 & \textbf{49.76}* & 40.58 & 34.46 & 36.55 & 34.94 & 46.54	\\	
 &  & gl & 47.35 & 46.20 & 42.66 & \textbf{48.45} & 50.95 & 50.73 & 45.68 & \textbf{52.88}* & 49.19 & 46.20 & 50.47 & 45.86 & -- \\	\cline{2-16}
 & teca & ca & 44.54 & 42.18 & 42.99 & \textbf{50.64} & 51.91 & 52.20 & 49.79 & \textbf{55.31}* & 53.33 & 54.04 & 55.13 & 46.76 & 54.27	\\	\cline{2-16}
 & qnlieu & eu & 51.26 & 50.42 & 51.26 & \textbf{53.36} & 51.26 & 51.26 & 50.42 & \textbf{53.36} & 52.52 & 47.90 & 50.00 & 50.84 & 55.88*	\\	\cline{2-16}
 & assin\_entailment & pt & 69.23 & \textbf{70.23} & 52.15 & 63.85 & 67.00 & \textbf{72.85}* & 53.02 & 71.50 & 58.00 & 72.62 & 70.30 & 62.30 & 71.70	\\	\hhline{===|====|========|=}
\multirow{7}{*}{Paraphrasing} & \multirow{4}{*}{paws} & en & 56.50 & 56.95 & 53.60 & \textbf{65.40} & 64.05 & 64.20 & 58.55 & \textbf{73.45}* & 65.75 & 70.50 & 71.00 & 60.00 & 64.65	\\	
 &  & ca & 57.05 & 54.90 & 53.55 & \textbf{64.15} & 67.45 & 69.55 & 60.75 & 72.35 & 68.70 & 71.30 & \textbf{72.95}* & 62.85 & 67.35	\\	
 &  & es & 55.95 & 54.65 & 54.40 & \textbf{59.45} & 60.30 & 63.60 & 57.50 & \textbf{70.40}* & 64.50 & 66.75 & 69.45 & 62.25 & 67.50	\\	
 &  & gl & 54.85 & 51.75 & 54.35 & \textbf{59.70} & 63.20 & \textbf{69.55} & 55.20 & 69.50 & 67.20 & 68.35 & 68.45 & 63.50 & 69.10	\\	\cline{2-16}
 & parafraseja & ca & 61.35 & 57.70 & 59.23 & \textbf{65.08} & 65.83 & 65.88 & 62.38 & 67.42 & 67.00 & 66.62 & \textbf{67.50}* & 63.50 & 64.33	\\	\cline{2-16}
 & parafrases\_gl & gl & 56.80 & 54.76 & 54.42 & \textbf{58.84} & 54.42 & 57.14 & 58.16 & \textbf{61.56}* & 57.82 & 53.74 & 59.18 & 54.08 & 60.20	\\	\cline{2-16}
 & assin\_paraphrase & pt & 70.03 & \textbf{71.38} & 68.08 & 67.60 & 66.33 & \textbf{69.77}* & 69.17 & 64.10 & 64.65 & 64.80 & 65.67 & 64.58 & 62.58	\\	\hhline{===|====|========|=}
\multirow{21}{*}{QA} & \multirow{4}{*}{openbookqa} & en & 28.00 & 29.00 & 19.80 & \textbf{32.40} & 35.40 & 36.00 & 31.20 & \textbf{37.80}* & 37.40 & 37.60 & 33.20 & 36.00 & 37.80*	\\	
 &  & es & 31.40 & 30.60 & 26.40 & \textbf{34.80} & \textbf{41.60}* & 40.60 & 31.40 & 41.00 & 38.80 & 38.00 & 38.40 & 39.20 & --	\\	
 &  & ca & \textbf{29.40} & 24.20 & 25.20 & 27.80 & \textbf{38.80}* & 35.60 & 33.40 & 35.00 & 33.60 & 35.40 & 31.00 & 28.00 & 38.40	\\	
 &  & gl & 25.40 & 27.80 & 23.80 & \textbf{28.60} & 34.40 & \textbf{35.20}* & 27.60 & 33.80 & 33.00 & 31.60 & 29.00 & 28.80 & 35.00	\\	\cline{2-16}
 & \multirow{3}{*}{xquad} & en & 64.87 & 63.78 & 43.71 & \textbf{78.26} & 77.74 & 81.68 & 69.23 & \textbf{83.73}* & 82.89 & 81.92 & 80.02 & nan & 81.53	\\	
 &  & es & 57.59 & 57.45 & 44.58 & \textbf{68.96} & 72.26 & \textbf{78.18}* & 63.66 & 77.79 & 76.81 & 76.17 & 75.54 & 67.93 & 74.03	\\	% TODO[julia]: negritos
 &  & ca & 57.81 & 51.72 & 42.94 & \textbf{67.20} & 72.84 & 77.01 & 59.59 & \textbf{77.29}* & 76.36 & 75.24 & 74.02 & 65.71 & 75.43	\\	\cline{2-16}
 & \multirow{3}{*}{piqa} & en & 73.61 & 73.23 & 63.66 & \textbf{78.78} & 80.03 & 80.47 & 73.61 & \textbf{81.72} & 80.63 & 80.96 & 79.16 & 76.99 & 81.77*	\\	
 &  & ca & 63.82 & 60.28 & \textbf{63.87} & 62.02 & \textbf{71.27} & 69.04 & 70.51 & 70.62 & 65.78 & 65.67 & 64.25 & 60.94 & 74.86*	\\	
 &  & eu & \textbf{56.86} & 54.52 & 52.56 & 53.21 & \textbf{63.67}* & 53.59 & 53.59 & 59.75 & 55.88 & 53.32 & 54.58 & 53.92 & -- \\	\cline{2-16}
 & \multirow{2}{*}{arc\_easy} & en & 72.14 & 71.30 & 55.77 & \textbf{81.36} & 82.2 & 84.30 & 69.44 & \textbf{87.33}* & 84.76 & 83.46 & 80.26 & 78.49 & 85.40	\\	
 &  & ca & 55.30 & 50.72 & 51.39 & \textbf{56.19} & 71.72 & 73.27 & 59.68 & \textbf{76.68} & 67.63 & 69.02 & 65.07 & 55.26 & 78.87*	\\	\cline{2-16}
 & \multirow{2}{*}{arc\_challenge} & en & 35.41 & 35.58 & 24.49 & \textbf{49.15} & 52.82 & 55.38 & 35.24 & \textbf{63.57}* & 53.92 & 55.55 & 48.89 & 47.35 & 58.70	\\	
 &  & ca & 27.65 & 27.05 & 27.13 & \textbf{33.70} & 45.56 & 48.72 & 33.53 & \textbf{52.13}* & 40.70 & 43.00 & 40.10 & 32.68 & 51.62	\\	\cline{2-16}
 & \multirow{2}{*}{siqa} & en & 44.78 & 45.04 & 38.95 & \textbf{51.69} & 50.31 & 54.55 & 45.39 & \textbf{55.83}* & 53.38 & 53.74 & 51.94 & 48.87 & 53.48	\\	
 &  & ca & \textbf{43.04} & 39.97 & 38.89 & 42.12 & \textbf{49.85} & 48.11 & 47.34 & 48.36 & 47.54 & 46.78 & 46.16 & 41.66 & 53.07*	\\	\cline{2-16}
 & catalanqa & ca & 67.19 & 62.61 & 54.53 & \textbf{74.85} & 82.6 & 85.22 & 73.42 & \textbf{85.86}* & 85.25 & 82.48 & 82.75 & 76.25 & 84.41	\\	\cline{2-16}
 & coqcat & ca & 60.56 & 51.43 & 48.44 & \textbf{66.91} & 76.15 & 75.96 & 65.95 & \textbf{79.06}* & 78.65 & 74.82 & 72.97 & 65.38 & 78.68	\\	\cline{2-16}
 & eus\_exams\_eu & eu & 26.11 & 25.64 & 25.92 & \textbf{33.53} & 41.04 & 31.16 & 24.82 & \textbf{51.20} & 44.33 & 31.49 & 31.88 & 31.72 & 55.12*	\\	\cline{2-16}
 & eus\_proficiency & eu & 24.09 & 24.28 & 23.87 & \textbf{25.44} & 39.72 & 26.48 & 24.09 & \textbf{40.47} & 32.83 & 25.81 & 24.59 & 25.25 & 54.25*	\\	\cline{2-16}
 & eus\_trivia & eu & 28.05 & 28.80 & 28.45 & \textbf{34.58} & 52.36 & 37.32 & 27.06 & \textbf{52.30} & 44.02 & 34.52 & 34.58 & 33.88 & 63.62*	\\	\hhline{===|====|========|=}
\multirow{7}{*}{Reading Comprehension} & \multirow{5}{*}{belebele} & en & 22.44 & 23.67 & 26.78 & \textbf{71.89} & 57.33 & 77.33 & 30.11 & \textbf{92.11}* & 87.44 & 84.00 & 70.67 & 64.89 & 80.67	\\	
 &  & es & 23.78 & 22.67 & 22.22 & \textbf{64.22} & 51.56 & 73.56 & 26.78 & \textbf{87.56}* & 82.00 & 75.67 & 69.56 & 58.67 & 74.89	\\	
 &  & ca & 23.22 & 23.67 & 22.22 & \textbf{58.89} & 53.78 & 72.22 & 28.56 & \textbf{86.89}* & 80.67 & 75.56 & 67.56 & 55.56 & 78.44	\\	
 &  & eu & 23.89 & 26.00 & 23.44 & \textbf{41.89} & 46.78 & 37.00 & 22.11 & \textbf{80.78}* & 61.56 & 36.11 & 35.89 & 37.78 & 70.33	\\	
 &  & gl & 22.44 & 23.44 & 22.11 & \textbf{60.67} & 52.89 & 73.78 & 28.44 & \textbf{86.33}* & 80.89 & 68.11 & 61.89 & 55.89 & 75.67	\\	
 &  & pt & 23.11 & 25.33 & 26.67 & \textbf{62.89} & 52.33 & 73.89 & 28.78 & \textbf{88.33}* & 83.44 & 78.22 & 68.00 & 58.00 & 76.33	\\	\cline{2-16}
 & eus\_reading & eu & \textbf{28.41} & 27.27 & 26.99 & 27.56 & 33.52 & 31.82 & 27.27 & nan & \textbf{45.45} & 28.69 & 26.99 & 27.84 & 52.56* \\ \hhline{===|====|========|=}
\multirow{3}{*}{Summarization} & cabreu & ca & \textbf{22.84} & 18.16 & 12.42 & 12.70 & 26.75 & 26.11 & 15.70 & 14.43 & 26.66 & \textbf{28.10}* & 26.66 & 21.28 & 24.95	\\	\cline{2-16}
 & xlsum\_es & es & 0.81 & 1.82 & 0.85 & \textbf{1.81} & 3.86 & 5.52 & 1.77 & 3.48 & 05.04 & 5.47 & \textbf{5.62}* & 1.27 & 3.65	\\	\cline{2-16}
 & summarization\_gl & gl & 03.07 & 4.85 & 03.07 & \textbf{6.33} & 4.89 & 8.94 & 4.65 & 8.84 & 8.55 & \textbf{9.66}* & 9.32 & 4.44 & 7.39	\\	\hhline{===|====|========|=}
\multirow{7}{*}{Translation} & \multirow{5}{*}{flores} & es & 20.05 & 19.77 & 12.32 & \textbf{20.62} & 23.43 & 23.65 & 18.60 & \textbf{24.61} & 22.90 & 20.05 & 21.96 & 21.26 & 25.12*	\\	
 &  & ca & \textbf{24.93} & 21.29 & 15.26 & 23.88 & 30.63 & 29.78 & 24.39 & \textbf{30.76} & 27.91 & 25.27 & 25.33 & 22.76 & 32.97*	\\	
 &  & eu & \textbf{8.96} & 1.26 & 0.77 & 6.43 & \textbf{16.95} & 4.36 & 04.03 & 15.93 & 13.25 & 2.63 & 3.75 & 5.18 & 19.85*	\\	
 &  & gl & \textbf{22.38} & 20.41 & 9.10 & 21.98 & 27.75 & 27.59 & 17.14 & \textbf{28.35} & 25.75 & 18.71 & 21.68 & 21.14 & 30.19*	\\	
 &  & pt & 25.62 & 25.74 & 9.21 & \textbf{26.96} & 30.32 & 32.23 & 19.68 & \textbf{32.72} & 30.00 & 26.30 & 26.39 & 28.30 & 33.85*	\\	\cline{2-16}
 & phrases\_va & ca & 78.91 & 78.78 & \textbf{80.02} & 78.09 & 91.60 & 87.28 & \textbf{91.18} & 86.34 & 85.22 & 82.50 & 81.10 & 79.09 & 94.13*	\\	\cline{2-16}
 & phrases\_es & es & \textbf{67.72} & 61.39 & 65.78 & 60.75 & 73.47 & 70.21 & \textbf{75.18} & 69.00 & 66.47 & 64.41 & 64.75 & 59.88 & 75.64*	\\	\hhline{===|====|========|=}
\multirow{6}{*}{Truthfulness} & \multirow{2}{*}{truthfulqa\_gen} & en & 23.49 & 25.31 & 0.25 & \textbf{28.74} & 28.6 & \textbf{35.31}* & 21.64 & 33.28 & 21.03 & 31.95 & 26.08 & 18.27 & 28.18	\\	
 &  & gl & 18.86 & 18.42 & 6.41 & 20.88 & \textbf{22.08} & 23.21 & 18.29 & \textbf{23.29}* & 18.91 & 18.95 & 21.57 & 12.92 & 20.97	\\	\cline{2-16}
 & \multirow{2}{*}{truthfulqa\_mc1} & en & 23.13 & 22.52 & \textbf{25.34} & 24.11 & 28.52 & \textbf{33.29}* & 22.15 & 29.87 & 28.52 & 28.15 & 26.56 & 22.77 & 27.29	\\	
 &  & gl & 22.52 & 24.24 & 22.89 & \textbf{26.44} & 23.26 & \textbf{29.25}* & 23.99 & 24.85 & 27.54 & 22.28 & 24.24 & 25.34 & 26.93	\\	\cline{2-16}
 & \multirow{2}{*}{truthfulqa\_mc2} & en & 37.37 & 36.35 & \textbf{42.61} & 36.24 & 42.69 & \textbf{48.50}* & 35.93 & 45.39 & 45.19 & 42.58 & 40.31 & 37.72 & 40.20	\\	
 &  & gl & 32.89 & \textbf{34.54} & 33.42 & 33.78 & 34.19
 & \textbf{40.08} & 33.00 & 34.07 & 38.25 & 31.56 & 33.33 & 34.80 & 37.39	\\	
  
   
\end{tabular}%
\end{adjustbox}
\caption{Results of \textit{base} models to LM Evaluation Harness tasks.}
\label{tab:base-models-results}
\end{table}

\subsubsection{Instructed Model Performance} \label{it_perf}

In Table \ref{tab:instruct-models-results}, the instructed Salamandra 2B and 7B models (both v1.1) are evaluated against the instructed variants of the models included in Table \ref{tab:base-models-results}. Tables \ref{tab:llm-judge-es} to \ref{tab:llm-judge-it} present the results of instructed models using our \llmJudge{} setup. In these tables, the first number in each cell corresponds to the mean score of the assistant, while the second one is the mean variance across the prompt styles (lower means the assistant is more robust). A dash is present when the assistant was not successfully evaluated in more than 90\% of the queries. This mostly happens in the extreme summarization task, where some of the assistants did not have enough context length to fit the whole query. In other cases, an assistant gave totally unrelated answers, and the judge model refused to evaluate them.

We must highlight that the \SalamandraInstructed{} models, unlike others included in our comparison, have not yet undergone any additional post-training processes, which typically involve multiple rounds of refinement to better align model outputs with user expectations. This is ongoing work, and we anticipate significant performance improvements as we incorporate these techniques into our models. In addition, it is important to note that these evaluations were conducted in a 5-shot setup to maintain consistency throughout this report. It is our intention to include 0-shot evaluations in the near future, as this setup more closely matches the training paradigm and typical use of instructed models.

Compared to their non-instructed counterparts, Salamandra 2B and 7B instructed models show improved performance in nearly half of the tasks. This variability in improvement is also observed across the other model families. For Salamandra models, notable gains are seen in tasks such as NLI, paraphrasing, QA, and the Belebele reading comprehension task, where the improvements are particularly strong.

Conversely, there are significant performance drops in translation tasks, the generative truthfulness task (\emph{truthfulqa\_gen}) and certain generative QA tasks such as XQuAD and CatalanQA. The decrease in translation tasks is particularly worrying, as it causes Salamandra to slip from its leading position with respect to the base models in this domain. Such performance patterns require further investigation, especially in light of similar trends in other models such as Gemma-2 2B, which shows remarkable gains in some areas—for example, an increase from 0.18 to 0.48 in MCC for the English linguistic acceptability task.

Compared to other instructed models of similar size, the instructed Salamandra models largely preserve the performance trends of their base versions. However, a notable exception occurs in mathematical reasoning tasks, where all models exhibit significant performance declines. Interestingly, Salamandra models outperform Gemma-2 models in certain cases, despite Gemma-2 retaining its lead in the 2B range. In the 7B range, the dominance of Gemma-2 models becomes less pronounced, with Salamandra 7B, Llama 8B, and Mistral 7B following as the next best-performing models.

The results of the \llmJudge{} evaluation are mostly in agreement with those of the LM Evaluation Harness, with Gemma-2 9B and Mistral 7B taking the top spots for most tasks and languages in the 7B range. Again, Gemma-2 seems to dominate in the 2B range, with the notable exception of translation, where Salamandra 2B seems to perform on par with and even outperform it in some languages.

\begin{comment}
\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{l|ll|llll|llllllll}
\multicolumn{1}{c}{\textbf{Category}} &
  \multicolumn{1}{c}{\textbf{Task}} &
  \multicolumn{1}{c}{\textbf{Lang.}} &
  \begin{turn}{90}\textbf{salamandra-2b-it}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM-1.7B-Instruct}\end{turn} &
  \begin{turn}{90}\textbf{FLOR-1.3B-Instructed}\end{turn} &
  \begin{turn}{90}\textbf{gemma-2-2b-it}\end{turn} &
  \begin{turn}{90}\textbf{salamandra-7b-it}\end{turn} &
  \begin{turn}{90}\textbf{Teuken-7B-instruct-sigma-v0.5}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM-9B-Instruct}\end{turn} &
  \begin{turn}{90}\textbf{occiglot-7b-eu5-Instruct}\end{turn} &
  \begin{turn}{90}\textbf{FLOR-6.3B-Instructed}\end{turn} &
  \begin{turn}{90}\textbf{Mistral-7B-v0.3-Instruct-v0.3}\end{turn} &
  \begin{turn}{90}\textbf{gemma-2-9b-it}\end{turn} &
  \begin{turn}{90}\textbf{Llama-3.1-8B-Instruct}\end{turn} \\ \hhline{===|====|========}
\multirow{9}{*}{Commonsense Reasoning} & \multirow{5}{*}{xstorycloze} & en & 69.16 & 70.75 & 61.68 & \textbf{82.59} & 78.49 & 85.11 & 82.06 & 81.07 & 71.54 & \textbf{87.36} & 82.59 & 84.25 \\
 & & es & 64.33 & 64.46 & 64.53 & \textbf{71.74} & 73.13 & 76.44 & 74.39 & 73.46 & 64.73 & 74.39 & 77.04 & \textbf{77.50} \\
 & & ca & 64.26 & 61.88 & 64.59 & \textbf{66.51} & 73.73 & 68.76 & 71.87 & 69.69 & 66.64 & 73.40 & 74.78 & \textbf{75.12} \\
 & & eu & \textbf{56.25} & 49.97 & 49.64 & 51.69 & \textbf{65.06} & 51.89 & 51.36 & 51.16 & 51.56 & 49.83 & 64.20 & 60.36 \\
 & & gl &        &       &        &       &        &       &        &       &       &        &       &       \\ \cline{2-15}
 & \multirow{4}{*}{copa} & en & 77.00 & 73.00 & 69.00 & \textbf{88.00} & 93.00 & 94.00 & 92.00 & 92.00 & 77.00 & 93.00 & \textbf{94.00} & \textbf{94.00} \\
 & & es &        &       &        &       &        &       &        &       &       &        &       &       \\
 & & ca & \textbf{70.60} & 65.60 & 67.40 & 67.40 & \textbf{82.80} & 69.80 & 82.20 & 72.40 & 78.60 & 76.60 & 82.60 & 81.60 \\
 & & eu & \textbf{57.60} & 51.00 & 53.00 & 51.20 & \textbf{67.80} & 48.20 & 50.80 & 50.20 & 50.40 & 49.80 & 64.00 & 58.80 \\ \hhline{===|====|========}
\multirow{4}{*}{Linguistic Acceptability} & cola   & en & 0.01 & 0.03 & -0.02 & \textbf{0.47} & 0.39 & 0.23 & 0.47 & 0.35 & 0.00 & 0.46 & \textbf{0.61} & 0.46 \\ \cline{2-15}
 & escola            & es & 0.00 & 0.00 & -0.01 & \textbf{0.13} & 0.29 & 0.12 & 0.26 & 0.23 & -0.02 & 0.06 & \textbf{0.41} & 0.31 \\ \cline{2-15}
 & catcola           & ca & 0.08 & -0.02 & -0.03 & \textbf{0.13} & \textbf{0.31} & 0.05 & 0.26 & 0.07 & -0.01 & 0.07 & 0.29 & 0.14 \\ \cline{2-15}
 & galcola           & gl & 0.03 & 0.06 & 0.07 & \textbf{0.10} & 0.18 & 0.03 & 0.14 & 0.08 & 0.05 & 0.13 & \textbf{0.39} & 0.19 \\ \hhline{===|====|========}
\multirow{5}{*}{Math} & \multirow{5}{*}{mgsm} & en & 0.00   & 0.00  & 0.00   & 0.00  & 0.00   & 0.00  & 0.00   & 0.00  & 0.00  & 0.00   & 0.00  & 0.00  \\
 & & es & 0.01 & \textbf{0.02} & 0.00 & 0.00 & \textbf{0.04} & 0.02 & 0.03 & 0.00 & 0.01 & 0.02 & 0.00 & 0.03 \\
 & & ca & \textbf{0.02} & 0.00 & 0.01 & 0.00 & \textbf{0.04} & 0.03 & 0.01 & 0.00 & 0.01 & 0.01 & 0.00 & 0.02 \\
 & & eu & \textbf{0.02} & \textbf{0.02} & \textbf{0.02} & 0.00 & 0.04 & 0.02 & 0.03 & 0.03 & 0.01 & 0.00 & \textbf{0.10} & 0.04 \\
 & & gl & \textbf{0.02} & 0.01 & 0.00 & 0.00 & 0.03 & 0.01 & \textbf{0.04} & 0.00 & 0.01 & 0.02 & 0.02 & 0.02 \\ \hhline{===|====|========}
\multirow{12}{*}{NLI} & \multirow{4}{*}{wnli} & en & \textbf{54.93} & 53.52 & 50.70 & \textbf{54.93} & 59.15 & 50.70 & 60.56 & 64.79 & 49.30 & 73.24 & \textbf{80.28} & 76.06 \\
 & & ca & 56.34 & 56.34 & 53.52 & \textbf{63.38} & 64.79 & 59.15 & 67.61 & 59.15 & 56.34 & 43.66 & \textbf{81.69} & 66.20 \\
 & & es & 49.30 & 54.93 & 54.93 & \textbf{59.15} & 60.56 & 61.97 & 63.38 & 69.01 & 52.11 & 67.61 & \textbf{77.46} & 71.83 \\
 & & eu & 49.30 & 47.89 & 43.66 & \textbf{54.93} & 56.34 & 50.70 & 46.48 & 47.89 & 47.89 & 45.07 & 59.15 & \textbf{61.97}  \\ \cline{2-15}
 & \multirow{5}{*}{xnli} & en & 47.47 & 47.51 & 43.98 & \textbf{48.55} & 51.49 & \textbf{57.63} & 51.77 & 55.46 & 51.61 & 52.29 & 51.20 & 54.50 \\
 & & ca & \textbf{49.20} & 46.59 & 48.96 & 47.95 & 53.45 & \textbf{54.14} & 50.92 & 53.98 & 50.60 & 52.57 & 51.08 & 52.37 \\
 & & es & 42.65 & 43.65 & 45.94 & \textbf{48.11} & 50.84 & \textbf{52.29} & 46.71 & 49.24 & 48.19 & 48.80 & 47.07 & 48.71 \\
 & & eu & \textbf{41.06} & 33.01 & 34.14 & 34.78 & \textbf{47.34} & 38.33 & 33.01 & 35.43 & 37.52 & 33.98 & 45.09 & 42.51 \\
 & & gl &        &       &        &       &        &       &        &       &       &        &       &       \\ \cline{2-15}
 & teca              & ca & 45.44 & 41.14 & 43.08 & \textbf{47.14} & 54.79 & 53.28 & 52.39 & 55.88 & 50.68 & \textbf{56.83} & 52.10 & 54.42 \\ \cline{2-15}
 & qnlieu            & eu & 53.36 & 53.36 & 51.26 & \textbf{54.62} & 51.26 & 55.88 & 50.84 & 50.84 & 50.84 & 50.42 & \textbf{66.39} & 57.56 \\ \cline{2-15}
 & assin\_entailment & pt & \textbf{72.95} & 68.60 & 53.20 & 71.95 & 70.50 & 52.52 & 74.45 & 70.53 & 64.50 & \textbf{76.40} & 74.83 & 68.08 \\ \hhline{===|====|========}
\multirow{7}{*}{Paraphrasing} & \multirow{4}{*}{paws} & en & 56.30 & 54.95 & 52.50 & \textbf{63.60} & 63.40 & 71.15 & 68.75 & 70.25 & 61.45 & \textbf{73.65} & 67.00 & 69.80 \\
 & & ca & 55.85 & 54.90 & 53.15 & \textbf{64.90} & 64.35 & 68.65 & 72.45 & 69.60 & 64.60 & \textbf{73.60} & 70.20 & 72.75 \\
 & & es & 55.85 & 54.20 & 54.15 & \textbf{62.35} & 60.75 & 69.65 & 70.25 & 66.75 & 59.50 & \textbf{72.15} & 68.30 & 70.45 \\
 & & gl & 56.80 & 52.35 & 53.10 & \textbf{64.05} & 62.45 & 65.75 & 70.50 & 67.25 & 50.05 & 71.75 & \textbf{71.80} & 70.00 \\ \cline{2-15}
 & parafraseja       & ca & 59.60 & 59.13 & 59.58 & \textbf{63.45} & 64.15 & 68.00 & 67.75 & 66.55 & 66.90 & 68.55 & 66.12 & \textbf{69.85} \\ \cline{2-15}
 & parafrases\_gl    & gl & 54.76 & \textbf{57.48} & 52.38 & 57.14 & 58.50 & \textbf{65.31} & 59.86 & 57.48 & 53.40 & 62.24 & 59.52 & 60.20 \\ \cline{2-15}
 & assin\_paraphrase & pt & 72.05 & 70.28 & 69.95 & \textbf{72.88} & 70.00 & 62.52 & 68.30 & 66.25 & \textbf{71.50} & 66.55 & 67.58 & 67.22 \\ \hhline{===|====|========}
\multirow{21}{*}{QA} & \multirow{4}{*}{openbookqa} & en & 27.60 & 26.80 & 20.80 & \textbf{42.40} & 38.80 & 41.80 & 42.00 & 37.20 & 27.00 & 43.20 & \textbf{45.20} & 41.60 \\
 & & es &        &       &        &       &        &       &        &       &       &        &       &       \\
 & & ca & \textbf{29.20} & 26.40 & 27.60 & 27.20 & \textbf{40.60} & 32.20 & 39.40 & 31.00 & 30.80 & 38.00 & 38.20 & 36.40 \\
 & & gl & 23.60 & 27.60 & 24.00 & \textbf{29.00} & \textbf{37.20} & 32.00 & 36.40 & 28.80 & 26.60 & 31.40 & 35.60 & 33.60 \\ \cline{2-15}
 & \multirow{3}{*}{xquad} & en & 52.22 & 65.81 & 47.12 & \textbf{76.16} & 70.07 & 33.67 & 81.24 & 36.95 & 67.92 & 75.73 & 78.70 & \textbf{84.81} \\
 & & es & 43.31 & 55.98 & 48.20 & \textbf{66.81} & 63.20 & 23.68 & 71.07 & 37.61 & 57.54 & 71.34 & 72.17 & \textbf{74.31} \\
 & & ca & 48.73 & 52.60 & 50.88 & \textbf{63.55} & 67.17 & 36.13 & 67.00 & 37.02 & 61.33 & 69.22 & 71.58 & \textbf{72.31} \\ \cline{2-15}
 & \multirow{3}{*}{piqa} & en & 73.50 & 73.50 & 63.87 & \textbf{79.43} & 80.58 & 80.30 & 80.69 & 79.76 & 73.67 & 82.05 & \textbf{81.12} & 81.07 \\
 & & ca & \textbf{65.02} & 60.94 & 64.47 & 60.23 & \textbf{73.39} & 61.53 & 69.31 & 65.29 & 71.27 & 66.87 & 69.10 & 68.01 \\
 & & eu &        &       &        &       &        &       &        &       &       &        &       &       \\ \cline{2-15}
 & \multirow{2}{*}{arc\_easy} & en & 71.46 & 69.65 & 54.76 & \textbf{82.79} & 82.87 & 80.68 & 85.73 & 79.84 & 64.02 & 83.04 & \textbf{88.01} & 85.23 \\
 & & ca & 53.41 & 50.51 & 51.89 & \textbf{56.10} & 73.57 & 59.18 & 74.75 & 65.66 & 61.07 & 70.20 & \textbf{75.51} & 70.41 \\ \cline{2-15}
 & \multirow{2}{*}{arc\_challenge} & en & 37.29 & 36.09 & 25.85 & \textbf{52.13} & 54.52 & 57.08 & 56.74 & 50.60 & 32.51 & 60.75 & \textbf{66.04} & 59.47 \\
 & & ca & 28.50 & 26.88 & 26.79 & \textbf{35.84} & 45.90 & 40.10 & 48.98 & 41.21 & 32.51 & 45.56 & \textbf{52.30} & 44.97 \\ \cline{2-15}
 & \multirow{2}{*}{siqa} & en & 47.90 & 47.59 & 39.46 & \textbf{55.68} & 54.45 & 58.03 & 57.27 & 55.12 & 45.75 & 57.63 & \textbf{59.62} & 58.03 \\
 & & ca & 43.65 & 41.81 & 39.56 & \textbf{44.63} & 51.84 & 47.85 & 49.08 & 46.72 & 45.34 & \textbf{52.05} & 50.77 & 51.18 \\ \cline{2-15}
 & catalanqa         & ca & 61.17 & 60.50 & 65.94 & \textbf{70.36} & 78.05 & 42.20 & 72.98 & 45.79 & 76.30 & 75.24 & 79.83 & \textbf{80.55} \\ \cline{2-15}
 & coqcat            & ca & \textbf{62.15} & 52.60 & 48.18 & 53.72 & \textbf{74.17} & 33.67 & 52.05 & 51.53 & 4.93 & 68.75 & 69.53 & 67.32 \\ \cline{2-15}
 & eus\_exams\_eu    & eu & 25.41 & 25.22 & 24.86 & \textbf{35.08} & 45.98 & 30.97 & 30.07 & 32.03 & 25.21 & 31.61 & \textbf{49.57} & 45.13 \\ \cline{2-15}
 & eus\_proficiency  & eu & 24.78 & 25.01 & 23.72 & \textbf{26.68} & \textbf{43.92} & 25.42 & 26.41 & 23.82 & 23.51 & 25.73 & 36.45 & 33.02 \\ \cline{2-15}
 & eus\_trivia       & eu & 27.41 & 26.47 & 28.40 & \textbf{37.78} & 50.38 & 34.29 & 39.71 & 35.63 & 29.33 & 36.56 & \textbf{51.25} & 46.24 \\ \hhline{===|====|========}
\multirow{7}{*}{Reading Comprehension} & \multirow{5}{*}{belebele} & en & 28.22 & 27.67 & 25.11 & \textbf{84.33} & 71.33 & 65.22 & 81.00 & 70.67 & 22.89 & 86.56 & \textbf{93.56} & 92.78 \\
 & & es & 28.89 & 26.78 & 24.78 & \textbf{76.22} & 68.78 & 60.11 & 76.78 & 66.78 & 23.00 & 77.78 & \textbf{90.00} & 87.67 \\
 & & ca & 27.89 & 28.33 & 23.33 & \textbf{72.67} & 67.67 & 56.67 & 72.78 & 66.11 & 23.44 & 76.33 & \textbf{90.22} & 87.22 \\
 & & eu & 29.67 & 27.00 & 23.11 & \textbf{46.44} & 60.67 & 38.44 & 38.22 & 31.33 & 22.78 & 38.67 & \textbf{81.11} & 73.22 \\
 & & gl & 29.00 & 26.67 & 22.78 & \textbf{72.33} & 68.44 & 56.33 & 72.78 & 59.00 & 22.89 & 70.00 & \textbf{89.33} & 86.44 \\
 & & pt & 27.44 & 26.56 & 23.22 & \textbf{77.22} & 67.11 & 61.33 & 75.11 & 67.67 & 23.00 & 80.44 & \textbf{90.89} & 88.89 \\ \cline{2-15}
 & eus\_reading & eu & 28.12 & \textbf{28.98} & 25.28 & 24.43 & 48.01 & 28.41 & 26.99 & 27.84 & 24.15 & 25.57 & 44.32 & \textbf{49.43}
 \\ \hhline{===|====|========}
\multirow{3}{*}{Summarization} & cabreu & ca & \textbf{23.32} & 21.46 & 13.63 & 12.10 & 23.51 & 16.18 & 24.32 & 24.97 & 11.79 & 23.14 & 16.24 & \textbf{25.18} \\ \cline{2-15}
 & xlsum\_es         & es & 1.60 & 1.59 & 2.56 & \textbf{3.42} & 2.80 & 1.67 & 2.96 & 2.83 & 1.73 & 3.08 & 4.89 & \textbf{5.73}  \\ \cline{2-15}
 & summarization\_gl & gl & 4.13 & 4.93 & 3.86 & \textbf{5.43} & 4.63 & 3.78 & 6.50 & \textbf{8.52} & 1.90 & 6.17 & 6.91 & 7.89  \\ \hhline{===|====|========}
\multirow{7}{*}{Translation} & \multirow{5}{*}{flores} & es & 15.03 & \textbf{20.81} & 10.21 & 20.61 & 14.95 & 15.91 & 24.33 & 17.91 & 15.01 & 20.27 & \textbf{24.36} & 22.68 \\
 & & ca & 19.43 & \textbf{23.95} & 12.90 & 23.61 & 20.49 & 17.64 & \textbf{30.42} & 20.21 & 20.24 & 25.31 & 30.21 & 27.93 \\
 & & eu & \textbf{7.21} & 1.54 & 0.93 & 6.07 & 10.99 & 4.19 & 4.48 & 3.16 & 3.33 & 3.00 & \textbf{15.76} & 13.34 \\
 & & gl & 17.44 & \textbf{23.12} & 8.41 & 21.53 & 18.81 & 16.16 & \textbf{28.79} & 17.35 & 13.56 & 18.85 & 28.10 & 25.79 \\
 & & pt & 19.37 & \textbf{27.29} & 8.27 & 26.84 & 20.74 & 21.16 & \textbf{32.61} & 21.05 & 18.36 & 26.20 & 32.44 & 29.91 \\ \cline{2-15}
 & phrases\_va       & ca & 59.10 & 77.55 & \textbf{77.89} & 77.45 & 51.50 & 57.77 & 83.92 & 45.83 & \textbf{86.67} & 80.69 & 81.34 & 58.42 \\ \cline{2-15}
 & phrases\_es       & es & 46.03 & \textbf{60.91} & 59.49 & 56.51 & 44.19 & 36.79 & 68.27 & 35.52 & \textbf{69.25} & 62.52 & 65.34 & 64.91 \\ \hhline{===|====|========}
\multirow{6}{*}{Truthfulness} & \multirow{2}{*}{truthfulqa\_gen} & en & 9.89 & \textbf{16.85} & 12.99 & 11.59 & 4.49 & 7.85 & 18.72 & 15.30 & \textbf{20.50} & 8.34 & 14.16 & 7.06 \\
 & & gl & 9.10 & \textbf{18.68} & 14.22 & 6.41 & 7.56 & 3.53 & 10.33 & 9.18 & 3.46 & 6.03 & \textbf{15.71} & 5.76 \\ \cline{2-15}
 & \multirow{2}{*}{truthfulqa\_mc1} & en & 26.44 & 26.07 & 25.21 & \textbf{40.39} & 29.99 & 40.27 & 33.41 & 32.19 & 26.68 & \textbf{50.06} & 43.70 & 40.15 \\
 & & gl & 24.36 & 23.75 & 22.89 & \textbf{31.33} & 27.91 & 29.38 & 31.58 & 27.78 & 25.83 & 28.89 & \textbf{39.17} & 28.15 \\ \cline{2-15}
 & \multirow{2}{*}{truthfulqa\_mc2} & en & 42.57 & 41.31 & 42.44 & \textbf{55.75} & 46.39 & 57.60 & 49.55 & 48.74 & 43.30 & \textbf{66.26} & 61.11 & 55.05 \\
 & & gl & 34.01 & 34.33 & 33.45 & \textbf{42.19} & 37.44 & 41.25 & 41.38 & 37.01 & 36.36 & 40.12 & \textbf{48.70} & 39.70 \\
 
\end{tabular}%
\end{adjustbox}
\caption{}
\label{tab:instruct-models-results}
\end{table}
\end{comment}


\begin{table}[ht]
\centering
\begingroup %%
\footnotesize %%
\begin{adjustbox}{width=0.95\textwidth,center=\textwidth}
\begin{tabular}{l|ll|
>{\columncolor[HTML]{EFEFEF}}l lll|
>{\columncolor[HTML]{EFEFEF}}l lllllll}
\multicolumn{1}{c}{\textbf{Category}} &
  \multicolumn{1}{c}{\textbf{Task}} &
  \multicolumn{1}{c}{\textbf{Lang.}} &
  \begin{turn}{90}\textbf{Salamandra 2B It}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM 1.7B It}\end{turn} &
  \begin{turn}{90}\textbf{FLOR 1.3B It}\end{turn} &
  \begin{turn}{90}\textbf{Gemma-2 2B it}\end{turn} &
  \begin{turn}{90}\textbf{Salamandra 7B It}\end{turn} &
  \begin{turn}{90}\textbf{EuroLLM 9B It}\end{turn} &
  \begin{turn}{90}\textbf{FLOR 6.3B It}\end{turn} &
  \begin{turn}{90}\textbf{Gemma-2 9B It}\end{turn} &
  \begin{turn}{90}\textbf{Llama-3.1 8B It}\end{turn} &
  \begin{turn}{90}\textbf{Mistral-7B It}\end{turn} &
  \begin{turn}{90}\textbf{Occiglot-eu5 7B It}\end{turn} &
  \begin{turn}{90}\textbf{Teuken 7B It}\end{turn} \\ \hhline{===|====|========}


\multirow{9}{*}{Commonsense Reasoning} & \multirow{5}{*}{xstorycloze} & en & 69.16 & 70.75 & 61.68 & \textbf{82.59} & 78.49 & 82.06 & 71.54 & 82.59 & 84.25 & \textbf{87.36} & 81.07 & 85.11	\\	
 &  & es & 64.33 & 64.46 & 64.53 & \textbf{71.74} & 73.13 & 74.39 & 64.73 & 77.04 & \textbf{77.50} & 74.39 & 73.46 & 76.44	\\	
 &  & ca & 64.26 & 61.88 & 64.59 & \textbf{66.51} & 73.73 & 71.87 & 66.64 & 74.78 & \textbf{75.12} & 73.40 & 69.69 & 68.76\\	
 &  & eu & \textbf{56.25} & 49.97 & 49.64 & 51.69 & \textbf{65.06} & 51.36 & 51.56 & 64.20 & 60.36 & 49.83 & 51.16 & 51.89	\\	
 &  & gl & 63.6 & 61.55 & 51.29 & 62.41 & 75.05 & 72.6 & 56.39 & 72.4 & 71.48 & 58.84 & 63.67 & 67.57 \\ \cline{2-15}
 & \multirow{4}{*}{copa} & en & 77.00 & 73.00 & 69.00 & \textbf{88.00} & 93.00 & 92.00 & 77.00 & \textbf{94.00} & \textbf{94.00} & 93.00 & 92.00 & 94.00	\\	
 &  & es & 74.00 & 71.00 & 71.20 & 78.20 & 86.00 & 87.80 & 77.20 & \textbf{89.60} & 85.80 & 81.20 & 88.60 & 84.60	\\
 &  & ca & \textbf{70.60} & 65.60 & 67.40 & 67.40 & \textbf{82.80} & 82.20 & 78.60 & 82.60 & 81.60 & 76.60 & 72.40 & 69.80	\\	
 &  & eu & \textbf{57.60} & 51.00 & 53.00 & 51.20 & \textbf{67.80} & 50.80 & 50.40 & 64.00 & 58.80 & 49.80 & 50.20 & 48.20	\\	\hhline{===|====|========}
\multirow{4}{*}{Linguistic Acceptability} & cola & en & 0.01 & 0.03 & -0.02 & \textbf{0.47} & 0.39 & 0.47 & 0.00 & \textbf{0.61} & 0.46 & 0.46 & 0.35 & 0.23	\\	\cline{2-15}
 & escola & es & 0.00 & 0.00 & -0.01 & \textbf{0.13} & 0.29 & 0.26 & -0.02 & \textbf{0.41} & 0.31 & 0.06 & 0.23 & 0.12	\\	\cline{2-15}
 & catcola & ca & 0.08 & -0.02 & -0.03 & \textbf{0.13} & \textbf{0.31} & 0.26 & -0.01 & 0.29 & 0.14 & 0.07 & 0.07 & 0.05	\\	\cline{2-15}
 & galcola & gl & 0.03 & 0.06 & 0.07 & \textbf{0.10} & 0.18 & 0.14 & 0.05 & \textbf{0.39} & 0.19 & 0.13 & 0.08 & 0.03	\\	\hhline{===|====|========}
\multirow{4}{*}{Math} & \multirow{4}{*}{mgsm} & es & 0.01 & \textbf{0.02} & 0.00 & 0.00 & \textbf{0.04} & 0.03 & 0.01 & 0.00 & 0.03 & 0.02 & 0.00 & 0.02	\\	
 &  & ca & \textbf{0.02} & 0.00 & 0.01 & 0.00 & \textbf{0.04} & 0.01 & 0.01 & 0.00 & 0.02 & 0.01 & 0.00 & 0.03	\\	
 &  & eu & \textbf{0.02} & \textbf{0.02} & \textbf{0.02} & 0.00 & 0.04 & 0.03 & 0.01 & \textbf{0.10} & 0.04 & 0.00 & 0.03 & 0.02	\\	
 &  & gl & \textbf{0.02} & 0.01 & 0.00 & 0.00 & 0.03 & \textbf{0.04} & 0.01 & 0.02 & 0.02 & 0.02 & 0.00 & 0.01	\\	\hhline{===|====|========}
\multirow{12}{*}{NLI} & \multirow{4}{*}{wnli} & en & \textbf{54.93} & 53.52 & 50.70 & \textbf{54.93} & 59.15 & 60.56 & 49.30 & \textbf{80.28} & 76.06 & 73.24 & 64.79 & 50.70	\\	
 &  & ca & 56.34 & 56.34 & 53.52 & \textbf{63.38} & 64.79 & 67.61 & 56.34 & \textbf{81.69} & 66.20 & 43.66 & 59.15 & 59.15	\\	
 &  & es & 49.30 & 54.93 & 54.93 & \textbf{59.15} & 60.56 & 63.38 & 52.11 & \textbf{77.46} & 71.83 & 67.61 & 69.01 & 61.97	\\	
 &  & eu & 49.30 & 47.89 & 43.66 & \textbf{54.93} & 56.34 & 46.48 & 47.89 & 59.15 & \textbf{61.97} & 45.07 & 47.89 & 50.70	\\	\cline{2-15}
 & \multirow{5}{*}{xnli} & en & 47.47 & 47.51 & 43.98 & \textbf{48.55} & 51.49 & 51.77 & 51.61 & 51.20 & 54.50 & 52.29 & 55.46 & \textbf{57.63}	\\	
 &  & ca & \textbf{49.20} & 46.59 & 48.96 & 47.95 & 53.45 & 50.92 & 50.60 & 51.08 & 52.37 & 52.57 & 53.98 & \textbf{54.14}	\\	
 &  & es & 42.65 & 43.65 & 45.94 & \textbf{48.11} & 50.84 & 46.71 & 48.19 & 47.07 & 48.71 & 48.80 & 49.24 & \textbf{52.29}	\\	
 &  & eu & \textbf{41.06} & 33.01 & 34.14 & 34.78 & \textbf{47.34} & 33.01 & 37.52 & 45.09 & 42.51 & 33.98 & 35.43 & 38.33	\\	
 &  & gl & 48.71 & 47.39 & 44.14 & 46.46 & 52.41 & 50.73 & 45.38 & 50.17 & 51.35 & 50.13 & 51.91 & 51.75 \\	\cline{2-15}
 & teca & ca & 45.44 & 41.14 & 43.08 & \textbf{47.14} & 54.79 & 52.39 & 50.68 & 52.10 & 54.42 & \textbf{56.83} & 55.88 & 53.28	\\	\cline{2-15}
 & qnlieu & eu & 53.36 & 53.36 & 51.26 & \textbf{54.62} & 51.26 & 50.84 & 50.84 & \textbf{66.39} & 57.56 & 50.42 & 50.84 & 55.88	\\	\cline{2-15}
 & assin\_entailment & pt & \textbf{72.95} & 68.60 & 53.20 & 71.95 & 70.50 & 74.45 & 64.50 & 74.83 & 68.08 & \textbf{76.40} & 70.53 & 52.52	\\	\hhline{===|====|========}
\multirow{7}{*}{Paraphrasing} & \multirow{4}{*}{paws} & en & 56.30 & 54.95 & 52.50 & \textbf{63.60} & 63.40 & 68.75 & 61.45 & 67.00 & 69.80 & \textbf{73.65} & 70.25 & 71.15	\\	
 &  & ca & 55.85 & 54.90 & 53.15 & \textbf{64.90} & 64.35 & 72.45 & 64.60 & 70.20 & 72.75 & \textbf{73.60} & 69.60 & 68.65	\\	
 &  & es & 55.85 & 54.20 & 54.15 & \textbf{62.35} & 60.75 & 70.25 & 59.50 & 68.30 & 70.45 & \textbf{72.15} & 66.75 & 69.65	\\	
 &  & gl & 56.80 & 52.35 & 53.10 & \textbf{64.05} & 62.45 & 70.50 & 50.05 & \textbf{71.80} & 70.00 & 71.75 & 67.25 & 65.75	\\	\cline{2-15}
 & parafraseja & ca & 59.60 & 59.13 & 59.58 & \textbf{63.45} & 64.15 & 67.75 & 66.90 & 66.12 & \textbf{69.85} & 68.55 & 66.55 & 68.00	\\	\cline{2-15}
 & parafrases\_gl & gl & 54.76 & \textbf{57.48} & 52.38 & 57.14 & 58.50 & 59.86 & 53.40 & 59.52 & 60.20 & 62.24 & 57.48 & \textbf{65.31}	\\	\cline{2-15}
 & assin\_paraphrase & pt & 72.05 & 70.28 & 69.95 & \textbf{72.88} & 70.00 & 68.30 & \textbf{71.50} & 67.58 & 67.22 & 66.55 & 66.25 & 62.52	\\	\hhline{===|====|========}
\multirow{21}{*}{QA} & \multirow{4}{*}{openbookqa} & en & 27.60 & 26.80 & 20.80 & \textbf{42.40} & 38.80 & 42.00 & 27.00 & \textbf{45.20} & 41.60 & 43.20 & 37.20 & 41.80	\\	
 &  & es & 29.80 & 28.60 & 28.60 & 36.80 & 43.60 & 41.60 & 31.80 & 44.00 & 44.60 & 40.60 & 38.00 & \textbf{47.00} \\	
 &  & ca & \textbf{29.20} & 26.40 & 27.60 & 27.20 & \textbf{40.60} & 39.40 & 30.80 & 38.20 & 36.40 & 38.00 & 31.00 & 32.20	\\	
 &  & gl & 23.60 & 27.60 & 24.00 & \textbf{29.00} & \textbf{37.20} & 36.40 & 26.60 & 35.60 & 33.60 & 31.40 & 28.80 & 32.00	\\	\cline{2-15}
 & \multirow{3}{*}{xquad} & en & 52.22 & 65.81 & 47.12 & \textbf{76.16} & 70.07 & 81.24 & 67.92 & 78.70 & \textbf{84.81} & 75.73 & 36.95 & 33.67	\\	
 &  & es & 43.31 & 55.98 & 48.20 & \textbf{66.81} & 63.20 & 71.07 & 57.54 & 72.17 & \textbf{74.31} & 71.34 & 37.61 & 23.68	\\	
 &  & ca & 48.73 & 52.60 & 50.88 & \textbf{63.55} & 67.17 & 67.00 & 61.33 & 71.58 & \textbf{72.31} & 69.22 & 37.02 & 36.13	\\	\cline{2-15}
 & \multirow{3}{*}{piqa} & en & 73.50 & 73.50 & 63.87 & \textbf{79.43} & 80.58 & 80.69 & 73.67 & \textbf{81.12} & 81.07 & 82.05 & 79.76 & 80.30	\\	
 &  & ca & \textbf{65.02} & 60.94 & 64.47 & 60.23 & \textbf{73.39} & 69.31 & 71.27 & 69.10 & 68.01 & 66.87 & 65.29 & 61.53	\\	
 &  & eu &  58.12 & 54.08 & 53.05 & 53.43 & \textbf{63.67} & 53.38 & 53.21 & 58.28 & 56.37 & 53.1 & 53.76 & 53.81 \\ \cline{2-15}
 & \multirow{2}{*}{arc\_easy} & en & 71.46 & 69.65 & 54.76 & \textbf{82.79} & 82.87 & 85.73 & 64.02 & \textbf{88.01} & 85.23 & 83.04 & 79.84 & 80.68	\\	
 &  & ca & 53.41 & 50.51 & 51.89 & \textbf{56.10} & 73.57 & 74.75 & 61.07 & \textbf{75.51} & 70.41 & 70.20 & 65.66 & 59.18	\\	\cline{2-15}
 & \multirow{2}{*}{arc\_challenge} & en & 37.29 & 36.09 & 25.85 & \textbf{52.13} & 54.52 & 56.74 & 32.51 & \textbf{66.04} & 59.47 & 60.75 & 50.60 & 57.08	\\	
 &  & ca & 28.50 & 26.88 & 26.79 & \textbf{35.84} & 45.90 & 48.98 & 32.51 & \textbf{52.30} & 44.97 & 45.56 & 41.21 & 40.10	\\	\cline{2-15}
 & \multirow{2}{*}{siqa} & en & 47.90 & 47.59 & 39.46 & \textbf{55.68} & 54.45 & 57.27 & 45.75 & \textbf{59.62} & 58.03 & 57.63 & 55.12 & 58.03	\\	
 &  & ca & 43.65 & 41.81 & 39.56 & \textbf{44.63} & 51.84 & 49.08 & 45.34 & 50.77 & 51.18 & \textbf{52.05} & 46.72 & 47.85	\\	\cline{2-15}
 & catalanqa & ca & 61.17 & 60.50 & 65.94 & \textbf{70.36} & 78.05 & 72.98 & 76.30 & 79.83 & \textbf{80.55} & 75.24 & 45.79 & 42.20	\\	\cline{2-15}
 & coqcat & ca & \textbf{62.15} & 52.60 & 48.18 & 53.72 & \textbf{74.17} & 52.05 & 4.93 & 69.53 & 67.32 & 68.75 & 51.53 & 33.67	\\	\cline{2-15}
 & eus\_exams\_eu & eu & 25.41 & 25.22 & 24.86 & \textbf{35.08} & 45.98 & 30.07 & 25.21 & \textbf{49.57} & 45.13 & 31.61 & 32.03 & 30.97	\\	\cline{2-15}
 & eus\_proficiency & eu & 24.78 & 25.01 & 23.72 & \textbf{26.68} & \textbf{43.92} & 26.41 & 23.51 & 36.45 & 33.02 & 25.73 & 23.82 & 25.42	\\	\cline{2-15}
 & eus\_trivia & eu & 27.41 & 26.47 & 28.40 & \textbf{37.78} & 50.38 & 39.71 & 29.33 & \textbf{51.25} & 46.24 & 36.56 & 35.63 & 34.29	\\	\hhline{===|====|========}
\multirow{7}{*}{Reading Comprehension} & \multirow{5}{*}{belebele} & en & 28.22 & 27.67 & 25.11 & \textbf{84.33} & 71.33 & 81.00 & 22.89 & \textbf{93.56} & 92.78 & 86.56 & 70.67 & 65.22	\\	
 &  & es & 28.89 & 26.78 & 24.78 & \textbf{76.22} & 68.78 & 76.78 & 23.00 & \textbf{90.00} & 87.67 & 77.78 & 66.78 & 60.11	\\	
 &  & ca & 27.89 & 28.33 & 23.33 & \textbf{72.67} & 67.67 & 72.78 & 23.44 & \textbf{90.22} & 87.22 & 76.33 & 66.11 & 56.67	\\	
 &  & eu & 29.67 & 27.00 & 23.11 & \textbf{46.44} & 60.67 & 38.22 & 22.78 & \textbf{81.11} & 73.22 & 38.67 & 31.33 & 38.44	\\	
 &  & gl & 29.00 & 26.67 & 22.78 & \textbf{72.33} & 68.44 & 72.78 & 22.89 & \textbf{89.33} & 86.44 & 70.00 & 59.00 & 56.33	\\	
 &  & pt & 27.44 & 26.56 & 23.22 & \textbf{77.22} & 67.11 & 75.11 & 23.00 & \textbf{90.89} & 88.89 & 80.44 & 67.67 & 61.33	\\	\cline{2-15}
 & eus\_reading & eu & 28.12 & \textbf{28.98} & 25.28 & 24.43 & 48.01 & 26.99 & 24.15 & 44.32 & \textbf{49.43} & 25.57 & 27.84 & 28.41	\\ \hhline{===|====|========}
\multirow{3}{*}{Summarization} & cabreu & ca & \textbf{23.32} & 21.46 & 13.63 & 12.10 & 23.51 & 24.32 & 11.79 & 16.24 & \textbf{25.18} & 23.14 & 24.97 & 16.18	\\	\cline{2-15}
 & xlsum\_es & es & 1.60 & 1.59 & 2.56 & \textbf{3.42} & 2.80 & 2.96 & 1.73 & 4.89 & \textbf{5.73} & 03.08 & 2.83 & 1.67	\\	\cline{2-15}
 & summarization\_gl & gl & 4.13 & 4.93 & 3.86 & \textbf{5.43} & 4.63 & 6.50 & 1.90 & 6.91 & 7.89 & 6.17 & \textbf{8.52} & 3.78	\\	\hhline{===|====|========}
\multirow{7}{*}{Translation} & \multirow{5}{*}{flores} & es & 15.03 & \textbf{20.81} & 10.21 & 20.61 & 14.95 & 24.33 & 15.01 & \textbf{24.36} & 22.68 & 20.27 & 17.91 & 15.91	\\	
 &  & ca & 19.43 & \textbf{23.95} & 12.90 & 23.61 & 20.49 & \textbf{30.42} & 20.24 & 30.21 & 27.93 & 25.31 & 20.21 & 17.64	\\	
 &  & eu & \textbf{7.21} & 1.54 & 0.93 & 06.07 & 10.99 & 4.48 & 3.33 & \textbf{15.76} & 13.34 & 3.00 & 3.16 & 4.19	\\	
 &  & gl & 17.44 & \textbf{23.12} & 8.41 & 21.53 & 18.81 & \textbf{28.79} & 13.56 & 28.10 & 25.79 & 18.85 & 17.35 & 16.16	\\	
 &  & pt & 19.37 & \textbf{27.29} & 8.27 & 26.84 & 20.74 & \textbf{32.61} & 18.36 & 32.44 & 29.91 & 26.20 & 21.05 & 21.16	\\	\cline{2-15}
 & phrases\_va & ca & 59.10 & 77.55 & \textbf{77.89} & 77.45 & 51.50 & 83.92 & \textbf{86.67} & 81.34 & 58.42 & 80.69 & 45.83 & 57.77	\\	\cline{2-15}
 & phrases\_es & es & 46.03 & \textbf{60.91} & 59.49 & 56.51 & 44.19 & 68.27 & \textbf{69.25} & 65.34 & 64.91 & 62.52 & 35.52 & 36.79	\\	\hhline{===|====|========}
\multirow{6}{*}{Truthfulness} & \multirow{2}{*}{truthfulqa\_gen} & en & 9.89 & \textbf{16.85} & 12.99 & 11.59 & 4.49 & 18.72 & \textbf{20.50} & 14.16 & 7.06 & 8.34 & 15.30 & 7.85	\\	
 &  & gl & 9.10 & \textbf{18.68} & 14.22 & 6.41 & 7.56 & 10.33 & 3.46 & \textbf{15.71} & 5.76 & 06.03 & 9.18 & 3.53	\\	\cline{2-15}
 & \multirow{2}{*}{truthfulqa\_mc1} & en & 26.44 & 26.07 & 25.21 & \textbf{40.39} & 29.99 & 33.41 & 26.68 & 43.70 & 40.15 & \textbf{50.06} & 32.19 & 40.27	\\	
 &  & gl & 24.36 & 23.75 & 22.89 & \textbf{31.33} & 27.91 & 31.58 & 25.83 & \textbf{39.17} & 28.15 & 28.89 & 27.78 & 29.38	\\	\cline{2-15}
 & \multirow{2}{*}{truthfulqa\_mc2} & en & 42.57 & 41.31 & 42.44 & \textbf{55.75} & 46.39 & 49.55 & 43.30 & 61.11 & 55.05 & \textbf{66.26} & 48.74 & 57.60	\\	
 &  & gl & 34.01 & 34.33 & 33.45 & \textbf{42.19} & 37.44 & 41.38 & 36.36 & \textbf{48.70} & 39.70 & 40.12 & 37.01 & 41.25	\\	


\end{tabular}%
\end{adjustbox}
\endgroup %%
\caption{Results of \textit{instructed} models to LM Evaluation Harness tasks.}
\label{tab:instruct-models-results}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Commonsense Reasoning}  & Coherence (1-5) &2.87 / 0.76 & 2.36 / 0.69 & 1.55 / 0.77 & \textbf{3.46} / 0.45 & 2.84 / 0.78 & 3.60 / 0.48 & 1.59 / 0.44 & 3.41 / 0.37 & 3.66 / 0.42 & \textbf{3.68} / 0.42 & 2.32 / 1.26 \\
\midrule
\textbf{Paraphrasing}  & Completeness (1-5) &3.82 / 0.55 & 2.70 / 1.36 & 3.05 / 1.25 & \textbf{3.95} / 0.21 & 3.79 / 0.57 & 3.96 / 0.28 & 3.09 / 1.36 & \textbf{4.18} / 0.19 & 3.98 / 0.21 & 3.96 / 0.24 & 2.58 / 0.97 \\
 & Completeness (0/1) &0.87 / 0.07 & 0.56 / 0.17 & 0.44 / 0.16 & \textbf{0.92} / 0.04 & 0.88 / 0.06 & 0.95 / 0.03 & 0.64 / 0.14 & \textbf{0.98} / 0.01 & 0.93 / 0.04 & 0.93 / 0.04 & 0.42 / 0.14 \\
 & Grammatical Correctness (0/1) &0.93 / 0.04 & 0.71 / 0.15 & 0.69 / 0.14 & \textbf{0.98} / 0.01 & 0.94 / 0.03 & 0.97 / 0.02 & 0.76 / 0.12 & \textbf{0.99} / 0.00 & 0.97 / 0.02 & 0.97 / 0.02 & 0.60 / 0.15 \\
\midrule
\textbf{Reading Comprehension}  & Comprehension (1-5) &3.41 / 0.46 & 2.81 / 0.72 & 1.65 / 0.79 & \textbf{3.60} / 0.53 & 3.38 / 0.41 & \textbf{3.79} / 0.33 & 2.98 / 0.57 & 3.57 / 0.55 & 3.53 / 0.46 & 3.71 / 0.39 & 2.91 / 0.78 \\
 & Relevance (0/1) &0.86 / 0.05 & 0.64 / 0.11 & 0.49 / 0.17 & \textbf{0.89} / 0.05 & 0.85 / 0.05 & \textbf{0.94} / 0.03 & 0.68 / 0.09 & 0.90 / 0.04 & 0.89 / 0.04 & 0.93 / 0.04 & 0.41 / 0.14 \\
\midrule
\textbf{Extreme Summarization}  & Informativeness (1-5) &3.37 / 0.32 & 3.11 / 0.46 & 1.77 / 0.64 & \textbf{3.67} / 0.22 & 3.22 / 0.36 & 3.61 / 0.18 & --- / --- & 3.66 / 0.22 & 3.66 / 0.23 & \textbf{3.80} / 0.12 & 2.96 / 0.70 \\
 & Conciseness (1-5) &3.04 / 0.33 & 2.45 / 0.48 & \textbf{3.54} / 1.30 & 3.41 / 0.22 & 3.04 / 0.34 & 3.37 / 0.19 & --- / --- & \textbf{3.47} / 0.20 & 3.44 / 0.21 & 3.32 / 0.18 & 2.22 / 0.49 \\
\midrule
\textbf{Mathematics}  & Reasoning (1-5) &3.18 / 0.72 & 2.47 / 0.69 & 1.66 / 0.44 & \textbf{4.07} / 0.36 & 3.15 / 0.67 & \textbf{4.35} / 0.28 & 1.60 / 0.34 & 4.12 / 0.29 & 4.31 / 0.30 & 4.07 / 0.35 & 3.55 / 0.67 \\
 & Correctness (0/1) &0.65 / 0.13 & 0.35 / 0.13 & 0.35 / 0.14 & \textbf{0.94} / 0.02 & 0.63 / 0.12 & 0.97 / 0.01 & 0.15 / 0.06 & \textbf{0.98} / 0.01 & 0.97 / 0.02 & 0.92 / 0.05 & 0.77 / 0.10 \\
\midrule
\textbf{Translation form Language}  & Accuracy (1-5) &4.03 / 0.19 & 3.66 / 0.29 & 1.96 / 1.13 & \textbf{4.04} / 0.29 & 4.03 / 0.17 & 3.90 / 0.24 & 2.03 / 0.57 & \textbf{4.36} / 0.18 & 4.02 / 0.29 & 3.92 / 0.27 & 3.53 / 0.83 \\
 & Fluency (1-5) &3.69 / 0.16 & 3.45 / 0.22 & 2.64 / 0.79 & \textbf{3.73} / 0.20 & 3.72 / 0.15 & 3.62 / 0.17 & 1.95 / 0.42 & \textbf{3.97} / 0.16 & 3.72 / 0.20 & 3.65 / 0.19 & 3.41 / 0.45 \\
\midrule
\textbf{Translation to Language}  & Accuracy (1-5) &4.22 / 0.16 & 3.80 / 0.53 & 2.43 / 1.44 & \textbf{4.38} / 0.19 & 4.18 / 0.18 & 4.32 / 0.22 & 3.35 / 0.92 & \textbf{4.52} / 0.20 & 4.33 / 0.20 & 4.18 / 0.35 & 3.70 / 1.11 \\
 & Fluency (1-5) &3.86 / 0.11 & 3.49 / 0.40 & 2.94 / 0.68 & \textbf{3.94} / 0.14 & 3.82 / 0.11 & 3.92 / 0.16 & 3.11 / 0.67 & \textbf{4.03} / 0.18 & 3.93 / 0.15 & 3.75 / 0.23 & 3.53 / 0.51 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in Spanish.}
\label{tab:llm-judge-es}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Commonsense Reasoning}  & Coherence (1-5) &2.75 / 0.84 & 1.96 / 0.71 & --- / --- & \textbf{2.86} / 0.51 & 2.68 / 0.61 & 3.34 / 0.48 & 2.17 / 0.83 & 3.05 / 0.52 & 3.41 / 0.42 & \textbf{3.51} / 0.42 & 1.55 / 0.53 \\
\midrule
\textbf{Paraphrasing}  & Grammatical Correctness (0/1) &0.88 / 0.06 & 0.62 / 0.16 & 0.71 / 0.13 & \textbf{0.97} / 0.02 & 0.90 / 0.05 & 0.93 / 0.04 & 0.69 / 0.13 & \textbf{0.99} / 0.01 & 0.94 / 0.03 & 0.95 / 0.03 & 0.61 / 0.16 \\
 & Completeness (0/1) &0.74 / 0.10 & 0.47 / 0.18 & 0.37 / 0.16 & \textbf{0.90} / 0.06 & 0.81 / 0.09 & 0.85 / 0.07 & 0.58 / 0.15 & \textbf{0.95} / 0.03 & 0.87 / 0.06 & 0.90 / 0.06 & 0.40 / 0.14 \\
 & Completeness (1-5) &3.49 / 0.78 & 2.48 / 1.71 & 2.94 / 1.31 & \textbf{3.97} / 0.28 & 3.58 / 0.72 & 3.70 / 0.62 & 2.78 / 1.58 & \textbf{4.09} / 0.20 & 3.76 / 0.33 & 3.91 / 0.26 & 2.62 / 1.03 \\
\midrule
\textbf{Reading Comprehension}  & Relevance (0/1) &0.83 / 0.06 & 0.59 / 0.12 & 0.58 / 0.17 & \textbf{0.89} / 0.05 & 0.82 / 0.05 & \textbf{0.92} / 0.03 & 0.67 / 0.09 & 0.83 / 0.06 & 0.87 / 0.04 & 0.91 / 0.04 & 0.47 / 0.14 \\
 & Comprehension (1-5) &3.22 / 0.42 & 2.68 / 0.73 & 1.82 / 0.87 & \textbf{3.62} / 0.40 & 3.22 / 0.46 & \textbf{3.72} / 0.35 & 2.89 / 0.52 & 3.38 / 0.55 & 3.52 / 0.38 & 3.66 / 0.41 & 3.14 / 0.69 \\
\midrule
\textbf{Extreme Summarization}  & Conciseness (1-5) &2.88 / 0.51 & 2.15 / 0.49 & \textbf{3.78} / 1.17 & 3.37 / 0.23 & 2.81 / 0.58 & 3.28 / 0.21 & --- / --- & \textbf{3.45} / 0.20 & 3.34 / 0.19 & 3.31 / 0.25 & 2.29 / 0.52 \\
 & Informativeness (1-5) &3.55 / 0.31 & 3.00 / 0.64 & 2.09 / 0.91 & \textbf{3.59} / 0.27 & 3.55 / 0.36 & 3.61 / 0.21 & --- / --- & 3.65 / 0.22 & 3.64 / 0.19 & \textbf{3.78} / 0.12 & 3.14 / 0.67 \\
\midrule
\textbf{Mathematics}  & Correctness (0/1) &0.63 / 0.14 & 0.16 / 0.09 & 0.32 / 0.15 & \textbf{0.93} / 0.03 & 0.68 / 0.13 & 0.94 / 0.03 & 0.13 / 0.06 & \textbf{0.98} / 0.01 & 0.94 / 0.03 & 0.92 / 0.04 & 0.47 / 0.15 \\
 & Reasoning (1-5) &3.03 / 0.72 & 1.77 / 0.49 & 1.77 / 0.45 & \textbf{3.85} / 0.40 & 3.12 / 0.73 & 4.06 / 0.40 & 1.51 / 0.30 & 4.08 / 0.30 & \textbf{4.13} / 0.41 & 4.04 / 0.33 & 2.68 / 0.67 \\
\midrule
\textbf{Translation form Language}  & Accuracy (1-5) &\textbf{4.17} / 0.17 & 2.98 / 1.05 & 2.24 / 1.30 & 3.92 / 0.35 & 4.14 / 0.18 & 3.97 / 0.22 & 1.48 / 0.56 & \textbf{4.40} / 0.15 & 4.06 / 0.30 & 3.99 / 0.34 & 2.41 / 1.06 \\
 & Fluency (1-5) &\textbf{3.78} / 0.11 & 2.78 / 0.77 & 2.88 / 0.69 & 3.59 / 0.26 & 3.77 / 0.14 & 3.64 / 0.21 & 1.47 / 0.45 & \textbf{3.96} / 0.15 & 3.69 / 0.22 & 3.60 / 0.20 & 2.64 / 0.66 \\
\midrule
\textbf{Translation to Language}  & Accuracy (1-5) &\textbf{4.09} / 0.19 & 3.37 / 0.79 & 2.18 / 1.40 & 4.01 / 0.31 & 4.14 / 0.17 & 4.21 / 0.24 & 2.98 / 1.23 & \textbf{4.34} / 0.22 & 4.13 / 0.21 & 4.00 / 0.34 & 2.29 / 1.20 \\
 & Fluency (1-5) &\textbf{3.75} / 0.12 & 3.13 / 0.53 & 2.80 / 0.79 & 3.69 / 0.21 & 3.77 / 0.14 & 3.81 / 0.20 & 2.76 / 0.93 & \textbf{3.95} / 0.17 & 3.77 / 0.17 & 3.67 / 0.19 & 2.55 / 0.82 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in Catalan.}
\label{tab:llm-judge-ca}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Commonsense Reasoning}  & Coherence (1-5) &2.77 / 0.72 & 2.20 / 0.88 & --- / --- & \textbf{3.23} / 0.58 & 2.73 / 0.68 & 3.45 / 0.44 & 2.91 / 0.71 & 3.31 / 0.43 & 3.43 / 0.53 & \textbf{3.47} / 0.38 & 2.20 / 1.14 \\
\midrule
\textbf{Paraphrasing}  & Completeness (1-5) &3.24 / 0.77 & 2.22 / 1.23 & 2.86 / 1.20 & \textbf{3.98} / 0.23 & 3.34 / 0.79 & 3.55 / 0.73 & 3.82 / 0.43 & \textbf{3.97} / 0.21 & 3.77 / 0.39 & 3.74 / 0.49 & 2.55 / 1.06 \\
 & Completeness (0/1) &0.76 / 0.10 & 0.49 / 0.16 & 0.32 / 0.14 & \textbf{0.94} / 0.03 & 0.76 / 0.10 & 0.84 / 0.09 & 0.87 / 0.07 & \textbf{0.93} / 0.04 & 0.90 / 0.06 & 0.85 / 0.08 & 0.40 / 0.15 \\
 & Grammatical Correctness (0/1) &0.84 / 0.07 & 0.65 / 0.15 & 0.66 / 0.14 & \textbf{0.97} / 0.02 & 0.87 / 0.06 & 0.89 / 0.06 & 0.91 / 0.05 & \textbf{0.98} / 0.01 & 0.94 / 0.03 & 0.91 / 0.05 & 0.61 / 0.15 \\
\midrule
\textbf{Reading Comprehension}  & Comprehension (1-5) &2.99 / 0.51 & 2.92 / 0.94 & 1.60 / 0.71 & \textbf{3.46} / 0.52 & 2.97 / 0.53 & \textbf{3.63} / 0.33 & 2.72 / 0.70 & 3.26 / 0.66 & 3.36 / 0.54 & 3.56 / 0.46 & 3.18 / 0.78 \\
 & Relevance (0/1) &0.79 / 0.08 & 0.70 / 0.12 & 0.52 / 0.17 & \textbf{0.89} / 0.06 & 0.78 / 0.07 & \textbf{0.93} / 0.04 & 0.59 / 0.12 & 0.82 / 0.08 & 0.84 / 0.06 & 0.91 / 0.04 & 0.51 / 0.14 \\
\midrule
\textbf{Extreme Summarization}  & Informativeness (1-5) &3.46 / 0.36 & 3.05 / 0.66 & 1.70 / 0.57 & \textbf{3.63} / 0.28 & 3.43 / 0.43 & 3.64 / 0.18 & --- / --- & 3.67 / 0.24 & 3.52 / 0.24 & \textbf{3.87} / 0.08 & 3.21 / 0.70 \\
 & Conciseness (1-5) &2.95 / 0.40 & 2.26 / 0.62 & \textbf{3.64} / 1.19 & 3.39 / 0.22 & 2.92 / 0.46 & 3.42 / 0.18 & --- / --- & \textbf{3.46} / 0.19 & 3.38 / 0.19 & 3.37 / 0.21 & 2.35 / 0.57 \\
\midrule
\textbf{Mathematics}  & Reasoning (1-5) &2.99 / 0.68 & 2.08 / 0.60 & 1.66 / 0.43 & \textbf{3.88} / 0.40 & 3.06 / 0.57 & 4.01 / 0.45 & 1.44 / 0.27 & 4.01 / 0.26 & \textbf{4.17} / 0.34 & 3.90 / 0.39 & 3.10 / 0.59 \\
 & Correctness (0/1) &0.64 / 0.14 & 0.34 / 0.13 & 0.32 / 0.14 & \textbf{0.90} / 0.04 & 0.67 / 0.13 & 0.94 / 0.03 & 0.11 / 0.05 & \textbf{0.98} / 0.01 & 0.97 / 0.02 & 0.88 / 0.06 & 0.64 / 0.13 \\
\midrule
\textbf{Translation form Language}  & Fluency (1-5) &\textbf{3.84} / 0.13 & 3.49 / 0.33 & 2.76 / 0.67 & 3.83 / 0.20 & 3.80 / 0.12 & 3.74 / 0.22 & 2.72 / 0.54 & \textbf{4.02} / 0.15 & 3.72 / 0.20 & 3.62 / 0.19 & 3.27 / 0.74 \\
 & Accuracy (1-5) &4.13 / 0.21 & 3.73 / 0.45 & 2.11 / 1.17 & \textbf{4.13} / 0.27 & 4.15 / 0.16 & 4.04 / 0.31 & 2.81 / 0.69 & \textbf{4.35} / 0.18 & 3.96 / 0.30 & 3.87 / 0.37 & 3.23 / 1.33 \\
\midrule
\textbf{Translation to Language}  & Fluency (1-5) &3.53 / 0.14 & 3.23 / 0.27 & 2.72 / 0.73 & \textbf{3.62} / 0.19 & 3.55 / 0.14 & 3.54 / 0.17 & 1.89 / 0.60 & \textbf{3.77} / 0.17 & 3.54 / 0.18 & 3.49 / 0.21 & 3.08 / 0.70 \\
 & Accuracy (1-5) &3.90 / 0.15 & 3.51 / 0.33 & 2.17 / 1.16 & \textbf{3.92} / 0.28 & 3.90 / 0.16 & 3.88 / 0.18 & 1.92 / 0.81 & \textbf{4.17} / 0.23 & 3.93 / 0.18 & 3.77 / 0.35 & 3.02 / 1.27 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in Galician.}
\label{tab:llm-judge-gl}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Commonsense Reasoning}  & Coherence (1-5) &2.44 / 0.66 & 2.25 / 1.35 & --- / --- & \textbf{2.47} / 0.99 & 2.14 / 0.54 & 1.67 / 0.92 & 1.69 / 1.00 & \textbf{2.69} / 0.41 & 2.55 / 0.50 & 1.73 / 0.66 & 1.92 / 0.74 \\
\midrule
\textbf{Reading Comprehension}  & Relevance (0/1) &0.58 / 0.12 & 0.15 / 0.08 & 0.49 / 0.17 & \textbf{0.75} / 0.11 & 0.56 / 0.11 & 0.22 / 0.10 & 0.15 / 0.08 & 0.53 / 0.16 & \textbf{0.75} / 0.11 & 0.54 / 0.15 & 0.34 / 0.13 \\
 & Comprehension (1-5) &2.39 / 0.44 & 1.39 / 0.36 & 1.33 / 0.26 & \textbf{2.89} / 0.64 & 2.41 / 0.40 & 1.78 / 0.65 & 1.73 / 0.62 & 2.44 / 0.74 & \textbf{2.77} / 0.53 & 2.67 / 0.91 & 2.52 / 0.81 \\
\midrule
\textbf{Mathematics}  & Correctness (0/1) &\textbf{0.65} / 0.12 & 0.07 / 0.04 & 0.23 / 0.12 & 0.62 / 0.13 & 0.69 / 0.12 & 0.38 / 0.15 & 0.05 / 0.03 & \textbf{0.97} / 0.02 & 0.86 / 0.06 & 0.53 / 0.14 & 0.46 / 0.16 \\
 & Reasoning (1-5) &\textbf{2.75} / 0.51 & 1.17 / 0.14 & 1.65 / 0.43 & 2.60 / 0.61 & 2.84 / 0.55 & 2.08 / 0.98 & 1.15 / 0.12 & \textbf{3.76} / 0.29 & 3.40 / 0.48 & 2.42 / 0.66 & 2.52 / 0.77 \\
\midrule
\textbf{Translation form Language}  & Fluency (1-5) &3.20 / 0.20 & 2.19 / 0.67 & 2.66 / 0.52 & \textbf{3.33} / 0.49 & 3.37 / 0.17 & 2.95 / 0.55 & 1.82 / 0.69 & \textbf{3.56} / 0.18 & 3.26 / 0.31 & 3.26 / 0.31 & 2.77 / 0.54 \\
 & Accuracy (1-5) &3.44 / 0.32 & 2.22 / 1.00 & 2.06 / 0.88 & \textbf{3.48} / 0.82 & 3.61 / 0.24 & 3.08 / 0.97 & 1.88 / 1.30 & \textbf{3.84} / 0.23 & 3.54 / 0.57 & 3.37 / 0.57 & 2.34 / 0.96 \\
\midrule
\textbf{Translation to Language}  & Fluency (1-5) &\textbf{3.18} / 0.13 & 2.59 / 0.90 & 2.76 / 0.61 & 2.57 / 0.54 & 3.17 / 0.12 & 2.23 / 0.68 & 1.98 / 1.23 & \textbf{3.32} / 0.18 & 3.10 / 0.23 & 2.91 / 0.37 & 2.78 / 0.73 \\
 & Accuracy (1-5) &\textbf{3.58} / 0.20 & 2.79 / 1.48 & 2.11 / 1.12 & 2.66 / 0.86 & 3.56 / 0.20 & 2.32 / 1.06 & 2.21 / 1.81 & \textbf{3.72} / 0.21 & 3.45 / 0.40 & 3.11 / 0.64 & 2.40 / 1.27 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in Basque.}
\label{tab:llm-judge-eu}
\end{table}

    \begin{table}[ht]
    \centering
    \begin{adjustbox}{width=\textwidth,center=\textwidth}
    \begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
    Task & Metric & 
    \begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
    \begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
    \begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
    \begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
    \begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
    \begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
    \begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
    \begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
    \begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
    \begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
    \begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
    \midrule
    \textbf{Commonsense Reasoning}  & Coherence (1-5) &3.41 / 0.67 & 3.26 / 0.64 & 1.48 / 0.77 & \textbf{3.95} / 0.39 & 3.48 / 0.72 & 3.97 / 0.42 & 1.46 / 0.38 & 3.81 / 0.46 & \textbf{4.20} / 0.32 & 4.19 / 0.32 & 3.06 / 1.18 \\
    \midrule
    \textbf{Paraphrasing}  & Grammatical Correctness (0/1) &0.95 / 0.03 & 0.90 / 0.04 & 0.59 / 0.15 & \textbf{0.99} / 0.00 & 0.98 / 0.01 & 0.99 / 0.01 & 0.67 / 0.18 & 0.99 / 0.01 & 0.99 / 0.01 & \textbf{0.99} / 0.00 & 0.92 / 0.04 \\
     & Completeness (1-5) &3.94 / 0.36 & 3.64 / 0.85 & 2.68 / 1.33 & \textbf{4.12} / 0.19 & 4.02 / 0.29 & 4.13 / 0.27 & 2.72 / 1.70 & \textbf{4.21} / 0.20 & 4.05 / 0.23 & 4.06 / 0.14 & 3.77 / 0.43 \\
     & Completeness (0/1) &0.92 / 0.04 & 0.85 / 0.08 & 0.31 / 0.14 & \textbf{0.97} / 0.02 & 0.95 / 0.03 & 0.96 / 0.02 & 0.61 / 0.19 & 0.97 / 0.02 & 0.97 / 0.02 & \textbf{0.97} / 0.02 & 0.87 / 0.07 \\
    \midrule
    \textbf{Reading Comprehension}  & Relevance (0/1) &0.84 / 0.05 & 0.71 / 0.08 & 0.34 / 0.14 & \textbf{0.88} / 0.05 & 0.86 / 0.05 & 0.91 / 0.04 & 0.59 / 0.09 & 0.86 / 0.05 & 0.87 / 0.05 & \textbf{0.93} / 0.03 & 0.67 / 0.12 \\
     & Comprehension (1-5) &3.50 / 0.50 & 3.07 / 0.63 & 1.53 / 0.69 & \textbf{3.69} / 0.52 & 3.53 / 0.47 & 3.71 / 0.45 & 2.85 / 0.61 & 3.47 / 0.56 & 3.60 / 0.50 & \textbf{3.81} / 0.43 & 3.52 / 0.61 \\
    \midrule
    \textbf{Extreme Summarization}  & Conciseness (1-5) &3.26 / 0.30 & 3.17 / 0.25 & 2.58 / 1.45 & \textbf{3.52} / 0.16 & 3.35 / 0.22 & 3.49 / 0.17 & --- / --- & \textbf{3.63} / 0.15 & 3.57 / 0.17 & 3.53 / 0.16 & 2.91 / 0.48 \\
     & Informativeness (1-5) &3.32 / 0.29 & 3.23 / 0.28 & 1.25 / 0.22 & \textbf{3.74} / 0.13 & 3.25 / 0.26 & 3.70 / 0.15 & --- / --- & 3.79 / 0.13 & 3.72 / 0.15 & \textbf{3.85} / 0.10 & 3.62 / 0.32 \\
    \midrule
    \textbf{Mathematics}  & Correctness (0/1) &0.72 / 0.11 & 0.60 / 0.13 & 0.21 / 0.11 & \textbf{0.96} / 0.02 & 0.70 / 0.12 & 0.98 / 0.01 & 0.10 / 0.05 & \textbf{0.99} / 0.00 & 0.98 / 0.01 & 0.97 / 0.02 & 0.77 / 0.10 \\
     & Reasoning (1-5) &3.43 / 0.62 & 3.18 / 0.64 & 1.28 / 0.27 & \textbf{4.16} / 0.25 & 3.45 / 0.58 & 4.27 / 0.24 & 1.50 / 0.28 & 4.20 / 0.22 & 4.26 / 0.24 & \textbf{4.29} / 0.27 & 3.76 / 0.58 \\
    \midrule
    \textbf{Translation form Language}  & Fluency (1-5) &\textbf{3.83} / 0.10 & 3.54 / 0.21 & --- / --- & 3.80 / 0.16 & 3.82 / 0.14 & 3.68 / 0.16 & 2.90 / 0.35 & \textbf{4.00} / 0.14 & 3.86 / 0.17 & 3.76 / 0.17 & 3.13 / 0.63 \\
     & Accuracy (1-5) &\textbf{4.28} / 0.15 & 3.88 / 0.28 & 1.51 / 0.73 & 4.15 / 0.22 & 4.26 / 0.17 & 4.05 / 0.25 & 3.07 / 0.43 & \textbf{4.43} / 0.15 & 4.26 / 0.18 & 4.12 / 0.27 & 3.27 / 0.98 \\
    \midrule
    \textbf{Translation to Language}  & Accuracy (1-5) &4.53 / 0.13 & 4.15 / 0.34 & 2.46 / 1.38 & \textbf{4.55} / 0.20 & 4.52 / 0.15 & 4.55 / 0.18 & 2.50 / 1.03 & \textbf{4.70} / 0.12 & 4.53 / 0.17 & 4.52 / 0.16 & 3.99 / 1.19 \\
     & Fluency (1-5) &\textbf{4.13} / 0.14 & 3.79 / 0.31 & 2.89 / 0.68 & 4.13 / 0.18 & 4.12 / 0.16 & 4.12 / 0.17 & 2.42 / 0.78 & \textbf{4.29} / 0.17 & 4.13 / 0.13 & 4.08 / 0.14 & 3.75 / 0.70 \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Results of \textit{instructed} models to \llmJudge{} tasks in English.}
    \label{tab:llm-judge-en}
    \end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Paraphrasing}  & Completeness (0/1) &0.73 / 0.11 & 0.56 / 0.17 & 0.28 / 0.14 & \textbf{0.93} / 0.04 & 0.82 / 0.09 & 0.90 / 0.05 & 0.75 / 0.10 & \textbf{0.96} / 0.02 & 0.93 / 0.04 & 0.92 / 0.04 & 0.56 / 0.15 \\
 & Grammatical Correctness (0/1) &0.86 / 0.07 & 0.64 / 0.17 & 0.60 / 0.16 & \textbf{0.98} / 0.01 & 0.90 / 0.06 & 0.95 / 0.03 & 0.73 / 0.10 & \textbf{0.98} / 0.01 & 0.96 / 0.02 & 0.96 / 0.02 & 0.75 / 0.13 \\
 & Completeness (1-5) &3.49 / 0.71 & 2.53 / 1.42 & 2.63 / 1.37 & \textbf{4.02} / 0.21 & 3.64 / 0.57 & 3.85 / 0.50 & 3.53 / 0.59 & \textbf{4.03} / 0.21 & 3.95 / 0.28 & 3.98 / 0.25 & 3.07 / 0.87 \\
\midrule
\textbf{Reading Comprehension}  & Relevance (0/1) &0.81 / 0.05 & 0.65 / 0.10 & 0.43 / 0.16 & \textbf{0.86} / 0.05 & 0.79 / 0.06 & 0.89 / 0.04 & 0.45 / 0.10 & 0.85 / 0.04 & 0.87 / 0.05 & \textbf{0.92} / 0.03 & 0.34 / 0.14 \\
 & Comprehension (1-5) &3.27 / 0.36 & 2.87 / 0.70 & 1.69 / 0.91 & \textbf{3.56} / 0.45 & 3.28 / 0.41 & \textbf{3.67} / 0.37 & 2.50 / 0.60 & 3.46 / 0.42 & 3.58 / 0.45 & 3.66 / 0.35 & 2.96 / 0.81 \\
\midrule
\textbf{Extreme Summarization}  & Informativeness (1-5) &3.47 / 0.31 & 3.18 / 0.45 & 1.44 / 0.44 & \textbf{3.71} / 0.18 & 3.21 / 0.32 & 3.75 / 0.16 & --- / --- & 3.77 / 0.15 & 3.77 / 0.15 & \textbf{3.83} / 0.12 & 3.17 / 0.70 \\
 & Conciseness (1-5) &3.25 / 0.30 & 2.99 / 0.38 & 3.07 / 1.40 & \textbf{3.52} / 0.17 & 3.32 / 0.20 & 3.54 / 0.17 & --- / --- & \textbf{3.61} / 0.18 & 3.59 / 0.16 & 3.56 / 0.17 & 2.36 / 0.58 \\
\midrule
\textbf{Mathematics}  & Correctness (0/1) &0.62 / 0.13 & 0.25 / 0.12 & 0.31 / 0.15 & \textbf{0.92} / 0.03 & 0.63 / 0.13 & 0.93 / 0.03 & 0.06 / 0.03 & 0.97 / 0.01 & \textbf{0.98} / 0.01 & 0.93 / 0.04 & 0.70 / 0.12 \\
 & Reasoning (1-5) &3.02 / 0.56 & 2.16 / 0.64 & 1.50 / 0.40 & \textbf{3.86} / 0.32 & 3.07 / 0.62 & 4.10 / 0.38 & 1.33 / 0.20 & 4.05 / 0.26 & \textbf{4.21} / 0.34 & 4.03 / 0.35 & 3.27 / 0.61 \\
\midrule
\textbf{Translation form Language}  & Fluency (1-5) &\textbf{3.72} / 0.15 & 3.34 / 0.41 & --- / --- & 3.70 / 0.23 & 3.75 / 0.13 & 3.61 / 0.21 & 2.00 / 0.49 & \textbf{3.95} / 0.18 & 3.81 / 0.17 & 3.64 / 0.18 & 3.09 / 0.71 \\
 & Accuracy (1-5) &\textbf{4.09} / 0.25 & 3.56 / 0.55 & 1.72 / 0.94 & 4.04 / 0.34 & 4.12 / 0.16 & 4.00 / 0.27 & 2.06 / 0.58 & \textbf{4.39} / 0.21 & 4.14 / 0.22 & 4.02 / 0.29 & 3.11 / 1.15 \\
\midrule
\textbf{Translation to Language}  & Accuracy (1-5) &4.24 / 0.15 & 3.76 / 0.57 & 2.04 / 1.22 & \textbf{4.25} / 0.25 & 4.25 / 0.17 & 4.39 / 0.20 & 2.42 / 0.87 & \textbf{4.48} / 0.21 & 4.31 / 0.18 & 4.18 / 0.30 & 3.00 / 1.45 \\
 & Fluency (1-5) &3.86 / 0.13 & 3.43 / 0.39 & 2.62 / 0.84 & \textbf{3.87} / 0.17 & 3.85 / 0.13 & 3.95 / 0.14 & 2.33 / 0.68 & \textbf{4.02} / 0.15 & 3.90 / 0.12 & 3.77 / 0.17 & 3.07 / 0.92 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in French.}
\label{tab:llm-judge-fr}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Paraphrasing}  & Completeness (0/1) &0.83 / 0.08 & 0.60 / 0.16 & 0.38 / 0.16 & \textbf{0.91} / 0.05 & 0.85 / 0.07 & 0.89 / 0.06 & 0.50 / 0.13 & \textbf{0.93} / 0.03 & 0.86 / 0.07 & 0.92 / 0.04 & 0.49 / 0.16 \\
 & Grammatical Correctness (0/1) &0.91 / 0.05 & 0.72 / 0.13 & 0.65 / 0.16 & \textbf{0.98} / 0.02 & 0.94 / 0.04 & 0.94 / 0.04 & 0.59 / 0.15 & \textbf{0.98} / 0.01 & 0.95 / 0.03 & 0.95 / 0.03 & 0.71 / 0.13 \\
 & Completeness (1-5) &3.65 / 0.57 & 2.62 / 1.32 & 2.92 / 1.34 & \textbf{3.94} / 0.21 & 3.76 / 0.54 & 3.85 / 0.49 & 2.57 / 0.98 & \textbf{4.00} / 0.19 & 3.81 / 0.48 & 3.90 / 0.28 & 2.85 / 1.14 \\
\midrule
\textbf{Reading Comprehension}  & Comprehension (1-5) &3.33 / 0.45 & 2.85 / 0.59 & 1.73 / 0.99 & \textbf{3.59} / 0.50 & 3.28 / 0.42 & \textbf{3.72} / 0.34 & 2.00 / 0.57 & 3.44 / 0.53 & 3.59 / 0.44 & 3.67 / 0.42 & 2.71 / 0.77 \\
 & Relevance (0/1) &0.83 / 0.05 & 0.65 / 0.09 & 0.42 / 0.15 & \textbf{0.88} / 0.06 & 0.80 / 0.06 & \textbf{0.92} / 0.03 & 0.25 / 0.08 & 0.87 / 0.05 & 0.90 / 0.04 & 0.91 / 0.04 & 0.30 / 0.11 \\
\midrule
\textbf{Mathematics}  & Correctness (0/1) &0.65 / 0.12 & 0.24 / 0.11 & 0.35 / 0.15 & \textbf{0.92} / 0.03 & 0.70 / 0.13 & 0.96 / 0.02 & 0.02 / 0.01 & \textbf{0.99} / 0.01 & 0.99 / 0.01 & 0.94 / 0.03 & 0.81 / 0.09 \\
 & Reasoning (1-5) &3.17 / 0.73 & 1.95 / 0.59 & 1.58 / 0.44 & \textbf{3.93} / 0.36 & 3.25 / 0.64 & 4.24 / 0.41 & 1.09 / 0.07 & 4.09 / 0.29 & \textbf{4.25} / 0.35 & 4.00 / 0.40 & 3.78 / 0.62 \\
\midrule
\textbf{Translation form Language}  & Accuracy (1-5) &\textbf{4.02} / 0.23 & 3.45 / 0.58 & 1.94 / 1.18 & 3.97 / 0.31 & 4.05 / 0.15 & 3.93 / 0.23 & 1.38 / 0.44 & \textbf{4.27} / 0.19 & 4.06 / 0.26 & 3.98 / 0.23 & 3.28 / 1.23 \\
 & Fluency (1-5) &\textbf{3.70} / 0.18 & 3.27 / 0.38 & --- / --- & 3.64 / 0.24 & 3.71 / 0.15 & 3.64 / 0.19 & 1.46 / 0.45 & \textbf{3.88} / 0.14 & 3.73 / 0.21 & 3.63 / 0.15 & 3.28 / 0.71 \\
\midrule
\textbf{Translation to Language}  & Accuracy (1-5) &4.07 / 0.18 & 3.42 / 0.67 & 2.20 / 1.27 & \textbf{4.09} / 0.27 & 4.08 / 0.15 & 4.20 / 0.24 & 1.56 / 0.55 & \textbf{4.35} / 0.20 & 4.16 / 0.20 & 3.95 / 0.21 & 3.90 / 0.90 \\
 & Fluency (1-5) &\textbf{3.77} / 0.14 & 3.20 / 0.50 & 2.71 / 0.68 & 3.77 / 0.20 & 3.76 / 0.12 & 3.84 / 0.15 & 1.65 / 0.55 & \textbf{3.92} / 0.12 & 3.81 / 0.13 & 3.66 / 0.14 & 3.72 / 0.41 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in German.}
\label{tab:llm-judge-de}
\end{table}

\begin{table}[ht]
\centering
\begin{adjustbox}{width=\textwidth,center=\textwidth}
\begin{tabular}{@{}ll>{\columncolor[HTML]{EFEFEF}}cccc|>{\columncolor[HTML]{EFEFEF}}ccccccc@{}}
Task & Metric & 
\begin{turn}{90}\textbf{Salamandra 2B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 1.7B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 1.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 2B}\end{turn} &
\begin{turn}{90}\textbf{Salamandra 7B}\end{turn} &
\begin{turn}{90}\textbf{EuroLLM 9B}\end{turn} &
\begin{turn}{90}\textbf{FLOR 6.3B}\end{turn} &
\begin{turn}{90}\textbf{Gemma-2 9B}\end{turn} &
\begin{turn}{90}\textbf{Llama-3.1 8B}\end{turn} &
\begin{turn}{90}\textbf{Mistral-7B-v0.3}\end{turn} &
\begin{turn}{90}\textbf{Teuken 7B}\end{turn} \\
\midrule
\textbf{Reading Comprehension}  & Relevance (0/1) &0.81 / 0.06 & 0.65 / 0.10 & 0.53 / 0.17 & \textbf{0.87} / 0.05 & 0.81 / 0.06 & \textbf{0.91} / 0.03 & 0.38 / 0.10 & 0.87 / 0.05 & 0.88 / 0.05 & 0.89 / 0.05 & 0.32 / 0.13 \\
 & Comprehension (1-5) &3.32 / 0.49 & 2.93 / 0.73 & 1.90 / 0.93 & \textbf{3.61} / 0.55 & 3.31 / 0.47 & \textbf{3.72} / 0.45 & 2.30 / 0.69 & 3.62 / 0.59 & 3.60 / 0.44 & 3.67 / 0.43 & 2.77 / 0.88 \\
\midrule
\textbf{Translation form Language}  & Accuracy (1-5) &\textbf{4.20} / 0.15 & 3.74 / 0.43 & 2.25 / 1.43 & 4.08 / 0.37 & 4.17 / 0.19 & 4.04 / 0.30 & 1.86 / 0.69 & \textbf{4.43} / 0.20 & 4.23 / 0.19 & 4.04 / 0.30 & 3.08 / 1.28 \\
 & Fluency (1-5) &\textbf{3.81} / 0.12 & 3.44 / 0.30 & 2.79 / 0.71 & 3.68 / 0.26 & 3.80 / 0.14 & 3.69 / 0.24 & 1.83 / 0.57 & \textbf{4.03} / 0.16 & 3.83 / 0.15 & 3.71 / 0.18 & 3.09 / 0.73 \\
\midrule
\textbf{Translation to Language}  & Fluency (1-5) &3.78 / 0.14 & 3.47 / 0.39 & 2.81 / 0.77 & \textbf{3.89} / 0.19 & 3.76 / 0.14 & 3.88 / 0.17 & 2.04 / 0.72 & \textbf{3.99} / 0.15 & 3.86 / 0.16 & 3.76 / 0.17 & 3.41 / 0.52 \\
 & Accuracy (1-5) &4.14 / 0.18 & 3.76 / 0.49 & 2.34 / 1.44 & \textbf{4.20} / 0.26 & 4.10 / 0.18 & 4.28 / 0.23 & 2.04 / 0.93 & \textbf{4.42} / 0.19 & 4.22 / 0.22 & 4.11 / 0.28 & 3.47 / 1.11 \\
\hline
\end{tabular}
\end{adjustbox}
\caption{Results of \textit{instructed} models to \llmJudge{} tasks in Italian.}
\label{tab:llm-judge-it}
\end{table}


\begin{comment}
    

\subsection{Strengths and Weaknesses}
% inspired in a section from phi3 paper, could also go in the safety chapter
% TODO JAB.

We consider the \llmJudge{} results on instructed models preliminary and unreliable. The main reason behind this is that there is no literature validating the use of \PrometheusLarge{} for tasks in languages other than English. Thus the evidence we have of this working is anecdotal. This highlights the importance of creating multilingual judge evaluation benchmarks based on human-annotated data.

We also have evidence that \PrometheusLarge{} can focus on overall text quality and instruction following rather than on the specific rubric given to the model, and is limited by its own ability at the task at hand. This is exemplified by the fact that most models get passing scores in the \emph{mathematical correctness} metric in the \llmJudge{} setup, while having negligible scores in the LM Evaluation Harness, and \PrometheusLarge{} assigning very bad scores for the same metric when only the correct numerical answer in the \verb|prediction| field.
\todo{JAB revisar}
\end{comment}