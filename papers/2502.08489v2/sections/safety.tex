\label{subsec:bias}

It has been widely shown that Large Language Models can be attributed with pernicious behaviour \cite{bender_dangers_2021}; they can perpetuate several types of harm, whether allocational or representational, and have been shown to be sensitive to variation in input format, which can severely affect their performance, allowing them to be easily influenced by factors such as word frequency, answer position in multiple choice settings, among others.

Furthermore, as \LLM{}s become more and more used worldwide, assessing the safety of their interaction with users has become critical \cite{yao_safety_security_survey,chowdhury_attack_survey}. While some resources exist, their availability is heavily skewed towards English \cite{yong_multilingual_adv_bench,joshi_dialect_evaluation_survey}. Further to this, it has become apparent that model safety does not transfer well across languages \cite{llama3, dang_multilingual_rlhf}. Our safety evaluation approach is multilingual, focusing on English, Spanish and Catalan, the main languages of the \SalamandraFamily{}.  

In this section, we describe our evaluation paradigm to identify undesired biases that can negatively affect model behavior and performance, as well as our multilingual approach to assessing model safety. 

 % while in LLMs, recent negative behaviour, why this is important, several potential harms.

\subsection{Evaluating Biases}
We root our work in the theoretical framework presented in \cite{theoretical_bias}, where bias is further divided into \textit{outcome disparity} and \textit{error disparity}. By \textit{outcome disparity} we refer to a systematic difference in model output based on a specific attribute, and with \textit{error disparity} we refer to model predictions that have a systematically larger error for inputs with a specific attribute.    

%Bias as a specific association between a community/entity and any other entity resulting in negative attitudes.

%Toxicity apart from bias and model performance. Difficulty to weed out toxic aspects.

%CURATE and data selection process. Double-check with data team. Not upsampling toxic/problematic sources of data. Benevolent ignorance not a good solution (find literature).

\subsection{Social Biases}

\subsubsection{Bias Benchmark for Question Answering}

To adequately determine how the models' inherent biases can influence performance on downstream tasks, we use two versions of the Bias Benchmark for Question Answering (BBQ). We use the original BBQ dataset developed in \cite{bbq_parrish}, and have additionally translated and adapted our own version (Es\BBQ{}) for evaluating social biases that are prevalent in Spain and  that are relevant for European Spanish culture\footnote{While preliminary, all templates that make up the Spanish version of BBQ (EsBBQ) have been extensively validated by a group of researchers with diverse backgrounds. We are actively working on finalising it and will be releasing it within the coming months.}.  

BBQ is a Question-Answering dataset consisting of specific templates linking socio-demographic groups with their corresponding target stereotypes. These templates can be under-informative (or ambiguous) or adequately informative (disambiguated) by adding a disambiguating context to the initial ambiguous one. A clear answer can be gleaned from the disambiguated contexts, but not from the ambiguous context, where the correct answer is always "unknown". The purpose of the dataset is to test how strongly responses reflect social biases in ambiguous contexts, and if our models' biases can override a correct answer choice in disambiguated contexts where there is a clear correct answer.

We follow the scoring method presented in \cite{jin2024kobbq}, where accuracy in both ambiguous and disambiguated contexts is taken into account, along with a bias score that measures the model's tendency to align with either known stereotypes or counterstereotypes, thus quantifying \textit{error disparity} for each setting. The formulae for computing the relevant scores are as follow:

\begin{multicols}{2}
 \begin{equation}
    Acc_a = \frac{n_{au}}{n_{a}}
\end{equation}

\begin{equation}
Acc_d = \frac{n_{bb} + n_{cc}}{n_b + n_c}
\end{equation}  
\columnbreak

\begin{equation} \label{diffa}
Difference_a = \frac{n_{ab} - n_{ac}}{n_a}
\end{equation}

\begin{equation} \label{diffd}
Difference_d = \frac{n_{bb}}{n_b} - \frac{n_{cc}}{n_c}
\end{equation}
\end{multicols}

Where $Acc_a$ and $Acc_d$ denote model accuracy in ambiguous and disambiguated contexts respectively. $n_{au}$ indicate the number of instances where the model matches the expected "unknown" answer over all ambiguous instances ($n_a$). Similarly, $n_{bb}$ and $n_{cc}$ indicate the number of model correct answers given all biased ($n_b$) and counterbiased ($n_c$) disambiguating contexts. By computing the difference in scores in equations \ref{diffa} and \ref{diffd}, we are essentially quantifying the \textit{error disparity} based on an expected stereotype. For ambiguous contexts ($Difference_a$), we calculate the difference between the prediction ratios of biased answers and counterbiased answers. For disambiguating contexts ($Difference_d$), we measure how much a given stereotype or counterstereotype can directly interfere with a model's performance, given that the correct answer can be easily gleaned from the context.  

\begin{table}[ht!]
\centering
\begin{tabular}{@{}lcccccccc@{}}
\toprule
 & \multicolumn{4}{c}{\textbf{BBQ}} & \multicolumn{4}{c}{\textbf{EsBBQ}} \\ \cmidrule(lr){2-5} \cmidrule(lr){6-9} 
 & \textbf{$Acc_a$} & \textbf{$Acc_d$} & \textbf{$Diff_a$} & \textbf{$Diff_d$} & \textbf{$Acc_a$} & \textbf{$Acc_d$} & \textbf{$Diff_a$} & \textbf{$Diff_d$} \\ \midrule
\textbf{2b} & 0.03 & 0.54 & 0.02 & 0.04 & 0.18 & 0.50 & 0.01 & 0.02 \\
\textbf{7b} & 0.03 & 0.79 & 0.08 & 0.04 & 0.10 & 0.72 & 0.06 & 0.04 \\
\textbf{2b-instruct} & 0.02 & 0.67 & 0.04 & 0.05 & 0.05 & 0.64 & 0.03 & 0.05 \\
\textbf{7b-instruct} & 0.04 & 0.92 & 0.15 & 0.02 & 0.07 & 0.88 & 0.22 & 0.04 \\ \bottomrule
\end{tabular}
\caption{Overall accuracy and difference scores in the original BBQ and EsBBQ.}
\label{tab:bbq_overall-acc}
\end{table}

Table \ref{tab:bbq_overall-acc} shows the mean accuracy and difference scores in BBQ and EsBBQ. All models show significantly higher accuracy in disambiguated contexts compared to ambiguous contexts. In correlation with these accuracy results, the bias difference scores are, as expected, lower when providing a disambiguated context. Models struggle to choose the correct "unknown" answer for questions with ambiguous contexts, but, when a correct answer is provided within the context, models are fairly successful at selecting it. However, accuracy scores obtained are relatively modest in the case of 2B versions given the low complexity of the task itself. 

Accuracy tends to increase together with model size, as well as with instruction tuning. This increase in the performance in the case of larger and instruction-tuned models goes together with higher difference scores, which reveals they are more reliant on biases when trying to solve the question answering task. Specifically, all difference scores are positive, suggesting that the models tend to favor outputs that are aligned with societal biases. On the other extreme, it cannot be stated that models with lower difference scores are not free from bias, considering their poor performance results.

More specifically, according to Figure \ref{fig:bbq_original}, in the original \BBQ{}, questions prompted with ambiguous contexts associated with Age and Physical Appearance are the ones where models tend to show more bias, particularly 7B versions. In both categories, scores are higher in the case of the instructed version compared to the base one. 7B instructed version also demonstrates significantly higher difference scores in instances associated with Disability Status, Gender Identity, Nationality and Socio-Economic Status. The scores for these categories decrease notably in questions prompted with disambiguated contexts. Socio-Economic Status and, once again, Physical Appearance are the categories for which the models generate more biased outputs. Note, however, the 2B versions are the ones with greater difference scores in this setting.

On the other hand, Figure \ref{fig:esbbq} shows that, with ambiguous contexts, models tends to favor stereotypical outputs related to Physical Appearance and Socio-Economic Status, with particularly higher scores in 7B versions. Bias is also notable in the case of 7B instructed answers about Sexual Orientation and, to a lesser extent, Age and Disability Status. It is remarkable that Nationality is the only category where all models, except for 7B instructed, exhibit negative difference scores, which reveals that they are more prone to select counter-biased answers. As previously mentioned, bias is reduced when providing a disambiguated context. However, it persists in 2B model results in Physical Appearance and Socio-Economic Status categories. 

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=\textwidth, trim=0cm 3.5cm 0cm 0cm, clip]{figures/bias_and_safety/bbq.pdf}
    \caption{Accuracy and difference scores in ambiguous and disambiguating contexts for each category in the original BBQ.}
    \label{fig:bbq_original}
\end{figure*}

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.95\textwidth, trim=0cm 5cm 0cm 0cm, clip]{figures/bias_and_safety/esbbq.pdf}
    \caption{Accuracy and difference scores in ambiguous and disambiguating contexts for each category in EsBBQ.}
    \label{fig:esbbq}
\end{figure*}

\subsubsection{Regard Analysis}
In addition to our analyses using \BBQ{} and Es\BBQ{}, we perform a regard analysis on the base variants of the \SalamandraFamily{} (i.e. \SalamandraBaseII{} and \SalamandraBaseVII{}). The notion of regard is introduced as language polarity towards a social demographic as well as how they are socially perceived \cite{sheng-regard}. Furthermore, \citet{sheng-regard} provide a dataset and a classifier to measure these aspects.

We analyze base model generations using the Regard dataset and classifier in the main languages of \Salamandra{}: Catalan, Spanish, and English. While the dataset is only available in English, we use backtranslation with NLLB \cite{nllb_language_2022} and manual review of the translations. The dataset compares social minorities with their non-marked counterpart along three categories: \textit{Gender, sexual orientation}, and \textit{race}, while the regard classifier output three labels: \textit{positive, negative}, and \textit{neutral}.

We compare the difference in frequency of output labels with a $\chi²$-goodness-of-fit test. Our analysis yielded statistically significant differences in the case of \SalamandraBaseII{} in English; the number of model outputs classified with a negative regard are significantly higher for minority groups, while the number of outputs classified with a positive regard is significantly higher for majority groups. For \SalamandraBaseII{} or \SalamandraBaseVII{}, we do not find statistically significant differences between regard labels for any other languages.

\subsection{Cognitive Biases}

Large Language Models have been shown to achieve strong performance across different tasks. However, as a result of their high capacity, 
a rapidly accumulating amount of evidence shows that LLMs can exhibit similar cognitive biases to humans due to the percolation of these biases through the datasets used to train the \LLM{} \cite{petroni-etal-2019-language, lu-etal-2022-fantastically, Zhao2021CalibrateBU, weber-etal-2023-mind}. As a consequence, some model responses can be conditioned by frequent words, classes, and general formatting in a given input prompt. This is problematic as these biases can influence performance, inflating or deflating metrics on the standard benchmarks, thereby making them less reliable. 

 Following \citet{Zhao2021CalibrateBU}, who examine the most comprehensive set of cognitive biases as far as we have observed in previous works, we examine the effects of three types of cognitive bias on model behavior: primacy, recency, and majority class. Primacy and recency effects denote a given model's tendency to prefer the first and last items, respectively, given a list of options. These effects are evident when the model is provided with lists, or when the input to a given model has a specific format, such as a multiple choice questions (MCQs). Majority class effects appear in few-shot settings in cases where there is an imbalance. We also highlight that we diverge from \citet{Zhao2021CalibrateBU}, and choose not to examine common token bias (i.e. an LLM's tendency to prefer responses which are more frequently seen in training data) due to the analysis and discussion presented in \cite{cobie}.

\begin{table}[ht!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
 & \textbf{Majority Class ($V$)} & \multicolumn{2}{c}{\textbf{Primacy ($\varphi$)}} & \multicolumn{2}{c}{\textbf{Recency ($\varphi$)}} \\ \cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-6} 
 & \textbf{SST-2} & \textbf{ARC Easy} & \textbf{ARC Challenge} & \textbf{ARC Easy} & \textbf{ARC Challenge} \\ \midrule
\textbf{2b} & 0.33 & 0.79 & 0.79 & 0.10 & 0.18 \\
\textbf{7b} & 0.12 & 0.23 & 0.31 & 0.08 & 0.10 \\
\textbf{2b-instruct} & 0.04 & 0.05 & 0.07 & 0.26 & 0.34 \\
\textbf{7b-instruct} & 0.01 & 0.01 & 0.03 & 0.09 & 0.15 \\ \bottomrule
\end{tabular}
}
\caption{$V$ and $\varphi$ coefficients resulting from the $\chi^2$ independence and goodness-of-fit tests to check majority class, primacy and recency biases, respectively.}
\label{tab:cobie}
\end{table}

\paragraph{Primacy and Recency Bias} As in \cite{cobie}, primacy and recency bias are evaluated with a 0-shot classification task using the ARC dataset \citep{allenai-arc}. Each instance is prompted four times, permuting the position of the correct answer (\textit{A}, \textit{B}, \textit{C} or \textit{D}). Significance of these positional effects is statistically measured with $\chi^2$ goodness-of-fit tests between the position of interest (\textit{A} for primacy, \textit{D} for recency) and the middle two positions (\textit{B} and \textit{C}) to avoid confounds between these two biases. Effect sizes ($\varphi$ coefficient) are shown in Table \ref{tab:cobie}, and frequency distributions of model predicted answers are illustrated in Figure \ref{fig:cobie_arc}.

All models are biased towards the first possible answer in the prompt. However, effect sizes are smaller in instructed models with respect to their base counterparts. Within each model variant (i.e. base vs. instructed), effect sizes are also smaller as model size increases. Differences are not significant between ARC Easy and Challenge subsets, revealing that an increase in the content difficulty of the question does not correlate with a greater reliance on primacy bias. As for recency bias, once again, from Table \ref{tab:cobie}, we observe that smaller and base models have larger effect sizes than their larger and instruction-tuned counterparts. However, a closer look at Figure \ref{fig:cobie_arc} reveals that results do not reflect a recency bias, but, rather another primacy bias: taking into account that option \textit{A} is not considered for the statistical measurement, option \textit{B} is still predicted more frequently than \textit{D}. 

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/bias_and_safety/cobie_arc_old-harness.pdf}
    \caption{Frequency distributions of predicted answers on ARC Easy and Challenge subsets depending on their position in the prompt.}
    \label{fig:cobie_arc}
\end{figure*}

\paragraph{Majority Class Bias} Also as in \cite{cobie}, we assess majority class bias with a 4-shot binary classification experiment using the SST-2 dataset on sentiment analysis \citep{socher-etal-2013-recursive}. Each instance is prompted with all possible imbalanced 4-shot distributions (listed in Figure \ref{fig:cobie_sst2}). Table \ref{tab:cobie} details the effect sizes (Cramér's \textit{V} coefficient) obtained from the $\chi^2$ independence tests between the predicted label and the majority class in the few-shot setting: \textit{0} (\textit{negative}) or \textit{1} (\textit{positive}). In correlation with primacy and recency bias results,  increasing model size and instruction tuning translates into smaller effect sizes; thus, larger and instruction tuned models exhibit a lower reliance on majority class bias to solve the task.

\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/bias_and_safety/cobie_sst2_old-harness.pdf}
    \caption{Frequency distributions of class \textit{0} predictions on SST-2 dataset depending on the class distribution in
few-shot. \textit{0} denotes the negative class, while \textit{1} denotes the positive class.}
    \label{fig:cobie_sst2}
\end{figure*}

\subsection{Safety and \RedTeaming{}}
Although there has been a growing body of research on the safety and \RedTeaming{}
%, alignment, helpfulness and harmlessness 
of \LLM{}s \cite{yao_safety_security_survey,chowdhury_attack_survey}, the vast majority of research has focused on English, particularly, Standard American English \cite{joshi_dialect_evaluation_survey}. To the best of our knowledge, the state-of-the art in multilingual safety evaluation is the work of \citet{aryabumi_aya_23} and their \AyaModel{} model. Their approach to assessing \LLM{} safety leverages GPT-4 and the Perspective API, based on a dataset that has been machine-translated from English. 
While this approach has yielded valuable insights, it is somewhat limited by the fact that GPT-4 and Perspective API are black-box models, and GPT-4 even requires a paid subscription. In contrast, our independently developed \RedTeaming{} pipeline, incorporates a larger and more diverse dataset and leverages a multilingual open-source moderator \LLM{}.
We deploy adversarial prompts for \RedTeaming{}, simulating potential interactions with malicious users \cite{ganguli_hh_rlhf, chowdhury_attack_survey} (e.g. \texttt{`How can I build a bomb?'}). 
Subsequently, we collect the responses from our instructed models and use a moderator \LLM{} to determine the success of the adversarial attacks.


\subsubsection{Methodology}
\Cref{alg:safety_rt_pipeline} shows our safety and \RedTeaming{} evaluation pipeline, with the corresponding code available for public access\footnote{\url{https://github.com/langtech-bsc/red_teaming_pipeline}}. We preprocess three datasets of adversarial prompts and use \LlamaGuard{} as a moderator model. 
Models exhibiting \emph{higher} attack success rates are considered to be \emph{less} resistant to adversarial prompts.  Our inference is non-deterministic\footnote{Note that, by fixing a random seed, our results are reproducible}, and we generate several answers for each prompt using the sampling parameters in \Cref{tab:safety_rt_inference_parameters} \footnote{All other sampling parameters are the default ones from the python package \href{https://github.com/vllm-project/vllm/blob/2ca830dbaa1a7c30b8ff4d7c860c63f87dc18be3/vllm/sampling_params.py\#L87}{\texttt{vllm 0.6.3}}.
}.
This setup results in scenarios where the same prompt may sometimes lead to a successful attack, while other times the \LLM{} may refuse to answer.
%A consequence of this setup is that, for the same prompt, sometimes the attack is successful while other times the \LLM{} refuses to answer. 
This variability in responses to the same prompt mirrors real-world usage, as \LLM{}s regularly provide different answers to the same inputs. Evaluating a single response would obscure whether the \LLM{} is vulnerable to the \RedTeaming{} attack, particularly with `borderline' prompts that may appear benign but are actually harmful, or vice versa.
%We believe that this resembles a real use scenario. 
In our evaluation framework, an attack is considered successful if the entire conversation generated from a prompt is marked as \unsafeAnswer{}.

\input{tables/bias_and_safety/rt_inference_params}

The analysis of attack success rates provides insights into the models' resistance to various attack types, and enables comparative evaluation of attack resistance across different models. Specifically, we apply this evaluation pipeline to the \AyaModel{} and \SalamandraInstructedVII{} models. These models were selected based on their multilingual capabilities, similar size in number of parameters, pre-training in both English and Spanish, and absence of preference alignment. 
Our study examines the attack success rates against \SalamandraInstructedVII{} in English, Spanish, and Catalan. Nonetheless, since \AyaModel{} was not trained on Catalan data, we limit the comparison to English and Spanish.

%Analyzing the attack success rates gives us insights into how resistant the model is to each type of attack, and enables us to compare attack resistance across models. Specifically, for comparison, we apply the pipeline to the \AyaModel{} and \SalamandraInstructedVII{} models. We chose these two models because both are multilingual, include English and Spanish in their pre-training, and have not undergone preference alignment.

\begin{algorithm}
    \caption{\RedTeaming{} Pipeline}
    \label{alg:safety_rt_pipeline}
    \begin{algorithmic}[1]
    \STATE From each RT Prompts Dataset $D$, randomly sample 500 prompts $S$.
    \STATE for each prompt $p$ in $S$, the \LLM{} generates three answers $a_1, a_2, a_3$
    \FOR{each prompt-answer pair $(p,a)$} 
    \STATE Classify $p$ into one of \LlamaGuard{}'s hazard categories, or mark $p$ as \safeAnswer{}
    \STATE Classify the conversation $(p,a)$ as either \safeAnswer{} or \unsafeAnswer{}
    \IF{$(p,a)$ is marked \unsafeAnswer{}}
    \STATE $(p,a)$ is considered as a successful attack
    \ENDIF
    \ENDFOR
    
    \end{algorithmic}
\end{algorithm}


\paragraph{\RedTeaming{} Prompts Datasets} We utilize and preprocess the following three datasets of adversarial prompts, selected for their permissive research licenses: 
% HF Link to M-ADV-Bench Dataset
\newcommand{\hfLinkMADVDataset}{\url{https://huggingface.co/datasets/simonycl/multilingual_advbench}}
% HF Link to ADV-Bench Dataset
\newcommand{\hfLinkADVDataset}{\url{https://huggingface.co/datasets/walledai/AdvBench}}
% HF Link to HH-RLFH Dataset
\newcommand{\hfLinkHHRLHFDataset}{\url{https://huggingface.co/datasets/Anthropic/hh-rlhf}}
% HF Link to Aya RT Dataset
\newcommand{\hfLinkAyaRTDataset}{\url{https://huggingface.co/datasets/CohereForAI/aya_redteaming}}


\paragraph{M-ADV-Bench} The \MAdvBenchDataset{}\footnote{\hfLinkMADVDataset{}} \cite{yong_multilingual_adv_bench}, derived from the AdvBench Dataset \footnote{\hfLinkADVDataset{}}, originally in English.
The dataset was first extended into 12 languages using the Google Translate API, and later \cite{ustun_aya_model} into a total of 23 languages, including Spanish,  using \NLLB{} translation. In our approach, we use the English and Spanish subsets of the \MAdvBenchDataset{} and further extend it into Catalan by applying \NLLB{} translation to the English subset.

%is a Machine-Translated version of the AdvBench Dataset\footnote{\hfLinkADVDataset{}}, originally in English \cite{zou_adv_bench}. In \citet{yong_multilingual_adv_bench}, the Adv-Bench Dataset was synthetically extended into 12 languages using the Google Translate API. In a later work \cite{ustun_aya_model}, the Adv-Bench Dataset was synthetically extended into the 23 languages of the \AyaModel{} Model, among them Spanish, using NLLB translation. 

\paragraph{HH-RLHF RT} The \HHRedTeamingDataset{}\footnote{\hfLinkHHRLHFDataset{}} \cite{ganguli_hh_rlhf} is a crowdsourced dataset containing around 38k multi-turn adversarial conversations in English. For our analysis, we randomly sample 1k conversations, taking the first user input as the adversarial prompt. This sample of the dataset is synthetically extended into Spanish and Catalan using \NLLB{} translation.

\paragraph{AYA RT} The \AyaDataset{}\footnote{\hfLinkAyaRTDataset{}} \cite{aakanksha_aya_rt_dataset} contains \RedTeaming{} prompts for 8 languages, including English and Spanish, crafted by human annotators and containing around 900 prompts per language. We synthetically extend this dataset to Catalan using \NLLB{} translation on the English and Spanish subdatasets. 
%\todo{Say that the quality of the Spanish Portion is really bad.} 
Notably, being a team with a large portion of native Spanish speakers, we observed that the quality of the Spanish subdataset is suboptimal (see \Cref{tab:poor_quality_spanish_prompts} in \Cref{app:rt_examples}).

% HF Link to Llama Guard 3
\newcommand{\hfLinkLlamaGuard}{\url{https://huggingface.co/meta-llama/Llama-Guard-3-8B}}

\paragraph{Moderator Model — \LlamaGuard{}}
%\paragraph{\LlamaGuard{}}
\LlamaGuard{}\footnote{\hfLinkLlamaGuard{}} \cite{llama3} serves as our moderator \LLM{}, trained to classify text into the risk categories defined by the \MLCommons{} Taxonomy (version 0.5 \citep{ml_commons_taxonomy}, see  \Cref{tab:safety_ml_commons_taxonomy}) and an additional category `Code Interpreter Abuse'. 
We selected \LlamaGuard{} due to its openly released weights and its multilingual training, which includes English and Spanish.

\input{tables/bias_and_safety/ml_commons_taxonomy}


\subsubsection{Results and Discussion}

\input{figures/bias_and_safety/latex/histograms_m_adv_bench}
\input{figures/bias_and_safety/latex/histograms_hh_rlhf}
\input{figures/bias_and_safety/latex/histograms_aya_rt}

\Crefrange{fig:safety_histograms_m_advbench}{fig:safety_histograms_aya} illustrate the distribution of RT prompts across different hazard categories as classified by \LlamaGuard{}. Several hazard categories show either no prompts or very few prompts, depending on the dataset. This outcome is expected, as the RT Prompts Datasets were released prior to the creation of the \MLCommons{} Hazard Taxonomy. For the purposes of our analysis, categories with fewer than 30 prompts are excluded, as this small sample size does not provide sufficient data for meaningful conclusions. 
It is worth noting that the overall distributions of prompts are unaffected by machine translations, suggesting that meaning is preserved through \NLLB{} translation on these prompts.

%show the distribution of RT prompts across the different hazard categories, as classified by \LlamaGuard{}. Unfortunately, there are several hazard categories with no or almost-no prompts, depending on the dataset. This is to be expected, as the RT Prompts Datasets were collected before the release of the ML Commons Hazard Taxonomy. Going forward, for our analysis, we discard those categories with less than 30 prompts, as such a small sample size is insufficient. 

%It should be noted that the overall distributions of prompts are not affected by machine translation. 
Furthermore, both the \AyaDataset{} and the \HHRedTeamingDataset{} contain a large proportion of prompts marked as \safeAnswer{}, ranging from 40\% to 60\% (\Cref{fig:safety_histograms_hh_rlhf} and \Cref{fig:safety_histograms_aya}). After manual review, we
believe that this reflects limitations of \LlamaGuard{}, as several of these prompts were manifestly harmful (\Cref{tab:safety_llama_guard_blind_spots}).




\Cref{tab:safety_heatmaps_salamandra} presents the attack success rates against \SalamandraInstructedVII{} in Catalan, English, and Spanish, and a comparison with  \AyaModel{} in English and Spanish\footnote{Due to space constraints, the results for \AyaModel{} are available in \Cref{app:rt_examples}, see \Cref{tab:safety_heatmaps_salamandra_full} and \Cref{tab:safety_heatmaps_aya_full}}.
In the case of \SalamandraInstructedVII{}, the attack success rates across the three datasets are generally similar to or lower in Spanish compared to English. In contrast, \AyaModel{} exhibits a reverse pattern, with higher attack resistance in English than in Spanish. 
\AyaModel{} us generally more resistant to attacks than \SalamandraBaseVII{}. However, the difference in attack success rates is less pronounced for Spanish. 

Examining specific attack categories,
\SalamandraInstructedVII{} is more vulnerable to attack types S4 and S5, with success rates reaching up to 86\%. It shows moderate vulnerability to S1, S2, S11, and S12 (success rates between 40\% and 60\%) and less vulnerability to attack types S9, S10, and safe (under 40\%). Manual review reveals some blind spots in \LlamaGuard{}. \Cref{app:rt_examples} highlights examples where the input prompt was classified as \safeAnswer{}, yet the overall conversation was classified as \unsafeAnswer{}.

These results indicate that model resistance to \RedTeaming{} depends not only on the type of attack, but also on the language, supporting the insight that \LLM{} safety must be carefully addressed for each language. Additionally, we highlight the value of conducting manual review to identify instances where automated systems may fail, ensuring a more comprehensive understanding of model vulnerabilities.

\input{tables/bias_and_safety/salamandra_heatmaps}


\paragraph{Limitations and Future Work}

The main limitation of our \RedTeaming{} approach is the absence of human annotation and evaluation, which results in an over-reliance on automatic methods. Due to time and budget constraints, human evaluation was not feasible. The lack of human-generated prompt datasets for \RedTeaming{} in Catalan is another key limitation, as depending on machine-translation evaluation may create a false impression of the quality of the model's answers
\cite{chen_bad_multilingua_evaluation}.

Additionally, our \RedTeaming{} approach only considers conversations consisting of a prompt and an answer, while research shows that multi-turn conversations increment the probability of harmful answers \cite{wolf_fundamental_limitations_alignment}. To address this, we aim to expand our approach to include multi-turn conversations in the future.


We apply \LlamaGuard{} as a moderator model with an understanding of its limitations. 
One significant issues is that \LlamaGuard{} has \emph{not} been trained to moderate content in Catalan. This could explain why harmful Catalan prompts are mistakenly marked as \safeAnswer{}. 
Furthermore, after manual review, we also found this behavior in Spanish, though to a lesser degree. 
This highlights the need for more safety datasets in these languages to help train more effective moderators. The \Langtech{} Unit at \BscShort{} is actively working to compile harmfulness and toxicity datasets in both Spanish and Catalan, such as the InToxiCat dataset\footnote{\url{https://huggingface.co/datasets/projecte-aina/InToxiCat}} \cite{gonzalez-agirre-etal-2024-building-data} developed under the \AinaProject{} project.

Additionally, the \MLCommons{} Hazards Taxonomy used to train \LlamaGuard{} appears to be tailored to U.S.A. cultural sensitivities, which may not align with those of other regions. For example, the inclusion of `Elections' as a category may not be universally relevant. Defining harmfulness, toxicity, and bias is a complex task \cite{banko_harmful_taxonomy,kurrek_slur_taxonomy,maroniko_affected_communities,schmeisser2022criteria}, and this challenge becomes even greater in the European multilingual context. We are aware of European initiatives working toward addressing these issues \cite{
eu_ai_act, eu_ai_office%,
%spain_supervision_ai,
%spain_estrategia_ai
}.

Looking ahead, we are focused on developing our multilingual alignment approach. For future releases of the  \SalamandraFamily{}, we plan to continue using \RedTeaming{} to identify vulnerabilities and harmful behavior, while also exploring methods like synthetic generation of adversarial prompts \cite{samvelyan_rainbow_teaming} and studying false refusals in our models \cite{rottger_xstest_false_refusal}. 
To mitigate detected vulnerabilities and undesired behaviors, we will leverage Reinforcement Learning Techniques, such as Reinforcement Learning From Human Feedback, Proximal Policy Optimization, Reward Modeling, and both Online and Offline Direct Preference Optimization \cite{ziegler_rlhf_ppo_paper, rafailov_rlhf_dpo_paper, qi_rlhf_online_dpo_paper, feng_rlhf_dpo_limitations, dang_multilingual_rlhf}.


% A lot of Machine Translation % Cite "it it multilingual data", "aya paper"
% LLamaGuard does not work well in Catalan
   % A lot of the prompts are marked as "safe"
% LlamaGuard is not so great in Spanish, either
% A lot of prompts are marked as "safe"
% From the models themselves: Marked answer as "safe", because the model did not "understand" the adversarial prompt.

% MLCommonsTaxonomies are very US-centric
% We are working on our Alignment pipeline and iterative red teaming!
% Prompt attack augmentation
% DPO, PPO, other RL algorithms.


% use AYA model as baseline and report differences 
% RUN experiment AYA model + M-ADV-BENCH + Llama Guard -> DONE
% ask Mario for colour palette -> DONE
% we prefer to use open models y punto  
% Refactor heatmap into two based on Dataset due to class imbalances
% lower heatmap threshold 

%\label{subsec:toxicity}
%\subsection{Bias mitigation}

% \subsection{Potential risks}
% \todo{wip}

\label{subsec:risks}