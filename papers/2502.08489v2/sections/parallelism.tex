One of the main technical challenges when training large language models at scale is the memory bottleneck imposed by the limited VRAM memory of modern processors. The reason is that, during training, not only the model parameters must be stored in the device's memory, but also the optimizer states, gradients, and activations. As a result, model weights have to be distributed across several devices, which makes parallelism and memory management techniques extremely necessary. We will be using intra-layer tensor parallelism along with inter-layer pipeline parallelism for model sharding, as well as activation checkpointing and flash attention to reduce the memory footprint. 

Another major challenge is compute efficiency, because even if it was possible to fit an entire model in a single GPU, the training time on that sole device would be unrealistically large. Fortunately, this can be easily tackled by employing data parallelism, which consists in distributing several data shards across copies of the model. This is the most common form of parallelism due to its simplicity, and can be seamlessly combined with model parallelism.

However, different forms of parallelism interact in non-trivial ways. Ideally, it should be possible to achieve a close-to-linear scaling, but the reality is that distributed training can be very communication-intensive. In addition to that, suboptimal combinations of parallelism parameters can easily lead to low throughputs. In order to avoid this, it is crucial to be aware of the trade-offs between memory footprint, device utilization, and amount of communication.

Finding the right balance is key to exploit the full potential of a big cluster such as MareNostrum 5. The catch is that, since each HPC system has its own idiosyncrasies, the most effective setup for large scale pre-training can only be identified through empirical testing. For this reason, we leveraged an optimization strategy over various hyper-parameters to find the training recipes that would maximize throughput in our particular environment. The number of training steps was capped in these preliminary runs, just enough for the iteration time to stabilize. Table \ref{tab:parallelism} summarizes the final selection of hyper-parameters for each model size. It is important to highlight that the number of nodes used during training was not constant throughout the process. Given that the supercomputer is a shared resource, the availability of nodes was subject to fluctuations based on overall demand.  As a result, the number of nodes utilized for training was adjusted accordingly, with periods of increased resources allowing the use of more nodes, and periods of limited resources necessitating a reduction in the number of nodes. In particular, the number of GPUs used to train the 7B model had to be occasionally downgraded to 256, and the 40B model training had to switch between 2,048 and 512 GPUs. 

\input{tables/parallelism_params}

As in can be seen in the table above, the 2 billion parameter model is small enough to not require partitioning, so only data parallelism was used in order to accelerate training. The 7B model, on the other hand, does require tensor parallelism \cite{megatron-lm} to mitigate out-of-memory errors. The tensor parallel size was set to 4, which is the maximum that can be safely used in 4-GPU servers without dramatically increasing the amount of communication \cite{interleaving}. Finally, for the 40B model, it was necessary to employ a combination of 4-way tensor parallelism within nodes and 2-way pipeline parallelism across nodes. In any case, the number of model replicas that can be allocated for model parallelism is inferred by dividing the number of GPU devices by the model parallel size, which is the product of tensor and pipeline parallel sizes. 

Regarding the tokenizer, for efficiency reasons, it is important to ensure that the per-GPU vocabulary size is multiple of 128 \cite{megatron-lm}. This means that the total size should be divisible by 128 times the amount of model chunks when employing parallelism techniques. If this is not the case, training frameworks such as NeMo can automatically extend the vocabulary with pad tokens, but it is preferable to have an optimal size by default.

%%% NEMOTRON-4
% We use a distributed optimizer to shard the optimizer state over the data-parallel replicas and reduce the memory footprint of training.
% The degree of data parallelism scaled from 16 to 64 as the batch size was ramped up. 

%%% YI
% We use the following techniques to tackle the memory and communication restrictions: 
% (1) ZeRO-1 to remove the memory consumption by partitioning optimizer states cross data-parallel processes; 
% (2) TP combined with PP within each compute node to avoid inter-node communication bottleneck, and the 3D parallel strategy is well designed and optimized to avoid using activation checkpointing and minimize the pipeline bubbles;
% (4) topology-aware resource allocation (ranking strategy) to minimize the communication across different layers of switches, which is the limitation of a typical fat-tree-topology