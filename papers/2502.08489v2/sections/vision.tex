To enable the model to process various modalities, we have adapted Salamandra to handle both images and videos. This was achieved through late-fusion techniques, which involve integrating a pre-trained encoder, a base large language model (LLM), and a projector. The training process mainly focuses on transforming the encoder's image embeddings to align with the LLM, enabling the model to comprehend a new modality.

% Similarly to the instruction-tuned variants discussed in the previous section (see \ref{subsec:instruct}), these vision-enabled models have not undergone an alignment phase.

\subsubsection{Data Mixture}

All data used to train our vision models was obtained under public licenses. This includes both multimodal and text-only data.

The data distribution used for fine-tuning is illustrated in Figure \ref{fig:visual_data_distr} . Most of it was sourced from LLaVA OneVision's \cite{llavaonevision} pre-processed data. This includes data from AI2D \cite{ai2d}, Cambrian \cite{cambrian1}, and high-quality datasets such as re-captioned detailed description data from LLaVA Next \cite{llavanext}. Diverse thematic data were included to enhance the model's capabilities in subtasks such as grounding, optical character recognition (OCR), document understanding, and mathematics. Additionally, we incorporated multilingual text-only data in various European languages and high-quality text-only data in Spanish, Catalan, Galician, and Basque, which were also used in the instruction tuning stage (see Section \ref{subsec:it-data}).

The prompt format and tokenizer remain consistent with those used for instruction-tuned models, with the addition of two new special tokens: \texttt{<image>} and \texttt{<video>}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figures/visual/visual_data_distribution.pdf}
    \caption{Overview of data distribution in visual instruction tuning phases. In total, the dataset contains 6.1 million instances, of which 842,000 are text-only. \textbf{(Left)} Language distribution in the text-only dataset. \textbf{(Center)} Distribution of multimodal versus text-only data. \textbf{(Right)} Distribution of task types across the multimodal dataset.}
    \label{fig:visual_data_distr}
\end{figure}

\subsubsection{Visual Instruction-Tuning}

We employed the LLaVA OneVision technique \cite{llavaonevision} to train Vision Salamandra. The model comprises a pre-trained encoder (Google SigLIP \cite{siglip} - 14 patches, 384x384 resolution), our Salamandra Instructed 7B as the LLM, and a 2-layer perceptron as the projector.

The training process was conducted in four phases:

\begin{itemize}
\item Phase 1: Pre-training. The multilayer perceptron projector was pre-trained from scratch to align image and text embeddings.
\item Phase 2: Pre-training continued with higher-quality data (e.g. re-captions, OCR).
\item Phase 3: Models were instruction-tuned to better understand user instructions and perform the requested tasks (e.g. Visual Question Answering, OCR), using single-image and text-only data.
\item Phase 4: A mixture of data types was introduced, including single images, multiple images, video and text.
\end{itemize}

In the first phase, only the projector was trained, as the encoder and the LLM were frozen. In subsequent phases (2, 3 and 4), gradients were enabled for all modules, allowing full training. Image preprocessing during the first two phases used AnyRes\_Max\_5 \cite{llavanext}, while later phases employed AnyRes\_Max\_9 for higher-quality image understanding.

For examples of the model's usage, please refer to Appendix \ref{app:visualLMexamples}.