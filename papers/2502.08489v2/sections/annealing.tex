Building on recent trends in the literature \cite{llama3, nemotron4}, we adopt a final pre-training phase with a selected subset of high-quality tokens, aimed at completing training with the best available data and refine the model's output format to better match practical usage standards. Specifically, for the training of Llama 3 models \cite{llama3}, they refer to it as an annealing phase, in which the last 40 million tokens are up-weighted high-quality data sources, and the final model is an average of model checkpoints during this phase. In the Nemotron 4 340B model \cite{nemotron4} (and something similar is done for its 15B version \cite{nemotron4-15b}), the last 1 trillion tokens of an 8 trillion token pre-training uses a different data distribution and a steeper, lower magnitude learning rate schedule, which is reported to improve model performance. Most of the data they include in this phase consists of the highest quality sources encountered during the initial pre-training, supplemented with additional data to adapt the model to the downstream task format. Specifically, this includes question-answering data and sources with lower performance during pre-training, which are up-weighted to improve the model's task-specific performance. 

\subsubsection{Data Mixture}
For this phase, five data sources were selected. The first is FineWeb-Edu, a subset of educational web content automatically filtered from the FineWeb dataset \cite{penedo_fineweb_2024}, which has been shown to enhance performance on knowledge and reasoning benchmarks in downstream tasks. Another source of educational content included is Wikipedia, which, along with Colossal Oscar \cite{brack_community_2024}, helps maintain the multilingual language distribution established during pre-training. In addition, we include a subset of the StarCoder dataset \cite{li_starcoder_2023}, as previous studies \cite{ma2023trainingstagedoescode, yang2024llmwizardcodewand} have highlighted the relevant role of code data in pre-training to improve general reasoning capabilities in LLMs. Finally, we incorporate data structured in a task-oriented format, such as answering questions or completing sentences. For this purpose, we use a subset of the Aya Collection dataset \cite{singh_aya_2024}, which compiles prompts and completions from various sources reformulated using manually written instructional-style templates. Many of these datasets were automatically translated into multiple languages, and while we acknowledge the potential quality issues with translated datasets, this approach provided data for most of the languages present in our pre-training phase (only missing Serbo-Croatian and Occitan). We performed pre-processing on this dataset, filtering out languages not present in the pre-training corpus and removing datasets that were part of our instruction-tuning data or evaluation benchmarks. To introduce greater format variability, the documents from the Aya Collection are constructed by concatenating the input and target fields using one of six different separators, such as a newline, two newlines, or a single space, among others. 

The final dataset is composed of 55.51\% FineWeb-Edu, 25.32\% Colossal Oscar, 8.38\% Wikipedia, 7.17\% Aya Collection, and 3.63\% StarCoder, totalling 315 billion tokens.

\subsubsection{Training}
For training optimization in the continued training phase, we use once again the Adam optimizer \cite{adam_original, adam}, with $\beta_1 = 0.9$, $\beta_2 = 0.95$, epsilon set to $1 \times 10^{-8}$, and the weight decay to 0.1. We adopt a cosine learning rate schedule with a peak learning rate equal to the minimum learning rate of the pre-training phase. We did not warm up the learning rate; instead, it decays all the way down to one-tenth of the peak value.

% mirar sobretot seccio 3.1.3 del paper de llama3

% llama3 3.1.4: During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.

% e.g. nemotron-4 340b: We find that switching the data distribution and learning rate decay schedule at the end of model training significantly improves model quality. Concretely, after having pretrained for 8T tokens, we use the same loss objective and perform continued training on 1T additional tokens. In this additional phase of continued training, we utilize two distinct data distributions.... blabla

% e.g. nemotron-4 15b: Similar to recent work (Google, 2023), we find that switching the data distribution and learning rate decay schedule at the end of model training greatly improves model quality. Concretely, after having trained over the entirety of our 8T pre-training dataset, we use the same loss objective and perform continued training on small number of tokens in comparison to the pre-training tokens. In this additional phase of continued training, we utilize two distinct data distributions.... blabla
