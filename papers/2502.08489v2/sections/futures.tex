This technical report introduces Salamandra, a family of highly multilingual large language models with sizes ranging from 2 to 40 billion parameters. This suite of decoder-only Transformer-based models has been trained from scratch on a diverse and carefully curated dataset, encompassing 35 European languages as well as various programming languages. Our models achieve competitive results when compared to similar-sized open-source models on automatic benchmarks, but they still lag behind models developed by leading tech companies.

All base checkpoints and their fine-tuned counterparts are made publicly available under a permissive Apache 2.0 license. In addition to the open weights, we also release our training recipes and evaluation framework. This transparency aims to foster further research and the development of new applications, particularly for languages that are often poorly represented in current large language models, thus contributing to a more inclusive LLM landscape. Our hope is that, by sharing our models and documenting our experience, NLP practitioners can be empowered to build upon the work presented in this technical report.

As future work, we plan to openly release improved versions of our models. The first step will be to update the weights of Salamandra 40B as soon as its training concludes, but our planned releases for the future also include aligned versions and new modalities. Beyond that, we see this project as an ongoing effort and intend to continue making incremental improvements across several key areas, such as data collection and post-training strategies. The lack of alignment with human preferences is a major limitation of this project, which is why we are committed to start working on alignment techniques that would certainly take the Salamandra family to a new level. On the data front, we will continue to gather pre-training data from under-represented languages, and we will pay special attention to the collection and generation of high-quality datasets for instruction and preference tuning.

% extended context, optimized architectures (moe), multimodality, alignment, synthetic data generation, alternative training methodologies (distillation)
