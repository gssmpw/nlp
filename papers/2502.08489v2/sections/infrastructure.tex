% SGirona slides: https://www.bsc.es/sites/default/files/public/u2416/20230220_marenostrum5_sergi_girona_bsc.pdf

Salamandra was trained in MareNostrum 5\footnote{\url{https://www.bsc.es/marenostrum/marenostrum-5}}, a pre-exascale EuroHPC supercomputer hosted and operated by Barcelona Supercomputing Center. The accelerated partition is based on Intel Sapphire Rapids and Nvidia Hopper GPUs (H100s) \cite{h100}, totaling 1,120 nodes with four 64GB H100 GPUs each. On the other hand, the general-purpose compute partition has 6,480 nodes based on Intel Sapphire Rapids. Each node has 112 cores and 256 GB of main memory. However, for data processing tasks, a small subsystem of high-memory nodes with 1,024 GB was used.

The peak performance is around 46 petaflops for the general-purpose partition and 260 petaflops for the accelerated partition. The network topology is fat-tree in both cases. Inter-node communication uses InfiniBand NDR 200, while the GPUs within each node are connected via NVLink. 

Regarding the storage system, MareNostrum 5 has a total net capacity of 248 PetaBytes on SSD and hard disks. However, during the actual training, data was transferred to the internal memory of each node to optimize input/output (IO) operations and improve processing speed. During our preliminary experiments, we noticed that this would significantly reduce the time spent reading chunks of data from disk, which posed a significant bottleneck.

It is worth mentioning that the node configuration of our accelerated cluster is rather atypical, featuring Nvidia H100 GPUs with 64 GB of VRAM, instead of the more standard 80 GB configuration. Additionally, each node is equipped with 4 GPUs, as opposed to the 8 GPUs commonly found in Nvidia's DGX systems\footnote{\url{https://www.nvidia.com/en-gb/data-center/dgx-h100}}. As will be further detailed in the following section, these hardware features will influence the choice of hyper-parameters for parallelism. Specifically, the reduced VRAM memory limits the model chunks' size, while the lower number of GPUs constrains the level of tensor parallelism that can be achieved.  Consequently, careful tuning of such parameters is necessary to optimize performance within these constraints. 
