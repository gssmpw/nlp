\begin{table}[h!]
\centering
\small
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
Model & Nodes & GPUs & TP & PP & DP & MBS & GBS & Context & Batch Size & Tokens \\
\midrule
2B   & 64 & 256     & 1 & 1 & 256 & 1 & 512   & 8,192 & $\sim$4M & 12.9T \\
7B   & 128 & 512     & 4 & 1 & 128 & 2 & 512   & 8,192 & $\sim$4M & 12.9T \\
40B  & 512 & 2,048   & 4 & 2 & 128 & 1 & 1,024 & 4,096 & $\sim$4M & $\sim$9T \\ \bottomrule
\end{tabular}
\caption{Number of accelerators and parallelism hyper-parameters used to train each model.}
\label{tab:parallelism}
\end{table}
% total_gpu_hours = iteration time (s) × number of iterations × number of GPUs ÷ 3600 s/hour