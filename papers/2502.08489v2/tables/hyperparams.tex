
\begin{table}[ht!]
	\centering
	\begin{tabular}{l|>{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{3cm}}
	\toprule
    & \textbf{2B}  & \textbf{7B}  & \textbf{40B}\\
	\midrule
    Context Length & 8,192 & 8,192 & 4,096 \\
    % init\_method\_std & 0.014 & 0.02 & 0.007 \\
	Peak Learning Rate & $2 \times 10^{-4}$ & $3  \times 10^{-4}$ & $5 \times 10^{-5}$ \\
    Min. Learning Rate & $2 \times 10^{-5}$ & $3  \times 10^{-5}$ & $9 \times 10^{-6}$ \\
    GQA & No & Yes & Yes \\
	Activation Function   & \multicolumn{3}{c}{SwiGLU} \\
	Vocabulary Size       & \multicolumn{3}{c}{256,000} \\
    Optimizer & \multicolumn{3}{c}{AdamW ($\beta_1=0.9, \beta_2=0.95, \epsilon=1\times10^{-8}$)} \\
    Scheduler & \multicolumn{3}{c}{CosineAnnealing} \\
    Warmup Steps & \multicolumn{3}{c}{2,000} \\
    Constant Steps & \multicolumn{3}{c}{0} \\
	Positional Embeddings & \multicolumn{3}{c}{RoPE ($\theta=10,000$, $pctg=1.0$)} \\
    Gradient Clipping & \multicolumn{2}{c}{1.0} & \multicolumn{1}{c}{[0.3 - 1.0]} \\
    Weight Decay & \multicolumn{3}{c}{0.1} \\
    Floating-point Precision & \multicolumn{3}{c}{BFloat16} \\
	\bottomrule
	\end{tabular}
	\caption{Pretraining hyper-parameters for Salamandra 2B, 7B, and 40B.}
	\label{tab:pretraining_params}
\end{table}