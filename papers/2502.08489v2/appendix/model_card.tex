\begin{longtable}[c]{ p{.2\textwidth} | p{.7\textwidth} } 
\toprule
\multicolumn{2}{c}{\textbf{Model Summary}} \\
\toprule
\multicolumn{1}{l|}{Developers} & Language Technologies @ Barcelona Supercomputing Center. \\ \midrule
\multicolumn{1}{l|}{Description} & Standalone Large Language Model.  \\ \midrule
\multicolumn{1}{l|}{License} & Apache 2.0. \\ \midrule
\multicolumn{1}{l|}{Status} & Static model trained on an offline dataset. \\ \midrule
\multicolumn{1}{l|}{Architecture} & Dense decoder-only model based on the original \mbox{Transformer} architecture with a few variations. See \refnumtitle{subsec:arch}. \\ \midrule
\multicolumn{1}{l|}{Variants} & 2B, 7B and 40B parameters, both base and instruction-tuned. Aligned versions to be released in the future.\\ \midrule
\multicolumn{1}{l|}{Initialization} & Random weights. \\ \midrule
\multicolumn{1}{l|}{In/Out Format} & Text. \\ \midrule
\multicolumn{1}{l|}{Dependencies} & None. \\
\toprule
\multicolumn{2}{c}{\textbf{Training}}      \\ \toprule
\multicolumn{1}{l|}{Hardware} & Accelerated partition from \marenostrum{}, which is composed of Nvidia Hopper GPUs. See \refnumtitle{subsec:infra}. \\ \midrule
\multicolumn{1}{l|}{Software} & NeMo Framework \cite{nemo} for pre-training, FastChat \cite{fastchat} for instruction tuning and LLaVA-OneVision \cite{llavaonevision} for vision fine-tuning. \\
% \multicolumn{1}{l|}{Compute Requirements} & Not reported. \\
% \multicolumn{1}{l|}{Carbon Footprint} & check llama/llama2/bloom papers. \\
\toprule

\multicolumn{2}{c}{\textbf{Data}} \\ \toprule
\multicolumn{1}{l|}{Overview} & Pre-trained on trillions of tokens from publicly available sources. The fine-tuning data includes a mixture of instruction datasets openly released by third parties. \\ \midrule
\multicolumn{1}{l|}{Training Data} & See \hyperref[table:datasets]{Table \ref{table:datasets}} from the datasheet (\hyperref[app:datasheet]{Appendix \ref{app:datasheet}}). \\ \midrule
\multicolumn{1}{l|}{Evaluation Data} & See \refnumtitle{harness-ibero}. \\ \midrule
\multicolumn{1}{l|}{Fine-tuning Data} & See \refnumtitle{subsec:it-data}. \\
\toprule

\multicolumn{2}{c}{\textbf{Evaluation}} \\ \toprule
\multicolumn{1}{l|}{Framework} & EleutherAI's LM-Evaluation-Harness \cite{harness}. \\ \midrule
\multicolumn{1}{l|}{Benchmark} & 5-shot evaluation in several downstream tasks. \\ \midrule
\multicolumn{1}{l|}{Results} & Reported in \refnumtitle{base_perf} and \mbox{\refnumtitle{it_perf}}. \\ 
\toprule

\multicolumn{2}{c}{\textbf{Usage and Limitations}} \\ \toprule
\multicolumn{1}{l|}{Application} & Salamandra is capable of open-ended text generation and can be used for both research and commercial applications. \\ \midrule
\multicolumn{1}{l|}{Known Caveats} & Do not use for downstream applications without prior assessment and mitigation of safety and fairness concerns. \\ \midrule
\multicolumn{1}{l|}{Sensitive Use} & Refrain from using for malicious purposes that may violate \mbox{applicable} laws or regulations. \\ \midrule
\multicolumn{1}{l|}{Ethics and Risks} & Reported in \refnumtitle{sec:safety}. \\
\bottomrule
\caption{Salamandra's Model Card \cite{model_card}.}
\label{tab:modelcard}
\end{longtable}