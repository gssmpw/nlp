We develop our own set of rubrics to evaluate
assistants' responses for each task, tailoring the type of rubric—either 5-point Likert scale or binary—and the descriptors within the rubrics to the task.

We provide a descriptor for each value of the rubric, this is, 1 to 5 in the Likert scales or 0 and 1 in binary rubrics. Some studies have suggested that coarse-grained rubrics—where a judge is only instructed to provide a score for one or more criteria— may correlate more with human judgements than rubrics in which each value has a description \citep{kim2024biggenbenchprincipledbenchmark}. However, for interpretability purposes, we want to understand what each judge score means. We thus provide the judge with descriptors rather than relying on its own internal definition and score division for each criterion.

The decision to use binary rubrics in some cases stems from the experience of defining the descriptors for Likert scales. During this process, we found that when we tried to divide certain criteria into 5 distinct scores, the granularity was too fine, making it challenging even to expert humans to score responses accurately based on the rubric. We believe this is another advantage of moving away from coarse-grained rubrics.

The rubrics we used for each task and criterion are:

\paragraph{Common-sense reasoning.} We evaluate the \emph{ending\_coherence} in the responses.
\begin{lstlisting}[label=lst:judge_rubrics_commonsens1,caption={Rubric used for the \emph{ending\_coherence} criterion.}]
[Is the ending generated by the model coherent?]
Score 1: The ending is incoherent with the preceding sentences, creating an unexpected conclusion that does not logically follow from the story context.
Score 2: The ending has some elements of coherence, but it still contains inconsistencies or gaps in logic that disrupt the flow of the story.
Score 3: The ending is mostly coherent with the preceding story, but there may be minor logical inconsistencies or elements that feel slightly out of place.
Score 4: The ending is coherent and follows logically from the preceding sentences, maintaining the story's flow and consistency.
Score 5: The ending is perfectly coherent, providing a natural and seamless continuation that fully aligns with the preceding sentences and enhances the story's overall sense and purpose.
\end{lstlisting}

\paragraph{Mathematics.} We evaluate \emph{reasoning\_capability} and \emph{mathematical\_correctness}.
\begin{lstlisting}[label=lst:judge_rubrics_math1,caption={Rubric used for the \emph{reasoning\_capability} criterion.}]
[Does the model's answer demonstrate reasoning capability?]
Score 1: The answer demonstrates poor reasoning, with illogical arguments or conclusions that do not follow from the provided information.
Score 2: The answer shows weak reasoning, with some logical connections but also contains significant flaws or gaps in the argumentation.
Score 3: The answer demonstrates adequate reasoning, with generally logical arguments, but may have minor flaws or a lack of depth in the reasoning process.
Score 4: The answer shows strong reasoning, with well-structured arguments and conclusions that logically follow from the information provided.
Score 5: The answer demonstrates exceptional reasoning, with clear, coherent, and insightful arguments that are logically sound and well-supported by the information provided.
\end{lstlisting}
\newpage
\begin{lstlisting}[label=lst:judge_rubrics_math2,caption={Rubric used for the \emph{mathematical\_correctness} criterion.}]
[Is the model's answer mathematically correct?]
Score 0: The answer contains mathematical errors that render the solution incorrect or unreliable.
Score 1: The answer is mathematically correct, with accurate calculations and appropriate use of mathematical concepts.
\end{lstlisting}

\paragraph{Paraphrasing.} We evaluate the accuracy of the \emph{paraphrase\_generation}, as well as the \hbox{\emph{paraphrase\_grammatical\_correctness}} and \emph{paraphrase\_completeness} in the responses.
\begin{lstlisting}[label=lst:judge_rubrics_paraph1,caption={Rubric used for the \emph{paraphrase\_generation} criterion.}]
[Is the model's generated paraphrase accurate?]
Score 1: The generated paraphrase is highly inaccurate, failing to retain the original meaning and containing significant changes or errors that alter the intended message.
Score 2: The generated paraphrase shows some attempts to retain the original meaning, but it has multiple inaccuracies or awkward phrasing that distort the original intent.
Score 3: The generated paraphrase retains most of the original meaning but includes minor errors, omissions, or changes that slightly affect the intended message or clarity.
Score 4: The generated paraphrase accurately preserves the original meaning and conveys the intended message, with only minor inaccurate wording differences.
Score 5: The generated paraphrase perfectly retains the original meaning while using different wording or structure, demonstrating natural, fluent, and varied language that enhances the original expression.
\end{lstlisting}
\begin{lstlisting}[label=lst:judge_rubrics_paraph2,caption={Rubric used for the \emph{paraphrase\_grammatical\_correctness} criterion.}]
[Is the model's generated paraphrase grammatically correct and in the same language as the original sentence?]
Score 0: The paraphrase contains grammatical errors or is not in the same language as the original sentence.
Score 1: The paraphrase is grammatically correct and is in the same language as the original sentence.
\end{lstlisting}
\begin{lstlisting}[label=lst:paraph3,caption={Rubric used for the \emph{paraphrase\_completeness} criterion.}]
[Does the model's generated paraphrase convey the original meaning fully?]
Score 0: The paraphrase does not fully convey the meaning of the original sentence.
Score 1: The paraphrase fully conveys the meaning of the original sentence.
\end{lstlisting}

\newpage
\paragraph{Reading comprehension.} We evaluate \emph{passage\_comprehension} and \emph{answer\_relevance}.
\begin{lstlisting}[label=lst:judge_rubrics_read1,caption={Rubric used for the \emph{passage\_comprehension} criterion.}]
[Does the model's answer demonstrate understanding of the passage?]
Score 1: The answer demonstrates a lack of understanding of the passage, providing incorrect information or missing the main idea entirely.
Score 2: The answer shows limited understanding of the passage, with significant errors or omissions that indicate confusion or partial comprehension.
Score 3: The answer reflects a moderate understanding of the passage, capturing some of the key points but missing important details or nuances.
Score 4: The answer shows a good understanding of the passage, accurately capturing most key points and details with minor errors or omissions.
Score 5: The answer demonstrates excellent understanding of the passage, accurately capturing all key points and details with clarity and completeness.
\end{lstlisting}
\begin{lstlisting}[label=lst:judge_rubrics_read2,caption={Rubric used for the \emph{answer\_relevance} criterion.}]
[Is the model's answer relevant?]
Score 0: The answer is not relevant to the question, failing to address the topic or providing unrelated information.
Score 1: The answer is relevant to the question, directly addressing the topic with appropriate detail.
\end{lstlisting}

\paragraph{Summarization.} We evaluate \emph{summary\_informativeness} and \emph{summary\_conciseness}.
\begin{lstlisting}[label=lst:judge_rubrics_summ1,caption={Rubric used for the \emph{summary\_informativeness} criterion.}]
[Is the model's generated summary informative?]
Score 1: The summary fails to capture the main idea or key information from the original text, missing critical points.
Score 2: The summary captures only a small portion of the key information or includes irrelevant details, leading to an incomplete representation of the original text.
Score 3: The summary conveys the general idea of the original text but lacks some important details or includes minor inaccuracies.
Score 4: The summary captures the main idea and most of the key information, with only slight omissions or minor inaccuracies.
Score 5: The summary is highly informative, accurately capturing the main idea and all essential information from the original text in a single, concise sentence.
\end{lstlisting}
\begin{lstlisting}[label=lst:judge_rubrics_summ2,caption={Rubric used for the \emph{summary\_conciseness} criterion.}]
[Is the model's generated summary concise?]
Score 1: The summary is overly long or verbose, including unnecessary details that detract from the main point and violate the requirement of a one-sentence summary.
Score 2: The summary is somewhat concise but contains some redundant or extraneous information that could be removed to enhance brevity.
Score 3: The summary is mostly concise, but there are still some words or phrases that could be eliminated to improve brevity and clarity.
Score 4: The summary is concise and effectively communicates the necessary information with minimal extraneous details.
Score 5: The summary is exceptionally concise, conveying all the required information in a single, clear, and brief sentence without any unnecessary words or details.
\end{lstlisting}

\newpage
\paragraph{Translation.} We evaluate \emph{translation\_fluency} and \emph{translation\_accuracy}.
\begin{lstlisting}[label=lst:judge_rubrics_trans1,caption={Rubric used for the \emph{translation\_fluency} criterion.}]
[Is the model's generated translation fluent?]
Score 1: The translated text is incomprehensible, with severe errors making it impossible to understand.
Score 2: The translated text is disfluent, difficult to read, and requires effort to grasp the meaning.
Score 3: The translated text is understandable but sounds unnatural, with noticeable errors and awkward phrasing.
Score 4: The translated text is mostly smooth and natural, with only minor errors that do not affect comprehension.
Score 5: The translated text is flawless, perfectly natural, and indistinguishable from that of a native speaker.
\end{lstlisting}
\begin{lstlisting}[label=lst:judge_rubrics_trans2,caption={Rubric used for the \emph{translation\_accuracy} criterion.}]
[Is the model's generated translation accurate?]
Score 1: The translation is completely inaccurate, conveying none of the source text's meaning.
Score 2: The translation is mostly inaccurate, retaining only small fragments of the source text's meaning.
Score 3: The translation is somewhat accurate, with some key points conveyed but significant inaccuracies.
Score 4: The translation is mostly accurate, with most of the source text's meaning preserved and only minor inaccuracies.
Score 5: The translation is completely accurate, fully conveying all of the source text's meaning.
\end{lstlisting}