In order for the model to utilize tools, they must first be provided to it. The following code snippet demonstrates how to do so and how the model's multi-turn capabilities can be leveraged for tool-calling applications.

\begin{verbatim}
# Define tools
tools = [{
    "type": "function",
    "function": {
        "name": "get_current_weather",
        "description": "Retrieve the current weather for a specified location.",
        "parameters": {
            "type": "object",
            "properties": {
                "country": {
                    "type": "string",
                    "description": "The country to get the weather for, e.g., 'Spain'."
                }
            },
            "required": ["country"]
        }
    }
}]

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Initialize model
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    device_map="auto", 
    torch_dtype=torch.bfloat16,
)

# Define the user's message and context (system prompt embedded within the chat template)
messages = [
    {"role": "user", "content": "Can you tell me what the weather is like in Spain?"}
]

# Apply the chat template, integrating the tools for model interaction
prompt = tokenizer.apply_chat_template(
    messages,
    tools=tools,
    tokenize=False,
    add_generation_prompt=True,
)

print(prompt)
"""
OUTPUT:
<|im_start|>system
You are a function-calling AI model.
You are provided with function signatures within <tools> </tools> XML tags. 
You may call one or more functions to assist with the user query.
Don't make assumptions about what values to plug into functions.
<tools>
[{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Retrieve 
the current weather for a specified location.', 'parameters': {'type': 'object', 'properties': 
{'country': {'type': 'string', 'description': "The country to get the weather for, 
e.g., 'Spain'."}}, 'required': ['coun']}}}]
</tools>
For each function call return a json object with function name and arguments within 
<tool_call> </tool_call> tags with the following schema:
<tool_call>
{"name": <function-name>, "arguments": <args-dict>}
</tool_call><|im_end|>
<|im_start|>user
Can you tell me what the weather is like in Spain?<|im_end|>
<|im_start|>assistant
"""

inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")
output = model.generate(input_ids=inputs.to(model.device), max_new_tokens=400)

print(output)
"""
POSSIBLE OUTPUT:
<tool_call>
{"name": "get_current_weather", "arguments": { "country": "Spain" }}
</tool_call>
"""

# Simulate receiving a response from the tool (weather data in this case)
tool_response = {
    "country": "Spain",
    "city": "Madrid",
    "state": "Madrid",
    "temperature": {
        "value": 18,
        "unit": "Celsius"
    },
    "weather": {
        "description": "Clear sky",
        "icon": "01d"
    },
    "humidity": 45,
    "wind": {
        "speed": 15,
        "direction": "NE"
    }
}

# Pass the tool response back to the model for structured output
messages.append({"role": "assistant", "content": tool_response})
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors="pt")
output = model.generate(input_ids=inputs.to(model.device), max_new_tokens=400)

print(output)
"""
POSSIBLE OUTPUT:
The current weather in Madrid, Spain is clear sky with a temperature of 18Â°C.
The humidity level is at 45%, and the wind is blowing from the northeast at 15 km/h.
"""
\end{verbatim}