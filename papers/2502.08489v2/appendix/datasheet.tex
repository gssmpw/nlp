We provide an extense datasheet section following the best practices defined by \mbox{\citet{datasheets}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Motivation}\mbox{}\\

\textbf{For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.}

The purpose of creating this dataset is to pre-train the Salamandra family of multilingual models with high performance in a large number of European languages (35) and programming languages (92). We also want to represent the co-official languages of Spain: Spanish, Catalan, Galician and Basque. For this reason, we oversample these languages by a factor of 2.

There is a great lack of massive multilingual data, especially in minority languages \cite{ostendorff_llm-datasets_2024}, so part of our efforts in the creation of this pre-training dataset have resulted in the contribution to large projects such as the Community OSCAR \cite{brack_community_2024}, which includes 151 languages and 40T words, or CATalog \cite{palomar-giner_curated_2024}, the largest open dataset in Catalan in the world.

\textbf{Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?}

The dataset has been created by the Language Technologies unit (LangTech) of the Barcelona Supercomputing Center - Centro Nacional de Supercomputación (BSC-CNS), which aims to advance the field of natural language processing through cutting-edge research and development and the use of HPC. In particular, it was created by the unit's data team, the main contributors being José Javier Saiz, Ferran Espuña and Jorge Palomar.

However, the creation of the dataset would not have been possible without the collaboration of a large number of collaborators, partners and public institutions, which can be found in detail in the acknowledgements.

\textbf{Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.}

This work has been promoted and financed by the Government of Catalonia through the Aina project.

This work is funded by the Ministerio para la Transformación Digital y de la Función Pública and Plan de Recuperación, Transformación y Resiliencia - Funded by EU – NextGenerationEU within the framework of the project ILENIA with reference 2022/TL22/00215337. 

\paragraph{Composition}\mbox{}\\

\textbf{What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.}

The dataset consists entirely of text documents in various languages. Specifically, data was mainly sourced from the following databases and repositories:
\begin{itemize}
    \item Common Crawl: Repository that holds website data and is run by the Common Crawl non-profit organization. It is updated monthly and is distributed under the CC0 1.0 public domain license.
    \item GitHub: Community platform that allows developers to create, store, manage and share their code. Repositories are crawled and then distributed with their original licenses, which may vary from permissive to non-commercial licenses.
    \item Wikimedia: Database that holds the collection databases managed by the Wikimedia Foundation, including Wikipedia, Wikibooks, Wikinews, Wikiquote, Wikisource, and Wikivoyage. It is updated monthly and is distributed under Creative Commons Attribution-ShareAlike License 4.0.
    \item EurLex: Repository that holds the collection of legal documents from the European Union, available in all of the EU's 24 official languages and run by the Publications Office of the European Union. It is updated daily and is distributed under the Creative Commons Attribution 4.0 International license.
    \item Other repositories: Specific repositories were crawled under permission for domain-specific corpora, which include academic, legal, and newspaper repositories.
\end{itemize}

We provide a complete list of data sources in Table \ref{table:datasets}.

\textbf{How many instances are there in total (of each type, if appropriate)?}

The dataset contains a diverse range of instances across multiple languages, with notable adjustments for certain languages. English represents the largest portion, accounting for 39.31\% of the total data. Spanish was upsampled by a factor of 2, bringing its share to 16.12\%, and Catalan (1.97\%), Basque (0.24\%), and Galician (0.31\%) were also upsampled by 2. On the other hand, code-related data was downsampled by half, making up 5.78\% of the total. Other prominent languages include French (6.6\%), Russian (5.56\%), German (4.79\%), and Hungarian (4.59\%), with several additional languages contributing between 1\% and 2\%, and smaller portions represented by a variety of others.

%The dataset contains a diverse range of instances across multiple languages, with notable adjustments for certain languages. English represents the largest portion, accounting for 39.08\% of the total data. Spanish was upsampled by a factor of 2, bringing its share to 16.59\%, while Catalan (1.84\%), Basque (0.26\%), and Galician (0.36\%) were also upsampled by a factor of 2. On the other hand, code-related data was downsampled by half, making up 6.42\% of the total. Other prominent languages include French (6.59\%), Russian (5.39\%), German (4.25\%), and Hungarian (3.93\%), with several additional languages contributing between 1\% and 2\%, and smaller portions represented by a variety of others.

\textbf{Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).}

The dataset is a sample from multiple sources, with different weights based on the primary language of the content: Spanish, Catalan, Basque, and Galician content was upsampled by a factor of two, while programming languages were downsampled by a factor of half. Other sources were sampled in proportion to their occurrence.

\textbf{What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description.}

Each instance consists of a text document processed for deduplication, language identification, and source-specific filtering. Some documents required optical character recognition (OCR) to extract text from non-text formats such as PDFs.

\textbf{Is there a label or target associated with each instance? If so, please provide a description.}

Each instance is labelled with a unique identifier, the primary language of the content, and the URL for web-sourced instances. Additional labels were automatically assigned to detect specific types of content -harmful or toxic content- and to assign preliminary indicators of undesired qualities -very short documents, high density of symbols, etc.- which were used for filtering instances.

\textbf{Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.}

No significant information is missing from the instances.

\textbf{Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.}

Instances are related through shared metadata, such as source and language identifiers.

\textbf{Are there recommended data splits (e.g., training, validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.}

The dataset is randomly divided into training, validation and test sets, where the validation and test sets are each 1\% of the total corpus.

\textbf{Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.}

Despite removing duplicated instances within each source, redundancy remains at the paragraph and sentence levels, particularly in web-sourced instances where search engine optimization techniques and templates contribute to repeated textual patterns. Some instances may be also duplicated across sources due to format variations.

\textbf{Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.}

The dataset is self-contained and does not rely on external resources.

\textbf{Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor– patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.}

The dataset does not contain confidential data.

\textbf{Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. If the dataset does not relate to people, you may skip the remaining questions in this section.}

The dataset includes web-crawled content, which may overrepresent pornographic material across languages \cite{kreutzer_quality_2022}. Although pre-processing techniques were applied to mitigate offensive content, the heterogeneity and scale of web-sourced data make exhaustive filtering challenging, which makes it next to impossible to identify all adult content without falling into excessive filtering, which may negatively influence certain demographic groups \cite{dodge_documenting_2021}.

\textbf{Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.}

The dataset does not explicitly identify any subpopulations.

\textbf{Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.}

Web-sourced instances in the dataset may contain personally identifiable information (PII) that is publicly available on the Web, such as names, IP addresses, email addresses, and phone numbers. While it would be possible to indirectly identify individuals through the combination of multiple data points, the nature and scale of web data makes it difficult to parse such information. In any case, efforts are made to filter or anonymize sensitive data \cite{mina-etal-2024-extending}, but some identifiable information may remain in the dataset.

\textbf{Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.}

Given that the dataset includes web-sourced content and other publicly available documents, instances may inadvertently reveal financial information, health-related details, or forms of government identification, such as social security numbers \cite{subramani_detecting_2023}, especially if the content originates from less-regulated sources or user-generated platforms.

\paragraph{Collection Process}\mbox{}\\

\textbf{How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.}

This dataset is constituted by combining several sources, whose acquisition methods can be classified into three groups:
\begin{enumerate}
    \item Web-sourced datasets with some preprocessing available under permissive license.
    \item Domain-specific or language-specific raw crawls.
    \item Manually curated data obtained through collaborators, data providers (by means of legal assignment agreements) or open source projects (e.g. CATalog).
\end{enumerate}

\textbf{What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?}

The data collection process was carried out using three different mechanisms, each corresponding to one of the groups defined in the previous answer. The specific methods used and their respective validation procedures are outlined below:

\begin{enumerate}
    \item Open Direct Download: Data were obtained directly from publicly accessible sources, such as websites or repositories that provide open data downloads. We validate the data with a data integrity check, which ensures that the downloaded files are complete, uncorrupted and in the expected format and structure.
    \item Ad hoc scrapers or crawlers: Custom web scraping scripts or crawlers were used to extract data from various online sources where direct downloads were not available. These scripts navigate web pages, extract relevant data and store it in a structured format. We validate this method with software unit tests to evaluate the functionality of individual components of the scraping programs, checking for errors or unexpected behaviour. In addition, data integrity tests were performed to verify that the collected data remained complete throughout the extraction and storage process.
    \item Direct download via FTP, SFTP, API or S3: Some datasets were acquired using secure transfer protocols such as FTP (File Transfer Protocol), SFTP (Secure File Transfer Protocol), or API (Application Programming Interface) requests from cloud storage services such as Amazon S3. As with the open direct download method, data integrity tests were used to validate the completeness of the files to ensure that the files were not altered or corrupted during the transfer process.
\end{enumerate}

%According to the 3 previously defined groups, these are the mechanisms used in each of them:
%\begin{enumerate}
%    \item Open direct download. Validation: data integrity tests.
%    \item Ad-hoc scrapers or crawlers. Validation: software unit and data integrity tests.
%    \item Direct download via FTP, SFTP, API or S3. Validation: data integrity tests.
%\end{enumerate}

\textbf{If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?}

The sampling strategy was to use the whole dataset resulting from the filtering explained in the 'preprocessing/cleaning/labelling' section, with the particularity that an upsampling of 2 (i.e. twice the probability of sampling a document) was performed for the co-official languages of Spain (Spanish, Catalan, Galician, Basque), and a downsampling of 1/2 was applied for code (half the probability of sampling a code document, evenly distributed among all programming languages).

\textbf{Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were they paid)?}

This data is generally extracted, filtered and sampled by automated processes. The code required to run these processes has been developed entirely by members of the Language Technologies data team, or otherwise obtained from open-source software. Furthermore, there has been no monetary consideration for acquiring data from suppliers.

\textbf{Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.}

Data were acquired and processed from April 2023 to April 2024. However, as mentioned, much data has been obtained from open projects such as Common Crawl, which contains data from 2014, so it is the end date (04/2024) rather than the start date that is important.

\textbf{Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.}

No particular ethical review process has been carried out as the data is mostly open and not particularly sensitive. However, we have an internal evaluation team and a bias team to monitor ethical issues. In addition, we work closely with 'Observatori d'Ètica en Intel\(\cdot\)ligència Artificial' (OEIAC) and  'Agencia Española de Supervisión de la Inteligencia Artificial' (AESIA) to audit the processes we carry out from an ethical and legal point of view, respectively.

\paragraph{Preprocessing}\mbox{}\\ % Preprocessing/cleaning/labeling

\textbf{Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.}

No changes were made to the content of individual text document instances. However, the web-sourced documents underwent a filtering process based on specific criteria along two key dimensions:

\begin{itemize}
    \item Quality filtering: The text processing pipeline CURATE\cite{palomar-giner_curated_2024} calculates a quality score for each document based on a set of filtering criteria that identify undesirable textual characteristics. Any document with a score below the 0.8 threshold was excluded from the dataset.
    \item Harmful or adult content filtering: To reduce the amount of harmful or inappropriate material in the dataset, documents from Colossal OSCAR were filtered using the Ungoliant pipeline \cite{abadji_ungoliant_2021}, which uses the 'harmful\_pp' field, a perplexity-based score generated by a language model.
\end{itemize}

\textbf{Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.}

The original raw data was not kept.

\textbf{ Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.}

Yes, the preprocessing and filtering software is open-sourced. The CURATE \cite{palomar-giner_curated_2024} pipeline was used for CATalog and other curated sources, and the Ungoliant \cite{abadji_ungoliant_2021} pipeline was used for the OSCAR project.

\paragraph{Distribution}\mbox{}\\

\textbf{Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. }

The dataset will not be released or distributed to third parties. Any related question to distribution is omitted in this section.

%\textbf{How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?}

%\textbf{When will the dataset be distributed?}

%\textbf{Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.}

%\textbf{Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.}

%\textbf{Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.}

\paragraph{Maintenance}\mbox{}\\

\textbf{Who will be supporting/hosting/maintaining the dataset?}

The dataset will be hosted by the Language Technologies unit (LangTech) of the Barcelona Supercomputing Center (BSC). The team will ensure regular updates and monitor the dataset for any issues related to content integrity, legal compliance, and bias for the sources they are responsible for.

\textbf{How can the owner/curator/manager of the dataset be contacted (e.g., email address)?}

The data owner may be contacted with the email address langtech@bsc.es.

\textbf{Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)?}

The dataset will not be updated.

\textbf{If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.}

The dataset does not keep sensitive data that could allow direct identification of individuals, apart from the data that is publicly available in web-sourced content. Due to the sheer volume and diversity of web data, it is not feasible to notify individuals or manage data retention on an individual basis. However, efforts are made to mitigate the risks associated with sensitive information through pre-processing and filtering to remove identifiable or harmful content. Despite these measures, vigilance is maintained to address potential privacy and ethical issues.

\textbf{Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.}

Since the dataset will not be updated, only the final version will be kept.

\textbf{If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.}

The dataset does not allow for external contributions.

\medskip

\clearpage
\paragraph{Data Sources}\mbox{}\\
\label{sec:sources}
\input{tables/data_sources_longtable}
