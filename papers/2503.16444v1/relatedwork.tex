\section{Related Work}
\subsection{Static XAI}
Explainable Artificial Intelligence (XAI) refers to techniques that explain the learning process or the predictions of AI~\citep{yang2019evaluating}. Most existing techniques are static XAI, which provides a one-time explanation with no capability for further user interaction. These techniques can be broadly divided into two categories: self-explanatory models and post-hoc methods. Self-explanatory models are inherently transparent, offering clarity in their decision-making processes~\citep{lakkaraju2016interpretable, rudzinski2016multi,yang2017scalable, jain2019attention, wiegreffe-pinter-2019-attention}. 
The majority of recent XAI methods are post-hoc XAI methods, applied to already developed models that lack inherent transparency~\citep{selvaraju2017grad, ribeiro2016should, chen2021hydra, adadi2018peeking, bodria2021benchmarking}. 
There are two main groups of methods in post-hoc XAI, i.e., feature attribution methods and example-based methods.

\noindent \textbf{Feature Attribution.} Feature attribution methods explain model predictions by investigating the importance of input features to final predictions \citep{adadi2018peeking, danilevsky-etal-2020-survey}. There are two main types of feature attribution methods, gradient-based methods~\citep{cortez2013using, sundararajan2017axiomatic, selvaraju2017grad, simonyan2013deep, lundberg2017unified, wang2024gradient, kokalj2021bertShapley, li2016visualizing, wang2024gradient} and surrogate methods \citep{ribeiro2016should,hu2018locally, alvarez2017causal, liu2018interpretation, shih2018symbolic, ignatiev2019abduction}. Gradient-based methods employ gradients to evaluate the contribution of a model input on the model output. 
Surrogate methods leverage a simple and inherently interpretable model, such as a linear model, to locally approximate the behavior of the complex neural network. 

\noindent \textbf{Example-based Methods.} Example-based methods explain AI predictions by identifying a selection of data instances~\citep{adadi2018peeking, danilevsky-etal-2020-survey, pcnn2024nguyen}. These instances may be training data points with the most influence on the parameters of a prediction model~\citep{chen2021hydra, guo2021fastif}, counterfactual examples that alter predictions with minimal changes to inputs \citep{ wachter2017counterfactual,mothilal2020explaining, yin-neubig-2022-interpreting, ye2021connecting, ross2021explaining, wu2021polyjuice}, or prototypes that contain semantically similar parts to input instances \citep{croce2019auditing,jeyakumar2020can,kim2016examples}.

\hl{In this work, we focus primarily on feature attribution methods, as they directly highlight the importance of input features, making the decision-making process of models more intuitive for laypeople {\citep{kim2023help}}. Specifically, we select Grad-CAM, Integrated Gradients, and SHAP from gradient-based methods, as well as LIME from surrogate methods, to evaluate the effectiveness of different conversational evaluation systems.}

\subsection{Conversational XAI}
Human-Computer Interaction (HCI) researchers have recently proposed that XAI methods should involve conversation, aligning with the natural way humans explain to each other. Specifically, \citet{lombrozo2006structure} argues that explanations emerge from a conversational interaction between an explainer and an explainee. Similarly, \citet{miller2019explanation} emphasizes that explanations should include an interactive communication process, where the explainer provides the necessary information for the explainee to understand the causes of an event through dialogue. Building on this perspective of human explanations, recent works have introduced the concept of "explainability as dialogue," aiming to make explanations more accessible to a wide range of non-expert users \citep{liao2020questioning, feldhus2022mediators, lakkaraju2022rethinking}.

Despite much exploration of the role of conversation in explainability, the practical development of conversational XAI is still in its early stages, with limited methods available so far. \citet{shen2023convxai} applied conversational explanations to scientific writing tasks, finding improvements in productivity and sentence quality. Likewise, \citet{slack2023explaining} designed dialogue systems that help users better understand machine learning models in tasks like diabetes prediction, rearrest prediction, and loan default prediction. However, these systems rely on template-generated conversations and can only handle a limited set of predefined queries. Our work represents the first system capable of delivering free-form explanatory conversations about static explanations.

\subsection{Training with Synthetic Data}
The exceptional performance of Large Language Models (LLMs) and Vision Language Models (VLMs) in generating human-like text has encouraged researchers to explore their use as training data generators \citep{meng2022generating, ye-etal-2022-zerogen, guo2024generative, gao2023selfguided,meng2023tuning, ye2022progen}. For example, SuperGen \citep{meng2022generating} uses LLMs conditioned on label-descriptive prompts to generate training data for text classification tasks. 
FewGen \citep{meng2023tuning} finetune an LLM on few-shot samples and uses it to generate synthetic data for seven classification tasks in the GLUE benchmark. While LLMs and VLMs have shown promise in generating human-like texts, they still face the challenge of producing noisy and low-quality synthetic data. This may lead to decreased performance or perpetuated biases in the model trained on the data \citep{schwarz2021frequency, zhang2024trade,kirk2021bias, esiobu2023robbie, lee2022factuality, ji2023survey}.

To mitigate the detrimental effects of noisy and low-quality synthetic data from LLMs and VLMs, several methods have been proposed \citep{gao2023selfguided, guo2024generative, meng2023tuning, ye2022progen}. For example, ProGen \citep{ye2022progen} adjusts the weight of generated data points with regard to its influence on the validation loss, using influence function \citep{koh2017understanding}.  However, these strategies have primarily focused on generating data for classification tasks and on training small-scale task-specific models. Techniques such as applying the influence function to weigh data points are effective for smaller models. They present challenges and require a special design when adapted to LLMs \citep{grosse2023studying}.

In our work, we apply data generation to conversational explanations and utilize generated data to train the original VLM. We improved the quality of the generated data and significantly slowed down model degeneracy after multiple generation-training iterations (see \S \ref{sec:slowed-degeneration}). 

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/model_structure.pdf}
    \caption{The Overall Workflow of EMCEE. $V_i$ denotes the VLM and $D_i$ denotes the synthetic conversation data in the $i$-th iteration. Starting from a pretrained VLM $V_1$, we first generate diverse synthetic conversations $D_1$ with the repetition penalty. Next, we use a hallucination detector to clean synthetic data, producing cleaned data $D_1^{\text{\,clean}}$. We then finetune the VLM on $D_1^{\text{\,clean}}$, which creates $V_2$, and this process repeats.  
    % \hl{hallucination detector $\rightarrow$ hallucination detection and filtering}
    }
    \label{fig:model_structure}
    \vspace{-10pt}
\end{figure*}