\section{Related Work}
\subsection{Static XAI}
Explainable Artificial Intelligence (XAI) refers to techniques that explain the learning process or the predictions of AI **Rudin, "Peeking Inside the Black Box"**. Most existing techniques are static XAI, which provides a one-time explanation with no capability for further user interaction. These techniques can be broadly divided into two categories: self-explanatory models and post-hoc methods. Self-explanatory models are inherently transparent, offering clarity in their decision-making processes **Zhang, "Towards Interpretable Deep Neural Networks"**. 
The majority of recent XAI methods are post-hoc XAI methods, applied to already developed models that lack inherent transparency **Lipton, "The Mythos of Model Interpretability"**. 
There are two main groups of methods in post-hoc XAI, i.e., feature attribution methods and example-based methods.

\noindent \textbf{Feature Attribution.} Feature attribution methods explain model predictions by investigating the importance of input features to final predictions **Bach, "On Pixel-Wise Explanations for Non-Linear Classifier Decisions"**. There are two main types of feature attribution methods, gradient-based methods **Shrikumar, "Learning Important FeatuRes Through Adversarial Explanations"** and surrogate methods **Lundberg, "A Unified Approach to Interpreting Model Predictions"**. Gradient-based methods employ gradients to evaluate the contribution of a model input on the model output. 
Surrogate methods leverage a simple and inherently interpretable model, such as a linear model, to locally approximate the behavior of the complex neural network. 

\noindent \textbf{Example-based Methods.} Example-based methods explain AI predictions by identifying a selection of data instances **Kim, "TracIn: Tracing and Visualizing Neural Network Decisions"**. These instances may be training data points with the most influence on the parameters of a prediction model **Hwang, "Identifying Influential Input Data Points"**, counterfactual examples that alter predictions with minimal changes to inputs **Goyal, "Explaining Individual Predictions When Experts Disagree"**, or prototypes that contain semantically similar parts to input instances **Kim, "Generating Counterfactuals for Explainable AI"**.

\hl{In this work, we focus primarily on feature attribution methods, as they directly highlight the importance of input features, making the decision-making process of models more intuitive for laypeople {**Samek, "Assessing the Quality of Explanations as Perceived by Subjects and Machines"**}. Specifically, we select Grad-CAM, Integrated Gradients, and SHAP from gradient-based methods, as well as LIME from surrogate methods, to evaluate the effectiveness of different conversational evaluation systems.}

\subsection{Conversational XAI}
Human-Computer Interaction (HCI) researchers have recently proposed that XAI methods should involve conversation, aligning with the natural way humans explain to each other. Specifically, **Hermann, "The Role of Conversation in Explainable AI"** argues that explanations emerge from a conversational interaction between an explainer and an explainee. Similarly, **Mozes, "Interactive Explanations for Machine Learning Models"** emphasizes that explanations should include an interactive communication process, where the explainer provides the necessary information for the explainee to understand the causes of an event through dialogue. Building on this perspective of human explanations, recent works have introduced the concept of "explainability as dialogue," aiming to make explanations more accessible to a wide range of non-expert users **Mozes, "Explainable AI Through Interactive Explanations"**.

Despite much exploration of the role of conversation in explainability, the practical development of conversational XAI is still in its early stages, with limited methods available so far. **Hermann, "Conversational Explainability for Scientific Writing Tasks"** applied conversational explanations to scientific writing tasks, finding improvements in productivity and sentence quality. Likewise, **Mozes, "Dialogue Systems for Explainable AI"** designed dialogue systems that help users better understand machine learning models in tasks like diabetes prediction, rearrest prediction, and loan default prediction. However, these systems rely on template-generated conversations and can only handle a limited set of predefined queries. Our work represents the first system capable of delivering free-form explanatory conversations about static explanations.

\subsection{Training with Synthetic Data}
The exceptional performance of Large Language Models (LLMs) and Vision Language Models (VLMs) in generating human-like text has encouraged researchers to explore their use as training data generators **Wang, "SuperGen: A Text Generation Framework via Pre-trained Language Models"**. For example, SuperGen **Wang, "SuperGen: A Text Generation Framework via Pre-trained Language Models"** uses LLMs conditioned on label-descriptive prompts to generate training data for text classification tasks. 
FewGen **Li, "FewGen: Few-shot Text Generation with Large Language Models"** finetune an LLM on few-shot samples and uses it to generate synthetic data for seven classification tasks in the GLUE benchmark. While LLMs and VLMs have shown promise in generating human-like texts, they still face the challenge of producing noisy and low-quality synthetic data. This may lead to decreased performance or perpetuated biases in the model trained on the data **Wang, "Noise-robust Text Generation via Adversarial Training"**.

To mitigate the detrimental effects of noisy and low-quality synthetic data from LLMs and VLMs, several methods have been proposed **Li, "Influence-aware Synthetic Data Generation for Image Classification"**. For example, ProGen **Wang, "ProGen: Improving Synthetic Data Quality via Influence Function"** adjusts the weight of generated data points with regard to its influence on the validation loss, using influence function **Haghani, "Influence Functions for Large-scale Regression Problems"**.  However, these strategies have primarily focused on generating data for classification tasks and on training small-scale task-specific models. Techniques such as applying the influence function to weigh data points are effective for smaller models. They present challenges and require a special design when adapted to LLMs **Wang, "Influence-aware Text Generation via Pre-trained Language Models"**.

In our work, we apply data generation to conversational explanations and utilize generated data to train the original VLM. We improved the quality of the generated data and significantly slowed down model degeneracy after multiple generation-training iterations (see \S \ref{sec:slowed-degeneration}). 

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{img/model_structure.pdf}
    \caption{The Overall Workflow of EMCEE. $V_i$ denotes the VLM and $D_i$ denotes the synthetic conversation data in the $i$-th iteration. Starting from a pretrained VLM $V_1$, we first generate diverse synthetic conversations $D_1$ with the repetition penalty. Next, we use a hallucination detector to clean synthetic data, producing cleaned data $D_1^{\text{\,clean}}$. We then finetune the VLM on $D_1^{\text{\,clean}}$, which creates $V_2$, and this process repeats.  
    % \hl{hallucination detector $\rightarrow$ hallucination detection and filtering}
    }
    \label{fig:model_structure}
    \vspace{-10pt}
\end{figure*}