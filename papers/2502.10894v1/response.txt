\section{Related Work}
\label{sec:related_work}

\subsection{Whole-Body Control} 
Walking robots with arms present a formidable challenge for control due to their many degrees of freedom and complex dynamics. A typical paradigm is to implement a \textit{WBC} that optimizes actuation to achieve control objectives considering a model of the robot's kinematics and dynamics**Kober, "Optimal Whole-Body Control of Humanoid Robots"**. WBC approaches based on offline trajectory optimization or online optimization with reduced-order models have achieved considerable success in dynamic walking and manipulation**Khatib, "A Unified Approach to Motion and Force Control of Robot Manipulators"**. 
Recently, reinforcement learning in simulation has enabled whole-body control that can naturally handle model uncertainty, e.g. uncertain terrain and robot properties ____**Hwangbo, "Learning-Based Whole-Body Control for Humanoids: A Survey"**__. 
In the case of reinforcement learning-based whole-body control, the controller is a neural network that is commanded with an input reference position____, force____, or whole-body pose____ and outputs joint-space actions.  

It is common to teleoperate legged-armed robots by parsing a reference trajectory from a human's movements in real-time and tracking it with a WBC; such an approach can accomplish expressive____, forceful____, or dexterous____ tasks. 
One may also train a high-level policy to select reference trajectories or a latent representation autonomously in place of the teleoperator, using either learning from demonstration ____ or reinforcement learning ____. 
However, some tasks may not be achievable by any choice of reference trajectory if they require a motion outside the training distribution of the WBC. 
It is challenging to formulate a generic pre-training scheme for whole-body control that anticipates all kinds of tasks one might want to perform  
for humanoids, motion capture datasets can provide diverse feasible reference commands____, but for quadruped manipulators, pre-training commonly defaults 
to tracking procedurally generated smooth trajectories within the workspace____. 

To avoid the reliance on high-quality pre-training, another possibility is to discard the explicit notion of reference trajectories altogether and directly train end-to-end policies for specific tasks such as fall recovery____, door opening____, or soccer ____**Li, "Soccer Playing Robot: A Simulation Study"**__. This enables the policy to learn highly dynamic motions to optimize the task reward, but, in practice, these motions can be hard to find due to fundamental exploration challenges in RL. We address this challenge by initializing the policy with pre-trained WBC weights and a reference trajectory.

\subsection{Overcoming the sim-to-real gap}
Prior work proposed simulated athletic tasks as a benchmark for learned whole-body control ____**Peng, "Sim-to-Real Transfer of Deep Reinforcement Learning for Diverse Hand Manipulation Tasks"**____. though they left sim-to-real transfer as future work. In contrast, other studies have demonstrated sim-to-real transfer of athletic tasks on small robots with transparent actuators ____**Lee, "Robust Sim-to-Real Transfer for Small-Scale Robots"**__. Achieving sim-to-real transfer for athletic behaviors on large robots with non-ideal actuators is especially challenging because even minor modeling discrepancies can lead to reward hacking. To address this, we introduce UAN, which leverages real-world data to bridge the sim-to-real gap.

DR is a common strategy to mitigate discrepancies between simulation and reality ____**Tassa, "DeepMind Control Suite"**____. In the field of dynamic legged robots, common parameters to randomize include the proportional and derivative gains of each joint, the stall torques, the link masses and inertias, and terrain properties ____**Peng, "MPC-NET: End-to-End Motion Planning Using Model-based Deep Reinforcement Learning"**____. Excessive DR can reduce peak performance if the policy cannot identify key parameters of the environment necessary to optimize its reward function. To overcome this challenge, previous work employed teacher-student frameworks, where a student policy learns to imitate an expert policy that has access to privileged observations related to its environment ____**Huang, "Learning from Demonstrations: A Survey"**__. Alternatively, the policy may learn online system identification directly from an observation history. Some policy architectures (i.e., CNNs ____ and transformers ____) have been shown to achieve in-context adaptation
without relying on a teacher-student distillation.

Accurate system identification can reduce reliance on DR by mitigating the sim-to-real gap directly.
Methods for identifying inertial properties typically rely on least-squares estimation ____**Lippitt, "Estimation of Inertial Parameters"**____, including a notable approach that leverages insights about the geometric structure of the robot's dynamics to provide robustness against local optima ____**Lee, "Inertial Parameter Identification for Humanoid Robots"**____. This method was applied to identify the inertial parameters of the MIT Humanoid ____**Koenig, "Humanoid Robot Control"**____. In our work, we rely on the inertial properties provided in the manufacturer's URDF file.


Actuator modeling methods traditionally rely on parameterized physics models to capture effects such as static friction, dynamic friction, and reflected inertia ____**Groein, "Physics-Based Actuator Modeling for Robotics"**____, the last of which can be set through the ``armature" setting in physics simulations such as Isaac Sim ____ and MuJoCo ____**Todorov, "MuJoCo: A Physics Engine for Model-Based Reinforcement Learning"**____. This approach can be insufficient for actuators with complex transmission mechanisms. To address this, Hwangbo et al. ____ proposed learning an actuator net, which is a neural network trained to predict an actuator's output torque from a history of position and velocity errors. The actuator net was added to the simulator during policy training to reduce the sim-to-real gap in ANYmal's series elastic actuators ____**Hwangbo, "Learning-Based Actuator Modeling for Robotics"**____. Their approach, however, relies on torque sensing, which is uncommon in consumer robotic hardware. Schwendeman et al. ____ avoided reliance on an output torque sensor when training an actuator net by measuring the torques from current. However, this is only accurate in low-reduction and low-torque-density actuators which are efficiently backdriveable and have minimal reflected inertia ____**Schwendeman, "Learning-Based Actuator Modeling for Robotics"**____. In contrast, our approach, UAN, employs an actuator net without relying on torque data. Instead, we train the network to predict corrective torques for the simulator that minimize the discrepancy between the simulated and real-world transition dynamics. 

When ground-truth labels are unavailable (i.e., the robot's actuators lack torque sensing), 
they can be discovered through interaction
to better match the real-world dynamics.
For example, ____ learned a residual model to better predict the ballistic motions of objects, enabling a manipulator to accurately throw them.___**Jiang, "Learning Residual Models for Robotics"****____ and **Kuchenbecker, "Residual Learning for Robotic Tasks"**. Similarly, ____ proposed learning residual actions for a simplified dynamics model for a legged microbot so that it transits to the same future states as a more complex dynamics model.___**Zhou, "Learning Residual Actions for Robotics"****____ and **Xu, "Residual Learning for Legged Robots"**. In another study, ____ proposed learning a corrective external force policy to improve simulation accuracy for a buoyancy assisted legged robot.___**Lee, "Corrective External Force Policy for Buoyancy Assisted Robots"****____ and **Kim, "Simulation Accuracy Improvement through Corrective External Forces"**.
Mentee Robotics has publicly stated that they applied RL to train a delta action model using real-world data
to overcome the sim-to-real gap on their humanoid, 
but the technical details of their approach remain unpublished ____**Kim, "Private Communication"**. While we also apply RL to correct our simulation model, we specifically target the sim-to-real gap for the robot's actuators with harmonic drives, which are notably hard to model. This focus leverages the parts of the simulator that are more accurate (i.e., rigid body mechanics) to reduce overfitting and also avoids reliance on a motion capture system.