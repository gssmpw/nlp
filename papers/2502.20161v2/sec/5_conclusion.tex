\section{Conclusion}
In this paper, we tackle the imbalance update in the R-D optimization of LICs by framing it as a MOO problem. Viewing R-D optimization as a multi-objective task, we propose two solutions to address the imbalance caused by gradient divergence. Solution 1 uses a coarse-to-fine approach along the standard R-D optimization trajectory, employing gradient descent to refine the gradient weights iteratively. This method is well-suited for training LIC models from scratch, as it progressively balances rate and distortion objectives throughout the whole training process. Solution 2 formulates the balanced R-D optimization as a constrained-QP problem, providing a precise solution by solving the KKT conditions. Although Solution 2 requires more computation, it offers an accurate adjustment of gradient weights, making it ideal for fine-tuning pre-trained models. Extensive experiments validate the effectiveness of both solutions, showing consistent R-D performance improvements over standard training schemes across various LIC models and datasets with acceptable additional training costs.

\textbf{Limitations and Future Work.} Our approach balances the R-D optimization of LICs by introducing an additional subproblem~\cite{liu2024famo,sener2018multi}, which moderately increases training complexity. Future work could focus on designing more efficient algorithms to mitigate this added computational cost. Recent research indicates that Tchebycheff Scalarization~\cite{lin2024few,lin2024smooth} is a promising avenue, offering favorable theoretical properties along with low computational complexity. Additionally, we employ the softmax function to constrain weights within the probability simplex; however, projection-based methods could be also explored~\cite{krichene2015efficient,wang2013projection}.
