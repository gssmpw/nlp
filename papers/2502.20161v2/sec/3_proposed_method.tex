\section{Proposed method}
% \subsection{Preliminaries on LIC rate-distortion optimization}
\subsection{Preliminaries: rate-distortion optimization in learned image compression}

In LIC, the objective is to encode an image \( x \), drawn from a source distribution with probability density function \( p_{\text{source}} \), into a compact bit sequence for efficient storage or transmission. The receiver then reconstructs an approximation \( \hat{x} \) of the original image \( x \). The LIC process involves three main steps: encoding and quantization, entropy coding, and reconstruction.

\begin{itemize}
    \item \textbf{Encoding and Quantization}: First, each data point \( x \) is mapped to a de-correlated low-dimensional latent variable \( \hat{z} \) via an encoder function \( e(\cdot) \) followed by quantization \( Q(\cdot) \) to convert the continuous representation into discrete values, i.e., \( \hat{z} = Q(e(x)) \).

    \item \textbf{Entropy Coding}: After determining \( \hat{z} \), lossless entropy coding, such as Huffman coding~\cite{moffat2019huffman} or arithmetic coding~\cite{witten1987arithmetic}, is applied to produce a compressed bit sequence with length \( b(\hat{z}) \). Ideally, the entropy coding scheme approximates the theoretical bit rate, given by the entropy of \( \hat{z} \) under its marginal distribution. We assume that both the sender and receiver have access to an entropy model \( P(\hat{z}) \), which estimates the marginal probability distribution of \( \hat{z} \) and determines the expected bit length as \( b(\hat{z}) \approx -\log_2 P(\hat{z}) \). The goal is to ensure that \( P(\hat{z}) \) closely approximates the true marginal distribution \( p(\hat{z}) \), which is defined as
\(
p(\hat{z}) = \mathbb{E}_{x \sim p_{\text{source}}} \left[ \delta(\hat{z}, Q(e(x))) \right],
\)
where \( \delta \) is the Kronecker delta function. Under this approximation, the encoding length \( b(\hat{z}) \) is nearly optimal, as the average code length approaches the entropy \( -\log_2 p(\hat{z}) \) of \( \hat{z} \).

    \item \textbf{Reconstruction}: Once the receiver has obtained \( \hat{z} \), it reconstructs the approximation \( \hat{x} \) using a reconstruction function \( r(\cdot) \), such that \( \hat{x} = r(\hat{z}) \).
\end{itemize}

To optimize the LIC scheme, our goal is to minimize both the bit rate (rate) and the discrepancy between \( x \) and \( \hat{x} \) (distortion), where the distortion is measured by a function \( d(x, \hat{x}) \). This objective is formulated as a R-D loss using a Lagrangian multiplier:
\begin{equation}
\label{eq:rd}
    \mathcal{L}_\text{R-D} = \mathbb{E}_{x \sim p_{\text{source}}} \left[ \underbrace{-\log_2 P(\hat{z})}_{\text{rate}\ (\mathcal{L}_{\text{R}})} + \underbrace{\lambda d(x, \hat{x})}_{\text{distortion}\ (\mathcal{L}_{\text{D}})} \right],
\end{equation}
where \( \lambda \) is a Lagrange multiplier that controls the trade-off between rate (compression efficiency) and distortion (reconstruction quality). This formulation represents the Lagrangian relaxation of the distortion-constrained R-D optimization problem, aiming for efficient compression while maintaining high reconstruction fidelity. In practice, the gradient used to update LIC models is the sum of the gradients for each objective:
\(
d_t = \nabla \mathcal{L}_{\text{R}, t} + \nabla \mathcal{L}_{\text{D}, t}
\), where $t$ denotes the training iteration.


\begin{algorithm*}[t]
\caption{Balanced Rate-Distortion Optimization via Trajectory Optimization}
\label{alg:solution1}
\begin{algorithmic}[1]
\REQUIRE Initial network parameters $\theta_0$, initial softmax logits $\boldsymbol{\xi}_0$, learning rates $\alpha$ (for $\theta$) and $\beta$ (for $\boldsymbol{\xi}$), decay parameter $\gamma$, total iterations $T$
\STATE Initialize weights $\mathbf{w}_0 \leftarrow \text{Softmax}(\boldsymbol{\xi}_0)$
\FOR{$t = 0$ \TO $T-1$}
    \STATE Compute losses $\mathcal{L}_{\text{R}, t} = \mathcal{L}_{\text{R}}(\theta_t)$ and $\mathcal{L}_{\text{D}, t} = \mathcal{L}_{\text{D}}(\theta_t)$
    \STATE Compute gradients: $\nabla_\theta \mathcal{L}_{\text{R}, t} = \frac{\partial \mathcal{L}_{\text{R}, t}}{\partial \theta_t}$, \quad $\nabla_\theta \mathcal{L}_{\text{D}, t} = \frac{\partial \mathcal{L}_{\text{D}, t}}{\partial \theta_t}$
    \STATE \hspace{2.1cm} $\nabla_\theta \log \mathcal{L}_{\text{R}, t} = \frac{\nabla_\theta \mathcal{L}_{\text{R}, t}}{\mathcal{L}_{\text{R}, t}}$, \quad $\nabla_\theta \log \mathcal{L}_{\text{D}, t} = \frac{\nabla_\theta \mathcal{L}_{\text{D}, t}}{\mathcal{L}_{\text{D}, t}}$
    \STATE Compute normalization constant: $c_t = \left( \frac{w_{\text{R}, t}}{\mathcal{L}_{\text{R}, t}} + \frac{w_{\text{D}, t}}{\mathcal{L}_{\text{D}, t}} \right)^{-1}$
    \STATE Compute balanced gradient: $\mathbf{d}_t = c_t \left( w_{\text{R}, t} \nabla_\theta \log \mathcal{L}_{\text{R}, t} + w_{\text{D}, t} \nabla_\theta \log \mathcal{L}_{\text{D}, t} \right)$
    \STATE Update network parameters: $\theta_{t+1} = \theta_t - \alpha\, \mathbf{d}_t$
    \STATE Compute updated losses $\mathcal{L}_{\text{R}, t+1} = \mathcal{L}_{\text{R}}(\theta_{t+1})$ and $\mathcal{L}_{\text{D}, t+1} = \mathcal{L}_{\text{D}}(\theta_{t+1})$
    \STATE Compute $\delta_t = \begin{bmatrix} \frac{\partial w_{\text{R}, t}}{\partial \boldsymbol{\xi}_t} \\ \frac{\partial w_{\text{D}, t}}{\partial \boldsymbol{\xi}_t} \end{bmatrix}^\top \begin{bmatrix} \log \mathcal{L}_{\text{R}, t} - \log \mathcal{L}_{\text{R}, t+1} \\ \log \mathcal{L}_{\text{D}, t} - \log \mathcal{L}_{\text{D}, t+1} \end{bmatrix}$
    \STATE Update softmax logits: $\boldsymbol{\xi}_{t+1} = \boldsymbol{\xi}_t - \beta \left( \delta_t + \gamma\, \boldsymbol{\xi}_t \right)$
    \STATE Update weights: $\mathbf{w}_{t+1} = \text{Softmax}(\boldsymbol{\xi}_{t+1})$
\ENDFOR
\end{algorithmic}
\end{algorithm*}

\subsection{Balanced rate-distortion optimization}
To address the imbalance in optimizing rate and distortion discussed in Sec.~\ref{sec:intro}, we propose a balanced R-D optimization framework. This framework dynamically adjusts the contributions from each objective in the R-D loss function, aiming to achieve equal progress in both rate and distortion. Specifically, we redefine the update direction as \( d_t = w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t}\), where \(w_{\text{R},t}\) and \(w_{\text{L},t}\) are adaptive weights for the rate and distortion gradients. By maximizing the minimum improvement speed between rate and distortion at each step, our method finds the ideal gradient weights, promotes stable balanced optimization, and enhances overall model performance.


Inspired by  MOO techniques~\cite{sener2018multi, liu2024famo}, we reformulate the R-D optimization problem as a multi-objective optimization problem. Our goal is to simultaneously optimize rate minimization and distortion minimization with parameters \( \theta \in \mathbb{R}^m \). To achieve this, we modify the original Lagrangian R-D loss function to the following form:
\begin{equation}
\min_{\theta \in \mathbb{R}^m} \left\{ \mathcal{L}(\theta) = \mathcal{L}_{\text{R}}(\theta) + \mathcal{L}_{\text{D}}(\theta)  \right\},
\end{equation}
where the trade-off factor \( \lambda \) is now absorbed into \( \mathcal{L}_{\text{D}} \) to simplify the formulation. $m$ is the number of parameters.

The loss improvement speed at iteration \( t \) of task \(i \in \{\text{R}, \text{D}\}\) is defined as:
\begin{equation}
\label{eq:speed}
s_{i,t}(\alpha, d_t) = \frac{\mathcal{L}_{i,t} - \mathcal{L}_{i,t+1}}{\mathcal{L}_{i,t}},
\end{equation}
where \( \alpha \) is the step size and \( d_t \) is the update direction at \( t \), with \( \theta_{t+1} = \theta_t - \alpha d_t \). To achieve balanced optimization, we seek an update direction \( d_t \) that maximizes the minimum improvement speed between rate and distortion, ensuring that neither objective dominates the update. In other words, we maximize the smaller improvement speed between \( \mathcal{L}_{\text{R}} \) and \( \mathcal{L}_{\text{D}} \). This leads to the following saddle point problem~\cite{lin2020near} formulation:
{\small
\begin{equation}
\max_{d_t \in \mathbb{R}^m} \min \left( \frac{1}{\alpha} s_{\text{R},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2, \frac{1}{\alpha} s_{\text{D},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2 \right),
\end{equation}}
where \( \frac{1}{2} \|d_t\|^2 \) serves as a regularization term to prevent unbounded updates.
% where \( \frac{1}{2} \|d_t\|^2 \) serves as a regularization term to prevent an unbounded solution.

When the step size \( \alpha \) is small, one can approximate \( \mathcal{L}_{t+1} \approx \mathcal{L}_{t} - \alpha \nabla \mathcal{L}_t^\top d_{t} \) using a first-order Taylor expansion. This approximation simplifies the problem to:
{\small\begin{equation}
\begin{aligned}
&\max_{d_t \in \mathbb{R}^m} \min \left( \frac{1}{\alpha} s_{\text{R},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2, \frac{1}{\alpha} s_{\text{D},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2 \right)\\
&=\max_{d_t \in \mathbb{R}^m} \min \left( \frac{\nabla \mathcal{L}_{\text{R}, t}^\top d_t}{\mathcal{L}_{\text{R}, t}} - \frac{1}{2} \|d_t\|^2, \frac{\nabla \mathcal{L}_{\text{D}, t}^\top d_t}{\mathcal{L}_{\text{D}, t}} - \frac{1}{2} \|d_t\|^2 \right) \\
&= \max_{d_t \in \mathbb{R}^m} \left( \min \left( \nabla \log \mathcal{L}_{\text{R}, t}^\top d_t, \nabla \log \mathcal{L}_{\text{D}, t}^\top d_t \right) - \frac{1}{2} \|d_t\|^2 \right).
\end{aligned}
\end{equation}}

To avoid solving the high-dimensional primal problem directly (as \( d_t \in \mathbb{R}^m \) with potentially millions of parameters if \( \theta \) is a neural network), we follow previous work~\cite{sener2018multi,liu2024famo} to turn to the dual problem. Leveraging the Lagrangian duality theorem~\cite{boyd2004convex}, we can rewrite the optimization as a convex combination of gradients:
{\small
\begin{equation}
\begin{aligned}
&\max_{d_t \in \mathbb{R}^m} \left( \min \left( \nabla \log \mathcal{L}_{\text{R}, t}^\top d_t, \nabla \log \mathcal{L}_{\text{D}, t}^\top d_t \right) - \frac{1}{2} \|d_t\|^2 \right) \\
&= \max_{d_t \in \mathbb{R}^m} \min_{w \in \mathbb{S}_2} \left( w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t} \right)^\top d_t - \frac{1}{2} \|d_t\|^2 \\
&= \min_{w_t \in \mathbb{S}_2} \max_{d_t \in \mathbb{R}^m} \left( w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t} \right)^\top d_t - \frac{1}{2} \|d_t\|^2,
\end{aligned}
\end{equation}}
where the second equality follows from strong duality. Here, \( w_t \in \mathbb{S}_2 = \{ w \in \mathbb{R}_{\geq 0}^2 \mid w^\top \mathbf{1} = 1 \} \) represents the gradient weights in the 2-dimensional probabilistic simplex.

Let \( g(d_t, w) = (w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t})^\top d_t - \frac{1}{2} \|d_t\|^2 \). The optimal direction \( d_t^* \) is obtained by setting:
\begin{equation}
\small
\label{eq:opt_grad}
\frac{\partial g}{\partial d_t} = 0 \quad \Longrightarrow \quad d_t^* = w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t}.
\end{equation}

Substituting \( d_t^* \) back, we obtain:
\begin{equation}
\small
\label{eq:target}
\begin{aligned}
&\max_{d_t \in \mathbb{R}^m} \min \left( \frac{1}{\alpha} s_{\text{R},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2, \frac{1}{\alpha} s_{\text{D},t}(\alpha, d_t) - \frac{1}{2} \|d_t\|^2 \right) \\
&= \min_{w_t \in \mathbb{S}_2} \frac{1}{2} \left\| w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R}, t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D}, t} \right\|^2 \\
&= \min_{w_t \in \mathbb{S}_2} \frac{1}{2} \| J_t w_t \|^2,
\end{aligned}
\end{equation}
where \( J_t = \begin{bmatrix}
\nabla \log \mathcal{L}_{\text{R}, t}^\top \\
\nabla \log \mathcal{L}_{\text{D}, t}^\top
\end{bmatrix} \). Thus, our optimization problem reduces to finding \( w_t \) that satisfies Eq.~\ref{eq:target}.


\subsubsection{Solution 1: Gradient descent over trajectory}
\label{subsubsec:gd_over}
Rather than fully solving the optimization problem at each step, our proposed Solution 1 adopts a coarse-to-fine gradient descent approach~\cite{sener2018multi,liu2024famo}, incrementally refining the solution along the R-D optimization trajectory. In this approach, the gradient weights \( w_t \) are updated iteratively as follows:
\begin{equation}
w_{t+1} = w_{t} - \alpha_w \tilde{\delta},
\end{equation}
where
\begin{equation}
\tilde{\delta} = \nabla_w \frac{1}{2} \| J_t w_t \|^2 = J_t^\top J_t w_t.
\end{equation}

Using a first-order Taylor approximation, \( \log \mathcal{L}_{t+1} \approx \log \mathcal{L}_{t} - \alpha \nabla \log \mathcal{L}_t^\top d_{t} \), we have the following relationship:
\begin{equation}
\tilde{\delta} = J_t^\top J_t w_t = J_t^\top d_t \approx \frac{1}{\alpha} \begin{bmatrix} \log \mathcal{L}_{\text{R}, t} - \log \mathcal{L}_{\text{R}, t+1} \\ \log \mathcal{L}_{\text{D}, t} - \log \mathcal{L}_{\text{D}, t+1} \end{bmatrix}.
\footnote{To prevent negative values in \( \log \mathcal{L}_{i,t} \), we add 1 to \( \mathcal{L}_{i,t} \) before applying the logarithm, ensuring that the minimum value in the log domain is zero.}
\end{equation}

To ensure that \( w_t \) remains within the interior point of simplex \( \mathbb{S}_2 \), we reparametrize \( w_t \) using \( \xi_t \):
\begin{equation}
w_t = \text{Softmax}(\xi_t),
\end{equation}
where \( \xi_t \in \mathbb{R}^2 \) represents the unconstrained softmax logits. To give more weight to recent updates, we add a decay term~\cite{zhou2022convergence,liu2024famo}, leading to the following update for \( \xi_t \):
\begin{equation}
\xi_{t+1} = \xi_t - \beta(\delta_t + \gamma \xi_t),
\end{equation}
where
\begin{equation}
\delta_t = \begin{bmatrix} \nabla^\top w_{\text{R},t}(\xi) \\ \nabla^\top w_{\text{D},t}(\xi) \end{bmatrix} \begin{bmatrix} \log \mathcal{L}_{\text{R}, t} - \log \mathcal{L}_{\text{R}, t+1} \\ \log \mathcal{L}_{\text{D}, t} - \log \mathcal{L}_{\text{D}, t+1} \end{bmatrix}.
\end{equation}

After computing the weights \( w_t \), we renormalize them to ensure numerical stability~\cite{liu2024famo}. This renormalization is crucial as our update direction is a convex combination of the gradients of the log losses:
\begin{equation}
d_t = w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R},t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D},t} = \sum_{i \in \{\text{R}, \text{D}\}} \frac{w_{i,t}}{\mathcal{L}_{i,t}} \nabla \mathcal{L}_{i,t}.
\end{equation}
When \( \mathcal{L}_{i,t} \) becomes small, the multiplicative factor \( \frac{w_{i,t}}{\mathcal{L}_{i,t}} \) can grow large, potentially causing instability in the optimization. To mitigate this, we scale the gradient by a constant \( c_t \):
\begin{equation}
c_t = \left(\frac{w_{\text{R},t}}{\mathcal{L}_{\text{R},t}} + \frac{w_{\text{D},t}}{\mathcal{L}_{\text{D},t}}\right)^{-1}.
\end{equation}

The resulting balanced gradient, which is used to update the model parameters \( \theta \), is then given by:
\begin{equation}
\label{eq:sl1_d}
d_t = c_t \left(w_{\text{R},t} \nabla \log \mathcal{L}_{\text{R},t} + w_{\text{D},t} \nabla \log \mathcal{L}_{\text{D},t}\right).
\end{equation}

This method, referred to as Solution 1, is a coarse-to-fine gradient descent technique. It is particularly suited for training LIC models from scratch, as it incrementally balances rate and distortion optimization along the trajectory. The implementation is shown in Algorithm~\ref{alg:solution1}.


\begin{algorithm*}[t]
\caption{Balanced Rate-Distortion Optimization via Quadratic Programming}
\label{alg:solution2}
\begin{algorithmic}[1]
\REQUIRE Initial network parameters $\theta_0$, learning rate $\alpha$, total iterations $T$
\FOR{$t = 0$ \TO $T-1$}
    \STATE Compute losses $\mathcal{L}_{\text{R}, t} = \mathcal{L}_{\text{R}}(\theta_t)$ and $\mathcal{L}_{\text{D}, t} = \mathcal{L}_{\text{D}}(\theta_t)$
    \STATE Compute gradients: $\nabla_\theta \mathcal{L}_{\text{R}, t} = \frac{\partial \mathcal{L}_{\text{R}, t}}{\partial \theta_t}$, \quad $\nabla_\theta \mathcal{L}_{\text{D}, t} = \frac{\partial \mathcal{L}_{\text{D}, t}}{\partial \theta_t}$
    \STATE \hspace{2.1cm} $\nabla_\theta \log \mathcal{L}_{\text{R}, t} = \frac{\nabla_\theta \mathcal{L}_{\text{R}, t}}{\mathcal{L}_{\text{R}, t}}$, \quad $\nabla_\theta \log \mathcal{L}_{\text{D}, t} = \frac{\nabla_\theta \mathcal{L}_{\text{D}, t}}{\mathcal{L}_{\text{D}, t}}$
    \STATE Form matrix $J_t = \begin{bmatrix}  \nabla_\theta \log \mathcal{L}_{\text{R}, t} ^\top \\  \nabla_\theta \log \mathcal{L}_{\text{D}, t} ^\top \end{bmatrix}$
    \STATE Compute Hessian matrix: $Q = J_t^\top J_t = \begin{bmatrix} \| \nabla_\theta \log \mathcal{L}_{\text{R}, t} \|^2 & \langle \nabla_\theta \log \mathcal{L}_{\text{R}, t}, \nabla_\theta \log \mathcal{L}_{\text{D}, t} \rangle \\ \langle \nabla_\theta \log \mathcal{L}_{\text{R}, t}, \nabla_\theta \log \mathcal{L}_{\text{D}, t} \rangle & \| \nabla_\theta \log \mathcal{L}_{\text{D}, t} \|^2 \end{bmatrix}$
    \STATE Compute inverse $Q^{-1}$
    \STATE Compute weights: $\lambda = \frac{1}{\mathbf{1}^\top Q^{-1} \mathbf{1}}$, \quad $w_t = \lambda\, Q^{-1} \mathbf{1}$
    \STATE Apply softmax for numerical stability: $\tilde{w}_t = \text{Softmax}({w}_t)$
    \STATE Compute normalization constant: $c_t = \left( \frac{\tilde{w}_{\text{R}, t}}{\mathcal{L}_{\text{R}, t}} + \frac{\tilde{w}_{\text{D}, t}}{\mathcal{L}_{\text{D}, t}} \right)^{-1}$
    \STATE Compute balanced gradient: $\mathbf{d}_t = c_t \left( \tilde{w}_{\text{R}, t} \nabla_\theta \log \mathcal{L}_{\text{R}, t} + \tilde{w}_{\text{D}, t} \nabla_\theta \log \mathcal{L}_{\text{D}, t} \right)$
    \STATE Update network parameters: $\theta_{t+1} = \theta_t - \alpha\, \mathbf{d}_t$
\ENDFOR
\end{algorithmic}
\end{algorithm*}


\subsubsection{Solution 2: Quadratic programming}
Alternatively, we can formulate the weight optimization problem as a constrained-quadratic programming (QP) problem~\cite{nocedal1999numerical}:
\begin{equation}
\begin{aligned}
\min_{w_t} \quad & \frac{1}{2} \| J_t w_t \|^2 = \frac{1}{2} w_t^\top (J_t^\top J_t) w_t \\
\text{s.t.} \quad & w_{\text{R},t} + w_{\text{D},t} = 1, \\
& w_{\text{R},t}, w_{\text{D},t} \geq 0,
\end{aligned}
\end{equation}
where \( J_t = \begin{bmatrix} \nabla \log \mathcal{L}_{\text{R}, t}^\top \\ \nabla \log \mathcal{L}_{\text{D}, t}^\top \end{bmatrix} \) is an \( n \times 2 \) matrix containing the gradients of the log losses for rate and distortion.

Let \( Q = J_t^\top J_t \) represent the Hessian matrix. Since the gradients for rate and distortion typically reflect distinct objectives, they are rarely parallel, suggesting \( Q \) is positive definite~\cite{sener2018multi}. This property permits an analytical solution to the QP problem. The Hessian \( Q \) is given by:
\begin{equation}
Q = \begin{bmatrix} \|\nabla \log \mathcal{L}_{\text{R}, t}\|^2 & \langle \nabla \log \mathcal{L}_{\text{R}, t}, \nabla \log \mathcal{L}_{\text{D}, t} \rangle \\ \langle \nabla \log \mathcal{L}_{\text{R}, t}, \nabla \log \mathcal{L}_{\text{D}, t} \rangle & \|\nabla \log \mathcal{L}_{\text{D}, t}\|^2 \end{bmatrix}.
\end{equation}


% When the gradients are not parallel, the determinant of \( Q \) is positive, confirming that \( Q \) is positive definite.
To solve this QP problem, we introduce a Lagrange multiplier \( \lambda \) to enforce the equality constraint~\cite{boyd2004convex}, giving us the following Lagrangian:
\begin{equation}
L(w_t, \lambda) = \frac{1}{2} w_t^\top Q w_t - \lambda(w_{\text{R},t} + w_{\text{D},t} - 1).
\end{equation}

The weights \( w_t \) can then be obtained by applying the Karush-Kuhn-Tucker (KKT) conditions~\cite{mangasarian1994nonlinear}. The main KKT conditions for this problem are:

1. \textbf{Stationarity}: The gradient of the Lagrangian with respect to \( w_t \) must be zero,
   \begin{equation}
   \nabla_{w_t} L = Q w_t - \lambda \mathbf{1} = 0,
   \end{equation}
   where \( \mathbf{1} = [1, 1]^\top \). This yields \( w_t = \lambda Q^{-1} \mathbf{1} \).

2. \textbf{Primal Feasibility}: The weights must satisfy the equality constraint,
   \begin{equation}
   \mathbf{1}^\top w_t = 1.
   \end{equation}
Note: Neglecting dual feasibility and complementary slackness simplifies the solution process since non-negativity is guaranteed by a softmax projection applied subsequently.

By combining these conditions, we arrive at a simplified expression:
\begin{equation}
\mathbf{1}^\top w_t = \lambda \mathbf{1}^\top Q^{-1} \mathbf{1} = 1,
\end{equation}
which gives:
\begin{equation}
\lambda = \frac{1}{\mathbf{1}^\top Q^{-1} \mathbf{1}}, \quad w_t = \frac{Q^{-1} \mathbf{1}}{\mathbf{1}^\top Q^{-1} \mathbf{1}}.
\end{equation}


Since \( Q \) is positive definite, \( Q^{-1} \) exists, ensuring this solution is unique. To enforce non-negativity and enhance numerical stability, we then further project \( w_t \) onto the probability simplex \( \mathbb{S}_2 \) by simply using the softmax function:
\begin{equation}
\tilde{w}_t = \text{Softmax}\left({w}_t\right)=\text{Softmax}\left(\frac{Q^{-1} \mathbf{1}}{\mathbf{1}^\top Q^{-1} \mathbf{1}}\right).
\end{equation}
This step ensures \( \tilde{w}_t \in \mathbb{S}_2 \) and reduces potential numerical issues caused by large gradient variations.  It serves as an approximation that aligns with the optimal solution while maintaining similar performance despite slight deviations.


After determining the weights \( \tilde{w}_t \), we apply the same renormalization constant \( c_t \) as in Section~\ref{subsubsec:gd_over} to compute a balanced gradient for updating the model parameters \( \theta \), following the form in Eq.~\ref{eq:sl1_d}. Solution 2, with its analytical QP formulation, is particularly suitable for fine-tuning existing LIC models, offering a refined approach to balance rate and distortion objectives. The detailed implementation is presented in Algorithm~\ref{alg:solution2}.






% The detailed algorithms for both solutions are presented in Supplementary Material Section~\ref{sec:detailed_alg} as Algorithm~\ref{alg:solution1} and Algorithm~\ref{alg:solution2}.


