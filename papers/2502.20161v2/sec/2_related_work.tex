\section{Related work}
\label{sec:related}
\subsection{Learned image compression}
Learned image compression methods often rely on non-linear transform coding framework~\cite{balle2020nonlinear} to optimize the trade-off between bit-rate R and distortion D. Recent advances in LIC have focused on two main aspects:

\begin{itemize}
    \item \textbf{Transform Functions}: Various architectures and techniques are utilized to enhance the capacity of the transform functions. For instance, residual networks~\cite{he2022elic, cheng2020learned}, deformable convolutions~\cite{fu2024fast,zhang2024efficient}, and frequency decomposition~\cite{fu2024weconvene, ma2020end}. Invertible neural networks are explored in~\cite{xie2021enhanced, cai2024i2c}, while contextual clustering is leveraged in~\cite{zhang2024another, qi2024long}. Additionally, transformers are increasingly incorporated for further performance gains~\cite{liu2023learned, zhu2022transformer, zou2022devil, koyuncu2022contextformer, qian2022entroformer, li2024frequencyaware}.
    
    \item \textbf{Entropy Model Refinement}: Improvements in entropy models have involved hierarchical priors~\cite{ballé2018variational, hu2020coarse, duan2023qarv}, spatial autoregressive models~\cite{minnen2018joint}, channel autoregressive models~\cite{minnen2020channel}, and joint channel-spatial context models~\cite{jiang2023mlic, ma2021cross}. Methods like the two-pass checkerboard approach~\cite{he2021checkerboard}, codebooks~\cite{zhu2022unified}, and (lattice) vector quantization~\cite{ zhang2023lvqac, feng2023nvtc} are also explored to enhance efficiency.
\end{itemize}

Efforts have also focused on efficient implementation in LICs, such as slimmable sub-CAEs~\cite{Tao_2023_ICCV}, variable-bit-rate codecs~\cite{Wang2023evc, kamisli2024dcc_vbrlic}, knowledge distillation~\cite{fu2024fast,zhang2024theoretical}, shallow and linear decoders~\cite{yang2023computationally}, causal context losses~\cite{han2024causal}, and latent decorrelation losses~\cite{ALi2023towards}. In addition, studies on rate-distortion-complexity~\cite{minnen2023advancing, gao2024exploring, zhang2024efficient} seek to optimize the trade-off between computational cost and rate-distortion performance. Despite these advancements in BD-Rate reduction, a thorough exploration of R-D optimization in LICs is still needed.

\subsection{Multi-objective optimization}

Multi-objective optimization aims to minimize multiple objectives with a single solution. In non-trivial problems, there is typically no single solution that can optimize all objective functions simultaneously~\cite{miettinen1999nonlinear}. Numerous optimization methods have been proposed to approximate the Pareto set and front by generating a finite set of solutions~\cite{zhou2011multiobjective, li2015many}.

When all objective functions are differentiable, the gradient-based approach can be used to find a valid gradient direction that allows simultaneous improvement across all objectives~\cite{fliege2000steepest, schaffler2002stochastic, desideri2012mutiple} according to multi-objective Karush–Kuhn–Tucker
(KKT) conditions~\cite{mangasarian1994nonlinear}. The Multiple Gradient Descent Algorithm (MGDA)~\cite{desideri2012mutiple, sener2018multi} achieves this by calculating a valid gradient \( d_t = \sum_{i=1} w_i \nabla f_i(x) \) through solving the following quadratic programming problem at each iteration:
\begin{equation}
    \min_{w_i} \left\| \sum_{i=1} w_i \nabla f_i(x_t) \right\|_2^2, \quad \text{s.t.} \quad \sum_{i=1} w_i = 1, \quad w_i \geq 0.
\end{equation}

The current solution is then updated by gradient descent as \( \theta_{t+1} = \theta_t - \eta_t d_t \). If \( d_t = 0 \), it implies there is no valid gradient direction that can improve all objectives simultaneously, making \( \theta_t \) a Pareto stationary solution~\cite{desideri2012mutiple, fliege2019complexity}. This approach has also inspired various adaptive gradient methods in multi-task learning~\cite{yu2020gradient, liu2021conflict, momma2022multi, navon22a, zhou2022convergence, senushkin2023independent, fernando2023mitigating, liu2024famo, chen2024three, xiao2024direction, hu2024revisiting}.
These works highlight the promising performance of multi-objective optimization, which has not yet been thoroughly explored in the context of learned image compression R-D optimization.


