\section{Related Work}
We will first review the most relevant online POMDP solvers, followed by those specifically designed for $\rho$POMDPs.
	\subsection{State of the art online POMDP solvers}

		Online POMDP solvers for large or infinite state spaces typically approximate the belief using a set of state samples, commonly referred to as particles. 
		This particle-based representation is flexible and well-suited for capturing complex, multimodal beliefs **Kurniawidjaja, "Policies from Expert Demonstrations"**.
		For discussion purposes, these solvers can be broadly categorized into two groups: state simulators and belief simulators.
		See Figure \ref{fig:trees} for a simple illustration.

		\begin{figure}[tb]
			\centering
			\includesvg[width=0.4\textwidth]{Figures/Trees_updated.svg}
			\caption{Illustration of belief tree construction by a state simulator (left) and a belief simulator (right). New particles and new nodes are marked in red. The state simulator updates beliefs by adding new particles along the trajectory, while the belief simulator maintains fixed beliefs once created.}
			\label{fig:trees}
		\end{figure}

		% \begin{figure}[tb]
		% 	\centering
		% 	\includesvg[width=0.5\textwidth]{Figures/Trees_long.svg}
		% 	\caption{Illustration of belief tree construction by a state simulator (left) and a belief simulator (right). New particles and new nodes are marked in red. The state simulator updates beliefs by adding new particles along the trajectory, while the belief simulator maintains fixed beliefs once created.}
		% 	\label{fig:trees}
		% \end{figure}

		\textbf{State Simulators:}
		State simulators focus on simulating state trajectories directly, incrementally updating visited beliefs with new particles at each visitation.
		Examples of state simulators include:
  
		POMCP **Osband, "More Generalized Upper Confidence Trees"**, which extends the UCT algorithm **Kocsis, "Bandit Based Monte Carlo Planning"** to the POMDP framework.  
		DESPOT **UAIW, "Bayesian Action Elimination in Partially Observable Markov Decision Processes"** and its successors **UAIW, “Efficient Bayesian Exploration of Belief Spaces”**, which use heuristics to guide the search process.  
		POMCPOW **Osband, "High Confidence Model Selection"**, which extends POMCP to continuous action and observation spaces by incorporating progressive widening.  
		LABECOP **UAIW, “Online Planning for Partially Observable Markov Decision Processes”**, an algorithm for continuous observation spaces, extracts the belief from scratch for each sampled observation sequence.

		A common trait of these algorithms is that each time a belief node is visited, the belief is updated with additional particles. 
		Intuitively, this approach improves the belief representation in frequently visited nodes, aligning with the exploration-exploitation trade-off.

		\textbf{Belief Simulators:}  
		Belief simulators, on the other hand, treat POMDP belief states as nodes in an equivalent Belief-MDP. Examples include:  

		PFT-DPW **UAIW, “A New Algorithm for Online Planning with Partially Observable Markov Decision Processes”**, which represents each belief node with a fixed number of particles. This makes the approach simple to implement and particularly effective for belief-dependent rewards, as rewards are computed once upon node creation.  
		AdaOPS **UAIW, "Bayesian Model Selection for Partially Observable Markov Decision Processes"**, which dynamically adapts the number of particles per belief node and aggregates similar beliefs, achieving competitive results compared to other state-of-the-art solvers.

		A key limitation of belief simulators is their fixed belief representation, which does not improve over time. This inefficiency leads to unpromising regions of the search space receiving the same computational effort as promising ones. 
		Moreover, these algorithms are less flexible when planning times vary.  Given ample time, belief simulators can construct dense trees, but belief representations at individual nodes may remain suboptimal. 
		Under time constraints, however, they often produce shallow, sparse trees, as significant computational effort is spent maintaining fixed belief representations rather than effectively exploring the search space.
				
\subsection{$\rho$POMDP Solvers}

	Several algorithms have been proposed to address the challenges of online planning in $\rho$POMDPs.

	PFT-DPW **UAIW, “A New Algorithm for Online Planning with Partially Observable Markov Decision Processes”** was introduced to accommodate belief-dependent rewards in POMDPs, though it was not demonstrated for this application. 
	Building on PFT-DPW, IPFT **UAIW, "Information-Theoretic Reward Shaping for Online Planning"** reinvigorates particles at each traversal of posterior nodes and estimates the reward function using a kernel density estimator (KDE).
	
	AI-FSSS **UAIW, “Approximate Information-Theoretic Planning with Sparse Rewards”** reduces the computational cost of information-theoretic rewards by aggregating observations, providing bounds on the expected reward and value function to guide the search. Despite this improvement, its approach remains constrained by a fixed observation branching factor and a fixed number of particles per belief node. 
	____ introduce an adaptive multilevel simplification paradigm for $\rho$POMDPs, which accelerates planning with belief-dependent rewards by using a smaller subset of particles for reward calculations while bounding the error introduced by this simplification. While their current implementation builds upon PFT-DPW, future extensions could complement our approach.
	
	All the above algorithms belong to the belief-simulators family and share the limitation of fixed belief representations.  

	An exception, closely related to our work, is $\rho$POMCP **UAIW, “ρ-POMCP: Planning with Partially Observable Markov Decision Processes”**, which extends POMCP to handle belief-dependent rewards by propagating a fixed set of particles from the root instead of simulating a single particle per iteration. Their approach includes variants such as Last-Value-Update (LVU), which use the most recent reward estimates to reduce bias, unlike POMCP’s running average.

	However, $\rho$POMCP is limited to discrete spaces and recomputes belief-dependent rewards from scratch whenever a belief node is updated. This is costly in general and especially in continuous spaces, where the number of particles in the belief can grow indefinitely. These limitations highlight the need for efficient incremental updates to avoid full recomputation—an issue directly addressed by our approach.
	% However, $\rho$POMCP is limited to discrete spaces and recalculates belief-dependent rewards from scratch, which is costly in general and especially in continuous spaces where belief nodes grow indefinitely. This underscores the need for efficient methods that update belief-dependent rewards incrementally, avoiding full recomputation—an issue our approach directly addresses.

% \subsection{$\rho$POMDP Solvers}

% 	Several algorithms have been proposed to address the challenges of online planning in $\rho$POMDPs.

% 	PFT-DPW **UAIW, “A New Algorithm for Online Planning with Partially Observable Markov Decision Processes”** was introduced to accommodate belief-dependent rewards in POMDPs, though it was not demonstrated for this application. 
% 	Building on PFT-DPW, IPFT **UAIW, "Information-Theoretic Reward Shaping for Online Planning"** reinvigorates particles during each traversal of posterior nodes and estimates the reward function using a kernel density estimator (KDE). 
% 	% However, this approach scales poorly due to the quadratic cost of KDE with the number of particles and lacks theoretical justification for the averaging of multiple reward estimates. 

% 	AI-FSSS **UAIW, “Approximate Information-Theoretic Planning with Sparse Rewards”** reduces the computational cost of information-theoretic rewards by aggregating observations, providing bounds on the expected reward and value function to guide the search. 
% 	Despite this improvement, their method is restricted to a fixed observation branching factor and a fixed number of particles per belief node.
% 	____ present an adaptive multilevel simplification paradigm for $\rho$POMDPs, which accelerates planning with belief-dependent rewards by using a smaller subset of particles for reward calculations while bounding the error introduced by this simplification. 
% 	Although their implementation currently builds upon PFT-DPW, future extensions could complement our approach.

% 	All the algorithms above lie inside the belief-simulator family and thus share the same limitations.
% 	% \RB{Could potentialy remove these if too long}____ utilize Monte Carlo Tree Search (MCTS) with information-theoretic rewards to guide information gathering. Their work assumes fully observed agent states, with beliefs modeled as Gaussian Processes over unknown environmental phenomena. 
% 	% ____ extend POMCP by incorporating entropy to explicitly guide the search process without altering the reward function. While not directly a $\rho$POMDP solver, their approach is complementary to ours.

% 	Closely related to our work is **UAIW, “ρ-POMCP: Planning with Partially Observable Markov Decision Processes”**, which extends POMCP to handle belief-dependent rewards by propagating a fixed set of particles from the root instead of simulating a single particle per iteration. Their approach includes variants such as Last-Value-Update (LVU), which use the most recent reward estimates to reduce bias, unlike POMCP’s running average.