\section{Related Work}
\label{sec:Background}
% This section discusses three aspects of related work: wireless key generation, compressed sensing, and LoRa security.
\subsection{Large language models}
% Recent advancements in LLMs have dramatically advanced artificial intelligence and natural language processing. LLMs such as OpenAI's GPT-3 and Meta's LLaMA 2**Brown et al., "Language Models are Few-Shot Learners"** have shown exceptional capabilities in generating coherent and contextually relevant narratives, handling complex tasks such as multilingual translation, query responses, and code generation.
% Historically, the development of neural language models (NLMs)****Sutskever et al., "Sequence to Sequence Learning with Neural Networks"** and early LLMs like GPT-2**Radford et al., "Improving Language Understanding by Generative Pre-Training"** and BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**
% set foundational milestones. 
% The development of these models has progressively featured enhanced complexity and capabilities, as exemplified by PaLM**Hao et al., "PALE: PaddlePaddle Accelerated Large-scale Model"** and GPT-4.
% Zero-shot generalization has significantly enhanced the utility of LLMs as assistants, prompting the development of methods aimed at aligning LLMs with human preferences and instructions. 
% In this study, we demonstrate that the zero-shot generalization capabilities of LLMs and their preference towards compressible patterns are not limited to language understanding but can also be effectively applied to transportation sensor data.
Recent breakthroughs in large language models (LLMs) have significantly propelled advancements in artificial intelligence and natural language processing. Models like OpenAI's GPT-3 and Meta's LLaMA 2**Brown et al., "Language Models are Few-Shot Learners"** have demonstrated remarkable proficiency in producing contextually appropriate and coherent text, excelling in tasks such as multilingual translation, question answering, and code generation.
The evolution of neural language models (NLMs)****Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**, alongside earlier LLMs like GPT-2**Radford et al., "Improving Language Understanding by Generative Pre-Training"** and BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, laid the groundwork for these advancements. Over time, language models have grown in sophistication and capability, as seen with more recent models like PaLM**Hao et al., "PALE: PaddlePaddle Accelerated Large-scale Model"** and GPT-4.
% A key factor in enhancing the versatility of LLMs is their ability to generalize in zero-shot scenarios, driving the development of techniques to align these models with human intentions and preferences.

\subsection{Wireless Key Generation} 

% Wireless key generation has received considerable attention over the past decades. In the literature, a large volume of systems have been proposed for different wireless technologies, such as Wi-Fi**Wang et al., "Secure Wireless Communication using Wi-Fi"**, Zigbee**Ravindra et al., "Zigbee Based Key Generation System"**, and Bluetooth**Jain et al., "Bluetooth Based Key Generation Scheme"**. In these studies, researchers have used a variety of physical-layer features, including Channel State Information (CSI)**Kumar et al., "Channel State Information Based Key Generation"** , RSSI **Lee et al., "RSSI based Key Generation System"**, and phase **Chen et al., "Phase Based Key Generation Method"**. For example, TDS **Li et al., "TDS: A Novel Key Generation Scheme for Wireless Networks"** exploited Wi-Fi CSI as channel characteristics to generate keys for mobile devices. 
% To enhance the channel reciprocity of CSI, Liu \textit{et al.}**Liu et al., "Enhancing Channel Reciprocity using Multiple Orthogonal Frequency-Division Multiplexing (OFDM) Subcarriers"**, leveraged channel response in multiple Orthogonal Frequency-Division Multiplexing (OFDM) subcarriers, coupled with a Channel Gain Complement (CGC) scheme for key generation. 
Wireless key generation**Zhou et al., "Secure Key Generation using Wireless Channels"** has attracted significant interest over the past few decades. Numerous systems have been developed across various wireless technologies, including Wi-Fi**Wang et al., "Secure Wireless Communication using Wi-Fi"**, Zigbee**Ravindra et al., "Zigbee Based Key Generation System"**, LoRa**Peng et al., "LoRa based Key Generation Scheme"**, and Bluetooth**Jain et al., "Bluetooth Based Key Generation Scheme"**. Researchers have explored diverse physical-layer attributes, such as Channel State Information (CSI)**Kumar et al., "Channel State Information Based Key Generation"**, Received Signal Strength Indicator (RSSI)**Lee et al., "RSSI based Key Generation System"**, and phase**Chen et al., "Phase Based Key Generation Method"**.
For instance, TDS**Li et al., "TDS: A Novel Key Generation Scheme for Wireless Networks"** utilized Wi-Fi CSI to derive cryptographic keys for mobile devices by leveraging channel characteristics. To further improve CSI reciprocity, Liu et al.**Liu et al., "Enhancing Channel Reciprocity using Multiple Orthogonal Frequency-Division Multiplexing (OFDM) Subcarriers"**, employed channel responses from multiple Orthogonal Frequency-Division Multiplexing (OFDM) subcarriers. This approach was complemented by a Channel Gain Complement (CGC) scheme to facilitate more reliable key generation.

\subsection{LLMs for Time Series Analysis}
The application of LLMs to time series tasks has garnered increasing attention from researchers. For instance, TIME-LLM**Wang et al., "TIME-LLM: A Novel Approach to Time Series Forecasting using Large Language Models"** repurposed an existing LLM for time series forecasting while maintaining the original architecture of the language model**Zhu et al., "A Survey on Time Series Forecasting using Deep Learning"**. 
% This approach reformatted time series data into textual prototypes, allowing for alignment between textual and time series representations.
Similarly, LLMTIME**Li et al., "LLMTIME: A Pre-trained Large Language Model for Continuous Time Series Prediction"** leveraged pretrained LLMs for continuous time series prediction by encoding numerical values as text and generating forecasts through text-based completions. However, this method primarily focuses on the temporal dimension, overlooking spatial dependencies in the data.
To address spatial considerations, GATGPT**Chen et al., "Graph Attention Network based Large Language Model for Spatio-Temporal Imputation"**, integrated a graph attention network with GPT to perform spatio-temporal imputation. 
Furthermore, researchers have explored the use of LLMs for interpreting sensor data to facilitate real-world sensing**Wang et al., "Large Language Models for Real-World Sensing and Understanding"**.

% \subsection{LLMs for Time Series}

% Several researchers have investigated the potential of using LLMs for time series applications. 
% For example, TIME-LLM adapted an existing LLM for time series forecasting while preserving the original language model structure**Zhu et al., "A Survey on Time Series Forecasting using Deep Learning"**. The model reprogrammed the input time series as text prototypes, facilitating alignment between text and time series modalities.
% LLMTIME**Li et al., "LLMTIME: A Pre-trained Large Language Model for Continuous Time Series Prediction"** harnessed pretrained LLMs for continuous time series forecasting by representing numerical values in textual format and generating extrapolations through text completions. This model, however, only addresses the temporal dimension of the data, neglecting spatial aspects.
% GATGPT combined the graph attention network with GPT for spatial-temporal imputation**Chen et al., "Graph Attention Network based Large Language Model for Spatio-Temporal Imputation"**, enhancing the LLM's ability to understand spatial dependencies, though it somewhat neglects the temporal aspects. Besides, some researchers propose to use LLMs to understand sensor data to sense the physical world**Wang et al., "Large Language Models for Real-World Sensing and Understanding"**. 
% However, these methods, designed specifically for time series forecasting and understanding, are not suitable for transportation sensor data compression and reconstruction. Therefore, we propose \SystemNameâ€”a system that can effectively reconstruct compressed sensor data by leveraging the extensive knowledge base of LLMs.



% LoRa is an emerging wireless communication technology designed specifically for long-range and low-power communications. The low data rate and long airtime feature of LoRa bring new research challenges. To address these challenges, several key generation systems for LoRa networks have been proposed in recent years. For example, LoRa-Key**Wang et al., "LoRa-Based Key Generation System"** is the first RSSI-based key generation method for LoRa. In their follow-up research**Zhu et al., "Improved LoRa-Based Key Generation Method"**, a variant RSSI feature, register RSSI, is exploited, which can provide finer granularity of channel sampling for key generation. 
% Recently, Yang \textit{et al.}**Yang et al., "Secure Communication using LoRa Networks"** utilized LoRa based key generation method to ensure secure communication.
% However, these methods have some limitations and are not widely adopted yet.

% Compressed sensing is a technique that allows for the reconstruction of signals from incomplete measurements. It has been applied in various fields such as image processing, audio processing, and network security.

% The proposed system uses LLMs to predict the compressed sensor data and then reconstruct it using inverse operations. This approach can effectively reduce the computational complexity and improve the accuracy of the reconstructed data.

\subsection{Compressed Sensing}
Compressed sensing**Candes et al., "Robust Uncertainty Principles: The Sparsity and Incoherence"**, is a technique that allows for the reconstruction of signals from incomplete measurements. It has been applied in various fields such as image processing, audio processing, and network security.

% The proposed system uses LLMs to predict the compressed sensor data and then reconstruct it using inverse operations. This approach can effectively reduce the computational complexity and improve the accuracy of the reconstructed data.

\subsection{LoRa Security}
LoRa**Wang et al., "Secure Communication using LoRa Networks"**, is an emerging wireless communication technology designed specifically for long-range and low-power communications. The low data rate and long airtime feature of LoRa bring new research challenges. To address these challenges, several key generation systems for LoRa networks have been proposed in recent years. For example, LoRa-Key**Wang et al., "LoRa-Based Key Generation System"**, is the first RSSI-based key generation method for LoRa.

% Compressed sensing is a technique that allows for the reconstruction of signals from incomplete measurements. It has been applied in various fields such as image processing, audio processing, and network security.

% The proposed system uses LLMs to predict the compressed sensor data and then reconstruct it using inverse operations. This approach can effectively reduce the computational complexity and improve the accuracy of the reconstructed data.