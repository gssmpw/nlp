\section{Conclusion}
This paper introduced PGB, one-shot semi-structured pruning with a grouping strategy, as a fast and simple compression approach for transformer-based models. PGB efficiently compresses task-specific BERT models into lightweight and accurate versions within a few hours, contrasting with other SOTA methods that take more than a day to achieve comparable results. By finding an adaptively grouped architecture, PGB combines the advantages of structured pruning and unstructured pruning, offering both computational efficiency and high accuracy. Through extensive experiments, we validated that PGB is a practical solution for quickly compressing complex transformer architectures without 
 significant performance degradation.
