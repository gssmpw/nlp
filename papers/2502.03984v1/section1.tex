\section{Introduction}
% Hook(background)
Transformer-based models \cite{Vaswani2017} including BERT \cite{BERT}, RoBERTa \cite{Roberta} and GPT-3 \cite{GPT} have achieved great performance on natural language processing (NLP) tasks. However, all these models suffer from a large number of parameters, which often limits their applications due to high computational cost and memory usage. To overcome this limitation, extensive research has been conducted to reduce the model size of transformer architectures.

Recent works on compressing BERT adopt two primary strategies, pruning \cite{Han} and knowledge distillation (KD) \cite{KD}. Pruning can further be classified into two categories based on how many times pruning and recovery processes are performed: one-shot pruning \cite{SNIP} and iterative pruning \cite{Thinet,LoB}. Even though one-shot pruning is simple and computationally efficient as it conducts only one pruning phase, it tends to be less effective to maintain high accuracy. Therefore, the dominant approach for BERT is taking iterative steps of pruning and recovery while training with original dataset.

Furthermore, recent pruning methods \cite{DynaBERT,block,Xia} attempt to overcome the low pruning performance with the help of KD, which has been successful in maintaining high performance in BERT \cite{DistilB,TinyBERT}. However, the distillation process can be even more time-consuming than iterative pruning, and it is often too complicated to identify what aspects of the teacher model should be matched to the student model, particularly in the BERT architecture.

%figure 1
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.82 \columnwidth]{overflow.pdf}
    \caption{Permutation and Grouping for BERT (PGB), where weight matrices are grouped and all individual weights not belonging to groups are pruned.}
    \label{fig:overview}
\end{figure}

%Motivation
In this paper, we design a novel one-shot pruning method for a task-specific BERT, aiming to achieve both high compression efficiency and high accuracy. Due to the lack of recovery chances, it is quite challenging for one-shot pruning to maintain high performance. Our proposal is to devise a semi-structured pruning method, called \textit{Permutation and Grouping for BERT} (\ours), which combines the benefits of both unstructured and structured pruning. Thus, PGB effectively reduces the model size without sacrificing so much of the accuracy as in unstructured pruning, while ensuring computational efficiency as in structured pruning.

%Methodology Description
Our PGB method is illustrated in Figure \ref{fig:overview}. Basically, we apply a group-based pruning scheme \cite{DGC,Zhao} to each structure of BERT, including multi-head attention (MHA) and feed-forward network (FFN). More specifically, PGB constructs important groups of individual weights for each layer to be preserved and prune all other weights that are not in such a group. Although training with group-based architectures from scratch has been studied with a different purpose \cite{GroupFormer,groupbert}, pruning by weight grouping has never been tackled in the context of transformer-based models. A major challenge is how to preserve the original performance of a given task-specific BERT after pruning unimportant weights. To this end, PGB performs an optimal permutation procedure so that more important weights are clustered as a structure, and adaptively determines the number of important groups for each layer of either MHA or FFN, occasionally dropping the entire layer. Finally, its re-permutation process safely rearranges each weight to its original position. 

%Experiment results
A thorough experimental study is conducted by applying our PGB method to $\text{BERT}_{\text{BASE}}$ \cite{BERT} on the GLUE \cite{GLUE} and SQuAD \cite{SQuAD} benchmarks. Our experimental results show that PGB outperforms the state-of-the-art (SOTA) pruning methods in terms of both efficiency and accuracy.
