
\section{Experiments}
\subsection{Environment}
\paragraph{Datasets} 



We conduct our experiments using two benchmark datasets, $\text{SQuAD}$ \cite{SQuAD,SQuAD2} and seven tasks (QNLI, QQP, SST-2, CoLA, STS-B, MRPC, and RTE) in GLUE \cite{GLUE} on $\text{BERT}_{\text{BASE}}$ \cite{BERT}.
In our experiments, we use 2K samples from the training data for each benchmark dataset. Also, we perform re-finetuning on the pruned model using the training data of each NLP downstream task and evaluate on the corresponding dev dataset for each task. For detailed information on the benchmarks, please refer to \ref{app:gluedetail}. 

\paragraph{Compared methods}
In order to evaluate the performance of PGB, we conduct comparative experiments with the state-of-the-art structured pruning methods for BERT, including EBERT \cite{ebert}, DynaBERT \cite{DynaBERT}, and CoFi \cite{Xia}. In order to examine their pruning performance, we train each method without using distillation and data augmentation, both of which can commonly be applied to each pruning method.


\paragraph{Implementation details} 
Our PGB method is implemented using Python 3.7.15, PyTorch \cite{Pytorch} and CUDA 11.6, along with the Huggingface library \cite{Wolf}, which incorporates the latest NLP techniques. We set the hyperparmeter $N_{perm}$ that controls the number of sorting operations in the permutation step, and $G_{max}$ that indicates the maximum number of groups, ensuring that the size of the grouped model is within the given budget capacity. All the experiments are conducted on a PC with NVIDIA GeForce RTX A6000. We report that all experimental results are the average of 5 random seeds within a range between 0 and 10.

% Baseline model
As our target model, we use fine-tuned BERT \cite{BERT} for specific tasks. Even though we mainly compare the performance using $\text{BERT}_{\text{BASE}}$, we also conduct experiments using $\text{RoBERTa}_{\text{BASE}}$ and $\text{DistilBERT}_{\text{BASE}}$ \cite{DistilB} (refer to \ref{app:roberta} and \ref{app:distil}). Full experimental details can be found at \ref{app:exdetail}.

\subsection{Main Experimental Results}

\paragraph{Performance comparison}
Table \ref{tab:comACC} and Figure \ref{fig:comacc} show the performance comparison results of PGB with prior structured methods, using the seven tasks of GLUE and SQuAD. To equalize the resulting size of pruned models, we re-implement the released code of the compared methods by ourselves. For CoFi and DynaBERT, since there is no publicly available code for SQuAD, comparative experiments for these two methods were conducted exclusively on the GLUE benchmarks. Table \ref{tab:comACC} demonstrates the corresponding results with pruning rates 50\% and 88\%, where 88\% is the maximum pruning rate that can be achieved by all the compared methods. It is clearly observed that the proposed PGB method outperforms the previous structured pruning methods in all task-specific BERT models, which indicates that PGB is not only the fastest pruning method as observed in Table \ref{time}, but also highly effective to preserve the information of the original model. This empirically implies that we can properly compress transformer architectures with one-shot pruning, without relying on complicated and time-consuming methods.


Figure \ref{fig:comacc} shows how the compression performance changes when varying the target model size in terms of the number of FLOPs. We can observe that PGB generally achieves the best performance among the compared methods. More importantly, PGB tends to work better at higher compression rates, as the performance degradation of PGB gets smaller than those of the other methods as the size of compressed model decreases. These results imply that PGB is capable of maintaining high performance even at extreme compression cases. Considering the compression speed of PGB, we can claim that PGB is a computationally efficient yet accurate compression method for transformer architectures. 

\begin{table}[t!]
\begin{tabular}{clccccc}
\toprule
\multirow{2}{*}{ Pruning ratio} &\multirow{2}{*}{Method}& QNLI & QQP & SST-2 \\
            &       & Acc. & Acc. & Acc.   \\
\hline \hline
0\% & $\text{BERT}_{\text{BASE}}$ & 91.43 & 91.50 & 93.16   \\
\hline
\multirow{4}{*} {50\%} &\multirow{1}{*}{BMP} &  \multirow{1}{*}{89.40} & \multirow{1}{*}{90.30} & \multirow{1}{*}{90.71}    \\
 & \multirow{1}{*}{LayerDrop}   & \multirow{1}{*}{87.60} & \multirow{1}{*}{90.40} & \multirow{1}{*}{90.30}  \\
& \multirow{1}{*}{SNIP}         & \multirow{1}{*}{89.50} & \multirow{1}{*}{88.90} & \multirow{1}{*}{91.80} \\
& \multirow{1}{*}{$\textbf{PGB}$ (Ours)}    & \multirow{1}{*}{\textbf{90.34}} & \multirow{1}{*}{\textbf{91.06}} & \multirow{1}{*}{\textbf{92.30}} \\
\hline
\multirow{2}{*}{80\%}  &\multirow{1}{*}{BMP }   & \multirow{1}{*}{86.40} & \multirow{1}{*}{89.30} &\multirow{1}{*}{89.79}   \\
    & \multirow{1}{*}{\textbf{PGB} (Ours)}  &  \multirow{1}{*}{\textbf{87.12}} & \multirow{1}{*}{\textbf{90.40}} & \multirow{1}{*}{\textbf{89.84}}       \\
\bottomrule
\end{tabular}
\centering
\caption{Performance comparison with other SOTA pruning methods on $\text{BERT}_{\text{BASE}}$.}
\label{tab:more}
\end{table}

%----------------------------------------------------------------------------
\begin{table}
\centering
\begin{tabular}{c|cc|cc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{2}{c|}{40\%}  & \multicolumn{2}{c}{60\%} \\ \cmidrule(r){2-5}
                    & Vanilla  & Re-finetuned & Vanilla & Re-finetuned  \\
\hline

 SST-2 &  92.2  &  92.7  &90.3 & 91.6 \\

QNLI & 90.1  &  91.0   & 88.2&  89.4  \\

 MRPC &  84.8    &  85.1 & 80.9 & 83.5 \\
\bottomrule
\end{tabular}
\centering
\caption{Ablation study in PGB (ours) at 40\% and 60\% of reduced FLOPs before and after re-finetuning using GLUE.}
\label{tab:vanilla}
\end{table}
%-------------------------------------------------------------------------------------------
\paragraph{Comparison with other SOTA methods}
Table \ref{tab:more} presents additional comparisons between PGB and other state-of-the-art (SOTA) pruning methods, using the reported results from their respective experiments. The compared methods include: hybrid-based block movement pruning (BMP) \cite{block}, LayerDrop that drops certain layers of the model \cite{layer2}, and SNIP that prunes redundant mappings in residual modules \cite{TFsnip}. Similar to the findings in Table \ref{tab:comACC}, PGB shows minimum performance degradation. Of particular interest is the comparison with BMP \cite{block}, as it is introduced as another semi-structured pruning method in Section \ref{sec:related}. PGB turns out to outperform BMP even though the reported results of BMP are not from its fully semi-structured pruning scheme, but rather its improved hybrid version yet without distillation.
\begin{figure}[t!]
    \centering
    {\includegraphics[width=0.7\columnwidth]{squad_acc.pdf}}
    \caption{Performance comparison with structured pruning methods varying the Reduced FLOPs ratio on $\text{SQuAD}$ benchmarks.}
    \label{fig:squadacc}
\end{figure}
\begin{table*}[t!]
\centering
\begin{tabular}{l|c|c|c|c}
%\noalign{\smallskip}\noalign{\smallskip}\hline
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Time for Pruning + Re-finetuning}  & \multicolumn{1}{c|}{FLOPs} & \multicolumn{1}{c}{Accuracy} \\ \cmidrule(r){2-5}
 & (epochs)  &  (hours) & (G) & (\%) \\ 
\hline
EBERT \cite{ebert}      &  3 + 3       & $\leq$ 3.5 + 2        &    2.78    & 88.1 \\
DynaBERT \cite{DynaBERT}&  2 + 3        & $\leq$ 25 + 38        &    \textbf{2.60}   &   86.8  \\
CoFi \cite{Xia}     & 20 + 20           & $\leq$ 26 + 23        &    2.97    & 89.8 \\ \hline
 \textbf{PGB (ours)}& \textbf{0 + 3}  & $\leq$ \textbf{0.1 + 2} &    \textbf{2.60}  & \textbf{90.1} \\
%\hline
\bottomrule
\end{tabular}
\caption{Comparison of the pruning efficiency using QQP with 88\% pruning rate on $\text{BERT}_{\text{BASE}}$.}
\label{time}
\end{table*}
\paragraph{Experiments on SQuAD Benchmarks}\label{app:squad} 

We compare additional pruning methods with PGB for $\text{SQuAD}_{v1.1}$ \cite{SQuAD} and $\text{SQuAD}_{v2.0}$ \cite{SQuAD2}. The compared methods exclude knowledge distillation (KD) and data augmentation. Specifically, our approach is compared with PLATON \cite{platon}, representing the state-of-the-art in unstructured pruning, and EBERT \cite{ebert}. In this context, PLATON utilized the extended $\text{PLATON}_{\text{structure}}$ based on structured pruning. Our PGB method shows minimal performance degradation compared to other methods, as shown in Figure \ref{fig:squadacc}.


%Roberta result
\begin{figure*}[t!]
    \centering
    {\includegraphics[width=  \textwidth]{roberta.pdf}}
    \caption{PGB with $\text{BERT}_{\text{BASE}}$ and $\text{RoBERTa}_{\text{BASE}}$ }
    \label{fig:roberta}
\end{figure*}

\paragraph{Efficiency of pruning procedures}
To evaluate the efficiency of each pruning method, we measure how long each method takes to prune the original model and re-finetune the pruned model, as shown in Table \ref{time}. In particular, we report the case of pruning with 88\% on QQP, where the time cost was maximized. Thanks to its one-shot pruning scheme, our PGB method is clearly the fastest pruning method among all the compared methods, while maintaining the best accuracy after pruning and re-finetuning. PGB takes at most 2.1 hours to obtain the final compressed model, whereas most of the other methods takes more than a day to get the same-sized model. Table \ref{time} also reports the computational cost of each compressed model in terms of the number of FLOPs. Although PGB is a semi-structured pruning method, it manages to achieve a comparable model efficiency, without any specific hardware support, to those of the fully structured pruning methods, EBERT \cite{ebert}, DynaBERT \cite{DynaBERT} and CoFi \cite{Xia}.


\subsection{More Experimental Results}
\paragraph{Pruning on DistilBERT}\label{app:distil}
We conduct additional experiments on PGB with pruning rates of 40\% and 60\% for $\text{DistilBERT}_{\text{BASE}}$, and the results are presented in Table \ref{tab:distil}. While $\text{DistilBERT}_{\text{BASE}}$ is half the size of $\text{BERT}_{\text{BASE}}$, as indicated in Table \ref{tab:BERT}, the model configuration remains the same, excluding the number of layers. Consequently, the values of permutation ($N_{perm}$) and the maximum group number ($G_{max}$) are identical. The experimental results in Table \ref{tab:distil} demonstrate that even for a smaller model, our grouping-based pruning scheme does not result in substantial information degradation.  


\begin{table} [t!]
\begin{tabular}{cccccc}
\noalign{\smallskip}\noalign{\smallskip}\hline

Pruning Ratio & Method & QNLI &SST-2 &MRPC & STS-B   \\ 

\hline \hline
{0\%}& $\text{DistilBERT}_{\text{BASE}}$ & \multirow{1}{*}{88.6} & \multirow{1}{*}{91.3} & \multirow{1}{*}{84.8} &\multirow{1}{*}{85.8} \\

{40\%}  &\multirow{2}{*}{PGB (ours)}     &  \multirow{1}{*}{88.1}  & \multirow{1}{*}{91.2} & \multirow{1}{*}{84.6}  & \multirow{1}{*}{85.5}  \\

{60\%}  &         & \multirow{1}{*}{86.5}  & \multirow{1}{*}{89.3} &\multirow{1}{*}{83.8}& \multirow{1}{*}{84.3}  \\

\hline
\end{tabular}
\centering
\caption{Performance of PGB on $\text{DistilBERT}_{\text{BASE}}$ using 40\% and 60\% pruning rates}
\label{tab:distil}
\end{table}


\paragraph{Pruning on RoBERTa}\label{app:roberta}
Figure \ref{fig:roberta} shows the proposed PGB results with $\text{RoBERTa}_{\text{BASE}}$. Similar to BERT, PGB with pruning rates of 60\% on RoBERTa is able to  maintain over 98\% of the performance of the original model. When the Pruning rate surpasses 60\%, RoBERTa demonstrates better performance compared to BERT. However, as the sparsity ratio increases, the performance of BERT surpasses RoBERTa.


\subsection{Ablation Studies}\label{app:ablation}
\paragraph{Performance of PGB without re-finetuning}

Table \ref{tab:vanilla} presents the performance comparison between vanilla PGB, which does not perform re-finetuning, and PGB with re-finetuning. We measure the accuracy of the compressed model with 40\% and 60\%  of reduced FLOPs before and after re-finetuning on task-specific BERT models. We can observe that vanilla PGB itself is already effective to maintain high performance after pruning. For the SST-2 and QNLI tasks, the performance of vanilla PGB is either matches or surpasses that of other pruning methods shown in Figure \ref{fig:comacc}. This demonstrates the capability of our grouped pruning operation to maintain the original performance of large transformer-based models with just a single round of pruning.

\begin{figure}[t!]
    \centering
    {\includegraphics[width=0.7\columnwidth]{group_ablation.pdf}}
  
    \caption{Ablation study with respect to the number of Group ($G_{max}$) with $\tau$=1e-5}
    \label{fig:group_ablation}
\end{figure}

\begin{figure}[t!]
    \centering
    {\includegraphics[width=0.7\columnwidth]{tau_ablation.pdf}}

    \caption{Ablation study with respect to threshold ($\tau$) with $G_{max} = 6$}
    \label{fig:tau_ablation}
\end{figure}

\paragraph{Hyperparameter Sensitivity}
We conduct ablation studies to investigate the sensitivity to the hyperparameter utilized in the proposed PGB method. In Figures \ref{fig:group_ablation} and \ref{fig:tau_ablation}, we present visualization of the accuracy of the compressed model as we vary hyperparameter values, $G_{max}$ and $\tau$. 
Figure \ref{fig:group_ablation} indicates that when the number of groups is limited to a small number (i.e., ${G}_{max}=3$), there is a significant decrease in performance, which is probably because some important weights can be pruned in order to reach each target compressed size, just like in typical structured pruning. At the same time, however, it does not always mean that the larger the $G_{max}$ value, the better the final performance, as the performance also drops in the case of ${G}_{max}=8$. In terms of the threshold $\tau$, Figure \ref{fig:tau_ablation} demonstrates that there is a certain optimal point of $\tau$ to maximize the final performance. In our experiments, we found that $G_{max}=6$ and $\tau=1e-5$ produce the most promising results for compressed models.    


