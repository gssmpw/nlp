\newpage

\setcounter{table}{0}
\setcounter{figure}{0}

\section{Experimental Details}\label{app:exdetail}
\subsection{Details for Comparison Methods}\label{app:compare}
We conduct comparative experiments on structured pruning for BERT: CoFi \cite{Xia}, BMP (based Hybrid Filled) \cite{block}, DynaBERT \cite{DynaBERT} and EBERT \cite{ebert}. All these methods belong to the iterative pruning approach, which involves performing pruning during training to improve performance. CoFi \cite{Xia} consists of 2 stages: pruning and final finetuning. Each stage involves 20 epochs of training with layer-wise distillation. BMP \cite{block} selects appropriate components to prune at the block level over 20 epochs training stages. Subsequently, prediction layer distillation is performed on the pruned model. DynaBERT \cite{DynaBERT} utilizes a 2-stage training process with knowledge distillation to dynamically prune the width and depth size, followed by a final finetuning stage. The number of training epochs varies depending on the data size. For large datasets, the width-adaptive and width- and depth-adaptive stages are performed for 1 epoch of training each, while for small datasets, each stage is performed for 3 epochs of training. Additionally, in all cases, 3 additional epochs of finetuning are conducted. EBERT \cite{ebert} involves 3 epochs of joint training during the pruning stage, followed by 3 epochs of final finetuning on the pruned model.

\subsection{Hyperparameters}\label{app:hyper}

The detailed experimental setup for PGB is provided in Table \ref{tab:hyper}. During the PGB pruning process, we utilize two hyperparameters, namely $N_{perm}$ and $G_{max}$. Also, we use the hyperparameters of the original BERT model and those of prior pruning methods for BERT.


\begin{table*} [htb]
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccccccc}
\toprule
%\noalign{\smallskip}\noalign{\smallskip}\hline
Model & \#Params & \#Encoder Layer & Hidden dim. &\#Heads & $d$ & $d_{ffn}$ \\ 
\hline

$\text{BERT}_{\text{BASE}}$ & 85M & 12& 768 &12 & 768 & 3072 \\

$\text{RoBERTa}_{\text{BASE}}$ & 100M & 12 & 768 & 12 & 768 & 3072   \\

$\text{DistilBERT}_{\text{BASE}}$ & 43M & 6 & 768 & 12 & 768 & 3072   \\


\bottomrule
\end{tabular}
}
\centering
\caption{Specifications of $\text{BERT}_{\text{BASE}}$, $\text{DistilBERT}_{\text{BASE}}$ and $\text{RoBERTa}_{\text{BASE}}$ models}
\label{tab:BERT}
\end{table*}
%-------------------------- Hyperparameters ------------------------------------
\begin{table} []
\begin{tabular}{lcc}
\noalign{\smallskip}\noalign{\smallskip}\hline
Hyperparameters &     \\
\hline
batch size  & 32 (GLUE), 16 (SQuAD) \\
pruning/finetuning epochs&   0 / 3  \\
finetuning learning rate & 2e-5, 3e-5 \\
max sequence length & 128 (GLUE), 384 (SQuAD) \\
$G_{max}$ & 6  \\
$N_{perm}$ & 6 \\
$\tau$       & 1e-5 \\    
inference batch size & 128 (GLUE) / 32 (SQuAD) \\


\hline
\end{tabular}
\centering
\caption{Hyperparameter settings of PGB in experiments}
\label{tab:hyper}
\end{table}

\begin{table}[t!]
\begin{tabular}{cccc}
\toprule
Task & \#Train &  \#Dev  & Metrics \\
\hline \hline
QNLI & 105k & 5.5k & Acc. \\
QQP  & 364k & 40k  & Acc.\\
SST-2& 67k & 872  & Acc.\\
CoLA  & 8.5k & 1k  & Matthew's corr.\\
STS-B & 7k & 1.5k  & Spearman corr.\\
MRPC  & 3.7k & 408 & Acc.\\
RTE  & 2.5k & 276 & Acc. \\
$\text{SQuAD}_{\text{v1.1}}$  & 88k & 10.5k  & EM/F1\\
$\text{SQuAD}_{\text{v2.0}}$  & 132k & 12k  & EM/F1\\
\bottomrule
\end{tabular}
\centering
\caption{Dataset summary of GLUE and $\text{SQuAD}_{\text{v1.1}}$.}
\label{GLUE&SQuAD}
\end{table}


\subsection{Details About Benchmark Datasets}\label{app:gluedetail}
 
%우리는 classification task에 해당하는 GLUE benchmark dataset에 사용하여 실험하였다. 
We perform experiments using the GLUE \cite{GLUE} and SQuAD \cite{SQuAD,SQuAD2} benchmark datasets.
%GLUE is applicable to a classification task. Among the 9 types of data in GLUE, we exclude WNLI \cite{WNLI}, which represents unstable state.  
Each task of GLUE and SQuAD datasets can fall into the following categories:
\begin{itemize}
    \item Question Answering: $\text{SQuAD}_{v1.1}$, $\text{SQuAD}_{v2.0}$
    \item Sentence Pair Similarity: QQP, MRPC \cite{MRPC}, STS-B \cite{STS-B}
    \item Natural Language Inference: RTE, QNLI
    \item Single Sentence Task: SST-2 \cite{SST-2}, CoLA \cite{CoLA}
\end{itemize}
The detailed information regarding the dataset size and metric for each task is provided in Table \ref{GLUE&SQuAD}.
% ----------------------------- GLUE --------------------------



\subsection{Model Configurations}

The experimental models used in this paper include $\text{BERT}_{\text{BASE}}$, $\text{DistilBERT}_{\text{BASE}}$ and $\text{RoBERTa}_{\text{BASE}}$ architectures, each of which has its respective parameter configuration, as described in Table \ref{tab:BERT}. The notations $d$ and $d_{ffn}$ represent the dimensionality of matrices in MHA layers and FFN layers, respectively.

\subsection{Rows and Columns for each Weight Matrix in $\text{BERT}$} 

Our PGB method performs pruning on individual parameters of $W^{Q}$, $W^{K}$, $W^{V}$, $W^{O}$ in the MHA sub-layers, as well as $W^{(1)}$ and $W^{(2)}$ in the FFN sub-layers. The number of rows and columns for each weight matrix is as follows:
\begin{itemize}
    \item ($d$, $N_{H}\times \frac{d}{N_{H}}$) : $W^{Q}$, $W^{K}$, $W^{V}$, $W^{O}$ in MHA
    \item ($d$, $d_{ffn}$), ($d_{ffn}$,\; $d$) : $W^{(1)}$, $
    W^{(2)}$ in FFN
\end{itemize}

