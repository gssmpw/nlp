%-------------------------------------------------------------------------------------------
\begin{figure*}[t!]
    \centering
    {\includegraphics[width=\textwidth]{pruning.pdf}}
    \caption{The grouping procedure of PGB for weight matrices in BERT, which applies to both MHA and FFN layers}
    \label{fig:group}
\end{figure*}

%----------------------------------------------------------------------------------------------

\section{Method} \label{sec:method}
This section presents our one-shot semi-structured pruning method, called \textit{Permutation and Grouping for BERT} (PGB), which applies a grouping procedure to weight matrices of each structure of a given task-specific BERT, as illustrated in Figure \ref{fig:group}.

\subsection{Overall Process} \label{sec:overall}
Unlike existing grouped transformer architectures \cite{GroupFormer,groupbert} that are designed to be trained from scratch, our goal is to find the adaptive grouping for minimizing information loss in the original task-specific BERT. Therefore, as shown in Figure \ref{fig:group}, our PGB method performs the grouped pruning process for each individual weight matrix, rather than partitioning every part of the model into the fixed number of groups as in grouped transformers. In our pruned BERT architectures, the resulting number of groups for each layer can be different, and even a particular layer can be dropped as a whole when no important group is formed in the layer. After the pruning process, we apply the re-permutation step to every pruned weight matrix $\hat{W}$ to restore the original positions of the weights.

\paragraph{Problem formulation}
We first formulate our problem of grouped pruning on BERT. Consider a task-specific BERT $[\Theta_1, ..., \Theta_L]$ with $L$ layers, where $\Theta_i$ consists of weight matrices $W^{Q}, W^{K}, W^{V}, W^{O}$ in its MHA sub-layers, and $W^{(1)}, W^{(2)}$ in its FFN sub-layers. Given a target compression rate $\gamma$ and the task-specific dataset $\mathcal{D}$, our goal is to find the pruned architecture $[\widehat{\Theta}_{1}, ..., \widehat{\Theta}_{L}]$ such that:
\begin{equation}\label{eq:optim}
\begin{matrix}
\min & \sum_{i=1}^{L}  ||\F(\mathcal{D};\Theta_i) - \F(\mathcal{D};\widehat{\Theta}_{i})|| \\
\text{s.t.}& \sum_{i=1}^{L}\C(\widehat{\Theta}_{i}) \approx \gamma \cdot \sum_{i=1}^{L}\C({\Theta_i}),
\end{matrix}
\end{equation}
where $\F(\mathcal{D};\Theta)$ denotes the output of the model parameterized by $\Theta$ for the input data $\mathcal{D}$ and $\C(\cdot)$ is the number of parameters (or FLOPs). As mentioned above, each $\widehat{\Theta}_{i}$ can have different number of groups unlike the equally grouped transformer architectures \cite{GroupFormer,groupbert}.


\paragraph{PGB outline}
Algorithm \ref{alg:pgb} presents how our PGB method finds the group-based pruned architecture for a given task-specific BERT. The algorithm consists of two main phases, namely MHA pruning and FFN pruning. In accordance with previous studies \cite{block,Xia}, our approach focuses on pruning less weights in the MHA sub-layers, compared to those of the FFN sub-layers. Furthermore, since it is crucial for one-shot pruning to deal with a more challenging scenario of pruning a larger number of weights in either MHA or FFN sub-layers, we attempt to minimize the information loss in MHA sub-layers than in FFN sub-layers. To this end, we first take a more conservative approach for pruning weights in MHA sub-layers (Lines 2--5) by trying to find an optimal grouped architecture for each weight matrix $W$ based on its importance, which is performed by the \textsc{Group-Weight-Pruning} subroutine (refer to Algorithm \ref{alg:prune}). Then, we proceed a similar yet more aggressive pruning procedure for the FFN sub-layers (Lines 8--12), where multiple FFN layers that are least important can entirely be dropped in order to meet the remaining budget $C$. Therefore, instead of taking a sequential process across layers, we prioritize the more important layers over the less important ones within the remaining budget $C$.

\begin{algorithm}[t!]
\SetNoFillComment
\DontPrintSemicolon
	{
        \small
		\caption{\small \textsc{PGB-Compression}}\label{alg:pgb}

		\KwIn {$[\Theta_1, ..., \Theta_L]\triangleq$ a given model with $L$ layers, \\ 
                $\gamma\triangleq$ the target compression rate}
		\KwOut {$[\widehat{\Theta}_{1}, ..., \widehat{\Theta}_{L}]~\triangleq~$ the pruned model}

         
            $C \leftarrow \gamma \cdot \sum_{i=1}^{L}\C({\Theta_i})$ \;
            \tcc{MHA Pruning}
            \For {each layer $i \in [1, L]$}
    		{
                \For {each weight matrix $W$ in MHA of $\Theta_i$}
                {
                    $\widehat{W} \leftarrow$ \textsc{Group-Weight-Pruning}($W$) \;
                    $\widehat{\Theta}_{i}$.\textsc{Append}($\widehat{W}$) \;
                }
    		}
            $C \leftarrow C - \sum_{i=1}^{L}\C({\widehat{\Theta}_{i}})$ \;
            \tcc{FFN Pruning}
            \While{$C > 0$}
            {
                $j \leftarrow$ pick the unused layer with the largest importance score for FFN \;
                \For {each weight matrix $W$ in FFN of $\Theta_j$}
                {
                    $\widehat{W} \leftarrow$ \textsc{Group-Weight-Pruning}($W$) \;
                    $\widehat{\Theta}_{j}$.\textsc{Append}($\widehat{W}$) \;
                    $C \leftarrow C - \C({\widehat{W}})$\;
                }
    
        }
        \Return{$[\widehat{\Theta}_{1}, ..., \widehat{\Theta}_{L}]$}

 }
\end{algorithm} 


\subsection{Grouped Weight Pruning} 


In order to find the optimal grouping for each weight matrix of BERT, we adapt the technique of \textit{group convolution} pruning \cite{DGC,Zhao}, which is originally intended to prune filters in CNNs, not individual weights as in our problem setting.

The process of our grouped weight pruning is presented in Algorithm \ref{alg:prune}. For each weight matrix $W\in \mathbb{R}^{M\times N}$ in BERT, our method first adaptively determines the number $G$ of groups by measuring the degree of importance of $W$, and drop the entire matrix if the corresponding importance measure is not sufficiently high (Lines 1--3). Then, we permute the matrix to form groups of important weights into block diagonal matrices $g^{(i)}$'s (Line 6). Once the permuted matrix $\widetilde{W}$ is obtained, we extract only the top-left corner block of $\widetilde{W}$ (i.e., $\widetilde{W}[1:\frac{M}{G}, 1:\frac{N}{G}]$) to form a group matrix $g^{(i)}$ (Line 7), and discard all the weights in the region $\widetilde{W}[1:\frac{M}{G},~]$ and $\widetilde{W}[~, 1:\frac{N}{G}]$ from $\widetilde{W}$ (Line 8). The final $\widehat{W}$ would be represented as follows:

    \begin{equation}\label{eq:grouped}
    \begin{aligned}
        \widehat{W} &=\begin{vmatrix}
        g^{(1)}& 0 & \cdots & 0 \\
        0 & g^{(2)} & \cdots  & 0 \\
        \vdots & \vdots  & \ddots  & \vdots  \\
        0 & 0 & \cdots  & g^{(G)} \\
        \end{vmatrix}.
    \end{aligned}
    \end{equation}

\begin{algorithm}[t!]
\DontPrintSemicolon
	{
 
        \small
		\caption{\small \textsc{Grouped-Weight-Pruning}}\label{alg:prune}
     
		\KwIn {$W\triangleq$ a weight matrix of $M \times N$}
		\KwOut {$\widehat{W}~\triangleq~$ the pruned weight matrix}
        $G \leftarrow$ \textsc{Determine-Group-Numbers}($W$)\;
        \If{$G = 0$}{\Return{null}\;}
  
		$\widetilde{W}\leftarrow W$; $~~\widehat{W}\leftarrow$ \textit{null}\;
        \For{each group $i \in [1, G]$}
		{
            
            
                $\widetilde{W}\leftarrow$ \textsc{Permutation}($\widetilde{W}$)\;
			    $g^{(i)} \leftarrow \widetilde{W}[1:\frac{M}{G}~,~~1:\frac{N}{G}]$\;
                $\widetilde{W} \leftarrow \widetilde{W}[\frac{M}{G}:~,~~\frac{N}{G}:~]$\;
                $\widehat{W}$.\textsc{Append-Diagonal-Block}($g^{(i)}$)\;
            
		}
        \Return{$\widehat{W}$}\;
    
 }
\end{algorithm}

\paragraph{Finding the optimal permutation}
The permutation procedure (Line 6 in Algorithm \ref{alg:prune}) returns the optimal arrangement $\widetilde{W}$ of individual weights within the given matrix $W \in \mathbb{R}^{M\times N}$. The objective is to cluster more important weights together, forming a group located at the top-left corner block of $\widetilde{W}$. To this end, we determine the optimal pair of permutation vectors $\pi_r$ and $\pi_c$ for the rows and columns of $W$, respectively, that are used to rearrange the weights of $W$, resulting in the formation of $\widetilde{W}$ as follows:
\begin{equation}\label{eq:wtilde}
\begin{matrix}
\max\limits_{\pi_{r},\pi_{c}}& \I(\widetilde{W}[1:\frac{M}{G}, 1:\frac{N}{G}]) \\
\text{s.t.} & \widetilde{W} = W_{\pi_r, \pi_c},
\end{matrix}
\end{equation}
where $\I(\cdot)$ is the total importance score of a given weight matrix and $W_{\pi_r, \pi_c}$ is the resulting matrix when permuting the rows and columns of $W$ using $\pi_r$ and $\pi_c$, respectively. We calculate the importance scores of weights using the second-order information \cite{Brain,second-order, WoodFisher}, which allows us to quantify relative significance of the weights. The following example shows such optimal permutations for a matrix $W \in \mathbb{R}^{4\times 4}$ and $G=2$, assuming that each number in the matrix indicates the importance score of the corresponding weight:
$$
    \begin{vmatrix}
   1 & 0 & 0 & 2 \\
   0 & 1 & 1 & 0 \\     
    2 & 0 & 0 & 1  \\
    0 & 1 & 1 & 0  \\
    \end{vmatrix}  \xrightarrow[\pi_{c}={[1,4,2,3]}]{\pi_{r}=[1,3,2,4]} 
    \begin{vmatrix}
   1 & 2 & 0 & 0 \\
   2 & 1 & 0 & 0 \\     
    0 & 0 & 1 & 1  \\
    0 & 0 & 1 & 1  \\
    \end{vmatrix}.
$$

\paragraph{Heuristic solution for permutation}
Since weight matrices in BERT are typically high-dimensional, we employ an efficient heuristic algorithm \cite{Zhao} that finds sub-optimal permutation vectors. This algorithm alternatively sorts rows or columns based on the summation of importance scores corresponding to the weights of either row vectors within the region $\widetilde{W}[1:\frac{M}{G},~]$ or column vectors within the region $\widetilde{W}[~, 1:\frac{N}{G}]$. Since each sorting process for the rows or the columns changes the arrangement of the corresponding columns or rows, respectively, we repeat this pairwise sorting process a few times (e.g., 6 times in our experiments using BERT$_{\text{BASE}}$).


\paragraph{Adaptive group numbers}
The key property of our method is the adaptive determination of the number $G$ of groups (Line 1 in Algorithm \ref{alg:prune}), based on the importance of weights within $W \in \mathbb{R}^{M \times N}$. Basically, as the number of important weights in $W$ increases, we decrease $G$ and prune a smaller number of weights, whereas if $W$ contains fewer important weights, we increase $G$ to prune a greater number of weights. To this end, we devise the following strategy to adjust $G$ based on the count of weights whose important scores exceed a specified threshold $\tau$:
\begin{enumerate}
    \item Determine the count $n_{\tau}$ of weights in $W$ with importance scores higher than $\tau$.
    \item If $\frac{M\times N}{n_{\tau}} > G_{max}$, then prune the entire $W$.
    \item Otherwise, set $G$ to a value less than $\frac{M\times N}{n_{\tau}}$.
\end{enumerate}
The term $\frac{M\times N}{n_{\tau}}$ is derived from the fact that the number of parameters of a $G$-grouped matrix $\widehat{W}$ is equal to that of $W$ divided by $G$, i.e., $\frac{M\times N}{G}$. Therefore, to ideally cover all $n_{\tau}$ weights, $\widehat{W}$ should have at most $\frac{M\times N}{n_{\tau}}$ groups. Also, we introduce the hyperparameter $G_{max}$ to prevent the formation of excessive groups for non-critical weight matrices (e.g., $G_{max}=6$ in our experiments).

\subsection{Re-Permutation} \label{sec:repermute}

PGB finally performs the re-permutation procedure on every pruned weight matrix $\widehat{W}$ in all the layers of the model to identify the positions of the weights that correspond to the original model. This yields a re-permuted weight matrix $\widehat{W}^{*}$, wherein each weight is returned to its original positions. In this process,  we utilize the permutation vectors $\pi_{r}$ and $\pi_{c}$ that have been stored, and proceed the following operation: 
\begin{equation*}
\begin{aligned}
\widehat{W} \xrightarrow[argsort(\pi_{c})]{argsort(\pi_{r})} \widehat{W}^{*},
\end{aligned}
\end{equation*}
where $argsort(\pi)$ returns the corresponding re-permutation vectors that rearrange the shuffled weights back to their original positions. Note that the final $\widehat{W}^{*}$ after re-permutation is in the same form resulting from fine-grained unstructured pruning, but actual computation at inference time is efficiently performed only with each $g^{(i)} \subseteq \widehat{W}$ as in grouped transformer architectures \cite{GroupFormer,groupbert}.

\paragraph{Weight compensation}
To further restore the performance of the original task-specific BERT model, we update each unpruned weight in every $\widehat{W}^{*}$ by minimizing the following reconstruction error:
\begin{equation}\label{eq:reconstruction}
     \min \left\|\F(X;\widehat{W}^{*})-\F(X;W)\right\|_{2}^{2},
\end{equation}
where $\F(X; W)$ denotes the outputs for a sample dataset $X$.

%retraining
\paragraph{Re-Finetuning}
After all these steps, we perform re-finetuning in the same way as in the original BERT \cite{BERT} to recover the performance that is lost due to the pruning process.

%Experiment result
\begin{table*}
\adjustbox{width= \textwidth}{
\begin{tabular}{clcccccccccccc}

\noalign{\smallskip}\noalign{\smallskip}\toprule
{Pruning}& \multirow{2}{*}{Method}& \multirow{2}{*}{\# Param}  & QNLI & QQP & SST-2 & CoLA & STS-B & MRPC & RTE & $\text{SQuAD}_{1.1}$ & $\text{SQuAD}_{2.0}$ \\ %\cline{3-11}
Ratio&   &     & Acc. & Acc. & Acc. & Mcc. & Spearman & Acc. & Acc.&  EM/F1 & EM/F1   \\
\hline \hline  
0\% &$\text{BERT}_{\text{BASE}}$ & 85M & 91.4 & 91.5 & 93.2 & 58.9 & 89.2 & 86.3 & 66.8 & 80.8/88.3 & 70.9/74.2 \\
\hline 
% BMP$_{50\%}$  & 42.8M & - & 89.4 & 90.3 & 90.7  & - & - & -  & - & -   \\
% BMP$_{88\%}$ & 10.9M & - & 83.2 & 88.9 & 89.3 & - &  - & - & - & -  \\
\multirow{4}{*} {50\%}&EBERT  & 42M  & 89.9 & 90.6 & 90.8 & N.A. & 87.1  &  72.8 & 52.7 & 76.7/85.2 & 68.6/72.5 \\
                    &DynaBERT  & 42.8M  & 88.3 & 90.7 & 91.6& 51.2 & 86.4 & 77.8  & 63.5 & - &  - \\
                    &CoFi & 42.3M  & 88.8 & 90.6 & 90.1 & 53.6 & 88.0 & 83.5 & 56.7 & - & - \\
                    &\textbf {PGB (Ours)} & 42.5M & \textbf{90.3}& \textbf{91.1} & \textbf{92.3} & \textbf{54.9} & \textbf{88.8} & \textbf{84.3}  & \textbf{64.6}  & \textbf{78.0}/\textbf{86.8} & \textbf{69.6}/\textbf{73.5}\\
\hline
\multirow{4}{*} {88\%} &EBERT & 10.9M  & 81.8 & 88.1 & 87.5  & N.A. &  84.9 & N.A. & 49.5 & N.A. & N.A. \\
                    &DynaBERT & 10.7M  & 83.5 & 86.8 & 88.5 & 18.7 &  82.9 & 72.6 & 53.1 & - & -  \\
                    &CoFi & 10.4M  & 84.7 & 89.8 & 89.0 & 32.1 & 85.1 & 75.3 & 52.4 & - & - \\
                    &\textbf{PGB (Ours)} & 10.2M & \textbf{86.4} & \textbf{90.1} & \textbf{89.6}& \textbf{39.5} & \textbf{85.3} & \textbf{78.2} & \textbf{54.3}  & \textbf{71.5}/\textbf{81.2} & \textbf{65.9}/\textbf{69.7} \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison with structured pruning methods using 50\% and 88\% pruning rates on $\text{BERT}_{\text{BASE}}$, where N.A. indicates that the respective method do not achieve the specified level of sparsity.}
\label{tab:comACC}
\end{table*}

%-------------------------------------------------------------------------------------------------
\begin{figure*}[t!]
    \centering
    {\includegraphics[width= \textwidth]{param_acc.pdf}}
    %\subfigure[\label{fig:comacc:b} Accuracy v.s. FLOPs]{\includegraphics[width=1\textwidth,height=4cm]{EMNLP 2022/graph_FLOPs.pdf}}
    \caption{Performance comparison with structured pruning methods varying the reduced FLOPs ratio on $\text{BERT}_{\text{BASE}}$.}
    \label{fig:comacc}
\end{figure*}


