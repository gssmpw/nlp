\section{Background and Related Works}
\label{sec:related}
Pretrained language models ____ are mostly based on the transformer architecture ____ due to their effectiveness in various NLP tasks. This section first formally describes the typical transformer architecture and then discusses representative techniques for compressing large language models, namely distillation and pruning.


\subsection{Transformer Architecture}
% MHA
The typical transformer architecture consists of encoder and decoder layers, each of which commonly contains two main components: multi-head attention (MHA) and feed-forward network (FFN). In an MHA layer, there are $N_H$ self-attention heads, and each head $h \in [1, N_H]$ involves the following weight matrices, $W_{h}^{Q}, W_{h}^{K}, W_{h}^{V}, W_{h}^{O} \in \mathbb{R}^{{d} \times\frac{d}{N_H}}$. Then, the final output of the MHA layer is computed as follows:
$$
MHA(X)= \sum_{h = 1}^{N_{H}} Attn_{h}(X),
$$
where $Attn_{h}(X)$ is the output of the standard self-attention unit.

% FFN
The output of MHA is then fed into the corresponding FFN layer, which consists of weight matrices $W^{(1)}\in \mathbb{R}^{d \times d_{ffn}}$, $W^{(2)}\in \mathbb{R}^{d_{ffn} \times d}$, $b^{(1)}\in \mathbb{R}^{d_{ffn}}$ and $b^{(2)}\in \mathbb{R}^{d}$, where $d_{ffn}$ (usually $4\times d$) represents the dimension of the
intermediate hidden features. The output of the FFN layer can be computed as follows:
$$
FFN(A)=\sum_{i=1}^{d_{ffn}}GeLU(AW^{(1)}_{:,i}+b^{(1)})W^{(2)}_{i,:}+b^{(2)},
$$
where $A = MHA(X)$.

In transformer-based models, this same structure is repeatedly defined across multiples layers (e.g., 12 layers in $\text{BERT}_{\text{BASE}}$ ____) and each layer has another multiple heads (e.g., 12 heads in BERT$_{\text{BASE}}$ ____). Consequently, they have numerous trainable parameters, which motivates the NLP community to develop various compression methods for these models.


\subsection{Distillation}
Knowledge distillation (KD) ____ is a compression technique that trains a lightweight student model to mimic the soft output of a large teacher model, leading to competitive performance on downstream tasks. There are some studies where KD methods have been applied to transformer-based models. For example, DistilBERT ____ transfers knowledge to a student model of a fixed-size through a pre-training phase and an optional fine-tuning process. MiniLM ____ deeply describes the self-attention information of a task-agnostic teacher model by providing a detailed analysis of the self-attention matrices. Both TinyBERT ____ and MobileBERT ____ transfer knowledge during pretraining using a layer-by-layer strategy. TinyBERT ____ additionally performs distillation during fine-tuning. 

Although KD-based methods are shown to be effective at preserving high accuracy, training a student model can be time-consuming; as reported by CoFi ____, TinyBERT ____ requires 350 hours and CoFi ____ requires 20 hours for compression. Furthermore, 
it is not trivial to effectively distill the knowledge from a multi-layer teacher model with self-attention information to a student model with fewer layers, which involves the problem of layer selection and different loss functions.


\subsection{Pruning}
Pruning ____ is another popular compression scheme that removes unimportant weights or structures from the target neural network. Following a massive volume of pruning techniques on convolutional neural networks (CNNs), pruning for transformer family has also been studied, falling into either unstructured or structured pruning.

%Unstructured pruning
\paragraph{Unstructured pruning} 
In unstructured pruning, we remove individual weights that are not important, often aiming to reduce the memory storage itself for the target model while maintaining performance, such as the methods based on \textit{lottery ticket hypothesis} ____ and \textit{movement pruning} ____. However, in these unstructured pruning methods, it is difficult and complicated to make satisfactory speedup at inference time, often requiring a special hardware.

%Structured pruning
\paragraph{Structured pruning} 
Structured pruning is a simpler and more efficient way to reduce the model size, where we eliminate some structures of parameters that are considered less significant. In the case of transformer architectures, we can remove redundant attention heads ____, entire layers of either MHA or FFN ____, or neurons along with all their connecting weights of FFNs ____. However, such a coarse-grained pruning scheme inevitably suffers from severe drop in accuracy. Consequently, recent studies ____ have explored the combination of pruning with KD to further enhance the performance of compressed networks. Despite higher performance of these combined approaches, they sacrifice efficiency and simplicity in the compression process itself, as KD typically takes lengthy training times and involves complicated matching procedures.

%Semi-structured pruning
\paragraph{Semi-structured pruning} This paper proposes a semi-structured pruning method for BERT in order to achieve a good balance between efficiency and accuracy. To our best knowledge, \textit{block movement pruning} (BMP) ____ is the only one that can be categorized into semi-structured pruning for transformer family, which proposes a block-based pruning approach for each weight matrix of MHA and FFN layers. 

%Group-based pruning 
\paragraph{Group-based pruning} 
Our method is inspired by a grouping strategy, which has been frequently employed to reduce the computational complexity of CNNs. In this approach, a set of filters or channels are grouped together, and the objective is to minimize connectivity and computation between these groups ____. These grouped architectures are also incorporated into transformer-based models by a few recent works ____. However, they focus on designing more efficient architectures to be trained from scratch, not for compressing a trained task-specific model. This work is the first study on group-based pruning on BERT, aiming to compress the task-specific BERT while maintaining its original accuracy.