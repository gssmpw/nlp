%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url}
%% The amsmath package provides various useful equation environments.
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow, multicol}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{pifont}
%% The amsthm package provides extended theorem environments


\newcommand{\eat}[1]{}
\newcommand{\TODO}[1]{{\color{red}\sc ToDo: #1}}
\newcommand{\dchoi}[1]{{\color{cyan}\sc Choi:#1}}
\newcommand{\hlim}[1]{{\color{green}\sc Lim:#1}}
\newcommand{\smalltitle}[1]{\vspace{1mm}{\noindent\textbf{#1.}\hspace{1mm}}}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}

\def\I{\mathcal{I}}
\def\D{\mathcal{D}}
\def\L{\mathcal{L}}
\def\C{\mathcal{C}}
\def\F{\mathcal{F}}
\def\ours{{PGB\xspace}}


\journal{}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
\title{PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation}
% \tnotetext[label1]{}
\author[1]{Hyemin Lim}
\ead{hyemin8670@naver.com}

\author[1]{Jaeyeon Lee}
\ead{dlwodus159@naver.com}

\author[1]{Dong-Wan Choi\corref{cor1}}
\ead{dchoi@inha.ac.kr}

% \ead[url]{home page}
% \fntext[label2]{}
\cortext[cor1]{Corresponding author}
\affiliation[1]{
            organization={Department of Computer Science and Engineering, Inha University},
            addressline={100 Inharo},
            city={Incheon},
            % postcode={22212},
            % state={},
            country={South Korea}}
% \fntext[label3]{}

        

%%Research highlights
% \begin{highlights}
% \item 
% \end{highlights}

\input{abstract}
%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
BERT \sep One-shot pruning \sep Semi-structured pruning \sep Pretrained language models \sep Task-specific pruning

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}


%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
% \linenumbers

%% main text
%%

\input{section1}
\input{section2}
\input{section3}
\input{section4}
\input{section5}
\input{section6}

%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Mathematics
%% https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics

%% Use a table environment to create tables.
%% Refer following link for more details.
%% https://en.wikibooks.org/wiki/LaTeX/Tables

%% Refer following link for more details about bibliography and citations.
%% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management
% \bibliography{reference}

    
\begin{thebibliography}{00}
%% For authoryear reference style
%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

\bibitem{Vaswani2017}
  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin,
  \textit{Attention is All you Need}, Advances in Neural Information Processing Systems, 2017, pp. 5998–6008. 

\bibitem{BERT}
  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,
  \textit{{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, 2019, pp. 4171-–4186.

\bibitem{Roberta}
  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov,
  \textit{RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach}, CoRR, abs/1907.11692, 2019.
  
\bibitem{GPT}
  Tom B. Brown, Benjamin Mann,
               Nick Ryder,
               Melanie Subbiah,
               Jared Kaplan,
               Prafulla Dhariwal,
               Arvind Neelakantan,
               Pranav Shyam,
               Girish Sastry,
               Amanda Askell,
               Sandhini Agarwal,
               Ariel Herbert{-}Voss,
               Gretchen Krueger,
               Tom Henighan,
               Rewon Child,
               Aditya Ramesh,
               Daniel M. Ziegler,
               Jeffrey Wu,
               Clemens Winter,
               Christopher Hesse,
               Mark Chen,
               Eric Sigler,
               Mateusz Litwin,
               Scott Gray,
               Benjamin Chess,
               Jack Clark,
               Christopher Berner,
               Sam McCandlish,
               Alec Radford,
               Ilya Sutskever and
               Dario Amodei,
  \textit{Language Models are Few-Shot Learners},
  Advances in Neural Information Processing Systems, 2020, pp. 1877--1901.
  
\bibitem{Han}
  Song Han, Jeff Pool, Jeff Pool, John Tran, and William J. Dally,
  \textit{Learning both Weights and Connections for Efficient Neural Network},
  Advances in Neural Information Processing Systems, 2015, pp. 1135--1143. 
  
\bibitem {KD}
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.
  \textit{Learning both Weights and Connections for Efficient Neural Network},
  Advances in Neural Information Processing Systems , CoRR, abs/1503.02531, 2015.

\bibitem {SNIP}
  Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr,
  \textit{Snip: Single-shot network pruning based on connection sensitivity},
  arXiv preprint arXiv:1810.02340, 2018. 

\bibitem {Thinet}
  Jian-Hao Luo, Jianxin Wu, and Weiyao Lin,
  \textit{Thinet: A filter level pruning method for deep neural network compression},
  In Proceedings of the IEEE international conference on computer vision, 2017, pp. 5058--5066. 

\bibitem {LoB}
  Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin,
  \textit{The lottery ticket hypothesis for pretrained bert networks},
  Advances in neural information processing systems, 2020, 33:pp. 15834-15846.

\bibitem {DynaBERT}
  Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Li,
  \textit{Dynabert: Dynamic BERT with adaptive width and depth},
  In Advances in neural information processing systems, 2020, 33.

\bibitem {block}
  Fran{\c{c}}ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush,
  \textit{Block Pruning For Faster Transformers},
  In Empirical Methods in Natural Language Processing (EMNLP), 2021, pp. 10619--10629.

\bibitem {Xia}
  Mengzhou Xia, Zexuan Zhong, and Danqi Chen,
  \textit{Structured Pruning Learns Compact and Accurate Models},
  In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 1513--1528.

\bibitem {DistilB}
  Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf,
  \textit{Structured Pruning Learns Compact and Accurate Models}, CoRR, abs/1910.01108, 2019.

\bibitem {TinyBERT}
  Xiaoqi Jiao , Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang and Qun Liu,
  \textit{Structured Pruning Learns Compact and Accurate Models}, CoRR, abs/1910.01108, 2019.

\bibitem {DGC}
  Zhuo Su, Linpu Fang, Wenxiong Kang, Dewen Hu Matti Pietikäinen, and Li Liu,
  \textit{Dynamic group convolution for accelerating convolutional neural networks}, In Computer Vision–ECCV 2020: 16th European Conference, 2020, pp. 138--155.

\bibitem {Zhao}
  Ruizhe Zhao and Wayne Luk,
  \textit{Efficient structured pruning and architecture searching for group convolution}, In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 1961--1970.

\bibitem {GroupFormer}
Sungrae Park, Geewook Kim, Junyeop Lee, Junbum Cha, Ji-Hoon Kim, and Hwalsuk Lee,
\textit{Scale down Transformer by Grouping Features for a Lightweight Character-level Language Model}, Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 6883–-6893.

\bibitem {groupbert}
Ivan Chelombiev, Daniel Justus, Douglas Orr, Anastasia Dietrich, Frithjof Gressmann, Alexandros Koliousis, and Carlo Luschi,
\textit{Groupbert: Enhanced transformer architecture with efficient grouped structured}, arXiv preprint arXiv:2106.05822, 2021.

\bibitem {GLUE}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman,
\textit{{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding}, In International Conference on Learning Representations (ICLR), 2019.
               
\bibitem{SQuAD}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang,
\textit{SQuAD: 100, 000+ Questions for Machine Comprehension of Text}, In Empirical Methods in Natural Language Processing (EMNLP), 2016.


\bibitem {MiniLM}
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou,
\textit{MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
               of Pre-Trained Transformers}, In Advances in Neural Information Processing Systems, 2020, pp. 5776--5788.

\bibitem {MoB}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou,
\textit{MobileBERT: a Compact Task-Agnostic {BERT} for Resource-Limited Devices}, In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), 2020, pp. 2158--2170.

\bibitem {Mov}
Victor Sanh, Thomas Wolf, and Alexander Rush,
\textit{Movement pruning: Adaptive sparsity by fine-tuning}, Advances in Neural Information Processing Systems, 2020, 33: pp.20378--20389.

\bibitem {sixteen}
Paul Michel, Omer Levy, and Graham Neubig,
\textit{Are sixteen heads really better than one?}, Advances in Neural Information Processing Systems (Volume 32), 2019.

\bibitem {voita}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov,
\textit{Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned}, arXiv preprint arXiv:1905.09418, 2019.

\bibitem {SMP}
Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Qun Liu, and Maosong Sun,
\textit{Know what you don't need: Single-Shot Meta-Pruning for attention heads}, {AI} Open, 2021, 2: pp. 36--42.

\bibitem {layer1}
Angela Fan, Edouard Grave, and Armand Joulin,
\textit{Reducing Transformer Depth on Demand with Structured Dropout}, CoRR, abs/1909.11556, 2019.

\bibitem {layer2}
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov,
\textit{Poor Man's {BERT:} Smaller and Faster Transformer Models}, CoRR, abs/2004.03844, 2020.

\bibitem {earlybert}
Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu,
\textit{Earlybert: Efficient bert training via early-bird lottery tickets}, arXiv preprint arXiv:2101.00063, 2020.

\bibitem {deeproot}
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi,
\textit{Deep roots: Improving cnn efficiency with hierarchical filter groups}, Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1231--1240.

\bibitem {xie}
Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang, Lai, Richang Hong, and Guo-Jun Qi,
\textit{Interleaved structured sparse convolutional neural networks}, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8847--8856.

\bibitem {Brain}
Babak Hassibi, David G. Stork, and Gregory J. Wolff,
\textit{Interleaved structured sparse convolutional neural networks}, In Proceedings of International Conference on Neural Networks (ICNN'88), San Francisco, CA, USA, March 28 - April 1, 1993, pp. 293--299.

\bibitem {second-order}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz ,
\textit{Importance Estimation for Neural Network Pruning}, In {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2019, Long Beach, CA, USA, June 16-20,2019, pp. 11264--11272.

\bibitem {WoodFisher}
Sidak Pal Singh and Dan Alistarh,
\textit{WoodFisher: Efficient Second-Order Approximation for Neural Network
                  Compression}, In Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020.

\bibitem {SQuAD2}
Rajpurkar, Pranav and Jia, Robin and Liang, Percy,
\textit{Know what you don't know: Unanswerable questions for SQuAD}, arXiv preprint arXiv:1806.03822, 2018.

\bibitem {Pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K{\"{o}}pf, Edward Z. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,  Benoit Steiner, Lu Fang, Junjie Bai, and
                  Soumith Chintala,
\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library}, CoRR, abs/1912.01703, 2019.

\bibitem {Wolf}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien, Chaumond, Clement Delangue, Anthony Moi, Pierric  Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
\textit{Transformers: State-of-the-art natural language processing}, In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 38--45.

\bibitem {ebert}
Liu, Zejian and Li, Fanrong and Li, Gang and Cheng, Jian,
\textit{EBERT: Efficient BERT inference with dynamic structured pruning}, Findings of the Association for Computational Linguistics: ACL-IJCNLP, 2021, pp. 4814--4823.

\bibitem {TFsnip}
Zi Lin, Jeremiah Z. Liu, Zi Yang, Nan Hua, and Dan Roth,
\textit{Pruning Redundant Mappings in Transformer Models via Spectral-Normalized
                  Identity Prior}, Findings of the Association for Computational Linguistics: {EMNLP}
                  2020, Online Event, 16-20 November, 2020, pp. 719--730.

\bibitem {shufflenet}
Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian,
\textit{Shufflenet: An extremely efficient convolutional neural network for mobile devices}, In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6848--6856.

\bibitem {platon}
Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, and Tuo Zhao,
\textit{Platon: Pruning large transformer models with upper confidence bound of weight importance}, In Proceedings of the IEEE conference International Conference on Machine Learning, 2022, pp. 26809--26823.

\bibitem {SST-2}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts,
\textit{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}, In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013, pp. 1631--1642.

\bibitem {CoLA}
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman,
\textit{Neural Network Acceptability Judgments}, Transactions of the Association of Computational
Linguistics (TACL), 2019, 7: pp. 625--641.

\bibitem {STS-B}
Daniel M. Cer, Mona T. Diab, Eneko Agirre, I{\~{n}}igo Lopez{-}Gazpio, and Lucia Specia,
\textit{SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and
               Crosslingual Focused Evaluation}, In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017, 7: pp.1--14.

\bibitem {MRPC}
William B. Dolan and Chris Brockett, I{\~{n}}igo Lopez{-}Gazpio, and Lucia Specia,
\textit{Automatically Constructing a Corpus of Sentential Paraphrases}, In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, 2005.            
             

\end{thebibliography}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix
\input{appendix}



%% If you have bib database file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.


\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.


