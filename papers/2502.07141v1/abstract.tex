%This paper studies the stochastic gradient bandit algorithm, and proves a surprising result that using an arbitrary constant learning rate $\eta \in O(1)$, stochastic gradient achieves asymptotic convergence toward globally optimal policy almost surely. The use of arbitrarily large constant learning rates indicates that stochastic gradient bandit algorithm keeps working under situations far beyond smoothness and noise control in a satisfactory way without lack of exploration, since it is capable of handling the exploration-exploitation trade-off automatically and simultaneously. Comparing with existing literature, our proofs are based on novel findings and arguments on sample time and progress-noise relations, refreshing our understandings on how sample stochastic gradient methods behave in bandit settings.

We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely
using \emph{any} constant learning rate.
This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation
appropriately even in scenarios where standard smoothness and noise control
assumptions break down.
The proofs are based on novel findings about action sampling rates and the
relationship between cumulative progress and noise,
and extend the current understanding of how simple stochastic gradient methods
behave in bandit settings.

%\textcolor{red}{Authors (for ITP): Jincheng, Bo, Alekh, Sharan, Anant, Csaba, Dale}
