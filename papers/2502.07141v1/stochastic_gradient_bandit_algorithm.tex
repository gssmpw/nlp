\section{Setting and Background}

We consider the stochastic multi-armed bandit problem \citep{lattimore2020bandit}, specified by $K$ actions and a true mean reward vector $r \in \sR^K$, where for each action $a \in [K] \coloneqq \{1, 2, \dots, K \}$,
\begin{align}
\label{eq:true_mean_reward_expectation_bounded_sampled_reward}
    r(a) = \int_{-R_{\max}}^{R_{\max}}{ x \cdot P_a(x) \mu(d x)},
\end{align}
where $R_{\max} > 0$ is the reward range, $\mu$ is a finite measure over $[-R_{\max}, R_{\max}]$, and $P_a(x) \ge 0$ is the probability density function with respect to $\mu$.
We use $R_a$ to denote the reward distribution for action $a$ defined by the density $P_a$ and base measure $\mu$. The goal is to find a policy $\pi_{\theta} \in [0, 1]^K$ to achieve high expected reward,
\begin{align}
\label{eq:expected_reward}
    \max_{\theta \in \sR^K}{ \pi_{\theta}^\top r},
\end{align}
where $\pi_\theta$ is parameterized by $\theta \in \sR^K$. 

\textbf{The gradient bandit algorithm.} A natural idea to optimize \cref{eq:expected_reward} is to use stochastic gradient ascent, which is shown in \cref{alg:gradient_bandit_algorithm_sampled_reward} and known as the gradient bandit algorithm \citep[Section 2.8]{sutton2018reinforcement}. In \cref{alg:gradient_bandit_algorithm_sampled_reward}, in each iteration $t \ge 1$, the probability of pulling arm $a \in [K]$ is given as 
%$\probability{\left( a_t = a \right)} = \pi_{\theta_t}(a)$ such that $\pi_{\theta_t} = \softmax(\theta_t)$, where
\begin{align}
\label{eq:softmax}
    \pi_{\theta_t}(a) = [ \softmax(\theta_t) ](a) \coloneqq
    \frac{ \exp\{ \theta_t(a) \} }{ \sum_{a^\prime \in [K]}{ \exp\{ \theta_t(a^\prime) } \} }, \mbox{ \quad   for all } a \in [K],
\end{align}
where $\theta_t \in \sR^{K}$ is the parameter vector to be updated. The following proposition shows that \cref{alg:gradient_bandit_algorithm_sampled_reward} is an instance of stochastic gradient ascent with an unbiased gradient estimator \citep{sutton2018reinforcement,mei2024stochastic}.

\begin{figure}[t]
\centering
\vskip -0.1in
\begin{minipage}{.7\linewidth}
    \begin{algorithm}[H]
    \caption{Gradient bandit algorithm (without baselines)}
    \label{alg:gradient_bandit_algorithm_sampled_reward}
    \begin{algorithmic}
    \STATE {\bfseries Input:} initial parameters $\theta_1 \in \sR^K$, learning rate $\eta > 0$.
    \STATE {\bfseries Output:} policies $\pi_{\theta_t} = \softmax(\theta_t)$.
    \WHILE{$t \ge 1$}
   \STATE Sample an action $a_t \sim \pi_{\theta_t}(\cdot)$ and observe reward $R_t(a_t)\sim P_{a_t}$.
   \FOR{all $a \in [K]$}
   \IF{$a = a_t$}
   \STATE $\theta_{t+1}(a) \gets \theta_t(a) + \eta \cdot \left( 1 - \pi_{\theta_t}(a) \right) \cdot R_t(a_t)$.
   \ELSE
   \STATE $\theta_{t+1}(a) \gets \theta_t(a) - \eta \cdot \pi_{\theta_t}(a) \cdot R_t(a_t)$.
   \ENDIF
   \ENDFOR
   \ENDWHILE
   \end{algorithmic}
    \end{algorithm}
\end{minipage}
\end{figure}

\begin{proposition}[Proposition 2.3 of \citep{mei2024stochastic}]
\label{prop:gradient_bandit_algorithm_equivalent_to_stochastic_gradient_ascent_sampled_reward}
\cref{alg:gradient_bandit_algorithm_sampled_reward} is equivalent to the following update,
\begin{align}
\label{eq:stochastic_gradient_ascent_sampled_reward}
    \theta_{t+1} &\gets  \theta_{t} + \eta \cdot \frac{d \ \pi_{\theta_t}^\top \hat{r}_t}{d \theta_t} = \theta_t + \eta \cdot \left(  \diagonalmatrix{(\pi_{\theta_t})} - \pi_{\theta_t} \pi_{\theta_t}^\top \right) \hat{r}_t,
\end{align}
where $\mathbb{E}_t{ \Big[ \frac{d \ \pi_{\theta_t}^\top \hat{r}_t }{d \theta_t} \Big] } = \frac{d \ \pi_{\theta_t}^\top r}{d \theta_t }$, and $\EEt{\cdot}$ is defined with respect to randomness from on-policy sampling $a_t \sim \pi_{\theta_t}(\cdot)$ and reward sampling $R_t(a_t)\sim P_{a_t}$. The Jacobian of $\theta \mapsto \pi_\theta \coloneqq \softmax(\theta)$ is $\left( \frac{d \ \pi_{\theta}}{d \theta} \right)^\top = \diagonalmatrix{(\pi_{\theta})} - \pi_{\theta} \pi_{\theta}^\top \in \sR^{K \times K}$, and 
$\hat{r}_t(a) \coloneqq \frac{ \sI\left\{ a_t = a \right\} }{ \pi_{\theta_t}(a) } \cdot R_t(a)$ for all $a \in [K]$ is the importance sampling (IS) estimator, and we set $R_t(a)=0$ for all $a \not= a_t$. 
\end{proposition}
\textbf{Known results on the convergence of the gradient bandit algorithm.}
Since \cref{eq:expected_reward} corresponds to a smooth non-concave maximization problem over $\theta \in \sR^K$ \citep{mei2020global}, using \cref{alg:gradient_bandit_algorithm_sampled_reward} with decaying learning rates is sufficient to guarantee convergence to a stationary point \citep{robbins1951stochastic,nemirovski2009robust,ghadimi2013stochastic,zhang2020global}. However, this is insufficient to ensure the  globally optimal solution of \cref{eq:expected_reward} is reached, since there exist multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have been developed for PG methods in the true gradient setting \citep{agarwal2021theory,mei2020global}, where the algorithm has access to exact mean rewards. These results were later extended to achieve global convergence guarantees (almost surely) in the stochastic setting \citep{zhang2020sample,ding2021beyond,zhang2021convergence,yuan2022general,mei2024stochastic,lu2024towards}. However, these extended results have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and combating the inherent noise in stochastic gradients.

Despite these previous assumptions, there exists empirical and theoretical evidence that using a large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it has been observed that softmax policies learn even with extremely large learning rates such as $2^{14}$ \citep{garg2021alternate}. For logistic regression on linearly separable data, the objective has an exponential tail and the minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning rate $\eta \in \Theta(t)$ achieves accelerated $O(1/t^2)$ convergence \citep{wu2024large}. Though the objective in \cref{eq:expected_reward} has similar properties, unlike logistic regression, the problem we are considering is non-concave, so the same techniques cannot be directly applied. % and hence similar techniques are not applicable here. 
The most related results are from \citep{mei2024stochastic}, which proved that with a small problem specific constant learning rate, \cref{alg:gradient_bandit_algorithm_sampled_reward} achieves convergence to a globally optimal policy almost surely. However, as mentioned, the learning rate choices in \citep{mei2024stochastic} rely on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3, and 4.6), which cannot be directly applied here for a large learning rate.

Consequently, the use of large learning rates appear to render existing results and techniques inapplicable. Furthermore, with a large constant learning rate, it is unclear whether \cref{alg:gradient_bandit_algorithm_sampled_reward} will converge to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to the optimal arm in such cases. Resolving these questions requires new results that characterize the behavior of \cref{alg:gradient_bandit_algorithm_sampled_reward}, since the classical optimization and stochastic approximation convergence theories are no longer applicable, as explained. % (smoothness and noise control) are no longer applicable as explained.

%It has been recently discovered that with small step-sizes, exploration is handled automatically and the algorithm converges to the optimal arm without any explicit exploration \citep{mei2024stochastic}.


