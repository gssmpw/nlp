\section{Asymptotic Global Convergence of Gradient Bandit Algorithm}

We have seen that solving the non-concave maximization problem \cref{eq:expected_reward} using \cref{alg:gradient_bandit_algorithm_sampled_reward} with any constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here, we take a different perspective to investigate how \cref{alg:gradient_bandit_algorithm_sampled_reward} samples actions. For analysis, we make the following assumption about the reward distribution.
\begin{assumption}[True mean reward has no ties]
\label{assp:reward_no_ties}
For all $i, j \in [K]$, if $i \not= j$, then $r(i) \not= r(j)$.
\end{assumption}
\begin{remark}
Removing \cref{assp:reward_no_ties} remains an open question for future work, while we believe that \cref{alg:gradient_bandit_algorithm_sampled_reward} works without \cref{assp:reward_no_ties}. One piece of evidence to support this conjecture is that even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict one-hot policies has zero measure.
\end{remark}

\subsection{Failure Mode of Aggressive Updates}
\label{subsec:failure_model_aggressive_updates}

It has been observed that several accelerated PG methods in the true gradient setting, including natural PG \citep{kakade2002natural,agarwal2021theory} and normalized PG \citep{mei2021leveraging}, obtain worse results than standard softmax PG if combined with online sampling $a_t \sim \pi_{\theta_t}(\cdot)$ using constant learning rates \citep{mei2021understanding}. The failure mode in these cases is that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists a potentially sub-optimal action $a \in [K]$, such that with some constant probability, $a_t = a$ for all $t \ge 1$. Such an outcome implies that $\pi_{\theta_t}(a) \to 1$ as $t \to \infty$ \citep[Theorem 3]{mei2021understanding}. Since $a \in [K]$ could be a sub-optimal action with $r(a) < r(a^*) = \max_{a \in [K]} r(a)$, this results in a lack of exploration, and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to the optimal action $a^* \coloneqq \argmax_{a \in [K]}{ r(a) }$ with probability $1$.

\subsection{Stochastic Gradient Automatically Avoids Lack of Exploration}
\label{subsec:avoid_lack_of_exploration}

Our first key finding is that \cref{alg:gradient_bandit_algorithm_sampled_reward} does not keep sampling one action forever, no matter how large the constant learning rate is.  This property avoids the problem of a lack of exploration, in the sense that \cref{alg:gradient_bandit_algorithm_sampled_reward} will at least explore more than one action infinitely often. At first glance, this might not seem like a strong property, since the algorithm might somehow explore only sub-optimal actions forever.  %one might wonder what if the algorithm just explores over sub-optimal actions? 
However, we will argue below that this property coupled with additional arguments is sufficient to guarantee convergence to the globally optimal policy.

Let us now formally prove the above property. By \cref{alg:gradient_bandit_algorithm_sampled_reward}, for all $a \in [K]$, for all $t \ge 1$,
\begin{empheq}[left={\theta_{t+1}(a) \gets \theta_t(a) +\empheqbiglbrace~}]{align}
\label{eq:first_update}
    \eta \cdot \left( 1 - \pi_{\theta_t}(a) \right) \cdot R_t(a), & \quad \text{ if} \ a_t = a, \\ 
\label{eq:second_update}
    - \eta \cdot \pi_{\theta_t}(a) \cdot R_t(a_t), & \quad \text{ otherwise}.
\end{empheq}
We define $N_t(a)$ as the number of times action $a \in [K]$ is sampled up to iteration $t \ge 1$, i.e.,
\begin{align}
\label{eq:finite_sample_count}
    N_t(a) \coloneqq \sum_{s=1}^{t}{ \sI\left\{ a_s = a \right\} },
\end{align}
and its asymptotic limit $N_\infty(a) \coloneqq \lim_{t \to \infty}{ N_t(a) }$, which could possibly be infinity. For all $a \in [K]$, we have either $N_\infty(a) = \infty$ or $N_\infty(a) < \infty$, meaning that $a \in [K]$ is sampled infinitely often or only finitely many times asymptotically. First, we prove the following \cref{lem:finite_sample_time_implies_finite_parameter}, which shows that if an action $a \in [K]$ is sampled only finitely many times as $t \to \infty$, then the parameter corresponding to action $a$ is also finite, i.e., $\sup_{t \ge 1}{| \theta_t(a) |} < \infty$.
\begin{lemma}
\label{lem:finite_sample_time_implies_finite_parameter}
Using \cref{alg:gradient_bandit_algorithm_sampled_reward} with any constant $\eta \in \Theta(1)$, if $N_\infty(a) < \infty$ for an action $a \in [K]$, then we have, almost surely,
\begin{align}
\label{lem:finite_sample_time_implies_finite_parameter_claim}
    \sup_{t \ge 1}{ \theta_t(a) } < \infty, \text{ and } \inf_{t \ge 1}{ \theta_t(a) } > -\infty.
\end{align}
\end{lemma}
\cref{lem:finite_sample_time_implies_finite_parameter} will be used multiple times in the subsequent convergence arguments.

\paragraph{Proof sketch.} Since we assume action $a \in [K]$ is sampled finitely many times, the update given in the case depicted by~\cref{eq:first_update} happens finitely many times. Each update is bounded since the sampled reward is in $[-R_{\max}, R_{\max}]$ by \cref{eq:true_mean_reward_expectation_bounded_sampled_reward}, and the learning rate is a constant, i.e., $\eta \in \Theta(1)$. In \cref{alg:gradient_bandit_algorithm_sampled_reward}, $\theta_t(a)$ is still updated even when $a_t \ne a$, with the corresponding update given by the case depicted by \cref{eq:second_update}. Therefore, whether $\theta_t(a)$ is bounded depends on the cumulative probability $\sum_{s=1}^{t}{ \pi_{\theta_s}(a) }$ being summable as $t \to \infty$. According to the extended Borel-Cantelli lemma (\cref{lem:ebc}), we have, almost surely,
\begin{align}
\label{eq:ebc_result}
    \Big\{ \sum_{t \ge 1} \pi_{\theta_t}(a) =\infty \Big\} = \left\{ N_\infty(a )=\infty \right\},
\end{align}
which implies (by taking complements) that $\sum_{t \ge 1} \pi_{\theta_t}(a) < \infty$ if and only if $N_\infty(a) < \infty$. Therefore, if $a \in [K]$ is sampled finitely often, $\theta_t(a)$ will be updated in a bounded manner (using \cref{eq:first_update,eq:second_update}) as $t \to \infty$, hence establishing \cref{lem:finite_sample_time_implies_finite_parameter}. Detailed proofs for this lemma, as well as for all other results in this paper can be found in the appendix. % due to space constraints.

Given \cref{lem:finite_sample_time_implies_finite_parameter}, we can then establish 
%are ready to present  \cref{lem:at_least_two_actions_infinite_sample_time}, which formally states 
the above-mentioned finding about the exploration effect of~\cref{alg:gradient_bandit_algorithm_sampled_reward} in \cref{lem:at_least_two_actions_infinite_sample_time}.
\begin{lemma}[Avoiding a lack of exploration]
\label{lem:at_least_two_actions_infinite_sample_time}
Using \cref{alg:gradient_bandit_algorithm_sampled_reward} with any $\eta \in \Theta(1)$, there exists at least a pair of distinct actions $i, j \in [K]$ and $i \ne j$, such that, almost surely,
\begin{align}
\label{lem:at_least_two_actions_infinite_sample_time_claim}
    N_\infty(i) = \infty, \text{ and } N_\infty(j) = \infty.
\end{align}
\end{lemma}
\paragraph{Proof sketch.} 
The argument for the existence of one such action is straightforward, since by the pigeonhole principle, if there are finitely many actions, i.e., $K < \infty$, there must be at least one action $i \in [K]$ that is sampled infinitely often as $t \to \infty$.

The argument for the existence of a second such action is by contradiction. Suppose that all the other actions $j \in [K]$ with $j \ne i$ are sampled only finitely many times as $t \to \infty$. According to \cref{lem:finite_sample_time_implies_finite_parameter}, their corresponding parameters must remain finite, i.e., $\sup_{t \ge 1}{ |\theta_t(j)| } < \infty$ for all $j \in [K]$ with $j \ne i$. Now consider  $\theta_t(i)$. By assumption, the second update case for this parameter, \cref{eq:second_update}, happens only finitely often, since \cref{eq:second_update} can only occur when $a_t \ne i$. Therefore, the key question is whether the cumulative probability $\sum_{s=1}^{t} \left( 1 - \pi_{\theta_s}(i) \right)$ involved in the first case of the update,~\cref{eq:first_update}, is summable as $t \to \infty$. Note that $\sum_{s=1}^{t} \left( 1 - \pi_{\theta_s}(i) \right) = \sum_{s=1}^{t} \sum_{j \ne i}{ \pi_{\theta_s}(j) }$, which is indeed summable as $t \to \infty$, by the assumption and~\cref{eq:ebc_result}. This implies that   action $i$, which is sampled infinitely often, achieves a parameter magnitude, $\sup_{t \ge 1}{ |\theta_t(i)| } < \infty$, that remains bounded as $t\rightarrow\infty$. Using the softmax parameterization \cref{eq:softmax} in the above argument, we conclude that for all $a \in [K]$, $\inf_{t \ge 1}{ \pi_{\theta_t}(a)} > 0$, i.e., every action's probability remains bounded away from zero, and hence is not summable. Using~\cref{eq:ebc_result}, this implies that every action is sampled infinitely often, which contradicts the assumption that only action $i$ is sampled infinitely often as $t \to \infty$. 

\paragraph{Discussion.}
\cref{lem:at_least_two_actions_infinite_sample_time} implies that~\cref{alg:gradient_bandit_algorithm_sampled_reward} is not an aggressive method in the sense of \citep{mei2021understanding}, no matter how large the learning rate is, as long as it is constant, i.e., $\eta \in \Theta(1)$. According to \citep[Theorem 7]{mei2021understanding}, even if we fix the sampling in \cref{alg:gradient_bandit_algorithm_sampled_reward} to a sub-optimal action $a \in [K]$ forever, i.e., $a_t = a$ for all $t \ge 1$, its probability will not approach $1$ faster than $O(1/t)$, i.e., $1 - \pi_{\theta_t}(a) \in \Omega(1/t)$.  This means that there must be at least one another action $a^\prime \in [K]$ with $a^\prime \ne a$, such that $a^\prime$ will also be sampled  infinitely often. A more intuitive explanation is that the $\left( 1 - \pi_{\theta_t}(a) \right)$ term in \cref{eq:first_update} will be near $0$, which slows the speed of committing to a  deterministic policy on $a$ whenever $\pi_{\theta_t}(a)$ is close to $1$, % for any $a \in [K]$, 
which encourages exploration. Such natural exploratory behavior arises in \cref{alg:gradient_bandit_algorithm_sampled_reward} because of the softmax Jacobian $  \diagonalmatrix{(\pi_{\theta})} - \pi_{\theta} \pi_{\theta}^\top $ in the update shown in \cref{prop:gradient_bandit_algorithm_equivalent_to_stochastic_gradient_ascent_sampled_reward}, which determines the growth order of $\theta_t(a)$ for all $a \in [K]$ as $t \to \infty$, making the effect of a constant learning rate $\eta \in \Theta(1)$  asymptotically inconsequential.


\subsection{Warm up: Global Asymptotic Convergence when $K = 2$}

We now consider the simplest case, where we have only two possible actions. According to \cref{lem:at_least_two_actions_infinite_sample_time}, each of the two actions must be sampled infinitely often as $t \to \infty$. We now illustrate the second key result, that for both actions $a \in [K]$, the random sequence $\{ \theta_t(a) \}_{t \ge 1}$ follows the direction of the expected gradient for sufficiently large $t \ge 1$ almost surely. The proof uses a technique that has been previously used in \citep{mei2022role,mei2024stochastic} for small learning rates, but here we observe that the same technique continues to work for \cref{alg:gradient_bandit_algorithm_sampled_reward} no matter how large the learning rate is, as long as $\eta \in \Theta(1)$.

\begin{theorem}
\label{thm:two_action_global_convergence}
Let $K = 2$ and $r(1) > r(2)$. Using \cref{alg:gradient_bandit_algorithm_sampled_reward} with any $\eta \in \Theta(1)$, we have, almost surely, $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$, where $a^* \coloneqq \argmax_{a \in [K]}{ r(a) }$ (equal to Action $1$ in this case).
\end{theorem}
\paragraph{Proof sketch.} According to \cref{lem:at_least_two_actions_infinite_sample_time}, $N_\infty(1) = N_\infty(2) = \infty$. Denote the the reward gap as $\Delta \coloneqq r(a^*) - \max_{a \not= a^*}{ r(a) } > 0$, which becomes $\Delta = r(1) - r(2)$ for two actions. Since the stochastic gradient is unbiased (\cref{prop:gradient_bandit_algorithm_equivalent_to_stochastic_gradient_ascent_sampled_reward}), we have, for all $t \ge 1$ (detailed calculations omitted),
\begin{align}
\label{eq:two_action_case_optimal_action_param_submartingale_a}
    \EEt{\theta_{t+1}(a^*)} &= \theta_t(a^*) + \eta \cdot \pi_{\theta_t}(a^*) \cdot \left(  r(a^*) - \pi_{\theta_t}^\top r \right) \\
\label{eq:two_action_case_optimal_action_param_submartingale_b}
    &= \theta_t(a^*) + \eta \cdot \pi_{\theta_t}(a^*) \cdot \Delta \cdot \left( 1 - \pi_{\theta_t}(a^*) \right) > \theta_t(a^*).
\end{align}
A similar calculation shows that,
\begin{align}
\label{eq:two_action_case_suboptimal_action_param_supermartingale}
    \EEt{\theta_{t+1}(2)} = \theta_t(2) - \eta \cdot \pi_{\theta_t}(2) \cdot \Delta \cdot \left( 1 - \pi_{\theta_t}(2) \right) &< \theta_t(2),
\end{align}
which means that $\theta_t(a^*)$ is monotonically increasing in expectation and $\theta_t(2)$ is monotonically decreasing in expectation. In other words, $\{ \theta_t(a^*) \}_{t \ge 1}$ is a sub-martingale, while $\{ \theta_t(2) \}_{t \ge 1}$ is a  super-martingale. However, since $\theta_t \in \sR^K$ is unbounded, Doob's martingale convergence results cannot be directly applied, so we pursue a different argument. Following \citep{mei2022role,mei2024stochastic}, given an action $a \in [K]$, we define $P_t(a) \coloneqq \EEt{\theta_{t+1}(a)} -\theta_t(a) $ as the ``progress'', and define $W_t(a) \coloneqq \theta_t(a) - \chE_{t-1}{[ \theta_t(a)]}$ as the ``noise'', where $\theta_{t}(a) = W_t(a) + P_{t-1}(a) + \theta_{t-1}(a)$.  By recursion we can determine that,
\begin{align}
\label{eq:cumulative_noise_progress}
    \theta_t(a) = \EE{\theta_1(a)} + \sum_{s=1}^{t}{W_s(a)} + \sum_{s=1}^{t-1}{P_s(a)},
\end{align}
i.e., $\theta_t(a)$ is the result of ``cumulative progress'' and ``cumulative noise''. According to \citep[Theorem C.3]{mei2024stochastic}, the cumulative noise term can be bounded by using martingale concentration, where the order of the corresponding confidence interval is smaller than the order of the cumulative progress. 
% In particular, the noise term will appear under a square root 
Therefore, the summation will always be determined by the cumulative progress as $t \to \infty$. According to the calculations in~\cref{eq:two_action_case_optimal_action_param_submartingale_b,eq:two_action_case_suboptimal_action_param_supermartingale}, we have $P_t(a^*) > 0$ and $P_t(2) < 0$,  both of which are not summable. As a result, $\theta_t(a^*) \to \infty$ and $\theta_t(2) \to -\infty$ as $t \to \infty$, which implies that $\frac{ \pi_{\theta_t}(a^*)}{\pi_{\theta_t}(2)} = \exp\{ \theta_t(a^*) - \theta_t(a)\} \to \infty$, hence $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$.

\subsection{Global Asymptotic Convergence for all $K \ge 2$}

The illustrative two-action case shows that if $\pi_{\theta_t}^\top r \in (r(2), r(a^*))$ and if both actions are sampled infinitely often, then we have, almost surely $\theta_t(a^*) \to \infty$ and $\theta_t(2) \to - \infty$ as $t \to \infty$. However, the question at the beginning of \cref{subsec:avoid_lack_of_exploration} remains: when $K > 2$, if the two actions sampled infinitely often in \cref{lem:at_least_two_actions_infinite_sample_time} are both sub-optimal, will that result in a similar failure mode to the one described in \cref{subsec:failure_model_aggressive_updates}? The answer is no, which follows from our third key finding, which is based on another contradiction-based argument that establishes almost sure convergence to a globally optimal policy in the general $K > 2$ case.

\begin{theorem}
\label{thm:general_action_global_convergence}
Given $K \ge 2$, using \cref{alg:gradient_bandit_algorithm_sampled_reward} with any $\eta \in \Theta(1)$, we have, almost surely, $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$, where $a^* = \argmax_{a \in [K]}{ r(a) }$ is the optimal action.
\end{theorem}
\paragraph{Proof sketch.} We consider two cases: $N_\infty(a^*) < \infty$ and $N_\infty(a^*) = \infty$, corresponding to whether the optimal action is sampled finitely or infinitely often as $t \to \infty$. We argue that the first case ($N_\infty(a^*) < \infty$) is impossible, while for the second case ($N_\infty(a^*) = \infty$) we prove that $\theta_t(a^*) - \theta_t(a) \to \infty$ for all $a \in [K]$ with $r(a) < r(a^*)$, which implies $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$.

%\textit{First case.} Suppose that $N_\infty(a^*) < \infty$. We argue that this is not possible via contradiction. From our assumption and according to \cref{lem:finite_sample_time_implies_finite_parameter}, we have that $\inf_{t \ge 1}{ \theta_t(a^*)} > -\infty$. According to \cref{lem:expected_reward_range}, we have that for all large enough $t \ge 1$, $r(i_1) < \pi_{\theta_t}^\top r < r(i_2)$, where $i_1, i_2 \in [K]$ such that $N_\infty(i_1) = N_\infty(i_2) = \infty$ and $i_1 \ne i_2$. Since~\cref{lem:at_least_two_actions_infinite_sample_time} guarantees that there are at least two actions that are sampled infinitely often and since $N_\infty(a^*) < \infty$ from our assumption, we can conclude that $r(i_1) < r(i_2) < r(a^*)$. Now consider the sub-optimal action $i_1 \in [K]$. We have,
%\begin{align}
%\label{eq:general_case_first_case_suboptimal_action_param_supermartingale}
%    \EEt{\theta_{t+1}(i_1)} &= \theta_t(i_1) + \eta \cdot \pi_{\theta_t}(i_1) \cdot \left(  r(i_1) - \pi_{\theta_t}^\top r \right) < \theta_t(i_1),
%\end{align}
%After using similar arguments of \cref{eq:two_action_case_suboptimal_action_param_supermartingale,eq:cumulative_noise_progress} in the two-action case (\cref{thm:two_action_global_convergence}), implies that $\sup_{t \ge 1}{ \theta_t(i_1) } < \infty$. Therefore, we have,
%\begin{align}
%\label{eq:general_case_first_case_bounded_prob_ratio}
%    \sup_{t \ge 1}{ \frac{\pi_{\theta_t}(i_1)}{\pi_{\theta_t}(a^*)} } = \sup_{t \ge 1} \, \exp\{ \theta_t(i_1) - \theta_t(a^*) \}  < \infty \,.
%\end{align}
%On the other hand, since $N_\infty(i_1) = \infty$ (by \cref{lem:expected_reward_range}) and $N_\infty(a^*) < \infty$ (by assumption),~\cref{lem:unbouned_prob_ratio} implies that $\sup_{t \ge 1}{ \exp\{ \theta_t(i_1) - \theta_t(a^*)} \} = \infty$. This is a contradiction to~\cref{eq:general_case_first_case_bounded_prob_ratio}. 

\textit{First case.} Suppose that $N_\infty(a^*) < \infty$. We argue that this is impossible via contradiction. 
Given the assumption and \cref{lem:at_least_two_actions_infinite_sample_time} we know there must be at least two other sub-optimal actions $i_1, i_2 \in [K]$, $i_1 \ne i_2$, such that $N_\infty(i_1) = N_\infty(i_2) = \infty$.
In particular, let $i_1=\argmin_{a \in [K], N_\infty(a) = \infty} r(a)$
and $i_2=\argmax_{a \in [K], N_\infty(a) = \infty} r(a)$,
hence $r(i_1) < r(i_2) < r(a^*)$.
By \cref{lem:expected_reward_range} (see Appendix) we will also have $r(i_1) < \pi_{\theta_t}^\top r < r(i_2)$ for sufficiently large $t \ge 1$ , which implies for  action $i_1$,
\begin{align}
\label{eq:general_case_first_case_suboptimal_action_param_supermartingale}
    \EEt{\theta_{t+1}(i_1)} &= \theta_t(i_1) + \eta \cdot \pi_{\theta_t}(i_1) \cdot \left(  r(i_1) - \pi_{\theta_t}^\top r \right) < \theta_t(i_1),
\end{align}
for sufficiently large $t \ge 1$, which further implies
%A similar sequence of arguments to \cref{eq:two_action_case_suboptimal_action_param_supermartingale,eq:cumulative_noise_progress} in the two-action case (\cref{thm:two_action_global_convergence}) then imply 
that $\sup_{t \ge 1}{ \theta_t(i_1) } < \infty$. 
Meanwhile, for the optimal action $a^*$, the assumption and \cref{lem:finite_sample_time_implies_finite_parameter} imply that $\inf_{t \ge 1}{ \theta_t(a^*)} > -\infty$.
Combining these two observations gives,
\begin{align}
\label{eq:general_case_first_case_bounded_prob_ratio}
    \sup_{t \ge 1}{ \frac{\pi_{\theta_t}(i_1)}{\pi_{\theta_t}(a^*)} } = \sup_{t \ge 1} \, \exp\{ \theta_t(i_1) - \theta_t(a^*) \}  < \infty \,.
\end{align}
On the other hand, since $N_\infty(i_1) = \infty$ and $N_\infty(a^*) < \infty$ (by assumption), we then have
$\sup_{t \ge 1}{ \exp\{ \theta_t(i_1) - \theta_t(a^*)} \} = \infty$
by \cref{lem:unbouned_prob_ratio} (see Appendix), which contradicts \cref{eq:general_case_first_case_bounded_prob_ratio}. 



\textit{Second case.} Suppose that $N_\infty(a^*) = \infty$. We will argue that $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$ almost surely.
First, according to \cref{lem:at_least_two_actions_infinite_sample_time}, there exists at least one sub-optimal action $i_1\in[K]$, $i_1\neq a^*$, such that $N_\infty(i_1) = \infty$.
Let $i_1=\argmin_{a \in [K], N_\infty(a) = \infty} r(a)$.
By \cref{lem:expected_reward_range} and the definition of $a^*$, we have $r(i_1) < \pi_{\theta_t}^\top r < r(a^*)$ for all sufficiently large $t \ge 1$. Since $N_\infty(i_1) = \infty$, using similar calculations to~\cref{eq:two_action_case_suboptimal_action_param_supermartingale,eq:cumulative_noise_progress} in \cref{thm:two_action_global_convergence}, we have, $\theta_t(i_1) \to -\infty$ as $t \to \infty$. We also have, $\inf_{t \ge 1} \theta_t(a^*) > - \infty$ as $t \to \infty$. Hence, $\frac{ \pi_{\theta_t}(a^*) }{ \pi_{\theta_t}(i_1)} = \exp\{ \theta_t(a^*) - \theta_t(i_1) \} \to \infty$ as $t \to \infty$. 

Define $\gA_\infty \coloneqq \left\{ a \in [K] \ | \ N_\infty(a) = \infty \right\}$ as the set of actions that are sampled infinitely often, and note that $| \gA_\infty | \ge 2$ by \cref{lem:at_least_two_actions_infinite_sample_time}. Sort the action indices in $\gA_\infty$ according to their expected reward values in descending order, i.e.,
\begin{align}
\label{eq:general_case_second_case_descending_action_indices}
    r(a^*) > r(i_{|\gA_\infty| - 1}) > r(i_{|\gA_\infty| - 2}) > \cdots > r(i_2) > r(i_1).
\end{align}
\cref{assp:reward_no_ties} is used here to prevent two arms from having the same reward and thus guarantee the inequalities are strict in \cref{eq:general_case_second_case_descending_action_indices}. Next, using similar calculations as in \cref{lem:expected_reward_range}, we have,
\begin{align}
\label{eq:general_case_second_case_reward_difference}
    \pi_{\theta_t}^\top r - r(i_2) > \pi_{\theta_t}(a^*) \cdot \bigg[ r(a^*) - r(i_2) - \sum_{a^- \in \gA^-(i_2)}{ \frac{ \pi_{\theta_t}(a^-)}{ \pi_{\theta_t}(a^*) } \cdot ( r(i_2) - r(a^-)) } \bigg],
\end{align}
where $\gA^-(i_2) \coloneqq \left\{ a^- \in [K]: r(a^-) < r(i_2) \right\}$ is the set of actions that have lower mean reward than $i_2 \in [K]$, and note that $i_1 \in \gA^-(i_2)$. Using the above definitions, we can conclude that $i_1$ is the only arm in $\gA^-(i_2)$ that has been sampled infinitely often. According to \cref{lem:unbouned_prob_ratio}, for all $a^- \in \gA^-(i_2)$ with $a^- \ne i_1$, we have, $\frac{ \pi_{\theta_t}(a^*) }{ \pi_{\theta_t}(a^-)} \to \infty$ as $t \to \infty$, since $N_\infty(a^*) = \infty$ (by assumption) and $N_\infty(a^-) < \infty$ (by \cref{eq:general_case_second_case_descending_action_indices}). Therefore, for all sufficiently large $t$, the probability ratio in \cref{eq:general_case_second_case_reward_difference} $ \frac{ \pi_{\theta_t}(a^*)}{ \pi_{\theta_t}(a^-)} \to \infty$ for all $a^- \in \gA^-(i_2)$, which implies that, for all sufficiently large $t \ge 1$,
\begin{align}
\label{eq:general_case_second_case_positive_reward_difference_a}
    \pi_{\theta_t}^\top r - r(i_2) > 0.5 \cdot \pi_{\theta_t}(a^*) \cdot \big( r(a^*) - r(i_2) \big) > 0.
\end{align}
We have thus shown that $\pi_{\theta_t}^\top r > r(i_2)$. Recall that we had previously proved that $\pi_{\theta_t}^\top r > r(i_1)$. Hence, we will apply this argument recursively: after this point, $i_2 \in [K]$ will become the new ``$i_1 \in [K]$'', and a similar inequality to \cref{eq:two_action_case_suboptimal_action_param_supermartingale} will then hold for $i_2 \in [K]$ from similar calculations to \cref{eq:two_action_case_suboptimal_action_param_supermartingale,eq:cumulative_noise_progress} in \cref{thm:two_action_global_convergence},
establishing $\theta_t(i_2) \to -\infty$ as $t \to \infty$. 
This will imply that for all sufficiently large $t \ge 1$,
\begin{align}
\label{eq:general_case_second_case_positive_reward_difference_b}
    \pi_{\theta_t}^\top r - r(i_3) > 0.5 \cdot \pi_{\theta_t}(a^*) \cdot \big( r(a^*) - r(i_3) \big) > 0.
\end{align}
Continuing the recursive argument, we can conclude for all actions $a \in \gA_\infty$ with $a \ne a^*$ that $\frac{ \pi_{\theta_t}(a^*) }{ \pi_{\theta_t}(a)} \to \infty$ as $t \to \infty$. 
Meanwhile, for all actions $a \not\in \gA_\infty$, \cref{lem:unbouned_prob_ratio} also shows that
$\frac{ \pi_{\theta_t}(a^*) }{ \pi_{\theta_t}(a)} \to \infty$ as $t \to \infty$. Combining these two results yields the conclusion that for all sub-optimal actions $a \in [K]$ with $r(a) < r(a^*)$ we have $\frac{ \pi_{\theta_t}(a^*) }{ \pi_{\theta_t}(a)} \to \infty$ as $t \to \infty$, which implies $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$. Thus, we have established almost sure convergence to the globally optimal policy.

\paragraph{Discussion.} \cref{lem:at_least_two_actions_infinite_sample_time} is important to prove that the optimal arm will be sampled infinitely often. In particular,~\cref{lem:at_least_two_actions_infinite_sample_time} guarantees $| \gA_\infty | \ge 2$ and the existence of $i_1$ in \cref{eq:general_case_first_case_suboptimal_action_param_supermartingale}, which can then be used to construct the contradiction in \cref{eq:general_case_first_case_bounded_prob_ratio}. Without \cref{lem:at_least_two_actions_infinite_sample_time}, 
% the relation $r(i_1) < \pi_{\theta_t}^\top r < r(i_2)$ above \cref{eq:general_case_first_case_suboptimal_action_param_supermartingale} does not necessarily hold (since 
$| \gA_\infty |$ might be equal to $1$ and the failure mode in \cref{subsec:failure_model_aggressive_updates} can occur, resulting in \cref{alg:gradient_bandit_algorithm_sampled_reward} not sampling the optimal action infinitely often as $t \to \infty$.

\subsection{Asymptotic Rate of Convergence}

According to \cref{thm:general_action_global_convergence}, almost surely, $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$. Therefore, after a large enough time $\tau < \infty$, we have $\pi_{\theta_t}(a^*) \ge 1/2$, which implies that the ``progress'' term in \cref{eq:cumulative_noise_progress} can be lower bounded. With this, an asymptotic rate of convergence can be proved as follows.
\begin{theorem}
\label{thm:asymptotic_rate_of_convergence}
For a large enough $\tau > 0$, for all $T > \tau$, the average sub-optimality decreases at an $O\left(\frac{\ln(T)}{T} \right)$ rate. Formally, if $a^*$ is the optimal arm, then, for a constant $c$,
\begin{align*}
\frac{\sum_{s=\tau}^{T} r(a^*) - \langle \pi_s, r \rangle}{T}  & \leq \frac{c \, \ln(T)}{T - \tau}.
\end{align*}
\end{theorem}


