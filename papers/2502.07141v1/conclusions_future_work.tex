%!TEX root =  neurips_2024.tex

\vskip-2ex
\section{Conclusions and Future Directions}
\label{sec:conclusion}
\vskip-1ex

This work refines our understanding of stochastic gradient bandit algorithms by proving that it converges to a globally optimal policy almost surely with \emph{any} constant learning rate. Our new proof strategy based on the asymptotics of sample counts opens new directions for better characterizing exploration effects of stochastic gradient methods, while also suggesting interesting new questions. 
Characterizing the multiple stages of convergence remains another interesting future direction.
One interesting possibility is  that there might exist an optimal time-dependent scheme for \emph{increasing} the learning rate (such as $\eta \in O(\log{t})$) to accelerate convergence, rather than use a constant $\eta \in O(1)$. This is corroborated by our experiments:
As seen in \cref{fig:visualization_general_action_case}, small learning rates perform better during the early stages of optimization, while larger learning rates achieve faster convergence during the final stage. Other directions include extending our bandit results to the more general RL setting \citep{williams1992simple}, as well as extending our results for the softmax tabular parameterization to handle function approximation~\citep{agarwal2021theory}.

\textbf{Limitations:} While this work establishes a surprising asymptotic convergence result for any constant learning rate, it does not shed light on the effect of different learning rates on the convergence. Moreover, our analysis is limited to multi-armed bandits, and does not immediately extend to the general RL setting. These aspects are the main limitations of this paper.  
% , along with the lack of a more detailed empirical study are the main limitations of this paper. 
% Extensions to more general RL scenarios is another important direction for future work.

\textbf{Broader impact:} This is primarily theoretical work on a fundamental algorithm that is used broadly in RL applications. We expect these results to improve the research community's understanding of the basic stochastic gradient bandit method.

