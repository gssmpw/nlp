%!TEX root =  neurips_2024.tex
\vskip-2ex
\section{Simulation Study}
\label{sec:sim}
\vskip-1ex

To validate and enhance the theoretical findings, we ran experiments with a four action bandit environment
($K=4$)
with a true mean reward vector of $r = (0.2, 0.05, -0.1, -0.4)^\top \in \sR^4$.
The reward distribution $P_a$ for arm $1\le a \le 4$ is Gaussian, centered at $r(a)$
and with a standard deviation of $0.1$.
The environment is chosen to illustrate various phenomenon, which we discuss after presenting the results.
%We conduct a few simple simulations to empirically verify the theoretical findings.
The algorithm is \cref{alg:gradient_bandit_algorithm_sampled_reward}
with 
$\theta_1 = \rvzero \in \sR^K$.
%We  consider a stochastic bandit
%problem with $K = 4$ actions and . In each iteration $t \ge 1$ of \cref{alg:gradient_bandit_algorithm_sampled_reward}, given a sampled action $a_t \sim \pi_{\theta_t}(\cdot)$, the observed reward is generated as $R_t(a_t) = r(a_t) + 0.1 \cdot Z_t$, where $Z_t \sim \gN(0, 1)$ is random Gaussian noise. 

For comparison, assuming that the random rewards belong to the $[-1,1]$ interval,
the only result for the stochastic gradient bandit algorithm~\citep[Lemma 4.6]{mei2024stochastic}
that allowed a constant learning rate
required that the learning rate be less 
than $\eta_c = \frac{\Delta^2}{40 \cdot K^{3/2} \cdot R_{\max}^3 } = \frac{9}{128000} \approx 0.00007$, where we used $\Delta = 0.15$, $K = 4$, and $R_{\max} = 1$.
While technically, the result does not apply to our case where the reward distributions have unbounded support, the probability of the reward landing outside of $[-1,1]$ is in the order of $10^{-9}$.
Choosing $R_{\max}$ to be larger, this probability falls extremely quickly, which suggests that the above threshold is generous.
For the experiments
 we use the learning rates $\eta \in \{1, 10, 100, 1000\}$, that are several orders of magnitudes larger than $\eta_c$.
 For each learning rate, we plot the outcome of $10$ runs, corresponding to different random seeds.
Each run lasts $10^6 (\approx e^{14})$ iterations. 
The log-suboptimality gaps for the $4\times 10$ cases are shown on
Figures~\ref{fig::sub_optimality_gap_general_action_case_eta_1}-\ref{fig::sub_optimality_gap_general_action_case_eta_1000}, 
where they are plotted against the logarithm of time. 
Additional results for $K=2$ arms are shown in Appendix~\ref{app:sim}.
Note that in this example small sub-optimality implies that the optimal arm is chosen with high probability.
In what follows, we discuss the results in the plots. 

\paragraph{Asymptotic convergence.} %We run \cref{alg:gradient_bandit_algorithm_sampled_reward} on 
For the smaller learning rates of $\eta = 1$ and $10$, all $10$ seeds rapidly and steadily converge, reaching a sub-optimality of $e^{-14}$ or less. For $\eta = 100$ and $1000$, most of the runs reach even small error even faster, but some runs are ``stuck'' even after $10^6$ steps. Note that this does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of $\eta=100$, even after a long phase with little to no progress, a run can ``recover'' (see the grey curve). In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates are further discussed below.


% , while a few take longer to converge as we show in Appendix~\ref{app:sim}. 

\paragraph{Non-monotone objective value.} Using a very small learning rate guarantees monotonic improvement (in expectation) in the policy's expected reward~\citep{mei2024stochastic}. 
Conversely, a large learning rate results in non-monotonic evolution of the expected rewards $\{ \pi_{\theta_t}^\top r \}_{t \ge 1}$, even in the final stages of convergence, as can be seen clearly for 
the learning rates of $\eta = 1$ and $10$ in Figures~\ref{fig::sub_optimality_gap_general_action_case_eta_1} and~\ref{fig::sub_optimality_gap_general_action_case_eta_10}. For larger $\eta$, the non-monotone behavior happens over longer periods and is less visible in the plots. This is because using large learning rates causes the policy to rapidly increase the parameters for some action, after which the gradient becomes small, limiting further progress.
% and the recovery to the optimal arm is slower. This is related to the above-mentioned slower convergence of some runs for larger $\eta$.

\begin{figure}
\centering
\begin{subfigure}[b]{.39\linewidth}
\includegraphics[width=\linewidth]{figs/large_learning_rate_general_action_log_subopt_eta_1.00.pdf}
\caption{$\eta = 1$.}\label{fig::sub_optimality_gap_general_action_case_eta_1}
\end{subfigure}
\begin{subfigure}[b]{.39\linewidth}
\includegraphics[width=\linewidth]{figs/large_learning_rate_general_action_log_subopt_eta_10.00.pdf}
\caption{$\eta = 10$.}\label{fig::sub_optimality_gap_general_action_case_eta_10}
\end{subfigure}\\
\begin{subfigure}[b]{.39\linewidth}
\includegraphics[width=\linewidth]{figs/large_learning_rate_general_action_log_subopt_eta_100.00.pdf}
\caption{$\eta = 100$.}\label{fig::sub_optimality_gap_general_action_case_eta_100}
\end{subfigure}
\begin{subfigure}[b]{.39\linewidth}
\includegraphics[width=\linewidth]{figs/large_learning_rate_general_action_log_subopt_eta_1000.00.pdf}
\caption{$\eta = 1000$.}\label{fig::sub_optimality_gap_general_action_case_eta_1000}
\end{subfigure}
\caption{
Log sub-optimality gap, 
$\log{ (r(a^*) - \pi_{\theta_t}^\top r) } $, plotted against the logarithm of time,  $\log{t}$, in a $4$-action problem with various learning rates, $\eta$. 
Each subplot shows a run with a specific learning rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially all seeds will lead to a curve converging to zero ($-\infty$ in these plots). For a discussion of the results, see the text.,
}
\label{fig:visualization_general_action_case}
\vspace{-10pt}
\end{figure}
\paragraph{Rate of convergence.} Figures~\ref{fig::sub_optimality_gap_general_action_case_eta_1} and~\ref{fig::sub_optimality_gap_general_action_case_eta_10}, where the log-log plot has a slope of nearly $-1$, give some evidence that an $O(1/t)$ asymptotic rate is achieved. In general, such a rate cannot be improved in terms of $t$ \citep{lai1985asymptotically}. \cref{thm:asymptotic_rate_of_convergence} gives a weaker version of convergence rate over averaged iterates (not last iterate), which is slightly worse than $O(1/t)$. More work is needed to verify if the asymptotic convergence rate in \cref{thm:asymptotic_rate_of_convergence} is improvable or not. 

\paragraph{Different learning rates.} 
% The results do not characterize the detailed effects of different learning rates. Presumably, using $\eta = 1$ would lead to very different behavior in practice compared to using $\eta = 1000$. 
Two observations can be made from Figure~\ref{fig:visualization_general_action_case} regarding the effect of using different $\eta$ values: \textbf{First}, during the final stage of convergence when $r(a^*) - \pi_{\theta_t}^\top r \approx 0$, using larger $\eta$ results in faster convergence on average. As $\eta$ increases, the order of $\log{ (r(a^*) - \pi_{\theta_t}^\top r) } $ also changes from $e^{-14}$ ($\eta = 1$), to $e^{-20}$ ($\eta = 100$), and $e^{-200}$ ($\eta = 1000$). We conjecture that the asymptotic rate of convergence has an $O(1/\eta)$ dependence. 
% However, this can only be verified after proving a rate of convergence result, which still remains open. 
\textbf{Second}, using larger learning rates can take a longer time to enter the final stage of convergence. When $\eta = 1$ or $10$, all curves quickly enter the final stage of $r(a^*) - \pi_{\theta_t}^\top r \approx 0$. However, for larger $\eta$ values, $1/10$ runs ($\eta = 100$) and $3/10$ runs ($\eta = 1000$) result in $r(a^*) - \pi_{\theta_t}^\top r $ values far from $0$ even after $10^6$ iterations. These runs take orders of magnitude more iterations to eventually achieve $r(a^*) - \pi_{\theta_t}^\top r \approx 0$. These situations correspond to the policy $\pi_{\theta_t}$ getting stuck near sub-optimal corners of the simplex, meaning that $\pi_{\theta_t}(i) \approx 1$ for a sub-optimal action $i \in [K]$ with $r(i) < r(a^*)$. In such cases, even Softmax PG with the true gradient can remain stuck on a sub-optimal plateau for an extremely long time~\citep{mei2020global}. However, the reason why larger learning rates lead to longer plateaus in the stochastic setting remains unclear.

\paragraph{Trade-offs and multi-stage chracterizations of convergence.} Given the above observations, there appears to exist a trade-off for $\eta$: larger $\eta$ values result in faster convergence during the final stage where $r(a^*) - \pi_{\theta_t}^\top r \approx 0$, but at the the cost of taking far longer to enter this final stage of convergence. Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory manner, a more refined analysis that considers the different stages of convergence is required. 
% needed to better understand the effect of using different $\eta$ values.

