\clearpage
\section{Rate of Convergence}


\textbf{\cref{thm:asymptotic_rate_of_convergence}.}
For a large enough $\tau > 0$, for all $T > \tau$, the average sub-optimality decreases at an $O\left(\frac{\ln(T)}{T} \right)$ rate. Formally, if $a^*$ is the optimal arm, then, for a constant $c$
\begin{align*}
\frac{\sum_{s=\tau}^{T} r(a^*) - \langle \pi_s, r \rangle}{T}  & \leq \frac{c \, \ln(T)}{T - \tau}
\end{align*}
\begin{proof}
The progress for the optimal action is,
\begin{align}
    P_t(a^*) &= \eta \cdot \pi_{\theta_t}(a^*) \cdot (r(a^*) -  \pi_{\theta_t}^\top r ) \\
    &\geq \eta \cdot \Delta \cdot \pi_{\theta_t}(a^*) \cdot \big( 1 - \pi_{\theta_t}(a^*) \big). \qquad \big( \Delta \coloneqq r(a^*) - \max_{a \neq a^*}{r(a)} \big) \\
    &\ge 0.
\end{align}
Since $\lim_{t \to \infty}{\pi_{\theta_t}(a^*)} = 1$, we have for all large enough $t \ge 1$,
\begin{align}
    \pi_{\theta_t}(a^*) \ge 1/2.
\end{align}
Since $N_\infty(a^*) = \infty$ (\cref{thm:general_action_global_convergence}
) and $|\gA_\infty| \ge 2$ (\cref{lem:at_least_two_actions_infinite_sample_time}), we have,
\begin{align}
    \sum_{t=1}^{\infty}{(1 - \pi_t(a^*))} \ge \sum_{t=1}^{\infty}{\pi_t(i_1)} = \infty,
\end{align}
where $N_\infty(i_1) = \infty$ and $r(i_1) < r(a^*)$. Therefore, we have,
\begin{align}
    \sum_{t=1}^{\infty}{P_t(a^*)} = \infty.
\end{align}
The variance of noise is,
\begin{align}
    V_t(a^*) &\coloneqq \frac{5}{18} \cdot \sum_{s=1}^{t-1}  \pi_{\theta_s}(a^*) \cdot (1-\pi_{\theta_s}(a^*)),
\end{align}
which will be dominated by sum of $P_t(a^*)$ since $V_t(a^*)$ appears under square root (\cref{lem:parameter_noise_concentration}). Therefore, for all large enough $t \ge \tau$,
\begin{align}
    \theta_t(a^*) \ge C \cdot \sum_{s=\tau}^{t}{(1 - \pi_s(a^*))}.
\end{align}
%\begin{align}
%    \liminf_{t \to \infty}{ \frac{\theta_t(a^*)}{\sum_{s=\tau}^{t}{(1 - \pi_s(a^*))}}} \ge C   
%\end{align}
On the other hand, we argued recursively (in the proofs for \cref{thm:general_action_global_convergence})  that for all sub-optimal action $a \in [K]$ with $r(a) < r(a^*)$,
\begin{align}
    \sup_{t \ge 1} \theta_t(a) < \infty.
\end{align}
Therefore, we have, for all large enough $t \ge \tau$,
\begin{align}
    \theta_t(a^*) - \theta_t(a) \ge C \cdot \sum_{s=\tau}^{t}{(1 - \pi_s(a^*))},
\end{align}
which implies that,
\begin{align}
    \sum_{a \neq a^*}{ \exp\{ \theta_t(a) - \theta_t(a^*) \}} \le (K - 1) \cdot \exp\bigg\{ - C \cdot \sum_{s=\tau}^{t}{(1 - \pi_s(a^*))} \bigg\}.
\end{align}
Therefore, we have,
\begin{align}
    1 - \pi_t(a^*) \le \frac{1 - \pi_t(a^*)}{\pi_t(a^*)} = \sum_{a \neq a^*}{ \frac{\pi_t(a)}{\pi_t(a^*)}} \le (K - 1) \cdot \exp\bigg\{ - C \cdot \sum_{s=\tau}^{t-1}{(1 - \pi_s(a^*))} \bigg\}.
\end{align} 
Using~\cref{lemma:partial-sum-combination} with $x_n = \sum_{s=\tau}^{t-1}{(1 - \pi_s(a^*))} > 0$, $x_{n+1} = \sum_{s=\tau}^{t}{(1 - \pi_s(a^*))} > 0$, $c = C > 0$ and $B = K - 1 \geq 1$ gives us that for all $t > \tau$, 
\begin{align}
\sum_{s=\tau}^{t}{(1 - \pi_s(a^*))} & \leq \frac1{C} \ln(C t + e^{C M} ) + \frac{\pi^2}{12C} \,,
\label{eq:rate-inter}
\end{align}
where $M = \max\{K - 1, \frac{1}{C} \ln ((K - 1) \, C)), (1 - \pi_\tau(a^*)) \} = K - 1$. 

Finally, we use~\cref{eq:rate-inter} to bound the average sub-optimality. For any $s \geq \tau$ and $T > \tau$,
\begin{align*}
r(a^*) - \langle \pi_s, r \rangle &= \sum_{a \neq a^*} \pi_{s} \, [r(a^*) - r(a)] \leq 2 \, R_{\max} \, (1 - \pi_s(a^*)) \tag{Since $r(a) \in [-R_{\max},R_{\max}]$}. \\
\intertext{Summing from $s = \tau$ to $T$,}
\implies \frac{\sum_{s=\tau}^{T} r(a^*) - \langle \pi_s, r \rangle}{T} & \leq \frac{2 \, R_{\max} \, \left[\frac1{C} \ln(C \, T + e^{C M} ) + \frac{\pi^2}{12C} \right]}{T - \tau}. \qedhere
\end{align*}
\end{proof}

\begin{lemma}
Let $\{y_n\}$ be the solution to the difference equation $y_{n+1} = y_n + B \, e^{-cy_n}$ with $B \geq 1$, $c > 0$ and $y_0 \ge \max(B,\frac{1}{c}\ln (B \, c))$. Let $\{x_n\}$ be a nonnegative valued sequence such that $x_0 \le y_0$ and $x_{n+1} \le x_n + B \, e^{-cx_n}$ for all $n \ge 0$. Then, $x_n \le y_n$ for all $n \ge 0$.
\label{lemma:difference-inequality}
\end{lemma}

\begin{proof}
Define the function $f(y) = \max \{ x + B \, e^{-cx} : 0\le x \le y \}$. Clearly, $f$ is an increasing function of its argument.

\noindent \underline{Claim:} For $y\ge \max(B, \frac{1}{c} \ln (B \, c))$, $f(y) = y + B \, e^{-cy}$.
To prove this claim, for $x\in \mathbb{R}$, define
\begin{align}\label{eq:gdef}
g(x) := x + B \, e^{-cx}\,.
\end{align}
Function $g$ is increasing on $(\frac{1}{c} \ln (B \, c), \infty)$ and decreasing on $(-\infty,\frac{1}{c}\ln (B \, c))$. 

If $c < \nicefrac{1}{B}$, $\frac{1}{c}\ln (Bc) < 0$, hence $g$ is increasing on $(0, \infty)$. Hence, $f(y)=g(y)$, proving the claim. 

If $c \ge \nicefrac{1}{B}$, then $\frac{1}{c}\ln (B \, c) \ge 0$. Hence, $g$ is decreasing on $(0, \nicefrac{1}{c}\ln (B \, c))$ and then increasing on $(\nicefrac{1}{c}\ln (B \, c) )$. Since $y\ge \frac{1}{c} \ln(B \, c)$, $f(y) = \max(g(0),g(y))$. Since we also have $y\ge B > \frac{1}{c}\ln (B \, c)$, $g(y) \ge g(B) > B = g(0)$ and thus $f(y)=g(y)$, finishing the proof of the claim.

The difference equation for $y_n$ is $y_{n+1} = y_n + B \, e^{-cy_n}$. Since $y_0 \ge \max(B,\frac{1}{c}\ln (B \,c))$ and $y_n$ is increasing, we have $y_n \ge \max(B,\frac{1}{c}\ln (B \,c))$ for all $n$. 
Therefore, we can apply the equality from the previous claim to get 
\[
y_{n+1} = y_n + B \, e^{-cy_n} = f(y_n)\,.
\]
% S: commenting since we are not using this
% This implies 
% \[
% y_n = f(f(\cdots f(y_0) \cdots )) \qquad \text{(apply $f$ $n$ times)}
% \]
The sequence $(x_n)$ satisfies the inequality $x_{n+1} \le x_n + B \, e^{-cx_n} \le f(x_n)$.

Now, let us prove $x_n \le y_n$ using induction. The base case is $x_0 \le y_0$ (given). Assume $x_k \le y_k$ for some $k \ge 0$. Since $f$ is increasing, $x_k \le y_k$ implies $f(x_k) \le f(y_k)$. Using the properties $x_{k+1} \le f(x_k)$ and $f(y_k) = y_{k+1}$, we get $x_{k+1} \le f(x_k) \le f(y_k) = y_{k+1}$. By the principle of mathematical induction, $x_n \le y_n$ for all $n \ge 0$.
\end{proof}

\begin{lemma}
Let $c > 0$ and $B \geq 1$, and let $\{y_n\}_{n=0}^\infty$ be a sequence defined by the recurrence relation
\[
y_{n+1} = y_n + B \, e^{-c y_n}
\]
s.t. $y_0 \ge \max(B,\frac{1}{c}\ln (B \, c))$. Then, 
\[
y_n \leq \frac{1}{c} \ln(c n + e^{c y_0} ) + \frac{\pi^2}{12c}.
\]
\label{lemma:partial-sum-bound}
\end{lemma}

\begin{proof}
Define the  function:
\begin{align*}
    y(t) & := \frac1{c} \ln(c t + e^{c y_0} )\,, \qquad t\ge 0\,,
\end{align*}
and note that it is an increasing function, Moreover, $\dot y(t) (= \frac{d}{dt} y(t)) = e^{-c y(t)}$ for any $t \ge 0$. Hence,
\begin{align*}
y(t) = y_0 + \int_0^t e^{-c y(s)} \, ds\,, \qquad t\ge 0
\end{align*}

Define the function:
\begin{align*}
g(x) := x + B \, e^{-cx}\,,
\end{align*}
and note that $g$ is increasing when $y\ge \frac{1}{c} \ln (B \,c)$. Moreover, $y(0) = y_0$ and $y_{n+1} = g(y_n)$. 

Since $y(0) = y_0 \ge \frac{1}{c} \ln (B \,c))$ and both $\{y_n\}$ and $y(t)$ are increasing, $y_n \geq \frac{1}{c} \ln (B \, c)$ for all $n$ and , $y(t) \geq \frac{1}{c} \ln (B \, c)$ for all $t \geq 0$.

We first prove that
\begin{align}
    y(n)\le y_n\,, \qquad n=0,1,\dots\,.
    \label{eq:ynlb}
\end{align}
We prove \cref{eq:ynlb} by induction. The claim holds for $n=0$ by construction. Now assume that $y(n)\le y_n$ holds 
for some $n\ge 0$. Let us show that that $y(n+1)\le y_{n+1}$ also holds. For this note that
\begin{align*}
    y(n+1) 
    &=      y(n) + \int_n^{n+1} e^{-c y(s)} ds \\
    &\le    y(n) + \int_n^{n+1} e^{-c y(n)} ds  \tag{$t\mapsto y(t)$ is increasing}\\
    &=      y(n) + e^{-c y(n)} \\
    & \leq  y(n) + B \, e^{-c y(n)} \tag{since $B \geq 1$} \\
    &=   g(y(n)) \tag{definition of $g$} \\
    &\le    g(y_n) \tag{induction hypothesis, $g$ is increasing for $y\ge \frac{1}{c} \ln (B \,c)$ and $y_n, y(n) \ge \frac{1}{c} \, \ln (B \, c)$} \\
    &=      y_{n+1}\,. \tag{By definition of $y_{n+1}$}
\end{align*}    
By the principle of mathematical induction, $y(n) \le y_{n}$ for all $n \ge 0$.

Define $\Delta_n := y_n - y(n)$. From our previous inequality, we know that $\Delta_n \ge 0$. We now show that $\{\Delta_n\}_n$ is bounded, from which the desired statement follows immediately.
To show that $\{\Delta_n\}_n$ is bounded we will show that $\Delta_{n+1}-\Delta_n$ is summable.
To show this, we start by obtaining an expression for $\Delta_{n+1}-\Delta_n$. 
Let $A = e^{c y_0}$. Let $n\ge 0$.
Direct calculation gives
\begin{align*}
    \Delta_{n+1}-\Delta_n 
    & =  e^{-c y_n} - \frac{1}{c} \ln\left( 1 + \frac{c}{cn+A} \right)\,.
\end{align*}
Using that for all $x>0$, $\ln(1+x)\ge \frac{x}{1+\frac{x}{2}}$, we get 
\begin{align*}
    \Delta_{n+1}-\Delta_n 
    & \le e^{-c y_n} - \frac{1}{cn + A + \frac{c}{2}} \\
    & \le e^{-c y(n)} - \frac{1}{cn + A + \frac{c}{2}} \tag{from \cref{eq:ynlb}} \\
    & \le \frac{1}{cn +A } - \frac{1}{cn + A + \frac{c}{2}} \tag{definition of $y(n)$} \\
    & = \frac{1}{2c} \, \frac{1}{(n + \nicefrac{A}{c}) \, (n + \nicefrac{A}{c} + \frac{1}{2})}  \\
    & \le \frac{1}{2c} \frac{1}{(n+1)^2}\,. \qquad \tag{Since $y_0 \geq \frac{1}{c} \ln (B \,c))$, $A/c = B \ge 1$}
\end{align*}
For a fixed $m > 0$, summing up the above inequality from $n = 0$ to $m-1$,
\begin{align*}
\Delta_{m}  - \Delta_{0} & \leq \frac{1}{2c} \sum_{n = 0}^{m-1} \frac{1}{(n+1)^2} \leq \frac{\pi^2}{12c} \tag{Since $\sum_{i = 1}^{\infty} \frac{1}{i^2} = \frac{\pi^2}{6}$} \\
\implies \Delta_{m} & \leq \frac{\pi^2}{12c}.  \tag{Since $\Delta_0 = 0$}
\end{align*}
where $\pi = 3.14159\dots$. Hence, it follows that for any $n\ge 0$,
\begin{equation*}
y_n = y(n) + \Delta_n \leq y(n) + \frac{\pi^2}{12c} = \frac1{c} \ln(c n + e^{c y_0} ) + \frac{\pi^2}{12c}. \qedhere
\end{equation*}
\end{proof}

\begin{lemma}
Let $\{x_n\}$ be a nonnegative valued sequence such that $x_{n+1} \le x_n + B \, e^{-cx_n}$ for all $n \ge 0$ with $B \geq 1$, $c > 0$. Then, for all $n \ge 0$, 
\begin{align*}
x_n \le  \frac1{c} \ln(c n + e^{c M} ) + \frac{\pi^2}{12c} \,,
\end{align*}
where $M = \max\{B, \frac{1}{c} \ln (B \,c)), x_{0} \}$. 
\label{lemma:partial-sum-combination}    
\end{lemma}
\begin{proof}
Let $\{y_n\}$ be the solution to the difference equation $y_{n+1} = y_n + B \, e^{-cy_n}$ where $y_0 = \max\{B, \frac{1}{c} \ln (B \,c)), x_{0} \}$. Since $y_0 \geq x_0$ and $y_0 \ge \max(B,\frac{1}{c}\ln (B \, c))$, we can use~\cref{lemma:difference-inequality} to conclude that for all $n \ge 0$, 
\[
x_n \le y_n \,.
\] 

Furthermore, using~\cref{lemma:partial-sum-bound} we can conclude that, 
\begin{align*}
y_n \leq \frac{1}{c} \ln(c n + e^{c y_0} ) + \frac{\pi^2}{12c}.
\end{align*} 
Combining the above inequalities completes the proof. 
\end{proof}

% This section shows that \cref{alg:gradient_bandit_algorithm_sampled_reward} achieves a $O(1/t)$ asymptotic convergence rate, which is optimal and thus not improvable in terms of $t$ \citep{lai1985asymptotically}.
% \begin{theorem}
% \label{thm:asymptotic_rate_two_action_case}
% Let $K = 2$ and $r(1) > r(2)$. We have,
% \begin{align}
%     \EE{ r(a^*) - \pi_{\theta_t}^\top r } \in O(1/t).
% \end{align}
% \end{theorem}


% \textbf{\cref{thm:asymptotic_rate_two_action_case}.}
% Let $K = 2$ and $r(1) > r(2)$. We have,
% \begin{align}
%     \EE{ r(a^*) - \pi_{\theta_t}^\top r } \in O(1/t).
% \end{align}
% \begin{proof}
% Define the following quantity, for all $t \ge 1$,
% \begin{align}
%     q_t &\coloneqq \frac{1 - \pi_{\theta_t}(a^*) }{\pi_{\theta_t}(a^*)} \\
%     &= \exp\{ \theta_t(2) - \theta_t(a^*) \}.
% \end{align}
% We have, for all $t \ge 1$,
% \begin{align}
%     \EEt{ \log{q_{t+1}} } &= \EEt{ \theta_{t+1}(2) - \theta_{t+1}(a^*) } \\
%     &= \theta_t(2) - \theta_t(a^*) + \eta \cdot \big[ \pi_{\theta_t}(2) \cdot (r(2) - \pi_{\theta_t}^\top r) -  \pi_{\theta_t}(a^*) \cdot (r(a^*) - \pi_{\theta_t}^\top r)\big] \\
%     &= \log{q_t} - 2 \ \eta \cdot \Delta \cdot \pi_{\theta_t}(a^*) \cdot (1 - \pi_{\theta_t}(a^*)) \\
%     &= \log{q_t} - 2 \ \eta \cdot \Delta \cdot q_t \cdot \pi_{\theta_t}(a^*)^2.
% \end{align}
% Taking expectation on both sides in the above equation, we have,
% \begin{align}
%     \EE{ \log{q_{t+1}} } &= \EE{ \log{q_t} } - 2 \ \eta \cdot \Delta \cdot \EE{ q_t \cdot \pi_{\theta_t}(a^*)^2 } \\
%     &\le \EE{ \log{q_t} } - 2 \ \eta \cdot \Delta \cdot (1 - \epsilon)^2 \cdot \EE{ q_t },
% \end{align}
% where the last inequality is because of  $\pi_{\theta_t}(a^*) \to 1$ as $t \to \infty$ almost surely, which implies that $\pi_{\theta_t}(a^*) \geq 1 - \epsilon$ for all $t \ge t_0$. Summing up from $t = t_0$ to $T$ and by telescoping, we have,
% \begin{align}
%     2 \ \eta \cdot \Delta \cdot (1 - \epsilon)^2 \cdot \sum_{t=t_0}^{T}{ \EE{ q_t } } &\leq \EE{ \log{q_{t_0}} } - \EE{ \log{q_{T+1}} } \\
%     &\leq - \EE{ \log{q_{T+1}} },
% \end{align}
% where the last inequality is because of $q_t < 1$ for all $t \ge t_0$. If $- \EE{ \log{q_{T+1}} } \leq \log{T}$, then we have,
% \begin{align}
%     \frac{1}{T - t_0} \cdot \sum_{t=t_0}^{T}{ \EE{ q_t } } \le \frac{1}{2 \ \eta \cdot \Delta \cdot (1 - \epsilon)^2} \cdot \frac{\log{T}}{T - t_0},
% \end{align}
% which is an average rate of $O(\log{T} / (T - t_0))$. Otherwise, if $- \EE{ \log{q_{T+1}} } \geq \log{T}$, we have,
% \begin{align}
%     - \log{( \EE{ q_{T+1} } )} \leq - \EE{ \log{q_{T+1}} } \geq \log{T}
% \end{align}

% \textcolor{red}{which implies that,}
% \begin{align}
%     \EE{q_t} \le \frac{C}{t}.
% \end{align}
% Therefore, we have,
% \begin{align}
%     \EE{ r(a^*) - \pi_{\theta_t}^\top r } = \Delta \cdot \EE{ 1 - \pi_{\theta_t}(a^*) } \le \frac{C}{t}.
% \end{align}

% Define
% \begin{align}
%     p_t = \theta_t(a^*) - \theta_t(2).
% \end{align}
% We have,
% \begin{align}
%     \EEt{ p_{t+1} } &=  p_t + 2 \ \Delta \ \eta \cdot \pi_{\theta_t}(a^*) \cdot (1 - \pi_{\theta_t}(a^*) ) \\
%     &= p_t + 2 \ \Delta \ \eta \cdot \pi_{\theta_t}(a^*)^2 \cdot \exp\{  - p_t \} \\
%     &\ge p_t + 2 \ \Delta \ \eta \cdot c^2 \cdot \exp\{  - p_t \}.
% \end{align}
% Taking expectation, we have,
% \begin{align}
%     \EE{ p_{t+1} } &\ge \EE{ p_t } + 2 \ \Delta \ \eta \cdot c^2 \cdot \EE{ \exp\{  - p_t \} } \\
%     &\ge \EE{ p_t } + 2 \ \Delta \ \eta \cdot c^2 \cdot \exp\{  - \EE{  p_t } \} 
% \end{align}
% \textcolor{red}{If we have,}
% \begin{align}
%     \exp\{ \EE{ p_t } \} \ge C \cdot t,
% \end{align}
% then,
% \begin{align}
%      \exp\{ - \EE{ p_t } \} \in O(1/t).
% \end{align}
% What we need:
% \begin{align}
%     \frac{1 - \EE{\pi_{\theta_t}(a^*)}}{\EE{\pi_{\theta_t}(a^*)}}
% \end{align}
% or
% \begin{align}
%     1 - \EE{\pi_{\theta_t}(a^*)}.
% \end{align}
% \end{proof}


% \section{Two action no reward noise case}

% First we note that for two actions, we have 

% \begin{align}
%     P_t(a_1) =& \eta \pi_t(a_1)(r(a_1) - \pi_t(a_1) r(a_1) - (1-\pi_t(a_1)) r(a_2))\\ =& \eta \pi_t(a_1)(r(a_1) - r(a_2))(1-\pi_t(a_1))\\ 
%     =& \eta \Delta \pi_t(a_1)(1-\pi_t(a_1)).
% \end{align}

% For the noise term, we have 

% \begin{align}
%     W_t(a_1)/\eta =& (I_t(a_1) - \pi_t(a_1))r(a_t) - \pi_t(a_1)\Delta (1-\pi_t(a_1))\\
%     =& I_t(a_1)(1-\pi_t(a_1)) r(a_1) + (1-I_t(a_1))(-\pi_t(a_1))r(a_2) - \pi_t(a_1)\Delta (1-\pi_t(a_1))\\
%     =& I_t(a_1)(1-\pi_t(a_1))r(a_1) + I_t(a_1)(\pi_t(a_1) - 1)r(a_2) + (I_t(a_1) - \pi_t(a_1))r(a_2) - \pi_t(a_1)\Delta (1-\pi_t(a_1))\\
%     =& I_t(a_1)(1-\pi_t(a_1))\Delta+ (I_t(a_1) - \pi_t(a_1))r(a_2) - \pi_t(a_1)\Delta (1-\pi_t(a_1))\\
%     =& (I_t(a_1) - \pi_t(a_1))(1-\pi_t(a_1))\Delta+ (I_t(a_1) - \pi_t(a_1))r(a_2)
% \end{align}

% \begin{align}
%     \theta_{t+1}(a^*) - \theta_{t+1}(2) &= \theta_{t}(a^*) - \theta_{t}(2) + 2 \cdot \eta \cdot \pi_{\theta_t}(a^*) \cdot (1 - \pi_{\theta_t}(a^*)) \cdot \Delta
% \end{align}
% or
% \begin{align}
%     \log{\bigg( \frac{\pi_{\theta_{t+1}}(a^*)}{1 - \pi_{\theta_{t+1}}(a^*)} \bigg)} &= \log{\bigg( \frac{\pi_{\theta_{t}}(a^*)}{1 - \pi_{\theta_{t}}(a^*)} \bigg)} + + 2 \cdot \eta \cdot \pi_{\theta_t}(a^*) \cdot (1 - \pi_{\theta_t}(a^*)) \cdot \Delta
% \end{align}

