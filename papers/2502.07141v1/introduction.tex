\section{Introduction}

The stochastic gradient method has been ubiquitous in the field of machine learning for decades \citep{bottou2010large}. When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient is the well known policy gradient \citep{sutton1999policy} (or REINFORCE \citep{williams1992simple}) algorithm, where in each iteration an online %trajectory is sampled 
sample is gathered
using the current policy, from which a gradient estimate is obtained to conduct parameter updates. % to obtain gradient estimators and conduct gradient updates. 
In the simplest setting of a stochastic bandit problem \citep{lattimore2020bandit}, where decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to the stochastic gradient bandit algorithm \citep[Section 2.8]{sutton2018reinforcement}. Compared to other statistical methods, such as the upper confidence bound algorithm (UCB, \citep{lai1985asymptotically, auer2002finite}), and Thompson sampling (TS, \citep{thompson1933likelihood,agrawal2012analysis}), the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient, as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic gradient method is highly scalable and naturally applicable to large scale neural networks \citep{schulman2015trust,schulman2017proximal}.

However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently well established and comprehensive theoretical footing. Given its pervasive success and widespread application in RL \citep{schulman2017proximal} and fine-tuning for large language models \citep{ouyang2022training,rafailov2024direct}), it remains an important question to understand the success of stochastic gradient based algorithms in bandit-like settings, not only to bridge the gap between theory and practice, but also to identify more effective and robust variants. % for gradient based RL and policy optimization algorithms. 
In this paper, we make a significant contribution to the theoretical understanding of the stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but theoretically well established methods.
In particular, we establish the surprising result that:
%toward resolving this open question and bringing stochastic gradient bandit algorithms closer to the fold of these better understood methods on the theoretical front, by proving a surprising result that
\begin{center}
\emph{For \textcolor{red}{any constant learning rate} $\eta>0$, the stochastic gradient bandit algorithm is guaranteed to converge to the globally optimal policy almost surely.}
\end{center}
Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective of the value of this hyperparameter!
Analysis of this algorithm is challenging because it requires techniques for simultaneously handling non-convex optimization, stochastic approximation, and the exploration-exploitation trade-off. % at the same time.
Prior theoretical work on the stochastic gradient algorithm has primarily focused on
%While stochastic gradient has been primarily studied from the perspective of 
non-convex optimization and stochastic approximation, but understanding the simultaneous effect on exploration has been largely lacking.

Recently, significant progress has been made in establishing global convergence results for policy gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG converges to a globally optimal policy asymptotically as the number of iterations $t$ goes to infinity \citep{agarwal2021theory}. Subsequent work has demonstrated that the asymptotic rate of convergence is $O(1/t) $\citep{mei2020global}, albeit with problem and initialization dependent constants \citep{mei2020escaping,li2021softmax}. The rate and constant dependence in the true gradient setting have been improved via several techniques, including entropy regularization \citep{mei2020global}, normalization \citep{mei2021leveraging}, and using natural gradient (mirror descent) \citep{agarwal2021theory,cen2022fast,lan2023policy}.

Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using the current policy to collect samples, these accelerated methods all obtain worse asymptotic results than the standard Softmax PG \citep{mei2021understanding}, failing to converge to a global optimum without careful design choices~\citep{mei2022role}. Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest form, provided only that a sufficiently small learning constant rate $\eta \in \Theta(1)$ is used \citep{mei2024stochastic}.

For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost all current approaches, motivated by classical convergence analyses from stochastic optimization %techniques
\citep{robbins1951stochastic, ghadimi2013stochastic,zhang2020global,zhang2020sample,ding2021beyond,zhang2021convergence,yuan2022general,mei2024stochastic,denisov2020regret}. Stationary point convergence is guaranteed for learning rates sufficiently small with respect to the smoothness of the objective function, while also decaying to zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate control, many other techniques have been developed to control the effects of gradient noise, including regularization \citep{zhang2020sample,ding2021beyond}, variance reduction \citep{zhang2021convergence}, and carefully considering growth conditions \citep{yuan2022general,mei2024stochastic}.

The technical challenges we face in the current study can be understood in the following aspects: \textbf{(1)} Using an arbitrarily large constant learning rate for online stochastic gradient optimization immediately renders the smoothness and noise control techniques mentioned above inapplicable.
\textbf{(2)} With any constant learning rate $\eta>0$, the question of whether oscillation or convergence will ultimately occur needs to be addressed before even considering whether any convergence is to a global optimum.  This additional level of complexity arises because the optimization objective is not necessarily improved monotonically in expectation. Finally, 
\textbf{(3)} The gradient bandit algorithm does not use any exploration bonus, which means that new techniques are required to demonstrate that it adequately balances the exploration-exploitation trade-off.

In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of stochastic gradient when using any constant learning rate.  In particular, we establish the following.
\begin{itemize}[leftmargin=16pt, nosep]
    \item[\textbf{(i)}] In the stochastic online setting, with probability $1$, the stochastic gradient bandit algorithm will not keep sampling any single action forever, implying that it will exhibit a minimal form of exploration without any further modification. This asymptotic event (as $t \to \infty$) happens with probability $1$ and holds for any constant learning rate $\eta>0$.
    \item[\textbf{(ii)}] This result can then be leveraged to show that, as a consequence, given any constant learning rate, the stochastic gradient bandit algorithms will converge to the globally optimal policy as $t \to \infty$, with probability $1$. That is, the probability of sub-optimal actions decays to $0$, even though some of them are taken infinitely often asymptotically.
    %\item[\textbf{(iii)}] Finally, we establish that the stochastic gradient bandit algorithm with any constant learning rate achieves an $O(1/t)$ rate of convergence, which cannot be improved in terms of $t$.
    %\item[\textbf{(iv)}] \textcolor{red}{oscillation-rate trade-off for using very large learning rates?}
\end{itemize}




