\section{Related work}
\label{sec:related_work}
\noindent \textbf{Automatic Wikipedia article enhancement}: Automatic Wikipedia enhancement has been studied for more than a decade~\textbf{Bordes, Weston, Chopra, Mikolov, and Ranzato, "Question Answering with Structured Representations of Knowledge"}__. In recent times, ~\textbf{Lewis, Pagliardini, and Sappelli, "Pre-train, Prompt, Learn: AutoML for Automatic Question Answering"}__ leveraged RAG to create full length Wikipedia articles.\\ %Works such as  focuses on extending Wikipedia section from the available contents from the web, while other line of works ~\textbf{Dai et al., "Leakage-Resilient and Privacy-Preserving Knowledge Graph Completion via Adversarial Training"}__ concentrate on creating full length Wikipedia page. Many of the works rely on Information Extraction to enhance Wikipedia content. ~\textbf{Wang, Xu, Li, Feng, and Li, "Structure-Aware Wikipedia Article Generation with Retrieval-Augmented Generation"}__ propose a structure aware method to produce Wikipedia documents given relevant articles. They first cluster all headings in the document to construct the templates then retrieve result pages from the internet. This approach requires a set of in-domain documents as prerequisites to constitute the template, thereby rendering it unsuitable for addressing emerging topics. 
\noindent\textbf{Grounded content generation using RAG}: Augmenting language models (LMs) with retrieval at inference time is a typical way to leverage external knowledge stores~\textbf{Lewis et al., "Pre-train, Prompt, Learn: AutoML for Automatic Question Answering"}__. While some works use retrieval to construct demonstrations for in-context learning~\textbf{Tan and Liu, "Prefix-Tuning with Adversarial Examples for Text Classification"}__, others~\textbf{Zhou, Sun, Li, Deng, and Wang, "Retrieval-Augmented Language Model Pre-Training"}__ use retrieval to provide additional information for LMs to ground on.
While RAG is widely studied in question answering, how to use it for expanding a Wikipedia section is less investigated.\\
\noindent \textbf{Present work}: Although, there are several lines of work which are related to ours, none of them leverage personal narratives to improve Wikipedia articles. We carefully curate a set of autobiographies/biographies and develop algorithms so that the generated content is grounded on these narratives. In specific, we use a two-stage RAG pipeline for enhancing Wikipedia tail articles and outperform the most competing baseline.
% As a general framework, RAG is flexible in both the retrieval source and time. The retrieval sources can vary from domain databases~\textbf{Bajaj et al., "Domain Adaptation with Meta-Learning"}__, code documentation~\textbf{Kochenderfer and Chakraborty, "Automated Code Documentation using Deep Learning"}__, to the whole Internet~\textbf{Zhang, Chen, Zhang, Li, and Wang, "Internet-Sourced Information Retrieval for Question Answering"}__. Regarding the time, %
% besides a one-time retrieval before generation, the system can be designed to self-decide when %
% to retrieve across the course of the generation~\textbf{Vinyals et al., "Learning Long-Term Dependencies via Gradient-Based Meta-Learning"}.

% \noindent
% \textbf{Question Asking in NLP}\quad
% Question asking capabilities in NLP systems have expanded across several fronts, including generating clarification questions to understand user intents~\textbf{Zhou et al., "Improving Question Answering with Clarification Questions and Multi-Task Learning"}__, and breaking large questions into smaller ones to improve compositional reasoning~\textbf{Huang et al., "Breaking Large Questions into Smaller Ones for Compositional Reasoning"}__. While humans usually ask questions to learn new knowledge~\textbf{Lee, "Question Asking in Humans: A Study on Question-Answering Systems and Human Learning Behavior"}__, how to optimize question informativeness and specificity in information-seeking conversations remains less explored. The closest work is~\textbf{Zhang et al., "Defining Informativeness of Questions Using Unigram Precision Function for Information Seeking Conversations"} which defines the question informativeness using the unigram precision function and uses reinforcement learning to increase the question informativeness.

% \noindent \textbf{Grounded content generation}
% WebBrain ~\textbf{Wang et al., "WebBrain: A System for Grounding Content Generation on Web Corpus and Producing Short Factual Articles"}__ leverages a more complex system to ground on web corpus and produce short factual articles to answer queries.