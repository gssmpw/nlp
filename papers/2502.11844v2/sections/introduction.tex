\input{figures/intro/intro_fig.tex}
\section{Introduction} \label{sec:intro}

Automating software development is a key aspirational goal of Large Language Models (LLMs), promising to revolutionize the software industry \citep{lyu2024automatic}.
They have shown impressive capabilities in assisting developers by generating function-level completions \citep{humaneval,austin2021program_mbpp}, suggesting code patches \citep{swebench}, and solving algorithmic problems \citep{apps}. However, it remains unclear if LLMs are ready to autonomously generate larger-scale, deployment-ready code.

\paragraph{The Gap in LLM Code Benchmarking}
This gap in understanding LLMs' capabilities is also reflected in the current state of LLM benchmarking.
Namely, most current coding benchmarks assess LLMs' capabilities at function-level code writing and bug fixing \citep{humaneval,austin2021program_mbpp,muennighoff2023octopack}, or focus on specific domains such as algorithmic tasks or unit tests \citep{apps,mundler2024swtbench}.
Due to their simplicity, standard code benchmarks are becoming saturated quickly, with latest models, \eg \claudesonnet{} surpassing $92\%$ on \textsc{HumanEval} \citep{humaneval,anthropic2025claude35}.
On the other end, recent and more challenging benchmarks, \eg \textsc{SWE-Bench}~\citep{swebench}, target agentic systems built on top of LLMs and simultaneously test capabilities that are often orthogonal to their code generation capabilities, \eg tool use or relevant context retrieval.
Another key angle not captured by current coding benchmarks for functional correctness is the security of the generated code---a crucial prerequisite before LLM-generated code can be deployed in the real world.
On the other hand in code security evaluations, correctness and security are often measured on separate tasks \citep{pearce2022asleep,cyberseceval,safecoder,jenko2024practicalattacksblackboxcode}. Even if both aspects are considered on the same tasks, they remain restricted to individual functions \citep{seccodeplt,cweval}. 
This highlights the need for more challenging coding-focused benchmarks that model the realistic and complex task of generating correct and secure, deployment-ready code. 

\paragraph{\benchmark{}: Correct \& Secure Backends}
To bridge this gap in LLM-generated code benchmarking, we introduce \benchmark{}, a novel benchmark that tests the capability of LLMs to generate correct and secure backends. 
As the key component of modern web and cloud applications, backends represent a realistic target for the generation of challenging standalone modules.
Crucially, as the role of backends is to serve requests from potentially untrusted users, security is inherently critical. 
A single exploit can affect all users of the application, irrespective of their client-side setup. 
Consequently, \benchmark{} collects $28$ challenging backend scenarios, which are to be implemented in $14$ backend development frameworks across $6$ programming languages.
Combined, this results in $392$ challenging benchmark tasks, each requiring the LLM to fully implement a \emph{correct} and \emph{secure} backend application exposing API endpoints with specific functionalities.

To evaluate {correctness}, as part of each scenario, we include a suite of functional tests that the generated backend must pass.
Modeling real-world deployment, we approach {security} evaluation through the lens of untrusted users that run malicious queries against the API in order to expose vulnerabilities in the generated code. The success of any such malicious query \emph{guarantees} that the backend is insecure and would pose severe risks in deployment.
For each scenario, these exploits are developed by code security experts. To achieve high coverage of potential security threats, the exploits were iteratively refined on both LLM-generated and human-written solutions.
Notably, both the correctness and the security tests are agnostic to frameworks and programming languages, relying only on the API exposed by the backend.
This enables the testing of the generated code independently of implementation details beyond the exposed functionalities, reflecting a real-world setting.

\input{figures/overview/overview.tex}
\cref{fig:overview} provides an overview of \benchmark{} and a shortened example---the LLM is tasked to implement a calculator app (\emph{scenario}), exposing a compute endpoint in Python-Django (\emph{framework}). Then, the LLM's implementation is served in an isolated environment and the exposed API is tested for functional correctness and vulnerabilities. Importantly, \benchmark{} tests multiple potential vulnerabilities for each task, \eg CWEs 400 and 94 in our example.

\paragraph{Flagship LLMs Struggle}
We perform an extensive evaluation of $11$ state-of-the-art LLMs on \benchmark{}, including reasoning models, such as \openaiothree{}~\citep{o3minisystemcard} and \dsro{}~\citep{guo2025deepseek}.
As shown in~\cref{fig:intro_fig}, even flagship LLMs struggle to generate deployment-ready backends, not surpassing a mere $35\%$ correct and secure generation rate on \benchmark{}.
But security is not the only challenge that \benchmark{} poses to the models, even only in terms of functional correctness, the models struggle to fulfill the task in $\sim$$40\%$ of the cases.
These findings suggest that LLMs are not yet ready to autonomously tackle practical coding tasks, and once more highlight the importance of security in capability benchmarking \citep{pearce2022asleep,sven}.

\paragraph{Outlook}
We plan to release \benchmark{} to the community as a modular framework, easily extendable with new and more challenging tasks, enabling the continuous evaluation of future LLMs on deployment-ready code generation.

\paragraph{Key Contributions}
\begin{itemize}
    \item We introduce \benchmark{} (\cref{sec:method}), a novel benchmark that tests the LLMs' ability of end-to-end generation of deployment-ready backends, taking into account both functionality and security. \benchmark{} contains $392$ tasks, which specify $28$ challenging scenarios across $14$ important backend frameworks (\cref{sec:dataset_statistics}).
    \item We thoroughly evaluate $11$ state-of-the-art LLMs on \benchmark{}, assessing the generated code with functional tests and security exploits (\cref{sec:eval}), and find that all models struggle to generate correct and secure backend code.
    \item We perform a detailed study of models' performance, including the influence of security-specific prompting, scenario complexity, and backend framework choice on code correctness and security (\cref{sec:eval}).
\end{itemize}
 
