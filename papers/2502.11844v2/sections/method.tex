\section{Construction of \benchmark{}}
\label{sec:method}

In this section, we describe the structure and the construction process of \benchmark{}.
Starting from a broad overview, we proceed to discuss the individual components of \benchmark{} in detail, namely its scenarios, including the corresponding functionality tests and security exploits (\cref{ssec:method:scenarios}), and our task creation and execution process, including our choice of frameworks and evaluation metrics (\cref{ssec:method:tasks}).

\paragraph{Overview}
\benchmark{} contains $28$ \emph{scenarios}, each specifying the functionality of a backend application that is to be implemented.
Each scenario comes with a set of functional tests and security exploits used to test LLM-generated solutions.
Further, for implementing these scenarios, we select $14$ diverse \emph{frameworks} from $6$ programming languages.
Scenarios and frameworks are combined into \emph{tasks}, leading to a total of $392$ different challenging evaluation tasks, forming \benchmark{}. 

For each evaluation task, the model is prompted with the scenario specification and asked to generate application code in the target framework.
We run the resulting code inside an isolated Docker container, exposing its endpoints to the functional tests and security exploits of the scenario to test the correctness and security of the application.
Each application has access to the local file systems and may use an SQLite database to hold its state.
We finally test for passwords, unencrypted secrets, or artifacts of the executed exploits by inspecting the files in the execution environment.
In \cref{appendix:full_example} we provide a complete example of a task specification, example output by \qwenst{}, and the execution traces of the functional tests and security exploits.

\subsection{Scenarios} \label{ssec:method:scenarios}
Each scenario consists of a specification of the desired API endpoints, a corresponding plaintext description, and a set of framework-agnostic functional tests and security exploits.
Following real-world software development practices, the scenarios are specified in a unified way in the OpenAPI~\citep{openapi} format, describing the functionality, request format, and response parameters for each API endpoint.
Additionally, these specifications are transcribed into plaintext instructions for each scenario.
However, as we show in~\cref{sec:eval}, compared to plaintext instructions, providing models with the OpenAPI specifications makes the task of generating correct applications significantly less error-prone.

To select scenarios that reflect relevant use cases in terms of both functionality and security, we define four criteria. Each scenario should:
(i) represent a backend application that often occurs in real-world software development; 
(ii) have sufficient implementation complexity over existing function-level benchmarks; 
(iii) describe an application with potential security vulnerabilities;
and (iv)  be realizable correctly and securely in existing backend frameworks.

Guided by this, we filtered an initial set of proposed scenarios, and manually verified that the final set of $28$ scenarios meets the above criteria. The list of the final scenarios together with a short description and a list of each of their potential security vulnerabilities is included in~\cref{tab:scenarios} in \cref{appendix:infotables}.
Next, we describe the construction of functional and security tests in our scenarios in more detail.

\paragraph{Functional Tests}
Following industry-standard practices, and in line with prominent code functionality benchmarks \citep{humaneval,swebench}, we evaluate the correctness of LLM-generated applications using functional tests. 
These tests verify the end-to-end functionality of each endpoint of the backend application as described by the OpenAPI specification of the scenario. 
As the specifications are given on the API level, all our tests are framework-agnostic, and can be directly reused across different \benchmark{} tasks that use the same scenario. 
This modularity is a key advantage of \benchmark{}, as it enables the addition of future frameworks without needing to adjust the functional tests.
Our functional tests are created manually, and verified by running them on human-reviewed solutions to the benchmark tasks.

\paragraph{Security Evaluation}
Prior works often resort to static analyzers to measure security (\eg \citet{codeguard} or \citet{safecoder}), but such tools have several major limitations.
First, they are plagued both by false positives and false negatives \citep{barrierssast,sastvsllm,ami2024false}.
Second, they are often only available as a paid service, and as such limit reproducibility in the context of an open-source benchmark \citep{cyberseceval,sastvsllm,snykcode}.
Finally, to be applicable, they need to explicitly include support for a specific programming language and framework \citep{barrierssast,sastvsllm,ami2024false}.
Indeed, empirical studies of static analyzers have shown that detection rates vary significantly between vulnerabilities, languages, and frameworks, with entire classes of issues remaining completely undetected by static analysis~\citep{li2024llmsast, sastvsllm}.

With this in mind, we opted for a different approach to evaluate the security of LLM-generated solutions in \benchmark{}---using expert-written security exploits.
In contrast to static analyzers, this approach (i) provides a sound upper bound for security, (ii) is reproducible, and (iii) is framework-agnostic. 
Further, this approach is in line with recent~\citep{seccodeplt} and concurrent~\citep{cweval} code security benchmarks.
However, as our focus on complete backend applications is more accurately modeling industry practices, our exploits have direct real-world security implications.

Finally, we note that static analysis can reason about all possible execution paths of an application~\citep{CousotAbstractInterpretation1977}, while dynamic testing may leave certain code paths unexplored.
However, in our setting, the security exploits are derived directly from the application specification, which means they target concrete deployment-time vulnerabilities rather than abstract program states, making the theoretical completeness guarantees of static analysis less relevant.

\paragraph{Security Exploit Construction Process}
To create the security exploits for each \benchmark{} scenario, we start from a manually written set of suspected possible exploits.
These are further extended and adjusted by inspecting \gptfo{}-generated~\citep{gptfo} and human-written sample solutions, both manually and using a SaaS security analyzer, Snyk-Code~\citep{snykcode}.
Just as for functional tests, the security exploits are framework-agnostic.
We distinguish two exploit types: (i) black-box exploits, which only use the API endpoints exposed by the application, \eg path traversal or command code injection attacks, and (ii) white-box-like exploits, where artifacts created by the application are extracted from the execution environment and inspected, \eg password dictionary attacks on databases.
In particular, for white-box-like exploits, if a scenario requires a database, we specify the location of an SQLite database instance in the model prompt.
Then, after security testing, we perform a full scan of all tables in this database to detect any improperly stored sensitive data.
We provide a detailed overview of the security threats covered by \benchmark{} in~\cref{tab:cwes} (\cref{appendix:infotables}), and an example exploit in \cref{appendix:full_example}.
 
\subsection{Constructing and Evaluating Tasks}
\label{ssec:method:tasks}
\input{tables/frameworks.tex}
\benchmark{} tasks are constructed by instructing the implementation of a given scenario in a target backend framework.
As the scenarios themselves are framework-agnostic, they can be combined with framework of choice.
This, for the first time, enables the comprehensive and rigorous evaluation of different frameworks' impact on the correctness and security of LLM-generated code (\cref{sec:eval}).

\paragraph{Frameworks}
To realistically reflect the real-world diversity of backend applications in terms of implementation tools, and to allow for the evaluation of LLMs on their proficiency in frameworks with varying training data, we select a diverse mix of popular and more niche frameworks. For this, we orient ourselves by the StackOverflow Developer Survey \citep{stackoverflowsurvey} and the number of GitHub stars of each framework.
We provide an overview of all frameworks included in \benchmark{} in \cref{tab:frameworks}.

\paragraph{Evaluation Pipeline}
Each task in \benchmark{} is a combination of a scenario and a framework.
The LLMs are prompted with scenario specifications in OpenAPI format, and with the programming language and available packages defined by the framework. Our evaluation prompt templates are included in \cref{appendix:prompts}.
Next, we evaluate the LLM-generated code for correctness and security using the above tests and exploits.
In line with other advanced coding benchmarks \citep{redcode,swebench,mundler2024swtbench}, each test/exploit is executed in a Docker environment. 
This enables the reproducibility of the results, and ensures that the security exploits on the LLM-generated code cannot harm the benchmarking infrastructure.


