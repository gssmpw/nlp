\section{Related Work}

Below, we discuss works related to \benchmark{}.

\paragraph{Benchmarking Correctness}
Researchers have proposed various benchmarks to evaluate LLMs in generating functionally correct code.
Earlier benchmarks, such as HumanEval~\citep{humaneval}, MBPP~\citep{mbpp}, and APPS~\citep{apps}, focus on the task of generating short, algorithmic programming tasks.
More recently, several benchmarks have been developed to study more nuanced, complex scenarios.
These include domain-specific benchmarks, such as DS-1000~\citep{dsonek} for data science and Sketch2Code~\citep{sketch2code} for web frontends.
ODEX~\citep{odex} and BigCodeBench~\citep{bigcodebench} offer a more open-domain assessment by incorporating different libraries and applications.

However, all these benchmarks focus only on front-end designs or few-line, at most single-function tasks, void of a contextualizing application (in contrast to the focus on entire backend applications in \benchmark{}), and do not conduct security evaluations.
Therefore, \benchmark{} complements these benchmarks and can provide significant value to the community.
SWE-Bench~\citep{swebench} and RepoBench~\citep{repobench} focus on generating code edits or snippets given a repository context.
In contrast, \benchmark{} targets complete app generation from scratch.

\paragraph{Benchmarking Security}
While the primary focus of evaluating LLM-based code generation is on functionality, several benchmarks have been developed to assess security.
Notable among these are AsleepAtKeyboard~\citep{asleep}, SecurityEval~\citep{securityeval}, SafeCoder~\citep{safecoder}, CodeLMSec~\citep{codelmsec}, CyberSecEval~\citep{cyberseceval}, CodeGuard+~\citep{codeguard}, SecCodePLT~\citep{seccodeplt}, and CWEval~\citep{cweval}.

\benchmark{} stands apart from these benchmarks in three key ways.
First, the construction of \benchmark{} adopts a top-down approach by starting with real-world end-to-end coding scenarios, and then identifying potential CWEs in the generated code, often multiple per scenario.
In contrast, existing benchmarks are built with a bottom-up approach that crafts less realistic coding tasks around individual CWEs.
Second, \benchmark{} is more complex, as it evaluates code generation involving multiple functions and files, whereas prior benchmarks typically deal with single-function outputs.
Third, \benchmark{} has a specialized in-depth emphasis on backend applications, where the requirement of secure implementations is self-evident.


Secure code generation is not the only aspect of LLM evaluation in the context of cybersecurity. 
Other benchmarks focus on evaluating LLMs' cybersecurity capabilities on tasks that are orthogonal to ours. 
RedCode~\citep{redcode} studies the generation of code with malicious intent to exploit other users, and NYU CTF~\citep{nyuctf} and Cybench~\citep{cybench} evaluate LLMs on generating security exploits given vulnerable software.
