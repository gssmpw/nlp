\section{Additional Results}
\label{appendix:additional_results}
We present all additional results omitted from the main paper.
In~\cref{appendix:passfive} we extend our main results with the \passk{5} metric.
In~\cref{appendix:cwe_ocurence} we provide a report on the occurrence of CWEs in LLM-generated code.
In \cref{appendix:model_performance_across_scenarios} and \cref{appendix:model_performance_across_frameworks} we provide additional visualizations of the model performances across scenarios and frameworks, respectively.

\input{figures/pass_5_none/pass_5_none.tex}
\input{figures/pass_5_generic/pass_5_generic.tex}
\input{figures/pass_5_specific/pass_5_specific.tex}
\subsection{Pass@5 and SecPass@5}
\label{appendix:passfive}

Here, we present \passk{5} and \secpassk{5} results on \benchmark{}. First, we introduce this metric:

\paragraph{The Pass@k Metric}
To measure the overall performance of a given model when $k$ samples are allowed to be taken, the standard metric is the \passk{k}. 
This metric measures the likelihood that if the model has $k$ tries at solving a given task, it will succeed at least once (\ie pass all functional tests).
We use a low-variance unbiased estimator for calculating \passk{k} across a dataset of tasks, as introduced by \citet{humaneval}:
\begin{equation}
\label{eq:passk}
    \text{\passk{k}} \coloneqq \E_{\text{Tasks}} \left [ 1 -  \frac{\binom{n-c}{k}}{\binom{n}{k}} \right ],
\end{equation}
where $n$ denotes the number of solutions sampled from the model for a given task and $c$ denotes the number of correct solutions in those $n$ samples.

To measure security exposure, we use the \secpassk{k} metric, introduced by \citet{codeguard}. Namely, we reuse \cref{eq:passk}, but set $c$ to the count of solutions that both pass \emph{all} functional tests and are not compromised \emph{by any} of our security exploits.
This reflects real-world usages of generate code---security is concerned only if the generated code is functionally correct and will thus be incorporated into the codebase.
Our measured \secpassk{k} provides a strict \emph{upper bound} on the true \secpassk{k} of the model, \ie the real performance of the models can only be \emph{worse} than the already low number reported in \benchmark{} in \cref{sec:eval}.
This is because, while unlikely, the model generated code could contain vulnerabilities not covered by our exploits.

\paragraph{Results}
We extend our main results in~\cref{fig:main_results} with the \passk{5} (and the corresponding \secpassk{5}) metric, showing it alongside the \passk{1} and \secpassk{5} metrics for all three prompting types in~\cref{fig:pass_5_none,fig:pass_5_generic,fig:pass_5_specific}.
Note that we do not include the reasoning models, \openaiothree{}, \openaione{}, and \dsro{}, as due to computational (time and cost) and technical constraints (recurring unavailability of the APIs), these models were run only once per task, instead of the usual $10$ times. 
This does not enable the calculation of the \passk{5} and \secpassk{5} metrics reliably.

\subsection{CWE Occurrence} 
\label{appendix:cwe_ocurence}

Next, we provide a detailed report on the occurrence of CWEs in LLM-generated code.
For each scenario (\cref{tab:non:cwes_scenario,tab:gen:cwes_scenario,tab:sec:cwes_scenario}), framework (\cref{tab:none:cwes_env,tab:gen:cwes_env,tab:sec:cwes_env}), and model (\cref{tab:non:cwes_model,tab:gen:cwes_model,tab:sec:cwes_model}), we report the ratio of:
\begin{itemize}
    \item the number of model-generated backends that pass all functional tests and have a specific CWE, and 
    \item the number of model-generated backends that pass all functional tests and \textbf{could} have this CWE, per \cref{tab:scenarios}.
\end{itemize}
We present the results in $9$ tables, in which ``$/$'' indicates that no code for this scenario/framework/model could have the corresponding CWE, while $0.00$ indicates that no backends have it (or very few, as the ratios are rounded to $2$ digits). 

\input{tables/none_cwes_scenario}
\input{tables/generic_cwes_scenario.tex}
\input{tables/specific_cwes_scenario}
\input{tables/none_cwes_env}
\input{tables/generic_cwes_env.tex}
\input{tables/specific_cwes_env}
\input{tables/none_cwes_model}
\input{tables/generic_cwes_model.tex}
\input{tables/specific_cwes_model}

\clearpage
\subsection{Model Performance across Scenarios}
\label{appendix:model_performance_across_scenarios}
In \cref{fig:scenario_per_model_o1,fig:scenario_per_model_o3,fig:scenario_per_model_deepseek-ai-DeepSeek-R1,fig:scenario_per_model_gpt-4o,fig:scenario_per_model_claude-3-5-sonnet-latest,fig:scenario_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo,fig:scenario_per_model_deepseek-ai-DeepSeek-V3,fig:scenario_per_model_Qwen-Qwen2.5-Coder-32B-Instruct,fig:scenario_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo,fig:scenario_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo,fig:scenario_per_model_mistralai-codestral-2501}, we show the per-scenario breakdown of the \passk{1} and \secpassk{1} scores of each of the $11$ models used in our evaluation, in all three prompt settings.

\subsection{Model Performance across Frameworks}
\label{appendix:model_performance_across_frameworks}
Complementing the \openaione{} results in~\cref{fig:env_per_model_o1} shown in~\cref{sec:eval}, in \cref{fig:env_per_model_o3,fig:env_per_model_deepseek-ai-DeepSeek-R1,fig:env_per_model_gpt-4o,fig:env_per_model_claude-3-5-sonnet-latest,fig:env_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo,fig:env_per_model_deepseek-ai-DeepSeek-V3,fig:env_per_model_Qwen-Qwen2.5-Coder-32B-Instruct,fig:env_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo,fig:env_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo,fig:env_per_model_mistralai-codestral-2501}, we show the per-framework breakdown of the \passk{1} and \secpassk{1} scores of each of the other $10$ models used in our evaluation, in all three prompt settings.

\input{figures/scenario_per_model/scenario_per_model_o1/scenario_per_model_o1.tex}
\input{figures/scenario_per_model/scenario_per_model_o3-mini/scenario_per_model_o3-mini.tex}
\input{figures/scenario_per_model/scenario_per_model_deepseek-ai-DeepSeek-R1/scenario_per_model_deepseek-ai-DeepSeek-R1.tex}
\input{figures/scenario_per_model/scenario_per_model_gpt-4o/scenario_per_model_gpt-4o.tex}
\input{figures/scenario_per_model/scenario_per_model_claude-3-5-sonnet-latest/scenario_per_model_claude-3-5-sonnet-latest.tex}
\input{figures/scenario_per_model/scenario_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo/scenario_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo.tex}
\input{figures/scenario_per_model/scenario_per_model_deepseek-ai-DeepSeek-V3/scenario_per_model_deepseek-ai-DeepSeek-V3.tex}
\input{figures/scenario_per_model/scenario_per_model_Qwen-Qwen2.5-Coder-32B-Instruct/scenario_per_model_Qwen-Qwen2.5-Coder-32B-Instruct.tex}
\input{figures/scenario_per_model/scenario_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo/scenario_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo.tex}
\input{figures/scenario_per_model/scenario_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo/scenario_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo.tex}
\input{figures/scenario_per_model/scenario_per_model_mistralai-codestral-2501/scenario_per_model_mistralai-codestral-2501.tex}


\input{figures/env_per_model/env_per_model_o3-mini/env_per_model_o3-mini.tex}
\input{figures/env_per_model/env_per_model_deepseek-ai-DeepSeek-R1/env_per_model_deepseek-ai-DeepSeek-R1.tex}
\input{figures/env_per_model/env_per_model_gpt-4o/env_per_model_gpt-4o.tex}
\input{figures/env_per_model/env_per_model_claude-3-5-sonnet-latest/env_per_model_claude-3-5-sonnet-latest.tex}
\input{figures/env_per_model/env_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo/env_per_model_meta-llama-Llama-3.3-70B-Instruct-Turbo.tex}
\input{figures/env_per_model/env_per_model_deepseek-ai-DeepSeek-V3/env_per_model_deepseek-ai-DeepSeek-V3.tex}
\input{figures/env_per_model/env_per_model_Qwen-Qwen2.5-Coder-32B-Instruct/env_per_model_Qwen-Qwen2.5-Coder-32B-Instruct.tex}
\input{figures/env_per_model/env_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo/env_per_model_Qwen-Qwen2.5-72B-Instruct-Turbo.tex}
\input{figures/env_per_model/env_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo/env_per_model_Qwen-Qwen2.5-7B-Instruct-Turbo.tex}
\input{figures/env_per_model/env_per_model_mistralai-codestral-2501/env_per_model_mistralai-codestral-2501.tex}
