\section{Evaluation}
\label{sec:eval}

\paragraph{Experimental Setup}
We test $11$ state-of-the-art LLMs on \benchmark{}: \mbox{\openaione{}}~\citep{jaech2024openai}, \openaiothree~\citep{o3minisystemcard}, \gptfo{}~\citep{gptfo}, \claudesonnet{}~\citep{claudesonnet}, \dsro{}~\citep{guo2025deepseek}, \dsvt{}~\citep{liu2024deepseek}, \codestral{}~\citep{codestral}, \qwencoder{}~\citep{hui2024qwen2}, \llamat{}~\citep{dubey2024llama}, \qwenst{}~\citep{yang2024qwen2}, and \qwens{}~\citep{yang2024qwen2}---6 providers, 4 closed-source, and 7 open-source models. For each task, we sample $10$ solutions from all non-reasoning models at temperature $0.4$. For the reasoning models, \openaione{}, \openaiothree{}, and \dsro{}, we sample only $1$ solution, as they are both cost and time-intensive to evaluate.
We use temperature $0$ for \dsro{}, while for \openaione{} and \openaiothree{}, there is no modifiable temperature parameter.

The functionality instructions are provided as OpenAPI specifications. We show the advantage of these exact specifications against plaintext descriptions in a separate experiment, justifying our choice.
Following prior work \citep{humaneval,codeguard}, we measure the models' performance using the \passk{k} and \secpassk{k} metrics, with $k=1$ in the main paper.
These metrics measure the ratio of correct (\emph{all tests passed}), and correct and secure (\emph{all tests passed and no exploits succeeded}) programs across all generated solutions, respectively.
We introduce these metrics for generic $k$ in \cref{appendix:passfive}, and show experimental results on $k=5$.

\input{figures/main/main_fig.tex}
\paragraph{Main Results}
In \cref{fig:main_results}, we show each model's mean performance on \benchmark{}. Full \textcolor{mydarkred}{red} bars represent \secpassk{1} scores, which are extended in a lighter shade by the passing but incorrect programs of each model to show the \passk{1} score. First, we can observe that the benchmark is challenging even in terms of just functional correctness. \openaione{}, which has achieved impressive results on other coding benchmarks~\citep{jaech2024openai}, only scores $60\%$ \passk{1}. Further, a large portion of the correct solutions most models generate are insecure, posing a high risk if these backends were to be put into production.
Remarkably, the best-performing model in terms of functional correctness is not the best performer in terms of security. In fact, even three models outperform \openaione{} in terms of \secpassk{1}, \openaiothree, \claudesonnet{}, and \dsro{}, with \openaiothree{} achieving a $6\%$ higher score than \openaione{}.


\paragraph{Prompting for Security}
Next, we examine the impact of potential security-specific instructions in the prompt. For this, we define three different prompts: (i) a prompt without any security reminder, \ie the prompt used before; (ii) a prompt with a generic security reminder, where the model is instructed to follow security best practices; and (iii) a prompt with an unrealistic oracle security reminder, where the developer anticipates all the security vulnerabilities associated with the scenario and gives specific instructions to avoid them.
We show our results on a select set of top-performing models in \cref{fig:safety_prompting}.
We can see that while the unrealistic oracle-based security prompt leads to the highest \secpassk{1} score in all models, it generally decreases the overall number of passing programs, indicating that generating secure solutions is a complex task.
\input{figures/safety_prompting/safety_prompting.tex}
Note that obtaining the oracle knowledge for the third prompt type is highly non-trivial, and often impossible a priori in practice. Thus, we include this prompt type only to gain an understanding of the upper bound on the achievable security performance solely through prompting.
Notably, the three examined reasoning models, \openaione{}, \openaiothree{}, and \dsro{} show considerable improvement already on just the generic security reminder, while the non-reasoning models do not exhibit a significant improvement---signifying that strong reasoning capabilities are crucial for anticipating the often complex security vulnerabilities.

\paragraph{Impact of the Backend Framework}
In \cref{fig:env_per_model_o1}, we show the performance of \openaione{} across frameworks using all prompt types, and include such results on other models in \cref{appendix:model_performance_across_frameworks}.
We can observe that the chosen framework has a significant impact on both the correctness and the security of the generated backends across all prompt types.
This variation is strongly correlated with the popularity of the programming language and the complexity of the framework, with models achieving higher performance on frameworks of more popular languages (\eg Python or JavaScript) and struggling more with lower-resource and complex frameworks, such as Rust-Actix or PHP-Lumen. Crucially, in these frameworks, the models do not only struggle to produce functionally correct code, but even the few correct solutions they produce contain a higher share of vulnerabilities.
This result highlights that further progress is needed before current LLMs can be applied to security-critical coding tasks requiring the use of specific frameworks.

\input{figures/env_per_model/env_per_model_o1/env_per_model_o1.tex}
\paragraph{Differences Across Scenarios}
Next, we investigate the models' performance depending on each scenario.
We show per-scenario breakdowns of the \passk{1} and \secpassk{1} scores of each model on all prompts in \cref{appendix:model_performance_across_scenarios}.
We observe that for certain scenarios, \eg Logger or Forum, security reminders have a decisive impact, steering models that produce a high rate of insecure solutions towards outputting almost only secure solutions. In such cases, the models are primarily failing to pay attention to security aspects when not explicitly instructed to do so, but are otherwise capable of a secure implementation. This indicates that before LLMs can be integrated into production coding pipelines, security has to become an explicit development objective in addition to correctness.

We also observe large variations in functional correctness depending on the scenario.
To have a better understanding of the complexity of scenarios, in \cref{fig:scatter_main} we plot the \passk{1} of each scenario (averaged across all models and frameworks) against the number of tokens in the OpenAPI specification of that scenario (using \gptfo{}'s tokenizer). We observe a distinct correlation between the size of the OpenAPI specifications describing the endpoints of the backend and how difficult it is for models to generate the backend code. However, there are outlier scenarios with short specifications and only a few endpoints that models strongly struggle with. This indicates that \benchmark{} has both scenarios that are challenging due to many interacting endpoints, but also some that are challenging due to the complexity of the logic these endpoints individually require.

\input{figures/scatterplots/func_prompt_pass1_vs_api_len.tex}
\paragraph{Added Complexity of Security}
Exploiting the fact that \benchmark{} does not constrain the coding task to narrow, few-line contexts, we investigate the added complexity of security considerations in the generated solutions.
For this, we calculate the ratio of the average number of tokens of \emph{correct but exploitable solutions} and the average number of tokens of \emph{correct not-exploited solutions}.
We do this for each model and task, skipping tasks where a given model does not generate at least one of both of these solution types.
Averaging this ratio across all models and tasks, we find that security adds $5.9\%$ complexity in terms of the number of tokens in the generated solutions.
This complexity overhead of security is relatively consistent across models.
The only strong outlier is \claudesonnet{}, which finds secure solutions with a smaller token overhead of only $3.5\%$.

The overhead also varies across frameworks and scenarios.
Discarding frameworks where only a few samples could be found, Go-Gin and JavaScript-Express add considerable implementation overhead for secure solutions, with an average increase in token length of around $10\%$.
In contrast, the Python aiohttp framework adds a mere $0.9\%$.
Certain scenarios also induce high overhead. For instance, Calculator ($15.1\%$)---which takes an arithmetic expression from a user as a string and returns the result---can be easily implemented in most languages by evaluating the expression as a program (\texttt{eval}(\texttt{expression})). However, this is highly insecure, as the user could send executable malicious code that the server then evaluates. To avoid this, the server must add sanitization and safety checks before evaluating the expression, which adds considerable implementation overhead. We show this effect in a concrete case study on the Calculator scenario in \cref{appendix:full_example}.

\input{tables/text_vs_openapi.tex}
\paragraph{Plaintext Prompt vs. OpenAPI Specification}
To support our choice in using the OpenAPI format for specifying \benchmark{} scenarios and providing such precise specifications in the instructions to the models, we compare the performances of \openaiothree{}, \gptfo{}, and \claudesonnet{} when prompted with the OpenAPI specifications and with their plaintext transcriptions.
In \cref{tab:openapi_vs_text}, we show the performance gain when using the OpenAPI specifications instead of plaintext prompts.
We observe that all three models produce significantly more functionally correct backends when these are described by the OpenAPI specifications.
This result confirms our choice of using these specifications in prompts in our main experiments, and implies that well-established software engineering best practices in terms of rigorous requirement specification may remain important even in the age of LLM-powered automated software development. 

\paragraph{Additional Results}
In \cref{appendix:passfive}, we include extended versions of our main results presented above, showing the \passk{5} and \secpassk{5} scores for all non-reasoning models for each of the three prompt types.
In \cref{appendix:cwe_ocurence}, we present detailed results on the occurrence rates of CWEs in our experiments, across frameworks, models, and scenarios.
