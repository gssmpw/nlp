\section{Conclusion}
\label{sec:conclusion}

In this work, we proposed \benchmark{}, the first code generation benchmark that reflects the next frontier in autonomous coding, targeting standalone backend application development, a domain that is of high practical relevance and challenging both in terms of code functionality and security.
\benchmark{} combines $28$ scenarios and $14$ frameworks to produce $392$ evaluation tasks. We evaluate $11$ SOTA LLMs on \benchmark{} and find that even flagship LLMs rarely produce correct and secure code. 
We believe that success in generating secure and correct backends is a minimal requirement for LLMs before they can be used to generate production code---as such, \benchmark{} promotes progress towards the goal of automated software development by enabling rigorous evaluation.
