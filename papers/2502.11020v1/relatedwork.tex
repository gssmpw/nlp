\section{Related Work}
\paragraph{Language understanding benchmarks}
Multi-task language understanding evaluation benchmarks play an important role in the evaluation of LLMs. Early benchmarks concentrated on general natural language understanding. GLUE \cite{glue} and SuperGLUE \cite{superglue} were two such benchmarks that were widely adopted by the research community. These benchmarks were saturated quickly, due to the development of better LLMs. However, LLMs struggled more against benchmarks that required knowledge and reasoning. MMLU \cite{mmlu} and MMLU-Pro \cite{mmlupro} were more challenging since they required not only language understanding but also world knowledge. These general-purpose benchmarks gradually gave way to higher-level and more specialized benchmarks such as MATH \cite{math}, GPQA \cite{gpqa}, and MUSR \cite{musr}.

\paragraph{Multilingual benchmarks}
The development of multilingual LLMs also necessitated challenging multilingual benchmarks. Most of these benchmarks were developed through machine translation \cite{xnli, globalmmlu}. However, such datasets have been shown to contain cultural biases and translation artifacts \cite{translationese}. Global MMLU relied on machine and professional translation to \cite{globalmmlu}. INCLUDE consists of native data \cite{include}, but it is imbalanced, with different subject distributions in different languages. There is also a significant difference in required knowledge levels between languages, making a direct comparison impossible.

\paragraph{Benchmarks for Turkic languages}
SeaEval was one of the first LLM benchmarks to include Turkish \cite{seaeval}. Global MMLU contains Kyrgyz and Turkish subsets. INCLUDE contains Azerbaijani and Kazakh. MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval \cite{mrl2024} contains an Azerbaijani dataset, but it contains general language understanding tasks rather than world knowledge. Karde≈ü-NLU has introduced a multilingual language understanding benchmark \cite{kardes}. But again, this benchmark contains general language understanding tasks that require no world knowledge. There are also monolingual benchmarks. Mukayese was one of the earliest general language understanding benchmarks in Turkish \cite{mukayese}. TurkishMMLU and TR-MMLU \cite{tr-mmlu} were the first native MMLU alternatives for the Turkish language. Another pilot study was performed to evaluate LLMs in Kazakh language \cite{dollmsspeakkazakh}. While there are no peer-reviewed monolingual MMLU alternatives for Azerbaijani, there is a general language understanding benchmark \cite{allma}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{latn_vs_cyrl.png}
    \caption{A sample question from the parallel Uzbek dataset, available in both Cyrillic and Latin alphabets. This enables comparison of LLM performance across different scripts. English translation is provided for clarity.}
    \label{fig:dual_dataset}
\end{figure}