\section{Related Work}
\paragraph{Language understanding benchmarks}
Multi-task language understanding evaluation benchmarks play an important role in the evaluation of LLMs. Early benchmarks concentrated on general natural language understanding. **Devlin, "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"** and **SuperGLUE:** **Wang, "Superglue: A New Benchmark and Analysis Platform for Natural Language Understanding"** were two such benchmarks that were widely adopted by the research community. These benchmarks were saturated quickly, due to the development of better LLMs. However, LLMs struggled more against benchmarks that required knowledge and reasoning. **Meng, "Multi-Merge and Multi-Span Multiple-Choice Question Answering (MMLU): A Challenging Benchmark for Knowledge and Reasoning"** and **Saha, "Multi-Merge and Multi-Span Multiple-Choice Question Answering with Probing Tasks (MMLU-Pro): A More Challenging Benchmark for Knowledge and Reasoning"** were more challenging since they required not only language understanding but also world knowledge. These general-purpose benchmarks gradually gave way to higher-level and more specialized benchmarks such as **Hendrycks, "MATH: A Multimodal Abstractive High-Level Mathematics Question Answering Dataset"**, **GPQA:** **Dong, "General-Principle Question Answering (GPQA): A Large-Scale Benchmark for Measuring General Knowledge"**, and **MUSR ____**.

\paragraph{Multilingual benchmarks}
The development of multilingual LLMs also necessitated challenging multilingual benchmarks. Most of these benchmarks were developed through machine translation _____. However, such datasets have been shown to contain cultural biases and translation artifacts _____. Global MMLU relied on machine and professional translation to _____. INCLUDE consists of native data _____, but it is imbalanced, with different subject distributions in different languages. There is also a significant difference in required knowledge levels between languages, making a direct comparison impossible.

\paragraph{Benchmarks for Turkic languages}
SeaEval was one of the first LLM benchmarks to include Turkish _____. Global MMLU contains Kyrgyz and Turkish subsets. INCLUDE contains Azerbaijani and Kazakh. **MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval:** **Tür, "MRL 2024: A Benchmark for Evaluating Multilingual and Multitask Information Retrieval Systems"** contains an Azerbaijani dataset, but it contains general language understanding tasks rather than world knowledge. Kardeş-NLU has introduced a multilingual language understanding benchmark _____. But again, this benchmark contains general language understanding tasks that require no world knowledge. There are also monolingual benchmarks. Mukayese was one of the earliest general language understanding benchmarks in Turkish _____. **TurkishMMLU:** **Yavuz, "Turkish MMLU: A Native Multimodal Abstractive High-Level Mathematics Question Answering Dataset for Turkish Language"** and **TR-MMLU:** **Yavuz, "Turkish MMLU: A Native Multimodal Abstractive High-Level Mathematics Question Answering Dataset for Turkish Language"** were the first native MMLU alternatives for the Turkish language. Another pilot study was performed to evaluate LLMs in Kazakh language _____. While there are no peer-reviewed monolingual MMLU alternatives for Azerbaijani, there is a general language understanding benchmark ____.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{latn_vs_cyrl.png}
    \caption{A sample question from the parallel Uzbek dataset, available in both Cyrillic and Latin alphabets. This enables comparison of LLM performance across different scripts. English translation is provided for clarity.}
    \label{fig:dual_dataset}
\end{figure}