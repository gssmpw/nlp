\section{Related Work}
\paragraph{Handwriting OCR.} Most OCR engines, including those that can be run locally like Tesseract, are designed for use with printed text and are nearly useless for handwriting.
Several commercial OCR engines, such as Google Cloud Vision, Azure AI Vision and Amazon Textract, are designed for use on handwritten text at the page-level scale.

State-of-the-art HTR and OCR models \cite{fujitake2024dtrocr, li2023trocr, kim2021donut, huang2022layoutlmv3} are typically based on pre-trained vision Transformers (ViT; \citealt{vaswani2017attention, dosovitskiy2020image}) and may include recurrent components like LSTMs or CNNs \cite{breuel2013high, azawi2013normalizing, bora2020handwritten, yang2019handwriting}; the commercial handwriting-capable OCR engines mentioned above are likely similar in architecture to the best of these models, leveraging massive, pre-trained ViTs and language models. In general, such models are only somewhat effective on HTR tasks zero-shot, and SOTA is reached by fine-tuning on labeled data for the specific task.
This is fine for benchmarks, but for real-world tasks obtaining labeled training data is often prohibitively expensive. 
Furthermore, most benchmarks are concerned only with recognition at the character or line level. This can, of course, be aggregated to return document-level transcriptions, but this neglects the task of text \textit{detection}, and does not consider incidentals that occur in real documents --- headings, figures, scribbles, margin notes, imperfections in image quality, distractors, etc. 
This paper is concerned with transcription over multi-page documents in a holistic manner.


\paragraph{LLMs for OCR post-processing.}  Several works have investigated improving OCR transcription score with post-processing by a language model \cite{lund2011progressive, schaefer2020two, veninga2024llms, rigaud2019icdar}. 

LLM-aided OCR is a public tool that uses OCR output with an LLM post-processor to improve OCR transcription accuracy, but the authors do not provide any experimental results demonstrating improvement besides hand-picked examples. 
Similarly, BetterOCR is a tool that combines results from multiple OCR engines and passes them into an LLM, but again, only hand-picked examples are provided as experimental results. Furthermore, both tools are only designed for printed text, both operate at the page level (or at the `chunk' level within pages, in the case of BetterOCR), and neither use \textit{M}LLMs, only using LLMs for post-processing. 


\paragraph{Benchmarks.} Most OCR benchmarks are for machine-printed text, and only for single pages/images \cite{liu2023hidden}, such as receipts \cite{park2019cord, huang2019icdar2019}). Kleister \cite{gralinski2020kleister, stanislawek2021kleister} is a pair of multi-page, long-context key entity extraction benchmark tasks, but consists of only machine-printed text.

There are a number of HTR benchmarks, including historical documents, documents not written in English or with Latin characters  \cite{sanchez2019set, zhang2019icdar, causer2018making, dolfing2020scribblelens, serrano2010rodrigo, wigington2018start, carbonell2019end, yu2021benchmarking}, and transcription of numerical digits or mathematical expressions \cite{liu2023hidden, yuan2022syntax, diem2014icfhr}. None are explicitly concerned with multi-page documents, and most are at the line- or word-level. In this paper we will use the most well-known handwriting benchmark, the IAM Handwriting Database \cite{marti2002iam}, to construct our own multi-page documents.