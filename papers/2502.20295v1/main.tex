%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[submission]{aaai25}  % DO NOT CHANGE THIS % MAKES IT ANONYMOUS
\usepackage{aaai25} % NON-ANON
\nocopyright
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{amsmath}
\usepackage{makecell}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}


\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

\newcommand{\mytilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}
\newcommand{\bentham}{\texttt{Bentham}}
\newcommand{\iam}{\texttt{IAM}}
\newcommand{\firstpage}{\textsc{+first page}}


% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Judge a Book by Its Cover:\\Investigating Multi-Modal LLMs\\for Multi-Page Handwritten Document Transcription}
% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar\textsuperscript{\rm 2},
%     % J. Scott Penberthy\textsuperscript{\rm 3},
%     % George Ferguson\textsuperscript{\rm 4},
%     % Hans Guesgen\textsuperscript{\rm 5}
%     % Note that the comma should be placed after the superscript

%     1101 Pennsylvania Ave, NW Suite 300\\
%     Washington, DC 20004 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     proceedings-questions@aaai.org
% %
% % See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

% \iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    Benjamin Gutteridge\textsuperscript{\rm 1}\thanks{Includes work completed during an internship at QuantCo.},
    Matthew Jackson\textsuperscript{\rm 1},
    Toni Kukurin\textsuperscript{\rm 2},
    Xiaowen Dong\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}University of Oxford,
    \textsuperscript{\rm 2}QuantCo\\
    \{beng, jackson, xdong\}@robots.ox.ac.uk, toni.kukurin@quantco.com
}
% \fi


\begin{document}

\maketitle

\begin{abstract}
Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, \firstpage{}, which enhances MLLM transcription by providing the OCR output of the entire document along with \textit{just the first page image}. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that \firstpage{} improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page. 
\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
\begin{links}
    \link{Code}{https://github.com/BenGutteridge/judge-a-book}
\end{links}

\section{Introduction}
A significant proportion of all human-written text exists only in the form of physical handwritten documents. Accurate and cost-effective digitization of such documents would benefit numerous fields and industries by improving information accessibility, sharing, and processing. Digitized handwritten text could also provide a largely-untapped source of training data for language models.

Modern optical character recognition (OCR) software is adept at transcribing printed text, even from low-quality scans, but handwriting remains challenging \cite{liu2023hidden}. State-of-the-art models on handwriting text recognition (HTR) tasks \cite{li2023trocr, fujitake2024dtrocr} typically combine pre-trained vision Transformers \cite{dosovitskiy2020image, liu2021swin, huang2022layoutlmv3, xu2020layoutlm, kim2021donut} and language models \cite{devlin2018bert, liu2019roberta}, and rely on fine-tuning with labeled data to perform well. Unfortunately, labeling this training data, i.e. manually transcribing documents, is expensive, time-consuming, and often unrealistic in the real world. We are interested in models that can be deployed \textit{zero-shot} --- without training/fine-tuning or examples.

A second major challenge arises in processing \textit{multi-page documents}, which constitute most real-world handwritten records. These documents share contextual and structural features across pages, such as consistent handwriting, layout, image properties, and interrelated textual content. Yet, OCR systems typically operate at the page level, ignoring these cross-page dependencies, while most HTR models are benchmarked at the line or word level. This limited focus on multi-page document processing leaves significant room for improvement in leveraging shared context and relationships across pages.

Large language models (LLMs; \citealt{floridi2020gpt, achiam2023gpt, zhao2023survey}) and multi-modal LLMs with a vision component (MLLMs; \citealt{yin2024survey}) show promise for addressing these challenges. MLLMs in particular, with their ability to process images and text simultaneously, appear to be strong end-to-end handwriting transcribers in themselves, going beyond simply serving as post-processors for traditional OCR engines. 
Furthermore, such models, equipped with ever-increasing context window sizes --- such as the 128k-token limit of OpenAI's \textsc{gpt-4o} --- can theoretically process documents spanning dozens of pages in image and text formats simultaneously, and have demonstrated capability in long-context settings \cite{chen2023longlora, liu2024lost, kim2024fables, karpinska2024one}. Despite this potential, existing research has primarily explored LLMs for OCR post-processing, with limited focus on using MLLMs for handwritten or multi-page document transcription.

\paragraph{Contributions.} In this paper:
\begin{itemize}
    \item We investigate the transcription of \textit{multi-page, handwritten documents} using various configurations of commercial OCR engines and MLLMs, the latter being used both as end-to-end transcribers and as post-processors, with and without the vision component.
    \item We propose a method by which MLLMs can leverage contextual information from documents \textit{beyond text content only}, and without incurring the prohibitively high cost of full vision LLM processing, by providing \textbf{just a single page from a document}.
    \item We show that this method improves transcription quality, demonstrate the capability of MLLMs to beneficially extrapolate document features across pages for the transcription task, and show that our method sits on the cost-performance Pareto frontier as an intermediate approach to expensive MLLM and cheaper OCR methods.
\end{itemize}

\section{Related Work}

\paragraph{Handwriting OCR.} Most OCR engines, including those that can be run locally like Tesseract, are designed for use with printed text and are nearly useless for handwriting.
Several commercial OCR engines, such as Google Cloud Vision, Azure AI Vision and Amazon Textract, are designed for use on handwritten text at the page-level scale.

State-of-the-art HTR and OCR models \cite{fujitake2024dtrocr, li2023trocr, kim2021donut, huang2022layoutlmv3} are typically based on pre-trained vision Transformers (ViT; \citealt{vaswani2017attention, dosovitskiy2020image}) and may include recurrent components like LSTMs or CNNs \cite{breuel2013high, azawi2013normalizing, bora2020handwritten, yang2019handwriting}; the commercial handwriting-capable OCR engines mentioned above are likely similar in architecture to the best of these models, leveraging massive, pre-trained ViTs and language models. In general, such models are only somewhat effective on HTR tasks zero-shot, and SOTA is reached by fine-tuning on labeled data for the specific task.
This is fine for benchmarks, but for real-world tasks obtaining labeled training data is often prohibitively expensive. 
Furthermore, most benchmarks are concerned only with recognition at the character or line level. This can, of course, be aggregated to return document-level transcriptions, but this neglects the task of text \textit{detection}, and does not consider incidentals that occur in real documents --- headings, figures, scribbles, margin notes, imperfections in image quality, distractors, etc. 
This paper is concerned with transcription over multi-page documents in a holistic manner.


\paragraph{LLMs for OCR post-processing.}  Several works have investigated improving OCR transcription score with post-processing by a language model \cite{lund2011progressive, schaefer2020two, veninga2024llms, rigaud2019icdar}. 

LLM-aided OCR is a public tool that uses OCR output with an LLM post-processor to improve OCR transcription accuracy, but the authors do not provide any experimental results demonstrating improvement besides hand-picked examples. 
Similarly, BetterOCR is a tool that combines results from multiple OCR engines and passes them into an LLM, but again, only hand-picked examples are provided as experimental results. Furthermore, both tools are only designed for printed text, both operate at the page level (or at the `chunk' level within pages, in the case of BetterOCR), and neither use \textit{M}LLMs, only using LLMs for post-processing. 


\paragraph{Benchmarks.} Most OCR benchmarks are for machine-printed text, and only for single pages/images \cite{liu2023hidden}, such as receipts \cite{park2019cord, huang2019icdar2019}). Kleister \cite{gralinski2020kleister, stanislawek2021kleister} is a pair of multi-page, long-context key entity extraction benchmark tasks, but consists of only machine-printed text.

There are a number of HTR benchmarks, including historical documents, documents not written in English or with Latin characters  \cite{sanchez2019set, zhang2019icdar, causer2018making, dolfing2020scribblelens, serrano2010rodrigo, wigington2018start, carbonell2019end, yu2021benchmarking}, and transcription of numerical digits or mathematical expressions \cite{liu2023hidden, yuan2022syntax, diem2014icfhr}. None are explicitly concerned with multi-page documents, and most are at the line- or word-level. In this paper we will use the most well-known handwriting benchmark, the IAM Handwriting Database \cite{marti2002iam}, to construct our own multi-page documents.


\section{Proposed Method: Extrapolating Formatting and Learned OCR Error from a Single Page}
We propose the following MLLM prompting strategy for OCR post-processing of multi-page documents: \textbf{provide the MLLM with the OCR output for the entire document as well as \textit{just the first page image}}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{method_figs/firstpage.pdf}
    \caption{An illustration of how \firstpage{} works; the OCR text of a multi-page document is provided to an MLLM, along with \textit{just the first page image} of the document. Blue denotes the first page.}
    \label{fig:firstpage}
\end{figure}

Given that separate pages from the same document will have very similar formatting --- handwriting, structure, image lighting/angle,  etc. --- it is likely that the errors made by an OCR engine will be fairly consistent over all pages. 
We hypothesize that an MLLM should be able to \textit{learn, in-context, the mapping from the provided image to the first part of the noisy OCR input}, and use this to improve on the post-processing of the entire text. An example of how this works in practice is illustrated by Figure~\ref{fig:example_14_15}, as well as Figures~\ref{fig:example_draws}--\ref{fig:example_columns} in the Appendix.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{example_figs/14_15_example.pdf}
    \caption{An example of how \textsc{+first page} propagates OCR error corrections across pages. Though the MLLM only has access to the image of the first page, it uses the corrections that the OCR (i) frequently mistakes `i` for `1' and (ii) frequently mistakes words for numbers to correctly transcribe the word `in' on the unseen second page. See Figures~\ref{fig:example_draws}--\ref{fig:example_columns} in the Appendix for further examples.}
    \label{fig:example_14_15}
\end{figure}


The idea bears some similarity to few-shot prompting, in which one or more examples of input and desired output are provided within the prompt. The two input channels, the page image and the OCR text, can be thought of as the example input and target. In this case, however, rather than learning in-context to \textit{replicate} the OCR engine, the MLLM should (i) exercise its own judgement to identify OCR errors, (ii) identify how the OCR engine's choices should be corrected, and (iii) extrapolate this learned image $\rightarrow$ OCR text mapping to the remainder of the OCR output (i.e. `unseen' text). 

We believe MLLMs are an appropriate tool for these tasks, as they already demonstrate ability to recognize and correct OCR errors using local context and probable text patterns from their pre-training corpora, and they demonstrate the ability to learn new tasks from in-context examples \cite{dong2022survey}. 

The method described above, which we refer to as \firstpage{}, is motivated by the assumptions that (i) OCR engines are cheap, but error-prone, whereas (ii) MLLMs (either as OCR post-processors or as end-to-end transcribers) can often reduce such errors, but are expensive; also that (iii) separate pages across a document have shared features and contextual information that page-by-page LLM calls throw away.\footnote{We demonstrate (ii) in the Experiments section; for an example of (iii) see Figure~\ref{fig:iam_right} in the Appendix; separate pages are extremely similar in all but text content, yet this information is thrown away between MLLM calls.} \firstpage{} is an attempt to provide a simple, intermediate method between OCR and full-document MLLM transcription, and leverage this lost information. 

In addition to improving performance while balancing cost, we believe that \firstpage{} also provides some insight into LLM reasoning ability; namely that MLLMs are able to extrapolate formatting and OCR error information from images and to improve performances on tasks using out-of-sample, `unseen' text.


\section{Experimental Setup}
We experiment with various configurations of OCR engines, MLLMs and prompting strategies on a multi-page version of the IAM Handwriting Database benchmark dataset (\citealt{marti2002iam}; see Figures~\ref{fig:iam_left},~\ref{fig:iam_right} in the Appendix for example documents).
The task is to produce a whole-document transcription from either document page images, OCR output, or some combination of the two.

\paragraph{OCR and MLLMs.} We use three commercial OCR engines: Azure AI Vision, Google Cloud Vision and Amazon Textract (Tesseract was also tried, but failed to output anything meaningful on handwritten text).

For commercial LLMs we use \textsc{gpt-4o} and \textsc{gpt-4o-mini} with OpenAI's API, with default parameters and a temperature of 0. We use \texttt{`\textbackslash \textbackslash n[NEW\_PAGE]\textbackslash \textbackslash n`} as a page break marker for multi-page documents. Per-page OCR outputs are manually joined with this marker, and MLLMs are asked to maintain these markers or (in the vision-only case) to insert them into the transcription if appropriate.

\paragraph{Cost.} For our cost estimates for commercial tools we use the default values listed on their respective web pages. Most have pricing tiers based on scale, or a limited free allowance; for simplicity we take the lowest (non-batch) cost per run for each engine, and for the OpenAI API. See the Appendix for individual prices. 


\paragraph{Multi-page datasets.}
For IAM all of the documents are single pages, with the machine-typed text at the top of the page and the same text, handwritten, below it. We programmatically crop the images to contain only the handwritten part (using provided metadata) and then combine them at random by writer ID to produce 2- and 3- image multi-page documents. For IAM we use a subset of 268 multi-page documents, 210 with 2 pages and the rest with 3, and henceforth refer to this multi-page dataset as \iam{}. 

We also evaluate methods on documents of varying length in terms of page count, for which we construct a dataset of 10 documents per page count from 2-10 pages.


\paragraph{Methods.}
Below is an overview of the different methods used for experiments in this section, in approximate complexity(/cost) order. Figures~\ref{fig:firstpage} and \ref{fig:ocr_only}--\ref{fig:all_pages} illustrate some of these methods to make the pipeline clearer.

\begin{description}
    \item[\textsc{ocr only}]: just OCR engine output; can then be post-processed by an LLM or used as-is.
    \item[\textsc{ocr only pbp}]: page-by-page, OCR $\rightarrow$ LLM one page/API call at a time, joined by page breaks afterwards. \textsc{pbp} is an attribute that can also be applied to any of the other `all-at-once' methods, to mean independent processing of pages followed by concatenation.
    \item[\firstpage{}]: OCR plus the first page image $\rightarrow$ MLLM. The `\textsc{+}' prefix indicates a method that is \textit{in addition to} OCR output (the default).
    \item[\textsc{+chosen page}]: OCR plus a single page image chosen by a separate prompt to \textsc{gpt-4o-mini} $\rightarrow$ MLLM. The prompt includes the OCR output and asks for the best page ID for downstream MLLM post-processing. We would expect this approach to perform at least as well as \firstpage{}.
    \item[\textsc{vision*}]: all page images $\rightarrow$ MLLM, no OCR. `\textsc{*}' denotes a non-OCR method.
    \item[\textsc{+all pages}]: all page images \textit{and OCR output} $\rightarrow$ MLLM
    \item[\textsc{all ocr}]: the concatenated outputs of all three OCR engines $\rightarrow$ LLM
\end{description}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{method_figs/ocr_only.pdf}
    \caption{\textsc{ocr only} ($\rightarrow$ LLM) illustration.}
    \label{fig:ocr_only}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{method_figs/ocr_only_pbp.pdf}
    \caption{\textsc{ocr only pbp} illustration.}
    \label{fig:ocr_only_pbp}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{method_figs/vision.pdf}
    \caption{\textsc{vision*} illustration.}
    \label{fig:vision}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{method_figs/all_pages.pdf}
    \caption{\textsc{+all pages} illustration.}
    \label{fig:all_pages}
\end{figure}

\paragraph{Error catching.} As a post-processing step for MLLM methods, we perform simple common-sense checks for catastrophic MLLM error, such as repeating sections of text ad infinitum, or refusing to return an output due to an inadvertent triggering of OpenAI's guardrails (``Sorry, but I can't answer that...''). To avoid such outliers unfairly dragging down the overall score of a method (that uses OCR output), we compare the CER between OCR output and final MLLM output, and, if the CER is drastically different, use the OCR output by default.

\paragraph{Evaluation.} Evaluation is at the document level using Character Error Rate (CER; \citealt{morris2004and}), the most widely-used metric for OCR transcription. We also experimented with Average Normalized Levenshtein Similarity \cite{peer2024anls}, which is much more time-consuming to compute but, unlike CER, is sensitive to character order. We opted for CER as we found the two metrics approximately equally informative.


\section{Experiments}
Table~\ref{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked} shows performance of various methods on \iam{}, with Azure as the OCR baseline. We focus on Azure as it is the best-performing (and cheapest) of the three OCR engines, but similar tables for Google Vision and Textract can be found in the Appendix.

\begin{table}[H]
\centering
\begin{tabular}{@{}rrccc@{}}
\toprule
Method & $\rightarrow$ MLLM & CER &  \makecell{Rel.\\Imp.} & $\dfrac{\text{R.I.}}{c_\text{cost}}$ \\
\midrule
\midrule
\textsc{ocr only} & - & 0.036 & \cellcolor[RGB]{254,254,254}0.00 & \cellcolor[RGB]{254,254,254}0.00 \\
\textsc{ocr only} & \textsc{gpt-4o-mini} & 0.032 & \cellcolor[RGB]{233,245,233}0.11 & \cellcolor[RGB]{205,233,205}0.10 \\
\textsc{ocr pbp} & \textsc{gpt-4o-mini} & 0.025 & \cellcolor[RGB]{194,229,195}0.31 & \cellcolor[RGB]{115,195,117}0.28 \\
\textsc{ocr only} & \textsc{gpt-4o} & 0.033 & \cellcolor[RGB]{239,248,239}0.08 & \cellcolor[RGB]{240,248,240}0.03 \\
\textsc{ocr pbp} & \textsc{gpt-4o} & 0.029 & \cellcolor[RGB]{218,239,218}0.19 & \cellcolor[RGB]{219,239,219}0.07 \\
\textsc{+first page} & \textsc{gpt-4o-mini} & 0.015 & \cellcolor[RGB]{141,206,143}0.58 & \cellcolor[RGB]{174,220,175}0.16 \\
\textsc{+chosen page} & \textsc{gpt-4o-mini} & 0.029 & \cellcolor[RGB]{218,239,218}0.19 & \cellcolor[RGB]{230,244,230}0.05 \\
\textsc{+first page} & \textsc{gpt-4o} & 0.027 & \cellcolor[RGB]{206,234,207}0.25 & \cellcolor[RGB]{219,239,219}0.07 \\
\textsc{+chosen page} & \textsc{gpt-4o} & 0.025 & \cellcolor[RGB]{194,229,195}0.31 & \cellcolor[RGB]{214,237,215}0.08 \\
\textsc{vision*} & \textsc{gpt-4o} & 0.027 & \cellcolor[RGB]{206,234,207}0.25 & \cellcolor[RGB]{224,242,225}0.06 \\
\textsc{vision* pbp} & \textsc{gpt-4o} & \textbf{0.010} & \cellcolor[RGB]{115,195,117}0.72 & \cellcolor[RGB]{170,218,171}0.17 \\
\textsc{all ocr pbp} & \textsc{gpt-4o-mini} & 0.020 & \cellcolor[RGB]{168,218,170}0.44 & \cellcolor[RGB]{205,233,205}0.10 \\
\textsc{+all pages} & \textsc{gpt-4o} & 0.027 & \cellcolor[RGB]{206,234,207}0.25 & \cellcolor[RGB]{230,244,230}0.05 \\
\textsc{+all pages pbp} & \textsc{gpt-4o} & \textit{0.011} & \cellcolor[RGB]{120,197,122}0.69 & \cellcolor[RGB]{189,227,190}0.13 \\
\textsc{+all pages pbp} & \textsc{gpt-4o-mini} & \textbf{0.010} & \cellcolor[RGB]{115,195,117}0.72 & \cellcolor[RGB]{199,231,200}0.11 \\
\textsc{all ocr pbp} & \textsc{gpt-4o} & 0.021 & \cellcolor[RGB]{173,220,174}0.42 & \cellcolor[RGB]{224,242,225}0.06 \\\bottomrule
\end{tabular}
\caption{Relative performance of MLLMs and prompting strategies compared to the baseline \textbf{Azure} OCR engine on \iam{}. \textsc{pbp} denotes `page-by-page' and `*' denotes vision only, i.e. no OCR. Rows are ordered by total cost of processing the dataset. Rel. Imp. $ = (\text{CER}_\text{OCR} - \text{CER}_\text{method})/\text{CER}_\text{OCR}$; relative improvement in performance of a given method with respect to the raw OCR engine. $\text{R.I.}/c_\text{cost}$ is the relative improvement normalized by the cost ratio between the method and OCR only, i.e. $c_\text{cost} = \text{cost}_\text{method} / \text{cost}_\text{ocr}$.}
\label{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked}
\end{table}

\subsection{Discussion} 
We begin by comparing methods introduced in the previous section, before focusing specifically on \textsc{pbp} methods and \firstpage{}, and finally look at two trade-offs: the benefits of sharing information across pages versus the challenges of long contexts, and, more generally, the cost-performance trade-offs of our various methods.

\paragraph{MLLMs are powerful transcribers.} In general, the more sophisticated and expensive the method, the more beneficial it is, with \textsc{+all pages pbp} being the best overall. Using both OCR output \textit{and} vision components together yields better performance than either text-only post-processing or vision alone. Even so, \textsc{vision*}, and especially \textsc{vision* pbp} are surprisingly powerful end-to-end transcribers without any OCR input, much more so than any of the three commercial engines explicitly trained for that task.

\paragraph{Cost-effective OCR post-processing.} As the vision component of MLLMs is expensive, and OCR engines and text-only LLMs are (relatively) cheap, it is important to look at which strategies improve on OCR performance while remaining cost-effective.

All three engines are substantially helped by some sort of post-processing. \firstpage{} consistently improves performance, which is expected: we have seen that combining OCR output and images improves performance, so at minimum we would expect overall performance on a multi-page document to be brought up by improved performance on just the first page. We go beyond this and demonstrate below (see Table~\ref{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked_strict}) that this performance benefit goes beyond just the text corresponding to the first page, and that there is cross-page out-of-sample extrapolation.

\paragraph{Impact of choice of MLLM.} Surprisingly, \textsc{gpt-4o-mini} often performed better than \textsc{gpt-4o}. We are unsure why this is, but we speculate that the supposedly more capable model may have a tendency to do \textit{too much}; for OCR post-processing, often what is left alone is more important than what is changed. 

It is also worth noting that the vision components of \textsc{gpt-4o} and \textsc{gpt-4o-mini} may not be as different as the text components, given that the cost difference between models only applies to text input: \textsc{gpt-4o-mini} is \mytilde30x cheaper for text, but costs the same for image tokens (the API artificially inflates the number so the cost is equal). This would suggest that the vision backend is the same or very similar for both models, and may partly explain the lack of divergence in performance.

\paragraph{\textsc{pbp} and the impact of page count.} \label{section:pagecount} Another surprising finding was the severity of the impact of input length on performance for MLLMs.

All \textsc{pbp} methods perform better than their `all-at-once' counterparts, for only a small cost increase. Though there does seem to be evidence that contextual information can be leveraged for benefit across pages (discussed in the following subsection), much of this benefit is seemingly offset by \textsc{pbp}, i.e. simply processing pages independently and then concatenating them afterwards, as a commercial OCR engine would do. 

To verify this, we perform an experiment testing the performance of several methods and their \textsc{pbp} counterparts on IAM multi-page documents of varying lengths (constructed in the same way as \iam{} but with fixed page counts). We use 10 documents per document length, and plot relative CER improvement against page count in Figure~\ref{fig:per_page}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{plots/iam_multipage_2-10_pages_10_docs_split=1.00_seed=00_checked_per_pagecount.pdf}
    \caption{Comparing \textsc{pbp} and all-at-once performance for text only, vision only and mixed methods (all using \textsc{gpt-4o}) on \iam{} multi-page documents of varying page counts. Relative CER Improvement is the same as in Table~\ref{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked}.}
    \label{fig:per_page}
\end{figure}

We find that \textsc{pbp} methods retain similar performance over all page counts, while the all-at-once methods degrade as the number of pages increases. This is surprising, given that the context windows for \textsc{gpt-4o} and \textsc{gpt-4o-mini} are 128k tokens, and the number of tokens for a document page is only in the order of hundreds. As our prompts and inputs, while much shorter than the maximum context window length, are still longer than \textsc{pbp}, we speculate that culprit may still be the `lost in the middle' problem \cite{liu2024lost}, where Transformers' attentive power is spread thinly over long contexts, causing performance to weaken on tasks relating to text far from the start or end of the prompt. 

This may be compounded by the additional complexity of maintaining/inserting accurate page breaks --- anecdotally, we found that MLLMs required firm instructions regarding page breaks, or they would tend to be removed completely, or moved to `sensible' places in the text, such as the end of a sentence or section, against the formatting of the original handwritten document. 

We believe that further investigation into this long-context failure mode is warranted; we leave it to future work.

\paragraph{\firstpage{} out-of-sample analysis.} Table~\ref{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked_strict} shows the performance of \firstpage{} and other methods \textit{only on pages after the first}.

\begin{table}[H]
\centering
\begin{tabular}{@{}rrcc@{}}
\toprule
Method & $\rightarrow$ MLLM & CER &  \makecell{Rel.\\Imp.} \\
\midrule
\midrule
\textsc{ocr only} & - & 0.037 & \cellcolor[RGB]{254,254,254}0.00 \\
\textsc{ocr only} & \textsc{gpt-4o-mini} & 0.032 & \cellcolor[RGB]{218,239,218}0.14 \\
\textsc{ocr only} & \textsc{gpt-4o} & 0.033 & \cellcolor[RGB]{225,242,226}0.11 \\
\textsc{+first page} & \textsc{gpt-4o-mini} & \textbf{0.017} & \cellcolor[RGB]{115,195,117}0.54 \\
\textsc{+first page} & \textsc{gpt-4o} & \textit{0.028} & \cellcolor[RGB]{193,228,194}0.24 \\\bottomrule
\end{tabular}
\caption{Relative performance of MLLMs and prompting strategies compared to the baseline \textbf{Azure} OCR engine on \iam{}. Performance is given \textit{only for pages after the first}.}
\label{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked_strict}
\end{table}

We find that \firstpage{} performs significantly better than the \textsc{ocr only} baselines, even though the provided page contains \textit{none of the text that the model is ultimately being scored on}. The benefit here is entirely due to extrapolating non-textual and contextual information from a document image to similar images, and learning (at least to some extent) the mapping from image to OCR output, and exploiting knowledge of that mapping to reduce errors. 

This is a significant result, as it shows that multi-page transcription is a long-context problem, and that methods tackling this problem holistically with MLLMs can improve performance. We believe this merits further investigation.

 
\paragraph{Scaling challenges.} We have experiments for documents of 2-3 pages, but we would expect \firstpage{} to be especially cost-effective for longer documents. Hypothetically, the insights from a single page should be equally applicable across all pages with similar formatting, so the performance improvement for an $n$ page document should cost the same as improving performance for a one page document: the cost of processing just a single page. 

Unfortunately, in practice, for long documents this benefit seems to trade off with MLLM preference for a \textsc{pbp} approach (see Figure~\ref{fig:per_page}), where the benefit of having access to the first page image cannot be shared to subsequent pages. Future work will look at how to mitigate this performance breakdown, and therefore how to increase the cost-effectiveness of \firstpage{}. 

\paragraph{Scaling with prompt caching.} Prompt caching \cite{shi2024keep} could provide an alternate means of benefiting from \firstpage{} in a \textsc{pbp} setting. 

Prompt caching is when an intermediate model state is saved after having already been `primed' with the first part of a prompt, and is already used in some commercial LLMs. A \textsc{pbp} version of \firstpage{} would, without prompt caching, be just as expensive (and not nearly as informative) as \textsc{+all pages pbp}. With prompt caching, the first page would not need to be re-processed, and the performance benefits of both \textsc{pbp} and \firstpage{} could be achieved with cost scaling $ << \mathcal{O}(n)$.

\paragraph{Poor performance of \textsc{+chosen page}.} We hypothesized that  \textsc{+chosen page} should perform at least as well as \firstpage{}; in practice though, this was not the case, especially for \textsc{gpt-4o-mini} and for weaker OCR outputs. The decline in performance is likely due to the increased complexity of the prompt, and the MLLM being more likely to be confused by the concept of an $n$th page than by the first. Previous work has successfully used LLM decision-making for downstream tasks \cite{wu2024repoformer, schick2024toolformer}, suggesting that further investigation of this method is worthwhile.

\paragraph{The performance-cost trade-off.}
Figure~\ref{fig:pareto-iam} plots performance against cost for \iam{} for all methods described in this paper and draws a Pareto frontier of the optimal methods, trading off performance and cost. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{plots/iam_multipage_minpages=02_split=0.50_seed=00_checked_pareto.pdf}
    \caption{Performance and cost of all OCR engine, MLLM and method configurations on \texttt{IAM}, with a Pareto frontier showing trade-off between the best methods.\\
    \textbf{Pareto frontier methods} ($L \rightarrow R$): \textsc{vision* pbp}$\rightarrow$\textsc{4o},
Az \textsc{+first page}$\rightarrow$\textsc{4o-mini},
Az \textsc{ocr only pbp}$\rightarrow$\textsc{4o-mini},
Az \textsc{ocr only}$\rightarrow$\textsc{4o-mini},
Az~\textsc{ocr only}.}
    \label{fig:pareto-iam}
\end{figure}

In the real world, practitioners must balance cost and performance when deciding which tool to use. Standard methods --- e.g. using either an OCR engine or a MLLM --- are separated by a sizable gap in both performance and cost. Having a range of approaches on the Pareto frontier that bridge this gap permit practitioners to choose the best tool for the job. We can see that\textbf{ \firstpage{} sits on this frontier}, cementing its value as a tool in the transcription toolkit.

% Furthermore, we have shown that the benefit of \firstpage{} goes beyond simply serving as a `middle ground' between OCR only and MLLM; for example, an arbitrary sliding scale would be to use a cheap method on half of the document and an expensive method on the other half, combining the results and yielding overall performance and cost approximately midway between the two. We have seen that \firstpage{} can even improve performance on out-of-sample text, and leverage information across pages.


\section{Conclusion and Further Work}
In this work, we investigated the transcription of multi-page handwritten documents using various configurations of commercial OCR engines and MLLMs. We examined the effectiveness of different prompting strategies and proposed the \firstpage{} method, which provides the MLLM with the OCR output for the entire document along with just the first page image. Our experiments on a multi-page synthesis of the IAM Handwriting Database demonstrated that \firstpage{} improves transcription accuracy while balancing cost and performance. Notably, it is effective \textit{even on out-of-sample text}, as it leverages formatting and OCR error patterns from a single page to other, `unseen' pages. 

Future work will look at additional handwritten document datasets, explore mitigating the performance degradation observed with increasing document length, and further investigate the potential of prompt caching and other techniques to leverage information across pages in long document transcription.


\section{Appendix}

\subsection{Additional results}
Tables~\ref{tab:google_ocr_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked},~\ref{tab:textract_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked} show performance of various methods and MLLMs combined with output from the Google Cloud Vision and Amazon Textract OCR engines, similar to Table~\ref{tab:azure_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked} for Azure.

\begin{table}[h]
\centering
\begin{tabular}{@{}rrccc@{}}
\toprule
Method & $\rightarrow$ MLLM & CER &  \makecell{Rel.\\Imp.} & $\dfrac{\text{R.I.}}{c_\text{cost}}$ \\
\midrule
\midrule
\textsc{ocr only} & - & 0.095 & \cellcolor[RGB]{254,254,254}0.00 & \cellcolor[RGB]{254,254,254}0.00 \\
\textsc{ocr only} & \textsc{gpt-4o-mini} & 0.074 & \cellcolor[RGB]{220,240,221}0.22 & \cellcolor[RGB]{165,217,167}0.21 \\
\textsc{pbp} & \textsc{gpt-4o-mini} & 0.071 & \cellcolor[RGB]{216,238,216}0.25 & \cellcolor[RGB]{156,213,158}0.23 \\
\textsc{ocr only} & \textsc{gpt-4o} & 0.064 & \cellcolor[RGB]{202,232,203}0.33 & \cellcolor[RGB]{183,224,184}0.17 \\
\textsc{pbp} & \textsc{gpt-4o} & 0.064 & \cellcolor[RGB]{202,232,203}0.33 & \cellcolor[RGB]{186,225,187}0.16 \\
\textsc{vision*} & \textsc{gpt-4o} & 0.027 & \cellcolor[RGB]{141,206,143}0.72 & \cellcolor[RGB]{136,204,138}0.28 \\
\textsc{+chosen page} & \textsc{gpt-4o-mini} & 0.101 & \cellcolor[RGB]{254,245,242}-0.06 & \cellcolor[RGB]{254,246,244}-0.02 \\
\textsc{+first page} & \textsc{gpt-4o-mini} & 0.044 & \cellcolor[RGB]{170,218,171}0.54 & \cellcolor[RGB]{170,218,171}0.20 \\
\textsc{vision* pbp} & \textsc{gpt-4o} & \textbf{0.010} & \cellcolor[RGB]{115,195,117}0.89 & \cellcolor[RGB]{115,195,117}0.33 \\
\textsc{all ocr pbp} & \textsc{gpt-4o-mini} & 0.020 & \cellcolor[RGB]{130,202,132}0.79 & \cellcolor[RGB]{136,204,138}0.28 \\
\textsc{+first page} & \textsc{gpt-4o} & 0.040 & \cellcolor[RGB]{163,216,165}0.58 & \cellcolor[RGB]{165,217,167}0.21 \\
\textsc{+chosen page} & \textsc{gpt-4o} & 0.046 & \cellcolor[RGB]{173,220,174}0.52 & \cellcolor[RGB]{178,222,180}0.18 \\
\textsc{+all pages} & \textsc{gpt-4o} & 0.030 & \cellcolor[RGB]{148,209,149}0.68 & \cellcolor[RGB]{178,222,180}0.18 \\
\textsc{+all pages pbp} & \textsc{gpt-4o} & \textit{0.013} & \cellcolor[RGB]{119,197,121}0.86 & \cellcolor[RGB]{161,215,162}0.22 \\
\textsc{all ocr pbp} & \textsc{gpt-4o} & 0.021 & \cellcolor[RGB]{131,202,133}0.78 & \cellcolor[RGB]{183,224,184}0.17 \\
\textsc{+all pages pbp} & \textsc{gpt-4o-mini} & 0.021 & \cellcolor[RGB]{131,202,133}0.78 & \cellcolor[RGB]{183,224,184}0.17 \\\bottomrule
\end{tabular}
\caption{Relative performance of MLLMs and prompting strategies compared to the baseline \textbf{Google Cloud Vision} OCR engine on the \iam{} dataset. \textsc{pbp} denotes `page-by-page' and `*' denotes that no OCR engine was used. Rows are ordered by descending total cost of processing the dataset. Rel. Imp. is $\text{CER}_\text{OCR} - \text{CER}_\text{method})/\text{CER}_\text{OCR}$; relative improvement in performance of a given method with respect to the raw OCR engine. $\text{R.I.}/c_\text{cost}$ is the relative improvement normalized by the cost ratio between the method and OCR only, i.e. $c_\text{cost} = \text{cost}_\text{method} / \text{cost}_\text{ocr}$.}
\label{tab:google_ocr_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{@{}rrccc@{}}
\toprule
Method & $\rightarrow$ MLLM & CER &  \makecell{Rel.\\Imp.} & $\dfrac{\text{R.I.}}{c_\text{cost}}$ \\
\midrule
\midrule
\textsc{ocr only} & - & 0.050 & \cellcolor[RGB]{254,254,254}0.00 & \cellcolor[RGB]{254,254,254}0.00 \\
\textsc{ocr only} & \textsc{gpt-4o-mini} & 0.051 & \cellcolor[RGB]{254,252,251}-0.02 & \cellcolor[RGB]{254,247,245}-0.02 \\
\textsc{pbp} & \textsc{gpt-4o-mini} & 0.045 & \cellcolor[RGB]{241,249,241}0.10 & \cellcolor[RGB]{221,240,222}0.09 \\
\textsc{ocr only} & \textsc{gpt-4o} & 0.046 & \cellcolor[RGB]{243,250,243}0.08 & \cellcolor[RGB]{240,248,240}0.04 \\
\textsc{pbp} & \textsc{gpt-4o} & 0.041 & \cellcolor[RGB]{230,244,230}0.18 & \cellcolor[RGB]{221,240,222}0.09 \\
\textsc{vision*} & \textsc{gpt-4o} & 0.027 & \cellcolor[RGB]{191,228,193}0.46 & \cellcolor[RGB]{188,226,189}0.18 \\
\textsc{+first page} & \textsc{gpt-4o-mini} & 0.027 & \cellcolor[RGB]{191,228,193}0.46 & \cellcolor[RGB]{191,228,193}0.17 \\
\textsc{+chosen page} & \textsc{gpt-4o-mini} & 0.101 & \cellcolor[RGB]{250,105,73}-1.02 & \cellcolor[RGB]{250,105,73}-0.38 \\
\textsc{vision* pbp} & \textsc{gpt-4o} & \textbf{0.010} & \cellcolor[RGB]{144,208,146}0.80 & \cellcolor[RGB]{148,209,149}0.29 \\
\textsc{all ocr pbp} & \textsc{gpt-4o-mini} & 0.020 & \cellcolor[RGB]{172,219,173}0.60 & \cellcolor[RGB]{173,220,174}0.22 \\
\textsc{+first page} & \textsc{gpt-4o} & 0.034 & \cellcolor[RGB]{210,236,211}0.32 & \cellcolor[RGB]{213,237,214}0.11 \\
\textsc{+chosen page} & \textsc{gpt-4o} & 0.031 & \cellcolor[RGB]{202,232,203}0.38 & \cellcolor[RGB]{207,234,208}0.13 \\
\textsc{+all pages} & \textsc{gpt-4o} & 0.032 & \cellcolor[RGB]{205,233,205}0.36 & \cellcolor[RGB]{221,240,222}0.09 \\
\textsc{+all pages pbp} & \textsc{gpt-4o} & \textit{0.011} & \cellcolor[RGB]{148,209,149}0.78 & \cellcolor[RGB]{180,223,182}0.20 \\
\textsc{all ocr pbp} & \textsc{gpt-4o} & 0.021 & \cellcolor[RGB]{175,221,176}0.58 & \cellcolor[RGB]{207,234,208}0.13 \\
\textsc{+all pages pbp} & \textsc{gpt-4o-mini} & 0.016 & \cellcolor[RGB]{161,215,162}0.68 & \cellcolor[RGB]{202,232,203}0.14 \\\bottomrule
\end{tabular}
\caption{Relative performance of MLLMs and prompting strategies compared to the baseline \textbf{Amazon Textract} OCR engine on the \iam{} dataset. \textsc{pbp} denotes `page-by-page' and `*' denotes that no OCR engine was used. Rows are ordered by descending total cost of processing the dataset. Rel. Imp. is $\text{CER}_\text{OCR} - \text{CER}_\text{method})/\text{CER}_\text{OCR}$; relative improvement in performance of a given method with respect to the raw OCR engine. $\text{R.I.}/c_\text{cost}$ is the relative improvement normalized by the cost ratio between the method and OCR only, i.e. $c_\text{cost} = \text{cost}_\text{method} / \text{cost}_\text{ocr}$.}
\label{tab:textract_improvement_iam_multipage_minpages=02_split=0.50_seed=00_checked}
\end{table}

\subsection{Commercial tool costs}
Costs of commercial OCR engines and LLMs for estimates used in this paper are given in Tables~\ref{table:cost_ocr_engines},~\ref{table:cost_llms}.

Costs for each tool were taken from their respective webpages:
\begin{links}
\link{Azure}{https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/computer-vision/}
\link{Textract}{https://aws.amazon.com/textract/pricing/} 
\link{Google Vision}{https://cloud.google.com/vision/pricing} 
\link{OpenAI}{https://openai.com/api/pricing/}
\end{links}

\begin{table}[H]
\centering
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{OCR Engine}       & \textbf{Cost per 1k Calls (\$)} \\ \midrule
Azure AI Vision           & 1.00                             \\
Google Cloud Vision       & 1.50                             \\
Amazon Textract           & 1.50                             \\ \bottomrule
\end{tabular}
\caption{Pricing for OCR Engines}
\label{table:cost_ocr_engines}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{LLM}              & \multicolumn{2}{c}{\textbf{Cost per 1M Tokens (\$)}}            \\ \cmidrule(lr){2-3}
                          & \textbf{Input} & \textbf{Output} \\ \midrule
\textsc{gpt-4o}                    & 2.50                        & 10.00                         \\
\textsc{gpt-4o-mini}               & 0.15                        & 0.60                          \\ \bottomrule
\end{tabular}
\caption{Pricing for Language Models (LLMs)}
\label{table:cost_llms}
\end{table}

\subsection{Referenced tools}
Below are links to several online tools mentioned in this paper:
\begin{links}
    \link{Tesseract}{https://github.com/tesseract-ocr/tesseract}
    \link{LLM-Aided OCR}{https://github.com/Dicklesworthstone/llm_aided_ocr}
    \link{BetterOCR}{https://github.com/junhoyeo/BetterOCR}
\end{links}

\subsection{Examples of documents}
Figures~\ref{fig:iam_left} and \ref{fig:iam_right} show an example document from the IAM Handwriting Database, and cropped versions used for \iam{}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{example_figs/iam_raw_examplea01-000x.jpg}
    \caption{An example of a document from the IAM Handwriting Database.}
    \label{fig:iam_left}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{example_figs/iam_example_a01-000u.jpg} \\[0.5em]
    \includegraphics[width=0.75\linewidth]{example_figs/iam_example_a01-003u.jpg}
    \caption{An example of a constructed multi-page \texttt{IAM} document, combining two pages from the same writer with the machine-printed text cropped out.}
    \label{fig:iam_right}
\end{figure}

\subsection{Examples of \firstpage{} extrapolating OCR error correction from a single page}
\label{sec:firstpage_examples}
All examples use Google Cloud Vision as the OCR engine and \textsc{gpt-4o} as the (M)LLM.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{example_figs/draws_example.pdf}
    \caption{With \firstpage{}, the correctly-transcribed occurrence of `draws' in the first page can be extrapolated to the unseen `draw' on the second page.}
    \label{fig:example_draws}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{example_figs/healy_example.pdf}
    \caption{With \firstpage{}, the correctly-transcribed occurrence of the name `Mr Healy' in the first page can be extrapolated to the unseen occurence on the second page.}
    \label{fig:example_healy}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{example_figs/wide_example.pdf}
    \caption{\firstpage{} corrects where \textsc{ocr only}$\rightarrow$LLM gets it wrong, despite \textit{only having access only to the garbled OCR output} and not the image of the word `wide' shown above. Suggests some degree of reasoning using the seemingly irrelevant first page text --- i.e. it can see that `m's on page 1 look similar to `w's and reason that `mide' could be `wide'.}
    \label{fig:example_wide}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{example_figs/columns_example.pdf}
    \caption{An unusual case where the OCR engine erroneously breaks the text into two columns for both pages. This is shown using the dotted line and underlining of the transcribed text. OCR$\rightarrow$LLM preserves this error. \firstpage{} trivially corrects it in the first page --- it has access to the image --- \textit{but also corrects it in the second page}, even though it has no access to that image. \firstpage{} correctly rearranges the text on page 2 using only context and the inferred formatting from page 1.}
    \label{fig:example_columns}
\end{figure}



\section{Acknowledgments}
Parts of this work were completed by B.G. while undertaking an internship at QuantCo, working with T.K.

B.G. and M.J. acknowledge support from the EPSRC Centre for Doctoral Training in AIMS (EP/S024050/1).

X.D. acknowledges support from the Oxford-Man Institute of Quantitative Finance and the EPSRC (EP/T023333/1).

We are grateful for anonymous reviewer feedback.

The authors would like to thank the organizers of the AAAI-25 Workshop on Document Understanding and Intelligence for accepting this paper for presentation, to be held at the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25) in Philadelphia, PA, USA.

\paragraph{Figures.} We acknowledge the use of (both original and edited) open-licensed SVG vectors from SVG Repo:
\begin{links}
\link{Images clipart by Ionicons}{https://www.svgrepo.com/svg/327088/images-sharp}
\link{Robot clipart by Konstantin Filatov}{https://www.svgrepo.com/svg/521818/robot}
\link{Documents clipart by SVG repo}{https://www.svgrepo.com/svg/139884/documents-papers}
\end{links}
% QuantCo, EPSRC/AIMS

\bibliography{aaai25, my, extras}

\end{document}
