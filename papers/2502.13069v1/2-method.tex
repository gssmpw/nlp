\section{Method}
\label{sec:Method}


\subsection{Dataset}
In our experiments, we simulate well-specified and underspecified inputs using the SWE-Bench Verified dataset, a refined subset of 500 issues from the SWE-Bench dataset. The SWE-Bench dataset~\cite{jimenez2024swebenchlanguagemodelsresolve} consists of real-world GitHub issues, their corresponding pull requests (PRs), and unit tests from 12 Python repositories. The SWE-Bench Verified dataset~\cite{SWE_Bench_Verified} is designed to provide a more reliable estimate of an LLM's ability by pruning issues that were underspecified or contained invalid unit tests. The task of an LLM is to modify the state of the repository at the time of creation of the issue and resolve it. The test cases are used to verify the patch generated by the agent. 

Given that the Verified subset contains only sufficiently specified issues, we assume that these issues do not require disambiguation. Therefore, for each SWE-Bench Verified issue, we consider two forms, as shown in Figure~\ref{fig:settings}:

\begin{enumerate}[itemsep=0pt, topsep=0pt]
    \item \textbf{Fully specified issue}: The original and detailed GitHub issue.
    \item \textbf{Underspecified issue}: A summarized version generated using GPT-4o, where the model is asked to preserve specific terminology is preserved but reduce the amount of detailed content (complete prompt in \secref{subsec:prompt}).
\end{enumerate}
\vspace{-8pt}
\subsection{Agentic Framework}
\paragraph{Agent Environment} The OpenHands~\cite{wang2024openhandsopenplatformai} agentic framework equips the LLM with an interactive environment that extends its capabilities beyond static code generation. The agent operates within a structured execution environment where it can iteratively refine code, plan tasks, and run commands using integrated tools. It has the ability to edit files, break down complex instructions into executable steps, and execute both Bash and Python scripts within a secure sandbox. This controlled environment enables the agent to analyze execution outputs, detect and debug errors, and refine its approach based on observed results, ensuring adaptability and correctness in solving complex programming tasks.

\paragraph{Selected Models} We use \textit{Claude Sonnet 3.5}~\cite{anthropic2024claudesonnet} as one of the proprietary models due to its superior performance on SWE-Bench. \textit{Claude Haiku 3.5}~\cite{anthropic2024claude35haiku} is included as the second proprietary model to investigate the impact of model parameterization, as both models likely share similar training methodologies but differ significantly in the number of parameters. Additionally, we evaluate \textit{Llama 3.1 70B-Instruct}~\cite{llama3} and \textit{Deepseek-v2}~\cite{deepseekv2} as two open-weight frontier models.
% \vspace{-3pt}

\paragraph{User Proxy} We employ GPT-4o~\cite{openai2024gpt4o} as a user proxy to simulate user-agent interactions~\cite{xu2024theagentcompanybenchmarkingllmagents, zhou2024haicosystem}. The user proxy is provided with the fully specified version of the task, allowing the coding agent to extract the necessary information through interaction. It is instructed to respond based solely on the information available in the full issue and will reply with \textit{I don't have that information} if relevant details are missing. This approach prevents the user proxy from hallucinating incorrect information and encourages clear, negative responses when needed. The full prompt is shown in \secref{subsec:interaction}.
\FloatBarrier

\subsection{Study Design}


We use three distinct settings to evaluate models across the 500 issues from SWE-Bench Verified shown in Figure~\ref{fig:settings} and described below.
\begin{itemize}

    \item \textbf{Full Setting}: This is the traditional SWE-Bench setting for resolving GitHub issues. The coding agent is provided with the fully specified task and the interaction is disabled. It represents the agent's performance in an unambiguous scenario, where the agent has access to \textit{full} information, simulating ideal conditions.
    \item \textbf{Hidden setting}: A summarized version of the issue is provided to the coding agent with the user-agent interaction disabled to mimic the lack of detail that can occur in task descriptions. We do not give any interaction-related instructions, and all models default to non-interactive behavior. Specific details are \textit{hidden} from the coding agent.
    \item \textbf{Interaction Setting}: The coding agent receives a summarized task, while the user proxy model gets the fully specified task. Interaction is enabled through prompting, allowing the agent to query the proxy for specific details. The models do not interact with the user without an explicit prompt. In addition to the full issue, the proxy has access to file locations that need modification and can provide them when queried. This setup allows us to evaluate which models proactively seek navigational information and examine how this interaction influences the success of the solution process across models.
\end{itemize}


