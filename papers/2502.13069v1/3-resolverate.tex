\section{RQ1: Interactive Problem Solving}
\label{sec:ProblemSolving}

Effectively addressing ambiguity requires a model to integrate information from user interactions to form a clear plan and successfully solve the task. Our first experiment holistically evaluates the model’s ability to leverage interaction and improve performance. The model must not only process the initial task description, but also query users to extract relevant details while filtering out irrelevant information. 

\subsection{Experimental Setup}
The hypothesis of the experiment is that different language models will exhibit varying performance with interaction based on their incorporation of the provided information, leading to different levels of improvement over the Hidden setting. We evaluate the models across the three settings and conduct two Wilcoxon-Signed Rank tests with a significance level of 0.05 to determine significant performance differences between the Hidden and Interaction settings, and between the Interaction and Full settings for every model. Here, we modify the prompt to make interaction with the user compulsory in the Interaction setting\footnote{Without compulsory interaction, the model defaults to non-interactive behavior for most issues, as seen in the Hidden setting. Full prompt in \secref{subsec:interaction}}. Ideally, the Interaction setting should approach the performance of the full setting. The coding agent has a maximum of 30 turns to generate a solution patch.

\subsection{Leveraging Interaction in Ambiguity}
In this experiment, each model is tested in the \textit{Hidden}, \textit{Interaction}, and \textit{Full settings} to evaluate its ability to leverage interaction and optimize performance on underspecified issues. The results, as shown in Figure~\ref{fig:resolve_rates}, confirm the expected increase in resolve rates as more information becomes available to the agent. While the difference between the Hidden and Interaction settings is \textit{significant} for every model (Table~\ref{tab:wilcoxon_results}), emphasizing the impact of interaction on the trajectory, the performance gap between the Interaction and Full settings is also \textit{significant} across all models, highlighting the unrealized potential. Specifically, for the Hidden vs. Interaction settings, proprietary models show stronger evidence of a significant difference. These results suggest that the ability to leverage interaction varies across models, with proprietary models demonstrating greater effectiveness in utilizing interaction compared to open-weight models.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\columnwidth]{resolve_rates.pdf}
	\caption{Resolve rates (in \%) across different settings: Hidden (underspecified issues), Interaction (underspecified issues with user interaction), and Full (fully specified issues)}
    \vspace{-12pt}
	\label{fig:resolve_rates}
\end{figure}


Using interaction, the Claude Sonnet and Haiku agents recreate 80\% of the performance in the Full setting. However, with Deepseek and Llama 3.1, the relative performance is lower, of 59\% and 54\%, respectively. Claude Sonnet 3.5's high resolve rate in the Hidden setting is likely due to its superior programming acumen. The performance is surprising, as a human would be able to decipher little about the expectations given the summarized issue. Better programming models can potentially extract more information from the stack trace by reproducing the error themselves. We observe that the Claude Haiku model achieves a performance relative to the Full setting similar to that of the Claude Sonnet model, despite having inferior coding abilities. Thus, there is no direct correlation between the number of parameters or coding ability and a model's ability to leverage interaction. This hints towards better training practices that can lead to better integration of the new information.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Comparison} & \textbf{p-value} \\
        \midrule
        \multirow{2}{*}{Llama 3.1 70B} & Hidden vs Interaction & 0.0023 \\
                                       & Interaction vs Full & 3.87e-06 \\
        \midrule
        \multirow{2}{*}{Claude Haiku 3.5} & Hidden vs Interaction & 2.18e-14  \\
                                          & Interaction vs Full & 1.65e-09  \\
        \midrule
        \multirow{2}{*}{Claude Sonnet 3.5} & Hidden vs Interaction & 8.55e-19  \\
                                           & Interaction vs Full & 1.28e-12 \\
        \midrule
        \multirow{2}{*}{Deepseek-v2} & Hidden vs Interaction & 0.0023  \\
                                       & Interaction vs Full & 2.87e-07 \\
        \bottomrule
    \end{tabular}
    \caption{Wilcoxon Signed-Rank Test Results for Hidden vs Interaction setting and for Interaction vs Full setting for each model}
    \label{tab:wilcoxon_results}
\end{table}
This experiment highlights the importance of interaction in mitigating ambiguity. Since many real-world software engineering problems are underspecified~\cite{SWE_Bench_Verified}, interactive systems are essential for ensuring alignment and reducing safety risks. However, current models default to non-interactive behavior even when faced with ambiguity and struggle to match the performance seen in well-specified settings. While interactive trajectories show performance gains over non-interactive approaches for ambiguous inputs, the improvement is not statistically significant, indicating strong potential for improvement.

\subsection{Impact of Interaction Details on Model Performance}
\begin{table*}[ht]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Navigational Info (\%)} & \textbf{Resolve Rate With (\%)} & \textbf{Resolve Rate Without (\%)} \\
\midrule
Claude Sonnet 3.5 & 8.96  & 59.52 & 37.94 \\
Claude Haiku 3.5  & 24.67 & 36.94 & 24.78 \\
Deepseek-v2     & 30.70 & 13.19 & 4.62  \\
Llama 3.1 70B     & 30.28 & 6.34  & 4.28  \\
\bottomrule
\end{tabular}
\caption{The percentage of issues where navigational information was acquired in the Interaction setting, along with the corresponding resolve rates with and without navigational information. Navigational information refers to file locations requiring modification, helping to avoid tedious code exploration. The resolve rates offer insight into how the information obtained during interaction impacts overall performance.}
\vspace{-15pt}

\label{tab:navigational_questions_files}
\end{table*}

In the Interaction setting of the previous experiment, the information gained can be broadly categorized into two types: \textbf{informational}, which relates to the expected behavior or nature of the error, and \textbf{navigational}, which pertains to the locations of the files to modify. While informational details are typically obtained in nearly every interaction, the models request navigational details less frequently. We measure the resolve rates separately for instances where the model asks for navigational details and when it does not, examining the impact on performance when models must rely only on informational details versus when navigational details are also accessible.

As seen in Table~\ref{tab:navigational_questions_files}, requesting navigational details improves performance across all models by providing cues beyond described behavior and errors. However, some models rely too heavily on this information and struggle when it’s missing. Smaller models like Llama 3.1 and Deepseek-v2 request file locations more often but underperform without them. Claude models, particularly Sonnet, better leverage informational cues, achieving higher resolve rates even without navigational details. Deepseek, by contrast, performs worse than its Hidden setting when file locations are absent, highlighting its dependence. This reliance leads to wasted turns searching for errors instead of identifying them efficiently. Llama 3.1 performs better than Hidden without file locations but gains little when they are provided, likely due to poor detail extraction (Section~\secref{sec:QuestionQuality}). Ideally, LLMs should generalize across diverse interaction types, as users may not always provide specific details, improving robustness in real-world software engineering tasks.

\textbf{\textit{Takeaway:}} Interaction has significant potential to improve model performance in ambiguous tasks, but models, particularly the less strong open-weight models, struggle to leverage it effectively. Proprietary models like Claude Sonnet 3.5 and Haiku 3.5 achieve nearly 80\% of their Full setting performance, with Haiku improving by 74\% over its Hidden setting performance through effective integration of both informational and navigational cues. The lack of correlation between model size and its ability to utilize interaction suggests that better training practices play a more crucial role. In contrast, models like Deepseek-v2 and Llama 3.1 show limited gains, primarily due to their challenges in utilizing broader informational cues, which hinders their adaptability in ambiguous tasks.







