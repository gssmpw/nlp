\section{RQ3: Question Quality}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{questions.pdf}
    \vspace{-20pt}
    \caption{Agent questions and user responses to the same underspecified input are shown for Llama 3.1 70B, Deepseek-v2, and Claude Haiku 3.5. The examples highlight specific interaction patterns and differences in handling ambiguity. The corresponding model inputs are detailed in Table~\ref{tab:issue-analysis}.}
    \vspace{-12pt}
    \label{fig:questions}
\end{figure*}
\label{sec:QuestionQuality}
To gather missing information from underspecified inputs, the quality of an agentâ€™s questions is crucial. While \secref{sec:ProblemSolving} evaluates task completion, the model performance in the experiment is influenced by the coding ability. Here, we focus solely on the quality of the questions posed by the agent to the user, measuring how effectively models extract relevant information under the assumption that users have the necessary details.

\subsection{Experimental Setup}
In this experiment, we evaluate the quality of the interactions between the agent and the user in the Interaction setting. We measure the novelty and detail level of the information obtained from the user's answers to evaluate the quality, quantifying the new knowledge relative to the existing understanding of the agent. We employ two techniques to quantify the information obtained. 
\begin{enumerate}[itemsep=0pt, topsep=0pt]
\item \textbf{Cosine Distance}: 
We compute the cosine distance ($1-cos(P,Q)$) between the embeddings of the summarized task \( E_{\text{before}} \) and the cumulative knowledge after interaction with the user \( E_{\text{after}} \) using a text embedding model. Lower distances indicate redundant user input, while higher values show meaningful information gain. We use OpenAI's text-embedding-3-small as our embedding model. 

\item \textbf{LLM-as-judge (GPT-4o)}: The model scores the user answers on a scale of 1 to 5, where a higher score corresponds to more new and detailed information in the user's response, such as specific files causing errors or function behavior. The prompt to the model includes the summarized issue, agent questions, and user responses for better context.
\end{enumerate}


\subsection{Information Gain from Interaction}
For the quantitative evaluation of the quality of the question, both the cosine distance and the LLM-as-judge methods suggest a similar result, in which the Llama model performs significantly worse than the other models, whereas the other models achieve very similar information gains, as seen in Figure~\ref{fig:question_quality}.

The Llama model has an average cosine distance of 0.101 when the embedding of the summarized issue is compared to the embedding of the user response appended to the summarized issue. Deepseek achieves the highest cosine distance of 0.142, while the Claude Sonnet and Haiku models achieve very similar cosine distances of 0.136 and 0.135.

Using LLM as a judge, we evaluate the specificity of the details present in the answers. Here again, the Llama 3.1 model achieves a significantly worse average score of 3.58 than the other models which see similar performance of around 4 out of 5.



\begin{figure}[h!]
	\centering
	\includegraphics[width=180pt]{question_quality.pdf}
	\caption{Information Gain measured using (a) Cosine Distance Scores and (b) LLM-as-Judge Scores 
    }
	\vspace{-12pt}\label{fig:question_quality}
\end{figure}


\subsection{Qualitative Analysis of Questions}

The quantitative results can be further supported by a qualitative evaluation of the questions. Sample question-answer pairs reflecting common trends are shown in Figure~\ref{fig:questions}. The Llama model asks fewer questions on average than other models in one message for user interaction, as seen in Table~\ref{tab:average_questions}, and often poses overly general questions like, \textit{Are there any existing workarounds or temporary fixes?}. These template-like questions are unproductive and less likely to gather useful information. 

Deepseek, on the other hand, asks the most questions per message, allowing it to extract more information. Its questions, such as \textit{Are there any existing tests or examples that demonstrate the issue?}, aim to extract, edge cases, documentation, or tests, and while common across multiple issues, they are reasonable and yield valuable details. But most questions are very specific and detailed, querying about the expected behavior. Often, due to the specificity of the question, the user might not have the required information.


Claude Sonnet asks fewer questions than Deepseek, likely because it explores the codebase first. The questions do not have easily discernible patterns and match the Deepseek model in specificity. The Haiku model, in contrast, follows a consistent template, typically asking three questions regardless of the input, although sub-questions may be present. Haiku's questions are more keyword-driven based on the input, while Sonnet's are based on a deeper understanding of the issue and codebase.

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Avg. Number of Questions} \\
\midrule
Claude Sonnet 3.5 & 3.80 \\
Claude Haiku 3.5  & 3.49 \\
Deepseek-v2     & 4.57 \\
Llama 3.1 70B     & 2.61 \\
\bottomrule
\end{tabular}
}
\caption{The average number of questions asked by different models in an interaction.}
\label{tab:average_questions}
\end{table}

\textbf{\textit{Takeaway:}} Models that balance specificity and question quantity, like the Claude models, achieve greater information gain and superior interaction quality compared to models that ask too few, too many, or templated questions. While Deepseek benefits from asking numerous detailed questions, it risks overwhelming the user. In contrast, Llama underperforms due to its reliance on generic or irrelevant questions.
\vspace{-12pt}