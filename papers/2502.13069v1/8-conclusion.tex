\section{Conclusion}
\label{sec:Conclusion}
This work evaluates proprietary and open-weight models in agentic frameworks for handling ambiguity in software engineering. In code generation, to effectively integrate new information into the solution, an agent must detect ambiguity and ask targeted questions. Our key findings are:
\begin{itemize}[itemsep=0pt, topsep=0pt]
    \item Given an underspecified input, Claude Sonnet 3.5 and Claude Haiku 3.5 with interaction can achieve 80\% of their performance with a well-specified input. In contrast, open-weight models struggle: Deepseek relies on navigational cues to locate relevant files, while Llama 3.1 70B extracts limited information from the user.
    \item LLMs do not interact unless explicitly prompted, and their ambiguity detection is highly sensitive to prompt variations. Only Claude Sonnet 3.5 achieves a higher accuracy of 84\% in distinguishing between well-specified and underspecified input.

    \item Claude Sonnet 3.5, Haiku 3.5, and Deepseek effectively extract new, detailed user information, whereas Llama 3.1 struggles to ask the right questions.
    
\end{itemize}
Despite these advances, a gap remains between resolve rates for underspecified vs. fully specified issues. Open-weight models need better interaction strategies to improve resolution, while proprietary models, particularly Claude Haiku 3.5, require stronger prompting to engage interactively. This work establishes the current state-of-the-art in handling ambiguity through interaction, breaking the resolution process into multiple steps.


