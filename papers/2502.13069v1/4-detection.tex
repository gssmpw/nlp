\section{RQ2: Ambiguity Detection}
\label{sec:AmbiguityDetection}
In real-world LLM and agent applications, task descriptions and prompts can vary in quality~\cite{SWE_Bench_Verified}.
To detect ambiguity, a model must recognize unclear expectations or identify missing key information in its planned approach. However, interacting unnecessarily when sufficient information is already available can introduce inefficiencies and place an undue burden on the user. Here, we examine the capabilities of LLMs to detect ambiguous instructions in software engineering contexts.

\subsection{Experimental Setup}
In this experiment, each issue is presented in either the \textit{Full setting} or the \textit{Hidden setting}. The objective is to identify patterns in how models choose to interact based on the input type. Ideally, the model should have a high interaction rate for the summarized inputs and a negligible interaction rate for the well-specified inputs. 

In the instructions which outline the task, we present the agent with an option to interact during its solution trajectory and design three instructions with varying levels of encouragement to interact with the user. We track the input type the model chooses to interact with. The instructions, listed in order of increasing encouragement to interact, are: \textit{Neutral}, where the agent is told it can ask questions if anything is unclear), \textit{Moderate Encouragement}, where the agent is told to carefully check that all necessary information is available and only proceed after everything is clear, and \textit{Strong Encouragement}, where the agent is told that asking questions is critical to task success~(full prompts in \secref{sec:appendix}). 

\begin{table*}[h!]
    \centering
    \renewcommand{\arraystretch}{1.1} 
    \setlength{\tabcolsep}{4pt} 
    \begin{tabular}{lccc ccc ccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Neutral}} & \multicolumn{3}{c}{\textbf{Moderate Encouragement}} & \multicolumn{3}{c}{\textbf{Strong Encouragement}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
         & \textbf{Accuracy ↑} & \textbf{FPR ↓} & \textbf{FNR ↓} & \textbf{Accuracy ↑} & \textbf{FPR ↓} & \textbf{FNR ↓} & \textbf{Accuracy ↑} & \textbf{FPR ↓} & \textbf{FNR ↓} \\
        \midrule
        Claude Sonnet 3.5  & 0.60 & 0.00 & 0.81 & \textbf{0.84} & \textbf{0.24} & \textbf{0.09} & 0.76 & 0.36 & 0.10 \\
        Claude Haiku 3.5   & 0.54 & 0.00 & 0.97 & 0.57 & 0.02 & 0.90 & 0.63 & 0.06 & 0.66 \\
        Deepseek-v2      & 0.69 & 0.30 & 0.31 & 0.57 & 0.08 & 0.83 & 0.51 & 0.04 & 0.94 \\
        Llama 3.1 70B      & 0.48 & 0.46 & 0.57 & 0.47 & 0.95 & 0.09 & 0.52 & 0.93 & 0.06 \\
        \bottomrule
    \end{tabular}
    \caption{Model performance in ambiguity detection across prompts with varying levels of interaction encouragement. FPR refers to cases where the model interacted unnecessarily, while FNR refers to cases where it failed to interact when needed. A model that reliably distinguishes between underspecified and well-specified issues should have high accuracy, low FPR, and low FNR.}
    \vspace{-12pt}
    \label{tab:detection_metrics}
\end{table*}


\subsection{Effect of Different Prompts}
Experiments to detect ambiguity demonstrate that, using prompt engineering, we can control the level of interaction with the user, as shown in Table~\ref{tab:detection_metrics}. But this interactivity is not possible without clearly specifying it in the prompt wherein without any specific mention of interaction, the models almost never interact for any of the summarized issue inputs. 

The Claude Sonnet model performs best with Moderate Encouragement, achieving the highest overall accuracy of 84\% across all variations. Its counterpart from the same model family, Claude Haiku, is hesitant to interact even with Strong Encouragement. The Claude models show a drop in accuracy in cases where interaction is not needed as their overall interaction increases, indicating that the interaction fails to target underspecified inputs effectively. For the Deepseek model, we observe that the Neutral prompt gives the best results as interactivity surprisingly decreases with more encouragement. The accuracy in both the cases where interaction was desired and not desired is around 70\%, which shows that the model is capable of distinguishing between well-specified and underspecified issues to some extent. The Llama model displays a greater, but arbitrary, tendency to interact across all prompts than other models. 

\subsection{Detection across Models}
While interaction levels can be adjusted with prompting, both summarized issues and full issues have equal probability of being selected for interaction as interactivity increases, particularly with smaller models. Despite the stark difference in the language and detail of summarized issues and fully specified issues, the models, except Claude Sonnet, fail to reliably distinguish them, indicating that LLMs struggle to detect ambiguity even in obvious cases. All models, including Claude Sonnet, show big changes in the ambiguity detection behavior with prompt variations. Interestingly, Sonnet outperforms Haiku, likely due to its more extensive instruction tuning or Human Feedback training, which helps it better follow instructions and achieve the desired interactive trajectory. Surprisingly, even Deepseek adapts better to the task than Haiku.

\textbf{\textit{Takeaway:}}
Prompt engineering can influence model interactivity but fails to consistently improve ambiguity detection across models. When interaction is not explicitly prompted, models default to non-interactive behavior. Claude Sonnet shows some ability to distinguish ambiguous inputs, but other models, including Claude Haiku and Llama 3.1, struggle even with clear cues. This inconsistency reveals that models are not inherently equipped to detect underspecified tasks. Improving ambiguity detection requires dedicated training, not just prompt modifications.