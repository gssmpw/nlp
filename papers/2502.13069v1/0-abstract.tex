
\begin{abstract}
AI agents are increasingly being deployed to automate tasks, often based on ambiguous and underspecified user instructions. Making unwarranted assumptions and failing to ask clarifying questions can lead to suboptimal outcomes, safety risks due to tool misuse, and wasted computational resources. In this work, we study the ability of LLM agents to handle ambiguous instructions in interactive code generation settings by evaluating proprietary and open-weight models on their performance across three key steps: (a) leveraging interactivity to improve performance in ambiguous scenarios, (b) detecting ambiguity, and (c) asking targeted questions. Our findings reveal that models struggle to distinguish between well-specified and underspecified instructions. However, when models interact for underspecified inputs, they effectively obtain vital information from the user, leading to significant improvements in performance and underscoring the value of effective interaction. Our study highlights critical gaps in how current state-of-the-art models handle ambiguity in complex software engineering tasks and structures the evaluation into distinct steps to enable targeted improvements.\footnote{Code and data can be accessed at \url{https://github.com/sani903/InteractiveSWEAgents}}
\end{abstract}
\begin{figure}[th]
	\centering
	\includegraphics[width=\columnwidth]{main-refined.pdf}
    \vspace{-20pt}
	\caption{Interactive agents mitigate resource wastage and reduce misalignment in ambiguous settings.}
    \vspace{-15pt}
	\label{fig:overview}
\end{figure}
\vspace{-18pt}