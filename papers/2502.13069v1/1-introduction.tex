\section{Introduction}
\label{sec:Introduction}


Large Language Models (LLMs) are increasingly used as chatbots in task-oriented workflows to improve productivity~\cite{peng2023impactaideveloperproductivity, NBERw31161}, with the user providing a task instruction which the model completes. 
Due to the interactive nature of chatbots, the performance depends on the information provided in the user's prompt. Users often provide non-descriptive instructions, which poses critical challenges in successfully completing the task~\cite{SWE_Bench_Verified}. The ambiguity can lead not only to erroneous outcomes, but also to significant safety issues~\cite{kim2024aligninglanguagemodelsexplicitly, karli2023extended}. 


This ambiguity can lead to more severe consequences in task automation scenarios, where AI agents are equipped with powerful tools~\cite{wang2024openhandsopenplatformai,lu2024aiscientistfullyautomated,huang2024agentcodermultiagentbasedcodegeneration,zhou2024haicosystem}. In software engineering settings, agents must navigate complex codebases, make architectural decisions, and modify critical systemsâ€”all while operating with potentially incomplete or ambiguous instructions. When human developers face such ambiguity, they engage in clarifying dialogue to gather missing context~\cite{testoni_humansaskquestions, purver2004clarification}. However, current AI systems often proceed with incomplete understanding, leading to costly mistakes and misaligned solutions, as demonstrated in Figure~\ref{fig:overview}. 

In this work, we systematically evaluate the interaction capabilities of commonly used open and proprietary LLMs when addressing underspecified instructions in agentic code settings (\S\ref{sec:Method}). We examine three research questions to address the problem for code generation.
\begin{enumerate}[itemsep=0pt, topsep=0pt]
    \item \textbf{Interactive problem solving}: Can LLMs appropriately leverage interaction with the user to improve performance in ambiguous settings?
    \item \textbf{Detection of ambiguity}: Can LLMs identify whether a given task description is underspecified and ask clarifying questions?
    \item \textbf{Question quality}: Can LLMs generate meaningful and targeted questions that gather the necessary information to complete the task?
\end{enumerate}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{revised_settings.pdf}
    \vspace{-20pt}
    \caption{The three settings in order: Full, Hidden, and Interaction}
    \label{fig:settings}
    \vspace{-12pt}
\end{figure*}
We evaluate the research questions separately to ensure independence between them. We use the Github issues from SWE-Bench Verified~\cite{SWE_Bench_Verified} to simulate well-specified inputs, and the summarized variants of the same Github issues as underspecified inputs for the experiments. A simulated user~\cite{xu2024theagentcompanybenchmarkingllmagents, zhou2024sotopiainteractiveevaluationsocial}, equipped with the full, well-specified issue, simulates real conversations where the user has additional context, which is provided only when prompted with the appropriate questions. This multi-stage approach allows for targeted improvements in individual aspects, offering a pathway to enhance overall system performance.


 Through our evaluations across the different settings, we find that interactivity can boost performance on underspecified inputs by up to 74\% over the non-interactive settings but the performance varies between models~(\secref{sec:ProblemSolving}). LLMs default to non-interactive behavior without explicit encouragement, and even with it, they struggle to distinguish between underspecified and well-specified inputs. Claude Sonnet 3.5 is the only evaluated LLM that achieves notable accuracy (84\%) in making this distinction. Prompt engineering offers limited improvement, and its effectiveness varies across models~(\secref{sec:AmbiguityDetection}). When interacting, LLMs generally pose questions capable of extracting relevant details, but some models, such as Llama 3.1 70B, fail to obtain sufficient specificity~(\secref{sec:QuestionQuality}). In summary, this study underscores the importance of interactivity in LLMs for agentic workflows, particularly in real-world tasks where prompt quality varies significantly.

\vspace{-10pt}