% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|ccc|ccc|ccc|ccc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & \multicolumn{6}{c}{\textbf{MBPP}} & \multicolumn{6}{c}{\textbf{Code Contests}} \\
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.1-8B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c}{\itshape Llama-3.1-8B-Instruct} \\
% \midrule
% & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 \\
% \midrule
% Base ($t=1$) & & & & & & & & & & & & \\
% \hspace{1em} + SFT & & & & & & & & & & & & \\
% \midrule
% SiM ($t=3$) & & & & & & & & & & & & \\
% \hspace{1em} + SFT & & & & & & & & & & & & \\
% \midrule
% Multi ($t=3$) & & & & & & & & & & & & \\
% \hspace{1em} + SFT & & & & & & & & & & & & \\
% \midrule
% DAGGER ($t=3$) & & & & & & & & & & & & \\
% \hspace{1em} + Iter 1 & & & & & & & & & & & & \\
% \hspace{1em} + Iter 2 & & & & & & & & & & & & \\
% \hspace{1em} + Iter 3 & & & & & & & & & & & & \\
% \midrule
% Ours ($t=3$) & & & & & & & & & & & & \\
% \hspace{1em} + Iter 1 & & & & & & & & & & & & \\
% \hspace{1em} + Iter 2 & & & & & & & & & & & & \\
% \hspace{1em} + Iter 3 & & & & & & & & & & & & \\
% \bottomrule
% \end{tabular}
% \caption{\GG{This is an empty template with multiple datasets.} Performance comparison of approaches across MBPP and Code Contests datasets with different models.}
% \label{tab:comparison}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|ccc|ccc|ccc|ccc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & \multicolumn{6}{c}{\textbf{MBPP}} & \multicolumn{6}{c}{\textbf{Code Contests}} \\
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.1-8B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c}{\itshape Llama-3.1-8B-Instruct} \\
% \midrule
% & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 \\
% \midrule
% Base ($t=1$) & $52.0$ & 63 & 74 & 58 & 69 & 78 & 39 & 45 & 54 & 42 & 51 & 60 \\
% \hspace{1em} + SFT & $51.0_{-1.0}$ & 62 & 74 & 57 & 68 & 78 & 37 & 44 & 52 & 41 & 49 & 59 \\
% \midrule
% SiM ($t=3$) & $56.0$ & 66 & 77 & 61 & 72 & 82 & 43 & 51 & 60 & 47 & 55 & 65 \\
% \hspace{1em} + SFT & $57.0_{+1.0}$ & 67 & 78 & 62 & 73 & 82 & 44 & 52 & 60 & 48 & 56 & 65 \\
% \midrule
% Multi ($t=3$) & $61.0$ & 71 & 81 & 65 & 75 & 84 & 46 & 54 & 62 & 49 & 57 & 66 \\
% \hspace{1em} + SFT & $62.0_{+1.0}$ & 72 & 82 & 66 & 76 & 85 & 47 & 55 & 63 & 50 & 58 & 67 \\
% \midrule
% DAGGER ($t=3$) & $63.0$ & 73 & 83 & 67 & 77 & 86 & 48 & 57 & 65 & 51 & 59 & 68 \\
% \hspace{1em} + Iter 1 & $64.0_{+1.0}$ & 75 & 84 & 68 & 78 & 87 & 49 & 57 & 65 & 52 & 60 & 69 \\
% \hspace{1em} + Iter 2 & $66.0_{+3.0}$ & 76 & 85 & 69 & 79 & 88 & 51 & 58 & 66 & 53 & 62 & 70 \\
% \hspace{1em} + Iter 3 & $67.0_{+4.0}$ & 76 & 86 & 70 & 80 & 89 & 51 & 59 & 67 & 54 & 62 & 71 \\
% \midrule
% Ours ($t=3$) & $63.0$ & 73 & 83 & 68 & 78 & 87 & 49 & 57 & 65 & 53 & 61 & 69 \\
% \hspace{1em} + Iter 1 & $66.0_{+3.0}$ & 74 & 85 & 69 & 79 & 88 & 50 & 59 & 66 & 53 & 61 & 70 \\
% \hspace{1em} + Iter 2 & $69.0_{+6.0}$ & 75 & 85 & 70 & 80 & 89 & 51 & 59 & 67 & 54 & 62 & 71 \\
% \hspace{1em} + Iter 3 & $71.0_{+8.0}$ & 76 & 86 & 71 & 81 & 90 & 52 & 60 & 68 & 55 & 63 & 72 \\
% \bottomrule
% \end{tabular}
% \caption{\GG{These is a template with dummy values} Performance comparison of approaches across MBPP and Code Contests datasets with different models.}
% \label{tab:comparison}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|ccc|ccc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & \multicolumn{6}{c}{\textbf{MBPP}} \\
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} & \multicolumn{3}{c}{\itshape Llama-3.1-8B-Instruct} \\
% % \midrule
% & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 \\
% \midrule
% Base ($t=1$) & $\mathbf{38.8}$ & $\mathbf{45.8}$ & $\mathbf{56.9}$ & $\mathbf{58.4}$ & $\mathbf{71.6}$ & $\mathbf{78.8}$ \\
% \hspace{1em} + SFT & $37.1_{-1.9}$ & $50.7_{+4.9}$ & $58.9_{+2.0}$ & $60.3_{+1.2}$ & $72.0_{+0.4}$ & $77.8_{-1.0}$ \\
% \midrule
% SiM ($t=3$) & $\mathbf{21.7}$ & $\mathbf{46.2}$ & $\mathbf{55.8}$ & $\mathbf{58.2}$ & $\mathbf{71.8}$ & $\mathbf{78.0}$ \\
% \hspace{1em} + SFT & $37.0_{+14.3}$ & $50.9_{+4.7}$ & $57.9_{+2.1}$ & $60.7_{+2.5}$ & $72.2_{+0.4}$ & $77.1_{-0.9}$ \\
% \midrule
% Multi ($t=3$) & $48.0$ & $73.0$ & $83.0$ & $67.0$ & $78.0$ & $86.0$ \\
% \hspace{1em} + SFT & $49.0_{+1.0}$ & $71.0_{-2.0}$ & $80.0_{-3.0}$ & $68.0_{+1.0}$ & $77.0_{-1.0}$ & $84.0_{-2.0}$ \\
% \midrule
% DAgger ($t=3$) & $48.0$ & $73.0$ & $83.0$ & $67.0$ & $78.0$ & $86.0$ \\
% \hspace{1em} + Iter 1 & $49.0_{+1.0}$ & $75.0_{+2.0}$ & $84.0_{+1.0}$ & $68.0_{+1.0}$ & $79.0_{+1.0}$ & $87.0_{+1.0}$ \\
% \hspace{1em} + Iter 2 & $51.0_{+3.0}$ & $76.0_{+3.0}$ & $85.0_{+2.0}$ & $69.0_{+2.0}$ & $80.0_{+2.0}$ & $88.0_{+1.0}$ \\
% \hspace{1em} + Iter 3 & $52.0_{+4.0}$ & $76.0_{+3.0}$ & $86.0_{+1.0}$ & $70.0_{+3.0}$ & $81.0_{+3.0}$ & $89.0_{+1.0}$ \\
% \midrule
% Ours ($t=3$) & $48.0$ & $73.0$ & $83.0$ & $67.0$ & $78.0$ & $86.0$ \\
% \hspace{1em} + Iter 1 & $51.0_{+3.0}$ & $74.0_{+1.0}$ & $85.0_{+2.0}$ & $68.0_{+1.0}$ & $79.0_{+1.0}$ & $88.0_{+2.0}$ \\
% \hspace{1em} + Iter 2 & $54.0_{+6.0}$ & $77.0_{+4.0}$ & $89.0_{+6.0}$ & $70.0_{+3.0}$ & $81.0_{+3.0}$ & $89.0_{+3.0}$ \\
% \hspace{1em} + Iter 3 & $56.0_{+8.0}$ & $78.0_{+5.0}$ & $90.0_{+7.0}$ & $72.0_{+5.0}$ & $83.0_{+5.0}$ & $90.0_{+4.0}$ \\
% \bottomrule
% \end{tabular}
% \caption{\GG{These is a template with dummy values for expected trends. Ignore absolute values. Base and +SFT represents temperature 0 rollouts on the instruct and SFT trained model for 1 turn (t=1). SiM is the Single-Turn in Multi-Turn baseline where we rollout single-turn for 3 turns (t=3) until passing public tests. Multi rollouts for 3 steps and conditions on execution feedback and history (unlike SiM). DAgger aggregates data by filtering out problems with all negatives and uses the majority vote positive completion as y* to set as the last step of negatives in problems with at least 1 positive. Ours uses our best single-step reward model to choose y* for labeling negatives. We expect Base $<$ SiM $<$ Multi $<$ DAgger $<$ Ours and expect bigger gains in iterative training with Ours.}}
% \label{tab:comparison}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|c|cc|cc|cc|cc}
% \toprule
% \multirow{3}{*}{\textbf{Approach}} 
% & & \multicolumn{4}{c}{\textbf{Llama-3.2-1B-Instruct}} & \multicolumn{4}{c}{\textbf{Llama-3.1-8B-Instruct}} \\
% & & \multicolumn{2}{c|}{\itshape MBPP} 
% & \multicolumn{2}{c|}{\itshape HumanEval} 
% & \multicolumn{2}{c|}{\itshape MBPP} 
% & \multicolumn{2}{c}{\itshape HumanEval} \\
% & Iter & N=1 & N=5 & N=1 & N=5 & N=1 & N=5 & N=1 & N=5 \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{6}{c}{\textit{Single-Turn}} \\
% \midrule
% Base  & - & 33.0 & 42.2 & 27.8 & 33.2 & 55.4 & 64.2 & 63.9 & 65.2 \\
% \STaR & - & 34.4 & 41.8 & 32.9 & 37.8 & 58.0 & 62.8 & 66.2 & 70.2 \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{6}{c}{\textit{Multiple-Turn}} \\
% \midrule
% Base   & -    & 37.3 &  42.7 & 31.5 & 32.9  & 65.8 & 68.7 & 72.4 & 73.2 \\
% \MSTaR & -    & 37.4 &  42.3 & 34.2 & 38.2  & 66.1 & \textbf{69.6} & 75.4 & \textbf{77.8}  \\
% \midrule
% \MSTaRP  & 1               & 38.1 & 42.9 & 33.1 & 37.2  & 66.5 & 69.5 & 75.6 & 76.2 \\
% \hspace{6em} & 2           & 32.9 & 37.6 & 33.3 & 35.2  & 65.5 & 68.5 & \textbf{75.7} & 74.4 \\
% \midrule
% \method (Ours) & 1         & \textbf{44.0} & \textbf{49.6} & 36.3 & 41.3  & 67.9 & 69.1 & 75.1 & 77.0  \\
% \hspace{6em}   & 2         & 42.2 & 47.2 & \textbf{40.0} & \textbf{45.5}  & \textbf{67.7} & 69.3 & 75.3 & 77.2  \\
% % \midrule
% % \method++  & 1             & 41.3 & 46.8 & 33.2 & 38.8  & 68.4 &      & 75.3 &      \\
% % \hspace{1em} & 2           & 38.6 & 45.1 & 37.4 & 45.5  & 67.6 &      & 75.5 &      \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of our method \method with baselines across MBPP and HumanEval datasets on P@1 and BoN (with N=5) accuracy.  We initialize all methods with Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct models. We observe that the learned verifier improves BoN performance across methods. \method outperforms competing methods with 1B model and performs comparably on 8B model.}
% \label{tab:main_results}
% \end{table*}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|c|cc|cc|cc|cc}
% \toprule
% \multirow{3}{*}{\textbf{Approach}} 
% & & \multicolumn{4}{c}{\textbf{P@1}} & \multicolumn{4}{c}{\textbf{BoN}} \\
% & & \multicolumn{2}{c|}{\textbf{Llama-3.2-1B-Ins}} & \multicolumn{2}{c|}{\textbf{Llama-3.1-8B}} & \multicolumn{2}{c|}{\textbf{Llama-3.2-1B}} & \multicolumn{2}{c}{\textbf{Llama-3.1-8B}} \\
% & Iter & \itshape MBPP & \itshape HumanEval & \itshape MBPP & \itshape HumanEval & \itshape MBPP & \itshape HumanEval & \itshape MBPP & \itshape HumanEval \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{6}{c}{\textit{Single-Turn}} \\
% \midrule
% Base  & - & 33.0 & 27.8 & 55.4 & 63.9 & 42.2 & 33.2 & 64.2 & 65.2 \\
% \STaR & - & 34.4 & 32.9 & 58.0 & 66.2 & 41.8 & 37.8 & 62.8 & 70.2 \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{6}{c}{\textit{Multiple-Turn}} \\
% \midrule
% Base   & -    & 37.3 & 31.5 & 65.8 & 72.4  & 42.7 & 32.9 & 68.7 & 73.2 \\
% \MSTaR & -    & 37.4 & 34.2 & 66.1 & 75.4  & 42.3 & 38.2 & \textbf{69.6} & \textbf{77.8}  \\
% \midrule
% \MSTaRP  & 1               & 38.1 & 33.1 & 66.5 & 75.6  & 42.9 & 37.2 & 69.5 & 76.2 \\
% \hspace{6em} & 2           & 32.9 & 33.3 & 65.5 & \textbf{75.7}  & 37.6 & 35.2 & 68.5 & 74.4 \\
% \midrule
% \method (Ours) & 1         & \textbf{44.0} & 36.3 & 67.9 & 75.1  & \textbf{49.6} & 41.3 & 69.1 & 77.0  \\
% \hspace{6em}   & 2         & 42.2 & \textbf{40.0} & \textbf{67.7} & 75.3  & 47.2 & \textbf{45.5} & 69.3 & 77.2  \\
% % \midrule
% % \method++  & 1             & 41.3 & 33.2 & 68.4 & 75.3  & 46.8 &      &      &      \\
% % \hspace{1em} & 2           & 38.6 & 37.4 & 67.6 & 75.5  & 45.1 &      & 45.5 &      \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of our method \method with baselines across MBPP and HumanEval datasets on P@1 and BoN (with N=5) accuracy.  We initialize all methods with Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct models. We observe that the learned verifier improves BoN performance across methods. \method outperforms competing methods with 1B model and performs comparably on 8B model.}
% \label{tab:main_results}
% \end{table*}

% \begin{table}[t]
% \centering
% \begin{tabular}{l|c|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & & \multicolumn{2}{c|}{\textbf{1B-Instruct}} & \multicolumn{2}{c}{\textbf{8B-Instruct}} \\
% & Iter & \itshape MBPP & \itshape HE & \itshape MBPP & \itshape HE \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{Single-Turn}} \\
% \midrule
% Base  & - & 33.0 & 27.8 & 55.4 & 63.9 \\
% \STaR & - & 34.4 & 32.9 & 58.0 & 66.2 \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{Multiple-Turn}} \\
% \midrule
% Base   & -    & 37.3 & 31.5 & 65.8 & 72.4  \\
% \MSTaR & -    & 37.4 & 34.2 & 66.1 & 75.4  \\
% \midrule
% \MSTaRP  & 1               & 38.1 & 33.1 & 66.5 & 75.6  \\
% \hspace{6em} & 2           & 32.9 & 33.3 & 65.5 & \textbf{75.7}  \\
% \midrule
% \method (Ours) & 1         & \textbf{44.0} & 36.3 & 67.9 & 75.1  \\
% \hspace{6em}   & 2         & 42.2 & \textbf{40.0} & \textbf{67.7} & 75.3  \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of our method \method with baselines across MBPP and HumanEval datasets on P@1 accuracy. %We initialize all methods with Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct models. 
% \method outperforms competing methods with the 1B model and performs comparably on the 8B model.
% }
% \label{tab:p1_results}
% \end{table}

% \begin{table}[t]
% \centering
% \begin{tabular}{l|c|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & & \multicolumn{2}{c|}{\textbf{Llama-3.2-1B}} & \multicolumn{2}{c}{\textbf{Llama-3.1-8B}} \\
% & Iter & \itshape MBPP & \itshape HE & \itshape MBPP & \itshape HE \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{Single-Turn}} \\
% \midrule
% Base  & - & 42.2 & 33.2 & 64.2 & 65.2 \\
% \STaR & - & 41.8 & 37.8 & 62.8 & 70.2 \\
% \midrule
% \multicolumn{3}{c}{} & \multicolumn{2}{c}{\textit{Multiple-Turn}} \\
% \midrule
% Base   & -    & 42.7 & 32.9 & 68.7 & 73.2  \\
% \MSTaR & -    & 42.3 & 38.2 & \textbf{69.6} & \textbf{77.8}  \\
% \midrule
% \MSTaRP  & 1               & 42.9 & 37.2 & 69.5 & 76.2  \\
% \hspace{6em} & 2           & 37.6 & 35.2 & 68.5 & 74.4  \\
% \midrule
% \method (Ours) & 1         & \textbf{49.6} & 41.3 & 69.1 & 77.0  \\
% \hspace{6em}   & 2         & 47.2 & \textbf{45.5} & 69.3 & 77.2  \\
% \bottomrule
% \end{tabular}
% \caption{
% Comparison of our method \method with baselines across MBPP and HumanEval datasets on BoN (with N=5) accuracy. The verifier is learned using on-policy samples from the generator. \method outperforms competing methods with the 1B model and performs comparably on the 8B model.
% }
% \label{tab:bon_results}
% \end{table}

% Remove N as column as N is fixed.
% Check with temperature=0.0
\begin{table}[h]
\centering
\begin{tabular}{lcccccc}
\toprule
\multirow{1}{*}{\textbf{Approach}} 
& & \multicolumn{2}{c}{\textbf{Llama-3.2-1B}} & \multicolumn{2}{c}{\textbf{Llama-3.1-8B}} \\
& N & \itshape MBPP & \itshape HE & \itshape MBPP & \itshape HE \\
\midrule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{\textit{Single-Turn}} \\
\midrule
Instruct & 1 & 36.5 & 28.0 & 52.1 & 59.8 \\ % 39.0 & 32.9 & 59.0 & 70.1  \\
% Random   & 15 & 30.7 \\ % 33.0 & 27.8 & 55.4 & 63.9 \\
% LV  & 15 &  \\ % 42.2 & 33.2 & 64.2 & 65.2 \\
% PT  & 15 \\ % & 42.2 & 33.2 & 64.2 & 65.2 \\
% PT+LV  & 15 \\ % & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
STaR & 1 & 35.7 & 34.1 & 53.7 & 54.9 \\ % 37.2 & 39.0 & 58.6 & 67.1 \\
% Random   & 15 & \\ % 34.4 & 32.9 & 58.0 & 66.2 \\
% LV  & 15 & \\ % 41.8 & 37.8 & 62.8 & 70.2 \\
% PT  & 15 \\% & %42.2 & 33.2 & 64.2 & 65.2 \\
% PT+LV  & 15 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
\midrule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{\textit{Multi-Turn}} \\
\midrule
Instruct & 1 & 38.9 & 29.3 & 58.9 & 60.4 \\%& 40.6 & 36.0 & 66.4 & \textbf{78.0} \\
% Random   & 5 & 34.4 & 23.0 & 59.3 & 57.9 \\% & 37.3 & 31.5 & 65.8 & 72.4 \\
% LV  & 5 & 40.3 & 27.0 & 61.1 & 61.2 \\% & 42.7 & 32.9 & \underline{68.7} & 73.2 \\
% PT  & 5 & 48.6 & 31.9 & 67.2 & 60.4 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
\ \ \emph{+BoN}  & 5 & 48.5 & 34.3 & \underline{68.1} & 61.2 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
Multi-STaR & 1 & 36.7 & 33.5 & 57.7 & 59.8 \\%& 40.6 & 40.2 & 66.6 & 76.2 \\
% Random   & 5 & 35.6 & 30.5 & 59.2 & 57.7\\%& 37.4 & 34.2 & 66.1 & 75.4 \\
% LV  & 5 & 39.8 & 31.9 & 61.2 & 62.8 \\%& 42.3 & 38.2 & \textbf{69.6} & \underline{77.8} \\
% PT  & 5 & 46.7 & 37.6 & 67.6 & 60.0\\% & 42.2 & 33.2 & 64.2 & 65.2 \\
\ \ \emph{+BoN}  & 5 & 47.9 & 39.6 & \underline{68.6} & \textbf{63.2} \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
% \midrule
% \textbf{\method~(OV)} & \multicolumn{4}{c}{} \\ % Make this large
% Greedy & 1 \\ %& 34.8 & 34.1 & 66.0 & 75.6 \\
% Random   & 5 & 31.6 & 29.7 & 59.4 & 58.3 \\ %& 32.9 & 33.3 & 65.5 & 75.7 \\
% Verifier  & 5 & 35.2 & 31.3 & 62.7 & 60.6 \\ % & 37.6 & 35.2 & 68.5 & 74.4 \\
% PT  & 5 & 43.1 & 37.8 & 67.4 & 61.8 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% PT+Verifier & 5 & 43.0 & 38.8 & 67.5 & 61.2 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
\method & 1 & 37.9 & 35.4 & 62.3 & 57.9 \\%42.0 & \underline{44.5} & \underline{69.4} & 73.2 \\
% Random & 5 & 37.9 & 31.5 & 60.5 & 59.1 \\%42.2 & 40.0 & 67.9 & 75.3 \\
% LV  & 5 & 45.1 & 35.4 & 64.3 & 60.4 \\%\textbf{47.2} & \textbf{45.5} & \underline{69.3} & \underline{77.2} \\
% PT  & 5 & \underline{49.8} & 39.0 & \underline{68.7} & 59.1 \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
\ \ \emph{+BoN} & 5 & \textbf{50.7} & \textbf{41.7} & \textbf{68.8} & \underline{62.2} \\% & 42.2 & 33.2 & 64.2 & 65.2 \\
% Public Tests & 5 \\
\bottomrule
\end{tabular}
\caption{
Comparison of our method \method with baselines across MBPP and HumanEval datasets. % on Best-of-N~(BoN) accuracy. 
% Note each method uses a learned verifier trained with on-policy samples from the generator at the end of training. 
$N=1$ denotes generating solutions with 0 temperature. 
The Best-of-N~(BoN) accuracy is computed with $N=5$ candidate solutions at each where the public tests and learned verifier is used for selection.
% Random represents selecting a random solution at each turn and Verifier represents using a learned verifier for ranking~(sampled with 0.7 temperature). 
We observe that \method outperforms competing methods based on Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct models. 
The best performance for each dataset and model-sized is highlighted in \textbf{bold} and similar performances (within 1\%) are \underline{underlined}.
}
\label{tab:bon_results}
\end{table}

% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|ccc|ccc|ccc|ccc}
% \toprule
% \multirow{2}{*}{\textbf{Approach}} 
% & \multicolumn{6}{c}{\textbf{Llama-3.2-1B-Instruct}} & \multicolumn{6}{c}{\textbf{Llama-3.1-8B-Instruct}} \\
% & \multicolumn{3}{c|}{\itshape MBPP} 
% & \multicolumn{3}{c|}{\itshape HumanEval} 
% & \multicolumn{3}{c|}{\itshape MBPP} 
% & \multicolumn{3}{c}{\itshape HumanEval} \\
% \midrule
% & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 \\
% \midrule
% \multicolumn{2}{c}{} & \multicolumn{10}{c}{\textit{Single-Turn}} \\
% \midrule
% Base & 33.01 & 51.79 & 60.0 & 27.8 & 48.76 & 61.2 & 55.4 & 71.5 & 78.7 &63.9 & 80.4 & 87.2 \\
% STaR & 34.4 & - & 58.9 & 32.9 & - & 66.8 & 58.0 & 72.0 & 78.1 & 66.2 & 81.7 & 87.5 \\
% \midrule
% \multicolumn{2}{c}{} & \multicolumn{10}{c}{\textit{Multi-Turn}} \\
% \midrule
% Base & 37.3 & 55.8 & 64.8 & 31.5 & 53.1 & 66.5 & 55.3 & 71.9 & 79.2 \\
% Multi-STaR & 37.4 & 54.5 & 65.4 & 34.2 & 57.8 & 70.7 & 58.2 & 71.4 & 77.4 \\
% \midrule
% Multi-STaR++   Iter 1 & 38.1 & 54.4 & 61.0 & 33.1 & 53.9 & 64.6 & & & \\
% \hspace{1em}   Iter 2 & 32.9 & 48.5 & 57.0 & 33.3 & 55.9 & 65.9 & & & \\
% % \hspace{1em} + Iter 3 & 28.8 & 43.5 & 53.2 & 27.6 & 48.7 & 59.1 & & & \\
% \midrule
% \method (Ours) Iter 1 & 44.0 & 59.1 & 67.4 & 36.3 & 56.2 & 65.8 & & & \\
% \hspace{1em}   Iter 2 & 42.2 & 57.5 & 65.0 & 40.0 & 61.5 & 73.7 & & & \\
% % \hspace{1em} + Iter 3 & 39.2 & 58.1 & 68.2 & 38.6 & 57.3 & 65.8 & & & \\
% % \midrule
% % Ours (RS+GT) &  &  &  & & & &  &  &  & & & \\
% % \hspace{1em} + Iter 1 & 41.3 & 57.4 & 65.4 & 33.2 & 54.9 & 68.3 & & & \\
% % \hspace{1em} + Iter 2 & 38.6 & 57.3 & 66.2 & 37.4 & 60.0 & 71.9 & & & \\
% % \hspace{1em} + Iter 3 & 33.1 & 51.9 & 62.2 & 33.9 & 54.5 & 64.0 & & & \\
% \bottomrule
% \end{tabular}
% \caption{Performance comparison of approaches across MBPP and HumanEval datasets with different models.}
% \label{tab:comparison}
% \end{table*}





% \begin{table*}[h!]
% \centering
% \begin{tabular}{l|cc|ccc|ccc|ccc|ccc}
% \toprule
% \multirow{3}{*}{\textbf{Approach}} 
% & \multicolumn{6}{c}{\textbf{MBPP}} & \multicolumn{6}{c}{\textbf{HumanEval}} \\
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.1-8B-Instruct} 
% & \multicolumn{3}{c|}{\itshape Llama-3.2-1B-Instruct} 
% & \multicolumn{3}{c}{\itshape Llama-3.1-8B-Instruct} \\
% \midrule
% & T & F & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 & p@1 & p@5 & p@15 \\
% \midrule
% Base & ST & N &33.01 & 51.79 & 60.0 &  & & & 27.8 & 48.76 & 61.2 & & & \\
% % \hspace{1em} + SFT & 34.4 & - & 58.9 &  & & & 32.85 & - & 66.76 & & & \\
% SFT & ST & Y & 34.4 & - & 58.9 &  & & & 32.85 & - & 66.76 & & & \\
% \midrule
% SiM ($t=3$) & MT & N & 21.7 & 46.2 & 55.8 & 58.2 & 71.8 & 78.0 & 34.1 & 35.4 & 50.9 & 67.1 & 80.8 & 86.2 \\
% \hspace{1em} + SFT & MT & N & & & & & & & & & & & & \\
% \midrule
% Base ($t=3$) & 37.3 & 55.8 & 64.8 & & & & 31.5 & 53.1 & 66.5 & & & \\
% SFT & 37.4 & 54.5 & 65.4 & & & & 34.2 & 57.8 & 70.7 & & & \\
% \midrule
% Ours (G) & 37.3 & 55.8 & 64.8 & & & & - & - & - & & & \\
% Ours (V) & 37.3 & 55.8 & 64.8 & & & & - & - & - & & & \\
% Ours (G+V) & 37.3 & 55.8 & 64.8 & & & & - & - & - & & & \\
% % \hspace{1em} + Iter 1 & 37.6 & 53.6 & 62.2 & & & & 35.3 & 66.0 & 70.1 & & & \\
% % \hspace{1em} + Iter 2 & 39.2 & 56.3 & 64.0 & & & & 36.6 & 60.9 & 71.3 & & & \\
% % \hspace{1em} + Iter 3 & 39.8 & 56.2 & 65.4 & & & & 38.4 & 61.5 & 72.6 & & & \\
% % \midrule
% % Ours ($t=3$) & & & & & & & & & & & & \\
% % \hspace{1em} + Iter 1 & & & & & & & & & & & & \\
% % \hspace{1em} + Iter 2 & & & & & & & & & & & & \\
% % \hspace{1em} + Iter 3 & & & & & & & & & & & & \\
% \bottomrule
% \end{tabular}
% \caption{Performance comparison of approaches across MBPP and HumanEval datasets with different models.}
% \label{tab:comparison}
% \end{table*}