% % \subsubsection{Training Hyperparameters}
% % All models (generators and verifiers) were fine-tuned with the same hyperparameters:
\vspace{-2em}
\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Generator} & \textbf{Verifier} \\
        \midrule
        Training Epochs & 2 & 2 \\
        Learning Rate & $5 \times 10^{-7}$ & $1 \times 10^{-6}$ \\
        Batch Size & 32 & 64 \\
        Max seq length & 8192 & 2048 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters for SFT and RM training.}
    \label{tab:training_hyperparams}
\end{table}
\vspace{-2em}

\subsubsection{Training Parameters}

Table~\ref{tab:training_hyperparams} contains hyperparameters for training the generator and reward model on both models (Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct) and datasets (MBPP and HumanEval). We perform 2 iterations of training with \method, starting from the base model each iteration. All training runs were on machines with either 4 RTX 6000 Ada Generation GPUs for 1B models with 48 GB of memory per GPU or 4 H100 GPUs for 8B models with 80 GB of memory per GPU.

\subsubsection{Inference Parameters}
We use SGLang~\cite{zheng2024sglangefficientexecutionstructured} to serve our models for inference. Greedy experiments use temperature 0 with flags \textit{--disable-radix-cache --max-running-request 1} to ensure deterministic results while BoN search experiments use a temperature of 0.7. All experiments are capped to 1000 tokens per completion per turn.