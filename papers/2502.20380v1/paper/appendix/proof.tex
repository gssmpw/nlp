% \newpage
\subsection{Proof of Theorem 3.2}
\label{appendix:proof}
The proof relies on two important results. 

The first is the Performance Difference Lemma (PDL)~\citep{kakade2002approximately} which states that the performance difference between any two policies can be expressed as the sum of advantages. 
\begin{equation}
    \label{eq:pdl}
    J(\pi) - J(\pi') = \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^{\pi}_t } \left[ \sum_{a_t} A^{\pi'}(s_t,a_t) \pi(a_t|s_t) \right] 
\end{equation}
where $s_t \sim d^{\pi}_t$ is the induced state distribution by $\pi$, and $A^{\pi'}(s_t,a_t) = Q^{\pi'}(s_t, a_t) - V^{\pi'} (s_t)$ is the advantage w.r.t. $\pi'$.

We apply the PDL between the expert $\pi^*$ and the learner $\pi$
\begin{equation}
  J(\pi^\star) - J(\pi) = \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^{\pi}_t } \left[ \sum_{a_t} A^{\star}(s_t,a_t) \left(  \pi^\star(a_t | s_t) - \pi(a_t| s_t)\right) \right] 
\end{equation}
where the result follows from $\left(\sum_{a_t} A^{\star}(s_t,a_t) \pi^\star(a_t|s_t) = 0\right)$

According to the one-step recoverable MDP definition, $A^{\star}(s,a) \leq 1$ for all $(s,a)$. Hence we can bound the performance difference as

\begin{equation*}
\begin{aligned}
    J(\pi^\star) - J(\pi) &= \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^\pi_t } \left[ \sum_{a_t} A^{\star}(s_t,a_t) \left( \pi^\star(a|s_t) - \pi(a|s_t) \right) \right] &  \\
     & \leq  ||A^{\star} (., .)||_\infty \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^\pi_t } ||\pi(.|h_t) - \pi^\star(.|s_t)||_1 & \text{(Holder's Inequality)} \\
     & \leq  \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^\pi_t } ||\pi(.|s_t) - \pi^\star(.|s_t)||_1 & \text{(One step recoverability)} \\
\end{aligned}
\end{equation*}

The second result we use us from interactive imitation learning \textsc{DAgger}~\cite{ross2011reduction} that reduces imitation learning to no-regret online learning. 
\textsc{DAgger} shows that with $\pi^\star$ as the expert teacher guarantees that after $N$ iterations, it will find at least one policy
\begin{equation}
\label{eq:dagger:priv}
\mathbb{E}_{s \sim d^\pi} ||\pi(.|s) - \pi^\star (.|s)||_1 \leq \mathbb{E}_{s \sim d^\pi} ||\pi_{\rm class}(.|s) - \pi^\star(.|s)||_1 + \gamma(N) 
\end{equation}
where $\gamma(N)$ is the average regret, and $d^\pi$ is the time average distribution of states induced by policy $\pi$, $\pi_{\rm class}$ is the best policy in policy class.

Plugging this in we have
\begin{equation*}
\begin{aligned}
    J(\pi^\star) - J(\pi) & \leq  \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^\pi_t } ||\pi(.|s_t) - \pi^\star(.|s_t)||_1 \\
     & \leq  \sum_{t=1}^T \mathbb{E}_{ s_t \sim d^\pi_t } ||\pi_{\rm class}(.|s_t) - \pi^\star(.|s_t)||_1 + \gamma(N) & \text{From (\ref{eq:dagger:priv}) }\\
     & \leq T( \epsilon + \gamma(N))\\
\end{aligned}
\end{equation*}