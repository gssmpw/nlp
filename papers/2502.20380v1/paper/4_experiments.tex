\section{Experiments}

Through our experiments, we aim to analyze (1) How does \method compare to other state-of-the-art methods? (2) Does the learned verifier help during training and inference-time? (3) Which loss function works better for learning a verifier?


\subsection{Setup}
\paragraph{Models.} The generator model in \method is initialized with Llama-3.2-1B-Instruct or Llama-3.1-8B-Instruct~\citep{dubey2024llama}.
The learned verifiers are initialized with the same models as generators and have a randomly initialized linear layer to predict a scalar score~\citep{stiennon2020learning}.

\paragraph{Datasets.} We conduct experiments on MBPP~\citep{austin2021program} and HumanEval~\citep{chen2021evaluating} where the agent needs to generate code solutions in Python given natural language descriptions.
We train the methods on the MBPP training set which comprises 374 problems and evaluate on the MBPP test set and HumanEval (HE) dataset which have 500 and 164 problems.
We further describe the prompts and the split of public and private tests in Appendix~\ref{app:prompts} and~\ref{app:tests}.

\paragraph{Baselines.} We compare \method with single and multi-turn baselines. 
For the single and multi-turn settings, we report metrics by generating solutions from Llama models which we denote as Instruct. 
We also compare with STaR~\citep{zelikman2022star} where the correct solutions of the Instruct model are used for fine-tuning~(FT). We also compare to a multi-turn version of STaR, called \MSTaR.
Here, we collect multi-turn rollouts using the Instruct model and use trajectories terminating in a correct code solution for FT.
For multi-turn BoN search, we collect the solutions that pass public tests, and then we select the best one judged by a learned verifier. Note that this verifier is specifically trained for each generator.% 

\paragraph{Metrics.}
We measure the performance with the \emph{BoN} accuracy, which quantifies the accuracy of the solution chosen by a verifier from $N$ candidate solutions.
The generator is allowed $T=3$ turns and the final turn is used for evaluation over private tests.
At each turn, the verifier ranks $N=5$ solutions~(unless stated otherwise) provided by the generator.
For the BoN performance, we sample with a temperature of $0.7$.
We also report the accuracy of generating correct solutions via greedy decoding.

\subsection{Results}
In Table~\ref{tab:bon_results}, we compare the proposed algorithm \method with the baselines.
Here, we first evaluate the generators using code generated via greedy sampling for each problem~($N=1$). 
This measures the accuracy of generating a correct solution with a turn limit of $T=3$.
We observe that multi-turn methods~(both Instruct and \MSTaR) perform better than their single-turn variants showing the importance of incorporating execution feedback.
Our approach \method outperforms \MSTaR across both benchmarks with 1B-sized model demonstrating the efficacy of training generators with data obtained with a learned verifier.
To highlight, our method \method with a 1B parameter model achieves \textbf{1.9\%} performance gains compared to \MSTaR on the HumanEval dataset.
With an 8B-sized model, \method outperforms baselines on MBPP whereas there is a drop when compared on HumanEval.
\input{tables/main_table_template}

We further evaluate the effect of having a verifier for BoN search during inference, where a learned verifier selects the most promising candidate solution at each turn.
In Table~\ref{tab:bon_results}, we observe that all algorithms can benefit with BoN search.
Remarkably, \method attains a performance gain of up to \textbf{12.8}\% with the multi-turn BoN approach compared to greedy.
Moreover, \method outperforms leading baselines with BoN search on MBPP and HumanEval datasets by \textbf{2.8}\% and \textbf{2.1}\% with 1B sized-model and performs comparably on 8B-sized model.

\input{figures/verifier_inference_analysis}
\subsection{Analysis}
% Our results in Table~\ref{tab:bon_results} presents the performance gains of \method compared to the leading multi-turn baselines.
To understand the improvements, we conduct a component-wise ablation study where we 1) check the effect of oracle and learned verifiers for relabeling to train the generator~(\ref{sec:verifier_relabeling}), 2) evaluate different generators trained with and without learned verifiers~(\ref{sec:results_varying_gen}), 3) check which verifier performs better multi-turn BoN search at test-time~(\ref{sec:verifier_test_time}), 4) assess scaling behaviors at inference time with number of candidate generations~($N$) at each turn~(\ref{sec:results_test_time_scaling}), and 5) study the benefits of learned verifiers during evaluation~(\ref{sec:verifier_loss_function}).

\subsubsection{Verifier for relabeling}
\label{sec:verifier_relabeling}
We compare different verifiers for relabeling data for training the generator.
In contrast to \method where the learned verifier is used to relabel~(LV), we compare with relabeling using a correct solution~(attains an oracle reward $R=1$) for the corresponding prompt~(OV).
We also compare with a variant where the generator is fine-tuned over combinations of data relabeled with both the oracle verifier and the learned verifier (OV+LV). 
Here, we concatenate the FT dataset obtained using both LV and OV.
In Figure~\ref{fig:verifier_for_expert}, we present results with the 1B-sized models across benchmarks and observe that having corrections with the oracle verifier outcome does not perform well.
However, relabeling with both verifiers OV+LV outperforms the OV variant, further demonstrating that gains in the generator learned by \method are coming from relabeling with a learned verifier.
Lastly, the LV variant performed best on MBPP and comparably on HumanEval dataset when compared with LV+OV.
\input{tables/results_mucoder-r}

\subsubsection{Varying the generator}
\label{sec:results_varying_gen}
In this section, we compare the multi-turn agents where the generator is trained with an oracle verifier~(\MSTaR) or a learned verifier~(\method).
We evaluate the ability of the trained generator to utilize execution feedback and improve the code response across turns. 
We report the BoN accuracy till a turn $t$, which denotes the BoN accuracy of obtaining a correct solution till turn~$t$. 
In Figure~\ref{fig:varying_generator}, we present the results with 1B-sized models.
We observe that BoN accuracy improves with turns for \method whereas the baseline agents show marginal improvements with successive turns.
We believe that using a learned verifier for relabeling improves the generator's ability to generate solutions with high reward values, and indeed recover better at every turn by utilizing the execution feedback. 

\subsubsection{Verifier at test-time}
\label{sec:verifier_test_time}

\input{tables/verifier_test}
In our experiments with multi-turn BoN~(Table~\ref{tab:verifier_test}), we pick the best solution based on the outcome of public tests and the scores of the learned verifier.
In this experiment, we study different verifiers for ranking the candidate solutions at each turn.
We test with \emph{Random} strategy where the policy randomly picks from the $N$ solutions at each step.
We compare to the public tests~(PT) outcome that picks any solution that passes the public test.
Note that this involves evaluating all generated solutions at every turn with public tests.
We also compare to selecting a solution based on scores obtained via the learned verifier only~(LV).
This is crucial as in certain applications such privileged information like public tests are not available and the agents can benefit from learned verifiers during inference.
Lastly, we compare with the combination of public tests and leveraging the scores of the learned verifier to break ties at each turn~(PT+LV).

In Table~\ref{tab:verifier_test}, we compare Base, \MSTaR and \method with different verifiers at test-time. 
We observe that LV outperforms Random strategy which shows that a learned verifier indeed aids in selecting better solutions among generations. 
Given the benefits of learned verifiers for multi-turn BoN search, they can be a good alternative when public tests are not available.
Furthermore, using the outcome of public tests~(PT) performed better than using learned verifiers~(LV) except on the HumanEval datset for 8B-sized model.
We believe that this gap can be further reduced by learning more powerful verifiers and leave it for future research.
Interestingly, the hierarchical approach (PT+LV) that uses the learned verifier to break ties on the outcomes of public tests performed best across methods and datasets. 
We hypothesize that using learned verifiers is beneficial in two scenarios.
Firstly, if multiple solutions pass the public tests, then the learned verifier can filter out incorrect solutions which may not pass private tests.
Secondly, if all candidate solutions are incorrect, then the learned verifier should choose the most promising solution at each turn.
This is crucial as picking a better solution with the learned verifier can lead to more relevant feedback for recovering the true solution.

\subsubsection{Test-time scaling}
\label{sec:results_test_time_scaling}
\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{assets/test_time_scaling_new.png}
\vspace{-15pt}
\caption{
Test-time scaling with different values of candidate solutions~$N$ at each turn and different ways of learning verifiers.
We compare with verifiers learned on samples from \method and base policy.
The candidate solutions are obtained from the 1B generator of \method at each turn.
We observe that the BoN performance improves with larger values of N on both datasets.
The verifier learned with on-policy samples perform better.
}
\label{fig:test_time_scaling}
\end{figure}
In the multi-turn setting, the number of candidate solutions can rise exponentially with the number of turns.
To avoid this, \method uses the learned verifier during inference to select the most promising candidate among $N$ candidates at each turn, leading to a linearly increasing number of calls to the generator.
We study the inference-time scaling behaviors of \method where we scale the number of candidate generations~$N$ at each turn. 
Figure~\ref{fig:test_time_scaling} plots the BoN with different values of $N$ ($1 \leq N \leq 11$).
The performance gains diminishes for larger $N$ on both datasets.
On the MBPP dataset, the performance gains diminish with $N\geq5$, whereas on HumanEval dataset a dip is observed for $N>7$.

In this section, we further study the importance of training the verifier with on-policy rollouts from the generator.
We present a comparison of a verifier trained with samples from the Llama-3.2-1B-Instruct model (Base Verifier) and a verifier learned with samples from the generator of \method. 
Note that we use the generator of \method to obtain candidate solutions at each turn during evaluation. 
In Figure~\ref{fig:test_time_scaling}, we also present the scaling behaviors of different learned verifiers.
We observe that using a verifier trained with on-policy samples obtained via the generator of \method performs better and showed significant improvements of up to \textbf{4.3}\% for different values of candidate solutions~$N$.

Figure~\ref{fig:spot_check_RM} presents a qualitative example of multi-turn Best-of-N search with \method. 
Through this example, we demonstrate the advantages of dense scores from the learned verifier at facilitating efficient search across turns. 
We generate $N=5$ code solutions at each turn and show the top 3 ranked solutions using the dense scores.
At the first turn, we observe that the last solution $y_1^3$ is less accurate than the other 2 solutions $y_1^1$ and $y_1^2$.
The top ranked solution is used to collect the environment feedback, upon which the generator comes up with $N$ new candidate solutions.
Upon the top 3 solutions, the last two snippets are similar to the candidates from the previous turn. 
However, the top ranked solution is a novel solution and is more accurate as the generated code learns to extract a single digit and multiply it. 
With the execution feedback, \method generates 2 correct responses-- $y_3^1$ and $y_3^2$ and learned verifier chooses one of them compared to the incorrect response $y^3_3$.

\input{tables/verifier_ablation}
\input{figures/loss_ablation}
\input{figures/qualitative_examples}

\subsubsection{Loss function for Verifier}
\label{sec:verifier_loss_function}
As described in \ref{sec:training_learned_verifiers}, we compare against different loss functions for training the verifier.
For this experiment, we first generate multiple single step rollouts and label them via oracle verifier.
Given oracle labels, we train verifiers with two loss functions -- BCE and BT.
During inference, the learned verifier picks the best ranked solution among the $N$ solutions provided by the generator.
Similar to \citep{cobbe2021training}, we report the BoN plot with different values of N obtained by first sampling $N$ candidate solutions, choosing the top-ranked solution using the learned verifier, and then evaluating the solution against public and private tests. 
We calculate this metric over multiple samples for each value of $N$.
In Figure~\ref{fig:verifier_loss_functions}, we observe that the verifier trained with BT loss consistently outperforms the verifier trained on BCE loss on both MBPP and HumanEval.