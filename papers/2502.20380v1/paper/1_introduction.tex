\section{Introduction}
Software engineers often iteratively refine their code based on execution errors. A common strategy for machine code generation is thus to repair code using execution feedback at test time \citep{chen2024teaching, wang2024executable, zhao2024commit0}. However, prompting alone is insufficient as it cannot teach how to recover from all possible errors within a limited context.

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figures/rl_llm_code_notation_fix.pdf}
\vspace{-10pt}
\caption{(a) We define the task of multi-turn code generation where for an initial problem~$x$, the generator $\pi_\theta$ provides a solution~$y_1$.
This solution is evaluated with the public test to get execution feedback~$o_1$.
At a turn $t$, the generator is conditioned on the history to generate solution $y_t\sim\pi_\theta(.|x,y_{<t},o_{<t})$.
The rollout ends when the turn limit is reached or the public tests pass upon which the solution is executed on private tests.
Since, the agents can generate the optimal solution at any turn, this is a 1-step recoverable process.
(b) Training loop of our method \method~-- which comprises of a generator and a learned verifier.
During each iteration, rollouts are collected using $\pi_{\theta}$ and we train a verifier $R_{\phi}$ to rank candidate solutions for a prompt.
The verifier $R_{\phi}$ is then used to construct a local expert and relabel the collected rollouts.
Lastly, the generator is fine-tuned with this expert dataset.
}
\label{fig:da_method}
\end{figure*}

We need to train models that can learn from execution feedback during training. Existing approaches fall into either single-turn or multi-turn settings. In the single-turn setting, methods either train without execution feedback~\citep{zelikman2022star} or perform one-step corrections~\citep{welleck2023generating, ni2024next}. However, these struggle to iteratively correct errors over multiple turns. Multi-turn approaches, on the other hand, rely on complex reinforcement learning (RL)~\citep{gehring2024rlef, kumar2024training, zhou2024archer} to optimize long-term rewards. While effective in principle, these methods suffer from sparse learning signals which makes learning inefficient. 

Our key insight is that code generation is \emph{a one-step recoverable Markov Decision Process (MDP), implying that the correct code can be recovered from any intermediate state in a single step}. This allows us to greedily maximize a one-step reward instead of relying on complex multi-step reward optimization. As a result, this reduces the problem from reinforcement learning, which requires exploration and credit assignment, to imitation learning, where the model simply learns to mimic correct code, leading to a more stable and efficient training process.

We propose \method, a simple and scalable approach for multi-turn code generation from execution feedback. During training, \method follows an \emph{expert iteration}~\cite{anthony2017thinkingfastslowdeep} framework with a \emph{local search expert}, enabling iterative improvement of both the generator and the expert. The process begins by rolling out the current code generator to collect interaction data with execution feedback. A single-step verifier is then trained on this data and utilized to guide a local search expert in refining the code and generating training labels. Finally, the generator is fine-tuned using these labels.
Given recent trends of test-time scaling in generating high quality solutions~\citep{brown2024large,snell2024scaling,wu2024inference}, \method also uses the learned verifier for inference-time scaling.
Here, \method samples $N$ trajectories; at each step, \method picks the best code solution ranked by the learned verifier. 

The key contributions of this work are as follows: 
\begin{enumerate}[nosep, leftmargin=0.2in]
    \item A novel framework, \emph{\method}, for training code generators and verifiers through multi-turn execution feedback. We add theoretical analysis of performance bounds using the property of one-step recoverability for this task.
    \item We propose a \emph{multi-turn Best-of-N~(BoN) approach} for inference-time scaling and present benefits of a learned verifier to select the code solution at each turn. 
    \item Our approach \method outperforms leading multi-turn approaches on MBPP~\cite{austin2021program} and HumanEval~\cite{chen2021evaluating} benchmarks. Our ablations demonstrate that learned verifiers aid in learning better generators and show promising scaling law trends with higher inference budgets.
\end{enumerate}
