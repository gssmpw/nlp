\section{Related Work}

\paragraph{Prompting To Solve Multi Step Tasks} A common framework for tackling multi-step tasks with LLMs is prompting-based agentic systems. 
Self-Debugging \cite{chen2023teachinglargelanguagemodels} asks the LLM to iteratively improve code by providing execution feedback while CodeT \cite{chen2022codetcodegenerationgenerated} asks the LLM to generate test cases. 
AlphaCodium \cite{ridnik2024codegenerationalphacodiumprompt} first reflects on input instructions, generates and filters from multiple code generations, and finally iterates on public and self-generated test cases. MapCoder \cite{islam2024mapcodermultiagentcodegeneration} incorporates four agents to generate example problems, plans and code, and then perform debugging. However, prompting-based agents yield limited improvements.

\paragraph{Training LLMs for Multi Step Tasks} Some work has explored explicitly training critics or reward models for multi-step reasoning tasks. In the coding domain, CodeRL \cite{le2022coderlmasteringcodegeneration} trains a token-level critic to aid in code generation and to perform inference-time search. CodeRL's mechanics are similar to our method, but their generator is not trained for multi-step: CodeRL trains a ``code repairer" which conditions on one erroneous code completion while our generator incorporates multiple. ARCHER \cite{zhou2024archer}, which frames multi-step tasks via a two-level hierarchical MDP, where the higher level MDP considers completions as actions and the lower level MDP considers tokens as actions. Another line of work utilizes Monte Carlo Tree Search (MCTS) methods for training: rStar-Math \cite{guan2025rstarmathsmallllmsmaster} trains a policy preference model to boost small LMs' math abilities to match or exceed large reasoning-based LMs and ReST-MCTS \cite{zhang2024restmctsllmselftrainingprocess} trains a process reward model (PRM) similarly to Math-Shepherd \cite{wang2024mathshepherdverifyreinforcellms}. Although \method's BoN search resembles a tree search, our key insight that multi-step code generation resembles a one-step recoverable MDP allows us to collect training trajectories much more efficiently. Finally, some work has explored using verifiers only during inference time. In ``Let's Verify Step by Step" \cite{lightman2023letsverifystepstep}, the authors demonstrate that PRMs trained on erroneous math solutions annotated by humans outperform outcome reward models for filtering multiple inference time generations. Meanwhile, AlphaCode \cite{Li_2022} trains a test generator to evaluate multiple code solutions.

Other works omit learning a critic or reward model altogether. In the coding domain, RLEF \cite{gehring2024rlefgroundingcodellms} derives rewards only on the executor's result on test cases and syntax checkers, and PPOCoder \cite{shojaee2023executionbasedcodegenerationusing} additionally considers semantic and syntactic alignment, generated via data flow graphs and abstract syntax trees respectively, with a reference solution. The ``oracle" rewards in these methods may not be informative for training, and in the case of PPOCoder, require complex constructs. We empirically show that having a reward model is beneficial by comparing \method against the \MSTaR baseline. Meanwhile, SCoRe \cite{kumar2024traininglanguagemodelsselfcorrect} splits training into a ``generator" and ``correction" phase, thus restricting the total number of turns to 2. RISE \cite{qu2024recursiveintrospectionteachinglanguage} generates recovery steps via a more powerful LLM or by selecting a sampled completion via the oracle rewards. Both methods are less efficient than \method, which doesn't require generating corrections beyond generating training trajectories. Finally, FireAct \cite{chen2023fireactlanguageagentfinetuning} and LEAP \cite{choudhury2024betterteacherllmagents} FT ReAct style agents while RL4VLM \cite{zhai2024finetuninglargevisionlanguagemodels} and GLAM \cite{carta2024groundinglargelanguagemodels} studies training LLMs with interactive environment feedback.