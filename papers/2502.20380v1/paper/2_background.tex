\section{Background}

\paragraph{Multi-turn Code Generation as a MDP.}
In multi-turn code generation, an agent iteratively refines a program to maximize its correctness on private test cases. Given an initial problem prompt $x$, at each turn $t$, the agent generates a complete code snippet $y_t$ and executes it on a set of public tests. The outcomes $o_t$ from these tests serve as observations that guide subsequent refinements. This process continues until the agent generates a code snippet $y_t$ that passes all public tests, at which point the episode terminates, or until the maximum number of turns $T$ is reached without success. The first successful code, $y_t$, is then evaluated on private tests to compute the correctness score $C(x, y_t) \in \{0, 1\}$. 

We model this as a Markov Decision Process (MDP), where the state is the interaction history $s_t = \{x, y_1, o_1, \dots, y_{t-1}, o_{t-1}\}$ where $s_1 = \{x\}$, and the action is the code $a_t = y_t$. The oracle reward is defined as $R(s_t, a_t) = R(x, a_t) = C(x, y_t)$ if $y_t$ passes all public and private tests (terminating the episode), or $0$ otherwise.


During training, given a dataset of problem prompts $\mathcal{D}$, the goal is to find a generator $\pi_\theta(y_t | x, y_1, o_1, \dots, y_{t-1}, o_{t-1})$, that maximizes the cumulative discounted reward $R(x,y_t)$:
\begin{equation} 
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y_t \sim \pi_\theta( \cdot | s_t )} \left[ \sum_{t=1}^{T} \gamma^t R(x, y_t) \right].
\end{equation}
