% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

%
% File nodalida2025.tex
%
% Contact:  Sara Stymne
% Email:    sara.stymne@lingfil.uu.se
%
% Based on the instruction file for NoDaLiDa 2023 by Mark Fishel which in turn were
% Based on the instruction file for NoDaLiDa 2021 by Lilja Øvrelid which in turn were
% Based on the instruction file for NoDaLiDa 2019 by Barbara Plank and Mareike Hartmann which in turn were based on the instruction files from NoDaLiDa 2017 and 2015 by
% Beata Megyesi (beata.megyesi@lingfil.uu.se) and EACL 2014
% which in turn was based on the instruction files for previous 
% ACL and EACL conferences. The BibTeX file is based on NAACL 2019
% style files, which in turn are based on style files for ACL 2018 and NAACL 2018, which were
% Based on the style files for ACL-2015, with some improvements
%  taken from the NAACL-2016 style
% Based on the style files for ACL-2014, which were, in turn,
% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
% EACL-2009, IJCNLP-2008...
% Based on the style files for EACL 2006 by 
% e.agirre@ehu.es or Sergi.Balari@uab.es
% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{nodalida2025}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

% packages
\usepackage{amsmath} % For better equation formatting (cass)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{arydshln}
\usepackage{wasysym}
\usepackage{verbatimbox} %(cass)
\usepackage{tablefootnote} % for table footnotes
\usepackage{hyperref}

\aclfinalcopy % Uncomment this line for the final submission

\newcommand{\todo}[1]{{\color{red} #1}}

\title{OCR Error Post-Correction with LLMs in Historical Documents: No Free Lunches}


\author{Jenna Kanerva, Cassandra Ledins, Siiri K{\"a}pyaho, and Filip Ginter \\
TurkuNLP, Department of Computing \\
University of Turku, Finland \\
\{jmnybl, caledi, siakap, figint\}@utu.fi\\
} 


%\author{First Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\\And
%  Second Author \\
%  Affiliation / Address line 1 \\
%  Affiliation / Address line 2 \\
%  Affiliation / Address line 3 \\
%  \texttt{email@domain} \\}


\date{}

\begin{document}
\maketitle
\begin{abstract}
Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora.
\end{abstract}
%of historical language, one in English and the other in Finnish.

\section{Introduction}

Digitizing and transcribing historical documents and literature is vital for preserving our cultural heritage and making it accessible for modern digital research methods. The transcription process relies on OCR, which naturally introduces noise into the output. The noise varies in severity depending on the quality of the source material and the OCR technology used, impacting the research usage of the data \citep{chiron2017impact}. The OCR output at two noise levels is illustrated in Figure~\ref{fig:noise-example}. Although modern OCR systems are becoming increasingly accurate \citep{li2022trocr}, reprocessing large collections of historical literature remains a significant challenge, as the resources available to the institutions maintaining these collections are often insufficient for such an undertaking. Consequently, OCR error post-correction has been suggested as means of improving the historical collections without the need to repeat the whole transcription process \citep{nguyen2021survey}.

Recent studies \cite{boros2024postcorrection,bourne2024clocrc} have proposed the application of LLMs to the task, with varying degrees of success. Currently, there is no clear consensus as to how LLMs can be applied to the task and how to deal with the various methodological challenges it poses. Our objective is to address some of these challenges as well as to assess several open LLMs to correct OCR-generated text when prompted to. We study hyperparameter optimization, quantization levels, input lengths, output post-processing and several novel correction methods so as to benchmark and improve the LLM performance on this task.

As our long-term goal is to post-correct two large historical datasets, one in English and the other in Finnish, we focus on these two languages as well as open-weight LLMs, since post-correction of large datasets with commercial models is infeasible cost-wise.

\begin{figure}
\small
    \begin{flushleft}\textbf{Mild noise (0.04 CER):}\end{flushleft}
    \setbox0=\vbox{
    \begin{verbatim}
 A work of art, (be it what it may, house,  
 pi&ure, book, or  garden,) however 
 beautiful in it's underparts, loses half 
 it's value, if the gneralfcope 
 of it be not obvio',s to conception.\end{verbatim} %\nEven the wild scene of nature, however pleas-\ning in itsef, is still more pleaing, if the eye\nis able to combine it into a whole.
    }
    \fbox{\box0}
    \begin{flushleft}\textbf{Severe noise (0.19 CER):}\end{flushleft}
    \setbox1=\vbox{
    \begin{verbatim}
 bke up at Sx in the Mo.r aig.  ll the 
 eauing Withr he went from Cbaud to Cbhh 
 every Suday, «d from Play. bote~PIOoaB 
 cu evi Niuht m the Week, but  vd \end{verbatim} % eryirndtheOrinae l of the Pi*ur which\n\ndwelt in his-Bom. In * Word, his Attention\n: r wny Ting but his Pafn .was utterly gone.
    }
    \fbox{\box1}
    \caption{Example extracts of texts at two different OCR noise levels from the ECCO dataset of 18th century literature.}
    \label{fig:noise-example}
\end{figure}

%text in comment is the continuation of the examples in case we need more

\section{Related work}

Despite decades of active research, post-correction of historical documents remains a challenge. The ICDAR 2017 and 2019 shared tasks \citep{icdar2017,icdar2019} addressed the lack of adequate benchmarks for evaluating OCR performance across several languages, introducing two tracks: detecting OCR errors, and correcting previously detected errors. This setting has naturally guided the development towards two-stage systems, and the best performing models in the ICDAR 2019 edition were based on the BERT model fine-tuned separately for each task \citep{icdar2019}. Such two-step approaches are still actively pursued, with e.g.\ \citet{beshirov2024postocrtextcorrectionbulgarian} applying a BERT classifier for error detection, and an LSTM-based seq2seq model for error correction in Bulgarian.

%As noted by \citet{icdar2019}, an overall improvement in error detection performance was observed in the 2019 edition of the shared task, with . 

% here 1-2 sentence description of these shared task
% was this two-step approach, first identify erroneus words, then suggest how to fix a word?
% cite some paper(s) from ICDAR who do this two-step thing


% cite this paper:
%Post-OCR Text Correction for Bulgarian Historical Documents (2024)
% https://arxiv.org/pdf/2409.00527
% two-steps approach: bert base binary classifier (is word correct or not), lstm based seq2seq error fixing

Recently, LLMs have been effectively applied to text correction problems, for example, \citet{penteado2023evaluating} and \citet{ostling-etal-2024-evaluation} demonstrated that LLMs perform well in grammatical error correction. Naturally, LLMs have been proposed also to the OCR post-correction task, in line with the two broad paradigms of LLM use: fine-tuning for the post-correction task and purely prompt-based zero-shot generation. Fine-tuning was applied e.g.\ by \citet{soper-etal-2021-bart} who fine-tune the BART model on the English subset of the ICDAR 2017 data and apply it to English Newspaper text. \citet{veninga2024msc} fine-tunes the character-based ByT5 model on the ICDAR 2019 data, with a prompt-based Llama model as a baseline. Similarly, \citet{madarasz2024ocrcleaning} apply the mT5 model to historical Hungarian scientific literature, and \citet{dereza-etal-2024-million} applies the BART model to historical Irish--English bilingual data. 


In the zero-shot, prompt-based line of work, \citet{boros2024postcorrection} evaluated a variety of models and prompts on several multilingual historical datasets. Interestingly, the results of the study were mostly negative, concluding that LLMs (including the commercial GPT-4 model) are not effective at correcting transcriptions of historical documents, in many cases the LLM actually decreasing the quality instead of improving it. \citet{bourne2024clocrc} conducted a similar study on three historical English datasets, arriving at the opposite conclusion. They achieved over 60\% reduction of character error rate at best, with most of the evaluated models improving the data quality.

Finally, several studies also pursue approaches that include the original image as an input, together with the OCR output to be post-corrected. Here, e.g.\ \citet{chen2024trocrmeets} combine a state-of-the-art transformer-based OCR system with the character-based CharBERT model for handwritten text recognition, and \citet{fahandari2024farsi} propose a model iterating between OCR and post-correction steps for Farsi. Such image-text approaches are, nevertheless, beyond the scope of the present study.



%--- Data ---

%Historical Ink: 19th Century Latin American Spanish Newspaper Corpus with LLM OCR Correction (2024) % Data paper, methods not very well described and no evaluation results
%https://arxiv.org/pdf/2407.12838

%Slovenian: https://www.clarin.si/repository/xmlui/handle/11356/1907

%French, English, German, Italian: https://huggingface.co/datasets/PleIAs/Post-OCR-Correction

\section{Data}


In our study, we utilize manually corrected samples of two large historical datasets, one for English and the other for Finnish.

\begin{table*}[] % TABLE UPDATED 6.12. -J
    \centering
    \begin{tabular}{l|l|rrrrrr}
       Dataset  & Language & Pages & OCR words & GT words & OCR w./pg. & CER & WER  \\\hline
        ECCO-TCP & English & 301,937 & 67,549,822  & 64,519,266 & 223.72 & 0.07 & 0.22 \\
        NLF GT   & Finnish & 449     & 449,088     & 461,305    & 1000.20 & 0.09 & 0.28 \\
    \end{tabular}
    \caption{Dataset statistics after preprocessing in terms of whitespace delimited words. \emph{OCR w./pg.} denotes for mean OCR words per page, and CER and WER are average page-level character and word error rates in the data, weighted by the page length. For details about the metrics, see Section~\ref{sec:metrics}.}
    \label{tab:data-statistics}
\end{table*}

% ECCO/TCP OCR characters: 306,074,679
% ECCO/TCP GT characters: 301,863,835
% NLF GT OCR characters: 3,112,933
% NLF GT GT characters: 3,192,621

\subsection{English ECCO-TCP}
\label{lab:subsection-ecco}

Eighteenth Century Collections Online (ECCO) \citep{gale_ecco} is a dataset of over 180,000 digitized publications (books and pamphlets) originally printed in the 18th century Britain and its overseas colonies, Ireland, as well as the United States. While mainly in English, some texts appear in other languages. The collection was created by the software and education company Gale by scanning and OCRing the publications. ECCO has significantly impacted 18th-century historical research despite its known limitations \citep{gregg_2021,tolonen2021ecco}.

%Although the ECCO dataset contains some known limitations \citep{gregg_2021,tolonen2021ecco}, its impact on history research of the 18th century has been remarkable \citep{gregg_2021}.
%- the impact on the study of the 18th century remarkable, however limitations and the biased nature of the collection are discussed for example in:
% https://www.cambridge.org/core/elements/old-books-and-digital-publishing-eighteenthcentury-collections-online/058DB12DE06A4C00770B46DCFAE1D25E
% https://www.researchgate.net/publication/351136155_Corpus_Linguistics_and_Eighteenth_Century_Collections_Online_ECCO

While ECCO contains only OCR engine output, the ECCO-TCP initiative\footnote{\texttt{https://textcreationpartnership.org/tcp-texts/ ecco-tcp-eighteenth-century-collections-online/}} provides highly accurate, human-made text versions for 2,473 publications from the original collection \citep{gregg2022nature}. In this study, we use a dataset from the Helsinki Computational History Group\footnote{\url{https://github.com/COMHIS}}, where clean ECCO-TCP texts are paired with their corresponding ECCO OCR publications, creating an OCR ground truth dataset \citep{hill2019quantifying}. The data is paired on page level, resulting in a dataset of 338K pages.
%for which we have both OCR and ground truth (GT) versions available.

%- "The TCP is a library-based initiative to produce SGML/XML-encoded editions of early print books"
% https://deepblue.lib.umich.edu/bitstream/handle/2027.42/87997/Welzenbach-DHCS-2011.pdf
%- ECCO-TCP dataset (Text Creation Partnership) contains 2,473 (+ 628 unfinished) manually keyed texts from ECCO (only ECCO 1?), done between 2005-2009
%- original data produced by ECCO-TCP in SGML format, now mostly as XML
%- files available for download at
% https://www.dropbox.com/scl/fo/odtdrh2uzc9arlqsx4fn3/AC8NHey70dE8YK6npd3hrQ8?dl=0&e=1&rlkey=pcqpcue5ntdyofkufjeluhhnc
%- also, not sure if most of the data can be found here:
% https://github.com/Text-Creation-Partnership/ECCO-TCP/tree/master
%or if this is unrelated
%- Discussion on how the texts to include in the ECCO-TCP dataset have been selected:
% https://researchspace.bathspa.ac.uk/15161/1/15161.pdf


% blank: (1411 times in ECCO-TCP, 65 in ECCO-OCR, of which 40 on the same page in both datasets)
To prepare data for post-correction evaluation, we applied several filtering steps. First, we excluded 1,436 pages (0.4\%) marked as blank in ECCO-TCP, ECCO OCR, or both. We also removed 5,782 pages (1.7\%) containing fewer than 150 non-whitespace characters in either collection. Further filtering was necessary in cases of substantial mismatch between OCR and GT pages, typically with large chunks of text missing in either OCR or GT, or otherwise an obvious lack of correspondence. A brief manual analysis identified as typical causes (1) very noisy OCR output with a large amount of non-alphanumerical characters, likely from OCR engine transcribing an image; (2) OCR and GT containing approximately the same text, but in different order due to misidentified reading order or column layout; and (3) significant length differences between pages, possibly from errors in automated page alignments, unrecognized regions left out in the OCR process, or omissions in the ECCO-TCP data. 

To identify such instances, we align each OCR and GT page pair on their non-whitespace characters\footnote{Using global string alignment as implemented in the PairwiseAligner module in biopython.} and slide a 100-character window across the alignment. If in any window less than 10\% of characters were aligned as a match, the page was discarded from the dataset. In total, 28,907 (8.6\%) pages were removed by this process. 
%from the initial 338,062 pages. 

Our filtering produced a dataset of 301,937 well-aligned pages (89.3\% of the initial ECCO-TCP pages). While we do not filter by language, nearly all the data is in English, with other languages appearing only very rarely. 
% 338,062 initial ECCO-TCP pages

% The data were initially randomly divided into development data (238 books / 32,959 pages), and test data (237 books / 29,322 pages) sets at book level. The remaining  (1900 books / 239,656 pages).

%Results of the filtering:
%blanks: 1436 / 338062 (0.42\%)
%too short: 5782 / 338062 (1.71\%)
%gappy: 28907 / 338062 (8.55\%)
%Pages remaining: 301937 (89.31\%)

%-Train/dev/test split
%- randomly allocated, no stratification
%- split was done before filtering out the unsatisfactory documents, on book level (number of pages in a book not taken into consideration)



\subsection{Finnish NLF Ground Truth Data} %name?

For Finnish, we use the National Library of Finland (NLF) OCR ground truth dataset\footnote{\url{http://digi.nationallibrary.fi}} of \citet{kettunen2018creating,kettunen2020ground}, specifically intended for OCR quality evaluation. The data draws from the National Library's collection of digitized newspapers, and consists of 479 pages randomly chosen from 188 different Finnish newspapers and journals published between 1836--1918, all printed in the Fraktur font. 

% from data readme: "Plase, cite http://digi.nationallibrary.fi if you use the materials" DONE!
%The publications are from 24 different years, published between 1836-1918.

% OCR software: ABBYY FineReader 11
The ground truth was created by manually correcting the OCR system output with reference to the original scans. The dataset contains texts produced by three different OCR software (ABBYY FineReader 7/8, ABBYY FineReader 11, and Tesseract) along with the ground truth. In this work, we use output produced by ABBYY FineReader 7/8, which is the OCR engine that has been used to digitize the majority of the NLF collection and therefore gives most useful information for a possible future post-correction effort targeting it.

We applied the same filtering procedure as for the ECCO-TCP data, resulting in the removal of 29 pages (6\%), and we further removed one page written in Swedish. The final dataset consists of 449 pages, with 449,088 OCR words, and 461,305 GT words of text. The key statistics for both datasets are shown in Table~\ref{tab:data-statistics}.

% The data was randomly split into three subsets on page level into a development set of 44 documents (9.8\%), and test set of 43 documents (9.6\%). The remaining 362 documents (80.6\%), are unused, set aside as training data for possible later fine-tuning study.

%In the preprocessing step, one document written mainly in Swedish was identified and removed\footnote{Note that some of the remaining pages may still have a small proportion of Swedish included (e.g.\ a Finnish newspaper page including one Swedish advertisement).}, and the data was further filtered with the same procedure than the ECCO data described above, resulting in 29 pages (6\% of the data) being removed due to the high misalignment between OCR and GT, hence the size of . 

%As the data was already aligned at word level, no further alignment was needed for reformatting the data as json objects.




\section{Experiments}
\label{sec:basic-experiments}

First, we set out to evaluate the basic performance of different LLMs on the OCR error correction task and establish how the generation and model hyperparameters (e.g.\ sampling parameters and quantization) affect the results. 

The page lengths in our data vary, with the ECCO-TCP pages on average at 200 words, and the Finnish newspaper pages at about 1,000 words. To improve comparability of the results, we split the pages to segments of 200 OCR words for English and 100 OCR words for Finnish, both corresponding to roughly 300 sub-words in OpenAI's GPT-4 tokenization for the language in question. The length of roughly 300 sub-words was established as suitable in our initial experiments, however, we will carry out a more detailed evaluation of segment lengths as a separate experiment in Section~\ref{sec:segment-length}.

Since the Finnish data is originally word-aligned, obtaining these shorter-than-page segments is trivial. For the English data, which is only page-aligned, we utilize the character-level OCR-GT alignments produced during data filtering (described in Section~\ref{lab:subsection-ecco}), allowing us to find corresponding points. In cases where the segment boundary falls within a region of poor alignment, we shift the boundary to the next reliable word (the word starting the next aligned region). Therefore, the exact segment length may vary depending on how well the OCR and GT strings could be aligned.

Given the substantial volume of our data, and the number of LLM runs necessary in our experiments, we randomly sample for each language a development and a test set, each containing 200 examples (i.e.\ segments of about 300 sub-words in length). These constitute 244K+243K GT characters for English, and 162K+165K GT characters for Finnish. The development set is used to set the generation parameters and the test set is used to report all results, unless otherwise stated.

% chars, including whitespace:
%fin_test_sample200.jsonl --- 165223
%fin_dev_sample200.jsonl --- 162209
%en_test_sample200.jsonl --- 242693
%en_dev_sample200.jsonl --- 244486


% details to condider:
% - word = whitespace delimited characters
% - Additionally, no further consideration was given for the initial length of the OCR text, which resulted in some of the texts being suboptimally split (e.g. an OCR text of 202 words being split into 200 and 2 words).
% - Biopython's PairwiseAligner was used to align the non-whitespace versions of the OCR and GT strings. .... The strings were then mapped back to their original formats, with the exception of leading and trailing whitespace being removed.
% - If this index was aligned with an end index of a GT word, the initial indices were used for the split. Alternatively, if the indices were aligned but not end indices of any words, the actual end indices were searched and used for the split.
% -In segmenting, it was verified that each character (both OCR or GT) can be part of exactly one segment, meaning overlaps or deletions are not allowed and concatenation of the segments would result in the original page.

% For the Finnish, the data was already aligned at word level, making it simple to split at every 100th OCR word, and getting the corresponding GT words independent of the length of the GT.


\subsection{Evaluation Metric}
\label{sec:metrics}

%\begin{equation}
%    \text{CER} = \frac{S + D + I}{N} = \frac{S + D + I}{S + D + C}
%    \label{eq:cer}
%\end{equation}
%where, S is the number of substitutions, D the number of deletions, I the number of insertions, C the number of correct characters and finally N is the number of characters in the ground truth, which also equals the sum of substitutions, deletions, and correct characters. 

As a primary evaluation metric, we use Character Error Rate (CER) defined as the sum of character substitutions, deletions and insertions, divided by the length of the ground truth string. In line with the common practice in OCR post-correction literature, we mainly report relative CER reduction defined for one examples as $\text{CER\%} = {(\text{CER}_{\text{orig}}-\text{CER}_{\text{post}})}/{\text{CER}_{\text{orig}}}\times 100$
where \emph{orig} and \emph{post} refer to before and after correction, respectively. The overall CER\% is calculated as an average of example-wise CER\% weighted by example lengths in terms of OCR character count. The example-wise CER\% values are further clipped not to go below -100\% to prevent extremely large negative scores in cases where most of the text is omitted. The CER\% therefore works on a range between -100\% and 100\%.

Many downstream applications utilizing historical corpora, such as various literature search interfaces, operate at the level of words rather than characters. Therefore, the main results are reported also in terms of Word Error Rate (WER) and its relative improvement (WER\%). This metric is much like CER, but on the level of words.\footnote{We use the HuggingFace \emph{evaluate} package implementation of both CER and WER.}

Finally, we apply few normalization steps before the evaluation. First, all unicode whitespace characters ($\backslash$s+) are normalized into a single whitespace. Secondly, in line with similar works \citep{duong-etal-2021-unsupervised,kettunen-paakkonen-2016-measuring}, we apply two normalization steps to address systematic differences between historical and modern spellings. In the English ECCO-TCP ground truth data, the long-s character \boxed{\text{\longs}} \ appears in places where modern English would use \boxed{\text{s}}. Similarly, in older Finnish historical texts \boxed{\text{w}} is often used where modern Finnish uses \boxed{\text{v}}. These spelling variations do not alter meaning, and we choose to disregard them by applying Unicode NFKC normalization, which handles both canonically equivalent and compatible transformations (including converting \boxed{\text{\longs}} to \boxed{\text{s}}) for both languages. Additionally, for Finnish, we replace all occurrences of \boxed{\text{w}} with \boxed{\text{v}} before evaluation, as modern Finnish does not use \boxed{\text{w}} except in loanwords or proper names, which occur only very rarely, making the difference negligible. 

\subsection{Models and Generation Parameters}

We evaluate several top-tier open-weight models as well as OpenAI's GPT-4o (v. 2024-08-06). The latter is included mostly for comparison, since it would not be cost-wise feasible to apply it to post-correction at a large scale, unlike open-weight models which can be applied on academic super-computing infrastructure. The evaluated open models are: Llama-3-8B-Instruct, Llama-3.1-8B-Instruct, and Llama-3.1-70B-Instruct from Meta \citep{llama3modelcard,llama3.1modelcard}, Mixtral-8x7B-Instruct-v0.1 from MistralAI \citep{jiang2024mixtral}, and Gemma-2-9B-it and Gemma-2-27B-it from Google \citep{gemma_2024}. It is noteworthy that while several of the open models are multilingual, none officially report supporting Finnish.

%\begin{description}
%    \item[Llama3-8B-Instruct] is a conversational model released by Meta. \citep{llama3modelcard} The Llama 3 family of models are optimized for dialogue in English, but trained on multilingual data.
%    \item[Llama3.1-8B/70B-Instruct] is newer release in Meta's model families. \citep{llama3.1modelcard} The Llama 3.1 model family is optimized for multilingual dialogue including 7 officially supported languages (English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai), however, the model has been trained on a broader collection of languages.
%    \item[Mixtral-8x7B-Instruct-v0.1] is a Sparse Mixture of Experts model released by MistralAI \cite{jiang2024mixtral}. The model is multilingual and is shown to work well at least on French, German, Spanish, Italian, and English, but is trained on a broader collection of languages.
%    \item[Gemma-2-9b/27b-it] is a lightweight instruction fine-tuned model released by Google \citep{gemma_2024}. The model is trained primarily on English data.
%    \item[GPT-4o] v. 2024-08-06
%\end{description}

All models are run on the Ollama framework\footnote{\url{https://ollama.com/}} (v. 0.3.8) for fast inference, using the default 4-bit quantization unless otherwise stated. Other quantization levels are experimented separately, and reported in Section~\ref{sec:quantization}. All parameters of the framework and models are set to default except for the ones explored in Section~\ref{sec:parameters}. Note that we will not repeat the "Instruct" in model names in tables and figures for space considerations.

\subsection{LLM Overgeneration Removal} \label{lab:subsection-llm-overgen-removal}

LLM outputs often include undesired content in addition to the requested output. In most cases, the undesired text appears either before the corrected text (e.g.\ \emph{"Here is your corrected text:"}), or after the corrected text has been provided (e.g.\ hallucinated continuation, or an additional commentary). This was noted also by \citet{boros2024postcorrection}, who applied simple heuristics for removing any unwanted text, such as removal of whitespace, parts of the prompt, and specific phrases commonly appearing in the model's output, together with trimming the predicted text if it exceeded 1.5 times the original. 
% for example extra commentary listing the corrections made, or unprompted continuation of the input text.

Therefore, we base our overgeneration removal on automatically aligning the generated output against the original input on character level, and filtering out leading and trailing texts which do not align well to the input. For this purpose, we utilize character-level local sequence alignment\footnote{Unlike global sequence alignment, its local counterpart does not penalize leading and trailing misalignments. We use the implementation in the \emph{biopython} package, with open-gap-score -1 and extend-gap-score -0.5} of the model's output and the OCR text, and recover the region between the first and the last aligned characters. The alignment is configured to ignore whitespace and the '-' character, to avoid text formatting discrepancies having an impact on the outcome of the alignment.


\subsection{Parameter Optimization}
\label{sec:parameters}

The model generation parameters naturally affect the quality of the output and we therefore optimize the most critical parameters of the open-weight model generation on a held-out development set. As discussed earlier, this set is not used in any subsequent experiments. 

Using the Optuna hyperparameter optimization library \citep{optuna_2019}, we set the temperature, top\_k and top\_p parameters. For each model and each language, we test 100 runs with different parameter combinations. Subsequently, the 10 best runs in terms of CER were selected, creating a range of possible best parameters. These ranges generally overlap across models but not across languages, therefore we pick a set of parameters for each language. The final parameters are chosen as the median value of the 10 best runs of every model. For English, the parameters are temperature 0.26, top\_k 65, and top\_p 0.66. For Finnish, the final parameters are temperature 0.14, top\_k 30, and top\_p 0.60.
%English: Temperature 0.26 - topK 65 - topP 0.66
%Finnish: Temperature 0.14 - topK 30 - topP 0.60



%optuna has been running for 40 hours, most of the models have 100 different runs on the 200 examples of the dev set.

%-using chunked data (about 300 tokens) representative size of ecco TCP page


\section{Results}

% Results without whitespace normalization, Finnish included
%\begin{table*}[]
%    \centering
%    \begin{tabular}{l|cccc} \\
%        Model & Eng CER & Eng WER & Fin CER & Fin WER \\\hline
%        Meta-Llama-3-8B-Instruct & 10.5 & 22.5 & -110.8 & -32.9 \\
%        Meta-Llama-3.1-8B-Instruct & 16.0 & 18.4 & -184.4 & -46.6 \\
%        Meta-Llama-3.1-70B-Instruct & 31.6 & 26.6 & -65.5 & -10.7 \\
%        Mixtral-8x7B-Instruct-v0.1 & -25.3 & 9.4 & -177.1 & -55.0 \\
%        gemma-2-9b-it & 21.3 & 18.2 & -29.1 & -4.7 \\
%        gemma-2-27b-it & 25.7 & 17.4 & -21.1 & -0.5 \\
%       gpt-4o & 55.5 & 59.6 & 11.6 & 33.6 \\
%    \end{tabular}
%    \caption{CER Improvement with fixed parameters evaluated on test set.}
%    \label{tab:main-results}
%\end{table*}

%UPDATED 5/12 with whitespace normalization and clipping
\begin{table}[]
    \centering
    \begin{tabular}{l|rr|rr}
              & \multicolumn{2}{c}{English} & \multicolumn{2}{c}{Finnish}\\
                      & CER & WER & CER & WER \\
        Model         &  \%  & \%  & \%  & \% \\\hline
        Llama-3-8B    & 7.3 & 31.4 & -68.8 & -28.2 \\
        Llama-3.1-8B  & 19.5 & 37.7 & -65.7 & -30.1 \\
        Llama-3.1-70B & 38.7 & 46.3 & -47.0 & -8.9 \\
        Mixtral-8x7B  & -14.9 & 19.1 & -76.5 & -40.5 \\
        Gemma-2-9B    & 28.2 & 38.4 & -24.0 & -4.1 \\
        Gemma-2-27B   & 35.6 & 37.8 & -19.1 & 0.0 \\
        GPT-4o        & 58.1 & 59.1 & 11.9 & 33.5 \\
    \end{tabular}
    \caption{Overall CER and WER relative improvement.}
    \label{tab:main-results}
\end{table}
% Results with whitespace normalization, no clipping
        % Llama-3-8B & 1.1 & 30.1 & -110.8 & -33.4 \\
        % Llama-3.1-8B & 18.3 & 37.3 & -184.3 & -52.9 \\
        % Llama-3.1-70B & 37.0 & 46.1 & -65.5 & -10.8 \\
        % Mixtral-8x7B & -43.7 & 14.6 & -176.9 & -59.7 \\
        % Gemma-2-9B & 25.9 & 38.1 & -29.1 & -4.7 \\
        % Gemma-2-27B & 34.4 & 37.6 & -21.1 & -0.4 \\
        % GPT-4o & 50.5 & 57.7 & 11.6 & 33.4 \\

% Results before whitespace normalization
        %Llama-3-8B & 10.5 & 22.5 & -110.8 & -32.9 \\
        %Llama-3.1-8B & 16.0 & 18.4 & -184.4 & -46.6 \\
        %Llama-3.1-70B & 31.6 & 26.6 & -65.5 & -10.7 \\
        %Mixtral-8x7B & -25.3 & 9.4 & -177.1 & -55.0 \\
        %Gemma-2-9B & 21.3 & 18.2 & -29.1 & -4.7 \\
        %Gemma-2-27B & 25.7 & 17.4 & -21.1 & -0.5 \\
        %GPT-4o & 55.5 & 59.6 & 11.6 & 33.6 \\

% Results with whitespace normalization + hyphen handling
        %Llama-3-8B & 1.0 & 23.0 & -110.8 & -33.4 \\
        %Llama-3.1-8B & 18.5 & 33.3 & -184.3 & -52.9 \\
        %Llama-3.1-70B & 37.1 & 43.5 & -65.5 & -10.8 \\
        %Mixtral-8x7B & -44.0 & 5.5 & -176.9 & -59.7 \\
        %Gemma-2-9B & 26.0 & 37.4 & -29.1 & -4.7 \\
        %Gemma-2-27B & 34.9 & 40.3 & -21.1 & -0.4 \\
        %GPT-4o & 50.3 & 54.5 & 11.6 & 33.4 \\

% Results with whitespace normalization and hyphen removal
        %Llama-3-8B & -0.2 & 28.7 & -110.9 & -33.0 \\
        %Llama-3.1-8B & 21.1 & 40.6 & -185.1 & -52.6 \\
        %Llama-3.1-70B & 41.0 & 52.2 & -66.1 & -10.2 \\
        %Mixtral-8x7B & -48.2 & 8.5 & -177.0 & -59.6 \\
        %Gemma-2-9B & 28.0 & 43.7 & -29.3 & -4.6 \\
        %Gemma-2-27B & 38.8 & 48.0 & -20.9 & 0.1 \\
        %GPT-4o & 54.3 & 62.3 & 12.2 & 34.1 \\


% files ending with "q4_0_{en/fi}_test.jsonl" used from /scratch/project_2005072/cassandra/ocr-postcorrection-lm/alignment-and-gridsearch/quantization_output

% gpt-4o test data from Slack
% model_correction_aligned used as key for predictions

% both use NFKC normalizations, Finnish uses also w-->v normalization

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/original_vs_new_cer_no-title.png}
    \caption{CER before and after correction on English test data (Llama-3.1-70B).}
    \label{fig:before-after-cer}
\end{figure}


The main results are shown in Table~\ref{tab:main-results}. For English, six out of seven models achieve positive improvement, ranging from 7.3\% (Llama-3-8B) to 58.1\% (GPT-4o) in terms of CER\%. GPT-4o outperforms all open models by a large margin, the next best model (Llama-3.1-70B) being almost 20pp worse. However, the Llama-3.1-70B still shows a notable improvement of 38.7\%. In Figure~\ref{fig:before-after-cer} we illustrate the CER values for English test examples before and after the Llama-3.1-70B correction. Most examples demonstrate improved CER scores, regardless of whether they initially had mild or severe noise levels.

In terms of WER\%, all models show positive improvement on English, the two best models achieving an improvement of 59.1\% (GPT-4o) and 46.3\% (Llama-3.1-70B). The relative order of the models seems to mostly follow the number of model parameters, bigger models generally performing better, expect for Mixtral which is clearly worse than the others, and the two Gemma models performing almost equally in terms of WER\%, although the Gemma-2-27B version clearly outperforms the 8B model in terms of CER\%.  

For Finnish, on the other hand, GPT-4o is the only model achieving a positive improvement in either metric, albeit considerably smaller in absolute terms than for English with 11.9 CER\% and 33.5 WER\%. Seeing these entirely negative results for Finnish, we are forced to conclude that prompt-based OCR post-correction is presently infeasible for this language using any of the tested open-weight models. This is disappointing, but not surprising since none of the models officially support Finnish.\footnote{We made also preliminary experiments with the well-known Finnish Poro model of \citet{luukkonen2024poro34bblessingmultilinguality}, but the results were considerably worse than the models in our study, and we did not pursue it any further.}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/fi_en_diff_v2.png}
    \caption{An example in both languages illustrating historical language artifacts alongside the corresponding GPT-4o generated output.}
    \label{fig:example-generation}
\end{figure*}

The striking effect of a common but meaning-preserving difference between historical and modern language becomes apparent when measuring the effect of modern spelling produced by the LLMs such as the \boxed{\text{\longs}} vs.\ \boxed{\text{s}} and \boxed{\text{w}} vs.\ \boxed{\text{v}} variation discussed in Section~\ref{sec:metrics}. A typical example for both languages is illustrated in Figure~\ref{fig:example-generation}. Without the applied normalization, the results of GPT-4o would have been 34.9 CER\% and 35.5 WER\% (compared to 58.1\% and 59.1\% with normalization) for English, and -10.1 CER\% and -4.8 WER\% (compared to 11.9\% and 33.5\%) for Finnish. This demonstrates a substantial impact on the reported scores, and while the relative model ranking is unlikely to change, we can see that the conclusion w.r.t.\ this model's performance on Finnish would have been the opposite, and the improvements seen in English would have been lot smaller.

% Results with whitespace normalization & clipping
%       GPT-4o English & 58.1 & 59.1 & 34.9 & 35.5 \\
%       GPT-4o Finnish & 11.9 & 33.5 & -10.1 & -4.8 \\

% gpt-4 results without whitespace normalization applied
% CER/WER are the original numbers from the main table
                % CER  & WER  &  CER  & WER \\
               % |-- norm ---|  |-- not norm -|  
%       English & 55.5 & 59.6 & 30.8  & 35.0 \\
%       Finnish & 11.6 & 33.6 & -12.5 & -7.5 \\

% Results with whitespace normalization applied
% GPT-4o English & 50.5 & 57.7 & 30.8 & 35.2 \\
% GPT-4o Finnish & 11.6 & 33.4 & -12.5 & -7.6\\

Given the entirely negative results for Finnish with the open-weight models, we carry out all further analyses on English only. Furthermore, we also remove the Mixtral-8x7B from follow-up experiments as it performs notably worse than the other models.


\subsection{LLM Overgeneration Removal}

%UPDATE 5/12/24 : whitespace normalization with clipping 
\begin{table}[]
    \centering
    \begin{tabular}{l|rr}
    & \multicolumn{2}{c}{Overg. removal}  \\
      Model  & w/o & with  \\
       \hline
        Llama-3-8B & -74.1 & 7.3 \\
        Llama-3.1-8B & -57.4 & 19.5 \\
        Llama-3.1-70B & -53.6 & 38.7 \\
        %Mixtral-8x7B & -67.6 & -14.9 \\
        Gemma-2-9B & 28.1 & 28.2 \\
        Gemma-2-27B & 35.3 & 35.6 \\
        GPT-4o & 53.7 & 58.1 \\
    \end{tabular}
    \caption{The CER improvement on English test data with and w/o the overgeneration removal step.}
    \label{tab:post-processing}
\end{table}
 % using q4_0 models
 % English: CER improvement
 % Finnish: CER improvement with modernization (W --> v) (deleted)

% Results with whitespace normalization, no clipping
        % Llama-3-8B & -484.9 & 1.1 \\
        % Llama-3.1-8B & -436.7 & 18.3 \\
        % Llama-3.1-70B & -450.0 & 37.0 \\
        % %Mixtral-8x7B & -19993.1 & -43.7 \\
        % Gemma-2-9B & 25.8 & 25.9 \\
        % Gemma-2-27B & 34.0 & 34.4 \\
        % GPT-4o & 45.3 & 50.5 \\

% Results without whitespace normalization
%        Llama-3-8B & -294.4 & 10.5 \\
%        Llama-3.1-8B & -280.4 & 16.0 \\
%        Llama-3.1-70B & -262.3 & 31.6 \\
%        Mixtral-8x7B-Instruct-v0.1 & -16301.3 & -25.3 \\
%        Gemma-2-9B & 21.2 & 21.3 \\
%        Gemma-2-27B & 25.5 & 25.7 \\
%        GPT-4o & 51.7 & 55.5 \\

Next we measure the effect of the alignment-based overgeneration removal method described in Section~\ref{lab:subsection-llm-overgen-removal}, i.e.\ we evaluate the raw model-generated output against the post-processed version of the generated output. The results are shown in Table~\ref{tab:post-processing}. For the Llama family models, the results without this post-processing step are highly negative, whereas all Llama models achieve positive improvements when this step is applied. This highlights the necessity of post-processing for the Llama models, which very systematically generate an additional explanation together with the requested output. An example of a typical Llama generation is:
% comparing the correction quality when using the raw output as generated by the model, as compared to the same generated output with the additional post-processing step

\begin{small}
\begin{verbatim}
 Here is the corrected text: {{answer}}
 I corrected the following errors:
 * "pi\&ure" -> "picture"
 * "it's" -> "its"  (multiple instances)
 * "gneralfcope" -> "general scope"
 ...
\end{verbatim}
\end{small}
%* "obvio',"s" -> "obvious" * "itsef" -> "itself" * "pleafing" -> "pleasing" (multiple instances) * "unpleafing" -> "unpleasing" * "Palla-" -> "Palladian" * "bfill" -> "brook" * "gurgle," -> "gurgle, as it runs." (added comma for clarity) * "Sheniftone" -> "Shenstone" * "tho" -> "though""

On the other hand, Gemma models seem to be largely unaffected, as they generally tend to not produce any additional text. For the GPT-4o model, we also notice a small gain when applying the post-processing, as it occasionally generates explanatory phrases like \emph{"Here is the corrected text:"}.


% TODO: mixtral dropped, make sure this is sytematic
%This method is not enough to help Mixtral-8x7B-Instruct-v0.1, which is still observed to deteriorate the the text no matter the language.

% UPDATE 5/12/24: Results with whitespace normalization and clipping
\subsection{Quantization and Performance}
\label{sec:quantization}


Since the historical text collections to which post-correction would potentially be applied comprise millions of pages of text, it is necessary to strike balance between accuracy and computational resources. Among the most important factors here is model quantization, i.e.\ real number representation with fewer bits. High degrees of quantization substantially reduce model memory footprint and increase inference speed, but can be assumed to potentially degrade model performance. We therefore evaluate the models at the 4 bit Q4\_0 quantization (default setting in Ollama), and at the standard 16 bit fp16 floating point representation. 

The results are reported in Table~\ref{tab:quantization}. As expected, the fp16 quantization performs better for all the evaluated models, with a gain of 2.5-4.7pp, except for Llama-3.1-8B where we do not experience a significant difference between 4bit and fp16 models. The relative ranking of the selected models is preserved regardless the quantization level, and using fp16 does not help less performing models to outrank any of the originally best performing 4bit quantized models. The improvement comes at a high cost in terms of memory footprint. As seen in the table, the best improvement is unsurprisingly achieved by the largest model, where the memory requirement increases from 43Gb to 132Gb. It is of consideration that even with 4bit quantization, using the largest Llama-3.1-70B model would necessitate 2 GPUs (assuming 32GB GPU memory), instantly doubling the GPU hours required to complete the task compared to other models which can fit on one GPU.

% q4 models, with post-processing, same parameters
% Memory usage taken from "seff" GPU-memory peak. table layout can be changed/fixed

% Results with whitespace normalization
%         Llama-3-8B & 1.1 & 7.8 & 6.31 & 16.1 \\
%         Llama-3.1-8B & 18.3 & 16.9 & 6.31 & 16.1 \\
%         Llama-3.1-70B & 37.0 & 41.7 & 43.6 & 132.1* \\
% %        Mixtral-8x7B-Instruct-v0.1 & -43.7 --- & - & - \\ 
%         Gemma-2-9B & 25.9 & 29.9 & 8.87 & 20.9 \\
%         Gemma-2-27B & 34.4 & 37.6 & 19.2 & 58.9 \\
        
% Results without whitespace normalization
%       Model & q4 & fp16 & q4 & fp16 \\\hline
%       Llama-3-8B & 10.5 & 13.5 & 6.31 & 16.1 \\
%       Llama-3.1-8B & 16.0 & 16.2 & 6.31 & 16.1 \\
%       Llama-3.1-70B & 31.6 & --- & 43.6 & 132.1* \\
%       Mixtral-8x7B-Instruct-v0.1 & -25.3 --- & - & - \\ 
%       Gemma-2-9B & 21.3 & 23.2 & 8.87 & 20.9 \\
%       Gemma-2-27B & 25.7 & 27.4 & 19.2 & 58.9 \\


\begin{table}[]
    \centering
    \begin{tabular}{l|rr|rr}
    %& \multicolumn{2}{c|}{CER\%} & \multicolumn{2}{c}{\parbox{2cm}{\centering Peak Memory \\ (Gb)}} \\
    & \multicolumn{2}{c|}{CER\%} & \multicolumn{2}{c}{{\centering Memory (Gb)}} \\
        Model & q4 & fp16 & q4 & fp16\phantom{*} \\\hline
        Llama-3-8B & 7.3 & 12.0 & 6.3 & 16.1\phantom{*} \\
        Llama-3.1-8B & 19.5 & 19.4 & 6.3 & 16.1\phantom{*} \\
        Llama-3.1-70B & 38.7 & 42.6 & 43.6 & 132.1* \\
%        Mixtral-8x7B-Instruct-v0.1 & -14.9 --- & - & - \\ 
        Gemma-2-9B & 28.2 & 30.7 & 8.9 & 20.9\phantom{*} \\
        Gemma-2-27B & 35.6 & 38.1 & 19.2 & 58.9\phantom{*} \\
%        GPT-4o        & ---  & --- & --- & ---  \\ % this is here to keep table 
    \end{tabular}
    \caption{CER improvement on English test data using 4bit quantized (q4) and fp16 models, alongside peak memory usage. * in the memory consumption indicates the number was obtained using the HuggingFace library, as we were not able to run the Llama-3.1-70B model with fp16 through Ollama.}
    \label{tab:quantization}
\end{table}





%\begin{figure*}
%    \centering
%    \includegraphics[width=1\linewidth]{figures/fi_en_diff_v2.png}
%    \caption{An example in both languages illustrating historical language artifacts alongside the corresponding GPT-4o generated output.}
%    \label{fig:example-generation}
%\end{figure*}





% gpt-4 results
% CER/WER are the original numbers from the main table

% UPDATE 5/12/24:
% with whitespace normalizing and clipping (English)
% GPT-4o & 58.1 & 59.1 & 34.9 & 35.5 \\
% with whitespace normalizing (English)
% GPT-4o & 50.5 & 57.7 & 30.8 & 35.2 \\ 

%\begin{table}[]
%    \centering
%    \begin{tabular}{l|rrrr} 
%        & \multicolumn{2}{c}{normalized} & \multicolumn{2}{c}{not normalized}\\
%        gpt-4o & CER & WER & CER & WER \\
%               &  \% & \%  & \%  & \% \\\hline
%        English & 55.5 & 59.6 & 30.8 & 35.0 \\
%        Finnish & 11.6 & 33.6 & -12.5 & -7.5 \\
%    \end{tabular}
%    \caption{The effect of accounting for text normalization and spelling variation in evaluation.}
%    \label{tab:mod}
%\end{table}




%Discussion - effects of normalizing and modernizing


%Notes on why the models are failing for Finnish
%- The models fail to recognize misspelled words in the OCR produced text, leaving them untouched.
%- Sometimes a correction is rather a concatenation of subwords that may look Finnish on the surface, not an actual Finnish word.
%- Sometimes single words are correct Finnish, but they do not form a grammatically meaningful unit.
%- w is often misidentified as m by the OCR software. It is typical that the models do not correct these words, even if they are not Finnish.






\section{Segment Length and Continuation} % or maybe something like "Correcting long documents"
\label{sec:segment-length}

The results in the previous sections were reported for text segments about 300 sub-words in length. The actual texts in the historical collections are naturally substantially longer, necessitating splitting the input into segments of appropriate length. This raises two related questions: (1) how long the input segments should optimally be for best post-correction accuracy, and (2) how should the outputs be combined to minimize degradation on segment boundaries.

Our English data is on the level of pages, which we cannot simply naively concatenate, we need other means to obtain sufficiently long documents. For this experiment, we sample long pages of at least 600 whitespace delimited OCR words in length from the development data, taking at maximum two pages from any one book. This resulted in a sample of 53 development set pages.

%, each divided into minimum of 2 segments.\footnote{Since the page level ECCO-TCP data has a limited number of very long pages (the median being in approx.\ 200 words), and the page length has a strong bias towards very limited number of books, for these experiments we increase the development data size by dividing the original training data 50/50 to development and test data.}


\begin{figure} % FIGURE UPDATED 6.12. -J
    \centering
    \includegraphics[width=.95\linewidth]{figures/segment_length_en_CER.png}
    \caption{CER\% improvement for English when using different segment lengths.}
    \label{fig:segment-length}
\end{figure}

These sampled pages are then divided into non-overlapping segments of 50, 100, 200, and 300 words, using the same alignment-based splitting strategy as described in Section~\ref{sec:basic-experiments}. The segments are corrected individually and the CER\% improvement over the segments is calculated. The results are shown in Figure~\ref{fig:segment-length}. Shorter segments (50--100 words) get notably worse CER\% score for all models, with the gains diminishing past about 200--300 words, but our page-level data does not have long-enough examples to allow us to reach the point where the performance would start consistently decreasing as the segments become too long. In the future, we plan to develop a book-level version of the data, and study the correction performance on even longer segments.


\subsection{Post-correction on Segment Boundaries}

Presently, post-correction studies either do not address segment-wise correction of longer texts as it is not necessary for the datasets they study, or split the input into non-overlapping segments, whose corrections are simply concatenated. This may potentially disrupt text continuity since neither word nor sentence boundaries can be reliably adhered to in the noisy OCR input. Furthermore, it also means that no left context is available for the correction of the beginning of each segment. This may potentially have a negative effect on the correction in the region around segment boundary. Here we quantify this effect and explore several straightforward methods for its mitigation.

%Providing context improves OCR post-correction performance \cite{bourne2024clocrc}. However, page boundaries often disrupt text continuity, as they may cut sentences or words in half. Combined with OCR errors, this lack of continuity can mislead models. When correcting pages sequentially, additional context can be provided by including text outside the immediate input boundaries.

For the prompt optimization, we use the same sample of 53 pages as in the previous section, and the final results are reported on a similar sample of 50 pages from the test data. In order to maximize the number of examples of segment boundaries for evaluation, we generate---from each page---pairs of segments 200+200 words long, with stride of 100 words. With this method, each page of at least 600 words produces a minimum of 3 examples of neighboring segment pairs. The final development and test samples include 194 and 208 such examples, respectively.

On these examples, we evaluate the following methods of post-correction on segment boundaries: (1) {\it Baseline}: Each segment is corrected independently with the same prompt and the outputs are concatenated; (2) {\it Left-corrected-concatenate (LCC)}: The left segment is corrected first and given as prior context for the correction of the right segment, the model is instructed to only correct the right segment; and (3) {\it Left-uncorrected-concatenate (LUC)}: The uncorrected left segment is provided for context in the correction of the right segment. The primary advantage of this method is that correcting the right segment does not need the left segment to be corrected first, making parallelization of the process much simpler technically.

The overall results across these strategies are shown in Table~\ref{tab:merging-overall} and suggest that at present only the two largest models are able to follow the more complex prompts necessitated when merging neighboring segment corrections. The smaller models occasionally suffer from omitting part of the text to correct, which did not seem to occur if only one text was given at a time. For the two models improving on performance, we inspect the boundary effect more closely in Table~\ref{tab:cer-on-seams} where we report CER\% calculated on $\pm 10$ words around the boundary of the two segments.\footnote{The $\pm 10$ word boundaries were human-verified to ensure that the evaluation occurred at the same boundary, even in more complex examples where words were omitted and/or added.} Here we see that both methods effectively incorporate the provided additional context, substantially improving the post-correction of the right segment at the boundary, whereas the baseline system's performance on the right side is notably worse compared to its performance on the left side.

%However, as reported in Table~\ref{tab:merging-overall}, the more complicated prompt may cause unexpected degradation in performance when the whole text is considered.

% UPDATE 5/12/24: with whitespace normalization and clipping
\begin{table}[t]
    \centering
    \begin{tabular}{l|rrr}
              & \multicolumn{3}{c}{Method}\\
        Model & Bas. &  LCC & LUC \\\hline
        Llama-3-8B & -0.2 & -2.9 & -2.8 \\
        Llama-3.1-8B & 13.7 & 8.4 & 10.6 \\
        Llama-3.1-70B & 33.2 & 36.0 & 34.6 \\
        Gemma-2-9B & 29.2 & 27.9 & 28.3 \\
        Gemma-2-27B & 38.1 & 39.9 & 39.7 \\
    \end{tabular}
    \caption{CER improvement on English test data with different correction methods.}
    \label{tab:merging-overall}
\end{table}

% with whitespace normalization
        % Llama-3-8B & -2.6 & -8.2 & -6.3 \\
        % Llama-3.1-8B & 13.7 & 6.9 & 10.0 \\
        % Llama-3.1-70B & - & - & - \\
        % Gemma-2-9B & 29.2 & 27.9 & 28.3 \\
        % Gemma-2-27B & 38.1 & 39.9 & 39.7 \\

% Alignment methods table 26/11/2024
%        Llama-3-8B & 16.0 &  -85.8 & -69.6 \\
%        Llama-3.1-8B & 19.4  & -94.7 & -84.5 \\
%        Llama-3.1-70B & 37.3 & 25.2 & 36.2 \\
%        Gemma-2-9b & 30.6 &  -87.1 & -83.9 \\
%        Gemma-2-27b & 34.9  & 24.5 & 18.8 \\

% test set 5/12
\begin{table}[h]
    \centering
    \begin{tabular}{l|cc|c|c}
        %Model & Baseline & 2 & 3 &  4 & 5 \\\hline
               & \multicolumn{4}{c}{Method}\\\hline
              & \multicolumn{2}{c|}{Baseline} 
              & LCC 
              & LUC \\
                Model      & L & R   &  R &   R\\\hline
        Llama-3.1-70B & 29.1 &\ 9.8 & 21.4 & 21.7 \\
        gemma-2-27b & 34.5 & 18.7  & 29.7 & 33.7 \\
    \end{tabular}
    \caption{CER\% around segment boundaries with different correction methods. L and R stand for left and right of the boundary.}
    \label{tab:cer-on-seams}
\end{table}

%dev set numbers with old prompt 26/11
%& \multicolumn{4}{c}{Method}\\\hline
 %             & \multicolumn{2}{c|}{Baseline} 
  %            & LCC 
   %           & LUC \\
    %            Model      & L & R   &  R &   R\\\hline
     %   Llama-3.1-70B & 15.7&\ 9.5 & 21.2 & 28.5 \\
      %  gemma-2-27b & 31.9&14.6  & 29.0 & 25.4 \\

% Results derived from scratch/project_2005072/cassandra/ocr-postcorrection-lm/alignment-and-gridsearch/final_output/ from files that include the string "basic" or "fancy{2,3,4}" and not the string "NOTFINISHED"
% predictions: ["originals"]["model_correction_aligned"]["1"] & ["originals"]["model_correction_aligned"]["2"] " ".joined together if method == basic
% predictions: ["originals"]["model_correction_aligned_joined"] if method == fancy{2,3,4}
% references: ["originals"]["output_1"] & ["originals"]["output_2"] " ".joined together
% originals: ["originals"]["input_1"] & ["originals"]["input_2"] " ".joined together



\section{Conclusion}

We set out to establish the ability of recent open-weight models to post-correct OCR errors in a zero-shot, prompt-based setting. In the first set of experiments, we established that for historical English these models achieved notable improvements (Llama-3.1-70B-Instruct reaching a CER improvement of 38.7\%), even though still far behind the commercial GPT-4o model (58.1 CER\%). We also demonstrate the necessity of post-processing to remove any additional, model generated text, and present an effective string alignment technique to address this. We also highlight the effect of segment length, which may have a substantial negative impact on the outcome if set too short.


Unlike for English, for Finnish we find poor performance across the board and need to conclude that zero-shot post-correction with open-weight models remains currently out of reach for historical Finnish. 

In a separate set of experiments we examine how segment-wise correction of long documents should be approached. We devise and evaluate a number of methods to incorporate additional context for the correction of individual segments. We find that some of these methods have a strong positive effect in the immediate proximity of segment boundaries, however, for smaller models the more complicated prompt may cause unexpected degradation in performance when the whole text is considered. Further work will be necessary to resolve these issues.

As future work, we will pursue a large correction run of the ECCO collection as well as a fine-tuned model for Finnish post-correction. All datasets and evaluation scripts used in this study are available at \url{https://github.com/TurkuNLP/ocr-postcorrection-lm} to support result replication and comparability.


\section*{Limitations}

Our work includes certain limitations, which we will discuss next. First, during data preprocessing, we discarded a proportion of documents (\textasciitilde10\% for English, \textasciitilde6\% for Finnish) that our correction methods may not be able to address. These documents include cases with severe alignment issues between OCR output and ground truth. We acknowledge that our post-correction method, which relies entirely on the OCR system's output, cannot recover text where significant portions are missing, therefore setting an upper-boundary for the method. Further analysis is needed to investigate the causes of these gaps and to determine how much, if any, of this missing information could potentially be addressed through post-correction.

We also find that OCR post-correction evaluation suffers from various dataset and metric issues, some of which we have already discussed (e.g.\ normalization). In related work (including our own study conducted directly on our long-term target corpora), results are reported on varying datasets and evaluations metrics. These challenges make it difficult to achieve comparable results across studies and languages, potentially contributing to some of the contradictory conclusions reported in prior work. Clearly, more work will be needed to establish a set of standard benchmarks that resolve most of the data and evaluation issues.


Finally, reporting pure numeric improvements does not address all aspects of downstream data usability. While an improved word error rate has a direct, positive effect on certain applications (e.g.\ lexical search), its impact on others (e.g.\ close reading) may be less straightforward or proportional. 


\section*{Acknowledgments}

This work was carried out in the \textit{Human Diversity} University profilation programme (PROFI-7) of the Research Council of Finland, as well as in the context of several other research projects supported by the Research Council of Finland. Computational resources were provided by CSC --- the Finnish IT Center for Science.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliographystyle{acl_natbib}
\bibliography{llm_ocr_main_article}


\end{document}
