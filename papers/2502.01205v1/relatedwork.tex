\section{Related work}
Despite decades of active research, post-correction of historical documents remains a challenge. The ICDAR 2017 and 2019 shared tasks \citep{icdar2017,icdar2019} addressed the lack of adequate benchmarks for evaluating OCR performance across several languages, introducing two tracks: detecting OCR errors, and correcting previously detected errors. This setting has naturally guided the development towards two-stage systems, and the best performing models in the ICDAR 2019 edition were based on the BERT model fine-tuned separately for each task \citep{icdar2019}. Such two-step approaches are still actively pursued, with e.g.\ \citet{beshirov2024postocrtextcorrectionbulgarian} applying a BERT classifier for error detection, and an LSTM-based seq2seq model for error correction in Bulgarian.

%As noted by \citet{icdar2019}, an overall improvement in error detection performance was observed in the 2019 edition of the shared task, with . 

% here 1-2 sentence description of these shared task
% was this two-step approach, first identify erroneus words, then suggest how to fix a word?
% cite some paper(s) from ICDAR who do this two-step thing


% cite this paper:
%Post-OCR Text Correction for Bulgarian Historical Documents (2024)
% https://arxiv.org/pdf/2409.00527
% two-steps approach: bert base binary classifier (is word correct or not), lstm based seq2seq error fixing

Recently, LLMs have been effectively applied to text correction problems, for example, \citet{penteado2023evaluating} and \citet{ostling-etal-2024-evaluation} demonstrated that LLMs perform well in grammatical error correction. Naturally, LLMs have been proposed also to the OCR post-correction task, in line with the two broad paradigms of LLM use: fine-tuning for the post-correction task and purely prompt-based zero-shot generation. Fine-tuning was applied e.g.\ by \citet{soper-etal-2021-bart} who fine-tune the BART model on the English subset of the ICDAR 2017 data and apply it to English Newspaper text. \citet{veninga2024msc} fine-tunes the character-based ByT5 model on the ICDAR 2019 data, with a prompt-based Llama model as a baseline. Similarly, \citet{madarasz2024ocrcleaning} apply the mT5 model to historical Hungarian scientific literature, and \citet{dereza-etal-2024-million} applies the BART model to historical Irish--English bilingual data. 


In the zero-shot, prompt-based line of work, \citet{boros2024postcorrection} evaluated a variety of models and prompts on several multilingual historical datasets. Interestingly, the results of the study were mostly negative, concluding that LLMs (including the commercial GPT-4 model) are not effective at correcting transcriptions of historical documents, in many cases the LLM actually decreasing the quality instead of improving it. \citet{bourne2024clocrc} conducted a similar study on three historical English datasets, arriving at the opposite conclusion. They achieved over 60\% reduction of character error rate at best, with most of the evaluated models improving the data quality.

Finally, several studies also pursue approaches that include the original image as an input, together with the OCR output to be post-corrected. Here, e.g.\ \citet{chen2024trocrmeets} combine a state-of-the-art transformer-based OCR system with the character-based CharBERT model for handwritten text recognition, and \citet{fahandari2024farsi} propose a model iterating between OCR and post-correction steps for Farsi. Such image-text approaches are, nevertheless, beyond the scope of the present study.



%--- Data ---

%Historical Ink: 19th Century Latin American Spanish Newspaper Corpus with LLM OCR Correction (2024) % Data paper, methods not very well described and no evaluation results
%https://arxiv.org/pdf/2407.12838

%Slovenian: https://www.clarin.si/repository/xmlui/handle/11356/1907

%French, English, German, Italian: https://huggingface.co/datasets/PleIAs/Post-OCR-Correction