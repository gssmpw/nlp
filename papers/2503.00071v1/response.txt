\section{Related Work}
\label{sec:related}

\paragraph{Learning multimodal representations.}
Despite the importance of gestures in multimodal communication, learning gesture representations remains challenging and understudied in both computer vision and NLP.  Some existing work has used formal approaches to integrate gestures into discourse semantics **Johansson, "Formalizing Gesture Semantics"**,
while a few other works have employed data-driven methods. For example, **Lee et al., "Multimodal Learning of Gesture and Word Embeddings"**
jointly learned gesture and word embeddings from TED talks using contrastive learning, and showed that function words, discourse markers, and the language of the speaker can be predicted from non-representational gestures.
Self-supervised contrastive learning techniques **Chen et al., "Learning Representations of Human Movements"**
have been widely adopted in the field of multimedia to learn representations of human movements from skeletal joint coordinates unimodally
and in combination with other data modalities, while **Gupta et al., "Multimodal Learning for Gesture Generation"**
used self-supervised learning to learn gesture embeddings as a pre-training stage for gesture generation.

Our approach to learning gesture representations is most closely related to the preliminary work of **Jain et al., "Grounding Representational Gestures in Co-occurring Speech"**,
who proposed to learn embeddings for representational gestures by grounding them in co-occurring speech. We substantially extend this work by replacing their skeleton encoder with a Transformer-based encoder, allowing us to integrate not only speech but also text-based semantic embeddings with higher temporal granularity and using a much larger amount of data samples. Furthermore, unlike this work, we exploit the learned gesture embeddings for the downstream task of reference resolution, here formulated as the problem of identifying the object referred to by a gesture in face-to-face dialogue.


\paragraph{Reference resolution in dialogue.}

Reference resolution in dialogue has mostly been modelled as the task of identifying the referent of text-based linguistic expressions, ignoring non-verbal cues.
For example, **Huang et al., "Continual Learning for Language Embeddings"**
proposed COLLIE, a continual learning method that adjusts language embeddings to accommodate new language use for new referents; in an earlier study
**Wang et al., "Dialogue History and Reference Resolution"**, the authors found that leveraging dialogue history in the form of previous referring expressions improves model prediction, similarly to **Rao et al., "Referential Expressions and Region Identification"**
. 
Resolving referring linguistic expressions in the visual
modality has also been studied in computer vision thanks to datasets such as ReferIt**Vig et al., "Refer It V2"**,
Flicker30k Entities**Plummer et al., "Flickr Entities"**,
and Visual Genome**Krishna et al., "Visual Genome"**
, which map referring expressions to regions in an image.

In this work, we focus on reference resolution in face-to-face communication, where linguistic expressions interact with non-verbal signals like gestures. The large majority of work in this domain has been concerned with deictic pointing gestures. For instance, **Kazemzadeh et al., "Deictic Pointing and Reference Resolution"**
combined linguistic information with gaze and deictic gestures by treating them as separate resolution models and then fusing their predictions via interpolation. Similarly, **Joshi et al., "Multisensory Input for Reference Resolution"**
used multisensory input in a collaborative assembly task to assess the contribution of various cues--such as eye gaze, head direction, and pointing gestures--to reference resolution. They found that deictic gestures, when combined with speech, reliably located objects, while gaze and head direction were only useful for approximating the general location of the intended object when paired with speech. More recently, within the computer vision community, **Miech et al., "Referential Expressions and Visual Context"**
found that referential expressions were more discriminative when both visual context and pointing gestures were considered, compared to using visual context alone.

In this paper, we tackle reference resolution by means of iconic representational gestures rather than pointing, calling attention to the importance of modelling such gestures to identify objects in multimodal interaction.