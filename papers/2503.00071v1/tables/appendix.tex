\section*{Appendix}

\section{Objects in the CABB Dataset}
\label{app:objects}
\noindent \citet{rasenberg2022primacy} segmented each gesture stroke in the CABB-S dataset and classified them into four categories: \emph{iconic} (depicting an aspect of the target object), \emph{deictic} (explicitly pointing with an extended finger or hand), \emph{other} (e.g., beat gestures or pragmatic signals like “you go ahead”), and \emph{undecided} when the gesture type was unclear. In our study, we focus exclusively on the iconic gestures, which refer to specific parts of the novel objects in the CABB setup (see Figure \ref{fig:all_object_and_subparts}).
The average number of gestures in each round per dialogue is shown in Figure \ref{fig:gesture_distribution}.
Each iconic gesture was annotated with a sub-part label---such as \texttt{06A} for a single sub-part or \texttt{06A+06B} for multiple sub-parts. When a gesture was annotated with multiple parts, we split it into separate samples corresponding to each sub-part. Additionally, a \texttt{main} label was assigned if the gesture targeted the object’s main part, and \texttt{general} is used when the gesture indicates a broad area (e.g., “the left side”).

\begin{figure*}
    \centering
    \includegraphics[width=0.80\linewidth]{latex//figures/all_object_and_subparts.png}
    \caption{The candidate objects and their sub-parts present in the CABB dataset \cite{eijk2022cabb}.}
    \label{fig:all_object_and_subparts}
\end{figure*}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex//figures/distribution_of_gestures_per_round.pdf}
    \caption{Distribution of gestures across rounds of interaction.}
    \label{fig:gesture_distribution}
\end{figure}

\section{Model Details}
\label{app:models}

\subsection{Pre-training Objectives}
In Section \ref{sec:models}, we introduced three pre-training architectures, each containing a combination of self-supervised learning objectives. Here, we provide a detailed technical overview of these losses. The proposed architectures exploit three modalities, namely 2D skeletal keypoints and joint prediction confidence scores $\boldsymbol{X}_i^g \in \mathbb{R}^{T_g \times 27 \times 3}$ for gestures, text-based semantics  $\boldsymbol{X}_i^t$, and raw speech signals $\boldsymbol{X}_i^s$. These inputs are encoded using one of the following encoders: our adaptation of DSTFormer $f_{\Theta_g}(\cdot)$ for skeletons (Section \ref{subsec:encoders}; \citet{zhu2023motionbert}), Dutch BERT-based model $f_{\Theta_t}(\cdot)$ for text \cite{devries2019bertje}, and \texttt{wav2vec2-xlsr-300} $f_{\Theta_s}(\cdot)$ for raw speech \cite{conneau2020unsupervised}.
\label{subsec:objectives}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{latex//figures/unimodal_approach.pdf}
    \caption{Unimodal architecture jointly optimizes masked reconstruction (left branch) and unimodal contrastive losses (middle and right branches). Note that cross-attention blocks are not utilized in this pre-training approach.}
    \label{fig:unimodal_approach}
\end{figure*}

\paragraph{Unimodal masked reconstruction loss.} 
We follow the original DSTFormer \cite{zhu2023motionbert} by randomly masking portions of the 2D keypoint input and learning to reconstruct them. Specifically, we accumulate a reconstruction loss between the original and predicted coordinates of masked keypoints  as follows:
\begin{equation} 
    \mathcal{L}_{k} = \sum_{t=1}^{T}  \sum_{j=1}^{J}  \delta_{t,j} ||\boldsymbol{\hat{x}}_{t,j} - \boldsymbol{x}_{t,j}||^2,
\end{equation}
where $\boldsymbol{\hat{x}}_{t,j} \in \mathbb{R}^2$ is the predicted coordinates of keypoint $j$ at timestep $t$, $\boldsymbol{x}_{t,j}$ is the ground truth keypoint, and $\delta_{t,j}$ is a weighting factor that accounts for confidence or visibility of the keypoints. 
To enforce spatial and temporal consistency, we introduce two additional reconstruction objectives $\mathcal{L}_{b}$ and $\mathcal{L}_{m}$: bone and motion reconstruction losses. The former ensures structural consistency by preserving the distances between adjacent keypoints $||\boldsymbol{x}_{t,j} - \boldsymbol{x}_{t,j-1}||$ across frames, while the latter minimizes the difference between the temporal displacement $||\boldsymbol{x}_{t,j} - \boldsymbol{x}_{t-1,j}||$ of predicted and ground truth keypoints. The overall objective for masked reconstruction is given by the average of $\mathcal{L}_{k}$, $\mathcal{L}_{b}$ and $\mathcal{L}_{m}$. Figure \ref{fig:unimodal_approach} illustrates how this objective is integrated into the unimodal pre-training architecture.

\paragraph{Unimodal contrastive loss.}
A unimodal contrastive loss is applied to different views of the same skeletal keypoint sequence distorted with simple augmentations, as illustrated in the middle and right branches of Figure \ref{fig:unimodal_approach}. Formally, for input skeleton $\boldsymbol{X}_i^g$, we obtain two augmented views $a(\boldsymbol{X}_i^g)$ and $a'(\boldsymbol{X}_i^g)$. These views are then passed through skeleton encoder $f_{\Theta_g}(\cdot)$, namely DSTFormer, and projection layers $g_{\Theta_g}(\cdot)$ to obtain projected features $\boldsymbol{z}_i^g = g_{\Theta_g}(f_{\Theta_g}(a(\boldsymbol{X}_i^g)))$ and $\boldsymbol{z}_j^g = g_{\Theta_g}(f_{\Theta_g}(a'(\boldsymbol{X}_i^g)))$. These representations are treated as a positive pair in a contrastive loss function, whereas all other views from a training mini-batch are considered negative \cite{chen2020simple}:

\begin{equation}
    l_{um}(i, j) = -log\frac{exp(\frac{s(\boldsymbol{z}^g_i, \boldsymbol{z}^g_j)}{\tau})}{\sum_{k=1}^{2b} \mathbb{I}_{[k \neq i]} exp(\frac{s(\boldsymbol{z}^g_i, \boldsymbol{z}^g_k)}{\tau})}.
    \label{eq:unimodal_contrastive_loss}
\end{equation}

\noindent
The loss maximizes cosine similarity $s(\cdot)$ for the positive pair and minimizes similarity with other views in a mini-batch of size $b$.

\paragraph{Multimodal contrastive loss.}

We propose a CLIP-like contrastive objective (depicted with a blue line in Figure \ref{fig:multimodal-x_approach}) mapping global representations of skeletons and co-occurring utterances in a joint feature space \cite{radford2021learning}. In detail, given projected representations of skeletons $\boldsymbol{z}_i^g = g_{\Theta_g}(f_{\Theta_g}(\boldsymbol{X}_i^g))$ and co-occurring utterances (\eg, text-based semantics) $\boldsymbol{z}_i^t = g_{\Theta_t}(f_{\Theta_t}(\boldsymbol{X}_i^t))$, the multimodal objective aims to maximize their similarity as follows:

\begin{equation}
    l^{g \rightarrow t}_{mm}(i)^ = -log\frac{exp(\frac{s(\boldsymbol{z}^g_i, \boldsymbol{z}^t_i)}{\tau})}{\sum_{k=1}^{b} exp(\frac{s(\boldsymbol{z}^g_i, \boldsymbol{z}^t_k)}{\tau})}.
    \label{eq:mm_loss}
\end{equation}

\noindent
The final error function accumulates losses $l^{g \rightarrow t}_{mm}$ and $l^{t \rightarrow g}_{mm}$ for each skeleton-utterance pair in a mini-batch.

\paragraph{Multimodal-X losses.}
Two losses are employed to optimize the multimodal-X architecture (Figure~\ref{fig:multimodal-x_approach}). First, the contrastive loss is computed between skeleton representations and pooled semantic embeddings in line with Equation \ref{eq:mm_loss}. Furthermore, we introduce an objective that leverages cross-attention in our adapted DSTFormer (Section~\ref{subsec:encoders}). Specifically, the representations of skeletons in one branch of the architecture are fused with semantic or speech embeddings via cross-attention layers (middle branch in Figure~\ref{fig:multimodal-x_approach}), while the other branch remains unimodal (right branch in the figure). We then apply the same contrastive formulation (Equation~\ref{eq:unimodal_contrastive_loss}) to align unimodal skeleton representations with the fused skeleton--crossmodal embeddings. This strategy encourages both robust unimodal representations and cross-modal alignment.

\input{latex/tables/paramters}
\section{Implementation Details}
\label{app:implementation_details}
All models are implemented using PyTorch \cite{Ansel_PyTorch_2_Faster_2024} and PyTorch-Lightning \cite{Falcon_PyTorch_Lightning_2019}. Training is performed on nodes with four NVIDIA A100-SXM4-40GB GPUs. The experimented three model types---unimodal, multimodal, and multimodal-X---each trained using its respective objective for a maximum of 100~epochs. We use Adam optimizer with a learning rate of 0.001. For multimodal-X, a per-GPU batch size of 96 (for a total of 384 across four GPUs) strikes a balance between VRAM utilization and achieving reliable convergence. For multimodal models, we could only fit a batch size of 64 per GPU due to using the additional model for masked reconstruction. We also used a batch size of 64 for the unimodal models. 
For the contrastive objective, we set the temperature to 0.1 by default. Masked reconstruction follows the DSTFormer \cite{zhu2023motionbert} configuration with a masking probability of 5\% and an equivalent amount of noise applied. We randomly split 90\% of our generated time windows in CABB-XL for pre-training and use the remaining 10\% for validation. The CABB-S dataset is reserved solely to monitor pre-training convergence and agreement with expert annotations; we select final checkpoints based on these performance metrics. Throughout model training, we employ data augmentations similar to those proposed by \citet{ghaleb2024an}. Namely, we apply various skeletal transformations (mirror, shift, scaling, rotation, jitter, shear) to ensure the models generalize to pose variability.

\subsection{Implementation of the skeleton encoder}
\label{app:skeleton_encoder}
The adapted DSTFormer encoder processes skeletal data with two parallel branches that separately attend to spatial and temporal features.  In its unimodal version (without cross-attention), the encoder consists of 4 blocks per branch (8 blocks overall), each containing standard attention and MLP layers with residual connections. The encoder’s output is then fed into a projection head---an MLP that maps the encoded features (e.g., from 256 to 128 dimensions)---to produce the final feature representation.

In the multimodal-x variant, each block contains an additional cross-attention module that fuses either text-based semantic embeddings or speech embeddings with the skeleton representation. This extension doubles the number of trainable parameters, resulting in about 22.0 million parameters when using semantic inputs and 24.2 million when using speech inputs. The difference in parameter counts is due to the different embedding sizes from the semantic (768 dimensions) and speech (1024 dimensions) backbones and the subsequent projection heads. Similar to the unimodal case, dedicated projection heads then map the features to the shared embedding space. The multimodal‑x model pre‑trained with text-based semantics takes approximately 15 hours to run, while the multimodal model runs for roughly 12 hours. The unimodal one requires considerably less time (around 8 hours for 100 epochs) since it does not rely on the backbone models of co-occurring speech. 

In Table \ref{tab:parameters}, we summarize the number of parameters for each model architecture.












