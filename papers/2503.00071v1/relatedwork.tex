\section{Related Work}
\label{sec:related}

\paragraph{Learning multimodal representations.}
Despite the importance of gestures in multimodal communication, learning gesture representations remains challenging and understudied in both computer vision and NLP.  Some existing work has used formal approaches to integrate gestures into discourse semantics \cite{lascarides2009formal,lai2024encoding}, while a few other works have employed data-driven methods. For example, \citet{abzaliev2022towards} jointly learned gesture and word embeddings from TED talks using contrastive learning, and showed that function words, discourse markers, and the language of the speaker can be predicted from non-representational gestures. 
Self-supervised contrastive learning techniques \cite{chen2020simple,radford2021learning} have been widely adopted in the field of multimedia to learn representations of human movements from skeletal joint coordinates unimodally \cite{thoker2021skeleton,zhu2023motionbert} and in combination with other data modalities \cite{brinzea2022contrastive, liu2024multi}, while \cite{lee2021crossmodal} used self-supervised learning to learn gesture embeddings as a pre-training stage for gesture generation.

Our approach to learning gesture representations is most closely related to the preliminary work of \citet{ghaleb2024learning}, who proposed to learn embeddings for representational gestures by grounding them in co-occurring speech. We substantially extend this work by replacing their skeleton encoder with a Transformer-based encoder, allowing us to integrate not only speech but also text-based semantic embeddings with higher temporal granularity and using a much larger amount of data samples. Furthermore, unlike this work, we exploit the learned gesture embeddings for the downstream task of reference resolution, here formulated as the problem of identifying the object referred to by a gesture in face-to-face dialogue.


\paragraph{Reference resolution in dialogue.}

Reference resolution in dialogue has mostly been modelled as the task of identifying the referent of text-based linguistic expressions, ignoring non-verbal cues. %
For example, \citet{skantze2022collie} proposed COLLIE, a continual learning method that adjusts language embeddings to accommodate new language use for new referents; in an earlier study  \cite{shore2018using}, the authors found that leveraging dialogue history in the form of previous referring expressions improves model prediction, similarly to \citet{takmaz2020refer}. 
Resolving referring linguistic expressions in the visual
modality has also been studied in computer vision thanks to datasets such as ReferIt~\cite{kazemzadeh2014referitgame}, Flicker30k Entities~\cite{plummer2015flickr30k},
and Visual Genome~\cite{krishna2017visual}, which map referring expressions to regions in an image.

In this work, we focus on reference resolution in face-to-face communication, where linguistic expressions interact with non-verbal signals like gestures. The large majority of work in this domain has been concerned with deictic pointing gestures. For instance, \citet{kennington2017simple} combined linguistic information with gaze and deictic gestures by treating them as separate resolution models and then fusing their predictions via interpolation. Similarly, \citet{kontogiorgos2018multimodal} used multisensory input in a collaborative assembly task to assess the contribution of various cues--such as eye gaze, head direction, and pointing gestures--to reference resolution. They found that deictic gestures, when combined with speech, reliably located objects, while gaze and head direction were only useful for approximating the general location of the intended object when paired with speech. More recently, within the computer vision community, \citet{chen2021yourefit} found that referential expressions were more discriminative when both visual context and pointing gestures were considered, compared to using visual context alone.

In this paper, we tackle reference resolution by means of iconic representational gestures rather than pointing, calling attention to the importance of modelling such gestures to identify objects in multimodal interaction.