\section{Related Work}
\textbf{Dialectal Diversity.} Addressing dialectal diversity in NLP remains a significant challenge due to inherent linguistic variations shaped by social and cultural contexts. Early research identified systemic biases in language models against non-standard dialects such as AAVE, highlighting issues like the misclassification of AAVE tweets as toxic and difficulties in syntactic parsing \cite{sap-etal-2019-risk, jorgensen-etal-2015-challenges}. Recent studies extend these findings to modern LLMs, revealing persistent dialect prejudice in evaluations related to employability, criminality, and medical diagnoses \cite{hofmann2024dialectprejudicepredictsai, fleisig2024linguisticbiaschatgptlanguage, blodgett2017racialdisparitynaturallanguage}.

\textbf{Benchmarking Approaches.} Benchmarking dialect robustness has primarily followed two approaches. The first employs rule-based lexical substitutions in frameworks like VALUE and Multi-VALUE \cite{ziems-etal-2022-value, ziems2023multivalueframeworkcrossdialectalenglish}. While scalable, these methods often fail to capture nuanced, context-dependent linguistic features essential for authentic dialect representation, such as AAVE’s habitual “be” \cite{green2002african, lippi1997english} or Chicano English’s Spanish-influenced prosody \cite{fought2003chicano, santa1993chicano}. The second approach relies on human-annotated translations for authenticity, as seen in datasets like ReDial and AraDiCE \cite{lin2025languagegapsevaluatingdialect, mousi2024aradicebenchmarksdialectalcultural}, but these typically focus on single dialects, limiting their applicability for comprehensive dialect fairness evaluations across multiple linguistic variations.

\textbf{Hybrid Human-Machine Methodologies.} Emerging hybrid approaches combine automated translation techniques with human validation to mitigate the limitations of purely rule-based or human-annotated methods. For example, AraDiCE \cite{mousi2024aradicebenchmarksdialectalcultural} integrates automated translations with native speaker post-edits for Arabic dialects, while ReDial \cite{lin2025languagegapsevaluatingdialect} leverages human validation to ensure cultural and linguistic fidelity. Similarly, AAVENUE \cite{gupta2024aavenuedetectingllmbiases} offers human-validated evaluations for AAVE in NLU tasks but remains restricted to a single dialect.

\textbf{Sociolinguistic Impact and Real-World Discrimination.} Beyond technical benchmarks, sociolinguistic studies have linked LLM biases to real-world discrimination—such as housing denials for AAVE speakers \cite{hofmann2024dialectprejudicepredictsai, purnell1999dialect} and biased criminal justice assessments \cite{fleisig2024linguisticbiaschatgptlanguage}. Multilingual initiatives like LLM for Everyone \cite{cahyawijaya2024llmeveryonerepresentingunderrepresented} advocate for continuous tuning of models to improve performance on underrepresented languages, an approach that aligns with our use of human-guided few-shot prompting informed by authentic linguistic examples \cite{ewave, platt1980english}.

\textbf{Remaining Gaps and Our Contribution.} Although prior work has deepened our understanding of dialect biases in NLP, significant gaps remain in developing comprehensive, multi-dialect benchmarks that integrate authentic linguistic features. \textbf{\methodname} addresses these gaps by providing a robust benchmark that combines both automated and human-validated translation methods, thereby fostering more equitable language technology development.