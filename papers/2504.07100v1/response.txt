\section{Related Work}
\textbf{Dialectal Diversity.} Addressing dialectal diversity in NLP remains a significant challenge due to inherent linguistic variations shaped by social and cultural contexts. Early research identified systemic biases in language models against non-standard dialects such as AAVE, highlighting issues like the misclassification of AAVE tweets as toxic and difficulties in syntactic parsing **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Too Powerful?"**. Recent studies extend these findings to modern LLMs, revealing persistent dialect prejudice in evaluations related to employability, criminality, and medical diagnoses **Blodgett et al., "Language (Technology) is Power: A Critical Survey of ‘Bias’ in NLP"**.

\textbf{Benchmarking Approaches.} Benchmarking dialect robustness has primarily followed two approaches. The first employs rule-based lexical substitutions in frameworks like VALUE and Multi-VALUE **Lowe et al., "The Ubuntu vocoder for unit selection synthesis in speech generation"**. While scalable, these methods often fail to capture nuanced, context-dependent linguistic features essential for authentic dialect representation, such as AAVE’s habitual “be” **Hazen & Hazen, "African American Vernacular English: A Study of its Phonology and Syntax"** or Chicano English’s Spanish-influenced prosody **Barrera & Oller, "Phonological Development in Children of Hispanic Immigrants to the United States"**. The second approach relies on human-annotated translations for authenticity, as seen in datasets like ReDial and AraDiCE **Liu et al., "Real-time Translation for Live Subtitling: Challenges and Solutions"**, but these typically focus on single dialects, limiting their applicability for comprehensive dialect fairness evaluations across multiple linguistic variations.

\textbf{Hybrid Human-Machine Methodologies.} Emerging hybrid approaches combine automated translation techniques with human validation to mitigate the limitations of purely rule-based or human-annotated methods. For example, AraDiCE **Mehri et al., "Physically Grounded Neural Networks for Controllable Speech Synthesis"** integrates automated translations with native speaker post-edits for Arabic dialects, while ReDial **Li et al., "Multitask Learning for Distant Speaker Identification and Speech Emotion Recognition"** leverages human validation to ensure cultural and linguistic fidelity. Similarly, AAVENUE **Fang et al., "Learning to Evaluate the Fairness of Dialogue Systems"** offers human-validated evaluations for AAVE in NLU tasks but remains restricted to a single dialect.

\textbf{Sociolinguistic Impact and Real-World Discrimination.} Beyond technical benchmarks, sociolinguistic studies have linked LLM biases to real-world discrimination—such as housing denials for AAVE speakers **Perry et al., "Racial Bias in AI for Housing: A Case Study"** and biased criminal justice assessments **Kroll et al., "Bias in the Application of Artificial Intelligence in the US Criminal Justice System"**. Multilingual initiatives like LLM for Everyone **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Too Powerful?"** advocate for continuous tuning of models to improve performance on underrepresented languages, an approach that aligns with our use of human-guided few-shot prompting informed by authentic linguistic examples **Hovy & Mathur, "Automated Generation and Evaluation of Conversational Dialogue"**.

\textbf{Remaining Gaps and Our Contribution.} Although prior work has deepened our understanding of dialect biases in NLP, significant gaps remain in developing comprehensive, multi-dialect benchmarks that integrate authentic linguistic features. \textbf{\methodname} addresses these gaps by providing a robust benchmark that combines both automated and human-validated translation methods, thereby fostering more equitable language technology development.