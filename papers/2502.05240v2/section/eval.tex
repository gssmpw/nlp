
\input{Table/datasets-1}
\section{Evaluation Methods and Benchmarks}
\label{sec:val}
Evaluation methods are crucial for providing a standardized framework to compare and assess various detection techniques. In this section, we first review existing evaluation datasets relevant to AI-generated media detection scenarios, followed by an overview of open-ended evaluation methods and metrics.

\subsection{Evaluation Datasets}
With the improvement in detection accuracy and the introduction of various AI legislation, detection tasks are no longer limited to binary classification tasks. Therefore, this section will focus on datasets containing AI-generated data. We select some representative and newest datasets, particularly those used for evaluating the interpretability of MLLMs and identifying forged regions or segments. Authentic methods benchmarked on real datasets, such as FFHQ~\cite{karras2019style}, ImageNet, and COCO~\cite{wu2023sepmark} are not discussed in this section.

\subsubsection{\textbf{Text}}
\textit{Binary classification} is a well-established design in the MGT benchmark. The target of the binary classification task is to ensure the provided text whether generated by AI. 
\begin{itemize}
    \item \textbf{HC3}~\cite{guo2023close} contains 40k questions and their corresponding answers from human experts and ChatGPT, covering a wide range of domains (open-domain, computer science, finance, medicine, law, and psychology). The HC3 dataset is a valuable resource for analyzing the linguistic and stylist characteristics of both humans and ChatGPT.
\end{itemize}


\textit{Localization} focuses on understanding how varying levels of involvement of LLMs affect the behavior of MGT detectors, specifically in identifying which parts of a text are AI-generated. These datasets and benchmarks include a mixture of HWT, MGT, and LLMs acting as polishers or extenders, manipulating sentences or phrases.
\begin{itemize}
    \item \textbf{Mage}~\cite{li2024mage} collects human-written texts from 7 distinct writing tasks (e.g., story generation, news writing, and scientific writing) and generates corresponding machine-generated texts with 27 LLMs (e.g., ChatGPT, LLaMA, and Bloom) under 3 representative prompt types. It categorizes the data into 8 testbeds, each exhibiting progressively higher levels of “wildness” in terms of distributional variance and detection complexity.
    \item \textbf{MIXSET}~\cite{zhang2024llm}  is the first dataset comprises a total of 3.6k mixtext instances and aims at the mixture of HWT and MGT, including both AI-revised HWT and human-revised MGT scenarios. 
    \item \textbf{Beemo}~\cite{artemova2024beemo} is a multi-author benchmark of LLM-generated \& expert-edited responses for fine-grained MGT detection, which counts 19.6k texts in total.
\end{itemize}
\subsubsection{\textbf{Image}}
\textit{Binary classification} of AI-generated image is also well established. There are many generator models, like Stable Diffusion, DALL-E2, and Midjourney. We select two main benchmarks to introduce.
\begin{itemize}
    \item\textbf{GenImage}~\cite{zhu2024genimage} comprises 2,681,167 images, segregated into 1,331,167 real and 1,350,000 fake images. The real images are subdivided into 1,281,167 images for training and 50,000 for testing.
    \item \textbf{Fake2M}~\cite{lu2024seeing} collects AI-generated images and a set of real photographs across eight categories: Multiperson, Landscape, Man, Woman, Record, Plant, Animal, and Object. It uses Midjourney-V5 to construct the aforementioned eight categories and collect real photos by searching for photos with the same text prompts used for creating AI-generated images in the previous paragraph.
\end{itemize}

    
\textit{Expalinablilty and localization} of AI-generated images are primarily addressed through two methods, as discussed in Sections \ref{sec:mllm} and \ref{sec:non-mllm}. Explainability tasks mainly use MLLMs for detection, reasoning, and fine-grained forgery analysis, providing an explanation for why the model classifies an image as real or fake. Localization tasks, on the other hand, focus on identifying the forged regions in the image and outputting the corresponding reasoning for the forgery detection.
\begin{itemize}
    \item \textbf{FakeBench}~\cite{li2024fakebench} examines LMMs with four evaluation criteria: detection, reasoning, interpretation, and fine-grained forgery analysis, to obtain deeper insights into image authenticity-relevant capabilities
    \item \textbf{SID-Set}~\cite{huang2024sida} 
    consists of 300k images (100k real, 100k synthetic, and 100k tampered images)with comprehensive annotations.
    
\end{itemize}


\subsubsection{\textbf{Video}}
\textit{Binary classification} of AI-generated video is still establishing. public video generation tools, including Stable Video Diffusion~\cite{ho2022video}, Pika~\cite{Pikalabs2024}, Gen-2~\cite{gen22024}, SORA~\cite{brooks2024video}. The majority of methods for detecting AI-generated videos focus on detecting frame-level forgeries and rely on image-level datasets.

\begin{itemize}
\item \textbf{GenVideo}~\cite{chen2024demamba} includes 1,078,838 generated videos and 1,223,511 real videos. The fake videos are a mix of those generated in-house and those collected from the internet, while the real videos are sourced from the Youku-mPLUG~\cite{xu2023youku}, Kinetics400~\cite{kay2017kinetics}, and MSR-VTT~\cite{xu2016msr} datasets. The dataset covers a wide range of content and motion variations.
\end{itemize}


\textit{Explainbility and localization} in AI-generated videos leverage the capabilities of Video-LMMs to provide human-readable text outputs and identify which frames or time periods are generated by AI.
\begin{itemize}

    \item \textbf{VANE}~\cite{bharadwaj2024vane} aims to evaluate the proficiency of Video-LMM in detecting and locating video anomalies and inconsistencies. It consists of 325 video clips and 559 challenging question-answer pairs from real-world video surveillance and AI-generated videos.
\end{itemize}

\subsubsection{\textbf{Audio}}
The AI-generated audio datasets are key tools for evaluating AI-generated audio detection techniques, most of which focus on \textit{binary classification} and attribution tasks. These datasets typically include audio samples generated through various models, such as Text-to-Speech, Voice Conversion, Text-to-Music, and deepfake models, covering real-world scenarios and supporting multiple languages.
\begin{itemize}
    \item \textbf{SONAR}~\cite{li2024sonar} encompasses a total of 2274 AI-synthesized audio samples produced by various TTS models and only includes fake audio samples in this dataset.
    \item \textbf{FakeMusicCaps}~\cite{comanducci2024fakemusiccaps} consists of 27,605 music tracks, totaling nearly 77 hours of content. Each track is converted to mono and downsampled to a sampling rate of 16 kHz. The dataset also includes multiple versions of the MusicCaps~\cite{agostinelli2023musiclm} dataset, which is re-generated using several state-of-the-art Text-to-Music techniques.
    \item \textbf{VoiceWukong}~\cite{yan2024voicewukong} includes 265,200 English and 148,200 Chinese deepfake voice samples, generating 38 data variants across six types of manipulations, forming an evaluation dataset for deepfake voice detection.
\end{itemize}

\subsubsection{\textbf{Multimodal}}
AI-generated multimodal content includes video, image, text, and audio modalities. However, there is currently only one dataset that encompasses both authentic detection and human-readable explainability, as well as the localization of forgery regions in images for MLLMs.
\begin{itemize}
    \item \textbf{LOKI}~\cite{ye2024loki} encompasses video, image, 3D, text, and audio modalities, consisting of 13k carefully curated questions across 28 subcategories with clearly defined difficulty levels. It includes coarse-grained true/false questions, in-domain multiple-choice questions, and fine-grained anomaly explanation questions, effectively evaluating models in synthetic data detection and reasoning explanation.
\end{itemize}

\subsection{Evaluation Metrics}
In this section, we introduce two primary categories of evaluation metrics: Close-Ended Metrics and Open-Ended Metrics. Detection is typically a classification task, where forgery detection performs media-level binary classification and fine-grained forgery detection conducts fine-grained classification. Therefore, most detection evaluation metrics are standard evaluation metrics commonly used in machine learning. However, tasks based on MLLMs not only rely on standard evaluation metrics but also need outputs of the MLLMs as evaluation metrics named MLLM-Aided metrics.
\subsubsection{Close-Ended Metrics}
\begin{itemize}
    \item \textbf{Accuracy(ACC)}\cite{bharadwaj2024vane,li2024mvbench,xu2023youku}: Accuracy measures the proportion of correctly classified instances (true positives and true negatives) out of the total number of instances, which is widely used in classification tasks like multiple-choice QA, image recognition and so on. The formulation is shown as follows:
        \[Accuracy = \frac{True \medspace Positives + True \medspace Negatives}{Total \medspace Samples}\]
    \item \textbf{Area Under the curve(AUC)}\cite{guo2024language,koutlis2024dimodif,zhang2024personamark}: AUC provides a single scalar to summarize the model's performance, which is particularly useful in scenarios where the distribution of classes is imbalanced, as it is not sensitive to the class distribution, making it a robust metric for model evaluation.
    \item \textbf{mean Average Precision (mAP)}\cite{cai2024av,koutlis2024dimodif}: mAP is generally used to measure Average Precision (AP) across all classes or categories. AP evaluates the precision-recall trade-off for a given class by calculating the area under the precision-recall curve. It is widely used in tasks like object detection to assess the quality of predictions in terms of both localization and classification.
    \item \textbf{Equal Error Rate (EER)}\cite{yan2024generalizing,li2024sonar,yu2024diffforensics}: EER is the point on the ROC curve that corresponds to having an equal probability of misclassifying a positive or negative sample. It is particularly relevant in scenarios where the goal is to evaluate the system's ability to correctly identify individuals.
    \item \textbf{F1 Score}\cite{li2024salad,fagni2021tweepfake,ji2024detecting,cheng2024beyond}: F1 score strikes a balance between Precision and Recall, offering an all-encompassing assessment of performance, which is especially valuable for binary classification tasks. Precision shows the proportion of true positives among predicted positives while Recall among actual positives.F1 score is defined as:
         \[F1 \medspace Score = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}\]
\end{itemize}

\subsubsection{Open-Ended Metrics}
\begin{itemize}
    \item \textbf{Scoring}: Scoring is a widely used method to tackle open tasks. Under this method, a model or human evaluators provide scores according to specified criteria. For example, LOKI \cite{ye2024loki} utilizes the GPT-4 model to assess response scores, implementing a 5-point scale from 1 (poor) to 5 (excellent) with the scoring criteria of Identification, Explanation, and Plausibility. Moreover, DREAMBENCH++ \cite{peng2024dreambench++} engages human evaluators to assess each sample's concept preservation and prompt following in it, aiming to gather authentic human preference data. Furthermore, Mllm-as-a-judge \cite{chen2024mllm} shows the comparative performance of MLLMs' vision-language judging ability according to human annotators, focusing on human agreement, analysis grading, and hallucination detection with a score from 1 to 5.
    \item \textbf{Comparison}: In contrast to scoring, direct comparison involves aligning the results of the assessed model with the results from sophisticated models or expert knowledge. This technique is frequently regarded as more straightforward and reliable than scoring. By using the Comparison metric in QA tasks, we can evaluate whether the model's output options and the correct answers are generated by AI. For instance, Guo\cite{guo2023close} invites volunteers to point out the AI-generated answers in a series of tests which consist of a question and a random response provided by either humans or ChatGPT. The result shows that expert testers who frequently use ChatGPT can identify the text results generated by ChatGPT more easily than those who have never used it. Moreover, in SNIFFER \cite{qi2024sniffer}, the author asks ten participants to evaluate the authenticity of each news piece (distinguishing between fake and real) and indicate their level of confidence (ranging from none to high) before and after considering SNIFFER's clarifications.
\end{itemize}