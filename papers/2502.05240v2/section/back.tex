\section{Background}
\label{sec:back}

\subsection{Generative Approaches for Different Modalities}
This section examines the various types of content generated by generative models, including text, image, video, audio, and multimodal content, along with the methods used in each domain.
% \begin{figure*}[!ht]
%   \centering
%     \includegraphics[width=1.0\linewidth]{Fig/illustration.pdf}
%     \caption{Illustrating different multimedia generation processes using large AI models}
%     \label{fig:illustration}
% \end{figure*}

\textbf{Text:} 
%介绍常用模型+multimodal的时候作为prompt
In AI-generated media, text generation is primarily achieved using Large Language Models (LLMs) like GPT-4o~\cite{openai2024}, LLaMA3~\cite{dubey2024llama}, and Claude 3.5 Sonnet~\cite{anthropic2023claude3}. These models leverage vast datasets to perform complex language tasks, including news creation~\cite{kreps2022all}, code generation~\cite{idrisov2024program}, and script drafting~\cite{buschek2024collage}.
Furthermore, text serves as a foundational input for generating other modalities. For instance, in text-to-image generation, models translate descriptive text prompts into corresponding visual content, bridging the gap between textual descriptions and visual representations. 

\textbf{Image:} 
%Text-guide Image Generation with Diffusion Models // LLMs
In the past two years, research powered by MLLMs has increasingly focused on achieving a more intuitive and interactive image generation process. As their foundation, diffusion models (DMs) are the dominant technology in image generation. Current research on diffusion models primarily revolves around three key formulations: denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising}, score-based generative models (SGMs)~\cite{song2019generative}, and stochastic differential equations (SDEs)~\cite{song2020score}. More advanced models guided by text have also emerged, including Stable Diffusion V2~\cite{rombach2022high}, Google Imagen2~\cite{deepmind2023imagen2}, and Midjourney~\cite{midjourney2023}. Notably, DALL·E 3~\cite{betker2023improving}, which integrates with GPT-4 and leverages the powerful text understanding capabilities of GPT-4. GPT-4 first processes and interprets the text, generating a structured semantic representation that is then used by DALL·E 3 for image generation. Users can interact with GPT-4 to modify aspects of the generated image, such as colors, styles, elements, or details. Additionally, MLLMs play a crucial role in image generation by unifying textual and visual modalities to create more dynamic outputs. Important examples include MiniGPT-4~\cite{zhu2023minigpt}, LLaVA~\cite{liu2024visual}, and Qwen-VL~\cite{bai2023qwen}.
%写一段生成的特点

\textbf{Video:} 
%Text-guide video Generation with Diffusion Models // LLMs
Intuitively, a video is an expansion of a series of images over time. Recently, DMs have emerged as the leading framework for Text-to-Video (TTV) generation. Within the DMs framework, there are two main categories: (1) frame-wise DMs and (2) spatio-temporal diffusion models. Frame-wise DMs, such as Meta’s Make-A-Video~\cite{singer2022make}, and DALL·E 2~\cite{openai2023dalle2} (\textit{when adapted for video}), apply the diffusion process to each individual frame. However, they must carefully address challenges related to maintaining consistency and smooth transitions between consecutive frames to avoid flickering or object deformation. On the other hand, spatio-temporal DMs, like SORA~\cite{brooks2024video}, Google DeepMind’s Veo~\cite{deepmind2023veo}, and Stable Video Diffusion~\cite{blattmann2023stable}, focus on capturing both spatial and temporal coherence across the entire video sequence. Additionally, similar to the previously introduced Image MLLMs, Video MLLMs also leverage the exceptional comprehension capabilities of LLMs to enhance video realism. Recent successful examples, such as LLaMA-VID~\cite{li2025llama} and VideoChat2~\cite{li2024mvbench}, through extensive use of diverse multi-modal data, including text, image, and video, and multi-stage alignment training, have achieved improved video understanding based on LLMs.

\textbf{Audio:}  
Most deep learning-based speech synthesis systems typically consist of two main components: (1) a Text-to-Speech (TTS) model that converts text into an acoustic feature, such as a mel-spectrogram, and (2) a vocoder that generates a time-domain speech waveform from this acoustic feature. Notably, DDPMs~\cite{ho2020denoising} can also be applied to audio generation~\cite{jeong2021diff}. Jeong et al. were the first to apply DDPMs for mel-spectrogram generation, where noise is transformed into a mel-spectrogram through diffusion time steps. Models like AudioLDM~\cite{liu2023audioldm}, Make-An-Audio~\cite{huang2023make}, and TANGO~\cite{ghosal2023text} all leverage the Latent Diffusion Model (LDM). Particularly, TANGO~\cite{ghosal2023text} uses LLMs as a frozen, instruction-tuned text encoder to provide strong text representation capabilities. Meanwhile, WavJourney~\cite{liu2023wavjourney} focuses on generating structured scripts and enabling user interaction for storytelling audio creation, UniAudio~\cite{yang2023uniaudio} emphasizes tokenization and sequence processing for various audio types, aiming to build a robust, adaptable universal audio generation model. The growing use of LLMs in audio generation—whether as conditioners for specific tasks~\cite{ghosal2023text}, sources of inspiration~\cite{yang2023uniaudio}, or interactive agents~\cite{liu2023wavjourney}—is transforming how we interact with sound and music.

\textbf{Multimodal:}
Multimodal generation represents the culmination of advancements across individual modalities, integrating text, image, video, and audio into cohesive and context-aware outputs. For example, 
Text-to-Image (TTI)~\cite{rombach2022high, deepmind2023imagen2, midjourney2023, bai2023qwen, liu2024visual, zhu2023minigpt}, Text-to-Video (TTV)~\cite{singer2022make,openai2023dalle2,brooks2024video, deepmind2023veo, blattmann2023stable} and Text-to-Speech (TTS)~\cite{ghosal2023text, liu2023wavjourney, yang2023uniaudio} tasks are multimodal systems that extend text-only generation by using textual prompts to guide visual content generation. Multimodal generation acts as an integrative framework, combining the specialized capabilities of single-modal systems to achieve a holistic understanding of content.


\subsection{Definition and Formulation}
%介绍一下有些有可解释性
\begin{enumerate}
    \item \textbf{Authenticity Detection}:
    Authenticity detection is a binary classification task that determines whether a given piece of media $X$ is authentic or AI-generated. Formally, the task is defined as:
$D = \{(X_i, Y_i)\}_{i=1}^N$
where $X_i$ represents the media sample (\textit{e.g., an image, video, or text}), and $Y_i\in \{real, fake\}$ indicates its authenticity. The detection model $F_\theta$, parameterized by $\theta$, maps input data to authenticity labels:
$F_\theta : X \rightarrow \{real, fake\}$.
The training objective is to optimize $\theta$ by minimizing a predefined loss function:
    \begin{equation}\label{eqn-1}
    \theta = \arg \min_{\theta} \frac{1}{N} \sum_{i=1}^{N} \text{Loss}(X_i, Y_i, \theta)
    \end{equation}
Extensions of this task may involve embedding watermarks during or after the generation process for post-verification, supporting forgery authentication, and copyright protection.


    \item \textbf{Explainability}:
    Explainability aims to provide human-interpretable reasoning for detection decisions, typically presented as natural language explanations or visual representations of salient features~\cite{dang2024explainable, zhao2024explainability}. The task can be further categorized into three levels: direct explanation (\textit{direct identification of forgery clues with few-shot in-context examples}), reasoning-based explanation (\textit{multi-hop reasoning and logical consistency evaluation}), and free-form fine-grained analysis (\textit{fine-grained analysis of forgery aspects, aligned with a predefined taxonomy of forgery cues}). For a given input $X$, generate an explanation $E$ that: (1) identifies relevant forgery clues $\mathcal{C} = \{c_1, c_2, \dots, c_k\}$; and (2) supports multi-layer forgery analysis (\textit{low-level, mid-level, high-level}). Formally, the task is defined as: 
    \begin{equation}\label{eqn-2}
    g(f(X; \theta), X; \phi) = E, \\
    \mathcal{L}_{\text{exp}} = \text{KL}(p(E \mid X, Y) \| q(C))
    \end{equation}
    where $p(E \mid X, Y)$ is the generated explanation distribution, and $f(X; \theta)$ is the detection model output.
   

    \item \textbf{Localization}:
    Forgery localization identifies specific regions or segments within the input that are manipulated or generated. This task is commonly framed as: Pixel-wise classification (\textit{for images, this involves predicting a forgery heatmap where each pixel indicates the likelihood of forgery}); Segment-wise classification (\textit{for videos, this extends to identifying forged regions across multiple frames with temporal consistency}); Bounding box detection (\textit{for coarse localization, bounding boxes can be used to enclose suspected forged regions}). Given an input $X\in \mathbb{R}^{H \times W \times C}$ (\textit{e.g., an image}), the localization model $h$, parameterized by $psi$, outputs one or more of the following: A forgery heatmap: $M \in [0, 1]^{H \times W} $, where $M(i, j)$ indicates the likelihood of forgery at pixel $(i, j)$. A binary mask: $\hat{M} \in \{0, 1\}^{H \times W} $, derived by applying a threshold $\tau$ to the heatmap. A set of bounding boxes: $ B=\{b_1, b_2, \dots, b_k\} $, where each $b_i=[x_{\text{min}}, y_{\text{min}}, x_{\text{max}}, y_{\text{max}}]$ specifies the coordinates of a forged region. The model can be represented as: 
    \begin{equation}\label{eqn-3}
        \begin{aligned}
        h(X; \psi) &= \{ \hat{M}, M, B \}
        \end{aligned}
    \end{equation}
        where $M \in [0, 1]^{H \times W}$, $\hat{M} \in \{0, 1\}^{H \times W}$, $B \in \mathbb{R}^{k \times 4}$.
\end{enumerate}





