\section{Regulation}
\label{sec:reg}

\input{Table/regulations}
In recent years, the rapid development of GenAI technologies has not only driven technological innovation and industrial advancement but also raised societal concerns, including the spread of misinformation, data privacy breaches, and ethical controversies. The rapid dissemination and difficult-to-monitor nature of AI-generated media have prompted governments and research institutions worldwide to focus on effectively regulating the applications and potential impacts of generative AI. Against this backdrop, we examine AI-generated media detection policies from four perspectives~\cite{shi2024large}: risk management frameworks, transparency requirements, technical neutrality, and industry participation. Risk management frameworks~\cite{novelli2024taking, zeng2024ai} evaluate how different countries identify, classify, and mitigate the potential risks of AI systems through policy and technical measures. Transparency requirements examine the implementation of policies on data source disclosure, algorithm transparency, and external audits.
The technical neutrality perspective explores whether AI regulations are enforced in a technology-neutral manner to avoid stifling innovation and industrial growth.
Industry participation analyzes the depth and breadth of collaboration between governments and enterprises in AI-generated media detection, including the interplay of legal mandates and voluntary contributions.
Analyzing these dimensions reveals differences in governance priorities across nations while providing valuable insights for researchers and policymakers to foster global collaboration and advancement in AI-generated media detection.

In 2024, the European Union (EU) passed the world’s first comprehensive artificial intelligence regulation, the Artificial Intelligence Act (AIA)~\cite{ArtificialIntelligenceAct}. It adopts a risk-based tiered regulatory approach, categorizing AI systems into four levels: minimal risk, limited risk, high risk, and unacceptable risk. Generative AI systems are generally classified as limited risk, requiring basic transparency obligations. The United States (US) emphasizes technical neutrality and industry self-regulation. The National Institute of Standards and Technology (NIST) introduced the AI Risk Management Framework (AI RMF) to guide developers in identifying and mitigating risks. Meanwhile, several legislative initiatives, such as the No AI Fraud Act and the COPIED Act, aim to protect intellectual property and combat deepfakes. China~\cite{ChinaAIGovernance2023} focuses on safety controls and ethical use within its governance framework. Policies like the Generative AI Service Management Provisions adopt an inclusive, risk-sensitive classification and grading approach, encouraging AI integration into national governance. A detailed comparison is presented in Table~\ref{tab:ai_governance}.

Looking ahead, global AI governance must balance innovation with regulation. Combining the EU’s tiered framework, the US’s technical neutrality and self-regulation model, and China’s classification-based oversight can promote multilateral collaboration and standardization. Policies should strengthen the integration of technology and ethics, enhancing governance flexibility and responsiveness. Industry stakeholders should actively participate in policy formulation, leveraging dynamic monitoring and transparency requirements to ensure AI safety and social responsibility, achieving a win-win for innovation and compliance.




%当前AI风险的定义与分类，指出风险的复杂性。随着人工智能技术的进步，与其部署相关风险也在增加，因此有必要对主要监管机构采取不同方法进行比较。因此我们探讨了美国、欧盟和中国采取的政策的区别。AIA是全球第一部全面的人工智能法案，采用基于风险的方法将人工智能风险分为四类，相比之下，美国的ai监管技术以技术中立和行业自律为主。中国实施了强调安全控制和道德使用的ai治理框架，鼓励人工智能融入国家治理体系，并对风险进行了列举。事实上，现在还没有。人工智能的治理不仅需要各国的努力还需要全球协调应对措施，以在促进创新的同时降低风险。正如越来越多的学者所强调的，单一静态分散的法规是不够管理复杂全球人工智能生态系统。事实上，现在急需一个全球人工智能风险分类法对ai risk进行基于具体场景的分类和集成式的风险评估模型，将AI风险管理从静态分类转向动态情境评估。因为随着通用人工智能GPAI的多功能性和应用的不可预测性，并且静态的风险评估方法可能低估或高估某些实际风险，使得传统的基于应用领域的风险评估框架难以有效使用。需要强调的是，不仅是需要明确的法律法规对AIrisk进行监管，还需要对应的AI监管技术以确保人工智能系统在整个生命周期中安全、负责和透明，并遵守道德和安全标准。AI法案和监管技术在发展的同时还需要满足以人为本、可持续性的人工智能治理模型。
