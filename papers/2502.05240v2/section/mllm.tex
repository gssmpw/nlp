\section{MLLM-based Detector}
\label{sec:mllm}
\begin{figure*}[!ht]
  \centering
    \includegraphics[width=1.0\linewidth]{Fig/MLLM-text.pdf}
    \caption{Illustrating of MLLM-based detection methodologies for AI-generated text}
    \label{fig:MLLM-text}
\end{figure*}
This paper primarily focuses on MLLM-based methods for detecting AI-generated media. Therefore, we first introduce relevant MLLM-based approaches. Before diving into these methods, it is worth noting that previous works~\cite{lin2024detecting, deng2024survey, yu2024fake} have reviewed some Non-MLLM-based methods.

As a product of advancements in Natural Language Processing (NLP) and Computer Vision (CV), MLLMs represent a significant milestone in AI. Compared to traditional Non-MLLM detection methods, MLLMs leverage their multimodal nature and reasoning abilities to offer several distinct advantages. First, their human-like cognitive abilities, enabled by Chain-of-Thought (CoT) and In-Context Learning (ICL), allow MLLMs not only to detect potential forgery traces in AI-generated media but also to reason about and explain their decision-making processes. Additionally, textual input and output empower MLLMs to support flexible query formats and provide human-interpretable contextual explanations. In terms of forgery analysis potential, MLLMs excel at identifying and describing visual forgery cues, conducting adaptive analyses driven by textual prompts, and validating authenticity through causal reasoning. These capabilities make MLLMs highly effective in supporting forgery detection in AI-generated media, particularly in identifying and describing forgery traces, performing flexible, text-driven analyses, and verifying authenticity through causal reasoning. In contrast, traditional Non-MLLM detection methods primarily focus on single-modal feature extraction and classification, often lacking interpretability and causal analysis capabilities. By addressing these limitations, MLLMs demonstrate their effectiveness in supporting AI-generated media detection. In the following sections, we will analyze the underlying technologies and methodologies in detail.
\subsection{Text}
 % 替换为你的图像文件
%\input{Table/attack_taxonomy}
\subsubsection{\textbf{Authenticity}}
MLLMs can be used in judgment of the authenticity of AI-generated text. The methods can be divided into five types: Statistical-based methods, Prompt-engineering, Self-consistency, Multi-Author, and Watermarking, all of which leverage the capability of MLLMs, as shown in Fig.~\ref{fig:MLLM-text} (a).
% \zekun{Statistical-based methods, prompt engineering, and self-consistency seem to be methods, whereas multi-author and watermarking appear to be issues. Placing all five at the same level might not be accurate.}
\begin{itemize}
\item \textbf{Statistical-based}
By examining statistical differences in language use, such as probability distributions or specific features, zero-shot methods can distinguish human writing from GPT-generated text, leveraging both shallow and deep characteristics. For shallow features, HowkGPT~\cite{vasilatos2023howkgpt} computes perplexity scores, establishing thresholds to distinguish their origins. 
DNA-GPT~\cite{yang2023dna} uses N-gram analysis or probability divergence. In the context of deep features, DetectLLM~\cite{su2023detectllm} introduces two methods DetectLLM-LRR and DetectLLM-NRR both leveraging log-rank information. DetectLLM-NRR focuses on accuracy with fewer perturbations, while DetectLLM-LRR emphasizes speed and efficiency. DetectGPT~\cite{mitchell2023detectgpt} leverages the negative curvature regions of the model's log probability function, without requiring additional training. Subsequently, Fast-DetectGPT~\cite{bao2023fast} introduces the concept of conditional probability curvature, which improves upon DetectGPT by replacing the computationally intensive perturbation step with a faster sampling step.

\item \textbf{Prompt Engineering}
Some researchers leverage MLLMs to detect In the LOKI study ~\cite{ye2024loki}, results show that MLLMs achieve only 61.5\% accuracy in judgment tasks asking, `Is the provided text generated by AI?'. However, accuracy increases to 89.2\% when the task is reformulated into a multiple-choice format, such as `Which of the following text is generated?'. The improvement stems from MLLMs' strength in contrastive analysis, as binary choice tasks allow direct comparison of subtle differences, unlike isolated judgment tasks relying solely on internal feature detection. Bhattacharjee et al.~\cite{bhattacharjee2024fighting} find that even though ChatGPT struggles to detect AI-generated text, it performs well in identifying human-written text. Zhang et al.~\cite{zhang2024detection} design various prompts, such as Base task-specific prompts, Style-specific prompts, and Evasion-optimized prompts to show the vulnerability of detectors.

\item \textbf{Self-consistency}
The self-consistency hypothesis suggests that, within a given input context, machine-generated text tends to make more predictable choices in words or tokens compared to humans. DetectGPT-SC~\cite{wang2023detectgpt} masks a portion of the input text and uses LLM to predict the masked words or tokens. It measures the consistency between the predictions and the original text to determine whether the text was generated by the LLMs. Additionally, numerous studies~\cite{nguyen2024simllm,zhu2023beat,mao2024raidar, hao2024learning} focus on utilizing LLMs to revise or rewrite sentences or phrases and then calculate the similarity between the original and the rewritten versions. SimLLM~\cite{nguyen2024simllm} uses candidate LLMs to proofread an input text, generating multiple versions and comparing their similarity to the original text to determine if the text was generated by an LLM. Zhu et al.~\cite{zhu2023beat} use ChatGPT to revise and analyze the similarity. Moreover, Raidar~\cite{mao2024raidar} prompts LLMs to rewrite the text, calculate the editing distance of the output, and exhibit high robustness in new content and multi-domain applications. Rewritelearning~\cite{hao2024learning} trains an LLM to rewrite input text, minimizing edits for AI-generated media while applying more edits to human-written text.


\item \textbf{Multi-Author}
Multi-Author core idea is to distinguish different authors (\textit{varying degrees of LLM intervention, e.g., partly written by AI, polished by AI}) rather than simply classify text as human-written or AI-generated. 
MIXSET~\cite{zhang2024llm} is the first dataset comprising human-written, machine-generated, and
human/LLM-refined machine-generated texts (MGTs) and focuses on multi-author binary classification. From then on, LLM-DetectAIve~\cite{abassy2024llm} provides a four-way classification task with the addition of three labels: ``human-written/machine-written", ``machine-written, then machine-humanized", ``human-written, then machine-polished". Beemo~\cite{artemova2024beemo} is a benchmark designed to evaluate AI-generated text detection in multi-author scenarios. LLMDetect~\cite{cheng2024beyond} introduces two tasks: LLM Role Recognition (LLM-RR) for multi-class classification and LLM Influence Measurement (LLM-IM) for quantifying LLM involvement, showing fine-tuned PLM-based models outperform advanced LLMs in detecting their outputs. 

\item \textbf{Watermarking}
To watermark LLMs, Kirchenbauer et al.~\cite{kirchenbauer2023watermark, kirchenbauerreliability} propose a method involving inserting signatures during the decoding stage. These methods categorize the vocabulary into ``red" and ``green" lists, restricting the LLM to decoding tokens from the green list. Subsequently, Christ et al.~\cite{christ2024undetectable} and Unigram-Watermark~\cite{zhaoprovable} suggest various algorithms for splitting the red and green lists or sampling tokens from the green list's probabilistic distribution to enhance the interpretability and robustness of watermarking mechanisms during the inference process. PersonaMark~\cite{zhang2024personamark} is a personalized text watermarking method that leverages sentence structure and user-specific hashing. By embedding unique watermarks, it guarantees copyright protection and user tracking of generated text while maintaining the text's naturalness and generation quality.
\end{itemize}
\input{Table/fig4}

\subsubsection{\textbf{Explainability}}
Traditionally, detecting LLM-generated text is often framed as a binary classification task. Methods are shown in Fig.~\ref{fig:MLLM-text} (b). However, there is also an ``undecided" category~\cite{ji2024detecting}, which is used to represent ambiguous texts that may originate from either humans or AI. This category is crucial for enhancing the explainability of detection results. By incorporating it, the system not only improves its reliability but also allows ordinary users to better understand the detection outcomes. Ji et al.~\cite{ji2024detecting} construct a dataset containing LLMs-generated text and human-generated text. Three human annotators are tasked with producing ternary labels along with explanation notes. They identify eight categories of explanations provided by human annotators, including spelling errors, grammatical errors, perplexity, logical errors, and unnecessary repetition.


\subsubsection{\textbf{Localization}}
Methods of localization are shown in Fig.~\ref{fig:MLLM-text} (a).
Gruda et al.~\cite{gruda2024three} have proposed three ways that ChatGPT can assist in academic writing. Similar to ``Multi-Author", LLMs play different roles based on varying user needs, from creating and drafting to polishing. The text totally written by AI is easier to detect than human-collaborated text. Some researchers quantify the involvement ratio of LLMs in content creation and localize which part of a phrase is written by AI.  LLMDetect~\cite{cheng2024beyond} offers an involvement ratio strategy. GigaCheck~\cite{tolstykh2024gigacheck} combines fine-tuned general-purpose LLMs to distinguish human-written texts from LLM-generated texts. Additionally, it employs a DETR-like model to localize AI-generated intervals in human-machine collaborative texts.

\begin{figure*}[!ht]
  \centering
    \includegraphics[width=1.0\linewidth]{Fig/MLLM-Image.pdf}
    \caption{Illustrating of MLLM-based detection methodologies for AI-generated images. ``Mask + Image → Text" approach is reproduced from~\cite{li2024forgerygpt}, ``Text + Image → Mask" approach is reproduced from~\cite{huang2024sida}, and Independent Mask Localization method is adapted from~\cite{lian2024large}}
    \label{fig:MLLM-image}
\end{figure*}

\subsection{Image}
\subsubsection{\textbf{Authenticity}}
For assessing image authenticity using MLLMs, we divide the approach into three categories: Prompt engineering, Fine-tuning, and Integration with external detectors, as shown in Fig.~\ref{fig:MLLM-image} (a).
\begin{itemize}
\item \textbf{Prompt-engineering}
Prompt engineering can be categorized into four types: Judgment prompts, Multiple-choice prompts, Score prompts, and In-context prompts. 
For \textbf{Judgment prompts}, the model is directly queried with questions (\textit{e.g., `Is the provided image generated by AI?'}~\cite{ye2024loki} \textit{, `Is this an example of a real image?'}~\cite{shi2024shield, huang2024visualcritic}). However, variations in phrasing, such as replacing ``real" with ``bonafide" or ``spoof"~\cite{shi2024shield}. LOKI~\cite{ye2024loki} shows that MLLMs may not be good at judging whether the input image is generated by AI. Mantis-8B shows the best performance only achieving 54.6\% accuracy, compared to 80.1\% for human evaluators. Nevertheless, Jia et al.~\cite{jia2024can} suggest that guiding MLLMs to focus on regions of an image likely to contain forgery clues (\textit{e.g., `Analyze the eye area'}) can enhance detection effectiveness. About \textbf{Multiple-choice prompts}, it gives MLLMs some choice (\textit{e.g., `Which of the following image is the generated image?'}~\cite{ye2024loki}). LOKI shows that MLLMs perform better in multiple-choice tasks compared to judgment tasks. GPT-4o achieves the best results, with an overall accuracy of 80.8\%, which is close to the human accuracy of 84.5\%. 
Also for \textbf{Score prompts}, MLLMs are tasked with providing a probability score for their judgments. Jia et al.~\cite{jia2024can} observe that such requests result in a 100\% rejection rate by GPT-4V.
In addition, \textbf{In-context prompts}, also referred to as one-shot questions, MLLMs are provided with examples to guide their detection (\textit{eg., The first set of images is of a real face, is the second set of images a real
face or a spoof face? Please answer `this image is a real face'})~\cite{shi2024shield}. It shows that MLLMs may give more accurate answers. Prompt engineering enhances the performance of MLLMs in detecting AI-generated images through flexible prompt design. However, it is highly sensitive to the specific design choices, with task formats and phrasing significantly impacting effectiveness. Additionally, its robustness may be limited in complex scenarios, particularly when faced with diverse or shifting data distributions.

\item \textbf{Fine-tuning}
To improve the MLLMs’ detection capabilities, fine-tuning involves adjusting model parameters using targeted datasets. $\textit{X}^2$-DFD~\cite{chen2024textit} comprises three modules: Model Feature Assessment (MFA), Strong Feature Strengthening (SFS) and Weak Feature Supplementing (WFS). MFA evaluates and ranks forgery-related features, while SFS leverages the top-ranked features to create an explainable training dataset. This dataset is used to fine-tune the MLLM, enhancing both detection accuracy and explainability. Similarly, Fakeshield~\cite{xu2024fakeshield} includes two key components. The Domain Tagging-Enhanced Forgery Detection Module generates domain-specific tags (\textit{e.g., Photoshop, DeepFake, AIGC}) and integrates image features with instruction-based textual inputs to produce tampering detection results and explanations. Lightweight LoRA fine-tuning techniques are employed to improve detection efficiency and maintain strong explainability.


\item \textbf{External detectors}
From the experiment results of ~\cite{ye2024loki}, we can find that MLLMs are not good at directly judging whether the image is generated by AI. Researchers have proposed integrating MLLMs with external detectors to enhance their feature discrimination capabilities. For instance, $\textit{X}^2$-DFD~\cite{chen2024textit} evaluates forgery-related features and ranks them based on detection performance, utilizing external detectors (\textit{e.g., blending-based detectors}~\cite{lin2025fake}) to strengthen the handling of weak feature areas. These external prediction scores are then incorporated into the MLLMs. Additionally, FFAA~\cite{huang2024ffaa} introduces a multi-answer intelligent decision system, which combines a cross-modal fusion module and a classification module to identify the best answer that aligns with an image's authenticity. This integration significantly enhances the accuracy and reliability of detection.

\end{itemize}

\subsubsection{\textbf{Explainability}}
The explainability of MLLMs is a remarkable feature, and recent studies have increasingly explored its potential. The methods are illustrated in Fig.~\ref{fig:MLLM-image} (b). Some works~\cite{jia2024can,shi2024shield,lian2024large,huang2024sida} directly query MLLMs with prompts such as `explain what the artifacts are'. However, prior investigations~\cite{jia2024can, shi2024shield} reveal that directly generating textual explanations often leads to hallucinations or overthinking, producing inaccurate outcomes or refusal to respond. Moreover, MLLMs often struggle to comprehensively perceive all relevant features, limiting their effectiveness in explainability. To address these limitations, researchers have employed approaches such as fine-tuning MLLMs~\cite{chen2024textit, huang2024ffaa, xu2024fakeshield} or integrating external modules~\cite{sun2024forgerysleuth}. These approaches aim to establish a comprehensive evaluation framework by categorizing features into three levels: low-level pixel features (\textit{e.g., noise, color, texture, sharpness, and AI-generated fingerprints}), middle-level visual features (\textit{e.g., traces of tampered regions or boundaries, lighting inconsistencies, perspective relationships, and physical constraints}), and high-level semantic anomalies (\textit{e.g., content that contradicts common sense, incites, or misleads}). This multi-level feature evaluation provides a holistic approach to enhancing the detection capabilities and explainability of MLLMs.


\subsubsection{\textbf{Localization}}
Binary classification tasks in forgery detection cannot inherently provide detailed insights into tampered regions. This limitation becomes more pronounced as modern generative models employ increasingly sophisticated forgery techniques, such as localized modifications (\textit{e.g., altering facial features like eyes or mouths}) or holistic image synthesis. To address this challenge, mask localization has emerged as a more flexible and effective approach, effectively capturing subtle forgeries and adapting to diverse scenarios. Existing methods can be categorized into two primary approaches: \textbf{Image-Text-Mask Alignment Localization} and \textbf{Independent Mask Localization}. The methods are illustrated in Fig.~\ref{fig:MLLM-image} (b).

\begin{itemize}
\item \textbf{Image-Text-Mask Alignment Localization}
In this approach, ``image" refers to the input image, ``text" represents the explainable textual output about forgery, and ``mask" indicates the localized forgery region. Further, methods in this category can be divided into two subcategories: ``Mask + Image → Text" and ``Text + Image → Mask". For \textbf{``Mask + Image → Text"}, Forgerygpt~\cite{li2024forgerygpt} employs a Mask Extraction Module to capture pixel-level features of tampered regions, using the FL-Expert to generate precise forgery masks and the Mask Encoder to transform mask features into tokens compatible with the MLLM. These mask, image, and text features are then fused and input into the MLLM, enabling accurate localization of tampered regions along with explainable outputs. 
About \textbf{``Text + Image → Mask"}, Fakeshield~\cite{xu2024fakeshield} introduces a tamper comprehension module to enhance the detection of forgery regions by aligning descriptive features of tampered areas with visual attributes. By integrating segmentation techniques based on the Segment Anything Model, it generates precise forgery masks. Similarly, SIDA~\cite{huang2024sida} extends MLLM with specialized tokens and leverages multi-head attention for the precise fusion of detection and segmentation features. Editscout~\cite{nguyen2024editscout} combines an MLLM-based reasoning query generation module and a segmentation model, where the [SEG] token bridges user prompts and images to produce binary masks for edited regions with minimal fine-tuning.

\item \textbf{Independent Mask Localization}
ForgeryTalker~\cite{lian2024large} proposes a method that employs an independent mask decoder to directly generate mask predictions, offering a more modular approach to forgery detection. This approach offers a modular method for forgery detection and sends tokens to LLMs to generate explainable text outputs.
\end{itemize}

\begin{figure*}[!ht]
  \centering
    \includegraphics[width=1.0\linewidth]{Fig/MLLM-Video_Audio.pdf}
    \caption{Illustrating of MLLM-based detection methodologies for AI-generated Video and Audio}
    \label{fig:MLLM-Video&Audio}
\end{figure*}

\subsection{Video}
MLLMs integrate linguistic and visual data to process videos by leveraging LLMs and connecting them with modality-specific encoders through interfaces like Q-former. Notable open-source Video-LLMs include: \textbf{VideoChat}~\cite{li2023videochat}: a chat-centric interactive system primarily designed for video content understanding and multimodal generation; \textbf{VideoChatGPT}~\cite{maaz2023video}: combines visual encoders with LLMs for video-based conversational analysis; ~\textbf{Video-LLaMA}~\cite{zhang2023video}: integrates audio and visual signals from videos using Q-former, enabling efficient handling of multimodal tasks; \textbf{LLaMA-VID}~\cite{li2025llama}: represents video frames as tokens containing contextual and content information, significantly improving video processing efficiency.

Currently, the primary focus of Video Anomaly Detection (VAD) tasks using MLLMs lies in identifying anomalies in real-world scenarios, such as criminal behavior and abnormal incidents. However, detecting AI-generated videos necessitates addressing specific artifacts, including violations of natural physics and frame flickering. The methods are illustrated in Fig.~\ref{fig:MLLM-Video&Audio} (a). Chang et al.~\cite{chang2024matters} provide a comprehensive summary of the common defects observed in generated videos, offering valuable insights into this emerging challenge.

    \subsubsection{\textbf{Authenticity}}
    The detector of AI-generated video can be divided into two categories: Frame-Level detector and Video-Level detector. Frame-Level detector primarily focuses on studying forgery traces at the image level, while Video-Level detector focuses on detecting forged videos, such as through temporal and frequency domain analysis. Existing methods that use MLLMs as detectors are mostly frame-level detection approaches combined with a consistency detector.
    
    \begin{itemize}
    \item \textbf{Frame-Level detector}
    LOKI~\cite{ye2024loki} also shows the video modality result of judgment and multiple-choice tasks of LLMs, both accuracy respectively 71.3\% and 77.3\% by GPT-4o. 
    MM-Det~\cite{song2024learning} leverages MLLMs for frame-level forgery detection and to generate explainable text. It also uses Vector Quantised-Variational AutoEncoder (VQ-VAE) to reconstruct video content, by comparing the residuals between the reconstructed video and the original video to amplify diffusion forgery features. Finally, it introduces an innovative attention mechanism in the Transformer network to balance the detection of intra-frame and inter-frame forgery traces, integrating global and local features. VANE-Bench~\cite{bharadwaj2024vane} is a benchmark that uses MLLMs to detect AI-generated anomalies, including sudden appearance and disappearant objects, violating natural physics. 

    \item \textbf{Watermarking}
    Li et al.~\cite{li2024video} propose a multi-modal video watermarking approach. They embed imperceptible watermarks into strategically selected keyframes using a flow-based mechanism, ensuring minimal visual disruption. Additionally, the approach uses multiple loss functions to balance watermark robustness and video content integrity, effectively preventing unauthorized access by video-based LLMs.
    
    \end{itemize}

    \subsubsection{\textbf{Explainability}}
    Despite the growing interest in utilizing MLLMs for AI-generated video detection, current research has yet to address the explainability of these methods. Future work could focus on developing frameworks that integrate MLLMs with interpretable visual analysis techniques to provide clear and actionable explanations.
    \subsubsection{\textbf{Localization}}
    Similarly, the localization of manipulated regions in AI-generated videos using MLLMs remains an unexplored area. Research in this direction could explore the potential of MLLMs to combine temporal and spatial features for precise localization, which is particularly challenging in dynamic video content.


    \subsection{Audio}
    Currently, both open-source and proprietary MLLMs offering audio input support remain limited. Moreover, most existing models primarily emphasize audio content comprehension, with relatively little focus on analyzing acoustic characteristics. The methods are illustrated in Fig.~\ref{fig:MLLM-Video&Audio} (b).
    \subsubsection{\textbf{Authenticity}}
    
    \begin{itemize}
    \item \textbf{Prompt-engineering}
    LOKI~\cite{ye2024loki} selects open-source models supporting audio input, such as Qwen-Audio~\cite{chu2023qwen}, SALMONN-7B~\cite{sun2024video} and GPT-4o. For judgment tasks, the accuracy of SALMONN-7B is only 51.2\%. Additionally, some models lack support for multiple-choice tasks. Among those that do, the highest accuracy is achieved by AnyGPT, reaching 50.3\%. Research on distinguishing real and fake audio using MLLMs and acoustic cues remains limited. However, datasets such as those introduced by LOKI~\cite{ye2024loki} and SONICS~\cite{rahman2024sonics} focus on detecting fake voices or music. The field of AI-generated audio detection with Multimodal foundational models is still in its early stages.
    \end{itemize}
    
    \subsubsection{\textbf{Explainability}}
    To date, no research has explored the explainability of audio MLLM-based methods. This represents a significant gap, as explainability is crucial for understanding the decision-making process of these models, particularly in identifying subtle acoustic forgeries. Future studies could focus on developing frameworks that incorporate interpretable audio analysis techniques, thereby improving the transparency and trustworthiness of MLLM-based methods.

    \subsubsection{\textbf{Localization}}
    Currently, there is no published research addressing localization capabilities in audio MLLM-based methods. Localization is critical for pinpointing specific manipulated segments within audio signals, especially in cases of partial or layered forgeries. Further research could investigate how multimodal alignment or segment-wise attention mechanisms might enhance localization accuracy in MLLM-based frameworks.

\subsection{Multimodal}
Having explored text-guided detection methods for individual modalities such as text, image, video, and audio, we now turn our focus to multimodal collaboration. These methods leverage language to guide MLLMs in understanding and processing features from other modalities, demonstrating strong cross-modal adaptability. By integrating features from image, video, and audio modalities, we aim to explore how the intrinsic connections among multimodal content can further enhance the accuracy and robustness of AI-generated media detection.
\subsubsection{\textbf{Authenticity}}
\begin{itemize}
    \item \textbf{Text-Image}
    A key focus in this domain is evaluating image-text consistency and providing explanations for MLLM judgments. Out-of-context (OOC) media misuse involves cases where individuals are required to assess the accuracy of the accompanying statement and evaluate whether the image and caption correspond to the same event. This form of misuse, in which authentic images are paired with false text, represents one of the simplest yet most effective ways to mislead audiences. SNIFFER~\cite{qi2024sniffer} is an MLLM specifically designed for detecting and interpreting OOC misinformation, combining image-text consistency analysis, external knowledge retrieval, and fine-grained instruction tuning. \cite{wu2023cheap} integrates GPT-3.5 to enhance the contextual understanding capabilities of the traditional COSMOS model, leveraging IoU, Sentence BERT, and Prompt Engineering to fuse multimodal information effectively. Fka-owl~\cite{liu2024fka} through knowledge-augmented Large Vision-Language Models(LVLMs) to detect fake news.
    For \textbf{watermarking tasks}, text-image integration necessitates incorporating metadata from the text component and the generation context. Liu et al.~\cite{liu2023t2iw} propose the T2IW framework, which seamlessly embeds a binary watermark into generated images using a joint generation process that combines text encoding and noise. VLPMarker~\cite{tang2023watermarking}, a watermarking method based on backdoor injection, utilizes orthogonal transformation techniques to protect CLIP model copyrights while maintaining model efficiency and accuracy.
    \item \textbf{Visual-Audio}
    %用LLM的方法：与end-to-end的方法不同，因为LLMs能够识别跨模态哪或跨模态中可能存在的空间和时空伪造痕迹不一样
    ~\cite{shahzad2024good} integrates visual frames, audio speech, and text prompts into ChatGPT to generate outputs encompassing audiovisual analysis, interpretation, and authenticity prediction. Their approach involves designing various prompts, including binary classification prompts, probability prediction prompts, and tasks to identify synthetic artifacts. Unlike end-to-end learning-based methods, ChatGPT can effectively detect spatial and spatiotemporal artifacts and inconsistencies within or across modalities. For \textbf{watermarking tasks}, V²A-Mark~\cite{zhang2024v2a} embeds localization and copyright watermarks into video frames and audio samples, which employs a temporal alignment and fusion module and a degradation prompt learning mechanism for visual data, along with a sample-level versatile watermark for the audio. 
    
\end{itemize}
