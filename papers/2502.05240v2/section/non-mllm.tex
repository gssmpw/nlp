\section{Non-LLM-based Detector}
\label{sec:non-mllm}

In addition to methods that use MLLMs, there are various traditional techniques to detect AI-generated media. These approaches employ specialized algorithms and can be categorized into modalities such as text, image, audio, and video, based on the type of data processed.
\input{Table/fig7}

\subsection{Text}
\subsubsection{\textbf{Authenticity}}
Text content detection methods primarily fall into three categories: stylistic-based, linguistics features-based methods, and watermarking. These approaches determine whether a text is AI-generated by analyzing stylistic features, linguistic structures, and watermarking respectively.
\begin{itemize}
    \item \textbf{Stylistic-based} Unlike traditional binary classification problems, stylistic-based methods focus on distinguishing the writing styles of different authors. Each AI model has its unique writing style, and identifying these distinct styles proves to be more effective than a simple binary classification task.
    DeTeCtive~\cite{guo2024detective} is a multi-task, multi-level contrastive learning framework that demonstrates superior performance in detecting AI-generated text across in-distribution and out-of-distribution scenarios. It also introduces a novel feature, Training-Free Incremental Adaptation, which enables adaptation to new data without retraining.
    Shah et al.~\cite{shah2023detecting} propose a novel approach combining features like vocabulary diversity, readability metrics, and semantic distribution with machine learning models for classification. Kumarage et al.~\cite{kumarage2023stylometric} leverage stylometric features with a PLM embedding to enhance the detection of AI-generated text.
    
    \item \textbf{Linguistics-based}
    Hamed et al.~\cite{hamed2023improving} employ an unsupervised approach using repetition patterns of higher-order n-grams as textual characteristics, achieving notable results. Gallé et al.~\cite{galle2021unsupervised} innovatively utilize bigram networks from authentic scientific articles as a benchmark for comparison with ChatGPT-generated content, attaining high accuracy. Both methods cleverly account for the relationships between words.
    
    \item \textbf{Watermarking}
    To watermark existing text, some researchers~\cite{yoo2023robust}~\cite{munyer2024deeptextmark}~\cite{yang2023watermarking} use synonym replacement or syntactic transformations while maintaining overall meaning. However, these methods often rely on specific rules that can lead to unnatural modifications, degrading text quality and making it easier for attackers to detect. To overcome these issues, AWT~\cite{abdelnabi2021adversarial} employs a transformer encoder to encode sentences and merge them with message embeddings, which are then processed by a transformer decoder to generate watermarked text. Detection involves analyzing the watermarked text via transformer encoder layers to extract hidden messages. Then, REMARK-LLM~\cite{zhang2024remark} utilizes a pretrained LLM for watermark insertion and includes a reparameterization step to create sparser token distributions, enabling it to embed twice as many signatures as AWT while still ensuring effective detection, thereby enhancing watermark payload capacity.
    
\end{itemize}

\subsubsection{\textbf{Explainability}}
%当前解释模块对普通用户（lay users）的可理解性仍然较差。现有系统往往难以用直观方式解释复杂的检测逻辑
GPTZero~\cite{gptzero} is an online closed-source detector, which relies on six features for explainability: readability, percent SAT,
simplicity, perplexity, burstiness, and average sentence length. However, it does not provide clarity
on how these features influence its final judgments. Mitrovic et al.~\cite{mitrovic2023chatgpt} use implemented Shapley Additive Explanations to reveal how features of ChatGPT-generated text (such as formality, politeness, and impersonality) influence the classification decisions of detection models.
Ji et al.~\cite{ji2024detecting} introduce a ternary classification framework consisting of human-writing text (HWT), MGT, and an ``undecided" category. Human annotators relabel the text with the newly added ``uncertain" category and provide explanations for their decisions. Current explanation modules still fail to provide intuitive understandability for non-expert users. Existing systems often struggle to intuitively explain the complex detection logic.

\subsubsection{\textbf{Localization}} Zhang et al.~\cite{zhang2024machine} leverage contextual information to analyze multiple sentences simultaneously, and divide the text into chunks and extracting features using fixed-parameter detection models, avoiding additional training. MFD~\cite{tao2024unveiling} framework identifies specific paragraphs or sentences generated by LLMs by combining low-level structural features, high-level semantic features, and deep linguistic features. It enhances robustness through contrastive learning.

\subsection{Image}
\subsubsection{\textbf{Authenticity}}

\begin{figure}[!ht]
  \centering
    \includegraphics[width=1.0\linewidth]{Fig/Non-MLLM-Image.pdf}
    \caption{Illustrating of Non-MLLM-based authenticity detection methodologies for AI-generated images. The methods are categorized into: (a)~\textit{Low-level} (b)~\textit{High-level} (c)~\textit{Reconstruction error} (d)~\textit{Watermarking}, (d) is reproduced from~\cite{luo2025digital}}
    \label{fig:Non-MLLM-Image}
\end{figure}

Image detection methods can be broadly categorized into four types: high-level, low-level approaches, reconstruction error-based methods, and watermarking methods. High-level methods analyze geometric information, such as abnormal lighting, shadows, and reflections. They also examine human anatomy, including pupil reflections and body abnormalities in images. In contrast, low-level~\cite{yang2021mtd} feature methods rely on spatial and frequency domain analysis, as well as identifying artificial fingerprints. Reconstruction error-based methods utilize the reconstruction capabilities of diffusion models, identifying anomalies by comparing differences between the original and reconstructed images.
Watermarking methods involve embedding watermarks either before or after image generation, enabling the detection of AI-generated images through dedicated watermark detectors. The methods are illustrated in Fig.~\ref{fig:Non-MLLM-Image}.
~\begin{itemize}
    \item ~\textbf{High-Level}
   High-level methods primarily analyze \textbf{geometric} information, such as abnormal lighting, shadows, and reflections, as well as \textbf{human anatomy}, including pupil shape reflection and abnormalities in the human body within images. FHAD~\cite{wang2024generated} detects fine-grained human body abnormalities and proposes solutions for missing or redundant body parts through reconstruction. Fraid~\cite{farid2022lighting,farid2022perspective} examines the geometric consistency of vanishing points, shadows, and reflections in generated images, as well as lighting consistency, using these inconsistencies for detection. Sarkar et al.~\cite{sarkar2024shadows} propose three classifiers based on object-shadow relationships, perspective fields, and line segment analysis, achieving good results. AIDE~\cite{yan2024sanity} employs a mixture of expert approach, combining low-level pixel statistics with high-level semantic features, effectively identifying various AI-generated images.
    
    \item ~\textbf{Low-Level}
    Low-level methods primarily focus on spatial and frequency domain information. In the \textbf{spatial} domain, PatchCraft~\cite{zhong2024patchcraft} enhances texture features through image scrambling and reconstruction, examining pixel correlations for detection with robustness to perturbations. LGrad~\cite{tan2023learning} utilizes CNNs to convert images into gradient representations, performing well in cross-model and cross-category tests.
    For \textbf{frequency} domain analysis, AUSOME~\cite{poredi2023ausome} employs discrete Fourier and cosine transforms to analyze diffusion model-generated images, identifying specific patterns in DALL-E 2 outputs. Wolter et al.~\cite{wolter2022wavelet} propose a wavelet packet-based multi-scale time-frequency analysis method, preserving spatial and frequency information. Synthbuster~\cite{bammey2023synthbuster} leverages frequency artifacts in diffusion model-generated images for detection. Frank et al.~\cite{frank2020leveraging} analyze artificial traces in GAN-generated images using discrete cosine transforms.
    Researchers have also examined \textbf{artificial fingerprints} in images. Corvi et al.~\cite{corvi2023intriguing} discover that various generators leave specific traces in images. SeDID~\cite{ma2023exposing} cleverly utilizes the deterministic reverse process of diffusion models, introducing the concept (\textit{e.g., time step, stride error}) to distinguish between real and synthetic images by analyzing error patterns at specific timesteps. Moreover, the E3~\cite{azizpour2024e3} framework uses transfer learning to create specialized expert embedders for different synthetic image generators, allowing accurate detection with minimal data. It combines embeddings from multiple experts through an Expert Knowledge Fusion Network to enhance detection performance, particularly for newly emerged generators. 
    
    \item ~\textbf{Reconstruction Error} 
    With the reconstruction capability of Diffusion models, researchers identify abnormal regions by comparing the differences between the original and reconstructed images. DIRE~\cite{wang2023dire} was the first detector proposed for diffusion-generated images. AEROBLADE~\cite{ricker2024aeroblade} utilizes autoencoder reconstruction errors from LDMs in a train-free method. FIRE~\cite{chu2024fire} detects diffusion-generated images by analyzing frequency-based reconstruction errors. DRCT~\cite{chendrct} builds on the aforementioned observation and employs contrastive learning to improve generalization by generating hard samples during the reconstruction process. In addition, SemGIR~\cite{yu2024semgir} utilizes an image-to-text approach followed by text-to-image regeneration, calculating the similarity between the original and re-generated images to distinguish AI-generated images.

    \item ~\textbf{Watermarking}
    EditGuard~\cite{zhang2024editguard} embeds dual invisible watermarks in images to achieve copyright protection and tamper localization. This method trains a unified Image-Bit Steganography Network (IBSN), which decouples the training process from specific tampering types, enhancing the model's generalizability and allowing it to operate effectively without labeled data for particular tampering scenarios.
    %watermarking for generative models
    Additionally, watermarks can be integrated into diffusion models. The watermarks embedded in generative models are static, meaning that they do not adjust based on changes in the generated content. 
    %during静态水印
    DiffusionShield~\cite{cui2023diffusionshield} generates watermarks in generative diffusion models (GDMs) using a blockwise strategy that segments the watermark into basic patches. Each user has a unique sequence of patches that encodes copyright information across their images. The method also utilizes joint optimization to improve efficiency and accuracy, allowing for the easy addition of new users without retraining.
    %latent diffusion models 水印
    Moreover, Latent Diffusion Models (LDMs) generate the image in the latent space of a pre-trained autoencoder. We argue that this latent space can be used to integrate watermarking into the generation process.
    ZoDiac~\cite{zhang2024robust} injects watermarks into the latent space of stable diffusion models during noise sampling, enhancing the invisibility and robustness of the watermarked images. LaWa~\cite{rezaei2024lawa} modifies latent features of pre-trained LDM to embed watermarks during image generation
    %插件动态水印
    However, some researchers have found ways to design watermarks that can be dynamically adjusted according to the context. WMAdapter~\cite{ci2024wmadapter} is a plugin that seamlessly integrates watermarking into the diffusion models in the diffusion process, enabling dynamic watermarking without the need for individual fine-tuning for each watermark. 

\end{itemize}

Moreover, a recent study~\cite{tan2024c2p} has found CLIP model does not truly understand the concepts of ``real" and ``forged". Instead, it detects deepfake content by identifying similar concepts or features. Therefore, C2P-CLIP~\cite{tan2024c2p} integrates category-related concepts (\textit{e.g., DeepFake, Camera}) into CLIP's image encoder through a text encoder, through the use of image-text contrastive learning techniques. Also, some researchers~\cite{kim2024correlation, song2024quality} have found that existing methods typically train detection models by mixing deepfake data with varying levels of forgery quality. These approaches may cause the model to overly rely on easily identifiable forgery traces in low-quality samples, which can negatively affect its generalization ability. To address this, FreDA~\cite{song2024quality} proposes improving the facial structure of low-quality samples by combining the low-frequency features of real images with the high-frequency features of forged images, thereby enhancing their realism.

\subsubsection{\textbf{Explainability}}
For Non-MLLM methods, explainability tends to focus more on interpretability, which involves explaining the internal decision-making mechanisms of the model, rather than producing human-understandable explanatory content.
Cifake~\cite{bird2024cifake} employs Gradient Class Activation Mapping (Grad-CAM) technology, revealing that the model primarily relies on subtle visual defects in the image background, rather than the features of the objects themselves, to differentiate between real and synthetic images. ASAP~\cite{huang2024asap} uses gradient-based methods to identify pixel groups that have the greatest impact on classification results, revealing key falsified patterns in AI-generated images.

\subsubsection{\textbf{Localization}}
The main methods for localizing AI-generated forgery regions extract diverse features and employ various feature fusion modules to improve detection accuracy. They also utilize different strategies to enhance tampered edge traces, enabling high-precision localization of forgery regions.
DA-HFNet~\cite{liu2024hfnet} extracts RGB features, noise fingerprint features, and frequency domain features. It employs a dual-attention fusion mechanism for multimodal features and a multi-scale feature interaction strategy, along with edge loss optimization, to accurately localize forged regions. DiffForensics~\cite{yu2024diffforensics} trains a module that can simultaneously extract both high-level and low-level features and proposes an Edge Cue Enhancement Module to strengthen the edge features of the tampered region. MoNFAP~\cite{miao2024mixture} framework integrates both detection and localization tasks while incorporating various noise features to enhance the clues for forgery detection.
Also, HiFi-Net++~\cite{guo2024language} categorizes forgery attributes into multiple levels, such as fully synthetic, diffusion models, conditional generation, etc. It employs multi-level classification learning to comprehensively represent forgery features. By capturing the contextual dependencies between forgery attributes through hierarchical relationships, the method outputs both forgery detection and localization results.
SAFIRE~\cite{kwon2024safire} addresses the image forgery localization problem from a more fundamental perspective. The approach divides an image into different source regions based on its origin. Each source region represents an independent part of the image, which may be captured, AI-generated, or tampered with through other means. SAFIRE uses a point-based hint mechanism, where a point in the image is utilized to segment the source region that contains it, thereby enabling the division of the image into distinct source regions.


\subsection{Video}
\subsubsection{\textbf{Authenticity}}
~\cite{chang2024matters} identifies three main issues in AI-generated videos: appearance, motion, and geometry. Appearance refers to the inconsistency in color and texture, often resulting in distortions, especially during transitions between video frames. Motion indicates that the motion trajectories of objects may not comply with physical laws. Geometry highlights that objects in generated videos frequently violate real-world geometric rules, such as spatial proportions, scale, and occlusion order. We observe that methods for detecting AI-generated videos can be categorized into two types: \textbf{Frame-level}, and \textbf{Video-level} approaches. Each of these methods is suited to different detection scenarios and requirements, enabling effective identification across various video authentication tasks.

\begin{itemize}
    \item \textbf{Frame-Level} 
    Similar to the classification approach used in MLLM detectors, frame-level detection primarily focuses on identifying forgery traces by extracting individual video frames. Bohacek~\cite{bohacek2024human} detects AI-generated human motion in videos by utilizing multi-modal embeddings, including CLIP-based models, to map the visual information of video frames to their corresponding textual descriptions within the same semantic space. Each frame is first classified as real or fake using an SVM. Then, the authenticity of the entire video is determined based on the majority of the frame predictions. AIGVDet~\cite{bai2024ai} extracts features and performs classification on the spatial and optical flow of each frame. The results from each frame are combined through a decision fusion module to determine whether the video is AI-generated.

    \item \textbf{Video-Level}
    In video-level analysis, the focus is on the unique characteristics of videos, such as temporal and spatial features. 
    For \textbf{temporal-based} methods, DIVID~\cite{liu2024turns} combines CNN and LSTM architectures to capture both spatial and temporal features by leveraging DIRE ~\cite{wang2023dire} values. This approach improves accuracy by incorporating explicit knowledge from reconstructed frames and temporal dependencies, thereby enhancing the detector's generalizability on OOD video datasets. In addition, He et al.~\cite{he2024exposing} find that temporal dependencies in real and generated videos differ significantly: Real videos are captured by camera devices, with very short time intervals between frames, resulting in high temporal redundancy. In contrast, AI video generation models generate videos by controlling the temporal continuity between frames in latent space. To address this, they leverage local motion information and global appearance variations through representation learning. The model combines these features using a channel attention mechanism for effective feature fusion.
    However, other approaches focus on the \textbf{spatial-temporal consistency}. 
    Yan et al.~\cite{yan2024generalizing} propose a Video-level Blending method to simulate inconsistencies in facial features across consecutive frames in deepfake videos. Additionally, they introduce a lightweight Spatio-temporal Adapter, a plugin that enhances CNN or ViT architectures to simultaneously capture both spatial and temporal features.
    DuB3D~\cite{ji2024distinguish} adopts a dual-branch architecture, with one branch processing the raw spatio-temporal data and the other handling optical flow data.
    Demamba~\cite{chen2024demamba} is a plug-and-play detector, which processes the spatial and temporal dimensions of features, modeling the spatio-temporal consistency between features through grouping and scanning. By aggregating global and local features, it utilizes an MLP to classify the video, outputting the probability of whether the video is real or fake.
    %video-fingerprint
    Moreover, generated videos leave distinct traces, similar to image \textbf{fingerprints}, which can be learned and detected after performing a Fourier transform. Vahdati et al.~\cite{vahdati2024beyond} find video generators leave different traces than image generators, combining frame and video-level analysis for classifier training.

    \item \textbf{Watermarking}
    Similar to image watermarking, video watermarking can be implemented frame by frame using image watermarking techniques. Additionally, it is crucial to consider temporal correlations and the robustness of the watermark in video watermarking. DVMark~\cite{luo2023dvmark} uses an end-to-end trainable multi-scale network for robust watermark embedding and extraction across various spatial-temporal clues. REVMark~\cite{zhang2023novel} focuses on improving the robustness against H.264/AVC compression via the temporal alignment module and DiffH264 distortion layer.
    \end{itemize}

\subsubsection{\textbf{Explainability}}
At present, there is no existing research that specifically explores the explainability of AI-generated video detection using a Non-MLLM detector, leaving this area open for future investigation.

\subsubsection{\textbf{Localization}}
Currently, no research paper specifically addresses the Localization of detecting AI-generated videos for Non-MLLM detectors.


\subsection{Audio}
\subsubsection{\textbf{Authenticity}}
\begin{itemize}
    \item \textbf{Fingerprint}
    Traditional audio detection methods often rely on handcrafted features that encompass both perceptual and physical attributes. Salvi et al.~\cite{salvi2024listening} suggest that each TTS model may have a unique ``fingerprint", which is derived from background noise and high-frequency components. 
    \item\textbf{Watermarking}
    %在生成音频的时候加入水印
    Deep-learning audio watermarking methods focus on multi-bit watermarking and follow a generator or detector framework.
    %Multi-Bit Watermarking：嵌入多个比特的信息来实现，可以传递更多的内容
    DeAR~\cite{liu2023dear} is designed to counter audio re-recording (AR) distortions by modeling these distortions through a pipeline of environmental reverberation, band-pass filtering, and Gaussian noise. The approach employs a differential time-frequency transform for optimal watermark embedding, allowing end-to-end training of the encoder and decoder without relying on predefined rules.
    AudioSeal~\cite{roman2024proactive} is a localized watermarking that jointly trains a generator and a detector to embed and robustly detect watermarks. The approach enhances detection accuracy by masking the watermark in random sections of the audio signal and extends to multi-bit watermarking, enabling the attribution of audio to specific models or versions without compromising the detection process.
    %Zero-Bit Watermarking：是一种不携带具体信息的水印方法，在音频信号中嵌入特定的模式或特征来表明某个音频片段是水印的
    Other researchers have explored zero-bit watermarking, which is better adapted for the detection of AI-generated media. 
    Wu et al.~\cite{wu2023adversarial} introduce small, imperceptible perturbations to the original audio, directing its deep features towards specific watermark characteristics. To ensure practical robustness, they utilize data augmentation and error-correcting coding techniques.
    \end{itemize}
    

\subsubsection{\textbf{Explainability}} About interpretability features, SLIM~\cite{zhu2024slim} addresses audio deepfake detection by exploiting the Style-Linguistics Mismatch between real and fake speech, where real speech exhibits a natural dependency between linguistic content and vocal style, while deepfakes break this dependency. It learns this dependency in two stages: first by contrasting the style and linguistic representations of real speech, and then by using these learned features to classify audio as real or fake.
SFAT-Net-3~\cite{cuccovillo2024audio} combines amplitude and phase encoding and introduces a more complex decoder to predict the F0, F1, and F2 phoneme trajectories.
Pascu et al.~\cite{pascu2024easy} use scalar features, such as Mean Unvoiced Segment Length, through the classifier to detect and offer interpretability in the process. 

\subsubsection{\textbf{Localization}}
%定位
For localization of AI-generated segments,
HarmoNet~\cite{liu2024harmonet} combines multi-scale harmonic F0 features with self-supervised learning representations and an attention mechanism and also introduces a new Partial Loss function to focus on the boundary between real and fake regions.
CFPRF~\cite{wu2024coarse} combines frame-level detection network and proposal refinement network with difference-aware feature learning and boundary-aware feature enhancement modules.

%绿色安全的检测
What's more, Green AI is important to protect users' rights.
Safeear~\cite{li2024safeear} develops a neural audio code that decouples semantic and acoustic information, providing a novel privacy-preserving approach for deepfake detection. 



\subsection{Multimodal}
\subsubsection{\textbf{Authenticity}}
\begin{itemize}
    \item \textbf{Text-visual}
    HAMMER~\cite{Shao2023CVPR}, based on hierarchical manipulation reasoning, integrates unimodal encoders, multimodal aggregators, and dedicated detection heads. It captures inter-modal interactions through manipulation-aware contrastive learning and modality-aware cross-attention for content detection. 
    
    \item \textbf{Audio-visual}
    AI-generated audio-visual detection often relies on content consistency detection methods~\cite{li2024zero}, while other researchers employ graph-based multimodal fusion strategies~\cite{yin2024fine} to enhance the detection process.
    Li et al.~\cite{li2024zero} propose a zero-shot detection method based on content consistency, which utilizes Automatic Speech Recognition and Visual Speech Recognition models to decode audio and video content, respectively, generating content sequences for both modalities. Then it calculates the edit distance between these two content sequences as a metric to measure the consistency between the audio and video modalities.
    Yin et al.~\cite{yin2024fine} constructs heterogeneous graphs using positional encoding, capturing intra- and inter-modal relationships through cross-modal graph interaction and dehomogenized graph pooling modules. 

    \item \textbf{Trimodal}
    For trimodal fusion detection methods, there is a notable fusion strategy that effectively integrates the three modalities.
    Yoon et al.~\cite{yoon2024triple} propose a trimodal deepfake detection method using zero-shot identity and one-shot deepfake baselines, implementing visual, auditory, and linguistic feature interaction through a two-stage approach, with residual connections and late fusion to prevent information loss.
    
\end{itemize}

\subsubsection{\textbf{Localization}}
There are only localization methods for visual-audio.
DiMoDif~\cite{koutlis2024dimodif} detects forged content by calculating the differences between audio and video signals and using these differences to identify forgeries. Additionally, it optimizes the localization accuracy of the forged regions by calculating the overlap between the predicted forged intervals and the ground truth annotations.
MMMS-BA~\cite{katamneni2024contextual} framework effectively captures the interaction between audio and video signals using a cross-modal attention mechanism across multiple modalities and sequences. Additionally, it performs deepfake detection and localization through classification and regression heads.