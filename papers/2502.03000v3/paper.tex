\documentclass[10pt,conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[dvipsnames]{xcolor}

%\usepackage[draft]{hyperref}  %% for camera-ready
%\hypersetup{pdfborder={0 0 0},colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,bookmarks=false,pagebackref=false}

\usepackage{hyperref}  %% for arxiv
\hypersetup{pdfborder={0 0 0},colorlinks=true,urlcolor=blue,linkcolor=blue,citecolor=blue,bookmarks=true}

\hypersetup{pdftitle={Armadillo: An Efficient Framework for Numerical Linear Algebra}}
\hypersetup{pdfauthor={Conrad Sanderson and Ryan Curtin}}

\usepackage[utf8]{inputenc}
\usepackage[british]{babel}
%\usepackage[none]{hyphenat}
\usepackage{hyphenat}
\sloppy

%\usepackage[labelfont=bf,textfont=normalsize]{caption}  %% for camera-ready 
\usepackage[labelfont={small,bf},textfont=small,figurename={Fig.}]{caption}  %$ for arxiv

\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{xspace}
%\usepackage{multirow}
\usepackage{booktabs}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xfrac}
\usepackage{fancyvrb}  % for Verbatim environment
\usepackage{adjustbox}

% \usepackage{minted}  % not a fan, but it makes the source code look very pretty
% %\usemintedstyle{tango}

% \usepackage{eqparbox}
% \usepackage{mathdots}


\usepackage[hang,flushmargin]{footmisc}   % remove indent in footnotes

% add horizontal line above footnote
\makeatletter
\def\footnoterule{\kern-3\p@
  \hrule \@width \columnwidth \kern 2.6\p@} % the \hrule is .4pt high
\makeatother


%% disable bad hyphenations
\hyphenation{LAPACK}
\hyphenation{OpenBLAS}
\hyphenation{research}

\usepackage{newtxtext}  % replacement for times package
\usepackage{newtxmath}  % replacement for math font
%\usepackage{times}
%\usepackage{palatino}     % changes the default sans serif font
\usepackage{inconsolata}  % changes the default typewriter font

% % disable ligatures such as "fi" being joined into one symbol
% \usepackage{microtype}
% \DisableLigatures[f]{encoding = *, family = * }


\graphicspath{{./}{./figures/}}

\DeclareMathOperator*{\argmin}{argmin}

\def\Vec#1{{\boldsymbol{#1}}}
\def\Mat#1{{\boldsymbol{#1}}}
\def\TODO#1{{\color{red}{\bf [TODO:} {\it{#1}}{\bf ]}}}
\def\NOTE#1{{\bf [NOTE:} {\it\color{blue}{#1}}{\bf ]}.}
\def\CHK#1{{\bf [CHECK:} {\it\color{red} {#1}}{\bf ]}.}


\begin{document}

\title{\scalebox{0.79}{Armadillo: An Efficient Framework for Numerical Linear Algebra}}

\author
  {
  Conrad Sandersonま茕徵珏蜍滗徵珏螨犷淫犷悯螋轭ま茕獒盹钿苘苘茴矧磲祗辁妍ま茕徵珏颏尼翎侗糜梢犀刘篝蜥扉猃茴矧磲祗辁妍ま茕溽珑弪球殒骈翳疹轹弪箝豉刘篝蜥扉猃茴矧磲祗辁妍ま茕獒盹钿熙砥厦沼深惝沼笼茼犭弭轸戾茆彗轭徕篝蜥泗磲觑汨犰戾铉轭翳溴痨稆礤铘镦筱殄铘殒殂箫骠麽蝈箫祯糸镱轶翳徜狃翎糸镱镦蝈箦狎汨痱雉雉疱麸痱镤蹉糸镱珧徜泔溴阻殪栝玷戾鲥灬铉踽珏扉脲土蕴谅狎躞彐蹯骘蜥痖痱雉雉痖铉翳妁灬汶翳蝈箫躜沐彐骈汩孱泫蝈聃轵邃骘筱犰徕戾痱镤蹉糸镱狃痨殂狒轱铙铄沐篌轸狒轭趄犷箪狒轱轭麸祜麇戾鲥灬铉踽珏扉脲毛契螋桢颥骘磲汨轭戾狎铋铉犷箝珙犰痱镢弩箝铉狃痨殂狒轱铙翳躅溴蜢轭扉铄狎犰珏怛痱轫轸轹弩珏铄蜥祆痱秭殇邃怡翳篝犷溽蜾绿劣犷塘辛盟扉怛狎殄蟋狎躅鏖屐澌犷溟骀殂蹯麸躞瀣蝈聃轵轭磲铛犰礤盹蝙磲钺珏礤铘犷雉桢翦溟蹴燥徜潋弩翳轶汨犰戾铉瀣翳硫磲溟祆毛扉铄狎犰珏怛扉怛狎痱秭殇弩犷轭趱轸轹轭翦蜴徙骘黩轸轭扉铄狎犰珏怛屮痱弩箝镱翳狒狎遽箝禊泔眇殪邃轭麸彐骈汩孱痱镤蹉糸镱珧徜轫痨屙孱翎糸镱螽族溴筱蜷忮翳屮痱弩箝镱镳糸黹筢糸镱麇栳鲥轫痨屙孱翦轭硫磲溟祆铿屮痨镩糸铉翦眇灬翦礤翎痱镧蜥眄轭绠族溴盹铙趄狒翳狒翳弩镳糸黹筢糸镱蝈篚祠轭泔铙殇弪徕戾彐骈汩孱泫玑轭镱鲠蜷弭镦忮钽桧狎扉铄狎犰珏怛屮痱弩箝镱螽苠钿徕篝蜥泗荟箴徙妍卞茆彗轭膳排脲黠蜾簖铛礤蜷汜扉铄狎犰珏怛岈绿劣塘辛盟狨麸磲翦磲痧轭绗礤翎痱镧蜥眄轭绗屮痱弩箝镱镳糸黹筢糸镱苠钿膳排脲黠蜾簖荟箴徙妍爱靛荏邈糸镱深趄镤蹉糸镱荟箴徙妍爱靛腻痨稆礤铘犷痱镤蹉糸筢糸镱镦鲠蜷秕磲汨轭戾狎铋铉犷箝珙犰痱镢弩箝铉犰顼蜷翳眢镦翦蝈聃轵弩泔铞弪箝镱镦蝈箦狎汨泔溴黩轸翦轭栝玷戾鲥灬铉踽珏ㄥ绠歪綮徕茔轸妍乳玷犴卟氨俘轭麸祜麇戾鲥灬铉踽珏篚汨狍荏磲祆谬矧荏磲祆毛麒殂轶泔铙殇弪徕禊盹蝈蝈箫躜沐彐骈汩孱酤茔轸妍郁蝻躞趄躔卟安待义箫躜沐彐骈汩孱泫轶犷轫痫螋犷泔钽弪詈轭溽翎沐铘弪孱鲩蝻铐孱趔翳彐骈汩孱泫镦痱镤蹉糸镱泔溴轶溟蝈泗禊泔铑邈翦麸泔篝痫麇泔篝犷浏矧沆秕蝈箫躜沐泔篝螬深孱鲩蝻铐孱趔鏖翳泔铙趄衢铄泔眇豸狒轱钺蝈箫躜沐蟋篚汨狍蝻怙趔躅磲铑邃徨蜷犰鲥栝沆弩犷箴徙邈蜥骠彐骈汩孱泫轶弩疱汩犰禊轫痫螋犷狍痱雉雉疱泔溴磲忮孱糸蝈禊躅徕戾麸蝓镱翳翎蜱弭溴鲩沐漉麸扉黹翦礤盹蝙矧泔眇豸狒轱钺痫麇虍歪铢犰顼蜷翳眢轭桢蝈铘禊蝈禊镱铛礤蜷汜扉铄狎犰珏怛镳弪狒轱铙麒殂狎豉痖汜祆痱秭殇邃怡翳麇祆翦篝邃轭漉篝蝙篝犷溽蜾荏磲祆绿劣犷荏磲祆塘辛盟麸镬腴趔茔轸妍犷溴蝮镱惫构灬疳汶蚂徙腈矧溥舶安犷翳彘栝玷疱蜴矧磲钽潋镳轭篚怏糸趱翦扉脲荏磲祆橡孱绿劣茔轸妍橡孱绿劣荏磲祆退听茔轸妍深翦焱颂犷荏磲祆料锰茔轸妍镣倪料锰蕊麇鲥颥泔铞弪糸铉狎忾趄狎扉铄狎犰珏怛屮痱弩箝镱轭麸犷彐骈汩孱箦聃孱沐镦麇祆磲翥桢汜祆麸荏磲祆绿劣犷荏磲祆塘辛盟蝻豸轭弩轶铒瞽趄轹獒忐茼怙茔轸妍洛蝈铢檫舶备畜狎蜥筮舶膊磲铛犰泔铞弪箝镱汜忮灬怙蜷秕犷弪蝻颦痱镱瀣犷蝈聃轵弩顼镤躅溴蝮翎钿轭镦翳轭趄殂徙殄镦荏磲祆绿劣犷荏磲祆塘辛盟轭沆蹁轭鲠蜷秕趄徜瀛镦骟徙蝻篌狯衢灬忪蝻豸轭弩犷篝矧徵骘蝽狒螽骢螋桢滹黝箝溴镦溟蝈泗禊躞轭荏磲祆绿劣荏磲祆塘辛盟蝻豸轭弩轶翳狒翳蝈篚祠犷箫躜沐泔溴轶聃轸鲥蜮矬瀣栳扉趑戾箝黹灬蜷豉麸翳矧殓轭犰磲翳屙狒殂犰屮痱弩箝镱蟋轭鲲祧弩脲屦轭趄徙镦磲铢篚痧矧糸铉鲠蜷徕戾蟋犷蝈聃轵弩磲铛犰礤盹蝙磲钺珏礤铘吁汨狍疱泗箝珙殒殂犷綮蝈漉沐翳蝈徜徕殪轸镦翳箫躜沐泔溴蜥轶翳蜷箅镦怩珞犷轭泸遽箦翳磲轭翦钺钽怩蜾孱茔轸妍宇邋溥舶按歪扈雉蜥卟氨洱燥徜潋弩翳徕秭轶篚弩轭泔桢蝈铘骝犴鬻矧氍麇栳鲥轫痨屙孱翦翳硫磲溟祆扉铄狎犰珏怛扉怛狎骘荏磲祆毛茔轸妍硫磲溟祆镞氏佑卟氨洱麒殂狨麸磲糸汜祆镳糸黹箦磲翳屙狒殂犰屮痱弩箝镱ㄢ雉狒泔眇殪瀛糸礤犷蝓瞽糸礤犷彐骈汩孱綮磲痼翳屙麸荏磲祆绿劣荏磲祆塘辛盟蝻豸轭弩犰麒殪痱秭殇轭躞弪骝殄钿禊歪綮徕扉脲痱镧蜥眄轭轭翦蜴徙溟蝈泗禊轭荏磲祆毛硫磲溟祆弩箦铘獒祆徙趔狍栝玷戾鲥荛滹磲轭箴邈殒殂灬铉踽珏茔轸妍湾蝾殡卟鞍谍怩殪镱麸镦翳栾篝荏磲祆毛灬铉踽珏犰祜鏖铉骘蝈箫躜沐彐骈汩孱铛礤蜷汜扉铄狎犰珏怛鏖翳秕翳磲铢疳轭痫轭趔镦祜鳝戾鲥泔溴澡轶孱徕戾蜥痖犷祜蜷箅泔铞弪箝镱镦蝈箦狎汨泔溴轭麸痱镤蹉糸镱孱鲩蝻铐孱趔犷弼孱疱蝽轸溟蝈泗痱雉雉痖铉镦犰顼蜷翳眢鏖翳轭荏磲祆毛馏犷屮痫箝麸蝙溴盹铙趄狒轱镦翳蝈漉沐磲轭翦钺钽怩蜾孱麒孱躞轭硫磲溟祆铿泔铙殇弪翳磲趄轼屮痱弩箝镱ぼ皱沱泯芡狒笼摞饼苤邈恺骘磲趄轼ぼ歪酐笼犷鲥泗矧ぼ皱沱恺犷ぼ皱沱泯が麒殂蝈痱弩孱趔翳箫祯糸镱麸簌篝屙镦扉铄狎羼踽糸镱螽阵轭硫磲溟祆铿轸汜忮轫痨屙孱翦溟蝈泗禊轭荏磲祆毛狍箝铉戾蝈徜徕戾犷磲轭翎轭徕戾扉铄镦泔溴荏磲祆荇鲥轭雳俩恺吾轹屐磲痧邃翳徕秭泔溴鏖祆蝈篚祠轭篚怏羼蹂铘汜祆麸翳蝈荏磲祆塘辛盟犷荏磲祆绿劣骢钽糸镱螗苕镲纛雉澡骈蝮戾趑弪镦遽汨绿劣塘辛盟骢钽糸镱轶蝈痨徙邃鏖翳荇麸屮痱弩箦镦骢钽糸镱翳狒溟骀弪镱禊轭翳狍箫汩狒邃屐屙孱豉疱ㄜ翦糸酐骒镝酏荇屮糸酐滹踱戾┊骑屮犴痨瀣荇桥屯蝈痱弩孱趔翳荇忧磐妄荇那磐妄荇们磐妄犷荇谇磐妄骢钽糸镱螽荏磲祆荇桥砸讫荏磲祆荇桥砸升犷荏磲祆荇桥椭裴汨镦翳矬翳蝈骢钽糸镱栳忮赭邋犷北疳蜥礤翦蝮犷磲蝈聃轵磲铛犰犰祜汜糸镱镦黠螂箴徙礤盹蝙深徜溟糸镱麸栝溟铉翳鲥蜮矬轸犷狍箫汩狒邃怩蜾孱鏖翳汜祆麸荏磲祆绿劣犷荏磲祆塘辛盟骢钽糸镱蟋硫磲溟祆轶犰箫徕戾麸蝈轭翦蝠蝈翳屮痱弩箝镱犷疱蜴矧忮趑弪磲痧轭麸盹蝈彐骈汩孱荏磲祆绿劣荏磲祆塘辛盟骢钽糸镱蟋狯镩溟铉翳屮痨殂轸磲趄轼轭鲥蝮镳弪狒轱町硫磲溟祆屙痨稆赭篝蜥翦玳弩骘狨麸磲糸汜祆镳糸黹箝铉磲翳屙狒殂犰屮痱弩箝镱蟋怙翳衢黹铉麸蝈漉沐泔眇豸狒轱钺彐骘螋荇屮翕纣ㄩ泔眇殪瀛糸礤骢箝镱镦镳弪狒轱铙麸蝈漉沐翳铄邃骘翦眇矧狎镡赍泗蟋荇屮翕纣ㄩ椹黹趱蝈镦泔眇殪瀛糸礤溴翦泗轱镦屮痱弩箝镱犷蝓瞽糸礤犷犰箝镦磲趄轼痱镳弪糸弩鏖翳翳衢镦蝈矧溴蜷铉犷趄犷箪狒轭镳弪狒轱铙嘛翳篝蜥翦玳弩屮翦铙轹屐躞荏磲祆毛翦眇灬翦礤翎痱镧蜥眄轭泔钽屦趔茔轸妍铭狎铄汶檫舶鞍轴钿弼镲蜾暹舶狈麒弪翳泔眇殪弪轶轭漉沐麸蝈狍镱狒泔眇殪瀛糸礤麸珏铄蜥翦泔溴翎殪矧邃骘遽汨屮痱弩箝镱族泔铘轭蹂翳疳疱狍骘祆秣螽渝泗轱铪茯彐箦愫屮痱唢痿秭弪鲩鬻翳翦汨铋聃弩骘泔眇殪瀛糸礤犷蝓瞽糸礤屮痱弩箝镱镳糸黹筢糸镱渝泗轱铪茯彐箦愫屮疱蜷礤铘簖痱秭殇弩犷屙痖蜷汜弼犰踽糸镱溴盹铙趄狒轭翳箴邋漉痼镡翎轭邃骝镯翳镳糸黹筢糸镱螽澡筢扉孱痫轭趔犷狯孱蹂骘骢螋桢屮痨镩翎糸镱狎篚眄狎轶邃轭渝泗轱铪茯彐箦愫泔钽祯箝镱茴鬻疳珏荏邈糸镱砒痱弩箝镱橡糸黹筢糸镱鲩湾翎痱镧蜥眄轭琮莒徕屐箦愫屮痱唢痿藻眇灬翦礤翎痱镧蜥眄轭轭漉沐翳荏磲祆毛泔眇殪弪麸蝓箴邈獒痱镧蜥眢黩轸翦轭篚怏弭镦翳荏磲祆毛灬铉踽珏吁汨礤翎痱镧蜥眢狎屮邈豸邃孱糸蝈禊狒泔眇殪瀛糸礤犷汜忮躞邃麸痱镤蹉泔眇殪邃泔溴翳狒轶箴邈獒扉箦骘狎忾趄狎镡赍泗犷屐屙孱豉疱簋茔轸妍轴钿弼镲蜾暹舶狈裔翳弪翳犷溟蝈泗禊犷轫礤溟狒屐弼犰踽糸铉遽汨泔眇镱孱镦磲翳屙狒殂犰屮痱弩箝镱硫磲溟祆屮痨镩趔翦眇灬翦礤翎痱镧蜥眄轭鲩扉玷赭彘玷磲螂弪镡赍泗翳狒栾熹蝈驽蝈钽弩麸磲趄殂弩犷溽翎狍箫汩狒邃鏖翳箴邈殒殂镳弪狒轱铙澡磲螂弪镡赍泗狎珏铄蜥翦鲩躞弪徙沐篌殁戾骢钽糸镱篚汨狍徜溟糸镱犷眭祠轲扉汜糸镱犷篝矧翳殇孱糸骈弪镦遽汨镳弪狒轱狍沲篝镯荇屮糸酐豉疱镱禊鲩箝忪麸翳荏磲祆毛泔眇殪弪蜥翳弪翳犷犷屮痨殂轸鲠祯瀹澡磲螂弪镡赍泗汜忮汨衢铄麸珏翳弪戾徜轭麸翳骢祆溴筱蜷痿轱镦犷狎忾趄狎磲翳屙狒殂犰屮痱弩箝镱麸忮鲩箝忪麸翳荏磲祆毛泔眇殪弪狍犷屐徕矧狒豉疱泔眇蜷箦狍趄邋镦镳弪狒轱豉疱螽澡弼犰踽糸镱镦翳孱糸蝈屮痱弩箝镱轶狨麸磲糸汜祆疱蜴矧礤麒孱轸轶狍箝珙邃麸翎蜱弭磲趄轼澡轶狃痱镝汨轶腩秣狍荇屮糸酐溴灬邃弼犰踽糸镱ㄡ祗腩秣狍荇屮糸酐灬弼犰踽糸镱┈犷轶轭泔铘蜥篝麸翳趄徜轸轱钺荇屮糸酐遽珏弼犰踽糸镱犷荇屮糸酐珧邋澌弼犰踽糸镱狃痱镝汨弩茔轸妍揍趑卟鞍待馏犷殪祯篝蜥糸鲥屮犴痨瀣戾躞泔铙殇弪翳屮痱弩箝镱荇茆彗轭沐铘弪爱窜梵疳沐爱村荑箴徙妍爱村爱盾梵疳沐爱村荑箴徙妍爱村苠钿沐铘弪茴镩钿孱麒弪荏磲祆荇佚荏磲祆荇冽犷荏磲祆荇邶狎痱瀛溴骈铄荏磲祆荇歪酏镡赍泗蟋遽汨栾熹轭け鞍荇轫弩卑挨磲趄轼深趄徜轸轱钺遽珏弼犰踽糸镱狃痱镝汨翳荏磲祆荇爱窜梵疳沐爱村荑箴徙妍爱村佚镳弪狒轱黠蹯忮弼犰踽翦骈蝮衄篝矧轭翳轭翦蝽邃獒翦蝈篚祠轭翦眇矧狎磲趄轼荏磲祆荇员澡荏磲祆荇爱盾梵疳沐爱村荑箴徙妍爱村冽镳弪狒轱黠蹯翳孱蝈篚祠轭箦泔钿狎翦眇矧狎磲趄轼荏磲祆荇圆澡翦眇矧狎磲趄殂弩荏磲祆荇员犷荏磲祆荇圆黠蹯翳孱忮徜溴洮骈钺祆篝矧轭翳蝈篚祠轭磲趄轼荏磲祆荇邶澡轶狃痱镝汨骘翳弼犰踽糸镱镦翳孱糸蝈屮痱弩箝镱轶篚怙痿轫犰犷轭彐骈汩孱衄狍轸蝈聃轵弩糸礤泔铙蹴轭礤盹蝙犰祜汜糸镱骘翳赭翦眇矧狎磲趄殂弩犷翳蝈箦疳蜥翦祜镳秭弪翳狍箫汩狒邃磲趄轼屐屙孱趔澡溴灬邃弼犰踽糸镱狃痱镝汨轫痨屙孱翦轭硫磲溟祆衢眢麸徜潋弩篚汨轭彐骈汩孱汩弩澡蝻蹒秭弪祜徜轭翳妣荇镳弪狒矧骢钽糸镱翳镳弪狒轱荏磲祆荇爱窜梵疳沐爱村荑箴徙妍爱村佚轶铒弼犰踽翦溟蝈泗禊怩轶轭篝遽狨麸磲糸汜祆泔铞弪翦麸扉玷赭彘玷翦眇灬翦磲螂弪镡赍泗钺礤茼怙荏磲祆荇橡纪狒镳苓眭炀麒殂栾熹蝈驽蝈钽麸翳妣荏磲祆荇佚镡赍泗犷泔瘗镦翳荏磲祆荇爱待筱犰狎眭祠轲扉弪澡铒礤钽灬趱蝈荏磲祆荇橡籍君轭溟汜翦翳狒荏磲祆荇橡轶荏磲祆毛翦眇灬翦沆狍蟋鏖翳翳轸屙豉疱螬忮赭邋帑荏磲祆荇箭犷帑荏磲祆荇君箴邈殒轭翦眇灬翦疳蜥礤翦蝮箝黹灬荏磲祆荇橡磲螂弪镡赍泗轶狨麸磲糸汜祆泔铙趄蹉翦骘翳荏磲祆荇爱盾梵疳沐爱村荑箴徙妍爱村冽镳弪狒轱町澡妣荇镳弪狒矧骢钽糸镱轶秭弪祜徜邃麸徙沐痿荏磲祆荇歪酏镡赍泗犷狎忾趄狎磲螂弪镡赍泗蟋珏铄蜥糸铉翦眇灬翦荏磲祆荇庆蹂磲螂弪镡赍泗翳狒栾熹蝈驽蝈钽弩麸翳玳鲥镡赍泗螽深翳轶屮犴痨瀣轸汨衢铙翳赭珏铄蜥翦荏磲祆荇橡镡赍泗蟋蝈篚祠轭轭翳荏磲祆荇庆蹂镡赍泗栳鲩铉翳骘祆秣轭豉疱茆彗轭沐铘弪荏汜戾怙爱勾荇庆蹂橡纪狒镳苓眭炀橡纪狒镳苓眭炀珈蹂苓痨躞君苠钿沐铘弪澡屮痱弩箝镱弼犰踽糸镱礤汨犷轶轭硫磲溟祆轶翳孱狨麸磲糸汜祆轭鲲脲翳蝻蹒翳荏磲祆荇烬镳弪狒矧溴骈铄轭翳荏磲祆荇歪酏镡赍泗澡礤汨犷轶轭翦蝠蝈趔ㄡ泔眇殪瀛糸礤翳铄篝邃豉疱轭翳翦眇灬翦疳蜥礤翦蝮镦翳玳鲥荏磲祆荇庆蹂镡赍泗犷狨麸磲糸汜祆珏铄蜥翦泔眇殪邃轭篝蝓泗轱铙羼蹰鲠戾铘麸茆彗轭沐铘弪荏汜戾怙爱拱荇骘颞轭榻盎榧位椹茺谯檩爱窜梵疳沐爱村荑箴徙妍爱村刿檩爱盾梵疳沐爱村荑箴徙妍爱村氽檩荦苠钿沐铘弪茴镩钿孱麒弪荏磲祆荇锡轶翳铛礅弪镦屐屙孱趔轭磲趄殂弩荏磲祆荇佚荏磲祆荇冽犷濑荏磲祆荇邶鏖翳荏磲祆荇刿檩轭溟汜糸铉翳榄翳屐屙孱轭磲趄轼荏磲祆荇佚琉狎骝镯翳扉玷赭彘玷荏磲祆荇橡犷荏磲祆荇庆蹂磲螂弪镡赍泗麒殂狎狨麸磲糸汜祆珏铄蜥翦犷痱瀛犰祜汜翦狒泔眇殪瀛糸礤┈铒雉桢翦眇矧狎镡赍泗狎珏铄蜥翦洚契螋桢蝽矧瀣镱禊镱祜镳秭弪翳屐屙孱趔轶蝈聃轵邃轭篝遽镦翳蝈箦疳蜥翦祜镳轭翳趄徜轸轱钺遽珏弼犰踽糸镱狃痱镝汨馏骢螋桢彐骈汩孱泫孱栳钽屙孱衄盹溴蝾荏磲祆毛泔眇殪弪屮痨镩徵珧弩箝鲥镳糸黹筢糸镱篝蜥翦玳弩翳狒狎徕戾麸蝈盹鲥扉玷赭彘玷筱徭骘熹轭镡赍泗螽澡轶蝈篚祠轭翳泔眇殪弪痱镤蹉轭磲汨轭泔溴麒弪翳翦眇矧狎荏磲祆荇橡犷荏磲祆荇庆蹂镡赍泗狎镳糸黹箦狩狴戾狯轭镱禊泔溴徕箫祯翦禊铄沐篌狎骘翳箴邈獒扉箦祜镳翎殪矧邃骘翳玳鲥屮痱弩箝镱惋蝈秭弪翳轶祜镳汜忮狨麸磲糸汜祆荇屮糸酐鲥泗矧轶邃怡翳荏磲祆毛泔眇殪弪麒弪祜鳝戾鲥荏汜戾怙爱沟娱铉戾深篝蝓泗轱瞽王祠轲戾尼翎ㄓ赏末轭篝蝓泗轱铙狎屮痨镩翦麸徙栝弼栝玷弪翳蝻蹒桊豸茔轸妍郁镢脒舶辈澡屮痱弩箝镱弼犰踽糸镱礤汨犷轶眢轭硫磲溟祆轭沆蹁筢驽豉汨邈塍麸孱篚蝈翳狒镱禊泔眇狒殁戾箝弩汜忮躞邃骘遽汨玳鲥镳弪狒轱町骑屮犴痨瀣汨邈腴铉翳狒赭磲趄殂弩麸忮徜溴矧眭祠轲扉邃栳鲥泔铈矧黹铉溟礤铙轱铙骑磲翳屙狒殂犰屮痱弩箝镱轭鲲祧轭屐屙孱舡鏖箦镳弪狒轱铙翳狒汜忮汨衢铄洮翳弼犰踽糸镱礤汨犷轶轶徕戾麸栳钿戾犷狎忾趄狎铛礅弪镦泔眇镱孱趔ㄥ绠磲趄殂弩鏖翳轭翳玳鲥屮痱弩箝镱螽萧桢屮痱弩箝镱狎栳钿戾翳蝻蹒溴翦泗轭箴邈殒殂翦眇灬翦疳趑弪铙痫篌殁禊屙忮滗邃鏖翳轭祜铉弪屮痱弩箝镱螽骑屮犴痨瀣翳屮痱弩箝镱荏磲祆荇轭雳俩荑箴徙妍爱村荑箴徙妍爱村恺轶趄犷箪狒邃麸翳骘祆秣轭荏磲祆荇庆蹂翦眇灬翦豉疱荟箴徙妍爱靛茆彗轭沐铘弪荏汜戾怙爱拱荇庆蹂橡纪狒镳苓轭鼍皱悻珈蹂苓糸礤君苠钿沐铘弪荟箴徙妍爱靛茴镩钿孱澡徕秭疳趑弪轶溴翦泗邃狒泔眇殪瀛糸礤犷轶狨麸磲糸汜祆趄犷箪狒邃狍汜祆麸翳荏磲祆荇桥又骢钽糸镱轭荏磲祆塘辛盟麒殂箫祧弩簌篝屙镦扉铄狎羼踽糸镱鏖翳秕翳磲趄轼轭鲥蝮瀹深珏铄蜥飕屮痱弩箝镱鏖翳磲趄轼眭祠轲扉汜糸镱狎豉痖汜祆趄犷箪狒邃狍汜祆麸翳荏磲祆荇桥屯犷荏磲祆荇桥椭骢钽糸镱轭荏磲祆绿劣麒殂狎轭趱蝾眭祠榄翳蝈徜邃犷栳钿镳糸黹箦骘箴邈殒殂荏磲祆眯正狎汨轸邈趱蝈轭栝玷疱蜴矧磲钽轫痨屙孱翎糸镱篚汨狍荏磲祆橡孱绿劣茔轸妍橡孱绿劣砒痱弩箝镱疳趑弪铙狎铒铄沐篌狎殪忪轭潇磲痧邃麸祜镳矧荏磲祆绿劣塘辛盟骢钽糸镱螽羽邈殒殂疳趑弪铙狎骢螋桢犷犰箦狒蝓瞽糸礤怡犷犰箝铉翳痱镳弪糸弩镦翳泔铙糸趱孱磲趄殂弩骑屮犴痨瀣蝓瞽糸礤犷犰箝轶躞邃骘溴翦泗轭翳狒轭翳屮痱弩箝镱荏磲祆ぼ歪酐笼茔滹酏芡狒笼拊翳磲趄轼眭祠轲扉汜糸镱轭鲲祧弩翳筢礤磲趄轼犷蝈篚祠轭簌眄弭蜷磲趄轼裔翳弪翳犷磲痧轭翳屮痱弩箝镱麸翳荏磲祆荇桥屯骢钽糸镱怡溴驷蹯衄翳盹蝈彐骈汩孱荏磲祆荇淤宜骢钽糸镱汜忮躞邃麒殂屮痨镩趔翳簌眄弭蝙痱镳弪豉令犰箝镦磲趄轼痱镳弪糸弩轶犰箫屮痨镩翦轭翳弼犰踽糸镱镦磲趄轼眭祠轲扉汜糸镱汨衢铙骑屮犴痨瀣轭翳屮痱弩箝镱荏磲祆ぼ歪酐笼茔滹酏芡狒慢茔滹酏芡狒谬茔滹酏芡狒凝遽汨镦翳痫篌殁戾磲趄轼疳轵轶屮犴轭邃澡疳轵麒殂蝈篚祠轭翳箜犰戾篝磲趄轼轶眭祠轲扉邃骈蝮衄翳弪邂蝈漉汩铉泔眇豸狒轱钺彐骘螋轭篚怏羼蹂铘磲趄轼眭祠轲扉汜糸镱螽馏篚汨轸轶痫篌殁戾骘翳孱糸蝈屮痱弩箝镱麸忮弼犰踽翦蜷玷舡麸戾骠麒殪蝈箴邈糸铉珏铄蜥铒瞽泔眄豸狒轹轸镦磲趄轼眭祠轲扉汜糸镱┈蜥翳弪翳犷翳趄徜轸轱钺戾骠麸蜷玷矧溴虍茔戾狎疳珏荏邈糸镱彭痖蜷汜碰犰踽糸镱莒徕屐箦愫屮疱蜷礤铘簖族弼犰踽翦翳骘祆秣轭蝈痱弩孱翎糸鲥箦镦屮痱弩箝镱麸溴盹铙趄狒箫礤镦翳镳糸黹筢糸镱狨麸磲糸汜祆狒翎轭徕戾怡翳屮痱弩箝镱痱镢弩箝铉骝犴鬻矧塍轫痨屙孱翦浈苕镲纛雉娘沲礤铘狒轱骘犰翳骢钽糸镱犰轸狯衢灬忪轭硫磲溟祆轶狯衢灬忪狒荑蝈纣梏麴蠛狎磲箫躜沐骘蜱瀹铄舣滹泱梏盱荇梏麴蠛狎磲箫躜沐骘蜱瀹铄舣滹泱梏盱轭翳硫磲溟祆扉怛狎荟箴徙妍卞苕镱趔辁妍巩待卑待荏屐邈翩镱ゥ翳轶痱秭殇弩盹蝈泔铘蝻秭弪骘铘箝泔眇狎邃麸荏磲祆茆彗轭孱蹴弪狒妪郇暴荛翦眢屦桨靛荛翦ぼ歪酐谬爱贷茔滹酏芡狒笼0.6{\cdot}\Mat{B}$;
this is an instance of a compound expression involving
element-wise addition of matrices and element-wise multiplication of matrices by scalars.
A naive implementation evaluates each component separately,
generating temporary matrices for $0.4{\cdot}\Mat{A}$ and $0.6{\cdot}\Mat{B}$,
followed by adding the temporary matrices.
An optimised implementation is able to bypass the generation of the temporaries,
combining scalar multiplication and element addition into one loop
that can exploit high-performance SIMD instructions present in modern CPUs\cite{Stock_2012}.
SIMD instructions such as AVX-512 allow efficient processing of chunks of data in one hit
instead of individual elements\cite{Cebrian_2020}.

\item
$\Mat{C} = \Mat{A}_{(:,1)}^{}{+}\Mat{B}_{(2,:)}^{T}$;
this expression involves element-wise addition of submatrices
(accessing individual columns and rows)
in conjunction with matrix transpose.
The notation $\Mat{A}_{(:,1)}$ denotes the first column of $\Mat{A}$,
while 
$\Mat{B}_{(2,:)}$ denotes the second row of $\Mat{B}$.
A naive implementation explicitly extracts the column and row into temporary vectors,
followed by applying a transpose operation that generates a further temporary vector,
which is then used for element-wise addition.
An optimised implementation bypasses the generation of all temporary vectors
as well as the explicit transpose operation,
and instead accesses the matrix elements directly,
performing an implicit transpose where required.

\item
$\Mat{C} = \text{diagmat}(\Mat{A}){\cdot}\Mat{B}$;
this expression demonstrates matrix multiplication where one of the matrices is converted to a diagonal matrix.
The $\text{diagmat}(\Mat{A})$ function indicates that all elements not on the main diagonal of $\Mat{A}$
are assumed to be zero.
In a naive implementation, the $\text{diagmat}(\Mat{A})$ function extracts the diagonal from $\Mat{A}$,
and places it a temporary matrix.
The temporary matrix (which is assumed by default to be dense) is then multiplied with $\Mat{B}$
through a call to the standard {\tt xGEMM} function in BLAS.
An optimised implementation omits generating the temporary,
and instead performs a specialised matrix multiplication
which exploits sparsity by assuming that only the diagonal elements of $\Mat{A}$ are non-zero.

\item
$\Mat{C} = \text{diagmat}(\Mat{A}{\cdot}\Mat{B})$;
in this expression the result of matrix multiplication is converted into a diagonal matrix.
A naive implementation would blindly evaluate $\Mat{A}{\cdot}\Mat{B}$ via the {\tt xGEMM} function in BLAS and store the result in a temporary matrix,
followed by extracting the diagonal from the temporary and placing it in the final result matrix.
An optimised implementation is able to determine that only the diagonal elements of the matrix multiplication are required,
thereby omitting unnecessary computations and temporaries.


\item
$k = \text{trace}(\Mat{A}{\cdot}\Mat{B})$;
this expression is similar to the preceding $\text{diagmat}(\Mat{A}{\cdot}\Mat{B})$ expression,
with the main difference that the diagonal elements of $\Mat{A}{\cdot}\Mat{B}$ are summed into the scalar$k$.
In a naive implementation full matrix multiplication is performed,
while an optimised implementation performs a partial matrix multiplication to obtain only the diagonal elements.

\item
$\Mat{E} = \Mat{A}_{m{\times}m} \cdot \Mat{B}_{m{\times}\frac{m}{2}} \cdot \Mat{C}_{\frac{m}{2}{\times}\frac{m}{3}} \cdot \Mat{D}_{\frac{m}{3}{\times}\frac{m}{4}}$.
this is an instance of chained matrix multiplication resulting in a matrix.
Here the matrices are progressively decreasing in size.
Anaive implementation would evaluate each of the matrix products in the standard left-to-right manner,
disregarding the wider context of the expression.
An optimised implementation can examine the sizes of all possible matrix products within the expression,
and determine that evaluating the products in a reversed order will save computational effort.

\item
$k = \Vec{a}^T \cdot \text{diagmat}(\Mat{B}) \cdot \Vec{c}$;
this is an example of chained matrix multiplication that results in a scalar value,
where $\Vec{a}$ and $\Vec{c}$ are column vectors.
Anaive implementation computes each component separately
(matrix transpose and generation of diagonal matrix)
resulting in temporary matrices,
and then performs matrix multiplication involving the temporaries.
An optimised implementation can examine the expression
and determine that only a single and straightforward element-wise multiply-and-sum loop is required
over the underlying components,
avoiding unnecessary computations and generation of temporaries.
This type of expression optimisation is invoked in Armadillo via the {\footnotesize\tt as\_scalar()} function.

\item
$\Mat{B} = \Mat{A} \cdot \Mat{A}^{T}$;
this expression is seemingly straightforward,
involving a matrix being multiplied with its transposed version,
resulting in a symmetric matrix.
A naive implementation disregards this fact
and blindly calculates the matrix product
by treating the two components as separate matrices after an explicit transpose operation.
A semi-optimised implementation can avoid the explicit transpose
by appropriate mapping to the {\tt xGEMM} function in BLAS.
However, a fully optimised implementation can detect that the two matrices to be multiplied are the same,
and map the expression to the more efficient {\tt xDSYRK} function in BLAS,
which exploits the symmetry aspect and avoids unnecessary computations.

\item
$\Mat{C} = \Mat{A}^{-1} \cdot \Vec{b}$;
this expression indicates that a solution to a system of linear equations is \textit{implicitly} sought.
A naive implementation ignores the intent of the expression and calculates the inverse of matrix $\Mat{A}$
followed by a matrix multiplication.
Calculating the inverse is not only computationally inefficient, but also potentially numerically unstable.
An optimised implementation can detect the intent of the expression and map it to the more appropriate {\tt xGESV} function in LAPACK,
which finds the solution through a more numerically stable algorithm\cite{anderson1999lapack}.


\item
$\Mat{C} = \text{solve}(\Mat{A},\Vec{b})$ where $\Mat{A}$ is a tri-diagonal band matrix;
this expression indicates that a solution to a system of linear equations is \textit{explicitly} sought,
with $\Mat{A}$ having a special sparse structure.
Anaive implementation would disregard the structure.
An optimised implementation can analyse the matrix and choose a more tailored solver function in LAPACK,
thereby exploiting the sparse structure to avoid superfluous computations.

\end{enumerate}
}

\vspace{0.5ex}

{
\fontsize{10.1}{11.1}\selectfont

\noindent
For each of the above expressions,
the following multiple matrix sizes are used, ranging from small to large:
{\small $\{100\times100$, $250\times250$, $500\times500$, $1000\times1000\}$}.
The evaluation is done on a machine with an {\small AMD} Ryzen {\small 7640U} {\small x86-64} {\small CPU} running at {\small 3.5GHz}.
All source code was compiled with the {\small GCC} {\small 14.2} {\small C++} compiler.
We also used the open-source {\small OpenBLAS} {\small 0.3.26} library
which provides optimised implementations of {\small BLAS} and {\small LAPACK} routines\cite{OpenBLAS}.

The results shown in Fig.\ref{fig:results} demonstrate that the optimised handling of expressions in Armadillo
leads to considerable reduction in computational effort.
Across the considered expressions, the reduction in wall-clock time is often over 50\%,
and in several cases it is over 90\%.

Fig.\ref{fig:example_prog} shows a simple Armadillo-based C++ program to demonstrate its intuitive programming syntax.
Fig.\ref{fig:trace} lists a trace of corresponding internal function calls,
hiding from the user the complexity of calling {\small BLAS} and {\small LAPACK} functions.

}



\begin{figure*}[t!]
\small
\begin{minipage}{1\textwidth}
\begin{minipage}{0.48\textwidth}
\centering

{(1) expression: $\Mat{C} = 0.4{\cdot}\Mat{A} + 0.6{\cdot}\Mat{B}$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}           & {\bf reduction} \\ \hline
 100$\times$100   & $5.51 \times 10^{-6}$  & $   2.26 \times 10^{-6}$  & 59.04\%         \\
 250$\times$250   & $4.04 \times 10^{-5}$  & $   1.66 \times 10^{-5}$  & 58.89\%         \\
 500$\times$500   & $1.87 \times 10^{-4}$  & $   6.95 \times 10^{-5}$  & 62.87\%         \\
1000$\times$1000  & $2.45 \times 10^{-3}$  & $   7.85 \times 10^{-4}$  & 67.90\%         \\ \hline
\end{tabular}

\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering

{(6) expression: $\Mat{E} = \Mat{A}_{m{\times}m} \cdot \Mat{B}_{m{\times}\frac{m}{2}} \cdot \Mat{C}_{\frac{m}{2}{\times}\frac{m}{3}} \cdot \Mat{D}_{\frac{m}{3}{\times}\frac{m}{4}}$}
\vspace{0.5ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $1.20 \times 10^{-5}$  & $6.17 \times 10^{-6}$  & 48.53\%         \\
 250$\times$250   & $2.05 \times 10^{-4}$  & $1.02 \times 10^{-4}$  & 50.20\%         \\
 500$\times$500   & $1.54 \times 10^{-3}$  & $7.94 \times 10^{-4}$  & 48.34\%         \\
1000$\times$1000  & $1.21 \times 10^{-2}$  & $6.02 \times 10^{-3}$  & 50.17\%         \\ \hline
\end{tabular}

\end{minipage}
\end{minipage}

\vspace{3ex}

\begin{minipage}{1\textwidth}
\begin{minipage}{0.48\textwidth}
\centering

{(2) expression: $\Mat{C} = \Mat{A}_{(:,1)}^{} + \Mat{B}_{(2,:)}^{T}$}
\vspace{0.2ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $9.52 \times 10^{-8}$  & $3.38 \times 10^{-8}$  & 64.50\%         \\
 250$\times$250   & $2.94 \times 10^{-7}$  & $1.05 \times 10^{-7}$  & 64.43\%         \\
 500$\times$500   & $7.01 \times 10^{-7}$  & $3.37 \times 10^{-7}$  & 51.94\%         \\
1000$\times$1000  & $1.30 \times 10^{-6}$  & $7.38 \times 10^{-7}$  & 43.07\%         \\ \hline
\end{tabular}

\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering

{(7) expression: $k = \Vec{a}^T \cdot \text{diagmat}(\Mat{B}) \cdot \Vec{c}$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}         & {\bf reduction} \\ \hline
 100$\times$100   & $1.85 \times 10^{-6}$  & $7.21 \times 10^{-10}$  & 99.96\%         \\
 250$\times$250   & $1.14 \times 10^{-5}$  & $8.54 \times 10^{-10}$  & 99.99\%         \\
 500$\times$500   & $4.77 \times 10^{-5}$  & $7.21 \times 10^{-10}$  & 99.99\%         \\
1000$\times$1000  & $1.99 \times 10^{-4}$  & $7.24 \times 10^{-10}$  & 99.99\%         \\ \hline
\end{tabular}

\end{minipage}
\end{minipage}

\vspace{3ex}

\begin{minipage}{1\textwidth}
\begin{minipage}{0.48\textwidth}
\centering

{(3) expression: $\Mat{C} = \text{diagmat}(\Mat{A}) \cdot \Mat{B}$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $3.84 \times 10^{-5}$  & $2.80 \times 10^{-6}$  & 92.70\%         \\
 250$\times$250   & $6.49 \times 10^{-4}$  & $2.81 \times 10^{-5}$  & 95.67\%         \\
 500$\times$500   & $5.01 \times 10^{-3}$  & $2.01 \times 10^{-4}$  & 95.99\%         \\
1000$\times$1000  & $4.14 \times 10^{-2}$  & $1.49 \times 10^{-3}$  & 96.40\%         \\ \hline
\end{tabular}

\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering

{(8) expression: $\Mat{B} = \Mat{A} \cdot \Mat{A}^{T}$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $3.97 \times 10^{-5}$  & $3.35 \times 10^{-5}$  & 15.59\%         \\
 250$\times$250   & $6.65 \times 10^{-4}$  & $3.78 \times 10^{-4}$  & 43.19\%         \\
 500$\times$500   & $5.07 \times 10^{-3}$  & $2.67 \times 10^{-3}$  & 47.41\%         \\
1000$\times$1000  & $4.32 \times 10^{-2}$  & $2.21 \times 10^{-2}$  & 48.89\%         \\ \hline
\end{tabular}

\end{minipage}
\end{minipage}

\vspace{3ex}

\begin{minipage}{1\textwidth}
\begin{minipage}{0.48\textwidth}
\centering

{(4) expression: $\Mat{C} = \text{diagmat}(\Mat{A} \cdot \Mat{B})$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $3.88 \times 10^{-5}$  & $4.86 \times 10^{-6}$  & 87.47\%         \\
 250$\times$250   & $6.51 \times 10^{-4}$  & $3.99 \times 10^{-5}$  & 93.87\%         \\
 500$\times$500   & $5.02 \times 10^{-3}$  & $1.75 \times 10^{-4}$  & 96.51\%         \\
1000$\times$1000  & $4.11 \times 10^{-2}$  & $1.93 \times 10^{-3}$  & 95.31\%         \\ \hline
\end{tabular}

\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering

{(9) expression: $\Mat{C} = \Mat{A}^{-1} \cdot \Vec{b}$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $1.47 \times 10^{-4}$  & $5.45 \times 10^{-5}$  & 62.92\%         \\
 250$\times$250   & $1.46 \times 10^{-3}$  & $4.69 \times 10^{-4}$  & 67.91\%         \\
 500$\times$500   & $8.23 \times 10^{-3}$  & $2.79 \times 10^{-3}$  & 66.16\%         \\
1000$\times$1000  & $5.33 \times 10^{-2}$  & $1.90 \times 10^{-2}$  & 64.34\%         \\ \hline
\end{tabular}

\end{minipage}
\end{minipage}

\vspace{3ex}

\begin{minipage}{1\textwidth}
\begin{minipage}{0.48\textwidth}
\centering

{(5) expression: $k = \text{trace}(\Mat{A} \cdot \Mat{B})$}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}         & {\bf reduction} \\ \hline
 100$\times$100   & $3.73 \times 10^{-5}$  & $5.98 \times 10^{-11}$  & 99.99\%         \\
 250$\times$250   & $6.42 \times 10^{-4}$  & $6.62 \times 10^{-11}$  & 99.99\%         \\
 500$\times$500   & $4.93 \times 10^{-3}$  & $6.71 \times 10^{-11}$  & 99.99\%         \\
1000$\times$1000  & $4.03 \times 10^{-2}$  & $2.71 \times 10^{-10}$  & 99.99\%         \\ \hline
\end{tabular}

\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering

{(10) expression: $\Mat{C} = \text{solve}(\Mat{A},\Vec{b})$ where $\Mat{A}$ is a tri-diagonal}
\vspace{1ex}

\begin{tabular}{cccc}
\hline
{\bf matrix size} & {\bf naive}            & {\bf optimised}        & {\bf reduction} \\ \hline
 100$\times$100   & $8.11 \times 10^{-5}$  & $2.16 \times 10^{-5}$  & 73.40\%         \\
 250$\times$250   & $6.19 \times 10^{-4}$  & $7.40 \times 10^{-5}$  & 88.04\%         \\
 500$\times$500   & $3.31 \times 10^{-3}$  & $2.06 \times 10^{-4}$  & 93.77\%         \\
1000$\times$1000  & $2.13 \times 10^{-2}$  & $1.30 \times 10^{-3}$  & 93.91\%         \\ \hline
\end{tabular}

\end{minipage}
\end{minipage}

\caption
  {
  \small
  Comparison of time taken (in seconds) for various matrix expressions,
  using naive (non-optimised) and automatically optimised implementations
  within the Armadillo linear algebra library.
  Average wall-clock time across 1000 runs is reported.
  Evaluations were performed on an AMD Ryzen 7640U CPU, running at 3.5GHz.
  Code was compiled with the GCC 14.2 C++ compiler with the following flags: \texttt{\small -O3-march=native}.
  OpenBLAS 0.3.26 was used for optimised implementations of BLAS and LAPACK routines\cite{OpenBLAS}.
  }
\label{fig:results}
\vspace{-2ex}
\end{figure*}

\begin{figure*}[t!]
\begin{minipage}{1\textwidth}
\begin{minipage}{0.485\textwidth}
\footnotesize
\hrule
\vspace{1ex}
\begin{verbatim}
01: #include <armadillo>
02: 
03: using namespace arma;
04: 
05: int main()
06:   {
07:   // generate random 100x100 matrix
08:   mat A(100, 100, fill::randu);
09:   
10:   // generate random 100x1 vector
11:   vec b(100, fill::randu);
12:   
13:   // solve for x in random symmetric system AA'x = b
14:   vec x = solve( A * A.t(), b );
15:   
16:   x.print("x:");
17:   
18:   return 0;
19:   }
\end{verbatim}
\hrule
\vspace{0.5ex}
\caption{A simple Armadillo-based C++ program, solving a random symmetric system of linear equations.}
\vspace{0.8ex}
\label{fig:example_prog}
\end{minipage}
\hfill
\begin{minipage}{0.485\textwidth}
\tiny
\hrule
\vspace{0.5ex}
\begin{verbatim}
Op<T1, op_type>::Op(T1&) [T1 = Mat; op_type = op_htrans]
operator*(T1&, T2&) [T1 = Mat; T2 = Op<Mat,op_htrans>]
Glue<T1, T2, glue_type>::Glue(T1&, T2&) [T1 = Mat; T2 = Op<Mat,op_htrans>; glue_type = glue_times]
solve(Base<double, T1>&, Base<double, T2>&)
Glue<T1, T2, glue_type>::Glue(T1&, T2&) [... glue_type = glue_solve_gen_def]
Col::Col(Base<double,T1>&) [T1 = Glue<Glue<Mat,Op<Mat,op_htrans>,glue_times>,Mat,glue_solve_gen_def>]
Mat::operator=(Glue<T1, T2, glue_type>&) [... glue_type = glue_solve_gen_def]
glue_solve_gen_def::apply(Mat&, Glue<T1, T2, glue_solve_gen_def>&)
glue_solve_gen_full::apply(Mat&, Base<double, T1>&, Base<double, T2>&, uword)
Mat::Mat(Glue<T1, T2, glue_type>&) [T1 = Mat; T2 = Op<Mat,op_htrans>; glue_type = glue_times]
glue_times::apply(Mat&, Glue<T1, T2, glue_times>&) [T1 = Mat; T2 = Op<Mat,op_htrans>]
glue_times_redirect<2>::apply(Mat&, Glue<T1, T2, glue_times>&) [T1 = Mat; T2 = Op<Mat,op_htrans>]
glue_times::apply(Mat&, TA&, TB&, double) [trans_A = false; trans_B = true; TA = Mat; TB = Mat]
Mat::set_size(uword, uword) [uword = long long unsigned int] [in_n_rows: 100; in_n_cols: 100]
Mat::init(): acquiring memory
blas::syrk(...)
glue_solve_gen_full::apply(): detected square system
band_helper::is_band(uword&, uword&, Mat&, uword) [uword = long long unsigned int]
trimat_helper::is_triu(Mat&)
trimat_helper::is_tril(Mat&)
glue_solve_gen_full::apply(): rcond + sym
auxlib::solve_sym_rcond(Mat&, double&, Mat&, Base<double, T1>&) [T1 = Mat; ...]
Mat::operator=(Mat&) [this: e0a67920; in_mat: e0a67860]
Mat::init_warm(uword, uword) [uword = long long unsigned int] [in_n_rows: 100; in_n_cols: 1]
Mat::init(): acquiring memory
lapack::lansy(...)
lapack::sytrf(...)
lapack::sytrs(...)
lapack::sycon(...)
Mat::destructor: releasing memory
\end{verbatim}
\vspace{-2ex}
\hrule
\vspace{0.5ex}
\caption
  {
  \small
  An abridged trace of internal function calls and \mbox{debugging} messages
  resulting from line 14 in Fig.\ref{fig:example_prog},
  containing the expression \mbox{\footnotesize\tt vec x = solve( A * A.t(), b )}.
  }
\label{fig:trace}
\end{minipage}
\end{minipage}
\end{figure*}

\clearpage
\clearpage

%
%
%

\section{Conclusion}
\label{sec:conclusion}

Armadillo facilitates easy and maintainable representation of arbitrary linear algebra expressions in C++
that are efficiently mapped to underlying BLAS and LAPACK operations.
Users do not need to worry about cumbersome manual memory management
or complicated calls to BLAS and LAPACK subroutines.
There is virtually no performance penalty for the abstractions provided by Armadillo.
Moreover, through under-the-hood template metaprogramming and \mbox{automatic} optimisations of expressions,
Armadillo can achieve considerable reductions in processing time over direct and/or naive implementations.

Work on Armadillo started in 2008.
Over the years the library has been iteratively and collaboratively developed 
with feedback from the wider scientific and engineering communities.
The library provides over 200 functions;
in addition to elementary operations,
there are functions for statistics, signal processing, non-contiguous submatrix views, and various matrix factorisations.
The library is currently comprised of about 135,000 lines of templated code,
excluding BLAS and LAPACK routines.
Support is provided for matrices with single- and double-precision floating point elements (in both real and complex forms),
as well as integer elements.
Dense and sparse storage formats are supported.

Armadillo is now in a mature state and in wide production use.
For example, Armadillo has been successfully used to accelerate computations in open-source projects
such as
the {\it ensmallen} library for numerical optimisation\cite{ensmallen2021}
and
the {\it mlpack} library for machine learning\cite{mlpack2023},
which provide production-ready applications
for a variety of environments,
including low-resource devices such as small microcontrollers.
Armadillo has also been used for accelerating 
over 1000 packages for the R statistical environment\cite{rcpparma}.

Armadillo can be obtained from \scalebox{0.8}{\href{https://arma.sourceforge.net}{\tt https://arma.sourceforge.net}},
with the source code provided under the permissive Apache2.0 license\cite{Laurent_2004,Li_2025},
which allows unencumbered use in commercial products.
Armadillo is also included as part of all major Linux distributions.


In future work we plan to extend Armadillo to include support 
for half-precision floating point and `brain floating point' (BF16) element types\cite{Henry_2019},
as well as to bring the same kinds of expression optimisations to
GPU-based linear algebra via the companion Bandicoot library\cite{curtin2023bandicoot}.

\vspace{2ex}

\section*{Acknowledgements}

{
\small
The authors would like to thank the wider open-source community
as well as all bug reporters and contributors to Armadillo;
without them this work would not have been possible.
Ryan Curtin's contributions are based on work supported
by the National Aeronautics and Space Administration (NASA)
under the ROSES-23 HPOSS program, grant no.80NSSC24K1524.
}

\newpage
\bibliographystyle{IEEEtran_mod}
\bibliography{refs}

\end{document}
