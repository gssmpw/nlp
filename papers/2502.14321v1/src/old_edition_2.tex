\section{Introduction}
Large language models (LLMs) have recently demonstrated significant potential across diverse fields. Building upon these strengths, researchers have begun integrating LLMs into autonomous agents equipped with modules for profiling, memorization, planning, and action~\cite{llm_agent_define}. While single-agent systems have made substantial progress by employing reflection strategies~\cite{react} and prompting paradigms~\cite{COT,TOT} to enhance reasoning and planning, they often face scalability and coordination challenges when tackling more complex or dynamic tasks~\cite{single_limit}.

To address these limitations, researchers have proposed LLM-based multi-agent systems (LLM-MAS), where multiple agents interact to achieve goals that exceed the capacity of a single agent. Recent studies underscore LLM-MAS effectiveness in contexts ranging from social simulation\cite{social_media_regulation}, to software engineering\cite{metagpt}, and recommendation systems~\cite{jd_recommendation_system}, illustrating the growing demand for more coordinated and intelligent multi-agent solutions. 

Previous surveys \cite{agent_survey_2} focuses on the components of an agent. However, for LLM-MAS, it only discusses two types of interactions: cooperative and adversarial. Similarly, \cite{agent_survey_1} examines the composition of LLM-based agents and highlights prospective applications. Although it addresses relationships among agents, task planning, and methods for enhancing efficiency, it does not elaborate on the composition of LLM-MAS architectures, interaction strategies among agents, or other relevant features. Subsequent works \cite{mas_survey_3,mas_survey_2} further summarize LLM-MAS. The former introduces LLM-MAS structures (equal-level, hierarchical, nested, dynamic) and emphasizes current challenges. The latter describes interfaces, profiling, and communication structures, but focuses primarily on application scenarios. Both discussions are largely confined to surface-level features such as system architecture but without providing sufficiently detailed insights into the internal workflows. In contrast, \cite{mas_survey_1} does introduce an LLM-MAS workflow, but from a single-agent perspective in the system—covering agent roles, action classifications, and organizational structures. While informative, this single-agent viewpoint cannot fully illustrate multi-agent interactions and workflows, particularly regarding communication paradigms and strategies. Other surveys target specific domains, such as \cite{mas_application_survey_simulation} for simulation and emulation, or \cite{mas_application_survey_1,mas_application_survey_2} for software engineering. Although these works offer valuable insights within their respective fields, they are less generalizable for broader applications.

Based on the above overview and analysis, existing discussions of LLM-MAS workflows remain insufficient. How to synthesize these workflows into a more detailed framework applicable across diverse tasks is the primary aim of this paper. We observe that a key distinction between LLM-MAS and single-agent systems is inter-agent communication, which enables agents to exchange ideas and coordinate plans. Inspired by ~\cite{communication_1,communication_2}, we propose decomposing the LLM-MAS workflow from the perspective of communication characteristics. We propose a two-level framework distinguished between system-level communication characteristics and system internal communication characteristics.  Specifically, we define LLM-MAS as an automated system driven by communication goals within a predefined communication architecture. Agents in this system have multiple communication strategies and paradigms, interacting with various communication objects to exchange diverse content to for task completion.

Through this macro-to-micro perspective, we bridge theory and practical design, offering a detailed taxonomy that can be applied across varied tasks. Our main contributions include:
\begin{itemize}
    \item \textbf{Comprehensive Framework:} We unify diverse LLM-MAS studies under a \textit{communication-centered} taxonomy, highlighting the roles of architecture, strategy, and content.
    \item \textbf{Deep Analysis of Communication Processes:} We dissect real-world examples and prototypes to illustrate how well-orchestrated communication leads to more effective multi-agent behavior.
    \item \textbf{Identification of Challenges and Opportunities:} We shed light on open issues like scalability, security, and multimodal integration, and offer potential research directions for both academia and industry.
\end{itemize}

The remainder of the paper is organized as follows. In Section 2, we provide background on LLM-based single-agent and multi-agent systems, highlighting the contrast in their communication demands. Section 3 elaborates our System-Level Communication Characteristics, presenting architectures and communication goals. Section 4 delves into System-Internal Communication aspects, including strategies, paradigms, and content. In Section 5, we discuss the key challenges and opportunities for advancing LLM-MAS, addressing issues such as security, scalability, and ethical implications. Finally, we conclude in Section 6 with future directions and a summary of our findings.

\section{Background}
\subsection{Single-Agent Systems}
Single-agent systems represent the cornerstone of multi-agent architectures, as they lay the groundwork for understanding how individual agents reason, plan, and act. In line with the taxonomy proposed by ~\cite{agent_survey_1}, this section outlines the composition and functionalities of LLM-based single-agent systems, providing the foundational concepts needed before delving into multi-agent scenarios.

An LLM-based single-agent system typically consists of three key components: Brain, Perception, and Action, each playing a distinct yet complementary role.

\paragraph{Brain:}A LLM that integrates short-term and long-term memory modules to reduce potential hallucinations and enhance the agent's reasoning and planning capabilities.

\paragraph{Perception:}Textual or multimodal inputs (visual/auditory), enabling richer state awareness.

\paragraph{Action:}Integration with external tools such as web APIs or real-world actuators~\cite{embodied_agents}, expanding the agent’s problem-solving scope.

\subsection{Multi-Agent Systems}
While single-agent systems exhibit strong individual reasoning, they often struggle with tasks requiring collective intelligence or large-scale coordination. Multi-agent systems (MAS)~\cite{mas_define} can address these limits by orchestrating multiple intelligent agents and leveraging communication as a key mechanism for goal alignment. 

Agents can cooperate, compete, or negotiate, depending on system objectives and architectural choices. Such flexibility in communication design sets the stage for LLM-MAS, where advanced language models enable more sophisticated inter-agent interactions across diverse application domains.

\section{System-Level Characteristics}
\subsection{Communication Architecture}
Communication architecture defines how agents in an LLM-based multi-agent system are organized and how information flows among them~\cite{mas_a_survey}. It thus shapes both the high-level design and the micro-level interactions. Depending on task complexity, the degree of agent autonomy, and the desired collaboration style, LLM-MAS can adopt one of five principal architectures: Flat, Hierarchical, Team, Society, or Hybrid.


\paragraph{Flat Architecture:}A flat architecture features decentralized agents, each operating at the same level with no central controller. Agents communicate peer-to-peer, enabling rapid and flexible interactions ideal for smaller-scale or highly dynamic tasks. For instance, \cite{dataset_generation} presents a flat system where peer agents collaboratively generate synthetic dialogues. Similarly, \cite{debate_improve_llm_1} leverages multiple LLM agents in a fact-checking setup where they critique each other's reasoning to reduce hallucinations. While this decentralized paradigm encourages high adaptability, it can become less efficient when the number of agents grows large, necessitating more structured coordination.



\paragraph{Hierarchical Architecture:}In a hierarchical architecture, higher-level agents oversee lower-level ones, forming a tree-like structure suited for complex tasks requiring role specialization. High-level agents coordinate overall strategy, while sub-agents focus on specific subtasks. For example, CausalGPT~\cite{casualgpt_reasoning} employs a top-level evaluator to validate solutions generated by lower-level agents, improving reasoning consistency. ChatDev~\cite{chatdev_software_development} organizes software development roles (e.g., designer, coder, tester) under a senior agent for project-level oversight. ~\cite{autodefense_against_jailbreak} designed the AutoDefense multi-agent harmful prompts and jailbreak attack defense framework. In this framework, under the control of the senior agent, some agents focus on filtering harmful content, while others are responsible for processing response evaluation. This hierarchical setup spreads the defense pressure of a single agent, ensuring that the system can adapt to various attack scenarios and remain resilient. Although hierarchical coordination can streamline decision flow, it risks bottlenecks or delays if the top-level agent becomes overloaded, especially in large-scale deployments.

\paragraph{Team Architecture:}In team-based architectures, agents are grouped into specialized teams, each tackling a particular aspect of a complex task. This approach leverages the complementary expertise of different agents and supports dynamic role adjustments. A prime example is MetaGPT~\cite{metagpt}, which applies standardized operating procedures to coordinate specialized software engineering roles (e.g., design, debugging). Similarly, MAGIS~\cite{magis_mas_for_github} divides agents into administrators, developers, and QA testers to streamline collaboration on GitHub issues. The POLCA framework ~\cite{polca_mas_for_political} extends team-based architectures to political coalition negotiations. The framework models the negotiation process as a series of interactions between agents, each representing the interests of a specific party to simulate negotiation dynamics and predict potential coalition outcomes. While team-based structures foster intra-team synergy, they also introduce overhead for inter-team communication and alignment.

\paragraph{Society Architecture:}Society-based architectures simulate a broader social environment, where agents follow certain norms or conventions (`social laws'') and exhibit emergent behaviors akin to human societies. Agents in these systems may have different roles and motivations, but they cooperate or compete to achieve collective goals or individual ambitions, reflecting the dynamics of human society. For example, \cite{stanf_villege} develops a sandbox environment where generative agents form friendships, organize gatherings, and adapt their actions based on past experiences. Meanwhile, \cite{sct_society} explores how agents evolve from a Hobbesian state of nature'' to peaceful societies governed by social contracts. In addition to social simulations, there are currently some simulations for more specific social activities such as economic activities. EconAgent~\cite{econagent} adopts a social framework that emphasizes macroeconomic activities, where agents represent households and firms within a dynamic economy. This system integrates market dynamics with agents' personal experiences to simulate realistic economic behaviors such as consumption, work, and investment. By incorporating memory and diverse decision-making processes, EconAgent provides insights into the impact of individual actions on large-scale economic phenomena. Such systems are well-suited for large-scale emergent phenomena, but can be complex to control or predict, as agent interactions evolve dynamically over time.

\paragraph{Hybrid Architecture:}Not all designs fit neatly into a particular architecture, and some architectures may contain features from several different styles. Therefore, architectures can be transformed into hybrid architectures through overlapping, nesting, and other means. Hybrid architectures are designed to complement each other, adapt to a wider range of scenarios, and arrange agents to achieve more complex, flexible, and efficient problem solving. For instance, \textit{FixAgent}~\cite{fixagent_mas_for_debug} uses a three-level agent design that dynamically adapts to different debugging complexities, merging hierarchical oversight with decentralized collaboration. Likewise, \textit{ChatSim}~\cite{chatsim_mas_make_scene} orchestrates scene simulation through both top-down control (scene rendering) and peer collaboration (assets integration) to overcome the limitations of previous scene simulations. ~\cite{mas_use_tool} provides another example of a hybrid architecture that allows an agent to dynamically search for the most relevant tool to complete a task. This hybrid approach integrates search, tool management, and task execution into a single framework, reducing inference costs, enhancing flexibility, and improving the agent's ability to scale across a variety of applications. Hybrid models can optimize flexibility and efficiency but may require more intricate design to avoid coordination overhead.

\subsection{Communication Goal}
Having defined the overarching communication architectures, we now turn to communication goals that fundamentally guide agent interactions in LLM-MAS. These goals determine whether agents collaborate, compete, or adopt a mixed approach, and significantly shape the behaviors, information-sharing patterns, and coordination strategies within the system, as they encapsulate the “why” behind the communication. In the following subsections, we detail these three goal categories and illustrate how they influence agent behaviors and decision-making processes.

\paragraph{Cooperation:}Cooperation is a central feature in numerous multi-agent scenarios, enabling agents to align their actions for collective benefit. Unlike traditional multi-agent systems, LLMs bring an added dimension to cooperation by providing advanced natural language understanding and generation capabilities, enabling more sophisticated and flexible forms of interaction.  In this survey, we partition cooperation into two main types:1) \emph{Direct Cooperation} and 2) \emph{Cooperation Through Debate}. Although they differ in interaction style and complexity, both approaches aim to integrate multiple agents’ expertise for improved problem-solving and decision-making. \textbf{Direct cooperation} refers to straightforward, unreserved collaboration among agents to achieve a mutual goal. In this framework, LLM-powered agents utilize their advanced natural language processing capabilities to communicate effectively and synchronize their activities. This level of transparency and alignment is particularly valuable when rapid consensus or integrated expertise is paramount. For instance,   some works harness direct cooperation to enhance LLM reasoning\cite{boostrapping,casualgpt_reasoning,autoagents} ,  while others employ it in collaborative code generation \cite{chatdev_software_development,soa_code_generation,magis_mas_for_github,mas_for_software_2,fixagent_mas_for_debug} and scientific experimentation\cite{chatsim_mas_make_scene,mas_use_tool}.The research community continues to explore advanced coordination models~\cite{agentcoord,govsim,mas_benchmark} for fine-grained task allocation, robust communication protocols, and scalability in agent collectives. The implication of \textbf{collaborating through debate} is that rather than unanimously accepting each other’s information, agents engage in critical dialogues, openly scrutinizing different perspectives before converging on a refined solution. Although this method may appear confrontational, its overarching aim is to resolve ambiguities, challenge incorrect assumptions, and ultimately improve the collective outcome. Such debates are especially beneficial in scenarios where the optimal course of action is unclear or where multiple viewpoints must be reconciled. Examples include factual reasoning\cite{debate_improve_llm_1} and fact-checking\cite{debate_2}, where rigorous back-and-forth discussions help validate information and reduce the likelihood of mistakes. By facilitating deep collaboration without sacrificing critical evaluation, debate-driven approaches can yield more robust and well-rounded decisions than simple consensus-based methods.

\paragraph{Competition:}Competition in LLM-MAS emerges when agents possess conflicting objectives or vie for shared resources, or outcomes. Unlike cooperative systems—which focus on aligning agents’ actions—competitive settings demand that each agent contest, or even persuade others to achieve the best outcome for the individual. The game-playing ability of agents in the system has been demonstrated in research. ~\cite{evaluating_llm_game_ability} In these adversarial contexts, LLMs do more than just process information—they actively shape the interaction by generating persuasive or deceptive narratives. For instance, \cite{social_media_regulation} employs multi-agent simulations to study how language evolves in polarizing environments. ~\cite{agent4debate} designed a dynamic multi-agent framework for competitive debate with human.  One significant advantage of competition in LLM-MAS is its potential to drive innovation and strategic sophistication. As agents continually adjust to rivals’ tactics and environmental feedback, they refine their own approaches in a never-ending loop of adaptation. For instance,~\cite{mas_for_poetry_generation} explored a poetry-generation task where agents strove to outperform each other through creative linguistic outputs. However, competition can also introduce instability or security risks, particularly if agents employ deceptive strategies or if the system lacks mechanisms to contain adversarial behavior. Balancing such complex dynamics with overall system performance is thus a core challenge for designing robust competitive environments.


\paragraph{Mix:}While purely cooperative or purely competitive settings offer conceptual clarity, many real-world applications exhibit crossovers between the two modes. Agents may collaborate on certain subgoals but compete fiercely over others, mirroring the complexities of social, economic, or political ecosystems. In LLM-MAS, this “mixed” paradigm grants agents the flexibility to form alliances when mutually beneficial while still pursuing self-interests. In practice, such mixed interactions are pivotal for capturing nuanced, dynamic behavior in domains like social simulations~\cite{community_knowledge_flooding,sct_society}, collective problem-solving, or negotiation tasks~\cite{polca_mas_for_political,richeliey_diplomacy_society}. Allowing agents to strike a balance between cooperative and adversarial behavior makes it's possible to emerge solutions that cannot be suggested by purely cooperative or competitive environments. Nonetheless, mixed-mode interactions also introduce higher complexity in mechanism design, communication protocols, and ethical considerations, as agents must consistently evaluate whether to help or hinder their peers to optimize both individual and collective gains.

\section{System Internal Characteristics}
\subsection{Communication Strategie}
In LLM-MAS, well-defined communication strategies determine how agents exchange information and manage interactions at a micro-level. These strategies specify \emph{when} and \emph{in what sequence} agents speak, often influencing the coherence, scalability, and decision-making quality of the system. More concretely, the choice of strategy can affect whether agents achieve consistent conclusions, how efficiently they converge on solutions, and how robustly they handle token or time constraints. This section introduces three prevalent strategies: One-by-One, Simultaneous-Talk, and Simultaneous-Talk with a Summarizer.

\paragraph{One-by-One:}The One-by-One strategy adopts a strict \emph{turn-taking} mechanism, where agents generate answers in a fixed, sequential order during each communication round. Before responding, the speaking agent incorporates all previous messages—thus preserving context—and then appends its new contribution. Because each agent integrates the entire conversation history, it is less prone to conflicts or contradictory statements within the same round. Hence, it is especially suitable for tasks demanding methodical exploration of ideas, or for scenarios where it is critical to reference and build upon earlier utterances. A typical example is \cite{chain_of_agents}, which chains multiple agents for lengthy text generation; each agent processes the previous agent’s summary to circumvent token limitations. Similarly, ~\cite{dataset_generation} generated a dataset by having agents speak one by one. However, while One-by-One effectively coordinates small numbers of agents, it can become \emph{time-consuming} or \emph{computationally expensive} as the system scales, since each agent must wait for all predecessors to finish. Furthermore, for dynamic systems, a strictly ordered protocol may reduce flexibility.


\paragraph{Simultaneous-Talk:}In contrast, the Simultaneous-Talk strategy allows agents to respond in parallel, eliminating the enforced turn-taking sequence. Rather than waiting for a designated “speaker slot,” each agent asynchronously generates messages during each round, then disseminates them to others. By doing so, the overall conversation unfolds in a more dynamic and unrestricted manner. A key advantage of Simultaneous-Talk is the potential for richer, more diverse brainstorming. When multiple agents express ideas in parallel, the system more rapidly uncovers different perspectives or solution paths. Additionally, fast-paced tasks or real-time systems may benefit from the concurrency inherent in Simultaneous-Talk. Nevertheless, parallel communication can introduce challenges in synchronization and consistency. If each agent only partially or asynchronously receives messages, they might operate with outdated context or produce conflicting information. Moreover, as the number of agents grows, the overhead of coordinating parallel updates becomes nontrivial, potentially leading to concurrency bottlenecks.

\paragraph{Simultaneous-Talk-with-Summarizer:}The Simultaneous-Talk-with-Summarizer strategy builds upon Simultaneous-Talk by introducing a dedicated “summarizer” agent. After each communication round, the summarizer compiles key messages and posts a concise overview back to the group. Incorporating this structured summary ensures that \emph{each} agent remains aligned with the most relevant points in subsequent interactions. By providing a consistent global state of the conversation, the summarizer agent helps prevent misunderstanding or divergence, especially in longer dialogues where it is challenging for each agent to track all parallel messages. This strategy often appears in tasks under a hierarchical or team structure, where designated leaders or summarizers handle task allocation and summarization, such as ~\cite{casualgpt_reasoning,social_media_regulation,agentcoord,polca_mas_for_political}. However, if a summarizer generate hallucination, the entire system may amplify it. Additionally, the process of waiting for the summarizer to generate a summary partially reintroduces sequential elements

\subsection{Communication Paradigm}
Within LLM-MAS, communication paradigms define how information is is represented, transmitted, and interpreted among agents, ultimately shaping the system’s interaction dynamics and performance. By leveraging the advanced language capabilities of LLMs, these paradigms enable rich, contextually nuanced exchanges that extend beyond traditional symbolic messages. Drawing on the taxonomy proposed by \cite{mas_a_survey}, we distinguish three common paradigms: Speech Act, Message Passing, Blackboard.

\paragraph{Speech Act:}The Speech Act paradigm stems from the notion that language serves not only as a medium for information exchange but also as a tool to perform actions. Within LLM-MAS, agents can leverage speech acts to influence each other's beliefs, intentions, or actions. For instance, a speaker agent might generate utterances intended to instruct, request, or persuade another agent, thus altering the system's state or achieving specific goals.This paradigm is particularly relevant in scenarios where agents need to dynamically adjust their behaviors based on the evolving context of the system, such as negotiations~\cite{polca_mas_for_political,richeliey_diplomacy_society}, collaborative problem-solving~\cite{debate_improve_llm_1}, or coordination tasks~\cite{chatdev_software_development}. A major advantage of the Speech Act paradigm is its expressiveness: agents can explicitly encode intentions, facilitating more intuitive cooperation or conflict resolution. However, it also introduces interpretation challenges: if agents misunderstand the utterance, this can lead to confusion or suboptimal decisions. Mitigating such ambiguities often requires well-defined speech act taxonomies and robust context tracking, especially in large-scale, rapidly changing environments.


\paragraph{Message Passing:}The Message Passing paradigm enables direct communication among agents, where information delivered point-to-point or via broadcast. In point-to-point mode, a sender targets a specific recipient, facilitating precise exchanges. Conversely, a broadcast message reaches multiple agents at once, allowing broader dissemination across the system. In LLM-MAS, the content of messages goes beyond simple data packets, often including natural language expressions with embedded reasoning and context. Adhering to common protocols or schemas ensures consistency and interpretability. Consequently, Message Passing excels in scenarios where rapid, efficient information dissemination is paramount.

\paragraph{Blackboard}The blackboard paradigm provides a centralized medium that serves as a common information repository where agents can publish updates, retrieve insights, and coordinate actions. Each agent uploads its findings, thereby making them accessible across the system. In LLM-MAS,  blackboard allows agents to publish status summaries, action proposals, or even textual justifications. This paradigm particularly suits highly coordination systems like collaborative decision-making or distributed problem-solving, where agents require a shared information pool to align strategies and actions. A typical application example is MetaGPT~\cite{metagpt}. This system implements a shared message repository, enabling agents to exchange updates and enhance communication efficiency. However, reliance on a single repository may create bottlenecks or raise security issues, as malicious or faulty contributions on the blackboard can mislead the entire group. Hence, role-based or permission-based mechanisms are often necessary to regulate read/write privileges. 

\subsection{Communication Object}
In LLM-MAS, the communication object denotes the entity or target that an agent interacts with. Its characteristics significantly influence how agents perceive and respond to their environment. In LLM-based settings, this idea becomes even more complex given the multitude of potential partners and the advanced language models governing agent interactions.

The communication object identifies the recipient of an agent's utterances. Understanding different communication objects within LLM-MAS clarifies how information flow affects decision-making, coordination, and goal achievement. This section explores four primary communication objects: communication with self, with agent, with human, and with environment.

\paragraph{Communication with Self:}Communication with self refers to an agent's internal dialogue, encompassing thought processes, planning, and goal reflection. In LLM-based systems, such self-talk is crucial for updating internal states, refining strategies, and reasoning about next steps, effectively mimicking higher-order cognition. For instance, an agent might produce internal summaries, test hypothesis, or even question its own reasoning to refine its behavior.


\paragraph{Communication with Other Agents:}Communication with other agents involves exchanges of information, commands, or requests among different entities in the system. In LLM-MAS, this interaction is crucial for achieving collaborative goals, resolving conflicts, and disseminating information. For instance, agents might share intermediate task results, negotiate resource allocation, or propose performance improvements.  Because LLM-powered agents can exchange complex linguistic constructs, their inter-agent communication is highly adaptive and dynamic. 

\paragraph{Communication with Environment:}Communication with the environment concerns how agents receive and adapt to external surroundings, including various forms of feedback and stimuli. The environment can encompass a range of factors, from physical conditions~\cite{embodied_agents} to fluctuations system states and external data inputs. In LLM-MAS, agents use their language models to interpret environmental signals through natural language processing and contextual understanding. When agents receive environmental feedback, they integrate this new information into their reasoning processes. This feedback loop enables agents to adapt strategies dynamically, re-evaluate decisions, and response appropriately as conditions evolve.

\paragraph{Communication with Human:}Communication with humans entails the interactions between agents and human participants. While LLM-MAS often focus on agent-to-agent communication, introducing humans as communication objects adds another layer of complexity. Human participants may issue commands, offer feedback, or request specific actions, requiring agents to interpret and respond appropriately in natural language. For instance, in PeerGPT~\cite{peergpt}, agents act as both team moderators and participants in collaborative learning with children, continuously processing children’s words and actions to refine the agents’ behavior. This highlights how LLM-MAS effectively integrate with human-driven processes.

\subsection{Communication Content}
Communication content refers to the actual information being exchanged between agents, which may be explicit or implicit. The nature of this content influences how agents interpret and act upon messages. In LLM-MAS, content can be explicit, where information is conveyed clearly and structurally, or implicit, where meaning is inferred from actions, feedback, and environmental signals.

\paragraph{Explicit communication:}Explicit communication entails direct information exchange with clearly defined and easily interpretable meaning, which proves especially useful for tasks demanding precise coordination or information sharing. Explicit communication is divided into 1) \textit{natural language} and 2) \textit {code and structured data}. \textbf{Natural language} reamins the most common form of explicit communication in LLM-MAS. These exchanges range from basic commands to advanced conversations where agents share detailed observations, propose ideas, or explain reasoning. Such flexibility empowers agents to convey rich, context-sensitive information and adjust language to the required level of formality or specificity. Besides natural language, explicit communication may encompass \textbf{code and structured data}. Such formats prove invaluable when agents must exchange precise instructions, configuration details, or factual information in a standardized manner. Structured communication is especially critical for planning, data retrieval, or algorithmic problem-solving, where clarity and consistency are paramount

\paragraph{Implicit communication:}Implicit communication, on the other hand, occurs when agents convey information indirectly, through their actions or environmental cues, rather than through explicit statements. Implicit communication relies on agents interpreting contextual cues or feedback. Implicit communication is divided into 1) \textit{behavioral feedback} and 2) \textit{environmental signal}. \textbf{Behavioral feedback} occurs when agents communicate information to others—or themselves—through actions or strategy adjustments, without explicit verbal cues. In LLM-MAS, these signals are woven into ongoing interactions. This feedback loop enables agents to adapt dynamically, aligning with system goals or adjusting based on outcomes. In evolving environments or collaborative tasks, behavioral feedback fosters shared understanding and effective coordination. \textbf{Environmental signals} represent another implicit communication channel, where environmental changes provide indirect feedback that shapes agent decisions and behavior. Such signals may include sensor data, system status updates, or contextual shifts relevant to an agent’s tasks. In LLM-MAS, agents are equipped to parse these signals and incorporate them into their continuous decision-making processes.

\section{Challenges and Opportunities}
As LLM-MAS continue to garner increasing attention in both research and application domains, their further development faces several significant challenges and emerging opportunities. We analyze several key challenges and research directions based on the content of the article.
\subsection{Optimizing Design}
The communication architecture is the foundational component of LLM-MAS. As task complexity increases, traditional communication architectures may no longer suffice. Therefore, the design of hybrid architectures is expected to be a key focus of future research. With more complex structures, the number of agents increases, leading to greater demands on computational resources. Therefore, a major challenge lies in developing communication paradigms that are both efficient and scalable, while also optimizing the allocation of computational resources. Concurrently, the increasing volume of internal system information poses another challenge. Ensuring that agents correctly interpret and understand this information, while minimizing the risk of hallucinations or misunderstandings, will be a crucial area of investigation.
\subsection{Advancing Research on Agent Competition}
In a competitive environment, agents can develop more complex strategies, improve decision-making, and promote innovative behaviors by employing techniques such as game theory. However, a key challenge lies in balancing competition and cooperation, as excessive competition may lead to inefficiency or instability. Future research can focus on finding the optimal balance between competition and cooperation, developing scalable competition strategies, and exploring how to safely and effectively integrate competition into real-world applications.
\subsection{Communicate Multimodal Content}
With the development of large multimodal models, agents in LLM-MAS should not be limited to text-based communication. Communication of multimodal content (text, images, audio, and video) should also be considered.This expansion into multimodal content enables more natural and context-aware interactions, thereby enhancing agents' adaptability and decision-making capabilities. However, there are some challenges to integrating multimodal content. A major issue is how to effectively present and coordinate different modalities in a coherent way that is comprehensible to all agents. In addition, agents not only have to process these different modalities, but also communicate them effectively to one another. Future research should focus on improving the fusion of multimodal data and designing stronger agents in key components for handing multimodal content. 
\subsection{Communication Security}
As LLM-MAS becomes more complex and integrated into real-world applications, ensuring the confidentiality, integrity, and authenticity of this communication is essential for maintaining the system's overall security and functionality. One of the main challenges of communication security is protection against malicious attacks such as eavesdropping, data tampering, or spoofing. A malicious attacker may exploit vulnerabilities in the communication protocol to inject misleading or harmful content. Furthermore, as agents interact in an increasingly open environment, the risk of unauthorized access to communication channels and data increases. Future research should focus on developing encryption and authentication protocols tailored for decentralized multi-agent environments. Another key challenge is integrating secure communication protocols that can adapt to the dynamic nature of LLM-MAS, where agents frequently join or leave the system. By addressing these security challenges, we can enable LLM-MAS to operate safely in scenarios that require higher security, such as autonomous vehicles and healthcare. 
