\section{Introduction}
Large language models (LLMs) have recently demonstrated significant potential across diverse fields. Meanwhile, LLM-based agents—centered on an LLM and equipped with profiling, memorization, planning, and action modules~\cite{llm_agent_define}—have attracted considerable attention. Researchers have enhanced a single agent’s perceptual, reasoning, and planning capabilities by providing it with reflection strategies~\cite{react} or prompting it to think in different ways~\cite{COT,TOT}. However, when presented with more complex and dynamic tasks, LLM-based single-agent systems often struggle to achieve satisfactory results~\cite{single_limit}.

To address these limitations, researchers have proposed LLM-based multi-agent systems (LLM-MAS). In such systems, agents with distinct profiles and functions interact and coordinate to accomplish tasks beyond the capabilities of a single agent. Recent studies have demonstrated the effectiveness of LLM-MAS in various areas, including social simulation~\cite{social_media_regulation}, code generation~\cite{metagpt}, and recommendation systems~\cite{jd_recommendation_system}. Given their broad application scope and developmental potential, experts from diverse fields are actively investigating LLM-MAS, resulting in a rapid increase in related publications. Likewise, several surveys have recently begun analyzing LLM-MAS from multiple perspectives, helping researchers to quickly grasp this emerging research area.

Previous surveys \cite{agent_survey_2} focuses on the components of an agent, including brain, perception, and action modules. However, for LLM-MAS, it only discusses two types of interactions: cooperative and adversarial. Similarly, \cite{agent_survey_1} examines the composition of LLM-based agents and highlights prospective applications. Although it addresses relationships among agents, task planning, and methods for enhancing efficiency, it does not elaborate on the composition of LLM-MAS architectures, interaction strategies among agents, or other relevant features. Subsequent works \cite{mas_survey_3,mas_survey_2} further summarize LLM-MAS. The former introduces LLM-MAS structures (equal-level, hierarchical, nested, dynamic) and emphasizes current challenges. The latter describes interfaces, profiling, and communication structures, but focuses primarily on application scenarios. Both discussions are largely confined to surface-level features such as system architecture but without providing sufficiently detailed insights into the internal workflows. In contrast, \cite{mas_survey_1} does introduce an LLM-MAS workflow, but from a single-agent perspective in the system—covering agent roles, action classifications, and organizational structures. While informative, this single-agent viewpoint cannot fully illustrate multi-agent interactions and workflows, particularly regarding communication paradigms and strategies. Other surveys target specific domains, such as \cite{mas_application_survey_simulation} for simulation and emulation, or \cite{mas_application_survey_1,mas_application_survey_2} for software engineering. Although these works offer valuable insights within their respective fields, they are less generalizable for broader applications.

Based on the above overview and analysis, existing discussions of LLM-MAS workflows remain insufficient. How to synthesize these workflows into a more detailed framework applicable across diverse tasks is the primary aim of this paper. We observe that a key distinction between LLM-MAS and single-agent systems is inter-agent communication, which enables agents to exchange ideas and coordinate plans. Inspired by ~\cite{communication_1,communication_2}, we propose decomposing the LLM-MAS workflow from the perspective of communication characteristics, distinguishing between system-level communication characteristics and system internal communication characteristics. This macro-to-micro viewpoint helps address the limitations of previous studies that focus on isolated features such as architecture or content. Specifically, we define LLM-MAS as an automated system driven by communication goals within a predefined communication architecture. Agents in this system have multiple communication strategies and paradigms, interacting with various communication objects to exchange diverse content to for task completion.


The remainder of this paper is organized as follows:

\section{Background}
\subsection{Single-Agent Systems}
Single-agent systems represent the cornerstone of multi-agent architectures, as they lay the groundwork for understanding how individual agents reason, plan, and act. In line with the taxonomy proposed by ~\cite{single_agent_survey}, this section outlines the composition and functionalities of LLM-based single-agent systems, providing the foundational concepts needed before delving into multi-agent scenarios.

An LLM-based single-agent system typically consists of three key components: Brain, Perception, and Action, each playing a distinct yet complementary role.

\paragraph{Brain:}At the core of an LLM-based agent is the large language model, enabling natural language understanding and generation. To mitigate potential hallucinations, short-term and long-term memory modules are often integrated to retain contextual information and enhance the agent’s reasoning and planning capabilities.

\paragraph{Perception:}Both traditional LLMs and multimodal LLMs can constitute LLM-based agents, allowing them to process textual, visual or auditory inputs. This broad perceptual space range equips the agent to make more informed decisions and adapt to diverse task requirements.

\paragraph{Action:}The agent's ability to invoke external tools such as web search, software applications, or APIs enhances its problem-solving capacity. Additionally, embodied agents \cite{embodied_agents}, which integrate with the physical world through sensors and actuators, can perform tangible actions in real-world environments, further broadening the scope of single-agent systems.

\subsection{Multi-Agent Systems}
Although LLM-based single-agent systems have demonstrated strong individual reasoning and decision-making capabilities, they remain limited in tasks that demand collective intelligence or large-scale simulations. In such scenarios, multi-agent systems, which are complex frameworks composed of multiple interacting intelligent agents ~\cite{mas_define}, are particularly well suited.

By leveraging the independent decision-making abilities of individual agents while supporting intricate inter-agent interactions, multi-agent systems facilitate both cooperative and competitive decision processes at scale. Crucially, communication serves as the primary mechanism through which agents exchange information, align objectives, and adapt their strategies.

Depending on the specific tasks and agents involved, communication characteristics such as communication architectures and strategies can vary widely, allowing for tailored approaches to an array of complex challenges. This flexibility sets the stage for understanding how LLM-based multi-agent systems (LLM-MAS) can further advance various application domains.

\section{System-Level Communication Characteristics}
Multiple autonomous agents in LLM-based Multi-Agent Systems (LLM-MAS) often mirror human organizational structures by distributing roles and interacting to achieve shared objectives. As these systems increase in complexity, understanding the overall flow of communication and coordination becomes critical for their success.

To address this need, we present a comprehensive and unified framework for LLM-MAS that centers on communication characteristics. This framework encompasses two core system-level aspects—communication architecture and communication goal—alongside four key system-internal attributes: communication strategy, communication paradigm, communication object, and communication content. Together, these elements form an integrated perspective on how LLM-based multi-agent systems operate and interact at a system-wide level.

\subsection{Communication Architecture}
Communication architecture in LLM-MAS specifies the fundamental structure governing how agents are organized, how they interconnect, and how information flows among them ~\cite{mas_a_survey}. Serving as a foundational layer, it dictates both the macro-level design and the micro-level interactions of the entire system. A robust architecture not only ensures cohesive operation for small-scale tasks with a few agents, but also scales to complex scenarios involving numerous agents.

In the context of LLM-MAS, communication architecture can vary significantly depending on the scale, the level of autonomy of the agents, and the nature of their interactions. This section explores various communication architectures that have been implemented in such systems, categorizing them based on the relationship between agents, the distribution of information, and the flatness or hierarchy of the system.

\subsubsection{Flat Architecture}

Flat architecture in LLM-based multi-agent systems denotes a decentralized structure where agents operate at the same level, without a central controller or task hierarchy. In this egalitarian setting, all agents are autonomous, communicate on equal footing, and can collaboratively (or competitively) contribute to shared objectives.

Such a flat design lends itself well to highly dynamic communication and is especially suitable for smaller systems that demand flexibility, direct interactions, and simplicity. Moreover, its scalability allows new agents to join without disrupting the existing communication structure.

For instance, ~\cite{dataset_generation} employs a peer-to-peer manner, where agents collaboratively retrieve and optimize data in a flat structure to goal-oriented synthetic dialogue generation tasks.

Beyond dialogue generation, multi-agent reasoning and decision-making tasks also benefit from the decentralized nature of flat architectures. ~\cite{debate_improve_llm_1} investigates multi-agent fact-checking in a flat system where agents openly critique each other’s reasoning, thereby reducing hallucinations and errors often seen in isolated models. Similarly, ~\cite{evaluating_llm_game_ability} studied a multi-agent game environment in a flat structure, allowing each agent to independently adapt its own strategy in response to other agents due to the lack of hierarchical control, and adjusts its strategy in real time based on  peer behavior. This real-time adjustment fosters a dynamic interactive system that leads to more robust decision-making through iterative learning among agents.

Finally, ~\cite{simulating_opinion_dynamic} highlights how a flat architecture enables the simulation of complex social behaviors in opinion dynamics. In this decentralized communication environment similar to real-world social systems, agents can communicate directly to influence and adjust each other's beliefs, leading to consensus or polarization, validating that the absence of central authority underscores the impact of open information sharing on community opinion and collective behavior.

\subsubsection{Hierarchical Architecture}

Contrasting with the flat design, the hierarchical architecture of LLM-MAS arranges agents in a tree-like structure where higher-level agents supervise and guide the tasks of lower-level ones. This clear division of roles is particularly suitable for complex tasks requiring specialization and coordination, enabling each agent operates within a defined scope while top-level agents oversee overall strategy and decision-making.

In the field of knowledge-based reasoning, hierarchical systems have shown their effectiveness. In the CausalGPT framework ~\cite{casualgpt_reasoning}, low-level agents provide solutions to problems, while high-level evaluators carefully check the consistency and logical validity of these solutions to enhance the reasoning process.

Hierarchical frameworks also perform well in applications in different professional fields: ~\cite{chatdev_software_development} designed a ChatDev software development framework, where professional agents follow a structured communication process. Under the coordination of the senior agent, each agent focuses on autonomously managing different stages of tasks including design, coding, and testing to ensure that each stage of the development process meets the project goals. 
~\cite{autodefense_against_jailbreak} designed the AutoDefense multi-agent harmful prompts and jailbreak attack defense framework. In this framework, under the control of the senior agent, some agents focus on filtering harmful content, while others are responsible for processing response evaluation. This hierarchical setup spreads the defense pressure of a single agent, ensuring that the system can adapt to various attack scenarios and remain resilient. 
Financial decision-making systems also greatly benefit from hierarchical architectures. In the FINCON ~\cite{fincon_decision_making} framework, the system is designed to simulate the organizational structure of a financial company,with lower-level agents focusing on information gathering and analysis, while managers coordinate the overall strategy and decision-making process. By incorporating a communication layer between managers and analysts, the system ensures that agents collaborate effectively towards a unified goal, thereby improving the quality of financial decisions and optimizing resource allocation. 
In scenarios where agents must interact with large amounts of information, such as in simulations that manipulate knowledge ~\cite{community_knowledge_flooding}, hierarchical structures can better provide control over how information propagates. Higher-level agents can supervise the flow of information to ensure that only valid and reliable knowledge is passed on to lower-level agents. This simulation provides a reliable way to control knowledge dissemination in multi-agent communities.

The hierarchical architecture provides a powerful framework for LLM-based multi-agent systems, making task execution more specialized, coordinated, and efficient. However, hierarchical organization may cause delays or bottlenecks, especially at the high-level agent, which is responsible for handling the communication of all subagents.

\subsubsection{Team Architecture}

In a team architecture for LLM-based multi-agent systems, agents are combined into cohesive teams that work together to solve complex tasks. These architectures take advantage of the different expertise of each agent and assign tasks based on the strengths and expertise of team members. Team goals can be updated, resulting in changes in the responsibilities, roles, and permissions of agents in the team. Each team can request information from agents in other teams to improve their own decision-making processes. This division of labor ensures that each agent focuses on specific aspects of the task, while communication and coordination between agents facilitate efficient problem solving. Team architectures are particularly advantageous for tasks that require both expertise and coordinated teamwork to achieve complex goals.

A prominent example of a team-based architecture is MetaGPT ~\cite{metagpt}, which introduces a metaprogramming framework for multi-agent collaboration in software engineering. In this system, specialized agents are assigned different roles based on standardized operating procedures (SOPs) that simplify workflows. Agents with specialized domain knowledge, such as design or debugging experts, collaborate to solve complex software engineering problems. 
Similarly, ~\cite{magis_mas_for_github} designed the MAGIS framework for software development, which consists of four types of agents: administrators, repository keepers, developers, and quality assurance engineers. By assigning these roles and collaborating with different sub-team members, MAGIS is able to solve GitHub issues more efficiently than traditional single-agent systems. 
The flexibility of team-based architectures is also demonstrated by the AgentCoord framework ~\cite{agentcoord}, which focuses on designing coordination strategies for multi-agent systems. AgentCoord enables users to explore various coordination strategies through a visual interface, allowing users to iteratively refine the interactions between agents based on collective goals. This interactive system uses LLM to generate and refine coordination strategies to ensure that each agent is aligned with the team's goals. The ability to intervene and modify coordination strategies enhances the user's ability to optimize the collaboration process and facilitates more effective teamwork in complex environments. 
The POLCA framework ~\cite{polca_mas_for_political} extends team-based architectures to political coalition negotiations. In this system, agents represent political parties that participate in negotiations to form coalitions. The framework models the negotiation process as a series of interactions between agents, each representing the interests of a specific party. POLCA simulates negotiation dynamics and predicts potential coalition outcomes. This approach highlights the applicability of team-based architectures in domains that require strategic negotiation and coordination between multiple agents with different goals.

\subsubsection{Society Architecture}

In a society-based architecture, multiple agents work in a shared environment, and their interactions shape and evolve the entire system. Agent societies are essentially open systems that are designed to simulate complex social dynamics, often simulating real-world social processes such as cooperation, conflict resolution, and the formation of social contracts. Agents in these systems may have different roles and motivations, but they cooperate or compete to achieve collective goals or individual ambitions, reflecting the dynamics of human society. A distinctive feature of societies is the set of constraints they impose on the behavior of subjects, often called social laws, norms or conventions, which provide a specific set of norms for the subjects within the society. These are rules or guidelines that agents must follow, providing a level of behavioral and interface consistency designed to promote coexistence. The framework is particularly useful for studying large-scale, emergent behavior in social, economic, or political settings because collective outcomes result from individual decisions.

~\cite{stanf_villege} is the first framework to simulate human behavior in a sandbox environment. The generative agents in this system can not only perform routine actions such as cooking, going to work, and socializing, but they can also remember and reflect on past experiences to plan future actions. By simulating interactions such as organizing gatherings, agents can autonomously create social activities, form relationships, and adjust their actions based on past experiences to generate new social behaviors.
Similarly, ~\cite{govsim} explored how LLM agents cooperate or exploit common resources in a simulated society. This setting introduces ethical dilemmas and strategic decisions focusing on the sustainability of collective action. This simulation demonstrates the feasibility of using LLM to study the basic mechanisms of cooperation, negotiation, and collective decision-making in social settings.
~\cite{sct_society} expanded the concept of social contract and explored the evolution of social structure from the perspective of Hobbesian social contract theory. In this simulation, LLM agents begin in a “state of nature” characterized by conflict, but as agents interact and social contracts form, they establish a peaceful society governed by sovereign institutions. This development shows the potential of agent systems to simulate political, economic, and social structures.

At the same time, in addition to social simulations, there are currently some simulations for more specific social activities such as economic activities and diplomatic activities. EconAgent~\cite{econagent} adopts a social framework that emphasizes macroeconomic activities, where agents represent households and firms within a dynamic economy. This system integrates market dynamics with agents' personal experiences to simulate realistic economic behaviors such as consumption, work, and investment. By incorporating memory and diverse decision-making processes, EconAgent provides insights into the impact of individual actions on large-scale economic phenomena.
~\cite{richeliey_diplomacy_society} introduce a society-based architecture designed for AI diplomacy. In this framework, LLM agents engage in multi-step negotiations, strategic planning, and social reasoning to solve complex diplomatic challenges. These agents evolve through self-play, enhancing their strategic decision-making autonomously. This process yields agents adept at long-term, goal-oriented diplomacy, illustrating the potential of multi-agent collaboration to model social structure and diplomacy.

\subsubsection{Hybrid Architecture}

Not all designs fit neatly into a particular architecture, and some architectures may contain features from several different styles. Therefore, architectures can be transformed into hybrid architectures through overlapping, nesting, and other means. Hybrid architectures are designed to complement each other, adapt to a wider range of scenarios, and arrange agents to achieve more complex, flexible, and efficient problem solving.

FixAgent ~\cite{fixagent_mas_for_debug} is a prominent example of a hybrid architecture that leverages multi-agent collaboration to solve the challenge of software debugging. Unlike previous approaches that focus on isolated debugging steps, FixAgent uses a three-level agent architecture inspired by human cognitive processes. Three different architectures are juxtaposed to adaptively handle errors of varying complexity.
In the field of autonomous driving, ChatSim ~\cite{chatsim_mas_make_scene} introduces an innovative hybrid scenario simulation approach. ChatSim hybridizes hierarchical and team architectures to enable editable, photo-realistic 3D simulations through natural language commands, leveraging the collaboration of LLM agents to handle different tasks: understanding language commands, rendering realistic scenes, and integrating external assets into the simulation. Through the hybrid architecture, ChatSim overcomes the limitations of previous scene simulations.
~\cite{mas_use_tool} provides another example of a hybrid architecture that allows agents to autonomously leverage a large library of tools to solve tasks. Unlike traditional systems that encode all available tools in a prompt, this architecture dynamically searches for the most relevant tools and adjusts strategies as needed. This hybrid approach that integrates search, tool management, and task execution into a single framework reduces reasoning costs, enhances flexibility, and improves the ability of agents to scale across a variety of applications.
Agent4Debate ~\cite{agent4debate} introduces another dynamic hybrid system for competitive debates. In this framework, four specialized agents—researchers, analysts, writers, and commentators—collaborate at various stages of the debate, from research and argument formation to rebuttal and summary. Each agent plays a specific role, but their interactions form a cohesive and competitive whole. The flexibility of the system allows it to dynamically adjust its strategy in real time, just like a human debater.
~\cite{shallwetalk} studied spontaneous collaboration among competing LLM agents. While many multi-agent systems rely on structured interactions, this approach emphasizes cooperation that emerges in a competitive environment without explicit instructions. This highlights the potential of hybrid architectures to support both competitive and cooperative behaviors in dynamic systems, thereby increasing their applicability to a range of real-world situations.

\subsection{Communication Goal}
Having defined the overarching communication architectures, we now turn to communication goals. These goals determine whether agents collaborate, compete, or adopt a mixed approach, and significantly shape the behaviors, information-sharing patterns, and coordination strategies within the system, as they encapsulate the “why” behind the communication.

\subsubsection{Cooperation}

Cooperation in LLM-based multi-agent systems plays a crucial role in enabling intelligent agents to effectively work together towards shared goals. Unlike traditional multi-agent systems, LLMs bring an added dimension to cooperation by providing advanced natural language understanding and generation capabilities, enabling more sophisticated and flexible forms of interaction. Cooperation within these systems can be categorized into two main types: 1) \emph{Direct Cooperation} and 2) \emph{Cooperation Through Debate}. Each type offers unique advantages in problem-solving, decision-making, and task execution.
\paragraph{Direct Cooperation:}

In the context of LLM-based systems, direct cooperation refers to the collaborative interaction among multiple agents through seamless information sharing, coordinated actions, and aligned objectives towards a common goal. In this cooperative framework, LLM-powered agents utilize their advanced natural language processing capabilities to communicate effectively and synchronize their activities. The employment of natural language facilitates more intuitive and flexible communication, enabling agents to clearly articulate their plans, request clarifications, and negotiate task assignments. This dynamic and adaptive interaction stands in contrast to traditional rule-based systems, offering greater flexibility and responsiveness in achieving collective objectives.

There are already extensive studies proving the effectiveness of direct cooperation. Within established frameworks, agents can fully accept each other's messages and leverage their individual capabilities to the fullest extent. It has been applied in various fields such as improving LLM capabilities~\cite{boostrapping,casualgpt_reasoning,autoagents}, code writing~\cite{chatdev_software_development,soa_code_generation,magis_mas_for_github,mas_for_software_2,fixagent_mas_for_debug}, scientific experiments~\cite{chatsim_mas_make_scene,mas_use_tool}, and world simulation~\cite{fincon_decision_making,classroom_simulation,peergpt}.

In addition, the scientific community continues to work on studying diverse cooperation models in order to achieve optimal direct collaboration between multiple agents.~\cite{agentcoord,govsim,mas_benchmark}

\paragraph{Cooperation Through Debate:}

Cooperation through debate in Large Language Model (LLM)-based systems introduces an additional layer of complexity by enabling agents to engage in structured dialogues where they present and critique differing perspectives rather than cooperating immediately. Although this process may appear confrontational, its primary objective is to refine ideas, resolve ambiguities, and enhance the collective decision-making process through rigorous debate.

In debate-driven cooperation, LLM-powered agents leverage their capacity to generate coherent and contextually rich arguments in natural language. This capability allows agents to challenge each other’s assumptions, identify potential flaws, and iteratively improve their proposed actions. Such an approach is particularly advantageous in scenarios where the optimal solution is not readily apparent or when the problem domain necessitates a thorough exploration of diverse viewpoints.

This form of cooperation is especially valuable for complex decision-making tasks, including factual reasoning~\cite{debate_improve_llm_1}, and fact checking~\cite{debate_2}, where the integration of multiple perspectives is essential. The proficiency of LLMs in handling nuanced language and simulating intelligent argumentation renders debate an effective strategy for deep collaboration. This is particularly true when agents are required to navigate multifaceted problems that demand more than mere consensus, facilitating a more robust and well-rounded decision-making process.

\subsubsection{Competition}

Competition in Large Language Model (LLM)-based multi-agent systems emerges when agents possess conflicting objectives or compete for shared resources, goals, or outcomes. Unlike cooperation, where agents synchronize their actions to achieve a collective objective, competition introduces a dynamic environment where agents must strategize and occasionally outmaneuver one another to maximize individual benefits or attain dominance within the system.The game-playing ability of agents in the system has been demonstrated in research. ~\cite{evaluating_llm_game_ability}

In the realm of LLM-based systems, competition exhibits distinctive characteristics attributable to the advanced linguistic capabilities of the agents. LLMs empower agents to engage in sophisticated competitive interactions, utilizing natural language not merely for information exchange but also for influencing, persuading, and even deceiving other agents. This linguistic proficiency renders the competitive dynamics in these systems more flexible and intricate compared to traditional multi-agent frameworks. ~\cite{social_media_regulation} demonstrated it by studying the evolution of language using multi-agent simulation. ~\cite{agent4debate} designed a dynamic multi-agent framework for competitive debate with human.

One of the key benefits of LLMs in competitive scenarios is their ability to adapt to changing conditions. As agents interact with each other and receive feedback from the environment, they can modify their language and strategies, continuously optimizing their approach to outpace their competitors. This constant adaptation, combined with the ability to generate creative and nuanced linguistic responses, allows for a dynamic and evolving competitive landscape.~\cite{mas_for_poetry_generation} attempted to use a competitive environment for the poetry generation task

Overall, competition in LLM-based multi-agent systems is a powerful mechanism that drives agents to refine their strategies, improve their performance, and navigate complex decision-making environments. The advanced language capabilities of these agents facilitate a deeper and more intricate form of competition, where communication itself becomes a tool for gaining an advantage.


\subsubsection{Mix}
In many real-world scenarios, multi-agent systems operate under mixed modes of interaction, where agents both cooperate and compete depending on the context. This hybrid approach mirrors the complexity of human societies and is particularly relevant in LLM-based multi-agent systems.

The mixed model is essential for capturing the nuanced dynamics of complex, dynamic environments. For LLM-based agents, this means that communication, negotiation, and decision-making strategies are not rigidly confined to either cooperative or competitive frameworks. Instead, these strategies adapt fluidly based on the agents' current needs and objectives, enabling more sophisticated and context-sensitive interactions.

In practice, mixed cooperation and competition can be seen in applications such as social simulations, collaborative problem-solving, and negotiation scenarios. ~\cite{polca_mas_for_political} used LLM-MAS to build a simulated political alliance framework, in which agents use Markov decision making and decide their relationships with other agents through their own thinking, completing the simulation modeling of human politics. ~\cite{richeliey_diplomacy_society} has studied the self-evolution of agents in complex diplomatic processes. ~\cite{community_knowledge_flooding} and ~\cite{sct_society} respectively modeled and analyzed social networks and human behavior in the real world, providing theoretical references for managers to specify normal.

Furthermore, this mix of cooperation and competition allows for emergent behaviors that can optimize the performance of the entire system. By allowing agents to interact in flexible ways, a mixed framework fosters dynamic adaptation to changing circumstances, leading to more robust and resilient outcomes. Agents can dynamically balance their efforts between collaboration and rivalry, maximizing both collective and individual outcomes in a fluid, responsive environment. ~\cite{mas_for_defence_attack} created a platform with attack agents and defense agents, aiming to continuously improve the ability of the defense agent through the confrontation game between the two parties to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent.

\section{System Internal Communication Characteristics}
\subsection{Communication Strategie}
In LLM-based multi-agent systems, effective communication relies on well-designed strategies that specify how agents control and implement interactions in accordance with the system’s framework and goals. Such strategies encompass communication processes, sequencing, and control methods.

Beyond determining the order and timing of interactions, these strategies also influence the system's ability to achieve consistent conclusions, uphold coherence, and tackle complex decision-making scenarios.

\subsubsection{One-by-One}
The One-by-One strategy is a sequential communication method in which agents take turns generating utterances during each round. In this strategy, the responses order is fixed in advance, and each agent considers the conversation's current state including prior contributions before replying. This setup resembles traditional debates or discussions, where participants listen to preceding ideas before offering their own.

In each round, an agent is prompted to respond by integrating all previous statements in the conversation and concatenating them into its input, ensuring it retains the full context. This design helps maintain coherence and avoid contradictions. The One-by-one strategy proves especially effective in scenarios where a clear and structured conversation flow is crucial, or when agents must reference all prior exchanges.

For instance, ~\cite{chain_of_agents}applies this chain-like structure to long-text tasks, having agents speak in turn while each processes the previous agent’s summary to circumvent token limitations in LLMs. Similarly, ~\cite{dataset_generation} generated a dataset by having agents speak one by one.

\subsubsection{Simultaneous-Talk}
The Simultaneous-Talk strategy contrasts with One-by-One by enabling agents to generate responses in parallel, forgoing a strict turn-taking sequence. Rather than following a predetermined order, each agent asynchronously generates its response during each iteration, mitigating the influence of speaking order on the overall conversation.

This parallel mechanism fosters a more dynamic and flexible interaction model, allowing agents to simultaneously engage with one another's ideas. By generating responses in real time, the system benefits from a broader diversity of inputs, yielding a fluid, collaborative exchange of information. The Simultaneous-Talk strategy proves especially helpful when rapid responses are required or multiple perspectives must be explored such as world simulation~\cite{simulating_opinion_dynamic,classroom_simulation,richeliey_diplomacy_society}. However,  it can also increase operational overhead and complicate scaling, especially in large systems, as numerous parallel processes must be synchronized efficiently.

\subsubsection{Simultaneous-Talk-with-Summarizer}
The Simultaneous-Talk-with-Summarizer strategy expands the Simultaneous-Talk model by introducing a dedicated "summarizer" agent. At the end of each iteration, once all controlled agents have produced their responses, the summarizer compiles the key contributions and offers a concise overview of the discussion. Then the resulting summary is incorporated into each agent's chat history, ensuring everyone remains aligned and informed. 

The introduction of a summarizer adds an extra layer of organization to the interaction, particularly in longer or more complex discussions, where it may be challenging for agents to track all points in detail without a high-level overview. This strategy helps agents stay aligned and informed about the ongoing discussion, allowing them to adjust their contributions based on the most relevant points raised by others. 

This strategy often appears in tasks under a hierarchical or team structure, where designated leaders or summarizers handle task allocation and summarization, such as ~\cite{casualgpt_reasoning,fincon_decision_making,social_media_regulation,agentcoord,polca_mas_for_political,recon_thinking}

\subsection{Communication Paradigm}
Communication paradigms define the ways information is represented, transmitted, and interpreted among agents in an LLM-based multi-agent system, thereby shaping interaction dynamics and overall performance. They serve as the implementation layer for actual communication, capitalizing on LLMs’ advanced language capabilities to enable rich and detailed exchanges.According to ~\cite{mas_a_survey}, three widely used paradigms for communication are: speech act, message passing, blackboard.

\subsubsection{Speech Act}
The Speech Act paradigm stems from the notion that language serves not only as a medium for information exchange but also as a tool to perform actions. Within LLM-MAS, agents can leverage speech acts to influence each other's beliefs, intentions, or actions. For instance, a speaker agent might generate utterances intended to instruct, request, or persuade another agent, thus altering the system's state or achieving specific goals.

LLMs greatly strengthen this paradigm by generating context-aware, semantically rich utterances. Such utterances span a variety of functions from directives("You should prioritize task A") to commitments ("I will handle task B"). Because LLMs align syntax and pragmatics with the target audience, they enable effective communication in dynamic contexts. This paradigm is particularly relevant in scenarios where agents need to dynamically adjust their behaviors based on the evolving context of the system, such as negotiations~\cite{polca_mas_for_political,richeliey_diplomacy_society}, collaborative problem-solving~\cite{debate_improve_llm_1}, or coordination tasks~\cite{chatdev_software_development}. 

\subsubsection{Message Passing}
The Message Passing paradigm enables direct communication among agents, where information delivered point-to-point or via broadcast. In point-to-point mode, a sender targets a specific recipient, facilitating precise exchanges. Conversely, a broadcast message reaches multiple agents at once, allowing broader dissemination across the system.

In LLM-MAS, the content of messages goes beyond simple data packets, often including natural language expressions with embedded reasoning, context, or even persuasive elements. Adhering to common protocols or schemas ensures consistency and interpretability. Consequently, Message Passing excels in scenarios where rapid, efficient information dissemination is paramount.

\subsubsection{Blackboard}
The Blackboard paradigm provides a centralized medium where agents collaboratively share and access information. A common repository—the “blackboard”—functions as a communication hub where agents post updates, retrieve insights, and coordinate actions. Each agent uploads its findings, thereby making them accessible across the system.

In LLM-MAS,  this paradigm leverages agents’ NLP capabilities, allowing the blackboard to handle unstructured or semi-structured text. Agents may publish status summaries, action proposals, or even textual justifications. To maintain coherence and secure data access, role-based or permission-based controls can be enforced, effectively regulating agent interactions with the shared repository

This paradigm particularly suits highly coordination systems like collaborative decision-making or distributed problem-solving, where agents require a shared information pool to align strategies and actions. A typical application example is MetaGPT~\cite{metagpt}. This system implements a shared message repository, enabling agents to exchange updates and enhance communication efficiency. 

\subsection{Communication Object}
In LLM-MAS, the communication object denotes the entity or target that an agent interacts with. Its characteristics significantly influence how agents perceive and respond to their environment. In LLM-based settings, this idea becomes even more complex given the multitude of potential partners and the advanced language models governing agent interactions.

The communication object identifies the recipient of an agent's utterances. Understanding different communication objects within LLM-MAS clarifies how information flow affects decision-making, coordination, and goal achievement. This section explores four primary communication objects: communication with self, with agent, with human, and with environment.

\subsubsection{Communication with Self}
Communication with self refers to an agent's internal dialogue, encompassing thought processes, planning, and goal reflection. In LLM-based systems, such self-talk is crucial for updating internal states, refining strategies, and reasoning about next steps, effectively mimicking higher-order cognition.

In LLM-MAS, agents often engage in self-reflection by generating language-based responses that analyze their current situation, predict future outcomes, or evaluate past actions. For instance, an agent might produce internal summaries, test hypothesis, or even question its own reasoning to refine its behavior. Such internal communication supports higher-order cognitive processes, such as decision-making, planning, and problem-solving. By leveraging advanced language generation, agents can structure their thoughts to navigate complex scenarios, making self-communication a vital component of intelligent agent behavior.

\subsubsection{Communication with Other Agents}
Communication with other agents involves exchanges of information, commands, or requests among different entities in the system. In LLM-MAS, this interaction is crucial for achieving collaborative goals, resolving conflicts, and disseminating information. Such communication may vary from cooperative dialogues to competitive exchanges, depending on the system’s structure and objectives.

For instance, agents might share intermediate task results, negotiate resource allocation, or propose performance improvements.  Because LLM-powered agents can exchange complex linguistic constructs, their inter-agent communication is highly adaptive and dynamic. Agents can instantly react to peers’ behaviors, revise strategies, and coordinate efforts in real time, enabling a fluid, responsive environment.

\subsubsection{Communication with Environment}
Communication with the environment concerns how agents receive and adapt to external surroundings, including various forms of feedback and stimuli. In LLM-MAS, the environment serves as a dynamic information source that shapes agent behavior and decision-making. Unlike communication with other agents, which often involves explicit information transfer, communication with the environment primarily involves interpreting external signals and adjusting actions accordingly.

The environment can encompass a range of factors, from physical conditions~\cite{embodied_agents} to fluctuations system states and external data inputs. When the environment shift, agents must incorporate new contexts into their decision-making. For instance, an agent operating in a dynamic system where traffic conditions, market trends, or user preferences change frequently, has to adjust its reasoning and actions accordingly.

In LLM-MAS, agents use their language models to interpret environmental signals through natural language processing and contextual understanding. When agents receive environmental feedback, they integrate this new information into their reasoning processes. This feedback loop enables agents to adapt strategies dynamically, re-evaluate decisions, and response appropriately as conditions evolve.


\subsubsection{Communication with Human}
Communication with humans entails the interactions between agents and human participants. While LLM-MAS often focus on agent-to-agent communication, introducing humans as communication objects adds another layer of complexity. Human participants may issue commands, offer feedback, or request specific actions, requiring agents to interpret and respond appropriately in natural language.

LLM-based agents excel in this domain because of their advanced NLP abilities, enabling them to interpret human input, grasp user intent, and generate human-like responses that maintain conversational flow. For instance, in PeerGPT~\cite{peergpt}, agents act as both team moderators and participants in collaborative learning with children, continuously processing children’s words and actions to refine the agents’ behavior. This highlights how LLM-MAS effectively integrate with human-driven processes.

\subsection{Communication Content}
Communication content refers to the actual information being exchanged between agents, which may be explicit or implicit. The nature of this content influences how agents interpret and act upon messages. In LLM-MAS, content can be explicit, where information is conveyed clearly and structurally, or implicit, where meaning is inferred from actions, feedback, and environmental signals.

\subsubsection{Explicit communication}
Explicit communication entails direct information exchange, with clearly defined and easily interpretable meaning. Such clarity ensures that both agents and humans can comprehend the content unambiguously, proving especially useful for tasks demanding precise coordination or information sharing.
\paragraph{Natural Language:}
Natural language reamins the most common form of explicit communication in LLM-MAS. Leveraging LLMs, agents can generate and interpret messages in a human-readable formats, thereby enabling seamless interactions. These exchanges range from basic commands to advanced conversations where agents share detailed observations, propose ideas, or explain reasoning.Such flexibility empowers agents to convey rich, context-sensitive information and adjust language to the required level of formality or specificity.
\paragraph{Code and Structured Information:}
Besides natural language, explicit communication may encompass code and structured data. Such formats prove invaluable when agents must exchange precise instructions, configuration details, or factual information in a standardized manner. Structured communication is especially critical for planning, data retrieval, or algorithmic problem-solving, where clarity and consistency are paramount

\subsubsection{Implicit communication}
Implicit communication, on the other hand, occurs when agents convey information indirectly, through their actions or environmental cues, rather than through explicit statements. Implicit communication relies on agents interpreting contextual cues or feedback, often without a direct message being sent. This form of communication can be more subtle, but it is equally important in LLM-MAS where agents must be able to infer intent, recognize environmental changes, or adjust based on feedback.

\paragraph{Behavioral Feedback:}
Behavioral feedback arises when agents communicate via actions or behavioral changes, thereby signaling information to others—or themselves—without explicit verbal cues. In LLM-MAS, these signals become part of a continuous interaction. For instance, if an agent revises its strategy in reaction to another agent’s behavior, it implicitly conveys that the prior context has been processed and addressed.
This feedback loop facilitates dynamic adaptation of strategies, enabling agents to align with system goals or correct their course based on observed outcomes. In evolving environments or collaborative tasks, behavioral feedback can serve as a potent mechanism for shared understanding and effective coordination.

\paragraph{Environmental Signal:}
Environmental signals represent another implicit communication channel, where environmental changes provide indirect feedback that shapes agent decisions and behavior. Such signals may include sensor data, system status updates, or contextual shifts relevant to an agent’s tasks. In LLM-MAS, agents are equipped to parse these signals and incorporate them into their continuous decision-making processes.


\section{Challenges and Opportunities}
As LLM-MAS continue to garner increasing attention in both research and application domains, their further development faces several significant challenges and emerging opportunities. We analyze several key challenges and research directions based on the content of the article.
\subsection{Designing Better Communication Architectures}
The communication architecture is the foundational component of LLM-MAS, as it dictates how agents exchange information and coordinate actions. As task complexity increases, traditional communication architectures may no longer suffice. Therefore, the design of hybrid architectures is expected to be a key focus of future research.

With more complex structures, the number of agents increases, leading to greater demands on computational resources. A major challenge lies in developing communication paradigms that are both efficient and scalable, while also optimizing the allocation of computational resources. Concurrently, the increasing volume of internal system information poses another challenge. Ensuring that agents correctly interpret and understand this information, while minimizing the risk of hallucinations or misunderstandings, will be a crucial area of investigation.
\subsection{Advancing Research on Agent Competition}
In a competitive environment, agents can develop more complex strategies, improve decision-making, and promote innovative behaviors by employing techniques such as game theory. However, a key challenge lies in balancing competition and cooperation, as excessive competition may lead to inefficiency or instability. Future research can focus on finding the optimal balance between competition and cooperation, developing scalable competition strategies, and exploring how to safely and effectively integrate competition into real-world applications.
\subsection{Communicate Multimodal Content}
With the development of large multimodal models, agents in LLM-MAS should not be limited to text-based communication. Communication of multimodal content (text, images, audio, and video) should also be considered.This expansion into multimodal content enables more natural and context-aware interactions, thereby enhancing agents' adaptability and decision-making capabilities.

However, there are some challenges to integrating multimodal content. A major issue is how to effectively present and coordinate different modalities in a coherent way that is comprehensible to all agents. For example, how can visual inputs such as images be effectively combined with textual information to form a shared understanding? In addition, agents not only have to process these different modalities, but also communicate them effectively to one another. Future research should focus on improving the fusion of multimodal data and designing stronger agents in key components for handing multimodal content. By addressing these challenges, multimodal communication can significantly enhance the capabilities of LLM-MAS, enabling it to handle more complex and dynamic tasks.
\subsection{Communication Security}
As LLM-MAS becomes more complex and integrated into real-world applications, the security of its communication channels becomes a critical issue. LLM-MAS agents exchange diverse information, including sensitive data and mission-critical instructions. Ensuring the confidentiality, integrity, and authenticity of this communication is essential for maintaining the system's overall security and functionality.

One of the main challenges of communication security is protection against malicious attacks such as eavesdropping, data tampering, or spoofing. Since LLM is capable of generating and interpreting large amounts of information, a malicious attacker may exploit vulnerabilities in the communication protocol to inject misleading or harmful content. Furthermore, as agents interact in an increasingly open environment, the risk of unauthorized access to communication channels and data increases, making secure communication mechanisms even more important. Future research should focus on developing encryption and authentication protocols tailored for decentralized multi-agent environments. Another key challenge is integrating secure communication protocols that can adapt to the dynamic nature of LLM-MAS, where agents frequently join or leave the system.

By addressing these security challenges, we can enable LLM-MAS to operate safely in scenarios that require higher security, such as autonomous vehicles and healthcare. 