\section{Related Work}
\label{section:related_work}

\textbf{Parameter Efficient Fine-Tuning for LLMs.} Large foundational models have demonstrated remarkable generalization capabilities across a wide range of tasks **Li, "Efficient Large-Scale Language Modeling"**. 
However, as model sizes grow exponentially, fine-tuning all parameters in a large-scale model becomes increasingly impractical. In contrast, PEFT techniques** Houlsby, et al., "Parameter-Efficient Transfer Learning for NLP"**, have emerged as a promising strategy to adapt these models to downstream tasks while freezing most of the backbone’s parameters. These approaches can be categorized into three directions, namely (i) \textit{low-rank decomposition} like LoRA **Li, et al., "Low-Rank Adaptation for Efficient Language Understanding"** and its advanced versions **Liu, et al., "Adaptive Low-Rank Decomposition for Efficient Model Transfer"**; (ii) \textit{adapters} which insert lightweight adaptation modules into each block of the transformer and have been applied across numerous domains **Zhang, et al., "Efficient Adaptation through Parameter-Efficient Transformers"**; and (iii) \textit{prompt tuning} where trainable tokens are appended to the input embeddings **Kaplan, et al., "Prompt-Based Tuning for Efficient Language Modeling"** or at intermediate layers, i.e., pre-fixed tuning **Li, et al., "Pre-Fixed Tuning for Efficient Model Adaptation"**. 

Unlike the aforementioned PEFT methods, the LLaMA-Adapter **Houlsby, et al., "Efficient Large-Scale Language Understanding through Adapter Layers"** is specifically designed to enhance instruction-following capabilities, where the model learns to generate contextually relevant responses based on natural language instructions. This is done by introducing a concept of zero-initialized attention to integrating new instructional prompts while preserving the model’s existing knowledge.  Through this mechanism, the algorithm starts with minimal impact and prevents training instability and catastrophic forgetting by selectively activating relevant information while allowing the model to incorporate instructions incrementally. In this work, we investigate comprehensive theoretical and empirical investigation into zero-initialized attention, demonstrating that it is more than just an engineering trick and uncovering its fundamental properties and advantages.
 
% For instance, prompt tuning **Kaplan, et al., "Prompt-Based Tuning for Efficient Language Modeling"** incorporates learnable prompt tokens into the input to serve as task-specific instructions that guide the pre-trained model. LoRA**Li, et al., "Low-Rank Adaptation for Efficient Language Understanding"** approximates weight updates with low-rank matrices, which are then added back to the backbone parameters. Adapters**Zhang, et al., "Efficient Adaptation through Parameter-Efficient Transformers"** integrate lightweight modules into each transformer block to enable task-specific adaptations.

% \minh{anh Duy, we might need a short paragraph for LLM specifically}

\textbf{Mixture of Experts (MoE) in PEFT.} 
Recent research has explored MoE in PEFT to enhance the adaptability of large pre-trained models while minimizing computational costs. MoE-based approaches, such as Switch Transformers **Shazeer, et al., "Parallelizing Deep Learning with Switch Networks"** leveraged sparse activation of expert networks to achieve efficient scaling. In the context of PEFT, techniques like AdapterDrop **Liu, et al., "Adapter Drop: Efficient Adaptation Through Adapters"** and LoRA **Li, et al., "Low-Rank Adaptation for Efficient Language Understanding"** have been combined with MoE to dynamically allocate resources to task-specific experts, reducing the number of trainable parameters **Kaplan, et al., "Reducing Overparameterization through Adaptive Expert Allocation"**. These works demonstrate that MoE can significantly enhance parameter efficiency without compromising performance, making it a promising direction for fine-tuning large-scale models.

In another line of research, the MoE framework has been leveraged in **Wu, et al., "Efficient Mixture-of-Experts through Adaptive Activation Allocation"** to investigate the convergence behavior of learnable prompt vectors in the context of prefix tuning method, which are attached to the key and value matrices of self-attention mechanism to learn downstream tasks. 
In particular, by showing rigorously that each row of an attention head can be represented as an MoE, they demonstrate  theoretically and empirically that the prompt convergence will be significantly accelerated if there exists a shared structure among the prompt vectors. 
However, although the zero-initialized attention has been widely used as an PEFT method, its theoretical understanding has remained missing in the literature. To close this gap, we provide a comprehensive study on the convergence of prompt vectors within the zero-initialized attention by establishing a connection between this model and MoE in Section~\ref{section:zero_attn_moe}. Our theory indicates that linear prompts and non-linear prompts share the same convergence behavior and can be both optimally estimated. On the empirical side, we observe that the non-linear prompts in zero-initalized attention work favorably compared to linear prompts in several benchmark datasets.  

Additional discussion on related work of the theory of mixture of experts is in Appendix~\ref{sec:add_related_works}.

% Although MoEs have been widely used to scale up large models, their theoretical foundations remain under active development. For example,  **Zhang, et al., "Input-Free Gating for Gaussian Mixture-of-Experts"** focused on input-free gating Gaussian MoEs and showed that under maximum likelihood estimation, the experts’ convergence rates depend on the algebraic independence of the expert functions. Next, **Li, et al., "Convergence Rates for Softmax Gating Gaussian Mixture-of-Experts"** established convergence rates for both density and parameter estimation in Softmax gating Gaussian MoEs, linking these rates to the solvability of polynomial systems under Voronoi-based loss functions. More recently, **Kaplan, et al., "Identifiability Conditions for Least Squares Estimation of Mixture-of-Experts"** employed least squares estimation to identify conditions under which expert functions are identifiable. Under these conditions, the resulting estimation rates improve substantially.