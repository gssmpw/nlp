[
  {
    "index": 0,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "touvron2023llama1",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      },
      {
        "key": "karimi2021compacter",
        "author": "Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian",
        "title": "Compacter: Efficient low-rank hypercomplex adapter layers"
      },
      {
        "key": "gao2024clip",
        "author": "Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu",
        "title": "Clip-adapter: Better vision-language models with feature adapters"
      },
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      },
      {
        "key": "li2024graphadapter",
        "author": "Li, Xin and Lian, Dongze and Lu, Zhihe and Bai, Jiawang and Chen, Zhibo and Wang, Xinchao",
        "title": "Graphadapter: Tuning vision-language models with dual knowledge graph"
      },
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-tuning: Optimizing continuous prompts for generation"
      },
      {
        "key": "zhou2022learning",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Learning to prompt for vision-language models"
      },
      {
        "key": "mangrulkar2022peft",
        "author": "Mangrulkar, Sourab and Gugger, Sylvain and Debut, Lysandre and Belkada, Younes and Paul, Sayak and Bossan, B",
        "title": "Peft: State-of-the-art parameter-efficient fine-tuning methods"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "karimi2021compacter",
        "author": "Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian",
        "title": "Compacter: Efficient low-rank hypercomplex adapter layers"
      },
      {
        "key": "zhang2023adalora",
        "author": "Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo",
        "title": "AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "gao2024clip",
        "author": "Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu",
        "title": "Clip-adapter: Better vision-language models with feature adapters"
      },
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      },
      {
        "key": "li2024graphadapter",
        "author": "Li, Xin and Lian, Dongze and Lu, Zhihe and Bai, Jiawang and Chen, Zhibo and Wang, Xinchao",
        "title": "Graphadapter: Tuning vision-language models with dual knowledge graph"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The power of scale for parameter-efficient prompt tuning"
      },
      {
        "key": "zhou2022learning",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Learning to prompt for vision-language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2021p",
        "author": "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie",
        "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks"
      },
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-tuning: Optimizing continuous prompts for generation"
      },
      {
        "key": "shi2023dept",
        "author": "Shi, Zhengxiang and Lipani, Aldo",
        "title": "Dept: Decomposed prompt tuning for parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2024llama",
        "author": "Zhang, Renrui and Han, Jiaming and Liu, Chris and Zhou, Aojun and Lu, Pan and Qiao, Yu and Li, Hongsheng and Gao, Peng",
        "title": "LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The power of scale for parameter-efficient prompt tuning"
      },
      {
        "key": "li2021prefix",
        "author": "Li, Xiang Lisa and Liang, Percy",
        "title": "Prefix-tuning: Optimizing continuous prompts for generation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ruckle2020adapterdrop",
        "author": "R{\\\"u}ckl{\\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna",
        "title": "Adapterdrop: On the efficiency of adapters in transformers"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2024mixlora",
        "author": "Li, Dengchun and Ma, Yingzi and Wang, Naizheng and Cheng, Zhiyuan and Duan, Lei and Zuo, Jie and Yang, Cal and Tang, Mingjie",
        "title": "Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts"
      },
      {
        "key": "chen2024llava",
        "author": "Chen, Shaoxiang and Jie, Zequn and Ma, Lin",
        "title": "Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms"
      },
      {
        "key": "chen2023adamv",
        "author": "Chen, Tianlong and Chen, Xuxi and Du, Xianzhi and Rashwan, Abdullah and Yang, Fan and Chen, Huizhong and Wang, Zhangyang and Li, Yeqing",
        "title": "Adamv-moe: Adaptive multi-task vision mixture-of-experts"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "le2024mixture",
        "author": "Le, Minh and Nguyen, An and Nguyen, Huy and Nguyen, Trang and Pham, Trang and Van Ngo, Linh and Ho, Nhat",
        "title": "Mixture of Experts Meets Prompt-Based Continual Learning"
      },
      {
        "key": "le2024revisiting",
        "author": "Le, Minh and Nguyen, Chau and Nguyen, Huy and Tran, Quyen and Le, Trung and Ho, Nhat",
        "title": "Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "nguyen2023demystifying",
        "author": "Nguyen, Huy and Nguyen, TrungTin and Ho, Nhat",
        "title": "Demystifying softmax gating function in Gaussian mixture of experts"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "nguyen2024least",
        "author": "Nguyen, Huy and Ho, Nhat and Rinaldo, Alessandro",
        "title": "On least squares estimation in softmax gating mixture of experts"
      },
      {
        "key": "nguyen2024sigmoid",
        "author": "Nguyen, Huy and Ho, Nhat and Rinaldo, Alessandro",
        "title": "Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts"
      }
    ]
  }
]