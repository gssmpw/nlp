\section{Related Work}
\label{section:related_work}

\textbf{Parameter Efficient Fine-Tuning for LLMs.} Large foundational models have demonstrated remarkable generalization capabilities across a wide range of tasks ____. 
However, as model sizes grow exponentially, fine-tuning all parameters in a large-scale model becomes increasingly impractical. In contrast, PEFT techniques____ have emerged as a promising strategy to adapt these models to downstream tasks while freezing most of the backbone’s parameters. These approaches can be categorized into three directions, namely (i) \textit{low-rank decomposition} like LoRA ____ and its advanced versions ____; (ii) \textit{adapters} which insert
lightweight adaptation modules into each block of the transformer and have been applied across
numerous domains ____; and (iii) \textit{prompt tuning} where trainable tokens are appended to the input embeddings ____ or at intermediate layers, i.e., pre-fixed tuning ____. 

Unlike the aforementioned PEFT methods, the LLaMA-Adapter ____ is specifically designed to enhance instruction-following capabilities, where the model learns to generate contextually relevant responses based on natural language instructions. This is done by introducing a concept of zero-initialized attention to integrating new instructional prompts while preserving the model’s existing knowledge.  Through this mechanism, the algorithm starts with minimal impact and prevents training instability and catastrophic forgetting by selectively activating relevant information while allowing the model to incorporate instructions incrementally. In this work, we investigate comprehensive theoretical and empirical investigation into zero-initialized attention, demonstrating that it is more than just an engineering trick and uncovering its fundamental properties and advantages.
 
% For instance, prompt tuning ____ incorporates learnable prompt tokens into the input to serve as task-specific instructions that guide the pre-trained model. LoRA____ approximates weight updates with low-rank matrices, which are then added back to the backbone parameters. Adapters____ integrate lightweight modules into each transformer block to enable task-specific adaptations.

% \minh{anh Duy, we might need a short paragraph for LLM specifically}

\textbf{Mixture of Experts (MoE) in PEFT.} 
Recent research has explored MoE in PEFT to enhance the adaptability of large pre-trained models while minimizing computational costs. MoE-based approaches, such as Switch Transformers ____ leveraged sparse activation of expert networks to achieve efficient scaling. In the context of PEFT, techniques like AdapterDrop ____ and LoRA ____ have been combined with MoE to dynamically allocate resources to task-specific experts, reducing the number of trainable parameters ____. These works demonstrate that MoE can significantly enhance parameter efficiency without compromising performance, making it a promising direction for fine-tuning large-scale models.

In another line of research, the MoE framework has been leveraged in ____ to investigate the convergence behavior of learnable prompt vectors in the context of prefix tuning method, which are attached to the key and value matrices of self-attention mechanism to learn downstream tasks. 
In particular, by showing rigorously that each row of an attention head can be represented as an MoE, they demonstrate  theoretically and empirically that the prompt convergence will be significantly accelerated if there exists a shared structure among the prompt vectors. 
However, although the zero-initialized attention has been widely used as an PEFT method, its theoretical understanding has remained missing in the literature. To close this gap, we provide a comprehensive study on the convergence of prompt vectors within the zero-initialized attention by establishing a connection between this model and MoE in Section~\ref{section:zero_attn_moe}. Our theory indicates that linear prompts and non-linear prompts share the same convergence behavior and can be both optimally estimated. On the empirical side, we observe that the non-linear prompts in zero-initalized attention work favorably compared to linear prompts in several benchmark datasets.  

Additional discussion on related work of the theory of mixture of experts is in Appendix~\ref{sec:add_related_works}.

% Although MoEs have been widely used to scale up large models, their theoretical foundations remain under active development. For example,  focused on input-free gating Gaussian MoEs and showed that under maximum likelihood estimation, the experts’ convergence rates depend on the algebraic independence of the expert functions. Next, ____ established convergence rates for both density and parameter estimation in Softmax gating Gaussian MoEs, linking these rates to the solvability of polynomial systems under Voronoi-based loss functions. More recently, ____ employed least squares estimation to identify conditions under which expert functions are identifiable. Under these conditions, the resulting estimation rates improve substantially.