@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{chen2023adamv,
  title={Adamv-moe: Adaptive multi-task vision mixture-of-experts},
  author={Chen, Tianlong and Chen, Xuxi and Du, Xianzhi and Rashwan, Abdullah and Yang, Fan and Chen, Huizhong and Wang, Zhangyang and Li, Yeqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17346--17357},
  year={2023}
}

@article{chen2024llava,
  title={Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms},
  author={Chen, Shaoxiang and Jie, Zequn and Ma, Lin},
  journal={arXiv preprint arXiv:2401.16160},
  year={2024}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{gao2024clip,
  title={Clip-adapter: Better vision-language models with feature adapters},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={International Journal of Computer Vision},
  volume={132},
  number={2},
  pages={581--595},
  year={2024},
  publisher={Springer}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{karimi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@article{le2024mixture,
  title={Mixture of Experts Meets Prompt-Based Continual Learning},
  author={Le, Minh and Nguyen, An and Nguyen, Huy and Nguyen, Trang and Pham, Trang and Van Ngo, Linh and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={38},
    year={2024},
}

@inproceedings{le2024revisiting,
    title={Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts},
    author={Le, Minh and Nguyen, Chau and Nguyen, Huy and Tran, Quyen and Le, Trung and Ho, Nhat},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{li2024graphadapter,
  title={Graphadapter: Tuning vision-language models with dual knowledge graph},
  author={Li, Xin and Lian, Dongze and Lu, Zhihe and Bai, Jiawang and Chen, Zhibo and Wang, Xinchao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2024mixlora,
  title={Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts},
  author={Li, Dengchun and Ma, Yingzi and Wang, Naizheng and Cheng, Zhiyuan and Duan, Lei and Zuo, Jie and Yang, Cal and Tang, Mingjie},
  journal={arXiv preprint arXiv:2404.15159},
  year={2024}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{mangrulkar2022peft,
  title={Peft: State-of-the-art parameter-efficient fine-tuning methods},
  author={Mangrulkar, Sourab and Gugger, Sylvain and Debut, Lysandre and Belkada, Younes and Paul, Sayak and Bossan, B},
  journal={URL: https://github. com/huggingface/peft},
  year={2022}
}

@article{nguyen2023demystifying,
  title={Demystifying softmax gating function in Gaussian mixture of experts},
  author={Nguyen, Huy and Nguyen, TrungTin and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={4624--4652},
  year={2023}
}

@article{nguyen2024least,
  title={On least squares estimation in softmax gating mixture of experts},
  author={Nguyen, Huy and Ho, Nhat and Rinaldo, Alessandro},
  journal={arXiv preprint arXiv:2402.02952},
  year={2024}
}

@article{nguyen2024sigmoid,
  title={Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts},
  author={Nguyen, Huy and Ho, Nhat and Rinaldo, Alessandro},
  journal={arXiv preprint arXiv:2405.13997},
  year={2024}
}

@article{ruckle2020adapterdrop,
  title={Adapterdrop: On the efficiency of adapters in transformers},
  author={R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2010.11918},
  year={2020}
}

@article{shi2023dept,
  title={Dept: Decomposed prompt tuning for parameter-efficient fine-tuning},
  author={Shi, Zhengxiang and Lipani, Aldo},
  journal={arXiv preprint arXiv:2309.05173},
  year={2023}
}

@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

@inproceedings{zhang2024llama,
  title={LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Zhou, Aojun and Lu, Pan and Qiao, Yu and Li, Hongsheng and Gao, Peng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

